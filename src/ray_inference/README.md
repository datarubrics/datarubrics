# Running inference

## Serving the infer server - VLLM

1. Modify the serving script `serve_infer_vllm.sh` to have the right TP size, model path, etc.
2. Serve the vllm server with `bash serve_infer_vllm.sh`
3. Look into `run_infer.sh` and confirm the arguments. You want to run this script when you're in the repo's root directory. The input folder is folder generated by `src/data_generation`. Flush size is how many iterations before we dump results to the `jsonl` file.
4. Look into `model_config` to change the infer args
5. Look into `src/ray_inference/models.py`, `_initialize_sglang_urls()`, and ensure that the API endpoints match up (port number and base address) with the serve script
6. Run inference with `bash ./src/ray_inference/run_infer.sh`

## Serving the infer server - SGLANG (Recommended)

Basically the same steps as above, but instead of running `bash serve_infer_vllm.sh`, run `bash serve_infer_sglang.sh`

Remember to double check on `src/ray_inference/models.py`, `_initialize_sglang_urls()`, to ensure that the ports line up. 