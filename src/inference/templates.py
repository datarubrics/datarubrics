from typing import Type, Literal, Any, Dict
from enum import Enum
from pydantic import BaseModel, ConfigDict, Field
import os
import json
import outlines

from .utils import *
from .constants import *

class Rubric(BaseModel):
    metric_name: str
    metric_description: str
    options: Dict[str, str]

####################################################
#############      SOURCE RUBRICS      #############
####################################################

SOURCE_TEXT_RUBRIC = Rubric(
    metric_name="Text Source",
    metric_description="This metric categorizes the origin of the text data within a dataset, focusing on whether the content is authored by humans, generated by models, or of unknown provenance. This metric allows multiple labels, except for 'Unknown Origin' and 'N/A', which must be used as single, exclusive labels. Each label indicates whether it applies to the dataset, with supporting evidence from the paper.",
    options={
        "Human Generated": "The text is entirely produced or captured by humans, e.g. original writing and human-performed translations.",
        "Model Generated": "The text is synthetically produced by AI models, e.g. model-generated translations.",
        "Unknown Origin": "The source of the text is not specified, not reported, or derived from web-crawl data with unclear provenance.",
        "N/A": "The dataset does not contain any text data."
    }
)

SOURCE_IMAGE_RUBRIC = Rubric(
    metric_name="Image Source",
    metric_description="This metric categorizes the origin of the image data within a dataset, focusing on whether the contents are human-captured visuals, synthetic imagery, or data of unclear provenance. This metric allows multiple labels, except for 'Unknown Origin' and 'N/A', which must be used as single, exclusive labels. Each label indicates whether it applies to the dataset, with supporting evidence from the paper.",
    options={
        "Human Generated": "The image is entirely produced by humans, e.g. photographs and hand-drawn images.",
        "Model Generated": "The image is synthetically produced by AI models, e.g. model-generated images.",
        "Unknown Origin": "The source of the image is not specified, not reported, or derived from web-crawl data with unclear provenance.",
        "N/A": "The dataset does not contain any image data."
    }
)

SOURCE_VIDEO_RUBRIC = Rubric(
    metric_name="Video Source",
    metric_description="This metric categorizes the origin of the video data within a dataset, focusing on whether the contents are human-captured footages, synthetic videos, or data of unclear provenance. This metric allows multiple labels, except for 'Unknown Origin' and 'N/A', which must be used as single, exclusive labels. Each label indicates whether it applies to the dataset, with supporting evidence from the paper.",
    options={
        "Human Generated": "The video is entirely produced or captured by humans, e.g. live-action recordings, home videos, documentary footages.",
        "Model Generated": "The video is synthetic, i.e. generated or heavily manipulated by AI models, including animations that are heavily edited or generated by AI.",
        "Unknown Origin": "The source of the video is not specified, not reported, or derived from web-crawl data with unclear provenance.",
        "N/A": "The dataset does not contain any video data."
    }
)

SOURCE_AUDIO_RUBRIC = Rubric(
    metric_name="Audio Source",
    metric_description="This metric categorizes the origin and nature of the audio data within a dataset, distinguishing between scripted and spontaneous human speech, conversational recordings, synthetic audio, and cases where the source is unknown. This metric allows multiple labels, except for 'Unknown Origin' and 'N/A', which must be used as single, exclusive labels. Each label indicates whether it applies to the dataset, with supporting evidence from the paper.",
    options={
        "Human Speech (Read)": "Audio is generated by humans reading scripted or prepared text.",
        "Human Speech (Conversation)": "Audio is sourced from natural human conversations, such as dialogues or discussions.",
        "Human Speech (Spontaneous)": "Audio consists of spontaneous human speech, not read from a script.",
        "Non-Human Sound": "Audio is a recording of non-human natural sounds, such as animals, weather, or ambient environments.",
        "Model Generated": "Audio is synthetically generated by AI models, e.g. TTS or model-generated effects.",
        "Unknown Origin": "The source of the audio is unclear, unreported, or derived from unidentified or unattributed sources (e.g., large-scale web crawl).",
        "N/A": "The dataset does not contain any audio data."
    }
)

SOURCE_GRAPH_RUBRIC = Rubric(
    metric_name="Graph Source",
    metric_description="This metric categorizes the origin of the graph data within a dataset, focusing on whether the content is created by humans, generated by models, or of unknown provenance. This metric allows multiple labels, except for 'Unknown Origin' and 'N/A', which must be used as single, exclusive labels. Each label indicates whether it applies to the dataset, with supporting evidence from the paper.",
    options={
        "Human Generated": "The graph is manually created or curated by humans, such as ontologies, citation graphs, or social networks constructed from surveys or human annotations.",
        "Model Generated": "The graph is synthetically produced, either by AI systems or codes.",
        "Unknown Origin": "The source of the graph is not specified, not reported, or derived from some data collections with unclear provenance.",
        "N/A": "The dataset does not contain any graph data."
    }
)

SOURCE_TABULAR_RUBRIC = Rubric(
    metric_name="Tabular Source",
    metric_description="This metric categorizes the origin of the tabular data within a dataset, focusing on whether the content is created or compiled by humans, generated by models, or of unknown provenance. This metric allows multiple labels, except for 'Unknown Origin' and 'N/A', which must be used as single, exclusive labels. Each label indicates whether it applies to the dataset, with supporting evidence from the paper.",
    options={
        "Human Generated": "The tabular data is entirely compiled, filled, or annotated by human, e.g. spreadsheets, survey results, and census data.",
        "Model Generated": "The tabular data is produced by AI models, e.g. outputs from text-to-table conversion, summarization, and produced through some generation tasks.",
        "Unknown Origin": "The source of the tabular data is not specified, not reported, or derived from web-crawl data with unclear provenance.",
        "N/A": "The dataset does not contain any tabular data."
    }
)

SOURCE_TIME_OR_SIGNAL_RUBRIC = Rubric(
    metric_name="Time or Signal Source",
    metric_description="This metric categorizes the origin of the time-series or signal-based data within a dataset, focusing on whether the content is compiled by humans, generated by models, or of unknown provenance. This metric allows multiple labels, except for 'Unknown Origin' and 'N/A', which must be used as single, exclusive labels. Each label indicates whether it applies to the dataset, with supporting evidence from the paper.",
    options={
        "Human Generated": "The time series or signal-based data is entirely logged by humans or captured by real-world sensors and instruments, e.g. handwritten logs, ECGs, IoT sensor data, or market tickers.",
        "Model Generated": "The time series or signal-based data is synthetically produced by AI models, e.g. synthetic forecasts or model-based extrapolations.",
        "Unknown Origin": "The source of the time series or signal-based data is not specified, not reported, or derived from web-crawl data with unclear provenance.",
        "N/A": "The dataset does not contain any time series or signal-based data."
    }
)

####################################################
##########      ANNOTATIONS RUBRICS      ###########
####################################################

ANNOTATIONS_HUMAN_ANNOT_GUIDELINES_RUBRIC = Rubric(
    metric_name="Human Annotations Guidelines",
    metric_description="This metric evaluates the presence and quality of human annotation guidelines provided for data labeling. It focuses on whether clear scoring rubrics and examples are included, particularly for subjective annotation tasks, which is essential for ensuring consistency and reproducibility. This metric allows multiple labels, except for 'N/A', which must be used as single, exclusive labels. Each label indicates whether it applies to the dataset, with supporting evidence from the paper.",
    options={
        "Has Instructions": "Human annotation guidelines include detailed instructions.",
        "Has Rubrics": "Human annotation guidelines include detailed rubrics.",
        "Has Examples": "Human annotation guidelines include clear examples.",
        "N/A": "No human annotation guidelines are provided"
    }
)

ANNOTATIONS_MODEL_ANNOT_GUIDELINES_RUBRIC = Rubric(
    metric_name="Model-based Annotations Guidelines",
    metric_description="This metric evaluates the presence and quality of model-based annotation guidelines provided for data labeling. It focuses on whether clear scoring rubrics and examples are included, particularly for subjective annotation tasks, which is essential for ensuring consistency and reproducibility. This metric allows multiple labels, except for 'N/A', which must be used as single, exclusive labels. Each label indicates whether it applies to the dataset, with supporting evidence from the paper.",
    options={
        "Has Instructions": "Model annotation guidelines include detailed instructions.",
        "Has Rubrics": "Model annotation guidelines include detailed rubrics.",
        "Has Examples": "Model annotation guidelines include clear examples.",
        "N/A": "No model annotation guidelines are provided"
    }
)

ANNOTATIONS_QUALITY_ASSURANCE_RUBRIC = Rubric(
    metric_name="Quality Assurance",
    metric_description="""
        This metric assesses the rigor and reliability of the quality assurance (QA) process used to validate dataset annotations or content. It captures whether QA was performed by experts, non-experts, machines, or not at all, and whether the process is transparently reported. This helps determine the overall trustworthiness and consistency of the data.
        Multiple labels are allowed, except for mutually exclusive categories. Specifically:
            - 'Single Human Expert' and 'Multiple Human Experts' cannot be both true.
            - 'Single Human Non-Expert' and 'Multiple Human Non-Experts' cannot be both true.
            - 'N/A' is a single-label option and cannot be combined with others.
        Each label indicates whether it applies to the dataset, with supporting evidence from the paper.""",
    options={
        "Single Human Expert": "Quality assurance is conducted by a single human annotator who is either a subject matter expert or a member of the target demographic. If there is no information about the annotator, then the annotator is not an expert.",
        "Multiple Human Experts": "Quality assurance is performed by multiple human annotators with subject matter expertise or belonging to the target demographic. If there is no information about the annotators, then the annotators are not experts.",
        "Single Human Non-Expert": "Quality assurance is conducted by a single human annotator without subject matter expertise.",
        "Multiple Human Non-Experts": "Quality assurance is conducted by multiple human annotators who do not possess subject matter expertise.",
        "Automatic Verification": "Quality assurance is conducted through the automated verification of code or formulas using algorithmic or rule-based techniques.",
        "AI Model": "Quality assurance is performed by an AI model as a judge.",
        "N/A": "No quality assurance process is applied, or none is documented."
    }
)

ANNOTATIONS_DATA_ANNOT_RUBRIC = Rubric(
    metric_name="Data Annotation",
    metric_description="""
        This metric assesses the annotation process used for the dataset, focusing on who performed the annotations and their level of expertise.
        Multiple labels are allowed, except for mutually exclusive categories. Specifically:
            - 'Single Human Expert' and 'Multiple Human Experts' cannot be both true.
            - 'Single Human Non-Expert' and 'Multiple Human Non-Experts' cannot be both true.
            - 'N/A' is a single-label option and cannot be combined with others.
        Each label indicates whether it applies to the dataset, with supporting evidence from the paper.""",
    options={
        "Single Human Expert": "Data annotation is conducted by a single human annotator who is either a subject matter expert or a member of the target demographic. If there is no information about the annotator, then the annotator is not an expert.",
        "Multiple Human Experts": "Data annotation is performed by multiple human annotators with subject matter expertise or belonging to the target demographic. If there is no information about the annotators, then the annotators are not experts.",
        "Single Human Non-Expert": "Data annotation is conducted by a single human annotator without subject matter expertise.",
        "Multiple Human Non-Experts": "Data annotation is conducted by multiple human annotators who do not possess subject matter expertise.",
        "AI Model with Verification": "Data annotation is performed by an AI model with verification using some human annotations.",
        "AI Model without Verification": "Data annotation is performed by an AI model without any verification of its quality.",
        "Automatic": "Data annotation is automatically done by a simulation or automatic process that is not an AI model.",
        "N/A": "No data annotation is applied, or none is documented."
    }
)

####################################################
############      UTILITY RUBRICS      #############
####################################################

UTILITY_DATA_NOVELTY_RUBRIC = Rubric(
    metric_name="Data Novelty",
    metric_description="This metric assesses the originality of the data based on how it was generated. Multiple labels may be selected, except for 'N/A', which is exclusive and cannot be combined with other options. Each label indicates whether it applies to the dataset, with supporting evidence from the paper.",
    options={
        "New Data from Human": "Original content created entirely from scratch by human contributors. It is not translated, adapted, or derived from pre-existing material.",
        "New Data from Model": "Original content generated entirely by AI or machine learning models without reference to or transformation of existing data.",
        "Human Translation": "Data produced by translating content from another language through human translators.",
        "Machine Translation": "Data generated by translating content from another language using machine translation systems.",
        "Collated": "Data collected or aggregated from existing sources without significant modification.",
        "Derived": "Data based on existing sources, with some modifications, transformations, or adaptations applied.",
        "N/A": "The data source or method of generation is not specified or documented."
    }
)

UTILITY_TASK_UTILITY_RUBRIC = Rubric(
    metric_name="Task Utility",
    metric_description="This metric identifies how the dataset is used within the machine learning pipeline. Understanding its utility helps clarify the dataset's purpose, relevance, and integration into model development or evaluation workflows. Multiple labels may be selected, except for 'N/A', which is exclusive and cannot be combined with other options. Each label indicates whether it applies to the dataset, with supporting evidence from the paper.",
    options={
        "Pre-Training": "The dataset is used exclusively for pre-training large models on general patterns, typically in an unsupervised or self-supervised manner.",
        "Post-Training (Supervised Fine-tuning)": "The dataset is used to fine-tune a pre-trained model using supervised learning methods.",
        "Post-Training (RL-based Methods)": "The dataset is used for reinforcement learning post-training techniques such as RLHF.",
        "Evaluation": "The dataset is used exclusively for evaluation, benchmarking, or performance measurement.",
        "Analysis": "The dataset is used primarily for analyzing trends, patterns, or characteristics rather than training or evaluation.",
        "Knowledge Base": "The dataset serves as a knowledge base to augment models (e.g., through retrieval-augmented generation).",
        "N/A": "No practical usage of the dataset is described or demonstrated in the paper."
    }
)

UTILITY_HUMAN_LANG_RUBRIC = Rubric(
    metric_name="Human Language Coverage",
    metric_description="This metric categorizes the linguistic scope of the dataset, indicating how many and which types of languages are included. This helps assess the dataset's applicability to multilingual or language-specific research. This metric only allows single label. Each label indicates whether it applies to the dataset, with supporting evidence from the paper.",
    options={
        "Multilingual": "The dataset contains content with more than two human languages.",
        "Bilingual": "The dataset contains content with exactly two human languages.",
        "Monolingual (English)": "The dataset contains only English content.",
        "Monolingual (Non-English)": "The dataset contains content with exactly one language that is non-English.",
        "Unknown": "The language(s) used in the dataset are not specified or documented.",
        "N/A": "The dataset does not contain any human language."
    }
)

UTILITY_NON_HUMAN_LANG_RUBRIC = Rubric(
    metric_name="Non-Human Language Coverage",
    metric_description="This metric identifies and categorizes any non-human languages represented in the dataset. This metric allows multiple labels, except for 'Unknown Origin' and 'N/A', which must be used as single, exclusive labels. Each label indicates whether it applies to the dataset, with supporting evidence from the paper.",
    options={
        "Code / Programming Language": "The dataset includes programming or structured code-related content (e.g., Python, HTML, SQL, bytecode).",
        "Mathematical and Logical Notation": "The dataset contains mathematical or formal logical expressions or symbolic representations.",
        "Biological and Non-Human Communication Systems": "The dataset includes biological sequences or non-human communication (e.g., DNA, animal signals, chemical signaling).",
        "Constructed Language": "The dataset includes fictional or artificially created languages such as Klingon or Esperanto.",
        "Unknown": "The presence or type of non-human language in the dataset is not specified.",
        "N/A": "The dataset does not contain any non-human language content."
    }
)

UTILITY_DOCUMENTATION_RUBRIC = Rubric(
    metric_name="Documentation",
    metric_description="This metric evaluates the transparency and completeness of the dataset creation documentation, which is crucial for reproducibility, ethical assessment, and downstream usability.",
)

UTILITY_REPRODUCIBILITY_RUBRIC = Rubric(
    metric_name="Reproducibility",
    metric_description="This metric pertains to whether the code used for constructing the dataset is made publicly available or not for reproducibility.",
)

@outlines.prompt
def construct_judge_prompt(paper_text: str, rubric: Rubric, format: str):
    """
    ### INSTRUCTION
    Carefully evaluate the quality and characteristics of the **new datasets** introduced in the paper using the rubric provided below.

    Please follow these rules:
    - **Only assess new datasets** that are introduced by the authors. Do **not** evaluate any pre-existing datasets mentioned in the paper.
    - Base your judgments **strictly on the content of the paper**. Do **not** infer or speculate beyond what is explicitly stated.
    - Use the rubric definitions to guide your labeling. Provide clear references and reasoning wherever applicable.
    
    ### PAPER
    {{ paper_text }}

    ### RUBRIC
    Metric: {{ rubric.metric_name }}
    Description: {{ rubric.metric_description }}

    Options:
    {% for key, value in rubric.options.items() %}
    {{ key }}: {{ value }}
    {% endfor %}

    ### RESPONSE FORMAT
    Return a JSON response in the following format:
    
    {{ format }}
    
    ### RESPONSE
    """
    
@outlines.prompt
def construct_judge_nooption_prompt(paper_text: str, rubric: Rubric, format: str):
    """
    ### INSTRUCTION
    Carefully evaluate the quality and characteristics of the **new datasets** introduced in the paper using the rubric provided below.

    Please follow these rules:
    - **Only assess new datasets** that are introduced by the authors. Do **not** evaluate any pre-existing datasets mentioned in the paper.
    - Base your judgments **strictly on the content of the paper**. Do **not** infer or speculate beyond what is explicitly stated.
    - Use the rubric definitions to guide your labeling. Provide clear references and reasoning wherever applicable.
    
    ### PAPER
    {{ paper_text }}

    ### RUBRIC
    Metric: {{ rubric.metric_name }}
    Description: {{ rubric.metric_description }}

    ### RESPONSE FORMAT
    Return a JSON response in the following format:
    
    {{ format }}
    
    ### RESPONSE
    """