WORLD CUISINES : A Massive-Scale Benchmark for Multilingual and
Multicultural Visual Question Answering on Global Cuisines
Genta Indra Winata∗♠,1,2, Frederikus Hudi∗♠,2,3, Patrick Amadeus Irawan∗♠,2,4,
David Anugraha∗♠,5, Rifki Afina Putri∗♠,2,6, Yutong Wang♠,7, Adam Nohejl♠,3,
Ubaidillah Ariq Prathama♠,4, Nedjma Ousidhoum♠,8, Afifa Amriani9,
Anar Rzayev6, Anirban Das1, Ashmari Pramodya3, Aulia Adila7, Bryan Wilie10,
Candy Olivia Mawalim7, Ching Lam Cheng11, Daud Abolade12,13,
Emmanuele Chersoni14, Enrico Santus9, Fariz Ikhwantri9, Garry Kuwanto15,
Hanyang Zhao16, Haryo Akbarianto Wibowo17, Holy Lovenia2,
Jan Christian Blaise Cruz2,17, Jan Wira Gotama Putra9, Junho Myung6,
Lucky Susanto18, Maria Angelica Riera Machin3, Marina Zhukova19,
Michael Anugraha9, Muhammad Farid Adilazuarda2,17, Natasha Santosa20,
Peerat Limkonchotiwat2,21, Raj Dabre22, Rio Alexander Audino4,
Samuel Cahyawijaya2,23, Shi-Xiong Zhang1, Stephanie Yulia Salim7, Yi Zhou8,
Yinxuan Gui11, David Ifeoluwa Adelani♣,12,24,25,26, En-Shiun Annie Lee♣,5,27,
Shogo Okada♣,7, Ayu Purwarianti♣,2,4, Alham Fikri Aji♣,2,17,18, Taro Watanabe♣,3,
Derry Tanti Wijaya♣,15,18, Alice Oh♣,6, Chong-Wah Ngo♣,11,
1Capital One 2SEACrowd 3NAIST 4ITB 5UofT 6KAIST 7JAIST 8Cardiff University
9Independent 10HKUST 11SMU 12Masakhane 13University of Lagos 14HK PolyU
15Boston University 16Columbia University 17MBZUAI 18Monash University 19UCSB
20Tokyo Tech21AI Singapore 22NICT 23Cohere 24McGill 25MILA
26Canada CIFAR AI Chair 27Ontario Tech
♠Main Authors ♣Senior Authors
Abstract
Vision Language Models (VLMs) often strug-
gle with culture-specific knowledge, particu-
larly in languages other than English and in
underrepresented cultural contexts. To eval-
uate their understanding of such knowledge,
we introduce WORLD CUISINES , a massive-
scale benchmark for multilingual and multicul-
tural, visually grounded language understand-
ing. This benchmark includes a visual question
answering (VQA) dataset with text-image pairs
across 30 languages and dialects, spanning 9
language families and featuring over 1 million
data points, making it the largest multicultural
VQA benchmark to date. It includes tasks for
identifying dish names and their origins. We
provide evaluation datasets in two sizes (12k
and 60k instances) alongside a training dataset
(1 million instances). Our findings show that
while VLMs perform better with correct loca-
tion context, they struggle with adversarial con-
texts and predicting specific regional cuisines
and languages. To support future research, we
release a knowledge base with annotated food
entries and images along with the VQA data.
∗ These authors contributed equally. This is an open-
source project, and the work was done outside of their af-
filiations. Contacts: genta.winata@capitalone.com and
frederikus.hudi.fe7@naist.ac.jp.
Figure 1: Images of stuffed pasta and dumplings
from our dataset showcase a similar culinary concept
across different cultures: wrapping meat, dairy (such as
cheese), or vegetables in dough. These dishes can be
prepared in various ways, including pan-frying, deep-
frying, steaming, or boiling.
1 Introduction
Food is an essential medium for the exchange of re-
gional cultures, serving to connect diverse peoples
and traditions (Wahlqvist, 2007). Analyzing var-
ious culinary practices provides valuable insights
into the cultural values, historical narratives, and
social customs of the communities that produce and
consume these foods (Holtzman, 2006). Further-
more, food plays a significant role in shaping lan-
arXiv:2410.12705v4  [cs.CL]  8 Feb 2025

# VQA # Lang./Dialect† # Countries # Food Entries # Images Parallel Data License
FoodieQA (Li et al., 2024b) 659 2 1 60 389 × CC BY-NC-ND 4.0World Wide Dishes (Magomere et al., 2024) 765 131 63 765 301 × CC-BY 4.0xGQA (Pfeiffer et al., 2022) 12,578 8 8 N/A 398 ✓ CC-BY 4.0MaXM‡(Changpinyo et al., 2023) 2,142 7 7 N/A 335 × CustomEVJVQA (Nguyen et al., 2023) 33,790 3 1 N/A 4,909 × N/ACulturalVQA (Nayak et al., 2024) 2,378 1 11 N/A 2,328 × N/ASEA-VQA (Urailertprasert et al., 2024) 1,999 1 8 N/A 515 × CustomCVQA (Romero et al., 2024) 9,000 26 28 1,834 4,560 ✓ VariousIndiFoodVQA (Agarwal et al., 2024) 16,716 1 1 255 414 × N/A
WC-VQA 1,152,000 30 189 2,414 6,045 ✓ CC BY-SA 4.0
Table 1: Data statistics for WC-VQA compared to existing VQA datasets. The data samples are sourced from their
respective publications. ‡The reported numbers are based on their human-annotated test set. †This entry includes
the language variations we collected for all languages.
guage, which serves as a proxy for cultural knowl-
edge (Freedman, 2021). Food choices often reflect
intricate community histories, societal transforma-
tions, and both individual and collective identities,
thereby creating a rich tapestry of cultural expres-
sion (Almerico, 2014). The relationship between
culture and food is dynamic; both evolve in tan-
dem over time, resulting in the emergence of new
dishes that are influenced by historical culinary
traditions (Anderson, 2014).
As a result, similar food concepts can be found
across different countries, reflecting a shared hu-
man culinary heritage. Researchers use food as
a proxy to model and analyze cultural dynamics,
helping to quantify cultural differences across re-
gions (Adilazuarda et al., 2024). Many cultures
have developed their own versions of “stuffed
pasta” or “dumplings”, each with unique ingre-
dients and preparation methods, often known by
different names (Gallani, 2015), as illustrated in
Figure 1. Small details like how the dumpling is
shaped can signal the cultural background. Con-
versely, some dishes share the same name but have
different meanings; for example, “jelly” in the U.S.
refers to a fruit spread, while in the U.K. and parts
of Asia, it refers to a gelatinous dessert (Poppe,
1992; Abe, 2013). This culinary diversity presents
a challenge for Vision Language Models (VLMs),
which must accurately recognize and differentiate
food items based on cultural context for applica-
tions like food recognition. These models navigate
the complexities of names, ingredients, and prepa-
ration styles that vary widely across regions. VLMs
have shown effectiveness in text captioning (Liu
et al., 2024b,c) and have been adapted to support
multiple languages (Geigle et al., 2023; Shin et al.,
2024).
However, there is limited research on evaluat-
ing the multicultural capabilities of VLMs, par-
ticularly in terms of multilinguality. The study
by Romero et al. (2024) introduce visual question
answering (VQA) from a multicultural perspective,
but it mainly focuses on knowledge and situational
context at a specific moment, which does not fully
assess the ability of VLMs to reason and differ-
entiate between cultures within a single question.
Moreover, another study on food VQA is limited to
Chinese culture and does not explore the broader
spectrum of global cultures (Li et al., 2024b). An
earlier investigation into cultural bias in language
models also found that cultural knowledge is lack-
ing (Naous et al., 2023). Therefore, further re-
search is necessary to address these limitations and
enhance our understanding of VLMs’ multicultural
and multilingual capabilities.
To facilitate a comprehensive analysis of mul-
tilingual and multicultural research, we develop
resources for evaluating VLMs. Table 1 summa-
rizes how our work compares to previous studies.
Our benchmark stands out for its cultural diversity,
offering more VQA datasets and broader language
and dialect coverage. Our major contributions can
be summarized in three-fold:
• We present WORLD CUISINES , the first mas-
sive scale benchmark consisting of 1 million
high-quality multilingual and multicultural
text-image pairs annotated by native speak-
ers in their local languages. We publicly re-
lease our resources, i.e., datasets, 1 code,2 and
leaderboard3 to advance future research in this
rapidly evolving field.
• We evaluate open-source and commercial
1We release WC-VQA at https://huggingface.co/
datasets/worldcuisines/vqa and WC-KB consisting
food, location, cuisine, and prompt templates at https://
huggingface.co/worldcuisines.
2We release our code at https://github.com/
worldcuisines/worldcuisines.
3We release our leaderboard at https://huggingface.
co/spaces/worldcuisines/worldcuisines.

Figure 2: WC-VQA in WORLD CUISINES comprises two primary tasks: (1) predicting dish names and (2)
predicting regional cuisines. Task 1 is further divided into three subtasks: (a) no-context, (b) contextualized, and (c)
adversarial. We also include two answer types: multiple-choice question (MCQ) and open-ended question (OEQ).
VLMs for cultural awareness through two
VQA tasks: predicting dish names from im-
ages and context, and identifying their geo-
graphical origin. We also assess the impact of
context, including adversarial scenarios.
• We create multilingual templates for queries
and context (such as the questions in QA pairs)
while preserving language varieties, including
dialects and registers. This is achieved by cre-
ating translations that incorporate different in-
flections, articles, and contractions. Our goal
is to ensure naturalness in each translation and
to use appropriate inflections for place names.
2 W ORLD CUISINES
We propose WORLD CUISINES , an open-source
benchmark designed to evaluate the cultural rel-
evance and understanding of VLMs. Figure 2 dis-
plays VQA examples in English, alongside selected
parallel translations in Japanese and French.
2.1 Overview
We develop both a VQA dataset (WC-VQA ) and
a curated KB for world cuisines ( WC-KB ). The
WC-VQA dataset is constructed using WC-KB ,
which serves as the primary data source. We design
two tasks as follows:
• Task 1: Dish Name Prediction. This task in-
volves predicting the name of a dish based on
its image, a question, and contextual informa-
tion. It comprises three subtasks, each with
distinct query types: (a) no-context question,
(b) contextualized question, and (c) adversar-
ial contextualized question.
• Task 2: Location prediction. The task is to
predict location where the food is commonly
consumed and originated given the dish image,
question, and a context.
WC-KB . A KB encompassing 2,414 dishes
worldwide includes 6,045 images and metadata,

Figure 3: WORLD CUISINES distribution of food entries by country in the World Map. The food entries are
distributed across 189 countries, with the highest concentration found in Asia, Europe, and North America. There
are also some entries from the continents of Africa, Oceania, and Central and South America.
Figure 4: Countries by number of assigned dishes, showing the top 50 countries.
covering both coarse-grained (e.g., stew) and fine-
grained categories (e.g., beef stew), locations, and
regional cuisines. It also features multilingual trans-
lations of 90 crowd-sourced prompt templates and
401 parallel data entries (i.e., multilingual informa-
tion) for location and regional cuisine information.
WC-VQA . A multilingual parallel VQA dataset
with 1 million samples encompassing over 30 lan-
guages and dialects, including various varieties and
registers, such as formal and casual styles, with
high-quality human annotations. The VQA is de-
signed to evaluate models’ ability to understand
cultural food names and their origins.
2.2 WC-KB Construction
Our data sources are gathered from Wikipedia4 and
Wikimedia Commons5 to ensure they can be eas-
ily redistributed under an accepted open-source li-
cense. The data construction process involves four
4Wikipedia web pages can be accessed at https://
wikipedia.org.
5Wikimedia Commons web pages can be accessed
at https://commons.wikimedia.org.
key steps: (1) dish selection, (2) metadata annota-
tion, (3) quality assurance, and (4) data compilation.
Figure 3 provides statistics on the regions covered
in our dataset, with detailed information available
in Table 9 in the Appendix. Figure 4 shows the
distribution of dish frequencies, highlighting the
top 50 countries with the most dishes.
2.2.1 Dish Selection
We compile a comprehensive list of dish names
sourced from Wikipedia. We manually review
pages that feature lists of dishes to determine
whether each dish is a specialty unique to a spe-
cific culture, as we aim to focus on dishes that have
distinct cultural significance. We exclude generic
categories, such as ice cream, which lacks a spe-
cific cultural association. We ensure that each dish
on our list has its own dedicated Wikipedia page.
If a dish does not have a Wikipedia page, it is also
excluded from our compilation. This meticulous
approach ensures that our dataset is both culturally
relevant and well-documented.

Data
Split
Task 1 (Dish Name) Task 2 Total
# VQA(a) no-context (b) contextualized (c) adversarial (Location)
# VQA # Images # VQA # Images # VQA # Images # VQA # Images
Train (1M) 270,300 3,383 267,930 3,555 271,770 3,589 270,000 3,361 1,080,000
Test Small (12k) 3,000 100 3,000 100 3,000 100 3,000 100 12,000
Test Large (60k) 15,000 500 15,000 500 15,000 499 15,000 499 60,000
Table 2: Dataset statistics for WC-VQA tasks for train, test small, and test large data splits. Total #VQA represents
the total number of VQA from Task 1 and Task 2.
2.2.2 Metadata Annotation
Given a dish name and its corresponding Wikipedia
page link, we then ask annotators to manually com-
pile metadata based on the provided information.
This metadata includes:
• Visual Representation: Images sources from
Wikimedia Commons are included, along with
their license information.
• Categorization: Dishes are classified into
both coarse-grained (e.g., rice, bread) and fine-
grained (e.g., fried rice, flatbread) categories.
• Description: Annotators provide a descrip-
tion of each dish based on the content from its
Wikipedia page, avoiding the use of the dish’s
name, origin, or any distinctive keywords that
uniquely identify the dish.
• Cuisine: The dish’s origin cuisine and any
cuisines with which it is strongly associated.
• Geographic Distribution: This includes the
dish’s associated countries, area (city or re-
gion), and broader continental region.
The metadata description, along with the example,
is further elaborated in the Appendix Table 4.
2.2.3 Quality Assurance
Before starting the quality assurance process, we
first identify common issues that arise during the
annotation and develop automated rules to detect
easily identifiable annotation errors, such as incor-
rect string formatting. Annotators are then asked to
correct these errors. To further ensure data quality
and validity, we conduct several rounds of quality
assurance. Initially, we focus on image quality by
removing instances where images are blurry, dark,
or contain distracting elements such as people or
other dishes. We also verify image licenses by
cross-referencing them with information on Wiki-
media Commons. Next, we refine the dish catego-
rization and descriptions, ensuring consistency in
category assignments and maintaining descriptions
free from “information breaches” (e.g., excluding
regional details from the description). We standard-
ize cuisine names and eliminate any redundancies.
Finally, we meticulously review all country and
area information to ensure its accuracy. This com-
prehensive approach guarantees the integrity and
reliability of our dataset.
2.2.4 Data Compilation
In this phase, we verify the overall quality check
done by annotators, and identify any potential in-
consistencies that are missed during the quality
assurance. Then, we compile the dataset by collect-
ing the metadata into a single file.
2.3 VQA Generation
In this phase, we generate VQA data by sampling
from WC-KB . An entry of VQA data comprises
visual image, question text, and answer text. This
process involves four stages: (1) conducting a sim-
ilarity search for dish names, (2) constructing ques-
tions and contexts, (3) translating these elements
into multiple languages, and (4) generating the
VQA triplets.
2.3.1 Dish Names Similarity Search
To identify similar dishes in our dataset, we follow
the approach from Winata et al. (2024) to employ a
multilingual model E5LARGE Instruct (Wang et al.,
2024) for computing text embedding. Formally,
given a dish x with name xname and text descrip-
tion xdesc, we use a multilingual model θ to com-
pute the embedding vector vx = θ({xname; xdesc}),
then apply cosine similarity to compute a score
s = similarity(vi, vj) between dish i and dish j.
For each dish, we consider the top-k most similar
dishes to generate distractors in the multiple choice
question.
2.3.2 Question and Context Construction
Dish name prediction (Task 1) is divided into three
question variations depending on the context: (1a)

Model
Task 1 (Dish Name) Task 2
(Location) Average(a) no-context (b) contextualized (c) adversarial
MCQ OEQ MCQ OEQ MCQ OEQ MCQ OEQ MCQ OEQ
Open-Source
Llava1.6 Vicuna 7B 34.57 1.59 43.48 4.03 34.84 1.41 32.24 9.29 36.28 4.08
Llava1.6 Vicuna 13B 40.17 2.79 48.17 5.85 39.05 2.57 37.79 10.16 41.30 5.34
Qwen2 VL Instruct 2B 41.65 7.98 42.29 8.13 39.69 6.74 47.85 14.55 42.87 9.35
Qwen2 VL Instruct 7B 61.48 6.76 67.85 10.36 53.52 6.12 55.90 21.03 59.69 11.07
Qwen2 VL Instruct 72B 74.19 12.67 80.79 21.31 62.43 8.37 61.90 27.27 69.83 17.40
Llama 3.2 Instruct 11B 59.93 18.75 64.12 22.96 53.17 13.39 57.93 31.58 58.79 21.67
Llama 3.2 Instruct 90B 77.69 16.93 82.92 23.60 63.96 10.87 67.87 31.31 73.11 20.68
Molmo-E 1B 18.81 0.01 24.22 0.23 19.55 0.01 18.97 1.54 20.39 0.45
Molmo-D 7B 46.01 2.89 55.95 3.66 41.61 2.31 33.35 11.45 44.23 5.08
Molmo-O 7B 39.96 5.15 44.93 6.03 38.41 3.51 29.81 10.07 38.28 6.19
Pangea 7B‡ 52.35 1.52 63.07 2.73 49.17 1.57 48.71 20.15 53.33 6.49
Aria 25B 58.61 4.99 69.29 9.17 52.82 3.39 42.82 16.20 55.89 8.44
Phi-3.5 Vision 4B 43.37 2.91 48.71 4.23 40.87 2.07 35.01 9.22 41.99 4.61
Pixtral 12B 56.65 1.22 70.69 2.94 52.12 1.09 46.67 14.43 56.53 4.92
NVLM-D 72B 69.82 4.71 78.93 10.29 52.12 2.89 51.97 16.68 63.21 8.64
Proprietary
GPT-4o 88.45 21.88 91.57 37.51 82.29 14.79 66.52 37.13 82.21 27.83
GPT-4o Mini 72.80 10.28 81.65 20.87 57.76 5.72 52.37 25.79 66.14 15.66
Gemini 1.5 Flash 77.05 12.81 80.97 15.16 69.13 6.46 71.53 30.03 74.67 16.12
Table 3: Accuracy (%) results of WC-VQA for Test Large (60k). MCQ and OEQ indicate multiple-choice question
and open-ended question, respectively. Best and second-best are bolded and underlined, respectively. ‡We employ
an optimized prompt provided by the authors (see Subsection E.1 in the Appendix for further details).
no-context question, where we simply ask for the
name of the dish without any provided context;
(1b) contextualized question where we provide ad-
ditional information related to cuisine or location;
and (1c) adversarial contextualized question which
are similar to the contextualized questions but may
include misleading location information to assess
the model’s robustness to irrelevant details.
For example, consider coxinha from Brazil,
shown in Figure 2 (1b). A query with additional
context here would be: “What is the common
name for this dish in Brazil?" Here, the
origin of coxinha, Brazil, serves as the context.
In contrast, adversarial context involves providing
misleading or irrelevant information in terms of
location or type of cuisine to assess the model’s
robustness to such distractions. For instance, in the
case of eggs benedict shown in Figure 2 (1c), an
adversarial context would be: “Yesterday I had
a nice lunch at a Korean restaurant. I
am about to have this dish now. What is
this dish called?” In this scenario, the model
should ignore the irrelevant detail (“nice lunch
at a Korean restaurant”) and focus solely on
the image and the question.
Only basic question without any provided con-
text is available for regional cuisine prediction
(Task 2). The data statistics for each task are pre-
sented in Table 2.
2.3.3 Multiple Language Translation
Question and Context. All questions and con-
texts are initially collected in English, which are
then carefully translated by native human speakers
into 30 language varieties: 23 different languages
with 7 languages having two different varieties
each. We instructed the translators to prioritize
the naturalness, and then followed by the diversity
of translations when the duplication occurs.
Food Name Alias. Using Wikipedia pages as our
primary source, we can verify if the English page
has translations available in other languages. This
enables us to extract dish names in multiple lan-
guages and compile them as translations for each
dish. We utilize both the Wikipedia page titles in
various languages and the alias text found on the
English page. These translations are especially
valuable for multilingual prompt translation, as
they allow us to use the dish’s native name instead
of its English equivalent, enhancing cultural rele-
vance and accuracy. We use the English name as
default when the translation is unavailable.
Locations and Cuisines. As there are more than
400 unique locations, including countries, cities,

(a) Multiple-choice question (MCQ).
(b) Open-ended question (OEQ).
Figure 5: Accuracy (%) categorized by language (left), language vitality (center), and language family (right). We
classify the language vitality by following the classification proposed by Joshi et al. (2020).
and areas, we first translate the English locations
into other languages by using GPT-4o, followed by
proofreading each translation by the native speak-
ers. The string values for the regional cuisines, i.e.,
the adjective form of the location in English, are
translated in the same manner as location.
Morphological Inflections. Indo-European lan-
guages, such as Czech or Spanish, are rich in in-
flectional morphology which involves word modifi-
cation to express different grammatical categories,
such as number, gender, or case. For example, the
equivalents of the phrases “in Japan” and “from
Japan” in Czech are “v Japonsk u” and “z Japon-
ska”, respectively. We provide a framework for
the human translators to use the inflections in the
prompt template to prioritize the naturalness while
keeping the inflections as few as possible.
2.3.4 Generating VQA Triplets
To ensure no overlap in train and test subsets, we
split the dishes and the multilingual-questions into
two subsets each, to ensure no dish or multilingual
questions leakage between train and test. For every
subset, we apply random sampling to get a pair of
dish and its multilingual-questions. We use the dish
entry in our WorldCuisines KB dataset to pick the
image and the location to be injected to the context,
if any. The answer candidates for multiple-choice
were picked by utilizing similarity search (Section
2.3.1). We repeat this process until we reach the
desired number of training or test samples, or until
all possible dish and question combinations are
used, discarding any duplicates.
3 Experiments
3.1 Experimental Setup
Metrics. We use accuracy as the primary met-
ric to evaluate predictions. For Task 2 (open-
ended), we employ BERTScore (Zhang et al., 2019)
with XLM-R Large (Conneau and Lample, 2019)
as a secondary metric to determine if the model-
generated content includes food names similar to
those in the gold labels. For open-ended ques-
tions, we compute the accuracy of each test sample
against multiple references, including translations
of the dish in different languages. This approach
allows us to accommodate predictions that may not
be in the expected language.
Models. We evaluate our benchmark on various
available VLMs, including 15 open-source models
and 3 proprietary models. During the inference