BetterBench: Assessing AI Benchmarks, Uncovering
Issues, and Establishing Best Practices
Anka Reuel ∗
Stanford University
Amelia Hardy∗
Stanford University
Chandler Smith
Northeastern University
Max Lamparth
Stanford University
Malcolm Hardy
Stanford University
Mykel J. Kochenderfer
Stanford University
Abstract
AI models are increasingly prevalent in high-stakes environments, necessitating1
thorough assessment of their capabilities and risks. Benchmarks are popular for2
measuring these attributes and for comparing model performance, tracking progress,3
and identifying weaknesses in foundation and non-foundation models. They can4
inform model selection for downstream tasks and inﬂuence policy initiatives.5
However, not all benchmarks are the same: their quality depends on their design6
and usability. In this paper, we develop an assessment framework considering 467
best practices across an AI benchmark’s lifecycle and evaluate 24 AI benchmarks8
against it. We ﬁnd that there exist large quality differences and that commonly used9
benchmarks suffer from signiﬁcant issues. We further ﬁnd that most benchmarks10
do not report statistical signiﬁcance of their results nor allow for their results to be11
easily replicated. To support benchmark developers in aligning with best practices,12
we provide a checklist for minimum quality assurance based on our assessment. We13
also develop a living repository of benchmark assessments to support benchmark14
comparability, accessible at betterbench.stanford.edu.15
1 Introduction16
AI systems are rapidly advancing and proliferating [ 58]. The increasing integration of AI, and in17
particular foundation models (FMs) [ 14], into decision-making systems has signiﬁcantly ampliﬁed18
its impact and has showcased both beneﬁts [ 9, 39, 57, 66] and risks [ 2, 75, 44, 86, 45, 30, 70]. Given19
the importance of correctly assessing a model’s capabilities and potential harms, AI evaluation is20
an essential discipline [ 15]. Current evaluation approaches include both internally (e.g., private21
testing on proprietary data) and externally developed techniques (e.g., scoring on public benchmarks)22
[74, 27, 73, 48, 32].23
Following the work of [ 67], we deﬁne a benchmark “as a particular combination of a dataset or sets24
of datasets [...], and a metric, conceptualized as representing one or more speciﬁc tasks or sets of25
abilities, picked up by a community of researchers as a shared framework for the comparison of26
methods” [67]. Using benchmarks to facilitate comparison, measure performance, track progress, and27
identify weaknesses has become a standard practice. For example, benchmarks are widely used by28
model developers to report performance and compare models upon release [ 3, 8], and as part of policy29
initiatives to support third-party model evaluations, such as as part of the UK AI Safety Institute’s30
∗(*) denotes equal contribution. Corrspeonding authors: anka.reuel@stanford.edu, ahardy@stanford.edu
Submitted to the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets
and Benchmarks. Do not distribute.

DESIGN DOCUMENTATION
1 
Define purpose, scope, and 
structure of the benchmark
Determine tasks, datasets, 
and evaluation metrics
IMPLEMENTATION
Construct the benchmark by 
collecting, processing, and 
annotating datasets
Protections against 
contamination and gameability
Describe benchmark tasks, datasets, 
and evaluation metrics
Explain design decisions and limitations
Provide resources for benchmark usage
MAINTENANCE
Address issues and incorporate 
feedback
Assess relevance of benchmark
RETIREMENT
Communicate retirement plan 
to stakeholders
Archive benchmark data, 
code, and documentation and 
mark benchmark as 'retired'
2 3 4 5 
Figure 1: Five stages of the benchmark lifecycle. A detailed description can be found in App. C.
Inspect framework for evaluating large language models (LLMs) [ 81] or Article 51 of the EU AI31
Act [1]. However, the ﬁdelity of this approach depends entirely on the benchmarks’ quality, where32
we deﬁne a high-quality benchmark as one that is interpretable, clear about its intended purpose33
and scope, and that is usable. To date, no structured assessment for the quality of AI benchmarks,34
including both FM and non-FM benchmarks, has been published, and no comparative analysis has35
been conducted to understand quality differences between widely used AI benchmarks. To address36
these gaps, our paper:37
• Presents a novel AI benchmark assessment framework evaluating the quality of AI bench-38
marks based on 46 criteria derived from expert interviews and domain literature39
• Scores 16 foundation model (FM) and 8 non-FM benchmarks (full list in App. D), ﬁnding40
quality differences across both categories41
• Provides insights into prevalent issues in current AI benchmarking practices based on our42
assessment43
• Creates a checklist for minimum quality assurance to support benchmark developers in44
aligning with best practices45
• Makes available a living repository 2 of benchmark assessments for users to analyze bench-46
marks’ quality and appropriateness for their usage contexts.47
We structure the paper as follows: Sec. 2 explores benchmarking in AI and other ﬁelds. Sec. 348
describes our assessment development, which combined literature and expert interviews, and details49
our benchmark scoring procedure. Sec. 4 presents our framework’s criteria, focusing on aspects50
under developers’ control to promote better benchmarks. Sec. 5 lists additional context-dependent51
design considerations. Sec. 6 reports ﬁndings from applying our framework to 24 benchmarks.52
Finally, Sec. 7 and Sec. 8 explore implications for future evaluations and discuss our work’s scope53
and limitations. We further outline open challenges with AI benchmarking in App. A, involved54
stakeholders in App. B, and the AI benchmark lifecycle in App. C.55
2 Related Work56
2.1 AI Benchmarking Practices and Challenges57
Our literature review of AI benchmarking practices identiﬁes two primary concerns: what a bench-58
mark measures and how this measurement is used. Regarding what a benchmark measures, [ 59]59
ﬁnd that current benchmarks for LLMs are insufﬁcient for assessing these models capabilities. A60
frequent concern in this context is the validity of evaluations [ 54, 76, 67]. Similarly, [ 62] ﬁnds61
2https://betterbench.stanford.edu
2

that the rapid advancement of AI models threatens benchmarks’ utility, as a large fraction of these62
evaluations are near saturation. [ 83] and [ 49] both address the narrow scope of existing benchmarks,63
with [49] advocating for approaches intended to reduce the socio-technical gap that exists between64
the capabilities that benchmarks are able to measure and the ability of models to meet user needs65
in downstream applications. With respect to how evaluations are used, [ 67] critiques the tendency66
of AI practitioners to overgeneralize benchmark results, highlighting how these scores present an67
inherently reductive view of model performance.68
In addition, the community has also recognized the importance of data curation and documentation69
in the context of evaluations. [ 65] put forth the idea of data cards as standardized documentation70
framework for datasets and [ 12] develop a framework and checklist for best practices in data curation.71
Finally, the FAIR principles [ 87] outline best practices for digital data access, based on the principles72
of Findability, Accessibility, Interoperability, and Reuse. While these efforts support the adoption73
of best practices in the context of data, they are insufﬁcient for assessing AI benchmarks, which74
extend data with infrastructure and evaluation methods, requiring additional guidelines to support the75
development of high-quality benchmarks and the decision-making of benchmark users.76
Hence, our work builds on and expands these guidelines, with the aim of advancing the analysis of77
AI benchmarking by presenting a ﬁrst-of-its-kind framework for the assessment of both foundation78
model and non-foundation model benchmarks. Unlike prior studies, such as [ 59] and [ 49], which79
focus on identifying limitations in limited contexts and scopes, our approach offers practical tools,80
empowering developers to address shortcomings and directly enhance benchmark quality: Our81
assessment spans a wider range of criteria across the benchmark lifecycle, from design (e.g., have82
domain experts been involved in the development?) to implementation (e.g., is the evaluation script83
available?), documentation (e.g., is the applicable license speciﬁed?), and maintenance (e.g., is a84
feedback channel available for users?). We give an overview of all our criteria in Sec. 4 and explain,85
justify, and provide scoring details for each criterion in App. K. We further provide a checklist of best86
practices derived from our analysis (App. J), offering guidance for improving AI benchmarks, rather87
than merely highlighting issues.88
2.2 Benchmarking Best Practices in Other Fields89
Our work is informed by benchmarking practices from ﬁelds beyond AI, ranging from transistor90
hardware [18] to environmental quality [ 16] to bioinformatics [ 7], and we identify common themes91
regarding what constitutes an effective benchmark. Where applicable, we incorporate these best92
practices into our assessment (Sec. 4):93
Designing for downstream utility. Many of the papers reviewed discuss the importance of a94
benchmark’s tasks being designed with real world applications in mind. [ 16] considers the best95
benchmarks to be situation-speciﬁc, [ 24] deﬁnes an ideal test set as one which reﬂects real world data,96
[7] proposes that benchmarks should be adapted to their intended applications, and [ 25] suggests97
that benchmarks be designed to ﬁt the diversity of downstream use cases. [ 77] emphasizes the98
importance of guaranteeing that tested methods only use information available in a practical setting99
and recommends checking that a benchmark simulates the envisioned usage.100
Ensuring validity. A frequent concern with benchmarking is the validity of evaluations [ 54, 76, 67].101
In educational testing, [ 60] outline a framework to ensure validity by providing guidelines for effective102
evidence collection. [ 22] outline what and how evidence can be collected and how it should be103
interpreted for tests “of attributes for which there is no adequate criterion” [ 22]. Measures that are104
used in other ﬁelds further include choosing a large test set to promote the statistical signiﬁcance of105
results [77] and updating a benchmark over time to prevent developers from overﬁtting it [ 7]. [ 7] also106
notes that the methods or approaches being evaluated should not be used to create the gold standard107
dataset.108
Prioritizing score interpretability. [7] highlights that benchmarks are particularly important when109
a wide variety of tools are available and it is difﬁcult for non-specialists to distinguish between110
them. Interpretability is important in not only selecting tools, but also deciding between benchmarks111
3

themselves. Effective benchmarks must provide transparent information regarding the procedural112
details of their experiments [ 18] and goals of the evaluation [ 10]. They should clearly describe the113
benchmark’s purpose and scope, as these are fundamental to its design and implementation [ 85].114
Regarding scope, [ 16] states that for environmental quality applications, benchmarks should never be115
the basis of ﬁnal decisions. With this in mind, they identify misleading benchmarks as the worst-case116
scenario. Furthermore, they state that a benchmark should not present its results as absolutes, instead117
ensuring that its evaluations are understandable inputs for decision makers [16].118
Guaranteeing accessibility. A good benchmark is easy to obtain and use [ 7, 77, 25, 10]. If a119
benchmark is run computationally, then its data and scripts must be available for results to be120
reproducible [77, 25, 10].121
3 Methodology122
Our benchmark assessment consists of 46 criteria based on our literature review and interviews123
with ﬁve primary groups of stakeholders. These groups, who also present the user personas of our124
assessment, are described in detail in App. B. Through our interview process, we deﬁned a ﬁve-stage125
benchmark lifecycle and identiﬁed objectives along it. In this section, we discuss our methodology126
for identifying stakeholders, developing criteria, and assessing benchmarks. A detailed ﬂow diagram127
of our methodology can be found in App. H.128
Step 1: Mapping the space. Initially, we surveyed the existing benchmark landscape (Sec. 2).129
Based on this review, we identiﬁed ﬁve stakeholder groups who present the user personas of our130
assessment (App. B). To understand their objectives with respect to benchmarking, we conducted131
unstructured interviews with representatives of all stakeholder groups, including 20+ policymakers,132
model developers, benchmark developers, model users, and AI researchers. During this process, we133
developed a ﬁve-stage model of the benchmark lifecycle (Fig. 5 and App. C) and mapped both the134
benchmarking objectives of the stakeholders and their communicated use cases for a benchmark135
assessment (App. B).136
Step 2: Translation to criteria. Based on Step 1, we identiﬁed tasks and objectives for each stage137
of the AI benchmark lifecycle and translated them into concrete criteria. We categorized these138
as: (a) criteria controlled by the benchmark developer where the authors and interviewees reached139
a normative consensus, (b) criteria controlled by the benchmark developer but context-dependent,140
difﬁcult for an external party to assess, or both and (c) aspects either outside the benchmark developer’s141
control or requiring further research. The assessment in Sec. 4 is limited to category (a) criteria. We142
cover considerations in (b) in Sec. 5, and those in (c) in App. A.143
Step 3: Validating the assessment. Initially, three authors independently scored the same benchmark144
to calibrate the assessment and identify potential misinterpretations of the criteria. We adapted and145
clariﬁed scoring guidelines (App. K) to address differing interpretations and uncertainties. To validate146
our assessment, we shared it with members of all stakeholder groups and revised it based on their147
feedback. Finally, we veriﬁed that our assessment, which in itself can be considered a benchmark,148
met all of our deﬁned criteria, where applicable (App. J.2).149
Step 4: Structuring the assessment. We evaluated 16 FM and 8 non-FM benchmarks. We prioritized150
commonly used benchmarks, such as those that were recently reported by model developers [ 8, 3]151
and aim to expand the number of assessed benchmarks continuously on our website betterbench.stan-152
ford.edu. Since our assessment considers varying information sources (ofﬁcial websites, papers,153
GitHub repositories published by the benchmark developers 3) that do not follow a standard structure,154
we manually evaluated all benchmarks. At least two authors independently reviewed each benchmark.155
They subsequently had to reach a consensus on the ﬁnal score and a third reviewer could be called to156
make the ﬁnal decision if a consensus could not be reached (this case did not occur).157
3We do not consider third-party information that was not released by the benchmark developers themselves.
4

Step 5: Scoring. We scored benchmarks on a discrete 0/5/10/15-point scale for each criterion: 15158
for fully meeting, 10 for partially meeting, 5 for mentioning without fulﬁlling, and 0 for neither159
referencing nor satisfying the criterion. Average scores were calculated for each benchmark lifecycle160
stage (design, implementation, documentation, and maintenance). An aggregate usability score,161
representing the weighted average of the implementation, documentation, and maintenance scores,162
was also introduced (see App. G for scoring details). We consider a mean score of 10 or higher to163
indicate a reasonably good benchmark for each aggregated scoring category, as it signiﬁes that, on164
average, the benchmark at least partially fulﬁlls all assessment criteria within the respective category.165
Step 6: Platform for continuous updates. Finally, we develop a supplementary website 4 to166
continuously publish assessment results using the scoring methodology in App. G, given the rapid167
development of new AI benchmarks. The website includes a community feedback channel for168
submitting new AI benchmarks and correcting previously posted scores if benchmarks are updated169
or stakeholders disagree with our evaluation. This provides benchmark users with an accessible,170
up-to-date database of existing benchmarks and their quality, enabling quick analysis of the most171
suitable benchmark for their application context.172
4 Assessment Criteria173
We separate our assessment criteria according to the phase of the benchmark lifecycle during which174
they would be fulﬁlled. Although the retirement stage is within the developer’s control, we do175
not include speciﬁc criteria for this phase within the current framework, because we cannot assess176
the retirement of active benchmarks. App. K contains full explanations, justiﬁcations, and scoring177
guidelines for each of the 46 criteria.178
4.1 Benchmark Design179
Design Criteria
1. Tested capability, characteristic, or concept is defined
2. How tested capability or concept translates to 
benchmark task is described
3. Domain experts are involved
4. Domain literature is integrated
5. Use cases or user personas are described
 . Differences to related benchmarks are explained
7. Input sensitivity is addressed
 . Has validated automatic evaluation
9. How benchmark score should or shouldn't be 
interpreted or used is described
10. How knowing about the tested concept is helpful in 
the real world is described
11. Informed performance metric choice
12. Metric floors and ceilings are included
13. Human performance level is included
14. Random performance level is included
Figure 2: Overview of assessment criteria for the benchmark design stage.
Benchmarks should clearly describe their goals and scope [ 85, 10, 54]. This includes deﬁning the180
tested capability or characteristic, describing how the tested capability translates to the benchmark181
task, and stating how knowing about the tested concept is helpful in real-world applications [ 54].182
These design choices should be informed by considering use cases and user personas for the bench-183
mark, involving domain experts, and integrating domain literature [ 82]. Clearly stating how the184
benchmark is different from related existing AI benchmarks is necessary to help benchmark users185
decide the applicability of a benchmark to their use case. A benchmark’s measurements must be186
interpretable [16], which requires an informed choice of performance metric(s) and a description of187
how the benchmark score should or shouldn’t be interpreted [ 48]. Including ﬂoors, ceilings, human188
performance levels, and random performance levels for the chosen metric(s) further assists users189
in understanding a model’s score [ 34]. If addressing input sensitivity and providing a validated190
automatic evaluation are possible, these measures enhance a benchmark’s robustness and accessibility191
[34].192
4betterbench.stanford.edu. Our assessment and results are released under a CC BY 4.0 license.
5

4.2 Benchmark Implementation193
Implementation Criteria
1. Evaluation code is available
2. Evaluation data or generation mechanism is accessible
3. Evaluation of models via API is supported
4. Evaluation of local models is supported
5. Globally unique identifier or encryption of evaluation 
instances is added
 . Task to identify if model has been trained on 
benchmark data is included
7. Script to replicate results is explicitly included
 . Statistical significance or uncertainty quantification 
of benchmark results is reported
9. Need for warnings for sensitive/harmful content is 
assessed
10. Build status is implemented
11. Release requirements are specified
Figure 3: Overview of assessment criteria for the benchmark implementation stage.
Criteria in the implementation stage focus on the availability of necessary code and infrastructure194
and the inclusion of key engineering features. To ensure reproducibility and scrutiny [ 77, 25, 10],195
a benchmark should provide working evaluation code, and make its evaluation data, prompts, or196
dynamic test environment accessible. A script should be available to replicate initial published197
results. In domains where models are often accessed via API, such as NLP, an ideal benchmark198
supports the evaluation of both API-based and local models. A benchmark can minimize the risks of199
contamination and gamiﬁcation by including a globally unique identiﬁer or encrypting evaluation200
instances. This is especially important for testing models that rely on web-scraped training data.201
Including a training_on_test_set task allows determining whether a model’s training data included202
benchmark examples [ 74]. As an additional measure, specifying clear release requirements informs203
users how to preserve the integrity of test results [6].204
4.3 Benchmark Documentation205
Figure 4: Overview of assessment criteria for the benchmark documentation stage.
Providing comprehensive and accessible documentation is crucial for the practicability and interpreta-206
tion of benchmarks [ 18]. Key information about a benchmark should be readily available and include207
documentation of benchmark construction processes [ 54], data collection [ 87] or test environment208
design, and its test tasks and their rationale [ 54]. Clearly documenting evaluation metric(s) and209
reporting the statistical signiﬁcance of results is necessary so that users can understand a benchmark’s210
actual signal [ 4]. To provide context and prevent misinterpretation, developers should document211
normative assumptions about benchmark properties and discuss the limitations of their benchmark.212
A benchmark’s codebase should contain a requirements ﬁle, a quick-start guide or demo code, a213
description of code ﬁle structure and contents, and in-line comments within all relevant ﬁles. Having214
a benchmark’s paper accepted at a peer-reviewed venue signals external scrutiny and adherence to215
certain standards. Lastly, developers should specify the applicable license to provide legal clarity and216
enable, e.g., commercial use.217
6

4.4 Benchmark Maintenance218
Maintenance Criteria
1. Code usability was checked within the last year
2. Maintained feedback channel for users is available
3. Contact person is listed
Figure 5: Overview of assessment criteria for the benchmark maintenance stage.
An optimally designed, implemented, and documented benchmark will cease to be useful if it is not219
maintained. Developers should regularly check code usability and maintain a feedback channel for220
users to report issues or suggest improvements. Providing contact details of a person responsible for221
the benchmark facilitates communication and support. Alternatively, if a benchmark is not maintained222
anymore, authors should include a corresponding statement indicating that the benchmark was retired223
in any ofﬁcial benchmark artefacts.224
5 Other Design Considerations225
This section presents design considerations for benchmark developers that were excluded from our226
assessment because their appropriateness is context-dependent, they are not easily veriﬁable, or both.227
Our aim with this list is to promote conscious design decisions regarding these considerations.228
General vs. speciﬁc benchmarks. Benchmark developers must decide whether to prioritize general229
or abstract knowledge and skills or speciﬁc contexts and domains. Broad concept benchmarks may230
contribute to understanding foundational characteristics of models, but often face challenges in231
real-world applicability and reliable testing (see App. A).232
Detecting small improvements. Benchmarks should be designed so that a 1% improvement can be233
reliably detected [ 34]. As [ 34] states, “the more difﬁcult it is to detect small amounts of progress,234
the more difﬁcult it becomes to make iterative progress on a benchmark.” Practically, this is likely235
dependent on evaluation data size and task diversity.236
Multi-modal assessment. As multi-modal models become increasingly common, benchmark de-237
velopers may want to consider designing tasks to assess the capabilities they want to test across238
modalities. Additional design considerations for multi-modal assessments include the increased239
complexity of mapping a tested concept to different modalities and the different output formats of the240
tested models [91].241
Versioning. Minor updates (e.g., removing faulty prompts) should be clearly indicated via task242
versioning [13]. Major updates require releasing new benchmark versions , as exempliﬁed by the243
AgentBench v0.1 and v0.2 releases [52].244
Dynamic vs. static benchmarks. Dynamic benchmarks may better address quick saturation (App. A)245
and contamination (App. A) issues but reduce result comparability and are easier to implement for246
some tasks (e.g., adding numbers) than others. Static benchmarks, on the other hand, tend to suffer247
from the issues outlined above.248
Gameability. An ideal benchmark is resilient to attempts to boost task performance without im-249
proving the fundamental capability being tested [ 7]. Existing benchmarks have been shown to be250
vulnerable to manipulation [ 6]. Speciﬁc guidelines have been proposed to prevent cheating and251
ensure evaluations reﬂect genuine model performance [94].252
Positionality statement. Positionality statements5 are a reﬂective account common in social sciences253
research. In them, researchers acknowledge how their background, experiences, and biases may have254
inﬂuenced their work. If developers believe such factors signiﬁcantly impacted their benchmark’s255
construction, they may provide a positionality statement for increased context and transparency.256
5Such statements were not included in the assessment to avoid pressuring benchmark developers to disclose
potentially sensitive personal information, even if such information inﬂuenced the benchmark design process.
7

Design
ImplementationDocumentation
Maintenance
0
5
10
15Average Score [a.u.]
Figure 6: Average and individual scores of all as-
sessed benchmarks per lifecycle stage.
Stage FM Non-FM All
Design 10.6 11.1 10.7
Implementation 5.5 7.4 6.1
Documentation 10.3 9.9 10.1
Maintenance 9.1 10.8 9.7
Table 1: Benchmark lifecycle scores averaged
over the 24 assessed benchmarks separated
for FM, non-FM, and All benchmarks com-
bined.
FM Non-FM All
Pearson ρ 0.721 0.318 0.655
p-value p 0.001 0.487 0.001
Table 2: Pearson correlation coefﬁcient for
FM, Non-FM, and All benchmarks between
the design and usability (weighted average of
implementation, documentation, and mainte-
nance stages) score as in Fig. 7.
6 Quantitative Results257
In this section, we present our assessment results. 6 Tab. 1 showcases the average scores per benchmark258
lifecycle stage, showing that for both FM and non-FM benchmarks, the implementation stage tends259
to be the weakest area, followed by maintenance. All criteria averages are reported in App. F. Some260
criteria have not been fulﬁlled by almost any benchmark (e.g., Standardized metadata is included ).261
Notably, both benchmark types are particularly weak for criteria supporting the reproducibility and262
interpretation of results: benchmarks get an average score of 3.75 on Including a script to replicate263
results and an average score of 5.62 on Reporting statistical signiﬁcance .264
While individual benchmark or criteria scores are deterministic, we can analyze statistical ﬂuctuations265
across categories and benchmarks. Fig. 7 compares the design and usability scores of FM and non-266
FM benchmarks. The overall average design score across all benchmarks is 10.7, and the weighted267
average usability score is 8.7. The difference in mean design and usability scores between FM and268
non-FM benchmarks is not statistically signiﬁcant (95% conﬁdence level), see Fig. 8 in App. E.269
Furthermore, we ﬁnd statistically signiﬁcant correlations between the design and usability scores270
for FM benchmarks alone and all benchmarks combined at the 95% conﬁdence level (Tab. 2). This271
suggests that, in both cases, benchmarks with poorer design tend to also be less usable, and vice272
versa.273
7 Discussion274
Not all benchmarks are of the same quality. Model developers frequently report performance275
on benchmarks that vary signiﬁcantly in quality. For instance, the widely-used MMLU benchmark276
scored the lowest in our assessment (weighted average: 5.5), while GPQA scored signiﬁcantly higher277
(weighted average: 11.0). However, recent communications introducing models like GPT-4 [ 3],278
Claude-3 [8], and Gemini [ 80] report results on both benchmarks without explicitly acknowledging279
their limitations or quality differences. This practice may be driven by the assumed expectation that280
reviewers want to see a wide range of metrics and the belief that readers should determine the most281
relevant metrics for their needs. The lack of clear guidance on AI benchmark quality and limitations282
may lead to incorrect conclusions about a model’s performance, even if developers do not intend to283
6Per-criterion scores for all benchmarks are released on our website betterbench.stanford.edu. Code to
replicate results will be available on GitHub upon publication.
8

0 5 10 15
Design Score [a.u.]
0
5
10
15Usability Score [a.u.]
BBQ
TruthfulQA
BOLDMMLU
ARC Challenge
WinoGrande
Human Eval
GSM8k
HellaSwag
Machiavelli
AgentBench MLCommons AI Safety v0.5
MMMU
GPQABIG-bench
DecodingTrust
Procgen
MedMNIST v2
Wordcraft
RL UnpluggedFinRL-Meta
SafeBench
Foundation Model
Non-Foundation Model
Figure 7: Design and usability score for all 24 assessed benchmarks. The usability score is the
weighted average of the implementation, documentation, and maintenance scores. Benchmarks were
split into foundation model and non-foundation model benchmarks, depending on the model group
they’re targeting.
mislead users. The UK AI Safety Institute’s Inspect framework [81] similarly includes both MMLU284
[33] and GPQA [ 68], potentially resulting in misleading evaluations. This is problematic because285
governments increasingly rely on evaluations for AI regulations and may use frameworks like Inspect286
[69] or individual benchmarks [1].287
Most benchmarks fail to distinguish signal and noise. Benchmark developers should not only288
report a single result for a model but also re-run their evaluation [ 13] with, e.g., different random289
seeds or sampling temperatures, and report the mean and variance for these intra-model evaluations.290
As benchmarks are primarily used to compare models, users must know the intra-model variance of a291
benchmark to determine whether observed inter-model variances are genuine performance differences292
or arise from noisy results. If intra-model variance bounds are tight and inter-model variance bounds293
are wide, benchmark users can conclude that there are genuine performance differences between294
models. However, if both intra- and inter-variance bounds are wide, statistical analysis is required to295
discern noise and actual signal. Yet, 14 out of 24 benchmarks did not perform multiple evaluations of296
the same model or report statistical signiﬁcance or uncertainty of results.297
Insufﬁcient implementation limits reproducibility and scrutiny of benchmarks. Our analysis298
reveals that scores for implementation stage criteria are the lowest across all assessed benchmarks.299
Notably, 17 out of 24 benchmarks do not provide easy-to-run scripts to replicate the results reported300
in the initial paper, and 4 out of 24 only provide scripts to replicate part of the results. This lack of301
accessibility hinders reproducibility and limits users’ ability to scrutinize the benchmarking process.302
In a ﬁeld where reproducibility is a signiﬁcant concern [ 43], providing materials to reproduce results303
is crucial for validating benchmark ﬁndings.304
Small changes can lead to signiﬁcant improvements in overall benchmark practices. Many of305
the criteria we have identiﬁed for improving AI benchmarks are relatively easy to implement, even306
for existing benchmarks. For example, adding code documentation and and a point of contact are not307
time consuming to add, yet can signiﬁcantly enhance usability, accountability, and ease of use.308
Necessity for higher benchmark development standards. As evidenced by the strong discrepancies309
in AI benchmark quality we found (Sec. 6 and App. F), there is a need to introduce additional checks310
for benchmarking practices to ensure a minimum quality standard for AI benchmarks. We assume that311
benchmark developers do not intentionally construct insufﬁcient benchmarks, but rather do so due to312
limited knowledge of what constitutes a good benchmark. By providing a checklist of best practices313
(App. J.1), we aim to make it easy for benchmark developers to adopt these recommendations and314
9

improve the quality of their benchmarks. In addition, some of the criteria we have identiﬁed in our315
expert interviews and from reviewing evaluation practices in other ﬁelds, such as including a build316
status in GitHub repositories that assesses whether the last commit successfully passed deﬁned unit317
tests [28], were relatively unknown and only implemented by 3 out of 24 benchmarks. Other criteria,318
like using globally unique identiﬁers or encrypting evaluation instances to avoid data contamination,319
have been pioneered by only a few of the assessed benchmarks [ 68, 74] but have not yet gained320
widespread adoption. By incorporating these criteria into our assessment, we aim to encourage321
benchmark developers to adopt these best practices in the ﬁeld of AI benchmarking.322
8 Limitations323
Our assessment assigns equal weight to all criteria, despite their varying levels of effort required for324
fulﬁllment and differing contributions to overall benchmark quality. The scoring system differentiates325
only four score categories to enable relatively objective evaluation through clear-cut criteria (App. K326
and App. G), but may miss nuances within each category. For example, a benchmark barely fulﬁlling327
a criterion and one almost entirely fulﬁlling it would receive the same 10-point score. Given the328
equal weighting and scoring, benchmark developers could potentially “game” the assessment by329
focusing on easily fulﬁlled criteria. However, we believe that even if a developer only implements330
easy-to-implement criteria, the resulting benchmark will still be of higher quality than one not331
meeting any criteria, thus fulﬁlling our work’s goal. Furthermore, assessing the construct validity of332
a benchmark and determining whether its approach to assessing a concept is truly effective would333
presumably require in-depth analysis by domain experts in the respective ﬁelds, which is beyond334
the scope of this assessment. Instead, we aim to provide benchmark developers with a blueprint for335
minimum quality assurances. Finally, our framework is intended for public benchmarks and future336
work is needed to extend it to private ones.337
9 Impact Statement338
By releasing the ﬁrst systematic assessment framework for AI benchmarks, we aim to encourage339
benchmark developers to construct higher-quality benchmarks and to contribute to community efforts340
to make AI evaluations more practicable and transparent. Higher-quality benchmarks resulting341
from the adoption of our framework and checklist can lead to better-informed model selection for342
downstream tasks, potentially reducing risks and improving outcomes in high-stakes applications.343
Our living repository of benchmark assessments promotes transparency and comparability, allowing344
benchmark users to make informed decisions when choosing benchmarks. However, there is a345
potential risk of misinterpretation of our results; our assessment only provides minimum quality346
assurances and is not sufﬁcient to assess the suitability of a benchmark for a concrete use case.347
The outputs of our evaluation do not contain sensitive or harmful content, but users may encounter348
such content during a benchmark assessment depending on the benchmark’s data. While we do not349
anticipate direct safety risks from releasing our framework, we acknowledge that strict adherence to350
some of our proposed criteria, such as the involvement of domain experts, may unequally impact351
researchers based on their access to resources and connections, potentially hindering the development352
of benchmarks from a broader range of research institutions and underrepresented communities,353
which could limit diversity in benchmark creation.354
References355
[1] Art. 51 Classiﬁcation of General-Purpose AI Models as General-Purpose AI Models with356
Systemic Risk - EU AI Act — euaiact.com. https://www.euaiact.com/article/51.357
[Accessed 31-07-2024].358
[2] Abubakar Abid, Maheen Farooqi, and James Zou. Persistent anti-muslim bias in large language359
models. In AAAI/ACM Conference on AI, Ethics, and Society , pages 298–306, 2021.360
10

[3] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni361
Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. GPT-4362
Technical Report. arXiv preprint arXiv:2303.08774 , 2023.363
[4] Rishabh Agarwal, Max Schwarzer, Pablo Samuel Castro, Aaron C Courville, and Marc Belle-364
mare. Deep reinforcement learning at the edge of the statistical precipice. Advances in neural365
information processing systems , 34:29304–29320, 2021.366
[5] Parveen Azam Ali and Roger Watson. Peer review and the publication process. Nursing open,367
3(4):193–202, 2016.368
[6] Norah Alzahrani, Hisham Abdullah Alyahya, et al. When Benchmarks are Targets: Revealing369
the Sensitivity of Large Language Model Leaderboards. ArXiv, abs/2402.01781, 2024.370
[7] Mohamed Radhouene Aniba, Olivier Poch, and Julie D. Thompson. Issues in bioinformat-371
ics benchmarking: the case study of multiple sequence alignment. Nucleic Acids Research ,372
38(21):7353–7363, 07 2010.373
[8] Anthropic. Introducing the next generation of Claude. https://www.anthropic.com/news/374
claude-3-family, 2024. Accessed on June 2, 2024.375
[9] David Baidoo-Anu and Leticia Owusu Ansah. Education in the era of generative artiﬁcial376
intelligence (AI): Understanding the potential beneﬁts of ChatGPT in promoting teaching and377
learning. Journal of AI , 7(1):52–62, 2023.378
[10] Thomas Bartz-Beielstein, Carola Doerr, Jakob Bossek, Sowmya Chandrasekaran, Tome Eftimov,379
Andreas Fischbach, Pascal Kerschke, Manuel López-Ibáñez, Katherine Mary Malan, Jason H.380
Moore, Boris Naujoks, Patryk Orzechowski, Vanessa V olz, Markus Wagner, and T. Weise.381
Benchmarking in Optimization: Best Practice and Open Issues. ArXiv, abs/2007.03488, 2020.382
[11] M. G. Bellemare, Y . Naddaf, J. Veness, and M. Bowling. The Arcade Learning Environment: An383
Evaluation Platform for General Agents. Journal of Artiﬁcial Intelligence Research , 47:253–279,384
jun 2013.385
[12] Eshta Bhardwaj, Harshit Gujral, Siyi Wu, Ciara Zogheib, Tegan Maharaj, and Christoph Becker.386
Machine learning data practices through a data curation lens: An evaluation framework. In The387
2024 ACM Conference on Fairness, Accountability, and Transparency , pages 1055–1067, 2024.388
[13] Stella Biderman, Hailey Schoelkopf, Lintang Sutawika, Leo Gao, Jonathan Tow, Baber Abbasi,389
Alham Fikri Aji, Pawan Sasanka Ammanamanchi, Sidney Black, Jordan Clive, Anthony DiPoﬁ,390
Julen Etxaniz, Benjamin Fattori, Jessica Zosa Forde, Charles Foster, Jeffrey Hsu, Mimansa391
Jaiswal, Wilson Y . Lee, Haonan Li, Charles Lovering, Niklas Muennighoff, Ellie Pavlick, Jason392
Phang, Aviya Skowron, Samson Tan, Xiangru Tang, Kevin A. Wang, Genta Indra Winata,393
François Yvon, and Andy Zou. Lessons from the Trenches on Reproducible Evaluation of394
Language Models. arXiv preprint arXiv:2405.14782 , 2024.395
[14] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von396
Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the397
opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258 , 2021.398
[15] Yupeng Chang, Xu Wang, Jindong Wang, Yuan Wu, Linyi Yang, Kaijie Zhu, Hao Chen,399
Xiaoyuan Yi, Cunxiang Wang, Yidong Wang, Wei Ye, Yue Zhang, Yi Chang, Philip S. Yu,400
Qiang Yang, and Xing Xie. A Survey on Evaluation of Large Language Models. ACM Trans.401
Intell. Syst. Technol. , 15(3), mar 2024.402
[16] Peter M Chapman. Environmental Quality Benchmarks — The Good, The Bad, and The Ugly.403
Environmental Science and Pollution Research , 25(4):3043–3046, 2018.404
11

[17] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto,405
Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul406
Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke407
Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad408
Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias409
Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-V oss, William Hebgen Guss, Alex410
Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain,411
William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant412
Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie413
Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and414
Wojciech Zaremba. Evaluating Large Language Models Trained on Code. arXiv preprint415
arXiv:2107.03374, 2021.416
[18] Zhihui Cheng, Chin-Sheng Pang, Peiqi Wang, Son T Le, Yanqing Wu, Davood Shahrjerdi,417
Iuliana Radu, Max C Lemme, Lian-Mao Peng, Xiangfeng Duan, et al. How to report and418
benchmark emerging ﬁeld-effect transistors. Nature Electronics, 5(7):416–423, 2022.419
[19] Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick,420
and Oyvind Tafjord. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning421
Challenge. arXiv preprint arXiv:1803.05457 , 2018.422
[20] Karl Cobbe, Christopher Hesse, Jacob Hilton, and John Schulman. Leveraging procedural423
generation to benchmark reinforcement learning. In Proceedings of the 37th International424
Conference on Machine Learning , ICML’20. JMLR.org, 2020.425
[21] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser,426
Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John427
Schulman. Training Veriﬁers to Solve Math Word Problems. arXiv preprint arXiv:2110.14168 ,428
2021.429
[22] Lee J Cronbach and Paul E Meehl. Construct validity in psychological tests. Psychological430
bulletin, 52(4):281, 1955.431
[23] David Dalrymple, Joar Skalse, Yoshua Bengio, Stuart Russell, Max Tegmark, Sanjit Seshia,432
Steve Omohundro, Christian Szegedy, Ben Goldhaber, Nora Ammann, et al. Towards Guar-433
anteed Safe AI: A Framework for Ensuring Robust and Reliable AI Systems. arXiv preprint434
arXiv:2405.06624, 2024.435
[24] Niek F de Jonge, Kevin Mildau, David Meijer, Joris JR Louwen, Christoph Bueschl, Florian436
Huber, and Justin JJ van der Hooft. Good practices and recommendations for using and bench-437
marking computational metabolomics metabolite annotation tools. Metabolomics, 18(12):103,438
2022.439
[25] Alex Dekhtyar and Jane Huffman Hayes. Good benchmarks are hard to ﬁnd: Toward the440
benchmark for information retrieval applications in software engineering. 2006.441
[26] Jwala Dhamala, Tony Sun, Varun Kumar, Satyapriya Krishna, Yada Pruksachatkun, Kai-Wei442
Chang, and Rahul Gupta. BOLD: Dataset and Metrics for Measuring Biases in Open-Ended443
Language Generation. In Proceedings of the 2021 ACM Conference on Fairness, Accountability,444
and Transparency, FAccT ’21, page 862872, New York, NY , USA, 2021. Association for445
Computing Machinery.446
[27] Michael Feffer, Anusha Sinha, Wesley Hanwen Deng, Zachary C. Lipton, and Hoda Hei-447
dari. Red-Teaming for Generative AI: Silver Bullet or Security Theater? arXiv preprint448
arXiv:2401.15897, 2024.449
[28] GitHub. Adding a workﬂow status badge. https://docs.github.450
com/en/actions/monitoring-and-troubleshooting-workflows/451
adding-a-workflow-status-badge , 2024.452
12

[29] Shahriar Golchin and Mihai Surdeanu. Time Travel in LLMs: Tracing Data Contamination in453
Large Language Models. CoRR, abs/2308.08493, 2023.454
[30] Declan Grabb, Max Lamparth, and Nina Vasan. Risks from Language Models for Automated455
Mental Healthcare: Ethics and Structure for Implementation. In First Conference on Language456
Modeling, 2024.457
[31] Caglar Gulcehre, Ziyu Wang, Alexander Novikov, Tom Le Paine, Sergio Gómez Colmenarejo,458
Konrad Zołna, Rishabh Agarwal, Josh Merel, Daniel Mankowitz, Cosmin Paduraru, Gabriel459
Dulac-Arnold, Jerry Li, Mohammad Norouzi, Matt Hoffman, Nicolas Heess, and Nando de460
Freitas. RL unplugged: a suite of benchmarks for ofﬂine reinforcement learning. In Proceedings461
of the 34th International Conference on Neural Information Processing Systems , NIPS ’20, Red462
Hook, NY , USA, 2020. Curran Associates Inc.463
[32] Muhammad Usman Hadi, al tashi, Rizwan Qureshi, Abbas Shah, Muhammad Irfan, Anas Zafar,464
Muhammad Bilal Shaikh, Naveed Akhtar, Jia Wu, Seyedali Mirjalili, Qasem Al-Tashi, Amgad465
Muneer, Mohammed Ali Al-garadi, Gru Cnn, and T5 RoBERTa. Large Language Models: A466
Comprehensive Survey of its Applications, Challenges, Limitations, and Future Prospects.467
[33] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and468
Jacob Steinhardt. Measuring Massive Multitask Language Understanding. In International469
Conference on Learning Representations (ICLR) , 2021.470
[34] Dan Hendrycks and Thomas Woodside. Devising ML Metrics. Center for AI Safety , 2024.471
[35] Matthew Hutson. Artiﬁcial intelligence faces reproducibility crisis. American Association for472
the Advancement of Science , 2018.473
[36] Alon Jacovi, Avi Caciularu, Omer Goldman, and Yoav Goldberg. Stop uploading test data474
in plain text: Practical strategies for mitigating data contamination by evaluation benchmarks.475
arXiv preprint arXiv:2305.10160 , 2023.476
[37] Minhao Jiang, Ken Ziyu Liu, Ming Zhong, Rylan Schaeffer, Siru Ouyang, Jiawei Han, and477
Sanmi Koyejo. Investigating Data Contamination for Pre-training Language Models. ArXiv,478
abs/2401.06059, 2024.479
[38] Minqi Jiang, Jelena Luketina, Nantas Nardelli, Pasquale Minervini, Philip H.S. Torr, Shimon480
Whiteson, and Tim Rocktäschel. WordCraft: An Environment for Benchmarking Commonsense481
Agents. In Workshop on Language in Reinforcement Learning (LaRel) , 2020.482
[39] Kevin B Johnson, Wei-Qi Wei, Dilhan Weeraratne, Mark E Frisse, Karl Misulis, Kyu Rhee,483
Juan Zhao, and Jane L Snowdon. Precision medicine, AI, and the future of personalized health484
care. Clinical and translational science , 14(1):86–93, 2021.485
[40] Keller Jordan. Calibrated chaos: Variance between runs of neural network training is harmless486
and inevitable. arXiv preprint arXiv:2304.01910 , 2023.487
[41] Sayash Kapoor, Emily M. Cantrell, Kenny Peng, Thanh Hien Pham, Christopher A. Bail,488
Odd Erik Gundersen, Jake M. Hofman, Jessica Hullman, Michael A. Lones, Momin M. Ma-489
lik, Priyanka Nanayakkara, Russell A. Poldrack, Inioluwa Deborah Raji, Michael Roberts,490
Matthew J. Salganik, Marta Serra-Garcia, Brandon M. Stewart, Gilles Vandewiele, and Arvind491
Narayanan. REFORMS: Consensus-based Recommendations for Machine-learning-based492
Science. Science Advances, 10(18):eadk3452, 2024.493
[42] Sayash Kapoor, Peter Henderson, and Arvind Narayanan. Promises and pitfalls of artiﬁcial494
intelligence for legal applications. arXiv preprint arXiv:2402.01656 , 2024.495
[43] Sayash Kapoor and Arvind Narayanan. Leakage and the reproducibility crisis in machine-496
learning-based science. Patterns, 4(9):100804, 2023.497
13

[44] Vasileia Karasavva and Aalia Noorbhai. The real threat of deepfake pornography: A review of498
canadian policy. Cyberpsychology, Behavior, and Social Networking , 24(3):203–209, 2021.499
[45] Max Lamparth, Anthony Corso, Jacob Ganz, Oriana Skylar Mastro, Jacquelyn Schneider, and500
Harold Trinkunas. Human vs. Machine: Behavioral Differences Between Expert Humans and501
Language Models in Wargame Simulations. arXiv preprint arXiv:2403.03407 , 2024.502
[46] David J Lewkowicz. The Concept of Ecological Validity: What Are Its Limitations and Is It503
Bad to Be Invalid? Infancy, 2(4):437–450, 2001.504
[47] Yucheng Li. An Open Source Data Contamination Report for Large Language Models. 2023.505
[48] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga,506
Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan,507
Bobby Yan, Ce Zhang, Christian Alexander Cosgrove, Christopher D Manning, Christopher Re,508
Diana Acosta-Navas, Drew Arad Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda509
Rong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng,510
Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab,511
Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya512
Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang,513
Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic Evaluation of Language514
Models. Transactions on Machine Learning Research , 2023. Featured Certiﬁcation, Expert515
Certiﬁcation.516
[49] Q Vera Liao and Ziang Xiao. Rethinking Model Evaluation as Narrowing The Socio-Technical517
Gap. In Workshop on Artiﬁcial Intelligence & Human Computer Interaction , volume 1, 2024.518
[50] Stephanie Lin, Jacob Hilton, and Owain Evans. TruthfulQA: Measuring How Models Mimic519
Human Falsehoods. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors,520
Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics521
(Volume 1: Long Papers) , pages 3214–3252, Dublin, Ireland, May 2022. Association for522
Computational Linguistics.523
[51] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,524
Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui525
Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie526
Tang. AgentBench: Evaluating LLMs as Agents. In The Twelfth International Conference on527
Learning Representations, 2024.528
[52] Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding,529
Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui530
Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie531
Tang. Introducing agentbench v0.2. Github, 2024.532
[53] Xiao-Yang Liu, Ziyi Xia, Jingyang Rui, Jiechao Gao, Hongyang Yang, Ming Zhu, Christina Dan533
Wang, Zhaoran Wang, and Jian Guo. FinRL-Meta: Market Environments and Benchmarks534
for Data-Driven Financial Reinforcement Learning. In Thirty-sixth Conference on Neural535
Information Processing Systems Datasets and Benchmarks Track , 2022.536
[54] Yu Lu Liu, Su Lin Blodgett, Jackie Chi Kit Cheung, Q Vera Liao, Alexandra Olteanu, and Ziang537
Xiao. ECBD: Evidence-Centered Benchmark Design for NLP. arXiv preprint arXiv:2406.08723 ,538
2024.539
[55] Manikanta Loya, Divya Anand Sinha, and Richard Futrell. Exploring the Sensitivity of LLMs’540
Decision-Making Capabilities: Insights from Prompt Variation and Hyperparameters. arXiv541
preprint arXiv:2312.17476, 2023.542
14

[56] MingYu Lu, Zachary Shahn, Daby Sow, Finale Doshi-Velez, and H Lehman Li-wei. Is deep543
reinforcement learning ready for practical applications in healthcare? a sensitivity analysis544
of duel-ddqn for hemodynamic management in sepsis patients. In AMIA Annual Symposium545
Proceedings, volume 2020, page 773. American Medical Informatics Association, 2020.546
[57] Kit-Kay Mak, Yi-Hang Wong, and Mallikarjuna Rao Pichika. Artiﬁcial intelligence in drug547
discovery and development. Drug Discovery and Evaluation: Safety and Pharmacokinetic548
Assays, pages 1–38, 2023.549
[58] Nestor Maslej, Loredana Fattorini, Raymond Perrault, Vanessa Parli, Anka Reuel, Erik Bryn-550
jolfsson, John Etchemendy, Katrina Ligett, Terah Lyons, James Manyika, Juan Carlos Niebles,551
Yoav Shoham, Russell Wald, and Jack Clark. The AI Index 2024 Annual Report. Institute for552
Human-Centered AI, 2024.553
[59] Timothy R McIntosh, Teo Susnjak, Tong Liu, Paul Watters, and Malka N Halgamuge. Inad-554
equacies of large language model benchmarks in the era of generative artiﬁcial intelligence.555
arXiv preprint arXiv:2402.09880 , 2024.556
[60] Robert J Mislevy, Linda S Steinberg, and Russell G Almond. Focus article: On the structure of557
educational assessments. Measurement: Interdisciplinary research and perspectives , 1(1):3–62,558
2003.559
[61] Arvind Narayanan and Sayash Kapoor. Evaluating LLMs is a mineﬁeld. https://www.cs.560
princeton.edu/~arvindn/talks/evaluating_llms_minefield/, 2023.561
[62] Simon Ott, Adriano Barbosa-Silva, Kathrin Blagec, Jan Brauner, and Matthias Samwald.562
Mapping global dynamics of benchmark creation and saturation in artiﬁcial intelligence. Nature563
Communications, 13(1):6793.564
[63] Alexander Pan, Jun Shern Chan, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside,565
Hanlin Zhang, Scott Emmons, and Dan Hendrycks. Do the rewards justify the means? mea-566
suring trade-offs between rewards and ethical behavior in the MACHIA VELLI benchmark. In567
Proceedings of the 40th International Conference on Machine Learning , ICML’23. JMLR.org,568
2023.569
[64] Alicia Parrish, Angelica Chen, Nikita Nangia, Vishakh Padmakumar, Jason Phang, Jana Thomp-570
son, Phu Mon Htut, and Samuel Bowman. BBQ: A hand-built bias benchmark for question571
answering. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Findings of572
the Association for Computational Linguistics: ACL 2022 , pages 2086–2105, Dublin, Ireland,573
May 2022. Association for Computational Linguistics.574
[65] Mahima Pushkarna, Andrew Zaldivar, and Oddur Kjartansson. Data cards: Purposeful and trans-575
parent dataset documentation for responsible ai. In Proceedings of the 2022 ACM Conference576
on Fairness, Accountability, and Transparency , pages 1776–1826, 2022.577
[66] E Fantin Irudaya Raj, M Appadurai, and K Athiappan. Precision farming in modern agriculture.578
In Smart Agriculture Automation Using Advanced Technologies: Data Analytics and Machine579
Learning, Cloud Architecture, Automation and IoT , pages 61–87. Springer, 2022.580
[67] Deborah Raji, Emily Denton, Emily M. Bender, Alex Hanna, and Amandalynne Paullada. AI581
and the Everything in the Whole Wide World Benchmark. In J. Vanschoren and S. Yeung,582
editors, Advances in Neural Information Processing Systems (NeurIPS) , 2021.583
[68] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien584
Dirani, Julian Michael, and Samuel R Bowman. GPQA: A graduate-level google-proof q&a585
benchmark. arXiv preprint arXiv:2311.12022 , 2023.586
15

[69] Anka Reuel, Lisa Soder, Benjamin Bucknall, and Trond Arne Undheim. Position: Technical587
Research and Talent is Needed for Effective AI Governance. Forty-ﬁrst International Conference588
on Machine Learning , 2024.589
[70] Juan-Pablo Rivera, Gabriel Mukobi, Anka Reuel, Max Lamparth, Chandler Smith, and Jacque-590
lyn Schneider. Escalation risks from language models in military and diplomatic decision-591
making. In The 2024 ACM Conference on Fairness, Accountability, and Transparency , pages592
836–898, 2024.593
[71] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. WinoGrande: an594
adversarial winograd schema challenge at scale. Commun. ACM, 64(9):99106, aug 2021.595
[72] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying Language Models’596
Sensitivity to Spurious Features in Prompt Design or: How I learned to start worrying about597
prompt formatting. arXiv preprint arXiv:2310.11324 , 2023.598
[73] Atsushi Shirafuji, Takumi Ito, Makoto Morishita, Yuki Nakamura, Yusuke Oda, Jun Suzuki, and599
Yutaka Watanobe. Prompt sensitivity of language model for solving programming problems. In600
New Trends in Intelligent Software Methodologies, Tools and Techniques , pages 346–359. IOS601
Press, 2022.602
[74] Aarohi Srivastava, Abhinav Rastogi, Abhishek Rao, et al. Beyond the Imitation Game: Quanti-603
fying and extrapolating the capabilities of language models. Transactions on Machine Learning604
Research, 2023.605
[75] Bernd Carsten Stahl and David Wright. Ethics and privacy in AI and big data: Implementing606
responsible research and innovation. IEEE Security & Privacy , 16(3):26–33, 2018.607
[76] Arjun Subramonian, Xingdi Yuan, Hal Daumé III, and Su Lin Blodgett. It takes two to tango:608
Navigating conceptualizations of NLP tasks and measurements of performance. arXiv preprint609
arXiv:2305.09022, 2023.610
[77] Johannes Söding and Michael Remmert. Protein sequence comparison and fold recogni-611
tion: progress and good-practice benchmarking. Current Opinion in Structural Biology ,612
21(3):404–411, 2011.613
[78] Makoto Takamoto, Timothy Praditia, Raphael Leiteritz, Dan MacKinlay, Francesco Alesiani,614
Dirk Pﬂüger, and Mathias Niepert. PDEBench: An Extensive Benchmark for Scientiﬁc Machine615
Learning. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and616
Benchmarks Track, 2022.617
[79] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy618
Liang, and Tatsunori B. Hashimoto. Stanford Alpaca: An Instruction-following LLaMA model.619
https://github.com/tatsu-lab/stanford_alpaca, 2023.620
[80] Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu,621
Radu Soricut, Johan Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly622
capable multimodal models. arXiv preprint arXiv:2312.11805 , 2023.623
[81] UK AI Safety Institute. Inspect. https://ukgovernmentbeis.github.io/inspect_ai/,624
2024.625
[82] Bertie Vidgen, Adarsh Agrawal, Ahmed M. Ahmed, Victor Akinwande, Namir Al-Nuaimi, Najla626
Alfaraj, Elie Alhajjar, Lora Aroyo, Trupti Bavalatti, Max Bartolo, Borhane Blili-Hamelin, Kurt627
Bollacker, Rishi Bomassani, Marisa Ferrara Boston, Siméon Campos, Kal Chakra, Canyu Chen,628
Cody Coleman, Zacharie Delpierre Coudert, Leon Derczynski, Debojyoti Dutta, Ian Eisenberg,629
James Ezick, Heather Frase, Brian Fuller, Ram Gandikota, Agasthya Gangavarapu, Ananya630
Gangavarapu, James Gealy, Rajat Ghosh, James Goel, Usman Gohar, Sujata Goswami, Scott A.631
Hale, Wiebke Hutiri, Joseph Marvin Imperial, Surgan Jandial, Nick Judd, Felix Juefei-Xu,632
16

Foutse Khomh, Bhavya Kailkhura, Hannah Rose Kirk, Kevin Klyman, Chris Knotz, Michael633
Kuchnik, Shachi H. Kumar, Srijan Kumar, Chris Lengerich, Bo Li, Zeyi Liao, Eileen Peters634
Long, Victor Lu, Sarah Luger, Yifan Mai, Priyanka Mary Mammen, Kelvin Manyeki, Sean635
McGregor, Virendra Mehta, Shafee Mohammed, Emanuel Moss, Lama Nachman, Dinesh Ji-636
nenhally Naganna, Amin Nikanjam, Besmira Nushi, Luis Oala, Iftach Orr, Alicia Parrish,637
Cigdem Patlak, William Pietri, Forough Poursabzi-Sangdeh, Eleonora Presani, Fabrizio Puletti,638
Paul Röttger, Saurav Sahay, Tim Santos, Nino Scherrer, Alice Schoenauer Sebag, Patrick639
Schramowski, Abolfazl Shahbazi, Vin Sharma, Xudong Shen, Vamsi Sistla, Leonard Tang,640
Davide Testuggine, Vithursan Thangarasa, Elizabeth Anne Watkins, Rebecca Weiss, Chris641
Welty, Tyler Wilbers, Adina Williams, Carole-Jean Wu, Poonam Yadav, Xianjun Yang, Yi Zeng,642
Wenhui Zhang, Fedor Zhdanov, Jiacheng Zhu, Percy Liang, Peter Mattson, and Joaquin Van-643
schoren. Introducing v0.5 of the AI Safety Benchmark from MLCommons. arXiv preprint644
arXiv:2404.12241, 2024.645
[83] Sumeet Wadhwani. AI Benchmarks: Why GenAI Scoreboards Need an Over-646
haul. https://www.spiceworks.com/tech/artificial-intelligence/articles/647
are-ai-benchmarks-reliable/ , 2024. Accessed on June 2, 2024.648
[84] Boxin Wang, Weixin Chen, Hengzhi Pei, Chulin Xie, Mintong Kang, Chenhui Zhang, Chejian649
Xu, Zidi Xiong, Ritik Dutta, Rylan Schaeffer, Sang T. Truong, Simran Arora, Mantas Mazeika,650
Dan Hendrycks, Zinan Lin, Yu Cheng, Sanmi Koyejo, Dawn Song, and Bo Li. DecodingTrust:651
A Comprehensive Assessment of Trustworthiness in GPT Models. In Advances in Neural652
Information Processing Systems (NeurIPS) , 2023.653
[85] Lukas M Weber, Wouter Saelens, Robrecht Cannoodt, Charlotte Soneson, Alexander654
Hapfelmeier, Paul P Gardner, Anne-Laure Boulesteix, Yvan Saeys, and Mark D Robinson.655
Essential guidelines for computational method benchmarking. Genome Biology, 20:1–12, 2019.656
[86] Laura Weidinger, Jonathan Uesato, Maribeth Rauh, Conor Grifﬁn, Po-Sen Huang, John Mellor,657
Amelia Glaese, Myra Cheng, Borja Balle, Atoosa Kasirzadeh, et al. Taxonomy of risks posed658
by language models. In ACM Conference on Fairness, Accountability, and Transparency , pages659
214–229, 2022.660
[87] Mark D Wilkinson, Michel Dumontier, IJsbrand Jan Aalbersberg, Gabrielle Appleton, Myles661
Axton, Arie Baak, Niklas Blomberg, Jan-Willem Boiten, Luiz Bonino da Silva Santos, Philip E662
Bourne, et al. The FAIR Guiding Principles for scientiﬁc data management and stewardship.663
Scientiﬁc data, 3(1):1–9, 2016.664
[88] Chejian Xu, Wenhao Ding, Weijie Lyu, Zuxin Liu, Shuai Wang, Yihan He, Hanjiang Hu, Ding665
Zhao, and Bo Li. SafeBench: A Benchmarking Platform for Safety Evaluation of Autonomous666
Vehicles. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and667
Benchmarks Track, 2022.668
[89] Jiancheng Yang, Rui Shi, Donglai Wei, Zequan Liu, Lin Zhao, Bilian Ke, Hanspeter Pﬁster, and669
Bingbing Ni. MedMNIST v2 - A large-scale lightweight benchmark for 2D and 3D biomedical670
image classiﬁcation. Scientiﬁc Data, 10(1):41, 2023.671
[90] Shuo Yang, Wei-Lin Chiang, Lianmin Zheng, Joseph E. Gonzalez, and Ion Stoica. Rethinking672
Benchmark and Contamination for Language Models with Rephrased Samples, 2023.673
[91] Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang,674
and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities.675
arXiv preprint arXiv:2308.02490 , 2023.676
[92] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens,677
Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun,678
Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and679
Wenhu Chen. MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning680
Benchmark for Expert AGI. In Proceedings of CVPR , 2024.681
17

[93] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. HellaSwag: Can682
a Machine Really Finish Your Sentence? In Proceedings of the 57th Annual Meeting of the683
Association for Computational Linguistics , 2019.684
[94] Kun Zhou, Yutao Zhu, Zhipeng Chen, Wentong Chen, Wayne Xin Zhao, Xu Chen, Yankai685
Lin, Ji-Rong Wen, and Jiawei Han. Don’t Make Your LLM an Evaluation Benchmark Cheater.686
ArXiv, abs/2311.01964, 2023.687
NeurIPS Checklist688
The checklist follows the references. Please read the checklist guidelines carefully for information on689
how to answer these questions. For each question, change the default [TODO] to [Yes] , [No] , or690
[N/A] . You are strongly encouraged to include a justiﬁcation to your answer , either by referencing691
the appropriate section of your paper or providing a brief inline description. For example:692
• Did you include the license to the code and datasets? [Yes] See Section ??.693
• Did you include the license to the code and datasets? [No] The code and the data are694
proprietary.695
• Did you include the license to the code and datasets? [N/A]696
Please do not modify the questions and only use the provided macros for your answers. Note that the697
Checklist section does not count towards the page limit. In your paper, please delete this instructions698
block and only keep the Checklist section heading above along with the questions/answers below.699
1. For all authors...700
(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s701
contributions and scope? [Yes] We support all our claims in Sec. 1 in Sec. 6 and702
App. F.703
(b) Did you describe the limitations of your work? [Yes] Limitations are described in704
Sec. 8 and Sec. 9.705
(c) Did you discuss any potential negative societal impacts of your work? [Yes] The706
broader impact of our work, including negative implications, is discussed in Sec. 9.707
(d) Have you read the ethics review guidelines and ensured that your paper conforms to708
them? [Yes] We conform to all points in the ethics review. For example, we do not709
work with PII or otherwise sensitive information and any potential negative impacts of710
our assessment were discussed in Sec. 9.711
2. If you are including theoretical results...712
(a) Did you state the full set of assumptions of all theoretical results? [N/A] Our work713
does not involve theoretical results.714
(b) Did you include complete proofs of all theoretical results? [N/A] Our work does not715
involve theoretical results.716
3. If you ran experiments (e.g. for benchmarks)...717
(a) Did you include the code, data, and instructions needed to reproduce the main experi-718
mental results (either in the supplemental material or as a URL)? [Yes] The code to719
replicate results will be added as supplementary material and published as part of a720
GitHub repo upon publication.721
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they722
were chosen)? [N/A] We’re not training a model and hence do not include training723
details. However, we provide all necessary information to replicate the results in our724
paper as part of the supplementary material.725
18

(c) Did you report error bars (e.g., with respect to the random seed after running experi-726
ments multiple times)? [Yes] We report statistical signiﬁcance results for our results,727
where applicable. See Section 6 and Appendix F.728
(d) Did you include the total amount of compute and the type of resources used (e.g., type729
of GPUs, internal cluster, or cloud provider)? [N/A] We did not train or modify a730
model and hence did not use signiﬁcant compute resources beyond standard laptops.731
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...732
(a) If your work uses existing assets, did you cite the creators? [Yes] We assess existing733
benchmarks and cite their creators where we mention them.734
(b) Did you mention the license of the assets? [Yes] Given that we do not use, distribute735
or modify the benchmarks we assess, we did not mention their license information. We736
release our assessment and results under the CC BY 4.0 license (Sec. 3).737
(c) Did you include any new assets either in the supplemental material or as a URL? [Yes]738
We provide all assessment results as part of this paper in App. F. They will be included739
as part of a repository of benchmark assessments on our website that we will release740
separately to preserve anonymity.741
(d) Did you discuss whether and how consent was obtained from people whose data you’re742
using/curating? [N/A] We did not use people’s personal data. We base our assessment743
on publicly available information by the respective benchmark developers.744
(e) Did you discuss whether the data you are using/curating contains personally identiﬁable745
information or offensive content? [N/A] We do not use any PII data and we mentioned746
in the paper that our content is not offensive.747
5. If you used crowdsourcing or conducted research with human subjects...748
(a) Did you include the full text of instructions given to participants and screenshots, if749
applicable? [N/A] We only conducted information-gathering, unstructured interviews750
without explicit instructions to interviewees. There were no formal instructions. How-751
ever, we did show the assessment criteria to interviewees at some point during each752
unstructured interview and asked for their feedback.753
(b) Did you describe any potential participant risks, with links to Institutional Review754
Board (IRB) approvals, if applicable? [N/A] We only conducted information-gathering755
interviews, which do not fall under the category of research with human subjects and756
hence do need an IRB approval.757
(c) Did you include the estimated hourly wage paid to participants and the total amount758
spent on participant compensation? [N/A] The interviews we conducted were only759
done with voluntary participants that were not compensated.760
19

A Open Challenges in AI Benchmarking761
Per the current state of the ﬁeld, some benchmark issues are not fully addressable by benchmark762
developer actions and decisions. This section discusses these issues and directs readers, where763
possible, to resources which cover these open problems in greater depth.764
Quick saturation. Rapid advancements in AI have led to the saturation of many benchmarks. Some765
benchmarks have been saturated within months of their release [ 58]. Addressing this issue involves766
evaluating current model performances and assessing whether the concept has already been solved,767
and determining if the benchmark can be made challenging given state-of-the-art capabilities of the768
models tested.769
Contamination. In Sec. 4.2, we discuss strategies to mitigate data contamination. However, even770
when fully adhered to, challenges remain. For example, benchmark developers cannot enforce model771
developers’ use of canary strings to avoid training on benchmark data. Preventing data contamination,772
particularly in models reliant on large amounts of web-scraped data, is a shared responsibility between773
benchmark and model developers. [ 90] offers further description of measures that can be taken on774
the model developer side. This issue is pressing, as contamination has been demonstrated in both FM775
[29, 37, 47] and non-FM [ 43, 41]. Future work across stakeholders is needed to effectively mitigate776
contamination and preserve benchmark validity.777
Poor construct validity. Construct validity refers to the degree to which a test or measurement778
tool accurately measures the construct it intends to measure [ 22]. [ 61] outline factors which make779
construct validity, especially in FM benchmarking, a challenge. They describe certain properties780
(e.g. factual accuracy) that arise from the interaction between the model and its user population,781
rather than from the model alone. To combat this, they suggest incorporating ecologically valid 7 user782
interactions into the assessment; yet, given the lack of transparency by model developers into actual783
user interactions, this criteria is difﬁcult to implement for benchmark developers. Alternately, [ 23]784
propose that guarantees be made through formal veriﬁcation, although this approach has not yet been785
tested in practice.786
Standardization of benchmark reporting. Due to the difﬁculties with construct validity, most787
benchmarks cannot provide an absolute signal and instead give a relative one by comparison of models788
on the same benchmark. This signal is often unavailable to potential model users, as there is no789
present standardization of benchmark reporting. Model developers report whichever benchmarks they790
see ﬁt without being obligated to provide a rationale, resulting in inconsistent reporting, especially791
apparent in the case of benchmarks relating to responsible AI concepts [ 58]. While this issue does792
not depend on further research, there is no consensus in theory or practice regarding how benchmark793
reporting should be standardized. Potential avenues towards standardization include publication of794
benchmark results through independent entities, market incentives such as government contracts, and795
mandatory reporting as part of AI legislation.796
B Stakeholders797
This section details the stakeholders that are involved in benchmark development and use processes.798
Benchmark developers. Benchmark developers are the individuals or teams who create bench-799
marks from scratch (e.g. BIG-Bench [ 74]), by expanding on previously developed benchmarks800
(e.g. MedMNIST v2 [ 89]), by integrating multiple existing benchmarks (e.g. HELM [ 48]), or by801
both expanding upon and integrating other benchmarks (e.g. Decoding Trust [ 84]). This groups802
objectives are developing benchmarks that accurately and comprehensively assess models capabilities803
or safety-critical characteristics and establishing standards for AI system evaluations that facilitate804
comparisons and drive progress on the speciﬁed tasks. There are three use cases for benchmark805
developers of our assessment, checklist, and website:806
7Ecological validity is the extent to which the ﬁndings of a research study are able to be generalized to
real-life settings [46]
20

• They use the checklist to understand best practices and guide their benchmark construction807
process pre-deployment.808
• They use the assessment to score their benchmark after constructing it to understand any809
shortcomings they may address to improve the overall benchmark quality.810
• They can use the website to ﬁnd related benchmarks and compare their benchmark quality811
to those.812
Model developers. Model developers are the individuals or teams who develop AI models for813
commercial use (e.g. GPT-4 [ 3]) or non-commercial purposes (e.g. Alpaca [ 79]). Their objectives in814
using benchmarks are demonstrating the performance of their models identifying areas for improve-815
ment which can guide model development and to establish credibility and encourage adoption by816
showcasing favorable relative performance. There are three use case for model developers of our817
assessment and website:818
• They can use the assessment results to decide which benchmarks to report819
• Model developers can reference our assessment results in their ofﬁcial reporting to indicate820
quality differences between benchmarks, if applicable821
• Model developers can use our website to ﬁnd relevant benchmarks to report for their model822
Model users. Model users are the individuals, organizations, or businesses which use or modify823
available AI models for various downstream applications (e.g. a company using ChatGPT to provide824
customer service). Their objective when using benchmark results is making informed decisions825
regarding which AI models are most suitable for their speciﬁc use cases. There are two use case for826
model users of our assessment and website:827
• If model developers dont reference our or any similar benchmark quality assessment, model828
users can refer to our assessment results on the website to understand quality differences in829
benchmarks reported by model developers.830
• They can also refer to our benchmark assessment results to decide between two related831
benchmarks who’s results may both be relevant for the model user’s application context. If832
one of these benchmarks has a higher quality, they may decide to prioritize that result based833
on our assessment.834
AI researchers. AI researchers are individuals or teams studying AI and related ﬁelds either at835
non-proﬁts, within academic institutions, in industry, or independently. One of researchers objectives836
is using benchmarks to evaluate the performance of novel AI architectures, training techniques, and837
approaches, and to compare these to other systems. Additionally, they have the objective of setting838
research agendas based on the model limitations and weaknesses revealed by benchmarks. There are839
two use case for AI researchers of our assessment and website:840
• Based on our website and assessment results, AI researchers may analyze benchmarking841
practices in more detail to understand challenges of benchmark developers and drive research842
on open questions in AI evaluations and AI benchmarking more broadly.843
• They can use our website to understand the overall AI benchmark landscape.844
Regulators and standard-setting organizations. Regulators and standard-setting organizations845
may be afﬁliated with government agencies, international bodies, and industry associations. In these846
roles, they are responsible for creating and enforcing standards and regulations for AI development847
and use. Examples of such entities are the AI Safety Institutes, the ISO, and the EU Commission.848
The objective of these stakeholders is using benchmarks to assess the compliance of AI models with849
established regulations, guidelines and standards for traits such as performance, fairness, and safety.850
For example, the UK AI Safety Institute recently released their Inspect evaluation framework [ 81]851
that includes several benchmarks that we scored in our assessment, among other evaluation strategies.852
There are two use case for model users of our assessment and website:853
21

• Regulators and standard-setting organizations can refer to our checklist to design regula-854
tory requirements, e.g., by only accepting benchmarks as proof for compliance by model855
developers that completed certain or all criteria in our checklist856
• They can also mandate that only benchmarks that achieved a certain score on our assessment857
may be used to proof compliance with regulatory requirements.858
C Benchmark Lifecycle859
Design. During the design stage, a benchmarks purpose, scope, and structure are deﬁned. This860
requires developers to identify key aspects of an AI system that the benchmark will assess. Based on861
this decision, they must determine the tasks, datasets, and evaluation metrics which will be used in862
their benchmark. To inform these decisions, developers consider the requirements of potential users,863
possibly collaborating with and gathering feedback from these and other stakeholders.864
Implementation. At this stage, the benchmark is constructed and all necessary components are865
aggregated. Developers collect, process, and (if applicable) annotate the datasets to be used for their866
tasks. They then create the evaluation scripts which allow models performance on this data to be867
measured. So that new models can be evaluated, developers may implement user interfaces and APIs868
which enable access to and interaction with the benchmark. This stage concludes with the initial869
testing and validation of benchmark components.870
Documentation. To facilitate the benchmarks use and interpretation, benchmark developers need871
to create comprehensive documentation. This includes preparing detailed descriptions of benchmark872
tasks, datasets, and evaluation metrics. Additionally, developers may provide instructions for how to873
access, use, and submit to the benchmark. Documenting design decisions, limitations, and potential874
biases enables stakeholders to make informed decisions regarding benchmark use. Creating resources875
for running the benchmark, such as quick-start guides, code documentation, and examples or tutorials876
is an essential step for accessibility.877
Maintenance. Once the benchmark and its documentation are released, developers must conduct878
regular maintenance to ensure ongoing usability. They may monitor benchmark usage and perfor-879
mance to identify areas for improvement and track users compliance with release requirements. Other880
tasks at this stage include addressing issues or bugs and incorporating user feedback into updates.881
Developers can regularly update documentation and support materials. Additionally, they can assess882
the continued relevance and utility of the benchmark by monitoring performance on the benchmark883
and responding to community feedback.884
Retirement. The ﬁnal phase of a benchmarks lifecycle is retirement. Benchmarks are phased out885
or replaced when they become saturated (i.e. model performance reaches the benchmark metrics886
ceiling), the task studied loses relevance, or better alternatives emerge. During retirement, developers887
communicate their plan to stakeholders and can provide guidance on transitioning to alternatives.888
They archive benchmark data, code, and documentation. As a benchmark is retired, developers may889
share insights gained with the AI community. Finally, they should clearly mark the benchmark as890
“retired” on channels for deployment and platforms publishing its results.891
D List of Assessed Benchmakrs892
We evaluate these 16 foundation model benchmarks (alphabetical order):893
• AgentBench [51]894
• ARC Challenge [19]895
• BBQ [64]896
22

• BIG-bench [74]897
• BOLD [26]898
• Codex HumanEval [17]899
• DecodingTrust [84]900
• GPQA [68]901
• GSM8k [21]902
• HellaSwag [93]903
• Machiavelli [63]904
• MLCommons AI Safety v0.5 [82]905
• MMLU [33]906
• MMMU [92]907
• TruthfulQA [50]908
• WinoGrande [71]909
We evaluate these 8 non-foundation model benchmarks (alphabetical order):910
• ALE [11]911
• FinRL-Meta [53]912
• MedMNIST v2 [89]913
• PDEBench [78]914
• Procgen [20]915
• RL Unplugged [31]916
• SafeBench [88]917
• Wordcraft [38]918
E Sensitivity Analysis Details919
-5.0 -2.5 0.0 2.5 5.0
Design Score Difference [a.u.]
-5.0
-2.5
0.0
2.5
5.0
Usability Score Difference [a.u.]
Mean(FM) - Mean(Non-FM)
95% Confidence Region
Figure 8: Calculating the difference between the mean Usability and Design score between foundation
model (FM) and non-foundation model (Non-FM) benchmarks with the data in Fig. 8. We show the
lack of statistical signiﬁcance of the difference using bootstrap resampling at a 95% conﬁdence level.
We show that the difference in mean usability score between FM and non-FM benchmarks in Fig. 8920
is not statistically signiﬁcant using bootstrap resampling at a 95% conﬁdence level.921
23

F Additional Results922
All individual benchmark scoring results, including justiﬁcations, can be found on betterbench.stan-923
ford.edu.924
F.1 Scores per lifecycle Stage925
MMLU
HellaSwag
GSM8k
ARC Challenge
ProcgenWordcraft
BOLD
FinRL-MetaWinoGrandeTruthfulQA
MMMU
BIG-bench
MedMNIST v2
GPQA
PDEBench
BBQ
RL Unplugged
Machiavelli
DecodingTrustAgentBenchHuman EvalSafeBench
ALE
MLCommons AI Safety v0.5
0
5
10
15Design
Foundation Model
Non-Foundation Model
Figure 9: In ascending order, design scores for each benchmark, separated for foundation model (FM)
and non-foundation model (Non-FM) benchmarks.
BOLDMMMU
ARC Challenge
BBQMMLU
HellaSwagHuman EvalMedMNIST v2
SafeBenchMachiavelli
GSM8k
WinoGrande
WordcraftTruthfulQAAgentBenchPDEBench
GPQA
BIG-bench
DecodingTrustRL Unplugged
MLCommons AI Safety v0.5
Procgen
FinRL-Meta
ALE
0
5
10
15Implementation
Foundation Model
Non-Foundation Model
Figure 10: In ascending order, implementation scores for each benchmark, separated for foundation
model (FM) and non-foundation model (Non-FM) benchmarks.
We show the scores for each benchmark and for each benchmark lifecycle stage as barplots (Design:926
Fig. 9, implementation: Fig. 10, documentation: Fig. 11, and maintenance Fig. 12). The scores for927
each benchmark for each individual category can be found on our website, betterbench.stanford.edu.928
For the bar plots for each stage, the benchmarks are shown in ascending order and marked as FM and929
non-FM benchmark.930
24

MMLU
Wordcraft
MMMU
HellaSwag
BOLD
MedMNIST v2
ProcgenGSM8k
SafeBenchMachiavelli
BBQ
RL UnpluggedARC Challenge
FinRL-Meta
ALE
Human EvalBIG-benchWinoGrande
MLCommons AI Safety v0.5
DecodingTrust
PDEBench
GPQA
AgentBenchTruthfulQA
0
5
10
15Documentation
Foundation Model
Non-Foundation Model
Figure 11: In ascending order, documentation scores for each benchmark, separated for foundation
model (FM) and non-foundation model (Non-FM) benchmarks.
MMLUGSM8k
TruthfulQA
BOLD
WinoGrandeHuman EvalRL Unplugged
Procgen
BIG-benchHellaSwag
ARC Challenge
SafeBenchWordcraftPDEBench
BBQ
MLCommons AI Safety v0.5
GPQA
MedMNIST v2
MMMU
AgentBenchFinRL-MetaMachiavelli
DecodingTrust
ALE
0
5
10
15Maintenance
Foundation Model
Non-Foundation Model
Figure 12: In ascending order, maintenance scores for each benchmark, separated for foundation
model (FM) and non-foundation model (Non-FM) benchmarks.
G Scoring931
We evaluate 24 benchmarks based on criteria grouped into category (a) (see Sec. 3), i.e., those932
controlled by the benchmark developer where the authors and interviewees reached a normative933
consensus. We use the following discrete point system to score each criteria:934
• Criteria not acknowledged and not addressed: 0 points935
• Criteria acknowledged but not addressed: 5 points936
• Criteria partially addressed: 10 points937
• Criteria fully addressed: 15 points938
• Criteria not relevant: n/a939
25

The highest possible score per category is 15, and the lowest is 0. The criteria span the benchmark940
lifecycle stages of design, implementation, documentation, and maintenance. Benchmark retirement941
is excluded from the assessment and scoring, since most benchmarks we looked at are still actively942
used and not saturated, and given that we cannot predict/anticipate if benchmark developers would943
in fact fulﬁll any criteria we’d list for this category. All individual evaluations are made publicly944
available.945
For each lifecycle stage, we calculate the average points earned across the relevant criteria for that946
stage, excluding any criteria scored as “n/a”. This results in four subscores:947
• sD = Design score948
• sI = Implementation score949
• sDo = Documentation score950
• sM = Maintenance score951
We do not differentiate the importance of criteria or effort to address them within each lifecycle stage,952
weighting them equally in the average. To provide an overall assessment of a benchmark’s design953
and usability, we aggregate the subscores into two key measures:954
• Design score SD:955
– Showcases how clear about a benchmark is about its intended purpose and scope and956
how interpretable it is957
– Equivalent to the design stage subscore sD958
• Usability score SU:959
– Indicates how easy the benchmark is use and how well it is documented and maintained960
– Weighted average of the implementation, documentation, and maintenance scores, see961
Equ. 1.962
SU = nIsI + nDosDo + nMsM
nI + nDo + nM
(1)
Where:963
• SU represents the usability score964
• sI represents the implementation score965
• sDo represents the documentation score966
• sM represents the maintenance score967
• nI represents the number of criteria in the implementation stage that are not n/a for the968
respective benchmark969
• nDo represents the number of criteria in the documentation stage that are not n/a for the970
respective benchmark971
• nM represents the number of criteria in the maintenance stage that are not n/a for the972
respective benchmark973
The discrete 0/5/10/15 point scale provides clearer differentiation between criteria that are not974
addressed, partially addressed, and fully addressed compared to a continuous scale. At the same time,975
it allows for a quantitative analysis compared to a letter grade scale like A/B/C/D. Allowing for an976
N/A option handles criteria that may not be applicable to certain benchmarks. The 0/5/10/15 scale977
also allows for more granular distinctions compared to a narrower scale like 0/1/2/3 in the ﬁnal scores:978
The difference between a score of 5 (acknowledged but not addressed) and 10 (partially addressed)979
is easier to see than between a 2 and 3 on a narrower scale. With a smaller range, the difference980
between scores is less meaningful and it is harder to separate the varying degrees of benchmark981
quality. Providing subscores for each lifecycle stage, while rolling them up into overall Design and982
Usability Scores, enables assessing benchmarks at both a category and aggregate level.983
26

H Methodology Flow Diagram984
Fig. 13 shows a detailed overview of the steps we took to derive the best practices that formed the985
basis of our AI benchmark assessment.986
Map objectives & use 
cases of stakeholder 
groups
Translate objectives 
& use case into 
requirements
Review & discuss 
with coauthors
Add, remove, and 
update criteria
Iterative 
requirements & 
lifecycle 
development – 
Phase 1
Sketch initial 
benchmark lifecycle 
framework
Identify stakeholder 
groups
Perform specific 
literature review for 
each criteria
Interview 
stakeholders
Update benchmark 
lifecycle framework
Develop initial set of 
criteria
Validate framework & 
criteria with 
stakeholders from 
each group
Review & discuss 
feedback with 
coauthors
Score set of 25 
benchmarks based 
on criteria
Analyse results
Add, clarify, or update 
criteria
Rescore previously 
scored benchmarks 
for added or updated 
criteria
Add, clarify, or update 
criteria
Iterative 
requirements 
development – 
Phase 2
Iterative 
requirements 
development – 
Phase 3
Scoring Phase
2nd scoring
In case of conflicts 
with first scoring: 
conflict resolution
In case of 
unsuccessful conflict 
resolution: 3rd scoring
Perform literature 
review
Figure 13: Flow diagram showing our detailed process how we derived the best practices for
benchmarks.
I Release Requirements987
1. Benchmark developers acknowledge that our checklist is a minimum quality assurance and988
not sufﬁcient for high-quality benchmark construction.989
2. Benchmark developers do not attempt to game our assessment, e.g. by just changing the990
code checked update on the GitHub repository side without actually checking their code’s991
usability.992
J BetterBench Checklist for Benchmark Developers993
In this section, we provide the assessment criteria as a checklist for benchmark developers to use994
during their benchmark construction process, pre-deployment of the benchmark. If benchmark995
developers want to list their benchmark on our website, they will also have to submit this checklist.996
On the website, we will further provide an easy-to-ﬁll-out checklist in L ATEXand .doc format that can997
be easily included as part of any benchmark documentation. In the second subsection, we will also998
add an example of a ﬁlled out checklist assessing BetterBench, which can be seen as a benchmark for999
benchmarks. Going through the checklist was part of the validation of our methodology, described in1000
Step 4 of the Sec. 3 section.1001
J.1 Template1002
• Benchmark Design1003
□ The tested capability, characteristic, or concept is deﬁned1004
– TODO | YES | NO | N/A1005
– Justiﬁcation:1006
□ How tested capability or concept translates to benchmark task is described1007
– YES | NO | N/A1008
– Justiﬁcation:1009
□ How knowing about the tested concept is helpful in the real world is described.1010
27

– YES | NO | N/A1011
– Justiﬁcation:1012
□ How benchmark score should or shouldn’t be interpreted/used is described1013
– YES | NO | N/A1014
– Justiﬁcation:1015
□ Domain experts are involved1016
– YES | NO | N/A1017
– Justiﬁcation:1018
□ Use cases and/or user personas are described1019
– YES | NO | N/A1020
– Justiﬁcation:1021
□ Domain literature is integrated1022
– YES | NO | N/A1023
– Justiﬁcation:1024
□ Informed performance metric choice1025
– YES | NO | N/A1026
– Justiﬁcation:1027
□ Metric ﬂoors and ceilings are included1028
– YES | NO | N/A1029
– Justiﬁcation:1030
□ Human performance level is included1031
– YES | NO | N/A1032
– Justiﬁcation:1033
□ Random performance level is included1034
– YES | NO | N/A1035
– Justiﬁcation:1036
□ Automatic evaluation is possible and validated1037
– YES | NO | N/A1038
– Justiﬁcation:1039
□ Differences to related benchmarks are explained1040
– YES | NO | N/A1041
– Justiﬁcation:1042
□ Input sensitivity is addressed1043
– YES | NO | N/A1044
– Justiﬁcation:1045
• Benchmark Implementation1046
□ The evaluation code is available1047
– YES | NO | N/A1048
– Justiﬁcation:1049
□ The evaluation data or generation mechanism is accessible1050
– YES | NO | N/A1051
– Justiﬁcation:1052
□ The evaluation of models via API is supported1053
– YES | NO | N/A1054
– Justiﬁcation:1055
□ The evaluation of local models is supported1056
– YES | NO | N/A1057
28

– Justiﬁcation:1058
□ A globally unique identiﬁer is added or evaluation instances are encrypted1059
– YES | NO | N/A1060
– Justiﬁcation:1061
□ A task to identify if model is included trained on benchmark data1062
– YES | NO | N/A1063
– Justiﬁcation:1064
□ A script to replicate results is explicitly included1065
– YES | NO | N/A1066
– Justiﬁcation:1067
□ Statistical signiﬁcance or uncertainty quantiﬁcation of benchmark results is reported1068
– YES | NO | N/A1069
– Justiﬁcation:1070
□ Need for warnings for sensitive/harmful content is assessed1071
– YES | NO | N/A1072
– Justiﬁcation:1073
□ A build status (or equivalent) is implemented1074
– YES | NO | N/A1075
– Justiﬁcation:1076
□ Release requirements are speciﬁed1077
– YES | NO | N/A1078
– Justiﬁcation:1079
• Benchmark Documentation1080
□ Requirements ﬁle or equivalent is available1081
– YES | NO | N/A1082
– Justiﬁcation:1083
□ Quick-start guide or demo is available1084
– YES | NO | N/A1085
– Justiﬁcation:1086
□ In-line code comments are used1087
– YES | NO | N/A1088
– Justiﬁcation:1089
□ Code documentation is available1090
– YES | NO | N/A1091
– Justiﬁcation:1092
□ Accompanying paper is accepted at peer-reviewed venue1093
– YES | NO | N/A1094
– Justiﬁcation:1095
□ Benchmark construction process is documented1096
– YES | NO | N/A1097
– Justiﬁcation:1098
□ Test tasks & rationale are documented1099
– YES | NO | N/A1100
– Justiﬁcation:1101
□ Assumptions of normative properties are documented1102
– YES | NO | N/A1103
– Justiﬁcation:1104
29

□ Limitations are documented1105
– YES | NO | N/A1106
– Justiﬁcation:1107
□ Data collection, test environment design, or prompt design process is documented1108
– YES | NO | N/A1109
– Justiﬁcation:1110
□ Evaluation metric is documented1111
– YES | NO | N/A1112
– Justiﬁcation:1113
□ Applicable license is speciﬁed1114
– YES | NO | N/A1115
– Justiﬁcation:1116
• Benchmark Maintenance1117
□ Code usability was checked within the last year1118
– YES | NO | N/A1119
– Justiﬁcation:1120
□ Maintained feedback channel for users is available1121
– YES | NO | N/A1122
– Justiﬁcation:1123
□ Contact person is listed1124
– YES | NO | N/A1125
– Justiﬁcation:1126
J.2 Example1127
As noted in Sec. 3, we assessed BetterBench against our own assessment framework to verify that the1128
framework is usable and practiable. This section showcases this assessment and gives an example of1129
a ﬁlled-out checklist, based on the template provided in App. J.1,1130
• Benchmark Design1131
□ The tested capability, characteristic, or concept is deﬁned1132
– YES1133
– Justiﬁcation: “We deﬁne a high-quality benchmark to be one that is clear about its1134
intended purpose and scope, and that is usable. To date, no structured assessment1135
for the quality of AI benchmarks, including both FM and non-FM benchmarks, has1136
been published to date, and no comparative analysis was conducted to understand1137
quality differences between widely used benchmarks in the ﬁeld. This paper1138
addresses these gaps”(Sec. 1)1139
□ How tested capability or concept translates to benchmark task is described1140
– YES1141
– Justiﬁcation: For detail, see Sec. 4 and App. K1142
□ How knowing about the tested concept is helpful in the real world is described.1143
– YES1144
– Justiﬁcation: Justiﬁcation: “By releasing the ﬁrst systematic assessment framework1145
for AI benchmarks, we aim to encourage benchmark developers to construct higher-1146
quality benchmarks and to contribute to community efforts to make AI evaluations1147
more practicable and transparent. Higher-quality benchmarks resulting from the1148
adoption of our framework and checklist can lead to better-informed model selection1149
for downstream tasks, potentially reducing risks and improving outcomes in high-1150
stakes applications” (Sec. 9).1151
30

□ How benchmark score should or shouldn’t be interpreted/used is described1152
– YES1153
– Justiﬁcation: “Our living repository of benchmark assessments promotes trans-1154
parency and comparability, allowing benchmark users to make informed decisions1155
when choosing benchmarks. However, there is a potential risk of misinterpretation1156
of our results; our assessment only provides minimum quality assurances and is not1157
sufﬁcient to assess the suitability of a benchmark for a concrete use case” (Sec. 9).1158
□ Domain experts are involved1159
– YES1160
– Justiﬁcation: “Initially, we surveyed the existing benchmark landscape (Sec. 2).1161
Based on this review, we identiﬁed ﬁve stakeholder groups who present the user1162
personas of our assessment (App. B). All stakeholder groups were represented1163
in subsequent unstructured interviews which included 20+ policymakers, model1164
developers, benchmark developers, model users, and AI researchers, to understand1165
their objectives w.r.t. benchmarking. During this process, we developed a ﬁve-1166
stage model of the benchmark lifecycle (Fig. 5 and App. C) and mapped the1167
benchmarking objectives of the stakeholders, along with their communicated use1168
cases of a benchmark assessment (App. B)” (Sec. 3).1169
□ Use cases and/or user personas are described1170
– YES1171
– Justiﬁcation: “We identiﬁed ﬁve stakeholder groups who present the user personas1172
of our assessment” (Sec. 3, see full personas and use cases in App. B).1173
□ Domain literature is integrated1174
– YES1175
– Justiﬁcation: “Our work is informed by benchmarking practices from ﬁelds be-1176
yond AI, ranging from transistor hardware [ 18] to environmental quality [ 16] to1177
bioinformatics [ 7], and identify common themes regarding what constitutes an1178
effective benchmark. When applicable, we incorporate these best practices into1179
our assessment (Sec. 4).” Citations for this literature, when used, are provided in1180
Sec. 4.1181
□ Informed performance metric choice1182
– YES1183
– Justiﬁcation: “The discrete 0/5/10/15 point scale provides clearer differentiation1184
between criteria that are not addressed, partially addressed, and fully addressed1185
compared to a continuous scale. At the same time, it allows for a quantitative1186
analysis compared to a letter grade scale like A/B/C/D. Allowing for an N/A option1187
handles criteria that may not be applicable to certain benchmarks.” Full details on1188
our scoring method are available in App. G.1189
□ Metric ﬂoors and ceilings are included1190
– YES1191
– Justiﬁcation: “The highest possible score per category is 15, and the lowest is 0”1192
(App. G).1193
□ Human performance level is included1194
– N/A1195
– Justiﬁcation: In our work, we manually evaluate AI benchmarks; a human could1196
not be used as an evaluation target in our context.1197
□ Random performance level is included1198
– N/A1199
– Justiﬁcation: Random generation cannot constitute an AI benchmark.1200
□ Automatic evaluation is possible and validated1201
– N/A1202
31

– Justiﬁcation: “Given the varying information sources (ofﬁcial websites, papers,1203
GitHub repositories published by the benchmark developers that we do consult1204
to assess benchmarks, and given that they do not follow a standard structure, we1205
manually evaluate all benchmarks” (Sec. 3).1206
□ Differences to related benchmarks are explained1207
– YES1208
– Justiﬁcation: “Unlike prior studies, such as [ 59] and [ 49], which focus on identify-1209
ing the limitations, our approach offers a practical evaluation, empowering develop-1210
ers to address shortcomings and enhance benchmark quality directly” (Sec. 2.1).1211
□ Input sensitivity is addressed1212
– N/A1213
– Justiﬁcation: Since our benchmark uses human evaluation, we select a single1214
phrasing for each criterion. As described in Sec. 3 these phrasings were devel-1215
oped iteratively to maximize clarity and minimize disagreement amongst multiple1216
annotators of the same benchmmark.1217
• Benchmark Implementation1218
□ The evaluation code is available1219
– N/A1220
– Justiﬁcation: We performed human evaluation which did not use code.1221
□ The evaluation data or generation mechanism is accessible1222
– N/A1223
– Justiﬁcation: We evaluate benchmarks based on “ofﬁcial websites, papers, GitHub1224
repositories published by the benchmark developers” (Sec. 3). The availability of1225
these materials is dependent on benchmark developers.1226
□ The evaluation of models via API is supported1227
– N/A1228
– Justiﬁcation: We evaluate benchmarks rather than models.1229
□ The evaluation of local models is supported1230
– N/A1231
– Justiﬁcation: We evaluate benchmarks rather than models.1232
□ A globally unique identiﬁer is added or evaluation instances are encrypted1233
– N/A1234
– Justiﬁcation: Our benchmark does not evaluate AI models or include any examples1235
which they could be contaminated by training on.1236
□ A task to identify if model is included trained on benchmark data1237
– N/A1238
– Justiﬁcation: Our benchmark does not evaluate AI models or include any examples1239
which they could be contaminated by training on.1240
□ A script to replicate results is explicitly included1241
– N/A1242
– Justiﬁcation: The code to replicate results will be added as supplementary material1243
and published as part of a GitHub repo upon publication.1244
□ Statistical signiﬁcance or uncertainty quantiﬁcation of benchmark results is reported1245
– YES1246
– Justiﬁcation: These results are reported in Sec. 6 and App. E.1247
□ Need for warnings for sensitive/harmful content is assessed1248
– YES1249
– Justiﬁcation: “The outputs of our evaluation do not contain sensitive or harmful1250
content, but users may encounter such content during a benchmark assessment1251
depending on the benchmark’s data” (Sec. 9).1252
32

□ A build status (or equivalent) is implemented1253
– YES1254
– Justiﬁcation: A build status will be included in the code released as part of a GitHub1255
repo upon publication.1256
□ Release requirements are speciﬁed1257
– YES1258
– Justiﬁcation: Release requirements are provided in App. I.1259
• Benchmark Documentation1260
□ Requirements ﬁle or equivalent is available1261
– YES1262
– Justiﬁcation: A requirements ﬁle will be included in the code released as part of a1263
GitHub repo upon publication.1264
□ Quick-start guide or demo is available1265
– YES1266
– Justiﬁcation: We provide a checklist to facilitate use of our benchmark in App. J1267
and an example of its use in App. J.2. Additionally, we will include a quick-start1268
guide for our code in the GitHub repo released upon publication.1269
□ In-line code comments are used1270
– YES1271
– Justiﬁcation: Our GitHub repository includes in-line code comments.1272
□ Code documentation is available1273
– YES1274
– Justiﬁcation: Our GitHub repository includes code documentation.1275
□ Accompanying paper is accepted at peer-reviewed venue1276
– N/A1277
– Justiﬁcation: Our paper is currently under submission at a peer-reviewed venue.1278
□ Benchmark construction process is documented1279
– YES1280
– Justiﬁcation: We describe our full process in Sec. 3.1281
□ Test tasks & rationale are documented1282
– YES1283
– Justiﬁcation: Deﬁnitions and justiﬁcations for all criteria are presented in App. K.1284
□ Assumptions of normative properties are documented1285
– YES1286
– Justiﬁcation:1287
□ Limitations are documented1288
– YES1289
– Justiﬁcation: We discuss limitations in Sec. 8.1290
□ Data collection, test environment design, or prompt design process is documented1291
– YES1292
– Justiﬁcation: We describe how we performed our evaluations in Sec. 3.1293
□ Evaluation metric is documented1294
– YES1295
– Justiﬁcation: “We deﬁne a high-quality benchmark to be one that is interpretable1296
and clear about its intended purpose and scope, and that is usable” Sec. 1. We1297
further describe how we operationalized “quality” and calculate its subcomponents1298
(design and usability) in Fig. 9 and Sec. 3.1299
□ Applicable license is speciﬁed1300
33

– YES1301
– Justiﬁcation: We release our assessment under CC BY 4.0 license, available on our1302
website (Sec. 3).1303
• Benchmark Maintenance1304
□ Code usability was checked within the last year1305
– YES1306
– Justiﬁcation: We have checked the usability of the code in our GitHub repository1307
and will verify it again upon publication.1308
□ Maintained feedback channel for users is available1309
– YES1310
– Justiﬁcation: “Finally, we develop a supplementary website to continuously publish1311
assessment results using the scoring methodology in App. G, given the rapid1312
development of new benchmarks. The website includes a community feedback1313
channel for submitting new AI benchmarks and correcting previously posted scores1314
if benchmarks are updated or stakeholders disagree with our evaluation” (Sec. 3).1315
□ Contact person is listed1316
– YES1317
– Justiﬁcation: Contact details will be listed on our website.1318
K Full Assessment Criteria1319
K.1 Benchmark Design1320
1. Deﬁnition of tested capability or characteristic1321
• Explanation: The benchmark developers mention and deﬁne what underlying capabil-1322
ity or characteristic of a model is supposed to be tested with the benchmark.1323
• Justiﬁcation: Deﬁning the objective of the benchmark is necessary for clarity in1324
its design. It also helps users determine if the benchmark aligns with their speciﬁc1325
application needs and ensures that users and developers have a shared understanding of1326
the concept being evaluated, facilitating consistent interpretation of results.1327
• Points:1328
– 0: Tested concept, capability, or characteristic not explicitly mentioned.1329
– 5: Tested concept explicitly mentioned and need for deﬁnition acknowledged, but1330
deﬁnition not provided.1331
– 10: Tested concept, capability, or characteristic explicitly mentioned but not deﬁned.1332
– 15: Tested concept, capability, or characteristic explicitly mentioned and deﬁned.1333
2. Description of how tested capability or concept translates to benchmark task1334
• Explanation: The benchmark developers describe how the tested capability or charac-1335
teristic translates to the task implemented in the benchmark/the task the model is tested1336
on in the benchmark.1337
• Justiﬁcation: Clearly explaining this translation ensures that the benchmark tasks accu-1338
rately reﬂect the intended tested capabilities and concepts, providing valid assessment1339
results.1340
• Points:1341
– 0: No description of how the tested capability or concept translates to the benchmark1342
task.1343
– 5: Acknowledgement that not describing how the tested capability or concept1344
translates to the benchmark task is an issue, but no description provided.1345
– 10: Description of how tested capability or concept translates to benchmark tasks1346
provided for some but not all tasks.1347
34

– 15: Description of how tested capability or concept translates to benchmark tasks1348
provided for all tasks.1349
3. Description of how knowing about the tested concept is helpful in the real world1350
• Explanation: The developers describe why it is useful to know about the tested1351
capability in the real world.1352
• Justiﬁcation: This description helps users understand the practical value of the bench-1353
mark, demonstrating how the tested capability impacts real-world applications and use1354
cases.1355
• Points:1356
– 0: No description of how knowing about the tested concept is helpful in the real1357
world.1358
– 5: Acknowledgement that not describing how knowing about the tested concept is1359
helpful in the real world is an issue, but no description provided.1360
– 10: Limited description of how knowing about the tested concept is helpful in the1361
real world.1362
– 15: Full description of how knowing about the tested concept is helpful in the real1363
world.1364
4. Description of use cases and user personas for the benchmark1365
• Explanation: A use case for an AI benchmark involves specifying a scenario in1366
which the AI system will be evaluated. This scenario should include the cultural and1367
geographic context and the type of interactions between humans and models [ 82], if1368
applicable. Additionally, user personas should be deﬁned to represent the different1369
types of users that might interact with the AI system, if applicable. As a concrete1370
example, [ 82] states “The use case for the v0.5 Benchmark is an adult chatting to a1371
general-purpose assistant in English. The cultural and geographic context is Western1372
Europe & North America. We deﬁne a use case as a set of interactions between human1373
and model to achieve a goal (or goals). [...] For the v0.5 Benchmark, we are focusing on1374
three personas: (i) a typical adult user; (ii) an adult user intent on malicious activities,1375
behaving in a technically non-sophisticated way; and (iii) an adult user at risk of harm,1376
behaving in a technically non-sophisticated way.”1377
• Justiﬁcation: Use cases set the context and scope of the benchmark. User personas1378
outline an understanding of the different types of interactions the benchmark developers1379
anticipate the tested AI system to be used in, e.g., ranging from typical users to those1380
with speciﬁc challenges or malicious intent. This approach ensures that the design of1381
the benchmark is closely related to real-world applications and that it’s effective across1382
diverse scenarios.1383
• Points:1384
– 0: The benchmark does not include any description of use cases or user personas.1385
– 5: The benchmark acknowledges the importance of use cases or user personas but1386
does not explicitly formulate or describe them.1387
– 10: The benchmark provides a partial description of use cases or user personas.1388
– 15: The benchmark fully describes use cases and user personas, specifying the1389
cultural and geographic context, types of human-model interactions (if applicable),1390
and representing different user types that might interact with the AI system (if1391
applicable).1392
– n/a: For AI systems that do not involve direct human interaction, such as those1393
used in industrial automation or scientiﬁc simulations, deﬁning user personas is not1394
relevant. However, real-world use cases should still be speciﬁed; in more theoretical1395
benchmarks, this use case might be to advance research.1396
5. Involvement of domain experts1397
35

• Explanation: Domain expert(s) who have a professional background or research1398
experience in the concept to be tested are either co-authors of the paper, or were1399
involved in the benchmark design process, i.e., the paper makes clear how they obtained1400
the expertise and how that informed the benchmark design.1401
• Justiﬁcation: Involving domain experts ensures that the benchmark design is informed1402
by deep, specialized knowledge, increasing its validity and relevance. This expertise1403
helps to create tasks that accurately assess the targeted capabilities and align with1404
real-world scenarios.1405
• Points:1406
– 0: None of the authors has a background in the benchmark domain and no external1407
experts were consulted during the design process.1408
– 5: The benchmark mentions the necessity for in-domain expertise but doesn’t1409
specify any further details.1410
– 10: The benchmark mentions that domain experts were consulted but not how their1411
insights inﬂuenced the benchmark design.1412
– 15: At least one of the co-authors has a professional or academic background in the1413
benchmark domain or the benchmark speciﬁed how external experts were consulted1414
and how that inﬂuenced the design process.1415
6. Integration of domain literature1416
• Explanation: The developers cite domain literature in the background section and1417
describe how insights from this literature informed the design of their benchmark or1418
cite relevant domain literature in the benchmark design process.1419
• Justiﬁcation: By consulting domain-speciﬁc literature, benchmark developers can1420
ensure that the tasks and evaluation criteria they include are representative and aligned1421
with the current state of knowledge in the ﬁeld. This literature often contains valuable1422
insights into best practices, established methodologies, and proven approaches for1423
evaluating the tested concept, which can be incorporated into the benchmark design to1424
enhance its reliability.1425
• Points:1426
– 0: The benchmark does not reference domain-speciﬁc literature.1427
– 5: The benchmark mentions the need to integrate domain literature but did not1428
address it in the background section or design process.1429
– 10: The benchmark references domain literature in the background or related work1430
section but does not describe how that domain literature informed the benchmark1431
design process.1432
– 15: The benchmark references domain literature throughout the paper and describes1433
how that domain literature informed the benchmark design process.1434
7. Description of how benchmark score should or shouldn’t be interpreted/used1435
• Explanation: The benchmark developers provide information about what benchmark1436
users can and cannot take away from the benchmark score.1437
• Justiﬁcation: Clarifying the interpretation of benchmark scores prevents misuse and1438
misinterpretation, ensuring that users draw accurate conclusions about a model’s1439
performance. This guidance helps users apply the scores appropriately within their1440
speciﬁc contexts, and understand if the benchmark can be used to assess a model for1441
their desired application context.1442
• Points:1443
– 0: The benchmark does not comment on how the benchmark scores should or1444
should not be interpreted.1445
– 5: The benchmark acknowledges that the benchmark scores need to be interpreted1446
but gives no guidance on how or how not to do that.1447
36

– 10: The benchmark describes how scores should or should not be interpreted or1448
used, but not both.1449
– 15: The benchmark describes how scores should and should not be interpreted or1450
used.1451
8. Informed choice of performance metric(s)1452
• Explanation: The developers describe how the performance metric for the deﬁned1453
benchmark task should be interpretable, meaningful, and standard for the task thats1454
being evaluated [ 34]. If a non-standard metric is selected, they describe their rationale1455
for choosing a non-standard metric.1456
• Justiﬁcation: The metric should be easily understood by the reader to build their own1457
opinion about the model’s capabilities, given the benchmark score. If a non-standard1458
metric is used, an explanation is necessary to clarify its relevance and ensure that users1459
can accurately interpret the results. [34]1460
• Points:1461
– 0: The benchmark does not mention an evaluation metric or does not explain the1462
choice of metric.1463
– 5: The benchmark acknowledges the need for an informed metric choice but does1464
not justify their metric choice.1465
– 10: The benchmark provides an explanation for the choice of some but not all of1466
their metrics.1467
– 15: The benchmark provides an explanation for the choice of all of their metrics.1468
9. Includes ﬂoors and ceilings for metric1469
• Explanation: The benchmark provides clear ﬂoors and ceilings for the metric(s) it1470
uses [34].1471
• Justiﬁcation: Establishing clear ﬂoors and ceilings for metrics ensures that users have1472
a reference point for understanding model performance. It helps users understand if a1473
benchmark is already saturated or if progress can be made on the task [ 34]. This also1474
allows benchmark developers to decide when a benchmark should be retired.1475
• Points:1476
– 0: The benchmark does not provide any metric ﬂoors or ceilings.1477
– 5: Floors and ceilings are shown in the results ﬁgure but not explicitly mentioned1478
in the text.1479
– 10: The benchmark provides ﬂoors and ceilings for some but not all evaluation1480
metrics.1481
– 15: The benchmark provides ﬂoors and ceilings for all evaluation metrics.1482
10. Includes human performance level1483
• Explanation: The benchmark explicitly states human performance measured on the1484
benchmark task [ 34]. It also explains how human performance was measured and if1485
this was the performance of an average or expert group of humans. The benchmark1486
notes if measuring human performance is not possible on the benchmark task and why.1487
• Justiﬁcation: Similar to the previous criteria, including human performance on a1488
benchmark allows the reader to put the models performance into perspective and allows1489
for a better interpretability of the benchmarking score [34].1490
• Points:1491
– 0: The benchmark does not state human performance and does not explain why1492
this is not applicable here.1493
– 5: The benchmark mentions human performance in passing but does not provide a1494
measurement or explanation.1495
– 10: The benchmark states human performance but does not explain how it was1496
obtained.1497
37

– 15: The benchmark states human performance and explains how it was obtained.1498
– n/a: The benchmark task cannot be completed by a human, and hence reporting1499
human performance is not possible.1500
11. Includes random performance level1501
• Explanation: The developers explicitly states the random performance measured on1502
the benchmark [34].1503
• Justiﬁcation: By establishing a baseline performance level achieved through random1504
guessing, generation, or selection, benchmark users can better understand the extent1505
to which a model’s performance stems from its inherent capabilities, rather than1506
mere chance or the benchmarks design and especially metric choices. This random1507
performance level serves as a reference point, allowing for a clearer assessment of the1508
model’s true effectiveness in tackling the speciﬁc task at hand.1509
• Points:1510
– 0: The benchmark does not state random performance and does not explain why1511
this is not applicable here.1512
– 5: The benchmark mentions random performance but does not provide quantitative1513
random performance on the benchmark task(s).1514
– 10: The benchmark states random performance for some but not all tasks.1515
– 15: The benchmark states random performance for all tasks.1516
– n/a: Measuring random performance on the benchmark task is not possible, and1517
hence reporting random performance is not applicable.1518
12. Addresses input sensitivity1519
• Explanation: The benchmark contains multiple input variations with the same semantic1520
meaning/intended to elicit the same response or output by the tested model. The1521
developers describe all relevant details such as how many different variations were1522
tested per prompt, and how the variations were designed. For language models, this1523
would mean including a variety of semantically (but not syntactically) equivalent1524
prompts to combat prompt sensitivity [ 73, 42, 55, 72]. For computer vision models,1525
this could mean inputting a normal, a blurred, and a cropped version of the same image,1526
etc.), while for reinforcement learning, this could mean measuring the sensitivity of1527
learned policies to input features [56].1528
• Justiﬁcation: Addressing input sensitivity in a benchmark ensures that the model’s1529
performance is consistent across semantically equivalent inputs, thus validating its1530
robustness. Including multiple variations per input and detailing their design allows for1531
inspection and replicable evaluation of the model’s capabilities. This serves the goal of1532
approximating intrinsic model capabilities or harms better rather than just measuring1533
“an artifact” [61] of your input.1534
• Points:1535
– 0: The benchmark does not mention or address input sensitivity.1536
– 5: The benchmark mentions the issue of input sensitivity but does not describe1537
experiments to test for it.1538
– 10: The benchmark includes some input variations with the same semantic meaning1539
but lacks thorough descriptions or details on the number of variations and their1540
design.1541
– 15: The benchmark contains multiple input variations with the same semantic1542
meaning, providing detailed descriptions of all relevant details such as the number1543
of variations per prompt and how they were designed.1544
13. Validated automatic evaluation available1545
• Explanation: Evaluating a model against a benchmark does not require human evalua-1546
tion in the process and the quality of the automated evaluation is validated (if applicable,1547
e.g., in the case of FM-based evaluations).1548
38

• Justiﬁcation: Requiring human feedback to evaluate performance on a benchmark will1549
signiﬁcantly limit the scalability of the benchmark and potentially introduce biases from1550
the human evaluators themselves. In addition, this may require an IRB for researchers,1551
and will be more costly than an automatic evaluation, creating “major barriers to entry”1552
[34].1553
• Points:1554
– 0: The benchmark does not provide any form of automatic evaluation and relies1555
entirely on human evaluation.1556
– 5: The benchmark mentions the beneﬁts of automatic evaluation but provides no or1557
limited automatic valuation.1558
– 10: The benchmark includes an automatic evaluation method but does not offer any1559
validation.1560
– 15: The benchmark includes an automatic evaluation method and describes how it1561
was validated as well as the results of the validation.1562
14. Explanation of differences to related benchmarks1563
• Explanation: The benchmark developers explain how their benchmark ﬁlls a gap1564
compared to existing benchmarks or how it expands on existing benchmarks or their1565
tested concepts.1566
• Justiﬁcation: Benchmark developers demonstrate the added value and relevance of1567
the new benchmark, justifying its necessity by addressing speciﬁc gaps in existing1568
benchmarks or by expanding on saturated benchmarks. This allows users to better1569
understand the differences between related benchmarks and determine which one to1570
use for their speciﬁc evaluation context.1571
• Points:1572
– 0: The benchmarks do not explain any differences or relevance to existing bench-1573
marks.1574
– 5: The benchmark brieﬂy mentions existing benchmarks but provides no explana-1575
tions of differences or added value.1576
– 10: The benchmark provides an explanation of how it ﬁlls a gap or expands on1577
existing benchmarks for some but not all mentioned related benchmarks.1578
– 15: The benchmark provides an explanation of how it ﬁlls a gap or expands on1579
existing benchmarks for all mentioned related benchmarks.1580
K.2 Benchmark Implementation1581
1. Availability of evaluation code1582
• Explanation: The benchmark developers make the code available for others to evaluate1583
their own models against the benchmark, e.g., as part of a GitHub repository.1584
• Justiﬁcation:1585
• Points: Without access to the benchmarking procedure itself, the benchmark cannot1586
be scrutinized by external parties to verify its reliability and adequacy, nor can it be1587
utilized for independent evaluations and comparisons by benchmark users. In addition,1588
if benchmark users have to write their evaluation code from scratch, its more likely that1589
seemingly minor implementation details affect the measured performance, hindering a1590
fair comparison [13].1591
– 0: The evaluation code is not publicly available.1592
– 5: The benchmark mentions the availability of evaluation code but does not provide1593
access to it.1594
– 10: The evaluation code is publicly available for some metrics described by the1595
benchmark.1596
– 15: The evaluation code is publicly available for all metrics described by the1597
benchmark.1598
39

2. Script to replicate results is explicitly included1599
• Explanation: The benchmark developers give access to the input, output, and evalua-1600
tion code, as well as all other necessary information (e.g., hyperparameters or random1601
seed set) that they used to create the initial benchmarking results presented in the paper.1602
• Justiﬁcation: Providing access to the input, output, and code allows for transparency1603
and reproducibility of the reported results, fostering trust into the benchmark, and1604
contributing to overcome the current reproducibility crisis in AI/ML research [35].1605
• Points:1606
– 0: The developers do not provide a script to reproduce the results.1607
– 5: The issue of result replicability is mentioned in the benchmark paper but not1608
addressed.1609
– 10: A script to reproduce some results in the benchmark paper is available.1610
– 15: A script to reproduce all results in the benchmark paper is available.1611
3. Accessibility of evaluation data, prompts, or dynamic environment1612
• Explanation: The benchmark developers make the evaluation data, prompts, or the1613
data/environment generation mechanism accessible. These do not have to be made1614
public in order to earn full points (if contamination is a concern, for example), but1615
some access to it for evaluation purposes, e.g., by hosting it privately on Hugging Face,1616
needs to be possible.1617
• Justiﬁcation: Without any accessibility of the evaluation data, prompts, or environment1618
generation mechanism, a benchmark cannot be used.1619
• Points:1620
– 0: No access to evaluation data, prompts, or data/environment generation mecha-1621
nism is provided.1622
– 5: The existence of evaluation data, prompts, or data/environment generation1623
mechanism is mentioned, but no concrete access is provided.1624
– 10: Partial access to evaluation data, prompts, or data/environment generation1625
mechanism is provided, allowing for limited evaluation.1626
– 15: Full access to evaluation data, prompts, or data/environment generation mecha-1627
nism is provided, enabling comprehensive evaluation.1628
4. Supports evaluation of models via API calls1629
• Explanation: The benchmark developers allow the benchmark evaluation of models1630
via API access, if applicable.1631
• Justiﬁcation: This criteria is dependent on the subﬁeld. In NLP, for example, closed-1632
source models such as GPT-4 are oftentimes only accessible via API. Without support1633
for API evaluation, they cannot be evaluated, which is especially problematic if such1634
models are the state-of-the-art models in the ﬁeld.1635
• Points:1636
– 0: The benchmark does not support evaluation of models via API calls.1637
– 5: The benchmark mentions the possibility of API evaluation but does not provide1638
concrete implementation details.1639
– 10: The benchmark supports evaluation of models via one API.1640
– 15: The benchmark supports evaluation of models via two or more APIs to different1641
models.1642
5. Supports evaluation of local models1643
• Explanation: The benchmark developers implement code to support the evaluation of1644
local models without API access.1645
• Justiﬁcation: Some model developers only host their models locally. A benchmark1646
should support the evaluation of those to allow for a wide variety of models to be1647
evaluated against the benchmark.1648
40

• Points:1649
– 0: The benchmark requires users to write their own code to evaluate a local model.1650
– 5: The benchmark mentions that local evaluation should be possible but doesn’t1651
provide corresponding code.1652
– 10: The benchmark provides minimal support for local model evaluation, requiring1653
signiﬁcant user effort.1654
– 15: The benchmark provides full support for local model evaluation with user-1655
friendly code.1656
6. Inclusion of a globally unique identiﬁer or encryption of evaluation instances1657
• Explanation: Benchmark developers include a globally unique identiﬁer (GUID) or1658
canary string in the main public evaluation code and all public evaluation prompt or1659
data ﬁles. Alternatively, they encrypt the test data ﬁles and make the key public.1660
• Justiﬁcation: Including a GUID in relevant (sub-)repositories, public code and data1661
repositories can support the identiﬁcation of data contamination in models [ 74], either1662
by allowing model developers to ﬁlter out the evaluation data out of large amounts1663
of web-scraped data or by allowing benchmark developers to identify which model1664
developers trained on their data and hence have created models that potentially perform1665
better than they would otherwise on the benchmark. Encrypted test data ﬁles prevent1666
non-adversarial crawling of such data; however, [ 36] advise against “using standard1667
obfuscation or compression methods that are not key-protected, since some crawling1668
systems include pipelines of automatic decompression or deobfuscation.”1669
• Points:1670
– 0: The benchmark does not include a GUID or encryption of evaluation instances.1671
– 5: The benchmark acknowledges the risk of contamination but does not address it.1672
– 10: The benchmark partially implements a GUID or encryption, but not consistently1673
across all relevant ﬁles.1674
– 15: The benchmark consistently includes a GUID or encryption across all relevant1675
ﬁles and repositories.1676
7. Inclusion of ’training_on_test_set’ task1677
• Explanation: The benchmark includes a task to identify if the model was trained on1678
the benchmark data.1679
• Justiﬁcation: Public benchmarks face the challenges that their evaluation data may be1680
web-scraped and used to train a model. A ’training_on_test_set’ task can serve as a1681
“post-hoc diagnosis of whether [... benchmark] data was used in model training.” [ 74]1682
• Points:1683
– 0: The benchmark does not include a ’training_on_test_set’ task.1684
– 5: The benchmark mentions the possibility that models were trained on its data but1685
does not provide a way to check it.1686
– 10: The benchmark includes a partial or limited implementation of a ’train-1687
ing_on_test_set’ task that only tests for part of the data used.1688
– 15: The benchmark includes a comprehensive ’training_on_test_set’ task.1689
8. Assess need for warnings for sensitive/harmful content1690
• Explanation: Benchmark developers explicitly mention in the paper if the evaluation1691
tasks or the expected output may contain sensitive or harmful content. If they do not1692
anticipate sensitive/harmful content in either case, they should explicitly state that.1693
• Justiﬁcation: By explicitly stating the presence of sensitive or harmful content and1694
issuing appropriate warnings, developers help users make informed decisions and take1695
necessary precautions. Even if developers do not expect sensitive or harmful content, if1696
they state that, they showcase to the benchmark users that they actually thought about1697
the possibility. Otherwise, users couldn’t be sure if the input or output doesn’t contain1698
problematic content or if the developers just forgot to include a warning.1699
41

• Points:1700
– 0: The benchmark does not mention that they checked for the presence or absence1701
of sensitive/harmful content in the evaluation tasks or expected output.1702
– 5: The benchmark mentions the general possibility of sensitive/harmful content but1703
does not provide clear statements or warnings.1704
– 10: The benchmark explicitly states the presence or absence of sensitive/harmful1705
content for either the evaluation tasks or the expected output.1706
– 15: The benchmark explicitly states the presence or absence of sensitive/harmful1707
content for both the evaluation tasks and the expected output.1708
9. Release requirements speciﬁed1709
• Explanation: Benchmark developers specify rules for benchmark users to “ensure1710
the integrity of test results” [ 82]. While not all benchmark developers will be able to1711
enforce the release requirements, they should at least specify them. One example is:1712
“1. Publishers do not train directly on or against the benchmark dataset and retract any1713
reported results if and when benchmark data is found to have been in training data. 2.1714
Techniques that are likely to increase the test performance without a commensurate1715
increase in safety factor are discouraged and may result in benchmark exclusion. [...]”1716
[82]1717
• Justiﬁcation: Written terms of use can help to set expectations and have a foundation1718
to address subsequent contamination or intentional gamiﬁcation attempts of the bench-1719
mark. Potential options they could mention in case of release requirement breaches are,1720
e.g., “publishing public statements correcting the public record” or “resulting in the1721
[model] being permanently banned from the benchmark” [ 82]; however, we will not1722
assess the enforcement ability or potential listed sanctions as part of this criteria, just1723
the statement of release requirements.1724
• Points:1725
– 0: The benchmark does not specify any release requirements for benchmark users.1726
– 5: The benchmark brieﬂy mentions the issue of potential gameability or misuse by1727
benchmark users but does not provide speciﬁc details.1728
– 10: The benchmark states dos and donts how to use the benchmark but does not1729
specify these as requirements for use.1730
– 15: The benchmark provides a set of release requirements for benchmark users.1731
10. Includes Build Statusor equivalent1732
• Explanation: A build status is a feature, typically implemented as a GitHub Action,1733
that indicates whether the most recent build of the benchmark was successful [ 28]. It1734
should be implemented for the benchmark’s evaluation code. It veriﬁes that the code is1735
running correctly after the latest commit.1736
• Justiﬁcation: A passing build status signiﬁes that the main evaluation code was usable1737
at the latest commit [ 28]. Including a build status or equivalent can help to ensure the1738
reliability and usability of the evaluation code. It allows benchmark users to quickly1739
determine if the code is functioning as intended, saving time and effort in identifying1740
potential issues.1741
• Points:1742
– 0: The benchmark neither references nor implements any form of build status or1743
equivalent.1744
– 5: The benchmark mentions the need for working evaluation code but does not1745
implement it in any meaningful way.1746
– 10: The benchmark partially implements a build status or equivalent by providing1747
the information in a less accessible manner.1748
– 15: The benchmark fully implements a build status or equivalent, clearly displaying1749
the status of the most recent build and providing easy access to the information.1750
42

K.3 Benchmark Documentation1751
1. Requirements ﬁle available1752
• Explanation: A requirements or environment ﬁle, or equivalent is available.1753
• Justiﬁcation: Ease of use is a key criteria for benchmark adoption. Providing a1754
requirements ﬁle allows for the quick installation of relevant packages at the correct1755
versions, e.g., within a virtual environment, to use the evaluation code.1756
• Points:1757
– 0: No requirements ﬁle or equivalent is provided.1758
– 5: A requirements ﬁle is mentioned but not provided.1759
– 10: A requirements ﬁle is provided but may be missing some dependencies or1760
versions.1761
– 15: A complete and accurate requirements ﬁle specifying all necessary dependen-1762
cies and versions is provided.1763
2. Quick-start guide or demo code available1764
• Explanation: The benchmark developers make a quick start guide or demo available1765
that walks step-by-step through how the benchmark can be used.1766
• Justiﬁcation: Similar to the criteria above, ease of use is a key criteria for benchmark1767
adoption. Providing a quick-start guide takes away any guesswork on the user side and1768
allows them to directly set up and use the benchmark without spending extra time on1769
setup issues.1770
• Points:1771
– 0: No quick-start guide or demo code is provided.1772
– 5: A quick-start guide or demo code is mentioned but not provided.1773
– 10: A quick-start guide or demo code is provided but may be missing some steps or1774
details.1775
– 15: A comprehensive, step-by-step quick-start guide or demo code is provided.1776
3. Includes informative In-line code comments1777
• Explanation: In-line code comments state the purpose, inputs, outputs, and functional-1778
ity of each code segment in all ﬁles relevant for the benchmark evaluation.1779
• Justiﬁcation: In-line documentation of code enhances clarity, understanding, and1780
reproducibility. It facilitates collaboration, maintainability, and makes debugging easier1781
for benchmark developers and users, should that be necessary.1782
• Points:1783
– 0: No in-line code comments are provided.1784
– 5: In-line code comments are sparse and do not adequately explain the purpose,1785
inputs, outputs, or functionality of the code.1786
– 10: Informative in-line code comments are present for most of the code but may be1787
lacking in detail or clarity for some code segments.1788
– 15: Comprehensive and informative in-line code comments are provided for all1789
relevant code segments, clearly explaining their purpose, inputs, outputs, and1790
functionality.1791
4. Code documentation available1792
• Explanation: A full documentation of the repository and code it entails is publicly1793
available. This includes, for example, an overview of the folder structure, the ﬁles in1794
the repo, an explanation of functions in the repo.1795
• Justiﬁcation: Detailed documentation of code enhances clarity, understanding, and1796
reproducibility. It facilitates collaboration, maintainability, and makes debugging easier1797
for benchmark developers and users, should that be necessary.1798
• Points:1799
43

– 0: No code documentation is provided.1800
– 5: Code documentation is mentioned but not provided.1801
– 10: Code documentation is minimal or incomplete, lacking important details about1802
the repository structure and functions.1803
– 15: Comprehensive code documentation is provided, including a clear overview1804
of the folder structure, ﬁles in the repo, and detailed explanations of all relevant1805
functions.1806
5. Documentation of test task categories & rationale1807
• Explanation: The benchmark developers deﬁne the tasks or task categories a model1808
is tested on and describe the rationale for choosing the tasks or task categories. The1809
rationale should explain how these tasks are relevant to the benchmark’s objectives,1810
what they aim to measure, and why they are important for evaluating the concept or1811
capability to be tested.1812
• Justiﬁcation: Documenting test tasks is essential for transparency and for allowing1813
public scrutiny of the benchmark. The rationale provides insight into the selection1814
process, demonstrating that the tasks are not arbitrary but are carefully chosen to reﬂect1815
real-world applications and user needs. Both help users decide if the benchmark is1816
adequate for their evaluation contexts.1817
• Points:1818
– 0: No documentation of test task categories or rationale is provided.1819
– 5: Test task categories are mentioned but they are neither deﬁned in detail and a1820
rationale for their selection is missing or inadequate.1821
– 10: Test task categories are deﬁned, but the rationale for their selection is not1822
provided.1823
– 15: Test task categories are clearly deﬁned, and a comprehensive rationale is1824
provided, explaining their relevance to the benchmark’s objectives, what they1825
measure, and their importance for evaluating the targeted concept or capability.1826
6. Documentation of assumptions about normative properties1827
• Explanation: If the benchmark measures properties that vary across cultural contexts1828
(e.g., politeness), then normative assumptions are explicitly stated. The benchmark1829
developers clearly deﬁne the cultural context and values that the benchmark adheres to,1830
explaining how the measured properties are conceptualized and operationalized within1831
the benchmark.1832
• Justiﬁcation: By explicitly stating normative assumptions, the authors provide trans-1833
parency about the cultural framework and values that guide the benchmark’s design1834
and evaluation criteria, which can subsequently ensure cultural sensitivity and mitigate1835
potential biases. It also facilitates informed decision-making for users of benchmarks,1836
speciﬁcally for culture-dependent use cases they’re interested in, such as measuring1837
toxicity or bias, for example.1838
• Points:1839
– 0: No documentation of normative assumptions is provided, even though the1840
benchmark measures culturally-dependent properties.1841
– 5: The potential inﬂuence and importance of cultural context on the benchmark is1842
acknowledged but normative assumptions aren’t stated.1843
– 10: Normative assumptions are stated, but the explanation of how they are concep-1844
tualized and operationalized within the benchmark is incomplete or lacks clarity.1845
– 15: Normative assumptions are explicitly and clearly stated, deﬁning the cultural1846
context and values that the benchmark adheres to, and explaining how the measured1847
properties are conceptualized and operationalized within the benchmark.1848
7. Documentation of limitations1849
44

• Explanation: Benchmark developers outline the limitations of the benchmark, includ-1850
ing but not limited to the tasks, contexts, and scenarios that are not covered by the1851
evaluation are acknowledged. It’s stated which use cases are out-of-scope.1852
• Justiﬁcation: Documenting a benchmark’s limitations is necessary for users to assess1853
its suitability for their speciﬁc evaluation needs. By understanding what the benchmark1854
does not cover, users can make informed decisions about whether the benchmark1855
aligns with their goals and whether additional evaluations (either in the form of other1856
benchmarks or private evaluations) may be required to complement the benchmark’s1857
results.1858
• Points:1859
– 0: No documentation of the benchmark’s limitations is provided.1860
– 5: Limitations of AI evaluations more broadly are brieﬂy mentioned but without1861
any detail and not applied to the speciﬁc benchmark.1862
– 10: Either limitations regarding the applicability and use of the benchmark or1863
limitations of the benchmark design are discussed, but not both.1864
– 15: Both limitations regarding the applicability and use of the benchmark and1865
limitations of the benchmark design are comprehensively discussed.1866
8. Documentation of benchmark construction process1867
• Explanation: Benchmark developers give a detailed account of the design process,1868
including the speciﬁc decisions made at each lifecycle stage, the rationale behind1869
them, and any trade-offs or compromises (e.g., balancing complexity vs. practicality)1870
considered.1871
• Justiﬁcation: Documenting the benchmark design process is essential for transparency,1872
as it allows users to understand how the benchmark was created and what factors1873
inﬂuenced its development. It allows users to assess the thoroughness and rigor of the1874
benchmark’s construction. This information further enables users to critically evaluate1875
whether the benchmark is suitable for their speciﬁc use case.1876
• Points:1877
– 0: No documentation of the benchmark construction process is provided.1878
– 5: The benchmark construction process is brieﬂy mentioned but lacks sufﬁcient1879
detail about the decisions made, rationale, and trade-offs considered.1880
– 10: The benchmark construction process is documented, including some decisions1881
made and their rationale, but the description lacks depth or fails to address important1882
aspects such as trade-offs or compromises.1883
– 15: The benchmark construction process is comprehensively documented, providing1884
a detailed account of the speciﬁc decisions made at each stage, the rationale behind1885
them, and any trade-offs or compromises considered.1886
9. Provision of a globally unique, persistent identiﬁer for a dataset and its metadata1887
• Explanation: The benchmark dataset and its associated metadata are assigned a1888
globally unique and persistent identiﬁer, such as a Digital Object Identiﬁer (DOI), to1889
ensure long-term accessibility and citability of the resource (FAIR Principles, 2024).1890
• Justiﬁcation: A persistent identiﬁer supports the ﬁndability and accessibility of the1891
benchmark and its dataset. It allows for unambiguous referencing of the data, facilitates1892
proper attribution, and ensures that the dataset can be located and accessed over time,1893
even if its physical location changes. This practice aligns with the FAIR (Findable,1894
Accessible, Interoperable, Reusable) principles, enhancing the benchmark’s scientiﬁc1895
value and reusability.1896
• Points:1897
– 0: The benchmark paper, dataset, and metadata are not assigned any persistent1898
identiﬁer.1899
45

– 5: The benchmark assigns persistent identiﬁers to the paper, the dataset, or the1900
metadata.1901
– 10: The benchmark assigns a persistent identiﬁer to two out of three (paper, dataset,1902
metadata).1903
– 15: The benchmark assigns a globally unique, persistent identiﬁer to the dataset, its1904
metadata, and the paper.1905
10. Inclusion of standardized metadata (e.g., following the Croissant standard)1906
• Explanation: The benchmark includes comprehensive, standardized metadata that1907
describes the dataset, its structure, and relevant information about its creation and usage.1908
This metadata adheres to established standards such as the Croissant standard, which is1909
designed speciﬁcally for machine learning datasets.1910
• Justiﬁcation: Standardized metadata is crucial for ensuring interoperability and1911
reusability of the benchmark dataset. It provides consistent and machine-readable1912
information about the dataset’s contents, structure, and provenance. This standard-1913
ization facilitates easier discovery, understanding, and integration of the dataset into1914
various research workﬂows. By following established standards like Croissant, the1915
benchmark enhances its utility across different platforms and tools in the machine1916
learning ecosystem.1917
• Points:1918
– 0: The benchmark does not include any structured metadata.1919
– 5: The benchmark includes some basic metadata, but it is not standardized or1920
comprehensive.1921
– 10: The benchmark includes comprehensive metadata that covers most aspects of1922
the dataset, but it does not fully adhere to a recognized standard like Croissant.1923
– 15: The benchmark includes complete, standardized metadata (e.g., following the1924
Croissant standard) that thoroughly describes all aspects of the dataset, ensuring1925
maximum interoperability and reusability.1926
11. Documentation of data sources and how the data was collected (if applicable)1927
• Explanation: The benchmark provides comprehensive documentation detailing the1928
origins of the data, the methods used for data collection, and, where applicable, dis-1929
cusses issues of data provenance and informed consent. They also list the license types1930
for all data used and how they ensured compliance with that license.1931
• Justiﬁcation: Thorough documentation of data sources and collection methods is1932
necessary for ensuring transparency, reproducibility, and ethical design of the bench-1933
mark. It allows users to understand the context and limitations of the data, assess its1934
appropriateness for their speciﬁc use cases, and make informed decisions about its1935
application. Furthermore, discussing data provenance and informed consent addresses1936
ethical considerations, particularly when dealing with sensitive or personal data, and1937
helps ensure compliance with data protection regulations.1938
• Points:1939
– 0: The benchmark provides no information about data sources or collection meth-1940
ods.1941
– 5: The benchmark mentions data sources but provides minimal details about1942
collection methods or ethical considerations.1943
– 10: The benchmark includes a detailed description of data sources and collection1944
methods, but lacks a discussion of data provenance, compliance with licensing, or1945
informed consent, where applicable.1946
– 15: The benchmark provides extensive documentation of data sources, collection1947
methods, and a thorough discussion of data provenance, compliance with licensing,1948
and informed consent, addressing relevant ethical and legal considerations.1949
12. Documentation of the data preprocessing steps taken1950
46

• Explanation: The benchmark provides a detailed account of all preprocessing steps1951
applied to the raw data before its inclusion in the ﬁnal dataset. This documentation1952
includes information on data cleaning, normalization, feature engineering, handling1953
of missing values, and any other transformations or manipulations performed on the1954
original data. If no data preprocessing was done, the authors state this explicitly.1955
• Justiﬁcation: Thorough documentation of preprocessing steps is necessary for ensur-1956
ing reproducibility and transparency of the benchmark. It allows users to understand1957
exactly how the ﬁnal dataset was created, which is key for interpreting results, repli-1958
cating experiments, and assessing the benchmark’s applicability to different use cases.1959
Additionally, this information helps identify potential biases or artifacts introduced1960
during preprocessing that could affect model performance or generalization.1961
• Points:1962
– 0: The benchmark provides no information about data preprocessing steps.1963
– 5: The benchmark mentions that preprocessing was done but offers minimal details1964
about the speciﬁc steps taken.1965
– 10: The benchmark includes a general description of preprocessing steps, but lacks1966
comprehensive details or fails to cover all aspects of the data preparation process.1967
– 15: The benchmark provides an exhaustive, step-by-step documentation of all1968
preprocessing procedures, including rationales for choices made and potential1969
impacts on the data.1970
13. Documentation of the data annotation process (if applicable)1971
• Explanation: The benchmark provides documentation of the data annotation process,1972
including the annotation guidelines, the qualiﬁcations and training of annotators, the1973
annotation tools used, quality control measures, and inter-annotator agreement metrics.1974
This documentation covers the entire workﬂow from raw data to the ﬁnal annotated1975
dataset.1976
• Justiﬁcation: Comprehensive documentation of the annotation process is necessary for1977
understanding the quality, reliability, and potential biases in the labeled data. It allows1978
users to assess the suitability of the dataset for their speciﬁc tasks and to interpret results1979
accurately. Transparent annotation documentation also enables reproducibility of the1980
labeling process, facilitates improvements in future iterations of the benchmark, and1981
helps in identifying and mitigating potential sources of bias or error in the annotations.1982
• Points:1983
– 0: The benchmark provides no information about the data annotation process.1984
– 5: The benchmark mentions that data was annotated but offers minimal details1985
about the process or guidelines used.1986
– 10: The benchmark includes a general description of the annotation process, includ-1987
ing guidelines and tools used, but lacks comprehensive details on quality control1988
measures or inter-annotator agreement.1989
– 15: The benchmark provides exhaustive documentation of the entire annotation pro-1990
cess, including detailed guidelines, annotator information, quality control measures,1991
inter-annotator agreement metrics, and discussions of potential biases or limitations1992
in the annotation approach.1993
14. Documentation of the representativeness of the data (if applicable)1994
• Explanation: The benchmark provides analysis and documentation of how representa-1995
tive the dataset or environment is of the target population or domain. This includes an1996
explanation of the sampling procedure used, any potential biases in the data collection1997
process, and how well the dataset captures the diversity and distribution of the intended1998
population or phenomenon being studied.1999
• Justiﬁcation: Understanding the representativeness of the data is necessary for assess-2000
ing the generalizability and validity of any conclusions drawn from models trained2001
47

or evaluated on the benchmark. It helps users identify potential limitations or biases2002
in the dataset that could affect model performance in real-world applications. Proper2003
documentation of representativeness also aids in interpreting benchmark results within2004
the context of the population it represents and highlights areas where the dataset may2005
need expansion or improvement to better cover underrepresented groups or scenarios.2006
• Points:2007
– 0: The benchmark provides no information about the representativeness of the data2008
or the sampling procedure used.2009
– 5: The benchmark mentions the importance of data representativeness but offers2010
minimal analysis or explanation of how representative the dataset actually is.2011
– 10: The benchmark includes a general discussion of data representativeness and the2012
sampling procedure, but lacks comprehensive analysis or fails to address potential2013
biases or limitations in representativeness.2014
– 15: The benchmark provides an in-depth analysis of data representativeness, in-2015
cluding detailed explanation of the sampling procedure, quantitative measures of2016
population coverage, discussion of potential biases, and acknowledgment of any2017
limitations in representativeness.2018
15. Standardized documentation2019
• Explanation: The benchmark utilizes a standardized documentation format, such2020
as data cards, to present the information about the dataset that is underlying to the2021
benchmark. This standardized approach ensures that all key aspects of the dataset are2022
systematically covered, including its composition, collection methodology, intended2023
uses, ethical considerations, and potential biases.2024
• Justiﬁcation: Adopting a standardized documentation scheme like data cards enhances2025
the usability and transparency of the benchmark. It provides a consistent, structured2026
format that makes it easier for users to quickly understand the dataset’s characteristics,2027
limitations, and appropriate use cases. Standardized documentation facilitates easier2028
comparison between datasets and benchmarks, promotes best practices in data reporting,2029
and helps identify potential issues or gaps in the dataset’s coverage.2030
• Points:2031
– 0: The benchmark does not use any standardized documentation scheme.2032
– 5: The benchmark includes some elements of standardized documentation, but2033
does not fully adhere to an established scheme like data cards.2034
– 10: The benchmark uses a standardized documentation scheme, but some sections2035
are incomplete or lack detail.2036
– 15: The benchmark fully implements a comprehensive standardized documentation2037
scheme (e.g., data cards), providing thorough and structured information on all2038
relevant aspects of the dataset.2039
16. Documentation of evaluation metric(s)2040
• Explanation: The evaluation metrics used are clearly speciﬁed and deﬁned, both for2041
standard and custom metrics tailored to the speciﬁc task or domain. The exact formulas2042
or processes used to calculate these metrics, along with any parameters or thresholds2043
employed, are made transparent.2044
• Justiﬁcation: Documenting the evaluation metrics and scoring process is essential2045
for enabling users to understand how the benchmark quantiﬁes model performance2046
and determines rankings or comparisons. By providing clear and detailed information2047
about the metrics and scoring methods, users can assess whether the chosen metrics are2048
appropriate for the task at hand, align with their own evaluation criteria, and provide a2049
fair and meaningful basis for comparing different models or approaches.2050
• Points:2051
– 0: No documentation of the evaluation metrics is provided.2052
48

– 5: The evaluation metrics are mentioned but not clearly deﬁned, and the exact2053
formulas or processes used to calculate them are not provided.2054
– 10: The evaluation metrics are deﬁned, but the documentation lacks some important2055
details, such as any parameters or thresholds employed.2056
– 15: The evaluation metrics are clearly speciﬁed. The exact formulas or processes2057
used to calculate these metrics, along with any parameters or thresholds employed,2058
are comprehensively documented.2059
17. Report statistical signiﬁcance of benchmark results for at least one model2060
• Explanation: Benchmark developers run statistical signiﬁcance tests on the benchmark2061
results. They report results for, e.g., more than one random seed, and provide variance2062
bounds. In cases where the benchmark is perfectly deterministic, this should be2063
explicitly stated.2064
• Justiﬁcation: Not doing statistical signiﬁcance testing can signiﬁcantly reduce the2065
validity, utility and conﬁdence in results [ 13]. Especially for benchmarks, we want to2066
understand how much of the results are due to noise and how much is caused by true2067
differences between the models tested.2068
• Points:2069
– 0: No statistical signiﬁcance testing or variance reporting is provided for the2070
benchmark results.2071
– 5: The need for valid benchmarks and/or statistical signiﬁcance or uncertainty2072
estimation is mentioned but not not addressed.2073
– 10: Benchmark developers if “bound the expected variation across model training2074
runs” [40], [13]2075
– 15: Benchmark developers run statistical signiﬁcance tests on the benchmark results2076
for at least one model and provide variance bounds or other uncertainty estimations.2077
In cases where the benchmark is perfectly deterministic, this is explicitly stated.2078
18. Accepted at peer-reviewed venue2079
• Explanation: The benchmark/its associated paper was accepted to a peer-reviewed2080
journal, conference, or similar venue.2081
• Justiﬁcation: Acceptance at a peer-reviewed venue signiﬁes that the benchmark2082
has undergone an evaluation by an external party, ensuring its validity, reliability, and2083
scientiﬁc merit [ 5]. This peer review process contributes to the credibility and assurance2084
to users that the benchmark meets established standards of quality and relevance [5].2085
• Points:2086
– 0: The benchmark/its associated paper has not been accepted at a peer-reviewed2087
venue.2088
– 5: The benchmark/its associated paper has been submitted to a peer-reviewed venue2089
but is still under review or awaiting acceptance.2090
– 10: The benchmark/its associated paper has been accepted at a peer-reviewed2091
workshop or symposium.2092
– 15: The benchmark/its associated paper has been accepted at a peer-reviewed2093
journal, conference, or similar high-proﬁle venue.2094
19. Speciﬁes applicable license2095
• Explanation: The benchmark developers clearly specify the applicable license for the2096
benchmark in the code repository or paper. This includes providing information about2097
the conditions under which the benchmark can be used, modiﬁed, and distributed.2098
• Justiﬁcation: Specifying the applicable license ensures legal clarity and compliance2099
for benchmark users and enables wider adoption, as commercial users might not be2100
able to use the benchmark if no license is speciﬁed.2101
• Points:2102
49

– 0: No license is speciﬁed for the benchmark.2103
– 5: A license is mentioned but not clearly speciﬁed or linked to in the code repository2104
or paper.2105
– 10: A license is speciﬁed but lacks some important details about the conditions2106
under which the benchmark can be used, modiﬁed, or distributed.2107
– 15: The applicable license for the benchmark is clearly speciﬁed in the code2108
repository or paper, providing comprehensive information about the conditions2109
under which the benchmark can be used, modiﬁed, and distributed.2110
K.4 Benchmark Maintenance2111
1. Code usability checked within the last year2112
• Explanation: The main ﬁles of the public code were updated within the last year 8, or2113
the developers checked that the benchmark code is still usable and explicitly state this2114
check in the README ﬁle, including the date of the check.2115
• Justiﬁcation: Over time, packages that the benchmark depends on may be updated and2116
become incompatible with the original evaluation/benchmark code. To ensure ongoing2117
usability, benchmark developers must check if their code can still be used at least once2118
a year9. This practice ensures that users can use the benchmark without encountering2119
and having to ﬁx issues due to outdated dependencies.2120
• Points:2121
– 0: No updates to the main ﬁles of the public code within the last year, and no2122
explicit statement of a usability check in the README ﬁle.2123
– 5: Updates to minor ﬁles in the repo were made (e.g., README ﬁle) but an explicit2124
statement of a usability check in the README ﬁle is not reported.2125
– 10: Updates to the main ﬁles of the public code were made within the last year, but2126
the build status check failed and wasn’t ﬁxed.2127
– 15: Updates to the main ﬁles of the public code within the last year, accompanied2128
by a successful build status check, or an explicit statement of a usability check in2129
the README ﬁle, including the date of the check was provided.2130
2. Maintained feedback channel for users2131
• Explanation: GitHub issues are acknowledged or addressed within three months. If2132
there are no open issues, benchmark developers would get full points.2133
• Justiﬁcation: Over time, users may ﬁnd issues with the benchmark tasks or imple-2134
mentation. To ensure continued usability, benchmark developers should address these2135
concerns in a reasonable amount of time. Promptly responding to user feedback helps2136
maintain the reliability and relevance of the benchmark.2137
• Points:2138
– 0: No acknowledgment or response to GitHub issues that are older than three2139
months10.2140
– 5: GitHub issues are mentioned as a way to provide feedback but there are GitHub2141
issues that were not responded to and that are older than three months.2142
– 10: All GitHub issues are acknowledged within three months, but not all are2143
addressed or resolved or were closed because the issue/feature request won’t be2144
attended to.2145
8We recognize that this criterion is just a proxy for checking code usability, but we assume that if the main
code was edited and a build status [28] passed, that the usability was sufﬁciently checked.
9The one-year threshold is somewhat arbitrary but out of experience of the authors, there is some transition
period until which old versions can still be reliably used and are maintained, which can vary from a few months
to a few years.
10This is an arbitrary cut-off time but it seemed reasonable to give developers extended time to respond to
open issues.
50

– 15: All GitHub issues are acknowledged and addressed within three months, or it is2146
clearly stated if an issue cannot be ﬁxed or if a feature request won’t be fulﬁlled.2147
Alternatively, there are no open issues 11.2148
3. Provide contact details of person responsible for benchmark2149
• Explanation: The benchmark should include contact details of the person responsible,2150
such as a corresponding author in the associated paper, a contact person listed on2151
GitHub or the website, or an available online feedback form.2152
• Justiﬁcation: Providing contact details ensures that users have a communication2153
channel for inquiries, feedback, or reporting issues related to the benchmark. This2154
transparency supports effective collaboration and resolution of problems, enhancing2155
the benchmark’s usability.2156
• Points:2157
– 0: It is not disclosed who developed the benchmark.2158
– 5: The benchmark developers are disclosed but no explicit contact details are2159
provided.2160
– 10: Contact details are provided but are incomplete or difﬁcult to ﬁnd, e.g., only as2161
part of terms of service on a website.2162
– 15: Contact details of the person responsible for the benchmark are easily accessible,2163
such as a corresponding author in the associated paper, a contact person listed on2164
GitHub or the website, or an available online feedback form.2165
11This is an imperfect proxy for a maintained feedback channel. It may be that the benchmark is working well
or it may be that the benchmark is not used enough for issues to occur. However, maintenance is a critical part of
benchmarks, and we hence decided to include an imperfect proxy rather than not including this criterion at all.
51