BLEND: A Benchmark for LLMs on Everyday
Knowledge in Diverse Cultures and Languages
Junho Myung1,∗, Nayeon Lee1,∗, Yi Zhou2,∗, Jiho Jin1, Rifki Afina Putri1,
Dimosthenis Antypas2, Hsuvas Borkakoty2, Eunsu Kim1, Carla Perez-Almendros2,
Abinew Ali Ayele3,4, Víctor Gutiérrez-Basulto2, Yazmín Ibáñez-García2, Hwaran Lee5,
Shamsuddeen Hassan Muhammad6, Kiwoong Park1, Anar Sabuhi Rzayev1, Nina White2,
Seid Muhie Yimam3, Mohammad Taher Pilehvar2, Nedjma Ousidhoum2,
Jose Camacho-Collados2, Alice Oh1
1KAIST, 2Cardiff University,3Universität Hamburg, 4Bahir Dar University,
5NA VER AI Lab,6Imperial College London
Abstract
Large language models (LLMs) often lack culture-specific knowledge of daily life,
especially across diverse regions and non-English languages. Existing benchmarks
for evaluating LLMs’ cultural sensitivities are limited to a single language or col-
lected from online sources such as Wikipedia, which do not reflect the mundane
everyday lifestyles of diverse regions. That is, information about the food people
eat for their birthday celebrations, spices they typically use, musical instruments
youngsters play, or the sports they practice in school is common cultural knowledge
but uncommon in easily collected online sources, especially for underrepresented
cultures. To address this issue, we introduce BLEND, a hand-crafted benchmark
designed to evaluate LLMs’ everyday knowledge across diverse cultures and lan-
guages. BLEND comprises 52.6k question-answer pairs from 16 countries/regions,
in 13 different languages, including low-resource ones such as Amharic, Assamese,
Azerbaijani, Hausa, and Sundanese. We construct the benchmark to include two
formats of questions: short-answer and multiple-choice. We show that LLMs
perform better for cultures that are highly represented online, with a maximum
57.34% difference in GPT-4, the best-performing model, in the short-answer format.
For cultures represented by mid-to-high-resource languages, LLMs perform better
in their local languages, but for cultures represented by low-resource languages,
LLMs perform better in English than the local languages. We make our dataset
publicly available at: https://github.com/nlee0212/BLEnD.
1 Introduction
Despite the worldwide usage of large language models (LLMs), capturing cultural everyday knowl-
edge specific to a particular country or region is challenging because such knowledge is often not
explicitly documented in online data sources like Wikipedia, which are commonly used to train LLMs.
For instance, the answers to mundane everyday questions such as “What can typically be found in
the backyard of houses in your country?" are not included in the training data of LLMs, except for a
handful of highly represented regions such as North America. Consequently, LLMs may provide
incorrect, incomplete, or nonsensical responses to everyday questions in underrepresented cultures,
*Equal contribution.
*Co-first authors: junho00211@kaist.ac.kr, nlee0212@kaist.ac.kr, zhouy131@cardiff.ac.UK
38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets and Benchmarks.
arXiv:2406.09948v2  [cs.CL]  16 Jan 2025

What do people eat on their birthdayin Ethiopia?What do people from Spainusually eat for dessert?
Manual Data Construction
1. Question Collection
2. Question Filtering & Translation
3. Answer Annotation
⇒ 500 questions ×16countries/regions (13languages)
4. Answer Aggregation
Who usually do the house choresin Assam?
What do people eat on their birthdayin Ethiopia?What do people from Spainusually eat for dessert?
Who usually do the house choresin Assam?
BLEND
대한민국사람들은생일에무엇을먹나요?(What do people eat on their birthdayin South Korea?)미역국(Seaweed Soup)케이크(Cake)생일케이크(Birthday Cake)갈비(Galbi)
1. Short-Answer Question(SAQ)
2. Multiple-Choice Question(MCQ)Q. What is the most common spice/herb used in dishes from Greece?A.Black PepperB.CuminC.EpazoteD.Oregano
(Ans. from the US)(Ans. from China)(Ans. from Mexico)
Q. Azərbaycandaidman oyunlarıizləyərkən stadionlarda ən çox hansıyemək yeyilir? A: Cips, …
Q.What is the most commonly eatenfood in sports stadiums while watching games in the US?A: Hot Dogs, …
Q. 在中国的体育场馆里观看比赛时，最常吃的食物是什么？A: 爆米花, …
Azerbaijan (Azerbaijani)
China (Chinese)
US (English)
…AzerbaijanCips(chips)Küftə(meatball)
China爆米花(popcorn)瓜子(sunflower seeds)
USHot DogsHot DogsPiePie
UKWest JavaKacang(Peanut)Seblak(Seblak)
LLM EvaluationWhat is the most commonly eatenfood in sports stadiums while watching games in {country/region}?
대한민국사람들은생일에무엇을먹나요?(What do people eat on their birthdayin South Korea?)미역국(Seaweed Soup)케이크생일케이크(Cake)(Birthday Cake)갈비(Galbi)
Stereotypical
Figure 1: The overall framework of dataset construction and LLM evaluation on BLEND. BLEND is
built through 4 steps: question collection, question filtering & translation, answer annotation, and
answer aggregation. The dataset includes the same questions in 13 different languages, answered from
16 different countries/regions. We evaluate LLMs by short-answer and multiple-choice questions.
even though these inquiries are frequently encountered in daily lives. This can lead to hallucinations
or stereotypical responses, potentially offending a large and diverse user base.
This challenge becomes even more evident in cross-lingual settings, as most LLMs are primarily
trained on English data reflecting Western perspectives [8, 20, 15]. They often reflect the stereotypes
present in the training data [19, 18, 21, 36, 13], hence these models would often respond based on
Western perspectives rather than reflecting actual diverse practices. Ideally, language models would
reflect the cultural norms of various regions around the world and generate culturally appropriate
content when responding in local languages of the regions, unless otherwise specified. To develop
multilingual LLMs with such cultural appropriateness, we first need to evaluate the cultural com-
monsense knowledge. However, there is no well-crafted multilingual multicultural benchmark that
captures the daily lives of people in diverse cultures.
To bridge this gap, we present BLEND, a Benchmark for LLMs on Everyday knowledge in Diverse
cultures and languages. The benchmark covers 13 languages spoken in 16 different countries and
regions shown in Table 1. Note that we include languages that are spoken in two regions with vastly
different cultures, such as South Korea and North Korea, both represented by the Korean language. To
effectively capture the cultural diversity of people’s daily lives, we recruit annotators who are native
speakers from various countries. The final dataset includes 500 socio-cultural question-answer pairs
for each country/region in 6 categories: food, sports, family, education, holidays/celebrations/leisure,
and work-life. To capture a comprehensive understanding of the cultural sensitivity of LLMs, we
create a set of questions and answers in two formats: short-answer and multiple-choice questions.
The overall framework for construction and evaluation of BLEND is shown in Figure 1. The statistics
of BLE ND are shown in Table 1 1. In total, BLE ND features an extensive collection of 52.6k
question-and-answer pairs, 15k short-answer and 37.6k multiple-choice.
Our experimental results on BLEND show that even current state-of-the-art LLMs exhibit unbalanced
cultural knowledge and unfair cultural biases across various countries and regions. The average
performance of all tested models on short answer questions about United States (US) culture in English
is 79.22%. In contrast, when asked about Ethiopian (ET) culture in Amharic, the average performance
1Throughout the paper, we use the two-letter ISO codes for each country/region and language, as shown in
Table 3.
2

Table 1: Statistics of the question samples within BLE ND. BLEND is composed of two question
types: Short Answer Questions (SAQ) and Multiple-Choice Questions (MCQ). The question samples
are generated based on the 500 question templates generated by annotators from all countries/regions.
SAQ MCQ
Country/Region Language Count Language Count
United States (US) English (en) 500
English (en)
1,942
United Kingdom (GB) English (en) 500 2,167
China (CN) English (en), Chinese (zh) 1,000 1,929
Spain (ES) English (en), Spanish (es) 1,000 1,931
Indonesia (ID) English (en), Indonesian (id) 1,000 1,995
Mexico (MX) English (en), Spanish (es) 1,000 1,899
South Korea (KR) English (en), Korean (ko) 1,000 2,512
Greece (GR) English (en), Greek (el) 1,000 2,734
Iran (IR) English (en), Persian (fa) 1,000 3,699
Algeria (DZ) English (en), Arabic (ar) 1,000 2,600
Azerbaijan (AZ) English (en), Azerbaijani (az) 1,000 2,297
North Korea (KP) English (en), Korean (ko) 1,000 2,185
West Java (JB) English (en), Sundanese (su) 1,000 2,345
Assam (AS) English (en), Assamese (as) 1,000 2,451
Northern Nigeria (NG) English (en), Hausa (ha) 1,000 2,008
Ethiopia (ET) English (en), Amharic (am) 1,000 2,863
Subtotal 15,000 37,557
Total 52,557
drops to only 12.18%, highlighting a significant performance gap in relatively underrepresented
cultures and languages. A similar trend is observed in the multiple-choice format, where the LLMs
are required to choose the correct answer for each target country/region, with answers from other
countries/regions presented as wrong options.
The main contributions of our paper are as follows:
• We present BLEND, a benchmark of carefully crafted 52.5k question-answer pairs that
reflect the everyday cultural knowledge across 16 countries/regions in 13 different languages.
• Within BLEND, we propose two types of questions to automatically measure the cultural
knowledge in LLMs: short-answer questions and multiple-choice questions.
• We conduct extensive experiments across 16 LLMs on BLE ND, showing a significant
performance gap between highly represented cultures and underrepresented cultures.
2 Related Work
Although LLMs generally incorporate extensive parametric knowledge from large text corpora
during pretraining [25], such models frequently display bias due to imbalanced representations in
the data sources [3]. Cultural knowledge is critical in enhancing the reasoning capabilities of LLMs,
contributing significantly to their success across various downstream applications.
Numerous studies have examined the socio-cultural aspects of LLMs. Previous work on cultural NLP
defines culture as the way of life of a specific group of people [10]. Most research on the cultural
knowledge of LLMs centers on the culture at a national level. Anacleto et al.[1] collect commonsense
knowledge about eating habits in Brazil, Mexico, and US through the Open Mind Common Sense
portal. GeoMLAMA [33] introduces 16 geo-diverse commonsense concepts and uses crowdsourcing
to compile knowledge from 5 different countries, each in its native languages. Nguyen et al. [22]
introduce a methodology to extract large-scale cultural commonsense knowledge from the Common
Crawl corpus on geography, religion, and occupations. CREHate [17] is a cross-cultural English hate
speech dataset covering annotations from 5 English-speaking countries. CultureAtlas [9] includes
textual data encapsulating the cultural norms from 193 countries, primarily sourced from Wikipedia
documents in English. However, the majority of these studies are conducted exclusively in English
and focus on more objective aspects of culture that are written in formal data sources.
3

Table 2: Detailed statistics of the number of questions per category for each country/region in Short
Answer Questions (SAQ) and Multiple-Choice Questions (MCQ).
Food Sports Family Education Holidays Work-life
SAQ 105 88 63 84 92 68
MCQ
United States (US) 642 393 60 173 500 174
United Kingdom (GB) 990 403 50 189 427 108
Spain (ES) 714 476 43 172 425 101
Mexico (MX) 489 491 39 183 578 119
Indonesia (ID) 471 369 60 212 699 184
China (CN) 475 349 74 200 705 126
South Korea (KR) 753 792 57 218 539 153
Algeria (DZ) 873 569 59 189 819 91
Greece (GR) 1,345 516 40 154 500 179
Iran (IR) 666 519 50 173 2,135 156
North Korea (KP) 784 430 78 228 476 189
Azerbaijan (AZ) 852 513 65 216 453 198
West Java (JB) 892 461 20 160 680 132
Assam (AS) 862 584 34 198 666 107
Northern Nigeria (NG) 647 421 50 207 508 175
Ethiopia (ET) 984 649 46 278 692 214
More recent studies have focused on the cultural knowledge of non-English speaking countries and
languages. For instance, CLIcK [ 14] and HAE-RAE Bench [ 29] evaluate LLMs’ knowledge in
Korean, while COPAL-ID [32], ID-CSQA [26], and IndoCulture [15] include culturally nuanced
questions in Indonesian. Nonetheless, we do not know of any work that has been done to compare
the cultural adaptiveness of LLMs across diverse languages and cultures using the same question set,
which would enable a direct comparison.
Other recent work focuses on capturing the everyday cultural nuances of LLMs using social net-
working platforms. StereoKG [7] extracts cultural stereotypes of five nationalities and five religious
groups from questions posted on X (formerly Twitter) and Reddit. However, this method produces a
significant amount of noisy and inappropriate assertions due to insufficient filtering. CAMEL [20]
includes masked prompts from naturally occurring contexts on X, focusing on Arabic content, and
CultureBank [28] is a collection of diverse perspectives and opinions on cultural descriptors, including
English comments from TikTok and Reddit. However, these datasets are limited to a single language
and rely solely on data available from social media, not able to capture people’s everyday behaviors
to the full extent [31].
In contrast to prior work, BLE ND is carefully human-crafted, capturing everyday life cultural
knowledge across 13 languages spoken in 16 different countries/regions including underrepresented
regions such as West Java and North Korea.
3 Construction of BLE ND
Language Coverage. We select languages with varying levels of resource availability using the
metrics defined by Joshi et al. [12]. The resource availability of languages included in BLE ND is
shown in Table 4 in the Appendix. Additionally, we involve at least one author who is a native speaker
of the language and originally from the country/region represented in the dataset to handle the data
inspection process 2.
Question Collection and Filtering. BLEND includes 500 question templates that reflect
daily life aspects across six socio-cultural categories: food, sports, family, education, holi-
days/celebrations/leisure, and work-life. To create these templates, we collect 10-15 questions
for each category from at least two native annotators per country/region. These annotators are asked
to generate culturally relevant questions about their countries while avoiding stereotypical questions.
The question generation guideline is shown in Appendix B.4. The collected questions are filtered
2North Korea was an exception, where we collaborated with a South Korean researcher studying North
Korean language.
4

US GB ES CN ID MX KR DZ GR IR KP AZ JB AS NG ET
US
GB
ES
CN
ID
MX
KR
DZ
GR
IR
KP
AZ
JB
AS
NG
ET
 0.6
0.8
1.0
1.2
1.4
1.6
1.8
Figure 2: Heatmap showing the average number of common lemmas within each question between
all country/region pairs. Pairs from the same countries/regions are shown in white. Higher numbers
of shared lemmas indicate that those countries/regions provide more similar answers compared to
other countries/regions (e.g., Indonesia and West Java).
to eliminate duplicates and country-specific items that can only apply to one country/region. For
example, items with proper nouns from a single country/region are excluded. Then the questions
are formatted into templates like “ What is a common snack for preschool kids in your country?”
Subsequently, ‘your country’ is replaced by the country/region names for localizing the questions.
Except for US and GB, the questions are translated into the local languages by the native speakers.
This process results in a comprehensive dataset of 15,000 short-answer questions, as shown in Table
1. The specific number of questions per topic is shown in Table 2.
Answer Annotation. To obtain the answers to the collected questions, we recruit annotators who
are native speakers of the target languages and are originally from the target regions/countries. We
ensure that the annotators have lived in these countries for over half of their lifetimes 3. For most
countries, we recruit annotators through Prolific 4. However, in cases where it is not possible to find
annotators through crowdsourcing platforms (i.e., DZ, KR, KP, AZ, JB, AS, NG, and ET), we directly
recruit five annotators who meet our criteria 5.
Annotators are required to give at least one short answer to each question and can offer up to three
responses if a single answer is insufficient. If an annotator does not know the answer, they can choose
from the following options: ‘not applicable to our culture, ’ ‘no specific answer for this question, ’
‘I don’t know the answer, ’or ‘others. ’By default, responses are collected from five annotators per
question. If an annotator chooses ‘I don’t know the answer’, we discard the response and collect a new
one. This process continues until five valid responses for each question are obtained, or more than
five annotators choose ‘I don’t know’. Examples of the collected questions with answers from each
country are presented in Figure 1. The guideline and the interface for answer annotation provided to
annotators are shown in Appendix B.5 and B.6.
Answer Aggregation. We request 1-2 annotators from each country to review the annotations and
remove invalid answers. These invalid answers appear to be due to some annotators misunderstanding
a question, leading to nonsensical answers. Additionally, due to the nature of natural language, there
are multiple variations of a single term (e.g., “go to bed” and “sleep”). We instruct the annotators to
group these variants into one to ensure the final dataset contains accurate vote counts for each answer.
We also ask the annotators to translate all the annotations into English. As a result, our final dataset
includes variants in local languages and English, along with a final vote count for answers to the
question.
3This condition was not fully met for North Korea due to a very limited pool of annotators.
4https://www.prolific.co/
5Tables 5 and 6 in the Appendix shows a detailed demographic distribution of the annotators.
5

Statistical Analysis on Annotations. We analyze the annotations to assess their quality and
consistency, as detailed in Table 7 in the Appendix. Despite the subjective nature of the questions,
the average level of agreement among annotators, calculated by the average of the maximum votes
for each question, is 3.16 out of 5 (63.2%). The balance within the dataset indicates that while there
is consensus on certain annotations, there is also a substantial variety in the answers within each
country, reflecting a diverse range of perspectives. We also present the average number of annotations
per question in Table 8 in the Appendix, to show the level of answer variance.
Table 9 in the Appendix presents the average number of’I don’t know’ responses per question. On
average, there were 1.01 out of 5 such responses per question, with a standard deviation of 0.35
(ranging from a high of 1.912 in Northern Nigeria to a low of 0.42 in South Korea). The frequency
of ’I don’t know’ responses was higher in the sports and holidays/celebrations/leisure categories,
likely due to questions on sports or holidays that are not widely recognized or celebrated in certain
countries or regions.
Furthermore, we measure the overlap of answers between countries/regions by calculating the number
of shared lemmas of the English versions of annotations to compare the trend between them and
show the result in Figure 2. The result indicates that countries/regions with closely aligned cultural
backgrounds exhibit higher overlaps in answers. The top pairs with the most similar responses are
Indonesia & West Java (a province in Indonesia), the United States & the United Kingdom, and Spain
& Mexico, likely due to shared historical, linguistic, or cultural ties that influence how questions are
understood and answered. On the other hand, the pairs with the lowest value are Northern Nigeria &
Greece/Ethiopia/South Korea. This could be due to the fact that Northern Nigeria has its own unique
regional culture captured in the dataset.
4 LLMs Cultural Knowledge Evaluation
We measure how the current LLMs perform on BLEND on the two task settings: short answer and
multiple-choice. Details for the experimental settings and the 16 evaluated models can be seen in
Appendix C.1.
4.1 Short Answer Questions (SAQ)
Experimental Setting. In this experiment, we measure LLMs’ performance on SAQ. The final
score for each country is calculated as the average score over two prompts: 1) directly ask LLMs to
provide the answer, and 2) add persona to the LLMs to make them act as a person from the target
country or region. The detailed prompts are shown in Appendix C.2.1. To compute the score, we first
mark the LLM’s response as correct if it is included in the human annotators’ responses to the same
question. Then we compute the percentage of questions to which LLM’s answer is correct. More
details on calculating the scores can be found in Appendix C.2.2.
We compute the scores for all the countries based on the results obtained for the local language and
English, respectively. We use lemmatizers and stemmers to handle highly inflectional languages such
as Arabic and variations in words. The details are shown in Appendix C.2.2. In addition, we remove
accents from words in languages that contain accents, such as Spanish and Greek, to ensure that the
annotations from human annotators match the responses of LLMs. When computing the scores, we
ignore questions for which three or more annotators do not know the answer.
4.1.1 LLM Performance on SAQ
Figure 3a presents the performance of five LLMs on short answer questions in the local languages
of target countries/regions. Table 10 shows the performance of all 16 LLMs evaluated. The results
indicate a consistent trend of lower performance for lower resource languages [12].
Highlighting just a few results, the average LLM performance for US, Spain, Iran, North Korea,
Northern Nigeria, and Ethiopia are 79.22%, 69.08%, 50.78%, 41.92%, 21.18%, and 12.18%, respec-
tively, indicating a significant drop in performance for underrepresented cultures. Countries that
share a common language but differ culturally show significant differences, for example, GPT-4,
the highest-performing model, shows a substantial performance disparity of 31.63% between South
Korea and North Korea. Similarly, between Spain and Mexico, GPT-4 exhibits a performance gap
6

US-en GB-en ES-es MX-es ID-id CN-zh KR-ko DZ-ar GR-el IR-fa KP-ko AZ-az JB-su AS-as NG-haET-am
0
20
40
60
80Scores
GPT-4
Claude-3-Opus
Gemini-1.0-Pro
Command R+
Qwen1.5-72B
HyperCLOVA-X
(a)
ES MX ID CN KR DZ GR IR KP AZ JB AS NG ET
0
10
20
30
40
50
60
70
80Scores
Local English
(b)
Figure 3: (a) LLMs’ performance on short answer questions for each country/region in the local
language. Models constructed from a Western country are shown in shades of blue, whereas those
built from a non-Western country are shown in shades of red. (b) Average performance of all LLMs
in local language and English on short answer questions. The grey error bars indicate the standard
deviations among all models.
of 4.35%. Our findings highlight the critical need for LLMs to be trained on more diverse datasets,
including low-resource languages and underrepresented cultures.
Performance of Region-Centric LLMs. Models built from non-Western countries tend to show
higher performance on that specific country/region. For example, as seen in Figure 3a, Qwen1.5-
72B [5], made by the Qwen Team in Alibaba 6 Group, shows highest performance on Chinese
among all models. HyperCLOV A-X [34], built from the NA VER7 HyperCLOV A Team, also shows
comparable results on Korean, even exceeding GPT-4 performance in North Korean cultural questions.
These language/region-specific models often benefit from customized datasets richer in local cultural
content and nuances, typically underrepresented in the more universally used datasets, leading to
higher performances in their regions.
Local language vs. English. We compare the average LLM performance when prompted in
local languages versus English, as shown in Figure 3b8. For cultures represented by high-resource
languages like Spanish and Chinese, the local languages show better performance across all models.
In contrast, in cultures represented by low-resource languages such as Azerbaijani, Sundanese, and
Amharic, English results in better performance (full results are shown in Table 11). This implies
that the models’ proficiency in a particular language significantly influences its performance and that
models tend to show better cultural sensitivity in the local language when they possess sufficient
6Chinese technology company (https://www.alibabagroup.com/)
7Korean technology company (https://www.navercorp.com/)
8Performance on the six models presented in Figure 3a on the English version of SAQ is shown in Figure 13.
7