Rethinking Evaluation Strategy for Temporal Link
Prediction through Counterfactual Analysis
Aniq Ur Rahman1 Alexander Modell2 Justin P. Coon1
1University of Oxford, U.K. 2Imperial College London, U.K.
aniq.rahman@eng.ox.ac.uk, a.modell@imperial.ac.uk, justin.coon@eng.ox.ac.uk
Abstract
In response to critiques of existing evaluation methods for Temporal Link Prediction
(TLP) models, we propose a novel approach to verify if these models truly capture
temporal patterns in the data. Our method involves a sanity check formulated as a
counterfactual question: “What if a TLP model is tested on a temporally distorted
version of the data instead of the real data?” Ideally, a TLP model that effectively
learns temporal patterns should perform worse on temporally distorted data com-
pared to real data. We provide an in-depth analysis of this hypothesis and introduce
two data distortion techniques to assess well-known TLP models. Our contributions
are threefold: (1) We introduce simple techniques to distort temporal patterns within
a graph, generating temporally distorted test splits of well-known datasets for sanity
checks. These distortion methods are applicable to any temporal graph dataset. (2)
We perform counterfactual analysis on TLP models such asJODIE, TGAT, TGN, and
CAWN to evaluate their capability in capturing temporal patterns across different
datasets. (3) We propose an alternative evaluation strategy for TLP, addressing
the limitations of binary classification and ranking methods, and introduce two
metrics – average time difference (ATD) and average count difference (ACD) – to
provide a comprehensive measure of a model’s predictive performance. The code
and datasets are available at: https://github.com/Aniq55/TLPCF.git.
1 Introduction
In static graphs, link prediction refers to the task of predicting whether an edge exists between two
nodes after having observed other edges in the graph. Temporal link prediction (TLP) is a dynamic
extension of link prediction wherein the task is to predict whether a link (edge) exists between any
two nodes in the future based on the historical observations (Qin and Yeung, 2023). The predictive
capability of TLP models make them useful in applications pertaining to dynamic graphs, such as
product recommendations (Qin et al., 2024; Fan et al., 2021), social network content or account
recommendation (Fan et al., 2019; Daud et al., 2020), fraud detection in financial networks (Kim
et al., 2024), and resource allocation, to name a few.
In the TLP literature (Kumar et al., 2019; Trivedi et al., 2019; Xu et al., 2020; Rossi et al., 2020;
Wang et al., 2020; Cong et al., 2023), the TLP task is treated as a binary classification problem where
the query
q1 : “Does an edge exist between the nodes u and v at time t?”
is processed by a model and then compared with the ground truth following which metrics such as
area under the receiver operating characteristic curve (AU-ROC), and average precision (AP) are
reported. The ground truth consists of positive samples, and a fixed number of random negative
samples. There are a couple of issues in the binary classification approach. Firstly, the timestamps in
the query are restricted to the timestamps present in the ground truth, which makes the evaluation
Preprint. Under review.

biased and does not test the model’s performance in the continuous time range. Secondly, checking
for the existence of an edge at a specific timestamp is an ill-posed question, and instead the existence
of an edge should be queried within a finite time-interval. Lastly, the negative edge sampling strategy,
and the number of negative samples per positive sample impact the performance metrics as seen in
EXH (Poursafaei and Rabbany, 2023).
Alternatively, in a rank-based approach, the query is formulated as:
q2 : “Which nodes are likely to have an edge with node u at time t?”
In this case, the model returns an ordered list of nodes arranged from most likely to least likely.
Then, the rank of the ground truth edge is returned if a match is found, and if not, a high number is
reported. For all the edges in the test data, metrics such as Mean Average Rank ( MAR) or Mean
Reciprocal Rank (MRR) can be reported to assess the performance of the model (Huang et al., 2024).
While the rank-based metrics are more intuitive than AU-ROC and AP, the issues regarding binary
classification mentioned above still remain unaddressed. To give a true picture of the predictive power
of the TLP models, a penalty term should be introduced to account for the nodes that are incorrectly
estimated to form an edge with node u at time t.
In a recent work, Poursafaei et al. (2022) highlighted that the state-of-the-art (SoTA) performance
of some TLP models on the standard benchmark datasets is near-perfect. This is counterintuitive
because TLP is a challenging task, even more challenging than link prediction of static graphs, due
to the additional degree of freedom in the data induced by the temporal dimension. The flaw in the
evaluation method is attributed to the limited negative sampling strategy, and the authors propose a
new negative edge sampling strategy which results in a different ranking of the baselines.
Inspired by the critique of the evaluation method, we propose a method to conduct sanity check of
the TLP models to determine if they truly capture the temporal patterns in the data. The sanity check
is formulated as the counterfactual question (Pearl, 2019):
“What if a TLP model which is trained on a temporal graph is tested on temporally
distorted version of the data instead of the real data?”
Ideally, a TLP model which is capable of learning the temporal patterns should perform worse on
temporally distorted data compared to the real data. We conduct an in-depth analysis of this argument
and introduce various data distortion techniques to assess well-known TLP models.
Contributions The contributions of our work can be summarised as follows:
• We introduce simple techniques to distort the temporal patterns within a graph. These
techniques are then used to generate temporally distorted version of the test split of some
famous datasets which can be used for sanity check. Moreover, the distortion methods can
be applied to any temporal graph dataset.
• We perform counterfactual analysis on TLP models such as JODIE (Kumar et al., 2019),
TGAT (Xu et al., 2020), TGN (Rossi et al., 2020), and CAWN (Wang et al., 2020) to check
whether they are capable of capturing the temporal patters within various datasets.
• We propose an alternative evaluation strategy for TLP through which the existing pitfalls
of binary classification and ranking methods can be avoided. We also propose two metrics:
average time difference (ATD), and average count difference (ACD) to measure the perfor-
mance of TLP models. These metrics can provide a holistic picture of a model’s predictive
performance.
Organization In Sec. 2, we define temporal graphs and the associated notations. We also provide
a brief overview of interpreting temporal graphs as point processes, which forms the theoretical
foundation of TLP. In Sec. 3, we formalize the counterfactual analysis through logical arguments,
and also propose data distortion techniques. The results of the counterfactual analysis are presented
in Sec. 4 along with the details of the datasets and TLP models used for evaluation. In Sec. 5, we
suggest a generative evaluation approach for TLP, and discuss the broader impact and limitations of
our work.
2

2 Preliminaries
2.1 Definitions
In TLP literature, continuous-time temporal graphs with ephemeral edges are often considered,
where edges represent interaction events between two nodes at a specific point in time. Alternatively,
temporal graphs can be defined with edges that appear at a certain time and either persist for a
duration (Celikkanat et al., 2024; Farzaneh and Coon, 2023) or accumulate indefinitely. In this work,
we focus on the ephemeral edge temporal graph, also known as interaction graphs (Qin et al., 2024)
or unevenly sampled edge sequence (Qin and Yeung, 2023).
Definition 2.1. A temporal graph with m ∈ N ephemeral edges formed between nodes in U and V
is defined as G = (U, V, E), where E ≜ {(ui, vi, ti) : i ∈ [m], ui ∈ U, vi ∈ V, ti ∈ R} denotes the
set of edges. The tuple (u, v, t) is referred to as an edge event.
While the definition caters to bipartite structure, with U = V, it can also represent general graphs.
Definition 2.2. The occurrences of a particular edge (u, v) in E is denoted as E(u,v) and defined as
E(u,v) ≜ {(u, v, t) : (u, v, t) ∈ E}.
Definition 2.3. The slice of edges in E with timestamps in the range (t1, t2) is denoted as E(t1, t2),
and defined as E(t1, t2) ≜ {(u, v, t) : (u, v, t) ∈ E, t∈ (t1, t2)}.
Definition 2.4. The timestamps in E consisting of m ∈ N edges can be extracted through a function
T : (U × V ×R)m → Rm as T (E) ≜ {t : (u, v, t) ∈ E}.
2.2 Point Process
Perry and Wolfe (2013) modelled the interaction events of a directed edge(u, v) as an inhomoegenous
Poisson point process. In a recent work on continuous-time representation learning on temporal
graphs, Modell et al. (2024) followed suit, and assumed E(u,v) to be sampled from an independent
inhomogenous Poisson point process with intensity λ(u,v)(t). The number of edge events (u, v)
between timestamps t1 and t2 follow a Poisson distribution with rate
R t2
t1
λ(u,v)(t) dt, i.e.,
|E(u,v)(t1, t2)| ∼Poisson
Z t2
t1
λ(u,v)(t) dt

. (1)
To connect the present to the past, Du et al. (2016) view the intensity functionλ⋆
(u,v)(t) as a nonlinear
function of the sample history, where ⋆ indicates that the function is conditioned on the history. The
conditional density function for edge (u, v) is written as
p⋆
(u,v)(t) = λ⋆
(u,v)(t) exp

−
Z t
t′
λ⋆
(u,v)(τ) dτ

, (2)
where t′ < tis the last time when edge (u, v) was observed. The goal is to find the parameters
λ⋆
(u,v)(t) : 0 < t≤ T which can describe the observation E(u,v). This is done by minimizing the
negative log likelihood (NLL) at the timestamps of edge occurrence (Shchur et al., 2021):
min
λ⋆
(u,v)(t) : 0<t≤T
−
X
t∈T (E(u,v))
log

λ⋆
(u,v)(t)

+
Z T
0
λ⋆
(u,v)(τ) dτ, T = max T
 
E(u,v)

. (3)
In (Shchur et al., 2021), the operation of a neural temporal point process is summarized as:
• The edge events in {(u, v, ti) : i ∈ [m]} are represented as feature vectors xi = fe(u, v, ti),
• The historical feature vectors are encoded into a state vector hi = fh(x1, ··· xi−1),
• The distribution of ti conditioned on the past is simply conditioned on hi.
The functions fe and fh, as well as the conditioning onhi, can be implemented using neural networks.
Conjecture 2.1. The samples from a neural temporal point process arelearnable, i.e., a model exists
which can perform temporal link predictions based on the past observations.
3

3 Counterfactual Analysis
Experiment Setup A model f is trained on a temporal graph Etrain and tested on Etest through the
binary classification approach resulting in metrics such as AU-ROC, and AP. In general, Etrain =
E(0, τ0), and Etest = E(τ0, T), i.e., the train and test data are chronologically split from the same
temporal graph which is assumed to be generated through a common causal mechanism.
In light of the experimental setup, we ask the question: “Would the modelf which is trained on Etrain
perform well if tested on a distorted version of Etest instead of Etest?” To formalise the question in
the counterfactual framework proposed by Pearl (2019), we consider the following statements:
x′: The test data is Etest.
x: The test data is a temporally distorted version of Etest.
y′: The performance metric is in the range (α − ϵ, min{1, α+ ϵ}).
y: The performance metric is strictly less than α − ϵ.
Then, the counterfactual question can be framed as P (yx | x′, y′) which stands for:
The probability that the prediction accuracy would be less than α − ϵ had the test
data been a temporally distorted version of Etest, given the prediction accuracy was
observed to be approximately α when the model was tested on Etest.
We link the counterfactual question to our hypothesis in the following proposition:
Proposition 3.1. If P (yx | x′, y′) ≈ 0 =⇒ model f cannot learn the temporal patterns in Etrain.
Proof. Consider the set of logical statements:
s1: The temporal graph E contains patterns that allow future edge predictions to be made based
on past information, i.e., G is learnable.
s2: The model f is capable of learning the patterns in a learnable temporal graph.
s3: Etrain = E(0, τ0), Etest = E(τ0, T).
s4: G′ = D(Etest), where D(·) is the temporal distortion function.
s5: The model f is trained on Etrain.
s6: The prediction metric reported by f on the real test data Etest is higher than the prediction
metric on the distorted data G′.
s1 ∧ s2 ∧ s3 ∧ s4 ∧ s5 =⇒ s6 (4)
¬s6 =⇒ ¬s1 ∨ ¬s2 ∨ ¬s3 ∨ ¬s4 ∨ ¬s5 (contraposition)
For the experimental setup s3 = 1, and s5 = 1. Assuming that the temporal graph G is learnable
s1 = 1, and that the function D(Etest) results in a temporally distorted version of Etest, i.e., s4 = 1,
we get ¬s6 =⇒ ¬s2. Alternatively, ¬s6 ≡ I(P (yx | x′, y′) ≈ 0), and ¬s2 is interpreted as “model
f is incapable of learning the temporal patterns in G”.
Figure 1: Example of Temporal distortion.
Example In Fig. 1, we show thatEtrain ∪Etest
is sampled from a point process with intensity
λ⋆(t), t∈ [0, T]. We generate E′ from another
point process with intensity λ′(t), t∈ [τ0, T].
We depict the intensity functions as two sinu-
soidal waves with different frequency and phase.
If a model f learns this intensity function by
observing Etrain, and then generates samples for
prediction, they would be more similar to Etest
than E′.
4

3.1 Temporal Distortion Techniques
Let E be a temporal graph sampled from a temporal point process with intensity λ⋆(t) for t ∈ [0, T].
Let E′ be data sampled from another point process with intensity λ′(t) for t ∈ [0, T].
Definition 3.1. The temporal graph E′ is δ-temporally distorted w.r.t. E if for some δ >0,
1
T
Z T
0
|λ∗(t) − λ′(t)|dt > δ. (5)
In practice, we do not have access to the true intensity functions, and have to compare the realisations
instead. Let E and E′ be two temporal graphs, then we measure the difference in their characteristics
through the following two metrics.
Definition 3.2. The average time difference (ATD) between E and E′ is defined as:
ATD(E, E′) ≜ 1
T|E|
X
(u,v,t)∈E
min
t′∈T

E′
(u,v)

∪{T}
|t − t′|, (6)
where T = max T (E) − min T (E).
Definition 3.3. The average count difference (ACD) between E and E′ is defined as:
ACD(E, E′) ≜ 1
|E|
X
(u,v,t)∈E
|E(u,v)(t − ¯τ, t+ ¯τ)| − |E′
(u,v)(t − ¯τ, t+ ¯τ)|
, (7)
where ¯τ = max T (E)−min T (E)
|E| .
Now that we are equipped with metrics to measure the difference between two temporal graphs, we
device distortion functions D(·) which can enable us to investigate the counterfactual question posed
earlier. We propose two distortion techniques DINTENSE (·, K) which creates K time-perturbed copies
of each edge events, and DSHUFFLE (·) wherein the timestamps of different edge events are shuffled.
Algorithm 1 DINTENSE
Input E, K∈ N
Output E′
1: E′ = ∅
2: τ0 ← min T (E)
3: T ← max T (E)
4: ¯τ ← T−τ0
|E|
5: for (u, v, t) ∈ Edo
6: for k ∈ [K] do
7: τ ∼ Uniform(−¯τ, ¯τ)
8: E′ ← E′ ∪ {(u, v, t+ τ)}
9: end for
10: end for
Algorithm 2 DSHUFFLE
Input E
Output E′
1: E′ = ∅
2: T ←T (E)
3: for (u, v, t) ∈ Edo
4: τ ∼ T
5: E′ ← E′ ∪ {(u, v, τ)}
6: T ← T \ {τ}
7: end for
INTENSE Let the real temporal graph data be denoted by
E = ∪(u,v)∈U×V E(u,v), and the distorted version be denoted by
E′ = ∪(u,v)∈U×V E′
(u,v). then, for each edge event (u, v, t) in
the real data E, we create K edge events (u, v, t+ τ) with τ
sampled uniformly from (−¯τ, ¯τ) for some ¯τ >0. Alternatively,
if it is known that E(u,v) is sampled from a point process with
intensity λ⋆
(u,v)(t), then we can generate E′
(u,v) by sampling
from another point process with intensity λ′
(u,v)(t), such that
λ′
(u,v)(t) = Kλ⋆
(u,v)(t), ∀(u, v) ∈ U × V.
The operation of DINTENSE is described in Algorithm 1.
SHUFFLE For any two edge events (u, v, t), (u′, v′, t′) ∈
E, we shuffle the timestamps in the distorted version, i.e.
(u, v, t′), (u′, v′, t) ∈ E′. The shuffling process is also called
label permutation (Chatterjee, 2018). In terms of the point pro-
cess, we can explain shuffling as follows. If E(u,v) is known to
be sampled from a point process with intensity λ⋆
(u,v)(t), then
E′
(u,v) can be generated by sampling from an inhomogenous
Poisson point process with intensity λ′
(u,v)(t), where
λ′
(u,v)(t) =
R T
0 λ⋆
(u,v)(t) dt
R T
0 λ⋆(t) dt
λ⋆(t), ∀(u, v) ∈ U × V.
We describe the operation of DSHUFFLE in Algorithm 2.
5

4 Experiment
Datasets We use the following datasets1 to perform counterfactual analysis:
• wikipedia (Kumar et al., 2019) describes a dynamic graph of interaction between the
editors and Wikipedia pages over a span of one month. The entries consist of the user ID,
page ID, and timestamp. The edge features are LIWC-feature vectors (Pennebaker et al.,
2001) of the edit text. The edge feature dimension is 172.
• reddit (Kumar et al., 2019) describes a bipartite interaction graph between the users
and subreddits. The interaction event is recorded with the IDs of the user, subreddit and
timestamp. Similar to wikipedia, the post content is converted into a LIWC-feature vector
of dimension 172 which serves as the edge feature.
• uci (Panzarasa et al., 2009) is a dynamic graph describing message-exchange among the
students at University of California at Irvine (UCI) from April to October 2004. The
interaction event consists of the user IDs, and timestamp.
Table 1: Number of nodes and
edges in temporal graph datasets.
Dataset |U ∪ V| |E|
wikipedia 9227 157474
reddit 10984 672447
uci 1899 59835
The scale of the datasets are presented in Table 1. The datasets
are chronologically split in the ratio 0.7 : 0.15 : 0.15 into train,
validation, and test sets, respectively.
Next, we use DINTENSE (·, 5) and DSHUFFLE (·) to create 10 tem-
porally distorted samples of the test splits of each dataset. In
Table 2, we present the ATD, and ACD by comparing the dis-
torted samples with the original test data of different datasets.
Through DINTENSE (·, 5), the ATD is negligible, however, the
ACD is close to 5. Through DSHUFFLE (·), the ACD is approximately 1 for wikipedia and reddit,
and close to 2 for uci. We also see an increase in ATD which is close to 0.1 for all datasets. Therefore,
the metrics ATDand ACD should be considered in conjunction to measure the dissimilarity of two
temporal graphs.
Table 2: Distortion measures on different datasets.
wikipedia reddit uci
ATD ACD ATD ACD ATD ACD
INTENSE 6.9e-6 ± 2e-8 4.479 ± 1.9e-3 1.6e-6 ± 2e-9 4.112 ± 3.9e-4 1.6e-5 ± 1.2e-7 7.214 ± 1.2e-2
SHUFFLE 0.078 ± 5.7e-4 1.093 ± 3.4e-4 0.099 ± 3e-4 1.033 ± 8e-5 0.132 ± 8.4e-4 1.877 ± 3.3e-3
Models We evaluate2 the performance of the following TLP models3 in light of Proposition 3.1:
• JODIE (Kumar et al., 2019) uses a recurrent neural network (RNN) to generate node
embeddings for each interaction event. The future embedding of a node is estimated through
a novel projection operator which is turn in used to predict future edge events.
• TGAT (Xu et al., 2020) relies on self-attention mechanism to generate node embeddings to
capture the temporal evolution of the graph structure.
• TGN (Rossi et al., 2020) combine memory modules with graph-based operators to create an
encoder-decoder pair capable of creating temporal node embeddings.
• CAWN (Wang et al., 2020) propose a novel strategy based on the law of triadic closure, where
temporal walks retrieve the dynamic graph motifs without explicitly counting and selecting
the motifs. The node IDs are replaced with the hitting counts to facilitate inductive inference.
For all the models we have forked the main branch of their original Github repositories, and added
additional arguments to account for the distortion technique, as well as more focused logging. We
wanted to evaluate GraphMixer (Cong et al., 2023) as it claims superior performance, however the
distorted datasets we generated were not compatible with the dataloader used in their codebase.
1The datasets can be downloaded from https://zenodo.org/records/7213796
2GPU: NVIDIA GeForce RTX™ 3060. CPU: 12th Gen Intel® Core™ i7-12700 × 20; 16.0 GiB.
3The optimal hyper-parameters reported by the models are used.
6

Results The models are evaluated under two settings: transductive, and inductive. In transductive
TLP, the nodes u, vin the positive sample (u, v, t) ∈ Etest were observed during training. In contrast,
in inductive TLP, at least one node inu, vis novel, and was not observed during training.
Table 3: Performance of the models JODIE, TGAT, TGN, and CAWN on three datasets, and their
temporally distorted versions denoted as INTENSE , and SHUFFLE . For each metric, we report
the mean, and the 95% confidence interval ( CI) as mean ± CI. We have marked the metrics in
blue for distortions that showed that a model was incapable of learning on a certain dataset as per
Proposition 3.1, and orange otherwise, with ϵ = 0.05.
JODIE wikipedia reddit uci
AU-ROC AP AU-ROC AP AU-ROC AP
transductive 0.9170 ± 3e-3 0.9137 ± 5e-3 0.9679 ± 4e-3 0.9654 ± 5e-3 0.8950 ± 3e-3 0.8726 ± 5e-3
INTENSE 0.9177 ± 7e-3 0.9078 ± 1e-2 0.9619 ± 9e-3 0.9567 ± 1e-2 0.9244 ± 2e-3 0.9129 ± 5e-3
SHUFFLE 0.9097 ± 2e-2 0.8962 ± 4e-2 0.9661 ± 1e-2 0.9613 ± 4e-2 0.8852 ± 3e-3 0.8509 ± 3e-3
inductive 0.8941 ± 4e-3 0.8970 ± 5e-3 0.9343 ± 9e-3 0.9138 ± 2e-2 0.7546 ± 8e-3 0.7310 ± 2e-2
INTENSE 0.9036 ± 1e-2 0.8972 ± 1e-2 0.9457 ± 3e-2 0.9308 ± 4e-2 0.8384 ± 3e-3 0.8332 ± 8e-3
SHUFFLE 0.9157 ± 1e-2 0.9078 ± 2e-2 0.9419 ± 3e-2 0.9251 ± 6e-3 0.7368 ± 5e-3 0.6994 ± 8e-3
TGAT wikipedia reddit uci
AU-ROC AP AU-ROC AP AU-ROC AP
transductive 0.9499 ± 2e-3 0.9528 ± 2e-3 0.9806 ± 6e-4 0.9818 ± 6e-4 0.7885 ± 1e-2 0.7694 ± 7e-3
INTENSE 0.9680 ± 2e-3 0.9691 ± 2e-3 0.9821 ± 6e-4 0.9825 ± 6e-4 0.8707 ± 1e-2 0.8637 ± 2e-2
SHUFFLE 0.9492 ± 5e-3 0.9532 ± 5e-3 0.9814 ± 7e-3 0.9826 ± 6e-3 0.7719 ± 1e-2 0.7336 ± 2e-2
inductive 0.9353 ± 2e-3 0.9401 ± 2e-3 0.9641 ± 1e-3 0.9658 ± 1e-3 0.7020 ± 8e-3 0.7008 ± 1e-2
INTENSE 0.9604 ± 2e-3 0.9621 ± 2e-3 0.9676 ± 8e-4 0.9676 ± 1e-3 0.8019 ± 2e-2 0.8095 ± 2e-2
SHUFFLE 0.9257 ± 7e-3 0.9304 ± 7e-3 0.9644 ± 7e-3 0.9664 ± 3e-3 0.6558 ± 7e-3 0.6324 ± 1e-2
TGN wikipedia reddit uci
AU-ROC AP AU-ROC AP AU-ROC AP
transductive 0.9370 ± 1e-3 0.9472 ± 1e-3 0.9545 ± 1e-3 0.9578 ± 1e-3 0.7826 ± 1e-2 0.7975 ± 1e-2
INTENSE 0.9898 ± 1e-3 0.9911 ± 6e-4 0.9723 ± 2e-3 0.9744 ± 2e-3 0.9653 ± 3e-3 0.9709 ± 3e-3
SHUFFLE 0.8310 ± 3e-2 0.8487 ± 3e-2 0.9533 ± 2e-3 0.9563 ± 2e-3 0.6722 ± 6e-2 0.6520 ± 4e-2
inductive 0.9374 ± 1e-3 0.9463 ± 1e-3 0.9299 ± 1e-3 0.9346 ± 1e-3 0.7714 ± 6e-3 0.7948 ± 6e-3
INTENSE 0.9903 ± 1e-3 0.9908 ± 6e-4 0.9617 ± 3e-3 0.9645 ± 3e-3 0.9592 ± 3e-3 0.9650 ± 2e-3
SHUFFLE 0.8194 ± 2e-2 0.8376 ± 3e-2 0.9266 ± 4e-3 0.9299 ± 3e-3 0.6245 ± 2e-2 0.6193 ± 9e-3
CAWN wikipedia reddit uci
AU-ROC AP AU-ROC AP AU-ROC AP
transductive 0.9886 ± 1e-4 0.9901 ± 1e-4 0.9864 ± 4e-3 0.9884 ± 3e-3 0.9162 ± 9e-4 0.9397 ± 8e-4
INTENSE 0.9977 ± 9e-5 0.9975 ± 8e-5 0.9931 ± 8e-5 0.9942 ± 7e-5 0.9848 ± 6e-4 0.9889 ± 7e-4
SHUFFLE 0.9868 ± 3e-4 0.9887 ± 3e-4 0.9859 ± 6e-4 0.9880 ± 2e-3 0.8495 ± 7e-3 0.8866 ± 2e-3
inductive 0.9877 ± 5e-4 0.9896 ± 4e-4 0.9833 ± 5e-3 0.9859 ± 3e-3 0.9052 ± 1e-2 0.9273 ± 2e-3
INTENSE 0.9972 ± 6e-4 0.9971 ± 1e-5 0.9929 ± 8e-5 0.9938 ± 8e-5 0.9810 ± 3e-3 0.9857 ± 2e-3
SHUFFLE 0.9876 ± 1e-2 0.9896 ± 6e-3 0.9826 ± 8e-4 0.9851 ± 1e-3 0.8383 ± 3e-2 0.8783 ± 3e-2
From Table 3 it is evident that none of the models are capable of distinguishing between the real data,
and data sampled from a five-times more intense version. However, we see that TGN is fairly robust
when the timestamps of the test data are shuffled, as its performance worsens the most compared to
other models. The performance gap between the real and distorted versions decrease as the dataset
size increases (see Table. 1).
In Fig. 2, and Fig. 3 we present the metric gap E[yx − y′] for x ∼ DINTENSE (x′, 5), and x ∼
DSHUFFLE (x′), respectively, for different models in categorical bar plots grouped by the dataset. We
7

check whether max yx < min y′ in an empirical way by checking if E[yx] + CI < E[y′] − CI′ =⇒
E[yx] − E[y′] < −(CI + CI′). Therefore, we plot E[yx] − E[y′] as coloured bars, and −(CI + CI′)
as black diamonds. Moreover, we indicate ϵ = 0.05 as the dashed black line passing through −0.05.
uci wikipedia reddit
0.0
0.1
JODIE
TGAT
TGN
CAWN
(a) AU-ROC change, transductive
uci wikipedia reddit
0.05
0.00
0.05
0.10
0.15
JODIE
TGAT
TGN
CAWN (b) AU-ROC change, inductive
uci wikipedia reddit
0.05
0.00
0.05
0.10
0.15
 JODIE
TGAT
TGN
CAWN
(c) AP change, transductive
uci wikipedia reddit
0.0
0.1
JODIE
TGAT
TGN
CAWN (d) AP change, inductive
Figure 2: Ex∼DINTENSE (x′,5)[yx − y′].
uci wikipedia reddit
0.100
0.075
0.050
0.025
0.000
JODIE
TGAT
TGN
CAWN
(a) AU-ROC change, transductive
uci wikipedia reddit
0.15
0.10
0.05
0.00
JODIE
TGAT
TGN
CAWN (b) AU-ROC change, inductive
uci wikipedia reddit
0.15
0.10
0.05
0.00
JODIE
TGAT
TGN
CAWN
(c) AP change, transductive
uci wikipedia reddit
0.15
0.10
0.05
0.00
JODIE
TGAT
TGN
CAWN (d) AP change, inductive
Figure 3: Ex∼DSHUFFLE (x′)[yx − y′].
The models evaluated in this work form the set of baselines to validate the performance of new models.
However, as we demonstrate, a higher metric alone is not indicative of good performance without
sanity checks. The counterfactual question helps make the evaluation more explainable, as models
that perform worse on temporally distorted data with high ATDand ACD can claim superiority over
modes that do not. An ideal TLP model should be able to capture the difference in the count of edge
events, as well as temporal shifts in the edge events.
8

5 Discussion
Moving away from the binary classification approach to assess the performance of temporal link
prediction, the research should explore a generative approach where after observing a temporal graph
from time t ∈ (0, τ0), the model can generate a temporal graph in t ∈ (τ0, T). This generated
temporal graph should be compared with the ground truth for similarity to assess the performance of
the model. The metrics ATDand ACD can be used to measure the difference in the timestamps, as
well as the edge counts along the time axis.
We showed that the performance gap in light of Proposition 3.1 decreases with increasing size of the
temporal graph, focus should be establish TLP models on smaller datasets, first in the transductive
setting, and then progress to inductive setting. In the generative method of evaluation, we can also
make use of other metrics that characterise a network, or a point process to add additional constraints.
Broader Impact We presented a framework, wherein we asked a counterfactual question, and
then designed intervention mechanisms by generating temporally distorted test sets. In the future,
researchers can devise their own temporal distortion techniques to assess the performance of a TLP
model, if they follow the binary classification approach to evaluation. Our aim is also to encourage
researchers to explore the gnerative evaluation strategy, and design TLP models which can gnerate
temporal graphs after observing the edge events in the past. While our work focused on temporal
graphs with ephemeral edges (see Definition 2.1), distortion techniques can also be designed for
interval graphs, where the edge events persist for a duration. In this work, rather than introducing
novel datasets, we present techniques for generating temporally distorted versions of any temporal
graph dataset. This makes the contribution relevant even for datasets which will be introduced in the
future.
Limitations Due to resource constraints, we could not evaluate the models on more datasets.
However, we aim to get additional results by the rebuttal period on the datasets used in Poursafaei
and Rabbany (2023). We also wanted to measure the performance of the models through ranking
metrics like MRR or MAR, but the distorted datasets were not compatible with the dataloader used
by Temporal Graph Benchmark (TGB) (Huang et al., 2024).
