Meta-Referential Games to Learn Compositional
Learning Behaviours
Anonymous Author(s)
Abstract
Human beings use compositionality to generalise from past to novel experiences,1
assuming that past experiences can be decomposed into fundamental atomic com-2
ponents that can be recombined in novel ways. We frame this as the ability to learn3
to generalise compositionally, and refer to behaviours making use of this ability as4
compositional learning behaviours (CLBs). Learning CLBs requires the resolution5
of a binding problem (BP). While it is another feat of intelligence that human beings6
perform with ease, it is not the case for artificial agents. Thus, in order to build arti-7
ficial agents able to collaborate with human beings, we develop a novel benchmark8
to investigate agents’ abilities to exhibit CLBs by solving a domain-agnostic ver-9
sion of the BP. Taking inspiration from the Emergent Communication, we propose10
a meta-learning extension of referential games, entitled Meta-Referential Games,11
to support our benchmark, the Symbolic Behaviour Benchmark (S2B). Baseline12
results and error analysis show that the S2B is a compelling challenge that we hope13
will spur the research community to develop more capable artificial agents.14
1 Introduction15
Defining compositional behaviours (CBs) as "the ability to generalise from combinations of trained-16
on atomic components to novel re-combinations of those very same components", we can define17
compositional learning behaviours (CLBs) as "the ability to generalise in an online fashionfrom a18
few combinations of never-before-seen atomic components to novel re-combinations of those very19
same components”. We employ the term online here to imply a few-shot learning context [Vinyals20
et al., 2016, Mishra et al., 2018] that demands that agents learn from, and then leverage some novel21
information, both over the course of a single lifespan, or episode, in our case of few-shot meta-RL22
(see Beck et al. [2023] for a review of meta-RL). Thus, in this paper, we investigate artificial agents’23
abilities for CLBs, which involve a few-shot learning aspect that is not present in CBs.24
Compositional Learning Behaviours as Symbolic Behaviours.Santoro et al. [2021] states that25
a symbolic entity does not exist in an objective sense but solely in relation to an “ interpreter who26
treats it as such”, and it ensues that there exists a set of behaviours, i.e. symbolic behaviours, that27
are consequences of agents engaging with symbols. Thus, in order to evaluate artificial agents in28
terms of their ability to collaborate with humans, we can use the presence or absence of symbolic29
behaviours. Among the different characteristic of symbolic behaviours, this work will primarily focus30
on the receptivity and constructivity aspects. Receptivity aspects amount to the ability to receive31
new symbolic conventions in an online fashion. For instance, when a child introduces an adult to32
their toys’ names, the adults are able to discriminate between those new names upon the next usage.33
Constructivity aspects amount to the ability to form new symbolic conventions in an online fashion.34
For instance, when facing novel situations that require collaborations, two human teammates can35
Submitted to the 38th Conference on Neural Information Processing Systems (NeurIPS 2024) Track on Datasets
and Benchmarks. Do not distribute.

come up with novel referring expressions to easily discriminate between different events occurring.36
Both aspects refer to abilities that support collaboration. Thus, this paper develops a benchmark to37
evaluate agents’ abilities in receptive and constructive behaviours, with a primary focus on CLBs.38
Binding Problem & Meta-Learning.Following Greff et al. [2020], we refer to the binding problem39
(BP) as the challenges in “dynamically and flexibly bind[/re-use] information that is distributed40
throughout the [architecture]” of some artificial agents (modelled with artificial neural networks here).41
We note that there is an inherent BP that requires solving for agents to exhibit CLBs. Indeed, over42
the course of a single episode (as opposed to a whole training process, in the case of CBs), agents43
must dynamically identify/segregate the component values from the observation of multiple stimuli,44
timestep after timestep, and then bind/(re-)use/(re-)combine this information (hopefully stored in45
some memory component of their architecture) in order to respond correctly to novel stimuli.Solving46
the BP instantiated in such a context, i.e. re-using previously-acquired information in ways that47
serve the current situation, is another feat of intelligence that human beings perform with ease,48
on the contrary to current state-of-the-art artificial agents. Thus, our benchmark must emphasise49
testing agents’ abilities to exhibit CLBs by solving a version of the BP. Moreover, we argue for a50
domain-agnostic BP, i.e. not grounded in a specific modality such as vision or audio, as doing so51
would limit the external validity of the test. We aim for as few assumptions as possible to be made52
about the nature of the BP we instantiate [Chollet, 2019]. This is crucial to motivate the form of the53
stimuli we employ, and we will further detail this in Section 3.1.54
Language Grounding & Emergence.In order to test the quality of some symbolic behaviours,55
our proposed benchmark needs to query the semantics that agents ( the interpreters) may extract56
from their experience, and it must be able to do so in a referential fashion (e.g. being able to57
query to what extent a given experience is referred to as, for instance, ‘the sight of a red tomato’),58
similarly to most language grounding benchmarks. Subsequently, acknowledging that the simplest59
form of collaboration is maybe the exchange of information, i.e. communication, via a given code,60
or language, we argue that the benchmark must therefore also allow agents to manipulate this61
code/language that they use to communicate. This property is known as the metalinguistic/reflexive62
function of languages [Jakobson, 1960]. It is mainly investigated in the current deep learning era63
within the field of Emergent Communication ( Lazaridou and Baroni [2020], and see Brandizzi64
[2023] and Denamganaï and Walker [2020a] for further reviews), via the use of variants of the65
referential games (RGs) [Lewis, 1969]. Thus, we take inspiration from the RG framework, where66
(i) the language domain represents a semantic domain that can be probed and queried, and (ii) the67
reflexive function of language is indeed addressed. Then, in order to instantiate different BPs at each68
episode , we propose a meta-learning extension to RGs, entitled Meta-Referential Games, and use69
this framework to build our benchmark. It results in our proposed Symbolic Behaviour Benchmark70
(S2B), which has the potential to test for many aspects of symbolic behaviours.71
After review of the background (Section 2) , we will present our contributions as follows: we propose72
the Symbolic Behaviour Benchmark to enables evaluation of symbolic behaviours in Section 3,73
presenting the Symbolic Continuous Stimulus (SCS) representation scheme which is able to instantiate74
a BP, on the contrary to common symbolic representations (Section 3.1), and our Meta-Referential75
Games framework, a meta-learning extension to RGs (Section 3.2);then we provide baseline results76
and error analysis in Section 4 showing that our benchmark is a compelling challenge that we hope77
will spur the research community.78
2 Background79
Figure 1: Illustration of a discriminative 2-
players / L-signal / N-round variant of a RG.
The first instance of an environment with a primary80
focus on efficient communication is the signaling81
game or referential game (RG) by Lewis [1969],82
where a speaker agent is asked to send a message to83
the listener agent, based on the state/stimulus of the84
world that it observed. The listener agent then acts upon the observed message by choosing one of85
2

the actions available to it. Both players’ goals are aligned (it features pure coordination/common86
interests), with the aim of performing the ‘best’ action given the observed state.In the recent deep87
learning era, many variants of the RG have appeared [Lazaridou and Baroni, 2020]. Following the88
nomenclature proposed in Denamganaï and Walker [2020b], Figure 1 illustrates in the general case a89
discriminative 2-players / L-signal / N-round / K-distractors / descriptive / object-centric variant,90
where the speaker receives a stimulus and communicates with the listener (up to N back-and-forth91
using messages of at mostL tokens each), who additionally receives a set ofK +1 stimuli (potentially92
including a semantically-similar stimulus as the speaker, referred to as an object-centric stimulus).93
The task is for the listener to determine, via communication with the speaker, whether any of its94
observed stimuli match the speaker’s. We highlight here features of RGs that will be relevant to how95
S2B is built, and then provide formalism used throughout the paper. The number of communication96
rounds N characterises (i) whether the listener agent can send messages back to the speaker agent97
and (ii) how many communication rounds can be expected before the listener agent is finally tasked98
to decide on an action. The basic (discriminative) RG is stimulus-centric, which assumes that99
both agents would be somehow embodied in the same body, and they are tasked to discriminate100
between given stimuli, that are the results of one single perception ‘system’. On the other hand, Choi101
et al. [2018] introduced an object-centric variant which incorporates the issues that stem from the102
difference of embodiment (which has been later re-introduced under the name Concept game by Mu103
and Goodman [2021]). The agents must discriminate between objects (or scenes) independently of104
the viewpoint from which they may experience them. In the object-centric variant, the game is more105
about bridging the gap between each other’s cognition rather than just finding a common language.106
The adjective ‘object-centric’ is used to qualify a stimulus that is different from another but actually107
present the same meaning (e.g. same object, but seen under a different viewpoint). Following the108
last communication round, the listener outputs a decision ( DL
i in Figure 2) about whether any of109
the stimulus it is observing matches the one (or a semantically similar one, in object-centric RGs)110
experienced by the speaker, and if so its action index must represent the index of the stimulus it111
identifies as being the same. The descriptive variant allows for none of the stimuli to be the same as112
the target one, therefore the action of index 0 is required for success. The agent’s ability to make the113
correct decision over multiple RGs is referred to as RG accuracy.114
Compositionality, Disentanglement & Systematicity.Compositionality is a phenomenon that115
human beings are able to identify and leverage thanks to the assumption that reality can be decomposed116
over a set of “disentangle[d,] underlying factors of variations” [Bengio, 2012], and our experience117
is a noisy, entangled translation of this factorised reality. This assumption is critical to the field118
of unsupervised learning of disentangled representations [Locatello et al., 2020] that aims to find119
“manifold learning algorithms” [Bengio, 2012], such as variational autoencoders (V AEs [Kingma and120
Welling, 2013]), with the particularity that the latent encoding space would consist of disentangled121
latent variables (see Higgins et al. [2018] for a formal definition). As a concept, compositionality122
has been the focus of many definition attempts. For instance, it can be defined as “the algebraic123
capacity to understand and produce novel combinations from known components”(Loula et al. [2018]124
referring to Montague [1970]) or as the property according to which “the meaning of a complex125
expression is a function of the meaning of its immediate syntactic parts and the way in which they are126
combined” [Krifka, 2001]. Although difficult to define, the commmunity seems to agree on the fact127
that it would enable learning agents to exhibit systematic generalisation abilities (also referred to as128
combinatorial generalisation [Battaglia et al., 2018]). While often studied in relation to languages, it is129
usually defined with a focus on behaviours. In this paper, we will refer to (linguistic) compositionality130
when considering languages, and interchangeably compositional behaviours or systematicity to refer131
to “the ability to entertain a given thought implies the ability to entertain thoughts with semantically132
related contents”[Fodor and Pylyshyn, 1988].133
Compositionality can be difficult to measure. Brighton and Kirby [2006]’s topographic similarity134
(topsim) which is acknowledged by the research community as the main quantitative metric [Lazari-135
dou et al., 2018, Guo et al., 2019, Słowik et al., 2020, Chaabouni et al., 2020, Ren et al., 2020].136
Recently, taking inspiration from disentanglement metrics, Chaabouni et al. [2020] proposed the137
posdis (positional disentanglement) and bosdis (bag-of-symbols disentanglement) metrics, that138
3

have been shown to be differently ‘opinionated’ when it comes to what kind of compositionality139
they capture. As hinted at by Choi et al. [2018], Chaabouni et al. [2020] and Dessi et al. [2021],140
compositionality and disentanglement appears to be two sides of the same coin, in as much as141
emergent languages are discrete and sequentially-constrained unsupervisedly-learned representations.142
In Section 3.1, we bridge further compositional language emergence and unsupervised learning of143
disentangled representations by asking what would an ideally-disentangled latent space look like? to144
build our proposed benchmark.145
Richness of the Stimuli & Systematicity.Chaabouni et al. [2020] found that compositionality is not146
necessary to bring about systematicity, as shown by the fact that non-compositional languages wielded147
by symbolic (generative) RG players were enough to support success in zero-shot compositional148
tests (ZSCTs). They found that the emergence of a posdis-compositional language was a sufficient149
condition for systematicity to emerge. Finally, they found a necessary condition to foster systematicity,150
that we will refer to as richness of stimuli condition (Chaa-RSC). It was framed as (i) having a large151
stimulus space |I| = ival
iattr , where iattr is the number of attributes/factor dimensions, and ival152
is the number of possible values on each attribute/factor dimension, and (ii) making sure that it153
is densely sampled during training, in order to guarantee that different values on different factor154
dimensions have been experienced together. In a similar fashion, Hill et al. [2019] also propose a155
richness of stimuli condition (Hill-RSC) that was framed as a data augmentation-like regularizer156
caused by the egocentric viewpoint of the studied embodied agent. In effect, the diversity of viewpoint157
allowing the embodied agent to observe over many perspectives the same and unique semantical158
meaning allows a form of contrastive learning that promotes the agent’s systematicity.159
3 Symbolic Behaviour Benchmark160
The version of the S2B1 that we present in this paper is focused on evaluating receptive and construc-161
tive behaviour traits via a single task built around 2-players multi-agent RL (MARL) episodes where162
players engage in a series of RGs (cf. lines 11 and 17 in Alg. 5 calling Alg. 3). We denote one such163
episode as a meta-RG and detail it in Section 3.2. Each RG within an episode consists of N + 2 RL164
steps, where N is the number of communication rounds available to the agents (cf. Section 2). At165
each RL step, agents both observe similar or different object-centric stimuli and act simultaneously166
from different actions spaces, depending on their role as the speaker or the listener of the game.167
Stimuli are presented to the agent using the Symbolic Continuous Stimulus (SCS) representation168
that we present in Section 3.1. Each RG in a meta-RG follows the formalism laid out in Section 2,169
with the exception that speaker and listener agents speak simultaneously and observe each other’s170
messages upon the next RL step. Thus, at step N + 1, the speaker’s action space consists solely of a171
no-operation (NO-OP) action while the listener’s action space consists solely of the decision-related172
action space. In practice, the environment simply ignores actions that are not allowed depending on173
the RL step. Next, step N + 2 is intended to provide feedback to the listener agent as its observation174
is replaced with the speaker’s observation (cf. line 12 and 18 in Alg. 5). Note that this is the exact175
stimulus that the speaker has been observing, rather than apossible object-centric sample. In Figure 3,176
we present SCS-represented stimuli, observed by a speaker over the course of a typical episode.177
3.1 Symbolic Continuous Stimulus representation178
Building about successes of the field of unsupervised learning of disentangled representations [Higgins179
et al., 2018], to the question what would an ideally-disentangled latent space look like?, we propose180
the Symbolic Continuous Stimulus (SCS) representation and provide numerical evidence of it in181
Appendix D.2. It is continuous and relying on Gaussian kernels, and it has the particularity of182
enabling the representation of stimuli sampled from differently semantically structured symbolic183
spaces while maintaining the same representation shape (later referred as the shape invariance184
property), as opposed to the one-/multi-hot encoded (OHE/MHE) vector representation commonly185
used when dealing with symbolic spaces. While the SCS representation is inspired by vectors186
1HIDDEN_FOR_REVIEW_PURPOSE
4

Figure 2: Left: Sampling of the necessary components to create the i-th RG ( RGi) of a meta-RG.
The target stimulus (red) and the object-centric target stimulus (purple) are both sampled from
the Target Distribution T Di, a set of O different stimuli representing the same latent semantic
meaning. The latter set and a set of K distractor stimuli (orange) are both sampled from a dataset of
SCS-represented stimuli (Dataset), which is instantiated from the current episode’s symbolic space,
whose semantic structure is sampled out of the meta-distribution of available semantic structure
over Ndim-dimensioned symbolic spaces. Right: Illustration of the resulting meta-RG with a focus
on the i-th RG RGi. The speaker agent receives at each step the target stimulus si
0 and distractor
stimuli (si
k)k∈[1;K], while the listener agent receives an object-centric version of the target stimulus
s′i
0 or a distractor stimulus (randomly sampled), and other distractor stimuli (si
k)k∈[1;K], with the
exception of the Listener Feedback stepwhere the listener agent receives feedback in the form
of the exact target stimulus si
0. The Listener Feedback step takes place after the listener agent has
provided a decision DL
i about whether the target meaning is observed or not and in which stimuli is
it instantiated, guided by the vocabulary-permutated message MS
i from the speaker agent.
sampled from V AE’s latent spaces, this representation is not learned and is not aimed to help the187
agent performing its task. It is solely meant to make it possible to define a distribution over infinitely188
many semantic/symbolic spaces, while instantiating a BP for the agent to resolve. Indeed, contrary to189
OHE/MHE representation, observation of one stimulus is not sufficient to derive the nature of the190
underlying semantic space that the current episode instantiates. Rather, it is only via a kernel density191
estimation on multiple samples (over multiple timesteps) that the semantic space’s nature can be192
inferred, thus requiring the agent to segregated and (re)combine information that is distributed over193
multiple observations. In other words, the benchmark instantiates a domain-agnostic BP. We provide194
in Appendix D.1 some numerical evidence to the fact that the SCS representation differentiates itself195
from the OHE/MHE representation because it instantiates a BP. Deriving the SCS representation196
from an idealised V AE’s latent encoding of stimuli of any domain makes it a domain-agnostic197
representation, which is an advantage compared to previous benchmark because domain-specific198
information can therefore not be leveraged to solve the benchmark.199
In details, the semantic structure of anNdim-dimensioned symbolic space is the tuple(d(i))i∈[1;Ndim]200
where Ndim is the number of latent/factor dimensions, d(i) is the number of possible symbolic201
values for each latent/factor dimension i. Stimuli in the SCS representation are vectors sampled202
from the continuous space [−1, +1]Ndim. In comparison, stimuli in the OHE/MHE representation203
are vectors from the discrete space {0, 1}dOHE where dOHE = ΣNdim
i=1 d(i) depends on the d(i)’s.204
Note that SCS-represented stimuli have a shape that does not depend on the d(i)’s values, this is the205
shape invariance property of the SCS representation (see Figure 4(bottom) for an illustration).206
In the SCS representation, the d(i)’s do not shape the stimuli but only the semantic structure, i.e.207
representation and semantics are disentangled from each other. The d(i)’s shape the semantic by208
enforcing, for each factor dimension i, a partitionaing of the [−1, +1] range into d(i) value sections.209
Each partition corresponds to one of the d(i) symbolic values available on the i-th factor dimension.210
Having explained how to build the SCS representation sampling space, we now describe how to211
sample stimuli from it. It starts with instantiating a specific latent meaning/symbol, embodied by212
latent values l(i) on each factor dimension i, such that l(i) ∈ [1; d(i)]. Then, the i-th entry of the213
stimulus is populated with a sample from a corresponding Gaussian distribution over the l(i)-th214
partition of the [−1, +1 range. It is denoted as gl(i) ∼ N(µl(i), σl(i)), where µl(i) is the mean of the215
Gaussian distribution, uniformly sampled to fall within the range of the l(i)-th partition, and σl(i) is216
the standard deviation of the Gaussian distribution, uniformly sampled over the range [ 2
12d(i) , 2
6d(i) ].217
µl(i) and σl(i) are sampled in order to guarantee (i) that the scale of the Gaussian distribution is large218
5

enough, but (ii) not larger than the size of the partition section it should fit in. Figure 3 shows an219
example of such instantiation of the different Gaussian distributions over each factor dimensions’220
[−1, +1] range.221
3.2 Meta-Referential Games222
Figure 3: Visualisation of the SCS-
represented stimuli (column) observed
by the speaker agent at each RG over the
course of one meta-RG, with Ndim = 3
and d(0) = 5 , d(1) = 5 , d(2) = 3 .
The supporting phase lasted for 19 RGs.
For each factor dimension i ∈ [0; 2], we
present on the right side of each plot the
kernel density estimations of the Gaus-
sian kernels N(µl(i), σl(i)) of each la-
tent value available on that factor dimen-
sion l(i) ∈ [1; d(i)]. Colours of dots,
used to represent the sampled value gl(i),
imply the latent value l(i)’s Gaussian
kernel from which said continuous value
was sampled. As per construction, for
each factor dimension, there is no over-
lap between the different latent values’
Gaussian kernels.
Thanks to the shape invariance property of the SCS repre-223
sentation, once a number of latent/factor dimension Ndim224
is choosen, we can synthetically generate many different225
semantically structured symbolic spaces while maintain-226
ing a consistent stimulus shape. This is critical since227
agents must be able to deal with stimuli coming from dif-228
ferently semantically structured Ndim-dimensioned sym-229
bolic spaces. In other words that are more akin to the230
meta-learning field, we can define a distribution over many231
kind of tasks, where each task instantiates a different se-232
mantic structure to the symbolic space our agent should233
learn to adapt to. Figure 2 highlights the structure of234
an episode, and its reliance on differently semantically235
structured Ndim-dimensioned symbolic spaces. Agents236
aim to coordinate efficiently towards scoring a high ac-237
curacy during the ZSCTs at the end of each RL episode.238
Indeed, a meta-RG is composed of two phases: a sup-239
porting phase where supporting stimuli are presented, and240
a querying/ZSCT phase where ZSCT-purposed RGs are241
played. During the querying phase, the presented target242
stimuli are novel combinations of the component values243
of the target stimuli presented during the supporting phase.244
Algorithms 4 and 5 contrast how a common RG differ245
from a meta-RG (in Appendix A). We emphasise that the246
supporting phase of a meta-RG does not involve updat-247
ing the parameters/weights of the learning agents, since248
this is a meta-learning framework of the few-shot learning249
kind (compare positions and dependencies of lines 21 in250
Alg. 5 and 6 in Alg. 4). During the supporting phase, each251
RG involves a different target stimulus until all the pos-252
sible component values on each latent/factor dimensions253
have been shown for at least S shots (cf. lines 3 − 7 in254
Alg. 5). While it amounts to at least S different target255
stimulus being shown, the number of supporting-phase256
RG played remains far smaller than the number of pos-257
sible training-purposed stimuli in the current episode’s258
symbolic space/dataset. Then, the querying phase sees all259
the testing-purposed stimuli being presented.Emphasising260
further, during one single RL episode, both supporting and querying RGs are played, without the261
agent’s parameters changing in-between the two phases, since learning CLBs involve agents adapting262
in an online/few-shot learning setting. The semantic structure of the symbolic space is randomly263
sampled at the beginning of each episode (cf. lines 2 − 3 in Alg. 5) The reward function proposed to264
both agents is null at all steps except on the N + 1-th step, being +1 if the listener agent decided265
correctly or, during the querying phase only, −2 if incorrect (cf. line 21 in Alg. 5).266
Vocabulary Permutation. We bring the readers attention on the fact that simply changing the267
semantic structure of the symbolic space, is not sufficient to force MARL agents to adapt specifically268
to the instantiated symbolic space at each episode. Indeed, they can learn to cheat by relying on269
an episode-invariant (and therefore independent of the instantiated semantic structure) emergent270
6

language (EL) which would encode the continuous values of the SCS representation like an analog-271
to-digital converter would. This cheating language would consist of mapping a fine-enough partition272
of the [−1, +1] range onto a fixed vocabulary in a bijective fashion (see Appendix C for more details).273
Therefore, in order to guard the MARL agents from making a cheating language emerge, we employ274
a vocabulary permutation scheme [Cope and Schoots, 2021] that samples at the beginning of each275
episode/task a random permutation of the vocabulary symbols (cf. line 1 in Alg. 2).276
Richness of the Stimulus.We further bridge the gap between Hill-RSC and Chaa-RSC by allowing277
the number of object-centric samplesO and the number of shotsS to be parameterized in the278
benchmark. S represents the minimal number of times any given component value may be observed279
throughout the course of an episode. Intuitively, throughout their lifespan, an embodied observer280
may only observe a given component (e.g. the value ‘blue’, on the latent/factor dimension ‘color’)281
a limited number of times (e.g. one time within a ‘blue car’ stimulus, and another time within a282
‘blue cup’ stimulus). These parameters allow the experimenters to account for both the Chaa-RSC’s283
sampling density of the different stimulus components and Hill-RSC’s diversity of viewpoints.284
4 Experiments285
Agent Architecture.The architectures of the RL agents that we consider are detailed in Appendix B.286
Optimization is performed via an R2D2 algorithm[Kapturowski et al., 2018] augmented with both the287
Value Decomposition Network[Sunehag et al., 2017] and theSimplified Action Decoder approach [Hu288
and Foerster, 2019]. As preliminary results showed poor performance, we follow Hill et al. [2020]289
and add an auxiliary reconstruction task to promote agents learning to use their core memory module.290
It consists of a mean squared-error between the stimuli observed at a given time step and a prediction291
conditioned on the current state of the core memory module after processing the current stimuli.292
4.1 Learning CLBs is Out-Of-Reach to State-of-the-Art MARL293
Table 1: Meta-RG ZSCT and Ease-of-Acquisition
(EoA) ZSCT accuracies and linguistic compositionality
measures (% ± s.t.d.) for the multi-agent context after
a sampling budget of 500k. The last column shows lin-
guist results when evaluating the Posdis-Speaker (PS).
Shots PS
Metric S = 1 S = 2
AccZSCT ↑ 53.6 ± 4.7 51 .6 ± 2.2 N/A
AccEoA ↑ 50.6 ± 8.8 50 .6 ± 5.8 N/A
topsim ↑ 29.6 ± 16.8 21 .3 ± 16.6 96 .7 ± 0
posdis ↑ 23.7 ± 20.8 13 .8 ± 12.8 92 .0 ± 0
bosdis ↑ 25.6 ± 22.9 19 .1 ± 17.5 11 .6 ± 0
Playing a meta-RG, the speaker aims at294
each episode to make emerge a new lan-295
guage (constructivity) and the listener aims296
to acquire it (receptivity) as fast as possible,297
before the querying-phase of the episode298
comes around. Critically, we assume that299
both agents must perform in accordance300
with the principles of CLBs as it is the only301
resolution approach. Indeed, there is no302
success without a generalizing and easy-303
to-learn EL, or, in other words, a (linguis-304
tically) compositional EL [Brighton and305
Kirby, 2001, Brighton, 2002]. Thus, we306
investigate whether agents are able to coordinate to learn to perform CLBs from scratch, which is307
tantamount to learning receptivity and constructivity aspects of CLBs in parallel.308
Evaluation & Results.We report the performance and compositionality of the behaviours in the multi-309
agent context in Table 1, on 3 random seeds of an LSTM-based model in the task with Ndim = 3,310
Vmin = 2, Vmax = 5, O = 4, and S = 1 or 2. As we assume no success without emergence of a311
(linguistically) compositional language, we measure the linguistic compositionality profile of the312
emerging languages by, firstly, freezing the speaker agent’s internal state (i.e. LSTM’s hidden and313
cell states) at the end of an episode and query what would be its subsequent utterances for all stimuli314
in the latest episode’s dataset (see Figure 2), and then compute the different compositionality metrics315
on this collection of utterances. We compare the compositionality profile of the ELs to that of a316
compositional language, in the sense of the posdis compositionality metric [Chaabouni et al., 2020]317
(see Figure 4(left) and Table 4 in Appendix B.2). This language is produced by a fixed, rule-based318
agent that we will refer to as the Posdis-Speaker (PS). Similarly, after the latest episode ends and the319
7

speaker agent’s internal state is frozen, we evaluate the EoA of the emerging languages by training a320
new, non-meta/common listener agentfor 512 epochs on the latest episode’s dataset with the frozen321
speaker agent using a descriptive-only/object-centric common RG and report its ZSCT accuracy (see322
Algorithm 3).Table 1 shows AccZSCT being around chance-level (50%), thus the meta-RL agents fail323
to coordinate together, despite the simplicity of the setting, meaning that learning CLBs from scratch324
is currently out-of-reach to state-of-the-art MARL agents, and therefore show the importance of our325
benchmark. As the linguistic compositionality measures are very low compared to the PS agent, and326
since the chance-leveled AccEoA implies that the emerging languages are not easy to learn, it leads us327
to think that the poor MARL performance is due to the lack of compositional language emergence.328
4.2 Single-Agent Listener-Focused RL Context329
Seeing that the multi-agent benchmark is out of reach to state-of-the-art cooperative MARL agents,330
we investigate a simplification along two axises. Firstly, we simplify to a single-agent RL problem331
by instantiating a fixed, rule-based agent as the speaker, which should remove any issues related332
to agents learning in parallel to coordinate. Secondly, we use the Posdis-Speaker agent, which333
should remove any issues related to the emergence of assumed-necessary compositional languages,334
which corresponds to the constructivity aspects of CLBs. These simplifications allow us to focus our335
investigation on the receptivity aspects of CLBs, which relates to the ability from the listener agent to336
acquire and leverage a newly-encountered compositional language at each episode.337
4.2.1 Symbol-Manipulation Induction Biases are Valuable338
Table 2: Meta-RG ZSCT accuracies (% ± s.t.d.).
LSTM ESBN DCEM
AccZSCT ↑ 86.0 ± 0.1 89 .4 ± 2.8 81 .9 ± 0.6
Firstly, in the simplest setting ofO = 1 and S =339
1, we hypothesise that symbol-manipulation bi-340
ases, such as efficient memory-addressing mech-341
anism (e.g. attention) and greater algorithm-342
learning abilities (e.g. explicit memory), should improve performance, and propose to test the343
Emergent Symbol Binding Network (ESBN) [Webb et al., 2020], the Dual-Coding Episodic Memory344
(DCEM) [Hill et al., 2020] and compare to baseline LSTM [Hochreiter and Schmidhuber, 1997].345
Evaluation & Results.We report in Table 2 the final ZSCT accuracies in the setting ofNdim = 3,346
Vmin = 2 , Vmax = 3 , with a sampling budget of 10M observations and 3 random seeds per347
architecture. LSTM performing better than DCEM is presumably due to the difficulty of the latter348
in learning to use its complex memory scheme (preliminary experiments involving a Differentiable349
Neural Computer (DNC - Graves et al. [2016]), on which the DCEM is built, show it struggling to350
learn to use its memory compared to LSTM - cf Appendix D.3). On the other hand, we interpret351
the best performance of the ESBN as being due to it being built over the LSTM, thus allowing its352
complex memory scheme to be bypassed until it becomes useful. We validate our hypothesis but353
carry on experimenting with the simpler LSTM model in order to facilitate analysis.354
4.3 Receptivity Aspects of CLBs Can Be Learned Sub-Optimally355
Table 3: Meta-RG ZSCT accuracies (% ± s.t.d.).
Shots
Samples S = 1 S = 2 S = 4
O = 1 62 .2 ± 3.7 73 .5 ± 2.4 75 .0 ± 2.3
O = 4 62 .8 ± 0.8 62 .6 ± 1.7 60 .2 ± 2.2
O = 16 64 .9 ± 1.7 62 .0 ± 2.0 61 .8 ± 2.1
Hypotheses. The SCS representation instanti-356
ates a BP even when O = 1 (cf. Appendix D.1),357
and we suppose that when O increases the BP’s358
complexity increases.Thus, it would stand to359
reason to expect performance to decrease when360
O increases (Hyp. 1). On the other hand, we361
would expect that increasing S would provide362
the learning agent with a denser sampling (in order to fulfill Chaa-RSC (ii)) , and thus performance363
is expected to increase as S increases (Hyp. 2). Indeed, increasing S amounts to giving more364
opportunities for the agents to estimate each Gaussian, thus relaxing the instantiated BP’s complexity.365
Evaluation & Results.We report in table 3 ZSCT accuracies on LSTM-based models (6 random366
seeds per settings) with Ndim = 3 and Vmin = 2, Vmax = 5. The chance threshold is 50%. When367
8

S = 1, increasing O is surprisingly correlated with non-significant increases in performance/sys-368
tematicity. On the otherhand, when S >1, accuracy distributions stay similar or decrease while O369
increases. Thus, overall, Hyp. 1 tends to be validated. Regarding Hyp. 2, when O = 1, increasing370
S (and with it the density of the sampling of the input space, i.e. Chaa-RSC (ii)) correlates with371
increases in systematicity. Thus, despite the difference of settings between common RG, in Chaabouni372
et al. [2020], and meta-RG here, we retrieve a similar result that Chaa-RSC promotes systematicity.373
On the other hand, our results show a statistically significant distinction between BPs of complexity374
associated with O >1 and those associated with O = 1. Indeed, when O >1, our results contradict375
Hyp.2 since accuracy distributions remain the same or decrease when S increases. Acknowledging376
the LSTMs’ notorious difficulty with integrating/binding information from past to present inputs377
over long dependencies, we explain these results based on the fact that increasing S also increases378
the length of each RL episode, thus the ‘algorithm’ learned by LSTM-based agents might fail to379
adequately estimate Gaussian kernel densities associated with each component value.380
5 Discussion381
Compositional Behaviours vs CLBs.The learning of compositional behaviours (CBs) is one of382
the central study in language grounding with benchmarks like SCAN [Lake and Baroni, 2018] and383
gSCAN [Ruis et al., 2020], as well as in the subfield of Emergent Communication (see Brandizzi384
[2023], Boldt and Mortensen [2023] for reviews), but none investigates nor allow testing for CLBs.385
Thus, our benchmark aims to fill in this gap. Without making the nuance, Lake [2019] and Lake386
and Baroni [2023] actually use CLBs a training paradigm, where a meta-learning extension of the387
sequence-to-sequence learning setting (i.e. CLB training) is shown to enable human-like systematic388
CBs. Contrary to our work, they evaluate AI’s abilities towards SCAN-specific CBs after SCAN-389
specific CLBs training. Given the demonstrated potential of CLBs, we leverage our proposed390
Meta-RG framework to propose a domain-agnostic CLB-focused benchmark for evaluation of CLBs391
abilities themselves, in order to address novel research questions around CLBs.392
Symbolic Behaviours & Binding Problem.Following Santoro et al. [2021]’s definition of symbolic393
behaviours, our benchmark is the first specifically-principled benchmark to evaluate systematically394
artificial agents’s abilities towards any symbolic behaviours. Similarly, while most challenging395
benchmark instantiates a version of the BP, as described by Greff et al. [2020], there is currently396
no principled benchmark that specifically investigates whether BP can be solved by artificial agents.397
Thus, not only does our benchmark fill that other gap, but it also instantiate a domain-agnostic version398
of the BP, which is critical in order to ascertain the external validity of conclusions that may be drawn399
from it. Indeed, domain-agnosticity guards us against confounders that could make the task solvable400
without fully solving the BP, e.g. by gaming some domain-specific aspects [Chollet, 2019].401
Limitations. Our experiments only evaluated state-of-the-art RL models and algorithms in the402
simplest configuration of our benchmark, and we leave it to future works to investigate more complex403
configurations and evaluate other classes of models, such as neuro-symbolic models [Yu et al., 2023]404
or large language models [Brown et al., 2020].405
In summary, we have proposed a novel benchmark to investigate artificial agents abilities at learning406
CLBs, by casting the problem of learning CLBs as a meta-reinforcement learning problem. It uses407
our proposed extension to RGs, entitled Meta-Referential Games, which contains an instantiation of a408
domain-agnostic BP. We provided baseline results for both the multi-agent tasks and the single-agent409
listener-focused tasks of learning CLBs in the context of our proposed benchmark. Our analysis410
of the behaviours in the multi-agent context highlighted the complexity for the speaker agent to411
invent a compositional language. But, when the language is already compositional, then a learning412
listener is able to acquire it and coordinate, albeit sub-optimally, with a rule-based speaker, in some413
of the simplest settings of our benchmark. Symbol-manipulation induction biases were found to be414
valuable, but, overall, our results show that our proposed benchmark is currently out of reach for415
current state-of-the-art artificial agents, and we hope it will spur the research community towards416
developing more capable artificial agents.417
9

Appendix E546
(d) Have you read the ethics review guidelines and ensured that your paper conforms to547
them? [Yes]548
2. If you are including theoretical results...549
(a) Did you state the full set of assumptions of all theoretical results? [N/A]550
(b) Did you include complete proofs of all theoretical results? [N/A]551
3. If you ran experiments (e.g. for benchmarks)...552
(a) Did you include the code, data, and instructions needed to reproduce the main experi-553
mental results (either in the supplemental material or as a URL)? [Yes]554
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they555
were chosen)? [Yes] Training details can be found in Section 4 and Appendix B,556
and hyperparameters have been selected using the Hyperparemeter Sweep feature of557
Weights&Biases[Biewald, 2020].558
(c) Did you report error bars (e.g., with respect to the random seed after running experi-559
ments multiple times)? [Yes] We reported standard deviation as % ± s.t.d. in tables or560
as shaded area in learning curve graphs.561
(d) Did you include the total amount of compute and the type of resources used (e.g., type562
of GPUs, internal cluster, or cloud provider)? [Yes] We detailed minimum compute563
requirements in Appendix B.564
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...565
(a) If your work uses existing assets, did you cite the creators? [N/A]566
(b) Did you mention the license of the assets? [N/A]567
(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]568
569
(d) Did you discuss whether and how consent was obtained from people whose data you’re570
using/curating? [N/A]571
(e) Did you discuss whether the data you are using/curating contains personally identifiable572
information or offensive content? [N/A]573
5. If you used crowdsourcing or conducted research with human subjects...574
(a) Did you include the full text of instructions given to participants and screenshots, if575
applicable? [N/A]576
(b) Did you describe any potential participant risks, with links to Institutional Review577
Board (IRB) approvals, if applicable? [N/A]578
(c) Did you include the estimated hourly wage paid to participants and the total amount579
spent on participant compensation? [N/A]580
13

A On Algorithmic Details of Meta-Referential Games581
In this section, we detail algorithmically how Meta-Referential Games differ from common RGs. We582
start by presenting in Algorithm 4 an overview of the common RGs, taking place inside a common583
supervised learning loop, and we highlight the following:584
(i) preparation of the data on which the referential game is played (highlighted in green),585
(ii) elements pertaining to playing a RG (highlighted in blue),586
(iii) elements pertaining to the supervised learning loop(highlighted in purple).587
Helper functions are detailed in Algorithm 1, 2 and 3. Next, we can now show in greater and588
contrastive details the Meta-Referential Game algorithm in Algorithm 5, where we highlight the589
following:590
(i) preparation of the data on which the referential game is played (highlighted in green),591
(ii) elements pertaining to playing a RG (highlighted in blue),592
(iii) elements pertaining to the meta-learning loop(highlighted in purple).593
(iv) elements pertaining to setup of a Meta-Referential Game (highlighted in red).594
Algorithm 1:Helper function : DataPrep
Given :
• a target stimuli s0,
• a dataset of stimuli Dataset,
• O : Number of Object-Centric samples in each Target Distribution over stimuli T D(·).
• K : Number of distractor stimuli to provide to the listener agent.
• FullObs : Boolean defining whether the speaker agent has full (or partial) observation.
• DescrRatio : Descriptive ratio in the range [0, 1] defining how often the listener agent is
observing the same semantic as the speaker agent.
1 s′
0, DTarget ← s0, 0;
2 if random(0, 1) > DescrRatiothen
3 s′
0 ∼ Dataset − T D(s0); ; /* Exclude target stimulus from listener’s
observation ... */
4 DTarget ← K + 1; ; /* ... and expect it to decide accordingly. */
5 end
6 else ifO >1 then
7 Sample an Object-Centric distractor s′
0 ∼ T D(s0);
8 end
9 Sample K distractor stimuli from Dataset − T D(s0): (si)i∈[1,K] ∼ Dataset − T D(s0);
10 ObsSpeaker ← {s0}; if FullObs then
11 ObsSpeaker ← {s0} ∪ {si|∀i ∈ [1, K]};
12 end
13 ObsListener ← {s′
0} ∪ {si|∀i ∈ [1, K]};
/* Shuffle listener observations and update index of target decision:
*/
14 ObsListener, DTarget ← Shuffle (ObsListener, DTarget );
Output : ObsSpeaker, ObsListener, DTarget ;
14

Algorithm 2:Helper function : MetaRGDatasetPreparation
Given :
• V : V ocabulary (finite set of tokens available),
• Ndim : Number of attribute/factor dimensions in the symbolic spaces,
• Vmin : Minimum number of possible values on each attribute/factor dimensions in the
symbolic spaces,
• Vmax : Maximum number of possible values on each attribute/factor dimensions in the
symbolic spaces,
1 Initialise random permutation of vocabulary: V ′ ← RandomP erm(V )
2 Sample semantic structure: (d(i))i∈[1,Ndim] ∼ U(Vmin; Vmax)Ndim ;
3 Generate symbolic space/dataset D((d(i))i∈[1,Ndim]);
4 Split dataset into supporting set Dsupport and querying set Dquery (((d(i))i∈[1,Ndim]) is omitted for
readability);
Output : V ′, D((d(i))i∈[1,Ndim]), Dsupport, Dquery;
Algorithm 3:Helper function : PlayRG
Given :
• Speaker and Listener agents,
• Set of speaker observations ObsSpeaker,
• Set of listener observations ObsListener,
• N : Number of communication rounds to play,
• L : Maximum length of each message,
• V : V ocabulary (finite set of tokens available),
1 Compute message MS = Speaker(ObsSpeaker|∅);
2 Initialise Communication Channel History: CommH ← [MS];
3 for round = 0, Ndo
4 Compute Listener’s reply ML
round, _ = Listener(ObsListener|CommH);
5 CommH ← CommH + [ML
round];
6 Compute Speaker’s reply MS
round = Speaker(ObsSpeaker|CommH);
7 CommH ← CommH + [MS
round];
8 end
9 Compute listener decision _, DL = Listener(ObsListener|CommH);
Output : Listener’s decision DL, Communication Channel History CommH;
15

Algorithm 4:Common Referential Game inside a Common Supervised Learning Loop
Given :
• a dataset of stimuli Dataset,
• a set of hyperparameters defining the RG:
– O : Number of Object-Centric samples in each Target Distribution over stimuli T D(·).
– N : Number of communication rounds to play.
– L : Maximum length of each message.
– V : V ocabulary (finite set of tokens available).
– K : Number of distractor stimuli to provide to the listener agent.
– FullObs : Boolean defining whether the speaker agent has full (or partial) observation.
– DescrRatio : Descriptive ratio in the range [0, 1] defining how often the listener agent
is observing the same semantic as the speaker agent.
– L : Loss function to use in the agents update.
Initialize :
• Speaker (·) and Listener(·) agents.
1 Systematically split Dataset into training and testing dataset, Dtrain and Dtest;
2 for epoch = 1, Nepoch do
3 for target stimulus s0 ∈ Dtrain do
/* Preparation of observations and target decision: */
4 ObsSpeaker, ObsListener, DTarget ← DataP rep(Dataset, s0, O, K,FullObs, DescrRatio)
/* Play Referential Game: */
5 DL, _ = PlayRG(Speaker, Listener, ObsSpeaker, ObsListener, N, L, V);
/* Supervised Learning Parameters Update on Training Stimulus Only:
*/
6 Update both speaker and listener agents’ parameters using the loss L(DTarget , DL);
7 end
8 Initialise ZSCT accuracy: AccZSCT ← 0;
9 for target stimulus s0 ∈ Dtest do
/* Preparation of observations and target decision: */
10 ObsSpeaker, ObsListener, DTarget ← DataP rep(Dataset, s0, O, K,FullObs, DescrRatio)
/* Play Referential Game: */
11 DL, _ = PlayRG(Speaker, Listener, ObsSpeaker, ObsListener, N, L, V);
/* Update ZSCT Accuracy: */
12 AccZSCT ← Update(AccZSCT, DTarget , DL);
13 end
14 end
595
16

Algorithm 5:Meta-Referential Game inside a Meta-Learning Loop
Given :
• Nepisode, Ndim : Number of episodes, and number of attribute/factor dimensions,
• S : Minimum number of Shots over which each possible value on each attribute/factor
dimension ought to be observed by the agents (as part of a target stimulus).
• Vmin, Vmax : Minimum and maximum number of possible values on each attribute/factor
dimensions in the symbolic spaces,
• T SS(D, S, S) : Target stimulus sampling function which samples from dataset D, given a
set of previously sampled stimuli S, while maximising the likelihood that each possible
value on each attribute/factor dimension are sampled at least S times.
• a set of hyperparameters defining the RG:
– O : Number of Object-Centric samples in each Target Distribution over stimuli T D(·).
– N : Number of communication rounds to play.
– L : Maximum length of each message.
– V : V ocabulary (finite set of tokens available).
– K : Number of distractor stimuli to provide to the listener agent.
– FullObs : Boolean defining whether the speaker agent has full (or partial) observation.
– DescrRatio : Descriptive ratio in the range [0, 1] defining how often the listener agent
is observing the same semantic as the speaker agent.
Initialize :
• Speaker (·) and Listener(·) agents.
1 for episode = 1, Nepisode do
/* Preparation of the symbolic space/dataset: */
2 V ′, Depisode, Dsupport
episode, Dquery
episode ← MetaRGDatasetPreparation (V, Ndim, Vmin, Vmax);
3 Initialise set of sampled supporting stimuli: Ssupport ← ∅;
4 repeat
5 Sample training-purposed target stimulus si
0 ∼ T SS(Dsupport
episode, Ssupport, S)
6 Ssupport ← Ssupport ∪ {si
0}; i ← i + 1;
7 until all values on each attribute/factor dimension have been instantiated at least S times;
8 Initialise RG index: i ← 0;
/* Supporting Phase: */
9 for target stimulus si
0 ∈ Ssupport do
10 Obsi
Speaker, Obsi
Listener, DTarget
i ← DataP rep(Dsupport
episode, si
0, O, K,FullObs, DescrRatio);
11 DL
i , CommHi = PlayRG(Speaker, Listener, Obsi
Speaker, Obsi
Listener, N, L, V′);
12 _, _ = Listener(Obsi
Speaker |CommHi) ; /* Listener-Feedback Step */
13 end
/* Querying/ZSCT Phase: */
14 Initialise ZSCT accuracy: AccZSCT ← 0;
15 for target stimulus si
0 ∈ Dquery
episode do
16 Obsi
Speaker, Obsi
Listener, DTarget
i ← DataP rep(Depisode, si
0, O, K,FullObs, DescrRatio);
17 DL
i , CommHi = PlayRG(Speaker, Listener, Obsi
Speaker, Obsi
Listener, N, L, V′);
18 _, _ = Listener(Obsi
Speaker |CommHi) ; /* Listener-Feedback Step */
/* Update ZSCT Accuracy: */
19 AccZSCT ← Update(AccZSCT, DTarget
i , DL
i ); i ← i + 1;
20 end
/* Meta-Learning Parameters Update on Whole Episode: */
21 Update both agents using rewards Ri =



1 if DTarget
i == DL
i
0 otherwise, during supporting phase
−2 otherwise, during querying phase
;
22 end
596
17

Figure 4: Top: visualisation on each column of the messages sent by the posdis-compositional
rule-based speaker agent over the course of the episode presented in Figure 3. Colours are encoding
the information of the token index, as a visual cue. Bottom: OHE/MHE and SCS representations
of example latent stimuli for two differently-structured symbolic spaces with Ndim = 3, i.e. on the
left for d(0) = 4, d(1) = 2, d(2) = 3, and on the right for d(0) = 3, d(1) = 3, d(2) = 3. Note the
shape invariance property of the SCS representation, as its shape remains unchanged by the change
in semantic structure of the symbolic space, on the contrary to the OHE/MHE representations.
B Agent architecture & training597
The baseline RL agents that we consider use a 3-layer fully-connected network with 512, 256, and598
finally 128 hidden units, with ReLU activations, with the stimulus being fed as input. The output599
is then concatenated with the message coming from the other agent in a OHE/MHE representation,600
mainly, as well as all other information necessary for the agent to identify the current step, i.e. the601
previous reward value (either +1 and 0 during the training phase or +1 and −2 during testing phase),602
its previous action in one-hot encoding, an OHE/MHE-represented index of the communication603
round (out of N possible values), an OHE/MHE-represented index of the agent’s role (speaker or604
listener) in the current game, an OHE/MHE-represented index of the current phase (either ’training’605
or ’testing’), an OHE/MHE representation of the previous RG’s result (either success or failure), the606
previous RG’s reward, and an OHE/MHE mask over the action space, clarifying which actions are607
available to the agent in the current step. The resulting concatenated vector is processed by another608
3-layer fully-connected network with 512, 256, and 256 hidden units, and ReLU activations, and then609
fed to the core memory module, which is here a 2-layers LSTM [Hochreiter and Schmidhuber, 1997]610
with 256 and 128 hidden units, which feeds into the advantage and value heads of a 1-layer dueling611
network [Wang et al., 2016].612
Table 5 highlights the hyperparameters used for the learning agent architecture and the learning613
algorithm, R2D2[Kapturowski et al., 2018]. More details can be found, for reproducibility purposes,614
in our open-source implementation at HIDDEN_FOR_REVIEW_PURPOSE.615
Training was performed for each run on 1 NVIDIA GTX1080 Ti, and the average amount of training616
time for a run is 18 hours for LSTM-based models, 40 hours for ESBN-based models, and 52 hours617
for DCEM-based models.618
18

B.1 ESBN & DCEM619
The ESBN-based and DCEM-based models that we consider have the same architectures and620
parameters than in their respective original work from Webb et al. [2020] and Hill et al. [2020], with621
the exception of the stimuli encoding networks, which are similar to the LSTM-based model.622
B.2 Rule-based speaker agent623
The rule-based speaker agents used in the single-agent task, where only the listener agent is a624
learning agent, speaks a compositional language in the sense of the posdis metric [Chaabouni et al.,625
2020], as presented in Table 4 for Ndim = 3, a maximum sentence length of L = 4, and vocabulary626
size |V | >= maxid(i) = 5, assuming a semantical space such that ∀i ∈ [1, 3], d(i) = 5.627
C Cheating language628 Table 4: Examples of the latent
stimulus to language utterance map-
ping of the posdis-compositional
rule-based speaker agent. Note that
token 0 is the EoS token.
Latent Dims Comp. Language
#1 #2 #3 Tokens
0 1 2 1, 2, 3, 0
1 3 4 2, 4, 5, 0
2 5 0 3, 6, 1, 0
3 1 2 4, 2, 3, 0
4 3 4 5, 4, 5, 0
The agents can develop a cheating language, cheating in the629
sense that it could be episode/task-invariant (and thus semantic630
structure invariant). This emerging cheating language would631
encode the continuous values of the SCS representation like an632
analog-to-digital converter would, by mapping a fine-enough633
partition of the[−1, +1] range onto the vocabulary in a bijective634
fashion.635
For instance, for a vocabulary size ∥V ∥ = 10, each symbol can636
be unequivocally mapped onto 2
10 -th increments over [−1, +1],637
and, by communicating Ndim symbols (assuming Ndim ≤638
L), the speaker agents can communicate to the listener the639
(digitized) continuous value on each dimension i of the SCS-represented stimulus. If maxjd(j) ≤640
∥V ∥ then the cheating language is expressive-enough for the speaker agent to digitize all possible641
stimulus without solving the binding problem, i.e. without inferring the semantic structure. Similarly,642
it is expressive-enough for the listener agent to convert the spoken utterances to continuous/analog-643
like values over the [−1, +1] range, thus enabling the listener agent to skirt the binding problem644
when trying to discriminate the target stimulus from the different stimuli it observes.645
D Further experiments:646
D.1 On the BP instantiated by the SCS representation647
Hypothesis. The SCS representation differs from the OHE/MHE one primarily in terms of the648
binding problem [Greff et al., 2020] that the former instantiates while the latter does not. Indeed,649
the semantic structure can only be inferred after observing multiple SCS-represented stimuli. We650
hypothesised that it is via the dynamic binding of information extracted from each observations that651
an estimation of a density distribution over each dimension i’s [−1, +1] range can be performed.652
And, estimating such density distribution is tantamount to estimating the number of likely gaussian653
distributions that partition each [−1, +1] range.654
Evaluation. Towards highlighting that there is a binding problem taking place, we show results of655
baseline RL agents (similar to main experiments in Section 4) evaluated on a simple single-agent656
recall task. The Recall task structure borrows from few-shot learning tasks as it presents over 2 shots657
all the stimuli of the instantiated symbolic space (not to be confused with the case for Meta-RG658
where all the latent/factor dimensions’ values are being presented over S shots – Meta-RGs do not659
necessarily sample the whole instantiated symbolic space at each episode, but the Recall task does).660
Each shot consists of a series of recall games, one for each stimulus that can be sampled from an661
Ndim = 3-dimensioned symbolic space. The semantic structure (d(i))i∈[1;Ndim] of the symbolic662
space is randomly sampled at the beginning of each episode, i.e. d(i) ∼ U(2; 5), where U(2; 5)is the663
19

uniform discrete distribution over the integers in [2; 5], and the number of object-centric samples is664
O = 1, in order to remove any confounder from object-centrism.665
Each recall game consists of two steps: in the first step, a stimulus is presented to the RL agent, and666
only a no-operation (NO-OP) action is made available, while, on the second step, the agent is asked667
to infer/recall the discrete l(i) latent value(as opposed to the representation of it that it observed,668
either in the SCS or OHE/MHE form) that the previously-presented stimulus had instantiated, on669
a given i-th dimension, where value i for the current game is uniformly sampled from U(1; Ndim)670
at the beginning of each game. The value of i is communicated to the agent via the observation671
on this second step of different stimulus that in the first step: it is a zeroed out stimulus with the672
exception of a 1 on the i-th dimension on which the inference/recall must be performed when using673
SCS representation, or over all the OHE/MHE dimensions that can encode a value for the i-th latent674
factor/attribute when using the OHE/MHE representation. On the second step, the agent’s available675
action space now consists of discrete actions over the range [1; maxjd(j)], where maxjd(j) is a676
hyperparameter of the task representing the maximum number of latent values for any latent/factor677
dimension. In our experiments, maxjd(j) = 5 . While the agent is rewarded at each game for678
recalling correctly, we only focus on the performance over the games of the second shot, i.e. on the679
games where the agent has theoretically received enough information to infer the density distribution680
over each dimension i’s [−1, +1] range. Indeed, observing the whole symbolic space once (on the681
first shot) is sufficient (albeit not necessary, specifically in the case of the OHE/MHE representation).682
Figure 5: 5-ways 2-shots accuracies
on the Recall task with different stim-
ulus representation (OHE:blue ; SCS;
orange).
Results. Figure 5 details the recall accuracy over all the683
games of the second shot of our baseline RL agent through-684
out learning. There is a large gap of asymptotic perfor-685
mance depending on whether the Recall task is evaluated686
using OHE/MHE or SCS representations. We attribute687
the poor performance in the SCS context to the instantia-688
tion of a BP. We note again that during those experiments689
the number of object-centric samples was kept at O = 1,690
thus emphasising that the BP is solely depending on the691
use of the SCS representation and does not require object-692
centrism.693
D.2 On the ideally-disentangled-ness of the SCS representation694
In this section, we verify our hypothesis that the SCS representation yields ideally-disentangled695
stimuli. We report on the FactorV AE ScoreKim and Mnih [2018], the Mutual Information Gap696
(MIG) Chen et al. [2018], and the Modularity ScoreRidgeway and Mozer [2018] as they have697
been shown to be part of the metrics that correlate the least among each other [Locatello et al.,698
2020], thus representing different desiderata/definitions for disentanglement. We report on the699
Ndim = 3-dimensioned symbolic spaces with ∀j, d(j) = 5 and O = 5. The measurements are700
of 100.0%, 94.8, and 98.9% for, respectivily, the FactorV AE Score, the MIG, and the Modularity701
Score, thus validating our design hypothesis about the SCS representation. We remark that the MIG702
and Modularity Score are sensitive to the number of object-centric samples O, which can be seen703
decreasing the measurements as low as 64.4% and 66.6% for O = 1. The FactorV AE Score is not704
affected, possibly due to its reliance on a deterministic classifier.705
D.3 Auxiliary Reconstruction Loss706
In the following, we investigate and compare the performance when using an LSTM [Hochreiter707
and Schmidhuber, 1997] or a Differentiable Neural Computer (DNC) [Graves et al., 2016] as core708
memory module, with or without the auxiliary reconstruction loss inspired from Hill et al. [2020].709
In the case of the LSTM, the prediction network of the reconstruction loss takes as input the LSTM710
hidden states, while in the case of the DNC, the input is the memory. Figure 6b shows the stimulus711
reconstruction accuracies for both architectures, highlighting a greater data-efficiency (and resulting712
20

asymptotic performance in the current observation budget) of the LSTM-based architecture, compared713
to the DNC-based one.714
Figure 6a shows the 4-ways (3 distractors descriptive meta-RGs) ZSCT accuracies of the different715
agents throughout learning. The ZSCT accuracy is the accuracy over querying-/testing-purpose716
stimuli only, after the agent has observed for two consecutive times (i.e. S = 2) the supportive717
training-purpose stimuli for the current episode. The DNC-based architecture has difficulty learning718
how to use its memory, even with the use of the auxiliary reconstruction loss, and therefore it utterly719
fails to reach better-than-chance ZSCT accuracies. On the otherhand, the LSTM-based architecture is720
fairly successful on the auxiliary reconstruction task, but it is not sufficient for training on the main721
task to really take-off. As expected from the fact that the benchmark instantiates a binding problem722
that requires relational responding, our results hint at the fact that the ability to use memory towards723
deriving valuable relations between stimuli seen at different time-steps is primordial. Indeed, only the724
agent that has the ability to use its memory element towards recalling stimuli starts to perform at a725
better-than-chance level. Thus, the auxiliary reconstruction loss is an important element to drive some726
success on the task, but it is also clearly not sufficient, and the rather poor results that we achieved727
using these baseline agents indicates that new inductive biases must be investigated to be able to728
solve the problem posed in our proposed benchmark.729
E Broader impact730
No technology is safe from being used for malicious purposes, which equally applies to our research.731
However, aiming to develop artificial agents that relies on the same symbolic behaviours and the same732
social assumptions (e.g. using CLBs) than human beings is aiming to reduce misunderstanding be-733
tween human and machines. Thus, the current work is targeting benevolent applications. Subsequent734
works around the benchmark that we propose are prompted to focus on emerging protocols in general735
(not just posdis-compositional languages), while still aiming to provide a better understanding of736
artificial agent’s symbolic behaviour biases and differences, especially when compared to human737
beings, thus aiming to guard against possible misunderstandings and misaligned behaviours. The738
current state of this work does not allow discussion of potential negative societal impact.739
(a)
 (b)
Figure 6: (a): 4-ways (3 distractors) zero-shot compositional test accuracies of different architectures.
5 seeds for architectures with DNC and LSTM, and 2 seeds for runs with DNC+Rec and LSTM+Rec,
where the auxiliary reconstruction loss is used. (b): Stimulus reconstruction accuracies for the
architectures augmented with the auxiliary reconstruction task. Accuracies are computed on binary
values corresponding to each stimulus’ latent dimension’s reconstructed value being close enough to
the ground truth value, with a threshold of 0.05 on each dimension, which correspond to a deviation
tolerance of 2.5% since the range in which SCS stimuli are instantiated is [−1, 1].
21

Table 5: Hyper-parameters values used in R2D2, with LSTM or DNC as the core memory module.
All missing parameters follow the ones in Ape-X [Horgan et al., 2018].
R2D2
Number of actors 32
Actor parameter update interval 1 environment step
Sequence unroll length 20
Sequence length overlap 10
Sequence burn-in length 10
N-steps return 3
Replay buffer size 5 × 104 observations
Priority exponent 0.9
Importance sampling exponent 0.6
Discount γ 0.997
Minibatch size 32
Optimizer Adam [Kingma and Ba, 2014]
Optimizer settings learning rate = 6.25 × 10−5, ϵ = 10−12
Target network update interval 2500 updates
Value function rescaling None
Core Memory Module
LSTM [Hochreiter and Schmidhuber, 1997] DNC [Graves et al., 2016]
Number of layers 2 LSTM-controller settings 2 hidden layers of size 128
Hidden layer size 256, 128 Memory settings 128 slots of size 32
Activation function ReLU Read/write heads 2 reading ; 1 writing
22