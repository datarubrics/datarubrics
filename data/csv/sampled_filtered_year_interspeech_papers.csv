id,url_link,html_url,title,category,year,abstract,score
bayerl22_interspeech,https://www.isca-archive.org/interspeech_2022/bayerl22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/bayerl22_interspeech.html,What can Speech and Language Tell us About the Working Alliance in Psychotherapy,Speech and Language in Health: From Remote Monitoring to Medical Conversations II,2022,"We are interested in the problem of conversational analysis and its application to the health domain. Cognitive Behavioral Therapy is a structured approach in psychotherapy, allowing the therapist to help the patient to identify and modify the malicious thoughts, behavior, or actions. This cooperative effort can be evaluated using the Working Alliance Inventory Observer-rated Shortened - a 12 items inventory covering task, goal, and relationship - which has a relevant influence on therapeutic outcomes. In this work, we investigate the relation between this alliance inventory and the spoken conversations (sessions) between the patient and the psychotherapist. We have delivered eight weeks of e-therapy, collected their audio and video call sessions and manually transcribed them. The spoken conversations have been annotated and evaluated with WAI ratings by professional therapists. We have investigated speech and language features and their association with WAI items. The feature types include turn dynamics, lexical entrainment and conversational descriptors extracted from the speech and language signals. Our findings provide strong evidence that a subset of these features are strong indicators of working alliance. To the best of our knowledge, this is the first and a novel study to exploit speech and language for characterising working alliance.",True
shin22_interspeech,https://www.isca-archive.org/interspeech_2022/shin22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/shin22_interspeech.html,Learning Audio-Text Agreement for Open-vocabulary Keyword Spotting,Spoken Term Detection and Voice Search,2022,"In this paper, we propose a novel end-to-end user-defined keyword spotting method that utilizes linguistically corresponding patterns between speech and text sequences. Unlike previous approaches requiring speech keyword enrollment, our method compares input queries with an enrolled text keyword sequence. To place the audio and text representations within a common latent space, we adopt an attention-based cross-modal matching approach that is trained in an end-to-end manner with monotonic matching loss and keyword classification loss. We also utilize a de-noising loss for the acoustic embedding network to improve robustness in noisy environments. Additionally, we introduce the LibriPhrase dataset, a new short-phrase dataset based on LibriSpeech for efficiently training keyword spotting models. Our proposed method achieves competitive results on various evaluation sets compared to other single-modal and cross-modal baselines.",True
ivanko22_interspeech,https://www.isca-archive.org/interspeech_2022/ivanko22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/ivanko22_interspeech.html,DAVIS: Driverâs Audio-Visual Speech recognition,Show and Tell I(VR),2022,"DAVIS is a driverâs audio-visual assistive system intended toÂ  improve accuracy and robustness of speech recognition of theÂ  most frequent driversâ requests in natural driving conditions.Â  Since speech recognition in driving condition is highlyÂ  challenging due to acoustic noises, active head turns, poseÂ  variation, distance to recording devices, lightning conditions,Â  etc. We rely on multimodal information and use both automaticÂ  lip-reading system for visual stream and ASR for audio stream processing. We have trained audio and video models on ownÂ  RUSAVIC dataset containing in-the-wild audio and video recordings of 20 drivers. The recognition application comprisesÂ  a graphical user interface and modules for audio and videoÂ  signal acquisition, analysis, and recognition. The obtainedÂ  results demonstrate rather high performance of DAVIS and alsoÂ  the fundamental possibility of recognizing speech commandsÂ  by using video modality, even in such difficult naturalÂ  conditions as driving.Â",True
wang22aa_interspeech,https://www.isca-archive.org/interspeech_2022/wang22aa_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/wang22aa_interspeech.html,Active Few-Shot Learning for Sound Event Detection,Acoustic Event Detection and Classification,2022,"Few-shot learning has shown promising results in sound event detection where the model can learn to recognize novel classes assuming a few labeled examples (typically five) are available at inference time. Most research studies simulate this process by sampling support examples randomly and uniformly from all test data with the target class label. However, in many real-world scenarios, users might not even have five examples at hand or these examples may be from a limited context and not representative, resulting in model performance lower than expected. In this work, we relax these assumptions, and to recover model performance, we propose to use active learning techniques to efficiently sample additional informative support examples at inference time. We developed a novel dataset simulating the long-term temporal characteristics of sound events in real-world environmental soundscapes. Then we ran a series of experiments with this dataset to explore the modeling and sampling choices that arise when combining few-shot learning and active learning, including different training schemes, sampling strategies, models, and temporal windows in sampling.",True
audibert22_interspeech,https://www.isca-archive.org/interspeech_2022/audibert22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/audibert22_interspeech.html,Intra-speaker phonetic variation in read speech: comparison with inter-speaker variability in a controlled population,Speech Emotion Recognition II,2022,"Our knowledge of speech is historically built on data averaged across speakers or comparing different speakers. We therefore know little about the variability of speech produced by the same speaker: to what extent does it vary from one repetition to another and on what dimensions? In this study, we document the stability of speech and voice characteristics in 9 French speakers, on the reading of two texts recorded over ten sessions on different days over a two-month period. 21 features related to temporal, spectral, f0 and harmonicity aspects as well as their modulation between consecutive chunks are studied. The stability of these features between sessions is evaluated in comparison with their variability between speakers. Results show that short-term variability of energy in the 0-1kHz band, mean F0 and the slope of the LTAS vary the most between sessions for a given speaker, and are also among the speech and voice features that vary the most between speakers in this small cohort, while modulation features between consecutive chunks remain more stable across sessions.",True
dang22_interspeech,https://www.isca-archive.org/interspeech_2022/dang22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/dang22_interspeech.html,Exploring Semi-supervised Learning for Audio-based COVID-19 Detection using FixMatch,Speech and Language in Health: From Remote Monitoring to Medical Conversations II,2022,"While there has been recent success in audio-based COVID-19 detection, challenges still exist in developing more reliable and generalised models due to the limited amount of high quality labelled audio recordings. With a substantial amount of unlabelled audio recordings available, exploring semi-supervised learning (SSL) may benefit COVID-19 detection by incorporating this extra data. In this paper, we propose a SSL framework which adjusted FixMatch, one of the most advanced SSL approaches, to audio signals and explored its effectiveness in COVID-19 detection. The proposed framework is validated with a crowd-sourced audio database collected from our app, and showed superior performance over supervised models with a maximum of 7.2\\% relative improvement. Furthermore, we demonstrated that the proposed framework significantly benefits model development using imbalanced datasets, which is a common challenge in clinical data. It can also improve model generalisation. This potentially paves a new pathway of utilising unlabelled data effectively to build more accurate and reliable COVID-19 detection tools.",True
yang22i_interspeech,https://www.isca-archive.org/interspeech_2022/yang22i_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/yang22i_interspeech.html,Mandarin Lombard Grid: a Lombard-grid-like corpus of Standard Chinese,"Speech Production, Perception and Multimodality",2022,"The Lombard effect is natural, whereby speakers automatically adjust the vocal effort to facilitate speech understanding in noise. Since real-world applications are generally involved in noisy environments, the Lombard effect of highly variable speech features due to changing background noise is one of those challenges to match these real scenarios. Existing Lombard corpora show variations in the background noise level, ranging from 35 to 96 dB sound pressure level (SPL). However, it remains unclear if we need to collect all SPLs to build a comprehensive Lombard corpus. And most existing Lombard corpora are built for English; however, Mandarin and English are different in pronunciation. This paper describes our effort to build the first open-source Lombard corpus of standard Chinese, the Mandarin Lombard Grid. The effort involves three steps: (1) Classify Mandarin Lombard styles according to different background noise levels. (2) Create the corpus containing each style. (3) Analyze Mandarin Lombard effects showing their differences from English. We found three critical Lombard styles ranging from 30 dB to 85 dB-SPL and built the corpus containing the three Lombard styles and one reference plain style. Lombard effect analyses on this corpus showed consistency and some differences from the English Lombard Grid corpus.",True
takeuchi22_interspeech,https://www.isca-archive.org/interspeech_2022/takeuchi22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/takeuchi22_interspeech.html,Introducing Auxiliary Text Query-modifier to Content-based Audio Retrieval,Acoustic scene analysis,2022,"The amount of audio data available on public websites is growing rapidly, and an efficient mechanism for accessing the desired data is necessary. We propose a content-based audio retrieval method that can retrieve a target audio that is similar to but slightly different from the query audio by introducing auxiliary textual information which describes the difference between the query and target audio. While the range of conventional content-based audio retrieval is limited to audio that is similar to the query audio, the proposed method can adjust the retrieval range by adding an embedding of the auxiliary text query-modifier to the embedding of the query sample audio in a shared latent space. To evaluate our method, we built a dataset comprising two different audio clips and the text that describes the difference. The experimental results show that the proposed method retrieves the paired audio more accurately than the baseline. We also confirmed based on visualization that the proposed method obtains the shared latent space in which the audio difference and the corresponding text are represented as similar embedding vectors.",True
ridouane22_interspeech,https://www.isca-archive.org/interspeech_2022/ridouane22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/ridouane22_interspeech.html,Complex sounds and cross-language influence: The case of ejectives in Omani Mehri,Phonetics and Phonology,2022,"Ejective consonants are known to considerably vary both cross-linguistically and within individual languages. This variability is often considered a consequence of the complex articulatory strategies involved in their production. Because they are complex, they might be particularly prone to sound change, especially under cross-language influence. In this study, we consider the production of ejectives in Mehri, a Semitic endangered language spoken in Oman where considerable influence from Arabic is expected. We provide acoustic data from seven speakers producing a list of items contrasting ejective and pulmonic alveolar and velar stops in word-initial (/#â/), word-medial (VâV), and word-final (Vâ#) positions. Different durational and non-durational correlates were examined. The relative importance of these correlates was quantified by the calculation of D-prime values for each. The key empirical finding is that the parameters used to signal ejectivity differ depending mainly on whether the stop is alveolar or velar. Specifically, ejective alveolar stops display characteristics of pharyngealization, similar to Arabic, but velars still maintain attributes of ejectivity in some word positions. We interpret these results as diagnostic of the sound change that is currently in progress, coupled with an ongoing context-dependent neutralization.",True
qin22_interspeech,https://www.isca-archive.org/interspeech_2022/qin22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/qin22_interspeech.html,Cross-Age Speaker Verification: Learning Age-Invariant Speaker Embeddings,Speaker Embedding and Diarization,2022,"Automatic speaker verification has achieved remarkable progress in recent years. However, there is little research on cross-age speaker verification due to insufficient data. In this paper, we mine cross-age test sets based on the VoxCeleb and propose our age-invariant speaker representation learning method. Since the VoxCeleb is collected from the YouTube platform, the dataset consists of cross-age data inherently. However, the meta-data does not contain the speaker age label. Therefore, we adopt the face age estimation method to predict the speaker age value from the associated visual data, then label the audio recording with the estimated age. We construct multiple Cross-Age test sets on VoxCeleb (Vox-CA), which deliberately select the positive trials with large age-gap. Also, the effect of nationality and gender is considered in selecting negative pairs to align with Vox-H cases. The baseline system performance drops from 1.939% EER on the Vox-H test set to 10.419\\% on the Vox-CA20 test set, which indicates how difficult the cross-age scenario is. Consequently, we propose an age-decoupling adversarial learning (ADAL) method to alleviate the negative effect of the age gap and reduce intra-class variance. Our method outperforms the baseline system by over 10% related EER reduction on the Vox-CA20 test set.",True
thithuuyen22_interspeech,https://www.isca-archive.org/interspeech_2022/thithuuyen22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/thithuuyen22_interspeech.html,Vietnamese Capitalization and Punctuation Recovery Models,ASR: Linguistic Components,2022,"Despite the rise of recent performant methods in Automatic Speech Recognition (ASR), such methods do not ensure proper casing and punctuation for their outputs. This problem has a significant impact on the comprehension of both Natural Language Processing (NLP) algorithms and human to process. Capitalization and punctuation restoration is imperative in pre-processing pipelines for raw textual inputs. For low resource languages like Vietnamese, public datasets for this task are scarce. In this paper, we contribute a public dataset for capitalization and punctuation recovery for Vietnamese; and propose a joint model for both tasks named JointCapPunc. Experimental results on the Vietnamese dataset show the effectiveness of our joint model compare to single model and previous joint learning model. We publicly release our dataset and the implementation of our model at https://github.com/anhtunguyen98/JointCapPunc",True
liu22t_interspeech,https://www.isca-archive.org/interspeech_2022/liu22t_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/liu22t_interspeech.html,MSDWild: Multi-modal Speaker Diarization Dataset in the Wild,Speaker Embedding and Diarization,2022,"Speaker diarization in real-world acoustic environments is a challenging task of increasing interest from both academia and industry. Although it has been widely accepted that incorporating visual information benefits audio processing tasks such as speech recognition, there is currently no fully released dataset that can be used for benchmarking multi-modal speaker diarization performance in real-world environments. In this paper, we release MSDWild, a benchmark dataset for multi-modal speaker diarization in the wild. The dataset is collected from public videos, covering rich real-world scenarios and languages. All video clips are naturally shot videos without over-editing such as lens switching. Audio and video are both released. In particular, MSDWild has a large portion of the naturally overlapped speech, forming an excellent testbed for cocktail-party problem research. Furthermore, we also conduct baseline experiments on the dataset using audio-only, visual-only, and audio-visual speaker diarization.",True
rose22_interspeech,https://www.isca-archive.org/interspeech_2022/rose22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/rose22_interspeech.html,End-to-End multi-talker audio-visual ASR using an active speaker attention module,Multimodal Systems,2022,"This paper presents a new approach for end-to-end audio-visual multi-talker speech recognition. The approach, referred to here as the visual context attention model (VCAM), is important because it uses the available video information to assign decoded text to one of multiple visible faces. This essentially resolves the label ambiguity issue associated with most multi-talker modeling approaches which can decode multiple label strings but cannot assign the label strings to the correct speakers. This is implemented as a transformer-transducer based end-to-end model and evaluated using a two speaker simulated audio-visual overlapping speech dataset created from YouTube videos. It is shown in the paper that the VCAM model improves performance with respect to previously reported audio-only and audio-visual multi-talker ASR systems. It is also shown that the attention model, trained end-to-end with the multi-talker A/V ASR model, is able perform active speaker detection on the two speaker overlapped speech test set.",True
deadman22_interspeech,https://www.isca-archive.org/interspeech_2022/deadman22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/deadman22_interspeech.html,Modelling Turn-taking in Multispeaker Parties for Realistic Data Simulation,Source Separation II,2022,"Simulation plays a crucial role in developing components of automatic speech recognition systems such as enhancement and diarization. In source separation and target-speaker extraction, datasets with high degrees of temporal overlap are used both in training and evaluation. However, this contrasts with the fact that people tend to avoid such overlap in real conversations. It is well known that artifacts introduced from pre-processing with no overlapping speech can be detrimental to recognition performance. This work proposes a finite-state based generative method trained on timing information in speech corpora, which leads to two main contributions. First, a method for generating arbitrary large datasets which follow desired statistics of real parties. Second, features extracted from the models are shown to have a correlation with speaker extraction performance. This leads to the contribution of quantifying how much difficulty in a mixture is due to turn-taking, factoring out other complexities in the signal. Models which treat speakers as independent produce poor generation and representation results. We improve upon this by proposing models which have states conditioned on whether another person is speaking.",True
kim22p_interspeech,https://www.isca-archive.org/interspeech_2022/kim22p_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/kim22p_interspeech.html,Evaluating User Perception of Speech Recognition System Quality with Semantic Distance Metric,"Summarization, Entity Extraction, Evaluation and Others",2022,"Measuring automatic speech recognition (ASR) system quality is critical for creating user-satisfying voice-driven applications. Word Error Rate (WER) has been traditionally used to evaluate ASR system quality; however, it sometimes correlates poorly with user perception/judgement of transcription quality. This is because WER weighs every word equally and does not consider semantic correctness which has a higher impact on user perception. In this work, we propose evaluating ASR output hypotheses quality with SemDist that can measure semantic correctness by using the distance between the semantic vectors of the reference and hypothesis extracted from a pre-trained language model. Our experimental results of 71K and 36K user annotated ASR output quality show that SemDist achieves higher correlation with user perception than WER. We also show that SemDist has higher correlation with downstream Natural Language Understanding (NLU) tasks than WER.",True
gent22_interspeech,https://www.isca-archive.org/interspeech_2022/gent22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/gent22_interspeech.html,Deep Learning for Prosody-Based Irony Classification in Spontaneous Speech,Automatic Analysis of Paralinguistics,2022,"Recognizing irony in speech and text can be challenging even for humans. For natural language processing (NLP) applications, irony recognition presents a unique challenge as irony alters the sentiment and meaning of the words themselves. Combining phonological insights from past literature on irony prosody and deep learning modeling, this research presents a new approach to irony classification in naturalistic speech data. A new corpus consisting of nearly five hours of irony-annotated, naturalistic, conversational speech data has been constructed for this study. A wide array of utterance-level and time-series acoustic features were extracted from this data and utilized in the training and fine-tuning of a series of deep learning approaches for irony classification. The best-performing model achieved an area under the curve of 0.811 in the speaker dependent condition, and 0.738 in the speaker independent condition, outperforming most irony classification models in the existing literature. In addition to the myriad real-world applications for this approach, its contribution to the understanding of prosodically-encoded augmentation of semantic content constitutes a significant step forward for research in the fields of linguistics and NLP.",True
chen22o_interspeech,https://www.isca-archive.org/interspeech_2022/chen22o_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/chen22o_interspeech.html,Audio-Visual Speech Recognition in MISP2021 Challenge: Dataset Release and Deep Analysis,Spoken Language Processing II,2022,"In this paper, we present the updated Audio-Visual Speech Recognition (AVSR) corpus of MISP2021 challenge, a large-scale audio-visual Chinese conversational corpus consisting of 141h audio and video data collected by far/middle/near microphones and far/middle cameras in 34 real-home TV rooms. To our best knowledge, our corpus is the first distant multi-microphone conversational Chinese audio-visual corpus and the first large vocabulary continuous Chinese lip-reading dataset in the adverse home-tv scenario. Moreover, we make a deep analysis of the corpus and conduct a comprehensive ablation study of all audio and video data in the audio-only/video-only/audio-visual systems. Error analysis shows video modality supplement acoustic information degraded by noise to reduce deletion errors and provide discriminative information in overlapping speech to reduce substitution errors. Finally, we also design a set of experiments such as frontend, data augmentation and end-to-end models for providing the direction of potential future work. The corpus and the code are released to promote the research not only in speech area but also for the computer vision area and cross-disciplinary research.",True
noguchi22_interspeech,https://www.isca-archive.org/interspeech_2022/noguchi22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/noguchi22_interspeech.html,VOT and F0 perturbations for the realization of voicing contrast in Tohoku Japanese,Phonetics and Phonology,2022,"Intervocalic voicing neutralization in Tohoku Japanese was studied based on auditory impressions until our previous research revealed this phenomenon by acoustically and quantitatively measuring voice onset time (VOT). While the research excluded fully pre-voiced tokens due to measurement difficulties, the current study included such tokens by using the method proposed in an earlier study. We also measured the onset F0 of the following vowel and preceding vowel duration to discuss the possibility of secondary cues for the voicing contrast. Our result confirmed the overlap in VOT values in the positive region, which agrees with our previous result, and revealed another overlap in the negative region from the fully pre-voiced tokens newly added in the current study. Moreover, a positive linear correlation between VOT and F0, known as consonant-intrinsic F0, was observed in Tohoku Japanese. Although our results did not support F0 of the following vowel and preceding vowel duration as a secondary cue, investigating acoustic cues may contribute to further increasing our understanding of Tohoku Japanese.",True
zhu22e_interspeech,https://www.isca-archive.org/interspeech_2022/zhu22e_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/zhu22e_interspeech.html,Filler Word Detection and Classification: A Dataset and Benchmark,Speech Segmentation II,2022,"Filler words such as uh' orum' are sounds or words people use to signal they are pausing to think. Finding and removing filler words from recordings is a common and tedious task in media editing. Automatically detecting and classifying filler words could greatly aid in this task, but few studies have been published on this problem to date.A key reason is the absence of a dataset with annotated filler words for model training and evaluation.In this work, we present a novel speech dataset, PodcastFillers, with 35K annotated filler words and 50K annotations of other sounds that commonly occur in podcasts such as breaths, laughter, and word repetitions.We propose a pipeline that leverages VAD and ASR to detect filler candidates and a classifier to distinguish between filler word types.We evaluate our proposed pipeline on PodcastFillers, compare to several baselines, and present a detailed ablation study.In particular, we evaluate the importance of using ASR and how it compares to a transcription-free approach resembling keyword spotting. We show that our pipeline obtains state-of-the-art results, and that leveraging ASR strongly outperforms a keyword spotting approach. We make PodcastFillers publicly available, in the hope that our work serves as a benchmark for future research.",True
zevallos22_interspeech,https://www.isca-archive.org/interspeech_2022/zevallos22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/zevallos22_interspeech.html,Data Augmentation for Low-Resource Quechua ASR Improvement,Low-Resource ASR Development I,2022,"Automatic Speech Recognition (ASR) is a key element in new services that helps users to interact with an automated system. Deep learning methods have made it possible to deploy systems with a word error rate close to only 5% for ASR of English. However, the use of these methods is only available for languages with hundreds or thousands of hours of audio and their corresponding transcriptions. For the so-called low- resource languages to speed up the availability of resources that can improve the performance of their ASR systems, methods of creating new resources on the basis of existing ones are being investigated. In this paper we describe our DA approach to improve the results of ASR models for low-resource and agglutinative languages. We carry out experiments developing an ASR for Quechua using the Wav2letter++ model. We reduced WER by 8.73% through our approach to the base model. The resulting ASR model obtained 22.75% WER and was trained with 99 hours of original resources and 99 hours of synthetic data obtained with a combination of text augmentation and synthetic speech generation.",True
zhou22g_interspeech,https://www.isca-archive.org/interspeech_2022/zhou22g_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/zhou22g_interspeech.html,Audio-Visual Wake Word Spotting in MISP2021 Challenge: Dataset Release and Deep Analysis,Spoken Dialogue Systems and Multimodality,2022,"In this paper, we describe and release publicly the audio-visual wake word spotting (WWS) database in the MISP2021 Challenge, which covers a range of scenarios of audio and video data collected by near-, mid-, and far-field microphone arrays, and cameras, to create a shared and publicly available database for WWS. The database and the code are released, which will be a valuable addition to the community for promoting WWS research using multi-modality information in realistic and complex conditions. Moreover, we investigated the different data augmentation methods for single modalities on an end-to-end WWS network. A set of audio-visual fusion experiments and analysis were conducted to observe the assistance from visual information to acoustic information based on different audio and video field configurations. The results showed that the fusion system generally improves over the single-modality (audio- or video-only) system, especially under complex noisy conditions.",True
diener22_interspeech,https://www.isca-archive.org/interspeech_2022/diener22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/diener22_interspeech.html,INTERSPEECH 2022 Audio Deep Packet Loss Concealment Challenge,Audio Deep PLC (Packet Loss Concealment) Challenge,2022,"Audio Packet Loss Concealment (PLC) is the hiding of gaps in audio streams caused by data transmission failures in packet switched networks. This is a common problem, and of increasing importance as end-to-end VoIP telephony and teleconference systems become the default and ever more widely used form of communication in business as well as in personal usage. This paper presents the INTERSPEECH 2022 Audio Deep Packet Loss Concealment challenge. We first give an overview of the PLC problem, and introduce some classical approaches to PLC as well as recent work. We then present the open source dataset released as part of this challenge as well as the evaluation methods and metrics used to determine the winner. We also briefly introduce PLCMOS, a novel data-driven metric that can be used to quickly evaluate the performance PLC systems. Finally, we present the results of the INTERSPEECH 2022 Audio Deep PLC Challenge, and provide a summary of important takeaways.",True
rumberg22_interspeech,https://www.isca-archive.org/interspeech_2022/rumberg22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/rumberg22_interspeech.html,kidsTALC: A Corpus of 3- to 11-year-old German Childrenâs Connected Natural Speech,Spoken Language Processing I,2022,"In this paper we present kidsTALC an audio dataset with orthographic and phonetic transcriptions of German children's speech collected to facilitate the development of speech based technological solutions. The dataset is part of a larger project aiming to develop machine-learning applications to support automation in child speech and language assessment for research and clinical purposes. At the same time, the interdisciplinary project was established to increase the accessibility of corpora of continuous child speech in Germany and globally to train accurate automated speech recognition tools for children. In the first stage we collected and transcribed 25 hours of continuous speech from typically developing children aged 3 Â½â11 years. Here, we discuss the key features of the dataset, data collection, transcription protocol and future datasets in the project. We also present important statistics of our dataset and will demonstrate the speech recognition performance of one baseline model on the dataset.",True
nagamine22_interspeech,https://www.isca-archive.org/interspeech_2022/nagamine22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/nagamine22_interspeech.html,Acquisition of allophonic variation in second language speech: An acoustic and articulatory study of English laterals by Japanese speakers,Speech Production,2022,"Acquisition of positional allophonic variation is seen as the foundation of a successful L2 speech learning. However, previous research has mostly focused on the phonemic contrast between English /l/ and /r/, providing little evidence in the acquisition of positional allophones, such as those in English /l/. The current study investigates the acoustics and articulation of allophonic variations in English laterals produced by Japanese speakers, focusing on the effects of syllabic positions and flanking vowels. Acoustic and articulatory data were obtained from five Japanese speakers in a simultaneous audio and high-speed ultrasound tongue imaging recording set-up while they read sentences containing syllable-initial and -final tokens of English /l/ in four different vowel contexts. Acoustic analysis was conducted on 500 tokens using linear-mixed effects modelling and the articulatory data were analysed using generalised additive mixed modelling. Syllable position and vowel context had significant effects on acoustics, while midsagittal tongue shape was more influenced by vowel context, with fewer positional effects. The results demonstrate that differences in acoustics not always be mirrored exactly by midsagittal tongue shape, suggesting multidimensionality of articulation in second language speech.",True
paul22_interspeech,https://www.isca-archive.org/interspeech_2022/paul22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/paul22_interspeech.html,Improving Data Driven Inverse Text Normalization using Data Augmentation and Machine Translation,Show and Tell IV,2022,"Inverse text normalization (ITN) is used to convert the spoken form output of an automatic speech recognition (ASR) system to a written form. Traditional handcrafted ITN rules can be complex to transcribe and maintain. Meanwhile neural modeling approaches require quality large-scale spoken-written pair exam ples in the same or similar domain as the ASR system (in-domain data), to train. Both these approaches require costly and complex annotation. In this paper, we present a data augmentation tech nique with neural machine translation that effectively generates rich spoken-written pairs for high and low resource languages effectively. We empirically demonstrate that ITN models (in tar get language) trained using our data augmentation with machine translation technique can achieve similar performance as ITN models (en) trained directly with in-domain language.",True
talkar22_interspeech,https://www.isca-archive.org/interspeech_2022/talkar22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/talkar22_interspeech.html,Speech Acoustics in Mild Cognitive Impairment and Parkinson's Disease With and Without Concurrent Drawing Tasks,Pathological Speech Analysis,2022,"Parkinson's disease (PD) is characterized by motor dysfunction; however, non-motor symptoms such as cognitive decline also have a dramatic impact on quality of life. Current assessments to diagnose cognitive impairment take many hours and require high clinician involvement. Thus, there is a need to develop new tools leading to quick and accurate determination of cognitive impairment to allow for appropriate, timely interventions. In this paper, individuals with PD, designated as either having no cognitive impairment (NCI) or mild cognitive impairment (MCI), undergo a speech-based protocol, involving reading or listing items within a category, performed either with or without a concurrent drawing task. From the speech recordings, we extract motor coordination-based features, derived from correlations across acoustic features representative of speech production subsystems. The correlation-based features are utilized in gaussian mixture models to discriminate between individuals designated NCI or MCI in both the single and dual task paradigms. Features derived from the laryngeal and respiratory subsystems, in particular, discriminate between these two groups with AUCs > 0.80. These results suggest that cognitive impairment can be detected using speech from both single and dual task paradigms, and that cognitive impairment may manifest as differences in vocal fold vibration stability.",True
arai22_interspeech,https://www.isca-archive.org/interspeech_2022/arai22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/arai22_interspeech.html,Syllable sequence of /a/+/ta/ can be heard as /atta/ in Japanese with visual or tactile cues,"Speech Production, Perception and Multimodality",2022,"In our previous work, we reported that the word /atta/ with a geminate consonant differs from the syllable sequence /a/+pause+/ta/ in Japanese; specifically, there are formant transitions at the end of the first syllable in /atta/ but not in /a/+pause+/ta/. We also showed that native Japanese speakers perceived /atta/ when a facial video of /atta/ was synchronously played with an audio signal of /a/+pause+/ta/. In that study, we utilized two video clips for the two utterances in which the speaker was asked to control only the timing of the articulatory closing. In that case, there was no guarantee that the videos would be the exactly same except for the timing. Therefore, in the current study, we use a physical model of the human vocal tract with a miniature robot hand unit to achieve articulatory movements for visual cues. We also provide tactile cues to the listener's finger because we want to test whether cues of another modality affect this perception in the same framework. Our findings showed that when either visual or tactile cues were presented with an audio stimulus, listeners more frequently responded that they heard /atta/ compared to audio-only presentations.",True
stephenson22_interspeech,https://www.isca-archive.org/interspeech_2022/stephenson22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/stephenson22_interspeech.html,"BERT, can HE predict contrastive focus? Predicting and controlling prominence in neural TTS using a language model",Speech Synthesis: Prosody Modeling,2022,"Several recent studies have tested the use of transformer language model representations to infer prosodic features for text-to-speech synthesis (TTS). While these studies have explored prosody in general, in this work, we look specifically at the prediction of contrastive focus on personal pronouns. This is a particularly challenging task as it often requires semantic, discursive and/or pragmatic knowledge to predict correctly. We collect a corpus of utterances containing contrastive focus and we evaluate the accuracy of a BERT model, finetuned to predict quantized acoustic prominence features, on these samples. We also investigate how past utterances can provide relevant information for this prediction. Furthermore, we evaluate the controllability of pronoun prominence in a TTS model conditioned on acoustic prominence features.",True
mitsui22_interspeech,https://www.isca-archive.org/interspeech_2022/mitsui22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/mitsui22_interspeech.html,End-to-End Text-to-Speech Based on Latent Representation of Speaking Styles Using Spontaneous Dialogue,Speaking Styles and Interaction Styles II,2022,"The recent text-to-speech (TTS) has achieved quality comparable to that of humans; however, its application in spoken dialogue has not been widely studied. This study aims to realize a TTS that closely resembles human dialogue. First, we record and transcribe actual spontaneous dialogues. Then, the proposed dialogue TTS is trained in two stages: first stage, variational autoencoder (VAE)-VITS or Gaussian mixture variational autoencoder (GMVAE)-VITS is trained, which introduces an utterance-level latent variable into variational inference with adversarial learning for end-to-end text-to-speech (VITS), a recently proposed end-to-end TTS model. A style encoder that extracts a latent speaking style representation from speech is trained jointly with TTS. In the second stage, a style predictor is trained to predict the speaking style to be synthesized from dialogue history. During inference, by passing the speaking style representation predicted by the style predictor to VAE/GMVAE-VITS, speech can be synthesized in a style appropriate to the context of the dialogue. Subjective evaluation results demonstrate that the proposed method outperforms the original VITS in terms of dialogue-level naturalness.",True
oh22_interspeech,https://www.isca-archive.org/interspeech_2022/oh22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/oh22_interspeech.html,Dynamic Vertical Larynx Actions Under Prosodic Focus,Acoustic Phonetics and Prosody,2022,"Recently, Lee (2018) observes that one vertical larynx movement (VLM) is associated with an Accentual Phrase (AP) in Seoul Korean. The current study builds on these findings by investigating the effect of prosodic focus on vertical larynx actions. Target sentences were designed to produce four APs (e.g., Joohyun sold six yards of shabby garden field; AP[Joohyun-SUBJ] AP[shabby garden field] AP[six yards-OBJ] AP[sold-DECL], presented in Korean) and were used to elicit focus on the initial word of the object phrase (e.g., six). Articulatory data on VLM is obtained from five Seoul Korean speakers using real-time MRI. Results indicate that quantifiable VLMs observed for each sentence range from 3 to 6 movements, with 4 movements per sentence being the most frequent. Sentences with focus have more instances of VLM per sentence than those without. Focused sentences exhibit significantly greater vertical larynx displacement around the region of focus than the control. Our findings have implications for prosodic planning and pitch resetting, and ongoing analyses examine how VLMs align with Accentual Phrases in Seoul Korean and correlate with fundamental frequency.",True
breiner22_interspeech,https://www.isca-archive.org/interspeech_2022/breiner22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/breiner22_interspeech.html,UserLibri: A Dataset for ASR Personalization Using Only Text,Language Modeling and Lexical Modeling for ASR,2022,"Personalization of speech models on mobile devices (on-device personalization) is an active area of research, but more often than not, mobile devices have more text-only data than paired audio-text data. We explore training a personalized language model on text-only data, used during inference to improve speech recognition performance for that user. We experiment on a user-clustered LibriSpeech corpus, supplemented with personalized text-only data for each user from Project Gutenberg. We release this User-Specific LibriSpeech (UserLibri) dataset to aid future personalization research. LibriSpeech audio-transcript pairs are grouped into 55 users from the test-clean dataset and 52 users from test-other. We are able to lower the average word error rate per user across both sets in streaming and nonstreaming models, including an improvement of 2.5 for the harder set of test-other users when streaming.",True
xu22_interspeech,https://www.isca-archive.org/interspeech_2022/xu22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/xu22_interspeech.html,Differential Time-frequency Log-mel Spectrogram Features for Vision Transformer Based Infant Cry Recognition,Multimodal Speech Emotion Recognition and Paralinguistics,2022,"Crying is the main way for babies to communicate with the outside world. Analyzing cry enables not only the identification of babies' needs/thoughts they want to express, but also the prediction of potential diseases. In general, it is much more difficult to recognize special needs and emotions from infant cry than adults, because infant cry does not contain any linguistic information and the emotional expression is not as rich as adults.In this work, we focus on the time-frequency characteristics of infant crying signals and propose a differential time-frequency log-Mel spectrogram features based vision transformer (ViT) approach for infant cry recognition (ICR). We first calculate the deltas of log-Mel spectrogram of infant crying sounds over time frames and frequencies, respectively. The log-Mels and deltas are then combined as a 3-D feature representation and fed into the ViT model for cry classification. Experimental results on the CRIED database show the superiority of the proposed system over comparison methods and that the combination of logMels, the time-frame delta and frequency-bin delta achieves the best performance. The proposed method is further validated on a self-recorded dataset.",True
lemaguer22_interspeech,https://www.isca-archive.org/interspeech_2022/lemaguer22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/lemaguer22_interspeech.html,Back to the Future: Extending the Blizzard Challenge 2013,"Speech Synthesis: Tools, Data, and Evaluation",2022,"Nowadays, speech synthesis technology is synonymous with the use of Deep Learning. To understand more about how synthesis systems have progressed with the advent of Deep Learning requires open-sourced speech resources that connect past and present technologies. This would allow direct comparisons. This paper presents such a resource by extending the 2013 edition of the Blizzard Challenge. Using this extension, we compare top-tier systems from the past to modern technologies in a controlled setting. From this edition, we selected the best representative of each historical synthesis technology, to which we added four systems representing combinations of modern acoustic models and neural vocoders. A large scale subjective evaluation was conducted to evaluate naturalness. Our results show that, as expected, modern technologies generate more natural synthetic speech. However, these systems are still not perceived to be as natural as the human voice. Crucially, we also observed that the Mean Opinion Score (MOS) of the historical systems dropped a full MOS point from their scores in the original edition. This demonstrates the relative nature of MOS: it should generally not be reported as an absolute value despite its origin as an absolute category rating.",True
mussakhojayeva22_interspeech,https://www.isca-archive.org/interspeech_2022/mussakhojayeva22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/mussakhojayeva22_interspeech.html,KSC2: An Industrial-Scale Open-Source Kazakh Speech Corpus,"Multi-, Cross-lingual and Other Topics in ASR I",2022,"We present the first industrial-scale open-source Kazakh speech corpus for automatic speech recognition research and development. Our corpus subsumes two previously presented corpora: 1) Kazakh speech corpus (KSC) and 2) Kazakh text-to-speech 2 (KazakhTTS2). We also provide additional data from other sources, including television news, television and radio programs, parliament speeches, and podcasts. Our corpus, which we have named KSC2, contains over a thousand hours of high-quality transcribed data, which is triple the size of KSC. KSC2 was manually transcribed with the help of native Kazakh speakers and validated via preliminary speech recognition experiments on various evaluation sets. Moreover, it contains utterances with Kazakh-Russian code-switching, a conversational practice common among Kazakh speakers. We believe that our corpus will facilitate speech processing research for Kazakh, which is widely considered an under-resourced language. To ensure the reproducibility of experiments, we share the KSC2 corpus, training recipes, and pretrained models.",True
li22e_interspeech,https://www.isca-archive.org/interspeech_2022/li22e_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/li22e_interspeech.html,DDS: A new device-degraded speech dataset for speech enhancement,Single-channel and multi-channel Speech Enhancement,2022,"A large and growing amount of speech content in real-life scenarios is being recorded on consumer-grade devices in uncontrolled environments, resulting in degraded speech quality. Transforming such low-quality device-degraded speech into high-quality speech is a goal of speech enhancement (SE). This paper introduces a new speech dataset, DDS, to facilitate the research on SE. DDS provides aligned parallel recordings of high-quality speech (recorded in professional studios) and a number of versions of low-quality speech, producing approximately 2,000 hours speech data. The DDS dataset covers 27 realistic recording conditions by combining diverse acoustic environments and microphone devices, and each version of a condition consists of multiple recordings from six microphone positions to simulate different noise and reverberation levels. We also test several SE baseline systems on the DDS dataset and show the impact of recording diversity on performance.",True
liebig22_interspeech,https://www.isca-archive.org/interspeech_2022/liebig22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/liebig22_interspeech.html,An investigation of regression-based prediction of the femininity or masculinity in speech of transgender people,Human Speech & Signal Processing,2022,"Transgender individuals often seek for voice modification to more closely have their voice matched with their new sex, and avoid potential stigmatization or even discrimination. Whereas treatment options such as voice therapy or surgery exist, a quantitative measure of the treatment outcome is missing. In this paper, we therefore propose a novel regression-based method to predict the perceived femininity or masculinity of a speaker's voice. To this end, 86 speakers (34 male, 35 female, 17 transgender) were recorded reading aloud a German standard passage. Subsequently a group of 28 laypersons and 13 experts rated the femininity/masculinity of these speech samples. Each spoken utterance was automatically analysed with respect to nine different pitch-, resonance- and voice quality-related acoustic features. The ratings were the targets for three prediction models (linear, logistic and decision tree regression) based on the extracted features. The results show that, generally, f0 and the vocal tract length (VTL) are the main predictors. Furthermore, the continuous outcome logistic regression model with f0, smoothed cepstral peak prominence (CPPS), Jitter and VTL as input features performed best and achieved promising results with a cross-validated root mean-squared error of 0.117 on the normalized ratings [0,1].",True
vitormenezes22_interspeech,https://www.isca-archive.org/interspeech_2022/vitormenezes22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/vitormenezes22_interspeech.html,Evaluation of different antenna types and positions in a stepped frequency continuous-wave radar-based silent speech interface,Pathological Speech Assessment,2022,"Silent speech interfaces (SSIs) are subject of growing interest, as they can enable speech communication even in the absence of the acoustic signal. Among sensing techniques used in SSIs, radar sensing has many desirable characteristics, such as non-invasiveness and comfort. Although promising results have been achieved with radar-based SSIs, some of its crucial parameters are yet to be investigated, e.g., the optimal type and position of the antennas. To fill this gap, this study investigated the performance of a radar-based SSI with 3 antenna types attached to 3 positions on the speaker's cheek (9 setups). A corpus of 25 phonemes uttered under co-articulation effects was recorded with the 9 setups by 2 native German speakers and then classified with respect to the phonemes. A linear mixed-effect model was fitted to the resulting recognition rates and likelihood ratio tests showed significance for the effects of antenna type and position. The two monopole-type antennas performed better than the Vivaldi-type antenna (2.7% Â± 2.8% and 6.2% Â± 3.0% improvement), and the two positions closer to the speaker's lips performed better than the most distant position (decrease of 2.8% Â± 0.9%). This provides more solid foundation for the development of this type of SSI.",True
patterson22_interspeech,https://www.isca-archive.org/interspeech_2022/patterson22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/patterson22_interspeech.html,Distance-Based Sound Separation,Spatial Audio,2022,"We propose the novel task of distance-based sound separation, where sounds are separated based only on their distance from a single microphone. In the context of assisted listening devices, proximity provides a simple criterion for sound selection in noisy environments that would allow the user to focus on sounds relevant to a local conversation. We demonstrate the feasibility of this approach by training a neural network to separate near sounds from far sounds in single channel synthetic reverberant mixtures, relative to a threshold distance defining the boundary between near and far. With a single nearby speaker and four distant speakers, the model improves scale-invariant signal to noise ratio by 4.4 dB for near sounds and 6.8 dB for far sounds.",True
wang22q_interspeech,https://www.isca-archive.org/interspeech_2022/wang22q_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/wang22q_interspeech.html,ECAPA-TDNN Based Depression Detection from Clinical Speech,Speech and Language in Health: From Remote Monitoring to Medical Conversations III,2022,"Depression is a serious mood disorder that has become one of the major diseases that endanger human mental health. The automatic detection of depression using speech signals has become a promising approach for the early diagnosis of depression currently. However, there is still a performance gap between clinical practice and research, considering the lab-recorded corpus was used in most of the current studies. Therefore, we collected a Chinese clinical depression corpus, of which 131 participants with their speech during the Hamilton Rating Scale for Depression (HAMD) interview were included in this study. Furthermore, we developed a depression speech detection system based on a Time-Delay Neural Network (TDNN) model to distinguish depression. Our approach achieves a mean F1 score of 90.8% and an accuracy of 90.4% by five-fold cross-validation. The result suggests that the developed TDNN-based model has a potential clinical meaning in the diagnosis of depression.",True
delvaux22_interspeech,https://www.isca-archive.org/interspeech_2022/delvaux22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/delvaux22_interspeech.html,Telling self-defining memories: An acoustic study of natural emotional speech productions,Phonetics I,2022,"Vocal cues in emotion encoding are rarely studied based on real-life, naturalistic emotional speech. In the present study, 20 speakers aged 25-35 were recorded while orally telling 5 successive self-defining autobiographic memories (SDM). By definition, this task is highly emotional, although emotional load and emotion regulation are expected to vary across SDM. Seven acoustic parameters were extracted: MeanF0, MedianFo, StandardDeviationF0, MinF0, MaxF0, Duration and SpeechRate. All SDM were manually transcribed, then their emotional lexicon was analysed using Emotaix. First, speech productions were examined in reference with SDM characteristics (specificity, integrative meaning and affective valence) as determined by 3 independent investigators. Results showed that overall the speech parameters did not change over the time course of the experiment, or as a function of integrative meaning. Specific memories were recounted at a higher speech rate and at greater length than non specific ones. SDM with positive affective valence were shorter and included less variability in fundamental frequency than negative SDM. Second, emotionally-charged (positive vs. negative; high vs. low arousal) vs. emotionally-neutral utterances as to Emotaix classification were compared over all SDM. Only a few significant effects were observed, which led us to discuss the role of emotion regulation in the SDM task.",True
liu22q_interspeech,https://www.isca-archive.org/interspeech_2022/liu22q_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/liu22q_interspeech.html,Automated Voice Pathology Discrimination from Continuous Speech Benefits from Analysis by Phonetic Context,Atypical Speech Analysis and Detection,2022,"In contrast to previous studies that look only at discriminating pathological voice from the normal voice, in this study we focus on the discrimination between cases of spasmodic dysphonia (SD) and vocal fold palsy (VP) using automated analysis of speech recordings. The hypothesis is that discrimination will be enhanced by studying continuous speech, since the different pathologies are likely to have different effects in different phonetic contexts. We collected audio recordings of isolated vowels and of a read passage from 60 patients diagnosed with SD (N=38) or VP (N=22). Baseline classifiers on features extracted from the recordings taken as a whole gave a cross-validated unweighted average recall of up to 75% for discriminating the two pathologies. We used an automated method to divide the read passage into phone-labelled regions and built classifiers for each phone. Results show that the discriminability of the pathologies varied with phonetic context as predicted. Since different phone contexts provide different information about the pathologies, classification is improved by fusing phone predictions, to achieve a classification accuracy of 83%. The work has implications for the differential diagnosis of voice pathologies and contributes to a better understanding of their impact on speech.",True
priyasad22_interspeech,https://www.isca-archive.org/interspeech_2022/priyasad22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/priyasad22_interspeech.html,Detecting Heart Failure Through Voice Analysis using Self-Supervised Mode-Based Memory Fusion,Atypical Speech Detection,2022,"Congestive Heart Failure (CHF) is a progressive disease that affects millions of people worldwide, severely impacting their quality of life. Missed detection of CHF and its progression affects life expectancy, thus it is critical to develop applications to continuously monitor CHF symptoms and disease progression in a patient-centric and cost-effective manner. This paper focuses on a novel non-invasive technique to identify CHF using patients' speech traits. Pulmonary congestion and breathlessness is the most common symptom of heart failure and one of the major contributors to hospitalisation. Since pulmonary congestion results in impairment of a patient's voice, we propose a novel, non invasive method for monitoring CHF through analysis of the patient's speech. We also introduce a new balanced dataset, containing voice recordings from both healthy participants and participants diagnosed with CHF, which contains voice alterations reflective of CHF status. We propose a novel deep machine learning architecture based on mode driven memory fusion for CHF recognition from audio recordings of subject's speech. We have achieved 90% accuracy under a subject-independent evaluation setting, highlighting the applicability of such methods for tele-health and home monitoring applications.",True
fox22_interspeech,https://www.isca-archive.org/interspeech_2022/fox22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/fox22_interspeech.html,Improving Contextual Recognition of Rare Words with an Alternate Spelling Prediction Model,ASR: Linguistic Components,2022,"Contextual ASR, which takes a list of bias terms as input along with audio, has drawn recent interest as ASR use becomes more widespread. We are releasing contextual biasing lists to accompany the Earnings21 dataset, creating a public benchmark for this task. We present baseline results on this benchmark using a pretrained end-to-end ASR model from the WeNet toolkit. We show results for shallow fusion contextual biasing applied to two different decoding algorithms. Our baseline results confirm observations that end-to-end models struggle in particular with words that are rarely or never seen during training, and that existing shallow fusion techniques do not adequately address this problem. We propose an alternate spelling prediction model that improves recall of rare words by 34.7% relative and of out-of-vocabulary words by 97.2% relative, compared to contextual biasing without alternate spellings. This model is conceptually similar to ones used in prior work, but is simpler to implement as it does not rely on either a pronunciation dictionary or an existing text-to-speech system.",True
yang22e_interspeech,https://www.isca-archive.org/interspeech_2022/yang22e_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/yang22e_interspeech.html,RaDur: A Reference-aware and Duration-robust Network for Target Sound Detection,Acoustic Event Detection and Classification,2022,"Target sound detection (TSD) aims to detect the target sound from a mixture audio given the reference information. Previous methods use a conditional network to extract a sound-discriminative embedding from the reference audio, and then use it to detect the target sound from the mixture audio. However, the network performs much differently when using different reference audios (\\text{e.g.} performs poorly for noisy and short-duration reference audios), and tends to make wrong decisions for transient events (\\text{i.e.} shorter than $1$ second). To overcome these problems, in this paper, we present a reference-aware and duration-robust network (RaDur) for TSD. More specifically, in order to make the network more aware of the reference information, we propose an embedding enhancement module to take into account the mixture audio while generating the embedding, and apply the attention pooling to enhance the features of target sound-related frames and weaken the features of noisy frames. In addition, a duration-robust focal loss is proposed to help model different-duration events. To evaluate our method, we build two TSD datasets based on UrbanSound and Audioset. Extensive experiments show the effectiveness of our methods.",True
taniguchi22_interspeech,https://www.isca-archive.org/interspeech_2022/taniguchi22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/taniguchi22_interspeech.html,Transformer-Based Automatic Speech Recognition with Auxiliary Input of Source Language Text Toward Transcribing Simultaneous Interpretation,Multimodal Systems,2022,"In the training programs of human simultaneous interpreters, trainee speech is transcribed into text for quality assessment. Though interpreter speech contains irregular speech events such as hesitations, filled pauses, and self-repairs, automatic speech recognition (ASR) is expected to be introduced to save labor of transcription. In the training programs, source language text can be used for ASR because the training materials are prepared in advance. Thus, we propose a Transformer-based end-to-end ASR with an auxiliary input of a source language text toward transcribing simultaneous interpretation. Because a sufficient amount of human interpreter speech with source language text is not available for training the model, we conducted the initial evaluation of the model by simulating speech with source language text by changing the inputs and outputs of large-scale corpora for developing end-to-end speech translation (ST). Our proposed model significantly reduced word error rates (WERs) for four ST corpora: MuST-C English speech - Netherlandic text, English speech - German text, CoVoST 2 English speech - Japanese text, and our original TED-based English speech - Japanese text corpus.",True
ghosh22b_interspeech,https://www.isca-archive.org/interspeech_2022/ghosh22b_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/ghosh22b_interspeech.html,DeToxy: A Large-Scale Multimodal Dataset for Toxicity Classification in Spoken Utterances,Spoken Language Processing I,2022,"Toxic speech is regarded as one of the crucial issues plaguing online social media today. Most recent work on toxic speech detection is constrained to the modality of text and written conversations with very limited work on toxicity detection from spoken utterances or using the modality of speech. In this paper, we introduce a new dataset DeToxy, the first publicly available toxicity annotated dataset for the English language. DeToxy is sourced from various openly available speech databases and consists of over 2 million utterances. We believe that our dataset would act as a benchmark for the relatively new and unexplored Spoken Language Processing (SLP) task of detecting toxicity from spoken utterances and boost further research in this space. Finally, we also provide strong unimodal baselines for our dataset and compare traditional two-step cascade and End-to-End (E2E) approaches. Our experiments show that in the case of spoken utterances, text-based approaches are largely dependent on gold human-annotated transcripts for their performance and also suffer from the problem of keyword bias. However, the presence of speech files in DeToxy helps facilitates the development of E2E speech models which alleviate both the above-stated problems by better capturing speech clues.",True
tan22_interspeech,https://www.isca-archive.org/interspeech_2022/tan22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/tan22_interspeech.html,Environment Aware Text-to-Speech Synthesis,"Speech Synthesis: Linguistic Processing, Paradigms and Other Topics II",2022,This study aims at designing an environment-aware text-to-speech (TTS) system that can generate speech to suit specific acoustic environments. It is also motivated by the desire to leverage massive data of speech audio from heterogeneous sources in TTS system development. The key idea is to model the acoustic environment in speech audio as a factor of data variability and incorporate it as a condition in the process of neural network based speech synthesis. Two embedding extractors are trained with two purposely constructed datasets for characterization and disentanglement of speaker and environment factors in speech. A neural network model is trained to generate speech from extracted speaker and environment embeddings. Objective and subjective evaluation results demonstrate that the proposed TTS system is able to effectively disentangle speaker and environment factors and synthesize speech audio that carries designated speaker characteristics and environment attribute. Audio samples are available online for demonstration.,True
yang22h_interspeech,https://www.isca-archive.org/interspeech_2022/yang22h_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/yang22h_interspeech.html,Open Source MagicData-RAMC: A Rich Annotated Mandarin Conversational(RAMC) Speech Dataset,Spoken Language Processing II,2022,"This paper introduces a high-quality rich annotated Mandarin conversational (RAMC) speech dataset called MagicData-RAMC. The MagicData-RAMC corpus contains 180 hours of conversational speech data recorded from native speakers of Mandarin Chinese over mobile phones with a sampling rate of 16 kHz. The dialogs in MagicData-RAMC are classified into 15 diversified domains and tagged with topic labels, ranging from science and technology to ordinary life. Accurate transcription and precise speaker voice activity timestamps are manually labeled for each sample. Speakers' detailed information is also provided. As a Mandarin speech dataset designed for dialog scenarios with high quality and rich annotations, MagicData-RAMC enriches the data diversity in the Mandarin speech community and allows extensive research on a series of speech-related tasks, including automatic speech recognition, speaker diarization, topic detection, keyword search, text-to-speech, etc. We also conduct several relevant tasks and provide experimental results to help evaluate the dataset.",True
jung22c_interspeech,https://www.isca-archive.org/interspeech_2022/jung22c_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/jung22c_interspeech.html,SASV 2022: The First Spoofing-Aware Speaker Verification Challenge,Spoofing-Aware Automatic Speaker Verification (SASV) I,2022,"The first spoofing-aware speaker verification (SASV) challenge aims to integrate research efforts in speaker verification and anti-spoofing. We extend the speaker verification scenario by introducing spoofed trials to the usual set of target and non-target trials. In contrast to the established ASVspoof challenge where the focus is upon separate, independently optimised spoofing detection and speaker verification sub-systems, SASV targets the development of integrated and jointly optimised solutions. Pre-trained spoofing detection and speaker verification models are provided as open source and are used in two baseline SASV solutions. Both models and baselines are freely available to participants and can be used to develop back-end fusion approaches or end-to-end solutions. Using the provided common evaluation protocol, 23 teams submitted SASV solutions. When assessed with target, non-target and spoofed trials, the best performing system reduces the equal error rate of a conventional speaker verification system from 23.83% to 0.13%. SASV challenge results are testament to the reliability of today's state-of-the-art approaches to spoofing detection and speaker verification.",True
flechl22_interspeech,https://www.isca-archive.org/interspeech_2022/flechl22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/flechl22_interspeech.html,End-to-end speech recognition modeling from de-identified data,"Multi-, Cross-lingual and Other Topics in ASR I",2022,"De-identification of data used for automatic speech recognition modeling is a critical component in protecting privacy, especially in the medical domain. However, simply removing all personally identifiable information (PII) from end-to-end model training data leads to a significant performance degradation in particular for the recognition of names, dates, locations, and words from similar categories. We propose and evaluate a two-step method for partially recovering this loss. First, PII is identified, and each occurrence is replaced with a random word sequence of the same category. Then, corresponding audio is produced via text-to-speech or by splicing together matching audio fragments extracted from the corpus. These artificial audio/label pairs, together with speaker turns from the original data without PII, are used to train models. We evaluate the performance of this method on in-house data of medical conversations and observe a recovery of almost the entire performance degradation in the general word error rate while still maintaining a strong diarization performance. Our main focus is the improvement of recall and precision in the recognition of PII-related words. Depending on the PII category, between 50% - 90% of the performance degradation can be recovered using our proposed method.",True
patel22_interspeech,https://www.isca-archive.org/interspeech_2022/patel22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/patel22_interspeech.html,Using cross-model learnings for the Gram Vaani ASR Challenge 2022,Low-Resource ASR Development II,2022,"In the diverse and multilingual land of India, Hindi is spoken as a first language by a majority of its population. Efforts are made to obtain data in terms of audio, transcriptions, dictionary, etc. to develop speech-technology applications in Hindi. Similarly, the Gram-Vaani ASR Challenge 2022 provides spontaneous telephone speech, with natural back-ground and regional variations in Hindi. The challenge provides: 100 hours of labeled train-set, 5 hours of labeled dev-set and 1000 hours of unlabeled data-set. For the 'Closed Challenge', we trained an End-to-End (E2E) Conformer model using speed perturbations, SpecAugment techniques and use VTLN to handle any unknown speaker groups in the blind evaluation set. On the dev-set, we achieved a 30.3% WER compared to the 34.8% WER by the Challenge E2E baseline. For the 'Self Supervised Closed Challenge', a semi-supervised learning approach is used. We generate pseudo-transcripts for the unlabeled data using a hybrid TDNN-3gram LM model and trained an E2E model. This is then used as a seed for retraining the E2E model with high confidence data. Cross-model learning and refining of the E2E model gave 25.3% WER on the dev-set compared to ~33-35% WER by the Challenge baseline that use wav2vec models.",True
liu22d_interspeech,https://www.isca-archive.org/interspeech_2022/liu22d_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/liu22d_interspeech.html,An End-to-End Macaque Voiceprint Verification Method Based on Channel Fusion Mechanism,Acoustic Event Detection and Classification,2022,"Primates are facing a serious survival crisis. Tracking the range of animal activities and population changes is of great significance for efficient animal protection. Primates are highly alert and inaccessible to humans so that it is difficult to track animals through direct observation, DNA fingerprinting, or marking methods. Primate recognition based on animal calls has the advantages of wide monitoring range, low equipment cost, and good concealment. In this work, we propose an effective macaque speech feature extraction structure, and innovatively propose a feature fusion mechanism to effectively obtain the feature representation of each call. Furthermore, we construct a public open source macaque voiceprint verification dataset. The experimental results show that the proposed method is superior to the existing state-of-the-art human voiceprint verification algorithms with different call durations. The equal error rate (EER) of our macaque voiceprint verification algorithm reaches 6.19%.",True
yoon22_interspeech,https://www.isca-archive.org/interspeech_2022/yoon22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/yoon22_interspeech.html,Predicting Emotional Intensity in Political Debates via Non-verbal Signals,Automatic Analysis of Paralinguistics,2022,"Non-verbal expressions of politicians are important in election. In particular, the emotional intensity of politician revealed in a debate can be strongly linked to voters' evaluation. This paper proposes a multimodal deep-learning model for predicting the perceived emotional intensity of a candidate, which utilizes voice, face, and gesture to capture the comprehensive information of one's emotional intensity revealed in a debate. We collect a dataset of political debate videos from the 2020 Democratic presidential primaries in the USA, and train the proposed model with randomly sampled clips from the debate videos. By applying the proposed model to 23 candidates in 11 debate videos, we show that the standard deviation of the perceived emotional intensity is positively correlated with the changes in candidates' favorability in public polls.",True
lu22c_interspeech,https://www.isca-archive.org/interspeech_2022/lu22c_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/lu22c_interspeech.html,"ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding",Speech Enhancement and Intelligibility,2022,"This paper presents recent progress on integrating speech separation and enhancement (SSE) into the ESPnet toolkit. Compared with the previous ESPnet-SE work, numerous features have been added, including recent state-of-the-art speech enhancement models with their respective training and evaluation recipes. Importantly, a new interface has been designed to flexibly combine speech enhancement front-ends with other tasks, including automatic speech recognition (ASR), speech translation (ST), and spoken language understanding (SLU). To showcase such integration, we performed experiments on carefully designed synthetic datasets for noisy-reverberant multi-channel ST and SLU tasks, which can be used as benchmark corpora for future research. In addition to these new tasks, we also use CHiME-4 and WSJ0-2Mix to benchmark multi- and single-channel SE approaches. Results show that the integration of SE front-ends with back-end tasks is a promising research direction even for tasks besides ASR, especially in the multi-channel scenario. The code is available online at \\url{https://github.com/ESPnet/ESPnet}. The multi-channel ST and SLU datasets, which are another contribution of this work, are released on HuggingFace.",True
maniati22_interspeech,https://www.isca-archive.org/interspeech_2022/maniati22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/maniati22_interspeech.html,SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis,"Speech Synthesis: Tools, Data, and Evaluation",2022,"In this work, we present the SOMOS dataset, the first large-scale mean opinion scores (MOS) dataset consisting of solely neural text-to-speech (TTS) samples. It can be employed to train automatic MOS prediction systems focused on the assessment of modern synthesizers, and can stimulate advancements in acoustic model evaluation. It consists of 20K synthetic utterances of the LJ Speech voice, a public domain speech dataset which is a common benchmark for building neural acoustic models and vocoders. Utterances are generated from 200 different TTS systems including a variety of vanilla neural acoustic models as well as models which allow prosodic variations. An LPCNet vocoder is used for all systems, so that the variations in the final samples depend only on the acoustic models. The synthesized utterances provide a balanced and adequate domain, length and phoneme coverage. MOS naturalness evaluations are collected via crowdsourcing on Amazon Mechanical Turk. We present in detail the design of the SOMOS dataset, as well as provide baseline results by training and evaluating state-of-the-art MOS prediction models, while we show the problems that these models face when assigned to evaluate TTS samples.",True
elbanna22_interspeech,https://www.isca-archive.org/interspeech_2022/elbanna22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/elbanna22_interspeech.html,Hybrid Handcrafted and Learnable Audio Representation for Analysis of Speech Under Cognitive and Physical Load,Speech Representation II,2022,"As a neurophysiological response to threat or adverse conditions, stress can affect cognition, emotion and behaviour with potentially detrimental effects on health in the case of sustained exposure. Since the affective content of speech is inherently modulated by an individual's physical and mental state, a substantial body of research has been devoted to the study of paralinguistic correlates of stress-inducing task load. Historically, voice stress analysis (VSA) has been conducted using conventional digital signal processing (DSP) techniques. Despite the development of modern methods based on deep neural networks (DNNs), accurately detecting stress in speech remains difficult due to the wide variety of stressors and considerable variability in the individual stress perception. To that end, we introduce a set of five datasets for task load detection in speech. The voice recordings were collected as either cognitive or physical stress was induced in the cohort of volunteers, with a cumulative number of more than a hundred speakers. We used the datasets to design and evaluate a novel self-supervised audio representation that leverages the effectiveness of handcrafted features (DSP-based) and the complexity of data-driven DNN representations. Notably, the proposed approach outperformed both extensive handcrafted feature sets and novel DNN-based audio representation learning approaches.",True
kitamura22_interspeech,https://www.isca-archive.org/interspeech_2022/kitamura22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/kitamura22_interspeech.html,Perceptual Evaluation of Penetrating Voices through a Semantic Differential Method,"Speech Production, Perception and Multimodality",2022,"Some speakers have penetrating voices that can be popped out and heard clearly, even in loud noise or from a long distance. This study investigated the voice quality of the penetrating voices using factor analysis. Eleven participants scored how the voices of 124 speakers popped out from the babble noise. By assuming the score as an index of penetration, ten each of high- and low-scored speakers were selected for a rating experiment with a semantic differential method. Forty undergraduate students rated a Japanese sentence produced by these speakers using 14 bipolar 7-point scales concerning voice quality. A factor analysis was conducted using the data of 13 scales (i.e., excluding one scale of penetrating from 14 scales). Three main factors were obtained: (1) powerful and metallic, (2) feminine, and (3) esthetic. The first factor (powerful and metallic) highly correlated with the ratings of penetrating. These results suggest that penetrating voices have multi-dimensional voice quality and that the characteristics of penetrating voice related to powerful and metallic aspects of voices.",True
yang22m_interspeech,https://www.isca-archive.org/interspeech_2022/yang22m_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/yang22m_interspeech.html,Minimizing Sequential Confusion Error in Speech Command Recognition,"Multi-, Cross-lingual and Other Topics in ASR II",2022,"Speech command recognition (SCR) has been commonly used on resource constrained devices to achieve hands-free user experience. However, in real applications, confusion among commands with similar pronunciations often happens due to the limited capacity of small models deployed on edge devices, which drastically affects the user experience. In this paper, inspired by the advances of discriminative training in speech recognition, we propose a novel minimize sequential confusion error (MSCE) training criterion particularly for SCR, aiming to alleviate the command confusion problem. Specifically, we aim to improve the ability of discriminating the target command from other commands on the basis of MCE discriminative criteria. We define the likelihood of different commands through connectionist temporal classification (CTC). During training, we propose several strategies to use prior knowledge creating a confusing sequence set for similar-sounding command instead of creating the whole non-target command set, which can better save the training resources and effectively reduce command confusion errors. Specifically, we design and compare three different strategies for confusing set construction. By using our proposed method, we can relatively reduce the False Reject Rate~(FRR) by 33.7% at 0.01 False Alarm Rate~(FAR) and confusion errors by 18.28% on our collected speech command set.",True
leemann22_interspeech,https://www.isca-archive.org/interspeech_2022/leemann22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/leemann22_interspeech.html,Factors affecting the percept of Yanny v. Laurel (or mixed): Insights from a large-scale study on Swiss German listeners,Speech Perception,2022,"In May 2018, Yanny v. Laurel went viral: when listening to the same audio clip, some people claimed to hear only Yanny, others insisted it must be Laurel, and some had a mixed percept. Phoneticians have identified the acoustic features which caused this perceptual ambiguity, but we still know little about the factors affecting individuals' perception of the illusion. We conducted a controlled study with 974 Swiss German listeners, balanced for age, gender, and regional origin. Overall, nearly two thirds heard Yanny, one quarter Laurel, and about 12% had a mixed percept. We found age, gender, and electronic device to play a significant role: younger, female, and laptop-using participants demonstrated higher proportions of Yanny responses. These findings contribute to the growing body of research on polyperceivable words.",True
webber22_interspeech,https://www.isca-archive.org/interspeech_2022/webber22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/webber22_interspeech.html,REYD â The First Yiddish Text-to-Speech Dataset and System,"Speech Synthesis: Tools, Data, and Evaluation",2022,"Modern text-to-speech (TTS) systems generate high-quality natural-sounding speech, but they only support a limited number of languages. Building data-hungry systems that require large amounts of accurately paired speech and text is challenging for languages with limited resources. Yiddish is a minority language that lacks many of the computational resources available in more widely-spoken languages. No modern TTS system exists for Yiddish. We introduce the Reading Electronic Yiddish Documents or REYD (Yiddish for 'speech') project. Found data is used to create a high-quality, hand-corrected TTS dataset. This dataset is used to train FastSpeech2, a state-of-the-art TTS system. A formal evaluation by expert and non-expert listeners found that the system produced speech that was both intelligible and natural-sounding. The results of this evaluation were used to further improve the dataset. The final hand-corrected dataset, code for creating a TTS system, trained models and other Yiddish text processing tools used in our work are publicly released. We hope the availability of these resources will enable new speech technology projects that better serve the needs of Yiddish-speaking communities.",True
markitantov22_interspeech,https://www.isca-archive.org/interspeech_2022/markitantov22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/markitantov22_interspeech.html,Biometric Russian Audio-Visual Extended MASKS (BRAVE-MASKS) Corpus: Multimodal Mask Type Recognition Task,Spoken Language Processing II,2022,"In this paper, we present a new multimodal corpus called Biometric Russian Audio-Visual Extended MASKS (BRAVE-MASKS), which is designed to analyze voice and facial characteristics of persons wearing various masks, as well as to develop automatic systems for bimodal verification and identification of speakers. In particular, we tackle the multimodal mask type recognition task (6 classes). As a result, audio, visual and multimodal systems were developed, which showed UAR of 54.83%, 72.02% and 82.01%, respectively, on the Test set. These performances are the baseline for the BRAVE-MASKS corpus to compare the follow-up approaches with the proposed systems.",True
aroudi22_interspeech,https://www.isca-archive.org/interspeech_2022/aroudi22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/aroudi22_interspeech.html,TRUNet: Transformer-Recurrent-U Network for Multi-channel Reverberant Sound Source Separation,Spatial Audio,2022,"In recent years, many deep learning techniques for single-channel sound source separation have been proposed using recurrent, convolutional and transformer networks. When multiple microphones are available, spatial diversity between speakers and background noise in addition to spectro-temporal diversity can be exploited by using multi-channel filters for sound source separation. Aiming at end-to-end multi-channel source separation, in this paper we propose a transformer-recurrent-U network (TRUNet), which directly estimates multi-channel filters from multi-channel input spectra. TRUNet consists of a spatial processing network with an attention mechanism across microphone channels aiming at capturing the spatial diversity, and a spectro-temporal processing network aiming at capturing spectral and temporal diversities. In addition to multi-channel filters, we also consider estimating single-channel filters from multi-channel input spectra using TRUNet. We train the network on a large reverberant dataset using a proposed combined compressed mean-squared error loss function, which further improves the sound separation performance. We evaluate the network on a realistic and challenging reverberant dataset, generated from measured room impulse responses of an actual microphone array. The experimental results on realistic reverberant sound source separation show that the proposed TRUNet outperforms state-of-the-art single-channel and multi-channel source separation methods.",True
huang22l_interspeech,https://www.isca-archive.org/interspeech_2022/huang22l_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/huang22l_interspeech.html,QbyE-MLPMixer: Query-by-Example Open-Vocabulary Keyword Spotting using MLPMixer,Spoken Language Processing I,2022,"Current keyword spotting systems are typically trained with a large amount of pre-defined keywords. Recognizing keywords in an open-vocabulary setting is essential for personalizing smart device interaction. Towards this goal, we propose a pure MLP-based neural network that is based on MLPMixer - an MLP model architecture that effectively replaces the attention mechanism in Vision Transformers. We investigate different ways of adapting the MLPMixer architecture to the QbyE open-vocabulary keyword spotting task. Comparisons with the state-of-the-art RNN and CNN models show that our method achieves better performance in challenging situations (10dB and 6dB environments) on both the publicly available Hey-Snips dataset and a larger scale internal dataset with 400 speakers. Our proposed model also has a smaller number of parameters and MACs compared to the baseline models.",True
li22j_interspeech,https://www.isca-archive.org/interspeech_2022/li22j_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/li22j_interspeech.html,TALCS: An open-source Mandarin-English code-switching corpus and a speech recognition baseline,Spoken Language Processing II,2022,"This paper introduces a new corpus of Mandarin-English code-switching speech recognitionâTALCS corpus, suitable for training and evaluating code-switching speech recognition systems. TALCS corpus is derived from real online one-to-one English teaching scenes in TAL education group, which contains roughly 587 hours of speech sampled at 16 kHz. To our best knowledge, TALCS corpus is the largest well labeled Mandarin-English code-switching open source automatic speech recognition (ASR) dataset in the world. In this paper, we will introduce the recording procedure in detail, including audio capturing devices and corpus environments. And the TALCS corpus is freely available for download under the permissive license[ https://ai.100tal.com/dataset]. Using TALCS corpus, we conduct ASR experiments in two popular speech recognition toolkits to make a baseline system, including ESPnet and Wenet. The Mixture Error Rate (MER) performance in the two speech recognition toolkits is compared in TALCS corpus. The experimental results implies that the quality of audio recordings and transcriptions are promising and the baseline system is workable.",True
robinson22_interspeech,https://www.isca-archive.org/interspeech_2022/robinson22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/robinson22_interspeech.html,When Is TTS Augmentation Through a Pivot Language Useful?,Low-Resource ASR Development I,2022,"Developing Automatic Speech Recognition (ASR) for low-resource languages is a challenge due to the small amount of transcribed audio data. For many such languages, audio and text are available separately, but not audio with transcriptions. Using text, speech can be synthetically produced via text-to-speech (TTS) system . However, many low-resource languages do not have quality TTS systems either. We propose an alternative: produce synthetic audio by running text from the target language through a trained TTS system for a higher-resource pivot language. We investigate when and how this technique is most effective in low-resource settings. In our experiments, using several thousand synthetic TTS data pairs and duplicating authentic data to balance yields optimal results. Our findings suggest that searching over a set of candidate pivot languages can lead to marginal improvements and that, surprisingly, ASR performance can at times by harmed by increases in measured TTS quality. Application of these findings improves ASR error rates by 64.5% and 45.0% CERR respectively for two low-resource languages: Guarani and Suba.",True
mathur22_interspeech,https://www.isca-archive.org/interspeech_2022/mathur22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/mathur22_interspeech.html,DocLayoutTTS: Dataset and Baselines for Layout-informed Document-level Neural Speech Synthesis,"Speech Synthesis: Linguistic Processing, Paradigms and Other Topics II",2022,"We propose a new task of synthesizing speech directly from semi-structured documents where the extracted text tokens from OCR systems may not be in the correct reading order due to the complex document layout. We refer to this task as layout-informed document-level TTS and present the DocSpeech dataset which consists of 10K audio clips of a single-speaker reading layout-enriched Word document. For each document, we provide the natural reading order of text tokens, their corresponding bounding boxes, and the audio clips synthesized in the correct reading order. We also introduce DocLayoutTTS, a Transformer encoder-decoder architecture that generates speech in an end-to-end manner given a document image with OCR extracted text. Our architecture simultaneously learns text reordering and mel-spectrogram prediction in a multi-task setup. Moreover, we take advantage of curriculum learning to progressively learn longer, more challenging document-level text utilizing both \\texttt{DocSpeech} and LJSpeech datasets. Our empirical results show that the underlying task is challenging. Our proposed architecture performs slightly better than competitive baseline TTS models with a pre-trained model providing reading order priors. We release samples of the DocSpeech dataset.",True
baird22_interspeech,https://www.isca-archive.org/interspeech_2022/baird22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/baird22_interspeech.html,State & Trait Measurement from Nonverbal Vocalizations: A Multi-Task Joint Learning Approach,Multimodal Speech Emotion Recognition and Paralinguistics,2022,"Humans infer a wide array of meanings from expressive nonverbal vocalizations, \\eg laughs, cries, and sighs. Thus far, computational research has primarily focused on the coarse classification of vocalizations such as laughs, but that approach overlooks significant variations in the meaning of distinct laughs (e.g., amusement, awkwardness, triumph) and the rich array of more nuanced vocalizations people form. Nonverbal vocalizations are shaped by the emotional state an individual chooses to convey, their wellbeing, and (as with the voice more broadly) their identity-related traits. In the present study, we utilize a large-scale dataset comprising more than 35 hours of densely labeled vocal bursts to model emotionally expressive states and demographic traits from nonverbal vocalizations. We compare a single-task and multi-task deep learning architecture to explore how models can leverage acoustic co-dependencies that may exist between the expression of 10 emotions by vocal bursts and the demographic traits of the speaker. Results show that nonverbal vocalizations can be reliably leveraged to predict emotional expression, age, and country of origin. In a multi-task setting, our experiments show that joint learning of emotional expression and demographic traits appears to yield robust results, primarily beneficial for the classification of a speaker's country of origin.",True
yi22b_interspeech,https://www.isca-archive.org/interspeech_2022/yi22b_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/yi22b_interspeech.html,ConferencingSpeech 2022 Challenge: Non-intrusive Objective Speech Quality Assessment (NISQA) Challenge for Online Conferencing Applications,Non-intrusive Objective Speech Quality Assessment (NISQA) Challenge for Online Conferencing Applications,2022,"With the advances in speech communication systems such as online conferencing applications, we can seamlessly work with people regardless of where they are. However, during online meetings, the speech quality can be significantly affected by background noise, reverberation, packet loss, network jitter, etc. Because of its nature, the speech quality is traditionally assessed in subjective tests in laboratories and lately also in crowdsourcing following the international standards from ITU-T Rec. P.800 series. However, those approaches are costly and cannot be applied to customer data. Therefore, an effective objective assessment approach is needed to evaluate or monitor the speech quality of the ongoing conversation. The ConferencingSpeech 2022 challenge targets the non-intrusive deep neural network models for the speech quality assessment task. We open-sourced a training corpus with more than 86K speech clips in different languages, with a wide range of synthesized and live degradations and their corresponding subjective quality scores through crowdsourcing. 18 teams submitted their models for evaluation in this challenge. The blind test sets included about 4300 clips from wide ranges of degradations. This paper describes the challenge, the datasets, and the evaluation methods and reports the final results.",True
meng22d_interspeech,https://www.isca-archive.org/interspeech_2022/meng22d_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/meng22d_interspeech.html,Evidence of Onset and Sustained Neural Responses to Isolated Phonemes from Intracranial Recordings in a Voice-based Cursor Control Task,Speech Articulation & Neural Processing,2022,"We developed a voice-based, self-paced cursor control task to collect corresponding intracranial neural data during isolated utterances of phonemes, namely vowel, nasal and fricative sounds. Two patients implanted with intracranial depth electrodes for clinical epilepsy monitoring performed closed-loop voice-based cursor control from real-time processing of microphone input. In post-hoc data analyses, we searched for neural features that correlated with the occurrence of non-specific speech sounds or specific phonemes. In line with previous studies, we observed onset and sustained responses to speech sounds at multiple recording sites within the superior temporal gyrus. Based on differential patterns of activation in narrow frequency bands up to 200 Hz, we tracked voice activity with 91% accuracy (chance level: 50%) and classified individual utterances into one of five phonemes with 68% accuracy (chance level: 20%). We propose that our framework could be extended to additional phonemes to better characterize neurophysiological mechanisms underlying the production and perception of speech sounds in the absence of language context. In general, our findings provide supplementary evidence and information toward the development of speech brain-computer interfaces using intracranial electrodes.",True
mateju22_interspeech,https://www.isca-archive.org/interspeech_2022/mateju22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/mateju22_interspeech.html,Overlapped Speech Detection in Broadcast Streams Using X-vectors,Speech Segmentation I,2022,"A new approach to overlapped speech detection (OSD) is introduced in this work. It is designed for real-time processing of streamed data and utilizes x-vectors as its input features. It thus allows us to reduce computational demands within the entire streaming data processing chain, where the same x-vectors can also be used for the related task of speaker diarization. Within our method, the x-vectors are extracted using a feed-forward sequential memory network (FSMN) and then fed into a simple neural classifier (speech or cross-talk), whose output is smoothed by a decoder based on weighted finite-state transducers (WFSTs). The evaluation is done on a Czech/Slovak broadcast dataset (we make this data public) and on the AMI meeting corpus. Our online method yields a solid performance while operating with a 2-second latency.",True
tran22_interspeech,https://www.isca-archive.org/interspeech_2022/tran22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/tran22_interspeech.html,An Efficient and High Fidelity Vietnamese Streaming End-to-End Speech Synthesis,"Speech Synthesis: Linguistic Processing, Paradigms and Other Topics II",2022,"In recent years, parallel end-to-end speech synthesis systems have outperformed the 2-stage TTS approaches in audio quality and latency. A parallel end-to-end speech like VITS can generate the audio with high MOS comparable to ground truth and achieve low latency on GPU. However, the VITS still has high latency when synthesizing long utterances on CPUs. Therefore, in this paper, we propose a streaming method for the parallel speech synthesis model like VITS to synthesize with the long texts effectively on CPU. Our system has achieved human-like speech quality in both the non-streaming and streaming mode on the in-house Vietnamese evaluation set, while the synthesis speed of our system is approximately four times faster than that of the VITS in the non-streaming mode. Furthermore, the customer perceived latency of our system in streaming mode is 25 times faster than the VITS on computer CPU. Our system in non-streaming mode achieves a MOS of 4.43 compared to ground-truth with MOS 4.56; it also has high-quality speech with a MOS of 4.35 in streaming mode. Finally, we release a Vietnamese single accent dataset used in our experiments.",True
fietkau22_interspeech,https://www.isca-archive.org/interspeech_2022/fietkau22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/fietkau22_interspeech.html,Relationship between the acoustic time intervals and tongue movements of German diphthongs,Speech Processing & Measurement,2022,"This study investigated the relationship between tongue movements during the production of German diphthongs and their acoustic time intervals. To this end, five subjects produced a set of logatomes that contained German primary, secondary, and peripheral diphthongs in the context of bilabial and labiodental consonants at three different speaking rates. During the utterances, tongue movements were measured by means of optical palatography (OPG), i.e. by optical distance sensing in the oral cavity, along with the acoustic speech signal. The analysis of the movement signals revealed that the diphthongs have s-shaped tongue trajectories that strongly resemble half cosine periods. In addition, acoustic and articulatory diphthong durations have a linear, but not proportional, relationship. Finally, the peak velocity and midpoint between the two targets of a diphthong are reached in the middle of both the acoustic and articulatory diphthong time intervals, regardless of the duration and type of diphthong. These results can help to model realistic tongue movements for diphthongs in articulatory speech synthesis.",True
yang22g_interspeech,https://www.isca-archive.org/interspeech_2022/yang22g_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/yang22g_interspeech.html,ASR Error Correction with Constrained Decoding on Operation Prediction,ASR: Linguistic Components,2022,"Error correction techniques remain effective to refine outputs from automatic speech recognition (ASR) models. Existing end-to-end error correction methods based on an encoder-decoder architecture process all tokens in the decoding phase, creating undesirable latency. In this paper, we propose an ASR error correction method utilizing the predictions of correction operations. More specifically, we construct a predictor between the encoder and the decoder to learn if a token should be kept (""K""), deleted (""D""), or changed (""C"") to restrict decoding to only part of the input sequence embeddings (the ""C"" tokens) for fast inference. Experiments on three public datasets demonstrate the effectiveness of the proposed approach in reducing the latency of the decoding process in ASR correction. It enhances the inference speed by at least three times (3.4 and 5.7 times) while maintaining the same level of accuracy (with WER reductions of 0.53% and 1.69% respectively) for our two proposed models compared to a solid encoder-decoder baseline. In the meantime, we produce and release a benchmark dataset contributing to the ASR error correction community to foster research along this line.",True
borsdorf22_interspeech,https://www.isca-archive.org/interspeech_2022/borsdorf22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/borsdorf22_interspeech.html,Blind Language Separation: Disentangling Multilingual Cocktail Party Voices by Language,Source Separation II,2022,"We introduce blind language separation (BLS) as novel research task, in which we seek to disentangle overlapping voices of multiple languages by language. BLS is expected to separate seen as well as unseen languages, which is different from the target language extraction task that works for one seen target language at a time. To develop a BLS model, we simulate a multilingual cocktail party database, of which each scene consists of two randomly selected languages, each represented by two randomly selected speakers. The database follows the recently proposed GlobalPhoneMCP database design concept that uses the audio data of the GlobalPhone 2000 Speaker Package. We show that a BLS model is able to learn the language characteristics so as to disentangle overlapping voices by language. We achieve a mean SI-SDR improvement of 12.63 dB over 231 test sets. The performance on the individual test sets varies depending on the language combination. Finally, we show that BLS can generalize well to unseen speakers and languages in the mixture.",True
dao22_interspeech,https://www.isca-archive.org/interspeech_2022/dao22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/dao22_interspeech.html,From Disfluency Detection to Intent Detection and Slot Filling,Spoken Dialogue Systems and Multimodality,2022,"We present the first empirical study investigating the influence of disfluency detection on downstream tasks of intent detection and slot filling. We perform this study for Vietnamese---a low-resource language that has no previous study as well as no public dataset available for disfluency detection. First, we extend the fluent Vietnamese intent detection and slot filling dataset PhoATIS by manually adding contextual disfluencies and annotating them. Then, we conduct experiments using strong baselines for disfluency detection and joint intent detection and slot filling, which are based on pre-trained language models. We find that: (i) disfluencies produce negative effects on the performances of the downstream intent detection and slot filling tasks, and (ii) in the disfluency context, the pre-trained multilingual language model XLM-R helps produce better intent detection and slot filling performances than the pre-trained monolingual language model PhoBERT, and this is opposite to what generally found in the fluency context.",True
perezramon22_interspeech,https://www.isca-archive.org/interspeech_2022/perezramon22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/perezramon22_interspeech.html,Generating iso-accented stimuli for second language research: methodology and a dataset for Spanish-accented English,Speech Perception,2022,"A non-native accent can be conveyed at both the segmental and suprasegmental level. Previous studies have developed techniques to isolate the effect of segmental foreign accent by splicing accented segments from a bilingual speaker into non-accented words produced by the same speaker. The current work addresses the issue of between-segment variability by developing a technique to convert from acoustically-equal accent gradations to perceptually-equal steps. The procedure is used to derive the first corpus of Spanish-accented English composed of lexical tokens each generated with one of five degrees of non-native accent. As an example application, corpus tokens are used to elicit accentedness judgements from four listener cohorts with first languages which differ as to whether they share the native language, the non-native (accented) language of the corpus or have a closer phonological inventory to one or the other. Findings highlight the importance of the relationship between listeners' phonological systems and those of the native and non-native languages of the corpus, especially for vowels, with respect to sensitivity to foreign accent.",True
bernhard22_interspeech,https://www.isca-archive.org/interspeech_2022/bernhard22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/bernhard22_interspeech.html,Acoustic Stress Detection in Isolated English Words for Computer-Assisted Pronunciation Training,"Multi-, Cross-lingual and Other Topics in ASR II",2022,"We propose a system for automatic lexical stress detection in isolated English words. It is designed to be part of the computer-assisted pronunciation training application MIAPARLE (miaparle.unige.ch) that specifically focuses on stress contrasts acquisition. Training lexical stress cannot be disregarded in language education as the accuracy in production highly affects the intelligibility and perceived fluency of an L2 speaker. The pipeline automatically segments audio input into syllables over which duration, intensity, pitch, and spectral information is calculated. Since the stress of a syllable is defined relative to its neighboring syllables, the values obtained over the syllables are complemented with differential values to the preceding and following syllables. The resulting feature vectors, retrieved from 1011 recordings of single words spoken by English natives, are used to train a Voting Classifier composed of four supervised classifiers, namely a Support Vector Machine, a Neural Net, a K Nearest Neighbor, and a Random Forest classifier. The approach determines syllables of a single word as stressed or unstressed with an F1 score of 94% and an accuracy of 96%.",True
rennie22_interspeech,https://www.isca-archive.org/interspeech_2022/rennie22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/rennie22_interspeech.html,Which Model is Best: Comparing Methods and Metrics for Automatic Laughter Detection in a Naturalistic Conversational Dataset,Automatic Analysis of Paralinguistics,2022,"Laughter is a common paralinguistic vocalization that has been shown to be used for controlling the flow of a conversation, nullifying previous statements, and managing conversations on delicate topics. Already there have been concerted efforts to develop methods for automatically detecting laughter in speech. Many of these studies use artificial datasets and report their model performance using the AUC metric. This paper replicates previous work on laughter detection on those artificial datasets and then extends them by validating the methods on a larger and more naturalistic dataset made up of 60 spontaneous conversations (120 speakers and roughly 12 hours of material in total) with the best performing model achieving an AUC of 90.39\\5 +/- 1.10 (precision=13.99 +/- 4.09, recall=76.36 +/- 12.00, F1=23.06 +/- 4.99). The paper then goes on to discuss the shortcomings with the current standard comparison metric in the field of AUC and suggests alternatives which may aid in the comparison and understanding of method's effectiveness.",True
zhu22_interspeech,https://www.isca-archive.org/interspeech_2022/zhu22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/zhu22_interspeech.html,ByT5 model for massively multilingual grapheme-to-phoneme conversion,"Speech Synthesis: Linguistic Processing, Paradigms and Other Topics II",2022,"In this study, we tackle massively multilingual grapheme-to-phoneme conversion through implementing G2P models based on ByT5. We have curated a G2P dataset from various sources that covers around 100 languages and trained large-scale multilingual G2P models based on ByT5. We found that ByT5 operating on byte-level inputs significantly outperformed the token-based mT5 model in terms of multilingual G2P. Pairwise comparison with monolingual models in these languages suggests that multilingual ByT5 models generally lower the phone error rate by jointly learning from a variety of languages. The pretrained model can further benefit low resource G2P through zero-shot prediction on unseen languages or provides pretrained weights for finetuning, which helps the model converge to a lower phone error rate than randomly initialized weights. To facilitate future research on multilingual G2P, we make available our code and pretrained multilingual G2P models at: https://github.com/lingjzhu/CharsiuG2P.",True
barker22_interspeech,https://www.isca-archive.org/interspeech_2022/barker22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/barker22_interspeech.html,The 1st Clarity Prediction Challenge: A machine learning challenge for hearing aid intelligibility prediction,Speech Intelligibility Prediction for Hearing-Impaired Listeners I,2022,"This paper reports on the design and outcomes of the 1st Clarity Prediction Challenge (CPC1) for predicting the intelligibility of hearing aid processed signals heard by individuals with a hearing impairment. The challenge was designed to promote the development of new intelligibility measures suitable for use in developing hearing aid algorithms. Participants were supplied with listening test data compromising 7233 responses from 27 individuals. Data was split between training and test sets in a manner that fostered a machine learning approach and allowed both closed-set (known listeners) and open-set (unseen listener/unseen system) evaluation. The paper provides a description of the challenge design including the datasets, the hearing aid algorithms applied, the listeners and the perceptual tests. The challenge attracted submissions from 15 systems. The results are reviewed and the paper summarises, compares and contrasts approaches.",True
kim22h_interspeech,https://www.isca-archive.org/interspeech_2022/kim22h_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/kim22h_interspeech.html,Dummy Prototypical Networks for Few-Shot Open-Set Keyword Spotting,Speech Segmentation I,2022,"Keyword spotting is the task of detecting a keyword in streaming audio. Conventional keyword spotting targets predefined keywords classification, but there is growing attention in few-shot (query-by-example) keyword spotting, e.g., N-way classification given M-shot support samples. Moreover, in real-world scenarios, there can be utterances from unexpected categories (open-set) which need to be rejected rather than classified as one of the N classes. Combining the two needs, we tackle few-shot open-set keyword spotting with a new benchmark setting, named splitGSC. We propose episode-known dummy prototypes based on metric learning to detect an open-set better and introduce a simple and powerful approach, Dummy Prototypical Networks (D-ProtoNets). Our D-ProtoNets shows clear margins compared to recent few-shot open-set recognition (FSOSR) approaches in the suggested splitGSC. We also verify our method on a standard benchmark, miniImageNet, and D-ProtoNets shows the state-of-the-art open-set detection rate in FSOSR.",True
kang22d_interspeech,https://www.isca-archive.org/interspeech_2022/kang22d_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/kang22d_interspeech.html,SpeechEQ: Speech Emotion Recognition based on Multi-scale Unified Datasets and Multitask Learning,Speech Emotion Recognition II,2022,"Speech emotion recognition (SER) has many challenges, but one of the main challenges is that each framework does not have a unified standard. In this paper, we propose SpeechEQ, a framework for unifying SER tasks based on a multi-scale unified metric. This metric can be trained by Multitask Learning (MTL), which includes two emotion recognition tasks of Emotion States Category (EIS) and Emotion Intensity Scale (EIS), and two auxiliary tasks of phoneme recognition and gender recognition. For this framework, we build a Mandarin SER dataset - SpeechEQ Dataset (SEQD). We conducted experiments on the public CASIA and ESD datasets in Mandarin, which exhibit that our method outperforms baseline methods by a relatively large margin, yielding 8.0% and 6.5% improvement in accuracy respectively. Additional experiments on IEMOCAP with four emotion categories (i.e., angry, happy, sad, and neutral) also show the proposed method achieves a state-of-the-art of both weighted accuracy (WA) of 78.16% and unweighted accuracy (UA) of 77.47%.",True
rajan22_interspeech,https://www.isca-archive.org/interspeech_2022/rajan22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/rajan22_interspeech.html,Oktoechos Classification in Liturgical Music Using SBU-LSTM/GRU,Acoustic Signal Representation and Analysis II,2022,"A distinguishing feature of the music repertoire of the Syrian tradition is the system of classifying melodies into eight tunes, called 'oktoe\\={c}hos'. It inspired many traditions, such as Greek and Indian liturgical music. In oktoe\\={c}hos tradition, liturgical hymns are sung in eight modes or eight colours (known as eight 'niram', regionally). In this paper, the automatic oktoe\\={c}hos genre classification is addressed using musical texture features (MTF), i-vectors and Mel-spectrograms through stacked bidirectional and unidirectional long-short term memory (SBU-LSTM) and GRU (SB-GRU) architectures. The performance of the proposed approaches is evaluated using a newly created corpus of liturgical music in Malayalam. SBU-LSTM and SB-GRU frameworks report average classification accuracy of 88.19\\% and 87.50\\%, with a significant margin over other frameworks. The experiments demonstrate the potential of stacked architectures in learning temporal information from MTF for the proposed task.",True
pandey22b_interspeech,https://www.isca-archive.org/interspeech_2022/pandey22b_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/pandey22b_interspeech.html,Production characteristics of obstruents in WaveNet and older TTS systems,"Speech Synthesis: Tools, Data, and Evaluation",2022,"Segmental properties of Text-to-Speech (TTS) synthesizers have been studied for their influence on various perceived attributes of synthesized speech. However, they have received very limited attention for modern, neural vocoder-based TTS. In this paper, we compare segmental properties of WaveNET vocoder voices with a natural voice, and the best-performing non-neural synthesizers of the 2013 Blizzard Challenge. We extended the 2013 dataset with two new voices generated using a WaveNET vocoder. Acoustic-phonetic features of obstruent consonants and their neighbouring vowels were compared between the natural voice and each of these TTS systems. Statistical analysis was conducted using the Kruskal-Wallis test, and Dunn's test. Compared to the reference natural voice, we find that the WaveNET vocoder performs very well in modelling vowels, but features like F0 at onset and spectral tilt show significant deviations from the natural voice. Among consonants, neural voices deviate most from natural in the context of voiceless fricatives. Compared to other TTS systems, several features (like vowel dispersions, and consonant duration) which had shown strong deviations from natural, were found to not differ from natural in the WaveNET vocoder systems.",True
suzuki22_interspeech,https://www.isca-archive.org/interspeech_2022/suzuki22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/suzuki22_interspeech.html,Representing 'how you say' with 'what you say': English corpus of focused speech and text reflecting corresponding implications,Emotional Speech Production and Perception,2022,"In speech communication, how something is said (paralinguistic information) is as crucial as what is said (linguistic information). As a type of paralinguistic information, English speech uses sentence stress, the heaviest prominence within a sentence, to convey emphasis. While different placements of sentence stress communicate different emphatic implications, current speech translation systems return the same translations if the utterances are linguistically identical, losing paralinguistic information. Concentrating on focus, a type of emphasis, we propose mapping paralinguistic information into the linguistic domain within the source language using lexical and grammatical devices. This method enables us to translate the paraphrased text representations instead of the transcription of the original speech and obtain translations that preserve paralinguistic information. As a first step, we present the collection of an English corpus containing speech that differed in the placement of focus along with the corresponding text, which was designed to reflect the implied meaning of the speech. Also, analyses of our corpus demonstrated that mapping of focus from the paralinguistic domain into the linguistic domain involved various lexical and grammatical methods. The data and insights from our analysis will further advance research into paralinguistic translation. The corpus will be published via LDC and our website.",True
kawano22_interspeech,https://www.isca-archive.org/interspeech_2022/kawano22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/kawano22_interspeech.html,Multimodal Persuasive Dialogue Corpus using Teleoperated Android,Speaking Styles and Interaction Styles I,2022,"We collected a corpus of persuasive dialogues containing multimodal information for building a persuasive dialogue system that encourages users to change their behaviors using multimodal information. The corpus is constructed with an android robot that was remotely controlled by the WoZ method during user interactions with the system. We transcribed the collected speech and annotated dialogue act labels. We also extracted the facial features of the dialogue participants. Pre- and post-questionnaires identified the subjects' personality, their awareness of the target domain of persuasion, the changes in their awareness before/after the persuasion, and whether they agreed to the persuasion during the dialogues. In addition, we conducted a follow-up survey with each subject to investigate whether the persuasion actually affected their behavioral change. Moreover, we built linear classifiers that predict persuasion success to investigate effective features.",True
wallbridge22_interspeech,https://www.isca-archive.org/interspeech_2022/wallbridge22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/wallbridge22_interspeech.html,Investigating perception of spoken dialogue acceptability through surprisal,Spoken Dialogue Systems,2022,"Surprisal is used throughout computational psycholinguistics to model a range of language processing behaviour. There is growing evidence that language model (LM) estimates of surprisal correlate with human performance on a range of written language comprehension tasks. Although communicative interaction is arguably the primary form of language use, most studies of surprisal are based on monological, written data. Towards the goal of understanding perception in spontaneous, natural language, we present an exploratory investigation into whether the relationship between human comprehension behaviour and LM-estimated surprisal holds when applied to dialogue, considering both written dialogue, and the lexical component of spoken dialogue. We use a novel judgement task of dialogue utterance acceptability to ask two questions: ""How well can people make predictions about written dialogue and transcripts of spoken dialogue?â and ""Does surprisal correlate with these acceptability judgements?â. We demonstrate that people can make accurate predictions about upcoming dialogue and that their ability differs between spoken transcripts and written conversation. We investigate the relationship between global and local operationalisations of surprisal and human acceptability judgements, finding a combination of both to provide the most predictive power",True
muller22_interspeech,https://www.isca-archive.org/interspeech_2022/muller22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/muller22_interspeech.html,Does Audio Deepfake Detection Generalize?,Privacy and Security in Speech Communication,2022,"Current text-to-speech algorithms produce realistic fakes of human voices, making deepfake detection a much-needed area of research. While researchers have presented various techniques for detecting audio spoofs, it is often unclear exactly why these architectures are successful: Preprocessing steps, hyperparameter settings, and the degree of fine-tuning are not consistent across related work. Which factors contribute to success, and which are accidental? In this work, we address this problem: We systematize audio spoofing detection by re-implementing and uniformly evaluating architectures from related work. We identify overarching features for successful audio deepfake detection, such as using cqtspec or logspec features instead of melspec features, which improves performance by 37% EER on average, all other factors constant. Additionally, we evaluate generalization capabilities: We collect and publish a new dataset consisting of 37.9 hours of found audio recordings of celebrities and politicians, of which 17.2 hours are deepfakes. We find that related work performs poorly on such real-world data (performance degradation of up to one thousand percent). This may suggest that the community has tailored its solutions too closely to the prevailing ASVSpoof benchmark and that deepfakes are much harder to detect outside the lab than previously thought.",True
wang22c_interspeech,https://www.isca-archive.org/interspeech_2022/wang22c_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/wang22c_interspeech.html,Disentangling the Impacts of Language and Channel Variability on Speech Separation Networks,Source Separation III,2022,"Because the performance of speech separation is excellent for speech in which two speakers completely overlap, research attention has been shifted to dealing with more realistic scenarios. However, domain mismatch between training/test situations due to factors, such as speaker, content, channel, and environment, remains a severe problem for speech separation. Speaker and environment mismatches have been studied in the existing literature. Nevertheless, there are few studies on speech content and channel mismatches. Moreover, the impacts of language and channel in these studies are mostly tangled. In this study, we create several datasets for various experiments. The results show that the impacts of different languages are small enough to be ignored compared to the impacts of different channels. In our experiments, training on data recorded by Android phones leads to the best generalizability. Moreover, we provide a new solution for channel mismatch by evaluating projection, where the channel similarity can be measured and used to effectively select additional training data to improve the performance of in-the-wild test data.",True
omahony22_interspeech,https://www.isca-archive.org/interspeech_2022/omahony22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/omahony22_interspeech.html,Combining conversational speech with read speech to improve prosody in Text-to-Speech synthesis,Speech Synthesis: Prosody Modeling,2022,"For isolated utterances, speech synthesis quality has improved immensely thanks to the use of sequence-to-sequence models. However, these models are generally trained on read speech and fail to generalise to unseen speaking styles. Recently, more research is focused on the synthesis of expressive and conversational speech. Conversational speech contains many prosodic phenomena that are not present in read speech. We would like to learn these prosodic patterns from data, but unfortunately, many large conversational corpora are unsuitable for speech synthesis due to low audio quality. We investigate whether a data mixing strategy can improve conversational prosody for a target voice based on monologue data from audiobooks by adding real conversational data from podcasts. We filter the podcast data to create a set of 26k question and answer pairs. We evaluate two FastPitch models: one trained on 20 hours of monologue speech from a single speaker, and another trained on 5 hours of monologue speech from that speaker plus 15 hours of questions and answers spoken by nearly 15k speakers. Results from three listening tests show that the second model generates more preferred question prosody.",True
lin22c_interspeech,https://www.isca-archive.org/interspeech_2022/lin22c_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/lin22c_interspeech.html,DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering,Spoken Language Processing I,2022,"Spoken Question Answering (SQA) is to find the answer from a spoken document given a question, which is crucial for personal assistants when replying to the queries from the users. Existing SQA methods all rely on Automatic Speech Recognition (ASR) transcripts. Not only does ASR need to be trained with massive annotated data that are time and cost-prohibitive to collect for low-resourced languages, but more importantly, very often the answers to the questions include name entities or out-of-vocabulary words that cannot be recognized correctly. Also, ASR aims to minimize recognition errors equally over all words, including many function words irrelevant to the SQA task. Therefore, SQA without ASR transcripts (textless) is always highly desired, although known to be very difficult. This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging unlabeled data for pre-training and fine-tuned by the SQA downstream task. The time intervals of spoken answers can be directly predicted from spoken documents. We also release a new SQA benchmark corpus, NMSQA, for data with more realistic scenarios. We empirically showed that DUAL yields results comparable to those obtained by cascading ASR and text QA model and robust to real-world data. Our code and model will be open-sourced.",True
kothare22_interspeech,https://www.isca-archive.org/interspeech_2022/kothare22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/kothare22_interspeech.html,Statistical and clinical utility of multimodal dialogue-based speech and facial metrics for Parkinson's disease assessment,Pathological Speech Assessment,2022,"We present a framework for characterising the statistical and clinical relevance of speech and facial metrics in Parkinson's disease (PD) extracted by a multimodal conversational platform. 38 people with PD (pPD) and 22 controls were recruited in an ongoing study and were asked to complete four interactive sessions, a week apart from each other. In each session, a virtual conversational agent, Tina, guided participants through a battery of standard tasks designed to elicit speech and facial behaviours. Speech and facial metrics were automatically extracted in real time, several of which showed statistically significant differences between pPD and controls. We explored which of these differences were greater than measurement error, a threshold defined as the minimally detectable change (MDC). Furthermore, we computed the minimal clinically important difference (MCID) with respect to the Communicative Participation Item Bank short form (CPIB-S) scale for these select metrics. Our results show that differences in metrics like duration and fundamental frequency (F0) of speech are captured beyond measurement error. We also discuss several confounding factors that need to be taken into consideration before making any clinical interpretation of changes in these metrics.",True
fara22_interspeech,https://www.isca-archive.org/interspeech_2022/fara22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/fara22_interspeech.html,Speech and the n-Back task as a lens into depression. How combining both may allow us to isolate different core symptoms of depression,Speech and Language in Health: From Remote Monitoring to Medical Conversations I,2022,"Embedded in any speech signal is a rich combination of cognitive, neuromuscular and physiological information. This richness makes speech a powerful signal in relation to a range of different health conditions, including major depressive disorders (MDD). One pivotal issue in speech-depression research is the assumption that depressive severity is the dominant measurable effect. However, given the heterogeneous clinical profile of MDD, it may actually be the case that speech alterations are more strongly associated with subsets of key depression symptoms. This paper presents strong evidence in support of this argument. First, we present a novel large, cross-sectional, multi-modal dataset collected at Thymia. We then present a set of machine learning experiments that demonstrate that combining speech with features from an n-Back working memory assessment improves classifier performance when predicting the popular eight-item Patient Health Questionnaire depression scale (PHQ-8). Finally, we present a set of experiments that highlight the association between different speech and n-Back markers at the PHQ-8 item level. Specifically, we observe that somatic and psychomotor symptoms are more strongly associated with n-Back performance scores, whilst the other items: anhedonia, depressed mood, change in appetite, feelings of worthlessness and trouble concentrating are more strongly associated with speech changes.",True
nguyen22d_interspeech,https://www.isca-archive.org/interspeech_2022/nguyen22d_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/nguyen22d_interspeech.html,Accent Conversion using Pre-trained Model and Synthesized Data from Voice Conversion,Voice Conversion and Adaptation III,2022,"Accent conversion (AC) aims to generate synthetic audios by changing the pronunciation pattern and prosody of source speakers (in source audios) while preserving voice quality and linguistic content. There has not been a parallel corpus that contains pairs of audios having the same contents yet coming from the same speakers in different accents, the authors hence work on a solution to synthesize one as training input. The training pipeline is conducted via two steps. First, a voice conversion (VC) model is constructed to synthesize a training data set, containing pairs of audios in the same voice but two different accents. Second, an AC model is trained with the synthesized data to convert a source accented speech to a target accented speech. Given the recognized success of self-supervised learning speech representation (wav2vec 2.0) on certain speech problems such as VC, speech recognition, speech translation, and speech-to-speech translation, we adopt this architecture with some customization to train the AC model in the second step. With just 9-hour synthesized training data, the encoder initialized by the weight of the pre-trained wav2vec 2.0 model outperforms the LSTM-based encoder.",True
benway22_interspeech,https://www.isca-archive.org/interspeech_2022/benway22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/benway22_interspeech.html,PERCEPT-R: An Open-Access American English Child/Clinical Speech Corpus Specialized for the Audio Classification of /É¹/,Pathological Speech Assessment,2022,"We present the PERCEPT-R corpus, a labeled corpus of child speakers of American English with typical speech and residual speech sound disorders affecting rhotics. We demonstrate the utility of age-and-gender normalized formants extracted from PERCEPT-R in training support vector classifiers to predict ground-truth perceptual judgments of ""rhoticâ (i.e., dialect-typical) and ""derhoticâ phones for novel speakers (mean of participant-specific f-metrics = .83; SD = .18, N = 281).",True
takamichi22_interspeech,https://www.isca-archive.org/interspeech_2022/takamichi22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/takamichi22_interspeech.html,J-MAC: Japanese multi-speaker audiobook corpus for speech synthesis,"Speech Synthesis: Tools, Data, and Evaluation",2022,"In this paper, we construct a Japanese audiobook speech corpus called ``J-MAC'' for speech synthesis research. With the success of reading-style speech synthesis, the research target is shifting to tasks that use complicated contexts. Audiobook speech synthesis is a good example that requires cross-sentence, expressiveness, etc. Unlike reading-style speech, speaker-specific expressiveness in audiobook speech also becomes the context. To enhance this research, we propose a method of constructing a corpus from audiobooks read by professional speakers. From many audiobooks and their texts, our method can automatically extract and refine the data without any language dependency. Specifically, we use vocal-instrumental separation to extract clean data, connectionist temporal classification to roughly align text and audio, and voice activity detection to refine the alignment. J-MAC is open-sourced in our project page. We also conduct audiobook speech synthesis evaluations, and the results give insights into audiobook speech synthesis.",True
lee22k_interspeech,https://www.isca-archive.org/interspeech_2022/lee22k_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/lee22k_interspeech.html,A Study on the Phonetic Inventory Development of Children with Cochlear Implants for 5 Years after Implantation,Pathological Speech Assessment,2022,"This paper investigates longitudinal phonetic inventories of vowels and consonants of Korean-speaking children with cochlear implants (CIs). They are based on speech data of 7 children with CI over 5 years PI to examine the entire speech production development. Phones produced at least twice by more than 50% children in spontaneous and imitation speech from 6 months to 5 years post-implantation (PI) are compiled in the inventories. The results show and differences and similarities between children with CI and with normal hearing (NH). The vowel and consonant inventories at 6 months PI are larger than those of NH children at 1 year of age whose hearing experience is longer, including liquid [É¾] and fricative [s]. It can be attributed to biological maturation of CI children. As in children with NH, there is an explosive increase in phonetic inventories during a year after 1-year of robust hearing experience and the inventories are almost complete after 3 years of PI. Phonetic inventories at each time are expected to be references to assess the developmental appropriateness in speech production and guides to direct habilitation goals.",True
meneses22_interspeech,https://www.isca-archive.org/interspeech_2022/meneses22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/meneses22_interspeech.html,SiDi KWS: A Large-Scale Multilingual Dataset for Keyword Spotting,Speech Segmentation I,2022,"Keyword spotting (KWS) has become a hot topic in speech processing due to the rise of commercial applications based on voice command detection, such as voice assistants. Like tasks in computer vision, natural language processing, and even speech processing, most current successful approaches for KWS rely on deep learning. However, differently from all those tasks, there is a lack of large-scale datasets designed for training and evaluating deep learning models for KWS. The current work presents SiDi KWS, a public large-scale multilingual dataset currently composed of 24.3 million audio recordings of labeled single-spoken keywords. It intends to boost the development of new KWS systems, especially those based on deep learning. That dataset has been created by applying automatic forced alignment on public datasets of transcribed speech. This work introduces SiDi KWS and KeywordMiner, an open-source framework used to generate that dataset, to benefit the speech processing research community.",True
pereztoro22_interspeech,https://www.isca-archive.org/interspeech_2022/pereztoro22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/pereztoro22_interspeech.html,Alzheimer's Detection from English to Spanish Using Acoustic and Linguistic Embeddings,Speech and Language in Health: From Remote Monitoring to Medical Conversations II,2022,"Cross-lingual approaches are growing in popularity in the machine learning domain, where large amounts of data are required to obtain better generalizations. Moreover, one of the biggest problems is the availability of clinical speech data, where most of the resources are in English. For instance, not many available Alzheimer's Disease (AD) corpora in different languages can be found in the literature. Despite the phonological and phonemic differences between Spanish and English, fortunately, there are also similarities between these two languages, e.g., around 40% of all words in English have a related word in Spanish. In this work, we want to investigate the feasibility of combining information from English and Spanish languages to discriminate AD. Two datasets were considered: part of the Pitt Corpus, which is composed of English speakers, and a Spanish AD dataset composed of speakers from Chile. We based our analysis on known acoustic (Wav2Vec) and word (BERT, RoBERTa) embeddings using different classifiers. Strong language dependencies were found, even using multilingual representations. We observed that linguistic information was more important for classifying English AD (F-Score=0.76) and acoustic for Spanish AD (F-Score=0.80). Using knowledge transferred from English to Spanish achieved F-scores of up to 0.85 for discriminating AD.",True
conneau22_interspeech,https://www.isca-archive.org/interspeech_2022/conneau22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/conneau22_interspeech.html,XTREME-S: Evaluating Cross-lingual Speech Representations,Spoken Language Processing III,2022,"We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual speech representations in many languages. XTREME-S covers four task families: speech recognition, classification, speech-to-text translation and retrieval. Covering 102 languages from 10+ language families, 3 different domains and 4 task families, XTREME-S aims to simplify multilingual speech representation evaluation, as well as catalyze research in ""universal"" speech representation learning. This paper describes the new benchmark and establishes the first speech-only and speech-text baselines using XLS-R and mSLAM on all downstream tasks. We motivate the design choices and detail how to use the benchmark. Datasets and fine-tuning scripts are made easily accessible through the HuggingFace platform (https://hf.co/datasets/google/xtreme_s).",True
yang23r_interspeech,https://www.isca-archive.org/interspeech_2023/yang23r_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/yang23r_interspeech.html,Auditory Attention Detection in Real-Life Scenarios Using Common Spatial Patterns from EEG,Biosignal-enabled Spoken Communication,2023,"Auditory attention detection (AAD) methods based on electroencephalography (EEG) could be used in neuro-steered hearing devices to help hearing-loss people improve their hearing ability. However, previous studies have mostly obtained EEG data in laboratory settings which limits the practical application of neuro-steered hearing devices. In this study, we employ a common spatial pattern (CSP) algorithm to perform AAD using EEG signals collected by a wireless mobile EEG system, from real-life scenarios when people are walking and sitting. The results show that the CSP method can achieve AAD accuracy between 81.3% and 87.5% when using different decision windows (1 s- 30 s), which is better than previous methods based on linear mapping methods and convolutional neural networks (CNN). This proves that the CSP algorithm can decode people's attention efficiently even outside the laboratory. Analysis of EEG frequency bands shows that the Î´ and Î² bands have high activity in attention tasks.",True
avila23_interspeech,https://www.isca-archive.org/interspeech_2023/avila23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/avila23_interspeech.html,Towards Cross-Language Prosody Transfer for Dialog,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 2",2023,"Speech-to-speech translation systems today do not adequately support use for dialog purposes. In particular, nuances of speaker intent and stance can be lost due to improper prosody transfer. We present an exploration of what needs to be done to overcome this. First, we developed a data collection protocol in which bilingual speakers re-enact utterances from an earlier conversation in their other language, and used this to collect an English-Spanish corpus, so far comprising 1871 matched utterance pairs. Second, we developed a simple prosodic dissimilarity metric based on Euclidean distance over a broad set of prosodic features. We then used these to investigate cross-language prosodic differences, measure the likely utility of three simple baseline models, and identify phenomena which will require more powerful modeling. Our findings should inform future research on cross-language prosody and the design of speech-to-speech translation systems capable of effective prosody transfer.",True
pham23b_interspeech,https://www.isca-archive.org/interspeech_2023/pham23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/pham23b_interspeech.html,Vietnam-Celeb: a large-scale dataset for Vietnamese speaker recognition,Speaker and Language Identification 1,2023,"The success of speaker recognition systems heavily depends on large training datasets collected under real-world conditions. While common languages like English or Chinese have vastly available datasets, low-resource ones like Vietnamese remain limited. This paper presents a large-scale spontaneous dataset gathered under noisy environments, with over 87,000 utterances from 1,000 Vietnamese speakers of many professions, covering 3 main Vietnamese dialects. To build the dataset, we propose a sophisticated construction pipeline that can also be applied to other languages, with efficient visual-aided processing techniques to boost data precision. With the state-of-the-art x-vector model, training with the proposed dataset shows an average absolute and relative EER improvement of 5.48% and 41.61% when compared to the model trained on VLSP 2021, a publicly available Vietnamese speaker dataset.",True
sikasote23_interspeech,https://www.isca-archive.org/interspeech_2023/sikasote23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/sikasote23_interspeech.html,Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 3",2023,"This work introduces Zambezi Voice, an open-source multilingual speech resource for Zambian languages. It contains two collections of datasets: unlabelled audio recordings of radio news and talk shows programs (160 hours) and labelled data (over 80 hours) consisting of read speech recorded from text sourced from publicly available literature books. The dataset is created for speech recognition but can be extended to multilingual speech processing research for both supervised and unsupervised learning approaches. To our knowledge, this is the first multilingual speech dataset created for Zambian languages. We exploit pretraining and cross-lingual transfer learning by finetuning the Wav2Vec2.0 large-scale multilingual pretrained model to build end-to-end (E2E) speech recognition models for our baseline models. The dataset is released publicly under a Creative Commons BY-NC-ND 4.0 license and can be accessed through the project repository.",True
amiriparian23_interspeech,https://www.isca-archive.org/interspeech_2023/amiriparian23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/amiriparian23_interspeech.html,Speech-Based Classification of Defensive Communication: A Novel Dataset and Results,Spoken Dialog Systems and Conversational Analysis 2,2023,"Defensive communication is known to have detrimental effects on the quality of social interactions. Hence, recognising and reducing defensive behaviour is crucial to improving professional and personal communication. We introduce DefComm-DB, a novel multimodal dataset comprising video recordings in which one of the following types of defensive communication is present: (i) verbally attacking the conversation partner, (ii) withdrawing from the communication, (iii) making oneself greater, and (iv) making oneself smaller. Subsequently, we present a machine learning approach for the automatic classification of DefComm-DB. In particular, we utilise wav2vec2, autoencoders, a pre-trained CNN and openSMILE for feature extraction from the audio modality. For the text stream, we apply ELECTRA and SBERT. On the unseen test set, our models achieve an Unweighted Average Recall of 49.4 % and 52.2 % for the audio and text modalities, respectively, showing the feasibility of the introduced challenge.",True
wells23_interspeech,https://www.isca-archive.org/interspeech_2023/wells23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/wells23_interspeech.html,A Low-Resource Pipeline for Text-to-Speech from Found Data With Application to Scottish Gaelic,Speech Synthesis,2023,"In this work we present an end-to-end pipeline for building a speech corpus and text-to-speech synthesis system for a new language without reference to any expert-defined linguistic resources. We segment and align over 85 hours of Scottish Gaelic recordings found online and select 2- and 8-hour subsets with comprehensive coverage of speech sounds based on self-supervised discrete acoustic unit sequences. We then compare FastPitch models trained on these relatively small data sets using character, acoustic unit and phone inputs. According to native speaker listening test judgements, characters serve well for Gaelic given its regular orthography, even in these limited data scenarios. We release our corpus building recipe so that others may easily apply our work to new languages.",True
ashihara23_interspeech,https://www.isca-archive.org/interspeech_2023/ashihara23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/ashihara23_interspeech.html,SpeechGLUE: How Well Can Self-Supervised Speech Models Capture Linguistic Knowledge?,Analysis of Neural Speech Representations,2023,"Self-supervised learning (SSL) for speech representation has been successfully applied in various downstream tasks, such as speech and speaker recognition. More recently, speech SSL models have also been shown to be beneficial in advancing spoken language understanding tasks, implying that the SSL models have the potential to learn not only acoustic but also linguistic information. In this paper, we aim to clarify if speech SSL techniques can well capture linguistic knowledge. For this purpose, we introduce SpeechGLUE, a speech version of the General Language Understanding Evaluation (GLUE) benchmark. Since GLUE comprises a variety of natural language understanding tasks, SpeechGLUE can elucidate the degree of linguistic ability of speech SSL models. Experiments demonstrate that speech SSL models, although inferior to text-based SSL models, perform better than baselines, suggesting that they can acquire a certain amount of general linguistic knowledge from just unlabeled speech data.",True
saito23b_interspeech,https://www.isca-archive.org/interspeech_2023/saito23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/saito23b_interspeech.html,CALLS: Japanese Empathetic Dialogue Speech Corpus of Complaint Handling and Attentive Listening in Customer Center,Speech Synthesis: Multilinguality; Evaluation,2023,"We present CALLS, a Japanese speech corpus that considers phone calls in a customer center as a new domain of empathetic spoken dialogue. The existing STUDIES corpus covers only empathetic dialogue between a teacher and student in a school. To extend the application range of empathetic dialogue speech synthesis (EDSS), we designed our corpus to include the same female speaker as the STUDIES teacher, acting as an operator in simulated phone calls. We describe a corpus construction methodology and analyze the recorded speech. We also conduct EDSS experiments using the CALLS and STUDIES corpora to investigate the effect of domain differences. The results show that mixing the two corpora during training causes biased improvements in the quality of synthetic speech due to the different degrees of expressiveness. Our project page of the corpus is http://sython.org/Corpus/STUDIES-2.",True
kondratenko23_interspeech,https://www.isca-archive.org/interspeech_2023/kondratenko23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/kondratenko23_interspeech.html,Hybrid Dataset for Speech Emotion Recognition in Russian Language,Speech Emotion Recognition 3,2023,"We present a new data set for speech emotion recognition (SER) tasks called Dusha. The corpus contains approximately 350 hours of data, more than 300 000 audio recordings of Russian speech, and their transcripts. Therefore it is the biggest open bi-modal data collection with an open license for SER tasks nowadays. This data set is the first speech emotion corpus in Russian, including both crowd-sourced acted and real-life emotions from podcasts, with multiple speakers and scalable data set size. Acted subset has a more balanced class distribution than the unbalanced real-life part consisting of audio podcasts. So the first one is suitable for model pre-training, and the second is elaborated for fine-tuning purposes, model approbation, and validation. This paper describes in detail our collecting procedure, pre-processing routine, annotation, and experiment with a baseline model to demonstrate some actual metrics which could be obtained with the Dusha data set.",True
polacek23_interspeech,https://www.isca-archive.org/interspeech_2023/polacek23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/polacek23_interspeech.html,Online Punctuation Restoration using ELECTRA Model for streaming ASR Systems,Speech Recognition: Technologies and Systems for New Applications 1,2023,"In this work, we propose a lightweight online approach to automatic punctuation restoration (APR), which can be utilized in speech transcription systems for, e.g., live captioning TV or radio streams. It uses only text input without prosodic features and a fine-tuned ELECTRA-Small model with a two-layer classification head. It allows for restoring question marks, commas, and periods with a very short inference time and a low latency of just three words. Our APR scheme is first tuned and compared to other architectures on a set of manual TV news transcripts. The resulting system is then compared to another real-time APR module utilizing a recurrent network and a combination of text and acoustic features. The test data we use contains automatic transcripts of radio talks and TV debates; we are also publishing this data. The results show that our APR module performs better than the above-mentioned system and yields on the two test sets an average F1 of 71.2% and 69.4%, respectively.",True
chang23c_interspeech,https://www.isca-archive.org/interspeech_2023/chang23c_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/chang23c_interspeech.html,Multimodal Speech Recognition for Language-Guided Embodied Agents,Speech Recognition: Technologies and Systems for New Applications 3,2023,"Benchmarks for language-guided embodied agents typically assume text-based instructions, but deployed agents will encounter spoken instructions. While Automatic Speech Recognition (ASR) models can bridge the input gap, erroneous ASR transcripts can hurt the agentsâ ability to complete tasks. We propose training a multimodal ASR model that utilizes the accompanying visual context to reduce errors in spoken instruction transcripts. We train our model on a dataset of synthetic spoken instructions, derived from the ALFRED household task dataset, where we simulate acoustic noise by systematically masking spoken words. We find that utilizing visual observations facilitates masked word recovery, with multimodal ASR models recovering up to 30% more masked words than unimodal baselines. We also find that spoken instructions transcribed by multimodal ASR models result in higher task completion success rates for a language-guided embodied agent. github.com/Cylumn/embodied-multimodal-asr",True
sullivan23_interspeech,https://www.isca-archive.org/interspeech_2023/sullivan23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/sullivan23_interspeech.html,On the Robustness of Arabic Speech Dialect Identification,Speaker and Language Identification 3,2023,"Arabic dialect identification (ADI) tools are an important part of the large-scale data collection pipelines necessary for training speech recognition models. As these pipelines require application of ADI tools to potentially out-of-domain data, we aim to investigate how vulnerable the tools may be to this domain shift. With self-supervised learning (SSL) models as a starting point, we evaluate transfer learning and direct classification from SSL features. We undertake our evaluation under rich conditions, with a goal to develop ADI systems from pretrained models and ultimately evaluate performance on newly collected data. In order to understand what factors contribute to model decisions, we carry out a careful human study of a subset of our data. Our analysis confirms that domain shift is a major challenge for ADI models. We also find that while self-training does alleviate this challenges, it may be insufficient for realistic conditions.",True
ye23b_interspeech,https://www.isca-archive.org/interspeech_2023/ye23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/ye23b_interspeech.html,"GigaST: A 10,000-hour Pseudo Speech Translation Corpus","Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 2",2023,"This paper introduces GigaST, a large-scale pseudo speech-to-text translation (ST) corpus. We create the corpus by translating the transcript in GigaSpeech, an English ASR corpus, into German and Chinese. The training set is translated by a strong machine translation system and the test set is translated by human. ST models trained with an addition of our corpus obtain new state-of-the-art results on the MuST-C English-German benchmark test set. We provide a detailed description of the translation process and verify its quality. We make the translated text data public and hope to facilitate research in speech translation. Additionally, we also release the training scripts on NeurST1 to make it easy to replicate our systems. GigaST dataset is available at https://st-benchmark.github.io/resources/GigaST.",True
shetty23_interspeech,https://www.isca-archive.org/interspeech_2023/shetty23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/shetty23_interspeech.html,Developmental Articulatory and Acoustic Features for Six to Ten Year Old Children,Connecting Speech-science and Speech-technology for Children's Speech,2023,"In this paper, we study speech development in children using longitudinal acoustic and articulatory data. Data were collected yearly from grade 1 to grade 4 from four female and four male children. We analyze acoustic and articulatory properties of four corner vowels: /Ã¦/, /i/, /u/, and /É/, each occurring in two different words (different surrounding contexts). Acoustic features include formant frequencies and subglottal resonances (SGRs). Articulatory features include tongue curvature degree (TCD) and tongue curvature position (TCP). Based on the analyses, we observe the emergence of sex-based differences starting from grade 2. Similar to adults, the SGRs divide the vowel space into high, low, front, and back regions at least as early as grade 2. On average, TCD is correlated with vowel height and TCP with vowel frontness. Children in our study used varied articulatory configurations to achieve similar acoustic targets.",True
neumann23b_interspeech,https://www.isca-archive.org/interspeech_2023/neumann23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/neumann23b_interspeech.html,"A Multimodal Investigation of Speech, Text, Cognitive and Facial Video Features for Characterizing Depression With and Without Medication",DiGo - Dialog for Good: Speech and Language Technology for Social Good,2023,"Clinical depression is one of the most common mental disorders and technology for remote assessment of depression, including monitoring of treatment responses, is gaining more and more importance. Using a cloud-based multimodal dialog platform, we conducted a crowdsourced study to investigate the effect of depression severity and antidepressant use on various acoustic, linguistic, cognitive, and orofacial features. Our findings show that multiple features from all tested modalities show statistically significant differences between subjects with no or minimal depression and subjects with more severe depression symptoms. Moreover, certain acoustic and visual features show significant differences between subjects with moderately severe or severe symptoms who take antidepressants and those who do not take any. Machine learning experiments show that subjects with and without medication can be better discriminated from each other at higher severity levels.",True
ryumina23_interspeech,https://www.isca-archive.org/interspeech_2023/ryumina23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/ryumina23_interspeech.html,Multimodal Personality Traits Assessment (MuPTA) Corpus: The Impact of Spontaneous and Read Speech,Resources for Spoken Language Processing,2023,"Automatic personality traits assessment (PTA) provides high-level, intelligible predictive inputs for subsequent critical downstream tasks, such as job interview recommendations and mental healthcare monitoring. In this work, we introduce a novel Multimodal Personality Traits Assessment (MuPTA) corpus. Our MuPTA corpus is unique in that it contains both spontaneous and read speech collected in the midly-resourced Russian language. We present a novel audio-visual approach for PTA that is used in order to set up baseline results on this corpus. We further analyze the impact of both spontaneous and read speech types on the PTA predictive performance. We find that for the audio modality, the PTA predictive performances on short signals are almost equal regardless of the speech type, while PTA using video modality is more accurate with spontaneous speech compared to read one regardless of the signal length.",True
shi23g_interspeech,https://www.isca-archive.org/interspeech_2023/shi23g_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/shi23g_interspeech.html,ML-SUPERB: Multilingual Speech Universal PERformance Benchmark,"Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 2",2023,"Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.",True
lennes23_interspeech,https://www.isca-archive.org/interspeech_2023/lennes23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/lennes23_interspeech.html,Pitch distributions in a very large corpus of spontaneous Finnish speech,"Phonetics, Phonology, and Prosody 2",2023,"Speakers differ in the pitch range they use in their speech. In order to analyze the functional aspects of pitch, the typical pitch range of each individual is needed as reference. However, systematically collected pitch data from a sufficiently large corpus have not been previously available. We analyze the pitch distributions of individual speakers in a subset of the Donate Speech Corpus, collected from speakers of Finnish in 2020-2021. We report pitch analysis results based on samples from 8197 speakers and 1475 hours of speech. We compare the results obtained from male and female speakers in different age groups.",True
fu23b_interspeech,https://www.isca-archive.org/interspeech_2023/fu23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/fu23b_interspeech.html,OTF: Optimal Transport based Fusion of Supervised and Self-Supervised Learning Models for Automatic Speech Recognition,"Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 2",2023,"Self-Supervised Learning (SSL) Automatic Speech Recognition (ASR) models have shown great promise over Supervised Learning (SL) ones in low-resource settings. However, the advantages of SSL are gradually weakened when the amount of labeled data increases in many industrial applications. To further improve the ASR performance when abundant labels are available, we first explore the potential of combining SL and SSL ASR models via analyzing their complementarity in recognition accuracy and optimization property. Then, we propose a novel Optimal Transport based Fusion (OTF) method for SL and SSL models without incurring extra computation cost in inference. Specifically, optimal transport is adopted to softly align the layer-wise weights to unify the two different networks into a single one. Experimental results on the public 1k-hour English LibriSpeech dataset and our in-house 2.6k-hour Chinese dataset show that OTF largely outperforms the individual models with lower error rates.",True
kirkland23_interspeech,https://www.isca-archive.org/interspeech_2023/kirkland23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/kirkland23_interspeech.html,Pardon my disfluency: The impact of disfluency effects on the perception of speaker competence and confidence,Perception of Paralinguistics,2023,"Disfluencies are a hallmark of spontaneous speech and play an important role in conversation, yet have been shown to negatively impact judgments about speakers. We explored the role of disfluencies in the perception of competence, sincerity and confidence in public speaking contexts, using synthesized spontaneous speech. In one experiment, listeners rated 30-40-second clips which varied in terms of whether they contained filled pauses, as well as the number and types of repetition. Both the overall number of disfluencies and the repetition type had an impact on competence and confidence, and disfluent speech was also rated as less sincere. In the second experiment, the negative effects of repetition type on competence were attenuated when participants attributed disfluency to anxiety.",True
chen23i_interspeech,https://www.isca-archive.org/interspeech_2023/chen23i_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/chen23i_interspeech.html,Investigating the Utility of Synthetic Data for Doctor-Patient Conversation Summarization,Speech and Language in Health: From Remote Monitoring to Medical Conversations 2,2023,"Large-scale pre-training has been a successful strategy for training transformer models. However, maintaining a large clinical dataset for pre-training is not always possible, and access to data in this domain can be time-limited and costly. We explore using synthetic data in pre-training sequence-to-sequence (seq-to-seq) transformer models to generate clinical notes from Doctor-Patient-Conversations (DoPaCos). Using a generative language model fine-tuned on authentic conversations, a synthetic DoPaCo dataset was created and used with a corpus of clinical notes to pre-train a Longformer-Encoder-Decoder (LED) model. Results show that synthetic data leads to comparable performance in the downstream summarization task compared to pre-training with authentic data. Pre-training on synthetic conversations first, followed by clinical notes, yields higher performance across most of our evaluation metrics.",True
pudo23_interspeech,https://www.isca-archive.org/interspeech_2023/pudo23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/pudo23_interspeech.html,MOCKS 1.0: Multilingual Open Custom Keyword Spotting Testset,Resources for Spoken Language Processing,2023,"The main purpose of this work is to create a comprehensive audio testset that can be used to evaluate custom keyword spotting (KWS) models and to benchmark different KWS solutions. We also propose a set of requirements that should be followed while creating testsets to evaluate custom KWS models. We consider multiple versions of the problem: text and audio-based keyword spotting, as well as offline and online (streaming) modes. Our testset named MOCKS is based on LibriSpeech and Mozilla Common Voice datasets. We used automatically generated alignments to extract parts of the recordings, which were split into keywords and test samples. The resulting testset contains almost 50,000 keywords. It contains audio data in English, French, German, Italian, and Spanish, but can be easily extended to other languages. MOCKS has been made publicly available to the research community. Initial KWS experiments run on MOCKS suggest that it can serve as a challenging testset for future research.",True
xin23b_interspeech,https://www.isca-archive.org/interspeech_2023/xin23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/xin23b_interspeech.html,Laughter Synthesis using Pseudo Phonetic Tokens with a Large-scale In-the-wild Laughter Corpus,Speech Synthesis: Prosody and Emotion,2023,"We present a large-scale in-the-wild Japanese laughter corpus and a laughter synthesis method. Previous work on laughter synthesis lacks not only data but also proper ways to represent laughter. To solve these problems, we first propose an in-the-wild corpus comprising 3.5 hours of laughter, which is to our best knowledge the largest laughter corpus designed for laughter synthesis. We then propose pseudo phonetic tokens (PPTs) to represent laughter by a sequence of discrete tokens, which are obtained by training a clustering model on features extracted from laughter by a pretrained self-supervised model. Laughter can then be synthesized by feeding PPTs into a text-to-speech system. We further show PPTs can be used to train a language model for unconditional laughter generation. Results of comprehensive subjective and objective evaluations demonstrate that the proposed method significantly outperforms a baseline method, and can generate natural laughter unconditionally.",True
hedeshy23_interspeech,https://www.isca-archive.org/interspeech_2023/hedeshy23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/hedeshy23_interspeech.html,CNVVE: Dataset and Benchmark for Classifying Non-verbal Voice,"Speech, Voice, and Hearing Disorders 1",2023,"Non-verbal voice expressions (NVVEs) have been adopted as a means of human-computer interaction in research studies. However, exploring non-verbal voice-based interactions has been constrained by the limited availability of suitable training data and computational methods for classifying such expressions, leading to a focus on simple binary inputs. We address this issue with a new dataset containing 950 audio samples comprising 6 classes of voice expressions. The data were collected from 42 speakers who donated voice recordings. The classifier was trained on the data using features derived from mel-spectrograms. Furthermore, we studied the effectiveness of data augmentation and improved over the baseline model accuracy significantly with a test accuracy of 96.6% in a 5-fold cross-validation. We have made CNVVE publicly accessible in the hope that it will serve as a benchmark for future research.",True
tran23_interspeech,https://www.isca-archive.org/interspeech_2023/tran23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/tran23_interspeech.html,Personalization for Robust Voice Pathology Detection in Sound Waves,Speech and Language in Health: From Remote Monitoring to Medical Conversations 1,2023,"Automatic voice pathology detection is promising for non-invasive screening and early intervention using sound signals. Nevertheless, existing methods are susceptible to covariate shifts due to background noises, human voice variations, and data selection biases leading to severe performance degradation in real-world scenarios. Hence, we propose a non-invasive framework that contrastively learns personalization from sound waves as a pre-train and predicts latent-spaced profile features through semi-supervised learning. It allows all subjects from various distributions (e.g., regionality, gender, age) to benefit from personalized predictions for robust voice pathology in a privacy-fulfilled manner. We extensively evaluate the framework on four real-world respiratory illnesses datasets, including Coswara, COUGHVID, ICBHI and our private dataset - ASound, under multiple covariate shift settings (i.e., cross-dataset), improving up to 4.12% in overall performance.",True
cahyawijaya23_interspeech,https://www.isca-archive.org/interspeech_2023/cahyawijaya23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/cahyawijaya23_interspeech.html,Cross-Lingual Cross-Age Adaptation for Low-Resource Elderly Speech Emotion Recognition,Acoustic Model Adaptation for ASR,2023,"Speech emotion recognition plays a crucial role in human-computer interactions. However, most speech emotion recognition research is biased toward English-speaking adults, which hinders its applicability to other demographic groups in different languages and age groups. In this work, we analyze the transferability of emotion recognition across three different languages--English, Mandarin Chinese, and Cantonese; and 2 different age groups--adults and the elderly. To conduct the experiment, we develop an English-Mandarin speech emotion benchmark for adults and the elderly, BiMotion, and a Cantonese speech emotion dataset, YueMotion. This study concludes that different language and age groups require specific speech features, thus making cross-lingual inference an unsuitable method. However, cross-group data augmentation is still beneficial to regularize the model, with linguistic distance being a significant influence on cross-lingual transferability.",True
fu23_interspeech,https://www.isca-archive.org/interspeech_2023/fu23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/fu23_interspeech.html,Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring,Speech Recognition: Technologies and Systems for New Applications 2,2023,"Speech fluency/disfluency can be evaluated by analyzing a range of phonetic and prosodic features. Deep neural networks are commonly trained to map fluency-related features into the human scores. However, the effectiveness of deep learning-based models is constrained by the limited amount of labeled training samples. To address this, we introduce a self-supervised learning (SSL) approach that takes into account phonetic and prosody awareness for fluency scoring. Specifically, we first pre-train the model using a reconstruction loss function, by masking phones and their durations jointly on a large amount of unlabeled speech and text prompts. We then fine-tune the pre-trained model using human-annotated scoring data. Our experimental results, conducted on datasets such as Speechocean762 and our non-native datasets, show that our proposed method outperforms the baseline systems in terms of Pearson correlation coefficients (PCC). Moreover, we also conduct an ablation study to better understand the contribution of phonetic and prosody factors during the pre-training stage.",True
chivriga23_interspeech,https://www.isca-archive.org/interspeech_2023/chivriga23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/chivriga23_interspeech.html,Large Dataset Generation of Synchronized Music Audio and Lyrics at Scale using Teacher-Student Paradigm,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 1",2023,"Large models (e.g., GPT-3, CLIP, DALL-E) show remarkable few-shot and zero-shot capabilities when trained on hundreds of millions of samples. Despite this trend, no publicly available synchronized music audio and lyrics dataset of sufficient scale exists, nor does a reliable evaluation benchmark to assess a model's performance. To address this issue, we build and release MusicLyric, a large public dataset with over 320k audio sequences and lyrics pairs for a total duration of 1,200 hours based on a collection of over 32,000 songs. The generation process is based on the teacher-student paradigm where the student seeks to outclass the teacher with more data available using the newly generated pseudo-alignments. The method is efficient and straightforward with at least 3 iterations needed to create high-quality data that can be scaled to a hundred thousand samples. We make our dataset, toolkit, and pre-trained models open-source.",True
li23z_interspeech,https://www.isca-archive.org/interspeech_2023/li23z_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/li23z_interspeech.html,PunCantonese: A Benchmark Corpus for Low-Resource Cantonese Punctuation Restoration from Speech Transcripts,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 2",2023,"Punctuation restoration from unsegmented speech transcripts is an essential task to improve the readability of transcripts and can facilitate various downstream NLP tasks. However, there is still lack of systematic studies on punctuation restoration for Cantonese as a low-resource language. This paper introduces a new Cantonese punctuation corpus named PunCantonese, which consists of annotated spoken transcripts and written-style Wikipedia sentences, covering the major punctuations such as â,.?!â and code-switched sentences in Cantonese and English. We also propose a Transformer-based punctuation model which exploits pre-trained multilingual language models, adopts multitask learning for style and punctuation prediction, and introduces a novel Jyutping embedding layer to inject the
phonetic features not explicitly available in Cantonese characters. Experimental results show that these methods are effective in improving punctuation restoration, and the Jyutping embedding layer brings an absolute F1 increase by more than 2%.",True
obrien23_interspeech,https://www.isca-archive.org/interspeech_2023/obrien23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/obrien23_interspeech.html,Differentiating acoustic and physiological features in speech for hypoxia detection,"Speech, Voice, and Hearing Disorders 2",2023,"In order to stave off the effects of hypoxia, speech may become limited at elevated altitudes. This paper evaluates the role of speech on acoustic and physiological features used to detect hypoxia. Acoustic, cerebral blood oxygenation, and cardiac signals were recorded from participants who completed control and normobaric hypoxia experimental conditions. Acoustic and physiological features were extracted from (non-)speech segments via a voice activity detection method. Support Vector Machines were used to evaluate hypoxia classification using independent and combined features produced at sea-level and simulated 5 km altitudes. Models were built upon a 4-fold cross-validation design and evaluated on an independent dataset. Our results confirmed the importance of physiological features when detecting hypoxia. When combined, acoustic features boosted performance by 10% at 5 km in comparison to sea-level. Hypoxia detection may be improved by distinguishing respiration from non-speech.",True
archerboyd23_interspeech,https://www.isca-archive.org/interspeech_2023/archerboyd23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/archerboyd23_interspeech.html,Head movements in two- and four-person interactive conversational tasks in noisy and moderately reverberant conditions,"Speech Perception, Production, and Acquisition 2",2023,"Multi-modal processing schemes are of increasing importance for adaptive hearing devices. However, more data is required to understand interactions in complex application scenarios. In this study, the speech and head movements of eight normal-hearing participants were recorded in two- and four-person interactive conversational tasks, with and without 4-talker babble noise at 75 dB(A) and reverberation times of 0.25 and 0.6 s. Two-person conversations showed a head movement (yaw) interquartile range of 11.6Â° while four-person conversations showed a statistically significantly different interquartile range of 21.9Â°. No effect of acoustic condition was observed. The recorded data were also successfully used to test a previously published hearing-device direction of arrival estimation algorithm that utilized head movement information and correlation lag between acoustic signals from the left and right ear.",True
sarabia23_interspeech,https://www.isca-archive.org/interspeech_2023/sarabia23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/sarabia23_interspeech.html,Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning,Analysis of Speech and Audio Signals 3,2023,"We present Spatial LibriSpeech, a spatial audio dataset with over 650 hours of 19-channel audio, first-order ambisonics, and optional distractor noise. Spatial LibriSpeech is designed for machine learning model training, and it includes labels for source position, speaking direction, room acoustics and geometry. Spatial LibriSpeech is generated by augmenting LibriSpeech samples with 200k+ simulated acoustic conditions across 8k+ synthetic rooms. To demonstrate the utility of our dataset, we train models on four spatial audio tasks, resulting in a median absolute error of 6.60Â° on 3D source localization, 0.43m on distance, 90.66ms on T30, and 2.74dB on direct-to-reverberant ratio estimation. We show that the same models generalize well to widely-used evaluation datasets, e.g., obtaining a median absolute error of 12.43Â° on 3D source localization on TUT Sound Events 2018, and 157.32ms on T30 estimation on ACE Challenge.",True
miodonska23_interspeech,https://www.isca-archive.org/interspeech_2023/miodonska23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/miodonska23_interspeech.html,Are retroflex-to-dental sibilant substitutions in Polish children's speech an example of a covert contrast? A preliminary acoustic study,"Speech Perception, Production, and Acquisition 1",2023,The study aimed to investigate whether the dental substitutions of retroflex voiceless fricatives (/Ê/ to [s]) in Polish children's speech are an example of a covert contrast. We analyzed speech samples collected through a picture naming test from 11 children showing this retroflex-to-dental production pattern. The language material included words with /Ê/ and /s/ in diversified word positions. We extracted a set of spectrum-based acoustic features from the recorded sibilants and conducted the analysis using linear mixed-effect models. The models showed that significant acoustic differences (p < 0.05) can be found between realizations of /s/ and /Ê/ substituted by [s]. The main differences were detected in the amplitudes of fricative formants and the energy levels in specific subbands of the frication noise. The study provides preliminary evidence of the existence of covert contrasts in the analyzed substitutions.,True
stemmer23_interspeech,https://www.isca-archive.org/interspeech_2023/stemmer23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/stemmer23_interspeech.html,Detection of Emotional Hotspots in Meetings Using a Cross-Corpus Approach,Paralinguistics 1,2023,"Speech emotion recognition for natural human-to-human conversations has many useful applications, including generating comprehensive meeting transcripts or detecting communication problems. We investigate the detection of emotional hotspots, i.e., regions of increased speaker involvement in technical meetings. As there is a scarcity of annotated, not-acted corpora, and to avoid introducing unwanted biases to our models, we follow a cross-corpus approach where models are trained on data from domains unrelated to the test data. In this work we propose a model ensemble trained on spontaneous phone conversations, political discussions and acted emotions. Evaluation is performed on the natural ICSI and AMI meeting corpora, where we used existing hotspot annotations for ICSI and created labels for the AMI corpus. A semi-supervised fine-tuning procedure is introduced to adapt the model. We show that an equal error rate of below 21% can be achieved using the proposed cross-corpus approach.",True
picheny23_interspeech,https://www.isca-archive.org/interspeech_2023/picheny23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/picheny23_interspeech.html,The MALACH Corpus: Results with End-to-End Architectures and Pretraining,Multilingual Models for ASR,2023,"The MALACH corpus contains approximately 375 hours of Holocaust survivor testimonies along with transcripts (for approximately half the data) and audio. It is an extremely difficult corpus for speech recognition, encompassing accented, emotional speech, in many cases from elderly survivors. Nevertheless, significant progress has been made on speech recognition on MALACH with WERs now typically hovering at a 20% level for hybrid speech recognition systems. The purpose of this paper is to examine if recent developments in end-to-end architectures and pretraining with self-supervision continue to drive down performance as they do on popular read corpora such as Librispeech. We also experiment with leveraging the large fraction of unlabeled corpus data by extracting pseudolabels produced from previously trained systems. It is found that the best system - a fine-tuned wav2vec2 system trained on labeled and pseudolabeled data - achieves a WER of 13.5%, a huge gain in performance.",True
shekar23_interspeech,https://www.isca-archive.org/interspeech_2023/shekar23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/shekar23_interspeech.html,Speaker Tracking using Graph Attention Networks with Varying Duration Utterances across Multi-Channel Naturalistic Data: Fearless Steps Apollo-11 Audio Corpus,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 1",2023,"Speaker tracking in spontaneous naturalistic data continues to be a major research challenge, especially for short turn-taking communications. The NASA Apollo-11 space mission brought astronauts to the moon and back, where team based voice communications were captured. Building robust speaker classification models for this corpus has significant challenges due to variability of speaker turns, imbalanced speaker classes, and time-varying background noise/distortions. This study proposes a novel approach for speaker classification and tracking, utilizing a graph attention network framework that builds upon pretrained speaker embeddings. The modelâs robustness is evaluated on a number of speakers (10-140), achieving classification accuracy of 90.78% for 10 speakers, and 79.86% for 140 speakers. Furthermore, a secondary investigation focused on tracking speakers-of-interest(SoI) during mission critical phases, essentially serves as a lasting tribute to the 'Heroes Behind the Heroes'.",True
wang23o_interspeech,https://www.isca-archive.org/interspeech_2023/wang23o_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/wang23o_interspeech.html,MAVD: The First Open Large-Scale Mandarin Audio-Visual Dataset with Depth Information,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 2",2023,"Audio-visual speech recognition (AVSR) gains increasing attention from researchers as an important part of human-computer interaction. However, the existing available Mandarin audio-visual datasets are limited and lack the depth information. To address this issue, this work establishes the MAVD, a new large-scale Mandarin multimodal corpus comprising 12,484 utterances spoken by 64 native Chinese speakers. To ensure the dataset covers diverse real-world scenarios, a pipeline for cleaning and filtering the raw text material has been developed to create a well-balanced reading material. In particular, the latest data acquisition device of Microsoft, Azure Kinect is used to capture depth information in addition to the traditional audio signals and RGB images during data acquisition. We also provide a baseline experiment, which could be used to evaluate the effectiveness of the dataset. The dataset and code will be released at https://github.com/SpringHuo/MAVD.",True
suwanbandit23_interspeech,https://www.isca-archive.org/interspeech_2023/suwanbandit23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/suwanbandit23_interspeech.html,Thai Dialect Corpus and Transfer-based Curriculum Learning Investigation for Dialect Automatic Speech Recognition,Resources for Spoken Language Processing,2023,"We release 840 hours of read speech multi-dialect ASR corpora consisting of 700 hours of main Thai dialect, named Thai-central, and 40 hours for each local dialect , named Thai-dialect, with transcripts and their translations to Thai. The dialects, selected to represent different regions of Thailand, are Khummuang, Korat, and Pattani. We also release the baseline dialectal ASR models trained using the curriculum learning approach. We found that the pre-training with the high-resource main dialect and target dialect generally yields the best performance. We believe that the availability of our corpora would contribute to the problem of low-resource Thai dialects. The corpus data will be available on Github.",True
kodali23_interspeech,https://www.isca-archive.org/interspeech_2023/kodali23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/kodali23_interspeech.html,Classification of Vocal Intensity Category from Speech using the Wav2vec2 and Whisper Embeddings,Health-Related Speech Analysis,2023,"In speech communication, talkers regulate vocal intensity resulting in speech signals of different intensity categories (e.g., soft, loud). Intensity category carries important information about the speaker's health and emotions. However, many speech databases lack calibration information, and therefore sound pressure level cannot be measured from the recorded data. Machine learning, however, can be used in intensity category classification even though calibration information is not available. This study investigates pre-trained model embeddings (Wav2vec2 and Whisper) in classification of vocal intensity category (soft, normal, loud, and very loud) from speech signals expressed using arbitrary amplitude scales. We use a new database consisting of two speaking tasks (sentence and paragraph). Support vector machine is used as a classifier. Our results show that the pre-trained model embeddings outperformed three baseline features, providing improvements of up to 7%(absolute) in accuracy.",True
okamoto23_interspeech,https://www.isca-archive.org/interspeech_2023/okamoto23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/okamoto23_interspeech.html,CAPTDURE: Captioned Sound Dataset of Single Sources,Source Separation,2023,"In conventional studies on environmental sound separation and synthesis using captions, datasets consisting of multiple-source sounds with their captions were used for model training. However, when we collect the captions for multiple-source sound, it is not easy to collect detailed captions for each sound source, such as the number of sound occurrences and timbre. Therefore, it is difficult to extract only the single-source target sound by the model-training method using a conventional captioned sound dataset. In this work, we constructed a dataset with captions for a single-source sound named CAPTDURE, which can be used in various tasks such as environmental sound separation and synthesis. Our dataset consists of 1,044 sounds and 4,902 captions. We evaluated the performance of environmental sound extraction using our dataset. The experimental results show that the captions for single-source sounds are effective in extracting only the single-source target sound from the mixture sound.",True
rumberg23_interspeech,https://www.isca-archive.org/interspeech_2023/rumberg23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/rumberg23_interspeech.html,Uncertainty Estimation for Connectionist Temporal Classification Based Automatic Speech Recognition,Connecting Speech-science and Speech-technology for Children's Speech,2023,"Predictive uncertainty estimation of deep neural networks is important when their outputs are used for high stakes decision making. We investigate token-level uncertainty of connectionist temporal classification (CTC) based automatic speech recognition models. We propose an approach, which considers that not all changes at frame-level lead to a change at token-level after CTC decoding. The approach shows promising performance for prediction of recognition errors on TIMIT, Mozilla Common Voice (MCV) and kidsTALC, a corpus of children's speech, using two different model architectures, while introducing only negligible computational overhead. Our approach identifies over 80% of a wav2vec2.0 model's errors on MCV by selecting 10% of the tokens. We further show, that the predictive uncertainty estimate relates to the uncertainty of a human annotator, by re-annotating 500 utterances of kidsTALC.",True
xiang23_interspeech,https://www.isca-archive.org/interspeech_2023/xiang23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/xiang23_interspeech.html,eSTImate: A Real-time Speech Transmission Index Estimator With Speech Enhancement Auxiliary Task Using Self-Attention Feature Pyramid Network,Speech Coding: Privacy,2023,"The Speech Transmission Index (STI) is a crucial metric for evaluating speech intelligibility, but its standard measurement method is too complicated for real-time applications. Though recently proposed deep learning based STI estimation schemes can effectively address the problem, existing methods still fall short of covering all possible STI scenarios. This paper presents eSTImate: an end-to-end deep learning system for real-time STI blind estimation that integrates the tasks of STI estimation and speech enhancement through a feature pyramid auxiliary learning architecture and incorporates multi-head attention mechanisms. The proposed model demonstrates the performance of state-of-the-art, achieving a low mean absolute error of 0.016 and root mean square error of 0.021 on the constructed dataset that covers the whole range of STI, highlighting its potential to provide accurate and consistent real-time STI estimation across diverse real-world scenarios.",True
zhang23r_interspeech,https://www.isca-archive.org/interspeech_2023/zhang23r_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/zhang23r_interspeech.html,Real-Time Personalised Speech Enhancement Transformers with Dynamic Cross-attended Speaker Representations,Speech Coding and Enhancement 1,2023,"Personalised speech enhancement (PSE) extracts only the speech of a target user and removes everything else from corrupted input audio. This can greatly improve on-device streaming audio processing, such as voice calls and speech recognition, which has strict requirements on model size and latency. To focus the PSE system on the target speaker, it is conditioned on a recording of the user's voice. This recording is usually summarised as a single static vector. However, a static vector cannot reflect all the target user's voice characteristics. Thus, we propose using the full recording. To condition on such a variable-length sequence, we propose fully Transformer-based PSE models with a cross-attention mechanism which generates target speaker representations dynamically. To better reflect the on-device scenario, we carefully design and publish a new PSE dataset. On the dataset, our proposed model significantly surpasses strong baselines while halving the model size and reducing latency.",True
ghaffarzadegan23_interspeech,https://www.isca-archive.org/interspeech_2023/ghaffarzadegan23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/ghaffarzadegan23_interspeech.html,Active Learning for Abnormal Lung Sound Data Curation and Detection in Asthma,Speech and Language in Health: From Remote Monitoring to Medical Conversations 1,2023,"Existing audio-based asthma monitoring solutions rely on feature engineering designs paired with contact-based auscultation which are brittle in practice and do not scale beyond point of care setups. Data-driven methods utilizing contactless microphones have the potential to address such limitations. These solutions are under-explored in healthcare due to high cost of data curation requiring physicians-in-the-loop. Here, we propose an active learning (AL) system to facilitate audio data collection and annotation. It detects lung sound abnormalities in asthma. AL reduces the annotation cost while increasing the model performance under a constrained annotation budget. It automatically extracts interesting audio segments from the continuous recordings, and efficiently annotates and trains anomaly detector model. The experimental results confirm the effectiveness of the proposed system as an enabler for larger scale data curation on a newly collected audio corpus for pediatric asthma.",True
simantiraki23_interspeech,https://www.isca-archive.org/interspeech_2023/simantiraki23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/simantiraki23_interspeech.html,The effect of masking noise on listenersâ spectral tilt preferences,"Speech Perception, Production, and Acquisition 1",2023,"Speech enhancement algorithms often focus on optimising intelligibility while neglecting other aspects of speech such as naturalness, quality and listening effort which may affect a listener's experience. This paper investigates the impact of spectral tilt on listeners' preferences, using a new corpus of Greek utterances. Participants adjusted spectral tilt with real-time feedback to select their preferred tilt in quiet and in the presence of speech-shaped noise at eight signal-to-noise ratios. Listeners displayed distinct preferences, with a tendency to select flatter tilts with increasing noise. Preferences were not random even for constant intelligibility, indicating that their adjustments were influenced by factors beyond the need to maintain comprehensibility. These findings have the potential to inform the design of speech enhancement algorithms that jointly optimise intelligibility and a listener's overall experience.",True
gonzalezmachorro23_interspeech,https://www.isca-archive.org/interspeech_2023/gonzalezmachorro23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/gonzalezmachorro23_interspeech.html,Towards Supporting an Early Diagnosis of Multiple Sclerosis using Vocal Features,"Speech, Voice, and Hearing Disorders 1",2023,"Multiple sclerosis (MS) is a neuroinflammatory disease that affects millions of people worldwide. Since dysarthria is prominent in people with MS (pwMS), this paper aims to identify acoustic features that differ between people with MS and healthy controls (HC). Additionally, we develop automatic classification methods to distinguish between pwMS and HC. In this work, we present a new dataset of a German-speaking cohort which contains 39 patients with low disability of relapsing MS and 16 HC. Findings suggest that certain interpretable speech features could be useful in diagnosing MS, and that machine learning methods could potentially support fast and unobtrusive screening in clinical practice. The study emphasises the importance of analysing free speech compared to read speech.",True
richter23_interspeech,https://www.isca-archive.org/interspeech_2023/richter23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/richter23_interspeech.html,"Remote Assessment for ALS using Multimodal Dialog Agents: Data Quality, Feasibility and Task Compliance",Analysis of Speech and Audio Signals 4,2023,"We investigate the feasibility, task compliance and audiovisual data quality of a multimodal dialog-based solution for remote assessment of Amyotrophic Lateral Sclerosis (ALS). 53 people with ALS and 52 healthy controls interacted with Tina, a cloud-based conversational agent, in performing speech tasks designed to probe various aspects of motor speech function while their audio and video was recorded. We rated a total of 250 recordings for audio/video quality and participant task compliance, along with the relative frequency of different issues observed. We observed excellent compliance (98%) and audio (95.2%) and visual quality rates (84.8%), resulting in an overall yield of 80.8% recordings that were both compliant and of high quality. Furthermore, recording quality and compliance were not affected by level of speech severity and did not differ significantly across end devices. These findings support the utility of dialog systems for remote monitoring of speech in ALS.",True
yang23y_interspeech,https://www.isca-archive.org/interspeech_2023/yang23y_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/yang23y_interspeech.html,On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation,Spoken Term Detection and Voice Search,2023,"Large self-supervised models are effective feature extractors, but their application is challenging under on-device budget constraints and biased dataset collection, especially in keyword spotting. To address this, we proposed a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting. Our approach used a teacher-student framework to transfer knowledge from a larger, more complex model to a smaller, light-weight model using dual-view cross-correlation distillation and the teacher's codebook as learning objectives. We evaluated our model's performance on an Alexa keyword spotting detection task using a 16.6k-hour in-house dataset. Our technique showed exceptional performance in normal and noisy conditions, demonstrating the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting tasks while working within on-device resource constraints.",True
fan23_interspeech,https://www.isca-archive.org/interspeech_2023/fan23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/fan23_interspeech.html,Language-specific Boundary Learning for Improving Mandarin-English Code-switching Speech Recognition,"Speech Recognition: Architecture, Search, and Linguistic Components 3",2023,"Code-switching speech recognition (CSSR) transcribes speech that switches between multiple languages or dialects within a single sentence. The main challenge in this task is that different languages often have similar pronunciations, making it difficult for models to distinguish between them. In this paper, we propose a method for solving the CSSR task from the perspective of language-specific acoustic boundary learning. We introduce language-specific weight estimators (LSWE) to model acoustic boundary learning in different languages separately. Additionally, a non-autoregressive (NAR) decoder and a language change detection (LCD) module are employed to assist in training. Evaluated on the SEAME corpus, our method achieves a state-of-the-art mixed error rate (MER) of 16.29% and 22.81% on the testman and testsge sets. We also demonstrate the effectiveness of our method on a 9000-hour in-house meeting code-switching dataset, where our method achieves a relatively 7.9% MER reduction.",True
zhang23h_interspeech,https://www.isca-archive.org/interspeech_2023/zhang23h_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/zhang23h_interspeech.html,CVTE-Poly: A New Benchmark for Chinese Polyphone Disambiguation,Speech Synthesis: Multilinguality; Evaluation,2023,"Conversion from graphemes to phonemes is an essential component in Text-To-Speech systems, and in Chinese, one main challenge is polyphone disambiguation-to determine the pronunciation of characters with multiple pronunciations. In this task, the benchmark dataset Chinese Polyphone disambiguation with Pinyin (CPP) suffers from two main limitations: Firstly, it contains some wrong labels in contrast to the newest official dictionary. Secondly, it is imbalanced and hence models learned from it show a learning bias towards frequently-used pronunciations and polyphones. In this paper, we refine CPP and release a new dataset named CVTE-poly, containing 845254 samples, nearly ten times the size of CPP and is more balanced. Besides, we propose a comprehensive measurement for polyphone disambiguation task, against the data imbalance problem. Experiments show that our simple but flexible baseline trained on CVTE-poly outperforms existing models, which demonstrate the benefit of our dataset.",True
dasare23_interspeech,https://www.isca-archive.org/interspeech_2023/dasare23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/dasare23_interspeech.html,The Role of Formant and Excitation Source Features in Perceived Naturalness of Low Resource Tribal Language TTS: An Empirical Study,Speech Quality Assessment,2023,"Text-to-speech synthesis is a prominent area in the speechprocessing domain that has significant use in reading digital content in a given language. In the proposed work, we worked on two tribal languages of India viz., Lambani and Soliga, which are zero-resource languages. The study began with a dataset collection for both tribal languages. Secondly, a Text-To-Speech (TTS) system was built separately based on the transfer learning approach. To validate the voice quality of TTS-generated speech, subjective as well as objective evaluations were performed. As a part of objective analysis, the voice source and vocal tract filter properties of the synthetic speech have been explored. The extensive study on various aspects of speech, such as LP residual, F0 contour, and formants (F1 &  sF2) has shown interesting results that can correlate to the subjective listening test results. The link to the original and synthetic speech can be found online.",True
mateju23_interspeech,https://www.isca-archive.org/interspeech_2023/mateju23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/mateju23_interspeech.html,Combining Multilingual Resources and Models to Develop State-of-the-Art E2E ASR for Swedish,"Speech Recognition: Architecture, Search, and Linguistic Components 3",2023,"In terms of automatic speech recognition (ASR), Swedish belongs to the group of less-resourced languages, as publicly available training data is limited to a few hundred hours of mostly read speech. To acquire larger amounts of more realistic data, we investigate the existing multilingual approaches, and also propose two new ones, which combine Swedish with previously created Norwegian data and models. We use them for efficient automatic harvesting of spoken Swedish from broadcast, parliament, YouTube, and audiobook archives. The combined models significantly speed up the harvesting process and improve the final Swedish end-to-end (E2E) ASR system. We evaluate it on datasets covering various applications and domains; they provide performance better than the state-of-the-art commercial cloud services. We have made all of our test datasets publicly available for future comparative experiments.",True
faustini23_interspeech,https://www.isca-archive.org/interspeech_2023/faustini23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/faustini23_interspeech.html,Composing Spoken Hints for Follow-on Question Suggestion in Voice Assistants,Question Answering from Speech,2023,"The adoption of voice assistants like Alexa or Siri has grown rapidly, allowing users instant access to information via voice search. Query suggestion is a standard feature of screen-based search experiences, allowing users to explore additional topics. However, this is not trivial to implement in voice-based settings. To enable this, we tackle the novel task of suggesting uestions with compact and natural voice hints to allow users to ask follow-up questions. We first define the task of composing speech-based hints, ground it in syntactic theory, and outline linguistic desiderata for spoken hints. We propose a sequence-to-sequence approach to generate spoken hints from a list of questions. Using a new dataset of 6, 681 input questions and human written hints, we evaluate models with automatic metrics and human evaluation. Results show that a naive approach of concatenating suggested questions creates poor voice hints. Our most sophisticated approach applies a linguistically-motivated pretraining task and was strongly preferred by humans for producing the most natural hints.",True
zeng23_interspeech,https://www.isca-archive.org/interspeech_2023/zeng23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/zeng23_interspeech.html,Improving Generalization Ability of Countermeasures for New Mismatch Scenario by Combining Multiple Advanced Regularization Terms,Speaker and Language Identification 1,2023,"The ability of countermeasure models to generalize from seen speech synthesis methods to unseen ones has been investigated in the ASVspoof challenge. However, a new mismatch scenario in which fake audio may be generated from real audio with unseen genres has not been studied thoroughly. To this end, we first use five different vocoders to create a new dataset called CN-Spoof based on the CN-Celeb1&2 datasets. Then, we design two auxiliary objectives for regularization via meta-optimization and a genre alignment module, respectively, and combine them with the main anti-spoofing objective using learnable weights for multiple loss terms. The results on our cross-genre evaluation dataset for anti-spoofing show that the proposed method significantly improved the generalization ability of the countermeasures compared with the baseline system in the genre mismatch scenario.",True
mallolragolta23_interspeech,https://www.isca-archive.org/interspeech_2023/mallolragolta23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/mallolragolta23_interspeech.html,The MASCFLICHT Corpus: Face Mask Type and Coverage Area Recognition from Speech,Speech and Language in Health: From Remote Monitoring to Medical Conversations 2,2023,"We present a novel speech dataset for face mask type and coverage area recognition collected with a smartphone. The dataset contains 2h 27m 55s of data from 30 German speakers (15f, 15m). The baseline results exploit the functionals of the eGeMAPS feature set, the Mel-spectrogram, and the spectrogram representations of the audio samples. To model the one-dimensional features, we investigate Support Vector Classifiers (SVC) and a neural network classifier. We extract salient information from the two-dimensional representations with Convolutional Neural Network (CNN) based encoders, coupled with a classification block. We use the Unweighted Average Recall (UAR) as the evaluation metric. For the face mask type and the coverage area recognition tasks (3-class problems), the best models on the test partition score a UAR of 49.3% and 47.8%, respectively. For the face mask type and coverage area recognition task (5-class problem), the optimal model on the test partition obtains a UAR of 35.0%.",True
wu23i_interspeech,https://www.isca-archive.org/interspeech_2023/wu23i_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/wu23i_interspeech.html,Speaker Embeddings as Individuality Proxy for Voice Stress Detection,Paralinguistics 2,2023,"Since the mental states of the speaker modulate speech, stress introduced by cognitive or physical loads could be detected in the voice. The existing voice stress detection benchmark has shown that the audio embeddings extracted from the Hybrid BYOL-S self-supervised model perform well. However, the benchmark only evaluates performance separately on each dataset, but does not evaluate performance across the different types of stress and different languages. Moreover, previous studies found strong individual differences in stress susceptibility. This paper presents the design and development of voice stress detection, trained on more than 100 speakers from 9 language groups and five different types of stress. We address individual variabilities in voice stress analysis by adding speaker embeddings to the hybrid BYOL-S features. The proposed method significantly improves voice stress detection performance with an input audio length of only 3-5 seconds.",True
li23e_interspeech,https://www.isca-archive.org/interspeech_2023/li23e_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/li23e_interspeech.html,Towards Robust Family-Infant Audio Analysis Based on Unsupervised Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio,Paralinguistics 1,2023,"To perform automatic family audio analysis, past studies have collected recordings using phone, video, or audio-only recording device like LENA, investigated supervised learning methods, and used or fine-tuned general-purpose embeddings learned from large pretrained models. In this study, we advance the audio component of a new infant wearable multi-modal device called LittleBeats (LB) by learning family audio representation via wav2vec 2.0 (W2V2) pretraining. We show given a limited number of labeled LB home recordings, W2V2 pretrained using 1k-hour of unlabeled home recordings outperforms oracle W2V2 pretrained on 52k-hour unlabeled audio in terms of parent/infant speaker diarization (SD) and vocalization classifications (VC) at home. Extra relevant external unlabeled and labeled data further benefit W2V2 pretraining and fine-tuning. With SpecAug and environmental speech corruptions, we obtain 12% relative gain on SD and moderate boost on VC. Code and model weights are available.",True
deb23_interspeech,https://www.isca-archive.org/interspeech_2023/deb23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/deb23_interspeech.html,BeAts: Bengali Speech Acts Recognition using Multimodal Attention Fusion,Multi-modal Systems,2023,"Spoken languages often utilise intonation, rhythm, intensity, and structure, to communicate intention, which can be interpreted differently depending on the rhythm of speech of their utterance. These speech acts provide the foundation of communication and are unique in expression to the language. Recent advancements in attention-based models, demonstrating their ability to learn powerful representations from multilingual datasets, have performed well in speech tasks and are ideal to model specific tasks in low resource languages. Here, we develop a novel multimodal approach combining two models, wav2vec2.0 for audio and MarianMT for text translation, by using multimodal attention fusion to predict speech acts in our prepared Bengali speech corpus. We also show that our model BeAts (Bengali speech acts recognition using Multimodal Attention Fusion) significantly outperforms both the unimodal baseline using only speech data and a simpler bimodal fusion using both speech and text data.",True
ratko23_interspeech,https://www.isca-archive.org/interspeech_2023/ratko23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/ratko23_interspeech.html,Opening or Closing? An Electroglottographic Analysis of Voiceless Coda Consonants in Australian English,Phonetics and Phonology: Languages and Varieties,2023,"In voiceless sounds, the glottis may be spread or constricted. Glottal spreading is associated with breathiness, and constriction with glottalisation. In many dialects of English, glottalisation often occurs with coda /t/ and sometimes with /p, k/, suggesting coda stop voicelessness is achieved through glottal constriction. Conversely, voiceless coda fricatives are associated with breathiness of the preceding vowel, with voicelessness achieved through glottal spreading. However, analyses specifically measuring glottal activity in coda consonant contexts in English are sparse. We conducted an electroglottographic analysis of vowels preceding voiceless codas /p, t, k, s/ to examine how coda voicelessness is achieved in Australian English (AusE). We found that coda /t/ and /p/ show glottal constriction towards vowel offset. Conversely, /k/ patterns with /s/ and exhibits glottal spreading. This suggests that different glottal configurations are used to achieve coda voicelessness in AusE.",True
nam23_interspeech,https://www.isca-archive.org/interspeech_2023/nam23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/nam23_interspeech.html,Disentangled Representation Learning for Multilingual Speaker Recognition,Speaker and Language Identification 3,2023,"The goal of this paper is to learn robust speaker representation for bilingual speaking scenario. The majority of the world's population speak at least two languages; however, most speaker recognition systems fail to recognise the same speaker when speaking in different languages.  Popular speaker recognition evaluation sets do not consider the bilingual scenario, making it difficult to analyse the effect of bilingual speakers on speaker recognition performance. In this paper, we publish a large-scale evaluation set named VoxCeleb1-B derived from VoxCeleb that considers bilingual scenarios. We introduce an effective disentanglement learning strategy that combines adversarial and metric learning-based methods. This approach addresses the bilingual situation by disentangling language-related information from speaker representation while ensuring stable speaker representation learning. Our language-disentangled learning method only uses language pseudo-labels without manual information.",True
bujnowski23_interspeech,https://www.isca-archive.org/interspeech_2023/bujnowski23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/bujnowski23_interspeech.html,"""Select language, modality or put on a mask!"" Experiments with Multimodal Emotion Recognition",Show and Tell: Health applications and emotion recognition,2023,"We propose a system designed for multimodal emotion recognition. Our research focuses on showing the impact of various signals in the emotion recognition process. Apart from reporting the average results of our models, we would like to encourage individual engagement of conference participants and explore how a unique emotional scene recorded on the spot can be interpreted by the models - for individual modalities as well as their combinations. Our models work for English, German and Korean. We show the comparison of emotion recognition accuracy for these 3 languages, including the influence of each modality. Our second experiment explores emotion recognition for people wearing face masks. We show that the use of face masks affects not only the video signal but also audio and text. To our knowledge, no other study shows the effects of wearing a mask for three modalities. Unlike other studies where masks are added artificially, we use real recordings with actors in masks.",True
xiao23b_interspeech,https://www.isca-archive.org/interspeech_2023/xiao23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/xiao23b_interspeech.html,"A Snoring Sound Dataset for Body Position Recognition: Collection, Annotation, and Analysis",Analysis of Speech and Audio Signals 4,2023,"Obstructive Sleep Apnea-Hypopnea Syndrome (OSAHS) is a chronic breathing disorder caused by a blockage in the upper airways. Snoring is a prominent symptom of OSAHS, and previous studies have attempted to identify the obstruction site of the upper airways by snoring sounds. Despite some progress, the classification of the obstruction site remains challenging in real-world clinical settings due to the influence of sleep body position on upper airways. To address this challenge, this paper proposes a snore-based sleep body position recognition dataset (SSBPR) consisting of 7570 snoring recordings, which comprises six distinct labels for sleep body position: supine, supine but left lateral head, supine but right lateral head, left-side lying, right-side lying and prone. Experimental results show that snoring sounds exhibit certain acoustic features that enable their effective utilization for identifying body posture during sleep in real-world scenarios.",True
koudounas23_interspeech,https://www.isca-archive.org/interspeech_2023/koudounas23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/koudounas23_interspeech.html,ITALIC: An Italian Intent Classification Dataset,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 2",2023,"Recent large-scale Spoken Language Understanding datasets focus predominantly on English and do not account for language-specific phenomena such as particular phonemes or words in different lects. We introduce ITALIC, the first large-scale speech dataset designed for intent classification in Italian. The dataset comprises 16,521 crowdsourced audio samples recorded by 70 speakers from various Italian regions and annotated with intent labels and additional metadata. We explore the versatility of ITALIC by evaluating current state-of-the-art speech and text models. Results on intent classification suggest that increasing scale and running language adaptation yield better speech models, monolingual text models outscore multilingual ones, and that speech recognition on ITALIC is more challenging than on existing Italian benchmarks. We release both the dataset and the annotation scheme to streamline the development of new Italian SLU models and language-specific datasets.",True
lavechin23_interspeech,https://www.isca-archive.org/interspeech_2023/lavechin23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/lavechin23_interspeech.html,BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models,Connecting Speech-science and Speech-technology for Children's Speech,2023,"Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech.",True
gao23c_interspeech,https://www.isca-archive.org/interspeech_2023/gao23c_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/gao23c_interspeech.html,A Novel Interpretable and Generalizable Re-synchronization Model for Cued Speech based on a Multi-Cuer Corpus,Multi-modal Systems,2023,"Cued Speech (CS) is a multi-modal visual coding system combining lip reading with several hand cues at the phonetic level to make the spoken language visible to the hearing impaired. Previous studies solved asynchronous problems between lip and hand movements by a cuer-dependent piecewise linear model for English and French CS. In this work, we innovatively propose three statistical measure on the lip stream to build an interpretable and generalizable model for predicting hand preceding time (HPT), which achieves cuer-independent by a proper normalization. Particularly, we build the first Mandarin CS corpus comprising annotated videos from five speakers including three normal and two hearing impaired individuals. Consequently, we show that the hand preceding phenomenon exists in Mandarin CS production with significant differences between normal and hearing impaired people. Extensive experiments demonstrate that our model outperforms the baseline and the previous state-of-the-art methods.",True
issa23_interspeech,https://www.isca-archive.org/interspeech_2023/issa23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/issa23_interspeech.html,Durational and Non-durational Correlates of Lexical and Derived Geminates in Arabic,"Phonetics, Phonology, and Prosody 2",2023,"This paper reports on the phonetic and phonological patterns of gemination in Tripolitanian Libyan Arabic (TLA). While previous studies on Arabic gemination have either focused on lexical geminates or reported results on data that contains both lexical and derived geminates, without investigating its effect on the phonetic output, the present study investigates the effect of the phonological status of a geminate on the phonetic realization. Several measurements were obtained including target segments duration, RMS amplitude and F1, F2 and F3 for the target consonants. Preliminary results suggest that the acoustic distinction between singleton and geminate consonants in TLA is dependent mainly on durational correlates. There was no evidence of differences in RMS amplitude between singleton and geminate consonants of any type. F1, F2 and F3 frequencies are found to show similar patterns for singleton and geminate types for all sounds, suggesting no gestural effects of gemination in TLA.",True
geneva23_interspeech,https://www.isca-archive.org/interspeech_2023/geneva23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/geneva23_interspeech.html,Accentor: An Explicit Lexical Stress Model for TTS Systems,Speech Synthesis: Expressivity,2023,"The accurate placement of word stress is a critical component of the correct pronunciation of words. Contemporary publicly available text-to-speech (TTS) datasets have a relatively narrow coverage of unique words, which causes modern neural TTS systems to synthesize speech that often suffers from lexical stress errors. In this work, we propose an efficient approach for explicitly modeling lexical stress knowledge with a dedicated Accentor neural network. The Accentor is trained separately on a large lexically diverse stress-annotated text corpus that is automatically compiled using an automatic speech recognition system. We demonstrate that the Accentor can be combined with a TTS acoustic model to reliably control the word stress encoded in the generated acoustic features. Experiments show that our approach increases the stress prediction accuracy by a factor of 12 in comparison to other modern TTS systems and improves the naturalness and comprehensibility of the synthesized speech.",True
ray23_interspeech,https://www.isca-archive.org/interspeech_2023/ray23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/ray23_interspeech.html,Compositional Generalization in Spoken Language Understanding,Spoken Dialog Systems and Conversational Analysis 1,2023,"State-of-the-art spoken language understanding (SLU) models have shown tremendous success in benchmark SLU datasets, yet they still fail in many practical scenario due to the lack of model compositionality when trained on limited training data. In this paper, we study two types of compositionality: novel slot combination, and length generalization. We first conduct in-depth analysis, and find that state-of-the-art SLU models often learn spurious slot correlations during training, which leads to poor performance in both compositional cases. To mitigate these limitations, we create the first compositional splits of benchmark SLU datasets and we propose the first compositional SLU model, including compositional loss and paired training that tackle each compositional case respectively. On both benchmark and compositional splits in ATIS and SNIPS, we show that our compositional SLU model significantly outperforms (up to 5% F1 score) state-of-the-art BERT SLU model.",True
shekar23b_interspeech,https://www.isca-archive.org/interspeech_2023/shekar23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/shekar23b_interspeech.html,Assessment of Non-Native Speech Intelligibility using Wav2vec2-based Mispronunciation Detection and Multi-level Goodness of Pronunciation Transformer,Speech Recognition: Technologies and Systems for New Applications 2,2023,"Automatic pronunciation assessment (APA) plays an important role in providing feedback for self-directed language learners in computer-assisted pronunciation training (CAPT). Several mispronunciation detection and diagnosis (MDD) systems have achieved promising performance based on end-to-end phoneme recognition. However, assessing the intelligibility of second language (L2) remains a challenging problem. One issue is the lack of large-scale labeled speech data from non-native speakers. Additionally, relying only on one aspect (e.g., accuracy) at a phonetic level may not provide a sufficient assessment of pronunciation quality and L2 intelligibility. It is possible to leverage segmental/phonetic-level features such as goodness of pronunciation (GOP), however, feature granularity may cause a discrepancy in prosodic-level (suprasegmental) pronunciation assessment. In this study, Wav2vec 2.0-based MDD and Goodness Of Pronunciation feature-based Transformer are employed to characterize L2 intelligibility. Here, an L2 speech dataset, with human-annotated prosodic (suprasegmental) labels, is used for multi-granular and multi-aspect pronunciation assessment and identification of factors important for intelligibility in L2 English speech. The study provides a transformative comparative assessment of automated pronunciation scores versus the relationship between suprasegmental features and listener perceptions, which taken collectively can help support the development of instantaneous assessment tools and solutions for L2 learners.",True
shi23c_interspeech,https://www.isca-archive.org/interspeech_2023/shi23c_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/shi23c_interspeech.html,Semantic VAD: Low-Latency Voice Activity Detection for Speech Interaction,Speech Activity Detection and Modeling,2023,"For speech interaction, voice activity detection (VAD) is often used as a front-end. However, traditional VAD algorithms usually need to wait for a continuous tail silence to reach a preset maximum duration before segmentation, resulting in a large latency that affects user experience. In this paper, we propose a novel semantic VAD for low-latency segmentation. Different from existing methods, a frame-level punctuation prediction task is added to the semantic VAD, and the artificial endpoint is included in the classification category in addition to the often-used speech presence and absence. To enhance the semantic information of the model, we also incorporate an automatic speech recognition (ASR) related semantic loss. Evaluations on an internal dataset show that the proposed method can reduce the average latency by 53.3% without significant deterioration of character error rate in the back-end ASR compared to the traditional VAD approach.",True
behera23_interspeech,https://www.isca-archive.org/interspeech_2023/behera23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/behera23_interspeech.html,Towards Multi-Lingual Audio Question Answering,Analysis of Speech and Audio Signals 1,2023,"Audio Question Answering (AQA) is a multi-modal translation task where a system analyzes an audio signal and a natural language question to generate a desirable natural language answer. AQA has been primarily studied through the lens of the English language. However, addressing AQA in other languages, in the same manner, would require a considerable amount of resources. This paper proposes scalable solutions to multi-lingual audio question answering on both data and modeling fronts. We propose mClothoAQA, a translation-based multi-lingual AQA dataset in eight languages. The dataset consists of 1991 audio files and nearly 0.3 million question-answer pairs. Finally, we introduce a multi-lingual AQA model and demonstrate its strong performance in eight languages. The dataset and code can be accessed at https://github.com/swarupbehera/mAQA.",True
zhang23b_interspeech,https://www.isca-archive.org/interspeech_2023/zhang23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/zhang23b_interspeech.html,Obstructive Sleep Apnea Detection using Pre-trained Speech Representations,Biosignal-enabled Spoken Communication,2023,"Obstructive sleep apnea (OSA) is a condition commonly affecting middle-aged men that can disturb sleep, cause daytime tiredness, and increase the risk of heart disease. Speech can serve as a valuable biomarker for identifying and predicting the severity of OSA due to its connection with changes in throat structure. This study proposes a new deep-learning-based method for detecting OSA by analyzing speech recordings of participants in sitting and lying positions. The method utilizes a Siamese structure that employs a pre-trained XLSR model to encode ten utterances for each position, reducing the amount of necessary training data and enabling comparison of throat structure changes between the two positions through voice analysis. The study also explores the use of patient characteristic features. Results show this approach achieves an F1 value of 0.725 on our in-house dataset, proving the feasibility of end-to-end speech OSA detection with foundation models.",True
tamayoflorez23_interspeech,https://www.isca-archive.org/interspeech_2023/tamayoflorez23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/tamayoflorez23_interspeech.html,HABLA: A Dataset of Latin American Spanish Accents for Voice Anti-spoofing,Speaker and Language Identification 1,2023,"Research on improving automatic speaker verification systems to detect speech spoofing has focused mainly on English, with little attention given to other languages creating a significant gap in language coverage. This paper introduces HABLA, the first voice anti-spoofing dataset in the Spanish language including Argentinian, Colombian, Peruvian, Venezuelan, and Chilean accents. The dataset provided by HABLA comprises over 22,000 authentic speech samples from male and female speakers hailing from five distinct Latin American nations as well as 58,000 spoof samples that were generated through the use of six different speech synthesis strategies, including recent voice conversion and text-to-speech algorithms. Finally, initial findings on the efficacy of pre-existing Antispoofing Systems models are presented along with concerns regarding their performance in languages other than English.",True
baghel23_interspeech,https://www.isca-archive.org/interspeech_2023/baghel23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/baghel23_interspeech.html,The DISPLACE Challenge 2023 - DIarization of SPeaker and LAnguage in Conversational Environments,Speaker and Language Diarization,2023,"In multilingual societies, social conversations often involve code-mixed speech. The current speech technology may not be well equipped to extract information from multi-lingual multi-speaker conversations. The DISPLACE challenge entails a first-of-kind task to benchmark speaker and language diarization on the same data, as the data contains multi-speaker conversations in multilingual code-mixed speech. The challenge attempts to highlight outstanding issues in speaker diarization (SD) in multilingual settings with code-mixing. Further, language diarization (LD) in multi-speaker settings also introduces new challenges, where the system has to disambiguate speaker switches with code switches. For this challenge, a natural multilingual, multi-speaker conversational dataset is distributed for development and evaluation purposes. The systems are evaluated on single-channel far-field recordings. We also release a baseline system and report the highlights of the system submissions.",True
sy23_interspeech,https://www.isca-archive.org/interspeech_2023/sy23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/sy23_interspeech.html,Measuring Language Development From Child-centered Recordings,Connecting Speech-science and Speech-technology for Children's Speech,2023,"Standard ways to measure child language development from spontaneous corpora rely on detailed linguistic descriptions of a language as well as exhaustive transcriptions of the child's speech, which today can only be done through costly human labor. We tackle both issues by proposing (1) a new language development metric (based on entropy) that does not require linguistic knowledge other than having a corpus of text in the language in question to train a language model, (2) a method to derive this metric directly from speech based on a smaller text-speech parallel corpus. Here, we present descriptive results on an open archive including data from six English-learning children as a proof of concept. We document that our entropy metric documents a gradual convergence of children's speech towards adults' speech as a function of age, and it also correlates moderately with lexical and morphosyntactic measures derived from morphologically-parsed transcriptions.",True
ding23_interspeech,https://www.isca-archive.org/interspeech_2023/ding23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/ding23_interspeech.html,Stable Speech Emotion Recognition with Head-k-Pooling Loss,Speech Emotion Recognition 1,2023,"Speech emotion recognition (SER) aims to detect the emotion of the speaker involved in a given utterance. Most existing SER methods focus on local speech features by stacking convolutions and training all segments of an utterance with an utterance-level label. Two deficiencies exist in these methods: i) learning only local speech features may be insufficient for SER due to the ambiguity of emotions; ii) consistent supervision of each segment may lead to label error propagation, as the true emotions of some segments may not match the utterance label. To solve the two issues, we first devise a global-local fusion network to model both long- and short-range relations in speech. Second, we tailor a novel head-k-pooling loss for SER tasks, which dynamically assigns labels for each segment and selectively performs loss calculation across segments. We test our method on the IEMOCAP and the newly collected ST-EMO dataset, and the results show its superiority and stability.",True
anwar23_interspeech,https://www.isca-archive.org/interspeech_2023/anwar23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/anwar23_interspeech.html,MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation,Resources for Spoken Language Processing,2023,"We introduce MuAViC, a multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation providing 1200 hours of audio-visual speech in 9 languages. It is fully transcribed and covers 6 English-to-X translation as well as 6 X-to-English translation directions. To the best of our knowledge, this is the first open benchmark for audio-visual speech-to-text translation and the largest open benchmark for multilingual audio-visual speech recognition. Our baseline results show that MuAViC is effective for building noise-robust speech recognition and translation models. We make the corpus available at https://github.com/facebookresearch/muavic.",True
sankar23_interspeech,https://www.isca-archive.org/interspeech_2023/sankar23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/sankar23_interspeech.html,Investigating the dynamics of hand and lips in French Cued Speech using attention mechanisms and CTC-based decoding,"Speech, Voice, and Hearing Disorders 2",2023,"Hard of hearing or profoundly deaf people make use of cued speech (CS) as a communication tool to understand spoken language. By delivering cues that are relevant to the phonetic information, CS offers a way to enhance lipreading. In literature, there have been several studies on the dynamics between the hand and the lips in the context of human production. This article proposes a way to investigate how a neural network learns this relation for a single speaker while performing a recognition task using attention mechanisms. Further, an analysis of the learnt dynamics is utilized to establish the relationship between the two modalities and extract automatic segments. For the purpose of this study, a new dataset has been recorded for French CS. Along with the release of this dataset, a benchmark will be reported for word-level recognition, a novelty in the automatic recognition of French CS.",True
javed23_interspeech,https://www.isca-archive.org/interspeech_2023/javed23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/javed23_interspeech.html,Svarah: Evaluating English ASR Systems on Indian Accents,Multilingual Models for ASR,2023,"India is the second largest English-speaking country in the world with a speaker base of roughly 130 million. Thus, it is imperative that automatic speech recognition (ASR) systems for English should be evaluated on Indian accents. Unfortunately, Indian speakers find a very poor representation in existing English ASR benchmarks such as LibriSpeech, Switchboard, Speech Accent Archive, etc. In this work, we address this gap by creating Svarah, a benchmark that contains 9.6 hours of transcribed English audio from 117 speakers across 65 geographic locations throughout India, resulting in a diverse range of accents. Svarah comprises both read speech and spontaneous conversational data, covering various domains, such as history, culture, tourism, etc., ensuring a diverse vocabulary. We evaluate 6 open source ASR models and 2 commercial ASR systems on Svarah and show that there is clear scope for improvement on Indian accents.",True
martinezsevilla23_interspeech,https://www.isca-archive.org/interspeech_2023/martinezsevilla23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/martinezsevilla23_interspeech.html,Insights into end-to-end audio-to-score transcription with real recordings: A case study with saxophone works,Analysis of Speech and Audio Signals 2,2023,"Neural end-to-end Audio-to-Score (A2S) transcription aims to retrieve a score that encodes the music content of an audio recording in a single step. Due to the recentness of this formulation, the existing works have exclusively addressed controlled scenarios with synthetic data that fail to provide conclusions applicable to real-world cases. In response to this gap in the literature, this work introduces a novel assortment of real saxophone recordings---together with their digital scores---and poses several experimental scenarios involving real and synthetic data. The obtained results confirm the adequacy of this A2S framework to deal with real data as well as proving the relevance of leveraging synthetic interpretations to improve the recognition rate in scenarios with real-data scarcity.",True
dineley23_interspeech,https://www.isca-archive.org/interspeech_2023/dineley23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/dineley23_interspeech.html,Towards robust paralinguistic assessment for real-world mobile health (mHealth) monitoring: an initial study of reverberation effects on speech,Speech and Language in Health: From Remote Monitoring to Medical Conversations 2,2023,"Speech is promising as an objective, convenient tool to monitor health remotely over time using mobile devices. Numerous paralinguistic features have been demonstrated to contain salient information related to an individual's health. However, mobile device specification and acoustic environments vary widely, risking the reliability of the extracted features. In an initial step towards quantifying these effects, we report the variability of 13 exemplar paralinguistic features commonly reported in the speech-health literature and extracted from the speech of 42 healthy volunteers recorded consecutively in rooms with low and high reverberation with one budget and two higher-end smartphones, and a condenser microphone. Our results show reverberation has a clear effect on several features, in particular voice quality markers. Our findings point to new research directions investigating how best to record and process in-the-wild speech for reliable longitudinal mobile health state assessment.",True
chua23_interspeech,https://www.isca-archive.org/interspeech_2023/chua23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/chua23_interspeech.html,MERLIon CCS Challenge: A English-Mandarin code-switching child-directed speech corpus for language identification and diarization,MERLIon CCS Challenge: Multilingual Everyday Recordings - Language Identification On Code-Switched Child-Directed Speech,2023,"To enhance the reliability and robustness of language identification(LID) and language diarization(LD) systems for heterogeneous populations and scenarios, there is a need for speech processing models to be trained on datasets that feature diverse language registers and speech patterns. We present the MERLIon CCS challenge, featuring a first-of-its-kind Zoom video call dataset of parent-child shared book reading, of over 30 hours with over 300 recordings, annotated by multilingual transcribers using a high-fidelity linguistic transcription protocol. The audio corpus features spontaneous and in-the-wild English-Mandarin code-switching, child-directed speech in non-standard accents with diverse language-mixing patterns recorded in a variety of home environments. This report describes the corpus, as well as LID and LD results for our baseline and several systems submitted to the MERLIon CCS challenge using the corpus.",True
yakovlev23_interspeech,https://www.isca-archive.org/interspeech_2023/yakovlev23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/yakovlev23_interspeech.html,VoxTube: a multilingual speaker recognition dataset,Speaker Recognition 1,2023,The objective of this paper is to advance the development of technologies in the fields of speaker recognition and speaker identification by introducing a large labeled audio database VoxTube collected from the open-source media. We propose a fully automated unsupervised approach for audio labeling that requires any pre-trained speaker recognition model. Collected with this approach from videos with CC BY license the VoxTube dataset contains more than 5.000 speakers with more than 4 million utterances pronounced in more than 10 languages. In our paper we show the VoxTube's high generalization ability across multiple domains by evaluating the accuracy metrics on various speaker recognition benchmarks. We also show how well this dataset complements an already existing VoxCeleb2 dataset.,True
gupta23_interspeech,https://www.isca-archive.org/interspeech_2023/gupta23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/gupta23_interspeech.html,Spoken Language Identification System for English-Mandarin Code-Switching Child-Directed Speech,MERLIon CCS Challenge: Multilingual Everyday Recordings - Language Identification On Code-Switched Child-Directed Speech,2023,"This work focuses on improving the Spoken Language Identification (LangId) system for a challenge that focuses on developing robust language identification systems that are reliable for non-standard, accented (Singaporean accent), spontaneous code-switched, and child-directed speech collected via Zoom. We propose a two-stage Encoder-Decoder-based E2E model. The encoder module consists of 1D depth-wise separable convolutions with Squeeze-and-Excitation (SE) layers with a global context. The decoder module uses an attentive temporal pooling mechanism to get fixed length time-independent feature representation. The total number of parameters in the model is around 22.1 M, which is relatively light compared to using some large-scale pre-trained speech models. We achieved an EER of 15.6% in the closed track and 11.1% in the open track (baseline system 22.1%). We also curated additional LangId data from YouTube videos (having Singaporean speakers), which will be released for public use.",True
gao23f_interspeech,https://www.isca-archive.org/interspeech_2023/gao23f_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/gao23f_interspeech.html,Human Transcription Quality Improvement,"Speech Perception, Production, and Acquisition 1",2023,"High quality transcription data is crucial for training automatic speech recognition (ASR) systems. However, the existing industry-level data collection pipelines are expensive to researchers, while the quality of crowdsourced transcription is low. In this paper, we propose a reliable method to collect speech transcriptions. We introduce two mechanisms to improve transcription quality: confidence estimation based reprocessing at labeling stage, and automatic word error correction at post-labeling stage. We collect and release LibriCrowd - a large-scale crowdsourced dataset of audio transcriptions on 100 hours of English speech. Experiment shows the Transcription WER is reduced by over 50%. We further investigate the impact of transcription error on ASR model performance and found a strong correlation. The transcription quality improvement provides over 10% relative WER reduction for ASR models. We release the dataset and code to benefit the research community.",True
kulkarni23_interspeech,https://www.isca-archive.org/interspeech_2023/kulkarni23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/kulkarni23_interspeech.html,ClArTTS: An Open-Source Classical Arabic Text-to-Speech Corpus,Speech Synthesis: Multilinguality; Evaluation,2023,"We present a Classical Arabic Text-to-Speech (ClArTTS) corpus to facilitate the development of end-to-end TTS systems for the Arabic language. The speech is extracted from a LibriVox audiobook, which is then processed, segmented, and manually transcribed and annotated. The ClArTTS corpus contains about 12 hours of speech from a single male speaker sampled at 40100 Hz. In this paper, we describe the process of corpus creation, details of corpus statistics, and a comparison with existing resources. Furthermore, we develop two TTS systems based on Grad-TTS and Glow-TTS and illustrate the performance of the resulting systems via subjective and objective evaluations. The ClArTTS corpus is publicly available at www.clartts.com for research purposes, along with the baseline TTS systems and an interactive demo.",True
eisenstein23_interspeech,https://www.isca-archive.org/interspeech_2023/eisenstein23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/eisenstein23_interspeech.html,MD3: The Multi-Dialect Dataset of Dialogues,Resources for Spoken Language Processing,2023,"We introduce a new dataset of conversational speech representing English from India, Nigeria, and the United States. The Multi-Dialect Dataset of Dialogues (MD3) strikes a new balance between open-ended conversational speech and task-oriented dialogue by prompting participants to perform a series of short information-sharing tasks. This facilitates quantitative cross-dialectal comparison, while avoiding the imposition of a restrictive task structure that might inhibit the expression of dialect features. Preliminary analysis of the dataset reveals significant differences in syntax and in the use of discourse markers. The dataset, which will be made publicly available with the publication of this paper, includes more than 20 hours of audio and more than 200,000 orthographically-transcribed tokens.",True
soltau23_interspeech,https://www.isca-archive.org/interspeech_2023/soltau23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/soltau23_interspeech.html,Speech Aware Dialog System Technology Challenge (DSTC11),Dialog Management,2023,"Most research on task oriented dialog modeling is based on written text input. However, practical dialog systems often use spoken input. Typically, input speech is converted into text using ASR, which are error-prone. Furthermore, most systems don't address the differences in written and spoken language. The research on this topic is stymied by the lack of a public corpus. Motivated by these considerations, we hosted a speech-aware dialog state tracking challenge and created a public corpus which can be used to investigate the performance gap between the written and spoken input. We created three spoken versions of the popular written-domain MultiWoz task and provide waveforms, ASR transcripts, and audio encodings to encourage wider participation from teams that may not have access to ASR systems. In this paper, we describe the corpus, report results from participating teams, provide preliminary analyses of their results, and summarize the current state-of-the-art in this domain.",True
li23y_interspeech,https://www.isca-archive.org/interspeech_2023/li23y_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/li23y_interspeech.html,CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 2",2023,"Audio-visual person recognition (AVPR) has received extensive attention. However, most datasets used for AVPR research so far are collected in constrained environments, and thus cannot reflect the true performance of AVPR systems in real-world scenarios. To meet the request for research on AVPR in unconstrained conditions, this paper presents a multi-genre AVPR dataset collected 'in the wild', named CN-Celeb-AV. This dataset contains more than 420k video segments from 1,136 persons from public media. In particular, we put more emphasis on two real-world complexities: (1) data in multiple genres; (2) segments with partial information. A comprehensive study was conducted to compare CN-Celeb-AV with two popular public AVPR benchmark datasets, and the results demonstrated that CN-Celeb-AV is more in line with real-world scenarios and can be regarded as a new benchmark dataset for AVPR research. The dataset also involves a development set
that can be used to boost the performance of AVPR systems in real-life situations. The dataset is free for researchers and can be downloaded from http://cnceleb.org/.",True
deshmukh23_interspeech,https://www.isca-archive.org/interspeech_2023/deshmukh23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/deshmukh23_interspeech.html,Audio Retrieval with WavText5K and CLAP Training,"Spoken Language Understanding, Summarization, and Information Retrieval",2023,"Text-based audio retrieval takes a natural language query to retrieve relevant audio files in a database. Most retrieval models are trained, optimized, and evaluated on a single dataset. In this paper, we quantify the effect of adding training data using three datasets and the effect on performance by evaluating the same model on two datasets. For our study, first, we introduce a new collection of about 5000 audio-text pairs called WavText5K. We qualitatively show how WavText5K differs from audio-text datasets and quantitatively show its effectiveness for retrieval. Our results show that adding more audio-text pairs does not necessarily improve performance. Second, we compare two effective audio encoders: CNN and audio transformers. We propose an architecture that demonstrates that utilizing both encoders improves the individual model's performance. Overall, using WavText5K and the proposed encoder combination outperforms the benchmark for AudioCaps and Clotho by 6% and 23%.",True
zhang23f_interspeech,https://www.isca-archive.org/interspeech_2023/zhang23f_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/zhang23f_interspeech.html,Emotion Classification with EEG Responses Evoked by Emotional Prosody of Speech,"Speech Perception, Production, and Acquisition 2",2023,"Emotion classification with EEG responses can be used in human-computer interaction, security, medical treatment, etc. Neural responses recorded via EEG can reflect more direct and objective emotional information than other behavioral signals (i.e., facial expression...). In most previous studies, only features of EEG were used as input for machine learning models. In this work, we assumed that the emotional features included in speech stimuli could assist in emotion recognition with EEG when the emotion is evoked by the emotional prosody of speech. An EEG data corpus was collected with specific speech stimuli, in which emotion was represented with only speech prosody and without semantic context. A novel EEG-Prosody CRNN model was proposed to classify four types of typical emotions. The classification accuracy can achieve at 82.85% when the prosody features of speech were integrated as input, which outperformed most audio-evoked EEG-based emotion classification methods.",True
demir23_interspeech,https://www.isca-archive.org/interspeech_2023/demir23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/demir23_interspeech.html,PoCaPNet: A Novel Approach for Surgical Phase Recognition Using Speech and X-Ray Images,Speech and Language in Health: From Remote Monitoring to Medical Conversations 2,2023,"Surgical phase recognition is a challenging and necessary task for the development of context-aware intelligent systems that can support medical personnel for better patient care and effective operating room management. In this paper, we present a surgical phase recognition framework that employs a Multi-Stage Temporal Convolution Network using speech and X-Ray images for the first time. We evaluate our proposed approach using our dataset that comprises 31 port-catheter placement operations and report 82.56 % frame-wise accuracy with eight surgical phases. Additionally, we investigate the design choices in the temporal model and solutions for the class-imbalance problem. Our experiments demonstrate that speech and X-Ray data can be effectively utilized for surgical phase recognition, providing a foundation for the development of speech assistants in operating rooms of the future.",True
li23w_interspeech,https://www.isca-archive.org/interspeech_2023/li23w_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/li23w_interspeech.html,Few-shot Class-incremental Audio Classification Using Stochastic Classifier,Automatic Audio Classification and Audio Captioning,2023,"It is generally assumed that number of classes is fixed in current audio classification methods, and the model can recognize pre-given classes only. When new classes emerge, the model needs to be retrained with adequate samples of all classes. If new classes continually emerge, these methods will not work well and even infeasible. In this study, we propose a method for few-shot class-incremental audio classification, which continually recognizes new classes and remember old ones. The proposed model consists of an embedding extractor and a stochastic classifier. The former is trained in base session and frozen in incremental sessions, while the latter is incrementally expanded in all sessions. Two datasets (NS-100 and LS-100) are built by choosing samples from audio corpora of NSynth and LibriSpeech, respectively. Results show that our method exceeds four baseline ones in average accuracy and performance dropping rate. Code is at https://github.com/vinceasvp/meta-sc.",True
koizumi23_interspeech,https://www.isca-archive.org/interspeech_2023/koizumi23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/koizumi23_interspeech.html,LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus,Speech Synthesis: Multilinguality; Evaluation,2023,"This paper introduces a new speech dataset called ""LibriTTS-R"" designed for text-to-speech (TTS) use. It is derived by applying speech restoration to the LibriTTS corpus, which consists of 585 hours of speech data at 24 kHz sampling rate from 2,456 speakers and the corresponding texts. The constituent samples of LibriTTS-R are identical to those of LibriTTS, with only the sound quality improved. Experimental results show that the LibriTTS-R ground-truth samples showed significantly improved sound quality compared to those in LibriTTS. In addition, neural end-to-end TTS trained with LibriTTS-R achieved speech naturalness on par with that of the ground-truth samples. The corpus is freely available for download from http://www.openslr.org/141/.",True
yang23b_interspeech,https://www.isca-archive.org/interspeech_2023/yang23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/yang23b_interspeech.html,Relation-based Counterfactual Data Augmentation and Contrastive Learning for Robustifying Natural Language Inference Models,"Spoken Language Understanding, Summarization, and Information Retrieval",2023,"Although pre-trained language models show good performance on various natural language processing tasks, they often rely on non-causal features and patterns to determine the outcome. For natural language inference tasks, previous results have shown that even a model trained on a large number of data fails to perform well on counterfactually revised data, indicating that the model is not robustly learning the semantics of the classes. In this paper, we propose a method in which we use token-based and sentence-based augmentation methods to generate counterfactual sentence pairs that belong to each class, and apply contrastive learning to help the model learn the difference between sentence pairs of different classes with similar contexts. Evaluation results with counterfactually-revised dataset and general NLI datasets show that the proposed method can improve the performance and robustness of the NLI model.",True
lin23e_interspeech,https://www.isca-archive.org/interspeech_2023/lin23e_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/lin23e_interspeech.html,Low-Resource Cross-Lingual Adaptive Training for Nigerian Pidgin,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 3",2023,"Developing effective spoken language processing systems for low-resource languages poses several challenges due to the lack of parallel data and limited resources for fine-tuning models. In this work, we target on improving upon both text classification and translation of Nigerian Pidgin (Naija) by collecting a large-scale parallel English-Pidgin corpus and further propose a framework of cross-lingual adaptive training that includes both continual and task adaptive training so as to adapt a base pre-trained model to low-resource languages. Our studies show that English pre-trained language models serve as a stronger prior than multilingual language models on English-Pidgin tasks with up to 2.38 BLEU improvements; and demonstrate that augmenting orthographic data and using task adaptive training with back-translation can have a significant impact on model performance.",True
niu23b_interspeech,https://www.isca-archive.org/interspeech_2023/niu23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/niu23b_interspeech.html,Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder,Speech and Language in Health: From Remote Monitoring to Medical Conversations 1,2023,"Emotion is a complex behavioral phenomenon, which is expressed and perceived through various modalities, such as language, vocal and facial expressions. Psychiatric research has suggested that the lack of emotional alignment between modalities is a symptom of emotion disorders. In this work, we quantify the mismatch between emotion expressed through language and acoustics, which we refer to as Emotional MisMatch (EMM), as an intermediate step for mood identification. We use a longitudinal dataset collected from people with Bipolar Disorder (BP) and show that symptomatic mood episodes show significantly more EMM, compared to euthymic moods. We propose a fully automatic mood identification pipeline with automatic speech transcription, emotion recognition, and EMM feature extraction. We find that EMM features, although smaller in size, outperform a language-based baseline, and consistently provide improvement when combined with language and/or raw emotion features on mood classification.",True
solanki23_interspeech,https://www.isca-archive.org/interspeech_2023/solanki23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/solanki23_interspeech.html,Do Vocal Breath Sounds Encode Gender Cues for Automatic Gender Classification?,Analysis of Speech and Audio Signals 4,2023,"The acoustic features of continuous speech, such as pitch (F0) and formant frequencies (F1, F2) have been utilized for gender classification. However, non-speech signals including vocal breath sounds have not been explored due to the absence of gender-specific acoustic features. This study investigates if vocal breath sounds carry gender information and if they can be used for automatic gender classification. The study examines the use of data-driven and knowledge-based features from breath sounds, classifier complexity, and the importance of breath signal segment location and duration. Results from experiments on 54 minutes of male and 52 minutes of female breath sounds demonstrate that classifiers with low-complexity and knowledge-based features (MFCC statistics) perform similarly to high-complexity classifiers with data-driven features. Breath segments of around 3 seconds are found to be the most suitable choice regardless of location, eliminating the need for breath cycle boundary marking.",True
deseyssel23_interspeech,https://www.isca-archive.org/interspeech_2023/deseyssel23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/deseyssel23_interspeech.html,"ProsAudit, a prosodic benchmark for self-supervised speech models",Invariant and Robust Pre-trained Acoustic Models,2023,"We present ProsAudit, a benchmark in English to assess structural prosodic knowledge in self-supervised learning (SSL) speech models. It consists of two subtasks, their corresponding metrics, and an evaluation dataset. In the protosyntax task, the model must correctly identify strong versus weak prosodic boundaries. In the lexical task, the model needs to correctly distinguish between pauses inserted between words and within words. We also provide human evaluation scores on this benchmark. We evaluated a series of SSL models and found that they were all able to perform above chance on both tasks, even when evaluated on an unseen language. However, non-native models performed significantly worse than native ones on the lexical task, highlighting the importance of lexical knowledge in this task. We also found a clear effect of size with models trained on more data performing better in the two subtasks.",True
bharati23_interspeech,https://www.isca-archive.org/interspeech_2023/bharati23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/bharati23_interspeech.html,Automatic Deep Neural Network-Based Segmental Pronunciation Error Detection of L2 English Speech (L1 Bengali),"Speech Perception, Production, and Acquisition 1",2023,"In the last few decades, English has become a popular language as it helps us to communicate with the global world. A large population of English learners find it challenging to achieve an 'acceptable' and 'intelligible' pronunciation. To overcome these issues, various computer-assisted pronunciation training tools are designed where automatic pronunciation error detection (APED) is a core component of the system. Most of the works of APED are based on European English speech, but there is no such work reported for Bengali English speech. This paper proposes a system for pronunciation error detection of L2 English speech (L1 Bengali) at phoneme/segmental level using a hybrid convolutional neural network and long short-term memory modules with CTC loss. Experiments are done based on newly created L2 English speaker (L1 Bengali) speech data. The results demonstrate that the proposed system outperforms the goodness of pronunciation-based methods by 15% in terms of F1 score using fbank.",True
hwang24b_interspeech,https://www.isca-archive.org/interspeech_2024/hwang24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/hwang24b_interspeech.html,Acquisition of high vowel devoicing in Japanese: A production experiment with three and four year olds,L1/L2 Acquisition and Cross-Linguistic Factors,2024,"The purpose of this study is to investigate the developmental path of high vowel devoicing (HVD) in Japanese. A picture-naming task was conducted with Japanese-learning preschoolers of three and four years old. The empirical data presented in this study allow us not only to make comparisons with the data from 4 year-olds in a previous study, but also to address the devoicing patterns for the 3-year old children, which have received little attention in the developmental literature on HVD. The results of twenty children reveal distinct patterns depending on position; Word-medially, the overall occurrence of HVD increases if we compare the average HVD rates between the ages of 3 and 4, but their rate is not yet reached at the adult-like level at the age of four. Word-finally, on the other hand, the rates are overall lower than the word-medial devoicing. Further, unlike the incremental pattern observed in the rates for the word-medial devoicing, no clear developmental advancement is found in this position. The presence or absence of the developmental advancement appears to support the qualitative differences between two types of HVDs in distinct positions.",True
ma24c_interspeech,https://www.isca-archive.org/interspeech_2024/ma24c_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ma24c_interspeech.html,FLEURS-R: A Restored Multilingual Speech Corpus for Generation Tasks,Speech Synthesis: Tools and Data,2024,"This paper introduces FLEURS-R, a speech restoration applied version of the Few-shot Learning Evaluation of Universal Representations of Speech (FLEURS) corpus. FLEURS-R maintains an N-way parallel speech corpus in 102 languages as FLEURS, with improved audio quality and fidelity by applying the speech restoration model Miipher. The aim of FLEURS-R is to advance speech technology in more languages and catalyze research in- cluding text-to-speech (TTS) and other speech generation tasks in low-resource languages. Comprehensive evaluations with the restored speech and TTS baseline models trained from the new corpus show that the new corpus obtained significantly improved speech quality while maintaining the semantic contents of the speech. The corpus is publicly released via Hugging Face.",True
richter24_interspeech,https://www.isca-archive.org/interspeech_2024/richter24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/richter24_interspeech.html,EARS: An Anechoic Fullband Speech Dataset Benchmarked for Speech Enhancement and Dereverberation,"Deep Learning-Based Speech Enhancement: Approaches, Scalability, and Evaluation",2024,"We release the EARS (Expressive Anechoic Recordings of Speech) dataset, a high-quality speech dataset comprising 107 speakers from diverse backgrounds, totalling in 100 hours of clean, anechoic speech data. The dataset covers a large range of different speaking styles, including emotional speech, different reading styles, non-verbal sounds, and conversational freeform speech. We benchmark various methods for speech enhancement and dereverberation on the dataset and evaluate their performance through a set of instrumental metrics. In addition, we conduct a listening test with 20 participants for the speech enhancement task, where a generative method is preferred. We introduce a blind test set that allows for automatic online evaluation of uploaded data. Dataset download links and automatic evaluation server can be found online.",True
zhou24e_interspeech,https://www.isca-archive.org/interspeech_2024/zhou24e_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/zhou24e_interspeech.html,YOLO-Stutter: End-to-end Region-Wise Speech Dysfluency Detection,Speech Disorders 2,2024,"Dysfluent speech detection is the bottleneck for disordered speech analysis and spoken language learning. Current state-of-the-art models are governed by rule-based systems which lack efficiency and robustness, and are sensitive to template design. In this paper, we propose YOLO-Stutter: a first end-to-end method that detects dysfluencies in a time-accurate manner. YOLO-Stutter takes  imperfect speech-text alignment as input, followed by a spatial feature aggregator, and a temporal dependency extractor to perform region-wise boundary and class predictions. We also introduce two dysfluency corpus, VCTK-Stutter and VCTK-TTS, that simulate natural spoken dysfluencies including repetition, block, missing, replacement, and prolongation. Our end-to-end method achieves state-of-the-art performance with a minimum number of trainable parameters for on both simulated data and real aphasia speech . Code and datasets are open-sourced at https://github.com/rorizzz/YOLO-Stutter",True
deshmukh24b_interspeech,https://www.isca-archive.org/interspeech_2024/deshmukh24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/deshmukh24b_interspeech.html,PAM: Prompting Audio-Language Models for Audio Quality Assessment,Generative Models for Speech and Audio,2024,"Audio quality is a key performance metric for various audio processing tasks, including generative modeling, however its objective measurement remains a challenge. Audio-Language Models (ALM) are pre-trained on millions of audio-text pairs that may contain information about audio quality, the presence of artifacts or noise. Given an audio input and a text prompt about quality, an ALM can calculate a similarity score between the two. We exploit this capability and introduce PAM, a truly reference-free metric for assessing audio quality for different audio processing tasks. Contrary to other âreference-freeâ metrics, PAM does not require computing embeddings on a reference dataset nor training a task-specific model on a costly set of human listening scores. We extensively evaluate PAM against established metrics and newly collected human listening scores on four tasks: text- to-audio (TTA), text-to-music generation (TTM), text-to-speech (TTS), and deep noise suppression (DNS). We perform multiple ablation studies with controlled audio distortions, in-the-wild setups, and prompt choices. Our evaluation shows that overall, PAM correlates strongly with human listening scores and performs better than existing metrics. These results demonstrate the potential of ALM for computing a general-purpose audio quality metric. Code and human listening scores will be released.",True
huang24f_interspeech,https://www.isca-archive.org/interspeech_2024/huang24f_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/huang24f_interspeech.html,Improving Neural Biasing for Contextual Speech Recognition by Early Context Injection and Text Perturbation,Contextual Biasing and Adaptation,2024,"Existing research suggests that automatic speech recognition(ASR) models can benefit from additional contexts (e.g., contact lists, user specified vocabulary). Rare words and named entities can be better recognized with contexts. In this work, we propose two simple yet effective techniques to improve context-aware ASR models. First, we inject contexts into the encoders at an early stage instead of merely at their last layers. Second, to enforce the model to leverage the contexts during training, we perturb the reference transcription with alternative spellings so that the model learns to rely on the contexts to make correct predictions. On LibriSpeech, our techniques together reduce the rare word error rate by 60% and 25% relatively compared to no biasing and shallow fusion, making the new state-of-the-art performance. On SPGISpeech and a real-world dataset ConEC, our techniques also yield good improvements over the baselines.",True
lin24m_interspeech,https://www.isca-archive.org/interspeech_2024/lin24m_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lin24m_interspeech.html,MinSpeech: A Corpus of Southern Min Dialect for Automatic Speech Recognition,"Accented Speech, Prosodic Features, Dialect, Emotion, Sound Classification",2024,"This paper presents MinSpeech, a speech corpus of Southern Min (also known as Hokkien), to propel research in dialect speech recognition. Despite the linguistic and cultural importance of Southern Min, there is still a notable scarcity of publicly accessible speech corpus for this dialect. MinSpeech provides 2237 hours of unlabeled audio and 1778 hours of labeled audio, sourced diversely and encompassing various contexts. Mandarin text is employed as labels to enable cross-linguistic alignment and transformation. Using this corpus, we have developed baseline systems, including supervised models (Kaldi Chain and Conformer) and two self-supervised models (Wav2vec 2.0 and HuBERT). These systems were assessed on an automatic speech recognition (ASR) task to the Southern Min dialect. Experiments illustrate that the corpus offers practical assistance and resources for speech processing of this dialect. MinSpeech dataset is available at https://minspeech.github.io/.",True
lehecka24_interspeech,https://www.isca-archive.org/interspeech_2024/lehecka24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lehecka24_interspeech.html,A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives,General Topics in ASR,2024,"In this paper, we are comparing monolingual Wav2Vec 2.0 models with various multilingual models to see whether we could improve speech recognition performance on a unique oral history archive containing a lot of mixed-language sentences. Our main goal is to push forward research on this unique dataset, which is an extremely valuable part of our cultural heritage. Our results suggest that monolingual speech recognition models are, in most cases, superior to multilingual models, even when processing the oral history archive full of mixed-language sentences from non-native speakers. We also performed the same experiments on the public CommonVoice dataset to verify our results. We are contributing to the research community by releasing our pre-trained models to the public.",True
bhogale24_interspeech,https://www.isca-archive.org/interspeech_2024/bhogale24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/bhogale24_interspeech.html,Empowering Low-Resource Language ASR via Large-Scale Pseudo Labeling,Speech Recognition with Large Pretrained Speech Models for Under-represented Languages (Special Session),2024,"In this study, we tackle the challenge of limited labeled data for low-resource languages in ASR, focusing on Hindi. Specifically, we explore pseudo-labeling, by proposing a generic framework combining multiple ideas from existing works. Our framework integrates multiple base models for transcription and evaluators for assessing audio-transcript pairs, resulting in robust pseudo-labeling for low resource languages. We validate our approach with a new benchmark, IndicYT, comprising diverse YouTube audio files from multiple content categories. Our findings show that augmenting pseudo labeled data from YouTube with existing training data leads to significant performance improvements on IndicYT, without affecting performance on out-of-domain benchmarks, demonstrating the efficacy of pseudo-labeled data in enhancing ASR capabilities for low-resource languages. The benchmark, code and models developed as a part of this work will be made publicly available.",True
pistor24_interspeech,https://www.isca-archive.org/interspeech_2024/pistor24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/pistor24_interspeech.html,Echoes of Implicit Bias Exploring Aesthetics and Social Meanings of Swiss German Dialect Features,Individual and Social Factors in Phonetics,2024,"This study investigates the phonaesthetics and perceptual dynamics of Swiss German dialects, focusing on how particular sound features influence subjective assessments and, in doing so, contribute to dialect stereotypes. By examining 24 linguistic features of Bern and Zurich German, including nine vowels and 15 consonants in single-word utterances, we aim to fill a research gap that has been previously overlooked, despite suggestions of importance. In an online perception study, we gathered evaluations from three distinct groups of raters (N = 46) from Bern, Zurich, and Hessen, Germany, across six categories from aesthetic dimensions to stereotypical dialect attributions. The findings reveal that rater origin determines the levels of importance on evaluation categories and that certain linguistic features can be identified that are closely linked with specific perceptions (e.g., stupid or arrogant), which may foster negative biases against dialect speakers.",True
jing24b_interspeech,https://www.isca-archive.org/interspeech_2024/jing24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/jing24b_interspeech.html,ParaCLAP â Towards a general language-audio model for computational paralinguistic tasks,"Audio Captioning, Tagging, and Audio-Text Retrieval",2024,"Contrastive language-audio pretraining (CLAP) has recently emerged as a method for making audio analysis more generalisable. Specifically, CLAP-style models are able to 'answer' a diverse set of language queries, extending the capabilities of audio models beyond a closed set of labels. However, CLAP relies on a large set of (audio, query) pairs for pretraining. While such sets are available for general audio tasks, like captioning or sound event detection, there are no datasets with matched audio and text queries for computational paralinguistic (CP) tasks. As a result, the community relies on generic CLAP models trained for general audio with limited success. In the present study, we explore training considerations for ParaCLAP, a CLAP-style model suited to CP, including a novel process for creating audio-language queries. We demonstrate its effectiveness on a set of computational paralinguistic tasks, where it is shown to surpass the performance of open-source state-of-the-art models. Our code and resources are publicly available at: https://github.com/KeiKinn/ParaCLAP",True
lai24_interspeech,https://www.isca-archive.org/interspeech_2024/lai24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lai24_interspeech.html,Voice Quality Variation in AAE: An Additional Challenge for Addressing Bias in ASR Models?,"Speech Science, Speech Technology, and Gender (Special Session)",2024,"Creaky voice, a non-modal phonation type often stigmatized in the U.S. media, has become increasingly prevalent in the speech of young Americans across ethnic and regional groups. This paper aims to add to our knowledge of voice quality variation and how it interacts with ASR, by conducting three analyses using a new African American English (AAE) dataset.  Acoustic analyses show robust differences between creaky voice and modal voice, suggesting cross-ethnic similarity in vocal fold articulation between AAE and Mainstream American English (MAE) speakers. In addition, we observed gender differences in creaky production both quantitatively (women > men) and qualitatively (women: medial partial creaks vs. men: final full creaks). This indicates that young AAE female speakers are participating in the phonation change taking place in MAE. We also found that the creakier the speech, the more errors in ASR output, suggesting the importance of incorporating voice quality into ASR systems.",True
mohapatra24_interspeech,https://www.isca-archive.org/interspeech_2024/mohapatra24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/mohapatra24_interspeech.html,Missingness-resilient Video-enhanced Multimodal Disfluency Detection,Speech Disorders 1,2024,"Most existing speech disfluency detection techniques only rely upon acoustic data. In this work, we present a practical multimodal disfluency detection approach that leverages available video data together with audio. We curate an audio-visual dataset and propose a novel fusion technique with unified weight-sharing modality-agnostic encoders to learn the temporal and semantic context. Our resilient design accommodates real-world scenarios where the video modality may sometimes be missing during inference. We also present alternative fusion strategies when both modalities are assured to be complete. In experiments across five disfluency-detection tasks, our unified multimodal approach significantly outperforms Audio-only unimodal methods, yielding an average absolute improvement of 10% (i.e., 10 percentage point increase) when both video and audio modalities are always available, and 7% even when video modality is missing in half of the samples.",True
schade24_interspeech,https://www.isca-archive.org/interspeech_2024/schade24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/schade24_interspeech.html,Understanding âunderstandingâ: presenting a richly annotated multimodal corpus of dyadic interaction,Show and Tell 2,2024,"This paper presents the MUNDEX corpus (MUltimodal UNDerstanding of EXplanations) together with past and current investigations using its data. The corpus is constructed to observe the dynamics of co-constructed communication and the understanding of explanations on multiple modalities in dyadic interactions. These modalities are annotated on several levels, including orthographic transcriptions, acoustic information, annotations of head movement, gaze, manual gestures, and further non-verbal behaviour as well as discourse annotations. Present
and past projects are also concerned with adding further to these annotations. The interlocutorsâ level of understanding is currently investigated in regard to several verbal and non-verbal behaviour markers of both the explaining and listening side.",True
choi24b_interspeech,https://www.isca-archive.org/interspeech_2024/choi24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/choi24b_interspeech.html,Self-Supervised Speech Representations are More Phonetic than Semantic,Neural Network Training for Speech Recognition,2024,"Self-supervised speech models (S3Ms) have become an effective backbone for speech applications. Various analyses suggest that S3Ms encode linguistic properties. In this work, we seek a more fine-grained analysis of the word-level linguistic properties encoded in S3Ms. Specifically, we curate a novel dataset of near homophone (phonetically similar) and synonym (semantically similar) word pairs and measure the similarities between S3M word representation pairs. Our study reveals that S3M representations consistently and significantly exhibit more phonetic than semantic similarity. Further, we question whether widely used intent classification datasets such as Fluent Speech Commands and Snips Smartlights are adequate for measuring semantic abilities. Our simple baseline, using only the word identity, surpasses S3M-based models. This corroborates our findings and suggests that high scores on these datasets do not necessarily guarantee the presence of semantic content.",True
jiang24b_interspeech,https://www.isca-archive.org/interspeech_2024/jiang24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/jiang24b_interspeech.html,Perceiver-Prompt: Flexible Speaker Adaptation in Whisper for Chinese Disordered Speech Recognition,Speech and Language in Health: from Remote Monitoring to Medical Conversations - 2 (Special Sessions),2024,"Disordered speech recognition profound implications for improving the quality of life for individuals afflicted with, for example, dysarthria. Dysarthric speech recognition encounters challenges including limited data, substantial dissimilarities between dysarthric and non-dysarthric speakers, and significant speaker variations stemming from the disorder. This paper introduces Perceiver-Prompt, a method for speaker adaptation that utilizes P-Tuning on the Whisper large-scale model. We first fine-tune Whisper using LoRA and then integrate a trainable Perceiver to generate fixed-length speaker prompts from variable-length inputs, to improve model recognition of Chinese dysarthric speech. Experimental results from our Chinese dysarthric speech dataset demonstrate consistent improvements in recognition performance with Perceiver-Prompt. Relative reduction up to 13.04% in CER is obtained over the fine-tuned Whisper.",True
zhang24l_interspeech,https://www.isca-archive.org/interspeech_2024/zhang24l_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/zhang24l_interspeech.html,DysArinVox: DYSphonia & DYSarthria mandARIN speech corpus,Speech Disorders 2,2024,"This paper introduces DysArinVox, a new pathological speech corpus in Chinese. It included 173 participants from 27 healthy individuals and 146 voice disorders, whose various types and severities of vocal impairments as diagnosed by speech pathology experts via auditory perceptual evaluations and laryngoscopic imagery. DysArinVox is designed to provide a high-quality Chinese resource for AI-driven diagnostics and prognostics. To ensure the efficiency of corpus collection, we meticulously crafted recording scripts represent Mandarin phonetically, ensuring comprehensive syllable representation with minimal lexical complexity. Additionally, incorporating laryngoscopic images of patients into the dataset offers extra visual information, facilitating the development of advanced diagnostic frameworks. To our knowledge, this database represents the most comprehensive corpus of Chinese pathological speech to date.",True
chakraborty24_interspeech,https://www.isca-archive.org/interspeech_2024/chakraborty24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/chakraborty24_interspeech.html,On Comparing Time- and Frequency-Domain Rhythm Measures in Classifying Assamese Dialects,Prosody,2024,"The rhythm measures are one of the primary means of differentiating languages and their dialects. They are broadly classified into two groups: time domain and frequency domain measures. The time-domain rhythm measures include temporal metrics like vocalic and non-vocalic durations and their derivatives, etc, and the frequency-domain rhythm measures comprise amplitude-modulated rhythm formant trajectory frequency and magnitude. To conduct this research, we focused on four Assamese regional varieties spoken in four districts of the Indian state of Assam. Data used in this study consists of read speech data of native Assamese speakers reading the Assamese translation of the ""North Wind and the Sun"" passage. The speech data was obtained from a total of 10 speakers for each of the four varieties. The average accuracy of dialect classification using quadratic discriminant analysis turns out to be 42% and 35%, respectively, for time- and frequency-domain rhythm measures.",True
pan24b_interspeech,https://www.isca-archive.org/interspeech_2024/pan24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/pan24b_interspeech.html,COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning,Spoken Language Models for Universal Speech Processing (Special Session),2024,"We present a cost-effective method to integrate speech into a large language model (LLM), resulting in a Contextual Speech Model with Instruction-following/in-context-learning Capabilities (COSMIC) multi-modal LLM. Using GPT-3.5, we generate Speech Comprehension Test Question-Answer (SQA) pairs from speech transcriptions for supervised instruction tuning. With under 30 million trainable parameters and only 450 hours of English speech data, COSMIC demonstrates emerging capabilities in instruction-following and in-context learning. Equipped with such capabilities, COSMIC achieves a maximum 33.18 BLEU score in 0-shot EN-to-X speech to text translation (S2TT) and a significant boost in the 1-shot setting. Additionally, there is an average 25.8% relative Word Error Rate (WER) reduction for 1-shot cross-domain adaptation. COSMIC exhibits a significant automatic speech recognition (ASR) accuracy gain in contextual biasing tasks due to its instruction-following capability.",True
mujtaba24_interspeech,https://www.isca-archive.org/interspeech_2024/mujtaba24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/mujtaba24_interspeech.html,Inclusive ASR for Disfluent Speech: Cascaded Large-Scale Self-Supervised Learning with Targeted Fine-Tuning and Data Augmentation,General Topics in ASR,2024,"Automatic speech recognition (ASR) systems often falter while processing stuttering-related disfluencies---such as involuntary blocks and word repetitions---yielding inaccurate transcripts.  A critical barrier to progress is the scarcity of large, annotated disfluent speech datasets. Therefore, we present an inclusive ASR design approach, leveraging large-scale self-supervised learning on standard speech followed by targeted fine-tuning and data augmentation on a smaller, curated dataset of disfluent speech. Our data augmentation technique enriches training datasets with various disfluencies, enhancing ASR processing of these speech patterns. Results show that fine-tuning wav2vec 2.0 with even a relatively small, labeled dataset, alongside data augmentation, can significantly reduce word error rates for disfluent speech. Our approach not only advances ASR inclusivity for people who stutter, but also paves the way for ASRs that can accommodate wider speech variations.",True
li24s_interspeech,https://www.isca-archive.org/interspeech_2024/li24s_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/li24s_interspeech.html,"MSR-86K: An Evolving, Multilingual Corpus with 86,300 Hours of Transcribed Audio for Speech Recognition Research",Multilingual ASR,2024,"Recently, multilingual artificial intelligence assistants, exemplified by ChatGPT, have gained immense popularity. As a crucial gateway to human-computer interaction, multilingual automatic speech recognition (ASR) has also garnered significant attention, as evidenced by systems like Whisper. However, the proprietary nature of the training data has impeded researchersâ efforts to study multilingual ASR. This paper introduces MSR-86K, an evolving, large-scale multilingual corpus for speech recognition research. The corpus is derived from publicly accessible videos on YouTube, comprising 15 languages and a total of 86,300 hours of transcribed ASR data. We also introduce how to use the MSR-86K corpus and other open-source corpora to train a robust multilingual ASR model that is competitive with Whisper. MSR-86K will be publicly released on HuggingFace, and we believe that such a large corpus will pave new avenues for research in multilingual ASR.",True
aziz24_interspeech,https://www.isca-archive.org/interspeech_2024/aziz24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/aziz24_interspeech.html,Audio Enhancement from Multiple Crowdsourced Recordings: A Simple and Effective Baseline,Multi-Channel Speech Enhancement,2024,"With the popularity of cellular phones, events are often recorded by multiple devices from different locations and shared on social media. Several different recordings could be found for many events. Such recordings are usually noisy, where noise for each device is local and unrelated to others. This case of multiple microphones at unknown locations, capturing local, uncorrelated noise, was rarely treated in the literature. In this work we propose a simple and effective crowdsourced audio enhancement method to remove local noises at each input audio signal. Then, averaging all cleaned source signals gives an improved audio of the event. We demonstrate the effectiveness of our method using synthetic audio signals, together with real-world recordings. This simple approach can set a new baseline for crowdsourced audio enhancement for more sophisticated methods which we hope will be developed by the research community. Code, dataset, and models are available.",True
kong24_interspeech,https://www.isca-archive.org/interspeech_2024/kong24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/kong24_interspeech.html,STraDa: A Singer Traits Dataset,Speech and Multimodal Resources,2024,"There is a limited amount of large-scale public datasets that contain downloadable music audio files and rich lead singer metadata. To provide such a dataset to benefit research in singing voices, we created Singer Traits Dataset (STraDa) with two subsets: automatic-strada and annotated-strada. The automatic-strada contains twenty-five thousand tracks across numerous genres and languages of more than five thousand unique lead singers, which includes cross-validated lead singer metadata as well as other track metadata. The annotated-strada consists of two hundred tracks that are balanced in terms of 2 genders, 5 languages, and 4 age groups. To show its use for model training and bias analysis thanks to its metadata's richness and downloadable audio files, we benchmarked singer sex classification (SSC) and conducted bias analysis.",True
vakirtzian24_interspeech,https://www.isca-archive.org/interspeech_2024/vakirtzian24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/vakirtzian24_interspeech.html,Speech Recognition for Greek Dialects: A Challenging Benchmark,Cross-Lingual and Multilingual Processing,2024,"Language technologies should be judged on their usefulness in real-world use cases. Despite recent impressive progress in automatic speech recognition (ASR), an often overlooked aspect in ASR research and evaluation is language variation in the form of non-standard dialects or language varieties.  To this end, this work introduces a challenging benchmark that focuses on four varieties of Greek (Aivaliot, Cretan, Griko, Messenian) encompassing challenges related to data availability, orthographic conventions, and complexities arising from language contact. Initial experiments with state-of-the-art models and established cross-lingual transfer techniques highlight the difficulty of adapting to such low-resource varieties.",True
ogun24_interspeech,https://www.isca-archive.org/interspeech_2024/ogun24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ogun24_interspeech.html,1000 African Voices: Advancing inclusive multi-speaker multi-accent speech synthesis,Speech Synthesis: Tools and Data,2024,"Recent advances in speech synthesis have enabled many useful applications like audio directions in Google Maps, screen readers, and automated content generation on platforms like TikTok. However, these systems are mostly dominated by voices sourced from data-rich geographies with personas representative of their source data. Although 3000 of the world's languages are domiciled in Africa, African voices and personas are under-represented in these systems. As speech synthesis becomes increasingly democratized, it is desirable to increase the representation of African English accents. We present Afro-TTS, the first pan-African accented English speech synthesis system able to generate speech in 86 African accents, with 1000 personas representing the rich phonological diversity across the continent for downstream application in Education, Public Health, and Automated Content Creation. Speaker interpolation retains naturalness and accentedness, enabling the creation of new voices.",True
huang24i_interspeech,https://www.isca-archive.org/interspeech_2024/huang24i_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/huang24i_interspeech.html,Analysis of articulatory setting for L1 and L2 English speakers using MRI data,Phonetics and Phonology of Second Language Acquisition,2024,"This paper investigates the extent to which the geographical region (country) where a speaker acquired their English language affects the articulatory setting in their speech. To obtain accurate measurements for evaluating articulatory setting, we utilized a large real-time MRI corpus of vocal tract articulation. The corpus was obtained from speakers from a variety of linguistic backgrounds producing continuous English speech. We use an automated pipeline to process and extract articulatory positional information from the MRI video data. This data is used to draw comparisons between English language speakers from the United States and speakers who acquired their English in India, Korea, and China. Analysis of the speaker groups reveals statistically significant articulatory setting posture differences in multiple places of articulation.",True
kothare24_interspeech,https://www.isca-archive.org/interspeech_2024/kothare24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/kothare24_interspeech.html,How Consistent are Speech-Based Biomarkers in Remote Tracking of ALS Disease Progression Across Languages? A Case Study of English and Dutch,Speech and Language in Health: from Remote Monitoring to Medical Conversations - 2 (Special Sessions),2024,"Previous work has demonstrated the utility of speech-based digital biomarkers for remotely tracking longitudinal progression in people with Amyotrophic Lateral Sclerosis (pALS). Here, we investigate the responsiveness of these biomarkers across languages for consistency. We collected audiovisual data using a cloud-based multimodal dialogue platform, where pALS interacted with a virtual guide to perform several speaking exercises. We automatically extracted speech, linguistic and orofacial metrics from 143 English-speaking pALS (36 bulbar onset, 107 non-bulbar onset) and 26 Dutch-speaking pALS (10 bulbar, 16 non-bulbar onset). We used growth curve models to estimate the trajectory of these metrics over time. We observe that for most of these metrics, English-speaking pALS and Dutch-speaking pALS follow similar trajectories, i.e. the slopes are not statistically different from each other, demonstrating the potential of such speech-based biomarkers for remote monitoring across languages.",True
heuser24_interspeech,https://www.isca-archive.org/interspeech_2024/heuser24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/heuser24_interspeech.html,Quantification of stylistic differences in human- and ASR-produced transcripts of African American English,Evaluation of Speech Technology Systems,2024,"Common measures of accuracy used to assess the performance of automatic speech recognition (ASR) systems, as well as human transcribers, conflate multiple sources of error. Stylistic differences, such as verbatim vs non-verbatim, can play a significant role in ASR performance evaluation when differences exist between training and test datasets. The problem is compounded for speech from underrepresented varieties, where the speech to orthography mapping is not as standardized. We categorize the kinds of stylistic differences between 6 transcription versions, 4 human- and 2 ASR-produced, of 10 hours of African American English (AAE) speech. Focusing on verbatim features and AAE morphosyntactic features, we investigate the interactions of these categories with how well transcripts can be compared via word error rate (WER). The results, and overall analysis, help clarify how ASR outputs are a function of the decisions made by the training dataâs human transcribers.",True
ahn24b_interspeech,https://www.isca-archive.org/interspeech_2024/ahn24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ahn24b_interspeech.html,VoxSim: A perceptual voice similarity dataset,Databases and Progress in Methodology,2024,"This paper introduces VoxSim, a dataset of perceptual voice similarity ratings. Recent efforts to automate the assessment of speech synthesis technologies have primarily focused on predicting mean opinion score of naturalness, leaving speaker voice similarity relatively unexplored due to a lack of extensive training data. To address this, we generate about 41k utterance pairs from the VoxCeleb dataset, a widely utilised speech dataset for speaker recognition, and collect nearly 70k speaker similarity scores through a listening test. VoxSim offers a valuable resource for the development and benchmarking of speaker similarity prediction models. We provide baseline results of speaker similarity prediction models on the VoxSim test set and further demonstrate that the model trained on our dataset gener-alises to the out-of-domain VCC2018 dataset.",True
jin24_interspeech,https://www.isca-archive.org/interspeech_2024/jin24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/jin24_interspeech.html,"LibriheavyMix: A 20,000-Hour Dataset for Single-Channel Reverberant Multi-Talker Speech Separation, ASR and Speaker Diarization","Noise Robustness, Far-Field, and Multi-Talker ASR",2024,"The evolving speech processing landscape is increasingly focused on complex scenarios like meetings or cocktail parties with multiple simultaneous speakers and far-field conditions. Existing methodologies for addressing these challenges fall into two categories: multi-channel and single-channel solutions. Single-channel approaches, notable for their generality and convenience, do not require specific information about microphone arrays. This paper presents a large-scale far-field overlapping speech dataset, crafted to advance research in speech separation, recognition, and speaker diarization. This dataset is a critical resource for decoding âWho said What and Whenâ in multitalker, reverberant environments, a daunting challenge in the field. Additionally, we introduce a pipeline system encompassing speech separation, recognition, and diarization as a foundational benchmark. Evaluations on the WHAMR! dataset validate the broad applicability of the proposed data.",True
zang24_interspeech,https://www.isca-archive.org/interspeech_2024/zang24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/zang24_interspeech.html,CtrSVDD: A Benchmark Dataset and Baseline Analysis for Controlled Singing Voice Deepfake Detection,"Acoustic Event Detection, Segmentation and Classification",2024,"Recent singing voice synthesis and conversion advancements necessitate robust singing voice deepfake detection (SVDD) models. Current SVDD datasets face challenges due to limited controllability, diversity in deepfake methods, and licensing restrictions. Addressing these gaps, we introduce CtrSVDD, a large-scale, diverse collection of bonafide and deepfake singing vocals. These vocals are synthesized using state-of-the-art methods from publicly accessible singing voice datasets. CtrSVDD includes 47.64 hours of bonafide and 260.34 hours of deepfake singing vocals, spanning 14 deepfake methods and involving 164 singer identities. We also present a baseline system with flexible front-end features, evaluated against a structured train/dev/eval split. The experiments show the importance of feature selection and highlight a need for generalization towards deepfake methods that deviate further from training distribution. The CtrSVDD dataset and baseline model weights are publicly accessible.",True
naini24_interspeech,https://www.isca-archive.org/interspeech_2024/naini24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/naini24_interspeech.html,WHiSER: White House Tapes Speech Emotion Recognition Corpus,Emotion Recognition: Resources and Benchmarks,2024,"There are several applications for speech-emotion recognition (SER) systems in areas such as security and defense and healthcare. SER systems have achieved high performance when they are trained and tested in similar conditions. However, the performance often drops in more realistic and diverse conditions. Most existing SER datasets are too controlled and do not capture complex scenarios relevant to practical applications. This paper presents the White House tapes speech emotion recognition (WHiSER) corpus, which includes distant speech with real emotions from conversations in the Oval Office in 1972. This dataset is unique because it combines natural emotional expressions with various background noises, making it a perfect tool to test and improve SER models. Its real-world complexity and authenticity make the WHiSER corpus an excellent corpus for advancing emotion recognition technology, offering insights into how human emotions can be accurately recognized in complex environments.",True
gothi24_interspeech,https://www.isca-archive.org/interspeech_2024/gothi24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/gothi24_interspeech.html,A Dataset and Two-pass System for Reading Miscue Detection,Speech Assessment,2024,"Automatic speech recognition (ASR) has long been viewed as a promising solution to the resource-intensive task of oral reading fluency assessment. The demands on ASR accuracy, however, tend to be high, especially when applied to obtaining reliable reading diagnostics. The prior knowledge of reading prompts is typically used to limit the system WER. The accurate detection of mispronounced words, which can be relatively few in number, while limiting false positives, remains challenging. In this work, we present a new manually transcribed dataset of 1,110 elementary school children reading connected text in L2 English with wide-ranging proficiencies. Apart from local features derived from alternate decodings under different linguistic context constraints, we use an additional deep acoustic model. We discuss the performance gains achieved in a second pass over initial hybrid ASR hypotheses.",True
anand24_interspeech,https://www.isca-archive.org/interspeech_2024/anand24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/anand24_interspeech.html,Enhancing Out-of-Vocabulary Performance of Indian TTS Systems for Practical Applications through Low-Effort Data Strategies,Speech Synthesis: Evaluation,2024,"Publicly available TTS datasets for low-resource languages like Hindi and Tamil typically contain 10-20 hours of data, leading to poor vocabulary coverage. This limitation becomes evident in downstream applications where domain-specific vocabulary coupled with frequent code-mixing with English, results in many OOV words. To highlight this problem, we create a benchmark containing OOV words from several real-world applications. Indeed, state-of-the-art Hindi and Tamil TTS systems perform poorly on this OOV benchmark, as indicated by intelligibility tests. To improve the modelâs OOV performance, we propose a low-effort and economically viable strategy to obtain more training data. Specifically, we propose using volunteers as opposed to high quality voice artists to record words containing character bigrams unseen in the training data. We show that using such inexpensive data, the model's performance improves on OOV words, while not affecting voice quality and in-domain performance.",True
nakagome24_interspeech,https://www.isca-archive.org/interspeech_2024/nakagome24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/nakagome24_interspeech.html,InterBiasing: Boost Unseen Word Recognition through Biasing Intermediate Predictions,Neural Network Architectures for ASR 2,2024,"Despite recent advances in end-to-end speech recognition methods, their output is biased to the training dataâs vocabulary, resulting in inaccurate recognition of unknown terms or proper nouns. To improve the recognition accuracy for a given set of such terms, we propose an adaptation parameter-free approach based on Self-conditioned CTC. Our method improves the recognition accuracy of misrecognized target keywords by substituting their intermediate CTC predictions with corrected labels, which are then passed on to the subsequent layers. First, we create pairs of correct labels and recognition error instances for a keyword list using Text-to-Speech and a recognition model. We use these pairs to replace intermediate prediction errors by the labels. Conditioning the subsequent layers of the encoder on the labels, it is possible to acoustically evaluate the target keywords. Experiments conducted in Japanese demonstrated that our method successfully improved the F1 score for unknown words.",True
huang24g_interspeech,https://www.isca-archive.org/interspeech_2024/huang24g_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/huang24g_interspeech.html,Active Speaker Detection in Fisheye Meeting Scenes with Scene Spatial Spectrums,Speaker recognition evaluation and resources,2024,"Active Speaker Detection (ASD) plays a crucial role in scene understanding tasks by determining whether an on-screen person in a given scene is speaking. In this work, to address the ASD in the context of multi-party roundtable meetings, we propose a novel approach that incorporates the fusion of spatial information of the scenes. To leverage the multiple data sources of the scenes, our method involves generating audio spatial spectrum heatmaps from the multi-channel audio and integrating them with the panoramic images. Additionally, we propose the novel FisheyeMeeting dataset, which combines fisheye panoramic video recordings with muti-channel audio captured from a six-channel circular microphone array. By enabling the multi-modal model to capture audio-visual cues in multi-party meeting scenes, our approach achieves an impressive 89.11% mAP on the FisheyeMeeting dataset. Notably, this outperforms the current SOTA methods by a significant 2.3% mAP improvement.",True
nafea24_interspeech,https://www.isca-archive.org/interspeech_2024/nafea24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/nafea24_interspeech.html,AraOffence: Detecting Offensive Speech Across Dialects in Arabic Media,Speech Type Classification,2024,"Natural language processing (NLP) has made efforts towards identifying toxicity and offensive content for the text and image modalities. Despite sharing similar concerns with text and images, such as increased access to online abuse using speech, speech offensiveness research trails behind. While NLP has primarily considered English language data, speech has emphasized under-represented languages such as Swahili and Wolof. In this work, we introduce ARAOFFENSE, a dataset of scripted media in Arabic dialects labelled for offensiveness. ARAOFFENSE contains 2146 instances, of which 475 are labelled as offensive, spanning 1.55 hours of audio. We assess the capabilities of speech models to detect offensive content and present a hard-to-beat multi-modal text and audio model which outperforms the baselines by 26+% in terms of the Matthews Correlation Coefficient. Our work thus presents the first benchmark for offensive speech detection in dialectical Arabic.",True
demopoulos24_interspeech,https://www.isca-archive.org/interspeech_2024/demopoulos24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/demopoulos24_interspeech.html,Preliminary Investigation of Psychometric Properties of a Novel Multimodal Dialog Based Affect Production Task in Children and Adolescents with Autism,Connecting Speech-science and Speech-technology for Childrenâs Speech (Special Session),2024,"Impairments in nonverbal communication are a defining feature of autism spectrum disorder (ASD) and can manifest as difficulty with, or even complete lack of, communication of emotional states via production of facial affect or vocal affect. The purpose of this study was to evaluate psychometric properties of a novel multimodal dialog based Affect Production Task (APT) in children and adolescents (ages 8-17) with a diagnosis of autism (N=72) or neurotypical controls (N=37). Participants completed activities designed to quantify objective facial and vocal affect production ability using audiovisual capture. Criterion, ecological, and discriminant validity were assessed. Psychometric performance across task conditions, age, sex, and race-ethnicity also was examined. Results of this initial psychometric evaluation suggest that the APT is a valid measure of affect production abilities in children and adolescents, and that psychometric performance is invariant to age, sex, or race/ethnicity.",True
rameau24_interspeech,https://www.isca-archive.org/interspeech_2024/rameau24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/rameau24_interspeech.html,"Developing Multi-Disorder Voice Protocols: A team science approach involving clinical expertise, bioethics, standards, and DEI.",Speech and Language in Health: from Remote Monitoring to Medical Conversations - 1 (Special Session),2024,"The world of voice biomarkers is rapidly evolving thanks to the use of artificial intelligence (AI) allowing large-scale analysis of voice, speech, and respiratory sound data. The Bridge2AI-Voice project aims to build a large-scale, ethically sourced, and diverse voice database of human voices linked to health information to help fuel Voice AI research, dubbed Audiomics. The current paper describes the development of protocols of data acquisition across 4 different adult cohorts of disease (voice, respiratory, neurodegenerative diseases, mood, and anxiety disorders) using a Team Science approach for broader adoption by the research community and feedback. Demographic Surveys, Confounders Assessments, Acoustic tasks, validated patient-reported outcome (PRO) questionnaires and clinician-validated diagnostic questions were grouped in a common PART A across all cohorts and individual PART B, with cohort-specific tasks.",True
macaire24_interspeech,https://www.isca-archive.org/interspeech_2024/macaire24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/macaire24_interspeech.html,Towards Speech-to-Pictograms Translation,Spoken Machine Translation 1,2024,"The automatic translation of speech into pictogram terms (Speech-to-Pictos) represents a novel NLP task with the potential to enhance communication for individuals with language impairments. Recent research has not explored the adaptation of state-of-the-art methods to this task, despite its significance. In this work, we investigate two approaches: (1) the cascade approach, which combines a speech recognition system with a machine translation system, and (2) the end-to-end approach, which tailors a speech translation system. We compare state-of-the-art architectures trained on an aligned speech-to-pictogram dataset, specially created and released for this study. We conduct an in-depth automatic and human evaluation to analyze their behavior on pictogram translation. The results highlight the cascade approachâs ability to generate relevant translations from everyday read speech, while the end-to-end approach achieves competitive results with challenging acoustic data.",True
ewert24_interspeech,https://www.isca-archive.org/interspeech_2024/ewert24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ewert24_interspeech.html,Does the Lombard Effect Matter in Speech Separation? Introducing the Lombard-GRID-2mix Dataset,Source Separation 2,2024,"Inspired by the human ability of selective listening, speech separation aims to equip machines with the capability to disentangle cocktail party soundscapes into the individual sound sources. Recently, neural network based algorithms have been studied to work reliably under various conditions. However, to the best of our knowledge, a change in the speaking style has not yet been studied. The Lombard effect, a reflexive change in speaking style triggered by noisy environments, is a typical behavior in everyday conversational situations. In this work, we introduce a new first of its kind dataset, called Lombard-GRID-2mix, to study speech separation for two-speaker mixtures on normal speech and Lombard speech. In a comprehensive study, we show that speech separation systems can be equipped to work for both normal speech and Lombard speech. We apply a carefully designed finetuning method to enable the system to work even if noise is present in the Lombard speech for different SNR ratios.",True
nguyen24_interspeech,https://www.isca-archive.org/interspeech_2024/nguyen24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/nguyen24_interspeech.html,Identifying Speakers in Dialogue Transcripts: A Text-based Approach Using Pretrained Language Models,Speaker Recognition 2,2024,"We introduce an approach to identifying speaker names in dialogue transcripts, a crucial task for enhancing content accessibility and searchability in digital media archives. Despite the advancements in speech recognition, the task of text-based speaker identification (SpeakerID) has received limited attention, lacking large-scale, diverse datasets for effective model training. Addressing these gaps, we present a novel, large-scale dataset derived from the MediaSum corpus, encompassing transcripts from a wide range of media sources. We propose novel transformer-based models tailored for SpeakerID, leveraging contextual cues within dialogues to accurately attribute speaker names. Through extensive experiments, our best model achieves a great precision of 80.3%, setting a new benchmark for SpeakerID. The data and code are publicly available here: https: //github.com/adobe-research/speaker-identification",True
rittergutierrez24_interspeech,https://www.isca-archive.org/interspeech_2024/rittergutierrez24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/rittergutierrez24_interspeech.html,Dataset-Distillation Generative Model for Speech Emotion Recognition,Speech Emotion Recognition,2024,"Deep learning models for speech rely on large datasets, presenting computational challenges. Yet, performance hinges on training data size. Dataset Distillation (DD) aims to learn a smaller dataset without much performance degradation when training with it.  DD has been investigated in computer vision but not yet in speech. This paper presents the first approach for DD to speech targeting Speech Emotion Recognition on IEMOCAP. We employ Generative Adversarial Networks (GANs) not to mimic real data but to distil key discriminative information of IEMOCAP that is useful for downstream training. The GAN then replaces the original dataset and can sample custom synthetic dataset sizes. It performs comparably when following the original class imbalance but improves performance by 0.3% absolute UAR with balanced classes. It also reduces dataset storage and accelerates downstream training by 95% in both cases and reduces speaker information which could help for a privacy application.",True
ward24_interspeech,https://www.isca-archive.org/interspeech_2024/ward24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ward24_interspeech.html,Towards a General-Purpose Model of Perceived Pragmatic Similarity,Speech Synthesis: Other Topics 1,2024,"Models for estimating the similarity between two utterances are fundamental in speech technology.  While fairly good automatic measures exist for semantic similarity, pragmatic similarity has not been previously explored. Using a new collection of thousands of human judgments of the pragmatic similarity between utterance pairs, we train and evaluate various predictive models. The best performing model, which uses 103 features selected from HuBert's 24th layer, correlates on average 0.74 with human judges for the highest-quality data subset, and it sometimes approaches human inter-annotator agreement. We also find evidence for some degree of generality across languages.",True
take24_interspeech,https://www.isca-archive.org/interspeech_2024/take24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/take24_interspeech.html,SaSLaW: Dialogue Speech Corpus with Audio-visual Egocentric Information Toward Environment-adaptive Dialogue Speech Synthesis,Speech Synthesis: Tools and Data,2024,"This paper presents SaSLaW, a spontaneous dialogue speech corpus containing synchronous recordings of what speakers speak, listen to, and watch. Humans consider the diverse environmental factors and then control the features of their utterances in face-to-face voice communications. Spoken dialogue systems capable of this adaptation to these audio environments enable natural and seamless communications. SaSLaW was developed to model human-speech adjustment for audio environments via first-person audio-visual perceptions in spontaneous dialogues. We propose the construction methodology of SaSLaW and display the analysis result of the corpus. We additionally conducted an experiment to develop text-to-speech models using SaSLaW and evaluate their performance of adaptations to audio environments. The results indicate that models incorporating hearing-audio data output more plausible speech tailored to diverse audio environments than the vanilla text-to-speech model.",True
doukhan24_interspeech,https://www.isca-archive.org/interspeech_2024/doukhan24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/doukhan24_interspeech.html,Gender Representation in TV and Radio: Automatic Information Extraction methods versus Manual Analyses,"Speech Science, Speech Technology, and Gender (Special Session)",2024,"This study investigates the relationship between automatic information extraction descriptors and manual analyses to describe gender representation disparities in TV and Radio. Automatic descriptors, including speech time, facial categorization and speech transcriptions are compared with channel reports on a vast 32,000-hour corpus of French broadcasts from 2023. Findings reveal systemic gender imbalances, with women underrepresented compared to men across all descriptors. Notably, manual channel reports show higher womenâs presence than automatic estimates and references to women are lower than their speech time. Descriptors share common dynamics during high and low audiences, war coverage, or private versus public channels. While women are more visible than audible in French TV, this trend is inverted in news with unseen journalists depicting male protagonists. A statistical test shows 3 main effects influencing references to women: program category, channel and speaker gender.",True
vinnikov24_interspeech,https://www.isca-archive.org/interspeech_2024/vinnikov24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/vinnikov24_interspeech.html,"NOTSOFAR-1 Challenge: New Datasets, Baseline, and Tasks for Distant Meeting Transcription","Noise, Far-Field, Multi-Talker, Enhancement, Audio Classification",2024,"We introduce the first Natural Office Talkers in Settings of Far-field Audio Recordings (NOTSOFAR) Challenge, datasets, and baseline system. The challenge focuses on distant speaker diarization and automatic speech recognition (DASR) in meeting scenarios, with single-channel and known-geometry multi-channel tracks, using a single device. We launch two new datasets: First, a benchmark dataset of 280 English meetings, averaging 6 minutes each, capturing a broad spectrum of acoustic and conversational patterns across 30 rooms with 4-8 attendees. Second, a 1000-hour simulated training dataset, synthesized for real-world generalization, incorporating 15,000 real acoustic transfer functions. The NOTSOFAR-1 Challenge aims to advance research in the field of DASR, providing key resources to unlock the potential of data-driven methods, which we believe are currently constrained by the absence of comprehensive high-quality training and benchmark datasets.",True
christ24_interspeech,https://www.isca-archive.org/interspeech_2024/christ24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/christ24_interspeech.html,This Paper Had the Smartest Reviewers - Flattery Detection Utilising an Audio-Textual Transformer-Based Approach,Spoken Language Understanding,2024,"Flattery is an important aspect of human communication that facilitates social bonding, shapes perceptions, and influences behaviour through strategic compliments and praise, leveraging the power of speech to build rapport effectively. Its automatic detection can thus enhance the naturalness of human-AI interactions. To meet this need, we present a novel audio textual dataset comprising 20 hours of speech and train machine learning models for automatic flattery detection. In particular, we employ pretrained AST, Wav2Vec2, and Whisper models for the speech modality, and Whisper TTS models combined with a RoBERTa text classifier for the textual modality. Subsequently, we build a multimodal classifier by combining text and audio representations. Evaluation on unseen test data demonstrates promising results, with Unweighted Average Recall scores reaching 82.46% in audio-only experiments, 85.97% in text-only experiments, and 87.16% using a multimodal approach.",True
xie24d_interspeech,https://www.isca-archive.org/interspeech_2024/xie24d_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/xie24d_interspeech.html,FakeSound: Deepfake General Audio Detection,Acoustic Event Detection and Classification 2,2024,"With the advancement of audio generation, generative models can produce highly realistic audios. However, the proliferation of deepfake general audio can pose negative consequences. Therefore, we propose a new task, deepfake general audio detection, which aims to identify whether audio content is manipulated and to locate deepfake regions. Leveraging an automated manipulation pipeline, a dataset named FakeSound for deepfake general audio detection is proposed, and samples can be viewed on website https://FakeSoundData.github.io. The average binary accuracy of humans on all test sets is consistently below 0.6, which indicates the difficulty humans face in discerning deepfake audio and affirms the efficacy of the FakeSound dataset. A deepfake detection model utilizing a general audio pre-trained model is proposed as a benchmark system. Experimental results demonstrate that the performance of the proposed model surpasses the state-of-the-art in deepfake speech detection and human testers.",True
afonja24_interspeech,https://www.isca-archive.org/interspeech_2024/afonja24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/afonja24_interspeech.html,Performant ASR Models for Medical Entities in Accented Speech,"Accented Speech, Prosodic Features, Dialect, Emotion, Sound Classification",2024,"Recent strides in automatic speech recognition (ASR) have accelerated their application in the medical domain where their performance on accented medical named entities (NE) such as drug names, diagnoses, and lab results, is largely unknown. We rigorously evaluate multiple ASR models on a clinical English dataset of 93 African accents. Our analysis reveals that despite some models achieving low overall word error rates (WER), errors in clinical entities are higher, potentially posing substantial risks to patient safety. To empirically demonstrate this, we extract clinical entities from transcripts, develop a novel algorithm to align ASR predictions with these entities, and compute medical NE Recall, medical WER, and character error rate. Our results show that fine-tuning on accented clinical speech improves medical WER by a wide margin (25-34 % relative), improving their practical applicability in healthcare environments.",True
labrak24_interspeech,https://www.isca-archive.org/interspeech_2024/labrak24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/labrak24_interspeech.html,Zero-Shot End-To-End Spoken Question Answering In Medical Domain,Speech and Language in Health: from Remote Monitoring to Medical Conversations - 2 (Special Sessions),2024,"In the rapidly evolving landscape of spoken question-answering (SQA), the integration of large language models (LLMs) has emerged as a transformative development. Conventional approaches often entail the use of separate models for question audio transcription and answer selection, resulting in significant resource utilization and error accumulation. To tackle these challenges, we explore the effectiveness of end-to-end (E2E) methodologies for SQA in the medical domain. Our study introduces a novel zero-shot SQA approach, compared to traditional cascade systems. Through a comprehensive evaluation conducted on a new open benchmark of 8 medical tasks and 48 hours of synthetic audio, we demonstrate that our approach requires up to 14.7 times fewer resources than a combined 1.3B parameters LLM with a 1.55B parameters ASR model while improving average accuracy by 0.5%. These findings underscore the potential of E2E methodologies for SQA in resource-constrained contexts.",True
adigwe24_interspeech,https://www.isca-archive.org/interspeech_2024/adigwe24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/adigwe24_interspeech.html,What do people hear? Listenersâ Perception of Conversational Speech,Speech Synthesis: Evaluation,2024,"Conversational agents are becoming increasingly popular, prompting the need for text-to-speech (TTS) systems that sound conversational. Previous research has focused on training TTS models on elicited or found conversational speech then measuring an improved listener preference. Preference ratings cannot pinpoint why TTS voices fall short of conversational expectations, underscoring our limited understanding of conversational speaking styles. In this pilot study, we conduct interviews with naive listeners who evaluate if speech was taken from a conversation or not, then give their explanation. Our results indicate that listeners are capable of distinguishing conversational utterances from read speech from acoustic features alone. While listenersâ explanations vary, they generally allude to pronunciation, rhythmic organisation, and inappropriate prosody. Using targeted prosodic modifications to synthesise speech, we shed light on the complexity of evaluating conversational style.",True
lu24f_interspeech,https://www.isca-archive.org/interspeech_2024/lu24f_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lu24f_interspeech.html,Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio,Speech and Multimodal Resources,2024,"With the proliferation of Large Language Model (LLM) based deepfake audio, there is an urgent need for effective detection methods. Previous deepfake audio generation methods typically involve a multi-step generation process, with the final step using a vocoder to predict the waveform from hand-crafted features. However, LLM-based audio is directly generated from discrete neural codecs in an end-to-end generation process, skipping the final step of vocoder processing. This poses a significant challenge for current audio deepfake detection (ADD) models based on vocoder artifacts. To effectively detect LLM-based deepfake audio, we focus on the core of the generation process, the conversion from neural codec to waveform. We propose Codecfake dataset, which is generated by seven representative neural codec methods.  Experiment results show that codec-trained ADD models exhibit a 41.406% reduction in average equal error rate compared to vocoder-trained ADD models on the Codecfake test set.",True
chen24y_interspeech,https://www.isca-archive.org/interspeech_2024/chen24y_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/chen24y_interspeech.html,CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge,Vision and Speech,2024,"The first Chinese Continuous Visual Speech Recognition Challenge aimed to probe the performance of Large Vocabulary Continuous Visual Speech Recognition (LVC-VSR) on two tasks: (1) Single-speaker VSR for a particular speaker and (2) Multi-speaker VSR for a set of registered speakers. The challenge yielded highly successful results, with the best submission significantly outperforming the baseline, particularly in the single-speaker task. This paper comprehensively reviews the challenge, encompassing the data profile, task specifications, and baseline system construction. It also summarises the representative techniques employed by the submitted systems, highlighting the most effective approaches. Additional information and resources about this challenge can be accessed through the official website at http://cnceleb.org/competition.",True
chen24f_interspeech,https://www.isca-archive.org/interspeech_2024/chen24f_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/chen24f_interspeech.html,Detecting Empathy in Speech,Analysis of Speakers States and Traits,2024,"Empathy is the ability to understand anotherâs feelings as if we were having those feelings ourselves. It has been shown to increase to peopleâs trust and likability. Much research has been done on creating empathetic responses in text in conversational systems, yet little work has been done to identify the acoustic-prosodic speech features that can create an empathetic-sounding voice. Our contributions include 1) collection of a new empathy speech dataset, 2) identifying interpretable acoustic-prosodic features that contribute to empathy expression and 3) benchmarking the empathy detection task.",True
gerczuk24_interspeech,https://www.isca-archive.org/interspeech_2024/gerczuk24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/gerczuk24_interspeech.html,Exploring Gender-Specific Speech Patterns in Automatic Suicide Risk Assessment,Analysis of Speakers States and Traits,2024,"In emergency medicine, timely intervention for patients at risk of suicide is often hindered by delayed access to specialised psychiatric care. To bridge this gap, we introduce a speech-based approach for automatic suicide risk assessment. Our study involves a novel dataset comprising speech recordings of 20 patients who read neutral texts. We extract four speech representations encompassing interpretable and deep features. Further, we explore the impact of gender-based modelling and phrase-level normalisation. By applying gender-exclusive modelling, features extracted from an emotion fine-tuned wav2vec2.0 model can be utilised to discriminate high- from low suicide risk with a balanced accuracy of 81%. Finally, our analysis reveals a discrepancy in the relationship of speech characteristics and suicide risk between female and male subjects. For men in our dataset, suicide risk increases together with agitation while voice characteristics of female subjects point the other way.",True
li24ma_interspeech,https://www.isca-archive.org/interspeech_2024/li24ma_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/li24ma_interspeech.html,A Functional Trade-off between Prosodic and Semantic Cues in Conveying Sarcasm,Analysis of Speakers States and Traits,2024,"This study investigates the acoustic features of sarcasm and disentangles the interplay between the propensity of an utterance being used sarcastically and the presence of prosodic cues signaling sarcasm. Using a dataset of sarcastic utterances compiled from television shows, we analyze the prosodic features within utterances and key phrases belonging to three distinct sarcasm categories (embedded, propositional, and illocutionary), which vary in the degree of semantic cues present, and compare them to neutral expressions. Results show that in phrases where the sarcastic meaning is salient from the semantics, the prosodic cues are less relevant than when the sarcastic meaning is not evident from the semantics, suggesting a trade-off between prosodic and semantic cues of sarcasm at the phrase level. These findings highlight a lessened reliance on prosodic modulation in semantically dense sarcastic expressions and a nuanced interaction that shapes the communication of sarcastic intent.",True
paraskevopoulos24_interspeech,https://www.isca-archive.org/interspeech_2024/paraskevopoulos24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/paraskevopoulos24_interspeech.html,The Greek podcast corpus: Competitive speech models for low-resourced languages with weakly supervised data,Cross-Lingual and Multilingual Processing,2024,"The development of speech technologies for languages with limited digital representation poses significant challenges, pri- marily due to the scarcity of available data. This issue is exacerbated in the era of large, data-intensive models. Recent research has underscored the potential of leveraging weak supervision to augment the pool of available data. In this study, we compile an 800-hour corpus of Modern Greek from podcasts and employ Whisper large-v3 to generate silver transcriptions. This corpus is utilized to fine-tune our models, aiming to assess the efficacy of this approach in enhancing ASR performance. Our analysis spans 16 distinct podcast domains, alongside evaluations on established datasets for Modern Greek. The findings indicate consistent WER improvements, correlating with increases in both data volume and model size. Our study confirms that assembling large, weakly supervised corpora serves as a cost-effective strategy for advancing speech technologies in under-resourced languages.",True
amiriparian24_interspeech,https://www.isca-archive.org/interspeech_2024/amiriparian24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/amiriparian24_interspeech.html,ExHuBERT: Enhancing HuBERT Through Block Extension and Fine-Tuning on 37 Emotion Datasets,Speech Emotion Recognition,2024,"Foundation models have shown great promise in speech emotion recognition (SER) by leveraging their pre-trained representations to capture emotion patterns in speech signals. To further enhance SER performance across various languages and domains, we propose a novel twofold approach. First, we gather EmoSet++, a comprehensive multi-lingual, multi-cultural speech emotion corpus with 37 datasets, 150,907 samples, and a total duration of 119.5 hours. Second, we introduce ExHuBERT, an enhanced version of HuBERT achieved by backbone extension and fine-tuning on EmoSet++. We duplicate each encoder layer and its weights, then freeze the first duplicate, integrating an extra zero-initialized linear layer and skip connections to preserve functionality and ensure its adaptability for subsequent fine-tuning. Our evaluation on unseen datasets shows the efficacy of ExHuBERT, setting a new benchmark for various SER tasks. Model and details on EmoSet++: https://huggingface.co/amiriparian/ExHuBERT.",True
tapo24_interspeech,https://www.isca-archive.org/interspeech_2024/tapo24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/tapo24_interspeech.html,Leveraging Speech Data Diversity to Document Indigenous Heritage and Culture,Spoken Term Detection and Speech Retrieval,2024,"The majority of the world's 7,000 languages lack a standardized writing system. In this paper we consider one such language, Bambara, which is rarely written down but is widely spoken in Mali and neighboring countries. We explore the task of using automatic speech recognition (ASR) to transcribe culturally significant recordings focused on two domains: archival linguistic and anthropological fieldwork and contemporary oral histories performed by griots, the traditional Mande history keepers. We describe our two 6.5-hour corpora then experiment with different data configurations and multi-stage tuning from pretrained multilingual models within two neural ASR architectures. We find that while the diversity in content, style, and recording quality across the two corpora presents challenges, their commonalities can sometimes be leveraged to improve ASR accuracy. We note, however, that the diverse qualities of these corpora diminish their utility for cross-domain ASR training.",True
weirich24_interspeech,https://www.isca-archive.org/interspeech_2024/weirich24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/weirich24_interspeech.html,Gender and age based f0-variation in the German Plapper Corpus,"Voice, Tones and F0",2024,"This study is the first exploration of data collected with the smartphone-app Plapper with which participants from Germany recorded themselves reading several sentences containing target sounds for future analyses of differences in fine phonetic detail and then donated their speech for inclusion to a large speech corpus. To this date, just short of 2.000 participants have contributed to this corpus. First analyses of differences in f0 on read speech from these German participants reveals an effect of age on mean f0 in females only and additional effects of self-rated femininity/masculinity (higher mean f0 in male and female speakers with higher self-rated femininity scores and vice versa). Also, there is an effect of region with speakers in the north of Germany inexplicably having lower mean f0 values than speakers from the other regions. Results leave room for speculation on the social meaning (what do speakers code and what do listeners interpret) of differences in f0.",True
lee24i_interspeech,https://www.isca-archive.org/interspeech_2024/lee24i_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lee24i_interspeech.html,Speech-MASSIVE: A Multilingual Speech Dataset for SLU and Beyond,Spoken Language Understanding,2024,"We present Speech-MASSIVE, a multilingual Spoken Language Understanding (SLU) dataset comprising the speech counterpart for a portion of the MASSIVE textual corpus. Speech-MASSIVE covers 12 languages from different families and inherits from MASSIVE the annotations for the intent prediction and slot-filling tasks. Our extension is prompted by the scarcity of massively multilingual SLU datasets and the growing need for versatile speech datasets to assess foundation models (LLMs, speech encoders) across languages and tasks. We provide a multimodal, multitask, multilingual dataset and report SLU baselines using both cascaded and end-to-end architectures in various training scenarios (zero-shot, few-shot, and full fine-tune). Furthermore, we demonstrate the suitability of Speech-MASSIVE for benchmarking other tasks such as speech transcription, language identification, and speech translation. The dataset, models, and code are publicly available at: https://github.com/hlt-mt/Speech-MASSIVE",True
suda24_interspeech,https://www.isca-archive.org/interspeech_2024/suda24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/suda24_interspeech.html,Who Finds This Voice Attractive? A Large-Scale Experiment Using In-the-Wild Data,Topics in Paralinguistics,2024,"This paper introduces CocoNut-Humoresque, an open-source large-scale speech likability corpus that includes speech segments and their per-listener likability scores. Evaluating voice likability is essential to designing preferable voices for speech systems, such as dialogue or announcement systems. In this study, we let 885 listeners rate 1800 speech segments of a wide range of speakers regarding their likability. When constructing the corpus, we also collected the multiple speaker attributes: genders, ages, and favorite YouTube videos. Therefore, the corpus enables the large-scale statistical analysis of voice likability regarding both speaker and listener factors. This paper describes the construction methodology and preliminary data analysis to reveal the gender and age biases in voice likability. In addition, the relationship between the likability and two acoustic features, the fundamental frequencies and the x-vectors of given utterances, is also investigated.",True
veliche24_interspeech,https://www.isca-archive.org/interspeech_2024/veliche24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/veliche24_interspeech.html,Towards measuring fairness in speech recognition: Fair-Speech dataset,Speech and Multimodal Resources,2024,"The current public datasets for speech recognition (ASR) tend not to focus specifically on the fairness aspect, such as performance across different demographic groups. This paper introduces a novel dataset, Fair-Speech, a publicly released corpus to help researchers evaluate their ASR models for accuracy across a diverse set of self-reported demographic information, such as age, gender, ethnicity, geographic variation and whether the participants consider themselves native English speakers. Our dataset includes approximately 26.5K utterances in recorded speech by 593 people in the United States, who were paid to record and submit audios of themselves saying voice commands. We also provide ASR baselines, including on models trained on transcribed and untranscribed social media videos and open source models.",True
hu24b_interspeech,https://www.isca-archive.org/interspeech_2024/hu24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/hu24b_interspeech.html,Automatic pitch accent classification through image classification,Prosody,2024,"The classification of pitch accents has posed significant challenges in automatic intonation labelling. Previous research primarily adopted feature-based approaches, predicting pitch accents using a finite set of features including acoustic features (F0, duration, intensity) and lexical features. In this study, we explored a novel approach, classifying pitch accents as images represented in pixels. To evaluate this methodâs effectiveness, we used a relatively simple classification task involving only two types of pitch accents (H* and L+H*). The training of a basic neural network model for classifying images of these two types of accents (N= 2,025) yielded an average accuracy of 93.5% across 10 runs on the test set, showcasing the potential effectiveness of this new approach.",True
kye24_interspeech,https://www.isca-archive.org/interspeech_2024/kye24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/kye24_interspeech.html,Affricates in Lushootseed,Segmentals,2024,"In this study, I conduct the first acoustic analysis of affricates in Lushootseed (Coast Salish branch). The findings reveal that several acoustic measurements, such as VOT, release frication duration, intensity, spectral moments (Center of Gravity), and voice onset quality, characterize affricate contrasts with respect to their place of articulation and laryngeal type (i.e., voiced, voiceless, ejective). This study raises questions concerning (a) the acoustic-articulatory correlates of affricates, (b) typological classification of ejective affricates, and (c) the diachronic status of voiced affricates in Coast Salish languages.",True
wu24p_interspeech,https://www.isca-archive.org/interspeech_2024/wu24p_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/wu24p_interspeech.html,CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from Codec-Based Speech Synthesis Systems,Speech Coding,2024,"Current state-of-the-art (SOTA) codec-based audio synthesis systems can mimic anyone's voice with just a 3-second sample from that specific unseen speaker.  Unfortunately, malicious attackers may exploit these technologies, causing misuse and security issues. Anti-spoofing models have been developed to detect fake speech.  However, the open question of whether current SOTA anti-spoofing models can effectively counter deepfake audios from codec-based speech synthesis systems remains unanswered. In this paper, we curate an extensive collection of contemporary SOTA codec models, employing them to re-create synthesized speech.  This endeavor leads to the creation of CodecFake, the first codec-based deepfake audio dataset.  Additionally, we verify that anti-spoofing models trained on commonly used datasets cannot detect synthesized speech from current codec-based speech generation systems. The proposed CodecFake dataset empowers these models to counter this challenge effectively.",True
szekely24_interspeech,https://www.isca-archive.org/interspeech_2024/szekely24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/szekely24_interspeech.html,An inclusive approach to creating a palette of synthetic voices for gender diversity,"Speech Science, Speech Technology, and Gender (Special Session)",2024,"Mainstream text-to-speech (TTS) technologies predominantly rely on binary, cisgender speech, failing to adequately represent the diversity of gender expansive (e.g., transgender and/or nonbinary) people. This poses challenges, particularly for users of Speech Generating Devices (SGDs) seeking TTS voices that authentically reflect their identity and desired expressive nuances. This paper introduces a novel approach for constructing a palette of controllable gender-expansive TTS voices using recordings from 14 gender-expansive speakers. We employ Constrained PCA to extract gender-independent speaker identity vectors from x-vectors, using acoustic Vocal Tract Length (aVTL) as a known component. The result is applied as a speaker embedding in neural TTS, allowing control over the aVTL and several emergent properties captured as a representation of the vocal space across speakers. In addition to quantitative metrics, we present a community evaluation conducted by nonbinary SGD users.",True
bonafos24_interspeech,https://www.isca-archive.org/interspeech_2024/bonafos24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/bonafos24_interspeech.html,Dirichlet process mixture model based on topologically augmented signal representation for clustering infant vocalizations,"Computational Models of Human Language Acquisition, Perception, and Production (Special Session)",2024,"Based on audio recordings made once a month during the first 12 months of a child's life, we propose a new method for clustering this set of vocalizations. We use a topologically augmented representation of the vocalizations, employing two persistence diagrams for each vocalization: one computed on the surface of its spectrogram and one on the Takens' embeddings of the vocalization. A synthetic persistent variable is derived for each diagram and added to the MFCCs (Mel-frequency cepstral coefficients). Using this representation, we fit a non-parametric Bayesian mixture model with a Dirichlet process prior to model the number of components. This procedure leads to a novel data-driven categorization of vocal productions. Our findings reveal the presence of 8 clusters of vocalizations, allowing us to compare their temporal distribution and acoustic profiles in the first 12 months of life.",True
pesan24_interspeech,https://www.isca-archive.org/interspeech_2024/pesan24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/pesan24_interspeech.html,BESST Dataset: A Multimodal Resource for Speech-based Stress Detection and Analysis,Speech and Multimodal Resources,2024,"The Brno Extended Stress and Speech Test (BESST) dataset is a new resource for the speech research community, offering multimodal audiovisual, physiological and psychological data that enable investigations into the interplay between stress and speech. In this paper, we introduce the BESST dataset and provide a details of its design, collection protocols, and technical aspects. The dataset comprises speech samples, physiological signals (including electrocardiogram, electrodermal activity, skin temperature, and acceleration data), and video recordings from 90 subjects performing stress-inducing tasks. It comprises 16.9 hours of clean Czech speech data, averaging 15 minutes of clean speech per participant. The data collection procedure involves the induction of cognitive and physical stress induced by Reading Span task (RSPAN) and Hand Immersion (HIT) task respectively. The BESST dataset was collected under stringent ethical standards and is accessible for research and development.",True
shirahata24_interspeech,https://www.isca-archive.org/interspeech_2024/shirahata24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/shirahata24_interspeech.html,Audio-conditioned phonemic and prosodic annotation for building text-to-speech models from unlabeled speech data,Speech Synthesis: Text Processing,2024,"This paper proposes an audio-conditioned phonemic and prosodic annotation model for building text-to-speech (TTS) datasets from unlabeled speech samples. For creating a TTS dataset that consists of label-speech paired data, the proposed annotation model leverages an automatic speech recognition (ASR) model to obtain phonemic and prosodic labels from unlabeled speech samples. By fine-tuning a large-scale pretrained ASR model, we can construct the annotation model using a limited amount of label-speech paired data within an existing TTS dataset. To alleviate the shortage of label-speech paired data for training the annotation model, we generate pseudo label-speech paired data using text-only corpora and an auxiliary TTS model. This TTS model is also trained with the existing TTS dataset. Experimental results show that the TTS model trained with the dataset created by the proposed annotation method can synthesize speech as naturally as the one trained with a fully-labeled dataset.",True
miodonska24_interspeech,https://www.isca-archive.org/interspeech_2024/miodonska24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/miodonska24_interspeech.html,Frication noise features of Polish voiceless dental fricative and affricate produced by children with and without speech disorder,Phonetics and Phonology: Segmentals and Suprasegmentals,2024,"The study presents frication noise acoustic features of Polish dental voiceless sibilants (fricative /s/ and affricate /tÍ¡s/) in the speech samples collected from 106 children (83 with normative dental articulation and 23 with disorderedâinterdentalâarticulation) aged 4;11â8;0. We aimed to 1) verify the differences between the characteristics of the frication noise accompanying these two sounds and 2) investigate the influence of interdentality on the frication noise features. The analysis employed features of the noise band (fricative formants, formant-related measures, and noise energies) and linear-mixed effect models. The results showed significant acoustic differences between the voiceless dental fricative and affricate. Experiments suggest that the place of articulation (dental/interdental) can also be distinguished based on the spectral features of the frication band; this finding may be employed in computer-aided speech diagnosis tools.",True
leduc24_interspeech,https://www.isca-archive.org/interspeech_2024/leduc24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/leduc24_interspeech.html,Real-time Speech Summarization for Medical Conversations,Spoken Document Summarization,2024,"In doctor-patient conversations, identifying medically relevant information is crucial, posing the need for conversation summarization. In this work, we propose the first deployable real-time speech summarization system for real-world applications in industry, which generates a local summary after every N speech utterances within a conversation and a global summary after the end of a conversation. Our system could enhance user experience from a business standpoint, while also reducing computational costs from a technical perspective. Secondly, we present VietMed-Sum which, to our knowledge, is the first speech summarization dataset for medical conversations. Thirdly, we are the first to utilize LLM and human annotators collaboratively to create gold standard and synthetic summaries for medical conversation summarization. Finally, we present baseline results of state-of-the-art models on VietMed-Sum. All code, data (English-translated and Vietnamese) and models are available online.",True
lin24j_interspeech,https://www.isca-archive.org/interspeech_2024/lin24j_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lin24j_interspeech.html,VoxBlink2: A 100K+ Speaker Recognition Corpus and the Open-Set Speaker-Identification Benchmark,Speaker recognition evaluation and resources,2024,"In this paper, we provide a large audio-visual speaker recognition dataset, VoxBlink2, which includes approximately 10M utterances with videos from 110K+ speakers in the wild. This dataset represents a significant expansion over the VoxBlink dataset, encompassing a broader diversity of speakers and scenarios by the grace of an optimized data collection pipeline. Afterward, we explore the impact of training strategies, data scale, and model complexity on speaker verification and finally establish a new single-model state-of-the-art EER at 0.170% and minDCF at 0.006% on the VoxCeleb1-O test set. Such remarkable results motivate us to explore speaker recognition from a new challenging perspective. We raise the Open-Set Speaker-Identification task, which is designed to either match a probe utterance with a known gallery speaker or categorize it as an unknown query. Associated with this task, we design concrete benchmark and evaluation protocols. The data and model resources can be found in http://voxblink2.github.io.",True
gao24c_interspeech,https://www.isca-archive.org/interspeech_2024/gao24c_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/gao24c_interspeech.html,Enhancing Voice Wake-Up for Dysarthria: Mandarin Dysarthria Speech Corpus Release and Customized System Design,Speech Disorders 3,2024,"Smart home technology has gained widespread adoption, facilitating effortless control of devices through voice commands. However, individuals with dysarthria, a motor speech disorder, face challenges due to the variability of their speech. This paper addresses the wake-up word spotting (WWS) task for dysarthric individuals, aiming to integrate them into real-world applications. To support this, we release the open-source Mandarin Dysarthria Speech Corpus (MDSC), a dataset designed for dysarthric individuals in home environments. MDSC encompasses information on age, gender, disease types, and intelligibility evaluations. Furthermore, we perform comprehensive experimental analysis on MDSC, highlighting the challenges encountered. We also develop a customized dysarthria WWS system that showcases robustness in handling intelligibility and achieving exceptional performance. MDSC will be released on https://www.aishelltech.com/AISHELL_6B.",True
yang24d_interspeech,https://www.isca-archive.org/interspeech_2024/yang24d_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/yang24d_interspeech.html,MSceneSpeech: A Multi-Scene Speech Dataset For Expressive Speech Synthesis,Speech Synthesis: Tools and Data,2024,"We introduce an open source high-quality Mandarin TTS dataset MSceneSpeech (Multiple Scene Speech Dataset), which is intended to provide resources for expressive speech synthesis. MSceneSpeech comprises numerous audio recordings and texts performed and recorded according to daily life scenarios. Each scenario includes multiple speakers and a diverse range of prosodic styles, making it suitable for speech synthesis that entails multi-speaker style and prosody modeling. We have established a robust baseline, through the prompting mechanism, that can effectively synthesize speech characterized by both user-specific timbre and scene-specific prosody with arbitrary text input. The open source MSceneSpeech Dataset and audio samples of our baseline are available at https://speechai-demo.github.io/MSceneSpeech/.",True
chen24c_interspeech,https://www.isca-archive.org/interspeech_2024/chen24c_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/chen24c_interspeech.html,MultiPA: A Multi-task Speech Pronunciation Assessment Model for Open Response Scenarios,Pronunciation Assessment,2024,"Pronunciation assessment models designed for open response scenarios enable users to practice language skills in a manner similar to real-life communication. However, previous open-response pronunciation assessment models have predominantly focused on a single pronunciation task, such as sentence-level accuracy, rather than offering a comprehensive assessment in various aspects. We propose MultiPA, a Multitask Pronunciation Assessment model that provides sentence-level accuracy, fluency, prosody, and word-level accuracy assessment for open responses. We examined the correlation between different pronunciation tasks and showed the benefits of multi-task learning. Our model reached the state-of-the-art performance on existing in-domain data sets and effectively generalized to an out-of-domain dataset that we newly collected. The experimental results demonstrate the practical utility of our model in real-world applications.",True
tomita24_interspeech,https://www.isca-archive.org/interspeech_2024/tomita24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/tomita24_interspeech.html,Analysis and Visualization of Directional Diversity in Listening Fluency of World Englishes Speakers in the Framework of Mutual Shadowing,Speech Assessment,2024,"English is spoken as a lingua franca with a diversity of pronunciations, called accents, and they have been well studied so far. In this study, a diversity of listening behaviors are focused on, and listening disfluencies are measured objectively while listening to World Englishes (WE). When speaker X listens to Y fluently, it does not always mean that Y listens to X fluently. After collecting different passages read aloud by different WE speakers, the collected oral passages are shadowed by the speakers themselves to quantify their listening disfluencies. Results show that, when X listens to Y, X's listening disfluency becomes larger when Y's pronunciation deviates from X's to a larger degree. Further, a method is proposed to visualize simultaneously a) how fluently a speaker listens to WE speakers and b) how fluently the WE speakers listen to that specific speaker. With this visualization, WE speakers are grouped based on their communicability in global contexts.",True
elie24b_interspeech,https://www.isca-archive.org/interspeech_2024/elie24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/elie24b_interspeech.html,Articulatory Configurations across Genders and Periods in French Radio and TV archives,"Speech Science, Speech Technology, and Gender (Special Session)",2024,"This paper studies changes in articulatory configurations across genders and periods using an inversion from acoustic to articulatory parameters. From a diachronic corpus based on French media archives spanning 60 years from 1955 to 2015, automatic transcription and forced alignment allowed extracting the central frame of each vowel. More than one million frames were obtained from over a thousand speakers across gender and age categories. Their formants were used from these vocalic frames to fit the parameters of Maeda's articulatory model. Evaluations of the quality of these processes are provided. We focus here on two parameters of Maedaâs model linked to total vocal tract length: the relative position of the larynx (higher for females) and the lips protrusion (more protruded for males). Implications for voice quality across genders are discussed. The effect across periods seems gender independent; thus, the assertion that females lowered their pitch with time is not supported.",True
saito24_interspeech,https://www.isca-archive.org/interspeech_2024/saito24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/saito24_interspeech.html,SRC4VC: Smartphone-Recorded Corpus for Voice Conversion Benchmark,Speech Synthesis: Tools and Data,2024,"We present SRC4VC, a new corpus containing 11 hours of speech recorded on smartphones by 100 Japanese speakers. Although high-quality multi-speaker corpora can advance voice conversion (VC) technologies, they are not always suitable for testing VC when low-quality speech recording is given as the input. To this end, we first asked 100 crowdworkers to record their voice samples using smartphones. Then, we annotated the recorded samples with speaker-wise recording-quality scores and utterance-wise perceived emotion labels. We also benchmark SRC4VC on any-to-any VC, in which we trained a multi-speaker VC model on high-quality speech and used the SRC4VC speakers' voice samples as the source in VC. The results show that the recording quality mismatch between the training and evaluation data significantly degrades the VC performance, which can be improved by applying speech enhancement to the low-quality source speech samples.",True
wang24b_interspeech,https://www.isca-archive.org/interspeech_2024/wang24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/wang24b_interspeech.html,GLOBE: A High-quality English Corpus with Global Accents for Zero-shot Speaker Adaptive Text-to-Speech,Speech and Multimodal Resources,2024,"This paper introduces GLOBE, a high-quality English corpus with worldwide accents, specifically designed to address the limitations of current zero-shot speaker adaptive Text-to-Speech (TTS) systems that exhibit poor generalizability in adapting to speakers with accents. Compared to commonly used English corpora, such as LibriTTS and VCTK, GLOBE is unique in its inclusion of utterances from 23,519 speakers and covers 164 accents worldwide, along with detailed metadata for these speakers. Compared to its original corpus, i.e., Common Voice, GLOBE significantly improves the quality of the speech data through rigorous filtering and enhancement processes, while also populating all missing speaker metadata. The final curated GLOBE corpus includes 535 hours of speech data at a 24 kHz sampling rate. Our benchmark results indicate that the speaker adaptive TTS model trained on the GLOBE corpus can synthesize speech with better speaker similarity and comparable naturalness than that trained on other popular corpora. We will release GLOBE publicly after acceptance. The GLOBE dataset is available at https://globecorpus.github.io/.",True
leung24_interspeech,https://www.isca-archive.org/interspeech_2024/leung24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/leung24_interspeech.html,Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech Synthesis,Speech Disorders 3,2024,"Automatic speech recognition (ASR) research has achieved impressive performance in recent years and has significant potential for enabling access for people with dysarthria (PwD) in augmentative and alternative communication (AAC) and home environment systems. However, progress in dysarthric ASR (DASR) has been limited by high variability in dysarthric speech and limited public availability of dysarthric training data. This paper demonstrates that data augmentation using text-to-dysarthic-speech (TTDS) synthesis for finetuning large ASR models is effective for DASR. Specifically, diffusion-based text-to-speech (TTS) models can produce speech samples similar to dysarthric speech that can be used as additional training data for fine-tuning ASR foundation models, in this case Whisper. Results show improved synthesis metrics and ASR performance for the proposed multi-speaker diffusion-based TTDS data augmentation for ASR fine-tuning compared to current DASR baselines.",True
goel24_interspeech,https://www.isca-archive.org/interspeech_2024/goel24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/goel24_interspeech.html,Exploring Multilingual Unseen Speaker Emotion Recognition: Leveraging Co-Attention Cues in Multitask Learning,"Accented Speech, Prosodic Features, Dialect, Emotion, Sound Classification",2024,"Advent of modern deep learning techniques has given rise to advancements in the field of Speech Emotion Recognition (SER). However, most systems prevalent in the field fail to generalize to speakers not seen during training. This study focuses on handling challenges of multilingual SER, specifically on unseen speakers. We introduce CAMuLeNet, a novel architecture leveraging co-attention based fusion and multitask learning to address this problem. Additionally, we benchmark pretrained encoders of Whisper, HuBERT, Wav2Vec2.0, and WavLM using 10-fold leave-speaker-out cross-validation on five existing multilingual benchmark datasets: IEMOCAP, RAVDESS, CREMA-D, EmoDB and CaFE and, release a novel dataset for SER on the Hindi language (BhavVani). CAMuLeNet shows an average improvement of approximately 8% over all benchmarks on unseen speakers determined by our cross-validation strategy.",True
turetzky24_interspeech,https://www.isca-archive.org/interspeech_2024/turetzky24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/turetzky24_interspeech.html,HebDB: a Weakly Supervised Dataset for Hebrew Speech Processing,Speech and Multimodal Resources,2024,"We present HebDB, a weakly supervised dataset for spoken language processing in the Hebrew language. HebDB offers roughly 2500 hours of natural and spontaneous speech recordings in the Hebrew language, consisting of a large variety of speakers and topics. We provide raw recordings together with a pre-processed, weakly supervised, and filtered version. The goal of HebDB is to further enhance research and development of spoken language processing tools for the Hebrew language. Hence, we additionally provide two baseline systems for Automatic Speech Recognition (ASR): (i) a self-supervised model; and (ii) a fully supervised model. We present the performance of these two methods optimized on HebDB and compare them to current multi-lingual ASR alternatives. Results suggest the proposed method reaches better results than the evaluated baselines considering similar model sizes. Dataset, code, and models are publicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/.",True
ryumina24_interspeech,https://www.isca-archive.org/interspeech_2024/ryumina24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ryumina24_interspeech.html,OCEAN-AI: open multimodal framework for personality traits assessment and HR-processes automatization,Show and Tell 3,2024,"Human personality traits (PT) reflect individual differences in patterns of thinking, feeling, and behaving. Knowledge on PT may be useful in many applied tasks in our everyday live. In this paper, we present a first open-source multimodal framework called OCEAN-AI for PT assessment (PTA) and HR-processes automatization. Our framework performs PTA analyzing three modalities, including audio, video, and text, and includes three processing modules. All the modules extract heterogeneous (deep neural and hand-crafted) features and use them for a com-
plex analysis of humanâs behavior. The final fourth module aggregates these six feature sets by a Siamese neural network with a gated attention mechanism. Our framework was tested on two free-available corpora, including First Impressions v2 and our MuPTA, and achieved the best results. Applying our framework, a user can automate solutions of some practical applied tasks, such as ranking potential candidates by professional responsibilities, forming efficient work teams and so on.",True
yan24b_interspeech,https://www.isca-archive.org/interspeech_2024/yan24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/yan24b_interspeech.html,Auditory Attention Decoding in Four-Talker Environment with EEG,Biosignal-enabled Spoken Communication,2024,"Auditory Attention Decoding (AAD) is a technique that determines the focus of a listener's attention in complex auditory scenes according to cortical neural responses. Existing research largely examines two-talker scenarios, insufficient for real-world complexity. This study introduced a new AAD database for a four-talker scenario with speeches from four distinct talkers simultaneously presented and spatially separated, and listeners' EEG was recorded. Temporal response functions (TRFs) analysis showed that attended speech TRFs are stronger than each unattended speech. AAD methods based on stimulus-reconstruction (SR) and cortical spatial lateralization were employed and compared. Results indicated decoding accuracy of 77.5% in 60s (chance level of 25%) using SR. Using auditory spatial attention detection (ASAD) methods also indicated high accuracy (94.7% with DenseNet-3D in 1s), demonstrating ASAD methods' generalization performance.",True
williams24_interspeech,https://www.isca-archive.org/interspeech_2024/williams24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/williams24_interspeech.html,Predicting Acute Pain Levels Implicitly from Vocal Features,Speech and Language in Health: from Remote Monitoring to Medical Conversations - 1 (Special Session),2024,"Evaluating pain in speech represents a critical challenge in high-stakes clinical scenarios, from analgesia delivery to emergency triage. Clinicians have predominantly relied on direct verbal communication of pain which is difficult for patients with communication barriers, such as those affected by stroke, autism, and learning difficulties. Many previous efforts have focused on multimodal data which does not suit all clinical applications. Our work is the first to collect a new English speech dataset wherein we have induced acute pain in adults using a cold pressor task protocol and recorded subjects reading sentences out loud. We report pain discrimination performance as F1 scores from binary (pain vs. no pain) and three-class (mild, moderate, severe) prediction tasks, and support our results with explainable feature analysis. Our work is a step towards providing medical decision support for pain evaluation from speech to improve care across diverse and remote healthcare settings.",True
maciejewski24_interspeech,https://www.isca-archive.org/interspeech_2024/maciejewski24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/maciejewski24_interspeech.html,Evaluating the Santa Barbara Corpus: Challenges of the Breadth of Conversational Spoken Language,Speaker Recognition 1,2024,"As speech technology has matured, there has been a push towards systems that can process conversational speech, reflecting the so-called âcocktail party problem,â which includes not only more challenging acoustic conditions, but also necessitates solutions to new problems, such as identifying who spoke when and processing multiple concurrent streams of speech. Such problems have been approached primarily via corpora comprising business meetings and dinner parties, overlooking the broad range of conversational dynamics and speaker demographics that fall under the category of multi-talker speech. To this end, we introduce the use of the Santa Barbara Corpus of Spoken American English for evaluation of speech technologyâincluding preparing the corpus and annotations for automatic processing, demonstrating the failure of state-of-the-art systems to withstand the heterogeneity of conditions, and highlighting the situations where standard methods struggle to perform at all.",True
netzorg24_interspeech,https://www.isca-archive.org/interspeech_2024/netzorg24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/netzorg24_interspeech.html,Speech After Gender: A Trans-Feminine Perspective on Next Steps for Speech Science and Technology,"Speech Science, Speech Technology, and Gender (Special Session)",2024,"As experts in voice modification, trans-feminine gender-affirming voice teachers have unique perspectives on voice that confound current understandings of speaker identity. To demonstrate this, we present the Versatile Voice Dataset (VVD), a collection of three speakers modifying their voices along gendered axes. The VVD illustrates that current approaches in speaker modeling, based on categorical notions of gender and a static understanding of vocal texture, fail to account for the flexibility of the vocal tract. Utilizing publicly-available speaker embeddings, we demonstrate that gender classification systems are highly sensitive to voice modification, and speaker verification systems fail to identify voices as coming from the same speaker as voice modification becomes more drastic. As one path towards moving beyond categorical and static notions of speaker identity, we propose modeling individual qualities of vocal texture such as pitch, resonance, and weight.",True
nijat24_interspeech,https://www.isca-archive.org/interspeech_2024/nijat24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/nijat24_interspeech.html,UY/CH-CHILD -- A Public Chinese L2 Speech Database of Uyghur Children,Databases and Progress in Methodology,2024,"Exploring the progression of pronunciation skills in second language (L2) acquisition among children presents an intriguing research avenue. Yet, the comprehension of this process for Uyghur children learning Chinese as their L2 has been constrained by a scarcity of speech data. To bridge this gap, we have developed the UY/CH-CHILD speech database, comprising 29,061 samples of Chinese words articulated by 106 Uyghur children from both kindergartens and primary schools. The database includes carefully labelled syllables and tones by native Chinese speakers. To showcase the utility of this novel resource, we conducted a comparative analysis of pronunciation errors between the kindergarten and primary school groups, unveiling interesting insights into the evolution of pronunciation proficiency in Uyghur children as they mature. 
The database can be downloaded online at http://child.cslt.org.",True
hai24_interspeech,https://www.isca-archive.org/interspeech_2024/hai24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/hai24_interspeech.html,DreamVoice: Text-Guided Voice Conversion,Speech Synthesis: Voice Conversion 3,2024,"Generative voice technologies are rapidly evolving, offering opportunities for more personalized and inclusive experiences. Traditional one-shot voice conversion (VC) requires a target recording during inference, limiting ease of usage in generating desired voice timbres. Text-guided generation offers an intuitive solution to convert voices to desired ""DreamVoices"" according to the users' needs. Our paper presents two major contributions to VC technology: (1) DreamVoiceDB, a robust dataset of voice timbre annotations for 900 speakers from VCTK and LibriTTS. (2) Two text-guided VC methods: DreamVC, an end-to-end diffusion-based text-guided VC model; and DreamVG, a versatile text-to-voice generation plugin that can be combined with any one-shot VC models. The experimental results demonstrate that our proposed methods trained on the DreamVoiceDB dataset generate voice timbres accurately aligned with the text prompt and achieve high-quality VC.",True
song24c_interspeech,https://www.isca-archive.org/interspeech_2024/song24c_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/song24c_interspeech.html,"ED-sKWS: Early-Decision Spiking Neural Networks for Rapid, and Energy-Efficient Keyword Spotting",Computational Resource Constrained ASR,2024,"Keyword Spotting (KWS) is essential in edge computing requiring rapid and energy-efficient responses. Spiking Neural Networks (SNNs) are well-suited for KWS for their efficiency and temporal capacity for speech. To further reduce the latency and energy consumption, this study introduces ED-sKWS, an SNN-based KWS model with an early-decision mechanism that can stop speech processing and output the result before the end of speech utterance. Furthermore, we introduce a Cumulative Temporal (CT) loss that can enhance prediction accuracy at both the intermediate and final timesteps. To evaluate early-decision performance, we present the SC-100 dataset including 100 speech commands with beginning and end timestamp annotation. Experiments on the Google Speech Commands v2 and our SC-100 datasets show that ED-sKWS maintains competitive accuracy with 61% timesteps and 52% energy consumption compared to SNN models without early-decision mechanism, ensuring rapid response and energy efficiency.",True
jia24_interspeech,https://www.isca-archive.org/interspeech_2024/jia24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/jia24_interspeech.html,LLM-Driven Multimodal Opinion Expression Identification,Multimodality and Foundation Models,2024,"Opinion Expression Identification (OEI) is essential in NLP for applications ranging from voice assistants to depression diagnosis. This study extends OEI to encompass multimodal inputs, underlining the significance of auditory cues in delivering emotional subtleties beyond the capabilities of text. We introduce a novel multimodal OEI (MOEI) task, integrating text and speech to mirror real-world scenarios. Utilizing CMU MOSEI and IEMOCAP datasets, we construct the CI-MOEI dataset. Additionally, Text-to-Speech (TTS) technology is applied to the MPQA dataset to obtain the CIM-OEI dataset. We design a template for the OEI task to take full advantage of the generative power of large language models (LLMs). Advancing further, we propose an LLM-driven method STOEI, which combines speech and text modal to identify opinion expressions. Our experiments demonstrate that MOEI significantly improves the performance while our method outperforms existing methods by 9.20% and obtains SOTA results.",True
lin24f_interspeech,https://www.isca-archive.org/interspeech_2024/lin24f_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lin24f_interspeech.html,ASA: An Auditory Spatial Attention Dataset with Multiple Speaking Locations,Biosignal-enabled Spoken Communication,2024,"Recent studies have demonstrated the feasibility of localizing an attended sound source from electroencephalography (EEG) signals in a cocktail party scenario. This is referred to as EEG-enabled Auditory Spatial Attention Detection (ASAD). Despite the promise, there is a lack of ASAD datasets. Most existing ASAD datasets are recorded from two speaking locations. To bridge this gap, we introduce a new Auditory Spatial Attention (ASA) dataset, featuring multiple speaking locations of sound sources. The new dataset is designed to challenge and refine deep neural network solutions in real-world applications. Furthermore, we build a channel attention convolutional neural network (CA-CNN) as a reference model for ASA, that serves as a competitive benchmark for future studies.",True
wei24_interspeech,https://www.isca-archive.org/interspeech_2024/wei24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/wei24_interspeech.html,Prompt Tuning for Speech Recognition on Unknown Spoken Name Entities,Contextual Biasing and Adaptation,2024,"This paper explores the challenge of recognising relevant but previously unheard named entities in spoken input. This scenario pertains to real-world applications where establishing an automatic speech recognition (ASR) model trained on new entity phrases may not be efficient. We propose a technique that involves fine-tuning a Whisper model with a list of entity phrases as prompts. We establish a task-specific dataset where stratification of different entity phrases supports evaluation of three different scenarios in which entities might be encountered. We focus our analysis on a seen-but-unheard scenario, reflecting a situation where only textual representations of novel entity phrases are available for a commercial banking assistant bot. We show that a model tuned to anticipate prompts reflecting novel named entities makes substantial improvements in entity recall over non-tuned baseline models, and meaningful improvements in performance over models fine-tuned without a prompt.",True
li24ha_interspeech,https://www.isca-archive.org/interspeech_2024/li24ha_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/li24ha_interspeech.html,DiveSound: LLM-Assisted Automatic Taxonomy Construction for Diverse Audio Generation,Speech and Audio Modelling,2024,"Audio generation has attracted significant attention. Despite remarkable enhancement in audio quality, existing models overlook diversity evaluation. This is partially due to the lack of a systematic sound class diversity framework and a matching dataset. To address these issues, we propose DiveSound, a novel framework for constructing multimodal datasets with in-class diversified taxonomy, assisted by large language models. As both textual and visual information can be utilized to guide diverse generation, DiveSound leverages multimodal contrastive representations in data construction. Our framework is highly autonomous and can be easily scaled up. We provide a text-audio-image aligned diversity dataset whose sound event class tags have an average of 2.42 subcategories. Text-to-audio experiments on the constructed dataset show a substantial increase of diversity with the help of the guidance of visual information. Our samples are available at https://divesounddemo.github.io",True
phukan24_interspeech,https://www.isca-archive.org/interspeech_2024/phukan24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/phukan24_interspeech.html,Towards Multilingual Audio-Visual Question Answering,Question Answering from Speech and Spoken Dialogue Systems,2024,"In this paper, we work towards extending Audio-Visual Question Answering (AVQA) to multilingual settings. Existing AVQA research has predominantly revolved around English and replicating it for addressing AVQA in other languages requires a substantial allocation of resources. As a scalable solution, we leverage machine translation and present two multilingual AVQA datasets for eight languages created from existing benchmark AVQA datasets. This prevents extra human annotation efforts of collecting questions and answers manually. To this end, we propose, MERA framework, by leveraging state-of-the-art (SOTA) video, audio, and textual foundation models for AVQA in multiple languages. We introduce a suite of models namely MERA-L, MERA-C, MERA-T with varied model architectures to benchmark the proposed datasets. We believe our work will open new research directions and act as a reference benchmark for future works in multilingual AVQA.",True
lin24_interspeech,https://www.isca-archive.org/interspeech_2024/lin24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lin24_interspeech.html,SimuSOE: A Simulated Snoring Dataset for Obstructive Sleep Apnea-Hypopnea Syndrome Evaluation during Wakefulness,Detection and Classification of Bioacoustic Signals,2024,"Obstructive Sleep Apnea-Hypopnea Syndrome (OSAHS) is a prevalent chronic breathing disorder caused by upper airway obstruction. Previous studies advanced OSAHS evaluation through machine learning-based systems trained on sleep snoring or speech signal datasets. However, constructing datasets for training a precise and rapid OSAHS evaluation system poses a challenge, since 1) it is time-consuming to collect sleep snores and 2) the speech signal is limited in reflecting upper airway obstruction. In this paper, we propose a new snoring dataset for OSAHS evaluation, named SimuSOE, in which a novel and time-effective snoring collection method is introduced for tackling the above problems. In particular, we adopt simulated snoring which is a type of snore intentionally emitted by patients to replace natural snoring. Experimental results indicate that the simulated snoring signal during wakefulness can serve as an effective feature in OSAHS preliminary screening.",True
lee24e_interspeech,https://www.isca-archive.org/interspeech_2024/lee24e_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lee24e_interspeech.html,Automatic Assessment of Speech Production Skills for Children with Cochlear Implants Using Wav2Vec2.0 Acoustic Embeddings,Hearing Disorders,2024,"This study introduces an automatic assessment model for speech production skills of children with cochlear implants (CIs) to support home-based speech therapy. The model employs acoustic embeddings from self-supervised models and considers speech traits of both normal hearing (NH) adults and children, which is a novel method for evaluating speech of children with disorders. It combines phoneme embeddings and two acoustic embeddings from Wav2Vec2.0 models, each trained on the speech of NH adults and children, via multi-head attention. Using a speech corpus of Korean-speaking children with CIs, our model outperforms single-embedding methods in a Pearson correlation coefficient between predicted and expert-rated scores, with a relative improvement of 51%. The results highlight the effectiveness of Wav2Vec2.0 acoustic embeddings and the importance of incorporating both of typical speech patterns of NH adults and children in assessing speech production skills in children with CIs.",True
sungbin24_interspeech,https://www.isca-archive.org/interspeech_2024/sungbin24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/sungbin24_interspeech.html,MultiTalk: Enhancing 3D Talking Head Generation Across Languages with Multilingual Video Dataset,Speech and Multimodal Resources,2024,"Recent studies in speech-driven 3D talking head generation have achieved convincing results in verbal articulations. However, generating accurate lip-syncs degrades when applied to input speech in other languages, possibly due to the lack of datasets covering a broad spectrum of facial movements across languages. In this work, we introduce a novel task to generate 3D talking heads from speeches of diverse languages. We collect a new multilingual 2D video dataset comprising over 420 hours of talking videos in 20 languages. With our proposed dataset, we present a multilingual enhanced model that incorporates language-specific style embeddings, enabling it to capture the unique mouth movements associated with each language. Additionally, we present a metric for assessing lip-sync accuracy in multilingual settings. We demonstrate that training a 3D talking head model with our proposed dataset significantly enhances its multilingual performance. Codes and datasets are available at https://multitalk.github.io/.",True
