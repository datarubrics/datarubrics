id,url_link,html_url,title,category,year,abstract,score
turrisi21_interspeech,https://www.isca-archive.org/interspeech_2021/turrisi21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/turrisi21_interspeech.html,EasyCall Corpus: A Dysarthric Speech Dataset,Disordered Speech,2021,"This paper introduces a new dysarthric speech command dataset in Italian,
called EasyCall corpus. The dataset consists of 21386 audio recordings
from 24 healthy and 31 dysarthric speakers, whose individual degree
of speech impairment was assessed by neurologists through the Therapy
Outcome Measure. The corpus aims at providing a resource for the development
of ASR-based assistive technologies for patients with dysarthria. In
particular, it may be exploited to develop a voice-controlled contact
application for commercial smartphones, aiming at improving dysarthric
patients’ ability to communicate with their family and caregivers.
Before recording the dataset, participants were administered a survey
to evaluate which commands are more likely to be employed by dysarthric
individuals in a voice-controlled contact application. In addition,
the dataset includes a list of non-commands (i.e., words near/inside
commands or phonetically close to commands) that can be leveraged to
build a more robust command recognition system. At present commercial
ASR systems perform poorly on the EasyCall Corpus as we report in this
paper. This result corroborates the need for dysarthric speech corpora
for developing effective assistive technologies. To the best of our
knowledge, this database represents the richest corpus of dysarthric
speech to date.",True
pan21_interspeech,https://www.isca-archive.org/interspeech_2021/pan21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/pan21_interspeech.html,Perceptual Contributions of Vowels and Consonant-Vowel Transitions in Understanding Time-Compressed Mandarin Sentences,Speech Enhancement and Intelligibility,2021,"Many early studies reported the importance of vowels and vowel-consonant
transitions to speech intelligibility. The present work assessed their
perceptual impacts to the understanding of time-compressed sentences,
which could be used to measure the temporal acuity during speech understanding.
Mandarin sentences were edited to selectively preserve vowel centers
or vowel-consonant transitional segments, and compress the rest regions
with equipment time compression rates (TCRs) up to 3, including conditions
only preserving vowel centers or vowel-consonant transitions. The processed
stimuli were presented to normal-hearing listeners to recognize. Results
showed that, consistent with the segmental contributions in understanding
uncompressed speech, the vowel-only time-compressed stimuli were highly
intelligible (i.e., intelligibility score >85%) at a TCR around
3, and vowel-consonant transitions carried important intelligibility
information in understanding time-compressed sentences. The time-compression
conditions in the present work provided higher intelligibility scores
than their counterparties in understanding the PSOLA-processed time-compressed
sentences with TCRs around 3. The findings in this work suggested that
the design of time compression processing could be guided towards selectively
preserving perceptually important speech segments (e.g., vowels) in
the future.",True
nguyen21_interspeech,https://www.isca-archive.org/interspeech_2021/nguyen21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/nguyen21_interspeech.html,User-Initiated Repetition-Based Recovery in Multi-Utterance Dialogue Systems,Spoken Dialogue Systems I,2021,"Recognition errors are common in human communication. Similar errors
often lead to unwanted behaviour in dialogue systems or virtual assistants.
In human communication, we can recover from them by repeating misrecognized
words or phrases; however in human-machine communication this recovery
mechanism is not available. In this paper, we attempt to bridge this
gap and present a system that allows a user to correct speech recognition
errors in a virtual assistant by repeating misunderstood words. When
a user repeats part of the phrase the system rewrites the original
query to incorporate the correction. This rewrite allows the virtual
assistant to understand the original query successfully. We present
an end-to-end 2-step attention pointer network that can generate the
the rewritten query by merging together the incorrectly understood
utterance with the correction follow-up. We evaluate the model on data
collected for this task and compare the proposed model to a rule-based
baseline and a standard pointer network. We show that rewriting the
original query is an effective way to handle repetition-based recovery
and that the proposed model outperforms the rule based baseline, reducing
Word Error Rate by 19% relative at 2% False Alarm Rate on annotated
data.",True
siminyu21_interspeech,https://www.isca-archive.org/interspeech_2021/siminyu21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/siminyu21_interspeech.html,Phoneme Recognition Through Fine Tuning of Phonetic Representations: A Case Study on Luhya Language Varieties,"Topics in ASR: Robustness, Feature Extraction, and Far-Field ASR",2021,"Models pre-trained on multiple languages have shown significant promise
for improving speech recognition, particularly for low-resource languages.
In this work, we focus on phoneme recognition using Allosaurus, a method
for multilingual recognition based on phonetic annotation, which incorporates
phonological knowledge through a language-dependent allophone layer
that associates a universal narrow phone-set with the phonemes that
appear in each language. To evaluate in a challenging real-world scenario,
we curate phone recognition datasets for Bukusu and Saamia, two varieties
of the Luhya language cluster of western Kenya and eastern Uganda.
To our knowledge, these datasets are the first of their kind. We carry
out similar experiments on the dataset of an endangered Tangkhulic
language, East Tusom, a Tibeto-Burman language variety spoken mostly
in India. We explore both zero-shot and few-shot recognition by fine-tuning
using datasets of varying sizes (10 to 1000 utterances). We find that
fine-tuning of Allosaurus, even with just 100 utterances, leads to
significant improvements in phone error rates.",True
zhou21_interspeech,https://www.isca-archive.org/interspeech_2021/zhou21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/zhou21_interspeech.html,Audio-Visual Information Fusion Using Cross-Modal Teacher-Student Learning for Voice Activity Detection in Realistic Environments,Voice Activity Detection and Keyword Spotting,2021,"We propose an information fusion approach to audio-visual voice activity
detection (AV-VAD) based on cross-modal teacher-student learning leveraging
on factorized bilinear pooling (FBP) and Kullback-Leibler (KL) regularization.
First, we design an audio-visual network by using FBP fusion to fully
utilize the interaction between audio and video modalities. Next, to
transfer the rich information in audio-based VAD (A-VAD) model trained
with a massive audio-only dataset to AV-VAD model built with relatively
limited multi-modal data, a cross-modal teacher-student learning framework
is then proposed based on cross entropy with regulated KL-divergence.
Finally, evaluated on an in-house dataset recorded in realistic conditions
using standard VAD metrics, the proposed approach yields consistent
and significant improvements over other state-of-the-art techniques.
Moreover, by applying our AV-VAD technique to an audio-visual Chinese
speech recognition task, the character error rate is reduced by 24.15%
and 8.66% from A-VAD and the baseline AV-VAD systems, respectively.",True
bonneau21_interspeech,https://www.isca-archive.org/interspeech_2021/bonneau21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/bonneau21_interspeech.html,Voicing Assimilations by French Speakers of German in Stop-Fricative Sequences,Voice and Voicing,2021,"Voicing assimilations inside groups of obstruents occur in opposite
directions in French and German, where they are respectively regressive
and progressive. The aim of the study is to investigate (1) whether
non native speakers (here French learners of German) are apt to acquire
subtle L2 specificities like assimilation direction, although they
are not aware of their very existence, or (2) whether their productions
depend essentially upon other factors, in particular consonant place
of articulation. To that purpose, a corpus made up of groups of obstruents
(/t/ followed by /z/, /v/ or /f/) embedded into sentences has been
recorded by 16 French learners of German (beginners and advanced speakers).
The consonants are separated by a word or a syllable boundary. Results,
derived from the analysis of consonant periodicity and duration, do
not stand for an acquisition of progressive assimilation, even by advanced
speakers, and do not show differences between the productions of advanced
speakers and beginners. On the contrary the boundary type and the consonant
place of articulation play an important role in the presence or absence
of voicing inside obstruent groups. The role of phonetic, universal
mechanisms against linguistic specific rules is discussed to interpret
the data.",True
urooj21_interspeech,https://www.isca-archive.org/interspeech_2021/urooj21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/urooj21_interspeech.html,Acoustic and Prosodic Correlates of Emotions in Urdu Speech,Voice and Voicing,2021,"Emotional speech corpora exhibit differences in duration, intensity
and fundamental frequency. We investigated acoustic as well as prosodic
correlates of emotional speech in Urdu. We recorded a corpus of 23
sentences from four speakers of Urdu covering four emotional states.
Main results show that: a) sadness exhibits lowest utterance rate,
lowest intensity and narrow pitch range, b) anger exhibits highest
utterance rate, highest intensity and wider pitch range, and c) happiness
exhibits higher utterance rate and wider pitch range as compared to
neutral and sadness; but no significant differences are found between
the intensity and pitch range of anger and happiness. The analysis
also shows differences in terms of pitch or phrase accents and boundary
tones.",True
soo21_interspeech,https://www.isca-archive.org/interspeech_2021/soo21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/soo21_interspeech.html,Sound Change in Spontaneous Bilingual Speech: A Corpus Study on the Cantonese n-l Merger in Cantonese-English Bilinguals,Voice and Voicing,2021,"In Cantonese and several other Chinese languages, /n/ is merging with
/l/. The Cantonese merger appears categorical, with /n/ becoming /l/
word-initially. This project aims to describe the status of /n/ and
/l/ in bilingual Cantonese and English speech to better understand
individual differences at the interface of crosslinguistic influence
and sound change. We examine bilingual speech using the SpiCE corpus,
composed of speech from 34 early Cantonese-English bilinguals. Acoustic
measures were collected on pre-vocalic nasal and lateral onsets in
both languages. If bilinguals maintain separate representations for
corresponding segments across languages, smaller differences between
/n/ and /l/ are predicted in Cantonese compared to English. Measures
of mid-frequency spectral tilt suggest that the /n/ and /l/ contrast
is robustly maintained in English, but not Cantonese. The spacing of
F2-F1 suggests small differences between Cantonese /n/ and /l/, and
robust differences in English. While cross-language categories appear
independent, substantial individual differences exist in the data.
These data contribute to the understanding of the /n/ and /l/ merger
in Cantonese and other Chinese languages, in addition to providing
empirical and theoretical insights into crosslinguistic influence in
early bilinguals.",True
lalhminghlui21_interspeech,https://www.isca-archive.org/interspeech_2021/lalhminghlui21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/lalhminghlui21_interspeech.html,Characterizing Voiced and Voiceless Nasals in Mizo,Voice and Voicing,2021,"Mizo has voicing contrasts in nasals. This study investigates the acoustic
properties of Mizo voiced and voiceless nasals using nasometric measurements.
The dual channel data obtained for Mizo nasals is separated into oral
and nasal channels and nasalance is calculated at every 10% of the
duration of the nasals. Apart from that, the amount of voicing and
duration of the nasals are also measured. The results show that nasalance
is affected by the place of articulation of the nasals. Additionally,
the voiceless nasals are found to be significantly longer than the
voiced nasals.",True
zwerts21_interspeech,https://www.isca-archive.org/interspeech_2021/zwerts21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/zwerts21_interspeech.html,Introducing a Central African Primate Vocalisation Dataset for Automated Species Classification,"The INTERSPEECH 2021 Computational Paralinguistics Challenge (ComParE) — COVID-19 Cough, COVID-19 Speech, Escalation & Primates",2021,"Automated classification of animal vocalisations is a potentially powerful
wildlife monitoring tool. Training robust classifiers requires sizable
annotated datasets, which are not easily recorded in the wild. To circumvent
this problem, we recorded four primate species under semi-natural conditions
in a wildlife sanctuary in Cameroon with the objective to train a classifier
capable of detecting species in the wild. Here, we introduce the collected
dataset, describe our approach and initial results of classifier development.
To increase the efficiency of the annotation process, we condensed
the recordings with an energy/change based automatic vocalisation detection.
Segmenting the annotated chunks into training, validation and test
sets, initial results reveal up to 82% unweighted average recall test
set performance in four-class primate species classification.",True
rizos21_interspeech,https://www.isca-archive.org/interspeech_2021/rizos21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/rizos21_interspeech.html,Multi-Attentive Detection of the Spider Monkey Whinny in the (Actual) Wild,"The INTERSPEECH 2021 Computational Paralinguistics Challenge (ComParE) — COVID-19 Cough, COVID-19 Speech, Escalation & Primates",2021,"We study deep bioacoustic event detection through multi-head attention
based pooling, exemplified by wildlife monitoring. In the multiple
instance learning framework, a core deep neural network learns a projection
of the input acoustic signal into a sequence of embeddings, each representing
a segment of the input. Sequence pooling is then required to aggregate
the information present in the sequence such that we have a single
clip-wise representation. We propose an improvement based on Squeeze-and-Excitation
mechanisms upon a recently proposed audio tagging ResNet, and show
that it performs significantly better than the baseline, as well as
a collection of other recent audio models. We then further enhance
our model, by performing an extensive comparative study of recent sequence
pooling mechanisms, and achieve our best result using multi-head self-attention
followed by concatenation of the head-specific pooled embeddings —
better than prediction pooling methods, as well as compared to other
recent sequence pooling tricks. We perform these experiments on a novel
dataset of spider monkey whinny calls we introduce here, recorded in
a rainforest in the South-Pacific coast of Costa Rica, with a promising
outlook pertaining to minimally invasive wildlife monitoring.",True
deshmukh21_interspeech,https://www.isca-archive.org/interspeech_2021/deshmukh21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/deshmukh21_interspeech.html,Improving Weakly Supervised Sound Event Detection with Self-Supervised Auxiliary Tasks,Acoustic Event Detection and Acoustic Scene Classification,2021,"While multitask and transfer learning has shown to improve the performance
of neural networks in limited data settings, they require pretraining
of the model on large datasets beforehand. In this paper, we focus
on improving the performance of weakly supervised sound event detection
in low data and noisy settings simultaneously without requiring any
pretraining task. To that extent, we propose a shared encoder architecture
with sound event detection as a primary task and an additional secondary
decoder for a self-supervised auxiliary task. We empirically evaluate
the proposed framework for weakly supervised sound event detection
on a remix dataset of the DCASE 2019 task 1 acoustic scene data with
DCASE 2018 Task 2 sounds event data under 0, 10 and 20 dB SNR. To ensure
we retain the localisation information of multiple sound events, we
propose a two-step attention pooling mechanism that provides a time-frequency
localisation of multiple audio events in the clip. The proposed framework
with two-step attention outperforms existing benchmark models by 22.3%,
12.8%, 5.9% on 0, 10 and 20 dB SNR respectively. We carry out an ablation
study to determine the contribution of the auxiliary task and two-step
attention pooling to the SED performance improvement.",True
wang21f_interspeech,https://www.isca-archive.org/interspeech_2021/wang21f_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/wang21f_interspeech.html,An Attention Self-Supervised Contrastive Learning Based Three-Stage Model for Hand Shape Feature Representation in Cued Speech,Diverse Modes of Speech Acquisition and Processing,2021,"Cued Speech (CS) is a communication system for deaf people or hearing
impaired people, in which a speaker uses it to aid a lipreader in phonetic
level by clarifying potentially ambiguous mouth movements with hand
shape and positions. Feature extraction of multi-modal CS is a key
step in CS recognition. Recent supervised deep learning based methods
suffer from noisy CS data annotations especially for hand shape modality.
In this work, we first propose a self-supervised contrastive learning
method to learn the feature representation of image without using labels.
Secondly, a small amount of manually annotated CS data are used to
fine-tune the first module. Thirdly, we present a module, which combines
Bi-LSTM and self-attention networks to further learn sequential features
with temporal and contextual information. Besides, to enlarge the volume
and the diversity of the current limited CS datasets, we build a new
British English dataset containing 5 native CS speakers. Evaluation
results on both French and British English datasets show that our model
achieves over 90% accuracy in hand shape recognition. Significant improvements
of 8.75% (for French) and 10.09% (for British English) are achieved
in CS phoneme recognition correctness compared with the state-of-the-art.",True
ferreira21_interspeech,https://www.isca-archive.org/interspeech_2021/ferreira21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/ferreira21_interspeech.html,RaSSpeR: Radar-Based Silent Speech Recognition,Diverse Modes of Speech Acquisition and Processing,2021,"Speech is our most natural and efficient way of communication and offers
a strong potential to improve how we interact with machines. However,
speech communication can sometimes be limited by environmental (e.g.,
ambient noise), contextual (e.g., need for privacy in a public place),
or health conditions (e.g., laryngectomy), hindering the consideration
of audible speech. In this regard, silent speech interfaces (SSI) have
been proposed (e.g., considering video, electromyography), however,
many technologies still face limitations regarding their everyday use,
e.g., the need to place equipment in contact with the speaker (e.g.,
electrodes/ultrasound probe), and raise technical (e.g., lighting conditions
for video) or privacy concerns. In this context, the consideration
of technologies that can help tackle these issues, e.g, by being contactless
and/or placed in the environment, can foster the widespread use of
SSI. In this article, continuous-wave radar is explored to assess its
potential for SSI. To this end, a corpus of 13 words was acquired,
for 3 speakers, and different classifiers were tested on the resulting
data. The best results, obtained using Bagging classifier, trained
for each speaker, with 5-fold cross-validation, yielded an average
accuracy of 0.826, an encouraging result that establishes promising
grounds for further exploration of this technology for silent speech
recognition.",True
graetzer21_interspeech,https://www.isca-archive.org/interspeech_2021/graetzer21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/graetzer21_interspeech.html,Clarity-2021 Challenges: Machine Learning Challenges for Advancing Hearing Aid Processing,Multi-Channel Speech Enhancement and Hearing Aids,2021,"In recent years, rapid advances in speech technology have been made
possible by machine learning challenges such as CHiME, REVERB, Blizzard,
and Hurricane. In the Clarity project, the machine learning approach
is applied to the problem of hearing aid processing of speech-in-noise,
where current technology in enhancing the speech signal for the hearing
aid wearer is often ineffective. The scenario is a (simulated) cuboid-shaped
living room in which there is a single listener, a single target speaker
and a single interferer, which is either a competing talker or domestic
noise. All sources are static, the target is always within ±30°
azimuth of the listener and at the same elevation, and the interferer
is an omnidirectional point source at the same elevation. The target
speech comes from an open source 40-speaker British English speech
database collected for this purpose. This paper provides a baseline
description of the round one Clarity challenges for both enhancement
(CEC1) and prediction (CPC1). To the authors’ knowledge, these
are the first machine learning challenges to consider the problem of
hearing aid speech signal processing.",True
lin21_interspeech,https://www.isca-archive.org/interspeech_2021/lin21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/lin21_interspeech.html,A Noise Robust Method for Word-Level Pronunciation Assessment,Spoken Language Processing I,2021,"The common approach for pronunciation evaluation is based on Goodness
of pronunciation (GOP). It has been found that GOP may perform worse
under noise conditions. Traditional methods compensate pronunciation
features to improve the performance of pronunciation assessment in
noise situations. This paper proposed a noise robust model for word-level
pronunciation assessment based on a domain adversarial training (DAT)
method. We treat the pronunciation assessment in the clean and noise
situations as the source and target domains. The network is optimized
by incorporating both the pronunciation assessment and noise domain
discrimination. The domain labels are generated from unsupervised methods
to adapt to various noise situations. We evaluate the model performance
based on English words recorded by Chinese English learners and labeled
by three experts. Experimental results show on average the proposed
model outperforms the baseline by 3% in Pearson correlation coefficients
(PCC) and 4% in accuracy under different noise conditions.",True
muguli21_interspeech,https://www.isca-archive.org/interspeech_2021/muguli21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/muguli21_interspeech.html,"DiCOVA Challenge: Dataset, Task, and Baseline System for COVID-19 Diagnosis Using Acoustics",The First DiCOVA Challenge: Diagnosis of COVID-19 Using Acoustics,2021,"The DiCOVA challenge aims at accelerating research in diagnosing COVID-19
using acoustics (DiCOVA), a topic at the intersection of speech and
audio processing, respiratory health diagnosis, and machine learning.
This challenge is an open call for researchers to analyze a dataset
of sound recordings, collected from COVID-19 infected and non-COVID-19
individuals, for a two-class classification. These recordings were
collected via crowdsourcing from multiple countries, through a website
application. The challenge features two tracks, one focusing on cough
sounds, and the other on using a collection of breath, sustained vowel
phonation, and number counting speech recordings. In this paper, we
introduce the challenge and provide a detailed description of the task,
and present a baseline system for the task.",True
suter21_interspeech,https://www.isca-archive.org/interspeech_2021/suter21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/suter21_interspeech.html,Neural Text Denormalization for Speech Transcripts,ASR Technologies and Systems,2021,"This paper presents a simple sequence-to-sequence approach to restore
standard orthography in raw, normalized speech transcripts, including
insertion of punctuation marks, prediction of capitalization, restoration
of numeric forms, formatting of dates and times, and other, fully data-driven
adjustments. We further describe our method to generate synthetic parallel
training data, and explore suitable performance metrics, which we align
with human judgment through subjective MOS-like evaluations.
  Our models for English,
Russian, and German have a word error rate of 6.36%, 4.88%, and 5.23%,
respectively. We focus on simplicity and reproducibility, make our
framework available under a BSD license, and share our base models
for English and Russian.",True
joglekar21_interspeech,https://www.isca-archive.org/interspeech_2021/joglekar21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/joglekar21_interspeech.html,Fearless Steps Challenge Phase-3 (FSC P3): Advancing SLT for Unseen Channel and Mission Data Across NASA Apollo Audio,ASR Technologies and Systems,2021,"The Fearless Steps Challenge (FSC) initiative was designed to host
a series of progressively complex tasks to promote advanced speech
research across naturalistic “Big Data” corpora. The Center
for Robust Speech Systems at UT-Dallas in collaboration with the National
Institute of Standards and Technology (NIST) and Linguistic Data Consortium
(LDC) conducted Phase-3 of the FSC series (FSC P3), with a focus on
motivating speech and language technology (SLT) system generalizability
across channel and mission diversity under the same training conditions
as in Phase-2. The FSC P3 introduced 10 hours of previously unseen
channel audio from Apollo-11 and 5 hours of novel audio from Apollo-13
to be evaluated over both previously established and newly introduced
SLT tasks with streamlined tracks. This paper presents an overview
of the newly introduced conversational analysis tracks, Apollo-13 data,
and analysis of system performance for matched and mismatched challenge
conditions. We also discuss the Phase-3 challenge results, evolution
of system performance across the three Phases, and next steps in the
Challenge Series.",True
leykum21_interspeech,https://www.isca-archive.org/interspeech_2021/leykum21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/leykum21_interspeech.html,Voice Quality in Verbal Irony: Electroglottographic Analyses of Ironic Utterances in Standard Austrian German,Phonation and Voicing,2021,"When using verbal irony in interpersonal communication, paraverbal
cues can reduce the risk of misunderstandings. Besides fundamental
frequency, intensity and duration, speakers could use voice quality
parameters to disambiguate between ironic and literal utterances. How
these paraverbal cues are used to mark irony appears to be language-
and/or culture-specific. Since the role of voice quality in ironic
utterances has not yet been investigated in Austrian German, the present
study addresses this issue. In addition to the acoustic signal, the
vocal fold vibration is recorded via electroglottography (EGG). The
detailed analysis of the EGG data as well as the acoustic data, provides
insight into voice quality characteristics of ironic and literal realisations
of short utterances. The analyses reveal that, in Standard Austrian
German, some differences in voice quality exist between ironic and
literal realisations of utterances: When being ironic, speakers’
voices tend to be breathier, creakier or rougher. Differences are more
pronounced in the older age group and in male speakers.",True
steinert21_interspeech,https://www.isca-archive.org/interspeech_2021/steinert21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/steinert21_interspeech.html,Audio-Visual Recognition of Emotional Engagement of People with Dementia,Health and Affect I,2021,"Dementia places an immeasurable burden on affected individuals and
caregivers. In addition to general cognitive decline, dementia has
a negative impact on communication. Technical activation systems are
thus in high demand, as cognitive activation may help to moderate the
decline. However, effective activation requires sustained engagement
— which, in turn, first needs to be reliably recognized. In this
study, we examine emotional engagement recognition for People with
Dementia (PwD) using non-intrusive biosignals resulting from speech
communication and facial expressions. PwD suffering from mild to severe
dementia used a tablet-based activation system over multiple sessions.
We demonstrate that they retained their ability to verbally express
emotional engagement even at severe stages of the disease. For recognition
of emotional engagement, we propose an architecture of Bidirectional
Long-Short-Term-Memory Networks that combines video information with
up to three speech-based feature sets (eGeMAPS, ComParE’13, DeepSpectrum).
Using data of 24 PwD, we show that adding speech improves recognition
performance significantly compared to a video-only model. Interestingly,
disease-progression did not appear to have a substantial impact on
recognition performance in this sample. We further discuss the opportunities
and challenges of detecting emotional engagement from speech in PwD.",True
hecker21_interspeech,https://www.isca-archive.org/interspeech_2021/hecker21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/hecker21_interspeech.html,Speaking Corona? Human and Machine Recognition of COVID-19 from Voice,Health and Affect I,2021,"With the COVID-19 pandemic, several research teams have reported successful
advances in automated recognition of COVID-19 by voice. Resulting voice-based
screening tools for COVID-19 could support large-scale testing efforts.
While capabilities of machines on this task are progressing, we approach
the so far unexplored aspect whether human raters can distinguish COVID-19
positive and negative tested speakers from voice samples, and compare
their performance to a machine learning baseline. To account for the
challenging symptom similarity between COVID-19 and other respiratory
diseases, we use a carefully balanced dataset of voice samples, in
which COVID-19 positive and negative tested speakers are matched by
their symptoms alongside COVID-19 negative speakers without symptoms.
Both human raters and the machine struggle to reliably identify COVID-19
positive speakers in our dataset. These results indicate that particular
attention should be paid to the distribution of symptoms across all
speakers of a dataset when assessing the capabilities of existing systems.
The identification of acoustic aspects of COVID-19-related symptom
manifestations might be the key for a reliable voice-based COVID-19
detection in the future by both trained human raters and machine learning
models.",True
nguyen21b_interspeech,https://www.isca-archive.org/interspeech_2021/nguyen21b_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/nguyen21b_interspeech.html,"Acoustic-Prosodic, Lexical and Demographic Cues to Persuasiveness in Competitive Debate Speeches",Health and Affect I,2021,"We analyze the acoustic-prosodic and lexical correlates of persuasiveness,
taking into account speaker, judge and debate characteristics in a
novel data set of 674 audio profiles, transcripts, evaluation scores
and demographic data from professional debate tournament speeches.
By conducting 10-fold cross validation experiments with linear, LASSO
and random forest regression, we predict how different feature combinations
contribute toward speech scores (i.e. persuasiveness) between men and
women. Overall, lexical features, i.e. word complexity, nouns, fillers
and hedges, are the most predictive features of speech evaluation scores;
in addition to the gender composition of judge panels and opponents.
In a combined lexical and demographic feature model, we achieve an
R2 of 0.40. Different lexical features predict speech evaluation
scores for male vs. female speakers, and further investigation is necessary
to understand whether differential evaluation standards applied across
genders. This work contributes a larger-scale debate data set in a
democratically relevant, competitive format with high external relevance
to persuasive speech education in other competitive settings.",True
zhang21i_interspeech,https://www.isca-archive.org/interspeech_2021/zhang21i_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/zhang21i_interspeech.html,Synchronising Speech Segments with Musical Beats in Mandarin and English Singing,Speech Signal Analysis and Representation I,2021,"Generating synthesised singing voice with models trained on speech
data has many advantages due to the models’ flexibility and controllability.
However, since the information about the temporal relationship between
segments and beats are lacking in speech training data, the synthesised
singing may sound off-beat at times. Therefore, the availability of
the information on the temporal relationship between speech segments
and music beats is crucial. The current study investigated the segment-beat
synchronisation in singing data, with hypotheses formed based on the
linguistics theories of P-centre and sonority hierarchy. A Mandarin
corpus and an English corpus of professional singing data were manually
annotated and analysed. The results showed that the presence of musical
beats was more dependent on segment duration than sonority. However,
the sonority hierarchy and the P-centre theory were highly related
to the location of beats. Mandarin and English demonstrated cross-linguistic
variations despite exhibiting common patterns.",True
lulich21_interspeech,https://www.isca-archive.org/interspeech_2021/lulich21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/lulich21_interspeech.html,Accelerometer-Based Measurements of Voice Quality in Children During Semi-Occluded Vocal Tract Exercise with a Narrow Straw in Air,"Voice Quality Characterization for Clinical Voice Assessment: Voice Production, Acoustics, and Auditory Perception",2021,"Non-invasive measures of voice quality, such as H1-H2, rely on oral
flow signals, inverse filtered speech signals, or corrections for the
effects of formants. Voice quality measures play especially important
roles in the assessment of voice disorders and the evaluation of treatment
efficacy. One type of treatment that is increasingly common in voice
therapy, as well as in voice training for singers and actors, is semi-occluded
vocal tract exercises (SOVTEs). The goal of SOVTEs is to change patterns
of vocal fold vibration and thereby improve voice quality and vocal
efficiency. Accelerometers applied to the skin of the neck have been
used to investigate subglottal acoustics, to inverse-filter speech
signals, and to obtain voice quality metrics. This paper explores the
application of neck-skin accelerometers to measure voice quality without
oral flow, inverse filtering, or formant correction. Accelerometer-based
measures (uncorrected K1-K2 and corrected K1*-K2*, analogous to microphone-based
H1-H2 and H1*-H2*) were obtained from typically developing children
with healthy voice, before and during SOVTEs. Traditional microphone-based
H1-H2 measures (corrected and uncorrected) were also obtained. Results
showed that K1-K2 and K1*-K2* were not substantially affected by vocal
tract acoustic changes in formant frequencies.",True
karpov21_interspeech,https://www.isca-archive.org/interspeech_2021/karpov21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/karpov21_interspeech.html,Golos: Russian Dataset for Speech Research,Miscellanous Topics in ASR,2021,"This paper introduces a novel Russian speech dataset called Golos,
a large corpus suitable for speech research. The dataset mainly consists
of recorded audio files manually annotated on the crowd-sourcing platform.
The total duration of the audio is about 1240 hours. We have made the
corpus freely available to download, along with the acoustic model
with CTC loss prepared on this corpus. Additionally, transfer learning
was applied to improve the performance of the acoustic model. In order
to evaluate the quality of the dataset with the beam-search algorithm,
we have built a 3-gram language model on the open Common Crawl dataset.
The total word error rate (WER) metrics turned out to be about 3.3%
and 11.5%.",True
oneill21_interspeech,https://www.isca-archive.org/interspeech_2021/oneill21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/oneill21_interspeech.html,"SPGISpeech: 5,000 Hours of Transcribed Financial Audio for Fully Formatted End-to-End Speech Recognition",Miscellanous Topics in ASR,2021,"In the English speech-to-text (STT) machine learning task, acoustic
models are conventionally trained on uncased Latin characters, and
any necessary orthography (such as capitalization, punctuation, and
denormalization of non-standard words) is imputed by separate post-processing
models. This adds complexity and limits performance, as many formatting
tasks benefit from semantic information present in the acoustic signal
but absent in transcription. Here we propose a new STT task: end-to-end
neural transcription with fully formatted text for target labels. We
present baseline Conformer-based models trained on a corpus of 5,000
hours of professionally transcribed earnings calls, achieving a CER
of 1.7. As a contribution to the STT research community, we release
the corpus free for non-commercial use.",True
riverincoutlee21_interspeech,https://www.isca-archive.org/interspeech_2021/riverincoutlee21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/riverincoutlee21_interspeech.html,Dialect Features in Heterogeneous and Homogeneous Gheg Speaking Communities,Phonetics I,2021,"This apparent and real time study analyses how dialect features in
the speech of children and adults are differently affected depending
on whether they live in homogeneous or heterogeneous speech communities.
The general hypotheses are that speakers in such high contact settings
as heterogeneous urban centers are more prone to innovation than speakers
in homogeneous tightly-knit communities, and that children accelerate
leveling, especially through schooling and socialization. This study
is of Gheg Albanian, a dialect spoken in and around the capital Tirana.
Two features were investigated: rounding of /a/ and vowel length contrasts.
Two groups of adults and children were compared: one from Tirana and
one from a nearby village. Additionally, the children were recorded
twice over a period of 12 months and were compared longitudinally.
The results showed that length contrasts were still present in both
communities and age groups. Rounding of /a/ was lost in the city, but
undergoing change in the village, with differences measured in apparent
time, but also in child speech within the 12-month span. Our study
further raises the issue of combining both apparent and real time data
within the same design.",True
yi21_interspeech,https://www.isca-archive.org/interspeech_2021/yi21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/yi21_interspeech.html,Half-Truth: A Partially Fake Audio Detection Dataset,Speech Coding and Privacy,2021,"Diverse promising datasets have been designed to further the development
of fake audio detection, such as ASVspoof databases. However, previous
datasets ignore an attacking situation, in which the hacker hides some
small fake clips in real speech audio. This poses a serious threat
since that it is difficult to distinguish the small fake clip from
the whole speech utterance. Therefore, this paper develops such a dataset
for half-truth audio detection (HAD). Partially fake audio in the HAD
dataset involves only changing a few words in an utterance. The audio
of the words is generated with the very latest state-of-the-art speech
synthesis technology. We can not only detect fake utterances but also
localize manipulated regions in a speech using this dataset. Some benchmark
results are presented on this dataset. The results show that partially
fake audio presents much more challenging than fully fake audio for
fake audio detection.",True
siegert21_interspeech,https://www.isca-archive.org/interspeech_2021/siegert21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/siegert21_interspeech.html,Effects of Prosodic Variations on Accidental Triggers of a Commercial Voice Assistant,Speech Coding and Privacy,2021,"The use of modern voice assistants has rapidly grown and they can be
found in more and more households. By design, these systems have to
scan every sound in their surroundings waiting for their respective
wake-word before being able to react to the users’ commands.
The drawback of this method is that phonetic similar expressions can
activate the voice assistant and thus speech utterances or whole private
conversations will be recorded and streamed to the cloud back-end for
further processing. Many news articles and scientific work reported
on inaccurate wake-word detection. Resulting in at least a user’s
confusion or at worst security breaches. The current paper is based
on a broader analysis of phonetic similar accidental triggers conducted
by Schönherr et al., they presented a systematic analysis to detect
accidental triggers, using a pronouncing dictionary and a weighted,
phone-based Levenshtein distance. In this work, the previously identified
accidental triggers are recorded by several speakers under various
conditions to investigate the influence of phonetic variances (i.e.
intonation and speaking/articulation rate) on the robustness of accidental
triggers in a real-world environment.",True
saito21_interspeech,https://www.isca-archive.org/interspeech_2021/saito21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/saito21_interspeech.html,VocalTurk: Exploring Feasibility of Crowdsourced Speaker Identification,Speech Perception II,2021,"This paper presents VocalTurk, a feasibility study of crowdsourced
speaker identification based on our worker dataset collected in Amazon
Mechanical Turk. Crowdsourced data labeling has already been acknowledged
in speech data processing nowadays, but empirical analysis that answer
to common questions such as “how accurate are workers capable
of labeling speech data?” and “what does a good
speech-labeling microtask interface look like?” still remain
underexplored, which would limit the quality and scale of the dataset
collection. Focusing on the speaker identification task in particular,
we thus conducted two studies in Amazon Mechanical Turk: i) hired 3,800+
unique workers to test their performances and confidences in giving
answers to voice pair comparison tasks, and ii) additionally assigned
more-difficult tasks of 1-vs-N voice set comparisons to 350+
top-scoring workers to test their accuracy-speed performances across
patterns of N = 1, 3, 5. The results revealed some positive findings
that would motivate speech researchers toward crowdsourced data labeling,
such as that the top-scoring workers were capable of giving labels
to our voice comparison pairs with 99% accuracy after majority voting,
as well as they were even capable of batch-labeling which significantly
shortened up to 34% of their completion time but still with no statistically-significant
degradation in accuracy.",True
lu21_interspeech,https://www.isca-archive.org/interspeech_2021/lu21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/lu21_interspeech.html,Streaming Multi-Talker Speech Recognition with Joint Speaker Identification,Streaming for ASR/RNN Transducers,2021,"In multi-talker scenarios such as meetings and conversations, speech
processing systems are usually required to transcribe the audio as
well as identify the speakers for downstream applications. Since overlapped
speech is common in this case, conventional approaches usually address
this problem in a cascaded fashion that involves speech separation,
speech recognition and speaker identification that are trained independently.
In this paper, we propose Streaming Unmixing, Recognition and Identification
Transducer (SURIT) — a new framework that deals with this problem
in an end-to-end streaming fashion. SURIT employs the recurrent neural
network transducer (RNN-T) as the backbone for both speech recognition
and speaker identification. We validate our idea on the LibrispeechMix
dataset — a multi-talker dataset derived from Librispeech, and
present encouraging results.",True
cieri21_interspeech,https://www.isca-archive.org/interspeech_2021/cieri21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/cieri21_interspeech.html,Using Games to Augment Corpora for Language Recognition and Confusability,"Speaker, Language, and Privacy",2021,"We present a Game with a Purpose to elicit judgements of the language
spoken in short audio clips of broadcast and conversational telephone
speech, the resulting corpus and their potential use in research on
language recognition and confusability.",True
menshikova21_interspeech,https://www.isca-archive.org/interspeech_2021/menshikova21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/menshikova21_interspeech.html,Lexical Entrainment and Intra-Speaker Variability in Cooperative Dialogues,"Communication and Interaction, Multimodality",2021,"In dialogues, intra-speaker variability is often explained by the relationship
between interlocutors. A person may speak differently with a friend
and a stranger or depending on the interlocutor’s gender or age
— in all these cases we expect speech entrainment, but the degree
of entrainment may vary. In this research, we measured lexical entrainment
in a series of dialogues, where each one of 20 “core” speakers
talked to five different interlocutors: a sibling, a close friend,
an unfamiliar person of the same gender and similar age, an unfamiliar
person of the other gender and similar age, and an unfamiliar person
of the same gender, greater age and higher job position. We hypothesized
that the degree of speech entrainment systematically varies according
to the type of interlocutor, across all the “core” speakers.
The following measures of entrainment were used: parts of speech statistics,
verb forms statistics, language style matching, and lexical density.
Our data have shown that a person speaks very similarly to his/her
sibling; dialogues with a friend or a same-gender stranger of similar
age show fewer similarities; the least “common language”
is observed in dialogues with a stranger of the opposite gender and
with a stranger of greater age and higher job position.",True
mittag21_interspeech,https://www.isca-archive.org/interspeech_2021/mittag21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/mittag21_interspeech.html,NISQA: A Deep CNN-Self-Attention Model for Multidimensional Speech Quality Prediction with Crowdsourced Datasets,"Speech Localization, Enhancement, and Quality Assessment",2021,"In this paper, we present an update to the NISQA speech quality prediction
model that is focused on distortions that occur in communication networks.
In contrast to the previous version, the model is trained end-to-end
and the time-dependency modelling and time-pooling is achieved through
a Self-Attention mechanism. Besides overall speech quality, the model
also predicts the four speech quality dimensions Noisiness,
Coloration, Discontinuity, and Loudness, and in
this way gives more insight into the cause of a quality degradation.
Furthermore, new datasets with over 13,000 speech files were created
for training and validation of the model. The model was finally tested
on a new, live-talking test dataset that contains recordings of real
telephone calls. Overall, NISQA was trained and evaluated on 81 datasets
from different sources and showed to provide reliable predictions also
for unknown speech samples. The code, model weights, and datasets are
open-sourced.",True
wang21s_interspeech,https://www.isca-archive.org/interspeech_2021/wang21s_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/wang21s_interspeech.html,CoVoST 2 and Massively Multilingual Speech Translation,Spoken Machine Translation,2021,"Speech translation (ST) is an increasingly popular topic of research,
partly due to the development of benchmark datasets. Nevertheless,
current datasets cover a limited number of languages. With the aim
to foster research into massive multilingual ST and ST for low resource
languages, we release CoVoST 2, a large-scale multilingual ST corpus
covering translations from 21 languages into English and from English
into 15 languages. This represents the largest open dataset available
to date for volume and language coverage. Data checks provide evidence
about the data quality. We provide extensive speech recognition (ASR),
machine translation (MT) and ST baselines. We demonstrate the value
of CoVoST 2 for multilingual ST research by leveraging it in 4 investigations:
simplify multilingual training by removing ASR pretraining, study multilingual
model scaling properties and investigate zero-shot and transfer learning
capabilities of models trained on CoVoST 2.",True
cho21_interspeech,https://www.isca-archive.org/interspeech_2021/cho21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/cho21_interspeech.html,Multi-Speaker Emotional Text-to-Speech Synthesizer,Show and Tell 2,2021,"We present a methodology to train our multi-speaker emotional text-to-speech
synthesizer that can express speech for 10 speakers’ 7 different
emotions. All silences from audio samples are removed prior to learning.
This results in fast learning by our model. Curriculum learning is
applied to train our model efficiently. Our model is first trained
with a large single-speaker neutral dataset, and then trained with
neutral speech from all speakers. Finally, our model is trained using
datasets of emotional speech from all speakers. In each stage, training
samples of each speaker-emotion pair have equal probability to appear
in mini-batches. Through this procedure, our model can synthesize speech
for all targeted speakers and emotions. Our synthesized audio sets
are available on our web page.",True
fragner21_interspeech,https://www.isca-archive.org/interspeech_2021/fragner21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/fragner21_interspeech.html,Autonomous Robot for Measuring Room Impulse Responses,Show and Tell 2,2021,"Far-field speech recognition for e.g. home automation or smart assistants
has to cope with moving speakers in reverberant environments. Simulating
stationary or even moving speakers in realistic environments enables
to make speech processing technology more robust. This paper introduces
an autonomous robot for recording a database of Room Impulse Responses
(RIRs) at a high spatial resolution. This supports the creation of
realistic simulation environments. These RIRs can be exploited to generate
multi-channel speech mixtures of static or moving speakers for various
applications.",True
machacek21_interspeech,https://www.isca-archive.org/interspeech_2021/machacek21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/machacek21_interspeech.html,Lost in Interpreting: Speech Translation from Source or Interpreter?,Spoken Language Processing II,2021,"Interpreters facilitate multi-lingual meetings but the affordable set
of languages is often smaller than what is needed. Automatic simultaneous
speech translation can extend the set of provided languages. We investigate
if such an automatic system should rather follow the original speaker,
or an interpreter to achieve better translation quality at the cost
of increased delay.
  To answer the question,
we release Europarl Simultaneous Interpreting Corpus (ESIC), 10 hours
of recordings and transcripts of European Parliament speeches in English,
with simultaneous interpreting into Czech and German. We evaluate quality
and latency of speaker-based and interpreter-based spoken translation
systems from English to Czech. We study the differences in implicit
simplification and summarization of the human interpreter compared
to a machine translation system trained to shorten the output to some
extent. Finally, we perform human evaluation to measure information
loss of each of these approaches.",True
bergler21_interspeech,https://www.isca-archive.org/interspeech_2021/bergler21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/bergler21_interspeech.html,ORCA-SLANG: An Automatic Multi-Stage Semi-Supervised Deep Learning Framework for Large-Scale Killer Whale Call Type Identification,Speech and Audio Analysis,2021,"Identification of animal-specific vocalization patterns is an imperative
requirement to decode animal communication. In bioacoustics, passive
acoustic recording setups are increasingly deployed to acquire large-scale
datasets. Previous knowledge about established animal-specific call
types is usually present due to historically conducted research. However,
time- and human-resource constraints, combined with a lack of available
machine-based approaches, only allow manual analysis of comparatively
small data corpora and strongly distort the actual data representation
and information value. Such data limitations cause restrictions in
terms of identifying existing population-, group-, and individual-specific
call types, sub-categories, as well as unseen vocalization patterns.
Thus, machine learning forms the basis for animal-specific call type
recognition, to facilitate more profound insights into communication.
The current study is the first fusing task-specific neural networks
to develop a fully automated, multi-stage, deep-learning-based framework,
entitled ORCA-SLANG, performing semi-supervised call type identification
in one of the largest animal-specific bioacoustic archives —
the Orchive. Orca/noise segmentation, denoising, and subsequent feature
learning provide robust representations for semi-supervised clustering/classification.
This results in a machine-annotated call type data repository containing
235,369 unique calls.",True
boes21_interspeech,https://www.isca-archive.org/interspeech_2021/boes21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/boes21_interspeech.html,Audiovisual Transfer Learning for Audio Tagging and Sound Event Detection,Speech and Audio Analysis,2021,"We study the merit of transfer learning for two sound recognition problems,
i.e., audio tagging and sound event detection. Employing feature fusion,
we adapt a baseline system utilizing only spectral acoustic inputs
to also make use of pretrained auditory and visual features, extracted
from networks built for different tasks and trained with external data.
  We perform experiments with these modified models on an audiovisual
multi-label data set, of which the training partition contains a large
number of unlabeled samples and a smaller amount of clips with weak
annotations, indicating the clip-level presence of 10 sound categories
without specifying the temporal boundaries of the active auditory events.
  For clip-based audio tagging, this transfer learning method grants
marked improvements. Addition of the visual modality on top of audio
also proves to be advantageous in this context.
  When it comes to generating
transcriptions of audio recordings, the benefit of pretrained features
depends on the requested temporal resolution: for coarse-grained sound
event detection, their utility remains notable. But when more fine-grained
predictions are required, performance gains are strongly reduced due
to a mismatch between the problem at hand and the goals of the models
from which the pretrained vectors were obtained.",True
nessler21_interspeech,https://www.isca-archive.org/interspeech_2021/nessler21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/nessler21_interspeech.html,Non-Intrusive Speech Quality Assessment with Transfer Learning and Subject-Specific Scaling,Speech and Audio Analysis,2021,"In communication systems, it is crucial to estimate the perceived quality
of audio and speech. The industrial standards for many years have been
PESQ, 3QUEST, and POLQA, which are intrusive methods. This restricts
the possibilities of using these metrics in real-world conditions,
where we might not have access to the clean reference signal. In this
work, we develop a new non-intrusive metric based on crowd-sourced
data. We build a new speech dataset by combining publicly available
speech, noises, and reverberations. Then we follow the ITU P.808 recommendation
to label the dataset with mean opinion scores (MOS). Finally, we train
a deep neural network to estimate the MOS from the speech data in a
non-intrusive way. We propose two novelties in our work. First, we
explore transfer learning by pre-training a model using a larger set
of POLQA scores and finetuning with the smaller (and thus cheaper)
human-labeled set. Secondly, we perform a subject-specific scaling
in the MOS scores to adjust for their different subjective scales.
Our model yields better accuracy than PESQ, POLQA, and other non-intrusive
methods when evaluated on the independent VCTK test set. We also report
misleading POLQA scores for reverberant speech.",True
diwan21_interspeech,https://www.isca-archive.org/interspeech_2021/diwan21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/diwan21_interspeech.html,MUCS 2021: Multilingual and Code-Switching ASR Challenges for Low Resource Indian Languages,Cross/Multi-Lingual and Code-Switched ASR,2021,"Recently, there is an increasing interest in multilingual automatic
speech recognition (ASR) where a speech recognition system caters to
multiple low resource languages by taking advantage of low amounts
of labelled corpora in multiple languages. With multilingualism becoming
common in today’s world, there has been increasing interest in
code-switching ASR as well. In code-switching, multiple languages are
freely interchanged within a single sentence or between sentences.
The success of low-resource multilingual and code-switching (MUCS)
ASR often depends on the variety of languages in terms of their acoustics,
linguistic characteristics as well as the amount of data available
and how these are carefully considered in building the ASR system.
In this MUCS 2021 challenge, we would like to focus on building MUCS
ASR systems through two different subtasks related to a total of seven
Indian languages, namely Hindi, Marathi, Odia, Tamil, Telugu, Gujarati
and Bengali. For this purpose, we provide a total of ~600 hours
of transcribed speech data, comprising train and test sets, in these
languages, including two code-switched language pairs, Hindi-English
and Bengali-English. We also provide baseline recipes for both the
subtasks with 30.73% and 32.45% word error rate on the MUCS test sets,
respectively.",True
gillick21_interspeech,https://www.isca-archive.org/interspeech_2021/gillick21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/gillick21_interspeech.html,Robust Laughter Detection in Noisy Environments,Health and Affect II,2021,"We investigate the problem of automatically identifying and extracting
laughter from audio files in noisy environments. We conduct an empirical
evaluation of several machine learning models using audio data of varying
sound quality, finding that while previously published methods work
relatively well in controlled environments, performance drops precipitously
in real-world settings with background noise. In the process, we contribute
a new dataset of laughter annotations on top of the existing AudioSet
corpus, with precise segmentations for the start and end points of
each laugh, and we present a new approach to laughter detection that
performs comparatively well in uncontrolled environments. We discuss
the utility of our approach as well as the importance of understanding
the variability of model performance in a range of real-world testing
environments.",True
tammewar21_interspeech,https://www.isca-archive.org/interspeech_2021/tammewar21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/tammewar21_interspeech.html,Emotion Carrier Recognition from Personal Narratives,Health and Affect II,2021,"Personal Narratives (PN) — recollections of facts, events, and
thoughts from one’s own experience — are often used in
everyday conversations. So far, PNs have mainly been explored for tasks
such as valence prediction or emotion classification (e.g. happy,
sad). However, these tasks might overlook more fine-grained information
that could prove to be relevant for understanding PNs. In this work,
we propose a novel task for Narrative Understanding: Emotion Carrier
Recognition (ECR). Emotion carriers, the text fragments that carry
the emotions of the narrator (e.g. loss of a grandpa, high school
reunion), provide a fine-grained description of the emotion state.
We explore the task of ECR in a corpus of PNs manually annotated with
emotion carriers and investigate different machine learning models
for the task. We propose evaluation strategies for ECR including metrics
that can be appropriate for different tasks.",True
billa21_interspeech,https://www.isca-archive.org/interspeech_2021/billa21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/billa21_interspeech.html,Leveraging Non-Target Language Resources to Improve ASR Performance in a Target Language,Neural Network Training Methods for ASR,2021,"This paper investigates approaches to improving automatic speech recognition
(ASR) performance in a target language using resources in other languages.
In particular, we assume that we have untranscribed speech in a different
language and a well trained ASR system in yet another language. Concretely,
we structure this as a multi-task problem, where the primary task is
acoustic model training in the target language, and the secondary task
is also acoustic model training but using a synthetic data set. The
synthetic data set consists of pseudo transcripts generated by decoding
the untranscribed speech using a well trained ASR model. We compare
and contrast this with using labeled data sets, i.e. matched audio
and human-generated transcripts, and show that our approach compares
favorably. In most cases, we see performance improvements, and in some
cases, depending on the selection of languages and nature of speech
data, performance exceeds that of systems using labeled data sets as
the secondary task. When extended to larger sets of data, we show that
the mismatched data approach performs similarly to in-language semi-supervised
training (SST) when the secondary task pseudo transcripts are generated
by ASR models trained on large diverse data sets.",True
liu21i_interspeech,https://www.isca-archive.org/interspeech_2021/liu21i_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/liu21i_interspeech.html,Targeted and Targetless Neutral Tones in Taiwanese Southern Min,Prosodic Features and Structure,2021,"This article is an acoustic study on the two types of neutral tone
in Taiwanese Southern Min (TSM). Recording materials included a set
of verb-clitic constructions with different preceding tones and clitics.
Pitch contours in different conditions were compared using Smoothing
Spline ANOVA. Our results confirmed that Type 1 neutral tone (NT1)
has a low pitch target and that Type 2 neutral tone (NT2) is contextually
dependent. Whether NT1 or NT2 is chosen has been treated as the lexical
idiosyncrasy of the clitics in question, with idiolectal and dialectal
variations. However, we found in this study that the onsets have a
bearing on determining the type of neutral tone: the more sonorous
the onset, the more possible it is for the clitic to be in NT2. In
sum, the two distinct types of neutral tones in TSM not only are unusual
among the neutral tones in Sinitic languages, but they also offer novel
data for the consonant-tone interaction.",True
gosy21_interspeech,https://www.isca-archive.org/interspeech_2021/gosy21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/gosy21_interspeech.html,The Interaction of Word Complexity and Word Duration in an Agglutinative Language,Prosodic Features and Structure,2021,"The mental lexicon comprises the representations of various words either
in a morphologically decomposed form, or in a conceptually non-decomposed
form. The durations of mono-morphemic and multimorphemic words are
assumed to contain information on the routes of their lexical access.
  The durations of Hungarian nouns with various lengths produced
spontaneously by 10 young and 10 elderly speakers (with 55 years of
difference between them) were measured. Findings showed significant
differences depending on the words’ complexity and on age. The
nouns both with and without suffixes were significantly longer in old
than in young speakers. The durational differences depending on age
were more pronounced in monomorphemic nouns as opposed to multimorphemic
nouns. Along with the increasing number of syllables of the nouns,
old speakers produced increasingly longer simple nouns (stems) than
young ones did.
  We suggest that multimorphemic nouns are accessed decompositionally
in spontaneous utterances when the stem activation is followed by the
activation of the suffixes. The specific storage and the corresponding
lexical access of the morphemes explain the longer durations of the
inflected nouns.",True
pan21b_interspeech,https://www.isca-archive.org/interspeech_2021/pan21b_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/pan21b_interspeech.html,Taiwan Min Nan (Taiwanese) Checked Tones Sound Change,Prosodic Features and Structure,2021,"The multifaced changes of Taiwan Min Nan (TMN) checked sandhi tones,
S3 and S5 were investigated as well as the checked base tones, B3 and
B5. Simultaneous EGG data, CQ_H and acoustic data, including duration,
f0 offset at 80% vowel interval, and spectral tilt H1*-A3*
from forty male and female speakers above 40 and under 30 years of
age were analyzed. Though different measures progress at different
paces, in general, as the coda stops [p, t, k, ʔ] from full stop
closure, to energy damping and finally to complete deletion, vowel
duration lengthening, f0 offset lowering, and more modal phonation
were observed. Gender effects were found on f0 offset and CQ_H offset.
The pace of progress is more advanced for base tone B5 with glottal
coda stops. After coda deletion, the contexts conditioning the anticipatory
co-articulation were removed and vowel and tone characteristics were
modified to be similar to those found in open syllables.",True
stefansdottir21_interspeech,https://www.isca-archive.org/interspeech_2021/stefansdottir21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/stefansdottir21_interspeech.html,"Articulatory Characteristics of Icelandic Voiced Fricative Lenition: Gradience, Categoricity, and Speaker/Gesture-Specific Effects",Prosodic Features and Structure,2021,"Icelandic voiced fricatives frequently reduce in connected speech.
However, systematic investigations of the phenomenon from acoustic
and articulatory perspectives are lacking. To further the understanding
of this lenition process, we present electromagnetic articulography
and acoustic data from four speakers concerning the intervocalic realization
of the dental and velar fricatives. The results show that lenition
is mostly gradient, but some speakers and places of articulation exhibit
two distinct modes suggesting a categorical distinction. Moreover,
in some tokens, the fricative constriction is absent from the articulatory
trajectories. Finally, the relation between lenition and speech rate,
style, and stress is also subject to speaker- and gesture-specific
effects. We conclude by evaluating how our findings challenge the common
assumptions, made in the literature, that lenition is a change in gestural
target or a perceptually driven phenomenon.",True
zandie21_interspeech,https://www.isca-archive.org/interspeech_2021/zandie21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/zandie21_interspeech.html,RyanSpeech: A Corpus for Conversational Text-to-Speech Synthesis,"Speech Synthesis: Tools, Data, Evaluation",2021,"This paper introduces RyanSpeech, a new speech corpus for research
on automated text-to-speech (TTS) systems. Publicly available TTS corpora
are often noisy, recorded with multiple speakers, or lack quality male
speech data. In order to meet the need for a high quality, publicly
available male speech corpus within the field of speech recognition,
we have designed and created RyanSpeech which contains textual
materials from real-world conversational settings. These materials
contain over 10 hours of a professional male voice actor’s speech
recorded at 44.1 kHz. This corpus’s design and pipeline make
RyanSpeech ideal for developing TTS systems in real-world applications.
To provide a baseline for future research, protocols, and benchmarks,
we trained 4 state-of-the-art speech models and a vocoder on RyanSpeech.
The results show 3.36 in mean opinion scores (MOS) in our best model.
We have made both the corpus and trained models for public use.",True
shi21c_interspeech,https://www.isca-archive.org/interspeech_2021/shi21c_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/shi21c_interspeech.html,AISHELL-3: A Multi-Speaker Mandarin TTS Corpus,"Speech Synthesis: Tools, Data, Evaluation",2021,"In this paper, we present AISHELL-3, a large-scale multi-speaker Mandarin
speech corpus which could be used to train multi-speaker Text-To-Speech
(TTS) systems. The corpus contains roughly 85 hours of emotion-neutral
recordings spanning across 218 native Chinese mandarin speakers. Their
auxiliary attributes such as gender, age group and native accents are
explicitly marked and provided in the corpus. Moreover, transcripts
in Chinese character-level and pinyin-level are provided along with
the recordings. We also present some data processing strategies and
techniques which match with the characteristics of the presented corpus
and conduct experiments on multiple speech-synthesis systems to assess
the quality of the generated speech samples, showing promising results.
The corpus is available online under Apache v2.0 license.",True
eng21_interspeech,https://www.isca-archive.org/interspeech_2021/eng21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/eng21_interspeech.html,Comparing Speech Enhancement Techniques for Voice Adaptation-Based Speech Synthesis,"Speech Synthesis: Tools, Data, Evaluation",2021,"This study investigates the use of speech enhancement techniques in
creating text-to-speech voices with degraded or noisy speech. A number
of synthetic voices were created using speech that was first degraded
by different noise types at various signal-to-noise ratios (SNRs),
then enhanced through four speech enhancement algorithms: Subspace,
Wiener filter, SEGAN and a DNN-based method. Subjective listening tests
show that the quality of the synthetic voices produced by subspace
and the DNN-based method enhanced speech outperforms the quality of
the voices created using Wiener filter or SEGAN enhanced speech at
low SNRs, and speech enhanced by the subspace method results in higher
quality synthetic speech at higher SNRs.",True
cui21c_interspeech,https://www.isca-archive.org/interspeech_2021/cui21c_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/cui21c_interspeech.html,EMOVIE: A Mandarin Emotion Speech Dataset with a Simple Emotional Text-to-Speech Model,"Speech Synthesis: Tools, Data, Evaluation",2021,"Recently, there has been an increasing interest in neural speech synthesis.
While the deep neural network achieves the state-of-the-art result
in text-to-speech (TTS) tasks, how to generate a more emotional and
more expressive speech is becoming a new challenge to researchers due
to the scarcity of high-quality emotion speech dataset and the lack
of advanced emotional TTS model. In this paper, we first briefly introduce
and publicly release a Mandarin emotion speech dataset including 9,724
samples with audio files and its emotion human-labeled annotation.
After that, we propose a simple but efficient architecture for emotional
speech synthesis called EMSpeech. Unlike those models which need additional
reference audio as input, our model could predict emotion labels just
from the input text and generate more expressive speech conditioned
on the emotion embedding. In the experiment phase, we first validate
the effectiveness of our dataset by an emotion classification task.
Then we train our model on the proposed dataset and conduct a series
of subjective evaluations. Finally, by showing a comparable performance
in the emotional speech synthesis task, we successfully demonstrate
the ability of the proposed model.",True
bakhturina21_interspeech,https://www.isca-archive.org/interspeech_2021/bakhturina21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/bakhturina21_interspeech.html,Hi-Fi Multi-Speaker English TTS Dataset,"Speech Synthesis: Tools, Data, Evaluation",2021,"This paper introduces a new multi-speaker English dataset for training
text-to-speech models. The dataset is based on LibriVox audiobooks
and Project Gutenberg texts, both in the public domain. The new dataset
contains about 292 hours of speech from 10 speakers with at least 17
hours per speaker sampled at 44.1 kHz. To select speech samples with
high quality, we considered audio recordings with a signal bandwidth
of at least 13 kHz and a signal-to-noise ratio (SNR) of at least 32
dB. The dataset is publicly released.",True
mussakhojayeva21_interspeech,https://www.isca-archive.org/interspeech_2021/mussakhojayeva21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/mussakhojayeva21_interspeech.html,KazakhTTS: An Open-Source Kazakh Text-to-Speech Synthesis Dataset,"Speech Synthesis: Tools, Data, Evaluation",2021,"This paper introduces a high-quality open-source speech synthesis dataset
for Kazakh, a low-resource language spoken by over 13 million people
worldwide. The dataset consists of about 93 hours of transcribed audio
recordings spoken by two professional speakers (female and male). It
is the first publicly available large-scale dataset developed to promote
Kazakh text-to-speech (TTS) applications in both academia and industry.
In this paper, we share our experience by describing the dataset development
procedures and faced challenges, and discuss important future directions.
To demonstrate the reliability of our dataset, we built baseline end-to-end
TTS models and evaluated them using the subjective mean opinion score
(MOS) measure. Evaluation results show that the best TTS models trained
on our dataset achieve MOS above 4 for both speakers, which makes them
applicable for practical use. The dataset, training recipe, and pretrained
TTS models are freely available.",True
reddy21_interspeech,https://www.isca-archive.org/interspeech_2021/reddy21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/reddy21_interspeech.html,INTERSPEECH 2021 Deep Noise Suppression Challenge,INTERSPEECH 2021 Deep Noise Suppression Challenge,2021,"The Deep Noise Suppression (DNS) challenge was designed to unify the
research efforts in the area of noise suppression targeted for human
perception. We recently organized a DNS challenge special session at
INTERSPEECH 2020 and ICASSP 2021. We open-sourced training and test
datasets for the wideband scenario along with a subjective evaluation
framework based on ITU-T standard P.808, which was used to evaluate
participants of the challenge. Many researchers from academia and industry
made significant contributions to push the field forward, yet even
the best noise suppressor was far from achieving superior speech quality
in challenging scenarios. In this version of the challenge organized
at INTERSPEECH 2021, we expanded our training and test datasets to
accommodate fullband scenarios and challenging test conditions. We
used ITU-T P.835 to evaluate the challenge winners as it gives additional
information about the quality of processed speech and residual noise.
The two tracks in this challenge focused on real-time denoising for
(i) wideband, and (ii) fullband scenarios. We also made available a
reliable non-intrusive objective speech quality metric for wideband
called DNSMOS for the participants to use during their development
phase.",True
leem21_interspeech,https://www.isca-archive.org/interspeech_2021/leem21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/leem21_interspeech.html,Separation of Emotional and Reconstruction Embeddings on Ladder Network to Improve Speech Emotion Recognition Robustness in Noisy Conditions,Emotion and Sentiment Analysis I,2021,"When speech emotion recognition (SER) is applied in an actual
application, the system should be able to cope with audio acquired
in a noisy, unconstrained environment. Most studies on noise-robust
SER require a parallel dataset with emotion labels, which is impractical
to collect, or use speech with artificially added noise, which does
not resemble practical conditions. This study builds upon the ladder
network formulation, which can effectively compensate the environmental
differences between a clean speech corpus and real-life recordings.
This study proposes a decoupled ladder network, which increases the
robustness of the SER system against the influences of non-stationary
background noise by decoupling the last hidden layer embedding into
emotion and reconstruction embeddings. This novel implementation allows
the emotion embedding to focus exclusively on building a discriminative
representation, without worrying about the reconstruction task. We
introduce a noisy version of the MSP-Podcast database, which contains
audio segments collected with a smartphone that simultaneously records
sentences from the corpus and non-stationary noise at different signal-to-noise
ratios (SNRs). We test the effectiveness of our proposed model
with this corpus, showing that the decoupled ladder network can increase
the performance of the regular ladder network when dealing with noisy
recordings.",True
hair21_interspeech,https://www.isca-archive.org/interspeech_2021/hair21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/hair21_interspeech.html,Assessing Posterior-Based Mispronunciation Detection on Field-Collected Recordings from Child Speech Therapy Sessions,Assessment of Pathological Speech and Language II,2021,"A critical component of child speech therapy is home practice with
a caregiver, who can provide feedback. However, caregivers oftentimes
struggle with accurately rating speech and with perceiving pronunciation
errors. One potential solution for this issue is to embed automatic
mispronunciation-detection (MPD) algorithms within digital speech therapy
applications. To address the need for MPD within child speech therapy,
we investigated posterior-based mispronunciation detection using a
custom corpus of disordered speech from children that had been manually
annotated by an expert clinician. Namely, we trained a family of phoneme-specific
logistic regression classifiers (LRC) and support vector machines (SVM)
on log posterior probability and log posterior ratio features. Our
results show that these classifiers outperformed baseline Goodness
of Pronunciation scoring by 11% and 10%, respectively. Even more importantly,
in an offline test, the LRC and SVM classifiers outperformed student
clinicians at identifying mispronunciations by 18% and 16%, respectively.
These results suggest that posterior-based mispronunciation detection
may be suitable to provide at-home therapy feedback for children.",True
mirheidari21_interspeech,https://www.isca-archive.org/interspeech_2021/mirheidari21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/mirheidari21_interspeech.html,Identifying Cognitive Impairment Using Sentence Representation Vectors,Assessment of Pathological Speech and Language II,2021,"The widely used word vectors can be extended at the sentence level
to perform a wide range of natural language processing (NLP) tasks.
Recently the Bidirectional Encoder Representations from Transformers
(BERT) language representation achieved state-of-the-art performance
for these applications. The model is trained with punctuated and well-formed
(writ-ten) text, however, the performance of the model drops significantly
when the input text is the — erroneous and unpunctuated —
output of automatic speech recognition (ASR).  We use a sliding window
and averaging approach for pre-processing text for BERT to extract
features for classifying three diagnostic categories relating to cognitive
impairment: neurodegenerative dis-order (ND), mild cognitive impairment
(MCI), and healthy controls (HC). The in-house dataset contains the
audio recordings of an intelligent virtual agent (IVA) who asks the
participants several conversational questions prompts in addition to
giving a picture description prompt. For the three-way classification,
we achieve a 73.88% F-score (accuracy: 76.53%) using the pre-trained,
uncased base BERT and for the two-way classifier (HC vs. ND) we achieve
89.80% (accuracy: 90%). We further improve these by using a prompt
selection technique, reaching the F-scores of 79.98% (accuracy: 81.63%)
and 93.56% (accuracy: 93.75%) respectively.",True
yue21b_interspeech,https://www.isca-archive.org/interspeech_2021/yue21b_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/yue21b_interspeech.html,Parental Spoken Scaffolding and Narrative Skills in Crowd-Sourced Storytelling Samples of Young Children,Assessment of Pathological Speech and Language II,2021,"A novel crowdsourcing project to gather children’s storytelling
based language samples using a mobile app was undertaken across the
United Kingdom. Parents’ scaffolding of children’s narratives
was observed in many of the samples. This study was designed to examine
the relationship of scaffolding and young children’s narrative
language ability in a story retell context which is analysed at the
macro-structural (total macro-structure score), the micro-structural
(mean length of utterances in morphemes) and verbal productivity (total
number of utterances) levels. Young children with and without scaffolding
were statistically compared. The interaction between the level of scaffolding
support, the grammar complexity and the narrative structure was explored.
A bidirectional relationship was observed between scaffolding and young
children’s narrative language ability. Young children with better
performance were observed to receive less scaffolding from parents.
Scaffolding was shown to support early narrative development of young
children and was more able to benefit those with low-level grammatical
complexity skills. It is crucial to encourage parental scaffolding
to be well-attuned to the child’s narrative ability.",True
haulcy21_interspeech,https://www.isca-archive.org/interspeech_2021/haulcy21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/haulcy21_interspeech.html,CLAC: A Speech Corpus of Healthy English Speakers,Assessment of Pathological Speech and Language II,2021,"This paper introduces the Crowdsourced Language Assessment Corpus (CLAC),
a speech corpus consisting of audio recordings and automatically-generated
transcripts for several speech and language tasks, as well as metadata
for each of the speakers. The CLAC was created to provide the community
with a collection of audio samples from various speakers that could
be used to learn a general representation for speech from healthy subjects,
as well as complement other health-related speech datasets, which tend
to be limited. In this paper, we describe the data collection protocol
and summarize the contents of the dataset. We also extract timing metrics
from the recordings of each task to explore what those metrics look
like for a large, English-speaking population. Lastly, we provide an
example of how the dataset can be used by comparing the metrics to
those extracted from a small sample of Frontotemporal Dementia subjects.
We hope that this dataset will help advance the state of the art in
the health and speech domain.",True
wang21v_interspeech,https://www.isca-archive.org/interspeech_2021/wang21v_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/wang21v_interspeech.html,Cross-Modal Knowledge Distillation Method for Automatic Cued Speech Recognition,Multimodal Systems,2021,"Cued Speech (CS) is a visual communication system for the deaf or hearing
impaired people. It combines lip movements with hand cues to obtain
a complete phonetic repertoire. Current deep learning based methods
on automatic CS recognition suffer from a common problem, which is
the data scarcity. Until now, there are only two public single speaker
datasets for French (238 sentences) and British English (97 sentences).
In this work, we propose a cross-modal knowledge distillation method
with teacher-student structure, which transfers audio speech information
to CS to overcome the limited data problem. Firstly, we pretrain a
teacher model for CS recognition with a large amount of open source
audio speech data, and simultaneously pretrain the feature extractors
for lips and hands using CS data. Then, we distill the knowledge from
teacher model to the student model with frame-level and sequence-level
distillation strategies. Importantly, for frame-level, we exploit multi-task
learning to weigh losses automatically, to obtain the balance coefficient.
Besides, we establish a five-speaker British English CS dataset for
the first time. The proposed method is evaluated on French and British
English CS datasets, showing superior CS recognition performance to
the state-of-the-art (SOTA) by a large margin.",True
rose21_interspeech,https://www.isca-archive.org/interspeech_2021/rose21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/rose21_interspeech.html,End-to-End Audio-Visual Speech Recognition for Overlapping Speech,Multimodal Systems,2021,"This paper investigates an end-to-end audio-visual (A/V) modeling approach
for transcribing utterances in scenarios where there are overlapping
speech utterances from multiple talkers. It assumes that overlapping
audio signals and video signals in the form of mouth-tracks aligned
with speech are available for overlapping talkers. The approach builds
on previous work in audio-only multi-talker ASR. In that work, a conventional
recurrent neural network transducer (RNN-T) architecture was extended
to include a masking model for separation of encoded audio features
and multiple label encoders to encode transcripts from overlapping
speakers. It is shown here that incorporating an attention weighted
combination of visual features in A/V multi-talker RNN-T models significantly
improves speaker disambiguation in ASR on overlapping speech relative
to audio-only performance. The A/V multi-talker ASR systems described
here are trained and evaluated on a two speaker A/V overlapping speech
dataset created from YouTube videos. A 17% reduction in WER was observed
for A/V multi-talker models relative to audio-only multi-talker models.",True
chen21m_interspeech,https://www.isca-archive.org/interspeech_2021/chen21m_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/chen21m_interspeech.html,Modeling Sensorimotor Adaptation in Speech Through Alterations to Forward and Inverse Models,Speech Production II,2021,"When speakers are exposed to auditory feedback perturbations of a particular
vowel, they not only adapt their productions of that vowel but also
transfer this change to other, untrained, vowels. However, current
models of speech sensorimotor adaptation, which rely on changes in
the feedforward control of specific speech units, are unable to account
for this type of generalization. Here, we developed a neural-network
based model to simulate speech sensorimotor adaptation, and assess
whether updates to internal control models can account for observed
patterns of generalization. Based on a dataset generated from the Maeda
plant, we trained two independent neural networks: 1) an inverse model,
which generates motor commands for desired acoustic outcomes and 2)
a forward model, which maps motor commands to acoustic outcomes (prediction).
When vowel formant perturbations were given, both forward and inverse
models were updated when there was a mismatch between predicted and
perceived output. Our results replicate behavioral experiments: the
model altered its production to counteract the perturbation, and showed
gradient transfer of this learning dependent on acoustic distance between
training and test vowels. These results suggest that updating paired
forward and inverse models provides a plausible account for sensorimotor
adaptation in speech.",True
chen21n_interspeech,https://www.isca-archive.org/interspeech_2021/chen21n_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/chen21n_interspeech.html,Human-to-Human Conversation Dataset for Learning Fine-Grained Turn-Taking Action,Spoken Dialogue Systems II,2021,"Conducting natural turn-taking behavior takes a crucial part in the
user experience of modern spoken dialogue systems. One way to build
such system is to learn those behaviors from real-world human-to-human
dialogues, which have the most diverse and fine-grained turn-taking
actions than any manual constructed sessions.
  In this paper, we
propose a Dataset — FTAD which could be used to learn turn-taking
policies directly from human. First, we design an annotation mechanism
to transform existing human-to-human dialogue session into structural
data with most fine-grained turn-taking actions reserved. Then we explored
a set of supervised learning tasks on it, showing the challenge and
potential of learning complete fine-grained turn-taking policies based
on such data.",True
jahchan21_interspeech,https://www.isca-archive.org/interspeech_2021/jahchan21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/jahchan21_interspeech.html,Towards an Accent-Robust Approach for ATC Communications Transcription,Automatic Speech Recognition in Air Traffic Management,2021,"Air Traffic Control (ATC) communications are a typical example where
Automatic Speech Recognition could face various challenges: audio data
are quite noisy due to the characteristics of capturing mechanisms.
All speakers involved use a specific English-based phraseology and
a significant number of pilots and controllers are non-native English
speakers. The aim of this work is to enhance pilot-ATC communications
by adding a Speech to Text (STT) capability that will transcribe ATC
speech into text on the cockpit interfaces to help the pilot understand
ATC speech in a more optimal manner (be able to verify what he/she
heard on the radio by looking at the text transcription, be able to
decipher non-native English accents from controllers, not lose time
asking the ATC to repeat the message several times). In this paper,
we first describe an accent analysis study which was carried out both
on a theoretical level but also with the help of feedback from several
hundred airline pilots. Then, we present the dataset that was set up
for this work. Finally, we describe the experiments we have implemented
and the impact of the speaker accent on the performance of a speech
to text engine.",True
szoke21_interspeech,https://www.isca-archive.org/interspeech_2021/szoke21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/szoke21_interspeech.html,Detecting English Speech in the Air Traffic Control Voice Communication,Automatic Speech Recognition in Air Traffic Management,2021,"Developing in-cockpit voice enabled applications require a real-world
dataset with labels and annotations. We launched a community platform
for collecting the Air-Traffic Control (ATC) speech, world-wide in
the ATCO2 project. Filtering out non-English speech is one
of the main components in the data processing pipeline. The proposed
English Language Detection (ELD) system is based on the embeddings
from Bayesian subspace multinomial model. It is trained on the word
confusion network from an ASR system. It is robust, easy to train,
and light weighted. We achieved 0.0439 equal-error-rate (EER), a 50%
relative reduction as compared to the state-of-the-art acoustic ELD
system based on x-vectors, in the in-domain scenario. Further, we achieved
an EER of 0.1352, a 33% relative reduction as compared to the acoustic
ELD, in the unseen language (out-of-domain) condition. We plan to publish
the evaluation dataset from the ATCO2 project.",True
kocour21_interspeech,https://www.isca-archive.org/interspeech_2021/kocour21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/kocour21_interspeech.html,Boosting of Contextual Information in ASR for Air-Traffic Call-Sign Recognition,Automatic Speech Recognition in Air Traffic Management,2021,"Contextual adaptation of ASR can be very beneficial for multi-accent
and often noisy Air-Traffic Control (ATC) speech. Our focus is call-sign
recognition, which can be used to track conversations of ATC operators
with individual airplanes. We developed a two-stage boosting strategy,
consisting of HCLG boosting and Lattice boosting. Both are implemented
as WFST compositions and the contextual information is specific to
each utterance. In HCLG boosting we give score discounts to individual
words, while in Lattice boosting the score discounts are given to word
sequences. The context data have origin in surveillance database of
OpenSky Network. From this, we obtain lists of call-signs that are
made more likely to appear in the best hypothesis of ASR. This also
improves the accuracy of the NLU module that recognizes the call-signs
from the best hypothesis of ASR.
  As part of ATCO2
project, we collected liveatc test set2. The boosting of call-signs
leads to 4.7% absolute WER improvement and 27.1% absolute increase
of Call-Sign recognition Accuracy (CSA). Our best result of 82.9% CSA
is quite good, given that the data is noisy, and WER 28.4% is relatively
high. We believe there is still room for improvement.",True
elie21_interspeech,https://www.isca-archive.org/interspeech_2021/elie21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/elie21_interspeech.html,Modeling the Effect of Military Oxygen Masks on Speech Characteristics,Automatic Speech Recognition in Air Traffic Management,2021,"Wearing an oxygen mask changes the speech production of speakers. It
indeed modifies the vocal apparatus and perturbs the articulatory movements
of the speaker. This paper studies the impact of the oxygen mask of
military aircraft pilots on formant trajectories, both dynamically
(variations of the formants at a utterance level) and globally (mean
value at the utterance level) for 12 speakers.
  A comparative analysis
of speech collected with and without an oxygen mask shows that the
mask has a significant impact on the formant trajectories, both on
the mean values and on the formant variations at the utterance level.
This impact is strongly dependent on the speaker and also on the mask
model. These observations suggest that the articulatory movements of
the speaker are modified by the presence of the mask.
  These observations
are validated via a preliminary ASR experiment that uses a data augmentation
technique based on articulatory perturbations that are driven by our
experimental observations.",True
medina21_interspeech,https://www.isca-archive.org/interspeech_2021/medina21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/medina21_interspeech.html,Importance of Parasagittal Sensor Information in Tongue Motion Capture Through a Diphonic Analysis,Speech Production I,2021,"Our study examines the information obtained by adding two parasagittal
sensors to the standard midsagittal configuration of an Electromagnetic
Articulography (EMA) observation of lingual articulation. In this work,
we present a large and phonetically balanced corpus obtained from an
EMA recording session of a single English native speaker reading 1899
sentences from the Harvard and TIMIT corpora. According to a statistical
analysis of the diphones produced during the recording session, the
motion captured by the parasagittal sensors has a low correlation to
the midsagittal sensors in the mediolateral direction. We perform a
geometric analysis of the lateral tongue by the measure of its width
and using a proxy of the tongue’s curvature that is computed
using the Menger curvature. To provide a better understanding of the
tongue sensor motion we present dynamic visualizations of all diphones.
Finally, we present a summary of the velocity information computed
from the tongue sensor information.",True
vaaras21_interspeech,https://www.isca-archive.org/interspeech_2021/vaaras21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/vaaras21_interspeech.html,Automatic Analysis of the Emotional Content of Speech in Daylong Child-Centered Recordings from a Neonatal Intensive Care Unit,Emotion and Sentiment Analysis II,2021,"Researchers have recently started to study how the emotional speech
heard by young infants can affect their developmental outcomes. As
a part of this research, hundreds of hours of daylong recordings from
preterm infants’ audio environments were collected from two hospitals
in Finland and Estonia in the context of so-called APPLE study. In
order to analyze the emotional content of speech in such a massive
dataset, an automatic speech emotion recognition (SER) system is required.
However, there are no emotion labels or existing in-domain SER systems
to be used for this purpose. In this paper, we introduce this initially
unannotated large-scale real-world audio dataset and describe the development
of a functional SER system for the Finnish subset of the data. We explore
the effectiveness of alternative state-of-the-art techniques to deploy
a SER system to a new domain, comparing cross-corpus generalization,
WGAN-based domain adaptation, and active learning in the task. As a
result, we show that the best-performing models are able to achieve
a classification performance of 73.4% unweighted average recall (UAR)
and 73.2% UAR for a binary classification for valence and arousal,
respectively. The results also show that active learning achieves the
most consistent performance compared to the two alternatives.",True
kanda21_interspeech,https://www.isca-archive.org/interspeech_2021/kanda21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/kanda21_interspeech.html,Large-Scale Pre-Training of End-to-End Multi-Talker ASR for Meeting Transcription with Single Distant Microphone,"Multi- and Cross-Lingual ASR, Other Topics in ASR",2021,"Transcribing meetings containing overlapped speech with only a single
distant microphone (SDM) has been one of the most challenging problems
for automatic speech recognition (ASR). While various approaches have
been proposed, all previous studies on the monaural overlapped speech
recognition problem were based on either simulation data or small-scale
real data. In this paper, we extensively investigate a two-step approach
where we first pre-train a serialized output training (SOT)-based multi-talker
ASR by using large-scale simulation data and then fine-tune the model
with a small amount of real meeting data. Experiments are conducted
by utilizing 75 thousand (K) hours of our internal single-talker recording
to simulate a total of 900K hours of multi-talker audio segments for
supervised pre-training. With fine-tuning on the 70 hours of the AMI-SDM
training data, our SOT ASR model achieves a word error rate (WER) of
21.2% for the AMI-SDM evaluation set while automatically counting speakers
in each test segment. This result is not only significantly better
than the previous state-of-the-art WER of 36.4% with oracle utterance
boundary information but also better than a result by a similarly fine-tuned
single-talker ASR model applied to beamformed audio.",True
delrio21_interspeech,https://www.isca-archive.org/interspeech_2021/delrio21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/delrio21_interspeech.html,Earnings-21: A Practical Benchmark for ASR in the Wild,"Multi- and Cross-Lingual ASR, Other Topics in ASR",2021,"Commonly used speech corpora inadequately challenge academic and commercial
ASR systems. In particular, speech corpora lack metadata needed for
detailed analysis and WER measurement. In response, we present Earnings-21,
a 39-hour corpus of earnings calls containing entity-dense speech from
nine different financial sectors. This corpus is intended to benchmark
ASR systems in the wild with special attention towards named entity
recognition. We benchmark four commercial ASR models, two internal
models built with open-source tools, and an open-source LibriSpeech
model and discuss their differences in performance on Earnings-21.
Using our recently released fstalign tool, we provide a candid
analysis of each model’s recognition capabilities under different
partitions. Our analysis finds that ASR accuracy for certain NER categories
is poor, presenting a significant impediment to transcript comprehension
and usage. Earnings-21 bridges academic and commercial ASR system
evaluation and enables further research on entity modeling and WER
on real world audio.",True
ali21b_interspeech,https://www.isca-archive.org/interspeech_2021/ali21b_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/ali21b_interspeech.html,Arabic Code-Switching Speech Recognition Using Monolingual Data,"Multi- and Cross-Lingual ASR, Other Topics in ASR",2021,"Code-switching in automatic speech recognition (ASR) is an important
challenge due to globalization. Recent research in multilingual ASR
shows potential improvement over monolingual systems. We study key
issues related to multilingual modeling for ASR through a series of
large-scale ASR experiments. Our innovative framework deploys a multi-graph
approach in the weighted finite state transducers (WFST) framework.
We compare our WFST decoding strategies with a transformer sequence
to sequence system trained on the same data. Given a code-switching
scenario between Arabic and English languages, our results show that
the WFST decoding approaches were more suitable for the intersentential
code-switching datasets. In addition, the transformer system performed
better for intrasentential code-switching task. With this study, we
release an artificially generated development and test sets, along
with ecological code-switching test set, to benchmark the ASR performance.",True
ryant21_interspeech,https://www.isca-archive.org/interspeech_2021/ryant21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/ryant21_interspeech.html,The Third DIHARD Diarization Challenge,Speaker Diarization II,2021,"DIHARD III was the third in a series of speaker diarization challenges
intended to improve the robustness of diarization systems to variability
in recording equipment, noise conditions, and conversational domain.
Speaker diarization was evaluated under two speech activity conditions
(diarization from a reference speech activity vs. diarization from
scratch) and 11 diverse domains. The domains span a range of recording
conditions and interaction types, including read audio-books, meeting
speech, clinical interviews, web videos, and, for the first time, conversational
telephone speech. A total of 30 organizations (forming 21 teams) from
industry and academia submitted 499 valid system outputs. The evaluation
results indicate that speaker diarization has improved markedly since
DIHARD I, particularly for two-party interactions, but that for many
domains (e.g., web video) the problem remains far from solved.",True
palmer21_interspeech,https://www.isca-archive.org/interspeech_2021/palmer21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/palmer21_interspeech.html,Spoken ObjectNet: A Bias-Controlled Spoken Caption Dataset,"Tools, Corpora and Resources",2021,"Visually-grounded spoken language datasets can enable models to learn
cross-modal correspondences with very weak supervision. However, modern
audio-visual datasets contain biases that undermine the real-world
performance of models trained on that data. We introduce Spoken ObjectNet,
which is designed to remove some of these biases and provide a way
to better evaluate how effectively models will perform in real-world
scenarios. This dataset expands upon ObjectNet, which is a bias-controlled
image dataset that features similar image classes to those present
in ImageNet.
  We detail our data collection pipeline, which features several
methods to improve caption quality, including automated language model
checks. Lastly, we show baseline results on image retrieval and audio
retrieval tasks. These results show that models trained on other datasets
and then evaluated on Spoken ObjectNet tend to perform poorly due to
biases in other datasets that the models have learned. We also show
evidence that the performance decrease is due to the dataset controls,
and not the transfer setting.",True
salesky21_interspeech,https://www.isca-archive.org/interspeech_2021/salesky21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/salesky21_interspeech.html,The Multilingual TEDx Corpus for Speech Recognition and Translation,"Tools, Corpora and Resources",2021,"We present the Multilingual TEDx corpus, built to support speech recognition
(ASR) and speech translation (ST) research across many non-English
source languages. The corpus is a collection of audio recordings from
TEDx talks in 8 source languages. We segment transcripts into sentences
and align them to the source-language audio and target-language translations.
The corpus is released along with open-sourced code enabling extension
to new talks and languages as they become available. Our corpus creation
methodology can be applied to more languages than previous work, and
creates multi-way parallel evaluation sets. We provide baselines in
multiple ASR and ST settings, including multilingual models to improve
translation performance for low-resource language pairs.",True
mortensen21_interspeech,https://www.isca-archive.org/interspeech_2021/mortensen21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/mortensen21_interspeech.html,Tusom2021: A Phonetically Transcribed Speech Dataset from an Endangered Language for Universal Phone Recognition Experiments,"Tools, Corpora and Resources",2021,"There is growing interest in ASR systems that can recognize phones
in a language-independent fashion [1, 2, 3]. There is additionally
interest in building language technologies for low-resource and endangered
languages. However, there is a paucity of realistic data that can be
used to test such systems and technologies. This paper presents a publicly
available, phonetically transcribed corpus of 2255 utterances (words
and short phrases) in the endangered Tangkhulic language East Tusom
(no ISO 639-3 code), a Tibeto-Burman language variety spoken mostly
in India. Because the dataset is transcribed in terms of phones, rather
than phonemes, it is a better match for universal phone recognition
systems than many larger (phonemically transcribed) datasets. This
paper describes the dataset and the methodology used to produce it.
It further presents basic benchmarks of state-of-the-art universal
phone recognition systems on the dataset as baselines for future experiments.",True
fu21b_interspeech,https://www.isca-archive.org/interspeech_2021/fu21b_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/fu21b_interspeech.html,"AISHELL-4: An Open Source Dataset for Speech Enhancement, Separation, Recognition and Speaker Diarization in Conference Scenario","Tools, Corpora and Resources",2021,"In this paper, we present AISHELL-4, a sizable real-recorded Mandarin
speech dataset collected by 8-channel circular microphone array for
speech processing in conference scenario. The dataset consists of 211
recorded meeting sessions, each containing 4 to 8 speakers, with a
total length of 120 hours. This dataset aims to bridge the advanced
research on multi-speaker processing and the practical application
scenario in three aspects. With real recorded meetings, AISHELL-4 provides
realistic acoustics and rich natural speech characteristics in conversation
such as short pause, speech overlap, quick speaker turn, noise, etc.
Meanwhile, accurate transcription and speaker voice activity are provided
for each meeting in AISHELL-4. This allows the researchers to explore
different aspects in meeting processing, ranging from individual tasks
such as speech front-end processing, speech recognition and speaker
diarization, to multi-modality modeling and joint optimization of relevant
tasks. Given most open source dataset for multi-speaker tasks are in
English, AISHELL-4 is the only Mandarin dataset for conversation speech,
providing additional value for data diversity in speech community.
We also release a PyTorch-based training and evaluation framework as
baseline system to promote reproducible research in this field.",True
chen21o_interspeech,https://www.isca-archive.org/interspeech_2021/chen21o_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/chen21o_interspeech.html,"GigaSpeech: An Evolving, Multi-Domain ASR Corpus with 10,000 Hours of Transcribed Audio","Tools, Corpora and Resources",2021,"This paper introduces GigaSpeech, an evolving, multi-domain English
speech recognition corpus with 10,000 hours of high quality labeled
audio suitable for supervised training, and 33,000 hours of total audio
suitable for semi-supervised and unsupervised training. Around 33,000
hours of transcribed audio is first collected from audiobooks, podcasts
and YouTube, covering both read and spontaneous speaking styles, and
a variety of topics, such as arts, science, sports, etc. A new forced
alignment and segmentation pipeline is proposed to create sentence
segments suitable for speech recognition training, and to filter out
segments with low-quality transcription. For system training, GigaSpeech
provides five subsets of different sizes, 10h, 250h, 1000h, 2500h,
and 10000h. For our 10,000-hour XL training subset, we cap the
word error rate at 4% during the filtering/ validation stage, and for
all our other smaller training subsets, we cap it at 0%. The DEV
and TEST evaluation sets, on the other hand, are re-processed
by professional human transcribers to ensure high transcription quality.
Baseline systems are provided for popular speech recognition toolkits,
namely Athena, ESPnet, Kaldi and Pika.",True
kim21k_interspeech,https://www.isca-archive.org/interspeech_2021/kim21k_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/kim21k_interspeech.html,Look Who’s Talking: Active Speaker Detection in the Wild,"Tools, Corpora and Resources",2021,"In this work, we present a novel audio-visual dataset for active speaker
detection in the wild. A speaker is considered active when his or her
face is visible and the voice is audible simultaneously. Although active
speaker detection is a crucial pre-processing step for many audio-visual
tasks, there is no existing active speaker detection dataset to evaluate
the performance using natural human speech. We therefore curate the
Active Speakers in the Wild (ASW) dataset which contains videos
and co-occurring speech segments with dense speech activity labels.
Videos and timestamps of audible segments are parsed and adopted from
VoxConverse, an existing speaker diarisation dataset that consists
of videos in the wild. Face tracks are extracted from the videos and
active segments are annotated based on the timestamps of VoxConverse
in a semi-automatic way. Two reference systems, one is self-supervised
and the other is supervised system, are evaluated on the dataset to
provide the baseline performances of ASW. Cross-domain evaluation and
case study are conducted, in order to show the negative effect of the
dubbed videos that are excluded in ASW.",True
ahmed21_interspeech,https://www.isca-archive.org/interspeech_2021/ahmed21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/ahmed21_interspeech.html,AusKidTalk: An Auditory-Visual Corpus of 3- to 12-Year-Old Australian Children’s Speech,"Tools, Corpora and Resources",2021,"Here we present AusKidTalk [1], an audio-visual (AV) corpus of Australian
children’s speech collected to facilitate the development of
speech based technological solutions for children. It builds upon the
technology and expertise developed through the collection of an earlier
corpus of Australian adult speech, AusTalk [2,3]. This multi-site initiative
was established to remedy the dire shortage of children’s speech
corpora in Australia and around the world that are sufficiently sized
to train accurate automated speech processing tools for children. We
are collecting ~600 hours of speech from children aged 3–12
years that includes single word and sentence productions as well as
narrative and emotional speech. In this paper, we discuss the key requirements
for AusKidTalk and how we designed the recording setup and protocol
to meet them. We also discuss key findings from our feasibility study
of the recording protocol, recording tools, and user interface.",True
garcesdiazmunio21_interspeech,https://www.isca-archive.org/interspeech_2021/garcesdiazmunio21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/garcesdiazmunio21_interspeech.html,Europarl-ASR: A Large Corpus of Parliamentary Debates for Streaming ASR Benchmarking and Speech Data Filtering/Verbatimization,"Tools, Corpora and Resources",2021,"We introduce Europarl-ASR, a large speech and text corpus of parliamentary
debates including 1 300 hours of transcribed speeches and 70 million
tokens of text in English extracted from European Parliament sessions.
The training set is labelled with the Parliament’s non-fully-verbatim
official transcripts, time-aligned. As verbatimness is critical for
acoustic model training, we also provide automatically noise-filtered
and automatically verbatimized transcripts of all speeches based on
speech data filtering and verbatimization techniques. Additionally,
18 hours of transcribed speeches were manually verbatimized to build
reliable speaker-dependent and speaker-independent development/test
sets for streaming ASR benchmarking. The availability of manual non-verbatim
and verbatim transcripts for dev/test speeches makes this corpus useful
for the assessment of automatic filtering and verbatimization techniques.
This paper describes the corpus and its creation, and provides off-line
and streaming ASR baselines for both the speaker-dependent and speaker-independent
tasks using the three training transcription sets. The corpus is publicly
released under an open licence.",True
kapoor21_interspeech,https://www.isca-archive.org/interspeech_2021/kapoor21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/kapoor21_interspeech.html,Towards Automatic Speech to Sign Language Generation,"Tools, Corpora and Resources",2021,"We aim to solve the highly challenging task of generating continuous
sign language videos solely from speech segments for the first time.
Recent efforts in this space have focused on generating such videos
from human-annotated text transcripts without considering other modalities.
However, replacing speech with sign language proves to be a practical
solution while communicating with people suffering from hearing loss.
Therefore, we eliminate the need of using text as input and design
techniques that work for more natural, continuous, freely uttered speech
covering an extensive vocabulary. Since the current datasets are inadequate
for generating sign language directly from speech, we collect and release
the first Indian sign language dataset comprising speech-level annotations,
text transcripts, and the corresponding sign-language videos. Next,
we propose a multi-tasking transformer network trained to generate
signer’s poses from speech segments. With speech-to-text as an
auxiliary task and an additional cross-modal discriminator, our model
learns to generate continuous sign pose sequences in an end-to-end
manner. Extensive experiments and comparisons with other baselines
demonstrate the effectiveness of our approach. We also conduct additional
ablation studies to analyze the effect of different modules of our
network. A demo video containing several results is attached to the
supplementary material.",True
cho21b_interspeech,https://www.isca-archive.org/interspeech_2021/cho21b_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/cho21b_interspeech.html,kosp2e: Korean Speech to English Translation Corpus,"Tools, Corpora and Resources",2021,"Most speech-to-text (S2T) translation studies use English speech as
a source, which makes it difficult for non-English speakers to take
advantage of the S2T technologies. For some languages, this problem
was tackled through corpus construction, but the farther linguistically
from English or the more under-resourced, this deficiency and underrepresentedness
becomes more significant. In this paper, we introduce kosp2e
(read as ‘kospi’), a corpus that allows Korean speech to
be translated into English text in an end-to-end manner. We adopt open
license speech recognition corpus, translation corpus, and spoken language
corpora to make our dataset freely available to the public, and check
the performance through the pipeline and training-based approaches.
Using pipeline and various end-to-end schemes, we obtain the highest
BLEU of 21.3 and 18.0 for each based on the English hypothesis, validating
the feasibility of our data. We plan to supplement annotations for
other target languages through community contributions in the future.",True
zhang21x_interspeech,https://www.isca-archive.org/interspeech_2021/zhang21x_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/zhang21x_interspeech.html,speechocean762: An Open-Source Non-Native English Speech Corpus for Pronunciation Assessment,"Tools, Corpora and Resources",2021,"This paper introduces a new open-source speech corpus named “speechocean762”
designed for pronunciation assessment use, consisting of 5000 English
utterances from 250 non-native speakers, where half of the speakers
are children. Five experts annotated each of the utterances at sentence-level,
word-level and phoneme-level. A baseline system is released in open
source to illustrate the phoneme-level pronunciation assessment workflow
on this corpus. This corpus is allowed to be used freely for commercial
and non-commercial purposes. It is available for free download from
OpenSLR, and the corresponding baseline system is published in the
Kaldi speech recognition toolkit.",True
luz21_interspeech,https://www.isca-archive.org/interspeech_2021/luz21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/luz21_interspeech.html,Detecting Cognitive Decline Using Speech Only: The ADReSSo Challenge,The ADReSSo Challenge: Detecting Cognitive Decline Using Speech Only,2021,"Building on the success of the ADReSS Challenge at Interspeech 2020,
which attracted the participation of 34 teams from across the world,
the ADReSSo Challenge targets three difficult automatic prediction
problems of societal and medical relevance, namely: detection of Alzheimer’s
Dementia, inference of cognitive testing scores, and prediction of
cognitive decline. This paper presents these prediction tasks in detail,
describes the datasets used, and reports the results of the baseline
classification and regression models we developed for each task. A
combination of acoustic and linguistic features extracted directly
from audio recordings, without human intervention, yielded a baseline
accuracy of 78.87% for the AD classification task, a root mean squared
error (RMSE) of 5.28 for prediction of cognitive scores , and 68.75%
accuracy (F1 = 66.67) for the cognitive decline prediction
task.",True
gretter21_interspeech,https://www.isca-archive.org/interspeech_2021/gretter21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/gretter21_interspeech.html,ETLT 2021: Shared Task on Automatic Speech Recognition for Non-Native Children’s Speech,Robust and Far-Field ASR,2021,"The paper presents the Second ASR Challenge for Non-native Children’s
Speech proposed as a Special Session at Interspeech 2021, following
the successful first challenge at Interspeech 2020. The goal of the
challenge is to advance research on non-native children’s speech
recognition technology, as speech technology still struggles when applied
to both children and non-native speakers. The audio data consists of
spoken responses provided by L2 students in the context of both English
and German speaking proficiency examinations, the latter language added
for 2021. Additional training data and a new evaluation set was released
for L2 English recorded by speakers of different native languages.
Participants could build systems for one or both languages. Each had
a closed track where a predetermined set of audio and linguistic resources
were selected, and an open track where additional data was allowed.
After a description of the released corpora, the paper analyzes the
results achieved by the participating systems. Some issues suggested
from these results are discussed.",True
cornell21_interspeech,https://www.isca-archive.org/interspeech_2021/cornell21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/cornell21_interspeech.html,Learning to Rank Microphones for Distant Speech Recognition,Robust and Far-Field ASR,2021,"Fully exploiting ad-hoc microphone networks for distant speech recognition
is still an open issue. Empirical evidence shows that being able to
select the best microphone leads to significant improvements in recognition
without any additional effort on front-end processing. Current channel
selection techniques either rely on signal, decoder or posterior-based
features. Signal-based features are inexpensive to compute but do not
always correlate with recognition performance. Instead decoder and
posterior-based features exhibit better correlation but require substantial
computational resources.
  In this work, we tackle
the channel selection problem by proposing MicRank, a learning to rank
framework where a neural network is trained to rank the available channels
using directly the recognition performance on the training set. The
proposed approach is agnostic with respect to the array geometry and
type of recognition back-end. We investigate different learning to
rank strategies using a synthetic dataset developed on purpose and
the CHiME-6 data. Results show that the proposed approach considerably
improves over previous selection techniques, reaching comparable and
in some instances better performance than oracle signal-based measures.",True
torresquintero21_interspeech,https://www.isca-archive.org/interspeech_2021/torresquintero21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/torresquintero21_interspeech.html,ADEPT: A Dataset for Evaluating Prosody Transfer,Speech Synthesis: Prosody Modeling II,2021,"Text-to-speech is now able to achieve near-human naturalness and research
focus has shifted to increasing expressivity. One popular method is
to transfer the prosody from a reference speech sample. There have
been considerable advances in using prosody transfer to generate more
expressive speech, but the field lacks a clear definition of what successful
prosody transfer means and a method for measuring it. We introduce
a dataset of prosodically-varied reference natural speech samples for
evaluating prosody transfer. The samples include global variations
reflecting emotion and interpersonal attitude, and local variations
reflecting topical emphasis, propositional attitude, syntactic phrasing
and marked tonicity. The corpus only includes prosodic variations that
listeners are able to distinguish with reasonable accuracy, and we
report these figures as a benchmark against which text-to-speech prosody
transfer can be compared. We conclude the paper with a demonstration
of our proposed evaluation methodology, using the corpus to evaluate
two text-to-speech models that perform prosody transfer.",True
borsdorf21b_interspeech,https://www.isca-archive.org/interspeech_2021/borsdorf21b_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/borsdorf21b_interspeech.html,GlobalPhone Mix-To-Separate Out of 2: A Multilingual 2000 Speakers Mixtures Database for Speech Separation,Source Separation III,2021,"Monaural speech separation has been well studied on various databases.
However, these databases mostly concern English speech. Research in
multi-speaker scenarios, such as speech recognition, speaker recognition,
speaker diarization, and speech separation calls for speaker mixtures
databases comprising multiple languages. In this paper, we propose
a new extensive multilingual database for speech separation tasks derived
from the GlobalPhone 2000 Speaker Package, called “GlobalPhone
Mix-to-Separate out of 2” (GlobalPhoneMS2). We describe the construction
of the database and conduct speech separation experiments in monolingual
and multilingual as well as seen and unseen languages settings. When
trained on a multilingual dataset, the networks improve their performances
for unseen languages, and across almost all seen languages. We show
that replacing a monolingual dataset with a trilingual one, while keeping
the data size roughly the same, helps to improve the performance in
most cases. We attribute this to a larger diversity in speech, language,
speaker, and recording characteristics. Based on the GlobalPhoneMS2
database, speech separation results for two-speaker mixing scenarios
are reported in 22 spoken languages for the first time.",True
hejna21_interspeech,https://www.isca-archive.org/interspeech_2021/hejna21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/hejna21_interspeech.html,Exploration of Welsh English Pre-Aspiration: How Wide-Spread is it?,Phonetics II,2021,"This study investigates how widespread pre-aspiration and local breathiness
are in English spoken in Wales, by speakers identifying as Welsh. While
the main purpose is to establish whether the phenomenon is generally
present in Welsh English, the data also enables us to explore whether
pre-aspiration might be conditioned by sex/gender, age, and the ability
to speak Welsh. An acoustic corpus of 45 speakers producing word-final
plosives and fricatives is analysed.
  Pre-aspiration and
local breathiness are produced by all speakers, representing 32 towns
and 16 areas included in the analyses. Pre-aspiration and breathiness
are more frequent and longer in L1 and L2 Welsh speakers than those
who do not speak Welsh at all. In general, no statistically significant
sex and age effects emerge.
  In addition, a gradient
allophony between pre-aspiration and glottalisation is reported for
all speakers in the plosive context: the more frequently they glottalise,
the less frequent the pre-aspiration. In fricatives, most speakers
do not glottalise. Regarding those who do, 1. some display no relationship
between pre-aspiration and glottalisation, and 2. a minority display
either an indication of gradient allophony between the two, or 3. a
positive correlation.",True
muhlack21_interspeech,https://www.isca-archive.org/interspeech_2021/muhlack21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/muhlack21_interspeech.html,Revisiting Recall Effects of Filler Particles in German and English,Phonetics II,2021,"This paper reports on two experiments that partially replicate an experiment
by Fraundorf and Watson (2011, J Mem. Lang.) on the recall effect of
filler particles. Their subjects listened to three passages of a story,
either with or without filler particles, which they had to retell afterwards.
They analysed the subjects’ retelling in terms of whether important
plot points were remembered or not. For their English data, they found
that filler particles facilitate the recall of the plot points significantly
compared to stories that did not include filler particles. As this
seems to be a convincing experimental design, we aimed at evaluating
this method as a web-based experiment which may, if found to be suitable,
easily be applied to other languages. Furthermore, we investigated
whether their results are found in German as well (Experiment 1), and
evaluated whether filler duration has an effect on recall performance
(Experiment 2). Our results could not replicate the findings of the
original study: in fact, the opposite effect was found for German.
In Experiment 1, participants performed better on recall in the fluent
condition, while no significant results were found for English in Experiment
2.",True
huang21i_interspeech,https://www.isca-archive.org/interspeech_2021/huang21i_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/huang21i_interspeech.html,"A Cross-Dialectal Comparison of Apical Vowels in Beijing Mandarin, Northeastern Mandarin and Southwestern Mandarin: An EMA and Ultrasound Study",Phonetics II,2021,"This paper is a comparative study of the articulation of the “apical
vowels” in three Mandarin dialects: Beijing Mandarin (BJM), Northeastern
Mandarin (NEM), and Southwestern Mandarin (SWM), using co-registered
EMA and ultrasound. Data from 5 BJM speakers, 5 NEM speakers and 4
SWM speakers in their twenties were analyzed and discussed. Our recording
materials include the dental and retroflex apical vowels, and their
er-suffixed forms. Results suggest that distinct lingual configurations
are found among the three dialects of Mandarin, even though these apical
vowels are not perceptually distinguishable. Specifically, the dental
apical vowel [ɿ] has a grooved tongue shape in BJM, a retracted
tongue dorsum in NEM, and a relatively flat tongue shape in SWM. The
retroflex apical vowel [ʅ] has a domed tongue shape as well as
a bunched tongue body in NEM, while a slightly domed tongue posture
is found in SWM. Moreover, the retroflex apical vowel [ʅ] is,
articulatorily speaking, very similar to the er-suffix in BJM
(cf. [10]). In sum, we observed yet another instance of the articulatory-acoustic
mismatch.",True
guevararukoz21_interspeech,https://www.isca-archive.org/interspeech_2021/guevararukoz21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/guevararukoz21_interspeech.html,Speech Perception and Loanword Adaptations: The Case of Copy-Vowel Epenthesis,Phonetics II,2021,"Japanese allows for almost no consonants in syllable codas. In loanwords,
illegal codas are transformed into onsets by means of vowel epenthesis.
The default epenthetic vowel in loanwords is [ɯ], and previous
work has shown that this [ɯ]-epenthesis reflects Japanese listeners’
perception of illegal coda consonants. Here, we focus on one of the
non-default cases: following coda [ç] and [x] the epenthetic vowel
is a copy of the preceding vowel. Using an identification and a discrimination
task, we provide evidence for the perceptual origin of this copy vowel
phenomenon: After [ç] and [x], Japanese listeners perceive more
often an epenthetic copy vowel than the default vowel [ɯ], whereas
after [k] it is the reverse.",True
meister21_interspeech,https://www.isca-archive.org/interspeech_2021/meister21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/meister21_interspeech.html,Developmental Changes of Vowel Acoustics in Adolescents,Phonetics II,2021,"The paper explores the developmental changes of vowel acoustics in
Estonian adolescents as a function of age and gender. Formant frequencies
F1–F4 and the duration of vowels were measured from read speech
samples of 305 native Estonian subjects (173 girls and 132 boys) aged
from 10 to 18 years. GAM framework was applied for the statistical
analysis. The results show that both the formant frequencies and the
vowel space area decrease gradually from 10 to 15 years in both gender
groups and the quality of vowels stabilizes at the age of 15–18
years, whereas gender-specific differences emerge around the age of
12–13. Age-related change in the duration of vowels shows similar
patterns with formants, however, with no gender difference. The findings
are in line with the results reported for adolescent speech in other
languages. The analysis results based on speech samples of the subjects
with normal linguistic development can be considered reference data
for distinguishing between normal and abnormal speech development.",True
billington21_interspeech,https://www.isca-archive.org/interspeech_2021/billington21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/billington21_interspeech.html,The Pacific Expansion: Optimizing Phonetic Transcription of Archival Corpora,Phonetics II,2021,"For most of the world’s languages, detailed phonetic analyses
across different aspects of the sound system do not exist, due in part
to limitations in available speech data and tools for efficiently processing
such data for low-resource languages. Archival language documentation
collections offer opportunities to extend the scope and scale of phonetic
research on low-resource languages, and developments in methods for
automatic recognition and alignment of speech facilitate the preparation
of phonetic corpora based on these collections. We present a case study
applying speech modelling and forced alignment methods to narrative
data for Nafsan, an Oceanic language of central Vanuatu. We examine
the accuracy of the forced-aligned phonetic labelling based on limited
speech data used in the modelling process, and compare acoustic and
durational measures of 17,851 vowel tokens for 11 speakers with previous
experimental phonetic data for Nafsan. Results point to the suitability
of archival data for large-scale studies of phonetic variation in low-resource
languages, and also suggest that this approach can feasibly be used
as a starting point in expanding to phonetic comparisons across closely-related
Oceanic languages.",True
shi21d_interspeech,https://www.isca-archive.org/interspeech_2021/shi21d_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/shi21d_interspeech.html,Polyphone Disambiguation in Mandarin Chinese with Semi-Supervised Learning,"Speech Synthesis: Linguistic Processing, Paradigms and Other Topics",2021,"The majority of Chinese characters are monophonic, while a special
group of characters, called polyphonic characters, have multiple pronunciations.
As a prerequisite of performing speech-related generative tasks, the
correct pronunciation must be identified among several candidates.
This process is called Polyphone Disambiguation. Although the problem
has been well explored with both knowledge-based and learning-based
approaches, it remains challenging due to the lack of publicly available
labeled datasets and the irregular nature of polyphone in Mandarin
Chinese. In this paper, we propose a novel semi-supervised learning
(SSL) framework for Mandarin Chinese polyphone disambiguation that
can potentially leverage unlimited unlabeled text data. We explore
the effect of various proxy labeling strategies including entropy-thresholding
and lexicon-based labeling. Qualitative and quantitative experiments
demonstrate that our method achieves state-of-the-art performance.
In addition, we publish a novel dataset specifically for the polyphone
disambiguation task to promote further researches.",True
chen21s_interspeech,https://www.isca-archive.org/interspeech_2021/chen21s_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/chen21s_interspeech.html,A Neural-Network-Based Approach to Identifying Speakers in Novels,"Speech Synthesis: Linguistic Processing, Paradigms and Other Topics",2021,"Identifying speakers in novels aims at determining who says a quote
in a given context by text analysis. This task is important for speech
synthesis systems to assign appropriate voices to the quotes when producing
audiobooks. However, existing approaches stick with manual features
and traditional machine learning classifiers, which constrain the accuracy
of speaker identification. In this paper, we propose a method to tackle
this challenging problem with the help of deep learning. We formulate
speaker identification as a scoring task and build a candidate scoring
network (CSN) based on BERT. Candidate-specific segments are put forward
to eliminate redundant context information. Moreover, a revision algorithm
is designed utilizing the speaker alternation pattern in two-party
dialogues. Experiments have been conducted using the dataset built
on the Chinese novel World of Plainness. The results show that
our proposed method reaches a new state-of-the-art performance with
an identification accuracy of 82.5%, which outperforms the baseline
using manual features by 12%.",True
mansbach21_interspeech,https://www.isca-archive.org/interspeech_2021/mansbach21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/mansbach21_interspeech.html,An Agent for Competing with Humans in a Deceptive Game Based on Vocal Cues,Speech Type Classification and Diagnosis,2021,"In this work we present the development of an autonomous agent capable
of competing with humans in a deception-based game. The agent predicts
whether a given statement is true or false based on vocal cues. To
this end, we develop a game for collecting a large scale and high quality
labeled sound data-set in a controlled environment in English and Hebrew.
We develop a model that can detect deception based on vocal statements
from the participants of the experiment, and show that the model is
more accurate than humans.
  We develop an agent
that uses the developed deception model and interacts with humans within
our deceptive environment. We show that our agent significantly outperforms
a simple agent that does not use the deception model; that is, it wins
significantly more games when played against human players. In addition,
we use our model to detect whether a statement will be perceived as
a lie or not by human subjects, based on its vocal cues.",True
mazumder21_interspeech,https://www.isca-archive.org/interspeech_2021/mazumder21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/mazumder21_interspeech.html,Few-Shot Keyword Spotting in Any Language,Spoken Term Detection & Voice Search,2021,"We introduce a few-shot transfer learning method for keyword spotting
in any language. Leveraging open speech corpora in nine languages,
we automate the extraction of a large multilingual keyword bank and
use it to train an embedding model. With just five training examples,
we fine-tune the embedding model for keyword spotting and achieve an
average F1 score of 0.75 on keyword classification for 180
new keywords unseen by the embedding model in these nine languages.
This embedding model also generalizes to new languages. We achieve
an average F1 score of 0.65 on 5-shot models for 260 keywords
sampled across 13 new languages unseen by the embedding model. We investigate
streaming accuracy for our 5-shot models in two contexts: keyword spotting
and keyword search. Across 440 keywords in 22 languages, we achieve
an average streaming keyword spotting accuracy of 87.4% with a false
acceptance rate of 4.3%, and observe promising initial results on keyword
search.",True
jia21b_interspeech,https://www.isca-archive.org/interspeech_2021/jia21b_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/jia21b_interspeech.html,"The 2020 Personalized Voice Trigger Challenge: Open Datasets, Evaluation Metrics, Baseline System and Results",Spoken Term Detection & Voice Search,2021,"The 2020 Personalized Voice Trigger Challenge (PVTC2020) addresses
two different research problems in a unified setup: joint wake-up word
detection with speaker verification on close-talking single microphone
data and far-field multi-channel microphone array data. Specially,
the second task poses an additional cross-channel matching challenge
on top of the far-field condition. To simulate the real-life application
scenario, the enrollment utterances are recorded from close-talking
cell-phone only, while the test utterances are recorded from both the
close-talking cell-phone and the far-field microphone arrays. This
paper introduces our challenge setup and the released database as well
as the evaluation metrics. In addition, we present a sequential two
stage end-to-end neural network baseline system trained with the proposed
database for speaker-dependent wake-up word detection. Results show
that state-of-the-art personalized voice trigger methods are still
based on the two stage design, however, this benchmark database could
also be used to evaluate multi-task joint learning methods. The official
website, the open-source baseline system and results of submitted systems
have been released.",True
wang21ea_interspeech,https://www.isca-archive.org/interspeech_2021/wang21ea_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/wang21ea_interspeech.html,"Auto-KWS 2021 Challenge: Task, Datasets, and Baselines",Spoken Term Detection & Voice Search,2021,"Auto-KWS 2021 challenge calls for automated machine learning (AutoML)
solutions to automate the process of applying machine learning to a
customized keyword spotting task. Compared with other keyword spotting
tasks, Auto-KWS challenge has the following three characteristics:
1) The challenge focuses on the problem of customized keyword spotting,
where the target device can only be awakened by an enrolled speaker
with his/her specified keyword. The speaker can use any language and
accent to define his keyword. 2) All data of the challenge is recorded
in realistic environment to simulate different user scenarios. 3) Auto-KWS
is a “code competition”, where participants need to submit
AutoML solutions, then the platform automatically runs the enrollment
and prediction steps with the submitted code. This challenge aims at
promoting the development of a more personalized and flexible keyword
spotting system. Two baseline systems are provided to all participants
as references.",True
zhang21ca_interspeech,https://www.isca-archive.org/interspeech_2021/zhang21ca_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/zhang21ca_interspeech.html,An Initial Investigation for Detecting Partially Spoofed Audio,Voice Anti-Spoofing and Countermeasure,2021,"All existing databases of spoofed speech contain attack data that is
spoofed in its entirety. In practice, it is entirely plausible that
successful attacks can be mounted with utterances that are only partially
spoofed. By definition, partially-spoofed utterances contain a mix
of both spoofed and bona fide segments, which will likely degrade the
performance of countermeasures trained with entirely spoofed utterances.
This hypothesis raises the obvious question: ‘Can we detect
partially-spoofed audio?’ This paper introduces a new database
of partially-spoofed data, named PartialSpoof, to help address this
question. This new database enables us to investigate and compare the
performance of countermeasures on both utterance- and segmental- level
labels. Experimental results using the utterance-level labels reveal
that the reliability of countermeasures trained to detect fully-spoofed
data is found to degrade substantially when tested with partially-spoofed
data, whereas training on partially-spoofed data performs reliably
in the case of both fully- and partially-spoofed utterances. Additional
experiments using segmental-level labels show that spotting injected
spoofed segments included in an utterance is a much more challenging
task even if the latest countermeasure models are used.",True
peterson21_interspeech,https://www.isca-archive.org/interspeech_2021/peterson21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/peterson21_interspeech.html,OpenASR20: An Open Challenge for Automatic Speech Recognition of Conversational Telephone Speech in Low-Resource Languages,OpenASR20 and Low Resource ASR Development,2021,"In 2020, the National Institute of Standards and Technology (NIST),
in cooperation with the Intelligence Advanced Research Project Activity
(IARPA), conducted an open challenge on automatic speech recognition
(ASR) technology for low-resource languages on a challenging data type
— conversational telephone speech. The OpenASR20 Challenge was
offered for ten low-resource languages — Amharic, Cantonese,
Guarani, Javanese, Kurmanji Kurdish, Mongolian, Pashto, Somali, Tamil,
and Vietnamese. A total of nine teams from five countries fully participated,
and 128 valid submissions were scored. This paper gives an overview
of the challenge setup and procedures, as well as a summary of the
results. The results show overall high word error rate (WER), with
the best results on a severely constrained training data condition
ranging from 0.4 to 0.65, depending on the language. ASR with such
limited resources remains a challenging problem. Providing a computing
platform may be a way to level the playing field and encourage wider
participation in challenges like OpenASR.",True
merkx21_interspeech,https://www.isca-archive.org/interspeech_2021/merkx21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/merkx21_interspeech.html,Semantic Sentence Similarity: Size does not Always Matter,Keyword Search and Spoken Language Processing,2021,"This study addresses the question whether visually grounded speech
recognition (VGS) models learn to capture sentence semantics without
access to any prior linguistic knowledge. We produce synthetic and
natural spoken versions of a well known semantic textual similarity
database and show that our VGS model produces embeddings that correlate
well with human semantic similarity judgements. Our results show that
a model trained on a small image-caption database outperforms two models
trained on much larger databases, indicating that database size is
not all that matters. We also investigate the importance of having
multiple captions per image and find that this is indeed helpful even
if the total number of images is lower, suggesting that paraphrasing
is a valuable learning signal. While the general trend in the field
is to create ever larger datasets to train models on, our findings
indicate other characteristics of the database can just as important.",True
vidal21_interspeech,https://www.isca-archive.org/interspeech_2021/vidal21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/vidal21_interspeech.html,Phone-Level Pronunciation Scoring for Spanish Speakers Learning English Using a GOP-DNN System,"Applications in Transcription, Education and Learning",2021,"In today’s globalized world being able to communicate in English
is crucial to many people. Computer assisted pronunciation training
(CAPT) systems can help students achieve English proficiency by providing
an accessible way to practice, offering personalized feedback. However,
phone-level pronunciation scoring is still a very challenging task,
with performance far from that of human annotators. In this paper we
compare and present results on the Spanish subset of the L2-ARCTIC
corpus and the new Epa-DB database, both containing non-native English
speech by native Spanish speakers and intended for the development
of pronunciation scoring systems. We show the most frequent errors
in each database and compare performance of a state-of-the-art goodness
of pronunciation (GOP) system. Results show that both databases have
similar error patterns and that performance is similar for most phones,
despite differences in recording conditions. For the EpaDB database
we also present an analysis of the errors per target phone. This study
validates the EpaDB collection and annotations, providing initial results
and contributing to the advancement of a challenging low-resource task.",True
lin21j_interspeech,https://www.isca-archive.org/interspeech_2021/lin21j_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/lin21j_interspeech.html,Deep Feature Transfer Learning for Automatic Pronunciation Assessment,"Applications in Transcription, Education and Learning",2021,"Automatic pronunciation assessment is commonly developed to evaluate
pronunciation quality of second language (L2) learners. Traditional
methods for automatic pronunciation assessment normally utilize speech
features such as Goodness of pronunciation (GOP), which may not provide
sufficient information for the pronunciation proficiency assessment
[1]. In this paper, we propose a transfer learning method for automatic
pronunciation assessment. We directly utilize the deep features from
the acoustic model instead of traditional features such as GOP, and
transfer the acoustic knowledge from ASR to a specific scoring module.
The scoring module is designed to consider the relationship among different
granularities in an utterance based on an attention mechanism. Only
this module is updated for faster transfer and adaptation of various
pronunciation assessment tasks. Experimental results based on the dataset
recorded by Chinese English-as-second-language (ESL) learners and the
Speechocean762 dataset demonstrate that the proposed method outperforms
the traditional GOP-based baselines in Pearson correlation coefficient
(PCC) and yields parameter-efficient transfer for different pronunciation
assessment tasks.",True
yan21d_interspeech,https://www.isca-archive.org/interspeech_2021/yan21d_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/yan21d_interspeech.html,Adaptive Text to Speech for Spontaneous Style,Speech Synthesis: Speaking Style and Emotion,2021,"While recent text to speech (TTS) models perform very well in synthesizing
reading-style (e.g., audiobook) speech, it is still challenging to
synthesize spontaneous-style speech (e.g., podcast or conversation),
mainly because of two reasons: 1) the lack of training data for spontaneous
speech; 2) the difficulty in modeling the filled pauses (um
and uh) and diverse rhythms in spontaneous speech. In this paper,
we develop AdaSpeech 3, an adaptive TTS system that fine-tunes a well-trained
reading-style TTS model for spontaneous-style speech. Specifically,
1) to insert filled pauses (FP) in the text sequence appropriately,
we introduce an FP predictor to the TTS model; 2) to model the varying
rhythms, we introduce a duration predictor based on mixture of experts
(MoE), which contains three experts responsible for the generation
of fast, medium and slow speech respectively, and fine-tune it as well
as the pitch predictor for rhythm adaptation; 3) to adapt to other
speaker timbre, we fine-tune some parameters in the decoder with few
speech data. To address the challenge of lack of training data, we
mine a spontaneous speech dataset to support our research this work
and facilitate future research on spontaneous TTS. Experiments show
that AdaSpeech 3 synthesizes speech with natural FP and rhythms in
spontaneous styles, and achieves much better MOS and SMOS scores than
previous adaptive TTS systems.",True
an21b_interspeech,https://www.isca-archive.org/interspeech_2021/an21b_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/an21b_interspeech.html,Improving Performance of Seen and Unseen Speech Style Transfer in End-to-End Neural TTS,Speech Synthesis: Speaking Style and Emotion,2021,"End-to-end neural TTS training has shown improved performance in speech
style transfer. However, the improvement is still limited by the training
data in both target styles and speakers. Inadequate style transfer
performance occurs when the trained TTS tries to transfer the speech
to a target style from a new speaker with an unknown, arbitrary style.
In this paper, we propose a new approach to style transfer for both
seen and unseen styles, with disjoint, multi-style datasets, i.e.,
datasets of different styles are recorded, each individual style is
by one speaker with multiple utterances. To encode the style information,
we adopt an inverse autoregressive flow (IAF) structure to improve
the variational inference. The whole system is optimized to minimize
a weighed sum of four different loss functions: 1) a reconstruction
loss to measure the distortions in both source and target reconstructions;
2) an adversarial loss to “fool” a well-trained discriminator;
3) a style distortion loss to measure the expected style loss after
the transfer; 4) a cycle consistency loss to preserve the speaker identity
of the source after the transfer. Experiments demonstrate, both objectively
and subjectively, the effectiveness of the proposed approach for seen
and unseen style transfer tasks. The performance of the new approach
is better and more robust than those of four baseline systems of the
prior art.",True
dao21_interspeech,https://www.isca-archive.org/interspeech_2021/dao21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/dao21_interspeech.html,Intent Detection and Slot Filling for Vietnamese,Spoken Language Understanding II,2021,"Intent detection and slot filling are important tasks in spoken and
natural language understanding. However, Vietnamese is a low-resource
language in these research topics. In this paper, we present the first
public intent detection and slot filling dataset for Vietnamese. In
addition, we also propose a joint model for intent detection and slot
filling, that extends the recent state-of-the-art JointBERT+CRF model
[1] with an intent-slot attention layer to explicitly incorporate intent
context information into slot filling via “soft” intent
label embedding. Experimental results on our Vietnamese dataset show
that our proposed model significantly outperforms JointBERT+CRF. We
publicly release our dataset and the implementation of our model.",True
saxon21_interspeech,https://www.isca-archive.org/interspeech_2021/saxon21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/saxon21_interspeech.html,End-to-End Spoken Language Understanding for Generalized Voice Assistants,Spoken Language Understanding II,2021,"End-to-end (E2E) spoken language understanding (SLU) systems predict
utterance semantics directly from speech using a single model. Previous
work in this area has focused on targeted tasks in fixed domains, where
the output semantic structure is assumed a priori and the input speech
is of limited complexity. In this work we present our approach to developing
an E2E model for generalized SLU in commercial voice assistants (VAs).
We propose a fully differentiable, transformer-based, hierarchical
system that can be pretrained at both the ASR and NLU levels. This
is then fine-tuned on both transcription and semantic classification
losses to handle a diverse set of intent and argument combinations.
This leads to an SLU system that achieves significant improvements
over baselines on a complex internal generalized VA dataset with a
43% improvement in accuracy, while still meeting the 99% accuracy benchmark
on the popular Fluent Speech Commands dataset. We further evaluate
our model on a hard test set, exclusively containing slot arguments
unseen in training, and demonstrate a nearly 20% improvement, showing
the efficacy of our approach in truly demanding VA scenarios.",True
cutler21_interspeech,https://www.isca-archive.org/interspeech_2021/cutler21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/cutler21_interspeech.html,INTERSPEECH 2021 Acoustic Echo Cancellation Challenge,INTERSPEECH 2021 Acoustic Echo Cancellation Challenge,2021,"The INTERSPEECH 2021 Acoustic Echo Cancellation Challenge is intended
to stimulate research in the area of acoustic echo cancellation (AEC),
which is an important part of speech enhancement and still a top issue
in audio communication. Many recent AEC studies report good performance
on synthetic datasets where the training and testing data may come
from the same underlying distribution. However, AEC performance often
degrades significantly on real recordings. Also, most of the conventional
objective metrics such as echo return loss enhancement and perceptual
evaluation of speech quality do not correlate well with subjective
speech quality tests in the presence of background noise and reverberation
found in realistic environments. In this challenge, we open source
two large datasets to train AEC models under both single talk and double
talk scenarios. These datasets consist of recordings from more than
5,000 real audio devices and human speakers in real environments, as
well as a synthetic dataset. We also open source an online subjective
test framework and provide an online objective metric service for researchers
to quickly test their results. The winners of this challenge are selected
based on the average Mean Opinion Score achieved across all different
single talk and double talk scenarios.",True
macdonald21_interspeech,https://www.isca-archive.org/interspeech_2021/macdonald21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/macdonald21_interspeech.html,Disordered Speech Data Collection: Lessons Learned at 1 Million Utterances from Project Euphonia,Speech Recognition of Atypical Speech,2021,"Speech samples from over 1000 individuals with impaired speech have
been submitted for Project Euphonia, aimed at improving automated speech
recognition systems for disordered speech. We provide an overview of
the corpus, which recently passed 1 million utterances (>1300 hours),
and review key lessons learned from this project. The reasoning behind
decisions such as phrase set composition, prompted vs extemporaneous
speech, metadata and data quality efforts are explained based on findings
from both technical and user-facing research.",True
venugopalan21_interspeech,https://www.isca-archive.org/interspeech_2021/venugopalan21_interspeech.pdf,https://www.isca-archive.org/interspeech_2021/venugopalan21_interspeech.html,Comparing Supervised Models and Learned Speech Representations for Classifying Intelligibility of Disordered Speech on Selected Phrases,Speech Recognition of Atypical Speech,2021,"Automatic classification of disordered speech can provide an objective
tool for identifying the presence and severity of a speech impairment.
Classification approaches can also help identify hard-to-recognize
speech samples to teach ASR systems about the variable manifestations
of impaired speech. Here, we develop and compare different deep learning
techniques to classify the intelligibility of disordered speech on
selected phrases. We collected samples from a diverse set of 661 speakers
with a variety of self-reported disorders speaking 29 words or phrases,
which were rated by speech-language pathologists for their overall
intelligibility using a five-point Likert scale. We then evaluated
classifiers developed using 3 approaches: (1) a convolutional neural
network (CNN) trained for the task, (2) classifiers trained on non-semantic
speech representations from CNNs that used an unsupervised objective
[1], and (3) classifiers trained on the acoustic (encoder) embeddings
from an ASR system trained on typical speech [2]. We found that the
ASR encoder’s embeddings considerably outperform the other two
on detecting and classifying disordered speech. Further analysis shows
that the ASR embeddings cluster speech by the spoken phrase, while
the non-semantic embeddings cluster speech by speaker. Also, longer
phrases are more indicative of intelligibility deficits than single
words.",True
oh22_interspeech,https://www.isca-archive.org/interspeech_2022/oh22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/oh22_interspeech.html,Dynamic Vertical Larynx Actions Under Prosodic Focus,Acoustic Phonetics and Prosody,2022,"Recently, Lee (2018) observes that one vertical larynx movement (VLM) is associated with an Accentual Phrase (AP) in Seoul Korean. The current study builds on these findings by investigating the effect of prosodic focus on vertical larynx actions. Target sentences were designed to produce four APs (e.g., Joohyun sold six yards of shabby garden field; AP[Joohyun-SUBJ] AP[shabby garden field] AP[six yards-OBJ] AP[sold-DECL], presented in Korean) and were used to elicit focus on the initial word of the object phrase (e.g., six). Articulatory data on VLM is obtained from five Seoul Korean speakers using real-time MRI. Results indicate that quantifiable VLMs observed for each sentence range from 3 to 6 movements, with 4 movements per sentence being the most frequent. Sentences with focus have more instances of VLM per sentence than those without. Focused sentences exhibit significantly greater vertical larynx displacement around the region of focus than the control. Our findings have implications for prosodic planning and pitch resetting, and ongoing analyses examine how VLMs align with Accentual Phrases in Seoul Korean and correlate with fundamental frequency.",True
r22_interspeech,https://www.isca-archive.org/interspeech_2022/r22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/r22_interspeech.html,Generalized Keyword Spotting using ASR embeddings,Spoken Machine Translation,2022,"Keyword Spotting (KWS) detects a set of pre-defined spoken keywords. Building a KWS system for an arbitrary set requires massive training datasets. We propose to use the text transcripts from an Automatic Speech Recognition (ASR) system alongside triplets for KWS training. The intermediate representation from the ASR system trained on a speech corpus is used as acoustic word embeddings for keywords. Triplet loss is added to the Connectionist Temporal Classification (CTC) loss in the ASR while training. This method achieves an Average Precision (AP) of 0.843 over 344 words unseen by the model trained on the TIMIT dataset. In contrast, the Multi-View recurrent method that learns jointly on the text and acoustic embeddings achieves only 0.218 for out-of-vocabulary words. This method is also applied to low-resource languages such as Tamil by converting Tamil characters to English using transliteration. This is a very challenging novel task for which we provide a dataset of transcripts for the keywords. Despite our model not generalizing well, we achieve a benchmark AP of 0.321 on over 38 words unseen by the model on the MSWC Tamil keyword set. The model also produces an accuracy of 96.2% for classification tasks on the Google Speech Commands dataset.",True
schmidt22_interspeech,https://www.isca-archive.org/interspeech_2022/schmidt22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/schmidt22_interspeech.html,PodcastMix: A dataset for separating music and speech in podcasts,Source Separation II,2022,"We introduce PodcastMix, a dataset formalizing the task of separating background music and foreground speech in podcasts. We aim at defining a benchmark suitable for training and evaluating (deep learning) source separation models. To that end, we release a large and diverse training dataset based on programatically generated podcasts. However, current (deep learning) models can incur into generalization issues, specially when trained on synthetic data. To target potential generalization issues, we release an evaluation set based on real podcasts for which we design objective and subjective tests. Out of our experiments with real podcasts, we find that current (deep learning) models may have generalization issues. Yet, these can perform competently, e.g., our best baseline separates speech with a mean opinion score of 3.84 (rating ``overall separation quality"" from 1 to 5). The dataset and baselines are accessible online.",True
borsdorf22_interspeech,https://www.isca-archive.org/interspeech_2022/borsdorf22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/borsdorf22_interspeech.html,Blind Language Separation: Disentangling Multilingual Cocktail Party Voices by Language,Source Separation II,2022,"We introduce blind language separation (BLS) as novel research task, in which we seek to disentangle overlapping voices of multiple languages by language. BLS is expected to separate seen as well as unseen languages, which is different from the target language extraction task that works for one seen target language at a time. To develop a BLS model, we simulate a multilingual cocktail party database, of which each scene consists of two randomly selected languages, each represented by two randomly selected speakers. The database follows the recently proposed GlobalPhoneMCP database design concept that uses the audio data of the GlobalPhone 2000 Speaker Package. We show that a BLS model is able to learn the language characteristics so as to disentangle overlapping voices by language. We achieve a mean SI-SDR improvement of 12.63 dB over 231 test sets. The performance on the individual test sets varies depending on the language combination. Finally, we show that BLS can generalize well to unseen speakers and languages in the mixture.",True
deadman22_interspeech,https://www.isca-archive.org/interspeech_2022/deadman22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/deadman22_interspeech.html,Modelling Turn-taking in Multispeaker Parties for Realistic Data Simulation,Source Separation II,2022,"Simulation plays a crucial role in developing components of automatic speech recognition systems such as enhancement and diarization. In source separation and target-speaker extraction, datasets with high degrees of temporal overlap are used both in training and evaluation. However, this contrasts with the fact that people tend to avoid such overlap in real conversations. It is well known that artifacts introduced from pre-processing with no overlapping speech can be detrimental to recognition performance. This work proposes a finite-state based generative method trained on timing information in speech corpora, which leads to two main contributions. First, a method for generating arbitrary large datasets which follow desired statistics of real parties. Second, features extracted from the models are shown to have a correlation with speaker extraction performance. This leads to the contribution of quantifying how much difficulty in a mixture is due to turn-taking, factoring out other complexities in the signal. Models which treat speakers as independent produce poor generation and representation results. We improve upon this by proposing models which have states conditioned on whether another person is speaking.",True
elbanna22_interspeech,https://www.isca-archive.org/interspeech_2022/elbanna22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/elbanna22_interspeech.html,Hybrid Handcrafted and Learnable Audio Representation for Analysis of Speech Under Cognitive and Physical Load,Speech Representation II,2022,"As a neurophysiological response to threat or adverse conditions, stress can affect cognition, emotion and behaviour with potentially detrimental effects on health in the case of sustained exposure. Since the affective content of speech is inherently modulated by an individual's physical and mental state, a substantial body of research has been devoted to the study of paralinguistic correlates of stress-inducing task load. Historically, voice stress analysis (VSA) has been conducted using conventional digital signal processing (DSP) techniques. Despite the development of modern methods based on deep neural networks (DNNs), accurately detecting stress in speech remains difficult due to the wide variety of stressors and considerable variability in the individual stress perception. To that end, we introduce a set of five datasets for task load detection in speech. The voice recordings were collected as either cognitive or physical stress was induced in the cohort of volunteers, with a cumulative number of more than a hundred speakers. We used the datasets to design and evaluate a novel self-supervised audio representation that leverages the effectiveness of handcrafted features (DSP-based) and the complexity of data-driven DNN representations. Notably, the proposed approach outperformed both extensive handcrafted feature sets and novel DNN-based audio representation learning approaches.",True
zhu22_interspeech,https://www.isca-archive.org/interspeech_2022/zhu22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/zhu22_interspeech.html,ByT5 model for massively multilingual grapheme-to-phoneme conversion,"Speech Synthesis: Linguistic Processing, Paradigms and Other Topics II",2022,"In this study, we tackle massively multilingual grapheme-to-phoneme conversion through implementing G2P models based on ByT5. We have curated a G2P dataset from various sources that covers around 100 languages and trained large-scale multilingual G2P models based on ByT5. We found that ByT5 operating on byte-level inputs significantly outperformed the token-based mT5 model in terms of multilingual G2P. Pairwise comparison with monolingual models in these languages suggests that multilingual ByT5 models generally lower the phone error rate by jointly learning from a variety of languages. The pretrained model can further benefit low resource G2P through zero-shot prediction on unseen languages or provides pretrained weights for finetuning, which helps the model converge to a lower phone error rate than randomly initialized weights. To facilitate future research on multilingual G2P, we make available our code and pretrained multilingual G2P models at: https://github.com/lingjzhu/CharsiuG2P.",True
mathur22_interspeech,https://www.isca-archive.org/interspeech_2022/mathur22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/mathur22_interspeech.html,DocLayoutTTS: Dataset and Baselines for Layout-informed Document-level Neural Speech Synthesis,"Speech Synthesis: Linguistic Processing, Paradigms and Other Topics II",2022,"We propose a new task of synthesizing speech directly from semi-structured documents where the extracted text tokens from OCR systems may not be in the correct reading order due to the complex document layout. We refer to this task as layout-informed document-level TTS and present the DocSpeech dataset which consists of 10K audio clips of a single-speaker reading layout-enriched Word document. For each document, we provide the natural reading order of text tokens, their corresponding bounding boxes, and the audio clips synthesized in the correct reading order. We also introduce DocLayoutTTS, a Transformer encoder-decoder architecture that generates speech in an end-to-end manner given a document image with OCR extracted text. Our architecture simultaneously learns text reordering and mel-spectrogram prediction in a multi-task setup. Moreover, we take advantage of curriculum learning to progressively learn longer, more challenging document-level text utilizing both \\texttt{DocSpeech} and LJSpeech datasets. Our empirical results show that the underlying task is challenging. Our proposed architecture performs slightly better than competitive baseline TTS models with a pre-trained model providing reading order priors. We release samples of the DocSpeech dataset.",True
tran22_interspeech,https://www.isca-archive.org/interspeech_2022/tran22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/tran22_interspeech.html,An Efficient and High Fidelity Vietnamese Streaming End-to-End Speech Synthesis,"Speech Synthesis: Linguistic Processing, Paradigms and Other Topics II",2022,"In recent years, parallel end-to-end speech synthesis systems have outperformed the 2-stage TTS approaches in audio quality and latency. A parallel end-to-end speech like VITS can generate the audio with high MOS comparable to ground truth and achieve low latency on GPU. However, the VITS still has high latency when synthesizing long utterances on CPUs. Therefore, in this paper, we propose a streaming method for the parallel speech synthesis model like VITS to synthesize with the long texts effectively on CPU. Our system has achieved human-like speech quality in both the non-streaming and streaming mode on the in-house Vietnamese evaluation set, while the synthesis speed of our system is approximately four times faster than that of the VITS in the non-streaming mode. Furthermore, the customer perceived latency of our system in streaming mode is 25 times faster than the VITS on computer CPU. Our system in non-streaming mode achieves a MOS of 4.43 compared to ground-truth with MOS 4.56; it also has high-quality speech with a MOS of 4.35 in streaming mode. Finally, we release a Vietnamese single accent dataset used in our experiments.",True
tan22_interspeech,https://www.isca-archive.org/interspeech_2022/tan22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/tan22_interspeech.html,Environment Aware Text-to-Speech Synthesis,"Speech Synthesis: Linguistic Processing, Paradigms and Other Topics II",2022,This study aims at designing an environment-aware text-to-speech (TTS) system that can generate speech to suit specific acoustic environments. It is also motivated by the desire to leverage massive data of speech audio from heterogeneous sources in TTS system development. The key idea is to model the acoustic environment in speech audio as a factor of data variability and incorporate it as a condition in the process of neural network based speech synthesis. Two embedding extractors are trained with two purposely constructed datasets for characterization and disentanglement of speaker and environment factors in speech. A neural network model is trained to generate speech from extracted speaker and environment embeddings. Objective and subjective evaluation results demonstrate that the proposed TTS system is able to effectively disentangle speaker and environment factors and synthesize speech audio that carries designated speaker characteristics and environment attribute. Audio samples are available online for demonstration.,True
diener22_interspeech,https://www.isca-archive.org/interspeech_2022/diener22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/diener22_interspeech.html,INTERSPEECH 2022 Audio Deep Packet Loss Concealment Challenge,Audio Deep PLC (Packet Loss Concealment) Challenge,2022,"Audio Packet Loss Concealment (PLC) is the hiding of gaps in audio streams caused by data transmission failures in packet switched networks. This is a common problem, and of increasing importance as end-to-end VoIP telephony and teleconference systems become the default and ever more widely used form of communication in business as well as in personal usage. This paper presents the INTERSPEECH 2022 Audio Deep Packet Loss Concealment challenge. We first give an overview of the PLC problem, and introduce some classical approaches to PLC as well as recent work. We then present the open source dataset released as part of this challenge as well as the evaluation methods and metrics used to determine the winner. We also briefly introduce PLCMOS, a novel data-driven metric that can be used to quickly evaluate the performance PLC systems. Finally, we present the results of the INTERSPEECH 2022 Audio Deep PLC Challenge, and provide a summary of important takeaways.",True
nagamine22_interspeech,https://www.isca-archive.org/interspeech_2022/nagamine22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/nagamine22_interspeech.html,Acquisition of allophonic variation in second language speech: An acoustic and articulatory study of English laterals by Japanese speakers,Speech Production,2022,"Acquisition of positional allophonic variation is seen as the foundation of a successful L2 speech learning. However, previous research has mostly focused on the phonemic contrast between English /l/ and /r/, providing little evidence in the acquisition of positional allophones, such as those in English /l/. The current study investigates the acoustics and articulation of allophonic variations in English laterals produced by Japanese speakers, focusing on the effects of syllabic positions and flanking vowels. Acoustic and articulatory data were obtained from five Japanese speakers in a simultaneous audio and high-speed ultrasound tongue imaging recording set-up while they read sentences containing syllable-initial and -final tokens of English /l/ in four different vowel contexts. Acoustic analysis was conducted on 500 tokens using linear-mixed effects modelling and the articulatory data were analysed using generalised additive mixed modelling. Syllable position and vowel context had significant effects on acoustics, while midsagittal tongue shape was more influenced by vowel context, with fewer positional effects. The results demonstrate that differences in acoustics not always be mirrored exactly by midsagittal tongue shape, suggesting multidimensionality of articulation in second language speech.",True
breiner22_interspeech,https://www.isca-archive.org/interspeech_2022/breiner22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/breiner22_interspeech.html,UserLibri: A Dataset for ASR Personalization Using Only Text,Language Modeling and Lexical Modeling for ASR,2022,"Personalization of speech models on mobile devices (on-device personalization) is an active area of research, but more often than not, mobile devices have more text-only data than paired audio-text data. We explore training a personalized language model on text-only data, used during inference to improve speech recognition performance for that user. We experiment on a user-clustered LibriSpeech corpus, supplemented with personalized text-only data for each user from Project Gutenberg. We release this User-Specific LibriSpeech (UserLibri) dataset to aid future personalization research. LibriSpeech audio-transcript pairs are grouped into 55 users from the test-clean dataset and 52 users from test-other. We are able to lower the average word error rate per user across both sets in streaming and nonstreaming models, including an improvement of 2.5 for the harder set of test-other users when streaming.",True
fietkau22_interspeech,https://www.isca-archive.org/interspeech_2022/fietkau22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/fietkau22_interspeech.html,Relationship between the acoustic time intervals and tongue movements of German diphthongs,Speech Processing & Measurement,2022,"This study investigated the relationship between tongue movements during the production of German diphthongs and their acoustic time intervals. To this end, five subjects produced a set of logatomes that contained German primary, secondary, and peripheral diphthongs in the context of bilabial and labiodental consonants at three different speaking rates. During the utterances, tongue movements were measured by means of optical palatography (OPG), i.e. by optical distance sensing in the oral cavity, along with the acoustic speech signal. The analysis of the movement signals revealed that the diphthongs have s-shaped tongue trajectories that strongly resemble half cosine periods. In addition, acoustic and articulatory diphthong durations have a linear, but not proportional, relationship. Finally, the peak velocity and midpoint between the two targets of a diphthong are reached in the middle of both the acoustic and articulatory diphthong time intervals, regardless of the duration and type of diphthong. These results can help to model realistic tongue movements for diphthongs in articulatory speech synthesis.",True
dejong22_interspeech,https://www.isca-archive.org/interspeech_2022/dejong22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/dejong22_interspeech.html,Speech imitation skills predict automatic phonetic convergence: a GMM-UBM study on L2,Speech Processing & Measurement,2022,"Phonetic convergence is the observation that two interlocutors adapt their speech towards one another on an acoustic-phonetic level. It happens automatically and unconsciously, but people can also deliberately imitate others when asked to do so. Here, we investigate to what degree people converge to their interlocutor in a scripted dialogue when they are and when they are not explicitly requested to imitate their interlocutor. More specifically, we collected two separate data sets, where Italian- and French-native participants read English sentences aloud in alternating speaking turns. The results of both groups with different language backgrounds were compared against each other. We used a Gaussian mixture model â universal background model (GMM-UBM) to assess phonetic convergence on the sentence level. The GMM-UBM configuration was optimized to make the best distinction between speakers on validation data. We found that people start to converge to one another while interacting compared to the baseline and even more substantially when explicitly asked to do so. Results are robust across data sets. More importantly, the degree of implicit convergence people display is related to how good of an explicit imitator they are, supporting the claim that the two phenomena are based on the same neurocognitive process.",True
patterson22_interspeech,https://www.isca-archive.org/interspeech_2022/patterson22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/patterson22_interspeech.html,Distance-Based Sound Separation,Spatial Audio,2022,"We propose the novel task of distance-based sound separation, where sounds are separated based only on their distance from a single microphone. In the context of assisted listening devices, proximity provides a simple criterion for sound selection in noisy environments that would allow the user to focus on sounds relevant to a local conversation. We demonstrate the feasibility of this approach by training a neural network to separate near sounds from far sounds in single channel synthetic reverberant mixtures, relative to a threshold distance defining the boundary between near and far. With a single nearby speaker and four distant speakers, the model improves scale-invariant signal to noise ratio by 4.4 dB for near sounds and 6.8 dB for far sounds.",True
aroudi22_interspeech,https://www.isca-archive.org/interspeech_2022/aroudi22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/aroudi22_interspeech.html,TRUNet: Transformer-Recurrent-U Network for Multi-channel Reverberant Sound Source Separation,Spatial Audio,2022,"In recent years, many deep learning techniques for single-channel sound source separation have been proposed using recurrent, convolutional and transformer networks. When multiple microphones are available, spatial diversity between speakers and background noise in addition to spectro-temporal diversity can be exploited by using multi-channel filters for sound source separation. Aiming at end-to-end multi-channel source separation, in this paper we propose a transformer-recurrent-U network (TRUNet), which directly estimates multi-channel filters from multi-channel input spectra. TRUNet consists of a spatial processing network with an attention mechanism across microphone channels aiming at capturing the spatial diversity, and a spectro-temporal processing network aiming at capturing spectral and temporal diversities. In addition to multi-channel filters, we also consider estimating single-channel filters from multi-channel input spectra using TRUNet. We train the network on a large reverberant dataset using a proposed combined compressed mean-squared error loss function, which further improves the sound separation performance. We evaluate the network on a realistic and challenging reverberant dataset, generated from measured room impulse responses of an actual microphone array. The experimental results on realistic reverberant sound source separation show that the proposed TRUNet outperforms state-of-the-art single-channel and multi-channel source separation methods.",True
dao22_interspeech,https://www.isca-archive.org/interspeech_2022/dao22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/dao22_interspeech.html,From Disfluency Detection to Intent Detection and Slot Filling,Spoken Dialogue Systems and Multimodality,2022,"We present the first empirical study investigating the influence of disfluency detection on downstream tasks of intent detection and slot filling. We perform this study for Vietnamese---a low-resource language that has no previous study as well as no public dataset available for disfluency detection. First, we extend the fluent Vietnamese intent detection and slot filling dataset PhoATIS by manually adding contextual disfluencies and annotating them. Then, we conduct experiments using strong baselines for disfluency detection and joint intent detection and slot filling, which are based on pre-trained language models. We find that: (i) disfluencies produce negative effects on the performances of the downstream intent detection and slot filling tasks, and (ii) in the disfluency context, the pre-trained multilingual language model XLM-R helps produce better intent detection and slot filling performances than the pre-trained monolingual language model PhoBERT, and this is opposite to what generally found in the fluency context.",True
zhou22g_interspeech,https://www.isca-archive.org/interspeech_2022/zhou22g_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/zhou22g_interspeech.html,Audio-Visual Wake Word Spotting in MISP2021 Challenge: Dataset Release and Deep Analysis,Spoken Dialogue Systems and Multimodality,2022,"In this paper, we describe and release publicly the audio-visual wake word spotting (WWS) database in the MISP2021 Challenge, which covers a range of scenarios of audio and video data collected by near-, mid-, and far-field microphone arrays, and cameras, to create a shared and publicly available database for WWS. The database and the code are released, which will be a valuable addition to the community for promoting WWS research using multi-modality information in realistic and complex conditions. Moreover, we investigated the different data augmentation methods for single modalities on an end-to-end WWS network. A set of audio-visual fusion experiments and analysis were conducted to observe the assistance from visual information to acoustic information based on different audio and video field configurations. The results showed that the fusion system generally improves over the single-modality (audio- or video-only) system, especially under complex noisy conditions.",True
ivanko22_interspeech,https://www.isca-archive.org/interspeech_2022/ivanko22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/ivanko22_interspeech.html,DAVIS: Driverâs Audio-Visual Speech recognition,Show and Tell I(VR),2022,"DAVIS is a driverâs audio-visual assistive system intended toÂ  improve accuracy and robustness of speech recognition of theÂ  most frequent driversâ requests in natural driving conditions.Â  Since speech recognition in driving condition is highlyÂ  challenging due to acoustic noises, active head turns, poseÂ  variation, distance to recording devices, lightning conditions,Â  etc. We rely on multimodal information and use both automaticÂ  lip-reading system for visual stream and ASR for audio stream processing. We have trained audio and video models on ownÂ  RUSAVIC dataset containing in-the-wild audio and video recordings of 20 drivers. The recognition application comprisesÂ  a graphical user interface and modules for audio and videoÂ  signal acquisition, analysis, and recognition. The obtainedÂ  results demonstrate rather high performance of DAVIS and alsoÂ  the fundamental possibility of recognizing speech commandsÂ  by using video modality, even in such difficult naturalÂ  conditions as driving.Â",True
ogayo22_interspeech,https://www.isca-archive.org/interspeech_2022/ogayo22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/ogayo22_interspeech.html,Building African Voices,Inclusive and Fair Speech Technologies I,2022,"Modern speech synthesis techniques can produce natural-sounding speech given sufficient high-quality data and compute resources. However, such data is not readily available for many languages. This paper focuses on speech synthesis for low-resourced African languages, from corpus creation to sharing and deploying the Text-to-Speech (TTS) systems. We first create a set of general-purpose instructions on building speech synthesis systems with minimum technological resources and subject-matter expertise. Next, we create new datasets and curate datasets from ""found"" data (existing recordings) through a participatory approach while considering accessibility, quality, and breadth. We demonstrate that we can develop synthesizers that generate intelligible speech with 25 minutes of created speech, even when recorded in suboptimal environments. Finally, we release the speech data, code, and trained voices for 12 African languages to support researchers and developers.",True
langheinrich22_interspeech,https://www.isca-archive.org/interspeech_2022/langheinrich22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/langheinrich22_interspeech.html,Glottal inverse filtering based on articulatory synthesis and deep learning,Phonetics I,2022,"We propose a new method to estimate the glottal vocal tract excitation from speech signals based on deep learning. To that end, a bidirectional recurrent neural network with long short-term memory units was trained to predict the glottal airflow derivative from the speech signal. Since natural reference data for this task is unobtainable at the required scale, we used the articulatory speech synthesizer VocalTractLab to generate a large dataset containing synchronous connected speech and glottal airflow signals for training. The trained model's performance was objectively evaluated by means of stationary synthetic signals from the OPENGLOT glottal inverse filtering benchmark dataset and by using our dataset of connected synthetic speech. Compared to the state of the art, the proposed model produced a more accurate estimation using OPENGLOT's physically synthesized signals but was less accurate for its computationally simulated signals. However, our model was much more accurate and plausible on the connected speech signals, especially for sounds with mixed excitation (e.g. fricatives) or sounds with pronounced zeros in their transfer function (e.g. nasals). Future work will introduce more variety into the training data (e.g. regarding pitch and phonation) and focus on estimating features of the glottal flow instead of the entire waveform.",True
delvaux22_interspeech,https://www.isca-archive.org/interspeech_2022/delvaux22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/delvaux22_interspeech.html,Telling self-defining memories: An acoustic study of natural emotional speech productions,Phonetics I,2022,"Vocal cues in emotion encoding are rarely studied based on real-life, naturalistic emotional speech. In the present study, 20 speakers aged 25-35 were recorded while orally telling 5 successive self-defining autobiographic memories (SDM). By definition, this task is highly emotional, although emotional load and emotion regulation are expected to vary across SDM. Seven acoustic parameters were extracted: MeanF0, MedianFo, StandardDeviationF0, MinF0, MaxF0, Duration and SpeechRate. All SDM were manually transcribed, then their emotional lexicon was analysed using Emotaix. First, speech productions were examined in reference with SDM characteristics (specificity, integrative meaning and affective valence) as determined by 3 independent investigators. Results showed that overall the speech parameters did not change over the time course of the experiment, or as a function of integrative meaning. Specific memories were recounted at a higher speech rate and at greater length than non specific ones. SDM with positive affective valence were shorter and included less variability in fundamental frequency than negative SDM. Second, emotionally-charged (positive vs. negative; high vs. low arousal) vs. emotionally-neutral utterances as to Emotaix classification were compared over all SDM. Only a few significant effects were observed, which led us to discuss the role of emotion regulation in the SDM task.",True
spinu22_interspeech,https://www.isca-archive.org/interspeech_2022/spinu22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/spinu22_interspeech.html,Voicing neutralization in Romanian fricatives across different speech styles,Phonetics I,2022,"Romance languages such as Italian or Spanish preserve fricative voicing contrasts in word-final position, while their neutralization has been reported for European Portuguese, but the behavior of Romanian fricatives remains understudied. Previous work with Romanian fricatives suggests a pattern of final devoicing but, due to the specific properties of the corpus analyzed, it is unclear if this is limited to the presence of secondary palatalization and/or the result of morphological conditioning. In this study, we apply speech processing tools to investigate the acoustic characteristics of the voicing contrast in fricatives in contemporary spoken Romanian. We examine a corpus of prepared speech from newscasts and semi-spontaneous TV debates (86 speakers) and compare our results to previous findings from a corpus of controlled experimental speech (31 speakers). Our classification tool employs cepstral coefficients and hidden Markov model (HMM)-defined temporal regions to identify the properties of these segments. Our findings conform to typological predictions regarding partial devoicing in coda position, especially at more posterior places, but we find little support for voicing neutralization in Romanian fricatives more generally. Our study thus documents the properties of Romanian fricatives and contributes to our understanding of the dynamics of contrast maintenance in phonological systems.",True
mussakhojayeva22_interspeech,https://www.isca-archive.org/interspeech_2022/mussakhojayeva22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/mussakhojayeva22_interspeech.html,KSC2: An Industrial-Scale Open-Source Kazakh Speech Corpus,"Multi-, Cross-lingual and Other Topics in ASR I",2022,"We present the first industrial-scale open-source Kazakh speech corpus for automatic speech recognition research and development. Our corpus subsumes two previously presented corpora: 1) Kazakh speech corpus (KSC) and 2) Kazakh text-to-speech 2 (KazakhTTS2). We also provide additional data from other sources, including television news, television and radio programs, parliament speeches, and podcasts. Our corpus, which we have named KSC2, contains over a thousand hours of high-quality transcribed data, which is triple the size of KSC. KSC2 was manually transcribed with the help of native Kazakh speakers and validated via preliminary speech recognition experiments on various evaluation sets. Moreover, it contains utterances with Kazakh-Russian code-switching, a conversational practice common among Kazakh speakers. We believe that our corpus will facilitate speech processing research for Kazakh, which is widely considered an under-resourced language. To ensure the reproducibility of experiments, we share the KSC2 corpus, training recipes, and pretrained models.",True
flechl22_interspeech,https://www.isca-archive.org/interspeech_2022/flechl22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/flechl22_interspeech.html,End-to-end speech recognition modeling from de-identified data,"Multi-, Cross-lingual and Other Topics in ASR I",2022,"De-identification of data used for automatic speech recognition modeling is a critical component in protecting privacy, especially in the medical domain. However, simply removing all personally identifiable information (PII) from end-to-end model training data leads to a significant performance degradation in particular for the recognition of names, dates, locations, and words from similar categories. We propose and evaluate a two-step method for partially recovering this loss. First, PII is identified, and each occurrence is replaced with a random word sequence of the same category. Then, corresponding audio is produced via text-to-speech or by splicing together matching audio fragments extracted from the corpus. These artificial audio/label pairs, together with speaker turns from the original data without PII, are used to train models. We evaluate the performance of this method on in-house data of medical conversations and observe a recovery of almost the entire performance degradation in the general word error rate while still maintaining a strong diarization performance. Our main focus is the improvement of recall and precision in the recognition of PII-related words. Depending on the PII category, between 50% - 90% of the performance degradation can be recovered using our proposed method.",True
qin22_interspeech,https://www.isca-archive.org/interspeech_2022/qin22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/qin22_interspeech.html,Cross-Age Speaker Verification: Learning Age-Invariant Speaker Embeddings,Speaker Embedding and Diarization,2022,"Automatic speaker verification has achieved remarkable progress in recent years. However, there is little research on cross-age speaker verification due to insufficient data. In this paper, we mine cross-age test sets based on the VoxCeleb and propose our age-invariant speaker representation learning method. Since the VoxCeleb is collected from the YouTube platform, the dataset consists of cross-age data inherently. However, the meta-data does not contain the speaker age label. Therefore, we adopt the face age estimation method to predict the speaker age value from the associated visual data, then label the audio recording with the estimated age. We construct multiple Cross-Age test sets on VoxCeleb (Vox-CA), which deliberately select the positive trials with large age-gap. Also, the effect of nationality and gender is considered in selecting negative pairs to align with Vox-H cases. The baseline system performance drops from 1.939% EER on the Vox-H test set to 10.419\\% on the Vox-CA20 test set, which indicates how difficult the cross-age scenario is. Consequently, we propose an age-decoupling adversarial learning (ADAL) method to alleviate the negative effect of the age gap and reduce intra-class variance. Our method outperforms the baseline system by over 10% related EER reduction on the Vox-CA20 test set.",True
liu22t_interspeech,https://www.isca-archive.org/interspeech_2022/liu22t_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/liu22t_interspeech.html,MSDWild: Multi-modal Speaker Diarization Dataset in the Wild,Speaker Embedding and Diarization,2022,"Speaker diarization in real-world acoustic environments is a challenging task of increasing interest from both academia and industry. Although it has been widely accepted that incorporating visual information benefits audio processing tasks such as speech recognition, there is currently no fully released dataset that can be used for benchmarking multi-modal speaker diarization performance in real-world environments. In this paper, we release MSDWild, a benchmark dataset for multi-modal speaker diarization in the wild. The dataset is collected from public videos, covering rich real-world scenarios and languages. All video clips are naturally shot videos without over-editing such as lens switching. Audio and video are both released. In particular, MSDWild has a large portion of the naturally overlapped speech, forming an excellent testbed for cocktail-party problem research. Furthermore, we also conduct baseline experiments on the dataset using audio-only, visual-only, and audio-visual speaker diarization.",True
liu22d_interspeech,https://www.isca-archive.org/interspeech_2022/liu22d_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/liu22d_interspeech.html,An End-to-End Macaque Voiceprint Verification Method Based on Channel Fusion Mechanism,Acoustic Event Detection and Classification,2022,"Primates are facing a serious survival crisis. Tracking the range of animal activities and population changes is of great significance for efficient animal protection. Primates are highly alert and inaccessible to humans so that it is difficult to track animals through direct observation, DNA fingerprinting, or marking methods. Primate recognition based on animal calls has the advantages of wide monitoring range, low equipment cost, and good concealment. In this work, we propose an effective macaque speech feature extraction structure, and innovatively propose a feature fusion mechanism to effectively obtain the feature representation of each call. Furthermore, we construct a public open source macaque voiceprint verification dataset. The experimental results show that the proposed method is superior to the existing state-of-the-art human voiceprint verification algorithms with different call durations. The equal error rate (EER) of our macaque voiceprint verification algorithm reaches 6.19%.",True
xu22c_interspeech,https://www.isca-archive.org/interspeech_2022/xu22c_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/xu22c_interspeech.html,Human Sound Classification based on Feature Fusion Method with Air and Bone Conducted Signal,Acoustic Event Detection and Classification,2022,"The human sound classification task aims at distinguishing different sounds made by human, which can be widely used in medical and health detection area. Different from other sounds in acoustic scene classification task, human sounds can be transmitted either through air or bone conduction. The bone conducted (BC) signal generated by a speaker has strong anti-noise properties and can assist the air conducted (AC) signal to extract additional acoustic features. In this paper, we explore the effect of the BC signal on human sound classification task. Two stream audios combing BC and AC signals are input to a CNN-based model. An attentional feature fusion method suitable for BC and AC signal features is proposed to improve the performance according to the complementarity between the two signal features. Further improvement can be obtained by using a BC signal feature enhancement method. Experiments on an open access and a self-built dataset show that fusing bone conducted signal can achieve 6.2%/17.4% performance improvement over the baseline with only AC signal as input. The results demonstrate the application value of bone conducted signals and the superior performance of the proposed methods.",True
yang22e_interspeech,https://www.isca-archive.org/interspeech_2022/yang22e_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/yang22e_interspeech.html,RaDur: A Reference-aware and Duration-robust Network for Target Sound Detection,Acoustic Event Detection and Classification,2022,"Target sound detection (TSD) aims to detect the target sound from a mixture audio given the reference information. Previous methods use a conditional network to extract a sound-discriminative embedding from the reference audio, and then use it to detect the target sound from the mixture audio. However, the network performs much differently when using different reference audios (\\text{e.g.} performs poorly for noisy and short-duration reference audios), and tends to make wrong decisions for transient events (\\text{i.e.} shorter than $1$ second). To overcome these problems, in this paper, we present a reference-aware and duration-robust network (RaDur) for TSD. More specifically, in order to make the network more aware of the reference information, we propose an embedding enhancement module to take into account the mixture audio while generating the embedding, and apply the attention pooling to enhance the features of target sound-related frames and weaken the features of noisy frames. In addition, a duration-robust focal loss is proposed to help model different-duration events. To evaluate our method, we build two TSD datasets based on UrbanSound and Audioset. Extensive experiments show the effectiveness of our methods.",True
wang22aa_interspeech,https://www.isca-archive.org/interspeech_2022/wang22aa_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/wang22aa_interspeech.html,Active Few-Shot Learning for Sound Event Detection,Acoustic Event Detection and Classification,2022,"Few-shot learning has shown promising results in sound event detection where the model can learn to recognize novel classes assuming a few labeled examples (typically five) are available at inference time. Most research studies simulate this process by sampling support examples randomly and uniformly from all test data with the target class label. However, in many real-world scenarios, users might not even have five examples at hand or these examples may be from a limited context and not representative, resulting in model performance lower than expected. In this work, we relax these assumptions, and to recover model performance, we propose to use active learning techniques to efficiently sample additional informative support examples at inference time. We developed a novel dataset simulating the long-term temporal characteristics of sound events in real-world environmental soundscapes. Then we ran a series of experiments with this dataset to explore the modeling and sampling choices that arise when combining few-shot learning and active learning, including different training schemes, sampling strategies, models, and temporal windows in sampling.",True
nguyen22_interspeech,https://www.isca-archive.org/interspeech_2022/nguyen22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/nguyen22_interspeech.html,A High-Quality and Large-Scale Dataset for English-Vietnamese Speech Translation,Spoken Language Processing II,2022,"In this paper, we introduce a high-quality and large-scale benchmark dataset for English-Vietnamese speech translation with 508 audio hours, consisting of 331K triplets of (sentence-lengthed audio, English source transcript sentence, Vietnamese target subtitle sentence). We also conduct empirical experiments using strong baselines and find that the traditional ""Cascaded"" approach still outperforms the modern ""End-to-End"" approach. To the best of our knowledge, this is the first large-scale English-Vietnamese speech translation study. We hope both our publicly available dataset and study can serve as a starting point for future research and applications on English-Vietnamese speech translation.",True
yang22h_interspeech,https://www.isca-archive.org/interspeech_2022/yang22h_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/yang22h_interspeech.html,Open Source MagicData-RAMC: A Rich Annotated Mandarin Conversational(RAMC) Speech Dataset,Spoken Language Processing II,2022,"This paper introduces a high-quality rich annotated Mandarin conversational (RAMC) speech dataset called MagicData-RAMC. The MagicData-RAMC corpus contains 180 hours of conversational speech data recorded from native speakers of Mandarin Chinese over mobile phones with a sampling rate of 16 kHz. The dialogs in MagicData-RAMC are classified into 15 diversified domains and tagged with topic labels, ranging from science and technology to ordinary life. Accurate transcription and precise speaker voice activity timestamps are manually labeled for each sample. Speakers' detailed information is also provided. As a Mandarin speech dataset designed for dialog scenarios with high quality and rich annotations, MagicData-RAMC enriches the data diversity in the Mandarin speech community and allows extensive research on a series of speech-related tasks, including automatic speech recognition, speaker diarization, topic detection, keyword search, text-to-speech, etc. We also conduct several relevant tasks and provide experimental results to help evaluate the dataset.",True
li22j_interspeech,https://www.isca-archive.org/interspeech_2022/li22j_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/li22j_interspeech.html,TALCS: An open-source Mandarin-English code-switching corpus and a speech recognition baseline,Spoken Language Processing II,2022,"This paper introduces a new corpus of Mandarin-English code-switching speech recognitionâTALCS corpus, suitable for training and evaluating code-switching speech recognition systems. TALCS corpus is derived from real online one-to-one English teaching scenes in TAL education group, which contains roughly 587 hours of speech sampled at 16 kHz. To our best knowledge, TALCS corpus is the largest well labeled Mandarin-English code-switching open source automatic speech recognition (ASR) dataset in the world. In this paper, we will introduce the recording procedure in detail, including audio capturing devices and corpus environments. And the TALCS corpus is freely available for download under the permissive license[ https://ai.100tal.com/dataset]. Using TALCS corpus, we conduct ASR experiments in two popular speech recognition toolkits to make a baseline system, including ESPnet and Wenet. The Mixture Error Rate (MER) performance in the two speech recognition toolkits is compared in TALCS corpus. The experimental results implies that the quality of audio recordings and transcriptions are promising and the baseline system is workable.",True
markitantov22_interspeech,https://www.isca-archive.org/interspeech_2022/markitantov22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/markitantov22_interspeech.html,Biometric Russian Audio-Visual Extended MASKS (BRAVE-MASKS) Corpus: Multimodal Mask Type Recognition Task,Spoken Language Processing II,2022,"In this paper, we present a new multimodal corpus called Biometric Russian Audio-Visual Extended MASKS (BRAVE-MASKS), which is designed to analyze voice and facial characteristics of persons wearing various masks, as well as to develop automatic systems for bimodal verification and identification of speakers. In particular, we tackle the multimodal mask type recognition task (6 classes). As a result, audio, visual and multimodal systems were developed, which showed UAR of 54.83%, 72.02% and 82.01%, respectively, on the Test set. These performances are the baseline for the BRAVE-MASKS corpus to compare the follow-up approaches with the proposed systems.",True
chen22o_interspeech,https://www.isca-archive.org/interspeech_2022/chen22o_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/chen22o_interspeech.html,Audio-Visual Speech Recognition in MISP2021 Challenge: Dataset Release and Deep Analysis,Spoken Language Processing II,2022,"In this paper, we present the updated Audio-Visual Speech Recognition (AVSR) corpus of MISP2021 challenge, a large-scale audio-visual Chinese conversational corpus consisting of 141h audio and video data collected by far/middle/near microphones and far/middle cameras in 34 real-home TV rooms. To our best knowledge, our corpus is the first distant multi-microphone conversational Chinese audio-visual corpus and the first large vocabulary continuous Chinese lip-reading dataset in the adverse home-tv scenario. Moreover, we make a deep analysis of the corpus and conduct a comprehensive ablation study of all audio and video data in the audio-only/video-only/audio-visual systems. Error analysis shows video modality supplement acoustic information degraded by noise to reduce deletion errors and provide discriminative information in overlapping speech to reduce substitution errors. Finally, we also design a set of experiments such as frontend, data augmentation and end-to-end models for providing the direction of potential future work. The corpus and the code are released to promote the research not only in speech area but also for the computer vision area and cross-disciplinary research.",True
lehecka22_interspeech,https://www.isca-archive.org/interspeech_2022/lehecka22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/lehecka22_interspeech.html,Exploring Capabilities of Monolingual Audio Transformers using Large Datasets in Automatic Speech Recognition of Czech,ASR Technologies and Systems,2022,"In this paper, we present our progress in pretraining Czech monolingual audio transformers from a large dataset containing more than 80 thousand hours of unlabeled speech, and subsequently fine-tuning the model on automatic speech recognition tasks using a combination of in-domain data and almost 6 thousand hours of out-of-domain transcribed speech. We are presenting a large palette of experiments with various fine-tuning setups evaluated on two public datasets (CommonVoice and VoxPopuli) and one extremely challenging dataset from the MALACH project. Our results show that monolingual Wav2Vec 2.0 models are robust ASR systems, which can take advantage of large labeled and unlabeled datasets and successfully compete with state-of-the-art LVCSR systems. Moreover, Wav2Vec models proved to be good zero-shot learners when no training data are available for the target ASR task.",True
perezramon22_interspeech,https://www.isca-archive.org/interspeech_2022/perezramon22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/perezramon22_interspeech.html,Generating iso-accented stimuli for second language research: methodology and a dataset for Spanish-accented English,Speech Perception,2022,"A non-native accent can be conveyed at both the segmental and suprasegmental level. Previous studies have developed techniques to isolate the effect of segmental foreign accent by splicing accented segments from a bilingual speaker into non-accented words produced by the same speaker. The current work addresses the issue of between-segment variability by developing a technique to convert from acoustically-equal accent gradations to perceptually-equal steps. The procedure is used to derive the first corpus of Spanish-accented English composed of lexical tokens each generated with one of five degrees of non-native accent. As an example application, corpus tokens are used to elicit accentedness judgements from four listener cohorts with first languages which differ as to whether they share the native language, the non-native (accented) language of the corpus or have a closer phonological inventory to one or the other. Findings highlight the importance of the relationship between listeners' phonological systems and those of the native and non-native languages of the corpus, especially for vowels, with respect to sensitivity to foreign accent.",True
leemann22_interspeech,https://www.isca-archive.org/interspeech_2022/leemann22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/leemann22_interspeech.html,Factors affecting the percept of Yanny v. Laurel (or mixed): Insights from a large-scale study on Swiss German listeners,Speech Perception,2022,"In May 2018, Yanny v. Laurel went viral: when listening to the same audio clip, some people claimed to hear only Yanny, others insisted it must be Laurel, and some had a mixed percept. Phoneticians have identified the acoustic features which caused this perceptual ambiguity, but we still know little about the factors affecting individuals' perception of the illusion. We conducted a controlled study with 974 Swiss German listeners, balanced for age, gender, and regional origin. Overall, nearly two thirds heard Yanny, one quarter Laurel, and about 12% had a mixed percept. We found age, gender, and electronic device to play a significant role: younger, female, and laptop-using participants demonstrated higher proportions of Yanny responses. These findings contribute to the growing body of research on polyperceivable words.",True
shin22_interspeech,https://www.isca-archive.org/interspeech_2022/shin22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/shin22_interspeech.html,Learning Audio-Text Agreement for Open-vocabulary Keyword Spotting,Spoken Term Detection and Voice Search,2022,"In this paper, we propose a novel end-to-end user-defined keyword spotting method that utilizes linguistically corresponding patterns between speech and text sequences. Unlike previous approaches requiring speech keyword enrollment, our method compares input queries with an enrolled text keyword sequence. To place the audio and text representations within a common latent space, we adopt an attention-based cross-modal matching approach that is trained in an end-to-end manner with monotonic matching loss and keyword classification loss. We also utilize a de-noising loss for the acoustic embedding network to improve robustness in noisy environments. Additionally, we introduce the LibriPhrase dataset, a new short-phrase dataset based on LibriSpeech for efficiently training keyword spotting models. Our proposed method achieves competitive results on various evaluation sets compared to other single-modal and cross-modal baselines.",True
fara22_interspeech,https://www.isca-archive.org/interspeech_2022/fara22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/fara22_interspeech.html,Speech and the n-Back task as a lens into depression. How combining both may allow us to isolate different core symptoms of depression,Speech and Language in Health: From Remote Monitoring to Medical Conversations I,2022,"Embedded in any speech signal is a rich combination of cognitive, neuromuscular and physiological information. This richness makes speech a powerful signal in relation to a range of different health conditions, including major depressive disorders (MDD). One pivotal issue in speech-depression research is the assumption that depressive severity is the dominant measurable effect. However, given the heterogeneous clinical profile of MDD, it may actually be the case that speech alterations are more strongly associated with subsets of key depression symptoms. This paper presents strong evidence in support of this argument. First, we present a novel large, cross-sectional, multi-modal dataset collected at Thymia. We then present a set of machine learning experiments that demonstrate that combining speech with features from an n-Back working memory assessment improves classifier performance when predicting the popular eight-item Patient Health Questionnaire depression scale (PHQ-8). Finally, we present a set of experiments that highlight the association between different speech and n-Back markers at the PHQ-8 item level. Specifically, we observe that somatic and psychomotor symptoms are more strongly associated with n-Back performance scores, whilst the other items: anhedonia, depressed mood, change in appetite, feelings of worthlessness and trouble concentrating are more strongly associated with speech changes.",True
xu22_interspeech,https://www.isca-archive.org/interspeech_2022/xu22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/xu22_interspeech.html,Differential Time-frequency Log-mel Spectrogram Features for Vision Transformer Based Infant Cry Recognition,Multimodal Speech Emotion Recognition and Paralinguistics,2022,"Crying is the main way for babies to communicate with the outside world. Analyzing cry enables not only the identification of babies' needs/thoughts they want to express, but also the prediction of potential diseases. In general, it is much more difficult to recognize special needs and emotions from infant cry than adults, because infant cry does not contain any linguistic information and the emotional expression is not as rich as adults.In this work, we focus on the time-frequency characteristics of infant crying signals and propose a differential time-frequency log-Mel spectrogram features based vision transformer (ViT) approach for infant cry recognition (ICR). We first calculate the deltas of log-Mel spectrogram of infant crying sounds over time frames and frequencies, respectively. The log-Mels and deltas are then combined as a 3-D feature representation and fed into the ViT model for cry classification. Experimental results on the CRIED database show the superiority of the proposed system over comparison methods and that the combination of logMels, the time-frame delta and frequency-bin delta achieves the best performance. The proposed method is further validated on a self-recorded dataset.",True
baird22_interspeech,https://www.isca-archive.org/interspeech_2022/baird22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/baird22_interspeech.html,State & Trait Measurement from Nonverbal Vocalizations: A Multi-Task Joint Learning Approach,Multimodal Speech Emotion Recognition and Paralinguistics,2022,"Humans infer a wide array of meanings from expressive nonverbal vocalizations, \\eg laughs, cries, and sighs. Thus far, computational research has primarily focused on the coarse classification of vocalizations such as laughs, but that approach overlooks significant variations in the meaning of distinct laughs (e.g., amusement, awkwardness, triumph) and the rich array of more nuanced vocalizations people form. Nonverbal vocalizations are shaped by the emotional state an individual chooses to convey, their wellbeing, and (as with the voice more broadly) their identity-related traits. In the present study, we utilize a large-scale dataset comprising more than 35 hours of densely labeled vocal bursts to model emotionally expressive states and demographic traits from nonverbal vocalizations. We compare a single-task and multi-task deep learning architecture to explore how models can leverage acoustic co-dependencies that may exist between the expression of 10 emotions by vocal bursts and the demographic traits of the speaker. Results show that nonverbal vocalizations can be reliably leveraged to predict emotional expression, age, and country of origin. In a multi-task setting, our experiments show that joint learning of emotional expression and demographic traits appears to yield robust results, primarily beneficial for the classification of a speaker's country of origin.",True
liu22q_interspeech,https://www.isca-archive.org/interspeech_2022/liu22q_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/liu22q_interspeech.html,Automated Voice Pathology Discrimination from Continuous Speech Benefits from Analysis by Phonetic Context,Atypical Speech Analysis and Detection,2022,"In contrast to previous studies that look only at discriminating pathological voice from the normal voice, in this study we focus on the discrimination between cases of spasmodic dysphonia (SD) and vocal fold palsy (VP) using automated analysis of speech recordings. The hypothesis is that discrimination will be enhanced by studying continuous speech, since the different pathologies are likely to have different effects in different phonetic contexts. We collected audio recordings of isolated vowels and of a read passage from 60 patients diagnosed with SD (N=38) or VP (N=22). Baseline classifiers on features extracted from the recordings taken as a whole gave a cross-validated unweighted average recall of up to 75% for discriminating the two pathologies. We used an automated method to divide the read passage into phone-labelled regions and built classifiers for each phone. Results show that the discriminability of the pathologies varied with phonetic context as predicted. Since different phone contexts provide different information about the pathologies, classification is improved by fusing phone predictions, to achieve a classification accuracy of 83%. The work has implications for the differential diagnosis of voice pathologies and contributes to a better understanding of their impact on speech.",True
talkar22_interspeech,https://www.isca-archive.org/interspeech_2022/talkar22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/talkar22_interspeech.html,Speech Acoustics in Mild Cognitive Impairment and Parkinson's Disease With and Without Concurrent Drawing Tasks,Pathological Speech Analysis,2022,"Parkinson's disease (PD) is characterized by motor dysfunction; however, non-motor symptoms such as cognitive decline also have a dramatic impact on quality of life. Current assessments to diagnose cognitive impairment take many hours and require high clinician involvement. Thus, there is a need to develop new tools leading to quick and accurate determination of cognitive impairment to allow for appropriate, timely interventions. In this paper, individuals with PD, designated as either having no cognitive impairment (NCI) or mild cognitive impairment (MCI), undergo a speech-based protocol, involving reading or listing items within a category, performed either with or without a concurrent drawing task. From the speech recordings, we extract motor coordination-based features, derived from correlations across acoustic features representative of speech production subsystems. The correlation-based features are utilized in gaussian mixture models to discriminate between individuals designated NCI or MCI in both the single and dual task paradigms. Features derived from the laryngeal and respiratory subsystems, in particular, discriminate between these two groups with AUCs > 0.80. These results suggest that cognitive impairment can be detected using speech from both single and dual task paradigms, and that cognitive impairment may manifest as differences in vocal fold vibration stability.",True
kawano22_interspeech,https://www.isca-archive.org/interspeech_2022/kawano22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/kawano22_interspeech.html,Multimodal Persuasive Dialogue Corpus using Teleoperated Android,Speaking Styles and Interaction Styles I,2022,"We collected a corpus of persuasive dialogues containing multimodal information for building a persuasive dialogue system that encourages users to change their behaviors using multimodal information. The corpus is constructed with an android robot that was remotely controlled by the WoZ method during user interactions with the system. We transcribed the collected speech and annotated dialogue act labels. We also extracted the facial features of the dialogue participants. Pre- and post-questionnaires identified the subjects' personality, their awareness of the target domain of persuasion, the changes in their awareness before/after the persuasion, and whether they agreed to the persuasion during the dialogues. In addition, we conducted a follow-up survey with each subject to investigate whether the persuasion actually affected their behavioral change. Moreover, we built linear classifiers that predict persuasion success to investigate effective features.",True
mitsui22_interspeech,https://www.isca-archive.org/interspeech_2022/mitsui22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/mitsui22_interspeech.html,End-to-End Text-to-Speech Based on Latent Representation of Speaking Styles Using Spontaneous Dialogue,Speaking Styles and Interaction Styles II,2022,"The recent text-to-speech (TTS) has achieved quality comparable to that of humans; however, its application in spoken dialogue has not been widely studied. This study aims to realize a TTS that closely resembles human dialogue. First, we record and transcribe actual spontaneous dialogues. Then, the proposed dialogue TTS is trained in two stages: first stage, variational autoencoder (VAE)-VITS or Gaussian mixture variational autoencoder (GMVAE)-VITS is trained, which introduces an utterance-level latent variable into variational inference with adversarial learning for end-to-end text-to-speech (VITS), a recently proposed end-to-end TTS model. A style encoder that extracts a latent speaking style representation from speech is trained jointly with TTS. In the second stage, a style predictor is trained to predict the speaking style to be synthesized from dialogue history. During inference, by passing the speaking style representation predicted by the style predictor to VAE/GMVAE-VITS, speech can be synthesized in a style appropriate to the context of the dialogue. Subjective evaluation results demonstrate that the proposed method outperforms the original VITS in terms of dialogue-level naturalness.",True
takamichi22_interspeech,https://www.isca-archive.org/interspeech_2022/takamichi22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/takamichi22_interspeech.html,J-MAC: Japanese multi-speaker audiobook corpus for speech synthesis,"Speech Synthesis: Tools, Data, and Evaluation",2022,"In this paper, we construct a Japanese audiobook speech corpus called ``J-MAC'' for speech synthesis research. With the success of reading-style speech synthesis, the research target is shifting to tasks that use complicated contexts. Audiobook speech synthesis is a good example that requires cross-sentence, expressiveness, etc. Unlike reading-style speech, speaker-specific expressiveness in audiobook speech also becomes the context. To enhance this research, we propose a method of constructing a corpus from audiobooks read by professional speakers. From many audiobooks and their texts, our method can automatically extract and refine the data without any language dependency. Specifically, we use vocal-instrumental separation to extract clean data, connectionist temporal classification to roughly align text and audio, and voice activity detection to refine the alignment. J-MAC is open-sourced in our project page. We also conduct audiobook speech synthesis evaluations, and the results give insights into audiobook speech synthesis.",True
webber22_interspeech,https://www.isca-archive.org/interspeech_2022/webber22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/webber22_interspeech.html,REYD â The First Yiddish Text-to-Speech Dataset and System,"Speech Synthesis: Tools, Data, and Evaluation",2022,"Modern text-to-speech (TTS) systems generate high-quality natural-sounding speech, but they only support a limited number of languages. Building data-hungry systems that require large amounts of accurately paired speech and text is challenging for languages with limited resources. Yiddish is a minority language that lacks many of the computational resources available in more widely-spoken languages. No modern TTS system exists for Yiddish. We introduce the Reading Electronic Yiddish Documents or REYD (Yiddish for 'speech') project. Found data is used to create a high-quality, hand-corrected TTS dataset. This dataset is used to train FastSpeech2, a state-of-the-art TTS system. A formal evaluation by expert and non-expert listeners found that the system produced speech that was both intelligible and natural-sounding. The results of this evaluation were used to further improve the dataset. The final hand-corrected dataset, code for creating a TTS system, trained models and other Yiddish text processing tools used in our work are publicly released. We hope the availability of these resources will enable new speech technology projects that better serve the needs of Yiddish-speaking communities.",True
pandey22b_interspeech,https://www.isca-archive.org/interspeech_2022/pandey22b_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/pandey22b_interspeech.html,Production characteristics of obstruents in WaveNet and older TTS systems,"Speech Synthesis: Tools, Data, and Evaluation",2022,"Segmental properties of Text-to-Speech (TTS) synthesizers have been studied for their influence on various perceived attributes of synthesized speech. However, they have received very limited attention for modern, neural vocoder-based TTS. In this paper, we compare segmental properties of WaveNET vocoder voices with a natural voice, and the best-performing non-neural synthesizers of the 2013 Blizzard Challenge. We extended the 2013 dataset with two new voices generated using a WaveNET vocoder. Acoustic-phonetic features of obstruent consonants and their neighbouring vowels were compared between the natural voice and each of these TTS systems. Statistical analysis was conducted using the Kruskal-Wallis test, and Dunn's test. Compared to the reference natural voice, we find that the WaveNET vocoder performs very well in modelling vowels, but features like F0 at onset and spectral tilt show significant deviations from the natural voice. Among consonants, neural voices deviate most from natural in the context of voiceless fricatives. Compared to other TTS systems, several features (like vowel dispersions, and consonant duration) which had shown strong deviations from natural, were found to not differ from natural in the WaveNET vocoder systems.",True
lemaguer22_interspeech,https://www.isca-archive.org/interspeech_2022/lemaguer22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/lemaguer22_interspeech.html,Back to the Future: Extending the Blizzard Challenge 2013,"Speech Synthesis: Tools, Data, and Evaluation",2022,"Nowadays, speech synthesis technology is synonymous with the use of Deep Learning. To understand more about how synthesis systems have progressed with the advent of Deep Learning requires open-sourced speech resources that connect past and present technologies. This would allow direct comparisons. This paper presents such a resource by extending the 2013 edition of the Blizzard Challenge. Using this extension, we compare top-tier systems from the past to modern technologies in a controlled setting. From this edition, we selected the best representative of each historical synthesis technology, to which we added four systems representing combinations of modern acoustic models and neural vocoders. A large scale subjective evaluation was conducted to evaluate naturalness. Our results show that, as expected, modern technologies generate more natural synthetic speech. However, these systems are still not perceived to be as natural as the human voice. Crucially, we also observed that the Mean Opinion Score (MOS) of the historical systems dropped a full MOS point from their scores in the original edition. This demonstrates the relative nature of MOS: it should generally not be reported as an absolute value despite its origin as an absolute category rating.",True
meyer22c_interspeech,https://www.isca-archive.org/interspeech_2022/meyer22c_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/meyer22c_interspeech.html,"BibleTTS: a large, high-fidelity, multilingual, and uniquely African speech corpus","Speech Synthesis: Tools, Data, and Evaluation",2022,"BibleTTS is a large, high-quality, open speech dataset for ten languages spoken in Sub-Saharan Africa. The corpus contains up to 86 hours of aligned, studio quality 48kHz single speaker recordings per language, enabling the development of high-quality text-to-speech models. The ten languages represented are: Akuapem Twi, Asante Twi, Chichewa, Ewe, Hausa, Kikuyu, Lingala, Luganda, Luo, and Yoruba. This corpus is a derivative work of Bible recordings made and released by the Open.Bible project from Biblica. We have aligned, cleaned, and filtered the original recordings, and additionally hand-checked a subset of the alignments for each language. We present results for text-to-speech models with Coqui TTS. The data is released under a commercial-friendly CC-BY-SA license.",True
maniati22_interspeech,https://www.isca-archive.org/interspeech_2022/maniati22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/maniati22_interspeech.html,SOMOS: The Samsung Open MOS Dataset for the Evaluation of Neural Text-to-Speech Synthesis,"Speech Synthesis: Tools, Data, and Evaluation",2022,"In this work, we present the SOMOS dataset, the first large-scale mean opinion scores (MOS) dataset consisting of solely neural text-to-speech (TTS) samples. It can be employed to train automatic MOS prediction systems focused on the assessment of modern synthesizers, and can stimulate advancements in acoustic model evaluation. It consists of 20K synthetic utterances of the LJ Speech voice, a public domain speech dataset which is a common benchmark for building neural acoustic models and vocoders. Utterances are generated from 200 different TTS systems including a variety of vanilla neural acoustic models as well as models which allow prosodic variations. An LPCNet vocoder is used for all systems, so that the variations in the final samples depend only on the acoustic models. The synthesized utterances provide a balanced and adequate domain, length and phoneme coverage. MOS naturalness evaluations are collected via crowdsourcing on Amazon Mechanical Turk. We present in detail the design of the SOMOS dataset, as well as provide baseline results by training and evaluating state-of-the-art MOS prediction models, while we show the problems that these models face when assigned to evaluate TTS samples.",True
rajan22_interspeech,https://www.isca-archive.org/interspeech_2022/rajan22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/rajan22_interspeech.html,Oktoechos Classification in Liturgical Music Using SBU-LSTM/GRU,Acoustic Signal Representation and Analysis II,2022,"A distinguishing feature of the music repertoire of the Syrian tradition is the system of classifying melodies into eight tunes, called 'oktoe\\={c}hos'. It inspired many traditions, such as Greek and Indian liturgical music. In oktoe\\={c}hos tradition, liturgical hymns are sung in eight modes or eight colours (known as eight 'niram', regionally). In this paper, the automatic oktoe\\={c}hos genre classification is addressed using musical texture features (MTF), i-vectors and Mel-spectrograms through stacked bidirectional and unidirectional long-short term memory (SBU-LSTM) and GRU (SB-GRU) architectures. The performance of the proposed approaches is evaluated using a newly created corpus of liturgical music in Malayalam. SBU-LSTM and SB-GRU frameworks report average classification accuracy of 88.19\\% and 87.50\\%, with a significant margin over other frameworks. The experiments demonstrate the potential of stacked architectures in learning temporal information from MTF for the proposed task.",True
bayerl22_interspeech,https://www.isca-archive.org/interspeech_2022/bayerl22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/bayerl22_interspeech.html,What can Speech and Language Tell us About the Working Alliance in Psychotherapy,Speech and Language in Health: From Remote Monitoring to Medical Conversations II,2022,"We are interested in the problem of conversational analysis and its application to the health domain. Cognitive Behavioral Therapy is a structured approach in psychotherapy, allowing the therapist to help the patient to identify and modify the malicious thoughts, behavior, or actions. This cooperative effort can be evaluated using the Working Alliance Inventory Observer-rated Shortened - a 12 items inventory covering task, goal, and relationship - which has a relevant influence on therapeutic outcomes. In this work, we investigate the relation between this alliance inventory and the spoken conversations (sessions) between the patient and the psychotherapist. We have delivered eight weeks of e-therapy, collected their audio and video call sessions and manually transcribed them. The spoken conversations have been annotated and evaluated with WAI ratings by professional therapists. We have investigated speech and language features and their association with WAI items. The feature types include turn dynamics, lexical entrainment and conversational descriptors extracted from the speech and language signals. Our findings provide strong evidence that a subset of these features are strong indicators of working alliance. To the best of our knowledge, this is the first and a novel study to exploit speech and language for characterising working alliance.",True
dang22_interspeech,https://www.isca-archive.org/interspeech_2022/dang22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/dang22_interspeech.html,Exploring Semi-supervised Learning for Audio-based COVID-19 Detection using FixMatch,Speech and Language in Health: From Remote Monitoring to Medical Conversations II,2022,"While there has been recent success in audio-based COVID-19 detection, challenges still exist in developing more reliable and generalised models due to the limited amount of high quality labelled audio recordings. With a substantial amount of unlabelled audio recordings available, exploring semi-supervised learning (SSL) may benefit COVID-19 detection by incorporating this extra data. In this paper, we propose a SSL framework which adjusted FixMatch, one of the most advanced SSL approaches, to audio signals and explored its effectiveness in COVID-19 detection. The proposed framework is validated with a crowd-sourced audio database collected from our app, and showed superior performance over supervised models with a maximum of 7.2\\% relative improvement. Furthermore, we demonstrated that the proposed framework significantly benefits model development using imbalanced datasets, which is a common challenge in clinical data. It can also improve model generalisation. This potentially paves a new pathway of utilising unlabelled data effectively to build more accurate and reliable COVID-19 detection tools.",True
pereztoro22_interspeech,https://www.isca-archive.org/interspeech_2022/pereztoro22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/pereztoro22_interspeech.html,Alzheimer's Detection from English to Spanish Using Acoustic and Linguistic Embeddings,Speech and Language in Health: From Remote Monitoring to Medical Conversations II,2022,"Cross-lingual approaches are growing in popularity in the machine learning domain, where large amounts of data are required to obtain better generalizations. Moreover, one of the biggest problems is the availability of clinical speech data, where most of the resources are in English. For instance, not many available Alzheimer's Disease (AD) corpora in different languages can be found in the literature. Despite the phonological and phonemic differences between Spanish and English, fortunately, there are also similarities between these two languages, e.g., around 40% of all words in English have a related word in Spanish. In this work, we want to investigate the feasibility of combining information from English and Spanish languages to discriminate AD. Two datasets were considered: part of the Pitt Corpus, which is composed of English speakers, and a Spanish AD dataset composed of speakers from Chile. We based our analysis on known acoustic (Wav2Vec) and word (BERT, RoBERTa) embeddings using different classifiers. Strong language dependencies were found, even using multilingual representations. We observed that linguistic information was more important for classifying English AD (F-Score=0.76) and acoustic for Spanish AD (F-Score=0.80). Using knowledge transferred from English to Spanish achieved F-scores of up to 0.85 for discriminating AD.",True
nguyen22d_interspeech,https://www.isca-archive.org/interspeech_2022/nguyen22d_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/nguyen22d_interspeech.html,Accent Conversion using Pre-trained Model and Synthesized Data from Voice Conversion,Voice Conversion and Adaptation III,2022,"Accent conversion (AC) aims to generate synthetic audios by changing the pronunciation pattern and prosody of source speakers (in source audios) while preserving voice quality and linguistic content. There has not been a parallel corpus that contains pairs of audios having the same contents yet coming from the same speakers in different accents, the authors hence work on a solution to synthesize one as training input. The training pipeline is conducted via two steps. First, a voice conversion (VC) model is constructed to synthesize a training data set, containing pairs of audios in the same voice but two different accents. Second, an AC model is trained with the synthesized data to convert a source accented speech to a target accented speech. Given the recognized success of self-supervised learning speech representation (wav2vec 2.0) on certain speech problems such as VC, speech recognition, speech translation, and speech-to-speech translation, we adopt this architecture with some customization to train the AC model in the second step. With just 9-hour synthesized training data, the encoder initialized by the weight of the pre-trained wav2vec 2.0 model outperforms the LSTM-based encoder.",True
wang22o_interspeech,https://www.isca-archive.org/interspeech_2022/wang22o_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/wang22o_interspeech.html,Adversarial Knowledge Distillation For Robust Spoken Language Understanding,Spoken Language Modeling and Understanding,2022,"In spoken dialog systems, Spoken Language Understanding (SLU) usually consists of two parts, Automatic Speech Recognition (ASR) and Natural Language Understanding (NLU). In practice, such decoupled ASR/NLU design is beneficial for fast model iteration on both components. However, it also leads to the problem that NLU model suffers from the errors introduced by ASR, which degrades the overall performance. Improving the NLU model through Knowledge Distillation (KD) from large Pre-trained Language Models (PLMs) is proved to be effective and has drawn a lot of attention recently. In this work, we propose a novel Robust Adversarial Knowledge Distillation (RAKD) framework by introducing adversarial training into knowledge distillation to improve the robustness of NLU model to ASR-error. We conduct experiments on our own built classification dataset from a real-world spoken dialog system as well as existing datasets, where our proposed framework is proved to yield significant improvement over competitive baselines.",True
wang22fa_interspeech,https://www.isca-archive.org/interspeech_2022/wang22fa_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/wang22fa_interspeech.html,Speech2Slot: A Limited Generation Framework with Boundary Detection for Slot Filling from Speech,Spoken Language Modeling and Understanding,2022,"Slot filling is an essential component of Spoken Language Understanding. In contrast to conventional pipeline approaches, which extract slots from the ASR output, end-to-end approaches directly get slots from speech within a classification or generation framework. However, classification relies on predefined categories, which is not scalable, and the generative model is decoding in an open-domain space, suffering from blurred boundaries of slots in speech. To address the shortcomings of these two formulations, we propose a new encoder-decoder framework for slot filling, named Speech2Slot, leveraging a limited generation method with boundary detection. We also released a large-scale Chinese spoken slot filling dataset named Voice Navigation Dataset in Chinese (VNDC). Experiments on VNDC show that our model is markedly superior to other approaches, outperforming the state-of-the-art slot filling approach with 6.65% accuracy improvement. We make our code (https://github.com/eehover/speech2slot) publicly available for researchers to replicate and build on our work.",True
muller22_interspeech,https://www.isca-archive.org/interspeech_2022/muller22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/muller22_interspeech.html,Does Audio Deepfake Detection Generalize?,Privacy and Security in Speech Communication,2022,"Current text-to-speech algorithms produce realistic fakes of human voices, making deepfake detection a much-needed area of research. While researchers have presented various techniques for detecting audio spoofs, it is often unclear exactly why these architectures are successful: Preprocessing steps, hyperparameter settings, and the degree of fine-tuning are not consistent across related work. Which factors contribute to success, and which are accidental? In this work, we address this problem: We systematize audio spoofing detection by re-implementing and uniformly evaluating architectures from related work. We identify overarching features for successful audio deepfake detection, such as using cqtspec or logspec features instead of melspec features, which improves performance by 37% EER on average, all other factors constant. Additionally, we evaluate generalization capabilities: We collect and publish a new dataset consisting of 37.9 hours of found audio recordings of celebrities and politicians, of which 17.2 hours are deepfakes. We find that related work performs poorly on such real-world data (performance degradation of up to one thousand percent). This may suggest that the community has tailored its solutions too closely to the prevailing ASVSpoof benchmark and that deepfakes are much harder to detect outside the lab than previously thought.",True
taniguchi22_interspeech,https://www.isca-archive.org/interspeech_2022/taniguchi22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/taniguchi22_interspeech.html,Transformer-Based Automatic Speech Recognition with Auxiliary Input of Source Language Text Toward Transcribing Simultaneous Interpretation,Multimodal Systems,2022,"In the training programs of human simultaneous interpreters, trainee speech is transcribed into text for quality assessment. Though interpreter speech contains irregular speech events such as hesitations, filled pauses, and self-repairs, automatic speech recognition (ASR) is expected to be introduced to save labor of transcription. In the training programs, source language text can be used for ASR because the training materials are prepared in advance. Thus, we propose a Transformer-based end-to-end ASR with an auxiliary input of a source language text toward transcribing simultaneous interpretation. Because a sufficient amount of human interpreter speech with source language text is not available for training the model, we conducted the initial evaluation of the model by simulating speech with source language text by changing the inputs and outputs of large-scale corpora for developing end-to-end speech translation (ST). Our proposed model significantly reduced word error rates (WERs) for four ST corpora: MuST-C English speech - Netherlandic text, English speech - German text, CoVoST 2 English speech - Japanese text, and our original TED-based English speech - Japanese text corpus.",True
gabeur22_interspeech,https://www.isca-archive.org/interspeech_2022/gabeur22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/gabeur22_interspeech.html,AVATAR: Unconstrained Audiovisual Speech Recognition,Multimodal Systems,2022,"Audio-visual automatic speech recognition (AV-ASR) is an extension of ASR that incorporates visual cues, often from the movements of a speaker's mouth. Unlike works that simply focus on the lip motion, we investigate the contribution of entire visual frames (visual actions, objects, background etc.). This is particularly useful for unconstrained videos, where the speaker is not necessarily visible. To solve this task, we propose a new sequence-to-sequence AudioVisual ASR TrAnsformeR (AVATAR) which is trained end-to-end from spectrograms and full-frame RGB. To prevent the audio stream from dominating training, we propose different word-masking strategies, thereby encouraging our model to pay attention to the visual stream. We demonstrate the contribution of the visual modality on the How2 AV-ASR benchmark, especially in the presence of simulated noise, and show that our model outperforms all other prior work by a large margin. Finally, we also create a new, real-world test bed for AV-ASR called VisSpeech, which demonstrates the contribution of the visual modality under challenging audio conditions.",True
rose22_interspeech,https://www.isca-archive.org/interspeech_2022/rose22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/rose22_interspeech.html,End-to-End multi-talker audio-visual ASR using an active speaker attention module,Multimodal Systems,2022,"This paper presents a new approach for end-to-end audio-visual multi-talker speech recognition. The approach, referred to here as the visual context attention model (VCAM), is important because it uses the available video information to assign decoded text to one of multiple visible faces. This essentially resolves the label ambiguity issue associated with most multi-talker modeling approaches which can decode multiple label strings but cannot assign the label strings to the correct speakers. This is implemented as a transformer-transducer based end-to-end model and evaluated using a two speaker simulated audio-visual overlapping speech dataset created from YouTube videos. It is shown in the paper that the VCAM model improves performance with respect to previously reported audio-only and audio-visual multi-talker ASR systems. It is also shown that the attention model, trained end-to-end with the multi-talker A/V ASR model, is able perform active speaker detection on the two speaker overlapped speech test set.",True
priyasad22_interspeech,https://www.isca-archive.org/interspeech_2022/priyasad22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/priyasad22_interspeech.html,Detecting Heart Failure Through Voice Analysis using Self-Supervised Mode-Based Memory Fusion,Atypical Speech Detection,2022,"Congestive Heart Failure (CHF) is a progressive disease that affects millions of people worldwide, severely impacting their quality of life. Missed detection of CHF and its progression affects life expectancy, thus it is critical to develop applications to continuously monitor CHF symptoms and disease progression in a patient-centric and cost-effective manner. This paper focuses on a novel non-invasive technique to identify CHF using patients' speech traits. Pulmonary congestion and breathlessness is the most common symptom of heart failure and one of the major contributors to hospitalisation. Since pulmonary congestion results in impairment of a patient's voice, we propose a novel, non invasive method for monitoring CHF through analysis of the patient's speech. We also introduce a new balanced dataset, containing voice recordings from both healthy participants and participants diagnosed with CHF, which contains voice alterations reflective of CHF status. We propose a novel deep machine learning architecture based on mode driven memory fusion for CHF recognition from audio recordings of subject's speech. We have achieved 90% accuracy under a subject-independent evaluation setting, highlighting the applicability of such methods for tele-health and home monitoring applications.",True
jung22c_interspeech,https://www.isca-archive.org/interspeech_2022/jung22c_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/jung22c_interspeech.html,SASV 2022: The First Spoofing-Aware Speaker Verification Challenge,Spoofing-Aware Automatic Speaker Verification (SASV) I,2022,"The first spoofing-aware speaker verification (SASV) challenge aims to integrate research efforts in speaker verification and anti-spoofing. We extend the speaker verification scenario by introducing spoofed trials to the usual set of target and non-target trials. In contrast to the established ASVspoof challenge where the focus is upon separate, independently optimised spoofing detection and speaker verification sub-systems, SASV targets the development of integrated and jointly optimised solutions. Pre-trained spoofing detection and speaker verification models are provided as open source and are used in two baseline SASV solutions. Both models and baselines are freely available to participants and can be used to develop back-end fusion approaches or end-to-end solutions. Using the provided common evaluation protocol, 23 teams submitted SASV solutions. When assessed with target, non-target and spoofed trials, the best performing system reduces the equal error rate of a conventional speaker verification system from 23.83% to 0.13%. SASV challenge results are testament to the reliability of today's state-of-the-art approaches to spoofing detection and speaker verification.",True
li22e_interspeech,https://www.isca-archive.org/interspeech_2022/li22e_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/li22e_interspeech.html,DDS: A new device-degraded speech dataset for speech enhancement,Single-channel and multi-channel Speech Enhancement,2022,"A large and growing amount of speech content in real-life scenarios is being recorded on consumer-grade devices in uncontrolled environments, resulting in degraded speech quality. Transforming such low-quality device-degraded speech into high-quality speech is a goal of speech enhancement (SE). This paper introduces a new speech dataset, DDS, to facilitate the research on SE. DDS provides aligned parallel recordings of high-quality speech (recorded in professional studios) and a number of versions of low-quality speech, producing approximately 2,000 hours speech data. The DDS dataset covers 27 realistic recording conditions by combining diverse acoustic environments and microphone devices, and each version of a condition consists of multiple recordings from six microphone positions to simulate different noise and reverberation levels. We also test several SE baseline systems on the DDS dataset and show the impact of recording diversity on performance.",True
kitamura22_interspeech,https://www.isca-archive.org/interspeech_2022/kitamura22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/kitamura22_interspeech.html,Perceptual Evaluation of Penetrating Voices through a Semantic Differential Method,"Speech Production, Perception and Multimodality",2022,"Some speakers have penetrating voices that can be popped out and heard clearly, even in loud noise or from a long distance. This study investigated the voice quality of the penetrating voices using factor analysis. Eleven participants scored how the voices of 124 speakers popped out from the babble noise. By assuming the score as an index of penetration, ten each of high- and low-scored speakers were selected for a rating experiment with a semantic differential method. Forty undergraduate students rated a Japanese sentence produced by these speakers using 14 bipolar 7-point scales concerning voice quality. A factor analysis was conducted using the data of 13 scales (i.e., excluding one scale of penetrating from 14 scales). Three main factors were obtained: (1) powerful and metallic, (2) feminine, and (3) esthetic. The first factor (powerful and metallic) highly correlated with the ratings of penetrating. These results suggest that penetrating voices have multi-dimensional voice quality and that the characteristics of penetrating voice related to powerful and metallic aspects of voices.",True
yang22i_interspeech,https://www.isca-archive.org/interspeech_2022/yang22i_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/yang22i_interspeech.html,Mandarin Lombard Grid: a Lombard-grid-like corpus of Standard Chinese,"Speech Production, Perception and Multimodality",2022,"The Lombard effect is natural, whereby speakers automatically adjust the vocal effort to facilitate speech understanding in noise. Since real-world applications are generally involved in noisy environments, the Lombard effect of highly variable speech features due to changing background noise is one of those challenges to match these real scenarios. Existing Lombard corpora show variations in the background noise level, ranging from 35 to 96 dB sound pressure level (SPL). However, it remains unclear if we need to collect all SPLs to build a comprehensive Lombard corpus. And most existing Lombard corpora are built for English; however, Mandarin and English are different in pronunciation. This paper describes our effort to build the first open-source Lombard corpus of standard Chinese, the Mandarin Lombard Grid. The effort involves three steps: (1) Classify Mandarin Lombard styles according to different background noise levels. (2) Create the corpus containing each style. (3) Analyze Mandarin Lombard effects showing their differences from English. We found three critical Lombard styles ranging from 30 dB to 85 dB-SPL and built the corpus containing the three Lombard styles and one reference plain style. Lombard effect analyses on this corpus showed consistency and some differences from the English Lombard Grid corpus.",True
arai22_interspeech,https://www.isca-archive.org/interspeech_2022/arai22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/arai22_interspeech.html,Syllable sequence of /a/+/ta/ can be heard as /atta/ in Japanese with visual or tactile cues,"Speech Production, Perception and Multimodality",2022,"In our previous work, we reported that the word /atta/ with a geminate consonant differs from the syllable sequence /a/+pause+/ta/ in Japanese; specifically, there are formant transitions at the end of the first syllable in /atta/ but not in /a/+pause+/ta/. We also showed that native Japanese speakers perceived /atta/ when a facial video of /atta/ was synchronously played with an audio signal of /a/+pause+/ta/. In that study, we utilized two video clips for the two utterances in which the speaker was asked to control only the timing of the articulatory closing. In that case, there was no guarantee that the videos would be the exactly same except for the timing. Therefore, in the current study, we use a physical model of the human vocal tract with a miniature robot hand unit to achieve articulatory movements for visual cues. We also provide tactile cues to the listener's finger because we want to test whether cues of another modality affect this perception in the same framework. Our findings showed that when either visual or tactile cues were presented with an audio stimulus, listeners more frequently responded that they heard /atta/ compared to audio-only presentations.",True
chen22i_interspeech,https://www.isca-archive.org/interspeech_2022/chen22i_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/chen22i_interspeech.html,InQSS: a speech intelligibility and quality assessment model using a multi-task learning network,"Speech Production, Perception and Multimodality",2022,"Speech intelligibility and quality assessment models are essential tools for researchers to evaluate and improve speech processing models. However, only a few studies have investigated multi-task models for intelligibility and quality assessment due to the limitations of available data. In this study, we released TMHINT-QI, the first Chinese speech dataset that records the quality and intelligibility scores of clean, noisy, and enhanced utterances. Then, we propose InQSS, a non-intrusive multi-task learning framework for intelligibility and quality assessment. We evaluated the InQSS on both the training-from-scratch and the pretrained models. The experimental results confirm the effectiveness of the InQSS framework. In addition, the resulting model can predict not only the intelligibility scores but also the quality scores of a speech signal.",True
bernhard22_interspeech,https://www.isca-archive.org/interspeech_2022/bernhard22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/bernhard22_interspeech.html,Acoustic Stress Detection in Isolated English Words for Computer-Assisted Pronunciation Training,"Multi-, Cross-lingual and Other Topics in ASR II",2022,"We propose a system for automatic lexical stress detection in isolated English words. It is designed to be part of the computer-assisted pronunciation training application MIAPARLE (miaparle.unige.ch) that specifically focuses on stress contrasts acquisition. Training lexical stress cannot be disregarded in language education as the accuracy in production highly affects the intelligibility and perceived fluency of an L2 speaker. The pipeline automatically segments audio input into syllables over which duration, intensity, pitch, and spectral information is calculated. Since the stress of a syllable is defined relative to its neighboring syllables, the values obtained over the syllables are complemented with differential values to the preceding and following syllables. The resulting feature vectors, retrieved from 1011 recordings of single words spoken by English natives, are used to train a Voting Classifier composed of four supervised classifiers, namely a Support Vector Machine, a Neural Net, a K Nearest Neighbor, and a Random Forest classifier. The approach determines syllables of a single word as stressed or unstressed with an F1 score of 94% and an accuracy of 96%.",True
yang22m_interspeech,https://www.isca-archive.org/interspeech_2022/yang22m_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/yang22m_interspeech.html,Minimizing Sequential Confusion Error in Speech Command Recognition,"Multi-, Cross-lingual and Other Topics in ASR II",2022,"Speech command recognition (SCR) has been commonly used on resource constrained devices to achieve hands-free user experience. However, in real applications, confusion among commands with similar pronunciations often happens due to the limited capacity of small models deployed on edge devices, which drastically affects the user experience. In this paper, inspired by the advances of discriminative training in speech recognition, we propose a novel minimize sequential confusion error (MSCE) training criterion particularly for SCR, aiming to alleviate the command confusion problem. Specifically, we aim to improve the ability of discriminating the target command from other commands on the basis of MCE discriminative criteria. We define the likelihood of different commands through connectionist temporal classification (CTC). During training, we propose several strategies to use prior knowledge creating a confusing sequence set for similar-sounding command instead of creating the whole non-target command set, which can better save the training resources and effectively reduce command confusion errors. Specifically, we design and compare three different strategies for confusing set construction. By using our proposed method, we can relatively reduce the False Reject Rate~(FRR) by 33.7% at 0.01 False Alarm Rate~(FAR) and confusion errors by 18.28% on our collected speech command set.",True
zhao22d_interspeech,https://www.isca-archive.org/interspeech_2022/zhao22d_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/zhao22d_interspeech.html,An Anchor-Free Detector for Continuous Speech Keyword Spotting,Spoken Language Processing III,2022,"Continuous Speech Keyword Spotting (CSKWS) is a task to detect predefined keywords in a continuous speech. In this paper, we regard CSKWS as a one-dimensional object detection task and propose a novel anchor-free detector, named AF-KWS, to solve the problem. AF-KWS directly regresses the center locations and lengths of the keywords through a single-stage deep neural network. In particular, AF-KWS is tailored for this speech task as we introduce an auxiliary unknown class to exclude other words from non-speech or silent background. We have built two benchmark datasets named LibriTop-20 and continuous meeting analysis keywords (CMAK) dataset for CSKWS. Evaluations on these two datasets show that our proposed AF-KWS outperforms reference schemes by a large margin, and therefore provides a decent baseline for future research.",True
conneau22_interspeech,https://www.isca-archive.org/interspeech_2022/conneau22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/conneau22_interspeech.html,XTREME-S: Evaluating Cross-lingual Speech Representations,Spoken Language Processing III,2022,"We introduce XTREME-S, a new benchmark to evaluate universal cross-lingual speech representations in many languages. XTREME-S covers four task families: speech recognition, classification, speech-to-text translation and retrieval. Covering 102 languages from 10+ language families, 3 different domains and 4 task families, XTREME-S aims to simplify multilingual speech representation evaluation, as well as catalyze research in ""universal"" speech representation learning. This paper describes the new benchmark and establishes the first speech-only and speech-text baselines using XLS-R and mSLAM on all downstream tasks. We motivate the design choices and detail how to use the benchmark. Datasets and fine-tuning scripts are made easily accessible through the HuggingFace platform (https://hf.co/datasets/google/xtreme_s).",True
yi22b_interspeech,https://www.isca-archive.org/interspeech_2022/yi22b_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/yi22b_interspeech.html,ConferencingSpeech 2022 Challenge: Non-intrusive Objective Speech Quality Assessment (NISQA) Challenge for Online Conferencing Applications,Non-intrusive Objective Speech Quality Assessment (NISQA) Challenge for Online Conferencing Applications,2022,"With the advances in speech communication systems such as online conferencing applications, we can seamlessly work with people regardless of where they are. However, during online meetings, the speech quality can be significantly affected by background noise, reverberation, packet loss, network jitter, etc. Because of its nature, the speech quality is traditionally assessed in subjective tests in laboratories and lately also in crowdsourcing following the international standards from ITU-T Rec. P.800 series. However, those approaches are costly and cannot be applied to customer data. Therefore, an effective objective assessment approach is needed to evaluate or monitor the speech quality of the ongoing conversation. The ConferencingSpeech 2022 challenge targets the non-intrusive deep neural network models for the speech quality assessment task. We open-sourced a training corpus with more than 86K speech clips in different languages, with a wide range of synthesized and live degradations and their corresponding subjective quality scores through crowdsourcing. 18 teams submitted their models for evaluation in this challenge. The blind test sets included about 4300 clips from wide ranges of degradations. This paper describes the challenge, the datasets, and the evaluation methods and reports the final results.",True
wang22q_interspeech,https://www.isca-archive.org/interspeech_2022/wang22q_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/wang22q_interspeech.html,ECAPA-TDNN Based Depression Detection from Clinical Speech,Speech and Language in Health: From Remote Monitoring to Medical Conversations III,2022,"Depression is a serious mood disorder that has become one of the major diseases that endanger human mental health. The automatic detection of depression using speech signals has become a promising approach for the early diagnosis of depression currently. However, there is still a performance gap between clinical practice and research, considering the lab-recorded corpus was used in most of the current studies. Therefore, we collected a Chinese clinical depression corpus, of which 131 participants with their speech during the Hamilton Rating Scale for Depression (HAMD) interview were included in this study. Furthermore, we developed a depression speech detection system based on a Time-Delay Neural Network (TDNN) model to distinguish depression. Our approach achieves a mean F1 score of 90.8% and an accuracy of 90.4% by five-fold cross-validation. The result suggests that the developed TDNN-based model has a potential clinical meaning in the diagnosis of depression.",True
stephenson22_interspeech,https://www.isca-archive.org/interspeech_2022/stephenson22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/stephenson22_interspeech.html,"BERT, can HE predict contrastive focus? Predicting and controlling prominence in neural TTS using a language model",Speech Synthesis: Prosody Modeling,2022,"Several recent studies have tested the use of transformer language model representations to infer prosodic features for text-to-speech synthesis (TTS). While these studies have explored prosody in general, in this work, we look specifically at the prediction of contrastive focus on personal pronouns. This is a particularly challenging task as it often requires semantic, discursive and/or pragmatic knowledge to predict correctly. We collect a corpus of utterances containing contrastive focus and we evaluate the accuracy of a BERT model, finetuned to predict quantized acoustic prominence features, on these samples. We also investigate how past utterances can provide relevant information for this prediction. Furthermore, we evaluate the controllability of pronoun prominence in a TTS model conditioned on acoustic prominence features.",True
omahony22_interspeech,https://www.isca-archive.org/interspeech_2022/omahony22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/omahony22_interspeech.html,Combining conversational speech with read speech to improve prosody in Text-to-Speech synthesis,Speech Synthesis: Prosody Modeling,2022,"For isolated utterances, speech synthesis quality has improved immensely thanks to the use of sequence-to-sequence models. However, these models are generally trained on read speech and fail to generalise to unseen speaking styles. Recently, more research is focused on the synthesis of expressive and conversational speech. Conversational speech contains many prosodic phenomena that are not present in read speech. We would like to learn these prosodic patterns from data, but unfortunately, many large conversational corpora are unsuitable for speech synthesis due to low audio quality. We investigate whether a data mixing strategy can improve conversational prosody for a target voice based on monologue data from audiobooks by adding real conversational data from podcasts. We filter the podcast data to create a set of 26k question and answer pairs. We evaluate two FastPitch models: one trained on 20 hours of monologue speech from a single speaker, and another trained on 5 hours of monologue speech from that speaker plus 15 hours of questions and answers spoken by nearly 15k speakers. Results from three listening tests show that the second model generates more preferred question prosody.",True
noguchi22_interspeech,https://www.isca-archive.org/interspeech_2022/noguchi22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/noguchi22_interspeech.html,VOT and F0 perturbations for the realization of voicing contrast in Tohoku Japanese,Phonetics and Phonology,2022,"Intervocalic voicing neutralization in Tohoku Japanese was studied based on auditory impressions until our previous research revealed this phenomenon by acoustically and quantitatively measuring voice onset time (VOT). While the research excluded fully pre-voiced tokens due to measurement difficulties, the current study included such tokens by using the method proposed in an earlier study. We also measured the onset F0 of the following vowel and preceding vowel duration to discuss the possibility of secondary cues for the voicing contrast. Our result confirmed the overlap in VOT values in the positive region, which agrees with our previous result, and revealed another overlap in the negative region from the fully pre-voiced tokens newly added in the current study. Moreover, a positive linear correlation between VOT and F0, known as consonant-intrinsic F0, was observed in Tohoku Japanese. Although our results did not support F0 of the following vowel and preceding vowel duration as a secondary cue, investigating acoustic cues may contribute to further increasing our understanding of Tohoku Japanese.",True
ridouane22_interspeech,https://www.isca-archive.org/interspeech_2022/ridouane22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/ridouane22_interspeech.html,Complex sounds and cross-language influence: The case of ejectives in Omani Mehri,Phonetics and Phonology,2022,"Ejective consonants are known to considerably vary both cross-linguistically and within individual languages. This variability is often considered a consequence of the complex articulatory strategies involved in their production. Because they are complex, they might be particularly prone to sound change, especially under cross-language influence. In this study, we consider the production of ejectives in Mehri, a Semitic endangered language spoken in Oman where considerable influence from Arabic is expected. We provide acoustic data from seven speakers producing a list of items contrasting ejective and pulmonic alveolar and velar stops in word-initial (/#â/), word-medial (VâV), and word-final (Vâ#) positions. Different durational and non-durational correlates were examined. The relative importance of these correlates was quantified by the calculation of D-prime values for each. The key empirical finding is that the parameters used to signal ejectivity differ depending mainly on whether the stop is alveolar or velar. Specifically, ejective alveolar stops display characteristics of pharyngealization, similar to Arabic, but velars still maintain attributes of ejectivity in some word positions. We interpret these results as diagnostic of the sound change that is currently in progress, coupled with an ongoing context-dependent neutralization.",True
buech22b_interspeech,https://www.isca-archive.org/interspeech_2022/buech22b_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/buech22b_interspeech.html,Pharyngealization in Amazigh: Acoustic and articulatory marking over time,Phonetics and Phonology,2022,"Pharyngealization refers to a secondary articulation whereby a set of consonants is produced with the backing of the tongue towards the pharyngeal wall. This typologically rare phenomenon appears in some Afroasiatic languages, including Arabic and Amazigh. While the phonetic characteristics of pharyngealization in Arabic have been investigated in-depth, the comparative data currently available on Amazigh is particularly scant. The present study aims to fill this gap and provides the first comprehensive study on the phonetic characteristics of pharyngealization in an Amazigh language. The empirical data comes from acoustic and articulatory recordings of six Tashlhiyt speakers producing a set of singleton and geminate plain coronals and their pharyngealized counterparts in intervocalic position. We analyze formant trajectories of the vowels surrounding plain and pharyngealized consonants and articulatory trajectories during the consonantal movements and examine how the manner of articulation, voicing, and length shape variability in the way the contrast is implemented. Results show that pharyngealization is principally characterized by a large drop of F2 of adjacent vowels at the acoustic level and an extensive lowering of the tongue body at the articulatory level. These attributes are consistent across voicing, length and manner of articulation.",True
barker22_interspeech,https://www.isca-archive.org/interspeech_2022/barker22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/barker22_interspeech.html,The 1st Clarity Prediction Challenge: A machine learning challenge for hearing aid intelligibility prediction,Speech Intelligibility Prediction for Hearing-Impaired Listeners I,2022,"This paper reports on the design and outcomes of the 1st Clarity Prediction Challenge (CPC1) for predicting the intelligibility of hearing aid processed signals heard by individuals with a hearing impairment. The challenge was designed to promote the development of new intelligibility measures suitable for use in developing hearing aid algorithms. Participants were supplied with listening test data compromising 7233 responses from 27 individuals. Data was split between training and test sets in a manner that fostered a machine learning approach and allowed both closed-set (known listeners) and open-set (unseen listener/unseen system) evaluation. The paper provides a description of the challenge design including the datasets, the hearing aid algorithms applied, the listeners and the perceptual tests. The challenge attracted submissions from 15 systems. The results are reviewed and the paper summarises, compares and contrasts approaches.",True
zevallos22_interspeech,https://www.isca-archive.org/interspeech_2022/zevallos22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/zevallos22_interspeech.html,Data Augmentation for Low-Resource Quechua ASR Improvement,Low-Resource ASR Development I,2022,"Automatic Speech Recognition (ASR) is a key element in new services that helps users to interact with an automated system. Deep learning methods have made it possible to deploy systems with a word error rate close to only 5% for ASR of English. However, the use of these methods is only available for languages with hundreds or thousands of hours of audio and their corresponding transcriptions. For the so-called low- resource languages to speed up the availability of resources that can improve the performance of their ASR systems, methods of creating new resources on the basis of existing ones are being investigated. In this paper we describe our DA approach to improve the results of ASR models for low-resource and agglutinative languages. We carry out experiments developing an ASR for Quechua using the Wav2letter++ model. We reduced WER by 8.73% through our approach to the base model. The resulting ASR model obtained 22.75% WER and was trained with 99 hours of original resources and 99 hours of synthetic data obtained with a combination of text augmentation and synthetic speech generation.",True
robinson22_interspeech,https://www.isca-archive.org/interspeech_2022/robinson22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/robinson22_interspeech.html,When Is TTS Augmentation Through a Pivot Language Useful?,Low-Resource ASR Development I,2022,"Developing Automatic Speech Recognition (ASR) for low-resource languages is a challenge due to the small amount of transcribed audio data. For many such languages, audio and text are available separately, but not audio with transcriptions. Using text, speech can be synthetically produced via text-to-speech (TTS) system . However, many low-resource languages do not have quality TTS systems either. We propose an alternative: produce synthetic audio by running text from the target language through a trained TTS system for a higher-resource pivot language. We investigate when and how this technique is most effective in low-resource settings. In our experiments, using several thousand synthetic TTS data pairs and duplicating authentic data to balance yields optimal results. Our findings suggest that searching over a set of candidate pivot languages can lead to marginal improvements and that, surprisingly, ASR performance can at times by harmed by increases in measured TTS quality. Application of these findings improves ASR error rates by 64.5% and 45.0% CERR respectively for two low-resource languages: Guarani and Suba.",True
bhanushali22_interspeech,https://www.isca-archive.org/interspeech_2022/bhanushali22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/bhanushali22_interspeech.html,Gram Vaani ASR Challenge on spontaneous telephone speech recordings in regional variations of Hindi,Low-Resource ASR Development I,2022,"This paper describes the corpus and baseline systems for the Gram Vaani Automatic Speech Recognition (ASR) challenge in regional variations of Hindi. The corpus for this challenge comprises the spontaneous telephone speech recordings collected by a social technology enterprise, Gram Vaani. The regional variations of Hindi together with spontaneity of speech, natural background and transcriptions with variable accuracy due to crowdsourcing make it a unique corpus for ASR on spontaneous telephonic speech. Around, 1108 hours of real-world spontaneous speech recordings, including 1000 hours of unlabelled training data, 100 hours of labelled training data, 5 hours of development data and 3 hours of evaluation data, have been released as a part of the challenge. The efficacy of both training and test sets are validated on different ASR systems in both traditional time-delay neural network-hidden Markov model (TDNN-HMM) frameworks and fully-neural end-to-end (E2E) setup. The word error rate (WER) and character error rate (CER) on eval set for a TDNN model trained on 100 hours of labelled data are 29.7% and 15.1%, respectively. While, in E2E setup, WER and CER on eval set for a conformer model trained on 100 hours of data are 32.9% and 19.0%, respectively.",True
triantafyllopoulos22_interspeech,https://www.isca-archive.org/interspeech_2022/triantafyllopoulos22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/triantafyllopoulos22_interspeech.html,Distinguishing between pre- and post-treatment in the speech of patients with chronic obstructive pulmonary disease,Pathological Speech Assessment,2022,"Chronic obstructive pulmonary disease (COPD) causes lung inflammation and airflow blockage leading to a variety of respiratory symptoms; it is also a leading cause of death and affects millions of individuals around the world. Patients often require treatment and hospitalisation, while no cure is currently available. As COPD predominantly affects the respiratory system, speech and non-linguistic vocalisations present a major avenue for measuring the effect of treatment. In this work, we present results on a new COPD dataset of 20 patients, showing that, by employing personalisation through speaker-level feature normalisation, we can distinguish between pre- and post-treatment speech with an unweighted average recall (UAR) of up to 82% in (nested) leave-one-speaker-out cross-validation. We further identify the most important features and link them to pathological voice properties, thus enabling an auditory interpretation of treatment effects. Monitoring tools based on such approaches may help objectivise the clinical status of COPD patients and facilitate personalised treatment plans.",True
lee22k_interspeech,https://www.isca-archive.org/interspeech_2022/lee22k_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/lee22k_interspeech.html,A Study on the Phonetic Inventory Development of Children with Cochlear Implants for 5 Years after Implantation,Pathological Speech Assessment,2022,"This paper investigates longitudinal phonetic inventories of vowels and consonants of Korean-speaking children with cochlear implants (CIs). They are based on speech data of 7 children with CI over 5 years PI to examine the entire speech production development. Phones produced at least twice by more than 50% children in spontaneous and imitation speech from 6 months to 5 years post-implantation (PI) are compiled in the inventories. The results show and differences and similarities between children with CI and with normal hearing (NH). The vowel and consonant inventories at 6 months PI are larger than those of NH children at 1 year of age whose hearing experience is longer, including liquid [É¾] and fricative [s]. It can be attributed to biological maturation of CI children. As in children with NH, there is an explosive increase in phonetic inventories during a year after 1-year of robust hearing experience and the inventories are almost complete after 3 years of PI. Phonetic inventories at each time are expected to be references to assess the developmental appropriateness in speech production and guides to direct habilitation goals.",True
vitormenezes22_interspeech,https://www.isca-archive.org/interspeech_2022/vitormenezes22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/vitormenezes22_interspeech.html,Evaluation of different antenna types and positions in a stepped frequency continuous-wave radar-based silent speech interface,Pathological Speech Assessment,2022,"Silent speech interfaces (SSIs) are subject of growing interest, as they can enable speech communication even in the absence of the acoustic signal. Among sensing techniques used in SSIs, radar sensing has many desirable characteristics, such as non-invasiveness and comfort. Although promising results have been achieved with radar-based SSIs, some of its crucial parameters are yet to be investigated, e.g., the optimal type and position of the antennas. To fill this gap, this study investigated the performance of a radar-based SSI with 3 antenna types attached to 3 positions on the speaker's cheek (9 setups). A corpus of 25 phonemes uttered under co-articulation effects was recorded with the 9 setups by 2 native German speakers and then classified with respect to the phonemes. A linear mixed-effect model was fitted to the resulting recognition rates and likelihood ratio tests showed significance for the effects of antenna type and position. The two monopole-type antennas performed better than the Vivaldi-type antenna (2.7% Â± 2.8% and 6.2% Â± 3.0% improvement), and the two positions closer to the speaker's lips performed better than the most distant position (decrease of 2.8% Â± 0.9%). This provides more solid foundation for the development of this type of SSI.",True
benway22_interspeech,https://www.isca-archive.org/interspeech_2022/benway22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/benway22_interspeech.html,PERCEPT-R: An Open-Access American English Child/Clinical Speech Corpus Specialized for the Audio Classification of /É¹/,Pathological Speech Assessment,2022,"We present the PERCEPT-R corpus, a labeled corpus of child speakers of American English with typical speech and residual speech sound disorders affecting rhotics. We demonstrate the utility of age-and-gender normalized formants extracted from PERCEPT-R in training support vector classifiers to predict ground-truth perceptual judgments of ""rhoticâ (i.e., dialect-typical) and ""derhoticâ phones for novel speakers (mean of participant-specific f-metrics = .83; SD = .18, N = 281).",True
kothare22_interspeech,https://www.isca-archive.org/interspeech_2022/kothare22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/kothare22_interspeech.html,Statistical and clinical utility of multimodal dialogue-based speech and facial metrics for Parkinson's disease assessment,Pathological Speech Assessment,2022,"We present a framework for characterising the statistical and clinical relevance of speech and facial metrics in Parkinson's disease (PD) extracted by a multimodal conversational platform. 38 people with PD (pPD) and 22 controls were recruited in an ongoing study and were asked to complete four interactive sessions, a week apart from each other. In each session, a virtual conversational agent, Tina, guided participants through a battery of standard tasks designed to elicit speech and facial behaviours. Speech and facial metrics were automatically extracted in real time, several of which showed statistically significant differences between pPD and controls. We explored which of these differences were greater than measurement error, a threshold defined as the minimally detectable change (MDC). Furthermore, we computed the minimal clinically important difference (MCID) with respect to the Communicative Participation Item Bank short form (CPIB-S) scale for these select metrics. Our results show that differences in metrics like duration and fundamental frequency (F0) of speech are captured beyond measurement error. We also discuss several confounding factors that need to be taken into consideration before making any clinical interpretation of changes in these metrics.",True
ghosh22_interspeech,https://www.isca-archive.org/interspeech_2022/ghosh22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/ghosh22_interspeech.html,Low-resource Low-footprint Wake-word Detection using Knowledge Distillation,Speech Segmentation II,2022,"As virtual assistants have become more diverse and specialized, so has the demand for application or brand-specific wake words. However, the wake-word-specific datasets typically used to train wake-word detectors are costly to create. In this paper, we explore two techniques to leverage acoustic modeling data for large-vocabulary speech recognition to improve a purpose-built wake-word detector: transfer learning and knowledge distillation. We also explore how these techniques interact with time-synchronous training targets to improve detection latency. Experiments are presented on the open-source ""Hey Snipsâ dataset and a more challenging in-house far-field dataset. Using phone-synchronous targets and knowledge distillation from a large acoustic model, we are able to improve accuracy across dataset sizes for both datasets while reducing latency.",True
zhu22e_interspeech,https://www.isca-archive.org/interspeech_2022/zhu22e_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/zhu22e_interspeech.html,Filler Word Detection and Classification: A Dataset and Benchmark,Speech Segmentation II,2022,"Filler words such as uh' orum' are sounds or words people use to signal they are pausing to think. Finding and removing filler words from recordings is a common and tedious task in media editing. Automatically detecting and classifying filler words could greatly aid in this task, but few studies have been published on this problem to date.A key reason is the absence of a dataset with annotated filler words for model training and evaluation.In this work, we present a novel speech dataset, PodcastFillers, with 35K annotated filler words and 50K annotations of other sounds that commonly occur in podcasts such as breaths, laughter, and word repetitions.We propose a pipeline that leverages VAD and ASR to detect filler candidates and a classifier to distinguish between filler word types.We evaluate our proposed pipeline on PodcastFillers, compare to several baselines, and present a detailed ablation study.In particular, we evaluate the importance of using ASR and how it compares to a transcription-free approach resembling keyword spotting. We show that our pipeline obtains state-of-the-art results, and that leveraging ASR strongly outperforms a keyword spotting approach. We make PodcastFillers publicly available, in the hope that our work serves as a benchmark for future research.",True
yang22g_interspeech,https://www.isca-archive.org/interspeech_2022/yang22g_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/yang22g_interspeech.html,ASR Error Correction with Constrained Decoding on Operation Prediction,ASR: Linguistic Components,2022,"Error correction techniques remain effective to refine outputs from automatic speech recognition (ASR) models. Existing end-to-end error correction methods based on an encoder-decoder architecture process all tokens in the decoding phase, creating undesirable latency. In this paper, we propose an ASR error correction method utilizing the predictions of correction operations. More specifically, we construct a predictor between the encoder and the decoder to learn if a token should be kept (""K""), deleted (""D""), or changed (""C"") to restrict decoding to only part of the input sequence embeddings (the ""C"" tokens) for fast inference. Experiments on three public datasets demonstrate the effectiveness of the proposed approach in reducing the latency of the decoding process in ASR correction. It enhances the inference speed by at least three times (3.4 and 5.7 times) while maintaining the same level of accuracy (with WER reductions of 0.53% and 1.69% respectively) for our two proposed models compared to a solid encoder-decoder baseline. In the meantime, we produce and release a benchmark dataset contributing to the ASR error correction community to foster research along this line.",True
thithuuyen22_interspeech,https://www.isca-archive.org/interspeech_2022/thithuuyen22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/thithuuyen22_interspeech.html,Vietnamese Capitalization and Punctuation Recovery Models,ASR: Linguistic Components,2022,"Despite the rise of recent performant methods in Automatic Speech Recognition (ASR), such methods do not ensure proper casing and punctuation for their outputs. This problem has a significant impact on the comprehension of both Natural Language Processing (NLP) algorithms and human to process. Capitalization and punctuation restoration is imperative in pre-processing pipelines for raw textual inputs. For low resource languages like Vietnamese, public datasets for this task are scarce. In this paper, we contribute a public dataset for capitalization and punctuation recovery for Vietnamese; and propose a joint model for both tasks named JointCapPunc. Experimental results on the Vietnamese dataset show the effectiveness of our joint model compare to single model and previous joint learning model. We publicly release our dataset and the implementation of our model at https://github.com/anhtunguyen98/JointCapPunc",True
fox22_interspeech,https://www.isca-archive.org/interspeech_2022/fox22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/fox22_interspeech.html,Improving Contextual Recognition of Rare Words with an Alternate Spelling Prediction Model,ASR: Linguistic Components,2022,"Contextual ASR, which takes a list of bias terms as input along with audio, has drawn recent interest as ASR use becomes more widespread. We are releasing contextual biasing lists to accompany the Earnings21 dataset, creating a public benchmark for this task. We present baseline results on this benchmark using a pretrained end-to-end ASR model from the WeNet toolkit. We show results for shallow fusion contextual biasing applied to two different decoding algorithms. Our baseline results confirm observations that end-to-end models struggle in particular with words that are rarely or never seen during training, and that existing shallow fusion techniques do not adequately address this problem. We propose an alternate spelling prediction model that improves recall of rare words by 34.7% relative and of out-of-vocabulary words by 97.2% relative, compared to contextual biasing without alternate spellings. This model is conceptually similar to ones used in prior work, but is simpler to implement as it does not rely on either a pronunciation dictionary or an existing text-to-speech system.",True
kim22p_interspeech,https://www.isca-archive.org/interspeech_2022/kim22p_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/kim22p_interspeech.html,Evaluating User Perception of Speech Recognition System Quality with Semantic Distance Metric,"Summarization, Entity Extraction, Evaluation and Others",2022,"Measuring automatic speech recognition (ASR) system quality is critical for creating user-satisfying voice-driven applications. Word Error Rate (WER) has been traditionally used to evaluate ASR system quality; however, it sometimes correlates poorly with user perception/judgement of transcription quality. This is because WER weighs every word equally and does not consider semantic correctness which has a higher impact on user perception. In this work, we propose evaluating ASR output hypotheses quality with SemDist that can measure semantic correctness by using the distance between the semantic vectors of the reference and hypothesis extracted from a pre-trained language model. Our experimental results of 71K and 36K user annotated ASR output quality show that SemDist achieves higher correlation with user perception than WER. We also show that SemDist has higher correlation with downstream Natural Language Understanding (NLU) tasks than WER.",True
yoon22_interspeech,https://www.isca-archive.org/interspeech_2022/yoon22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/yoon22_interspeech.html,Predicting Emotional Intensity in Political Debates via Non-verbal Signals,Automatic Analysis of Paralinguistics,2022,"Non-verbal expressions of politicians are important in election. In particular, the emotional intensity of politician revealed in a debate can be strongly linked to voters' evaluation. This paper proposes a multimodal deep-learning model for predicting the perceived emotional intensity of a candidate, which utilizes voice, face, and gesture to capture the comprehensive information of one's emotional intensity revealed in a debate. We collect a dataset of political debate videos from the 2020 Democratic presidential primaries in the USA, and train the proposed model with randomly sampled clips from the debate videos. By applying the proposed model to 23 candidates in 11 debate videos, we show that the standard deviation of the perceived emotional intensity is positively correlated with the changes in candidates' favorability in public polls.",True
saeki22d_interspeech,https://www.isca-archive.org/interspeech_2022/saeki22d_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/saeki22d_interspeech.html,Confusion Detection for Adaptive Conversational Strategies of An Oral Proficiency Assessment Interview Agent,Automatic Analysis of Paralinguistics,2022,"In this study, we present a model to detect user confusion in an online interview dialogue using conversational agents. Conversational agents have gained attention for reliable assessment of language learners' oral skills in interviews. Learners often face confusion, where they fail to understand what the system has said, and may end up unable to respond, leading to a conversational breakdown. It is thus crucial for the system to detect such a state and keep the interview going forward by repeating or rephrasing the previous system utterance. To this end, we first collected a dataset of user confusion using a psycholinguistic experimental approach and identified seven multimodal signs of confusion, some of which were unique to an online conversation. With the corresponding features, we trained a classification model of user confusion. An ablation study showed that the features related to self-talk and gaze direction were most predictive. We discuss how this model can assist a conversational agent to detect and resolve user confusion in real-time.",True
gent22_interspeech,https://www.isca-archive.org/interspeech_2022/gent22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/gent22_interspeech.html,Deep Learning for Prosody-Based Irony Classification in Spontaneous Speech,Automatic Analysis of Paralinguistics,2022,"Recognizing irony in speech and text can be challenging even for humans. For natural language processing (NLP) applications, irony recognition presents a unique challenge as irony alters the sentiment and meaning of the words themselves. Combining phonological insights from past literature on irony prosody and deep learning modeling, this research presents a new approach to irony classification in naturalistic speech data. A new corpus consisting of nearly five hours of irony-annotated, naturalistic, conversational speech data has been constructed for this study. A wide array of utterance-level and time-series acoustic features were extracted from this data and utilized in the training and fine-tuning of a series of deep learning approaches for irony classification. The best-performing model achieved an area under the curve of 0.811 in the speaker dependent condition, and 0.738 in the speaker independent condition, outperforming most irony classification models in the existing literature. In addition to the myriad real-world applications for this approach, its contribution to the understanding of prosodically-encoded augmentation of semantic content constitutes a significant step forward for research in the fields of linguistics and NLP.",True
rennie22_interspeech,https://www.isca-archive.org/interspeech_2022/rennie22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/rennie22_interspeech.html,Which Model is Best: Comparing Methods and Metrics for Automatic Laughter Detection in a Naturalistic Conversational Dataset,Automatic Analysis of Paralinguistics,2022,"Laughter is a common paralinguistic vocalization that has been shown to be used for controlling the flow of a conversation, nullifying previous statements, and managing conversations on delicate topics. Already there have been concerted efforts to develop methods for automatically detecting laughter in speech. Many of these studies use artificial datasets and report their model performance using the AUC metric. This paper replicates previous work on laughter detection on those artificial datasets and then extends them by validating the methods on a larger and more naturalistic dataset made up of 60 spontaneous conversations (120 speakers and roughly 12 hours of material in total) with the best performing model achieving an AUC of 90.39\\5 +/- 1.10 (precision=13.99 +/- 4.09, recall=76.36 +/- 12.00, F1=23.06 +/- 4.99). The paper then goes on to discuss the shortcomings with the current standard comparison metric in the field of AUC and suggests alternatives which may aid in the comparison and understanding of method's effectiveness.",True
kawa22_interspeech,https://www.isca-archive.org/interspeech_2022/kawa22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/kawa22_interspeech.html,Attack Agnostic Dataset: Towards Generalization and Stabilization of Audio DeepFake Detection,Self Supervision and Anti-Spoofing,2022,"Audio DeepFakes allow the creation of high-quality, convincing utterances and therefore pose a threat due to its potential applications such as impersonation or fake news. Methods for detecting these manipulations should be characterized by good generalization and stability leading to robustness against attacks conducted with techniques that are not explicitly included in the training. In this work, we introduce Attack Agnostic Dataset - a combination of two audio DeepFakes and one anti-spoofing datasets that, thanks to the disjoint use of attacks, can lead to better generalization of detection methods. We present a thorough analysis of current DeepFake detection methods and consider different audio features (front-ends). In addition, we propose a model based on LCNN with LFCC and mel-spectrogram front-end, which not only is characterized by a good generalization and stability results but also shows improvement over LFCC-based mode - we decrease standard deviation on all folds and EER in two folds by up to 5%.",True
defino22_interspeech,https://www.isca-archive.org/interspeech_2022/defino22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/defino22_interspeech.html,Prediction of L2 speech proficiency based on multi-level linguistic features,Speech Articulation & Neural Processing,2022,"This study investigates the possibility to use automatic, multi-level features for the prediction of L2 speech proficiency. The method was applied on a corpus containing audio recordings and transcripts for 38 Japanese learners of French who participated in a semi-spontaneous oral production task. Each learner's speech proficiency level was assessed by three experienced French teachers. Audio recordings were processed to extract features related to the pronunciation skills and phonetic fluency of the learners, while the transcripts were used to measure their lexical, syntactic, and discursive abilities in French. A Lasso regression using a leave-one-out cross-validation procedure was used to select relevant features and to accurately predict speech proficiency scores. The results show that five features related to the phonetic fluency (speech rate), lexical abilities (lexical density), discourse planning and elaboration skills (number of hesitation and false starts, mean utterance length) of the learners can be used to predict speech proficiency ratings (r=0.71, mean absolute error on a 5-point scale: 0.53).",True
meng22d_interspeech,https://www.isca-archive.org/interspeech_2022/meng22d_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/meng22d_interspeech.html,Evidence of Onset and Sustained Neural Responses to Isolated Phonemes from Intracranial Recordings in a Voice-based Cursor Control Task,Speech Articulation & Neural Processing,2022,"We developed a voice-based, self-paced cursor control task to collect corresponding intracranial neural data during isolated utterances of phonemes, namely vowel, nasal and fricative sounds. Two patients implanted with intracranial depth electrodes for clinical epilepsy monitoring performed closed-loop voice-based cursor control from real-time processing of microphone input. In post-hoc data analyses, we searched for neural features that correlated with the occurrence of non-specific speech sounds or specific phonemes. In line with previous studies, we observed onset and sustained responses to speech sounds at multiple recording sites within the superior temporal gyrus. Based on differential patterns of activation in narrow frequency bands up to 200 Hz, we tracked voice activity with 91% accuracy (chance level: 50%) and classified individual utterances into one of five phonemes with 68% accuracy (chance level: 20%). We propose that our framework could be extended to additional phonemes to better characterize neurophysiological mechanisms underlying the production and perception of speech sounds in the absence of language context. In general, our findings provide supplementary evidence and information toward the development of speech brain-computer interfaces using intracranial electrodes.",True
takeuchi22_interspeech,https://www.isca-archive.org/interspeech_2022/takeuchi22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/takeuchi22_interspeech.html,Introducing Auxiliary Text Query-modifier to Content-based Audio Retrieval,Acoustic scene analysis,2022,"The amount of audio data available on public websites is growing rapidly, and an efficient mechanism for accessing the desired data is necessary. We propose a content-based audio retrieval method that can retrieve a target audio that is similar to but slightly different from the query audio by introducing auxiliary textual information which describes the difference between the query and target audio. While the range of conventional content-based audio retrieval is limited to audio that is similar to the query audio, the proposed method can adjust the retrieval range by adding an embedding of the auxiliary text query-modifier to the embedding of the query sample audio in a shared latent space. To evaluate our method, we built a dataset comprising two different audio clips and the text that describes the difference. The experimental results show that the proposed method retrieves the paired audio more accurately than the baseline. We also confirmed based on visualization that the proposed method obtains the shared latent space in which the audio difference and the corresponding text are represented as similar embedding vectors.",True
wang22b_interspeech,https://www.isca-archive.org/interspeech_2022/wang22b_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/wang22b_interspeech.html,Opencpop: A High-Quality Open Source Chinese Popular Song Corpus for Singing Voice Synthesis,"Speech Synthesis: Singing, Multimodal, Crosslingual Synthesis",2022,"This paper introduces Opencpop, a publicly available high-quality Mandarin singing corpus designed for singing voicesynthesis (SVS). The corpus consists of 100 popular Mandarinsongs performed by a female professional singer. Audio filesare recorded with studio quality at a sampling rate of 44,100 Hzand the corresponding lyrics and musical scores are provided.All singing recordings have been phonetically annotated withphoneme boundaries and syllable (note) boundaries. To demon-strate the reliability of the released data and to provide a baselinefor future research, we built baseline deep neural network-basedSVS models and evaluated them with both objective metrics andsubjective mean opinion score (MOS) measure. Experimentalresults show that the best SVS model trained on our databaseachieves 3.70 MOS, indicating the reliability of the providedcorpus. Opencpop is released to the open-source community WeNet, and the corpus, as well as synthesized demos, can befound on the project homepage.",True
kunihara22b_interspeech,https://www.isca-archive.org/interspeech_2022/kunihara22b_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/kunihara22b_interspeech.html,Detection of Learners' Listening Breakdown with Oral Dictation and Its Use to Model Listening Skill Improvement Exclusively Through Shadowing,"Applications in Transcription, Education and Learning I",2022,"In language learners' speech, mispronounced words, word fragments, repairs, filled pauses, etc are often found, and they can be detected with ASR-based CALL systems. When learners are listening, some segments in a given utterance are often difficult to identify or misidentified due to lack of listening skill. In this study, we aim at detecting learners' listening breakdown to measure their listening skill. Listening skill is often quantified by imposing manual dictation on learners, but it has inevitable problems because manual dictation is generally an offline task. To solve the problems, oral dictation is imposed instead, and speaking breakdown is detected in the dictation utterances. Here, we assume that learners' speaking breakdown is attributed to their listening breakdown. This method is applied to measure their listening skill and to model its improvement exclusively through shadowing, which is oral dictation with a short delay and was introduced to language education originally as listening training. 35 Japanese university students attended a 42-day intensive shadowing training, and their shadowing utterances were analyzed to detect listening breakdown. Our model exhibits very monotonous improvement of listening skill as a function of how many days learners attended shadowing.",True
wallbridge22_interspeech,https://www.isca-archive.org/interspeech_2022/wallbridge22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/wallbridge22_interspeech.html,Investigating perception of spoken dialogue acceptability through surprisal,Spoken Dialogue Systems,2022,"Surprisal is used throughout computational psycholinguistics to model a range of language processing behaviour. There is growing evidence that language model (LM) estimates of surprisal correlate with human performance on a range of written language comprehension tasks. Although communicative interaction is arguably the primary form of language use, most studies of surprisal are based on monological, written data. Towards the goal of understanding perception in spontaneous, natural language, we present an exploratory investigation into whether the relationship between human comprehension behaviour and LM-estimated surprisal holds when applied to dialogue, considering both written dialogue, and the lexical component of spoken dialogue. We use a novel judgement task of dialogue utterance acceptability to ask two questions: ""How well can people make predictions about written dialogue and transcripts of spoken dialogue?â and ""Does surprisal correlate with these acceptability judgements?â. We demonstrate that people can make accurate predictions about upcoming dialogue and that their ability differs between spoken transcripts and written conversation. We investigate the relationship between global and local operationalisations of surprisal and human acceptability judgements, finding a combination of both to provide the most predictive power",True
huang22f_interspeech,https://www.isca-archive.org/interspeech_2022/huang22f_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/huang22f_interspeech.html,The VoiceMOS Challenge 2022,The VoiceMOS Challenge,2022,"We present the first edition of the VoiceMOS Challenge, a scientific event that aims to promote the study of automatic prediction of the mean opinion score (MOS) of synthetic speech. This challenge drew 22 participating teams from academia and industry who tried a variety of approaches to tackle the problem of predicting human ratings of synthesized speech. The listening test data for the main track of the challenge consisted of samples from 187 different text-to-speech and voice conversion systems spanning over a decade of research, and the out-of-domain track consisted of data from more recent systems rated in a separate listening test. Results of the challenge show the effectiveness of fine-tuning self-supervised speech models for the MOS prediction task, as well as the difficulty of predicting MOS ratings for unseen speakers and listeners, and for unseen systems in the out-of-domain setting.",True
mateju22_interspeech,https://www.isca-archive.org/interspeech_2022/mateju22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/mateju22_interspeech.html,Overlapped Speech Detection in Broadcast Streams Using X-vectors,Speech Segmentation I,2022,"A new approach to overlapped speech detection (OSD) is introduced in this work. It is designed for real-time processing of streamed data and utilizes x-vectors as its input features. It thus allows us to reduce computational demands within the entire streaming data processing chain, where the same x-vectors can also be used for the related task of speaker diarization. Within our method, the x-vectors are extracted using a feed-forward sequential memory network (FSMN) and then fed into a simple neural classifier (speech or cross-talk), whose output is smoothed by a decoder based on weighted finite-state transducers (WFSTs). The evaluation is done on a Czech/Slovak broadcast dataset (we make this data public) and on the AMI meeting corpus. Our online method yields a solid performance while operating with a 2-second latency.",True
meneses22_interspeech,https://www.isca-archive.org/interspeech_2022/meneses22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/meneses22_interspeech.html,SiDi KWS: A Large-Scale Multilingual Dataset for Keyword Spotting,Speech Segmentation I,2022,"Keyword spotting (KWS) has become a hot topic in speech processing due to the rise of commercial applications based on voice command detection, such as voice assistants. Like tasks in computer vision, natural language processing, and even speech processing, most current successful approaches for KWS rely on deep learning. However, differently from all those tasks, there is a lack of large-scale datasets designed for training and evaluating deep learning models for KWS. The current work presents SiDi KWS, a public large-scale multilingual dataset currently composed of 24.3 million audio recordings of labeled single-spoken keywords. It intends to boost the development of new KWS systems, especially those based on deep learning. That dataset has been created by applying automatic forced alignment on public datasets of transcribed speech. This work introduces SiDi KWS and KeywordMiner, an open-source framework used to generate that dataset, to benefit the speech processing research community.",True
kim22h_interspeech,https://www.isca-archive.org/interspeech_2022/kim22h_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/kim22h_interspeech.html,Dummy Prototypical Networks for Few-Shot Open-Set Keyword Spotting,Speech Segmentation I,2022,"Keyword spotting is the task of detecting a keyword in streaming audio. Conventional keyword spotting targets predefined keywords classification, but there is growing attention in few-shot (query-by-example) keyword spotting, e.g., N-way classification given M-shot support samples. Moreover, in real-world scenarios, there can be utterances from unexpected categories (open-set) which need to be rejected rather than classified as one of the N classes. Combining the two needs, we tackle few-shot open-set keyword spotting with a new benchmark setting, named splitGSC. We propose episode-known dummy prototypes based on metric learning to detect an open-set better and introduce a simple and powerful approach, Dummy Prototypical Networks (D-ProtoNets). Our D-ProtoNets shows clear margins compared to recent few-shot open-set recognition (FSOSR) approaches in the suggested splitGSC. We also verify our method on a standard benchmark, miniImageNet, and D-ProtoNets shows the state-of-the-art open-set detection rate in FSOSR.",True
liebig22_interspeech,https://www.isca-archive.org/interspeech_2022/liebig22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/liebig22_interspeech.html,An investigation of regression-based prediction of the femininity or masculinity in speech of transgender people,Human Speech & Signal Processing,2022,"Transgender individuals often seek for voice modification to more closely have their voice matched with their new sex, and avoid potential stigmatization or even discrimination. Whereas treatment options such as voice therapy or surgery exist, a quantitative measure of the treatment outcome is missing. In this paper, we therefore propose a novel regression-based method to predict the perceived femininity or masculinity of a speaker's voice. To this end, 86 speakers (34 male, 35 female, 17 transgender) were recorded reading aloud a German standard passage. Subsequently a group of 28 laypersons and 13 experts rated the femininity/masculinity of these speech samples. Each spoken utterance was automatically analysed with respect to nine different pitch-, resonance- and voice quality-related acoustic features. The ratings were the targets for three prediction models (linear, logistic and decision tree regression) based on the extracted features. The results show that, generally, f0 and the vocal tract length (VTL) are the main predictors. Furthermore, the continuous outcome logistic regression model with f0, smoothed cepstral peak prominence (CPPS), Jitter and VTL as input features performed best and achieved promising results with a cross-validated root mean-squared error of 0.117 on the normalized ratings [0,1].",True
kang22d_interspeech,https://www.isca-archive.org/interspeech_2022/kang22d_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/kang22d_interspeech.html,SpeechEQ: Speech Emotion Recognition based on Multi-scale Unified Datasets and Multitask Learning,Speech Emotion Recognition II,2022,"Speech emotion recognition (SER) has many challenges, but one of the main challenges is that each framework does not have a unified standard. In this paper, we propose SpeechEQ, a framework for unifying SER tasks based on a multi-scale unified metric. This metric can be trained by Multitask Learning (MTL), which includes two emotion recognition tasks of Emotion States Category (EIS) and Emotion Intensity Scale (EIS), and two auxiliary tasks of phoneme recognition and gender recognition. For this framework, we build a Mandarin SER dataset - SpeechEQ Dataset (SEQD). We conducted experiments on the public CASIA and ESD datasets in Mandarin, which exhibit that our method outperforms baseline methods by a relatively large margin, yielding 8.0% and 6.5% improvement in accuracy respectively. Additional experiments on IEMOCAP with four emotion categories (i.e., angry, happy, sad, and neutral) also show the proposed method achieves a state-of-the-art of both weighted accuracy (WA) of 78.16% and unweighted accuracy (UA) of 77.47%.",True
audibert22_interspeech,https://www.isca-archive.org/interspeech_2022/audibert22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/audibert22_interspeech.html,Intra-speaker phonetic variation in read speech: comparison with inter-speaker variability in a controlled population,Speech Emotion Recognition II,2022,"Our knowledge of speech is historically built on data averaged across speakers or comparing different speakers. We therefore know little about the variability of speech produced by the same speaker: to what extent does it vary from one repetition to another and on what dimensions? In this study, we document the stability of speech and voice characteristics in 9 French speakers, on the reading of two texts recorded over ten sessions on different days over a two-month period. 21 features related to temporal, spectral, f0 and harmonicity aspects as well as their modulation between consecutive chunks are studied. The stability of these features between sessions is evaluated in comparison with their variability between speakers. Results show that short-term variability of energy in the 0-1kHz band, mean F0 and the slope of the LTAS vary the most between sessions for a given speaker, and are also among the speech and voice features that vary the most between speakers in this small cohort, while modulation features between consecutive chunks remain more stable across sessions.",True
aminidigehsara22_interspeech,https://www.isca-archive.org/interspeech_2022/aminidigehsara22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/aminidigehsara22_interspeech.html,A user-friendly headset for radar-based silent speech recognition,"Miscellaneous Topics in Speech, Voice and Hearing Disorders",2022,"Silent speech interfaces allow speech communication to take place in the absence of the acoustic speech signal. Radar-based sensing with radio antennas on the speakers' face can be used as a non-invasive modality to measure speech articulation in such applications. One of the major challenges with this approach is the variability between different sessions, mainly due to the repositioning of the antennas on the face of the speaker. In order to reduce the impact of this influencing factor, we developed a wearable headset that can be 3D-printed with flexible materials and weighs only about 69 g. For evaluation, a radar-based word recognition experiment was performed, where five speakers recorded a speech corpus in multiple sessions, alternatively with the headset and with double-sided tape to place the antennas on the face. By using a bidirectional long short-term memory network for classification, an average intersession word accuracy of 76.50% and 68.18% was obtained using the headset and the tape, respectively. This indicates that the antenna (re-) positioning accuracy with the headset is not worse than that with the double-sided tape while providing other benefits.",True
cheng22c_interspeech,https://www.isca-archive.org/interspeech_2022/cheng22c_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/cheng22c_interspeech.html,A study of production error analysis for Mandarin-speaking Children with Hearing Impairment,"Miscellaneous Topics in Speech, Voice and Hearing Disorders",2022,"Investigating the speech acquisition of hearing-impaired children attracts considerable attentions in recent years. Previous studies that investigate Mandarin-speaking children with hearing impairment mostly focus on production of some specific phonemes. Besides, the phonemes are sometimes embedded in a limited number of speech materials or uttered by only a few speakers. In this study, we analyzed the pronunciation errors of all Mandarin vowels and consonant produced by 60 pre- or post-lingually hearing-impaired children. We designed a set of speech materials that consisted of 153 monosyllable and 145 disyllable commonly used words and had a comprehensive phonetic coverage. The analysis shows that monophthongs were produced less accurately than diphthongs and triphthongs. Bilabials and nasals and plosives were relatively easier for hearing-impaired children to acquire than other consonants with respect to articulation manner and place, respectively. The Mandarin affricates had the lowest accuracy. Substitution is the most frequent error patterns for initial consonants while deletion is the common error for final consonants. The findings of this study can shed light on pronunciation teaching of hearing-impaired children. Besides, the corpus can benefit developing computer-assisted speech assessment system.",True
patel22_interspeech,https://www.isca-archive.org/interspeech_2022/patel22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/patel22_interspeech.html,Using cross-model learnings for the Gram Vaani ASR Challenge 2022,Low-Resource ASR Development II,2022,"In the diverse and multilingual land of India, Hindi is spoken as a first language by a majority of its population. Efforts are made to obtain data in terms of audio, transcriptions, dictionary, etc. to develop speech-technology applications in Hindi. Similarly, the Gram-Vaani ASR Challenge 2022 provides spontaneous telephone speech, with natural back-ground and regional variations in Hindi. The challenge provides: 100 hours of labeled train-set, 5 hours of labeled dev-set and 1000 hours of unlabeled data-set. For the 'Closed Challenge', we trained an End-to-End (E2E) Conformer model using speed perturbations, SpecAugment techniques and use VTLN to handle any unknown speaker groups in the blind evaluation set. On the dev-set, we achieved a 30.3% WER compared to the 34.8% WER by the Challenge E2E baseline. For the 'Self Supervised Closed Challenge', a semi-supervised learning approach is used. We generate pseudo-transcripts for the unlabeled data using a hybrid TDNN-3gram LM model and trained an E2E model. This is then used as a seed for retraining the E2E model with high confidence data. Cross-model learning and refining of the E2E model gave 25.3% WER on the dev-set compared to ~33-35% WER by the Challenge baseline that use wav2vec models.",True
peterson22_interspeech,https://www.isca-archive.org/interspeech_2022/peterson22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/peterson22_interspeech.html,OpenASR21: The Second Open Challenge for Automatic Speech Recognition of Low-Resource Languages,Low-Resource ASR Development II,2022,"In 2021, the National Institute of Standards and Technology (NIST), in cooperation with the Intelligence Advanced Research Project Activity (IARPA), conducted OpenASR21, the second cycle of an open challenge series of automatic speech recognition (ASR) technology for low-resource languages. The OpenASR21 Challenge was offered for 15 low-resource languages. Five of these languages were new in 2021. OpenASR21 also introduced a case-sensitive scoring track on a wider set of data genres for three of the new languages, as a proxy for assessing ASR performance on proper nouns. The paper gives an overview of the challenge setup and results. Fifteen teams from seven countries made at least one required valid submission. 504 submissions were scored. Results show that ASR performance under a severely constrained training condition is still a challenge, with the best Word Error Rate (WER) ranging from 32\\% (Swahili) to 68\\% (Farsi). However, improvements over OpenASR20 were made by augmenting training data with perturbation and text-to-speech techniques along with system combination.",True
suzuki22_interspeech,https://www.isca-archive.org/interspeech_2022/suzuki22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/suzuki22_interspeech.html,Representing 'how you say' with 'what you say': English corpus of focused speech and text reflecting corresponding implications,Emotional Speech Production and Perception,2022,"In speech communication, how something is said (paralinguistic information) is as crucial as what is said (linguistic information). As a type of paralinguistic information, English speech uses sentence stress, the heaviest prominence within a sentence, to convey emphasis. While different placements of sentence stress communicate different emphatic implications, current speech translation systems return the same translations if the utterances are linguistically identical, losing paralinguistic information. Concentrating on focus, a type of emphasis, we propose mapping paralinguistic information into the linguistic domain within the source language using lexical and grammatical devices. This method enables us to translate the paraphrased text representations instead of the transcription of the original speech and obtain translations that preserve paralinguistic information. As a first step, we present the collection of an English corpus containing speech that differed in the placement of focus along with the corresponding text, which was designed to reflect the implied meaning of the speech. Also, analyses of our corpus demonstrated that mapping of focus from the paralinguistic domain into the linguistic domain involved various lexical and grammatical methods. The data and insights from our analysis will further advance research into paralinguistic translation. The corpus will be published via LDC and our website.",True
saito22_interspeech,https://www.isca-archive.org/interspeech_2022/saito22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/saito22_interspeech.html,STUDIES: Corpus of Japanese Empathetic Dialogue Speech Towards Friendly Voice Agent,Spoken Language Processing I,2022,"We present STUDIES, a new speech corpus for developing a voice agent that can speak in a friendly manner. Humans naturally control their speech prosody to empathize with each other. By incorporating this ``empathetic dialogue'' behavior into a spoken dialogue system, we can develop a voice agent that can respond to a user more naturally. We designed the STUDIES corpus to include a speaker who speaks with empathy for the interlocutor's emotion explicitly. We describe our methodology to construct an empathetic dialogue speech corpus and report the analysis results of the STUDIES corpus. We conducted a text-to-speech experiment to initially investigate how we can develop more natural voice agent that can tune its speaking style corresponding to the interlocutor's emotion. The results show that the use of interlocutor's emotion label and conversational context embedding can produce speech with the same degree of naturalness as that synthesized by using the agent's emotion label. Our project page of the STUDIES corpus is \\url{http://sython.org/Corpus/STUDIES}.",True
rumberg22_interspeech,https://www.isca-archive.org/interspeech_2022/rumberg22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/rumberg22_interspeech.html,kidsTALC: A Corpus of 3- to 11-year-old German Childrenâs Connected Natural Speech,Spoken Language Processing I,2022,"In this paper we present kidsTALC an audio dataset with orthographic and phonetic transcriptions of German children's speech collected to facilitate the development of speech based technological solutions. The dataset is part of a larger project aiming to develop machine-learning applications to support automation in child speech and language assessment for research and clinical purposes. At the same time, the interdisciplinary project was established to increase the accessibility of corpora of continuous child speech in Germany and globally to train accurate automated speech recognition tools for children. In the first stage we collected and transcribed 25 hours of continuous speech from typically developing children aged 3 Â½â11 years. Here, we discuss the key features of the dataset, data collection, transcription protocol and future datasets in the project. We also present important statistics of our dataset and will demonstrate the speech recognition performance of one baseline model on the dataset.",True
lin22c_interspeech,https://www.isca-archive.org/interspeech_2022/lin22c_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/lin22c_interspeech.html,DUAL: Discrete Spoken Unit Adaptive Learning for Textless Spoken Question Answering,Spoken Language Processing I,2022,"Spoken Question Answering (SQA) is to find the answer from a spoken document given a question, which is crucial for personal assistants when replying to the queries from the users. Existing SQA methods all rely on Automatic Speech Recognition (ASR) transcripts. Not only does ASR need to be trained with massive annotated data that are time and cost-prohibitive to collect for low-resourced languages, but more importantly, very often the answers to the questions include name entities or out-of-vocabulary words that cannot be recognized correctly. Also, ASR aims to minimize recognition errors equally over all words, including many function words irrelevant to the SQA task. Therefore, SQA without ASR transcripts (textless) is always highly desired, although known to be very difficult. This work proposes Discrete Spoken Unit Adaptive Learning (DUAL), leveraging unlabeled data for pre-training and fine-tuned by the SQA downstream task. The time intervals of spoken answers can be directly predicted from spoken documents. We also release a new SQA benchmark corpus, NMSQA, for data with more realistic scenarios. We empirically showed that DUAL yields results comparable to those obtained by cascading ASR and text QA model and robust to real-world data. Our code and model will be open-sourced.",True
nguyen22c_interspeech,https://www.isca-archive.org/interspeech_2022/nguyen22c_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/nguyen22c_interspeech.html,Building Vietnamese Conversational Smart Home Dataset and Natural Language Understanding Model,Spoken Language Processing I,2022,"Natural Language Understanding (NLU), which includes intent detection and slot tagging, plays an important role in any dialog system. This paper aims at building a first-ever conversational smart home dataset SmartNLU and NLU model for Vietnamese. Raw data were collected by asking participants provide or confirm the intents of and slot values in the user says that they sent or received in a smart home conversation until all were matched, using a Wizard-of-Oz set-up of a web tool. The data were then cleaned and processed to build templates of user says with empty slots. The entity strategy, which filled all slot values by the round-robin algorithm to templates, was empirically chosen to generate user says from collected templates, which made a total of 3,492/1,176/1,198 user says correspondingly for the training/validating/test sets. The dataset has been released for a challenge carried out in AIHub, and published for the community. Several state-of-the-art joint NLU models were experimented on the released dataset. The proposed NLU model, which added PhoBERT to the DIET architecture of Rasa framework, gave the best results. The sentence accuracy of the DIET+PhoBERT was considerably higher than (i.e. 4.3% to 11.7%) the one of others.",True
ghosh22b_interspeech,https://www.isca-archive.org/interspeech_2022/ghosh22b_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/ghosh22b_interspeech.html,DeToxy: A Large-Scale Multimodal Dataset for Toxicity Classification in Spoken Utterances,Spoken Language Processing I,2022,"Toxic speech is regarded as one of the crucial issues plaguing online social media today. Most recent work on toxic speech detection is constrained to the modality of text and written conversations with very limited work on toxicity detection from spoken utterances or using the modality of speech. In this paper, we introduce a new dataset DeToxy, the first publicly available toxicity annotated dataset for the English language. DeToxy is sourced from various openly available speech databases and consists of over 2 million utterances. We believe that our dataset would act as a benchmark for the relatively new and unexplored Spoken Language Processing (SLP) task of detecting toxicity from spoken utterances and boost further research in this space. Finally, we also provide strong unimodal baselines for our dataset and compare traditional two-step cascade and End-to-End (E2E) approaches. Our experiments show that in the case of spoken utterances, text-based approaches are largely dependent on gold human-annotated transcripts for their performance and also suffer from the problem of keyword bias. However, the presence of speech files in DeToxy helps facilitates the development of E2E speech models which alleviate both the above-stated problems by better capturing speech clues.",True
huang22l_interspeech,https://www.isca-archive.org/interspeech_2022/huang22l_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/huang22l_interspeech.html,QbyE-MLPMixer: Query-by-Example Open-Vocabulary Keyword Spotting using MLPMixer,Spoken Language Processing I,2022,"Current keyword spotting systems are typically trained with a large amount of pre-defined keywords. Recognizing keywords in an open-vocabulary setting is essential for personalizing smart device interaction. Towards this goal, we propose a pure MLP-based neural network that is based on MLPMixer - an MLP model architecture that effectively replaces the attention mechanism in Vision Transformers. We investigate different ways of adapting the MLPMixer architecture to the QbyE open-vocabulary keyword spotting task. Comparisons with the state-of-the-art RNN and CNN models show that our method achieves better performance in challenging situations (10dB and 6dB environments) on both the publicly available Hey-Snips dataset and a larger scale internal dataset with 400 speakers. Our proposed model also has a smaller number of parameters and MACs compared to the baseline models.",True
paul22_interspeech,https://www.isca-archive.org/interspeech_2022/paul22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/paul22_interspeech.html,Improving Data Driven Inverse Text Normalization using Data Augmentation and Machine Translation,Show and Tell IV,2022,"Inverse text normalization (ITN) is used to convert the spoken form output of an automatic speech recognition (ASR) system to a written form. Traditional handcrafted ITN rules can be complex to transcribe and maintain. Meanwhile neural modeling approaches require quality large-scale spoken-written pair exam ples in the same or similar domain as the ASR system (in-domain data), to train. Both these approaches require costly and complex annotation. In this paper, we present a data augmentation tech nique with neural machine translation that effectively generates rich spoken-written pairs for high and low resource languages effectively. We empirically demonstrate that ITN models (in tar get language) trained using our data augmentation with machine translation technique can achieve similar performance as ITN models (en) trained directly with in-domain language.",True
wang22c_interspeech,https://www.isca-archive.org/interspeech_2022/wang22c_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/wang22c_interspeech.html,Disentangling the Impacts of Language and Channel Variability on Speech Separation Networks,Source Separation III,2022,"Because the performance of speech separation is excellent for speech in which two speakers completely overlap, research attention has been shifted to dealing with more realistic scenarios. However, domain mismatch between training/test situations due to factors, such as speaker, content, channel, and environment, remains a severe problem for speech separation. Speaker and environment mismatches have been studied in the existing literature. Nevertheless, there are few studies on speech content and channel mismatches. Moreover, the impacts of language and channel in these studies are mostly tangled. In this study, we create several datasets for various experiments. The results show that the impacts of different languages are small enough to be ignored compared to the impacts of different channels. In our experiments, training on data recorded by Android phones leads to the best generalizability. Moreover, we provide a new solution for channel mismatch by evaluating projection, where the channel similarity can be measured and used to effectively select additional training data to improve the performance of in-the-wild test data.",True
lu22c_interspeech,https://www.isca-archive.org/interspeech_2022/lu22c_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/lu22c_interspeech.html,"ESPnet-SE++: Speech Enhancement for Robust Speech Recognition, Translation, and Understanding",Speech Enhancement and Intelligibility,2022,"This paper presents recent progress on integrating speech separation and enhancement (SSE) into the ESPnet toolkit. Compared with the previous ESPnet-SE work, numerous features have been added, including recent state-of-the-art speech enhancement models with their respective training and evaluation recipes. Importantly, a new interface has been designed to flexibly combine speech enhancement front-ends with other tasks, including automatic speech recognition (ASR), speech translation (ST), and spoken language understanding (SLU). To showcase such integration, we performed experiments on carefully designed synthetic datasets for noisy-reverberant multi-channel ST and SLU tasks, which can be used as benchmark corpora for future research. In addition to these new tasks, we also use CHiME-4 and WSJ0-2Mix to benchmark multi- and single-channel SE approaches. Results show that the integration of SE front-ends with back-end tasks is a promising research direction even for tasks besides ASR, especially in the multi-channel scenario. The code is available online at \\url{https://github.com/ESPnet/ESPnet}. The multi-channel ST and SLU datasets, which are another contribution of this work, are released on HuggingFace.",True
fernandez22_interspeech,https://www.isca-archive.org/interspeech_2022/fernandez22_interspeech.pdf,https://www.isca-archive.org/interspeech_2022/fernandez22_interspeech.html,Transplantation of Conversational Speaking Style with Interjections in Sequence-to-Sequence Speech Synthesis,"Speech Synthesis: Speaking Style, Emotion and Accents II",2022,"Sequence-to-Sequence Text-to-Speech architectures that directly generate low level acoustic features from phonetic sequences are known to produce natural and expressive speech when provided with adequate amounts of training data. Such systems can learn and transfer desired speaking styles from one seen speaker to another (in multi-style multi-speaker settings), which is highly desirable for creating scalable and customizable Human-Computer Interaction systems. In this work we explore one-to-many style transfer from a dedicated single-speaker conversational corpus with style nuances and interjections. We elaborate on the corpus design and explore the feasibility of such style transfer when assisted with Voice-Conversion-based data augmentation. In a set of subjective listening experiments, this approach resulted in high-fidelity style transfer with no quality degradation. However, a certain voice persona shift was observed, requiring further improvements in voice conversion.",True
xin23b_interspeech,https://www.isca-archive.org/interspeech_2023/xin23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/xin23b_interspeech.html,Laughter Synthesis using Pseudo Phonetic Tokens with a Large-scale In-the-wild Laughter Corpus,Speech Synthesis: Prosody and Emotion,2023,"We present a large-scale in-the-wild Japanese laughter corpus and a laughter synthesis method. Previous work on laughter synthesis lacks not only data but also proper ways to represent laughter. To solve these problems, we first propose an in-the-wild corpus comprising 3.5 hours of laughter, which is to our best knowledge the largest laughter corpus designed for laughter synthesis. We then propose pseudo phonetic tokens (PPTs) to represent laughter by a sequence of discrete tokens, which are obtained by training a clustering model on features extracted from laughter by a pretrained self-supervised model. Laughter can then be synthesized by feeding PPTs into a text-to-speech system. We further show PPTs can be used to train a language model for unconditional laughter generation. Results of comprehensive subjective and objective evaluations demonstrate that the proposed method significantly outperforms a baseline method, and can generate natural laughter unconditionally.",True
song23_interspeech,https://www.isca-archive.org/interspeech_2023/song23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/song23_interspeech.html,StyleS2ST: Zero-shot Style Transfer for Direct Speech-to-speech Translation,Statistical Machine Translation,2023,"Direct speech-to-speech translation (S2ST) has gradually become popular as it has many advantages compared with cascade S2ST. However, current research mainly focuses on the accuracy of semantic translation and ignores the speech style transfer from a source language to a target language. The lack of high-fidelity expressive parallel data makes such style transfer challenging, especially in more practical zero-shot scenarios. To solve this problem, we first build a parallel corpus using a multi-lingual multi-speaker text-to-speech synthesis (TTS) system and then propose the StyleS2ST model with cross-lingual speech style transfer ability based on a style adaptor on a direct S2ST system framework. Enabling continuous style space modeling of an acoustic model through parallel corpus training and non-parallel TTS data augmentation, StyleS2ST captures cross-lingual acoustic feature mapping from the source to the target language. Experiments show that StyleS2ST achieves good style similarity and naturalness in both in-set and out-of-set zero-shot scenarios.",True
repp23_interspeech,https://www.isca-archive.org/interspeech_2023/repp23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/repp23_interspeech.html,Alignment of Beat Gestures and Prosodic Prominence in German,Prosody,2023,"We present evidence on the alignment of beat gestures and prosodic prominence from a video corpus consisting of six German educational videos for students from six presenters. Our analysis of 120 beat gestures (with a substantial variety of hand shapes) shows that beat gestures almost always align with prosodically prominent syllables, i.e., syllables carrying a pitch accent. Specifically, the stroke always starts before, or - more often - on, a pitch-accented syllable; the apex mostly falls on the accented syllable (74%) but may also occur in subsequent syllables. The degree of prosodic prominence of the accented syllable (in terms of DIMA-prominence levels) is predictive for the position of the apex, which occurs within rather than after the accented syllable more often for higher degrees of prominence. These findings provide new insights into the alignment of prominence-lending features of prosody and gesture, thereby broadening the empirical landscape for beat gestures.",True
behera23_interspeech,https://www.isca-archive.org/interspeech_2023/behera23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/behera23_interspeech.html,Towards Multi-Lingual Audio Question Answering,Analysis of Speech and Audio Signals 1,2023,"Audio Question Answering (AQA) is a multi-modal translation task where a system analyzes an audio signal and a natural language question to generate a desirable natural language answer. AQA has been primarily studied through the lens of the English language. However, addressing AQA in other languages, in the same manner, would require a considerable amount of resources. This paper proposes scalable solutions to multi-lingual audio question answering on both data and modeling fronts. We propose mClothoAQA, a translation-based multi-lingual AQA dataset in eight languages. The dataset consists of 1991 audio files and nearly 0.3 million question-answer pairs. Finally, we introduce a multi-lingual AQA model and demonstrate its strong performance in eight languages. The dataset and code can be accessed at https://github.com/swarupbehera/mAQA.",True
polacek23_interspeech,https://www.isca-archive.org/interspeech_2023/polacek23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/polacek23_interspeech.html,Online Punctuation Restoration using ELECTRA Model for streaming ASR Systems,Speech Recognition: Technologies and Systems for New Applications 1,2023,"In this work, we propose a lightweight online approach to automatic punctuation restoration (APR), which can be utilized in speech transcription systems for, e.g., live captioning TV or radio streams. It uses only text input without prosodic features and a fine-tuned ELECTRA-Small model with a two-layer classification head. It allows for restoring question marks, commas, and periods with a very short inference time and a low latency of just three words. Our APR scheme is first tuned and compared to other architectures on a set of manual TV news transcripts. The resulting system is then compared to another real-time APR module utilizing a recurrent network and a combination of text and acoustic features. The test data we use contains automatic transcripts of radio talks and TV debates; we are also publishing this data. The results show that our APR module performs better than the above-mentioned system and yields on the two test sets an average F1 of 71.2% and 69.4%, respectively.",True
dasare23_interspeech,https://www.isca-archive.org/interspeech_2023/dasare23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/dasare23_interspeech.html,The Role of Formant and Excitation Source Features in Perceived Naturalness of Low Resource Tribal Language TTS: An Empirical Study,Speech Quality Assessment,2023,"Text-to-speech synthesis is a prominent area in the speechprocessing domain that has significant use in reading digital content in a given language. In the proposed work, we worked on two tribal languages of India viz., Lambani and Soliga, which are zero-resource languages. The study began with a dataset collection for both tribal languages. Secondly, a Text-To-Speech (TTS) system was built separately based on the transfer learning approach. To validate the voice quality of TTS-generated speech, subjective as well as objective evaluations were performed. As a part of objective analysis, the voice source and vocal tract filter properties of the synthetic speech have been explored. The extensive study on various aspects of speech, such as LP residual, F0 contour, and formants (F1 &  sF2) has shown interesting results that can correlate to the subjective listening test results. The link to the original and synthetic speech can be found online.",True
ding23_interspeech,https://www.isca-archive.org/interspeech_2023/ding23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/ding23_interspeech.html,Stable Speech Emotion Recognition with Head-k-Pooling Loss,Speech Emotion Recognition 1,2023,"Speech emotion recognition (SER) aims to detect the emotion of the speaker involved in a given utterance. Most existing SER methods focus on local speech features by stacking convolutions and training all segments of an utterance with an utterance-level label. Two deficiencies exist in these methods: i) learning only local speech features may be insufficient for SER due to the ambiguity of emotions; ii) consistent supervision of each segment may lead to label error propagation, as the true emotions of some segments may not match the utterance label. To solve the two issues, we first devise a global-local fusion network to model both long- and short-range relations in speech. Second, we tailor a novel head-k-pooling loss for SER tasks, which dynamically assigns labels for each segment and selectively performs loss calculation across segments. We test our method on the IEMOCAP and the newly collected ST-EMO dataset, and the results show its superiority and stability.",True
bujnowski23_interspeech,https://www.isca-archive.org/interspeech_2023/bujnowski23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/bujnowski23_interspeech.html,"""Select language, modality or put on a mask!"" Experiments with Multimodal Emotion Recognition",Show and Tell: Health applications and emotion recognition,2023,"We propose a system designed for multimodal emotion recognition. Our research focuses on showing the impact of various signals in the emotion recognition process. Apart from reporting the average results of our models, we would like to encourage individual engagement of conference participants and explore how a unique emotional scene recorded on the spot can be interpreted by the models - for individual modalities as well as their combinations. Our models work for English, German and Korean. We show the comparison of emotion recognition accuracy for these 3 languages, including the influence of each modality. Our second experiment explores emotion recognition for people wearing face masks. We show that the use of face masks affects not only the video signal but also audio and text. To our knowledge, no other study shows the effects of wearing a mask for three modalities. Unlike other studies where masks are added artificially, we use real recordings with actors in masks.",True
ray23_interspeech,https://www.isca-archive.org/interspeech_2023/ray23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/ray23_interspeech.html,Compositional Generalization in Spoken Language Understanding,Spoken Dialog Systems and Conversational Analysis 1,2023,"State-of-the-art spoken language understanding (SLU) models have shown tremendous success in benchmark SLU datasets, yet they still fail in many practical scenario due to the lack of model compositionality when trained on limited training data. In this paper, we study two types of compositionality: novel slot combination, and length generalization. We first conduct in-depth analysis, and find that state-of-the-art SLU models often learn spurious slot correlations during training, which leads to poor performance in both compositional cases. To mitigate these limitations, we create the first compositional splits of benchmark SLU datasets and we propose the first compositional SLU model, including compositional loss and paired training that tackle each compositional case respectively. On both benchmark and compositional splits in ATIS and SNIPS, we show that our compositional SLU model significantly outperforms (up to 5% F1 score) state-of-the-art BERT SLU model.",True
zhang23r_interspeech,https://www.isca-archive.org/interspeech_2023/zhang23r_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/zhang23r_interspeech.html,Real-Time Personalised Speech Enhancement Transformers with Dynamic Cross-attended Speaker Representations,Speech Coding and Enhancement 1,2023,"Personalised speech enhancement (PSE) extracts only the speech of a target user and removes everything else from corrupted input audio. This can greatly improve on-device streaming audio processing, such as voice calls and speech recognition, which has strict requirements on model size and latency. To focus the PSE system on the target speaker, it is conditioned on a recording of the user's voice. This recording is usually summarised as a single static vector. However, a static vector cannot reflect all the target user's voice characteristics. Thus, we propose using the full recording. To condition on such a variable-length sequence, we propose fully Transformer-based PSE models with a cross-attention mechanism which generates target speaker representations dynamically. To better reflect the on-device scenario, we carefully design and publish a new PSE dataset. On the dataset, our proposed model significantly surpasses strong baselines while halving the model size and reducing latency.",True
shi23g_interspeech,https://www.isca-archive.org/interspeech_2023/shi23g_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/shi23g_interspeech.html,ML-SUPERB: Multilingual Speech Universal PERformance Benchmark,"Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 2",2023,"Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.",True
fu23b_interspeech,https://www.isca-archive.org/interspeech_2023/fu23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/fu23b_interspeech.html,OTF: Optimal Transport based Fusion of Supervised and Self-Supervised Learning Models for Automatic Speech Recognition,"Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 2",2023,"Self-Supervised Learning (SSL) Automatic Speech Recognition (ASR) models have shown great promise over Supervised Learning (SL) ones in low-resource settings. However, the advantages of SSL are gradually weakened when the amount of labeled data increases in many industrial applications. To further improve the ASR performance when abundant labels are available, we first explore the potential of combining SL and SSL ASR models via analyzing their complementarity in recognition accuracy and optimization property. Then, we propose a novel Optimal Transport based Fusion (OTF) method for SL and SSL models without incurring extra computation cost in inference. Specifically, optimal transport is adopted to softly align the layer-wise weights to unify the two different networks into a single one. Experimental results on the public 1k-hour English LibriSpeech dataset and our in-house 2.6k-hour Chinese dataset show that OTF largely outperforms the individual models with lower error rates.",True
fu23_interspeech,https://www.isca-archive.org/interspeech_2023/fu23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/fu23_interspeech.html,Phonetic and Prosody-aware Self-supervised Learning Approach for Non-native Fluency Scoring,Speech Recognition: Technologies and Systems for New Applications 2,2023,"Speech fluency/disfluency can be evaluated by analyzing a range of phonetic and prosodic features. Deep neural networks are commonly trained to map fluency-related features into the human scores. However, the effectiveness of deep learning-based models is constrained by the limited amount of labeled training samples. To address this, we introduce a self-supervised learning (SSL) approach that takes into account phonetic and prosody awareness for fluency scoring. Specifically, we first pre-train the model using a reconstruction loss function, by masking phones and their durations jointly on a large amount of unlabeled speech and text prompts. We then fine-tune the pre-trained model using human-annotated scoring data. Our experimental results, conducted on datasets such as Speechocean762 and our non-native datasets, show that our proposed method outperforms the baseline systems in terms of Pearson correlation coefficients (PCC). Moreover, we also conduct an ablation study to better understand the contribution of phonetic and prosody factors during the pre-training stage.",True
shekar23b_interspeech,https://www.isca-archive.org/interspeech_2023/shekar23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/shekar23b_interspeech.html,Assessment of Non-Native Speech Intelligibility using Wav2vec2-based Mispronunciation Detection and Multi-level Goodness of Pronunciation Transformer,Speech Recognition: Technologies and Systems for New Applications 2,2023,"Automatic pronunciation assessment (APA) plays an important role in providing feedback for self-directed language learners in computer-assisted pronunciation training (CAPT). Several mispronunciation detection and diagnosis (MDD) systems have achieved promising performance based on end-to-end phoneme recognition. However, assessing the intelligibility of second language (L2) remains a challenging problem. One issue is the lack of large-scale labeled speech data from non-native speakers. Additionally, relying only on one aspect (e.g., accuracy) at a phonetic level may not provide a sufficient assessment of pronunciation quality and L2 intelligibility. It is possible to leverage segmental/phonetic-level features such as goodness of pronunciation (GOP), however, feature granularity may cause a discrepancy in prosodic-level (suprasegmental) pronunciation assessment. In this study, Wav2vec 2.0-based MDD and Goodness Of Pronunciation feature-based Transformer are employed to characterize L2 intelligibility. Here, an L2 speech dataset, with human-annotated prosodic (suprasegmental) labels, is used for multi-granular and multi-aspect pronunciation assessment and identification of factors important for intelligibility in L2 English speech. The study provides a transformative comparative assessment of automated pronunciation scores versus the relationship between suprasegmental features and listener perceptions, which taken collectively can help support the development of instantaneous assessment tools and solutions for L2 learners.",True
huu23_interspeech,https://www.isca-archive.org/interspeech_2023/huu23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/huu23_interspeech.html,"Mispronunciation detection and diagnosis model for tonal language, applied to Vietnamese",Speech Recognition: Technologies and Systems for New Applications 2,2023,"A tonal language is a language in which the meaning of words is not only determined by the sounds of the consonants and vowels, but also by the pitch or tone used to pronounce them. Mispronunciation Detection and Diagnosis (MD&D) of tonal languages is challenging since tone presentation is difficult to be detected correctly. There has been relatively little research conducted on tonal languages, with most focusing on Mandarin. Furthermore, there are no publicly available datasets and source codes for the task. This work constructs and publishes a Vietnamese dataset for experimenting with MD&D, as well as proposes an end-to-end model that utilizes pitch analysis to detect and diagnose mispronunciations for tonal languages, especially focusing on Vietnamese. Experiments show that the proposed model achieved a relative improvement in phone error rate of 7.1% and detection accuracy of 7.4% compared to a state-of-the-art baseline.",True
stemmer23_interspeech,https://www.isca-archive.org/interspeech_2023/stemmer23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/stemmer23_interspeech.html,Detection of Emotional Hotspots in Meetings Using a Cross-Corpus Approach,Paralinguistics 1,2023,"Speech emotion recognition for natural human-to-human conversations has many useful applications, including generating comprehensive meeting transcripts or detecting communication problems. We investigate the detection of emotional hotspots, i.e., regions of increased speaker involvement in technical meetings. As there is a scarcity of annotated, not-acted corpora, and to avoid introducing unwanted biases to our models, we follow a cross-corpus approach where models are trained on data from domains unrelated to the test data. In this work we propose a model ensemble trained on spontaneous phone conversations, political discussions and acted emotions. Evaluation is performed on the natural ICSI and AMI meeting corpora, where we used existing hotspot annotations for ICSI and created labels for the AMI corpus. A semi-supervised fine-tuning procedure is introduced to adapt the model. We show that an equal error rate of below 21% can be achieved using the proposed cross-corpus approach.",True
li23e_interspeech,https://www.isca-archive.org/interspeech_2023/li23e_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/li23e_interspeech.html,Towards Robust Family-Infant Audio Analysis Based on Unsupervised Pretraining of Wav2vec 2.0 on Large-Scale Unlabeled Family Audio,Paralinguistics 1,2023,"To perform automatic family audio analysis, past studies have collected recordings using phone, video, or audio-only recording device like LENA, investigated supervised learning methods, and used or fine-tuned general-purpose embeddings learned from large pretrained models. In this study, we advance the audio component of a new infant wearable multi-modal device called LittleBeats (LB) by learning family audio representation via wav2vec 2.0 (W2V2) pretraining. We show given a limited number of labeled LB home recordings, W2V2 pretrained using 1k-hour of unlabeled home recordings outperforms oracle W2V2 pretrained on 52k-hour unlabeled audio in terms of parent/infant speaker diarization (SD) and vocalization classifications (VC) at home. Extra relevant external unlabeled and labeled data further benefit W2V2 pretraining and fine-tuning. With SpecAug and environmental speech corruptions, we obtain 12% relative gain on SD and moderate boost on VC. Code and model weights are available.",True
zhang23b_interspeech,https://www.isca-archive.org/interspeech_2023/zhang23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/zhang23b_interspeech.html,Obstructive Sleep Apnea Detection using Pre-trained Speech Representations,Biosignal-enabled Spoken Communication,2023,"Obstructive sleep apnea (OSA) is a condition commonly affecting middle-aged men that can disturb sleep, cause daytime tiredness, and increase the risk of heart disease. Speech can serve as a valuable biomarker for identifying and predicting the severity of OSA due to its connection with changes in throat structure. This study proposes a new deep-learning-based method for detecting OSA by analyzing speech recordings of participants in sitting and lying positions. The method utilizes a Siamese structure that employs a pre-trained XLSR model to encode ten utterances for each position, reducing the amount of necessary training data and enabling comparison of throat structure changes between the two positions through voice analysis. The study also explores the use of patient characteristic features. Results show this approach achieves an F1 value of 0.725 on our in-house dataset, proving the feasibility of end-to-end speech OSA detection with foundation models.",True
yang23r_interspeech,https://www.isca-archive.org/interspeech_2023/yang23r_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/yang23r_interspeech.html,Auditory Attention Detection in Real-Life Scenarios Using Common Spatial Patterns from EEG,Biosignal-enabled Spoken Communication,2023,"Auditory attention detection (AAD) methods based on electroencephalography (EEG) could be used in neuro-steered hearing devices to help hearing-loss people improve their hearing ability. However, previous studies have mostly obtained EEG data in laboratory settings which limits the practical application of neuro-steered hearing devices. In this study, we employ a common spatial pattern (CSP) algorithm to perform AAD using EEG signals collected by a wireless mobile EEG system, from real-life scenarios when people are walking and sitting. The results show that the CSP method can achieve AAD accuracy between 81.3% and 87.5% when using different decision windows (1 s- 30 s), which is better than previous methods based on linear mapping methods and convolutional neural networks (CNN). This proves that the CSP algorithm can decode people's attention efficiently even outside the laboratory. Analysis of EEG frequency bands shows that the Î´ and Î² bands have high activity in attention tasks.",True
neumann23b_interspeech,https://www.isca-archive.org/interspeech_2023/neumann23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/neumann23b_interspeech.html,"A Multimodal Investigation of Speech, Text, Cognitive and Facial Video Features for Characterizing Depression With and Without Medication",DiGo - Dialog for Good: Speech and Language Technology for Social Good,2023,"Clinical depression is one of the most common mental disorders and technology for remote assessment of depression, including monitoring of treatment responses, is gaining more and more importance. Using a cloud-based multimodal dialog platform, we conducted a crowdsourced study to investigate the effect of depression severity and antidepressant use on various acoustic, linguistic, cognitive, and orofacial features. Our findings show that multiple features from all tested modalities show statistically significant differences between subjects with no or minimal depression and subjects with more severe depression symptoms. Moreover, certain acoustic and visual features show significant differences between subjects with moderately severe or severe symptoms who take antidepressants and those who do not take any. Machine learning experiments show that subjects with and without medication can be better discriminated from each other at higher severity levels.",True
addlesee23_interspeech,https://www.isca-archive.org/interspeech_2023/addlesee23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/addlesee23_interspeech.html,Understanding Disrupted Sentences Using Underspecified Abstract Meaning Representation,DiGo - Dialog for Good: Speech and Language Technology for Social Good,2023,"Voice assistant accessibility is generally overlooked as today's spoken dialogue systems are trained on huge corpora to help them understand the 'average' user. This raises frustrating barriers for certain user groups as their speech shifts from the average. People with dementia pause more frequently mid-sentence for example, and people with hearing impairments may mispronounce words learned post-diagnosis. We explore whether semantic parsing can improve accessibility for people with non-standard speech, and consequently become more robust to external disruptions like dogs barking, sirens passing, or doors slamming mid-utterance. We generate corpora of disrupted sentences paired with their underspecified Abstract Meaning Representation (AMR) graphs, and use these to train pipelines to understand and repair disruptions. Our best disruption recovery pipeline lost only 1.6% graph similarity f-score when compared to a model given the full original sentence.",True
chi23_interspeech,https://www.isca-archive.org/interspeech_2023/chi23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/chi23_interspeech.html,Unsupervised Code-switched Text Generation from Parallel Text,"Speech Recognition: Architecture, Search, and Linguistic Components 2",2023,"There has been great interest in developing automatic speech recognition (ASR) systems that can handle code-switched (CS) speech to meet the needs of a growing bilingual population. However, existing datasets are limited in size. It is expensive and difficult to collect real transcribed spoken CS data due to the challenges of finding and identifying CS data in the wild. As a result, many attempts have been made to generate synthetic CS data. Existing methods either require the existence of CS data during training, or are driven by linguistic knowledge. We introduce a novel approach of forcing a multilingual MT system that was trained on non-CS data to generate CS translations. Comparing against two prior methods, we show that simply leveraging the shared representations of two languages (Mandarin and English) yields better CS text generation and, ultimately, better CS ASR.",True
shekar23_interspeech,https://www.isca-archive.org/interspeech_2023/shekar23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/shekar23_interspeech.html,Speaker Tracking using Graph Attention Networks with Varying Duration Utterances across Multi-Channel Naturalistic Data: Fearless Steps Apollo-11 Audio Corpus,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 1",2023,"Speaker tracking in spontaneous naturalistic data continues to be a major research challenge, especially for short turn-taking communications. The NASA Apollo-11 space mission brought astronauts to the moon and back, where team based voice communications were captured. Building robust speaker classification models for this corpus has significant challenges due to variability of speaker turns, imbalanced speaker classes, and time-varying background noise/distortions. This study proposes a novel approach for speaker classification and tracking, utilizing a graph attention network framework that builds upon pretrained speaker embeddings. The modelâs robustness is evaluated on a number of speakers (10-140), achieving classification accuracy of 90.78% for 10 speakers, and 79.86% for 140 speakers. Furthermore, a secondary investigation focused on tracking speakers-of-interest(SoI) during mission critical phases, essentially serves as a lasting tribute to the 'Heroes Behind the Heroes'.",True
yan23_interspeech,https://www.isca-archive.org/interspeech_2023/yan23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/yan23_interspeech.html,Combining language corpora in a Japanese electromagnetic articulography database for acoustic-to-articulatory inversion,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 1",2023,"This paper presents an electromagnetic articulography database of Japanese sentences. The database includes aligned acoustics and articulatory data from seven males and three females, with a total of five recorded hours. The database is now in preparation for public release to further research in areas of acoustic-to-articulatory inversion, brain-machine interface communication systems, artificial speech synthesis, and dialect recognition. Moreover, based on this database we established an acoustic-to-articulatory inversion system using a deep, bidirectional, long short-term memory recurrent neural network structure. The results showed that, for the Japanese language, adding English corpora to the training was not beneficial for this speaker-independent model.",True
chivriga23_interspeech,https://www.isca-archive.org/interspeech_2023/chivriga23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/chivriga23_interspeech.html,Large Dataset Generation of Synchronized Music Audio and Lyrics at Scale using Teacher-Student Paradigm,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 1",2023,"Large models (e.g., GPT-3, CLIP, DALL-E) show remarkable few-shot and zero-shot capabilities when trained on hundreds of millions of samples. Despite this trend, no publicly available synchronized music audio and lyrics dataset of sufficient scale exists, nor does a reliable evaluation benchmark to assess a model's performance. To address this issue, we build and release MusicLyric, a large public dataset with over 320k audio sequences and lyrics pairs for a total duration of 1,200 hours based on a collection of over 32,000 songs. The generation process is based on the teacher-student paradigm where the student seeks to outclass the teacher with more data available using the newly generated pseudo-alignments. The method is efficient and straightforward with at least 3 iterations needed to create high-quality data that can be scaled to a hundred thousand samples. We make our dataset, toolkit, and pre-trained models open-source.",True
gonzalezmachorro23_interspeech,https://www.isca-archive.org/interspeech_2023/gonzalezmachorro23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/gonzalezmachorro23_interspeech.html,Towards Supporting an Early Diagnosis of Multiple Sclerosis using Vocal Features,"Speech, Voice, and Hearing Disorders 1",2023,"Multiple sclerosis (MS) is a neuroinflammatory disease that affects millions of people worldwide. Since dysarthria is prominent in people with MS (pwMS), this paper aims to identify acoustic features that differ between people with MS and healthy controls (HC). Additionally, we develop automatic classification methods to distinguish between pwMS and HC. In this work, we present a new dataset of a German-speaking cohort which contains 39 patients with low disability of relapsing MS and 16 HC. Findings suggest that certain interpretable speech features could be useful in diagnosing MS, and that machine learning methods could potentially support fast and unobtrusive screening in clinical practice. The study emphasises the importance of analysing free speech compared to read speech.",True
hedeshy23_interspeech,https://www.isca-archive.org/interspeech_2023/hedeshy23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/hedeshy23_interspeech.html,CNVVE: Dataset and Benchmark for Classifying Non-verbal Voice,"Speech, Voice, and Hearing Disorders 1",2023,"Non-verbal voice expressions (NVVEs) have been adopted as a means of human-computer interaction in research studies. However, exploring non-verbal voice-based interactions has been constrained by the limited availability of suitable training data and computational methods for classifying such expressions, leading to a focus on simple binary inputs. We address this issue with a new dataset containing 950 audio samples comprising 6 classes of voice expressions. The data were collected from 42 speakers who donated voice recordings. The classifier was trained on the data using features derived from mel-spectrograms. Furthermore, we studied the effectiveness of data augmentation and improved over the baseline model accuracy significantly with a test accuracy of 96.6% in a 5-fold cross-validation. We have made CNVVE publicly accessible in the hope that it will serve as a benchmark for future research.",True
chang23c_interspeech,https://www.isca-archive.org/interspeech_2023/chang23c_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/chang23c_interspeech.html,Multimodal Speech Recognition for Language-Guided Embodied Agents,Speech Recognition: Technologies and Systems for New Applications 3,2023,"Benchmarks for language-guided embodied agents typically assume text-based instructions, but deployed agents will encounter spoken instructions. While Automatic Speech Recognition (ASR) models can bridge the input gap, erroneous ASR transcripts can hurt the agentsâ ability to complete tasks. We propose training a multimodal ASR model that utilizes the accompanying visual context to reduce errors in spoken instruction transcripts. We train our model on a dataset of synthetic spoken instructions, derived from the ALFRED household task dataset, where we simulate acoustic noise by systematically masking spoken words. We find that utilizing visual observations facilitates masked word recovery, with multimodal ASR models recovering up to 30% more masked words than unimodal baselines. We also find that spoken instructions transcribed by multimodal ASR models result in higher task completion success rates for a language-guided embodied agent. github.com/Cylumn/embodied-multimodal-asr",True
yang23y_interspeech,https://www.isca-archive.org/interspeech_2023/yang23y_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/yang23y_interspeech.html,On-Device Constrained Self-Supervised Speech Representation Learning for Keyword Spotting via Knowledge Distillation,Spoken Term Detection and Voice Search,2023,"Large self-supervised models are effective feature extractors, but their application is challenging under on-device budget constraints and biased dataset collection, especially in keyword spotting. To address this, we proposed a knowledge distillation-based self-supervised speech representation learning (S3RL) architecture for on-device keyword spotting. Our approach used a teacher-student framework to transfer knowledge from a larger, more complex model to a smaller, light-weight model using dual-view cross-correlation distillation and the teacher's codebook as learning objectives. We evaluated our model's performance on an Alexa keyword spotting detection task using a 16.6k-hour in-house dataset. Our technique showed exceptional performance in normal and noisy conditions, demonstrating the efficacy of knowledge distillation methods in constructing self-supervised models for keyword spotting tasks while working within on-device resource constraints.",True
yang23j_interspeech,https://www.isca-archive.org/interspeech_2023/yang23j_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/yang23j_interspeech.html,Improving Small Footprint Few-shot Keyword Spotting with Supervision on Auxiliary Data,Spoken Term Detection and Voice Search,2023,"Few-shot keyword spotting (FS-KWS) models usually require large-scale annotated datasets to generalize to unseen target keywords. However, existing KWS datasets are limited in scale and gathering keyword-like labeled data is costly undertaking. To mitigate this issue, we propose a framework that uses easily collectible, unlabeled reading speech data as an auxiliary source. Self-supervised learning has been widely adopted for learning representations from unlabeled data; however, it is known to be suitable for large models with enough capacity and is not practical for training a small footprint FS-KWS model. Instead, we automatically annotate and filter the data to construct a keyword-like dataset, LibriWord, enabling supervision on auxiliary data. We then adopt multi-task learning that helps the model to enhance the representation power from out-of-domain auxiliary data. Our method notably improves the performance over competitive methods in the FS-KWS benchmark.",True
okamoto23_interspeech,https://www.isca-archive.org/interspeech_2023/okamoto23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/okamoto23_interspeech.html,CAPTDURE: Captioned Sound Dataset of Single Sources,Source Separation,2023,"In conventional studies on environmental sound separation and synthesis using captions, datasets consisting of multiple-source sounds with their captions were used for model training. However, when we collect the captions for multiple-source sound, it is not easy to collect detailed captions for each sound source, such as the number of sound occurrences and timbre. Therefore, it is difficult to extract only the single-source target sound by the model-training method using a conventional captioned sound dataset. In this work, we constructed a dataset with captions for a single-source sound named CAPTDURE, which can be used in various tasks such as environmental sound separation and synthesis. Our dataset consists of 1,044 sounds and 4,902 captions. We evaluated the performance of environmental sound extraction using our dataset. The experimental results show that the captions for single-source sounds are effective in extracting only the single-source target sound from the mixture sound.",True
tran23_interspeech,https://www.isca-archive.org/interspeech_2023/tran23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/tran23_interspeech.html,Personalization for Robust Voice Pathology Detection in Sound Waves,Speech and Language in Health: From Remote Monitoring to Medical Conversations 1,2023,"Automatic voice pathology detection is promising for non-invasive screening and early intervention using sound signals. Nevertheless, existing methods are susceptible to covariate shifts due to background noises, human voice variations, and data selection biases leading to severe performance degradation in real-world scenarios. Hence, we propose a non-invasive framework that contrastively learns personalization from sound waves as a pre-train and predicts latent-spaced profile features through semi-supervised learning. It allows all subjects from various distributions (e.g., regionality, gender, age) to benefit from personalized predictions for robust voice pathology in a privacy-fulfilled manner. We extensively evaluate the framework on four real-world respiratory illnesses datasets, including Coswara, COUGHVID, ICBHI and our private dataset - ASound, under multiple covariate shift settings (i.e., cross-dataset), improving up to 4.12% in overall performance.",True
niu23b_interspeech,https://www.isca-archive.org/interspeech_2023/niu23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/niu23b_interspeech.html,Capturing Mismatch between Textual and Acoustic Emotion Expressions for Mood Identification in Bipolar Disorder,Speech and Language in Health: From Remote Monitoring to Medical Conversations 1,2023,"Emotion is a complex behavioral phenomenon, which is expressed and perceived through various modalities, such as language, vocal and facial expressions. Psychiatric research has suggested that the lack of emotional alignment between modalities is a symptom of emotion disorders. In this work, we quantify the mismatch between emotion expressed through language and acoustics, which we refer to as Emotional MisMatch (EMM), as an intermediate step for mood identification. We use a longitudinal dataset collected from people with Bipolar Disorder (BP) and show that symptomatic mood episodes show significantly more EMM, compared to euthymic moods. We propose a fully automatic mood identification pipeline with automatic speech transcription, emotion recognition, and EMM feature extraction. We find that EMM features, although smaller in size, outperform a language-based baseline, and consistently provide improvement when combined with language and/or raw emotion features on mood classification.",True
ghaffarzadegan23_interspeech,https://www.isca-archive.org/interspeech_2023/ghaffarzadegan23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/ghaffarzadegan23_interspeech.html,Active Learning for Abnormal Lung Sound Data Curation and Detection in Asthma,Speech and Language in Health: From Remote Monitoring to Medical Conversations 1,2023,"Existing audio-based asthma monitoring solutions rely on feature engineering designs paired with contact-based auscultation which are brittle in practice and do not scale beyond point of care setups. Data-driven methods utilizing contactless microphones have the potential to address such limitations. These solutions are under-explored in healthcare due to high cost of data curation requiring physicians-in-the-loop. Here, we propose an active learning (AL) system to facilitate audio data collection and annotation. It detects lung sound abnormalities in asthma. AL reduces the annotation cost while increasing the model performance under a constrained annotation budget. It automatically extracts interesting audio segments from the continuous recordings, and efficiently annotates and trains anomaly detector model. The experimental results confirm the effectiveness of the proposed system as an enabler for larger scale data curation on a newly collected audio corpus for pediatric asthma.",True
ratko23_interspeech,https://www.isca-archive.org/interspeech_2023/ratko23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/ratko23_interspeech.html,Opening or Closing? An Electroglottographic Analysis of Voiceless Coda Consonants in Australian English,Phonetics and Phonology: Languages and Varieties,2023,"In voiceless sounds, the glottis may be spread or constricted. Glottal spreading is associated with breathiness, and constriction with glottalisation. In many dialects of English, glottalisation often occurs with coda /t/ and sometimes with /p, k/, suggesting coda stop voicelessness is achieved through glottal constriction. Conversely, voiceless coda fricatives are associated with breathiness of the preceding vowel, with voicelessness achieved through glottal spreading. However, analyses specifically measuring glottal activity in coda consonant contexts in English are sparse. We conducted an electroglottographic analysis of vowels preceding voiceless codas /p, t, k, s/ to examine how coda voicelessness is achieved in Australian English (AusE). We found that coda /t/ and /p/ show glottal constriction towards vowel offset. Conversely, /k/ patterns with /s/ and exhibits glottal spreading. This suggests that different glottal configurations are used to achieve coda voicelessness in AusE.",True
wu23i_interspeech,https://www.isca-archive.org/interspeech_2023/wu23i_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/wu23i_interspeech.html,Speaker Embeddings as Individuality Proxy for Voice Stress Detection,Paralinguistics 2,2023,"Since the mental states of the speaker modulate speech, stress introduced by cognitive or physical loads could be detected in the voice. The existing voice stress detection benchmark has shown that the audio embeddings extracted from the Hybrid BYOL-S self-supervised model perform well. However, the benchmark only evaluates performance separately on each dataset, but does not evaluate performance across the different types of stress and different languages. Moreover, previous studies found strong individual differences in stress susceptibility. This paper presents the design and development of voice stress detection, trained on more than 100 speakers from 9 language groups and five different types of stress. We address individual variabilities in voice stress analysis by adding speaker embeddings to the hybrid BYOL-S features. The proposed method significantly improves voice stress detection performance with an input audio length of only 3-5 seconds.",True
pham23b_interspeech,https://www.isca-archive.org/interspeech_2023/pham23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/pham23b_interspeech.html,Vietnam-Celeb: a large-scale dataset for Vietnamese speaker recognition,Speaker and Language Identification 1,2023,"The success of speaker recognition systems heavily depends on large training datasets collected under real-world conditions. While common languages like English or Chinese have vastly available datasets, low-resource ones like Vietnamese remain limited. This paper presents a large-scale spontaneous dataset gathered under noisy environments, with over 87,000 utterances from 1,000 Vietnamese speakers of many professions, covering 3 main Vietnamese dialects. To build the dataset, we propose a sophisticated construction pipeline that can also be applied to other languages, with efficient visual-aided processing techniques to boost data precision. With the state-of-the-art x-vector model, training with the proposed dataset shows an average absolute and relative EER improvement of 5.48% and 41.61% when compared to the model trained on VLSP 2021, a publicly available Vietnamese speaker dataset.",True
lee23c_interspeech,https://www.isca-archive.org/interspeech_2023/lee23c_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/lee23c_interspeech.html,The 2022 NIST Language Recognition Evaluation,Speaker and Language Identification 1,2023,"In 2022, the U.S. National Institute of Standards and Technology (NIST) conducted the latest Language Recognition Evaluation (LRE) in an ongoing series administered by NIST since 1996 to foster research in language recognition and to measure state-of-the-art technology. Similar to previous LREs, LRE22 focused on conversational telephone speech (CTS) and broadcast narrowband speech (BNBS) data. LRE22 also introduced new evaluation features, such as an emphasis on African languages, including low resource languages, and a test set consisting of segments containing between 3s and 35s of speech randomly sampled and extracted from longer recordings. A total of 21 research organizations, forming 16 teams, participated in this 3-month long evaluation and made a total of 65 valid system submissions to be evaluated. This paper presents an overview of LRE22 and an analysis of system performance over different evaluation conditions. The evaluation results suggest that Oromo and Tigrinya are easier to detect while Xhosa and Zulu are more challenging. A greater confusability is seen for some language pairs. When speech duration increased, system performance significantly increased up to a certain duration, and then a diminishing return on system performance is observed afterward.",True
tamayoflorez23_interspeech,https://www.isca-archive.org/interspeech_2023/tamayoflorez23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/tamayoflorez23_interspeech.html,HABLA: A Dataset of Latin American Spanish Accents for Voice Anti-spoofing,Speaker and Language Identification 1,2023,"Research on improving automatic speaker verification systems to detect speech spoofing has focused mainly on English, with little attention given to other languages creating a significant gap in language coverage. This paper introduces HABLA, the first voice anti-spoofing dataset in the Spanish language including Argentinian, Colombian, Peruvian, Venezuelan, and Chilean accents. The dataset provided by HABLA comprises over 22,000 authentic speech samples from male and female speakers hailing from five distinct Latin American nations as well as 58,000 spoof samples that were generated through the use of six different speech synthesis strategies, including recent voice conversion and text-to-speech algorithms. Finally, initial findings on the efficacy of pre-existing Antispoofing Systems models are presented along with concerns regarding their performance in languages other than English.",True
zeng23_interspeech,https://www.isca-archive.org/interspeech_2023/zeng23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/zeng23_interspeech.html,Improving Generalization Ability of Countermeasures for New Mismatch Scenario by Combining Multiple Advanced Regularization Terms,Speaker and Language Identification 1,2023,"The ability of countermeasure models to generalize from seen speech synthesis methods to unseen ones has been investigated in the ASVspoof challenge. However, a new mismatch scenario in which fake audio may be generated from real audio with unseen genres has not been studied thoroughly. To this end, we first use five different vocoders to create a new dataset called CN-Spoof based on the CN-Celeb1&2 datasets. Then, we design two auxiliary objectives for regularization via meta-optimization and a genre alignment module, respectively, and combine them with the main anti-spoofing objective using learnable weights for multiple loss terms. The results on our cross-genre evaluation dataset for anti-spoofing show that the proposed method significantly improved the generalization ability of the countermeasures compared with the baseline system in the genre mismatch scenario.",True
wang23o_interspeech,https://www.isca-archive.org/interspeech_2023/wang23o_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/wang23o_interspeech.html,MAVD: The First Open Large-Scale Mandarin Audio-Visual Dataset with Depth Information,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 2",2023,"Audio-visual speech recognition (AVSR) gains increasing attention from researchers as an important part of human-computer interaction. However, the existing available Mandarin audio-visual datasets are limited and lack the depth information. To address this issue, this work establishes the MAVD, a new large-scale Mandarin multimodal corpus comprising 12,484 utterances spoken by 64 native Chinese speakers. To ensure the dataset covers diverse real-world scenarios, a pipeline for cleaning and filtering the raw text material has been developed to create a well-balanced reading material. In particular, the latest data acquisition device of Microsoft, Azure Kinect is used to capture depth information in addition to the traditional audio signals and RGB images during data acquisition. We also provide a baseline experiment, which could be used to evaluate the effectiveness of the dataset. The dataset and code will be released at https://github.com/SpringHuo/MAVD.",True
li23y_interspeech,https://www.isca-archive.org/interspeech_2023/li23y_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/li23y_interspeech.html,CN-Celeb-AV: A Multi-Genre Audio-Visual Dataset for Person Recognition,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 2",2023,"Audio-visual person recognition (AVPR) has received extensive attention. However, most datasets used for AVPR research so far are collected in constrained environments, and thus cannot reflect the true performance of AVPR systems in real-world scenarios. To meet the request for research on AVPR in unconstrained conditions, this paper presents a multi-genre AVPR dataset collected 'in the wild', named CN-Celeb-AV. This dataset contains more than 420k video segments from 1,136 persons from public media. In particular, we put more emphasis on two real-world complexities: (1) data in multiple genres; (2) segments with partial information. A comprehensive study was conducted to compare CN-Celeb-AV with two popular public AVPR benchmark datasets, and the results demonstrated that CN-Celeb-AV is more in line with real-world scenarios and can be regarded as a new benchmark dataset for AVPR research. The dataset also involves a development set
that can be used to boost the performance of AVPR systems in real-life situations. The dataset is free for researchers and can be downloaded from http://cnceleb.org/.",True
avila23_interspeech,https://www.isca-archive.org/interspeech_2023/avila23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/avila23_interspeech.html,Towards Cross-Language Prosody Transfer for Dialog,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 2",2023,"Speech-to-speech translation systems today do not adequately support use for dialog purposes. In particular, nuances of speaker intent and stance can be lost due to improper prosody transfer. We present an exploration of what needs to be done to overcome this. First, we developed a data collection protocol in which bilingual speakers re-enact utterances from an earlier conversation in their other language, and used this to collect an English-Spanish corpus, so far comprising 1871 matched utterance pairs. Second, we developed a simple prosodic dissimilarity metric based on Euclidean distance over a broad set of prosodic features. We then used these to investigate cross-language prosodic differences, measure the likely utility of three simple baseline models, and identify phenomena which will require more powerful modeling. Our findings should inform future research on cross-language prosody and the design of speech-to-speech translation systems capable of effective prosody transfer.",True
koudounas23_interspeech,https://www.isca-archive.org/interspeech_2023/koudounas23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/koudounas23_interspeech.html,ITALIC: An Italian Intent Classification Dataset,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 2",2023,"Recent large-scale Spoken Language Understanding datasets focus predominantly on English and do not account for language-specific phenomena such as particular phonemes or words in different lects. We introduce ITALIC, the first large-scale speech dataset designed for intent classification in Italian. The dataset comprises 16,521 crowdsourced audio samples recorded by 70 speakers from various Italian regions and annotated with intent labels and additional metadata. We explore the versatility of ITALIC by evaluating current state-of-the-art speech and text models. Results on intent classification suggest that increasing scale and running language adaptation yield better speech models, monolingual text models outscore multilingual ones, and that speech recognition on ITALIC is more challenging than on existing Italian benchmarks. We release both the dataset and the annotation scheme to streamline the development of new Italian SLU models and language-specific datasets.",True
ye23b_interspeech,https://www.isca-archive.org/interspeech_2023/ye23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/ye23b_interspeech.html,"GigaST: A 10,000-hour Pseudo Speech Translation Corpus","Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 2",2023,"This paper introduces GigaST, a large-scale pseudo speech-to-text translation (ST) corpus. We create the corpus by translating the transcript in GigaSpeech, an English ASR corpus, into German and Chinese. The training set is translated by a strong machine translation system and the test set is translated by human. ST models trained with an addition of our corpus obtain new state-of-the-art results on the MuST-C English-German benchmark test set. We provide a detailed description of the translation process and verify its quality. We make the translated text data public and hope to facilitate research in speech translation. Additionally, we also release the training scripts on NeurST1 to make it easy to replicate our systems. GigaST dataset is available at https://st-benchmark.github.io/resources/GigaST.",True
li23z_interspeech,https://www.isca-archive.org/interspeech_2023/li23z_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/li23z_interspeech.html,PunCantonese: A Benchmark Corpus for Low-Resource Cantonese Punctuation Restoration from Speech Transcripts,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 2",2023,"Punctuation restoration from unsegmented speech transcripts is an essential task to improve the readability of transcripts and can facilitate various downstream NLP tasks. However, there is still lack of systematic studies on punctuation restoration for Cantonese as a low-resource language. This paper introduces a new Cantonese punctuation corpus named PunCantonese, which consists of annotated spoken transcripts and written-style Wikipedia sentences, covering the major punctuations such as â,.?!â and code-switched sentences in Cantonese and English. We also propose a Transformer-based punctuation model which exploits pre-trained multilingual language models, adopts multitask learning for style and punctuation prediction, and introduces a novel Jyutping embedding layer to inject the
phonetic features not explicitly available in Cantonese characters. Experimental results show that these methods are effective in improving punctuation restoration, and the Jyutping embedding layer brings an absolute F1 increase by more than 2%.",True
yakovlev23_interspeech,https://www.isca-archive.org/interspeech_2023/yakovlev23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/yakovlev23_interspeech.html,VoxTube: a multilingual speaker recognition dataset,Speaker Recognition 1,2023,The objective of this paper is to advance the development of technologies in the fields of speaker recognition and speaker identification by introducing a large labeled audio database VoxTube collected from the open-source media. We propose a fully automated unsupervised approach for audio labeling that requires any pre-trained speaker recognition model. Collected with this approach from videos with CC BY license the VoxTube dataset contains more than 5.000 speakers with more than 4 million utterances pronounced in more than 10 languages. In our paper we show the VoxTube's high generalization ability across multiple domains by evaluating the accuracy metrics on various speaker recognition benchmarks. We also show how well this dataset complements an already existing VoxCeleb2 dataset.,True
chen23i_interspeech,https://www.isca-archive.org/interspeech_2023/chen23i_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/chen23i_interspeech.html,Investigating the Utility of Synthetic Data for Doctor-Patient Conversation Summarization,Speech and Language in Health: From Remote Monitoring to Medical Conversations 2,2023,"Large-scale pre-training has been a successful strategy for training transformer models. However, maintaining a large clinical dataset for pre-training is not always possible, and access to data in this domain can be time-limited and costly. We explore using synthetic data in pre-training sequence-to-sequence (seq-to-seq) transformer models to generate clinical notes from Doctor-Patient-Conversations (DoPaCos). Using a generative language model fine-tuned on authentic conversations, a synthetic DoPaCo dataset was created and used with a corpus of clinical notes to pre-train a Longformer-Encoder-Decoder (LED) model. Results show that synthetic data leads to comparable performance in the downstream summarization task compared to pre-training with authentic data. Pre-training on synthetic conversations first, followed by clinical notes, yields higher performance across most of our evaluation metrics.",True
demir23_interspeech,https://www.isca-archive.org/interspeech_2023/demir23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/demir23_interspeech.html,PoCaPNet: A Novel Approach for Surgical Phase Recognition Using Speech and X-Ray Images,Speech and Language in Health: From Remote Monitoring to Medical Conversations 2,2023,"Surgical phase recognition is a challenging and necessary task for the development of context-aware intelligent systems that can support medical personnel for better patient care and effective operating room management. In this paper, we present a surgical phase recognition framework that employs a Multi-Stage Temporal Convolution Network using speech and X-Ray images for the first time. We evaluate our proposed approach using our dataset that comprises 31 port-catheter placement operations and report 82.56 % frame-wise accuracy with eight surgical phases. Additionally, we investigate the design choices in the temporal model and solutions for the class-imbalance problem. Our experiments demonstrate that speech and X-Ray data can be effectively utilized for surgical phase recognition, providing a foundation for the development of speech assistants in operating rooms of the future.",True
mallolragolta23_interspeech,https://www.isca-archive.org/interspeech_2023/mallolragolta23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/mallolragolta23_interspeech.html,The MASCFLICHT Corpus: Face Mask Type and Coverage Area Recognition from Speech,Speech and Language in Health: From Remote Monitoring to Medical Conversations 2,2023,"We present a novel speech dataset for face mask type and coverage area recognition collected with a smartphone. The dataset contains 2h 27m 55s of data from 30 German speakers (15f, 15m). The baseline results exploit the functionals of the eGeMAPS feature set, the Mel-spectrogram, and the spectrogram representations of the audio samples. To model the one-dimensional features, we investigate Support Vector Classifiers (SVC) and a neural network classifier. We extract salient information from the two-dimensional representations with Convolutional Neural Network (CNN) based encoders, coupled with a classification block. We use the Unweighted Average Recall (UAR) as the evaluation metric. For the face mask type and the coverage area recognition tasks (3-class problems), the best models on the test partition score a UAR of 49.3% and 47.8%, respectively. For the face mask type and coverage area recognition task (5-class problem), the optimal model on the test partition obtains a UAR of 35.0%.",True
dineley23_interspeech,https://www.isca-archive.org/interspeech_2023/dineley23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/dineley23_interspeech.html,Towards robust paralinguistic assessment for real-world mobile health (mHealth) monitoring: an initial study of reverberation effects on speech,Speech and Language in Health: From Remote Monitoring to Medical Conversations 2,2023,"Speech is promising as an objective, convenient tool to monitor health remotely over time using mobile devices. Numerous paralinguistic features have been demonstrated to contain salient information related to an individual's health. However, mobile device specification and acoustic environments vary widely, risking the reliability of the extracted features. In an initial step towards quantifying these effects, we report the variability of 13 exemplar paralinguistic features commonly reported in the speech-health literature and extracted from the speech of 42 healthy volunteers recorded consecutively in rooms with low and high reverberation with one budget and two higher-end smartphones, and a condenser microphone. Our results show reverberation has a clear effect on several features, in particular voice quality markers. Our findings point to new research directions investigating how best to record and process in-the-wild speech for reliable longitudinal mobile health state assessment.",True
li23ia_interspeech,https://www.isca-archive.org/interspeech_2023/li23ia_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/li23ia_interspeech.html,Discrimination of the Different Intents Carried by the Same Text Through Integrating Multimodal Information,Multimodal Speech Emotion Recognition,2023,"Many intent understanding studies neglect the impact of paralinguistic information, resulting in misunderstandings during speech interactions, particularly when different intentions are conveyed by the same text with varying paralinguistic information. To address this issue, this study developed a Chinese multimodal spoken language intention understanding dataset that features different spoken intentions for identical texts. Our proposed attention-based BiLSTM model integrates textual and acoustic features and introduces an acoustic information gate mechanism to supplement or correct linguistic intention with paralinguistic intention. Experimental results demonstrate that our multimodal integration model improves intent discrimination accuracy by 11.0% compared to models that incorporate only linguistic information. The result highlights the effectiveness of our proposed model for intent discrimination, particularly in cases with identical text but varying intentions.",True
hedegard23_interspeech,https://www.isca-archive.org/interspeech_2023/hedegard23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/hedegard23_interspeech.html,Filling the population statistics gap: Swiss German reference data on F0 and speech tempo for forensic contexts,"Phonetics, Phonology, and Prosody 1",2023,"The increased focus on big data in phonetics, and Bayesian statistics in the forensic sciences, prompt a fundamental issue in common applications of forensic phonetics. Relevant population distributions for most features, a key element when evaluating the similarity and distinctiveness of voices, remain lacking for a substantial number of languages and dialects. This paper provides population statistics for two phonetic features in the Swiss German context, speech tempo and F0, and outlines a potential method for big data analysis. The speech data is taken from 1000 SwG speakers and include two different style conditions: spontaneous and read speech. Results indicate significant variation for both parameters: we contradict previous findings on gender differences in speech tempo and note discrepancies for both features between the two styles. These findings constitute an important contribution to the field of forensic phonetics, as well as the field of general phonetics more broadly.",True
amiriparian23_interspeech,https://www.isca-archive.org/interspeech_2023/amiriparian23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/amiriparian23_interspeech.html,Speech-Based Classification of Defensive Communication: A Novel Dataset and Results,Spoken Dialog Systems and Conversational Analysis 2,2023,"Defensive communication is known to have detrimental effects on the quality of social interactions. Hence, recognising and reducing defensive behaviour is crucial to improving professional and personal communication. We introduce DefComm-DB, a novel multimodal dataset comprising video recordings in which one of the following types of defensive communication is present: (i) verbally attacking the conversation partner, (ii) withdrawing from the communication, (iii) making oneself greater, and (iv) making oneself smaller. Subsequently, we present a machine learning approach for the automatic classification of DefComm-DB. In particular, we utilise wav2vec2, autoencoders, a pre-trained CNN and openSMILE for feature extraction from the audio modality. For the text stream, we apply ELECTRA and SBERT. On the unseen test set, our models achieve an Unweighted Average Recall of 49.4 % and 52.2 % for the audio and text modalities, respectively, showing the feasibility of the introduced challenge.",True
martinezsevilla23_interspeech,https://www.isca-archive.org/interspeech_2023/martinezsevilla23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/martinezsevilla23_interspeech.html,Insights into end-to-end audio-to-score transcription with real recordings: A case study with saxophone works,Analysis of Speech and Audio Signals 2,2023,"Neural end-to-end Audio-to-Score (A2S) transcription aims to retrieve a score that encodes the music content of an audio recording in a single step. Due to the recentness of this formulation, the existing works have exclusively addressed controlled scenarios with synthetic data that fail to provide conclusions applicable to real-world cases. In response to this gap in the literature, this work introduces a novel assortment of real saxophone recordings---together with their digital scores---and poses several experimental scenarios involving real and synthetic data. The obtained results confirm the adequacy of this A2S framework to deal with real data as well as proving the relevance of leveraging synthetic interpretations to improve the recognition rate in scenarios with real-data scarcity.",True
xiang23_interspeech,https://www.isca-archive.org/interspeech_2023/xiang23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/xiang23_interspeech.html,eSTImate: A Real-time Speech Transmission Index Estimator With Speech Enhancement Auxiliary Task Using Self-Attention Feature Pyramid Network,Speech Coding: Privacy,2023,"The Speech Transmission Index (STI) is a crucial metric for evaluating speech intelligibility, but its standard measurement method is too complicated for real-time applications. Though recently proposed deep learning based STI estimation schemes can effectively address the problem, existing methods still fall short of covering all possible STI scenarios. This paper presents eSTImate: an end-to-end deep learning system for real-time STI blind estimation that integrates the tasks of STI estimation and speech enhancement through a feature pyramid auxiliary learning architecture and incorporates multi-head attention mechanisms. The proposed model demonstrates the performance of state-of-the-art, achieving a low mean absolute error of 0.016 and root mean square error of 0.021 on the constructed dataset that covers the whole range of STI, highlighting its potential to provide accurate and consistent real-time STI estimation across diverse real-world scenarios.",True
ashihara23_interspeech,https://www.isca-archive.org/interspeech_2023/ashihara23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/ashihara23_interspeech.html,SpeechGLUE: How Well Can Self-Supervised Speech Models Capture Linguistic Knowledge?,Analysis of Neural Speech Representations,2023,"Self-supervised learning (SSL) for speech representation has been successfully applied in various downstream tasks, such as speech and speaker recognition. More recently, speech SSL models have also been shown to be beneficial in advancing spoken language understanding tasks, implying that the SSL models have the potential to learn not only acoustic but also linguistic information. In this paper, we aim to clarify if speech SSL techniques can well capture linguistic knowledge. For this purpose, we introduce SpeechGLUE, a speech version of the General Language Understanding Evaluation (GLUE) benchmark. Since GLUE comprises a variety of natural language understanding tasks, SpeechGLUE can elucidate the degree of linguistic ability of speech SSL models. Experiments demonstrate that speech SSL models, although inferior to text-based SSL models, perform better than baselines, suggesting that they can acquire a certain amount of general linguistic knowledge from just unlabeled speech data.",True
yang23b_interspeech,https://www.isca-archive.org/interspeech_2023/yang23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/yang23b_interspeech.html,Relation-based Counterfactual Data Augmentation and Contrastive Learning for Robustifying Natural Language Inference Models,"Spoken Language Understanding, Summarization, and Information Retrieval",2023,"Although pre-trained language models show good performance on various natural language processing tasks, they often rely on non-causal features and patterns to determine the outcome. For natural language inference tasks, previous results have shown that even a model trained on a large number of data fails to perform well on counterfactually revised data, indicating that the model is not robustly learning the semantics of the classes. In this paper, we propose a method in which we use token-based and sentence-based augmentation methods to generate counterfactual sentence pairs that belong to each class, and apply contrastive learning to help the model learn the difference between sentence pairs of different classes with similar contexts. Evaluation results with counterfactually-revised dataset and general NLI datasets show that the proposed method can improve the performance and robustness of the NLI model.",True
deshmukh23_interspeech,https://www.isca-archive.org/interspeech_2023/deshmukh23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/deshmukh23_interspeech.html,Audio Retrieval with WavText5K and CLAP Training,"Spoken Language Understanding, Summarization, and Information Retrieval",2023,"Text-based audio retrieval takes a natural language query to retrieve relevant audio files in a database. Most retrieval models are trained, optimized, and evaluated on a single dataset. In this paper, we quantify the effect of adding training data using three datasets and the effect on performance by evaluating the same model on two datasets. For our study, first, we introduce a new collection of about 5000 audio-text pairs called WavText5K. We qualitatively show how WavText5K differs from audio-text datasets and quantitatively show its effectiveness for retrieval. Our results show that adding more audio-text pairs does not necessarily improve performance. Second, we compare two effective audio encoders: CNN and audio transformers. We propose an architecture that demonstrates that utilizing both encoders improves the individual model's performance. Overall, using WavText5K and the proposed encoder combination outperforms the benchmark for AudioCaps and Clotho by 6% and 23%.",True
deseyssel23_interspeech,https://www.isca-archive.org/interspeech_2023/deseyssel23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/deseyssel23_interspeech.html,"ProsAudit, a prosodic benchmark for self-supervised speech models",Invariant and Robust Pre-trained Acoustic Models,2023,"We present ProsAudit, a benchmark in English to assess structural prosodic knowledge in self-supervised learning (SSL) speech models. It consists of two subtasks, their corresponding metrics, and an evaluation dataset. In the protosyntax task, the model must correctly identify strong versus weak prosodic boundaries. In the lexical task, the model needs to correctly distinguish between pauses inserted between words and within words. We also provide human evaluation scores on this benchmark. We evaluated a series of SSL models and found that they were all able to perform above chance on both tasks, even when evaluated on an unseen language. However, non-native models performed significantly worse than native ones on the lexical task, highlighting the importance of lexical knowledge in this task. We also found a clear effect of size with models trained on more data performing better in the two subtasks.",True
saito23_interspeech,https://www.isca-archive.org/interspeech_2023/saito23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/saito23_interspeech.html,ChatGPT-EDSS: Empathetic Dialogue Speech Synthesis Trained from ChatGPT-derived Context Word Embeddings,Speech Synthesis: Representation Learning,2023,"We propose ChatGPT-EDSS, an empathetic dialogue speech synthesis (EDSS) method using ChatGPT for extracting dialogue context. ChatGPT is a chatbot that can deeply understand the content and purpose of an input prompt and appropriately respond to the user's request. We focus on ChatGPT's reading comprehension and introduce it to EDSS, a task of synthesizing speech that can empathize with the interlocutor's emotion. Our method first gives chat history to ChatGPT and asks it to generate three words representing the intention, emotion, and speaking style for each line in the chat. Then, it trains an EDSS model using the embeddings of ChatGPT-derived context words as the conditioning features. The experimental results demonstrate that our method performs comparably to ones using emotion labels or neural network-derived context embeddings learned from chat histories. The collected ChatGPT-derived context information is available at our project page.",True
gao23f_interspeech,https://www.isca-archive.org/interspeech_2023/gao23f_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/gao23f_interspeech.html,Human Transcription Quality Improvement,"Speech Perception, Production, and Acquisition 1",2023,"High quality transcription data is crucial for training automatic speech recognition (ASR) systems. However, the existing industry-level data collection pipelines are expensive to researchers, while the quality of crowdsourced transcription is low. In this paper, we propose a reliable method to collect speech transcriptions. We introduce two mechanisms to improve transcription quality: confidence estimation based reprocessing at labeling stage, and automatic word error correction at post-labeling stage. We collect and release LibriCrowd - a large-scale crowdsourced dataset of audio transcriptions on 100 hours of English speech. Experiment shows the Transcription WER is reduced by over 50%. We further investigate the impact of transcription error on ASR model performance and found a strong correlation. The transcription quality improvement provides over 10% relative WER reduction for ASR models. We release the dataset and code to benefit the research community.",True
simantiraki23_interspeech,https://www.isca-archive.org/interspeech_2023/simantiraki23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/simantiraki23_interspeech.html,The effect of masking noise on listenersâ spectral tilt preferences,"Speech Perception, Production, and Acquisition 1",2023,"Speech enhancement algorithms often focus on optimising intelligibility while neglecting other aspects of speech such as naturalness, quality and listening effort which may affect a listener's experience. This paper investigates the impact of spectral tilt on listeners' preferences, using a new corpus of Greek utterances. Participants adjusted spectral tilt with real-time feedback to select their preferred tilt in quiet and in the presence of speech-shaped noise at eight signal-to-noise ratios. Listeners displayed distinct preferences, with a tendency to select flatter tilts with increasing noise. Preferences were not random even for constant intelligibility, indicating that their adjustments were influenced by factors beyond the need to maintain comprehensibility. These findings have the potential to inform the design of speech enhancement algorithms that jointly optimise intelligibility and a listener's overall experience.",True
bharati23_interspeech,https://www.isca-archive.org/interspeech_2023/bharati23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/bharati23_interspeech.html,Automatic Deep Neural Network-Based Segmental Pronunciation Error Detection of L2 English Speech (L1 Bengali),"Speech Perception, Production, and Acquisition 1",2023,"In the last few decades, English has become a popular language as it helps us to communicate with the global world. A large population of English learners find it challenging to achieve an 'acceptable' and 'intelligible' pronunciation. To overcome these issues, various computer-assisted pronunciation training tools are designed where automatic pronunciation error detection (APED) is a core component of the system. Most of the works of APED are based on European English speech, but there is no such work reported for Bengali English speech. This paper proposes a system for pronunciation error detection of L2 English speech (L1 Bengali) at phoneme/segmental level using a hybrid convolutional neural network and long short-term memory modules with CTC loss. Experiments are done based on newly created L2 English speaker (L1 Bengali) speech data. The results demonstrate that the proposed system outperforms the goodness of pronunciation-based methods by 15% in terms of F1 score using fbank.",True
miodonska23_interspeech,https://www.isca-archive.org/interspeech_2023/miodonska23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/miodonska23_interspeech.html,Are retroflex-to-dental sibilant substitutions in Polish children's speech an example of a covert contrast? A preliminary acoustic study,"Speech Perception, Production, and Acquisition 1",2023,The study aimed to investigate whether the dental substitutions of retroflex voiceless fricatives (/Ê/ to [s]) in Polish children's speech are an example of a covert contrast. We analyzed speech samples collected through a picture naming test from 11 children showing this retroflex-to-dental production pattern. The language material included words with /Ê/ and /s/ in diversified word positions. We extracted a set of spectrum-based acoustic features from the recorded sibilants and conducted the analysis using linear mixed-effect models. The models showed that significant acoustic differences (p < 0.05) can be found between realizations of /s/ and /Ê/ substituted by [s]. The main differences were detected in the amplitudes of fricative formants and the energy levels in specific subbands of the frication noise. The study provides preliminary evidence of the existence of covert contrasts in the analyzed substitutions.,True
mateju23_interspeech,https://www.isca-archive.org/interspeech_2023/mateju23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/mateju23_interspeech.html,Combining Multilingual Resources and Models to Develop State-of-the-Art E2E ASR for Swedish,"Speech Recognition: Architecture, Search, and Linguistic Components 3",2023,"In terms of automatic speech recognition (ASR), Swedish belongs to the group of less-resourced languages, as publicly available training data is limited to a few hundred hours of mostly read speech. To acquire larger amounts of more realistic data, we investigate the existing multilingual approaches, and also propose two new ones, which combine Swedish with previously created Norwegian data and models. We use them for efficient automatic harvesting of spoken Swedish from broadcast, parliament, YouTube, and audiobook archives. The combined models significantly speed up the harvesting process and improve the final Swedish end-to-end (E2E) ASR system. We evaluate it on datasets covering various applications and domains; they provide performance better than the state-of-the-art commercial cloud services. We have made all of our test datasets publicly available for future comparative experiments.",True
fan23_interspeech,https://www.isca-archive.org/interspeech_2023/fan23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/fan23_interspeech.html,Language-specific Boundary Learning for Improving Mandarin-English Code-switching Speech Recognition,"Speech Recognition: Architecture, Search, and Linguistic Components 3",2023,"Code-switching speech recognition (CSSR) transcribes speech that switches between multiple languages or dialects within a single sentence. The main challenge in this task is that different languages often have similar pronunciations, making it difficult for models to distinguish between them. In this paper, we propose a method for solving the CSSR task from the perspective of language-specific acoustic boundary learning. We introduce language-specific weight estimators (LSWE) to model acoustic boundary learning in different languages separately. Additionally, a non-autoregressive (NAR) decoder and a language change detection (LCD) module are employed to assist in training. Evaluated on the SEAME corpus, our method achieves a state-of-the-art mixed error rate (MER) of 16.29% and 22.81% on the testman and testsge sets. We also demonstrate the effectiveness of our method on a 9000-hour in-house meeting code-switching dataset, where our method achieves a relatively 7.9% MER reduction.",True
cahyawijaya23_interspeech,https://www.isca-archive.org/interspeech_2023/cahyawijaya23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/cahyawijaya23_interspeech.html,Cross-Lingual Cross-Age Adaptation for Low-Resource Elderly Speech Emotion Recognition,Acoustic Model Adaptation for ASR,2023,"Speech emotion recognition plays a crucial role in human-computer interactions. However, most speech emotion recognition research is biased toward English-speaking adults, which hinders its applicability to other demographic groups in different languages and age groups. In this work, we analyze the transferability of emotion recognition across three different languages--English, Mandarin Chinese, and Cantonese; and 2 different age groups--adults and the elderly. To conduct the experiment, we develop an English-Mandarin speech emotion benchmark for adults and the elderly, BiMotion, and a Cantonese speech emotion dataset, YueMotion. This study concludes that different language and age groups require specific speech features, thus making cross-lingual inference an unsuitable method. However, cross-group data augmentation is still beneficial to regularize the model, with linguistic distance being a significant influence on cross-lingual transferability.",True
deb23_interspeech,https://www.isca-archive.org/interspeech_2023/deb23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/deb23_interspeech.html,BeAts: Bengali Speech Acts Recognition using Multimodal Attention Fusion,Multi-modal Systems,2023,"Spoken languages often utilise intonation, rhythm, intensity, and structure, to communicate intention, which can be interpreted differently depending on the rhythm of speech of their utterance. These speech acts provide the foundation of communication and are unique in expression to the language. Recent advancements in attention-based models, demonstrating their ability to learn powerful representations from multilingual datasets, have performed well in speech tasks and are ideal to model specific tasks in low resource languages. Here, we develop a novel multimodal approach combining two models, wav2vec2.0 for audio and MarianMT for text translation, by using multimodal attention fusion to predict speech acts in our prepared Bengali speech corpus. We also show that our model BeAts (Bengali speech acts recognition using Multimodal Attention Fusion) significantly outperforms both the unimodal baseline using only speech data and a simpler bimodal fusion using both speech and text data.",True
gao23c_interspeech,https://www.isca-archive.org/interspeech_2023/gao23c_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/gao23c_interspeech.html,A Novel Interpretable and Generalizable Re-synchronization Model for Cued Speech based on a Multi-Cuer Corpus,Multi-modal Systems,2023,"Cued Speech (CS) is a multi-modal visual coding system combining lip reading with several hand cues at the phonetic level to make the spoken language visible to the hearing impaired. Previous studies solved asynchronous problems between lip and hand movements by a cuer-dependent piecewise linear model for English and French CS. In this work, we innovatively propose three statistical measure on the lip stream to build an interpretable and generalizable model for predicting hand preceding time (HPT), which achieves cuer-independent by a proper normalization. Particularly, we build the first Mandarin CS corpus comprising annotated videos from five speakers including three normal and two hearing impaired individuals. Consequently, we show that the hand preceding phenomenon exists in Mandarin CS production with significant differences between normal and hearing impaired people. Extensive experiments demonstrate that our model outperforms the baseline and the previous state-of-the-art methods.",True
faustini23_interspeech,https://www.isca-archive.org/interspeech_2023/faustini23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/faustini23_interspeech.html,Composing Spoken Hints for Follow-on Question Suggestion in Voice Assistants,Question Answering from Speech,2023,"The adoption of voice assistants like Alexa or Siri has grown rapidly, allowing users instant access to information via voice search. Query suggestion is a standard feature of screen-based search experiences, allowing users to explore additional topics. However, this is not trivial to implement in voice-based settings. To enable this, we tackle the novel task of suggesting uestions with compact and natural voice hints to allow users to ask follow-up questions. We first define the task of composing speech-based hints, ground it in syntactic theory, and outline linguistic desiderata for spoken hints. We propose a sequence-to-sequence approach to generate spoken hints from a list of questions. Using a new dataset of 6, 681 input questions and human written hints, we evaluate models with automatic metrics and human evaluation. Results show that a naive approach of concatenating suggested questions creates poor voice hints. Our most sophisticated approach applies a linguistically-motivated pretraining task and was strongly preferred by humans for producing the most natural hints.",True
sun23c_interspeech,https://www.isca-archive.org/interspeech_2023/sun23c_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/sun23c_interspeech.html,Speech Entrainment in Chinese Story-Style Talk Shows: The Interaction Between Gender and Role,Sociophonetics,2023,"Speech entrainment is evident in short-and-short turn-taking, but entrainment in long-and-short turn-taking, like talk shows, is expected to be different but also evident. We examined three prosodic feature sets of pitch, intensity, and duration (speaking rate) to explore the impact of gender and role interaction between the host and guests on speech entrainment in a Mandarin Chinese talk show corpus. This research consistently showed that intensity remained the robust entraining feature, and the speaking rate was steadily divergent. Another vital result was that the rare occurrence of the final-rising pitch in question types led to dynamic local positive proximity. Besides, it was interesting to note that mixed-gender pairs with different roles showed more dynamic local positive proximity and synchrony on intensity and pitch than same-gender pairs. Taken together, these results suggest the complexity of gender and role interaction on speech entrainment in long-and-short turn-taking.",True
baghel23_interspeech,https://www.isca-archive.org/interspeech_2023/baghel23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/baghel23_interspeech.html,The DISPLACE Challenge 2023 - DIarization of SPeaker and LAnguage in Conversational Environments,Speaker and Language Diarization,2023,"In multilingual societies, social conversations often involve code-mixed speech. The current speech technology may not be well equipped to extract information from multi-lingual multi-speaker conversations. The DISPLACE challenge entails a first-of-kind task to benchmark speaker and language diarization on the same data, as the data contains multi-speaker conversations in multilingual code-mixed speech. The challenge attempts to highlight outstanding issues in speaker diarization (SD) in multilingual settings with code-mixing. Further, language diarization (LD) in multi-speaker settings also introduces new challenges, where the system has to disambiguate speaker switches with code switches. For this challenge, a natural multilingual, multi-speaker conversational dataset is distributed for development and evaluation purposes. The systems are evaluated on single-channel far-field recordings. We also release a baseline system and report the highlights of the system submissions.",True
sarabia23_interspeech,https://www.isca-archive.org/interspeech_2023/sarabia23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/sarabia23_interspeech.html,Spatial LibriSpeech: An Augmented Dataset for Spatial Audio Learning,Analysis of Speech and Audio Signals 3,2023,"We present Spatial LibriSpeech, a spatial audio dataset with over 650 hours of 19-channel audio, first-order ambisonics, and optional distractor noise. Spatial LibriSpeech is designed for machine learning model training, and it includes labels for source position, speaking direction, room acoustics and geometry. Spatial LibriSpeech is generated by augmenting LibriSpeech samples with 200k+ simulated acoustic conditions across 8k+ synthetic rooms. To demonstrate the utility of our dataset, we train models on four spatial audio tasks, resulting in a median absolute error of 6.60Â° on 3D source localization, 0.43m on distance, 90.66ms on T30, and 2.74dB on direct-to-reverberant ratio estimation. We show that the same models generalize well to widely-used evaluation datasets, e.g., obtaining a median absolute error of 12.43Â° on 3D source localization on TUT Sound Events 2018, and 157.32ms on T30 estimation on ACE Challenge.",True
li23q_interspeech,https://www.isca-archive.org/interspeech_2023/li23q_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/li23q_interspeech.html,Image-driven Audio-visual Universal Source Separation,Analysis of Speech and Audio Signals 3,2023,"This paper introduces an image-driven audio-visual universal source separation (ID-USS) and proposes ID-USS-Conformer. ID-USS aims to separate a target source from the mixture based on the input image that is consistent with the target. Importantly, ID-USS only focuses on the sound made by the target in this image, not on the description of the target or the semantic information of the picture. In detail, ID-USS-Conformer mainly consists of an Efficient-b3-based visual branch and a Conformer-based audio branch. The visual branch extracts the visual clue of the target from the input image. After the audio branch fuses the visual features, ID-USS-Conformer separates the target source from the mixture. We launch an ID-USS dataset and verify the effectiveness of ID-USS-Conformer on it. The ID-USS-Conformer has achieved a 10.139 dB signal-to-distortion ratio improvement in the test set and outperformed the compared methods.",True
lin23e_interspeech,https://www.isca-archive.org/interspeech_2023/lin23e_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/lin23e_interspeech.html,Low-Resource Cross-Lingual Adaptive Training for Nigerian Pidgin,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 3",2023,"Developing effective spoken language processing systems for low-resource languages poses several challenges due to the lack of parallel data and limited resources for fine-tuning models. In this work, we target on improving upon both text classification and translation of Nigerian Pidgin (Naija) by collecting a large-scale parallel English-Pidgin corpus and further propose a framework of cross-lingual adaptive training that includes both continual and task adaptive training so as to adapt a base pre-trained model to low-resource languages. Our studies show that English pre-trained language models serve as a stronger prior than multilingual language models on English-Pidgin tasks with up to 2.38 BLEU improvements; and demonstrate that augmenting orthographic data and using task adaptive training with back-translation can have a significant impact on model performance.",True
sikasote23_interspeech,https://www.isca-archive.org/interspeech_2023/sikasote23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/sikasote23_interspeech.html,Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages,"Spoken Language Translation, Information Retrieval, Summarization, Resources, and Evaluation 3",2023,"This work introduces Zambezi Voice, an open-source multilingual speech resource for Zambian languages. It contains two collections of datasets: unlabelled audio recordings of radio news and talk shows programs (160 hours) and labelled data (over 80 hours) consisting of read speech recorded from text sourced from publicly available literature books. The dataset is created for speech recognition but can be extended to multilingual speech processing research for both supervised and unsupervised learning approaches. To our knowledge, this is the first multilingual speech dataset created for Zambian languages. We exploit pretraining and cross-lingual transfer learning by finetuning the Wav2Vec2.0 large-scale multilingual pretrained model to build end-to-end (E2E) speech recognition models for our baseline models. The dataset is released publicly under a Creative Commons BY-NC-ND 4.0 license and can be accessed through the project repository.",True
ryumina23_interspeech,https://www.isca-archive.org/interspeech_2023/ryumina23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/ryumina23_interspeech.html,Multimodal Personality Traits Assessment (MuPTA) Corpus: The Impact of Spontaneous and Read Speech,Resources for Spoken Language Processing,2023,"Automatic personality traits assessment (PTA) provides high-level, intelligible predictive inputs for subsequent critical downstream tasks, such as job interview recommendations and mental healthcare monitoring. In this work, we introduce a novel Multimodal Personality Traits Assessment (MuPTA) corpus. Our MuPTA corpus is unique in that it contains both spontaneous and read speech collected in the midly-resourced Russian language. We present a novel audio-visual approach for PTA that is used in order to set up baseline results on this corpus. We further analyze the impact of both spontaneous and read speech types on the PTA predictive performance. We find that for the audio modality, the PTA predictive performances on short signals are almost equal regardless of the speech type, while PTA using video modality is more accurate with spontaneous speech compared to read one regardless of the signal length.",True
pudo23_interspeech,https://www.isca-archive.org/interspeech_2023/pudo23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/pudo23_interspeech.html,MOCKS 1.0: Multilingual Open Custom Keyword Spotting Testset,Resources for Spoken Language Processing,2023,"The main purpose of this work is to create a comprehensive audio testset that can be used to evaluate custom keyword spotting (KWS) models and to benchmark different KWS solutions. We also propose a set of requirements that should be followed while creating testsets to evaluate custom KWS models. We consider multiple versions of the problem: text and audio-based keyword spotting, as well as offline and online (streaming) modes. Our testset named MOCKS is based on LibriSpeech and Mozilla Common Voice datasets. We used automatically generated alignments to extract parts of the recordings, which were split into keywords and test samples. The resulting testset contains almost 50,000 keywords. It contains audio data in English, French, German, Italian, and Spanish, but can be easily extended to other languages. MOCKS has been made publicly available to the research community. Initial KWS experiments run on MOCKS suggest that it can serve as a challenging testset for future research.",True
eisenstein23_interspeech,https://www.isca-archive.org/interspeech_2023/eisenstein23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/eisenstein23_interspeech.html,MD3: The Multi-Dialect Dataset of Dialogues,Resources for Spoken Language Processing,2023,"We introduce a new dataset of conversational speech representing English from India, Nigeria, and the United States. The Multi-Dialect Dataset of Dialogues (MD3) strikes a new balance between open-ended conversational speech and task-oriented dialogue by prompting participants to perform a series of short information-sharing tasks. This facilitates quantitative cross-dialectal comparison, while avoiding the imposition of a restrictive task structure that might inhibit the expression of dialect features. Preliminary analysis of the dataset reveals significant differences in syntax and in the use of discourse markers. The dataset, which will be made publicly available with the publication of this paper, includes more than 20 hours of audio and more than 200,000 orthographically-transcribed tokens.",True
anwar23_interspeech,https://www.isca-archive.org/interspeech_2023/anwar23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/anwar23_interspeech.html,MuAViC: A Multilingual Audio-Visual Corpus for Robust Speech Recognition and Robust Speech-to-Text Translation,Resources for Spoken Language Processing,2023,"We introduce MuAViC, a multilingual audio-visual corpus for robust speech recognition and robust speech-to-text translation providing 1200 hours of audio-visual speech in 9 languages. It is fully transcribed and covers 6 English-to-X translation as well as 6 X-to-English translation directions. To the best of our knowledge, this is the first open benchmark for audio-visual speech-to-text translation and the largest open benchmark for multilingual audio-visual speech recognition. Our baseline results show that MuAViC is effective for building noise-robust speech recognition and translation models. We make the corpus available at https://github.com/facebookresearch/muavic.",True
suwanbandit23_interspeech,https://www.isca-archive.org/interspeech_2023/suwanbandit23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/suwanbandit23_interspeech.html,Thai Dialect Corpus and Transfer-based Curriculum Learning Investigation for Dialect Automatic Speech Recognition,Resources for Spoken Language Processing,2023,"We release 840 hours of read speech multi-dialect ASR corpora consisting of 700 hours of main Thai dialect, named Thai-central, and 40 hours for each local dialect , named Thai-dialect, with transcripts and their translations to Thai. The dialects, selected to represent different regions of Thailand, are Khummuang, Korat, and Pattani. We also release the baseline dialectal ASR models trained using the curriculum learning approach. We found that the pre-training with the high-resource main dialect and target dialect generally yields the best performance. We believe that the availability of our corpora would contribute to the problem of low-resource Thai dialects. The corpus data will be available on Github.",True
xiao23d_interspeech,https://www.isca-archive.org/interspeech_2023/xiao23d_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/xiao23d_interspeech.html,HK-LegiCoST: Leveraging Non-Verbatim Transcripts for Speech Translation,Resources for Spoken Language Processing,2023,"We introduce HK-LegiCoST, a new three-way parallel corpus of Cantonese-English translations, containing 600+ hours of Cantonese audio, its standard traditional Chinese transcript, and English translation, segmented and aligned at the sentence level. We describe the notable challenges in corpus preparation: segmentation, alignment of long audio recordings, and sentence-level alignment with non-verbatim transcripts. Such transcripts make the corpus suitable for speech translation research when there are significant differences between the spoken and written forms of the source language. Due to its large size, we are able to demonstrate competitive speech translation baselines on HK-LegiCoST and extend them to promising cross-corpus results on the FLEURS Cantonese subset. These results deliver insights into speech recognition and translation research in languages for which non-verbatim or ''noisy'' transcription is common due to various factors, including vernacular and dialectal speech.",True
chua23_interspeech,https://www.isca-archive.org/interspeech_2023/chua23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/chua23_interspeech.html,MERLIon CCS Challenge: A English-Mandarin code-switching child-directed speech corpus for language identification and diarization,MERLIon CCS Challenge: Multilingual Everyday Recordings - Language Identification On Code-Switched Child-Directed Speech,2023,"To enhance the reliability and robustness of language identification(LID) and language diarization(LD) systems for heterogeneous populations and scenarios, there is a need for speech processing models to be trained on datasets that feature diverse language registers and speech patterns. We present the MERLIon CCS challenge, featuring a first-of-its-kind Zoom video call dataset of parent-child shared book reading, of over 30 hours with over 300 recordings, annotated by multilingual transcribers using a high-fidelity linguistic transcription protocol. The audio corpus features spontaneous and in-the-wild English-Mandarin code-switching, child-directed speech in non-standard accents with diverse language-mixing patterns recorded in a variety of home environments. This report describes the corpus, as well as LID and LD results for our baseline and several systems submitted to the MERLIon CCS challenge using the corpus.",True
gupta23_interspeech,https://www.isca-archive.org/interspeech_2023/gupta23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/gupta23_interspeech.html,Spoken Language Identification System for English-Mandarin Code-Switching Child-Directed Speech,MERLIon CCS Challenge: Multilingual Everyday Recordings - Language Identification On Code-Switched Child-Directed Speech,2023,"This work focuses on improving the Spoken Language Identification (LangId) system for a challenge that focuses on developing robust language identification systems that are reliable for non-standard, accented (Singaporean accent), spontaneous code-switched, and child-directed speech collected via Zoom. We propose a two-stage Encoder-Decoder-based E2E model. The encoder module consists of 1D depth-wise separable convolutions with Squeeze-and-Excitation (SE) layers with a global context. The decoder module uses an attentive temporal pooling mechanism to get fixed length time-independent feature representation. The total number of parameters in the model is around 22.1 M, which is relatively light compared to using some large-scale pre-trained speech models. We achieved an EER of 15.6% in the closed track and 11.1% in the open track (baseline system 22.1%). We also curated additional LangId data from YouTube videos (having Singaporean speakers), which will be released for public use.",True
kodali23_interspeech,https://www.isca-archive.org/interspeech_2023/kodali23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/kodali23_interspeech.html,Classification of Vocal Intensity Category from Speech using the Wav2vec2 and Whisper Embeddings,Health-Related Speech Analysis,2023,"In speech communication, talkers regulate vocal intensity resulting in speech signals of different intensity categories (e.g., soft, loud). Intensity category carries important information about the speaker's health and emotions. However, many speech databases lack calibration information, and therefore sound pressure level cannot be measured from the recorded data. Machine learning, however, can be used in intensity category classification even though calibration information is not available. This study investigates pre-trained model embeddings (Wav2vec2 and Whisper) in classification of vocal intensity category (soft, normal, loud, and very loud) from speech signals expressed using arbitrary amplitude scales. We use a new database consisting of two speaking tasks (sentence and paragraph). Support vector machine is used as a classifier. Our results show that the pre-trained model embeddings outperformed three baseline features, providing improvements of up to 7%(absolute) in accuracy.",True
kathan23_interspeech,https://www.isca-archive.org/interspeech_2023/kathan23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/kathan23_interspeech.html,The effect of clinical intervention on the speech of individuals with PTSD: features and recognition performances,Health-Related Speech Analysis,2023,"Post-traumatic stress disorder (PTSD) is an anxiety disorder that can occur as a response to traumatic experiences, such as catastrophic events, and can have a detrimental influence on mental wellbeing. Furthermore, PTSD is present in 5-10 % of the population, making it a prevalent disorder in our time, thus necessitating a timely diagnosis and proper treatment. In this paper, we present results for PTSD detection based on speech recordings on a newly collected dataset consisting of 15 participants, including speakers with PTSD and a control group. Moreover, the dataset includes speech data immediately before and after a clinical intervention (i.e., acupuncture-supported psychotherapy), allowing us to examine the effect of a single treatment session. In our experiments, we achieve a best area under the curve (AUC) of 82 % using solely pre-treatment data. Finally, we analyse prominent acoustic patterns of individuals with PTSD compared to the control group.",True
tao23_interspeech,https://www.isca-archive.org/interspeech_2023/tao23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/tao23_interspeech.html,The Androids Corpus: A New Publicly Available Benchmark for Speech Based Depression Detection,Health-Related Speech Analysis,2023,"This paper presents the Androids Corpus, a new benchmark for speech-based automatic depression detection. The corpus is a collection of 228 recordings uttered by 118 native Italian speakers, including 64 who were diagnosed with depression by professional psychiatrists. Out of the 228 recordings, 112 contain read speech (all speakers read the same text) and 116 contain spontaneous speech (all speakers answer the same questions posed by an interviewer). For 110 speakers, including 58 diagnosed with depression, the corpus includes both read and spontaneous speech samples. Overall, the total duration of the material is 1 hour, 33 minutes and 49 seconds for read speech and 7 hours, 24 minutes and 22 seconds for spontaneous speech. Besides the data, the corpus includes experimental protocols that can be replicated, thus ensuring reproducibility of the experiments performed over it and comparison of the results.",True
li23w_interspeech,https://www.isca-archive.org/interspeech_2023/li23w_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/li23w_interspeech.html,Few-shot Class-incremental Audio Classification Using Stochastic Classifier,Automatic Audio Classification and Audio Captioning,2023,"It is generally assumed that number of classes is fixed in current audio classification methods, and the model can recognize pre-given classes only. When new classes emerge, the model needs to be retrained with adequate samples of all classes. If new classes continually emerge, these methods will not work well and even infeasible. In this study, we propose a method for few-shot class-incremental audio classification, which continually recognizes new classes and remember old ones. The proposed model consists of an embedding extractor and a stochastic classifier. The former is trained in base session and frozen in incremental sessions, while the latter is incrementally expanded in all sessions. Two datasets (NS-100 and LS-100) are built by choosing samples from audio corpora of NSynth and LibriSpeech, respectively. Results show that our method exceeds four baseline ones in average accuracy and performance dropping rate. Code is at https://github.com/vinceasvp/meta-sc.",True
archerboyd23_interspeech,https://www.isca-archive.org/interspeech_2023/archerboyd23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/archerboyd23_interspeech.html,Head movements in two- and four-person interactive conversational tasks in noisy and moderately reverberant conditions,"Speech Perception, Production, and Acquisition 2",2023,"Multi-modal processing schemes are of increasing importance for adaptive hearing devices. However, more data is required to understand interactions in complex application scenarios. In this study, the speech and head movements of eight normal-hearing participants were recorded in two- and four-person interactive conversational tasks, with and without 4-talker babble noise at 75 dB(A) and reverberation times of 0.25 and 0.6 s. Two-person conversations showed a head movement (yaw) interquartile range of 11.6Â° while four-person conversations showed a statistically significantly different interquartile range of 21.9Â°. No effect of acoustic condition was observed. The recorded data were also successfully used to test a previously published hearing-device direction of arrival estimation algorithm that utilized head movement information and correlation lag between acoustic signals from the left and right ear.",True
zhang23f_interspeech,https://www.isca-archive.org/interspeech_2023/zhang23f_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/zhang23f_interspeech.html,Emotion Classification with EEG Responses Evoked by Emotional Prosody of Speech,"Speech Perception, Production, and Acquisition 2",2023,"Emotion classification with EEG responses can be used in human-computer interaction, security, medical treatment, etc. Neural responses recorded via EEG can reflect more direct and objective emotional information than other behavioral signals (i.e., facial expression...). In most previous studies, only features of EEG were used as input for machine learning models. In this work, we assumed that the emotional features included in speech stimuli could assist in emotion recognition with EEG when the emotion is evoked by the emotional prosody of speech. An EEG data corpus was collected with specific speech stimuli, in which emotion was represented with only speech prosody and without semantic context. A novel EEG-Prosody CRNN model was proposed to classify four types of typical emotions. The classification accuracy can achieve at 82.85% when the prosody features of speech were integrated as input, which outperformed most audio-evoked EEG-based emotion classification methods.",True
wells23_interspeech,https://www.isca-archive.org/interspeech_2023/wells23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/wells23_interspeech.html,A Low-Resource Pipeline for Text-to-Speech from Found Data With Application to Scottish Gaelic,Speech Synthesis,2023,"In this work we present an end-to-end pipeline for building a speech corpus and text-to-speech synthesis system for a new language without reference to any expert-defined linguistic resources. We segment and align over 85 hours of Scottish Gaelic recordings found online and select 2- and 8-hour subsets with comprehensive coverage of speech sounds based on self-supervised discrete acoustic unit sequences. We then compare FastPitch models trained on these relatively small data sets using character, acoustic unit and phone inputs. According to native speaker listening test judgements, characters serve well for Gaelic given its regular orthography, even in these limited data scenarios. We release our corpus building recipe so that others may easily apply our work to new languages.",True
bhogale23_interspeech,https://www.isca-archive.org/interspeech_2023/bhogale23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/bhogale23_interspeech.html,Vistaar: Diverse Benchmarks and Training Sets for Indian Language ASR,"Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 4",2023,"Improving ASR systems is necessary to make new LLM-based use-cases accessible to people across the globe. In this paper, we focus on Indian languages, and make the case that diverse benchmarks are required to evaluate and improve ASR systems for Indian languages. To address this, we collate Vistaar as a set of 59 benchmarks across various language and domain combinations, on which we evaluate 3 publicly available ASR systems and 2 commercial systems. We also train IndicWhisper models by fine-tuning the Whisper models on publicly available training datasets across 12 Indian languages totalling to 10.7K hours. We show that IndicWhisper significantly improves on considered ASR systems on the Vistaar benchmark. Indeed, IndicWhisper has the lowest WER in 39 out of the 59 benchmarks, with an average reduction of 4.1 WER. We open-source all datasets, code and models - https://github.com/AI4Bharat/vistaar",True
kondratenko23_interspeech,https://www.isca-archive.org/interspeech_2023/kondratenko23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/kondratenko23_interspeech.html,Hybrid Dataset for Speech Emotion Recognition in Russian Language,Speech Emotion Recognition 3,2023,"We present a new data set for speech emotion recognition (SER) tasks called Dusha. The corpus contains approximately 350 hours of data, more than 300 000 audio recordings of Russian speech, and their transcripts. Therefore it is the biggest open bi-modal data collection with an open license for SER tasks nowadays. This data set is the first speech emotion corpus in Russian, including both crowd-sourced acted and real-life emotions from podcasts, with multiple speakers and scalable data set size. Acted subset has a more balanced class distribution than the unbalanced real-life part consisting of audio podcasts. So the first one is suitable for model pre-training, and the second is elaborated for fine-tuning purposes, model approbation, and validation. This paper describes in detail our collecting procedure, pre-processing routine, annotation, and experiment with a baseline model to demonstrate some actual metrics which could be obtained with the Dusha data set.",True
rumberg23_interspeech,https://www.isca-archive.org/interspeech_2023/rumberg23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/rumberg23_interspeech.html,Uncertainty Estimation for Connectionist Temporal Classification Based Automatic Speech Recognition,Connecting Speech-science and Speech-technology for Children's Speech,2023,"Predictive uncertainty estimation of deep neural networks is important when their outputs are used for high stakes decision making. We investigate token-level uncertainty of connectionist temporal classification (CTC) based automatic speech recognition models. We propose an approach, which considers that not all changes at frame-level lead to a change at token-level after CTC decoding. The approach shows promising performance for prediction of recognition errors on TIMIT, Mozilla Common Voice (MCV) and kidsTALC, a corpus of children's speech, using two different model architectures, while introducing only negligible computational overhead. Our approach identifies over 80% of a wav2vec2.0 model's errors on MCV by selecting 10% of the tokens. We further show, that the predictive uncertainty estimate relates to the uncertainty of a human annotator, by re-annotating 500 utterances of kidsTALC.",True
lavechin23_interspeech,https://www.isca-archive.org/interspeech_2023/lavechin23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/lavechin23_interspeech.html,BabySLM: language-acquisition-friendly benchmark of self-supervised spoken language models,Connecting Speech-science and Speech-technology for Children's Speech,2023,"Self-supervised techniques for learning speech representations have been shown to develop linguistic competence from exposure to speech without the need for human labels. In order to fully realize the potential of these approaches and further our understanding of how infants learn language, simulations must closely emulate real-life situations by training on developmentally plausible corpora and benchmarking against appropriate test sets. To this end, we propose a language-acquisition-friendly benchmark to probe spoken language models at the lexical and syntactic levels, both of which are compatible with the vocabulary typical of children's language experiences. This paper introduces the benchmark and summarizes a range of experiments showing its usefulness. In addition, we highlight two exciting challenges that need to be addressed for further progress: bridging the gap between text and speech and between clean speech and in-the-wild speech.",True
shetty23_interspeech,https://www.isca-archive.org/interspeech_2023/shetty23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/shetty23_interspeech.html,Developmental Articulatory and Acoustic Features for Six to Ten Year Old Children,Connecting Speech-science and Speech-technology for Children's Speech,2023,"In this paper, we study speech development in children using longitudinal acoustic and articulatory data. Data were collected yearly from grade 1 to grade 4 from four female and four male children. We analyze acoustic and articulatory properties of four corner vowels: /Ã¦/, /i/, /u/, and /É/, each occurring in two different words (different surrounding contexts). Acoustic features include formant frequencies and subglottal resonances (SGRs). Articulatory features include tongue curvature degree (TCD) and tongue curvature position (TCP). Based on the analyses, we observe the emergence of sex-based differences starting from grade 2. Similar to adults, the SGRs divide the vowel space into high, low, front, and back regions at least as early as grade 2. On average, TCD is correlated with vowel height and TCP with vowel frontness. Children in our study used varied articulatory configurations to achieve similar acoustic targets.",True
sy23_interspeech,https://www.isca-archive.org/interspeech_2023/sy23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/sy23_interspeech.html,Measuring Language Development From Child-centered Recordings,Connecting Speech-science and Speech-technology for Children's Speech,2023,"Standard ways to measure child language development from spontaneous corpora rely on detailed linguistic descriptions of a language as well as exhaustive transcriptions of the child's speech, which today can only be done through costly human labor. We tackle both issues by proposing (1) a new language development metric (based on entropy) that does not require linguistic knowledge other than having a corpus of text in the language in question to train a language model, (2) a method to derive this metric directly from speech based on a smaller text-speech parallel corpus. Here, we present descriptive results on an open archive including data from six English-learning children as a proof of concept. We document that our entropy metric documents a gradual convergence of children's speech towards adults' speech as a function of age, and it also correlates moderately with lexical and morphosyntactic measures derived from morphologically-parsed transcriptions.",True
soltau23_interspeech,https://www.isca-archive.org/interspeech_2023/soltau23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/soltau23_interspeech.html,Speech Aware Dialog System Technology Challenge (DSTC11),Dialog Management,2023,"Most research on task oriented dialog modeling is based on written text input. However, practical dialog systems often use spoken input. Typically, input speech is converted into text using ASR, which are error-prone. Furthermore, most systems don't address the differences in written and spoken language. The research on this topic is stymied by the lack of a public corpus. Motivated by these considerations, we hosted a speech-aware dialog state tracking challenge and created a public corpus which can be used to investigate the performance gap between the written and spoken input. We created three spoken versions of the popular written-domain MultiWoz task and provide waveforms, ASR transcripts, and audio encodings to encourage wider participation from teams that may not have access to ASR systems. In this paper, we describe the corpus, report results from participating teams, provide preliminary analyses of their results, and summarize the current state-of-the-art in this domain.",True
hope23_interspeech,https://www.isca-archive.org/interspeech_2023/hope23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/hope23_interspeech.html,Nonbinary American English speakers encode gender in vowel acoustics,"Phonetics, Phonology, and Prosody 2",2023,"Encoding of gender in speech is a well-researched phenomenon, especially as it concerns men and women. Men tend to produce certain acoustic characteristics in certain ways (low fundamental frequency (F0), lower formant frequencies, lower center of gravity for [s] in English) compared to women, though these characteristics also differ based on other group memberships (e.g. race, sexuality, etc). Those who are more feminine, regardless of categorical gender, have been found to produce an increase in F0 and a larger vowel space. However, these previous studies used largely cisgender women and men or only examined encoding of binary gender in speech and did not consider encoding of ""other"" or nonbinary gender in speech. This study recruited American English nonbinary speakers to record 400 utterances and correlated acoustic characteristics with multidimensional gender. Masculine, feminine, and ""other"" gender are significantly correlated with vowel acoustics.",True
issa23_interspeech,https://www.isca-archive.org/interspeech_2023/issa23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/issa23_interspeech.html,Durational and Non-durational Correlates of Lexical and Derived Geminates in Arabic,"Phonetics, Phonology, and Prosody 2",2023,"This paper reports on the phonetic and phonological patterns of gemination in Tripolitanian Libyan Arabic (TLA). While previous studies on Arabic gemination have either focused on lexical geminates or reported results on data that contains both lexical and derived geminates, without investigating its effect on the phonetic output, the present study investigates the effect of the phonological status of a geminate on the phonetic realization. Several measurements were obtained including target segments duration, RMS amplitude and F1, F2 and F3 for the target consonants. Preliminary results suggest that the acoustic distinction between singleton and geminate consonants in TLA is dependent mainly on durational correlates. There was no evidence of differences in RMS amplitude between singleton and geminate consonants of any type. F1, F2 and F3 frequencies are found to show similar patterns for singleton and geminate types for all sounds, suggesting no gestural effects of gemination in TLA.",True
lennes23_interspeech,https://www.isca-archive.org/interspeech_2023/lennes23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/lennes23_interspeech.html,Pitch distributions in a very large corpus of spontaneous Finnish speech,"Phonetics, Phonology, and Prosody 2",2023,"Speakers differ in the pitch range they use in their speech. In order to analyze the functional aspects of pitch, the typical pitch range of each individual is needed as reference. However, systematically collected pitch data from a sufficiently large corpus have not been previously available. We analyze the pitch distributions of individual speakers in a subset of the Donate Speech Corpus, collected from speakers of Finnish in 2020-2021. We report pitch analysis results based on samples from 8197 speakers and 1475 hours of speech. We compare the results obtained from male and female speakers in different age groups.",True
pavankalyan23_interspeech,https://www.isca-archive.org/interspeech_2023/pavankalyan23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/pavankalyan23_interspeech.html,Narrator or Character: Voice Modulation in an Expressive Multi-speaker TTS,Speech Synthesis: Expressivity,2023,"Current Text-to-Speech (TTS) systems are trained on audiobook data and perform well in synthesizing read-style speech. In this work, we are interested in synthesizing audio stories as narrated to children. The storytelling style is more expressive and requires perceptible changes of voice across the narrator and story characters. To address these challenges, we present a new TTS corpus of English audio stories for children with 32.7 hours of speech by a single female speaker with a UK accent. We provide evidence of the salient differences in the suprasegmentals of the narrator and character utterances in the dataset, motivating the use of a multi-speaker TTS for our application. We use a fine-tuned BERT model to label each sentence as being spoken by a narrator or character that is subsequently used to condition the TTS output. Experiments show our new TTS system is superior in expressiveness in both A-B preference and MOS testing compared to reading-style TTS and single-speaker TTS.",True
nguyen23_interspeech,https://www.isca-archive.org/interspeech_2023/nguyen23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/nguyen23_interspeech.html,Expresso: A Benchmark and Analysis of Discrete Expressive Speech Resynthesis,Speech Synthesis: Expressivity,2023,"Recent work has shown that it is possible to resynthesize high-quality speech based, not on text, but on low bitrate discrete units that have been learned learned in a self-supervised fashion and can therefore capture expressive aspects of speech that are hard to transcribe. The adoption of these methods is still limited by the fact that most speech synthesis datasets are read, severely limiting spontaneity and expressivity. Here, we introduce EXPRESSO, a high-quality expressive speech dataset for textless speech synthesis that includes both read speech and improvised dialogues rendered in 26 expressive styles. We illustrate the challenges and potentials of this dataset with an expressive resynthesis benchmark where the task is to encode the input in low-bitrate units and resynthesize it in a target voice while preserving content and style. We evaluate resynthesis quality with automatic metrics for different self-supervised discrete encoders, and explore tradeoffs between quality, bitrate and invariance to speaker and style. The dataset, evaluation metrics and baseline models will be open sourced.",True
wang23fa_interspeech,https://www.isca-archive.org/interspeech_2023/wang23fa_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/wang23fa_interspeech.html,ComedicSpeech: Text To Speech For Stand-up Comedies in Low-Resource Scenarios,Speech Synthesis: Expressivity,2023,"Text to Speech (TTS) models can generate natural and high-quality speech, but it is not expressive enough when synthesizing speech with dramatic expressiveness, such as stand-up comedies. Considering comedians have diverse personal speech styles, including personal prosody, rhythm, and fillers, it requires real-world datasets and strong speech style modeling capabilities, which brings challenges. In this paper, we construct a new dataset and develop ComedicSpeech, a TTS system tailored for the stand-up comedy synthesis in low-resource scenarios. First, we extract prosody representation by the prosody encoder and condition it to the TTS model in a flexible way. Second, we enhance the personal rhythm modeling by a conditional duration predictor. Third, we model the personal fillers by introducing comedian-related special tokens. Experiments show that ComedicSpeech achieves better expressiveness than baselines with only ten-minute training data for each comedian.",True
geneva23_interspeech,https://www.isca-archive.org/interspeech_2023/geneva23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/geneva23_interspeech.html,Accentor: An Explicit Lexical Stress Model for TTS Systems,Speech Synthesis: Expressivity,2023,"The accurate placement of word stress is a critical component of the correct pronunciation of words. Contemporary publicly available text-to-speech (TTS) datasets have a relatively narrow coverage of unique words, which causes modern neural TTS systems to synthesize speech that often suffers from lexical stress errors. In this work, we propose an efficient approach for explicitly modeling lexical stress knowledge with a dedicated Accentor neural network. The Accentor is trained separately on a large lexically diverse stress-annotated text corpus that is automatically compiled using an automatic speech recognition system. We demonstrate that the Accentor can be combined with a TTS acoustic model to reliably control the word stress encoded in the generated acoustic features. Experiments show that our approach increases the stress prediction accuracy by a factor of 12 in comparison to other modern TTS systems and improves the naturalness and comprehensibility of the synthesized speech.",True
huang23h_interspeech,https://www.isca-archive.org/interspeech_2023/huang23h_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/huang23h_interspeech.html,wav2vec 2.0 ASR for Cantonese-Speaking Older Adults in a Clinical Setting,"Speech Recognition: Signal Processing, Acoustic Modeling, Robustness, Adaptation 5",2023,"The lack of large-scale speech corpora for Cantonese and older adults has impeded the academia's research of automatic speech recognition (ASR) systems for the two. On the other hand, the recent success of self-supervised speech representation learning has shown its competitiveness in low-resource ASR. This work therefore studies the application of wav2vec 2.0 ASR using monolingual and cross-lingual pre-trained models on a developing speech corpus, CU-MARVEL, which is dedicated to the automated screening of neurocognitive disorders (NCD) for Cantonese-speaking older adults in Hong Kong. We detail our data preparation procedures for creating a monolingual wav2vec 2.0 model from scratch and further pre-training a cross-lingual model. We report the performance of our wav2vec 2.0 ASR models on the said corpus and present a preliminary analysis of the relationship between the ASR performance of older adult speech and various demographic characteristics.",True
sankar23_interspeech,https://www.isca-archive.org/interspeech_2023/sankar23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/sankar23_interspeech.html,Investigating the dynamics of hand and lips in French Cued Speech using attention mechanisms and CTC-based decoding,"Speech, Voice, and Hearing Disorders 2",2023,"Hard of hearing or profoundly deaf people make use of cued speech (CS) as a communication tool to understand spoken language. By delivering cues that are relevant to the phonetic information, CS offers a way to enhance lipreading. In literature, there have been several studies on the dynamics between the hand and the lips in the context of human production. This article proposes a way to investigate how a neural network learns this relation for a single speaker while performing a recognition task using attention mechanisms. Further, an analysis of the learnt dynamics is utilized to establish the relationship between the two modalities and extract automatic segments. For the purpose of this study, a new dataset has been recorded for French CS. Along with the release of this dataset, a benchmark will be reported for word-level recognition, a novelty in the automatic recognition of French CS.",True
obrien23_interspeech,https://www.isca-archive.org/interspeech_2023/obrien23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/obrien23_interspeech.html,Differentiating acoustic and physiological features in speech for hypoxia detection,"Speech, Voice, and Hearing Disorders 2",2023,"In order to stave off the effects of hypoxia, speech may become limited at elevated altitudes. This paper evaluates the role of speech on acoustic and physiological features used to detect hypoxia. Acoustic, cerebral blood oxygenation, and cardiac signals were recorded from participants who completed control and normobaric hypoxia experimental conditions. Acoustic and physiological features were extracted from (non-)speech segments via a voice activity detection method. Support Vector Machines were used to evaluate hypoxia classification using independent and combined features produced at sea-level and simulated 5 km altitudes. Models were built upon a 4-fold cross-validation design and evaluated on an independent dataset. Our results confirmed the importance of physiological features when detecting hypoxia. When combined, acoustic features boosted performance by 10% at 5 km in comparison to sea-level. Hypoxia detection may be improved by distinguishing respiration from non-speech.",True
shi23c_interspeech,https://www.isca-archive.org/interspeech_2023/shi23c_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/shi23c_interspeech.html,Semantic VAD: Low-Latency Voice Activity Detection for Speech Interaction,Speech Activity Detection and Modeling,2023,"For speech interaction, voice activity detection (VAD) is often used as a front-end. However, traditional VAD algorithms usually need to wait for a continuous tail silence to reach a preset maximum duration before segmentation, resulting in a large latency that affects user experience. In this paper, we propose a novel semantic VAD for low-latency segmentation. Different from existing methods, a frame-level punctuation prediction task is added to the semantic VAD, and the artificial endpoint is included in the classification category in addition to the often-used speech presence and absence. To enhance the semantic information of the model, we also incorporate an automatic speech recognition (ASR) related semantic loss. Evaluations on an internal dataset show that the proposed method can reduce the average latency by 53.3% without significant deterioration of character error rate in the back-end ASR compared to the traditional VAD approach.",True
javed23_interspeech,https://www.isca-archive.org/interspeech_2023/javed23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/javed23_interspeech.html,Svarah: Evaluating English ASR Systems on Indian Accents,Multilingual Models for ASR,2023,"India is the second largest English-speaking country in the world with a speaker base of roughly 130 million. Thus, it is imperative that automatic speech recognition (ASR) systems for English should be evaluated on Indian accents. Unfortunately, Indian speakers find a very poor representation in existing English ASR benchmarks such as LibriSpeech, Switchboard, Speech Accent Archive, etc. In this work, we address this gap by creating Svarah, a benchmark that contains 9.6 hours of transcribed English audio from 117 speakers across 65 geographic locations throughout India, resulting in a diverse range of accents. Svarah comprises both read speech and spontaneous conversational data, covering various domains, such as history, culture, tourism, etc., ensuring a diverse vocabulary. We evaluate 6 open source ASR models and 2 commercial ASR systems on Svarah and show that there is clear scope for improvement on Indian accents.",True
talafha23_interspeech,https://www.isca-archive.org/interspeech_2023/talafha23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/talafha23_interspeech.html,N-Shot Benchmarking of Whisper on Diverse Arabic Speech Recognition,Multilingual Models for ASR,2023,"Whisper, the recently developed multilingual weakly supervised model, is reported to perform well on multiple speech recognition benchmarks in both monolingual and multilingual settings. However, it is not clear how Whisper would fare under diverse conditions even on languages it was evaluated on such as Arabic. In this work, we address this gap by comprehensively evaluating Whisper on several varieties of Arabic speech for the ASR task. Our evaluation covers most publicly available Arabic speech data and is performed under n-shot (zero-, few-, and full) finetuning. We also investigate the robustness of Whisper under completely novel conditions such as in dialect-accented standard Arabic and in unseen dialects for which we develop evaluation data. Although Whisper zero-shot outperforms fully finetuned XLS-R models on all datasets, its performance deteriorates significantly in the zero-shot setting for five unseen dialects (i.e., Algeria, Jordan, Palestine, UAE, and Yemen).",True
picheny23_interspeech,https://www.isca-archive.org/interspeech_2023/picheny23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/picheny23_interspeech.html,The MALACH Corpus: Results with End-to-End Architectures and Pretraining,Multilingual Models for ASR,2023,"The MALACH corpus contains approximately 375 hours of Holocaust survivor testimonies along with transcripts (for approximately half the data) and audio. It is an extremely difficult corpus for speech recognition, encompassing accented, emotional speech, in many cases from elderly survivors. Nevertheless, significant progress has been made on speech recognition on MALACH with WERs now typically hovering at a 20% level for hybrid speech recognition systems. The purpose of this paper is to examine if recent developments in end-to-end architectures and pretraining with self-supervision continue to drive down performance as they do on popular read corpora such as Librispeech. We also experiment with leveraging the large fraction of unlabeled corpus data by extracting pseudolabels produced from previously trained systems. It is found that the best system - a fine-tuned wav2vec2 system trained on labeled and pseudolabeled data - achieves a WER of 13.5%, a huge gain in performance.",True
doukhan23_interspeech,https://www.isca-archive.org/interspeech_2023/doukhan23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/doukhan23_interspeech.html,Voice Passing : a Non-Binary Voice Gender Prediction System for evaluating Transgender voice transition,Perception of Paralinguistics,2023,"This paper presents a software allowing to describe voices using a continuous Voice Femininity Percentage (VFP). This system is intended for transgender speakers during their voice transition and for voice therapists supporting them in this process. A corpus of 41 French cis- and transgender speakers was recorded. A perceptual evaluation allowed 57 participants to estimate the VFP for each voice. Binary gender classification models were trained on external gender-balanced data and used on overlapping windows to obtain average gender prediction estimates, which were calibrated to predict VFP and obtained higher accuracy than F0 or vocal track length-based models. Training data speaking style and DNN architecture were shown to impact VFP estimation. Accuracy of the models was affected by speakers' age. This highlights the importance of style, age, and the conception of gender as binary or not, to build adequate statistical representations of cultural concepts.",True
kirkland23_interspeech,https://www.isca-archive.org/interspeech_2023/kirkland23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/kirkland23_interspeech.html,Pardon my disfluency: The impact of disfluency effects on the perception of speaker competence and confidence,Perception of Paralinguistics,2023,"Disfluencies are a hallmark of spontaneous speech and play an important role in conversation, yet have been shown to negatively impact judgments about speakers. We explored the role of disfluencies in the perception of competence, sincerity and confidence in public speaking contexts, using synthesized spontaneous speech. In one experiment, listeners rated 30-40-second clips which varied in terms of whether they contained filled pauses, as well as the number and types of repetition. Both the overall number of disfluencies and the repetition type had an impact on competence and confidence, and disfluent speech was also rated as less sincere. In the second experiment, the negative effects of repetition type on competence were attenuated when participants attributed disfluency to anxiety.",True
nam23_interspeech,https://www.isca-archive.org/interspeech_2023/nam23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/nam23_interspeech.html,Disentangled Representation Learning for Multilingual Speaker Recognition,Speaker and Language Identification 3,2023,"The goal of this paper is to learn robust speaker representation for bilingual speaking scenario. The majority of the world's population speak at least two languages; however, most speaker recognition systems fail to recognise the same speaker when speaking in different languages.  Popular speaker recognition evaluation sets do not consider the bilingual scenario, making it difficult to analyse the effect of bilingual speakers on speaker recognition performance. In this paper, we publish a large-scale evaluation set named VoxCeleb1-B derived from VoxCeleb that considers bilingual scenarios. We introduce an effective disentanglement learning strategy that combines adversarial and metric learning-based methods. This approach addresses the bilingual situation by disentangling language-related information from speaker representation while ensuring stable speaker representation learning. Our language-disentangled learning method only uses language pseudo-labels without manual information.",True
sullivan23_interspeech,https://www.isca-archive.org/interspeech_2023/sullivan23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/sullivan23_interspeech.html,On the Robustness of Arabic Speech Dialect Identification,Speaker and Language Identification 3,2023,"Arabic dialect identification (ADI) tools are an important part of the large-scale data collection pipelines necessary for training speech recognition models. As these pipelines require application of ADI tools to potentially out-of-domain data, we aim to investigate how vulnerable the tools may be to this domain shift. With self-supervised learning (SSL) models as a starting point, we evaluate transfer learning and direct classification from SSL features. We undertake our evaluation under rich conditions, with a goal to develop ADI systems from pretrained models and ultimately evaluate performance on newly collected data. In order to understand what factors contribute to model decisions, we carry out a careful human study of a subset of our data. Our analysis confirms that domain shift is a major challenge for ADI models. We also find that while self-training does alleviate this challenges, it may be insufficient for realistic conditions.",True
solanki23_interspeech,https://www.isca-archive.org/interspeech_2023/solanki23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/solanki23_interspeech.html,Do Vocal Breath Sounds Encode Gender Cues for Automatic Gender Classification?,Analysis of Speech and Audio Signals 4,2023,"The acoustic features of continuous speech, such as pitch (F0) and formant frequencies (F1, F2) have been utilized for gender classification. However, non-speech signals including vocal breath sounds have not been explored due to the absence of gender-specific acoustic features. This study investigates if vocal breath sounds carry gender information and if they can be used for automatic gender classification. The study examines the use of data-driven and knowledge-based features from breath sounds, classifier complexity, and the importance of breath signal segment location and duration. Results from experiments on 54 minutes of male and 52 minutes of female breath sounds demonstrate that classifiers with low-complexity and knowledge-based features (MFCC statistics) perform similarly to high-complexity classifiers with data-driven features. Breath segments of around 3 seconds are found to be the most suitable choice regardless of location, eliminating the need for breath cycle boundary marking.",True
xiao23b_interspeech,https://www.isca-archive.org/interspeech_2023/xiao23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/xiao23b_interspeech.html,"A Snoring Sound Dataset for Body Position Recognition: Collection, Annotation, and Analysis",Analysis of Speech and Audio Signals 4,2023,"Obstructive Sleep Apnea-Hypopnea Syndrome (OSAHS) is a chronic breathing disorder caused by a blockage in the upper airways. Snoring is a prominent symptom of OSAHS, and previous studies have attempted to identify the obstruction site of the upper airways by snoring sounds. Despite some progress, the classification of the obstruction site remains challenging in real-world clinical settings due to the influence of sleep body position on upper airways. To address this challenge, this paper proposes a snore-based sleep body position recognition dataset (SSBPR) consisting of 7570 snoring recordings, which comprises six distinct labels for sleep body position: supine, supine but left lateral head, supine but right lateral head, left-side lying, right-side lying and prone. Experimental results show that snoring sounds exhibit certain acoustic features that enable their effective utilization for identifying body posture during sleep in real-world scenarios.",True
richter23_interspeech,https://www.isca-archive.org/interspeech_2023/richter23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/richter23_interspeech.html,"Remote Assessment for ALS using Multimodal Dialog Agents: Data Quality, Feasibility and Task Compliance",Analysis of Speech and Audio Signals 4,2023,"We investigate the feasibility, task compliance and audiovisual data quality of a multimodal dialog-based solution for remote assessment of Amyotrophic Lateral Sclerosis (ALS). 53 people with ALS and 52 healthy controls interacted with Tina, a cloud-based conversational agent, in performing speech tasks designed to probe various aspects of motor speech function while their audio and video was recorded. We rated a total of 250 recordings for audio/video quality and participant task compliance, along with the relative frequency of different issues observed. We observed excellent compliance (98%) and audio (95.2%) and visual quality rates (84.8%), resulting in an overall yield of 80.8% recordings that were both compliant and of high quality. Furthermore, recording quality and compliance were not affected by level of speech severity and did not differ significantly across end devices. These findings support the utility of dialog systems for remote monitoring of speech in ALS.",True
koizumi23_interspeech,https://www.isca-archive.org/interspeech_2023/koizumi23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/koizumi23_interspeech.html,LibriTTS-R: A Restored Multi-Speaker Text-to-Speech Corpus,Speech Synthesis: Multilinguality; Evaluation,2023,"This paper introduces a new speech dataset called ""LibriTTS-R"" designed for text-to-speech (TTS) use. It is derived by applying speech restoration to the LibriTTS corpus, which consists of 585 hours of speech data at 24 kHz sampling rate from 2,456 speakers and the corresponding texts. The constituent samples of LibriTTS-R are identical to those of LibriTTS, with only the sound quality improved. Experimental results show that the LibriTTS-R ground-truth samples showed significantly improved sound quality compared to those in LibriTTS. In addition, neural end-to-end TTS trained with LibriTTS-R achieved speech naturalness on par with that of the ground-truth samples. The corpus is freely available for download from http://www.openslr.org/141/.",True
kulkarni23_interspeech,https://www.isca-archive.org/interspeech_2023/kulkarni23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/kulkarni23_interspeech.html,ClArTTS: An Open-Source Classical Arabic Text-to-Speech Corpus,Speech Synthesis: Multilinguality; Evaluation,2023,"We present a Classical Arabic Text-to-Speech (ClArTTS) corpus to facilitate the development of end-to-end TTS systems for the Arabic language. The speech is extracted from a LibriVox audiobook, which is then processed, segmented, and manually transcribed and annotated. The ClArTTS corpus contains about 12 hours of speech from a single male speaker sampled at 40100 Hz. In this paper, we describe the process of corpus creation, details of corpus statistics, and a comparison with existing resources. Furthermore, we develop two TTS systems based on Grad-TTS and Glow-TTS and illustrate the performance of the resulting systems via subjective and objective evaluations. The ClArTTS corpus is publicly available at www.clartts.com for research purposes, along with the baseline TTS systems and an interactive demo.",True
zhang23h_interspeech,https://www.isca-archive.org/interspeech_2023/zhang23h_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/zhang23h_interspeech.html,CVTE-Poly: A New Benchmark for Chinese Polyphone Disambiguation,Speech Synthesis: Multilinguality; Evaluation,2023,"Conversion from graphemes to phonemes is an essential component in Text-To-Speech systems, and in Chinese, one main challenge is polyphone disambiguation-to determine the pronunciation of characters with multiple pronunciations. In this task, the benchmark dataset Chinese Polyphone disambiguation with Pinyin (CPP) suffers from two main limitations: Firstly, it contains some wrong labels in contrast to the newest official dictionary. Secondly, it is imbalanced and hence models learned from it show a learning bias towards frequently-used pronunciations and polyphones. In this paper, we refine CPP and release a new dataset named CVTE-poly, containing 845254 samples, nearly ten times the size of CPP and is more balanced. Besides, we propose a comprehensive measurement for polyphone disambiguation task, against the data imbalance problem. Experiments show that our simple but flexible baseline trained on CVTE-poly outperforms existing models, which demonstrate the benefit of our dataset.",True
saito23b_interspeech,https://www.isca-archive.org/interspeech_2023/saito23b_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/saito23b_interspeech.html,CALLS: Japanese Empathetic Dialogue Speech Corpus of Complaint Handling and Attentive Listening in Customer Center,Speech Synthesis: Multilinguality; Evaluation,2023,"We present CALLS, a Japanese speech corpus that considers phone calls in a customer center as a new domain of empathetic spoken dialogue. The existing STUDIES corpus covers only empathetic dialogue between a teacher and student in a school. To extend the application range of empathetic dialogue speech synthesis (EDSS), we designed our corpus to include the same female speaker as the STUDIES teacher, acting as an operator in simulated phone calls. We describe a corpus construction methodology and analyze the recorded speech. We also conduct EDSS experiments using the CALLS and STUDIES corpora to investigate the effect of domain differences. The results show that mixing the two corpora during training causes biased improvements in the quality of synthetic speech due to the different degrees of expressiveness. Our project page of the corpus is http://sython.org/Corpus/STUDIES-2.",True
sharoni23_interspeech,https://www.isca-archive.org/interspeech_2023/sharoni23_interspeech.pdf,https://www.isca-archive.org/interspeech_2023/sharoni23_interspeech.html,SASPEECH: A Hebrew Single Speaker Dataset for Text To Speech and Voice Conversion,Speech Synthesis: Multilinguality; Evaluation,2023,"We present SASPEECH, a 30-hour single speaker Hebrew corpus accompanied by a text-to-speech (TTS) benchmark. Our TTS benchmark was developed with other low resource languages in mind, allowing it to be adapted and potentially generalized. For the proposed method to work, one must have several hours of recordings and transcripts or have their language included in the Whisper model. SASPEECH is the first large-scale high-quality open dataset of its kind. Thus, it allows a discussion of challenges Hebrew presents when incorporated into generative models. For instance: bridging the gap between modern Hebrew lettering which lacks diacritics and correct pronunciation. We also tackle prominent issues shared by low resource languages and examine how to evaluate output quality without a benchmark. We believe our work will facilitate future generative Hebrew tools and low resource language research. The corpus is publicly accessible at https://www.openslr.org/134.",True
xie24d_interspeech,https://www.isca-archive.org/interspeech_2024/xie24d_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/xie24d_interspeech.html,FakeSound: Deepfake General Audio Detection,Acoustic Event Detection and Classification 2,2024,"With the advancement of audio generation, generative models can produce highly realistic audios. However, the proliferation of deepfake general audio can pose negative consequences. Therefore, we propose a new task, deepfake general audio detection, which aims to identify whether audio content is manipulated and to locate deepfake regions. Leveraging an automated manipulation pipeline, a dataset named FakeSound for deepfake general audio detection is proposed, and samples can be viewed on website https://FakeSoundData.github.io. The average binary accuracy of humans on all test sets is consistently below 0.6, which indicates the difficulty humans face in discerning deepfake audio and affirms the efficacy of the FakeSound dataset. A deepfake detection model utilizing a general audio pre-trained model is proposed as a benchmark system. Experimental results demonstrate that the performance of the proposed model surpasses the state-of-the-art in deepfake speech detection and human testers.",True
ghaffarzadegan24_interspeech,https://www.isca-archive.org/interspeech_2024/ghaffarzadegan24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ghaffarzadegan24_interspeech.html,Sound of Traffic: A Dataset for Acoustic Traffic Identification and Counting,Acoustic Event Detection and Classification 2,2024,"We introduce Sound of Traffic, the largest publicly available dataset for traffic identification and counting to date. With over 415 hours of multichannel acoustic traffic data recorded in six different locations, it encompasses varying levels of traffic density and environmental conditions. In this work, we discuss strategies for automatic collection and alignment of large amount of labeled data, leveraging existing asynchronous urban sensors such as radar, cameras, and inductive coils. In addition to the dataset, we propose a simple baseline system for vehicle counting divided by type of the vehicle (passenger vs. commercial vehicle) and direction of travel (right-to-left and left-to-right), a fundamental task for traffic analysis. The dataset and baseline system serve as a starting point for researchers to develop more advanced algorithms and models in this field. The dataset can be accessed at https://zenodo.org/records/10700792 and https://zenodo.org/records/11209838.",True
jing24_interspeech,https://www.isca-archive.org/interspeech_2024/jing24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/jing24_interspeech.html,DB3V: A Dialect Dominated Dataset of Bird Vocalisation for Cross-corpus Bird Species Recognition,Detection and Classification of Bioacoustic Signals,2024,"In ornithology, bird species are known to have varieditâs widely acknowledged that bird species display diverse dialects in their calls across different regions. Consequently, computational methods to identify bird species onsolely through their calls face critsignificalnt challenges. There is growing interest in understanding the impact of species-specific dialects on the effectiveness of bird species recognition methods. Despite potential mitigation through the expansion of dialect datasets, the absence of publicly available testing data currently impedes robust benchmarking efforts. This paper presents the Dialect Dominated Dataset of Bird Vocalisation (D3BV), the first cross-corpus dataset that focuses on dialects in bird vocalisations. The D3BV comprises more than 25 hours of audio recordings from 10 bird species distributed across three distinct regions in the contiguous United States (CONUS). In addition to presenting the dataset, we conduct analyses and establish baseline models for cross-corpus bird recognition. The data and code are publicly available online: https://zenodo.org/records/11544734",True
lin24_interspeech,https://www.isca-archive.org/interspeech_2024/lin24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lin24_interspeech.html,SimuSOE: A Simulated Snoring Dataset for Obstructive Sleep Apnea-Hypopnea Syndrome Evaluation during Wakefulness,Detection and Classification of Bioacoustic Signals,2024,"Obstructive Sleep Apnea-Hypopnea Syndrome (OSAHS) is a prevalent chronic breathing disorder caused by upper airway obstruction. Previous studies advanced OSAHS evaluation through machine learning-based systems trained on sleep snoring or speech signal datasets. However, constructing datasets for training a precise and rapid OSAHS evaluation system poses a challenge, since 1) it is time-consuming to collect sleep snores and 2) the speech signal is limited in reflecting upper airway obstruction. In this paper, we propose a new snoring dataset for OSAHS evaluation, named SimuSOE, in which a novel and time-effective snoring collection method is introduced for tackling the above problems. In particular, we adopt simulated snoring which is a type of snore intentionally emitted by patients to replace natural snoring. Experimental results indicate that the simulated snoring signal during wakefulness can serve as an effective feature in OSAHS preliminary screening.",True
nakagome24_interspeech,https://www.isca-archive.org/interspeech_2024/nakagome24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/nakagome24_interspeech.html,InterBiasing: Boost Unseen Word Recognition through Biasing Intermediate Predictions,Neural Network Architectures for ASR 2,2024,"Despite recent advances in end-to-end speech recognition methods, their output is biased to the training dataâs vocabulary, resulting in inaccurate recognition of unknown terms or proper nouns. To improve the recognition accuracy for a given set of such terms, we propose an adaptation parameter-free approach based on Self-conditioned CTC. Our method improves the recognition accuracy of misrecognized target keywords by substituting their intermediate CTC predictions with corrected labels, which are then passed on to the subsequent layers. First, we create pairs of correct labels and recognition error instances for a keyword list using Text-to-Speech and a recognition model. We use these pairs to replace intermediate prediction errors by the labels. Conditioning the subsequent layers of the encoder on the labels, it is possible to acoustically evaluate the target keywords. Experiments conducted in Japanese demonstrated that our method successfully improved the F1 score for unknown words.",True
chen24c_interspeech,https://www.isca-archive.org/interspeech_2024/chen24c_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/chen24c_interspeech.html,MultiPA: A Multi-task Speech Pronunciation Assessment Model for Open Response Scenarios,Pronunciation Assessment,2024,"Pronunciation assessment models designed for open response scenarios enable users to practice language skills in a manner similar to real-life communication. However, previous open-response pronunciation assessment models have predominantly focused on a single pronunciation task, such as sentence-level accuracy, rather than offering a comprehensive assessment in various aspects. We propose MultiPA, a Multitask Pronunciation Assessment model that provides sentence-level accuracy, fluency, prosody, and word-level accuracy assessment for open responses. We examined the correlation between different pronunciation tasks and showed the benefits of multi-task learning. Our model reached the state-of-the-art performance on existing in-domain data sets and effectively generalized to an out-of-domain dataset that we newly collected. The experimental results demonstrate the practical utility of our model in real-world applications.",True
jain24_interspeech,https://www.isca-archive.org/interspeech_2024/jain24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/jain24_interspeech.html,Multimodal Segmentation for Vocal Tract Modeling,Biosignal-enabled Spoken Communication,2024,"Accurate modeling of the vocal tract is necessary to construct articulatory representations for interpretable speech processing and linguistics. However, vocal tract modeling is challenging because many internal articulators are occluded from external motion capture technologies. Real-time magnetic resonance imaging (RT-MRI) allows measuring precise movements of internal articulators during speech, but annotated datasets of MRI are limited in size due to time-consuming and computationally expensive labeling methods. We first present a deep labeling strategy for the RT-MRI video using a vision-only segmentation approach. We then introduce a multimodal algorithm using audio to improve segmentation of vocal articulators. Together, we set a new benchmark for vocal tract modeling in MRI video segmentation and use this to release labels for a 75-speaker RT-MRI dataset, increasing the amount of labeled public RT-MRI data of the vocal tract by over a factor of 9. The code and dataset labels can be found at rishiraij.github.io/multimodal-mri-avatar/.",True
yan24b_interspeech,https://www.isca-archive.org/interspeech_2024/yan24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/yan24b_interspeech.html,Auditory Attention Decoding in Four-Talker Environment with EEG,Biosignal-enabled Spoken Communication,2024,"Auditory Attention Decoding (AAD) is a technique that determines the focus of a listener's attention in complex auditory scenes according to cortical neural responses. Existing research largely examines two-talker scenarios, insufficient for real-world complexity. This study introduced a new AAD database for a four-talker scenario with speeches from four distinct talkers simultaneously presented and spatially separated, and listeners' EEG was recorded. Temporal response functions (TRFs) analysis showed that attended speech TRFs are stronger than each unattended speech. AAD methods based on stimulus-reconstruction (SR) and cortical spatial lateralization were employed and compared. Results indicated decoding accuracy of 77.5% in 60s (chance level of 25%) using SR. Using auditory spatial attention detection (ASAD) methods also indicated high accuracy (94.7% with DenseNet-3D in 1s), demonstrating ASAD methods' generalization performance.",True
lin24f_interspeech,https://www.isca-archive.org/interspeech_2024/lin24f_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lin24f_interspeech.html,ASA: An Auditory Spatial Attention Dataset with Multiple Speaking Locations,Biosignal-enabled Spoken Communication,2024,"Recent studies have demonstrated the feasibility of localizing an attended sound source from electroencephalography (EEG) signals in a cocktail party scenario. This is referred to as EEG-enabled Auditory Spatial Attention Detection (ASAD). Despite the promise, there is a lack of ASAD datasets. Most existing ASAD datasets are recorded from two speaking locations. To bridge this gap, we introduce a new Auditory Spatial Attention (ASA) dataset, featuring multiple speaking locations of sound sources. The new dataset is designed to challenge and refine deep neural network solutions in real-world applications. Furthermore, we build a channel attention convolutional neural network (CA-CNN) as a reference model for ASA, that serves as a competitive benchmark for future studies.",True
pistor24_interspeech,https://www.isca-archive.org/interspeech_2024/pistor24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/pistor24_interspeech.html,Echoes of Implicit Bias Exploring Aesthetics and Social Meanings of Swiss German Dialect Features,Individual and Social Factors in Phonetics,2024,"This study investigates the phonaesthetics and perceptual dynamics of Swiss German dialects, focusing on how particular sound features influence subjective assessments and, in doing so, contribute to dialect stereotypes. By examining 24 linguistic features of Bern and Zurich German, including nine vowels and 15 consonants in single-word utterances, we aim to fill a research gap that has been previously overlooked, despite suggestions of importance. In an online perception study, we gathered evaluations from three distinct groups of raters (N = 46) from Bern, Zurich, and Hessen, Germany, across six categories from aesthetic dimensions to stereotypical dialect attributions. The findings reveal that rater origin determines the levels of importance on evaluation categories and that certain linguistic features can be identified that are closely linked with specific perceptions (e.g., stupid or arrogant), which may foster negative biases against dialect speakers.",True
ewert24_interspeech,https://www.isca-archive.org/interspeech_2024/ewert24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ewert24_interspeech.html,Does the Lombard Effect Matter in Speech Separation? Introducing the Lombard-GRID-2mix Dataset,Source Separation 2,2024,"Inspired by the human ability of selective listening, speech separation aims to equip machines with the capability to disentangle cocktail party soundscapes into the individual sound sources. Recently, neural network based algorithms have been studied to work reliably under various conditions. However, to the best of our knowledge, a change in the speaking style has not yet been studied. The Lombard effect, a reflexive change in speaking style triggered by noisy environments, is a typical behavior in everyday conversational situations. In this work, we introduce a new first of its kind dataset, called Lombard-GRID-2mix, to study speech separation for two-speaker mixtures on normal speech and Lombard speech. In a comprehensive study, we show that speech separation systems can be equipped to work for both normal speech and Lombard speech. We apply a carefully designed finetuning method to enable the system to work even if noise is present in the Lombard speech for different SNR ratios.",True
jin24_interspeech,https://www.isca-archive.org/interspeech_2024/jin24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/jin24_interspeech.html,"LibriheavyMix: A 20,000-Hour Dataset for Single-Channel Reverberant Multi-Talker Speech Separation, ASR and Speaker Diarization","Noise Robustness, Far-Field, and Multi-Talker ASR",2024,"The evolving speech processing landscape is increasingly focused on complex scenarios like meetings or cocktail parties with multiple simultaneous speakers and far-field conditions. Existing methodologies for addressing these challenges fall into two categories: multi-channel and single-channel solutions. Single-channel approaches, notable for their generality and convenience, do not require specific information about microphone arrays. This paper presents a large-scale far-field overlapping speech dataset, crafted to advance research in speech separation, recognition, and speaker diarization. This dataset is a critical resource for decoding âWho said What and Whenâ in multitalker, reverberant environments, a daunting challenge in the field. Additionally, we introduce a pipeline system encompassing speech separation, recognition, and diarization as a foundational benchmark. Evaluations on the WHAMR! dataset validate the broad applicability of the proposed data.",True
huang24f_interspeech,https://www.isca-archive.org/interspeech_2024/huang24f_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/huang24f_interspeech.html,Improving Neural Biasing for Contextual Speech Recognition by Early Context Injection and Text Perturbation,Contextual Biasing and Adaptation,2024,"Existing research suggests that automatic speech recognition(ASR) models can benefit from additional contexts (e.g., contact lists, user specified vocabulary). Rare words and named entities can be better recognized with contexts. In this work, we propose two simple yet effective techniques to improve context-aware ASR models. First, we inject contexts into the encoders at an early stage instead of merely at their last layers. Second, to enforce the model to leverage the contexts during training, we perturb the reference transcription with alternative spellings so that the model learns to rely on the contexts to make correct predictions. On LibriSpeech, our techniques together reduce the rare word error rate by 60% and 25% relatively compared to no biasing and shallow fusion, making the new state-of-the-art performance. On SPGISpeech and a real-world dataset ConEC, our techniques also yield good improvements over the baselines.",True
wei24_interspeech,https://www.isca-archive.org/interspeech_2024/wei24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/wei24_interspeech.html,Prompt Tuning for Speech Recognition on Unknown Spoken Name Entities,Contextual Biasing and Adaptation,2024,"This paper explores the challenge of recognising relevant but previously unheard named entities in spoken input. This scenario pertains to real-world applications where establishing an automatic speech recognition (ASR) model trained on new entity phrases may not be efficient. We propose a technique that involves fine-tuning a Whisper model with a list of entity phrases as prompts. We establish a task-specific dataset where stratification of different entity phrases supports evaluation of three different scenarios in which entities might be encountered. We focus our analysis on a seen-but-unheard scenario, reflecting a situation where only textual representations of novel entity phrases are available for a commercial banking assistant bot. We show that a model tuned to anticipate prompts reflecting novel named entities makes substantial improvements in entity recall over non-tuned baseline models, and meaningful improvements in performance over models fine-tuned without a prompt.",True
porjazovski24_interspeech,https://www.isca-archive.org/interspeech_2024/porjazovski24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/porjazovski24_interspeech.html,Out-of-distribution generalisation in spoken language understanding,Spoken Language Understanding,2024,"Test data is said to be out-of-distribution (OOD) when it unexpectedly differs from the training data, a common challenge in real-world use cases of machine learning. Although OOD generalisation has gained interest in recent years, few works have focused on OOD generalisation in spoken language understanding (SLU) tasks. To facilitate research on this topic, we introduce a modified version of the popular SLU dataset SLURP, featuring data splits for testing OOD generalisation in the SLU task. We call our modified dataset SLURP For OOD generalisation, or SLURPFOOD. Utilising our OOD data splits, we find end-to-end SLU models to have limited capacity for generalisation. Furthermore, by employing model interpretability techniques, we shed light on the factors contributing to the generalisation difficulties of the models. To improve the generalisation, we experiment with two techniques, which improve the results on some, but not all the splits, emphasising the need for new techniques.",True
lee24i_interspeech,https://www.isca-archive.org/interspeech_2024/lee24i_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lee24i_interspeech.html,Speech-MASSIVE: A Multilingual Speech Dataset for SLU and Beyond,Spoken Language Understanding,2024,"We present Speech-MASSIVE, a multilingual Spoken Language Understanding (SLU) dataset comprising the speech counterpart for a portion of the MASSIVE textual corpus. Speech-MASSIVE covers 12 languages from different families and inherits from MASSIVE the annotations for the intent prediction and slot-filling tasks. Our extension is prompted by the scarcity of massively multilingual SLU datasets and the growing need for versatile speech datasets to assess foundation models (LLMs, speech encoders) across languages and tasks. We provide a multimodal, multitask, multilingual dataset and report SLU baselines using both cascaded and end-to-end architectures in various training scenarios (zero-shot, few-shot, and full fine-tune). Furthermore, we demonstrate the suitability of Speech-MASSIVE for benchmarking other tasks such as speech transcription, language identification, and speech translation. The dataset, models, and code are publicly available at: https://github.com/hlt-mt/Speech-MASSIVE",True
macaire24_interspeech,https://www.isca-archive.org/interspeech_2024/macaire24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/macaire24_interspeech.html,Towards Speech-to-Pictograms Translation,Spoken Machine Translation 1,2024,"The automatic translation of speech into pictogram terms (Speech-to-Pictos) represents a novel NLP task with the potential to enhance communication for individuals with language impairments. Recent research has not explored the adaptation of state-of-the-art methods to this task, despite its significance. In this work, we investigate two approaches: (1) the cascade approach, which combines a speech recognition system with a machine translation system, and (2) the end-to-end approach, which tailors a speech translation system. We compare state-of-the-art architectures trained on an aligned speech-to-pictogram dataset, specially created and released for this study. We conduct an in-depth automatic and human evaluation to analyze their behavior on pictogram translation. The results highlight the cascade approachâs ability to generate relevant translations from everyday read speech, while the end-to-end approach achieves competitive results with challenging acoustic data.",True
lee24e_interspeech,https://www.isca-archive.org/interspeech_2024/lee24e_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lee24e_interspeech.html,Automatic Assessment of Speech Production Skills for Children with Cochlear Implants Using Wav2Vec2.0 Acoustic Embeddings,Hearing Disorders,2024,"This study introduces an automatic assessment model for speech production skills of children with cochlear implants (CIs) to support home-based speech therapy. The model employs acoustic embeddings from self-supervised models and considers speech traits of both normal hearing (NH) adults and children, which is a novel method for evaluating speech of children with disorders. It combines phoneme embeddings and two acoustic embeddings from Wav2Vec2.0 models, each trained on the speech of NH adults and children, via multi-head attention. Using a speech corpus of Korean-speaking children with CIs, our model outperforms single-embedding methods in a Pearson correlation coefficient between predicted and expert-rated scores, with a relative improvement of 51%. The results highlight the effectiveness of Wav2Vec2.0 acoustic embeddings and the importance of incorporating both of typical speech patterns of NH adults and children in assessing speech production skills in children with CIs.",True
zhang24l_interspeech,https://www.isca-archive.org/interspeech_2024/zhang24l_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/zhang24l_interspeech.html,DysArinVox: DYSphonia & DYSarthria mandARIN speech corpus,Speech Disorders 2,2024,"This paper introduces DysArinVox, a new pathological speech corpus in Chinese. It included 173 participants from 27 healthy individuals and 146 voice disorders, whose various types and severities of vocal impairments as diagnosed by speech pathology experts via auditory perceptual evaluations and laryngoscopic imagery. DysArinVox is designed to provide a high-quality Chinese resource for AI-driven diagnostics and prognostics. To ensure the efficiency of corpus collection, we meticulously crafted recording scripts represent Mandarin phonetically, ensuring comprehensive syllable representation with minimal lexical complexity. Additionally, incorporating laryngoscopic images of patients into the dataset offers extra visual information, facilitating the development of advanced diagnostic frameworks. To our knowledge, this database represents the most comprehensive corpus of Chinese pathological speech to date.",True
zhou24e_interspeech,https://www.isca-archive.org/interspeech_2024/zhou24e_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/zhou24e_interspeech.html,YOLO-Stutter: End-to-end Region-Wise Speech Dysfluency Detection,Speech Disorders 2,2024,"Dysfluent speech detection is the bottleneck for disordered speech analysis and spoken language learning. Current state-of-the-art models are governed by rule-based systems which lack efficiency and robustness, and are sensitive to template design. In this paper, we propose YOLO-Stutter: a first end-to-end method that detects dysfluencies in a time-accurate manner. YOLO-Stutter takes  imperfect speech-text alignment as input, followed by a spatial feature aggregator, and a temporal dependency extractor to perform region-wise boundary and class predictions. We also introduce two dysfluency corpus, VCTK-Stutter and VCTK-TTS, that simulate natural spoken dysfluencies including repetition, block, missing, replacement, and prolongation. Our end-to-end method achieves state-of-the-art performance with a minimum number of trainable parameters for on both simulated data and real aphasia speech . Code and datasets are open-sourced at https://github.com/rorizzz/YOLO-Stutter",True
luz24_interspeech,https://www.isca-archive.org/interspeech_2024/luz24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/luz24_interspeech.html,Connected Speech-Based Cognitive Assessment in Chinese and English,TAUKADIAL Challenge: Speech-Based Cognitive Assessment in Chinese and English (Special Session),2024,"We present a novel benchmark dataset and prediction tasks for investigating approaches to assess cognitive function through analysis of connected speech. The dataset consists of speech samples and clinical information for speakers of Mandarin Chinese and English with different levels of cognitive impairment as well as individuals with normal cognition. These data have been carefully matched by age and sex by propensity score analysis to ensure balance and representativity in model training. The prediction tasks encompass mild cognitive impairment diagnosis and cognitive test score prediction. This framework was designed to encourage the development of approaches to speech-based cognitive assessment which generalise across languages. We illustrate it by presenting baseline prediction models that employ language-agnostic and comparable features for diagnosis and cognitive test score prediction. Unweighted average recall was 59.2% in diagnosis, and root mean squared error was 2.89 in score prediction.",True
peirolilja24_interspeech,https://www.isca-archive.org/interspeech_2024/peirolilja24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/peirolilja24_interspeech.html,Multi-speaker and multi-dialectal Catalan TTS models for video gaming,Show and Tell 1,2024,"Recently, we explored and trained different state-of-the-art text-to-speech (TTS) architectures for Catalan. We used existing datasets but also produced a new Catalan multi-accent dataset to train these architectures. The objective of our work is to improve the quality of current TTS systems in Catalan and export the resulting models for potential interactive applications and video games. For this reason, our set of multi-speaker and multi-accent Catalan TTS models are presented within a demo made in Unity. The users are able to interact with game characters which are attached to our Catalan TTS. While generated Catalan speech replies are reproduced, execution time, real-time
factor and transcription are shown on screen. Exported weights, such as data and demo source code, are released to the public.",True
huang24i_interspeech,https://www.isca-archive.org/interspeech_2024/huang24i_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/huang24i_interspeech.html,Analysis of articulatory setting for L1 and L2 English speakers using MRI data,Phonetics and Phonology of Second Language Acquisition,2024,"This paper investigates the extent to which the geographical region (country) where a speaker acquired their English language affects the articulatory setting in their speech. To obtain accurate measurements for evaluating articulatory setting, we utilized a large real-time MRI corpus of vocal tract articulation. The corpus was obtained from speakers from a variety of linguistic backgrounds producing continuous English speech. We use an automated pipeline to process and extract articulatory positional information from the MRI video data. This data is used to draw comparisons between English language speakers from the United States and speakers who acquired their English in India, Korea, and China. Analysis of the speaker groups reveals statistically significant articulatory setting posture differences in multiple places of articulation.",True
li24ma_interspeech,https://www.isca-archive.org/interspeech_2024/li24ma_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/li24ma_interspeech.html,A Functional Trade-off between Prosodic and Semantic Cues in Conveying Sarcasm,Analysis of Speakers States and Traits,2024,"This study investigates the acoustic features of sarcasm and disentangles the interplay between the propensity of an utterance being used sarcastically and the presence of prosodic cues signaling sarcasm. Using a dataset of sarcastic utterances compiled from television shows, we analyze the prosodic features within utterances and key phrases belonging to three distinct sarcasm categories (embedded, propositional, and illocutionary), which vary in the degree of semantic cues present, and compare them to neutral expressions. Results show that in phrases where the sarcastic meaning is salient from the semantics, the prosodic cues are less relevant than when the sarcastic meaning is not evident from the semantics, suggesting a trade-off between prosodic and semantic cues of sarcasm at the phrase level. These findings highlight a lessened reliance on prosodic modulation in semantically dense sarcastic expressions and a nuanced interaction that shapes the communication of sarcastic intent.",True
chen24f_interspeech,https://www.isca-archive.org/interspeech_2024/chen24f_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/chen24f_interspeech.html,Detecting Empathy in Speech,Analysis of Speakers States and Traits,2024,"Empathy is the ability to understand anotherâs feelings as if we were having those feelings ourselves. It has been shown to increase to peopleâs trust and likability. Much research has been done on creating empathetic responses in text in conversational systems, yet little work has been done to identify the acoustic-prosodic speech features that can create an empathetic-sounding voice. Our contributions include 1) collection of a new empathy speech dataset, 2) identifying interpretable acoustic-prosodic features that contribute to empathy expression and 3) benchmarking the empathy detection task.",True
gerczuk24_interspeech,https://www.isca-archive.org/interspeech_2024/gerczuk24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/gerczuk24_interspeech.html,Exploring Gender-Specific Speech Patterns in Automatic Suicide Risk Assessment,Analysis of Speakers States and Traits,2024,"In emergency medicine, timely intervention for patients at risk of suicide is often hindered by delayed access to specialised psychiatric care. To bridge this gap, we introduce a speech-based approach for automatic suicide risk assessment. Our study involves a novel dataset comprising speech recordings of 20 patients who read neutral texts. We extract four speech representations encompassing interpretable and deep features. Further, we explore the impact of gender-based modelling and phrase-level normalisation. By applying gender-exclusive modelling, features extracted from an emotion fine-tuned wav2vec2.0 model can be utilised to discriminate high- from low suicide risk with a balanced accuracy of 81%. Finally, our analysis reveals a discrepancy in the relationship of speech characteristics and suicide risk between female and male subjects. For men in our dataset, suicide risk increases together with agitation while voice characteristics of female subjects point the other way.",True
jing24b_interspeech,https://www.isca-archive.org/interspeech_2024/jing24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/jing24b_interspeech.html,ParaCLAP â Towards a general language-audio model for computational paralinguistic tasks,"Audio Captioning, Tagging, and Audio-Text Retrieval",2024,"Contrastive language-audio pretraining (CLAP) has recently emerged as a method for making audio analysis more generalisable. Specifically, CLAP-style models are able to 'answer' a diverse set of language queries, extending the capabilities of audio models beyond a closed set of labels. However, CLAP relies on a large set of (audio, query) pairs for pretraining. While such sets are available for general audio tasks, like captioning or sound event detection, there are no datasets with matched audio and text queries for computational paralinguistic (CP) tasks. As a result, the community relies on generic CLAP models trained for general audio with limited success. In the present study, we explore training considerations for ParaCLAP, a CLAP-style model suited to CP, including a novel process for creating audio-language queries. We demonstrate its effectiveness on a set of computational paralinguistic tasks, where it is shown to surpass the performance of open-source state-of-the-art models. Our code and resources are publicly available at: https://github.com/KeiKinn/ParaCLAP",True
anand24_interspeech,https://www.isca-archive.org/interspeech_2024/anand24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/anand24_interspeech.html,Enhancing Out-of-Vocabulary Performance of Indian TTS Systems for Practical Applications through Low-Effort Data Strategies,Speech Synthesis: Evaluation,2024,"Publicly available TTS datasets for low-resource languages like Hindi and Tamil typically contain 10-20 hours of data, leading to poor vocabulary coverage. This limitation becomes evident in downstream applications where domain-specific vocabulary coupled with frequent code-mixing with English, results in many OOV words. To highlight this problem, we create a benchmark containing OOV words from several real-world applications. Indeed, state-of-the-art Hindi and Tamil TTS systems perform poorly on this OOV benchmark, as indicated by intelligibility tests. To improve the modelâs OOV performance, we propose a low-effort and economically viable strategy to obtain more training data. Specifically, we propose using volunteers as opposed to high quality voice artists to record words containing character bigrams unseen in the training data. We show that using such inexpensive data, the model's performance improves on OOV words, while not affecting voice quality and in-domain performance.",True
adigwe24_interspeech,https://www.isca-archive.org/interspeech_2024/adigwe24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/adigwe24_interspeech.html,What do people hear? Listenersâ Perception of Conversational Speech,Speech Synthesis: Evaluation,2024,"Conversational agents are becoming increasingly popular, prompting the need for text-to-speech (TTS) systems that sound conversational. Previous research has focused on training TTS models on elicited or found conversational speech then measuring an improved listener preference. Preference ratings cannot pinpoint why TTS voices fall short of conversational expectations, underscoring our limited understanding of conversational speaking styles. In this pilot study, we conduct interviews with naive listeners who evaluate if speech was taken from a conversation or not, then give their explanation. Our results indicate that listeners are capable of distinguishing conversational utterances from read speech from acoustic features alone. While listenersâ explanations vary, they generally allude to pronunciation, rhythmic organisation, and inappropriate prosody. Using targeted prosodic modifications to synthesise speech, we shed light on the complexity of evaluating conversational style.",True
li24s_interspeech,https://www.isca-archive.org/interspeech_2024/li24s_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/li24s_interspeech.html,"MSR-86K: An Evolving, Multilingual Corpus with 86,300 Hours of Transcribed Audio for Speech Recognition Research",Multilingual ASR,2024,"Recently, multilingual artificial intelligence assistants, exemplified by ChatGPT, have gained immense popularity. As a crucial gateway to human-computer interaction, multilingual automatic speech recognition (ASR) has also garnered significant attention, as evidenced by systems like Whisper. However, the proprietary nature of the training data has impeded researchersâ efforts to study multilingual ASR. This paper introduces MSR-86K, an evolving, large-scale multilingual corpus for speech recognition research. The corpus is derived from publicly accessible videos on YouTube, comprising 15 languages and a total of 86,300 hours of transcribed ASR data. We also introduce how to use the MSR-86K corpus and other open-source corpora to train a robust multilingual ASR model that is competitive with Whisper. MSR-86K will be publicly released on HuggingFace, and we believe that such a large corpus will pave new avenues for research in multilingual ASR.",True
mujtaba24_interspeech,https://www.isca-archive.org/interspeech_2024/mujtaba24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/mujtaba24_interspeech.html,Inclusive ASR for Disfluent Speech: Cascaded Large-Scale Self-Supervised Learning with Targeted Fine-Tuning and Data Augmentation,General Topics in ASR,2024,"Automatic speech recognition (ASR) systems often falter while processing stuttering-related disfluencies---such as involuntary blocks and word repetitions---yielding inaccurate transcripts.  A critical barrier to progress is the scarcity of large, annotated disfluent speech datasets. Therefore, we present an inclusive ASR design approach, leveraging large-scale self-supervised learning on standard speech followed by targeted fine-tuning and data augmentation on a smaller, curated dataset of disfluent speech. Our data augmentation technique enriches training datasets with various disfluencies, enhancing ASR processing of these speech patterns. Results show that fine-tuning wav2vec 2.0 with even a relatively small, labeled dataset, alongside data augmentation, can significantly reduce word error rates for disfluent speech. Our approach not only advances ASR inclusivity for people who stutter, but also paves the way for ASRs that can accommodate wider speech variations.",True
lehecka24_interspeech,https://www.isca-archive.org/interspeech_2024/lehecka24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lehecka24_interspeech.html,A Comparative Analysis of Bilingual and Trilingual Wav2Vec Models for Automatic Speech Recognition in Multilingual Oral History Archives,General Topics in ASR,2024,"In this paper, we are comparing monolingual Wav2Vec 2.0 models with various multilingual models to see whether we could improve speech recognition performance on a unique oral history archive containing a lot of mixed-language sentences. Our main goal is to push forward research on this unique dataset, which is an extremely valuable part of our cultural heritage. Our results suggest that monolingual speech recognition models are, in most cases, superior to multilingual models, even when processing the oral history archive full of mixed-language sentences from non-native speakers. We also performed the same experiments on the public CommonVoice dataset to verify our results. We are contributing to the research community by releasing our pre-trained models to the public.",True
tran24b_interspeech,https://www.isca-archive.org/interspeech_2024/tran24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/tran24b_interspeech.html,VN-SLU: A Vietnamese Spoken Language Understanding Dataset,Spoken Language Understanding,2024,"Spoken Language Understanding (SLU) is a crucial task in spoken language processing. Despite the availability of numerous English datasets for research, there is a scarcity of resources for low-resource languages like Vietnamese. This paper introduces VN-SLU, the first dataset explicitly designed for Vietnamese SLU. VN-SLU includes 17,321 utterances from 240 Vietnamese speakers, obtained through novel crowd-sourcing methods. We propose a web tool for scenario generation and label validation, ensuring dataset quality and diversity. This tool prompts participants to confirm intents and slot values in smart home and virtual assistant dialogues, ensuring precise alignment. Experimental results highlight the challenging nature of the chosen test set sampling strategy in intent accuracy, SLU-F1, and utterance accuracy. Additionally, we explore the integration of pitch information into the Vietnamese SLU system. Results show improved performance compared to the baseline model.",True
pesan24_interspeech,https://www.isca-archive.org/interspeech_2024/pesan24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/pesan24_interspeech.html,BESST Dataset: A Multimodal Resource for Speech-based Stress Detection and Analysis,Speech and Multimodal Resources,2024,"The Brno Extended Stress and Speech Test (BESST) dataset is a new resource for the speech research community, offering multimodal audiovisual, physiological and psychological data that enable investigations into the interplay between stress and speech. In this paper, we introduce the BESST dataset and provide a details of its design, collection protocols, and technical aspects. The dataset comprises speech samples, physiological signals (including electrocardiogram, electrodermal activity, skin temperature, and acceleration data), and video recordings from 90 subjects performing stress-inducing tasks. It comprises 16.9 hours of clean Czech speech data, averaging 15 minutes of clean speech per participant. The data collection procedure involves the induction of cognitive and physical stress induced by Reading Span task (RSPAN) and Hand Immersion (HIT) task respectively. The BESST dataset was collected under stringent ethical standards and is accessible for research and development.",True
turetzky24_interspeech,https://www.isca-archive.org/interspeech_2024/turetzky24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/turetzky24_interspeech.html,HebDB: a Weakly Supervised Dataset for Hebrew Speech Processing,Speech and Multimodal Resources,2024,"We present HebDB, a weakly supervised dataset for spoken language processing in the Hebrew language. HebDB offers roughly 2500 hours of natural and spontaneous speech recordings in the Hebrew language, consisting of a large variety of speakers and topics. We provide raw recordings together with a pre-processed, weakly supervised, and filtered version. The goal of HebDB is to further enhance research and development of spoken language processing tools for the Hebrew language. Hence, we additionally provide two baseline systems for Automatic Speech Recognition (ASR): (i) a self-supervised model; and (ii) a fully supervised model. We present the performance of these two methods optimized on HebDB and compare them to current multi-lingual ASR alternatives. Results suggest the proposed method reaches better results than the evaluated baselines considering similar model sizes. Dataset, code, and models are publicly available under https://pages.cs.huji.ac.il/adiyoss-lab/HebDB/.",True
wang24b_interspeech,https://www.isca-archive.org/interspeech_2024/wang24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/wang24b_interspeech.html,GLOBE: A High-quality English Corpus with Global Accents for Zero-shot Speaker Adaptive Text-to-Speech,Speech and Multimodal Resources,2024,"This paper introduces GLOBE, a high-quality English corpus with worldwide accents, specifically designed to address the limitations of current zero-shot speaker adaptive Text-to-Speech (TTS) systems that exhibit poor generalizability in adapting to speakers with accents. Compared to commonly used English corpora, such as LibriTTS and VCTK, GLOBE is unique in its inclusion of utterances from 23,519 speakers and covers 164 accents worldwide, along with detailed metadata for these speakers. Compared to its original corpus, i.e., Common Voice, GLOBE significantly improves the quality of the speech data through rigorous filtering and enhancement processes, while also populating all missing speaker metadata. The final curated GLOBE corpus includes 535 hours of speech data at a 24 kHz sampling rate. Our benchmark results indicate that the speaker adaptive TTS model trained on the GLOBE corpus can synthesize speech with better speaker similarity and comparable naturalness than that trained on other popular corpora. We will release GLOBE publicly after acceptance. The GLOBE dataset is available at https://globecorpus.github.io/.",True
kong24_interspeech,https://www.isca-archive.org/interspeech_2024/kong24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/kong24_interspeech.html,STraDa: A Singer Traits Dataset,Speech and Multimodal Resources,2024,"There is a limited amount of large-scale public datasets that contain downloadable music audio files and rich lead singer metadata. To provide such a dataset to benefit research in singing voices, we created Singer Traits Dataset (STraDa) with two subsets: automatic-strada and annotated-strada. The automatic-strada contains twenty-five thousand tracks across numerous genres and languages of more than five thousand unique lead singers, which includes cross-validated lead singer metadata as well as other track metadata. The annotated-strada consists of two hundred tracks that are balanced in terms of 2 genders, 5 languages, and 4 age groups. To show its use for model training and bias analysis thanks to its metadata's richness and downloadable audio files, we benchmarked singer sex classification (SSC) and conducted bias analysis.",True
anderer24_interspeech,https://www.isca-archive.org/interspeech_2024/anderer24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/anderer24_interspeech.html,"MaViLS, a Benchmark Dataset for Video-to-Slide Alignment, Assessing Baseline Accuracy with a Multimodal Alignment Algorithm Leveraging Speech, OCR, and Visual Features",Speech and Multimodal Resources,2024,"This paper presents a benchmark dataset for aligning lecture videos with corresponding slides and introduces a novel multimodal algorithm leveraging features from speech, text, and images. It achieves an average accuracy of 0.82 in comparison to SIFT (0.56) while being approximately 11 times faster. Using dynamic programming the algorithm tries to determine the optimal slide sequence. The results show that penalizing slide transitions increases accuracy. Features obtained via optical character recognition (OCR) contribute the most to a high matching accuracy, followed by image features. The findings highlight that audio transcripts alone provide valuable information for alignment and are beneficial if OCR data is lacking. Variations in matching accuracy across different lectures highlight the challenges associated with video quality and lecture style. The novel multimodal algorithm demonstrates robustness to some of these challenges, underscoring the potential of the approach.",True
sungbin24_interspeech,https://www.isca-archive.org/interspeech_2024/sungbin24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/sungbin24_interspeech.html,MultiTalk: Enhancing 3D Talking Head Generation Across Languages with Multilingual Video Dataset,Speech and Multimodal Resources,2024,"Recent studies in speech-driven 3D talking head generation have achieved convincing results in verbal articulations. However, generating accurate lip-syncs degrades when applied to input speech in other languages, possibly due to the lack of datasets covering a broad spectrum of facial movements across languages. In this work, we introduce a novel task to generate 3D talking heads from speeches of diverse languages. We collect a new multilingual 2D video dataset comprising over 420 hours of talking videos in 20 languages. With our proposed dataset, we present a multilingual enhanced model that incorporates language-specific style embeddings, enabling it to capture the unique mouth movements associated with each language. Additionally, we present a metric for assessing lip-sync accuracy in multilingual settings. We demonstrate that training a 3D talking head model with our proposed dataset significantly enhances its multilingual performance. Codes and datasets are available at https://multitalk.github.io/.",True
veliche24_interspeech,https://www.isca-archive.org/interspeech_2024/veliche24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/veliche24_interspeech.html,Towards measuring fairness in speech recognition: Fair-Speech dataset,Speech and Multimodal Resources,2024,"The current public datasets for speech recognition (ASR) tend not to focus specifically on the fairness aspect, such as performance across different demographic groups. This paper introduces a novel dataset, Fair-Speech, a publicly released corpus to help researchers evaluate their ASR models for accuracy across a diverse set of self-reported demographic information, such as age, gender, ethnicity, geographic variation and whether the participants consider themselves native English speakers. Our dataset includes approximately 26.5K utterances in recorded speech by 593 people in the United States, who were paid to record and submit audios of themselves saying voice commands. We also provide ASR baselines, including on models trained on transcribed and untranscribed social media videos and open source models.",True
lu24f_interspeech,https://www.isca-archive.org/interspeech_2024/lu24f_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lu24f_interspeech.html,Codecfake: An Initial Dataset for Detecting LLM-based Deepfake Audio,Speech and Multimodal Resources,2024,"With the proliferation of Large Language Model (LLM) based deepfake audio, there is an urgent need for effective detection methods. Previous deepfake audio generation methods typically involve a multi-step generation process, with the final step using a vocoder to predict the waveform from hand-crafted features. However, LLM-based audio is directly generated from discrete neural codecs in an end-to-end generation process, skipping the final step of vocoder processing. This poses a significant challenge for current audio deepfake detection (ADD) models based on vocoder artifacts. To effectively detect LLM-based deepfake audio, we focus on the core of the generation process, the conversion from neural codec to waveform. We propose Codecfake dataset, which is generated by seven representative neural codec methods.  Experiment results show that codec-trained ADD models exhibit a 41.406% reduction in average equal error rate compared to vocoder-trained ADD models on the Codecfake test set.",True
kim24q_interspeech,https://www.isca-archive.org/interspeech_2024/kim24q_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/kim24q_interspeech.html,Automatic Children Speech Sound Disorder Detection with Age and Speaker Bias Mitigation,Pathological Speech Analysis 1,2024,"Addressing speech sound disorders (SSD) in early childhood is pivotal for mitigating cognitive and communicative impediments. Previous works on automatic SSD detection rely on audio features without considering the age and speaker bias which results in degraded performance. In this paper, we propose an SSD detection system in which debiasing techniques are applied to mitigate the biases. For the age bias, we use a multi-head model where the feature extractor is shared across different age groups but the final decision is made using the age-dependent classifier. For the speaker bias, we augment the dataset by mixing the audios of the multiple speakers in the same age group. When evaluated with our Korean SSD dataset, the proposed method showed significant improvements over previous approaches.",True
rameau24_interspeech,https://www.isca-archive.org/interspeech_2024/rameau24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/rameau24_interspeech.html,"Developing Multi-Disorder Voice Protocols: A team science approach involving clinical expertise, bioethics, standards, and DEI.",Speech and Language in Health: from Remote Monitoring to Medical Conversations - 1 (Special Session),2024,"The world of voice biomarkers is rapidly evolving thanks to the use of artificial intelligence (AI) allowing large-scale analysis of voice, speech, and respiratory sound data. The Bridge2AI-Voice project aims to build a large-scale, ethically sourced, and diverse voice database of human voices linked to health information to help fuel Voice AI research, dubbed Audiomics. The current paper describes the development of protocols of data acquisition across 4 different adult cohorts of disease (voice, respiratory, neurodegenerative diseases, mood, and anxiety disorders) using a Team Science approach for broader adoption by the research community and feedback. Demographic Surveys, Confounders Assessments, Acoustic tasks, validated patient-reported outcome (PRO) questionnaires and clinician-validated diagnostic questions were grouped in a common PART A across all cohorts and individual PART B, with cohort-specific tasks.",True
williams24_interspeech,https://www.isca-archive.org/interspeech_2024/williams24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/williams24_interspeech.html,Predicting Acute Pain Levels Implicitly from Vocal Features,Speech and Language in Health: from Remote Monitoring to Medical Conversations - 1 (Special Session),2024,"Evaluating pain in speech represents a critical challenge in high-stakes clinical scenarios, from analgesia delivery to emergency triage. Clinicians have predominantly relied on direct verbal communication of pain which is difficult for patients with communication barriers, such as those affected by stroke, autism, and learning difficulties. Many previous efforts have focused on multimodal data which does not suit all clinical applications. Our work is the first to collect a new English speech dataset wherein we have induced acute pain in adults using a cold pressor task protocol and recorded subjects reading sentences out loud. We report pain discrimination performance as F1 scores from binary (pain vs. no pain) and three-class (mild, moderate, severe) prediction tasks, and support our results with explainable feature analysis. Our work is a step towards providing medical decision support for pain evaluation from speech to improve care across diverse and remote healthcare settings.",True
weirich24_interspeech,https://www.isca-archive.org/interspeech_2024/weirich24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/weirich24_interspeech.html,Gender and age based f0-variation in the German Plapper Corpus,"Voice, Tones and F0",2024,"This study is the first exploration of data collected with the smartphone-app Plapper with which participants from Germany recorded themselves reading several sentences containing target sounds for future analyses of differences in fine phonetic detail and then donated their speech for inclusion to a large speech corpus. To this date, just short of 2.000 participants have contributed to this corpus. First analyses of differences in f0 on read speech from these German participants reveals an effect of age on mean f0 in females only and additional effects of self-rated femininity/masculinity (higher mean f0 in male and female speakers with higher self-rated femininity scores and vice versa). Also, there is an effect of region with speakers in the north of Germany inexplicably having lower mean f0 values than speakers from the other regions. Results leave room for speculation on the social meaning (what do speakers code and what do listeners interpret) of differences in f0.",True
naini24_interspeech,https://www.isca-archive.org/interspeech_2024/naini24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/naini24_interspeech.html,WHiSER: White House Tapes Speech Emotion Recognition Corpus,Emotion Recognition: Resources and Benchmarks,2024,"There are several applications for speech-emotion recognition (SER) systems in areas such as security and defense and healthcare. SER systems have achieved high performance when they are trained and tested in similar conditions. However, the performance often drops in more realistic and diverse conditions. Most existing SER datasets are too controlled and do not capture complex scenarios relevant to practical applications. This paper presents the White House tapes speech emotion recognition (WHiSER) corpus, which includes distant speech with real emotions from conversations in the Oval Office in 1972. This dataset is unique because it combines natural emotional expressions with various background noises, making it a perfect tool to test and improve SER models. Its real-world complexity and authenticity make the WHiSER corpus an excellent corpus for advancing emotion recognition technology, offering insights into how human emotions can be accurately recognized in complex environments.",True
kalluri24_interspeech,https://www.isca-archive.org/interspeech_2024/kalluri24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/kalluri24_interspeech.html,The Second DISPLACE Challenge: DIarization of SPeaker and LAnguage in Conversational Environments,Speaker and Language Identification and Diarization,2024,"The DIarization of SPeaker and LAnguage in Conversational Environments (DISPLACE) 2024 challenge is the second in the series of DISPLACE challenges, which involves tasks of speaker diarization (SD) and language diarization (LD) on a challenging multilingual conversational speech dataset.  In the DISPLACE 2024 challenge, we also introduced the task of automatic speech recognition (ASR) on this dataset. The dataset containing 158 hours of speech, consisting of both supervised and unsupervised mono-channel far-field recordings, was released for LD and SD tracks. Further, 12 hours of close-field mono-channel recordings were provided for the ASR track conducted on 5 Indian languages. The details of the dataset, baseline systems and the leader board results are highlighted in this paper.  We have also compared our baseline models and the team's performances on evaluation data of DISPLACE-2023 to emphasize the advancements made in this second version of the challenge.",True
wu24p_interspeech,https://www.isca-archive.org/interspeech_2024/wu24p_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/wu24p_interspeech.html,CodecFake: Enhancing Anti-Spoofing Models Against Deepfake Audios from Codec-Based Speech Synthesis Systems,Speech Coding,2024,"Current state-of-the-art (SOTA) codec-based audio synthesis systems can mimic anyone's voice with just a 3-second sample from that specific unseen speaker.  Unfortunately, malicious attackers may exploit these technologies, causing misuse and security issues. Anti-spoofing models have been developed to detect fake speech.  However, the open question of whether current SOTA anti-spoofing models can effectively counter deepfake audios from codec-based speech synthesis systems remains unanswered. In this paper, we curate an extensive collection of contemporary SOTA codec models, employing them to re-create synthesized speech.  This endeavor leads to the creation of CodecFake, the first codec-based deepfake audio dataset.  Additionally, we verify that anti-spoofing models trained on commonly used datasets cannot detect synthesized speech from current codec-based speech generation systems. The proposed CodecFake dataset empowers these models to counter this challenge effectively.",True
li24pa_interspeech,https://www.isca-archive.org/interspeech_2024/li24pa_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/li24pa_interspeech.html,GTR-Voice: Articulatory Phonetics Informed Controllable Expressive Speech Synthesis,Speech Synthesis: Expressivity and Emotion,2024,"Expressive speech synthesis aims to generate speech that captures a wide range of para-linguistic features, including emotion and articulation, though current research primarily emphasizes emotional aspects over the nuanced articulatory features mastered by professional voice actors. Inspired by this, we explore expressive speech synthesis through the lens of articulatory phonetics. Specifically, we define a framework with three dimensions: Glottalization, Tenseness, and Resonance (GTR), to guide the synthesis at the voice production level. With this framework, we record a high-quality speech dataset named GTR-Voice, featuring 20 Chinese sentences articulated by a professional voice actor across 125 distinct GTR combinations. We verify the framework and GTR annotations through automatic classification and listening tests, and demonstrate precise controllability along the GTR dimensions on two fine-tuned expressive TTS models. We open-source the dataset and TTS models.",True
saito24_interspeech,https://www.isca-archive.org/interspeech_2024/saito24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/saito24_interspeech.html,SRC4VC: Smartphone-Recorded Corpus for Voice Conversion Benchmark,Speech Synthesis: Tools and Data,2024,"We present SRC4VC, a new corpus containing 11 hours of speech recorded on smartphones by 100 Japanese speakers. Although high-quality multi-speaker corpora can advance voice conversion (VC) technologies, they are not always suitable for testing VC when low-quality speech recording is given as the input. To this end, we first asked 100 crowdworkers to record their voice samples using smartphones. Then, we annotated the recorded samples with speaker-wise recording-quality scores and utterance-wise perceived emotion labels. We also benchmark SRC4VC on any-to-any VC, in which we trained a multi-speaker VC model on high-quality speech and used the SRC4VC speakers' voice samples as the source in VC. The results show that the recording quality mismatch between the training and evaluation data significantly degrades the VC performance, which can be improved by applying speech enhancement to the low-quality source speech samples.",True
srinivasavaradhan24_interspeech,https://www.isca-archive.org/interspeech_2024/srinivasavaradhan24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/srinivasavaradhan24_interspeech.html,Rasa: Building Expressive Speech Synthesis Systems for Indian Languages in Low-resource Settings,Speech Synthesis: Tools and Data,2024,"We release Rasa, the first multilingual expressive TTS dataset for any Indian language, which contains 10 hours of neutral speech and 1-3 hours of expressive speech for each of the 6 Ekman emotions covering 3 languages: Assamese, Bengali, & Tamil. Our ablation studies reveal that just 1 hour of neutral and 30 minutes of expressive data can yield a Fair system as indicated by MUSHRA scores. Increasing neutral data to 10 hours, with minimal expressive data, significantly enhances expressiveness. This offers a practical recipe for resource-constrained languages, prioritizing easily obtainable neutral data alongside smaller amounts of expressive data. We show the importance of syllabically balanced data and pooling emotions to enhance expressiveness. We also highlight challenges in generating specific emotions, e.g., fear and surprise.",True
ma24c_interspeech,https://www.isca-archive.org/interspeech_2024/ma24c_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ma24c_interspeech.html,FLEURS-R: A Restored Multilingual Speech Corpus for Generation Tasks,Speech Synthesis: Tools and Data,2024,"This paper introduces FLEURS-R, a speech restoration applied version of the Few-shot Learning Evaluation of Universal Representations of Speech (FLEURS) corpus. FLEURS-R maintains an N-way parallel speech corpus in 102 languages as FLEURS, with improved audio quality and fidelity by applying the speech restoration model Miipher. The aim of FLEURS-R is to advance speech technology in more languages and catalyze research in- cluding text-to-speech (TTS) and other speech generation tasks in low-resource languages. Comprehensive evaluations with the restored speech and TTS baseline models trained from the new corpus show that the new corpus obtained significantly improved speech quality while maintaining the semantic contents of the speech. The corpus is publicly released via Hugging Face.",True
ma24d_interspeech,https://www.isca-archive.org/interspeech_2024/ma24d_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ma24d_interspeech.html,"WenetSpeech4TTS: A 12,800-hour Mandarin TTS Corpus for Large Speech Generation Model Benchmark",Speech Synthesis: Tools and Data,2024,"With the development of large text-to-speech (TTS) models and scale-up of the training data, state-of-the-art TTS systems have achieved impressive performance. In this paper, we present WenetSpeech4TTS, a multi-domain Mandarin corpus derived from the open-sourced WenetSpeech dataset. Tailored for the text-to-speech tasks, we refined WenetSpeech by adjusting segment boundaries, enhancing the audio quality, and eliminating speaker mixing within each segment. Following a more accurate transcription process and quality-based data filtering process, the obtained WenetSpeech4TTS corpus contains 12, 800 hours of paired audio-text data. Furthermore, we have created subsets of varying sizes, categorized by segment quality scores to allow for TTS model training and fine-tuning. VALLE and NaturalSpeech 2 systems are trained and fine-tuned on these subsets to validate the usability of WenetSpeech4TTS, establishing baselines on benchmark for fair comparison of TTS systems. The corpus and corresponding benchmarks are publicly available on huggingface.",True
yang24d_interspeech,https://www.isca-archive.org/interspeech_2024/yang24d_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/yang24d_interspeech.html,MSceneSpeech: A Multi-Scene Speech Dataset For Expressive Speech Synthesis,Speech Synthesis: Tools and Data,2024,"We introduce an open source high-quality Mandarin TTS dataset MSceneSpeech (Multiple Scene Speech Dataset), which is intended to provide resources for expressive speech synthesis. MSceneSpeech comprises numerous audio recordings and texts performed and recorded according to daily life scenarios. Each scenario includes multiple speakers and a diverse range of prosodic styles, making it suitable for speech synthesis that entails multi-speaker style and prosody modeling. We have established a robust baseline, through the prompting mechanism, that can effectively synthesize speech characterized by both user-specific timbre and scene-specific prosody with arbitrary text input. The open source MSceneSpeech Dataset and audio samples of our baseline are available at https://speechai-demo.github.io/MSceneSpeech/.",True
kawamura24_interspeech,https://www.isca-archive.org/interspeech_2024/kawamura24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/kawamura24_interspeech.html,LibriTTS-P: A Corpus with Speaking Style and Speaker Identity Prompts for Text-to-Speech and Style Captioning,Speech Synthesis: Tools and Data,2024,"We introduce LibriTTS-P, a new corpus based on LibriTTS-R that includes utterance-level descriptions (i.e., prompts) of speaking style and speaker-level prompts of speaker characteristics. We employ a hybrid approach to construct prompt annotations: (1) manual annotations that capture human perceptions of speaker characteristics and (2) synthetic annotations on speaking style. Compared to existing English prompt datasets, our corpus provides more diverse prompt annotations for all speakers of LibriTTS-R. Experimental results for prompt-based controllable TTS demonstrate that the TTS model trained with LibriTTS-P achieves higher naturalness than the model using the conventional dataset. Furthermore, the results for style captioning tasks show that the model utilizing LibriTTS-P generates 2.5 times more accurate words than the model using a conventional dataset. Our corpus, LibriTTS-P, is available at https://github.com/line/LibriTTS-P.",True
ogun24_interspeech,https://www.isca-archive.org/interspeech_2024/ogun24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ogun24_interspeech.html,1000 African Voices: Advancing inclusive multi-speaker multi-accent speech synthesis,Speech Synthesis: Tools and Data,2024,"Recent advances in speech synthesis have enabled many useful applications like audio directions in Google Maps, screen readers, and automated content generation on platforms like TikTok. However, these systems are mostly dominated by voices sourced from data-rich geographies with personas representative of their source data. Although 3000 of the world's languages are domiciled in Africa, African voices and personas are under-represented in these systems. As speech synthesis becomes increasingly democratized, it is desirable to increase the representation of African English accents. We present Afro-TTS, the first pan-African accented English speech synthesis system able to generate speech in 86 African accents, with 1000 personas representing the rich phonological diversity across the continent for downstream application in Education, Public Health, and Automated Content Creation. Speaker interpolation retains naturalness and accentedness, enabling the creation of new voices.",True
take24_interspeech,https://www.isca-archive.org/interspeech_2024/take24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/take24_interspeech.html,SaSLaW: Dialogue Speech Corpus with Audio-visual Egocentric Information Toward Environment-adaptive Dialogue Speech Synthesis,Speech Synthesis: Tools and Data,2024,"This paper presents SaSLaW, a spontaneous dialogue speech corpus containing synchronous recordings of what speakers speak, listen to, and watch. Humans consider the diverse environmental factors and then control the features of their utterances in face-to-face voice communications. Spoken dialogue systems capable of this adaptation to these audio environments enable natural and seamless communications. SaSLaW was developed to model human-speech adjustment for audio environments via first-person audio-visual perceptions in spontaneous dialogues. We propose the construction methodology of SaSLaW and display the analysis result of the corpus. We additionally conducted an experiment to develop text-to-speech models using SaSLaW and evaluate their performance of adaptations to audio environments. The results indicate that models incorporating hearing-audio data output more plausible speech tailored to diverse audio environments than the vanilla text-to-speech model.",True
shi24_interspeech,https://www.isca-archive.org/interspeech_2024/shi24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/shi24_interspeech.html,Singing Voice Data Scaling-up: An Introduction to ACE-Opencpop and ACE-KiSing,Speech Synthesis: Singing Voice Synthesis,2024,"In singing voice synthesis (SVS), generating singing voices from musical scores faces challenges due to limited data availability. This study proposes a unique strategy to address the data scarcity in SVS. We employ an existing singing voice synthesizer for data augmentation, complemented by detailed manual tuning, an approach not previously explored in data curation, to reduce instances of unnatural voice synthesis. This innovative method has led to the creation of two expansive singing voice datasets, ACE-Opencpop and ACE-KiSing, which are instrumental for large-scale, multi-singer voice synthesis. Through thorough experimentation, we establish that these datasets not only serve as new benchmarks for SVS but also enhance SVS performance on other singing voice datasets when used as supplementary resources. The corpora, pre-trained models, and their related training recipes are publicly available at ESPnet-Muskits (https://github.com/espnet/espnet).",True
tang24_interspeech,https://www.isca-archive.org/interspeech_2024/tang24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/tang24_interspeech.html,Pinyin Regularization in Error Correction for Chinese Speech Recognition with Large Language Models,LLM in ASR,2024,"Recent studies have demonstrated the efficacy of large language models (LLMs) in error correction for automatic speech recognition (ASR). However, much of the research focuses on the English language. This paper redirects the attention to Chinese. Firstly, we construct a specialized benchmark dataset aimed at error correction for Chinese ASR with 724K hypotheses-transcription pairs, named the Chinese Hypotheses Paradise dataset (ChineseHP), which contains a wide range of scenarios and presents significant challenges. Subsequently, we conduct a preliminary evaluation using the dataset for both direct-prompting and fine-tuning pre-trained LLMs. Furthermore, we propose a straightforward method of Pinyin regularization for prompts, which involves the transcription of Pinyin directly from text hypotheses. The experimental results reveal that Pinyin regularization consistently enhances the error-correcting ability of LLMs when compared with those without regularization. he dataset is available on the website.",True
ghosh24b_interspeech,https://www.isca-archive.org/interspeech_2024/ghosh24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ghosh24b_interspeech.html,LipGER: Visually-Conditioned Generative Error Correction for Robust Automatic Speech Recognition,Vision and Speech,2024,"Visual cues, like lip motion, have been shown to improve the performance of Automatic Speech Recognition (ASR) systems in noisy environments. We propose LipGER (Lip Motion aided Generative Error Correction), a novel framework for leveraging visual cues for noise-robust ASR. Instead of learning the cross-modal correlation between the audio and visual modalities, we make an LLM learn the task of visually-conditioned (generative) ASR error correction. Specifically, we instruct an LLM to predict the transcription from the N-best hypotheses generated using ASR beam-search. This is further conditioned on lip motions. This approach addresses key challenges in traditional AVSR learning, such as the lack of large-scale paired datasets and difficulties in adapting to new domains. We experiment on 4 datasets in various settings and show that LipGER improves the Word Error Rate in the range of 1.1%-49.2%. We also release LipHyp, a dataset with hypothesis-transcription pairs and lip motion cues.",True
chen24y_interspeech,https://www.isca-archive.org/interspeech_2024/chen24y_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/chen24y_interspeech.html,CNVSRC 2023: The First Chinese Continuous Visual Speech Recognition Challenge,Vision and Speech,2024,"The first Chinese Continuous Visual Speech Recognition Challenge aimed to probe the performance of Large Vocabulary Continuous Visual Speech Recognition (LVC-VSR) on two tasks: (1) Single-speaker VSR for a particular speaker and (2) Multi-speaker VSR for a set of registered speakers. The challenge yielded highly successful results, with the best submission significantly outperforming the baseline, particularly in the single-speaker task. This paper comprehensively reviews the challenge, encompassing the data profile, task specifications, and baseline system construction. It also summarises the representative techniques employed by the submitted systems, highlighting the most effective approaches. Additional information and resources about this challenge can be accessed through the official website at http://cnceleb.org/competition.",True
matsuura24_interspeech,https://www.isca-archive.org/interspeech_2024/matsuura24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/matsuura24_interspeech.html,"Sentence-wise Speech Summarization: Task, Datasets, and End-to-End Modeling with LM Knowledge Distillation",Spoken Document Summarization,2024,"This paper introduces a novel approach called sentence-wise speech summarization (Sen-SSum), which generates text summaries from a spoken document in a sentence-by-sentence manner. Sen-SSum combines the real-time processing of automatic speech recognition (ASR) with the conciseness of speech summarization. To explore this approach, we present two datasets for Sen-SSum: Mega-SSum and CSJ-SSum. Using these datasets, our study evaluates two types of Transformer-based models: 1) cascade models that combine ASR and strong text summarization models, and 2) end-to-end (E2E) models that directly convert speech into a text summary. While E2E models are appealing to develop compute-efficient models, they perform worse than cascade models. Therefore, we propose knowledge distillation for E2E models using pseudo-summaries generated by the cascade models. Our experiments show that this proposed knowledge distillation effectively improves the performance of the E2E model on both datasets.",True
leduc24_interspeech,https://www.isca-archive.org/interspeech_2024/leduc24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/leduc24_interspeech.html,Real-time Speech Summarization for Medical Conversations,Spoken Document Summarization,2024,"In doctor-patient conversations, identifying medically relevant information is crucial, posing the need for conversation summarization. In this work, we propose the first deployable real-time speech summarization system for real-world applications in industry, which generates a local summary after every N speech utterances within a conversation and a global summary after the end of a conversation. Our system could enhance user experience from a business standpoint, while also reducing computational costs from a technical perspective. Secondly, we present VietMed-Sum which, to our knowledge, is the first speech summarization dataset for medical conversations. Thirdly, we are the first to utilize LLM and human annotators collaboratively to create gold standard and synthetic summaries for medical conversation summarization. Finally, we present baseline results of state-of-the-art models on VietMed-Sum. All code, data (English-translated and Vietnamese) and models are available online.",True
sanguedolce24_interspeech,https://www.isca-archive.org/interspeech_2024/sanguedolce24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/sanguedolce24_interspeech.html,When Whisper Listens to Aphasia: Advancing Robust Post-Stroke Speech Recognition,Speech and Language in Health: from Remote Monitoring to Medical Conversations - 2 (Special Sessions),2024,"Despite recent advancements in Automatic Speech Recognition (ASR), its accuracy remains low for pathological speech, thereby limiting AI-based healthcare interventions in such settings. This work addresses this challenge by fine-tuning Whisper, an ASR known for its ability to capture high-dimensional features in healthy speech. Using our comprehensive dataset of patients with stroke, we fine-tuned Whisper and significantly reduced Word Error Rate (WER), surpassing previous work on severe aphasia. To demonstrate its generalisability, we tested the model on a separate database, AphasiaBank, and observed a lower WER despite variations in dialect, linguistics, and test protocols. Our result on the AphasiaBank was superior to previous ASRs trained on this database, confirming the generalisability of our approach. These outcomes not only address ASR limitations in impaired speech but also establish the foundations for standardised and versatile AI solutions for remote speech monitoring for timely diagnosis and intervention.",True
kothare24_interspeech,https://www.isca-archive.org/interspeech_2024/kothare24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/kothare24_interspeech.html,How Consistent are Speech-Based Biomarkers in Remote Tracking of ALS Disease Progression Across Languages? A Case Study of English and Dutch,Speech and Language in Health: from Remote Monitoring to Medical Conversations - 2 (Special Sessions),2024,"Previous work has demonstrated the utility of speech-based digital biomarkers for remotely tracking longitudinal progression in people with Amyotrophic Lateral Sclerosis (pALS). Here, we investigate the responsiveness of these biomarkers across languages for consistency. We collected audiovisual data using a cloud-based multimodal dialogue platform, where pALS interacted with a virtual guide to perform several speaking exercises. We automatically extracted speech, linguistic and orofacial metrics from 143 English-speaking pALS (36 bulbar onset, 107 non-bulbar onset) and 26 Dutch-speaking pALS (10 bulbar, 16 non-bulbar onset). We used growth curve models to estimate the trajectory of these metrics over time. We observe that for most of these metrics, English-speaking pALS and Dutch-speaking pALS follow similar trajectories, i.e. the slopes are not statistically different from each other, demonstrating the potential of such speech-based biomarkers for remote monitoring across languages.",True
labrak24_interspeech,https://www.isca-archive.org/interspeech_2024/labrak24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/labrak24_interspeech.html,Zero-Shot End-To-End Spoken Question Answering In Medical Domain,Speech and Language in Health: from Remote Monitoring to Medical Conversations - 2 (Special Sessions),2024,"In the rapidly evolving landscape of spoken question-answering (SQA), the integration of large language models (LLMs) has emerged as a transformative development. Conventional approaches often entail the use of separate models for question audio transcription and answer selection, resulting in significant resource utilization and error accumulation. To tackle these challenges, we explore the effectiveness of end-to-end (E2E) methodologies for SQA in the medical domain. Our study introduces a novel zero-shot SQA approach, compared to traditional cascade systems. Through a comprehensive evaluation conducted on a new open benchmark of 8 medical tasks and 48 hours of synthetic audio, we demonstrate that our approach requires up to 14.7 times fewer resources than a combined 1.3B parameters LLM with a 1.55B parameters ASR model while improving average accuracy by 0.5%. These findings underscore the potential of E2E methodologies for SQA in resource-constrained contexts.",True
jiang24b_interspeech,https://www.isca-archive.org/interspeech_2024/jiang24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/jiang24b_interspeech.html,Perceiver-Prompt: Flexible Speaker Adaptation in Whisper for Chinese Disordered Speech Recognition,Speech and Language in Health: from Remote Monitoring to Medical Conversations - 2 (Special Sessions),2024,"Disordered speech recognition profound implications for improving the quality of life for individuals afflicted with, for example, dysarthria. Dysarthric speech recognition encounters challenges including limited data, substantial dissimilarities between dysarthric and non-dysarthric speakers, and significant speaker variations stemming from the disorder. This paper introduces Perceiver-Prompt, a method for speaker adaptation that utilizes P-Tuning on the Whisper large-scale model. We first fine-tune Whisper using LoRA and then integrate a trainable Perceiver to generate fixed-length speaker prompts from variable-length inputs, to improve model recognition of Chinese dysarthric speech. Experimental results from our Chinese dysarthric speech dataset demonstrate consistent improvements in recognition performance with Perceiver-Prompt. Relative reduction up to 13.04% in CER is obtained over the fine-tuned Whisper.",True
schade24_interspeech,https://www.isca-archive.org/interspeech_2024/schade24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/schade24_interspeech.html,Understanding âunderstandingâ: presenting a richly annotated multimodal corpus of dyadic interaction,Show and Tell 2,2024,"This paper presents the MUNDEX corpus (MUltimodal UNDerstanding of EXplanations) together with past and current investigations using its data. The corpus is constructed to observe the dynamics of co-constructed communication and the understanding of explanations on multiple modalities in dyadic interactions. These modalities are annotated on several levels, including orthographic transcriptions, acoustic information, annotations of head movement, gaze, manual gestures, and further non-verbal behaviour as well as discourse annotations. Present
and past projects are also concerned with adding further to these annotations. The interlocutorsâ level of understanding is currently investigated in regard to several verbal and non-verbal behaviour markers of both the explaining and listening side.",True
hu24b_interspeech,https://www.isca-archive.org/interspeech_2024/hu24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/hu24b_interspeech.html,Automatic pitch accent classification through image classification,Prosody,2024,"The classification of pitch accents has posed significant challenges in automatic intonation labelling. Previous research primarily adopted feature-based approaches, predicting pitch accents using a finite set of features including acoustic features (F0, duration, intensity) and lexical features. In this study, we explored a novel approach, classifying pitch accents as images represented in pixels. To evaluate this methodâs effectiveness, we used a relatively simple classification task involving only two types of pitch accents (H* and L+H*). The training of a basic neural network model for classifying images of these two types of accents (N= 2,025) yielded an average accuracy of 93.5% across 10 runs on the test set, showcasing the potential effectiveness of this new approach.",True
chakraborty24_interspeech,https://www.isca-archive.org/interspeech_2024/chakraborty24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/chakraborty24_interspeech.html,On Comparing Time- and Frequency-Domain Rhythm Measures in Classifying Assamese Dialects,Prosody,2024,"The rhythm measures are one of the primary means of differentiating languages and their dialects. They are broadly classified into two groups: time domain and frequency domain measures. The time-domain rhythm measures include temporal metrics like vocalic and non-vocalic durations and their derivatives, etc, and the frequency-domain rhythm measures comprise amplitude-modulated rhythm formant trajectory frequency and magnitude. To conduct this research, we focused on four Assamese regional varieties spoken in four districts of the Indian state of Assam. Data used in this study consists of read speech data of native Assamese speakers reading the Assamese translation of the ""North Wind and the Sun"" passage. The speech data was obtained from a total of 10 speakers for each of the four varieties. The average accuracy of dialect classification using quadratic discriminant analysis turns out to be 42% and 35%, respectively, for time- and frequency-domain rhythm measures.",True
maciejewski24_interspeech,https://www.isca-archive.org/interspeech_2024/maciejewski24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/maciejewski24_interspeech.html,Evaluating the Santa Barbara Corpus: Challenges of the Breadth of Conversational Spoken Language,Speaker Recognition 1,2024,"As speech technology has matured, there has been a push towards systems that can process conversational speech, reflecting the so-called âcocktail party problem,â which includes not only more challenging acoustic conditions, but also necessitates solutions to new problems, such as identifying who spoke when and processing multiple concurrent streams of speech. Such problems have been approached primarily via corpora comprising business meetings and dinner parties, overlooking the broad range of conversational dynamics and speaker demographics that fall under the category of multi-talker speech. To this end, we introduce the use of the Santa Barbara Corpus of Spoken American English for evaluation of speech technologyâincluding preparing the corpus and annotations for automatic processing, demonstrating the failure of state-of-the-art systems to withstand the heterogeneity of conditions, and highlighting the situations where standard methods struggle to perform at all.",True
chen24h_interspeech,https://www.isca-archive.org/interspeech_2024/chen24h_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/chen24h_interspeech.html,"Improving Generalization of Speech Separation in Real-World Scenarios: Strategies in Simulation, Optimization, and Evaluation",Source Separation 1,2024,"Achieving robust speech separation for overlapping speakers in various acoustic environments with noise and reverberation re-mains an open challenge. Although existing datasets are available to train separators for specific scenarios, they do not effectively generalize across diverse real-world scenarios. In this paper, we present a novel data simulation pipeline that produces diverse training data from a range of acoustic environments and content, and propose new training paradigms to improve quality of a general speech separation model. Specifically, we first introduce AC-SIM, a data simulation pipeline that incorporates broad variations in both content and acoustics. Then we integrate multiple training objectives into the permutation invariant training (PIT) to enhance separation quality and generalization of the trained model. Finally, we conduct comprehensive ob- jective and human listening experiments across separation architectures and benchmarks to validate our methods, demonstrating substantial improvement of generalization on both non-homologous and real-world test sets.",True
muller24_interspeech,https://www.isca-archive.org/interspeech_2024/muller24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/muller24_interspeech.html,A New Approach to Voice Authenticity,Speech Privacy and Bandwidth Expansion,2024,"Voice faking poses significant societal challenges. Currently, the prevailing assumption is that unaltered human speech can always be considered genuine, while fake speech usually comes from text-to-speech (TTS) synthesis. We argue that this type of binary distinction is oversimplified. For instance, altered playback speeds can maliciously deceive listeners, as in the `Drunken Nancy Pelosi' incident. Similarly, editing of audio clips can be done ethically, e.g. for brevity or summarization in news reporting or podcasts, but editing can also create misleading narratives. In this paper, we propose a conceptual shift away from the longstanding binary paradigm of speech audio being either `fake' or `real'. Instead, we focus on pinpointing `voice edits', which encompass traditional modifications like filters and cuts, as well as neural synthesis. We delineate six categories of voice edits and curate a new challenge dataset, for which we present baseline voice edit detection systems.",True
afonja24_interspeech,https://www.isca-archive.org/interspeech_2024/afonja24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/afonja24_interspeech.html,Performant ASR Models for Medical Entities in Accented Speech,"Accented Speech, Prosodic Features, Dialect, Emotion, Sound Classification",2024,"Recent strides in automatic speech recognition (ASR) have accelerated their application in the medical domain where their performance on accented medical named entities (NE) such as drug names, diagnoses, and lab results, is largely unknown. We rigorously evaluate multiple ASR models on a clinical English dataset of 93 African accents. Our analysis reveals that despite some models achieving low overall word error rates (WER), errors in clinical entities are higher, potentially posing substantial risks to patient safety. To empirically demonstrate this, we extract clinical entities from transcripts, develop a novel algorithm to align ASR predictions with these entities, and compute medical NE Recall, medical WER, and character error rate. Our results show that fine-tuning on accented clinical speech improves medical WER by a wide margin (25-34 % relative), improving their practical applicability in healthcare environments.",True
javed24_interspeech,https://www.isca-archive.org/interspeech_2024/javed24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/javed24_interspeech.html,LAHAJA: A Robust Multi-accent Benchmark for Evaluating Hindi ASR Systems,"Accented Speech, Prosodic Features, Dialect, Emotion, Sound Classification",2024,"Hindi, one of the most spoken language of India, exhibits a diverse array of accents due to its usage among individuals from diverse linguistic origins. To enable a robust evaluation of Hindi ASR systems on multiple accents, we create a benchmark, LAHAJA, which contains read and extempore speech on a diverse set of topics and use cases, with a total of 12.5 hours of Hindi audio, sourced from 132 speakers spanning 83 districts of India. We evaluate existing open-source and commercial models on LAHAJA and find their performance to be poor. We then train models using different datasets and find that our model trained on multilingual data with good speaker diversity outperforms existing models by a significant margin. We also present a fine-grained analysis which shows that the performance declines for speakers from North-East and South India, especially with content heavy in named entities and specialized terminology.",True
kim24v_interspeech,https://www.isca-archive.org/interspeech_2024/kim24v_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/kim24v_interspeech.html,LearnerVoice: A Dataset of Non-Native English Learnersâ Spontaneous Speech,"Accented Speech, Prosodic Features, Dialect, Emotion, Sound Classification",2024,"Prevalent ungrammatical expressions and disfluencies in spontaneous speech from second language (L2) learners pose unique challenges to Automatic Speech Recognition (ASR) systems. However, few datasets are tailored to L2 learner speech. We publicly release LearnerVoice, a dataset consisting of 50.04 hours of audio and transcriptions of L2 learnersâ spontaneous speech. Our linguistic analysis reveals that transcriptions in our dataset contain L2S (L2 learnerâs Spontaneous speech) features, consisting of ungrammatical expressions and disfluencies (e.g., filler words, word repetitions, self-repairs, false starts), significantly more than native speech datasets. Fine-tuning whisper-small.en with LearnerVoice achieves a WER of 10.26%, 44.2% lower than vanilla whisper-small.en. Furthermore, our qualitative analysis indicates that 54.2% of errors from the vanilla model on LearnerVoice are attributable to L2S features, with 48.1% of them being reduced in the fine-tuned model.",True
lin24m_interspeech,https://www.isca-archive.org/interspeech_2024/lin24m_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lin24m_interspeech.html,MinSpeech: A Corpus of Southern Min Dialect for Automatic Speech Recognition,"Accented Speech, Prosodic Features, Dialect, Emotion, Sound Classification",2024,"This paper presents MinSpeech, a speech corpus of Southern Min (also known as Hokkien), to propel research in dialect speech recognition. Despite the linguistic and cultural importance of Southern Min, there is still a notable scarcity of publicly accessible speech corpus for this dialect. MinSpeech provides 2237 hours of unlabeled audio and 1778 hours of labeled audio, sourced diversely and encompassing various contexts. Mandarin text is employed as labels to enable cross-linguistic alignment and transformation. Using this corpus, we have developed baseline systems, including supervised models (Kaldi Chain and Conformer) and two self-supervised models (Wav2vec 2.0 and HuBERT). These systems were assessed on an automatic speech recognition (ASR) task to the Southern Min dialect. Experiments illustrate that the corpus offers practical assistance and resources for speech processing of this dialect. MinSpeech dataset is available at https://minspeech.github.io/.",True
goel24_interspeech,https://www.isca-archive.org/interspeech_2024/goel24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/goel24_interspeech.html,Exploring Multilingual Unseen Speaker Emotion Recognition: Leveraging Co-Attention Cues in Multitask Learning,"Accented Speech, Prosodic Features, Dialect, Emotion, Sound Classification",2024,"Advent of modern deep learning techniques has given rise to advancements in the field of Speech Emotion Recognition (SER). However, most systems prevalent in the field fail to generalize to speakers not seen during training. This study focuses on handling challenges of multilingual SER, specifically on unseen speakers. We introduce CAMuLeNet, a novel architecture leveraging co-attention based fusion and multitask learning to address this problem. Additionally, we benchmark pretrained encoders of Whisper, HuBERT, Wav2Vec2.0, and WavLM using 10-fold leave-speaker-out cross-validation on five existing multilingual benchmark datasets: IEMOCAP, RAVDESS, CREMA-D, EmoDB and CaFE and, release a novel dataset for SER on the Hindi language (BhavVani). CAMuLeNet shows an average improvement of approximately 8% over all benchmarks on unseen speakers determined by our cross-validation strategy.",True
gao24c_interspeech,https://www.isca-archive.org/interspeech_2024/gao24c_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/gao24c_interspeech.html,Enhancing Voice Wake-Up for Dysarthria: Mandarin Dysarthria Speech Corpus Release and Customized System Design,Speech Disorders 3,2024,"Smart home technology has gained widespread adoption, facilitating effortless control of devices through voice commands. However, individuals with dysarthria, a motor speech disorder, face challenges due to the variability of their speech. This paper addresses the wake-up word spotting (WWS) task for dysarthric individuals, aiming to integrate them into real-world applications. To support this, we release the open-source Mandarin Dysarthria Speech Corpus (MDSC), a dataset designed for dysarthric individuals in home environments. MDSC encompasses information on age, gender, disease types, and intelligibility evaluations. Furthermore, we perform comprehensive experimental analysis on MDSC, highlighting the challenges encountered. We also develop a customized dysarthria WWS system that showcases robustness in handling intelligibility and achieving exceptional performance. MDSC will be released on https://www.aishelltech.com/AISHELL_6B.",True
jiang24_interspeech,https://www.isca-archive.org/interspeech_2024/jiang24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/jiang24_interspeech.html,"Learnings from curating a trustworthy, well-annotated, and useful dataset of disordered English speech",Speech Disorders 3,2024,"Project Euphonia, a Google initiative, is dedicated to improving automatic speech recognition (ASR) of disordered speech. A central objective of the project is to create a large, high-quality, and diverse speech corpus. This report describes the projectâs latest advancements in data collection and annotation methodologies, such as expanding speaker diversity in the database, adding human-reviewed transcript corrections and audio quality tags to 350K (of the 1.2M total) audio recordings, and amassing a comprehensive set of metadata (including more than 40 speech characteristic labels) for over 75% of the speakers in the database. We report on the impact of transcript corrections on our machine-learning (ML) research, inter-rater variability of assessments of disordered speech patterns, and our rationale for gathering speech metadata. We also consider the limitations of using automated off-the-shelf annotation methods for assessing disordered speech.",True
leung24_interspeech,https://www.isca-archive.org/interspeech_2024/leung24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/leung24_interspeech.html,Training Data Augmentation for Dysarthric Automatic Speech Recognition by Text-to-Dysarthric-Speech Synthesis,Speech Disorders 3,2024,"Automatic speech recognition (ASR) research has achieved impressive performance in recent years and has significant potential for enabling access for people with dysarthria (PwD) in augmentative and alternative communication (AAC) and home environment systems. However, progress in dysarthric ASR (DASR) has been limited by high variability in dysarthric speech and limited public availability of dysarthric training data. This paper demonstrates that data augmentation using text-to-dysarthic-speech (TTDS) synthesis for finetuning large ASR models is effective for DASR. Specifically, diffusion-based text-to-speech (TTS) models can produce speech samples similar to dysarthric speech that can be used as additional training data for fine-tuning ASR foundation models, in this case Whisper. Results show improved synthesis metrics and ASR performance for the proposed multi-speaker diffusion-based TTDS data augmentation for ASR fine-tuning compared to current DASR baselines.",True
bhogale24_interspeech,https://www.isca-archive.org/interspeech_2024/bhogale24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/bhogale24_interspeech.html,Empowering Low-Resource Language ASR via Large-Scale Pseudo Labeling,Speech Recognition with Large Pretrained Speech Models for Under-represented Languages (Special Session),2024,"In this study, we tackle the challenge of limited labeled data for low-resource languages in ASR, focusing on Hindi. Specifically, we explore pseudo-labeling, by proposing a generic framework combining multiple ideas from existing works. Our framework integrates multiple base models for transcription and evaluators for assessing audio-transcript pairs, resulting in robust pseudo-labeling for low resource languages. We validate our approach with a new benchmark, IndicYT, comprising diverse YouTube audio files from multiple content categories. Our findings show that augmenting pseudo labeled data from YouTube with existing training data leads to significant performance improvements on IndicYT, without affecting performance on out-of-domain benchmarks, demonstrating the efficacy of pseudo-labeled data in enhancing ASR capabilities for low-resource languages. The benchmark, code and models developed as a part of this work will be made publicly available.",True
ahn24b_interspeech,https://www.isca-archive.org/interspeech_2024/ahn24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ahn24b_interspeech.html,VoxSim: A perceptual voice similarity dataset,Databases and Progress in Methodology,2024,"This paper introduces VoxSim, a dataset of perceptual voice similarity ratings. Recent efforts to automate the assessment of speech synthesis technologies have primarily focused on predicting mean opinion score of naturalness, leaving speaker voice similarity relatively unexplored due to a lack of extensive training data. To address this, we generate about 41k utterance pairs from the VoxCeleb dataset, a widely utilised speech dataset for speaker recognition, and collect nearly 70k speaker similarity scores through a listening test. VoxSim offers a valuable resource for the development and benchmarking of speaker similarity prediction models. We provide baseline results of speaker similarity prediction models on the VoxSim test set and further demonstrate that the model trained on our dataset gener-alises to the out-of-domain VCC2018 dataset.",True
nijat24_interspeech,https://www.isca-archive.org/interspeech_2024/nijat24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/nijat24_interspeech.html,UY/CH-CHILD -- A Public Chinese L2 Speech Database of Uyghur Children,Databases and Progress in Methodology,2024,"Exploring the progression of pronunciation skills in second language (L2) acquisition among children presents an intriguing research avenue. Yet, the comprehension of this process for Uyghur children learning Chinese as their L2 has been constrained by a scarcity of speech data. To bridge this gap, we have developed the UY/CH-CHILD speech database, comprising 29,061 samples of Chinese words articulated by 106 Uyghur children from both kindergartens and primary schools. The database includes carefully labelled syllables and tones by native Chinese speakers. To showcase the utility of this novel resource, we conducted a comparative analysis of pronunciation errors between the kindergarten and primary school groups, unveiling interesting insights into the evolution of pronunciation proficiency in Uyghur children as they mature. 
The database can be downloaded online at http://child.cslt.org.",True
amiriparian24_interspeech,https://www.isca-archive.org/interspeech_2024/amiriparian24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/amiriparian24_interspeech.html,ExHuBERT: Enhancing HuBERT Through Block Extension and Fine-Tuning on 37 Emotion Datasets,Speech Emotion Recognition,2024,"Foundation models have shown great promise in speech emotion recognition (SER) by leveraging their pre-trained representations to capture emotion patterns in speech signals. To further enhance SER performance across various languages and domains, we propose a novel twofold approach. First, we gather EmoSet++, a comprehensive multi-lingual, multi-cultural speech emotion corpus with 37 datasets, 150,907 samples, and a total duration of 119.5 hours. Second, we introduce ExHuBERT, an enhanced version of HuBERT achieved by backbone extension and fine-tuning on EmoSet++. We duplicate each encoder layer and its weights, then freeze the first duplicate, integrating an extra zero-initialized linear layer and skip connections to preserve functionality and ensure its adaptability for subsequent fine-tuning. Our evaluation on unseen datasets shows the efficacy of ExHuBERT, setting a new benchmark for various SER tasks. Model and details on EmoSet++: https://huggingface.co/amiriparian/ExHuBERT.",True
rittergutierrez24_interspeech,https://www.isca-archive.org/interspeech_2024/rittergutierrez24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/rittergutierrez24_interspeech.html,Dataset-Distillation Generative Model for Speech Emotion Recognition,Speech Emotion Recognition,2024,"Deep learning models for speech rely on large datasets, presenting computational challenges. Yet, performance hinges on training data size. Dataset Distillation (DD) aims to learn a smaller dataset without much performance degradation when training with it.  DD has been investigated in computer vision but not yet in speech. This paper presents the first approach for DD to speech targeting Speech Emotion Recognition on IEMOCAP. We employ Generative Adversarial Networks (GANs) not to mimic real data but to distil key discriminative information of IEMOCAP that is useful for downstream training. The GAN then replaces the original dataset and can sample custom synthetic dataset sizes. It performs comparably when following the original class imbalance but improves performance by 0.3% absolute UAR with balanced classes. It also reduces dataset storage and accelerates downstream training by 95% in both cases and reduces speaker information which could help for a privacy application.",True
shirahata24_interspeech,https://www.isca-archive.org/interspeech_2024/shirahata24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/shirahata24_interspeech.html,Audio-conditioned phonemic and prosodic annotation for building text-to-speech models from unlabeled speech data,Speech Synthesis: Text Processing,2024,"This paper proposes an audio-conditioned phonemic and prosodic annotation model for building text-to-speech (TTS) datasets from unlabeled speech samples. For creating a TTS dataset that consists of label-speech paired data, the proposed annotation model leverages an automatic speech recognition (ASR) model to obtain phonemic and prosodic labels from unlabeled speech samples. By fine-tuning a large-scale pretrained ASR model, we can construct the annotation model using a limited amount of label-speech paired data within an existing TTS dataset. To alleviate the shortage of label-speech paired data for training the annotation model, we generate pseudo label-speech paired data using text-only corpora and an auxiliary TTS model. This TTS model is also trained with the existing TTS dataset. Experimental results show that the TTS model trained with the dataset created by the proposed annotation method can synthesize speech as naturally as the one trained with a fully-labeled dataset.",True
cui24_interspeech,https://www.isca-archive.org/interspeech_2024/cui24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/cui24_interspeech.html,Spontaneous Speech-Based Suicide Risk Detection Using Whisper and Large Language Models,Multimodality and Foundation Models,2024,"The early detection of suicide risk is important since it enables the intervention to prevent potential suicide attempts. This paper studies the automatic detection of suicide risk based on spontaneous speech from adolescents, and collects a Mandarin dataset with 15 hours of suicide speech from more than a thousand adolescents aged from ten to eighteen for our experiments. To leverage the diverse acoustic and linguistic features embedded in spontaneous speech, both the Whisper speech model and textual large language models (LLMs) are used for suicide risk detection. Both all-parameter finetuning and parameter-efficient finetuning approaches are used to adapt the pre-trained models for suicide risk detection, and multiple audio-text fusion approaches are evaluated to combine the representations of Whisper and the LLM. The proposed system achieves a detection accuracy of 0.807 and an F1-score of 0.846 on the test set with 119 subjects, indicating promising potential for real suicide risk detection applications.",True
bujnowski24_interspeech,https://www.isca-archive.org/interspeech_2024/bujnowski24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/bujnowski24_interspeech.html,SAMSEMO: New dataset for multilingual and multimodal emotion recognition,Multimodality and Foundation Models,2024,"The task of emotion recognition using image, audio and text modalities has recently attained popularity due to its various potential applications. However, the list of large-scale multimodal datasets is very short and all available datasets have significant limitations. We present SAMSEMO, a novel dataset for multimodal and multilingual emotion recognition. Our collection of over 23k video scenes is multilingual as it includes video scenes in 5 languages (EN, DE, ES, PL and KO). Video scenes are heterogeneous, they come from diverse sources and are accompanied with rich manually collected metadata and emotion annotations. In the paper, we also study the valence and arousal of audio features of our data for the most important emotion classes and compare them with the features of CMU-MOSEI data. Moreover, we perform multimodal experiments for emotion recognition with SAMSEMO and show how to use a multilingual model to improve the detection of imbalanced classes.",True
jia24_interspeech,https://www.isca-archive.org/interspeech_2024/jia24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/jia24_interspeech.html,LLM-Driven Multimodal Opinion Expression Identification,Multimodality and Foundation Models,2024,"Opinion Expression Identification (OEI) is essential in NLP for applications ranging from voice assistants to depression diagnosis. This study extends OEI to encompass multimodal inputs, underlining the significance of auditory cues in delivering emotional subtleties beyond the capabilities of text. We introduce a novel multimodal OEI (MOEI) task, integrating text and speech to mirror real-world scenarios. Utilizing CMU MOSEI and IEMOCAP datasets, we construct the CI-MOEI dataset. Additionally, Text-to-Speech (TTS) technology is applied to the MPQA dataset to obtain the CIM-OEI dataset. We design a template for the OEI task to take full advantage of the generative power of large language models (LLMs). Advancing further, we propose an LLM-driven method STOEI, which combines speech and text modal to identify opinion expressions. Our experiments demonstrate that MOEI significantly improves the performance while our method outperforms existing methods by 9.20% and obtains SOTA results.",True
halpern24_interspeech,https://www.isca-archive.org/interspeech_2024/halpern24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/halpern24_interspeech.html,Quantifying the effect of speech pathology on automatic and human speaker verification,Pathological Speech Analysis 2,2024,"This study investigates how surgical intervention for speech pathology (specifically, as a result of oral cancer surgery) impacts the performance of an automatic speaker verification (ASV) system. Using two recently collected Dutch datasets with parallel pre and post-surgery audio from the same speaker, NKI-OC-VC and SPOKE, we assess the extent to which speech pathology influences ASV performance, and whether objective/subjective measures of speech severity are correlated with the performance. Finally, we carry out a perceptual study to compare judgements of ASV and human listeners. Our findings reveal that pathological speech negatively affects ASV performance, and the severity of the speech is negatively correlated with the performance. There is a moderate agreement in perceptual and objective scores of speaker similarity and severity, however, we could not clearly establish in the perceptual study, whether the same phenomenon also exists in human perception.",True
schubert24_interspeech,https://www.isca-archive.org/interspeech_2024/schubert24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/schubert24_interspeech.html,Challenges of German Speech Recognition: A Study on Multi-ethnolectal Speech Among Adolescents,"Speech Science, Speech Technology, and Gender (Special Session)",2024,"Despite significant advancements in speech recognition systems, challenges persist in accurately interpreting spontaneous speech from underrepresented groups like non-standard speakers or younger individuals. The difficulty increases when these conditions overlap. To further explore this topic, we employ a dataset featuring spontaneous as well as read speech from young speakers in Germany, including both, speakers from mono-ethnic and multi-ethnic backgrounds. Our study involves a comparative analysis of speech recognition performance, incorporating gender considerations, using three distinct Automatic Speech Recognition (ASR) engines: Whisper (OpenAI), NeMo (NVIDIA), and Wav2Vec2.0 (Meta AI). Furthermore, we conduct a comprehensive error analysis on the automatically generated transcripts, employing part-of-speech (POS) tagging. This allows us to discern the word types that pose the greatest challenge for comprehension by the ASR engines.",True
pelloin24_interspeech,https://www.isca-archive.org/interspeech_2024/pelloin24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/pelloin24_interspeech.html,Automatic Classification of News Subjects in Broadcast News: Application to a Gender Bias Representation Analysis,"Speech Science, Speech Technology, and Gender (Special Session)",2024,"This paper introduces a computational framework designed to delineate gender distribution biases in topics covered by French TV and radio news. We transcribe a dataset of 11.7k hours, broadcasted in 2023 on 21 French channels. A Large Language Model (LLM) is used in few-shot conversation mode to obtain a topic classification on those transcriptions. Using the generated LLM annotations, we explore the finetuning of a specialized smaller classification model, to reduce the computational cost. To evaluate the performances of these models, we construct and annotate a dataset of 804 dialogues. This dataset is made available free of charge for research purposes. We show that women are notably underrepresented in subjects such as sports, politics and conflicts. Conversely, on topics such as weather, commercials and health, women have more speaking time than their overall average across all subjects. We also observe representations differences between private and public service channels.",True
doukhan24_interspeech,https://www.isca-archive.org/interspeech_2024/doukhan24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/doukhan24_interspeech.html,Gender Representation in TV and Radio: Automatic Information Extraction methods versus Manual Analyses,"Speech Science, Speech Technology, and Gender (Special Session)",2024,"This study investigates the relationship between automatic information extraction descriptors and manual analyses to describe gender representation disparities in TV and Radio. Automatic descriptors, including speech time, facial categorization and speech transcriptions are compared with channel reports on a vast 32,000-hour corpus of French broadcasts from 2023. Findings reveal systemic gender imbalances, with women underrepresented compared to men across all descriptors. Notably, manual channel reports show higher womenâs presence than automatic estimates and references to women are lower than their speech time. Descriptors share common dynamics during high and low audiences, war coverage, or private versus public channels. While women are more visible than audible in French TV, this trend is inverted in news with unseen journalists depicting male protagonists. A statistical test shows 3 main effects influencing references to women: program category, channel and speaker gender.",True
szekely24_interspeech,https://www.isca-archive.org/interspeech_2024/szekely24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/szekely24_interspeech.html,An inclusive approach to creating a palette of synthetic voices for gender diversity,"Speech Science, Speech Technology, and Gender (Special Session)",2024,"Mainstream text-to-speech (TTS) technologies predominantly rely on binary, cisgender speech, failing to adequately represent the diversity of gender expansive (e.g., transgender and/or nonbinary) people. This poses challenges, particularly for users of Speech Generating Devices (SGDs) seeking TTS voices that authentically reflect their identity and desired expressive nuances. This paper introduces a novel approach for constructing a palette of controllable gender-expansive TTS voices using recordings from 14 gender-expansive speakers. We employ Constrained PCA to extract gender-independent speaker identity vectors from x-vectors, using acoustic Vocal Tract Length (aVTL) as a known component. The result is applied as a speaker embedding in neural TTS, allowing control over the aVTL and several emergent properties captured as a representation of the vocal space across speakers. In addition to quantitative metrics, we present a community evaluation conducted by nonbinary SGD users.",True
netzorg24_interspeech,https://www.isca-archive.org/interspeech_2024/netzorg24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/netzorg24_interspeech.html,Speech After Gender: A Trans-Feminine Perspective on Next Steps for Speech Science and Technology,"Speech Science, Speech Technology, and Gender (Special Session)",2024,"As experts in voice modification, trans-feminine gender-affirming voice teachers have unique perspectives on voice that confound current understandings of speaker identity. To demonstrate this, we present the Versatile Voice Dataset (VVD), a collection of three speakers modifying their voices along gendered axes. The VVD illustrates that current approaches in speaker modeling, based on categorical notions of gender and a static understanding of vocal texture, fail to account for the flexibility of the vocal tract. Utilizing publicly-available speaker embeddings, we demonstrate that gender classification systems are highly sensitive to voice modification, and speaker verification systems fail to identify voices as coming from the same speaker as voice modification becomes more drastic. As one path towards moving beyond categorical and static notions of speaker identity, we propose modeling individual qualities of vocal texture such as pitch, resonance, and weight.",True
lai24_interspeech,https://www.isca-archive.org/interspeech_2024/lai24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lai24_interspeech.html,Voice Quality Variation in AAE: An Additional Challenge for Addressing Bias in ASR Models?,"Speech Science, Speech Technology, and Gender (Special Session)",2024,"Creaky voice, a non-modal phonation type often stigmatized in the U.S. media, has become increasingly prevalent in the speech of young Americans across ethnic and regional groups. This paper aims to add to our knowledge of voice quality variation and how it interacts with ASR, by conducting three analyses using a new African American English (AAE) dataset.  Acoustic analyses show robust differences between creaky voice and modal voice, suggesting cross-ethnic similarity in vocal fold articulation between AAE and Mainstream American English (MAE) speakers. In addition, we observed gender differences in creaky production both quantitatively (women > men) and qualitatively (women: medial partial creaks vs. men: final full creaks). This indicates that young AAE female speakers are participating in the phonation change taking place in MAE. We also found that the creakier the speech, the more errors in ASR output, suggesting the importance of incorporating voice quality into ASR systems.",True
elie24b_interspeech,https://www.isca-archive.org/interspeech_2024/elie24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/elie24b_interspeech.html,Articulatory Configurations across Genders and Periods in French Radio and TV archives,"Speech Science, Speech Technology, and Gender (Special Session)",2024,"This paper studies changes in articulatory configurations across genders and periods using an inversion from acoustic to articulatory parameters. From a diachronic corpus based on French media archives spanning 60 years from 1955 to 2015, automatic transcription and forced alignment allowed extracting the central frame of each vowel. More than one million frames were obtained from over a thousand speakers across gender and age categories. Their formants were used from these vocalic frames to fit the parameters of Maeda's articulatory model. Evaluations of the quality of these processes are provided. We focus here on two parameters of Maedaâs model linked to total vocal tract length: the relative position of the larynx (higher for females) and the lips protrusion (more protruded for males). Implications for voice quality across genders are discussed. The effect across periods seems gender independent; thus, the assertion that females lowered their pitch with time is not supported.",True
miodonska24_interspeech,https://www.isca-archive.org/interspeech_2024/miodonska24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/miodonska24_interspeech.html,Frication noise features of Polish voiceless dental fricative and affricate produced by children with and without speech disorder,Phonetics and Phonology: Segmentals and Suprasegmentals,2024,"The study presents frication noise acoustic features of Polish dental voiceless sibilants (fricative /s/ and affricate /tÍ¡s/) in the speech samples collected from 106 children (83 with normative dental articulation and 23 with disorderedâinterdentalâarticulation) aged 4;11â8;0. We aimed to 1) verify the differences between the characteristics of the frication noise accompanying these two sounds and 2) investigate the influence of interdentality on the frication noise features. The analysis employed features of the noise band (fricative formants, formant-related measures, and noise energies) and linear-mixed effect models. The results showed significant acoustic differences between the voiceless dental fricative and affricate. Experiments suggest that the place of articulation (dental/interdental) can also be distinguished based on the spectral features of the frication band; this finding may be employed in computer-aided speech diagnosis tools.",True
maselli24_interspeech,https://www.isca-archive.org/interspeech_2024/maselli24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/maselli24_interspeech.html,Aerodynamics of Sakata labial-velar oral stops,Phonetics and Phonology: Segmentals and Suprasegmentals,2024,"The present contribution represents the first in-detail exploratory account of the aerodynamics of labial-velar oral stops in Sakata, a Bantu dialect cluster of southwestern Congo. Data collection took place at the phonetics laboratory facilities of UniversitÃ© de Mons with three speakers of central Sakata. Comparative data of labial-velar and plain bilabial oral stops are presented and analysed. Descriptive statistics of the relevant variables are discussed. Given each group of variables, MANOVA results are presented for specially tailored subsets of the whole dataset to investigate variance in the corpus. Sakata labial-velar stops are shown to differ from plain bilabials for duration, airflow, and pressure patterns. Voiceless labial-velar stops exhibit pressure and airflow values consistent with a more prominent lowering of the tongue root / larynx than their voiced counter- parts. Matches and mismatches with the available typological literature are also delineated and discussed.",True
erickson24_interspeech,https://www.isca-archive.org/interspeech_2024/erickson24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/erickson24_interspeech.html,Collecting Mandible Movement in Brazilian Portuguese,Phonetics and Phonology: Segmentals and Suprasegmentals,2024,"This paper reports on a corpus of Brazilian Portuguese (BP) mandible movements. The data was collected using a recently available technique, the MARRYS helmet, which allows for quick and reliable collection of mandible data of a large number of speakers. Audio and mandible were recorded from more than 90 L1 and L2 BP speakers. The recording process and signal synchronization are presented. A partial set of the corpus, based on 37 L1 speakers producing three sentences mostly composed of /a/ vowels, was segmented at the phone level. The jaw movements are compared to the sentence's prosodic structure. Results indicate that, similar to other languages, Brazilian Portuguese speakers show increased mandible lowering for stressed syllables but also quick mandible closing. Some interesting mandible opening and closing patterns are also reported on the prestress or post-stress positions in relation to the prosodic structure of Brazilian Portuguese.",True
suda24_interspeech,https://www.isca-archive.org/interspeech_2024/suda24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/suda24_interspeech.html,Who Finds This Voice Attractive? A Large-Scale Experiment Using In-the-Wild Data,Topics in Paralinguistics,2024,"This paper introduces CocoNut-Humoresque, an open-source large-scale speech likability corpus that includes speech segments and their per-listener likability scores. Evaluating voice likability is essential to designing preferable voices for speech systems, such as dialogue or announcement systems. In this study, we let 885 listeners rate 1800 speech segments of a wide range of speakers regarding their likability. When constructing the corpus, we also collected the multiple speaker attributes: genders, ages, and favorite YouTube videos. Therefore, the corpus enables the large-scale statistical analysis of voice likability regarding both speaker and listener factors. This paper describes the construction methodology and preliminary data analysis to reveal the gender and age biases in voice likability. In addition, the relationship between the likability and two acoustic features, the fundamental frequencies and the x-vectors of given utterances, is also investigated.",True
ahn24c_interspeech,https://www.isca-archive.org/interspeech_2024/ahn24c_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ahn24c_interspeech.html,Novel-view Acoustic Synthesis From 3D Reconstructed Rooms,Spatial Audio and Acoustics,2024,"We investigate the benefit of combining blind audio recordings with 3D scene information for novel-view acoustic synthesis. Given audio recordings from 2â4 microphones and the 3D geometry and material of a scene containing multiple unknown sound sources, we estimate the sound anywhere in the scene. We identify the main challenges of novel-view acoustic synthesis as sound source localization, separation, and dereverberation. While naively training an end-to-end network fails to produce high-quality results, we show that incorporating room impulse responses (RIRs) derived from 3D reconstructed rooms enables the same network to jointly tackle these tasks. Our method outperforms existing methods designed for the individual tasks, demonstrating its effectiveness at utilizing 3D visual information. In a simulated study on the Matterport3D-NVAS dataset, our model achieves near-perfect accuracy on source localization, a PSNR of 26.44 dB and a SDR of 14.23 dB for source separation and dereverberation, resulting in a PSNR of 25.55 dB and a SDR of 14.20 dB on novel-view acoustic synthesis. We release our code and model on our project website at https://github.com/apple/ml-nvas3d. Please wear headphones when listening to the results.",True
kim24n_interspeech,https://www.isca-archive.org/interspeech_2024/kim24n_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/kim24n_interspeech.html,Sound of Vision: Audio Generation from Visual Text Embedding through Training Domain Discriminator,Generative Models for Speech and Audio,2024,"Recent advancements in text-to-audio (TTA) models have demonstrated their ability to generate sound that aligns with user intentions. Despite this advancement, a notable limitation arises from the models' inability to effectively synthesize audio from visual-domain texts. In this study, we address this challenge by utilizing a novel dataset that pairs visual and acoustic-domain texts, derived using ChatGPT-3.5, and encoding switch through a domain discriminator. This approach ensures not only computational efficiency but also enhances the model's generalization, adaptability, and flexibility. It addresses concerns that training exclusively with visual texts might compromise audio generation quality from audio texts. This study presents a novel methodology for enhancing text-to-audio synthesis, demonstrating significant improvements in audio output fidelity from visual-text inputs.",True
deshmukh24b_interspeech,https://www.isca-archive.org/interspeech_2024/deshmukh24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/deshmukh24b_interspeech.html,PAM: Prompting Audio-Language Models for Audio Quality Assessment,Generative Models for Speech and Audio,2024,"Audio quality is a key performance metric for various audio processing tasks, including generative modeling, however its objective measurement remains a challenge. Audio-Language Models (ALM) are pre-trained on millions of audio-text pairs that may contain information about audio quality, the presence of artifacts or noise. Given an audio input and a text prompt about quality, an ALM can calculate a similarity score between the two. We exploit this capability and introduce PAM, a truly reference-free metric for assessing audio quality for different audio processing tasks. Contrary to other âreference-freeâ metrics, PAM does not require computing embeddings on a reference dataset nor training a task-specific model on a costly set of human listening scores. We extensively evaluate PAM against established metrics and newly collected human listening scores on four tasks: text- to-audio (TTA), text-to-music generation (TTM), text-to-speech (TTS), and deep noise suppression (DNS). We perform multiple ablation studies with controlled audio distortions, in-the-wild setups, and prompt choices. Our evaluation shows that overall, PAM correlates strongly with human listening scores and performs better than existing metrics. These results demonstrate the potential of ALM for computing a general-purpose audio quality metric. Code and human listening scores will be released.",True
aziz24_interspeech,https://www.isca-archive.org/interspeech_2024/aziz24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/aziz24_interspeech.html,Audio Enhancement from Multiple Crowdsourced Recordings: A Simple and Effective Baseline,Multi-Channel Speech Enhancement,2024,"With the popularity of cellular phones, events are often recorded by multiple devices from different locations and shared on social media. Several different recordings could be found for many events. Such recordings are usually noisy, where noise for each device is local and unrelated to others. This case of multiple microphones at unknown locations, capturing local, uncorrelated noise, was rarely treated in the literature. In this work we propose a simple and effective crowdsourced audio enhancement method to remove local noises at each input audio signal. Then, averaging all cleaned source signals gives an improved audio of the event. We demonstrate the effectiveness of our method using synthetic audio signals, together with real-world recordings. This simple approach can set a new baseline for crowdsourced audio enhancement for more sophisticated methods which we hope will be developed by the research community. Code, dataset, and models are available.",True
wang24j_interspeech,https://www.isca-archive.org/interspeech_2024/wang24j_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/wang24j_interspeech.html,HypR: A comprehensive study for ASR hypothesis revising with a reference corpus,Error Correction and Rescoring,2024,"With the development of deep learning, automatic speech recognition (ASR) has made significant progress. To further enhance the performance of ASR, revising recognition results is one of the lightweight but efficient manners. Various methods can be roughly classified into N -best reranking modeling and error correction modeling. The former aims to select the hypothesis with the lowest error rate from a set of candidates generated by ASR for a given input speech. The latter focuses on detecting recognition errors in a given hypothesis and correcting these errors to obtain an enhanced result. However, we observe that these studies are hardly comparable to each other, as they are usually evaluated on different corpora, paired with different ASR models, and even use different datasets to train the models. Accordingly, we first concentrate on providing an ASR hypothesis revising (HypR) dataset in this study. HypR contains several commonly used corpora (AISHELL-1, TED-LIUM 2, and LibriSpeech) and provides 50 recognition hypotheses for each speech utterance. The checkpoint models of ASR are also published. In addition, we implement and compare several classic and representative methods, showing the recent research progress in revising speech recognition results. We hope that the publicly available HypR dataset can become a reference benchmark for subsequent research and promote this field of research to an advanced level.",True
aimaiti24_interspeech,https://www.isca-archive.org/interspeech_2024/aimaiti24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/aimaiti24_interspeech.html,An Uyghur Extension to the MASSIVE Multi-lingual Spoken Language Understanding Corpus with Comprehensive Evaluations,Spoken Language Understanding,2024,"Spoken Language Understanding (SLU) plays a crucial role in task-oriented dialogues, and the development of SLU in various languages has been rapid. However, progress in Uyghur SLU research has been slow due to the lack of publicly available datasets. To address this issue, we extend the MASSIVE dataset to include Uyghur language, thus creating the first Uyghur SLU dataset, MASSIVE-UG. After incorporating MASSIVE-UG, the average overall accuracy of the other 51 languages has improved, demonstrating the reliability of the dataset constructed in this paper. Considering the agglutinative nature of Uyghur, we segmented it into stem and affix and conducted experiments using different embedding methods and multiple baselines. The experimental results indicate that the performance of Uyghur SLU is influenced by several factors, including representation, embedding, and modeling approach. The dataset and code are available at https://github.com/xjuspeech/MASSIVE-UG.",True
christ24_interspeech,https://www.isca-archive.org/interspeech_2024/christ24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/christ24_interspeech.html,This Paper Had the Smartest Reviewers - Flattery Detection Utilising an Audio-Textual Transformer-Based Approach,Spoken Language Understanding,2024,"Flattery is an important aspect of human communication that facilitates social bonding, shapes perceptions, and influences behaviour through strategic compliments and praise, leveraging the power of speech to build rapport effectively. Its automatic detection can thus enhance the naturalness of human-AI interactions. To meet this need, we present a novel audio textual dataset comprising 20 hours of speech and train machine learning models for automatic flattery detection. In particular, we employ pretrained AST, Wav2Vec2, and Whisper models for the speech modality, and Whisper TTS models combined with a RoBERTa text classifier for the textual modality. Subsequently, we build a multimodal classifier by combining text and audio representations. Evaluation on unseen test data demonstrates promising results, with Unweighted Average Recall scores reaching 82.46% in audio-only experiments, 85.97% in text-only experiments, and 87.16% using a multimodal approach.",True
uro24_interspeech,https://www.isca-archive.org/interspeech_2024/uro24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/uro24_interspeech.html,Detecting the terminality of speech-turn boundary for spoken interactions in French TV and Radio content,Spoken Dialogue Systems and Conversational Analysis 2,2024,"Transition Relevance Places are defined as the end of an utterance where the interlocutor may take the floor without interrupting the current speaker -i.e., a place where the turn is terminal. Analyzing turn terminality is useful to study the dynamic of turn-taking in spontaneous conversations. This paper presents an automatic classification of spoken utterances as Terminal or Non-Terminal in multi-speaker settings. We compared audio, text, and fusions of both approaches on a French corpus of TV and Radio extracts annotated with turn-terminality information at each speaker change. Our models are based on pre-trained self-supervised representations. We report results for different fusion strategies and varying context sizes. This study also questions the problem of performance variability by analyzing the differences in results for multiple training runs with random initialization. The measured accuracy would allow the use of these models for large-scale analysis of turn-taking.",True
bonafos24_interspeech,https://www.isca-archive.org/interspeech_2024/bonafos24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/bonafos24_interspeech.html,Dirichlet process mixture model based on topologically augmented signal representation for clustering infant vocalizations,"Computational Models of Human Language Acquisition, Perception, and Production (Special Session)",2024,"Based on audio recordings made once a month during the first 12 months of a child's life, we propose a new method for clustering this set of vocalizations. We use a topologically augmented representation of the vocalizations, employing two persistence diagrams for each vocalization: one computed on the surface of its spectrogram and one on the Takens' embeddings of the vocalization. A synthetic persistent variable is derived for each diagram and added to the MFCCs (Mel-frequency cepstral coefficients). Using this representation, we fit a non-parametric Bayesian mixture model with a Dirichlet process prior to model the number of components. This procedure leads to a novel data-driven categorization of vocal productions. Our findings reveal the presence of 8 clusters of vocalizations, allowing us to compare their temporal distribution and acoustic profiles in the first 12 months of life.",True
ryumina24_interspeech,https://www.isca-archive.org/interspeech_2024/ryumina24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ryumina24_interspeech.html,OCEAN-AI: open multimodal framework for personality traits assessment and HR-processes automatization,Show and Tell 3,2024,"Human personality traits (PT) reflect individual differences in patterns of thinking, feeling, and behaving. Knowledge on PT may be useful in many applied tasks in our everyday live. In this paper, we present a first open-source multimodal framework called OCEAN-AI for PT assessment (PTA) and HR-processes automatization. Our framework performs PTA analyzing three modalities, including audio, video, and text, and includes three processing modules. All the modules extract heterogeneous (deep neural and hand-crafted) features and use them for a com-
plex analysis of humanâs behavior. The final fourth module aggregates these six feature sets by a Siamese neural network with a gated attention mechanism. Our framework was tested on two free-available corpora, including First Impressions v2 and our MuPTA, and achieved the best results. Applying our framework, a user can automate solutions of some practical applied tasks, such as ranking potential candidates by professional responsibilities, forming efficient work teams and so on.",True
deluca24_interspeech,https://www.isca-archive.org/interspeech_2024/deluca24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/deluca24_interspeech.html,NumberLie: a game-based experiment to understand the acoustics of deception and truthfulness,"Phonetics, Phonology and Prosody",2024,"To record clearly defined natural deceptive speech with precise knowledge of the ground truth and immediate consequences for the lying subject we present here the NumberLie game. The NumberLie design enables simultaneous and isolated audio recording of five players in our state-of-the-art laboratory, or adapted to any number of players in an online setting, playing against each other in a number-based game revolving around deception and trustworthiness. We describe the technical solutions employed to guarantee precise labelling of statements as truths or lies and immediate consequences to each interaction, backed by a performance-based financial reward to motivate participants. The design is easily manipulated to tailor to specific research questions, maintaining constant or eliminating completely additional sources of variability.",True
vegarodriguez24_interspeech,https://www.isca-archive.org/interspeech_2024/vegarodriguez24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/vegarodriguez24_interspeech.html,Nasal Air Flow During Speech Production In Korebaju,Segmentals,2024,"Korebaju, also known as Koreguaje (ISO 639-3: coe), is a tonal language spoken by approximately 2,000 people in the Amazonian foothills of Colombia. As part of an ongoing research project, a native female speaker participated in our key study at our laboratory in France. Using the EVA2 station, we recorded synchronized acoustic signals (WAV), electroglottographic signals (EGG), vertical larynx movements (LT), and oral (OAF) and nasal (NAF) airflows to highlight the specific features of nasal consonants in this language. Previous research has noted the presence of a burst noise at the end of nasal consonant production. Our results reject the implosive or ejective realization of nasal consonants and dismiss the possibility of an oral release of the labial or buccal articulation. Simultaneously, our study aims to observe a potential nasal carryover effect on adjacent vowels due to the reported nasal harmony.",True
kye24_interspeech,https://www.isca-archive.org/interspeech_2024/kye24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/kye24_interspeech.html,Affricates in Lushootseed,Segmentals,2024,"In this study, I conduct the first acoustic analysis of affricates in Lushootseed (Coast Salish branch). The findings reveal that several acoustic measurements, such as VOT, release frication duration, intensity, spectral moments (Center of Gravity), and voice onset quality, characterize affricate contrasts with respect to their place of articulation and laryngeal type (i.e., voiced, voiceless, ejective). This study raises questions concerning (a) the acoustic-articulatory correlates of affricates, (b) typological classification of ejective affricates, and (c) the diachronic status of voiced affricates in Coast Salish languages.",True
nguyen24_interspeech,https://www.isca-archive.org/interspeech_2024/nguyen24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/nguyen24_interspeech.html,Identifying Speakers in Dialogue Transcripts: A Text-based Approach Using Pretrained Language Models,Speaker Recognition 2,2024,"We introduce an approach to identifying speaker names in dialogue transcripts, a crucial task for enhancing content accessibility and searchability in digital media archives. Despite the advancements in speech recognition, the task of text-based speaker identification (SpeakerID) has received limited attention, lacking large-scale, diverse datasets for effective model training. Addressing these gaps, we present a novel, large-scale dataset derived from the MediaSum corpus, encompassing transcripts from a wide range of media sources. We propose novel transformer-based models tailored for SpeakerID, leveraging contextual cues within dialogues to accurately attribute speaker names. Through extensive experiments, our best model achieves a great precision of 80.3%, setting a new benchmark for SpeakerID. The data and code are publicly available here: https: //github.com/adobe-research/speaker-identification",True
lodagala24_interspeech,https://www.isca-archive.org/interspeech_2024/lodagala24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lodagala24_interspeech.html,All Ears: Building Self-Supervised Learning based ASR models for Indian Languages at scale,Cross-Lingual and Multilingual Processing,2024,"The abundance of unlabeled speech and its ease of collection calls for the development of self-supervised learning (SSL) based speech foundation models, which have been effective across several downstream speech tasks. As a part of this work, we curate 29.5K hours of raw speech data across 24 Indian languages and multiple domains, to pre-train SSL models over 5 different architectures. We then fine-tune these models for the downstream Automatic Speech Recognition (ASR) task on 13 Indian languages and evaluate them over diverse benchmarks. In addition we measure the efficacy of these models by evaluating them over the SUPERB benchmark. Our work signifies the need for careful choice of the SSL objectives while emphasizing the benefits of multilingual pretraining. Our pre-trained models out-perform baseline models such as MMS-300M and IndicWav2Vec by 17.3% and 36.0% relative WER improvements respectively, on Indian language ASR.",True
paraskevopoulos24_interspeech,https://www.isca-archive.org/interspeech_2024/paraskevopoulos24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/paraskevopoulos24_interspeech.html,The Greek podcast corpus: Competitive speech models for low-resourced languages with weakly supervised data,Cross-Lingual and Multilingual Processing,2024,"The development of speech technologies for languages with limited digital representation poses significant challenges, pri- marily due to the scarcity of available data. This issue is exacerbated in the era of large, data-intensive models. Recent research has underscored the potential of leveraging weak supervision to augment the pool of available data. In this study, we compile an 800-hour corpus of Modern Greek from podcasts and employ Whisper large-v3 to generate silver transcriptions. This corpus is utilized to fine-tune our models, aiming to assess the efficacy of this approach in enhancing ASR performance. Our analysis spans 16 distinct podcast domains, alongside evaluations on established datasets for Modern Greek. The findings indicate consistent WER improvements, correlating with increases in both data volume and model size. Our study confirms that assembling large, weakly supervised corpora serves as a cost-effective strategy for advancing speech technologies in under-resourced languages.",True
vakirtzian24_interspeech,https://www.isca-archive.org/interspeech_2024/vakirtzian24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/vakirtzian24_interspeech.html,Speech Recognition for Greek Dialects: A Challenging Benchmark,Cross-Lingual and Multilingual Processing,2024,"Language technologies should be judged on their usefulness in real-world use cases. Despite recent impressive progress in automatic speech recognition (ASR), an often overlooked aspect in ASR research and evaluation is language variation in the form of non-standard dialects or language varieties.  To this end, this work introduces a challenging benchmark that focuses on four varieties of Greek (Aivaliot, Cretan, Griko, Messenian) encompassing challenges related to data availability, orthographic conventions, and complexities arising from language contact. Initial experiments with state-of-the-art models and established cross-lingual transfer techniques highlight the difficulty of adapting to such low-resource varieties.",True
gothi24_interspeech,https://www.isca-archive.org/interspeech_2024/gothi24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/gothi24_interspeech.html,A Dataset and Two-pass System for Reading Miscue Detection,Speech Assessment,2024,"Automatic speech recognition (ASR) has long been viewed as a promising solution to the resource-intensive task of oral reading fluency assessment. The demands on ASR accuracy, however, tend to be high, especially when applied to obtaining reliable reading diagnostics. The prior knowledge of reading prompts is typically used to limit the system WER. The accurate detection of mispronounced words, which can be relatively few in number, while limiting false positives, remains challenging. In this work, we present a new manually transcribed dataset of 1,110 elementary school children reading connected text in L2 English with wide-ranging proficiencies. Apart from local features derived from alternate decodings under different linguistic context constraints, we use an additional deep acoustic model. We discuss the performance gains achieved in a second pass over initial hybrid ASR hypotheses.",True
tomita24_interspeech,https://www.isca-archive.org/interspeech_2024/tomita24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/tomita24_interspeech.html,Analysis and Visualization of Directional Diversity in Listening Fluency of World Englishes Speakers in the Framework of Mutual Shadowing,Speech Assessment,2024,"English is spoken as a lingua franca with a diversity of pronunciations, called accents, and they have been well studied so far. In this study, a diversity of listening behaviors are focused on, and listening disfluencies are measured objectively while listening to World Englishes (WE). When speaker X listens to Y fluently, it does not always mean that Y listens to X fluently. After collecting different passages read aloud by different WE speakers, the collected oral passages are shadowed by the speakers themselves to quantify their listening disfluencies. Results show that, when X listens to Y, X's listening disfluency becomes larger when Y's pronunciation deviates from X's to a larger degree. Further, a method is proposed to visualize simultaneously a) how fluently a speaker listens to WE speakers and b) how fluently the WE speakers listen to that specific speaker. With this visualization, WE speakers are grouped based on their communicability in global contexts.",True
rajkhowa24_interspeech,https://www.isca-archive.org/interspeech_2024/rajkhowa24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/rajkhowa24_interspeech.html,TM-PATHVQA: 90000+ Textless Multilingual Questions for Medical Visual Question Answering,Question Answering from Speech and Spoken Dialogue Systems,2024,"In healthcare and medical diagnostics, Visual Question Answering (VQA) may emerge as a pivotal tool in scenarios where analysis of intricate medical images becomes critical for accurate diagnoses. Current text-based VQA systems limit their utility in scenarios where hands-free interaction and accessibility are crucial while performing tasks. A speech-based VQA system may provide a better means of interaction where information can be accessed while performing tasks simultaneously. To this end, this work implements a speech-based VQA system by introducing a Textless Multilingual Pathological VQA (TM-PathVQA) dataset, an expansion of the PathVQA dataset, containing spoken questions in English, German and French. This dataset comprises 98,397 multilingual spoken questions and answers based on 5,004 pathological images along with 70 hours of audio. Finally, this work benchmarks and compares TM-PathVQA systems implemented using various combinations of acoustic and visual features.",True
phukan24_interspeech,https://www.isca-archive.org/interspeech_2024/phukan24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/phukan24_interspeech.html,Towards Multilingual Audio-Visual Question Answering,Question Answering from Speech and Spoken Dialogue Systems,2024,"In this paper, we work towards extending Audio-Visual Question Answering (AVQA) to multilingual settings. Existing AVQA research has predominantly revolved around English and replicating it for addressing AVQA in other languages requires a substantial allocation of resources. As a scalable solution, we leverage machine translation and present two multilingual AVQA datasets for eight languages created from existing benchmark AVQA datasets. This prevents extra human annotation efforts of collecting questions and answers manually. To this end, we propose, MERA framework, by leveraging state-of-the-art (SOTA) video, audio, and textual foundation models for AVQA in multiple languages. We introduce a suite of models namely MERA-L, MERA-C, MERA-T with varied model architectures to benchmark the proposed datasets. We believe our work will open new research directions and act as a reference benchmark for future works in multilingual AVQA.",True
wan24b_interspeech,https://www.isca-archive.org/interspeech_2024/wan24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/wan24b_interspeech.html,CDSD: Chinese Dysarthria Speech Database,Dysarthric Speech Assessment,2024,"Dysarthric speech poses significant challenges for individuals with dysarthria, impacting their ability to communicate socially. Despite the widespread use of Automatic Speech Recognition (ASR), accurately recognizing dysarthric speech remains a formidable task, largely due to the limited availability of dysarthric speech data. To address this gap, we developed the Chinese Dysarthria Speech Database (CDSD), the most extensive collection of Chinese dysarthria data to date, featuring 133 hours of recordings from 44 speakers. Our benchmarks reveal a best Character Error Rate (CER) of 16.4%. Compared to the CER of 20.45% from our additional human experiments, Dysarthric Speech Recognition (DSR) demonstrates its potential in significant improvement of communication for individuals with dysarthria. The CDSD database will be made  publicly available at http://melab.psych.ac.cn/CDSD.html.",True
pan24b_interspeech,https://www.isca-archive.org/interspeech_2024/pan24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/pan24b_interspeech.html,COSMIC: Data Efficient Instruction-tuning For Speech In-Context Learning,Spoken Language Models for Universal Speech Processing (Special Session),2024,"We present a cost-effective method to integrate speech into a large language model (LLM), resulting in a Contextual Speech Model with Instruction-following/in-context-learning Capabilities (COSMIC) multi-modal LLM. Using GPT-3.5, we generate Speech Comprehension Test Question-Answer (SQA) pairs from speech transcriptions for supervised instruction tuning. With under 30 million trainable parameters and only 450 hours of English speech data, COSMIC demonstrates emerging capabilities in instruction-following and in-context learning. Equipped with such capabilities, COSMIC achieves a maximum 33.18 BLEU score in 0-shot EN-to-X speech to text translation (S2TT) and a significant boost in the 1-shot setting. Additionally, there is an average 25.8% relative Word Error Rate (WER) reduction for 1-shot cross-domain adaptation. COSMIC exhibits a significant automatic speech recognition (ASR) accuracy gain in contextual biasing tasks due to its instruction-following capability.",True
hwang24b_interspeech,https://www.isca-archive.org/interspeech_2024/hwang24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/hwang24b_interspeech.html,Acquisition of high vowel devoicing in Japanese: A production experiment with three and four year olds,L1/L2 Acquisition and Cross-Linguistic Factors,2024,"The purpose of this study is to investigate the developmental path of high vowel devoicing (HVD) in Japanese. A picture-naming task was conducted with Japanese-learning preschoolers of three and four years old. The empirical data presented in this study allow us not only to make comparisons with the data from 4 year-olds in a previous study, but also to address the devoicing patterns for the 3-year old children, which have received little attention in the developmental literature on HVD. The results of twenty children reveal distinct patterns depending on position; Word-medially, the overall occurrence of HVD increases if we compare the average HVD rates between the ages of 3 and 4, but their rate is not yet reached at the adult-like level at the age of four. Word-finally, on the other hand, the rates are overall lower than the word-medial devoicing. Further, unlike the incremental pattern observed in the rates for the word-medial devoicing, no clear developmental advancement is found in this position. The presence or absence of the developmental advancement appears to support the qualitative differences between two types of HVDs in distinct positions.",True
lin24j_interspeech,https://www.isca-archive.org/interspeech_2024/lin24j_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/lin24j_interspeech.html,VoxBlink2: A 100K+ Speaker Recognition Corpus and the Open-Set Speaker-Identification Benchmark,Speaker recognition evaluation and resources,2024,"In this paper, we provide a large audio-visual speaker recognition dataset, VoxBlink2, which includes approximately 10M utterances with videos from 110K+ speakers in the wild. This dataset represents a significant expansion over the VoxBlink dataset, encompassing a broader diversity of speakers and scenarios by the grace of an optimized data collection pipeline. Afterward, we explore the impact of training strategies, data scale, and model complexity on speaker verification and finally establish a new single-model state-of-the-art EER at 0.170% and minDCF at 0.006% on the VoxCeleb1-O test set. Such remarkable results motivate us to explore speaker recognition from a new challenging perspective. We raise the Open-Set Speaker-Identification task, which is designed to either match a probe utterance with a known gallery speaker or categorize it as an unknown query. Associated with this task, we design concrete benchmark and evaluation protocols. The data and model resources can be found in http://voxblink2.github.io.",True
huang24g_interspeech,https://www.isca-archive.org/interspeech_2024/huang24g_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/huang24g_interspeech.html,Active Speaker Detection in Fisheye Meeting Scenes with Scene Spatial Spectrums,Speaker recognition evaluation and resources,2024,"Active Speaker Detection (ASD) plays a crucial role in scene understanding tasks by determining whether an on-screen person in a given scene is speaking. In this work, to address the ASD in the context of multi-party roundtable meetings, we propose a novel approach that incorporates the fusion of spatial information of the scenes. To leverage the multiple data sources of the scenes, our method involves generating audio spatial spectrum heatmaps from the multi-channel audio and integrating them with the panoramic images. Additionally, we propose the novel FisheyeMeeting dataset, which combines fisheye panoramic video recordings with muti-channel audio captured from a six-channel circular microphone array. By enabling the multi-modal model to capture audio-visual cues in multi-party meeting scenes, our approach achieves an impressive 89.11% mAP on the FisheyeMeeting dataset. Notably, this outperforms the current SOTA methods by a significant 2.3% mAP improvement.",True
hoang24b_interspeech,https://www.isca-archive.org/interspeech_2024/hoang24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/hoang24b_interspeech.html,VSASV: a Vietnamese Dataset for Spoofing-Aware Speaker Verification,Speaker recognition evaluation and resources,2024,"Recent research in improving speaker verification systems to detect spoofed speech has seen a concentrated focus on English language, while the performance of such systems in other languages remains unexplored. This paper introduces the VSASV dataset for Spoofing-Aware Speaker Verification (SASV) in Vietnamese language. The dataset comprises over 174,000 spoofed utterances and 164,000 authentic utterances from 1,382 speakers, which were generated with the latest spoofing techniques to encourage the development of SASV systems in this language. We also provide experimental results on the efficacy of the different state-of-the-art anti-spoofing systems on Vietnamese language.",True
nafea24_interspeech,https://www.isca-archive.org/interspeech_2024/nafea24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/nafea24_interspeech.html,AraOffence: Detecting Offensive Speech Across Dialects in Arabic Media,Speech Type Classification,2024,"Natural language processing (NLP) has made efforts towards identifying toxicity and offensive content for the text and image modalities. Despite sharing similar concerns with text and images, such as increased access to online abuse using speech, speech offensiveness research trails behind. While NLP has primarily considered English language data, speech has emphasized under-represented languages such as Swahili and Wolof. In this work, we introduce ARAOFFENSE, a dataset of scripted media in Arabic dialects labelled for offensiveness. ARAOFFENSE contains 2146 instances, of which 475 are labelled as offensive, spanning 1.55 hours of audio. We assess the capabilities of speech models to detect offensive content and present a hard-to-beat multi-modal text and audio model which outperforms the baselines by 26+% in terms of the Matthews Correlation Coefficient. Our work thus presents the first benchmark for offensive speech detection in dialectical Arabic.",True
salman24_interspeech,https://www.isca-archive.org/interspeech_2024/salman24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/salman24_interspeech.html,Towards Naturalistic Voice Conversion: NaturalVoices Dataset with an Automatic Processing Pipeline,Speech Synthesis: Voice Conversion 3,2024,"Voice conversion (VC) research traditionally depends on scripted or acted speech, which lacks the natural spontaneity of real-life conversations. While natural speech data is limited for VC, our study focuses on filling in this gap. We introduce a novel data-sourcing pipeline that makes the release of a natural speech dataset for VC, named NaturalVoices. The pipeline extracts rich information in speech such as emotion and signal-to-noise ratio (SNR) from raw podcast data, utilizing recent deep learning methods and providing flexibility and ease of use. NaturalVoices marks a large-scale, spontaneous, expressive, and emotional speech dataset, comprising over 3,800 hours speech sourced from the original podcasts in the MSP-Podcast dataset. Objective and subjective evaluations demonstrate the effectiveness of using our pipeline for providing natural and expressive data for VC, suggesting the potential of NaturalVoices for broader speech generation tasks.",True
hai24_interspeech,https://www.isca-archive.org/interspeech_2024/hai24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/hai24_interspeech.html,DreamVoice: Text-Guided Voice Conversion,Speech Synthesis: Voice Conversion 3,2024,"Generative voice technologies are rapidly evolving, offering opportunities for more personalized and inclusive experiences. Traditional one-shot voice conversion (VC) requires a target recording during inference, limiting ease of usage in generating desired voice timbres. Text-guided generation offers an intuitive solution to convert voices to desired ""DreamVoices"" according to the users' needs. Our paper presents two major contributions to VC technology: (1) DreamVoiceDB, a robust dataset of voice timbre annotations for 900 speakers from VCTK and LibriTTS. (2) Two text-guided VC methods: DreamVC, an end-to-end diffusion-based text-guided VC model; and DreamVG, a versatile text-to-voice generation plugin that can be combined with any one-shot VC models. The experimental results demonstrate that our proposed methods trained on the DreamVoiceDB dataset generate voice timbres accurately aligned with the text prompt and achieve high-quality VC.",True
song24c_interspeech,https://www.isca-archive.org/interspeech_2024/song24c_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/song24c_interspeech.html,"ED-sKWS: Early-Decision Spiking Neural Networks for Rapid, and Energy-Efficient Keyword Spotting",Computational Resource Constrained ASR,2024,"Keyword Spotting (KWS) is essential in edge computing requiring rapid and energy-efficient responses. Spiking Neural Networks (SNNs) are well-suited for KWS for their efficiency and temporal capacity for speech. To further reduce the latency and energy consumption, this study introduces ED-sKWS, an SNN-based KWS model with an early-decision mechanism that can stop speech processing and output the result before the end of speech utterance. Furthermore, we introduce a Cumulative Temporal (CT) loss that can enhance prediction accuracy at both the intermediate and final timesteps. To evaluate early-decision performance, we present the SC-100 dataset including 100 speech commands with beginning and end timestamp annotation. Experiments on the Google Speech Commands v2 and our SC-100 datasets show that ED-sKWS maintains competitive accuracy with 61% timesteps and 52% energy consumption compared to SNN models without early-decision mechanism, ensuring rapid response and energy efficiency.",True
heuser24_interspeech,https://www.isca-archive.org/interspeech_2024/heuser24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/heuser24_interspeech.html,Quantification of stylistic differences in human- and ASR-produced transcripts of African American English,Evaluation of Speech Technology Systems,2024,"Common measures of accuracy used to assess the performance of automatic speech recognition (ASR) systems, as well as human transcribers, conflate multiple sources of error. Stylistic differences, such as verbatim vs non-verbatim, can play a significant role in ASR performance evaluation when differences exist between training and test datasets. The problem is compounded for speech from underrepresented varieties, where the speech to orthography mapping is not as standardized. We categorize the kinds of stylistic differences between 6 transcription versions, 4 human- and 2 ASR-produced, of 10 hours of African American English (AAE) speech. Focusing on verbatim features and AAE morphosyntactic features, we investigate the interactions of these categories with how well transcripts can be compared via word error rate (WER). The results, and overall analysis, help clarify how ASR outputs are a function of the decisions made by the training dataâs human transcribers.",True
choi24b_interspeech,https://www.isca-archive.org/interspeech_2024/choi24b_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/choi24b_interspeech.html,Self-Supervised Speech Representations are More Phonetic than Semantic,Neural Network Training for Speech Recognition,2024,"Self-supervised speech models (S3Ms) have become an effective backbone for speech applications. Various analyses suggest that S3Ms encode linguistic properties. In this work, we seek a more fine-grained analysis of the word-level linguistic properties encoded in S3Ms. Specifically, we curate a novel dataset of near homophone (phonetically similar) and synonym (semantically similar) word pairs and measure the similarities between S3M word representation pairs. Our study reveals that S3M representations consistently and significantly exhibit more phonetic than semantic similarity. Further, we question whether widely used intent classification datasets such as Fluent Speech Commands and Snips Smartlights are adequate for measuring semantic abilities. Our simple baseline, using only the word identity, surpasses S3M-based models. This corroborates our findings and suggests that high scores on these datasets do not necessarily guarantee the presence of semantic content.",True
zang24_interspeech,https://www.isca-archive.org/interspeech_2024/zang24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/zang24_interspeech.html,CtrSVDD: A Benchmark Dataset and Baseline Analysis for Controlled Singing Voice Deepfake Detection,"Acoustic Event Detection, Segmentation and Classification",2024,"Recent singing voice synthesis and conversion advancements necessitate robust singing voice deepfake detection (SVDD) models. Current SVDD datasets face challenges due to limited controllability, diversity in deepfake methods, and licensing restrictions. Addressing these gaps, we introduce CtrSVDD, a large-scale, diverse collection of bonafide and deepfake singing vocals. These vocals are synthesized using state-of-the-art methods from publicly accessible singing voice datasets. CtrSVDD includes 47.64 hours of bonafide and 260.34 hours of deepfake singing vocals, spanning 14 deepfake methods and involving 164 singer identities. We also present a baseline system with flexible front-end features, evaluated against a structured train/dev/eval split. The experiments show the importance of feature selection and highlight a need for generalization towards deepfake methods that deviate further from training distribution. The CtrSVDD dataset and baseline model weights are publicly accessible.",True
li24ha_interspeech,https://www.isca-archive.org/interspeech_2024/li24ha_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/li24ha_interspeech.html,DiveSound: LLM-Assisted Automatic Taxonomy Construction for Diverse Audio Generation,Speech and Audio Modelling,2024,"Audio generation has attracted significant attention. Despite remarkable enhancement in audio quality, existing models overlook diversity evaluation. This is partially due to the lack of a systematic sound class diversity framework and a matching dataset. To address these issues, we propose DiveSound, a novel framework for constructing multimodal datasets with in-class diversified taxonomy, assisted by large language models. As both textual and visual information can be utilized to guide diverse generation, DiveSound leverages multimodal contrastive representations in data construction. Our framework is highly autonomous and can be easily scaled up. We provide a text-audio-image aligned diversity dataset whose sound event class tags have an average of 2.42 subcategories. Text-to-audio experiments on the constructed dataset show a substantial increase of diversity with the help of the guidance of visual information. Our samples are available at https://divesounddemo.github.io",True
zhang24h_interspeech,https://www.isca-archive.org/interspeech_2024/zhang24h_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/zhang24h_interspeech.html,"URGENT Challenge: Universality, Robustness, and Generalizability For Speech Enhancement","Deep Learning-Based Speech Enhancement: Approaches, Scalability, and Evaluation",2024,"The last decade has witnessed significant advancements in deep learning-based speech enhancement (SE). However, most existing SE research has limitations on the coverage of SE sub-tasks, data diversity and amount, and evaluation metrics. To fill this gap and promote research toward universal SE, we establish a new SE challenge, named URGENT, to focus on the universality, robustness, and generalizability of SE. We aim to extend the SE definition to cover different sub-tasks to explore the limits of SE models, starting from denoising, dereverberation, bandwidth extension, and declipping. A novel framework is proposed to unify all these sub-tasks in a single model, allowing the use of all existing SE approaches. We collected public speech and noise data from different domains to construct diverse evaluation data. Finally, we discuss the insights gained from our preliminary baseline experiments based on both generative and discriminative SE methods with 12 curated metrics.",True
richter24_interspeech,https://www.isca-archive.org/interspeech_2024/richter24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/richter24_interspeech.html,EARS: An Anechoic Fullband Speech Dataset Benchmarked for Speech Enhancement and Dereverberation,"Deep Learning-Based Speech Enhancement: Approaches, Scalability, and Evaluation",2024,"We release the EARS (Expressive Anechoic Recordings of Speech) dataset, a high-quality speech dataset comprising 107 speakers from diverse backgrounds, totalling in 100 hours of clean, anechoic speech data. The dataset covers a large range of different speaking styles, including emotional speech, different reading styles, non-verbal sounds, and conversational freeform speech. We benchmark various methods for speech enhancement and dereverberation on the dataset and evaluate their performance through a set of instrumental metrics. In addition, we conduct a listening test with 20 participants for the speech enhancement task, where a generative method is preferred. We introduce a blind test set that allows for automatic online evaluation of uploaded data. Dataset download links and automatic evaluation server can be found online.",True
ward24_interspeech,https://www.isca-archive.org/interspeech_2024/ward24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/ward24_interspeech.html,Towards a General-Purpose Model of Perceived Pragmatic Similarity,Speech Synthesis: Other Topics 1,2024,"Models for estimating the similarity between two utterances are fundamental in speech technology.  While fairly good automatic measures exist for semantic similarity, pragmatic similarity has not been previously explored. Using a new collection of thousands of human judgments of the pragmatic similarity between utterance pairs, we train and evaluate various predictive models. The best performing model, which uses 103 features selected from HuBert's 24th layer, correlates on average 0.74 with human judges for the highest-quality data subset, and it sometimes approaches human inter-annotator agreement. We also find evidence for some degree of generality across languages.",True
vinnikov24_interspeech,https://www.isca-archive.org/interspeech_2024/vinnikov24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/vinnikov24_interspeech.html,"NOTSOFAR-1 Challenge: New Datasets, Baseline, and Tasks for Distant Meeting Transcription","Noise, Far-Field, Multi-Talker, Enhancement, Audio Classification",2024,"We introduce the first Natural Office Talkers in Settings of Far-field Audio Recordings (NOTSOFAR) Challenge, datasets, and baseline system. The challenge focuses on distant speaker diarization and automatic speech recognition (DASR) in meeting scenarios, with single-channel and known-geometry multi-channel tracks, using a single device. We launch two new datasets: First, a benchmark dataset of 280 English meetings, averaging 6 minutes each, capturing a broad spectrum of acoustic and conversational patterns across 30 rooms with 4-8 attendees. Second, a 1000-hour simulated training dataset, synthesized for real-world generalization, incorporating 15,000 real acoustic transfer functions. The NOTSOFAR-1 Challenge aims to advance research in the field of DASR, providing key resources to unlock the potential of data-driven methods, which we believe are currently constrained by the absence of comprehensive high-quality training and benchmark datasets.",True
borsdorf24_interspeech,https://www.isca-archive.org/interspeech_2024/borsdorf24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/borsdorf24_interspeech.html,wTIMIT2mix: A Cocktail Party Mixtures Database to Study Target Speaker Extraction for Normal and Whispered Speech,"Noise, Far-Field, Multi-Talker, Enhancement, Audio Classification",2024,"Target speaker extraction (TSE) seeks to single out a target speaker's voice from a given speech mixture signal with the help of a target reference signal. This algorithm enables novel speech applications such as smart hearing aids. A TSE system has to work reliably in any everyday conversational situation. This may also include speakers who switch naturally between normal and whispered speech modes. This work represents the first attempt to perform TSE for whispered speech. For this, we construct a new first of its kind database, called wTIMIT2mix, which comprises two-speaker speech mixtures and target speaker reference signals given in both normal and whispered speech modes. Our results on TSE show that if these conditions are included in the training, a model can be equipped to work under all closed-set conditions.",True
tapo24_interspeech,https://www.isca-archive.org/interspeech_2024/tapo24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/tapo24_interspeech.html,Leveraging Speech Data Diversity to Document Indigenous Heritage and Culture,Spoken Term Detection and Speech Retrieval,2024,"The majority of the world's 7,000 languages lack a standardized writing system. In this paper we consider one such language, Bambara, which is rarely written down but is widely spoken in Mali and neighboring countries. We explore the task of using automatic speech recognition (ASR) to transcribe culturally significant recordings focused on two domains: archival linguistic and anthropological fieldwork and contemporary oral histories performed by griots, the traditional Mande history keepers. We describe our two 6.5-hour corpora then experiment with different data configurations and multi-stage tuning from pretrained multilingual models within two neural ASR architectures. We find that while the diversity in content, style, and recording quality across the two corpora presents challenges, their commonalities can sometimes be leveraged to improve ASR accuracy. We note, however, that the diverse qualities of these corpora diminish their utility for cross-domain ASR training.",True
mohapatra24_interspeech,https://www.isca-archive.org/interspeech_2024/mohapatra24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/mohapatra24_interspeech.html,Missingness-resilient Video-enhanced Multimodal Disfluency Detection,Speech Disorders 1,2024,"Most existing speech disfluency detection techniques only rely upon acoustic data. In this work, we present a practical multimodal disfluency detection approach that leverages available video data together with audio. We curate an audio-visual dataset and propose a novel fusion technique with unified weight-sharing modality-agnostic encoders to learn the temporal and semantic context. Our resilient design accommodates real-world scenarios where the video modality may sometimes be missing during inference. We also present alternative fusion strategies when both modalities are assured to be complete. In experiments across five disfluency-detection tasks, our unified multimodal approach significantly outperforms Audio-only unimodal methods, yielding an average absolute improvement of 10% (i.e., 10 percentage point increase) when both video and audio modalities are always available, and 7% even when video modality is missing in half of the samples.",True
gong24_interspeech,https://www.isca-archive.org/interspeech_2024/gong24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/gong24_interspeech.html,AS-70: A Mandarin stuttered speech dataset for automatic speech recognition and stuttering event detection,Speech Disorders 1,2024,"The rapid advancements in speech technologies over the past two decades have led to human-level performance in tasks like automatic speech recognition (ASR) for fluent speech. However, the efficacy of these models diminishes when applied to atypical speech, such as stuttering. This paper introduces AS-70, the first publicly available Mandarin stuttered speech dataset, which stands out as the largest dataset in its category. Encompassing conversational and voice command reading speech, AS-70 includes verbatim manual transcription, rendering it suitable for various speech-related tasks. Furthermore, baseline systems are established, and experimental results are presented for ASR and stuttering event detection (SED) tasks. By incorporating this dataset into the model fine-tuning, significant improvement in the state-of-the-art ASR models, e.g., Whisper and Hubert, are observed, enhancing their inclusivity in addressing stuttered speech.",True
demopoulos24_interspeech,https://www.isca-archive.org/interspeech_2024/demopoulos24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/demopoulos24_interspeech.html,Preliminary Investigation of Psychometric Properties of a Novel Multimodal Dialog Based Affect Production Task in Children and Adolescents with Autism,Connecting Speech-science and Speech-technology for Childrenâs Speech (Special Session),2024,"Impairments in nonverbal communication are a defining feature of autism spectrum disorder (ASD) and can manifest as difficulty with, or even complete lack of, communication of emotional states via production of facial affect or vocal affect. The purpose of this study was to evaluate psychometric properties of a novel multimodal dialog based Affect Production Task (APT) in children and adolescents (ages 8-17) with a diagnosis of autism (N=72) or neurotypical controls (N=37). Participants completed activities designed to quantify objective facial and vocal affect production ability using audiovisual capture. Criterion, ecological, and discriminant validity were assessed. Psychometric performance across task conditions, age, sex, and race-ethnicity also was examined. Results of this initial psychometric evaluation suggest that the APT is a valid measure of affect production abilities in children and adolescents, and that psychometric performance is invariant to age, sex, or race/ethnicity.",True
baumann24_interspeech,https://www.isca-archive.org/interspeech_2024/baumann24_interspeech.pdf,https://www.isca-archive.org/interspeech_2024/baumann24_interspeech.html,Automatic Evaluation of a Sentence Memory Test for Preschool Children,Connecting Speech-science and Speech-technology for Childrenâs Speech (Special Session),2024,"Assessment of memory capabilities in preschool-aged children is crucial for early detection of potential speech development impairments or delays. We present an approach for the automatic evaluation of a standardized sentence memory test specifically for preschool children. Our methodology leverages automatic transcription of recited sentences and evaluation based on natural language processing techniques. We demonstrate the effectiveness of our approach on a dataset comprised of recited sentences from preschool-aged children, incorporating ratings of semantic and syntactic correctness. The best performing systems achieve an F1 score of 91.7% for semantic correctness and 86.1% for syntactic correctness using automatic transcripts. Our results showcase the potential of automated evaluation systems in providing reliable and efficient assessments of memory capabilities in early childhood, facilitating timely interventions and support for children with language development needs.",True
