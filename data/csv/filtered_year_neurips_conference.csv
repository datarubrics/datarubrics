id,title,abstract,track,conference,year,status,author,score,url_link
-I6La-GJS5b,Deep Credal Neural Network: Characterization of Imprecision Between Categories,"Quantification and reduction of uncertainty in deep learning techniques have received much attention but ignored how to characterize the imprecision caused by such uncertainty. In some tasks, we prefer to obtain an imprecise result rather than being willing or unable to bear the cost of an error. For this purpose, we present a deep credal neural network (DCNN) based on the theory of belief functions, aiming to assign samples that are indistinguishable for specific categories to the union of these, called meta-category. In DCNN, a designed mechanism assigns multiple labels to some training samples to constrain the known loss functions. Once assigned, it indicates that these samples may be in an overlapping region of different categories, or the original label is wrong. Afterward, the training labels are reconstructed and therefore classify the test samples. Once assigned to any meta-category, the prediction of this test sample is imprecise. Experiments based on some remarkable networks have shown that DCNN can not only improve accuracy but also reasonably characterize imprecision both in the training and test sets.",Datasets & Benchmarks,NeurIPS,2021,Reject,Zuowei Zhang;Zhunga LIU;Arnaud Martin;Kuang Zhou,False,https://openreview.net/pdf?id=-I6La-GJS5b
-or413Lh_aF,Benchmarking Data-driven Surrogate Simulators for Artificial Electromagnetic Materials,"Artificial electromagnetic materials (AEMs), including metamaterials, derive their electromagnetic properties from geometry rather than chemistry. With the appropriate geometric design, AEMs have achieved exotic properties not realizable with conventional materials (e.g., cloaking or negative refractive index). However, understanding the relationship between the AEM structure and its properties is often poorly understood. While computational electromagnetic simulation (CEMS) may help design new AEMs, its use is limited due to its long computational time. Recently, it has been shown that deep learning can be an alternative solution to infer the relationship between an AEM geometry and its properties using a (relatively) small pool of CEMS data. However, the limited publicly released datasets and models and no widely-used benchmark for comparison have made using deep learning approaches even more difficult. Furthermore, configuring CEMS for a specific problem requires substantial expertise and time, making reproducibility challenging. Here, we develop a collection of three classes of AEM problems: metamaterials, nanophotonics, and color filter designs. We also publicly release software, allowing other researchers to conduct additional simulations for each system easily. Finally, we conduct experiments on our benchmark datasets with three recent neural network architectures: the multilayer perceptron (MLP), MLP-mixer, and transformer. We identify the methods and models that generalize best over the three problems to establish the best practice and baseline results upon which future research can build.",Datasets & Benchmarks,NeurIPS,2021,Poster,Yang Deng;Juncheng Dong;Simiao Ren;Omar Khatib;Mohammadreza Soltani;Vahid Tarokh;Willie Padilla;Jordan Malof,True,https://openreview.net/pdf?id=-or413Lh_aF
-v4OuqNs5P,Habitat-Matterport 3D Dataset (HM3D): 1000 Large-scale 3D Environments for Embodied AI,"We present the Habitat-Matterport 3D (HM3D) dataset. HM3D is a large-scale dataset of 1,000 building-scale 3D reconstructions from a diverse set of real-world locations. Each scene in the dataset consists of a textured 3D mesh reconstruction of interiors such as multi-ﬂoor residences, stores, and other private indoor spaces.

HM3D surpasses existing datasets available for academic research in terms of physical scale, completeness of the reconstruction, and visual ﬁdelity. HM3D contains 112.5k m^2 of navigable space, which is 1.4 - 3.7× larger than other building-scale datasets (MP3D, Gibson). When compared to existing photorealistic 3D datasets (Replica, MP3D, Gibson, ScanNet), rendered images from HM3D have 20 - 85% higher visual ﬁdelity w.r.t. counterpart images captured with real cameras, and HM3D meshes have 34 - 91% fewer artifacts due to incomplete surface reconstruction.

The increased scale, ﬁdelity, and diversity of HM3D directly impacts the performance of embodied AI agents trained using it. In fact, we ﬁnd that HM3D is ‘pareto optimal’ in the following sense – agents trained to perform PointGoal navigation on HM3D achieve the highest performance regardless of whether they are evaluated on HM3D, Gibson, or MP3D. No similar claim can be made about training on other datasets. HM3D-trained PointNav agents achieve 100% performance on Gibson-test dataset, suggesting that it might be time to retire that episode dataset. The HM3D dataset, analysis code, and pre-trained models are publicly released: https://aihabitat.org/datasets/hm3d/.",Datasets & Benchmarks,NeurIPS,2021,Poster,Santhosh Kumar Ramakrishnan;Aaron Gokaslan;Erik Wijmans;Oleksandr Maksymets;Alexander Clegg;John M Turner;Eric Undersander;Wojciech Galuba;Andrew Westbury;Angel X Chang;Manolis Savva;Yili Zhao;Dhruv Batra,True,https://openreview.net/pdf?id=-v4OuqNs5P
-wVVl_UPr8,The PAIR-R24M Dataset for Multi-animal 3D Pose Estimation,"Understanding the biological basis of social and collective behaviors in animals is a key goal of the life sciences, and may yield important insights for engineering intelligent multi-agent systems. A critical step in interrogating the mechanisms underlying social behaviors is a precise readout of the 3D pose of interacting animals. While approaches for multi-animal pose estimation are beginning to emerge, they remain challenging to compare due to the lack of standardized training and benchmark datasets. Here we introduce the PAIR-R24M (Paired Acquisition of Interacting oRganisms - Rat) dataset for multi-animal 3D pose estimation, which contains 24.3 million frames of RGB video and 3D ground-truth motion capture of dyadic interactions in laboratory rats. PAIR-R24M contains data from 18 distinct pairs of rats and 24 different viewpoints. We annotated the data with 11 behavioral labels and 3 interaction categories to facilitate benchmarking in rare but challenging behaviors. To establish a baseline for markerless multi-animal 3D pose estimation, we developed a multi-animal extension of DANNCE, a recently published network for 3D pose estimation in freely behaving laboratory animals. As the first large multi-animal 3D pose estimation dataset, PAIR-R24M will help advance 3D animal tracking approaches and aid in elucidating the neural basis of social behaviors.",Datasets & Benchmarks,NeurIPS,2021,Poster,Jesse D Marshall;Ugne Klibaite;amanda gellis;Diego E Aldarondo;Bence Olveczky;Timothy W DUNN,True,https://openreview.net/pdf?id=-wVVl_UPr8
0OyT3oVv_Rk,Design-Bench: Benchmarks for Data-Driven Offline Model-Based Optimization,"Black-box model-based optimization (MBO) problems, where the goal is to find a design input that maximizes an unknown objective function,  are ubiquitous in a wide range of domains, such as the design of proteins, DNA sequences, aircraft, and robots.  Solving model-based optimization problems typically requires actively querying the unknown objective function on design proposals, which means physically building the candidate molecule, aircraft, or robot, testing it, and storing the result. This process can be expensive and time-consuming, and one might instead prefer to optimize for the best design using only the data one already has. This setting -- called offline MBO -- poses substantial and different algorithmic challenges than more commonly studied online techniques. A number of recent works have demonstrated success with offline MBO for high-dimensional optimization problems using high-capacity deep neural networks. However, the lack of standardized benchmarks in this emerging field is making progress difficult to track. To address this, we present Design-Bench, a benchmark for offline MBO with a unified evaluation protocol and reference implementations of recent methods. Our benchmark includes a suite of diverse and realistic tasks derived from real-world optimization problems in biology, materials science, and robotics that present distinct challenges for offline MBO. Our benchmark and reference implementations are publicly available at: https://github.com/brandontrabucco/design-bench",Datasets & Benchmarks,NeurIPS,2021,Reject,Brandon Trabucco;Xinyang Geng;Aviral Kumar;Sergey Levine,True,https://openreview.net/pdf?id=0OyT3oVv_Rk
0uQIr4XA77f,STEP: Segmenting and Tracking Every Pixel,"The task of assigning semantic classes and track identities to every pixel in a video is called video panoptic segmentation. Our work is the first that targets this task in a real-world setting requiring dense interpretation in both spatial and temporal domains. As the ground-truth for this task is difficult and expensive to obtain, existing datasets are either constructed synthetically or only sparsely annotated within short video clips. To overcome this, we introduce a new benchmark encompassing two datasets, KITTI-STEP, and MOTChallenge-STEP. The datasets contain long video sequences, providing challenging examples and a test-bed for studying long-term pixel-precise segmentation and tracking under real-world conditions. We further propose a novel evaluation metric Segmentation and Tracking Quality (STQ) that fairly balances semantic and tracking aspects of this task and is more appropriate for evaluating sequences of arbitrary length. Finally, we provide several baselines to evaluate the status of existing methods on this new challenging dataset. We have made our datasets, metric, benchmark servers, and baselines publicly available, and hope this will inspire future research.",Datasets & Benchmarks,NeurIPS,2021,Poster,Mark Weber;Jun Xie;Maxwell D Collins;Yukun Zhu;Paul Voigtlaender;Hartwig Adam;Bradley Green;Andreas Geiger;Bastian Leibe;Daniel Cremers;Aljosa Osep;Laura Leal-Taixé;Liang-Chieh Chen,True,https://openreview.net/pdf?id=0uQIr4XA77f
1Y9fPheTgpp,Robustness Disparities in Commercial Face Detection,"Facial detection and analysis systems have been deployed by large companies and critiqued by scholars and activists for the past decade. Critiques that focus on system performance analyze disparity of the system's output, i.e., how frequently is a face detected for different Fitzpatrick skin types or perceived genders. However, we focus on the robustness of these system outputs under noisy natural perturbations. We present the first of its kind detailed benchmark of the robustness of two such systems: Amazon Rekognition and Microsoft Azure. We use both standard and recently released academic facial datasets to quantitatively analyze trends in robustness for each. Qualitatively across all the datasets and systems, we find that photos of individuals who are \\\\emph{older}, \\\\emph{masculine presenting}, of \\\\emph{darker skin type}, or have \\\\emph{dim lighting} are more susceptible to errors than their counterparts in other identities.",Datasets & Benchmarks,NeurIPS,2021,Reject,Samuel Dooley;Tom Goldstein;John P Dickerson,False,https://openreview.net/pdf?id=1Y9fPheTgpp
1dq2MVDXot-,Understanding Interlocking Dynamics of Cooperative Rationalization,"Selective rationalization explains the prediction of complex neural networks by finding a small subset of the input that is sufficient to predict the neural model output. The selection mechanism is commonly integrated into the model itself by specifying a two-component cascaded system consisting of a rationale generator, which makes a binary selection of the input features (which is the rationale), and a predictor, which predicts the output based only on the selected features. The components are trained jointly to optimize prediction performance. In this paper, we reveal a major problem with such cooperative rationalization paradigm --- model interlocking. Inter-locking arises when the predictor overfits to the features selected by the generator thus reinforcing the generator's selection even if the selected rationales are sub-optimal. The fundamental cause of the interlocking problem is that the rationalization objective to be minimized is concave with respect to the generator’s selection policy. We propose a new rationalization framework, called A2R, which introduces a third component into the architecture, a predictor driven by soft attention as opposed to selection. The generator now realizes both soft and hard attention over the features and these are fed into the two different predictors. While the generator still seeks to support the original predictor performance, it also minimizes a gap between the two predictors. As we will show theoretically, since the attention-based predictor exhibits a better convexity property, A2R can overcome the concavity barrier. Our experiments on two synthetic benchmarks and two real datasets demonstrate that A2R can significantly alleviate the interlock problem and find explanations that better align with human judgments.",main,NeurIPS,2021,Poster,Mo Yu;Yang Zhang;Shiyu Chang;Tommi S. Jaakkola,True,https://openreview.net/pdf?id=1dq2MVDXot-
1k4rJYEwda-,HPOBench: A Collection of Reproducible Multi-Fidelity Benchmark Problems for HPO,"To achieve peak predictive performance, hyperparameter optimization (HPO) is a crucial component of machine learning and its applications. Over the last years, the number of efficient algorithms and tools for HPO grew substantially. At the same time, the community is still lacking realistic, diverse, computationally cheap, and standardized benchmarks. This is especially the case for multi-fidelity HPO methods. To close this gap, we propose HPOBench, which includes 7 existing and 5 new benchmark families, with a total of more than 100 multi-fidelity benchmark problems. HPOBench allows to run this extendable set of multi-fidelity HPO benchmarks in a reproducible way by isolating and packaging the individual benchmarks in containers. It also provides surrogate and tabular benchmarks for computationally affordable yet statistically sound evaluations. To demonstrate HPOBench’s broad compatibility with various optimization tools, as well as its usefulness, we conduct an exemplary large-scale study evaluating 13 optimizers from 6 optimization tools. We provide HPOBench here: https://github.com/automl/HPOBench.",Datasets & Benchmarks,NeurIPS,2021,Poster,Katharina Eggensperger;Philipp Müller;Neeratyoy Mallik;Matthias Feurer;Rene Sass;Aaron Klein;Noor Awad;Marius Lindauer;Frank Hutter,True,https://openreview.net/pdf?id=1k4rJYEwda-
1vC5GFOXuhM,CCNLab: A Benchmarking Framework for Computational Cognitive Neuroscience,"CCNLab is a benchmark for evaluating computational cognitive neuroscience models on empirical data. As a starting point, its focus is classical conditioning, which studies how animals predict reward and punishment in the environment. CCNLab includes a collection of simulations of seminal experiments expressed under a common API, as wells as tools for visualizing and comparing simulated data with empirical data. CCNLab is broad, incorporating representative experiments from different categories of phenomena; flexible, allowing the straightforward addition of new experiments; and easy-to-use, so researchers can focus on developing better models. We envision CCNLab as a testbed for unifying computational theories of learning in the brain. We also hope that it can broadly accelerate neuroscience research and facilitate interaction between the fields of neuroscience, psychology, and artificial intelligence.",Datasets & Benchmarks,NeurIPS,2021,Poster,Nikhil Xie Bhattasali;Momchil Tomov;Samuel Gershman,False,https://openreview.net/pdf?id=1vC5GFOXuhM
1xDTDk3XPW,A Large-Scale Database for Graph Representation Learning,"With the rapid emergence of graph representation learning, the construction of new large-scale datasets are necessary to distinguish model capabilities and accurately assess the strengths and weaknesses of each technique. By carefully analyzing existing graph databases, we identify 3 critical components important for advancing the field of graph representation learning: (1) large graphs, (2) many graphs, and (3) class diversity. To date, no single graph database offers all of these desired properties. We introduce MalNet , the largest public graph database ever constructed, representing a large-scale ontology of malicious software function call graphs. MalNet contains over 1.2 million graphs, averaging over 15k nodes and 35k edges per graph, across a hierarchy of 47 types and 696 families. Compared to the popular REDDIT-12K database, MalNet offers 105x more graphs, 44x larger graphs on average, and 63x more classes. We provide a detailed analysis of MalNet, discussing its properties and provenance, along with the evaluation of state-of-the-art machine learning and graph neural network techniques. The unprecedented scale and diversity of MalNet offers exciting opportunities to  advance the frontiers of graph representation learning--enabling new discoveries and research into imbalanced classification, explainability and the impact of class hardness. The database is publicly available at www.mal-net.org.",Datasets & Benchmarks,NeurIPS,2021,Poster,Scott Freitas;Yuxiao Dong;Joshua Neil;Duen Horng Chau,True,https://openreview.net/pdf?id=1xDTDk3XPW
3ZQqjt_Q6b,EventNarrative: A Large-scale Event-centric Dataset for Knowledge Graph-to-Text Generation,"We introduce EventNarrative, a knowledge graph-to-text dataset from publicly available open-world knowledge graphs. Given the recent advances in event-driven Information Extraction (IE), and that prior research on graph-to-text only focused on entity-driven KGs, this paper focuses on event-centric data. However, our data generation system can still be adapted to other types of KG data. Existing large-scale datasets in the graph-to-text area are non-parallel, meaning there is a large disconnect between the KGs and text. The datasets that have a paired KG and text, are small scale and manually generated or generated without a rich ontology, making the corresponding graphs sparse. Furthermore, these datasets contain many unlinked entities between their KG and text pairs. EventNarrative consists of approximately 230,000 graphs and their corresponding natural language text, six times larger than the current largest parallel dataset. It makes use of a rich ontology, all the KGs entities are linked to the text, and our manual annotations confirm a high data quality. Our aim is two-fold: to help break new ground in event-centric research where data is lacking and to give researchers a well-defined, large-scale dataset in order to better evaluate existing and future knowledge graph-to-text models. We also evaluate two types of baselines on EventNarrative: a graph-to-text specific model and two state-of-the-art language models, which previous work has shown to be adaptable to the knowledge graph-to-text domain.",Datasets & Benchmarks,NeurIPS,2021,Poster,Anthony Colas;Ali Sadeghian;Yue Wang;Daisy Zhe Wang,True,https://openreview.net/pdf?id=3ZQqjt_Q6b
3_hgF1NAXU7,CrowdSpeech and Vox DIY: Benchmark Dataset for Crowdsourced Audio Transcription,"Domain-specific data is the crux of the successful transfer of machine learning systems from benchmarks to real life. In simple problems such as image classification, crowdsourcing has become one of the standard tools for cheap and time-efficient data collection: thanks in large part to advances in research on aggregation methods. However, the applicability of crowdsourcing to more complex tasks  (e.g., speech recognition) remains limited due to the lack of principled aggregation methods for these modalities. The main obstacle towards designing aggregation methods for more advanced applications is the absence of training data, and in this work, we focus on bridging this gap in speech recognition. For this, we collect and release CrowdSpeech --- the first publicly available large-scale dataset of crowdsourced audio transcriptions. Evaluation of existing and novel aggregation methods on our data shows room for improvement, suggesting that our work may entail the design of better algorithms. At a higher level, we also contribute to the more general challenge of developing the methodology for reliable data collection via crowdsourcing. In that, we design a principled pipeline for constructing datasets of crowdsourced audio transcriptions in any novel domain. We show its applicability on an under-resourced language by constructing VoxDIY --- a counterpart of CrowdSpeech for the Russian language. We also release the code that allows a full replication of our data collection pipeline and share various insights on best practices of data collection via crowdsourcing.",Datasets & Benchmarks,NeurIPS,2021,Poster,Nikita Pavlichenko;Ivan Stelmakh;Dmitry Ustalov,True,https://openreview.net/pdf?id=3_hgF1NAXU7
3g5KdPD8LIL,CUD-NET: Color Universal Design Neural Filter for the Color Weakness,"Information on images should be visually understood to anyone, including the color weakness. However, it is not recognizable if color that seems distorted to the color weakness meets an adjacent object. We suggest CUD-NET based on convolutional deep neural network to generate color universal design (CUD) images that satisfy both color preservation and distinguishment of color for input images. CUD-NET regresses the node point of the piecewise linear function based on information of input images and comprises a specific filter per image. We present the following methods to generate CUD images for the color weakness. First, we refine the CUD dataset on specific criteria by color experts. Second, the input image information is expanded through the pre-processing specialized on the color weakness vision. Third, we suggest a multi-modal feature fusion architecture that combines features to process expanded images. Finally, we suggest a deformable loss function by the composition of the predicted image through the model to avoid the one-to-many problems of the dataset.",main,NeurIPS,2021,Reject,Sunyong Seo;Hana Kim;Jinho Park,True,https://openreview.net/pdf?id=3g5KdPD8LIL
41QJ--DLjoD,Landmark-RxR: Solving Vision-and-Language Navigation with Fine-Grained Alignment Supervision,"In Vision-and-Language Navigation (VLN) task, an agent is asked to navigate inside 3D indoor environments following given instructions. Cross-modal alignment is one of the most critical challenges in VLN because the predicted trajectory needs to match the given instruction accurately. In this paper, we address the cross-modal alignment challenge from the perspective of fine-grain. Firstly, to alleviate weak cross-modal alignment supervision from coarse-grained data, we introduce a human-annotated fine-grained VLN dataset, namely Landmark-RxR. Secondly, to further enhance local cross-modal alignment under fine-grained supervision, we investigate the focal-oriented rewards with soft and hard forms, by focusing on the critical points sampled from fine-grained Landmark-RxR. Moreover, to fully evaluate the navigation process, we also propose a re-initialization mechanism that makes metrics insensitive to difficult points, which can cause the agent to deviate from the correct trajectories. Experimental results show that our agent has superior navigation performance on Landmark-RxR, en-RxR and R2R. Our dataset and code are available at https://github.com/hekj/Landmark-RxR.",main,NeurIPS,2021,Poster,Keji He;Yan Huang;Qi Wu;Jianhua Yang;Dong An;Shuanglin Sima;Liang Wang,True,https://openreview.net/pdf?id=41QJ--DLjoD
43mYF598ZDB,The CLEAR Benchmark: Continual LEArning on Real-World Imagery,"Continual learning (CL) is widely regarded as crucial challenge for lifelong AI. However, existing CL benchmarks, e.g. Permuted-MNIST and Split-CIFAR, make use of artificial temporal variation and do not align with or generalize to the real- world. In this paper, we introduce CLEAR, the first continual image classification benchmark dataset with a natural temporal evolution of visual concepts in the real world that spans a decade (2004-2014). We build CLEAR from existing large-scale image collections (YFCC100M) through a novel and scalable low-cost approach to visio-linguistic dataset curation. Our pipeline makes use of pretrained vision-language models (e.g. CLIP) to interactively build labeled datasets, which are further validated with crowd-sourcing to remove errors and even inappropriate images (hidden in original YFCC100M). The major strength of CLEAR over prior CL benchmarks is the smooth temporal evolution of visual concepts with real-world imagery, including both high-quality labeled data along with abundant unlabeled samples per time period for continual semi-supervised learning. We find that a simple unsupervised pre-training step can already boost state-of-the-art CL algorithms that only utilize fully-supervised data. Our analysis also reveals that mainstream CL evaluation protocols that train and test on iid data artificially inflate performance of CL system. To address this, we propose novel ""streaming"" protocols for CL that always test on the (near) future. Interestingly, streaming protocols (a) can simplify dataset curation since today’s testset can be repurposed for tomorrow’s trainset and (b) can produce more generalizable models with more accurate estimates of performance since all labeled data from each time-period is used for both training and testing (unlike classic iid train-test splits).",Datasets & Benchmarks,NeurIPS,2021,Poster,Zhiqiu Lin;Jia Shi;Deepak Pathak;Deva Ramanan,True,https://openreview.net/pdf?id=43mYF598ZDB
48CBzhshpcS,iShape: A First Step Towards Irregular Shape Instance Segmentation,"In this paper, we introduce a brand new dataset to promote the study of instance segmentation for objects with irregular shapes. Our key observation is that though irregularly shaped objects widely exist in daily life and industrial scenarios, they received little attention in the instance segmentation field due to the lack of corresponding datasets. To fill this gap, we propose iShape, an irregular shape dataset for instance segmentation. Unlike most existing instance segmentation datasets of regular objects, iShape has many characteristics that challenge existing instance segmentation algorithms, such as large overlaps between bounding boxes of instances, extreme aspect ratios, and large numbers of connected components per instance. We benchmark popular instance segmentation methods on iShape and find their performance drop dramatically. Hence, we propose an affinity-based instance segmentation algorithm, called ASIS, as a stronger baseline. ASIS explicitly combines perception and reasoning to solve Arbitrary Shape Instance Segmentation including irregular objects. Experimental results show that ASIS outperforms the state-of-the-art on iShape.",Datasets & Benchmarks,NeurIPS,2021,Reject,Lei Yang;Yan Zi Wei;Wei Sun;Yisheng HE;Zhenhang Huang;Haibin Huang;Haoqiang Fan,True,https://openreview.net/pdf?id=48CBzhshpcS
4Y3vzl_NEyG,CBLUE: A Chinese Biomedical Language Understanding Evaluation Benchmark,"Artificial intelligence (AI), along with the recent progress in biomedical language understanding, is gradually offering great promise for medical practice. With the development of biomedical language understanding benchmarks, AI applications are widely used in the medical field. However, most such benchmarks are limited to English, which has made it challenging to replicate many of the successes in English for other languages. To facilitate research in this direction, we collect real-world biomedical data and present the first Chinese Biomedical Language Understanding Evaluation (CBLUE) benchmark: a collection of natural language understanding tasks including named entity recognition, information extraction, single-sentence/sentence-pair classification, and an associated online platform for model evaluation, comparison, and analysis. To establish evaluation on these tasks, we report empirical results with the current 11  pre-trained Chinese models, and results show that state-of-the-art neural models perform by far worse than the human ceiling. Our benchmark is released at https://tianchi.aliyun.com/dataset/dataDetail?dataId=95414&lang=en-us.

",Datasets & Benchmarks,NeurIPS,2021,Reject,Mosha Chen;Bi Zhen;Xiaozhuan Liang;Lei Li;Ningyu Zhang;Xin Shang;Kangping Yin;Chuanqi Tan;Jian Xu;Fei Huang;Luo Si;Yuan Ni;Guotong Xie;Zhifang Sui;Baobao Chang;Hui Zong;Zheng Yuan;Linfeng Li;Jun Yan;Hongying Zan;Kunli Zhang;Buzhou Tang;Qingcai Chen,True,https://openreview.net/pdf?id=4Y3vzl_NEyG
4lvu1B4Y0E,ComSum: Commit Messages Summarization and Meaning Preservation,"We present ComSum, a data set of 7 million commit messages for text summarization. When documenting commits, software code changes, both a message and its summary are posted. We gather and filter those to curate developers' work summarization data set.
Along with its growing size, practicality and challenging language domain, the data set benefits from the living field of empirical software engineering. 
As commits follow a typology, we propose to not only evaluate outputs by Rouge, but by their meaning preservation.",Datasets & Benchmarks,NeurIPS,2021,Reject,Leshem Choshen;Idan Amit,True,https://openreview.net/pdf?id=4lvu1B4Y0E
4pf_pOo0Dt,Self-Supervised Learning with Data Augmentations Provably Isolates Content from Style,"Self-supervised representation learning has shown remarkable success in a number of domains. A common practice is to perform data augmentation via hand-crafted transformations intended to leave the semantics of the data invariant. We seek to understand the empirical success of this approach from a theoretical perspective. We formulate the augmentation process as a latent variable model by postulating a partition of the latent representation into a content component, which is assumed invariant to augmentation, and a style component, which is allowed to change. Unlike prior work on disentanglement and independent component analysis, we allow for both nontrivial statistical and causal dependencies in the latent space. We study the identifiability of the latent representation based on pairs of views of the observations and prove sufficient conditions that allow us to identify the invariant content partition up to an invertible mapping in both generative and discriminative settings. We find numerical simulations with dependent latent variables are consistent with our theory. Lastly, we introduce Causal3DIdent, a dataset of high-dimensional, visually complex images with rich causal dependencies, which we use to study the effect of data augmentations performed in practice.",main,NeurIPS,2021,Poster,Julius Von Kügelgen;Yash Sharma;Luigi Gresele;Wieland Brendel;Bernhard Schölkopf;Michel Besserve;Francesco Locatello,True,https://openreview.net/pdf?id=4pf_pOo0Dt
5HR3vCylqD,SustainBench: Benchmarks for Monitoring the Sustainable Development Goals with Machine Learning,"Progress toward the United Nations Sustainable Development Goals (SDGs) has been hindered by a lack of data on key environmental and socioeconomic indicators, which historically have come from ground surveys with sparse temporal and spatial coverage. Recent advances in machine learning have made it possible to utilize abundant, frequently-updated, and globally available data, such as from satellites or social media, to provide insights into progress toward SDGs. Despite promising early results, approaches to using such data for SDG measurement thus far have largely evaluated on different datasets or used inconsistent evaluation metrics, making it hard to understand whether performance is improving and where additional research would be most fruitful. Furthermore, processing satellite and ground survey data requires domain knowledge that many in the machine learning community lack. In this paper, we introduce SustainBench, a collection of 15 benchmark tasks across 7 SDGs, including tasks related to economic development, agriculture, health, education, water and sanitation, climate action, and life on land. Datasets for 11 of the 15 tasks are released publicly for the first time. Our goals for SustainBench are to (1) lower the barriers to entry for the machine learning community to contribute to measuring and achieving the SDGs; (2) provide standard benchmarks for evaluating machine learning models on tasks across a variety of SDGs; and (3) encourage the development of novel machine learning methods where improved model performance facilitates progress towards the SDGs.",Datasets & Benchmarks,NeurIPS,2021,Poster,Christopher Yeh;Chenlin Meng;Sherrie Wang;Anne Driscoll;Erik Rozi;Patrick Liu;Jihyeon Lee;Marshall Burke;David B. Lobell;Stefano Ermon,True,https://openreview.net/pdf?id=5HR3vCylqD
5Str2l1vmr-,The Benchmark Lottery,"The world of empirical machine learning (ML) strongly relies on benchmarks in order to determine the relative effectiveness of different algorithms and methods. This paper proposes the notion of a benchmark lottery that describes the overall fragility of the ML benchmarking process. The benchmark lottery postulates that many factors, other than fundamental algorithmic superiority, may lead to a method being perceived as superior. On multiple benchmark setups that are prevalent in the ML community, we show that the relative performance of algorithms may be altered significantly simply by choosing different benchmark tasks, highlighting the fragility of the current paradigms and potential fallacious interpretation derived from benchmarking ML methods. Given that every benchmark makes a statement about what it perceives to be important, we argue that this might lead to biased progress in the community. We discuss implications of the observed phenomena and provide recommendations on mitigating them using multiple machine learning domains and communities as use cases, including natural language processing, computer vision, information retrieval, recommender systems and reinforcement learning.",Datasets & Benchmarks,NeurIPS,2021,Reject,Mostafa Dehghani;Yi Tay;Alexey A. Gritsenko;Zhe Zhao;Neil Houlsby;Fernando Diaz;Donald Metzler;Oriol Vinyals,False,https://openreview.net/pdf?id=5Str2l1vmr-
5qsptDcsdEj,Continual World: A Robotic Benchmark For Continual Reinforcement Learning,"  Continual learning (CL) --- the ability to continuously learn, building on previously acquired knowledge --- is a natural requirement for long-lived autonomous reinforcement learning (RL) agents. While building such agents, one needs to balance opposing desiderata, such as constraints on capacity and compute, the ability to not catastrophically forget, and to exhibit positive transfer on new tasks. Understanding the right trade-off is conceptually and computationally challenging, which we argue has led the community to overly focus on catastrophic forgetting.  In response to these issues, we advocate for the need to prioritize forward transfer and propose Continual World, a benchmark consisting of realistic and meaningfully diverse robotic tasks built on top of Meta-World  as a testbed. Following an in-depth empirical evaluation of existing CL methods, we pinpoint their limitations and highlight unique algorithmic challenges in the RL setting. Our benchmark aims to provide a meaningful and computationally inexpensive challenge for the community and thus help better understand the performance of existing and future solutions. Information about the benchmark, including the open-source code, is available at https://sites.google.com/view/continualworld.",main,NeurIPS,2021,Poster,Maciej Wolczyk;Michał Zając;Razvan Pascanu;Łukasz Kuciński;Piotr Miłoś,True,https://openreview.net/pdf?id=5qsptDcsdEj
6fmgB38rLI1,Multimodal and Multilingual Embeddings for Large-Scale Speech Mining,"We present an approach to encode a speech signal into a fixed-size representation which minimizes the cosine loss with the existing massively multilingual LASER text embedding space. Sentences are close in this embedding space, independently of their language and modality, either text or audio. Using a similarity metric in that multimodal embedding space, we perform mining of audio in German, French, Spanish and English from Librivox against billions of sentences from Common Crawl. This yielded more than twenty thousand hours of aligned speech translations.  To evaluate the automatically mined speech/text corpora, we train neural speech translation systems for several languages pairs. Adding the mined data, achieves significant improvements in the BLEU score on the CoVoST2 and the MUST-C test sets with respect to a very competitive baseline. Our approach can also be used to directly perform speech-to-speech mining, without the need to first transcribe or translate the data. We obtain more than one thousand three hundred hours of aligned speech in French, German, Spanish and English. This speech corpus has the potential to boost research in speech-to-speech translation which suffers from scarcity of natural end-to-end training data. All the mined multimodal corpora will be made freely available.",main,NeurIPS,2021,Spotlight,Paul-Ambroise Duquenne;Hongyu Gong;Holger Schwenk,True,https://openreview.net/pdf?id=6fmgB38rLI1
6lE4dQXaUcb,CodeXGLUE: A Machine Learning Benchmark Dataset for Code Understanding and Generation,"Benchmark datasets have a significant impact on accelerating research in programming language tasks. In this paper, we introduce CodeXGLUE, a benchmark dataset to foster machine learning research for program understanding and generation. CodeXGLUE includes a collection of 10 tasks across 14 datasets and a platform for model evaluation and comparison. CodeXGLUE also features three baseline systems, including the BERT-style, GPT-style, and Encoder-Decoder models, to make it easy for researchers to use the platform. The availability of such data and baselines can help the development and validation of new methods that can be applied to various program understanding and generation problems.",Datasets & Benchmarks,NeurIPS,2021,Poster,Shuai Lu;Daya Guo;Shuo Ren;Junjie Huang;Alexey Svyatkovskiy;Ambrosio Blanco;Colin Clement;Dawn Drain;Daxin Jiang;Duyu Tang;Ge Li;Lidong Zhou;Linjun Shou;Long Zhou;Michele Tufano;MING GONG;Ming Zhou;Nan Duan;Neel Sundaresan;Shao Kun Deng;Shengyu Fu;Shujie LIU,True,https://openreview.net/pdf?id=6lE4dQXaUcb
6lYwZ6yknRw,Towards a universal dataset and metrics for training and evaluating table extraction models,"Recently, interest has grown in applying machine learning approaches to the problem of table structure inference and extraction from unstructured documents. However, progress in this area has been challenging not only to make but to measure, due to several issues that arise in both training and evaluating such systems from labeled data. This includes challenges as fundamental as the lack of a single definitive ground truth output for a given input sample and the lack of an ideal metric for measuring partial correctness for this task. To address these we propose a new dataset, PubMed Tables One Million (PubTables1M), and a new class of metric, grid table similarity (GriTS). PubTables1M is nearly twice as large as the current largest comparable dataset, can be used for models across multiple architectures and modalities, and addresses issues such as ambiguity and lack of consistency in the annotations. We apply DETR to table extraction for the first time and show that object detection models trained on images and bounding boxes derived from this data produce excellent results out-of-the-box for all three tasks of detection, structure recognition, and functional analysis. In addition to releasing the data, we describe the dataset creation process in detail to enable others to build on our work and to ensure forward and backward compatibility of this data for combining it with other datasets created for these tasks. It is our hope that this data and the proposed metrics can further progress in this area by serving as a single source of data for training and evaluation of a wide variety of models for table extraction.",Datasets & Benchmarks,NeurIPS,2021,Reject,Brandon Smock;Rohith Pesala;Robin Abraham,True,https://openreview.net/pdf?id=6lYwZ6yknRw
6nblryHxVbO,WildfireDB: An Open-Source Dataset Connecting Wildfire Occurrence with Relevant Determinants,"Modeling fire spread is critical in fire risk management. Creating data-driven models to forecast spread remains challenging due to the lack of comprehensive data sources that relate fires with relevant covariates. We present the first comprehensive and open-source dataset that relates historical fire data with relevant covariates such as weather, vegetation, and topography. Our dataset, named WildfireDB, contains over 17 million data points that capture how fires spread in continental USA in the last decade. In this paper, we describe the algorithmic approach used to process and integrate the data, describe the dataset, and present benchmark results regarding data-driven models that can be learned to forecast the spread of wildfires.",Datasets & Benchmarks,NeurIPS,2021,Poster,Samriddhi Singla;Ayan Mukhopadhyay;Michael Wilbur;Tina Diao;Vinayak Gajjewar;Ahmed Eldawy;Mykel Kochenderfer;Ross D Shachter;Abhishek Dubey,True,https://openreview.net/pdf?id=6nblryHxVbO
6tGP5Z-QbMb,An Uncertainty Principle is a Price of Privacy-Preserving Microdata,"Privacy-protected microdata are often the desired output of a differentially private algorithm since  microdata is familiar and convenient for downstream users. However, there is a statistical price for this kind of convenience. We show that an uncertainty principle governs the trade-off between accuracy for a population of interest (``sum query'') vs. accuracy for its component sub-populations (``point queries''). Compared to differentially private query answering systems that are not required to produce microdata, accuracy can degrade by a logarithmic factor. For example, in the case of pure differential privacy, without the microdata requirement, one can provide noisy answers to the sum query and all point queries while guaranteeing that each answer has squared error $O(1/\\\\epsilon^2)$. With the microdata requirement, one must choose between allowing an additional $\\\\log^2(d)$ factor ($d$ is the number of point queries) for some point queries or allowing an extra $O(d^2)$ factor for the sum query. We present lower bounds for pure, approximate, and concentrated differential privacy. We propose mitigation strategies and create a collection of benchmark datasets that can be used for public study of this problem.
",main,NeurIPS,2021,Poster,John M. Abowd;Robert Ashmead;Ryan Cumings-Menon;Simson L. Garfinkel;Daniel Kifer;Philip Leclerc;William Sexton;Ashley E Simpson;Christine Task;Pavel Zhuravlev,True,https://openreview.net/pdf?id=6tGP5Z-QbMb
6vWuYzkp8d,Discovering and Achieving Goals via World Models,"How can artificial agents learn to solve many diverse tasks in complex visual environments without any supervision? We decompose this question into two challenges: discovering new goals and learning to reliably achieve them. Our proposed agent, Latent Explorer Achiever (LEXA), addresses both challenges by learning a world model from image inputs and using it to train an explorer and an achiever policy via imagined rollouts. Unlike prior methods that explore by reaching previously visited states, the explorer plans to discover unseen surprising states through foresight, which are then used as diverse targets for the achiever to practice. After the unsupervised phase, LEXA solves tasks specified as goal images zero-shot without any additional learning. LEXA substantially outperforms previous approaches to unsupervised goal reaching, both on prior benchmarks and on a new challenging benchmark with 40 test tasks spanning across four robotic manipulation and locomotion domains. LEXA further achieves goals that require interacting with multiple objects in sequence. Project page: https://orybkin.github.io/lexa/",main,NeurIPS,2021,Poster,Russell Mendonca;Oleh Rybkin;Kostas Daniilidis;Danijar Hafner;Deepak Pathak,True,https://openreview.net/pdf?id=6vWuYzkp8d
6vZVBkCDrHT,CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks,"Over the last several decades, software has been woven into the fabric of every aspect of our society. As software development surges and code infrastructure of enterprise applications ages, it is now more critical than ever to increase software development productivity and modernize legacy applications. Advances in deep learning and machine learning algorithms have enabled breakthroughs in computer vision, speech recognition, natural language processing and beyond, motivating researchers to leverage AI techniques to improve software development efficiency. Thus, the fast-emerging research area of “AI for Code” has garnered new interest and gathered momentum. In this paper, we present a large-scale dataset \\\\textit{CodeNet}, consisting of over 14 million code samples and about 500 million lines of code in 55 different programming languages, which is aimed at teaching AI to code. In addition to its large scale, CodeNet has a rich set of high-quality annotations to benchmark and help accelerate research in AI techniques for a variety of critical coding tasks, including code similarity and classification, code translation between a large variety of programming languages, and code performance (runtime and memory) improvement techniques. Additionally, CodeNet provides sample input and output test sets for 98.5\\\\% of the code samples, which can be used as an oracle for determining code correctness and potentially guide reinforcement learning for code quality improvements. As a usability feature, we provide several pre-processing tools in CodeNet to transform source code into representations that can be readily used as inputs into machine learning models. Results of code classification and code similarity experiments using the CodeNet dataset are provided as a reference. We hope that the scale, diversity and rich, high-quality annotations of CodeNet will offer unprecedented research opportunities at the intersection of AI and Software Engineering.",Datasets & Benchmarks,NeurIPS,2021,Poster,Ruchir Puri;David S Kung;Geert Janssen;Wei Zhang;Giacomo Domeniconi;Vladimir Zolotov;Julian Dolby;Jie Chen;Mihir Choudhury;Lindsey Decker;Veronika Thost;Luca Buratti;Saurabh Pujar;Shyam Ramji;Ulrich Finkler;Susan Malaika;Frederick Reiss,True,https://openreview.net/pdf?id=6vZVBkCDrHT
701FtuyLlAd,FS-Mol: A Few-Shot Learning Dataset of Molecules,"Small datasets are ubiquitous in drug discovery as data generation is expensive and can be restricted for ethical reasons (e.g. in vivo experiments). A widely applied technique in early drug discovery to identify novel active molecules against a protein target is modelling quantitative structure-activity relationships (QSAR). It is known to be extremely challenging, as available measurements of compound activities range in the low dozens or hundreds. However, many such related datasets exist, each with a small number of datapoints, opening up the opportunity for few-shot learning after pre-training on a substantially larger corpus of data. At the same time, many few-shot learning methods are currently evaluated in the computer-vision domain. We propose that expansion into a new application, as well as the possibility to use explicitly graph-structured data, will drive exciting progress in few-shot learning. Here, we provide a few-shot learning dataset (FS-Mol) and complementary benchmarking procedure. We define a set of tasks on which few-shot learning methods can be evaluated, with a separate set of tasks for use in pre-training. In addition, we implement and evaluate a number of existing single-task, multi-task, and meta-learning approaches as baselines for the community. We hope that our dataset, support code release, and baselines will encourage future work on this extremely challenging new domain for few-shot learning.",Datasets & Benchmarks,NeurIPS,2021,Poster,Megan Stanley;John F Bronskill;Krzysztof Maziarz;Hubert Misztela;Jessica Lanini;Marwin Segler;Nadine Schneider;Marc Brockschmidt,True,https://openreview.net/pdf?id=701FtuyLlAd
70XgWMiBeH7,New Zealand Open Environmental Science Data sets,"Data Science on environmental spatio-temporal data is becoming a critical and challenging research topic due to the changing nature and rapidly increasing volume of available data. To this end, we would like to introduce TAIAO data repository, comprising of over 30 datasets of various types including images, videos, textual and tabular data. ",Datasets & Benchmarks,NeurIPS,2021,Reject,Nick Jin Sean Lim;Jacob Montiel;Albert Bifet,True,https://openreview.net/pdf?id=70XgWMiBeH7
73OmmrCfSyy,Mind the Gap: Assessing Temporal Generalization in Neural Language Models,"Our world is open-ended, non-stationary, and constantly evolving; thus what we talk about and how we talk about it change over time. This inherent dynamic nature of language contrasts with the current static language modelling paradigm, which trains and evaluates models on utterances from overlapping time periods. Despite impressive recent progress, we demonstrate that Transformer-XL language models perform worse in the realistic setup of predicting future utterances from beyond their training period, and that model performance becomes increasingly worse with time. We find that, while increasing model size alone—a key driver behind recent progress—does not solve this problem, having models that continually update their knowledge with new information can indeed mitigate this performance degradation over time. Hence, given the compilation of ever-larger language modelling datasets, combined with the growing list of language-model-based NLP applications that require up-to-date factual knowledge about the world, we argue that now is the right time to rethink the static way in which we currently train and evaluate our language models, and develop adaptive language models that can remain up-to-date with respect to our ever-changing and non-stationary world. We publicly release our dynamic, streaming language modelling benchmarks for WMT and arXiv to facilitate language model evaluation that takes temporal dynamics into account.",main,NeurIPS,2021,Spotlight,Angeliki Lazaridou;Adhiguna Kuncoro;Elena Gribovskaya;Devang Agrawal;Adam Liska;Tayfun Terzi;Mai Gimenez;Cyprien de Masson d'Autume;Tomáš Kočiský;Sebastian Ruder;Dani Yogatama;Kris Cao;Susannah Young;Phil Blunsom,True,https://openreview.net/pdf?id=73OmmrCfSyy
74TZg9gsO8W,WaveFake: A Data Set to Facilitate Audio Deepfake Detection,"Deep generative modeling has the potential to cause significant harm to society. Recognizing this threat, a magnitude of research into detecting so-called ""Deepfakes'' has emerged. This research most often focuses on the image domain, while studies exploring generated audio signals have---so-far---been neglected. In this paper we make three key contributions to narrow this gap. First, we provide researchers with an introduction to common signal processing techniques used for analyzing audio signals. Second, we present a novel data set, for which we collected nine sample sets from five different network architectures, spanning two languages. Finally, we supply practitioners with two baseline models, adopted from the signal processing community, to facilitate further research in this area.",Datasets & Benchmarks,NeurIPS,2021,Poster,Joel Frank;Lea Schönherr,True,https://openreview.net/pdf?id=74TZg9gsO8W
79shW3z5Eaq,Datasets for Online Controlled Experiments,"Online Controlled Experiments (OCE) are the gold standard to measure impact and guide decisions for digital products and services. Despite many methodological advances in this area, the scarcity of public datasets and the lack of a systematic review and categorization hinder its development. We present the first survey and taxonomy for OCE datasets, which highlight the lack of a public dataset to support the design and running of experiments with adaptive stopping, an increasingly popular approach to enable quickly deploying improvements or rolling back degrading changes. We release the first such dataset, containing daily checkpoints of decision metrics from multiple, real experiments run on a global e-commerce platform. The dataset design is guided by a broader discussion on data requirements for common statistical tests used in digital experimentation. We demonstrate how to use the dataset in the adaptive stopping scenario using sequential and Bayesian hypothesis tests and learn the relevant parameters for each approach. ",Datasets & Benchmarks,NeurIPS,2021,Poster,C. H. Bryan Liu;Ângelo Cardoso;Paul Couturier;Emma J. McCoy,True,https://openreview.net/pdf?id=79shW3z5Eaq
7Bywt2mQsCe,Measuring Mathematical Problem Solving With the MATH Dataset,"Many intellectual endeavors require mathematical problem solving, but this skill remains beyond the capabilities of computers. To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-by-step solution which can be used to teach models to generate answer derivations and explanations. To facilitate future research and increase accuracy on MATH, we also contribute a large auxiliary pretraining dataset which helps teach models the fundamentals of mathematics. Even though we are able to increase accuracy on MATH, our results show that accuracy remains relatively low, even with enormous Transformer models. Moreover, we find that simply increasing budgets and model parameter counts will be impractical for achieving strong mathematical reasoning if scaling trends continue. While scaling Transformers is automatically solving most other text-based tasks, scaling is not currently solving MATH. To have more traction on mathematical problem solving we will likely need new algorithmic advancements from the broader research community.",Datasets & Benchmarks,NeurIPS,2021,Poster,Dan Hendrycks;Collin Burns;Saurav Kadavath;Akul Arora;Steven Basart;Eric Tang;Dawn Song;Jacob Steinhardt,True,https://openreview.net/pdf?id=7Bywt2mQsCe
7FHnnENUG0,Modeling Worlds in Text,"We provide a dataset that enables the creation of learning agents that can build knowledge graph-based world models of interactive narratives. Interactive narratives---or text-adventure games---are partially observable environments structured as long puzzles or quests in which an agent perceives and interacts with the world purely through textual natural language. Each individual game typically contains hundreds of locations, characters, and objects---each with their own unique descriptions---providing an opportunity to study the problem of giving language-based agents the structured memory necessary to operate in such worlds. Our dataset provides 24198 mappings between rich natural language observations and: (1) knowledge graphs that reflect the world state in the form of a map; (2) natural language actions that are guaranteed to cause a change in that particular world state. The training data is collected across 27 games in multiple genres and contains a further 7836 heldout instances over 9 additional games in the test set. We further provide baseline models using rules-based, question-answering, and sequence learning approaches in addition to an analysis of the data and corresponding learning tasks.",Datasets & Benchmarks,NeurIPS,2021,Poster,Prithviraj Ammanabrolu;Mark Riedl,True,https://openreview.net/pdf?id=7FHnnENUG0
7hQLXPnfrqk,Arena: A Scalable and Configurable Benchmark for Policy Learning,"We believe current benchmarks for policy learning lack two important properties: scalability and configurability. The growing literature on modeling policies as graph neural networks calls for an object-based benchmark where the number of objects can be arbitrarily scaled and the mechanics can be freely configured. We introduce the Arena benchmark, a scalable and configurable benchmark for policy learning. Arena provides an object-based game-like environment where the number of objects can be arbitrarily scaled and the mechanics can be configured with a large degree of freedom. In this way, arena is designed to be an all-in-one environment that uses scaling and configuration to smoothly interpolates multiple dimensions of decision making that require different degrees of inductive bias.",Datasets & Benchmarks,NeurIPS,2021,Reject,Sirui Xu;Shuang Liu;Tongzhou Mu;Zhiwei Jia;Yiran Wu;Hao Su,False,https://openreview.net/pdf?id=7hQLXPnfrqk
7l1Ygs3Bamw,CUAD: An Expert-Annotated NLP Dataset for Legal Contract Review,"Many specialized domains remain untouched by deep learning, as large labeled datasets require expensive expert annotators. We address this bottleneck within the legal domain by introducing the Contract Understanding Atticus Dataset (CUAD), a new dataset for legal contract review. CUAD was created with dozens of legal experts from The Atticus Project and consists of over 13,000 annotations. The task is to highlight salient portions of a contract that are important for a human to review. We find that Transformer models have nascent performance, but that this performance is strongly influenced by model design and training dataset size. Despite these promising results, there is still substantial room for improvement. As one of the only large, specialized NLP benchmarks annotated by experts, CUAD can serve as a challenging research benchmark for the broader NLP community.",Datasets & Benchmarks,NeurIPS,2021,Poster,Dan Hendrycks;Collin Burns;Anya Chen;Spencer Ball,True,https://openreview.net/pdf?id=7l1Ygs3Bamw
8HkbjXqbAnz,Geon3D: Benchmarking 3D Shape Bias towards Building Robust Machine Vision,"Human vision, unlike existing machine vision systems, is surprisingly robust to environmental variation, including both naturally occuring disturbances (e.g., fog, snow, occlusion) and artificial corruptions (e.g., adversarial examples). Such robustness, at least in part, arises from our ability to infer 3D geometry from 2D retinal projections---the ability to go from images to their underlying causes, including the 3D scene. How can we design machine learning systems with such strong shape bias? In this work, we view 3D reconstruction as a pretraining method for building more robust vision systems. Recent studies explore the role of shape bias in the robustness of vision models. However, most current approaches to increase shape bias based on ImageNet take an indirect approach, attempting to instead reduce texture bias via structured data augmentation. These approaches do not directly nor fully exploit the relationship between 2D features and their underlying 3D shapes. To fill this gap, we introduce a novel dataset called Geon3D, which is derived from objects that emphasize variation across shape features that the human visual system is thought to be particularly sensitive. This dataset enables, for the first time, a controlled setting where we can isolate the effect of ``3D shape bias'' in robustifying neural networks, and informs more direct approaches to increase shape bias by exploiting 3D vision tasks. Using Geon3D, we find that CNNs pretrained on 3D reconstruction are more resilient to viewpoint change, rotation, and shift than regular CNNs. Further, when combined with adversarial training, 3D reconstruction pretrained models improve adversarial and common corruption robustness over vanilla adversarially-trained models. This suggests that incorporating 3D shape bias is a promising direction for building robust machine vision systems.",Datasets & Benchmarks,NeurIPS,2021,Reject,Yutaro Yamada;Yuval Kluger;Sahand Negahban;Ilker Yildirim,True,https://openreview.net/pdf?id=8HkbjXqbAnz
8RxxwAut1BI,MLPerf Tiny Benchmark,"Advancements in ultra-low-power tiny machine learning (TinyML) systems promise to unlock an entirely new class of smart applications. However, continued progress is limited by the lack of a widely accepted and easily reproducible benchmark for these systems. To meet this need, we present MLPerf Tiny, the first industry-standard benchmark suite for ultra-low-power tiny machine learning systems. The benchmark suite is the collaborative effort of more than 50 organizations from industry and academia and reflects the needs of the community. MLPerf Tiny measures the accuracy, latency, and energy of machine learning inference to properly evaluate the tradeoffs between systems. Additionally, MLPerf Tiny implements a modular design that enables benchmark submitters to show the benefits of their product, regardless of where it falls on the ML deployment stack, in a fair and reproducible manner. The suite features four benchmarks: keyword spotting, visual wake words, image classification, and anomaly detection.",Datasets & Benchmarks,NeurIPS,2021,Poster,Colby Banbury;Vijay Janapa Reddi;Peter Torelli;Nat Jeffries;Csaba Kiraly;Jeremy Holleman;Pietro Montino;David Kanter;Pete Warden;Danilo Pau;Urmish Thakker;antonio torrini;jay cordaro;Giuseppe Di Guglielmo;Javier Duarte;Honson Tran;Nhan Tran;niu wenxu;xu xuesong,True,https://openreview.net/pdf?id=8RxxwAut1BI
8Y50dBbmGU,CSFCube - A Test Collection of Computer Science Research Articles for Faceted Query by Example,"Query by Example is a well-known information retrieval task in which a document is chosen by the user as the search query and the goal is to retrieve relevant documents from a large collection. However, a document often covers multiple aspects of a topic. To address this scenario we introduce the task of faceted Query by Example in which users can also specify a finer grained aspect in addition to the input query document. We focus on the application of this task in scientific literature search. We envision models which are able to retrieve scientific papers analogous to a query scientific paper along specifically chosen rhetorical structure elements as one solution to this problem. In this work, the rhetorical structure elements, which we refer to as facets,  indicate objectives, methods, or results of a scientific paper. We introduce and describe an expert annotated test collection to evaluate models trained to perform this task. Our test collection consists of a diverse set of 50 query documents in English, drawn from computational linguistics and machine learning venues. We carefully follow the annotation guideline used by TREC for depth-k pooling (k = 100 or 250) and the resulting data collection consists of graded relevance scores with high annotation agreement. State of the art models evaluated on our dataset show a significant gap to be closed in further work. Our dataset may be accessed here: https://github.com/iesl/CSFCube",Datasets & Benchmarks,NeurIPS,2021,Poster,Sheshera Mysore;Tim O'Gorman;Andrew McCallum;Hamed Zamani,True,https://openreview.net/pdf?id=8Y50dBbmGU
8nvgnORnoWr,Therapeutics Data Commons: Machine Learning Datasets and Tasks for Drug Discovery and Development,"Therapeutics machine learning is an emerging field with incredible opportunities for innovation and impact. However, advancement in this field requires the formulation of meaningful tasks and careful curation of datasets. Here, we introduce Therapeutics Data Commons (TDC), the first unifying platform to systematically access and evaluate machine learning across the entire range of therapeutics. To date, TDC includes 66 AI-ready datasets spread across 22 learning tasks and spanning the discovery and development of safe and effective medicines. TDC also provides an ecosystem of tools and community resources, including 33 data functions and diverse types of  data splits, 23 strategies for systematic model evaluation, 17 molecule generation oracles, and 29 public leaderboards. All resources are integrated and accessible via an open Python library. We carry out extensive experiments on selected datasets, demonstrating that even the strongest algorithms fall short of solving key therapeutics challenges, including distributional shifts, multi-scale and multi-modal learning, and robust generalization to novel data points. We envision that TDC can facilitate algorithmic advances and considerably accelerate machine-learning model development, validation and transition into biomedical and clinical implementation. TDC is available at https://tdcommons.ai. ",Datasets & Benchmarks,NeurIPS,2021,Poster,Kexin Huang;Tianfan Fu;Wenhao Gao;Yue Zhao;Yusuf H Roohani;Jure Leskovec;Connor W. Coley;Cao Xiao;Jimeng Sun;Marinka Zitnik,True,https://openreview.net/pdf?id=8nvgnORnoWr
9-LSfSU74n-,A Dataset for Answering Time-Sensitive Questions,"Time is an important dimension in our physical world. Lots of facts can evolve with respect to time. For example, the U.S. President might change every four years. Therefore, it is important to consider the time dimension and empower the existing QA models to reason over time. However, the existing QA datasets contain rather few time-sensitive questions, hence not suitable for diagnosing or benchmarking the model's temporal reasoning capability. In order to promote research in this direction, we propose to construct a time-sensitive QA dataset. The dataset is constructed by 1) mining time-evolving facts from WikiData and aligning them to their corresponding Wikipedia page, 2) employing crowd workers to verify and calibrate these noisy facts, 3) generating question-answer pairs based on the annotated time-sensitive facts. Our dataset poses challenges in the aspect of both temporal understanding and temporal reasoning. We evaluate different SoTA long-document QA systems like BigBird and FiD on our dataset. The best-performing model FiD can only achieve 46\\\\% accuracy, still far behind the human performance of 87\\\\%. We demonstrate that these models are still lacking the ability to perform consistent temporal reasoning. Therefore, we believe that our dataset could serve as a benchmark to develop NLP models more sensitive to temporal shifts.",Datasets & Benchmarks,NeurIPS,2021,Poster,Wenhu Chen;Xinyi Wang;William Yang Wang,True,https://openreview.net/pdf?id=9-LSfSU74n-
96ULbah4DC,Hierarchical Reinforcement Learning with Timed Subgoals,"Hierarchical reinforcement learning (HRL) holds great potential for sample-efficient learning on challenging long-horizon tasks. In particular, letting a higher level assign subgoals to a lower level has been shown to enable fast learning on difficult problems. However, such subgoal-based methods have been designed with static reinforcement learning environments in mind and consequently struggle with dynamic elements beyond the immediate control of the agent even though they are ubiquitous in real-world problems. In this paper, we introduce Hierarchical reinforcement learning with Timed Subgoals (HiTS), an HRL algorithm that enables the agent to adapt its timing to a dynamic environment by not only specifying what goal state is to be reached but also when. We discuss how communicating with a lower level in terms of such timed subgoals results in a more stable learning problem for the higher level. Our experiments on a range of standard benchmarks and three new challenging dynamic reinforcement learning environments show that our method is capable of sample-efficient learning where an existing state-of-the-art subgoal-based HRL method fails to learn stable solutions.",main,NeurIPS,2021,Poster,Nico Gürtler;Dieter Büchler;Georg Martius,True,https://openreview.net/pdf?id=96ULbah4DC
9E3dTIMxL8S,VALUE: A Multi-Task Benchmark for Video-and-Language Understanding Evaluation,"Most existing video-and-language (VidL) research focuses on a single dataset, or multiple datasets of a single task. In reality, a truly useful VidL system is expected to be easily generalizable to diverse tasks, domains, and datasets. To facilitate the evaluation of such systems, we introduce Video-And-Language Understanding Evaluation (VALUE)  benchmark, an assemblage of 11 VidL datasets over 3 popular tasks: (i) text-to-video retrieval; (ii) video question answering; and (iii) video captioning. VALUE benchmark aims to cover a broad range of video genres, video lengths, data volumes, and task difficulty levels. Rather than focusing on single-channel videos with visual information only, VALUE promotes models that leverage information from both video frames and their associated subtitles, as well as models that share knowledge across multiple tasks. We evaluate various baseline methods with and without large-scale VidL pre-training, and systematically investigate the impact of video input channels, fusion methods, and different video representations. We also study the transferability between tasks, and conduct multi-task learning under different settings. The significant gap between our best model and human performance calls for future study for advanced VidL models. VALUE is available at https://value-benchmark.github.io/. ",Datasets & Benchmarks,NeurIPS,2021,Poster,Linjie Li;Jie Lei;Zhe Gan;Licheng Yu;Yen-Chun Chen;Rohit Pillai;Yu Cheng;Luowei Zhou;Xin Eric Wang;William Yang Wang;Tamara Lee Berg;Mohit Bansal;Jingjing Liu;Lijuan Wang;Zicheng Liu,False,https://openreview.net/pdf?id=9E3dTIMxL8S
9KArJb4r5ZQ,COVID-19 Sounds: A Large-Scale Audio Dataset for Digital Respiratory Screening,"Audio signals are widely recognised as powerful indicators of overall health status, and there has been increasing interest in leveraging sound for affordable COVID-19 screening through machine learning. However, there has also been scepticism regarding the initial efforts, due to perhaps the lack of reproducibility, large datasets and transparency which unfortunately is often an issue with machine learning for health. To facilitate the advancement and openness of audio-based machine learning for respiratory health, we release a dataset consisting of 53,449 audio samples (over 552 hours in total) crowd-sourced from 36,116 participants through our COVID-19 Sounds app. Given its scale, this dataset is comprehensive in terms of demographics and spectrum of health conditions. It also provides participants' self-reported COVID-19 testing status with 2,106 samples tested positive. To the best of our knowledge, COVID-19 Sounds is the largest multi-modal dataset of COVID-19 respiratory sounds: it consists of three modalities including breathing, cough, and voice recordings. Additionally, in this paper, we report on several benchmarks for two principal research tasks: respiratory symptoms prediction and COVID-19 prediction. For these tasks we demonstrate performance with a ROC-AUC of over 0.7, confirming both the promise of machine learning approaches based on these types of datasets as well as the usability of our data for such tasks.  We describe a realistic experimental setting that hopes to pave the way to a fair performance evaluation of future models. In addition, we reflect on how the released dataset can help to scale some existing studies and enable new research directions, which inspire and benefit a wide range of future works.",Datasets & Benchmarks,NeurIPS,2021,Poster,"Tong Xia;Dimitris Spathis;Chlo{\\\\""e} Brown;J Ch;Andreas Grammenos;Jing Han;Apinan Hasthanasombat;Erika Bondareva;Ting Dang;Andres Floto;Pietro Cicuta;Cecilia Mascolo",True,https://openreview.net/pdf?id=9KArJb4r5ZQ
9_eyi9ymJcZ,Multimodal AutoML on Tables with Text Fields,"We consider the design of automated supervised learning systems for data tables that not only contain numeric/categorical columns, but text fields as well. Here we assemble 15 multimodal data tables that each contain some text fields and stem from a real business application. Over this benchmark, we evaluate numerous multimodal AutoML strategies, including standard two-stage approaches where NLP is used to featurize the text such that AutoML for tabular data can then be applied. We identify practically superior strategies based on multimodal adaptations of Transformer networks and stack ensembling of these networks with classical tabular models. Compared with human data science teams, the best fully automated methodology discovered through our benchmark manages to rank 1st place when fit to the raw text/tabular data in two MachineHack prediction competitions and 2nd place (out of 2380 teams) in Kaggle's Mercari Price Suggestion Challenge.",Datasets & Benchmarks,NeurIPS,2021,Reject,Xingjian Shi;Jonas Mueller;Nick Erickson;Mu Li;Alex Smola,True,https://openreview.net/pdf?id=9_eyi9ymJcZ
A_TVp2HtxPS,Physics-Aware Downsampling with Deep Learning for Scalable Flood Modeling,"Background. Floods are the most common natural disaster in the world, affecting the lives of hundreds of millions. Flood forecasting is therefore a vitally important endeavor, typically achieved using physical water flow simulations, which rely on accurate terrain elevation maps. However, such simulations, based on solving partial differential equations, are computationally prohibitive on a large scale. This scalability issue is commonly alleviated using a coarse grid representation of the elevation map, though this representation may distort crucial terrain details, leading to significant inaccuracies in the simulation.\\\\\\\\
Contributions. We train a deep neural network to perform physics-informed downsampling of the terrain map: we optimize the coarse grid representation of the terrain maps, so that the flood prediction will match the fine grid solution. For the learning process to succeed, we configure a dataset specifically for this task. We demonstrate that with this method, it is possible to achieve a significant reduction in computational cost, while maintaining an accurate solution. A reference implementation accompanies the paper as well as documentation and code for dataset reproduction.",main,NeurIPS,2021,Poster,Niv Giladi;Zvika Ben-Haim;Sella Nevo;Yossi Matias;Daniel Soudry,True,https://openreview.net/pdf?id=A_TVp2HtxPS
Ah5CMODl52,Decrypting Cryptic Crosswords: Semantically Complex Wordplay Puzzles as a Target for NLP,"Cryptic crosswords, the dominant crossword variety in the UK, are a promising target for advancing NLP systems that seek to process semantically complex, highly compositional language. Cryptic clues read like fluent natural language but are adversarially composed of two parts: a definition and a wordplay cipher requiring character-level manipulations. Expert humans use creative intelligence to solve cryptics, flexibly combining linguistic, world, and domain knowledge. In this paper, we make two main contributions. First, we present a dataset of cryptic clues as a challenging new benchmark for NLP systems that seek to process compositional language in more creative, human-like ways. After showing that three non-neural approaches and T5, a state-of-the-art neural language model, do not achieve good performance, we make our second main contribution: a novel curriculum approach, in which the model is first fine-tuned on related tasks such as unscrambling words. We also introduce a challenging data split, examine the meta-linguistic capabilities of subword-tokenized models, and investigate model systematicity by perturbing the wordplay part of clues, showing that T5 exhibits behavior partially consistent with human solving strategies. Although our curricular approach considerably improves on the T5 baseline, our best-performing model still fails to generalize to the extent that humans can. Thus, cryptic crosswords remain an unsolved challenge for NLP systems and a potential source of future innovation.",main,NeurIPS,2021,Poster,Joshua Rozner;Christopher Potts;Kyle Mahowald,True,https://openreview.net/pdf?id=Ah5CMODl52
Ayf90B1yESX,The Medkit-Learn(ing) Environment: Medical Decision Modelling through Simulation,"The goal of understanding decision-making behaviours in clinical environments is of paramount importance if we are to bring the strengths of machine learning to ultimately improve patient outcomes. Mainstream development of algorithms is often geared towards optimal performance in tasks that do not necessarily translate well into the medical regime---due to several factors including the lack of public availability of realistic data, the intrinsically offline nature of the problem, as well as the complexity and variety of human behaviours. We therefore present a new benchmarking suite designed specifically for medical sequential decision modelling: the Medkit-Learn(ing) Environment, a publicly available Python package providing simple and easy access to high-fidelity synthetic medical data. While providing a standardised way to compare algorithms in a realistic medical setting, we employ a generating process that disentangles the policy and environment dynamics to allow for a range of customisations, thus enabling systematic evaluation of algorithms’ robustness against specific challenges prevalent in healthcare.",Datasets & Benchmarks,NeurIPS,2021,Poster,Alex Chan;Ioana Bica;Alihan Hüyük;Daniel Jarrett;Mihaela van der Schaar,True,https://openreview.net/pdf?id=Ayf90B1yESX
B-d4pw2JdT,MAIN: A Multi-agent Indoor Navigation Benchmark for Cooperative Learning,"Previous works have proposed many multi-agent reinforcement learning methods to study this problem in diverse multi-agent environments. However, these environments have two limitations, which make them unsuitable for real-world applications: 1) the agent observes clean and formatted data from the environment instead of perceiving the noisy observation by themselves from the first-person perspective; 2) large domain gap between the environment and the real world scenarios. In this paper, we propose a Multi-Agent Indoor Navigation (MAIN) benchmark, where agents navigate to reach goals in a 3D indoor room with realistic observation inputs. In the MAIN environment, each agent observes only a small part of a room via an embodied view. Less information is shared between their observations and the observations have large variance. Therefore, the agents must learn to cooperate with each other in exploration and communication to achieve accurate and efficient navigation. We collect a large-scale and challenging dataset to research on the MAIN benchmark. We examine various multi-agent methods based on current research works on our dataset. However, we find that the performances of current MARL methods does not improve by the increase of the agent amount. We find that communication is the key to addressing this complex real-world cooperative task. By Experimenting on four variants of communication models, we show that the model with recurrent communication mechanism achieves the best performance in solving MAIN. ",Datasets & Benchmarks,NeurIPS,2021,Reject,Fengda Zhu;Siyi Hu;Yi Zhang;Haodong Hong;Yi Zhu;Xiaojun Chang;Xiaodan Liang,True,https://openreview.net/pdf?id=B-d4pw2JdT
BlcUQYxknbX,What Ails One-Shot Image Segmentation: A Data Perspective,"One-shot image segmentation (OSS) methods enable semantic labeling of image pixels without supervised training with an extensive dataset. They require just one example (image, mask) pair per target class. Most neural-network-based methods train on a large subset of dataset classes and are evaluated on a disjoint subset of classes. We posit that the data used for training induces negative biases and affects the accuracy of these methods. Specifically, we present evidence for a \\\\textit{Class Negative Bias} (CNB) arising from treating non-target objects as background during training, and \\\\textit{Salience Bias} (SB), affecting the segmentation accuracy for non-salient target class pixels. We also demonstrate that by eliminating CNB and SB, significant gains can be made over the existing state-of-the-art. Next, we argue that there is a significant disparity between real-world expectations from an OSS method and its accuracy reported on existing benchmarks. To this end, we propose a new evaluation dataset - Tiered One-shot Segmentation (TOSS) - based on the PASCAL $5^i$ and FSS-1000 datasets, and associated metrics for each tier. The dataset enforces uniformity in the measurement of accuracy for existing methods and affords fine-grained insights into the applicability of a method to real applications. The paper includes extensive experiments with the TOSS dataset on several existing OSS methods. The intended impact of this work is to point to biases in training and introduce nuances and uniformity in reporting results for the OSS problem. The evaluation splits of the TOSS dataset and instructions for use are available at \\\\url{https://github.com/fewshotseg/toss}.",Datasets & Benchmarks,NeurIPS,2021,Poster,Mayur Hemani;Abhinav Patel;Tejas Shimpi;Anirudha Ramesh;Balaji Krishnamurthy,True,https://openreview.net/pdf?id=BlcUQYxknbX
BwzYI-KaHdr,PASS: An ImageNet replacement for self-supervised pretraining without humans,"Computer vision has long relied on ImageNet and other large datasets of images sampled from the Internet for pretraining models. However, these datasets have ethical and technical shortcomings, such as containing personal information taken without consent, unclear license usage, biases, and, in some cases, even problematic image content. On the other hand, state-of-the-art pretraining is nowadays obtained with unsupervised methods, meaning that labelled datasets such as ImageNet may not be necessary, or perhaps not even optimal, for model pretraining. We thus propose an unlabelled dataset PASS: Pictures without humAns for Self-Supervision. PASS only contains images with CC-BY license and complete attribution metadata, addressing the copyright issue. Most importantly, it contains no images of people at all, and also avoids other types of images that are problematic for data protection or ethics. We show that PASS can be used for pretraining with methods such as MoCo-v2, SwAV and DINO. In the transfer learning setting, it yields similar downstream performances to ImageNet pretraining even on tasks that involve humans, such as human pose estimation. PASS does not make existing datasets obsolete, as for instance it is insufficient for benchmarking. However, it shows that model pretraining is often possible while using safer data, and it also provides the basis for a more robust evaluation of pretraining methods.",Datasets & Benchmarks,NeurIPS,2021,Poster,Yuki M Asano;Christian Rupprecht;Andrew Zisserman;Andrea Vedaldi,True,https://openreview.net/pdf?id=BwzYI-KaHdr
ByPR_hOE_EY,Identifying and Benchmarking Natural Out-of-Context Prediction Problems,"Deep learning systems frequently fail at out-of-context (OOC) prediction, the problem of making reliable predictions on uncommon or unusual inputs or subgroups of the training distribution. To this end, a number of benchmarks for measuring OOC performance have been recently introduced.  In this work, we introduce a framework unifying the literature on OOC performance measurement, and demonstrate how rich auxiliary information can be leveraged to identify candidate sets of OOC examples in existing datasets.  We present NOOCh: a suite of naturally-occurring ""challenge sets"", and show how varying notions of context can be used to probe specific OOC failure modes. Experimentally, we explore the tradeoffs between various learning approaches on these challenge sets and demonstrate how the choices made in designing OOC benchmarks can yield varying conclusions.",main,NeurIPS,2021,Poster,David Madras;Richard Zemel,True,https://openreview.net/pdf?id=ByPR_hOE_EY
ByiVJWsHVKc,SODA10M: A Large-Scale 2D Self/Semi-Supervised Object Detection Dataset for Autonomous Driving,"Aiming at facilitating a real-world, ever-evolving and scalable autonomous driving system, we present a large-scale dataset for standardizing the evaluation of different self-supervised and semi-supervised approaches by learning from raw data, which is the first and largest dataset to date. Existing autonomous driving systems heavily rely on `perfect' visual perception models (i.e., detection) trained using extensive annotated data to ensure safety. However, it is unrealistic to elaborately label instances of all scenarios and circumstances (i.e., night, extreme weather, cities) when deploying a robust autonomous driving system. Motivated by recent advances of self-supervised and semi-supervised learning, a promising direction is to learn a robust detection model by collaboratively exploiting large-scale unlabeled data and few labeled data. Existing datasets (i.e., BDD100K, Waymo) either provide only a small amount of data or covers limited domains with full annotation, hindering the exploration of large-scale pre-trained models. Here, we release a Large-Scale 2D Self/semi-supervised Object Detection dataset for Autonomous driving, named as SODA10M, containing 10 million unlabeled images and 20K images labeled with 6 representative object categories. To improve diversity, the images are collected within 27833 driving hours under different weather conditions, periods and location scenes of 32 different cities. We provide extensive experiments and deep analyses of existing popular self-supervised and semi-supervised approaches, and some interesting findings in autonomous driving scope. Experiments show that SODA10M can serve as a promising pre-training dataset for different self-supervised learning methods, which gives superior performance when finetuning with different downstream tasks (i.e., detection, semantic/instance segmentation) in autonomous driving domain. This dataset has been used to hold the ICCV2021 SSLAD challenge. More information can refer to https://soda-2d.github.io.",Datasets & Benchmarks,NeurIPS,2021,Poster,Jianhua Han;Xiwen Liang;Hang Xu;Kai Chen;Lanqing HONG;Jiageng Mao;Chaoqiang Ye;Wei Zhang;Zhenguo Li;Xiaodan Liang;Chunjing Xu,True,https://openreview.net/pdf?id=ByiVJWsHVKc
C0GmZH2RnVR,Breaking the Dilemma of Medical Image-to-image Translation,"Supervised Pix2Pix and unsupervised Cycle-consistency are two modes that dominate the field of medical image-to-image translation. However, neither modes are ideal. The Pix2Pix mode has excellent performance. But it requires paired and well pixel-wise aligned images, which may not always be achievable due to respiratory motion or anatomy change between times that paired images are acquired. The Cycle-consistency mode is less stringent with training data and works well on unpaired or misaligned images. But its performance may not be optimal. In order to break the dilemma of the existing modes, we propose a new unsupervised mode called RegGAN for medical image-to-image translation. It is based on the theory of ""loss-correction"". In RegGAN, the misaligned target images are considered as noisy labels and the generator is trained with an additional registration network to fit the misaligned noise distribution adaptively. The goal is to search for the common optimal solution to both image-to-image translation and registration tasks. We incorporated RegGAN into a few state-of-the-art image-to-image translation methods and demonstrated that RegGAN could be easily combined with these methods to improve their performances. Such as a simple CycleGAN in our mode surpasses latest NICEGAN even though using less network parameters. Based on our results, RegGAN outperformed both Pix2Pix on aligned data and Cycle-consistency on misaligned or unpaired data. RegGAN is insensitive to noises which makes it a better choice for a wide range of scenarios, especially for medical image-to-image translation tasks in which well pixel-wise aligned data are not available. Code and dataset are available at https://github.com/Kid-Liet/Reg-GAN.",main,NeurIPS,2021,Spotlight,Lingke Kong;Chenyu Lian;Detian Huang;ZhenJiang Li;Yanle Hu;Qichao Zhou,True,https://openreview.net/pdf?id=C0GmZH2RnVR
CI0T_3l-n1,Do Neural Optimal Transport Solvers Work? A Continuous Wasserstein-2 Benchmark,"Despite the recent popularity of neural network-based solvers for optimal transport (OT), there is no standard quantitative way to evaluate their performance. In this paper, we address this issue for quadratic-cost transport---specifically, computation of the Wasserstein-2 distance, a commonly-used formulation of optimal transport in machine learning. To overcome the challenge of computing ground truth transport maps between continuous measures needed to assess these solvers, we use input-convex neural networks (ICNN) to construct pairs of measures whose ground truth OT maps can be obtained analytically. This strategy yields pairs of continuous benchmark measures in high-dimensional spaces such as spaces of images. We thoroughly evaluate existing optimal transport solvers using these benchmark measures. Even though these solvers perform well in downstream tasks, many do not faithfully recover optimal transport maps. To investigate the cause of this discrepancy, we further test the solvers in a setting of image generation. Our study reveals crucial limitations of existing solvers and shows that increased OT accuracy does not necessarily correlate to better results downstream.",main,NeurIPS,2021,Poster,Alexander Korotin;Lingxiao Li;Aude Genevay;Justin Solomon;Alexander Filippov;Evgeny Burnaev,True,https://openreview.net/pdf?id=CI0T_3l-n1
CSi1eu_2q96,Automatic Construction of Evaluation Suites for Natural Language Generation Datasets,"Machine learning approaches applied to NLP are often evaluated by summarizing their performance in a single number, for example accuracy. Since most test sets are constructed as an i.i.d. sample from the overall data, this approach overly simplifies the complexity of language and encourages overfitting to the head of the data distribution. As such, rare language phenomena or text about underrepresented groups are not equally included in the evaluation. To encourage more in-depth model analyses, researchers have proposed the use of multiple test sets, also called challenge sets, that assess specific capabilities of a model. In this paper, we develop a framework based on this idea which is able to generate controlled perturbations and identify subsets in text-to-scalar, text-to-text, or data-to-text settings. By applying this framework to the GEM generation benchmark, we propose an evaluation suite made of 80 challenge sets, demonstrate the kinds of analyses that it enables and shed light onto the limits of current generation models.",Datasets & Benchmarks,NeurIPS,2021,Poster,Simon Mille;Kaustubh Dhole;Saad Mahamood;Laura Perez-Beltrachini;Varun Gangal;Mihir Kale;Emiel van Miltenburg;Sebastian Gehrmann,True,https://openreview.net/pdf?id=CSi1eu_2q96
CXyZrKPz4CU,Physion: Evaluating Physical Prediction from Vision in Humans and Machines,"While current vision algorithms excel at many challenging tasks, it is unclear how well they understand the physical dynamics of real-world environments. Here we introduce Physion, a dataset and benchmark for rigorously evaluating the ability to predict how physical scenarios will evolve over time. Our dataset features realistic simulations of a wide range of physical phenomena, including rigid and soft- body collisions, stable multi-object configurations, rolling, sliding, and projectile motion, thus providing a more comprehensive challenge than previous bench- marks. We used Physion to benchmark a suite of models varying in their architecture, learning objective, input-output structure, and training data. In parallel, we obtained precise measurements of human prediction behavior on the same set of scenarios, allowing us to directly evaluate how well any model could approximate human behavior. We found that vision algorithms that learn object-centric representations generally outperform those that do not, yet still fall far short of human performance. On the other hand, graph neural networks with direct access to physical state information both perform substantially better and make predictions that are more similar to those made by humans. These results suggest that extracting physical representations of scenes is the main bottleneck to achieving human-level and human-like physical understanding in vision algorithms. We have publicly released all data and code to facilitate the use of Physion to benchmark additional models in a fully reproducible manner, enabling systematic evaluation of progress towards vision algorithms that understand physical environments as robustly as people do.",Datasets & Benchmarks,NeurIPS,2021,Poster,Daniel Bear;Elias Wang;Damian Mrowca;Felix Jedidja Binder;Hsiao-Yu Tung;RT Pramod;Cameron Holdaway;Sirui Tao;Kevin A. Smith;Fan-Yun Sun;Li Fei-Fei;Nancy Kanwisher;Joshua B. Tenenbaum;Daniel LK Yamins;Judith E Fan,True,https://openreview.net/pdf?id=CXyZrKPz4CU
CeByDMy0YTL,Environment Generation for Zero-Shot Compositional Reinforcement Learning,"Many real-world problems are compositional – solving them requires completing interdependent sub-tasks, either in series or in parallel, that can be represented as a dependency graph. Deep reinforcement learning (RL) agents often struggle to learn such complex tasks due to the long time horizons and sparse rewards. To address this problem, we present Compositional Design of Environments (CoDE), which trains a Generator agent to automatically build a series of compositional tasks tailored to the RL agent’s current skill level. This automatic curriculum not only enables the agent to learn more complex tasks than it could have otherwise, but also selects tasks where the agent’s performance is weak, enhancing its robustness and ability to generalize zero-shot to unseen tasks at test-time. We analyze why current environment generation techniques are insufficient for the problem of generating compositional tasks, and propose a new algorithm that addresses these issues. Our results assess learning and generalization across multiple compositional tasks, including the real-world problem of learning to navigate and interact with web pages. We learn to generate environments composed of multiple pages or rooms, and train RL agents capable of completing wide-range of complex tasks in those environments. We contribute two new benchmark frameworks for generating compositional tasks, compositional MiniGrid and gMiniWoB for web navigation. CoDE yields 4x higher success rate than the strongest baseline, and demonstrates strong performance of real websites learned on 3500 primitive tasks.",main,NeurIPS,2021,Poster,Izzeddin Gur;Natasha Jaques;Yingjie Miao;Jongwook Choi;Manoj Tiwari;Honglak Lee;Aleksandra Faust,True,https://openreview.net/pdf?id=CeByDMy0YTL
DAkP1TT_Ubm,ZeroWaste Dataset: Towards Automated Waste Recycling,"Less than 35% of recyclable waste is being actually recycled in the US, which leads to increased soil and sea pollution and is one of the major concerns of environmental researchers as well as the common public. At the heart of the problem is the inefficiencies of the waste sorting process (separating paper, plastic, metal, glass, etc.) due to the extremely complex and cluttered nature of the waste stream. Automated waste detection strategies have a great potential to enable more efficient, reliable and safer waste sorting practices, but the literature lacks comprehensive datasets and methodology for the industrial waste sorting solutions. In this paper, we take a step towards computer-aided waste detection and present the first in-the-wild industrial-grade waste detection and segmentation dataset, ZeroWaste. This dataset contains over 1800 fully segmented video frames collected from a real waste sorting plant along with waste material labels for training and evaluation of the segmentation methods, as well as over 6000 unlabeled frames that can be further used for semi-supervised and self-supervised learning techniques. ZeroWaste also provides frames of the conveyor belt before and after the sorting process, comprising a novel setup that can be used for weakly-supervised segmentation. We present baselines for fully-, semi- and weakly-supervised segmentation methods. Our experimental results demonstrate that state-of-the-art segmentation methods struggle to correctly detect and classify target objects which suggests the challenging nature of our proposed in-the-wild dataset. We believe that ZeroWaste will catalyze research in object detection and semantic segmentation in extreme clutter as well as applications in the recycling domain. Our project page can be found at http://ai.bu.edu/zerowaste/.",Datasets & Benchmarks,NeurIPS,2021,Reject,Dina Bashkirova;Ziliang Zhu;James Akl;Fadi Alladkani;Ping Hu;Vitaly Ablavsky;Berk Calli;Sarah Adel Bargal;Kate Saenko,True,https://openreview.net/pdf?id=DAkP1TT_Ubm
DE8MOQIgFTK,Adversarial Examples Make Strong Poisons,"The adversarial machine learning literature is largely partitioned into evasion attacks on testing data and poisoning attacks on training data.  In this work, we show that adversarial examples, originally intended for attacking pre-trained models, are even more effective for data poisoning than recent methods designed specifically for poisoning. In fact, adversarial examples with labels re-assigned by the crafting network remain effective for training, suggesting that adversarial examples contain useful semantic content, just with the ""wrong"" labels (according to a network, but not a human). Our method, adversarial poisoning, is substantially more effective than existing poisoning methods for secure dataset release, and we release a poisoned version of ImageNet, ImageNet-P, to encourage research into the strength of this form of data obfuscation.
",main,NeurIPS,2021,Poster,Liam H Fowl;Micah Goldblum;Ping-yeh Chiang;Jonas Geiping;Wojciech Czaja;Tom Goldstein,True,https://openreview.net/pdf?id=DE8MOQIgFTK
DPHsCQ8OpA,Habitat 2.0: Training Home Assistants to Rearrange their Habitat,"We introduce Habitat 2.0 (H2.0), a simulation platform for training virtual robots in interactive 3D environments and complex physics-enabled scenarios. We make comprehensive contributions to all levels of the embodied AI stack – data, simulation, and benchmark tasks. Specifically, we present: (i) ReplicaCAD: an artist-authored, annotated, reconfigurable 3D dataset of apartments (matching real spaces) with articulated objects (e.g. cabinets and drawers that can open/close); (ii) H2.0: a high-performance physics-enabled 3D simulator with speeds exceeding 25,000 simulation steps per second (850x real-time) on an 8-GPU node, representing 100x speed-ups over prior work; and, (iii) Home Assistant Benchmark (HAB): a suite of common tasks for assistive robots (tidy the house, stock groceries, set the table) that test a range of mobile manipulation capabilities. These large-scale engineering contributions allow us to systematically compare deep reinforcement learning (RL) at scale and classical sense-plan-act (SPA) pipelines in long-horizon structured tasks, with an emphasis on generalization to new objects, receptacles, and layouts. We find that (1) flat RL policies struggle on HAB compared to hierarchical ones; (2) a hierarchy with independent skills suffers from ‘hand-off problems’, and (3) SPA pipelines are more brittle than RL policies.",main,NeurIPS,2021,Spotlight,Andrew Szot;Alexander Clegg;Eric Undersander;Erik Wijmans;Yili Zhao;John M Turner;Noah D Maestre;Mustafa Mukadam;Devendra Singh Chaplot;Oleksandr Maksymets;Aaron Gokaslan;Vladimír Vondruš;Sameer Dharur;Franziska Meier;Wojciech Galuba;Angel X Chang;Zsolt Kira;Vladlen Koltun;Jitendra Malik;Manolis Savva;Dhruv Batra,True,https://openreview.net/pdf?id=DPHsCQ8OpA
DSo2Zteb9l8,NeoRL: A Near Real-World Benchmark for Offline Reinforcement Learning,"Offline reinforcement learning (RL) aims at learning a good policy from a batch of collected data, without extra interactions with the environment during training. However, current offline RL benchmarks commonly have a large reality gap, because they involve large datasets collected by highly exploratory policies, and the trained policy is directly evaluated in the environment. In real-world situations, running an overly exploratory policy is prohibited to ensure system safety, the data is commonly very limited, and a trained policy should be carefully evaluated before deployment.
In this paper, we present a Near real-world offline RL benchmark, named NeoRL, which contains datasets from various domains with controlled sizes, and extra test datasets for offline policy evaluation. We evaluate recent SOTA offline RL algorithms on NeoRL, through both online evaluation and purely offline evaluation. The empirical results demonstrate that the tested offline RL algorithms become less competitive to BC on many datasets, and the current offline policy evaluation methods can hardly select truly effective policies. We hope this work will shed some light on future research and draw more attention when deploying RL in real-world systems.",Datasets & Benchmarks,NeurIPS,2021,Reject,Rong-Jun Qin;Xingyuan Zhang;Songyi Gao;Zhen Xu;Shengkai Huang;Zewen Li;Weinan Zhang;Yang Yu,True,https://openreview.net/pdf?id=DSo2Zteb9l8
DfGu8WwT0d,Large Scale Learning on Non-Homophilous Graphs: New Benchmarks and Strong Simple Methods,"Many widely used datasets for graph machine learning tasks have generally been homophilous, where nodes with similar labels connect to each other. Recently, new Graph Neural Networks (GNNs) have been developed that move beyond the homophily regime; however, their evaluation has often been conducted on small graphs with limited application domains. We collect and introduce diverse non-homophilous datasets from a variety of application areas that have up to 384x more nodes and 1398x more edges than prior datasets. We further show that existing scalable graph learning and graph minibatching techniques lead to performance degradation on these non-homophilous datasets, thus highlighting the need for further work on scalable non-homophilous methods. To address these concerns, we introduce LINKX --- a strong simple method that admits straightforward minibatch training and inference. Extensive experimental results with representative simple methods and GNNs across our proposed datasets show that LINKX achieves state-of-the-art performance for learning on non-homophilous graphs. Our codes and data are available at https://github.com/CUAI/Non-Homophily-Large-Scale.",main,NeurIPS,2021,Poster,Derek Lim;Felix Matthew Hohne;Xiuyu Li;Sijia Linda Huang;Vaishnavi Gupta;Omkar Prasad Bhalerao;Ser-Nam Lim,True,https://openreview.net/pdf?id=DfGu8WwT0d
DjzPaX8AT0z,A Channel Coding Benchmark for Meta-Learning,"Meta-learning provides a popular and effective family of methods for data-efficient learning of new tasks. However, several important issues in meta-learning have proven hard to study thus far. For example, performance degrades in real-world settings where meta-learners must learn from a wide and potentially multi-modal distribution of training tasks; and when distribution shift exists between meta-train and meta-test task distributions. These issues are typically hard to study since the shape of task distributions, and shift between them are not straightforward to measure or control in standard benchmarks. We propose the channel coding problem as a benchmark for meta-learning. Channel coding is an important practical application where task distributions naturally arise, and fast adaptation to new tasks is practically valuable. We use this benchmark to study several aspects of meta-learning, including the impact of task distribution breadth and shift on meta-learner performance, which can be controlled in the coding problem. Going forward, this benchmark provides a tool for the community to study the capabilities and limitations of meta-learning, and to drive research on practically robust and effective meta-learners.",Datasets & Benchmarks,NeurIPS,2021,Poster,Rui Li;Ondrej Bohdal;Rajesh K Mishra;Hyeji Kim;Da Li;Nicholas Donald Lane;Timothy Hospedales,False,https://openreview.net/pdf?id=DjzPaX8AT0z
EHfBhk6IkYF,Hangul Fonts Dataset: a Hierarchical and Compositional Dataset for Investigating Learned Representations,"Hierarchy and compositionality are common latent properties in many natural and scientific datasets. Determining when a deep network's hidden activations represent hierarchy and compositionality is important both for understanding deep representation learning and for applying deep networks in domains where interpretability is crucial. However, current benchmark machine learning datasets either have little hierarchical or compositional structure, or the structure is not known. This gap impedes precise analysis of a network's representations and thus hinders development of new methods that can learn such properties. To address this gap, we developed a new benchmark dataset with known hierarchical and compositional structure. The Hangul Fonts Dataset (HFD) is comprised of 35 fonts from the Korean writing system (Hangul), each with 11,172 blocks (syllables) composed from the product of initial, medial, and final glyphs. All blocks can be grouped into a few geometric types which induces a hierarchy across blocks. In addition, each block is composed of individual glyphs with rotations, translations, scalings, and naturalistic style variation across fonts. We find that both shallow and deep unsupervised methods only show modest evidence of hierarchy and compositionality in their representations of the HFD compared to supervised deep networks. Supervised deep network representations contain structure related to the geometric hierarchy of the glyphs, but the compositional structure of the data is not evident. Thus, HFD enables the identification of shortcomings in existing methods, a critical first step toward developing new machine learning algorithms to extract hierarchical and compositional structure in the context of naturalistic variability.",Datasets & Benchmarks,NeurIPS,2021,Reject,Jesse A. Livezey;Ahyeon Hwang;Jacob Yeung;Kristofer Bouchard,True,https://openreview.net/pdf?id=EHfBhk6IkYF
EQbyD_KG6w-1h,Self-Supervised Bug Detection and Repair,"Machine learning-based program analyses have recently shown the promise of integrating formal and probabilistic reasoning towards aiding software development. However, in the absence of large annotated corpora, training these analyses is challenging. Towards addressing this, we present BugLab, an approach for self-supervised learning of bug detection and repair.  BugLab co-trains two models: (1) a detector model that learns to detect and repair bugs in code, (2) a selector model that learns to create buggy code for the detector to use as training data. A Python implementation of BugLab improves by 30% upon baseline methods on a test dataset of 2374 real-life bugs and finds 19 previously unknown bugs in open-source software.",main,NeurIPS,2021,Poster,Miltiadis Allamanis;Henry Richard Jackson-Flux;Marc Brockschmidt,True,https://openreview.net/pdf?id=EQbyD_KG6w-1h
EfgNF5-ZAjM,STAR: A Benchmark for Situated Reasoning in Real-World Videos,"Reasoning in the real world is not divorced from situations. How to capture the present knowledge from surrounding situations and perform reasoning accordingly is crucial and challenging for machine intelligence. This paper introduces a new benchmark that evaluates the situated reasoning ability via situation abstraction and logic-grounded question answering for real-world videos, called Situated Reasoning in Real-World Videos (STAR). This benchmark is built upon the real-world videos associated with human actions or interactions, which are naturally dynamic, compositional, and logical. The dataset includes four types of questions, including interaction, sequence, prediction, and feasibility. We represent the situations in real-world videos by hyper-graphs connecting extracted atomic entities and relations (e.g., actions, persons, objects, and relationships). Besides visual perception, situated reasoning also requires structured situation comprehension and logical reasoning. Questions and answers are procedurally generated. The answering logic of each question is represented by a functional program based on a situation hyper-graph. We compare various existing video reasoning models and find that they all struggle on this challenging situated reasoning task. We further propose a diagnostic neuro-symbolic model that can disentangle visual perception, situation abstraction, language understanding, and functional reasoning to understand the challenges of this benchmark. ",Datasets & Benchmarks,NeurIPS,2021,Poster,Bo Wu;Shoubin Yu;Zhenfang Chen;Joshua B. Tenenbaum;Chuang Gan,True,https://openreview.net/pdf?id=EfgNF5-ZAjM
FQLzQqGEAH,Really Doing Great at Estimating CATE? A Critical Look at ML Benchmarking Practices in Treatment Effect Estimation,"The machine learning (ML) toolbox for estimation of heterogeneous treatment effects from observational data is expanding rapidly, yet many of its algorithms have been evaluated only on a very limited set of semi-synthetic benchmark datasets. In this paper, we investigate current benchmarking practices for ML-based conditional average treatment effect (CATE) estimators, with special focus on empirical evaluation based on the popular semi-synthetic IHDP benchmark. We identify problems with current practice and highlight that semi-synthetic benchmark datasets, which (unlike real-world benchmarks used elsewhere in ML) do not necessarily reflect properties of real data, can systematically favor some algorithms over others  -- a fact that is rarely acknowledged but of immense relevance for interpretation of empirical results. Further, we argue that current evaluation metrics evaluate performance only for a small subset of possible use cases of CATE estimators, and discuss alternative metrics relevant for applications in personalized medicine. Additionally, we discuss alternatives for current benchmark datasets, and implications of our findings for benchmarking in CATE estimation. ",Datasets & Benchmarks,NeurIPS,2021,Poster,Alicia Curth;David Svensson;Jim Weatherall;Mihaela van der Schaar,False,https://openreview.net/pdf?id=FQLzQqGEAH
FZBtIpEAb5J,ClimART: A Benchmark Dataset for Emulating Atmospheric Radiative Transfer in Weather and Climate Models,"  Numerical simulations of Earth's weather and climate require substantial amounts of computation. This has led to a growing interest in replacing subroutines that explicitly compute physical processes with approximate machine learning (ML) methods that are fast at inference time. Within weather and climate models, atmospheric radiative transfer (RT) calculations are especially expensive. 
  This has made them a popular target for neural network-based emulators.  
  However, prior work is hard to compare due to the lack of a comprehensive dataset and standardized best practices for ML benchmarking.
  To fill this gap, we build a large dataset, ClimART, with more than 10 million samples from present, pre-industrial, and future climate conditions, based on the Canadian Earth System Model.
  ClimART poses several methodological challenges for the ML community, such as multiple out-of-distribution test sets, underlying domain physics, and a trade-off between accuracy and inference speed.
  We also present several novel baselines that indicate shortcomings of datasets and network architectures used in prior work.",Datasets & Benchmarks,NeurIPS,2021,Poster,Salva Rühling Cachay;Venkatesh Ramesh;Jason N. S. Cole;Howard Barker;David Rolnick,True,https://openreview.net/pdf?id=FZBtIpEAb5J
FgYTwJbjbf,FFA-IR: Towards an Explainable and Reliable Medical Report Generation Benchmark,"The automatic generation of long and coherent medical reports given medical images (e.g. Chest X-ray and Fundus Fluorescein Angiography (FFA)) has great potential to support clinical practice. Researchers have explored advanced methods from computer vision and natural language processing to incorporate medical domain knowledge for the generation of readable medical reports. However, existing medical report generation (MRG) benchmarks lack both explainable annotations and reliable evaluation tools, hindering the current research advances from two aspects: firstly, existing methods can only predict reports without accurate explanation, undermining the trustworthiness of the diagnostic methods; secondly, the comparison among the predicted reports from different MRG methods is unreliable using the evaluation metrics of natural-language generation (NLG). To address these issues, in this paper, we propose an explainable and reliable MRG benchmark based on FFA Images and Reports (FFA-IR). Specifically, FFA-IR is large, with 10,790 reports along with 1,048,584 FFA images from clinical practice; it includes explainable annotations, based on a schema of 46 categories of lesions; and it is bilingual, providing both English and Chinese reports for each case. Besides using the widely used NLG metrics, we propose a set of nine human evaluation criteria to evaluate the generated reports. We envision FFA-IR as a testbed for explainable and reliable medical report generation. We also hope that it can broadly accelerate medical imaging research and facilitate interaction between the fields of medical imaging, computer vision, and natural language processing.",Datasets & Benchmarks,NeurIPS,2021,Poster,Mingjie Li;Wenjia Cai;Rui Liu;Yuetian Weng;Xiaoyun Zhao;Cong Wang;Xin Chen;Zhong Liu;Caineng Pan;Mengke Li;yingfeng zheng;Yizhi Liu;Flora D. Salim;Karin Verspoor;Xiaodan Liang;Xiaojun Chang,True,https://openreview.net/pdf?id=FgYTwJbjbf
FkDZLpK1Ml2,ATOM3D: Tasks on Molecules in Three Dimensions,"Computational methods that operate on three-dimensional (3D) molecular structure have the potential to solve important problems in biology and chemistry. Deep neural networks have gained significant attention, but their widespread adoption in the biomolecular domain has been limited by a lack of either systematic performance benchmarks or a unified toolkit for interacting with 3D molecular data. To address this, we present ATOM3D, a collection of both novel and existing benchmark datasets spanning several key classes of biomolecules. We implement several types of 3D molecular learning methods for each of these tasks and show that they consistently improve performance relative to methods based on one- and two-dimensional representations. The choice of architecture proves to be important for performance, with 3D convolutional networks excelling at tasks involving complex geometries, graph networks performing well on systems requiring detailed positional information, and the more recently developed equivariant networks showing significant promise. Our results indicate that many molecular problems stand to gain from 3D molecular learning, and that there is potential for substantial further improvement on many tasks. To lower the barrier to entry and facilitate further developments in the field, we also provide a comprehensive suite of tools for dataset processing, model training, and evaluation in our open-source atom3d Python package. All datasets are available for download from www.atom3d.ai.",Datasets & Benchmarks,NeurIPS,2021,Poster,Raphael John Lamarre Townshend;Martin Vögele;Patricia Adriana Suriana;Alexander Derry;Alexander Powers;Yianni Laloudakis;Sidhika Balachandar;Bowen Jing;Brandon M. Anderson;Stephan Eismann;Risi Kondor;Russ Altman;Ron O. Dror,True,https://openreview.net/pdf?id=FkDZLpK1Ml2
Fkpr2RYDvI1,SynthBio: A Case Study in Faster Curation of Text Datasets,"NLP researchers need more, higher-quality text datasets. Human-labeled datasets are expensive to collect, while datasets collected via automatic retrieval from the web such as WikiBio [Lebret 2016] are noisy and can include undesired biases. Moreover, data sourced from the web is often included in datasets used to pretrain models, leading to inadvertent cross-contamination of training and test sets. In this work we introduce a novel method for efficient dataset curation: we use a large language model to provide seed generations to human raters, thereby changing dataset authoring from a writing task to an editing task. We use our method to curate SynthBio - a new evaluation set for WikiBio - comprised of structured attribute lists describing fictional individuals, mapped to natural language biographies. We show that our dataset of fictional biographies is less noisy than WikiBio, and also more balanced with respect to gender and nationality.",Datasets & Benchmarks,NeurIPS,2021,Poster,Ann Yuan;Daphne Ippolito;Vitaly Nikolaev;Chris Callison-Burch;Andy Coenen;Sebastian Gehrmann,True,https://openreview.net/pdf?id=Fkpr2RYDvI1
G1muTb5zuO7,What Would Jiminy Cricket Do? Towards Agents That Behave Morally,"When making everyday decisions, people are guided by their conscience, an internal sense of right and wrong, to behave morally. By contrast, artificial agents may behave immorally when trained on environments that ignore moral concerns, such as violent video games. With the advent of generally capable agents that pretrain on many environments, mitigating inherited biases towards immoral behavior will become necessary. However, prior work on aligning agents with human values and morals focuses on small-scale settings lacking in semantic complexity. To enable research in larger, more realistic settings, we introduce Jiminy Cricket, an environment suite of 25 text-based adventure games with thousands of semantically rich, morally salient scenarios. Via dense annotations for every possible action, Jiminy Cricket environments robustly evaluate whether agents can act morally while maximizing reward. To improve moral behavior, we leverage language models with commonsense moral knowledge and develop strategies to mediate this knowledge into actions. In extensive experiments, we find that our artificial conscience approach can steer agents towards moral behavior without sacrificing performance.",Datasets & Benchmarks,NeurIPS,2021,Poster,Dan Hendrycks;Mantas Mazeika;Andy Zou;Sahil Patel;Christine Zhu;Jesus Navarro;Dawn Song;Bo Li;Jacob Steinhardt,True,https://openreview.net/pdf?id=G1muTb5zuO7
GEcWUTN1v1v,Native Chinese Reader: A Dataset Towards Native-Level Chinese Machine Reading Comprehension,"We present Native Chinese Reader (NCR),  a new machine reading comprehension  MRC) dataset with particularly long articles in both modern and classical Chinese. NCR is collected from the exam questions for the Chinese course in China’s high schools, which are designed to evaluate the language proficiency of native Chinese youth.  Existing Chinese MRC datasets are either domain-specific or focusing on short contexts of a few hundred characters in modern Chinese only. By contrast, NCR contains 8390 documents with an average length of 1024 characters covering a wide range of Chinese writing styles, including modern articles, classical literature and classical poetry.  A total of  20477  questions on these documents also require strong reasoning abilities and common sense to figure out the correct answers. We implemented multiple baseline models using popular Chinese pre-trained models and additionally launched an online competition using our dataset to examine the limit of current methods.  The best model achieves 59% test accuracy while human evaluation shows an average accuracy of 79%, which indicates a significant performance gap between current MRC models and native Chinese speakers.",Datasets & Benchmarks,NeurIPS,2021,Poster,Shusheng Xu;Yichen Liu;Xiaoyu Yi;Siyuan Zhou;Huizi Li;Yi Wu,True,https://openreview.net/pdf?id=GEcWUTN1v1v
GF9cSKI3A_q,Adversarial GLUE: A Multi-Task Benchmark for Robustness Evaluation of Language Models,"Large-scale pre-trained language models have achieved tremendous success across a wide range of natural language understanding (NLU) tasks, even surpassing human performance. However, recent studies reveal that the robustness of these models can be challenged by carefully crafted textual adversarial examples. While several individual datasets have been proposed to evaluate model robustness, a principled and comprehensive benchmark is still missing. In this paper, we present Adversarial GLUE (AdvGLUE), a new multi-task benchmark to quantitatively and thoroughly explore and evaluate the vulnerabilities of modern large-scale language models under various types of adversarial attacks. In particular, we systematically apply 14 textual adversarial attack methods to GLUE tasks to construct AdvGLUE, which is further validated by humans for reliable annotations. Our ﬁndings are summarized as follows. (i) Most existing adversarial attack algorithms are prone to generating invalid or ambiguous adversarial examples, with around 90% of them either changing the original semantic meanings or misleading human annotators as well. Therefore, we perform a careful ﬁltering process to curate a high-quality benchmark. (ii) All the language models and robust training methods we tested perform poorly on AdvGLUE, with scores lagging far behind the benign accuracy. We hope our work will motivate the development of new adversarial attacks that are more stealthy and semantic-preserving, as well as new robust language models against sophisticated adversarial attacks. AdvGLUE is available at https://adversarialglue.github.io.",Datasets & Benchmarks,NeurIPS,2021,Poster,Boxin Wang;Chejian Xu;Shuohang Wang;Zhe Gan;Yu Cheng;Jianfeng Gao;Ahmed Hassan Awadallah;Bo Li,True,https://openreview.net/pdf?id=GF9cSKI3A_q
GVe2IvtZtVY,CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions,"Humans are able to perceive, understand and reason about physical events. Developing models with similar physical understanding capabilities is a long standing goal of artificial intelligence. As a step towards this goal, in this work, we introduce CRAFT, a new visual question answering dataset that requires causal reasoning about physical forces and object interactions. It contains 58K video and question pairs that are generated from 10K videos from 20 different virtual environments, containing various objects in motion that interact with each other and the scene. Two question categories from CRAFT include previously studied descriptive and counterfactual questions. Besides, inspired by the theories of force dynamics in cognitive linguistics, we introduce new question categories that involve understanding the interactions of objects through the notions of cause, enable, and prevent. Our results demonstrate that even though these tasks seem to be simple and intuitive for humans, the evaluated baseline models, including existing state-of-the-art methods, do not yet deal with the challenges posed in our benchmark dataset.",Datasets & Benchmarks,NeurIPS,2021,Reject,Tayfun Ates;M. Şamil Ateşoğlu;Çağatay Yiğit;Ilker Kesen;Mert Kobas;Erkut Erdem;Aykut Erdem;Tilbe Goksun;Deniz Yuret,True,https://openreview.net/pdf?id=GVe2IvtZtVY
Gln7zxMffae,Relational Pattern Benchmarking on the Knowledge Graph Link Prediction Task,"Knowledge graphs (KGs) encode facts about the world in a graph data structure where entities, represented as nodes, connect via relationships, acting as edges. KGs are widely used in Machine Learning, e.g., to solve Natural Language Processing based tasks. Despite all the advancements in KGs, they plummet when it comes to completeness. Link Prediction based on KG embeddings targets the sparsity and incompleteness of KGs. Available datasets for Link Prediction do not consider different graph patterns, making it difficult to measure the performance of link prediction models on different KG settings. This paper presents a diverse set of pragmatic datasets to facilitate flexible and problem-tailored Link Prediction and Knowledge Graph Embeddings research. We define graph relational patterns, from being entirely inductive in one set to being transductive in the other. For each dataset, we provide uniform evaluation metrics. We analyze the models over our datasets to compare the model’s capabilities on a specific dataset type. Our analysis of datasets over state-of-the-art models provides a better insight into the suitable parameters for each situation, optimizing the KG-embedding-based systems.",Datasets & Benchmarks,NeurIPS,2021,Poster,Afshin Sadeghi;Hirra Abdul Malik;Diego Collarana;Jens Lehmann,True,https://openreview.net/pdf?id=Gln7zxMffae
H-d5634yVi,Chest ImaGenome Dataset for Clinical Reasoning,"Despite the progress in automatic detection of radiologic findings from Chest X-Ray (CXR) images in recent years, a quantitative evaluation of the explainability of these models is hampered by the lack of locally labeled datasets for different findings. With the exception of a few expert-labeled small-scale datasets for specific findings, such as pneumonia and pneumothorax, most of the CXR deep learning models to date are trained on global ""weak"" labels extracted from text reports, or trained via a joint image and unstructured text learning strategy. Inspired by the Visual Genome effort in the computer vision community, we constructed the first Chest ImaGenome dataset with a scene graph data structure to describe 242,072 images. Local annotations are automatically produced using a joint rule-based natural language processing (NLP) and atlas-based bounding box detection pipeline. Through a radiologist constructed CXR ontology, the annotations for each CXR are connected as an anatomy-centered scene graph, useful for image-level reasoning and multimodal fusion applications. Overall, we provide: i) 1,256 combinations of relation annotations between 29 CXR anatomical locations (objects with bounding box coordinates) and their attributes, structured as a scene graph per image, ii) over 670,000 localized comparison relations (for improved, worsened, or no change) between the anatomical locations across sequential exams, as well as ii) a manually annotated gold standard scene graph dataset from 500 unique patients.",Datasets & Benchmarks,NeurIPS,2021,Poster,Joy T Wu;Nkechinyere Nneka Agu;Ismini Lourentzou;Arjun Sharma;Joseph Alexander Paguio;Jasper Seth Yao;Edward Christopher Dee;William G Mitchell;Satyananda Kashyap;Andrea Giovannini;Leo Anthony Celi;Mehdi Moradi,True,https://openreview.net/pdf?id=H-d5634yVi
HQ-6VDYUxGn,"PROCAT: Product Catalogue Dataset for Implicit Clustering, Permutation Learning and Structure Prediction","In this dataset paper we introduce PROCAT, a novel e-commerce dataset containing expertly designed product catalogues consisting of individual product offers grouped into complementary sections. We aim to address the scarcity of existing datasets in the area of set-to-sequence machine learning tasks, which involve complex structure prediction. The task's difficulty is further compounded by the need to place into sequences rare and previously-unseen instances, as well as by variable sequence lengths and substructures, in the form of diversely-structured catalogues. PROCAT provides catalogue data consisting of over 1.5 million set items across a 4-year period, in both raw text form and with pre-processed features containing information about relative visual placement. In addition to this ready-to-use dataset, we include baseline experimental results on a proposed benchmark task from a number of joint set encoding and permutation learning model architectures.",Datasets & Benchmarks,NeurIPS,2021,Poster,Mateusz Maria Jurewicz;Leon Derczynski,True,https://openreview.net/pdf?id=HQ-6VDYUxGn
HrhaC-bLC5U,Timers and Such: A Practical Benchmark for Spoken Language Understanding with Numbers,"This paper introduces Timers and Such, a new open source dataset of spoken English commands for common voice control use cases involving numbers. We describe the gap in existing spoken language understanding datasets that Timers and Such fills, the design and creation of the dataset, and experiments with a number of ASR-based and end-to-end baseline models, the code for which has been made available as part of the SpeechBrain toolkit.",Datasets & Benchmarks,NeurIPS,2021,Poster,Loren Lugosch;Piyush Papreja;Mirco Ravanelli;Abdelwahab HEBA;Titouan Parcollet,True,https://openreview.net/pdf?id=HrhaC-bLC5U
IfzTefIU_3j,Occluded Video Instance Segmentation: Dataset and ICCV 2021 Challenge,"Although deep learning methods have achieved advanced video object recognition performance in recent years, perceiving heavily occluded objects in a video is still a very challenging task. To promote the development of occlusion understanding, we collect a large-scale dataset called OVIS for video instance segmentation in the occluded scenario. OVIS consists of 296k high-quality instance masks and 901 occluded scenes. While our human vision systems can perceive those occluded objects by contextual reasoning and association, our experiments suggest that current video understanding systems cannot. On the OVIS dataset, all baseline methods encounter a significant performance degradation of about 80\\\\% in the heavily occluded object group, which demonstrates that there is still a long way to go in understanding obscured objects and videos in a complex real-world scenario. To facilitate the research on new paradigms for video understanding systems, we launched a challenge basing on the OVIS dataset. The submitted top-performing algorithms have achieved much higher performance than our baselines. In this paper, we will introduce the OVIS dataset and further dissect it by analyzing the results of baselines and submitted methods. The OVIS dataset and challenge information can be found at \\\\url{http://songbai.site/ovis}.",Datasets & Benchmarks,NeurIPS,2021,Poster,Jiyang Qi;Yan Gao;Yao Hu;Xinggang Wang;Xiaoyu Liu;Xiang Bai;Serge Belongie;Alan Yuille;Philip Torr;Song Bai,True,https://openreview.net/pdf?id=IfzTefIU_3j
IsK8iKbL-I,Empirical Study of Off-Policy Policy Evaluation for Reinforcement Learning,"We offer an experimental benchmark and empirical study for off-policy policy evaluation (OPE) in reinforcement learning, which is a key problem in many safety critical applications. Given the increasing interest in deploying learning-based methods, there has been a flurry of recent proposals for OPE method, leading to a need for standardized empirical analyses.  Our work takes a strong focus on diversity of experimental design to enable stress testing of OPE methods.  We provide a comprehensive benchmarking suite to study the interplay of different attributes on method performance. We distill the results into a summarized set of guidelines for OPE in practice. Our software package, the Caltech OPE Benchmarking Suite (COBS), is open-sourced and we invite interested researchers to further contribute to the benchmark.",Datasets & Benchmarks,NeurIPS,2021,Poster,Cameron Voloshin;Hoang Minh Le;Nan Jiang;Yisong Yue,False,https://openreview.net/pdf?id=IsK8iKbL-I
IsYUDnbIqay,Modeling and Optimizing Laser-Induced Graphene,"A lot of technological advances depend on next-generation materials, such as graphene, which enables a raft of new applications, for example better electronics. Manufacturing such materials is often difficult; in particular, producing graphene at scale is an open problem. We provide a series of datasets that describe the optimization of the production of laser-induced graphene, an established manufacturing method that has shown great promise. We pose three challenges based on the datasets we provide -- modeling the behavior of laser-induced graphene production with respect to parameters of the production process, transferring models and knowledge between different precursor materials, and optimizing the outcome of the transformation over the space of possible production parameters. We present illustrative results, along with the code used to generate them, as a starting point for interested users. The data we provide represents an important real-world application of machine learning; to the best of our knowledge, no similar datasets are available.",Datasets & Benchmarks,NeurIPS,2021,Reject,Lars Kotthoff;Sourin Dey;Vivek Jain;Alexander Tyrrell;Hud Wahab;Patrick Johnson,True,https://openreview.net/pdf?id=IsYUDnbIqay
J0d-I8yFtP,The Neural MMO Platform for Massively Multiagent Research,"Neural MMO is a computationally accessible research platform that combines large agent populations, long time horizons, open-ended tasks, and modular game systems. Existing environments feature subsets of these properties, but Neural MMO is the first to combine them all. We present Neural MMO as free and open source software with active support, ongoing development, documentation, and additional training, logging, and visualization tools to help users adapt to this new setting. Initial baselines on the platform demonstrate that agents trained in large populations explore more and learn a progression of skills. We raise other more difficult problems such as many-team cooperation as open research questions which Neural MMO is well-suited to answer. Finally, we discuss current limitations of the platform, potential mitigations, and plans for continued development.",Datasets & Benchmarks,NeurIPS,2021,Poster,Joseph Suarez;Yilun Du;Clare Zhu;Igor Mordatch;Phillip Isola,False,https://openreview.net/pdf?id=J0d-I8yFtP
J4Nl2qRMDrR,ClevrTex: A Texture-Rich Benchmark for Unsupervised Multi-Object Segmentation,"There has been a recent surge in methods that aim to decompose and segment scenes into multiple objects in an unsupervised manner, i.e., unsupervised multi-object segmentation. Performing such a task is a long-standing goal of computer vision, offering to unlock object-level reasoning without requiring dense annotations to train segmentation models. Despite significant progress, current models are developed and trained on visually simple scenes depicting mono-colored objects on plain backgrounds. The natural world, however, is visually complex with confounding aspects such as diverse textures and complicated lighting effects. In this study, we present a new benchmark called ClevrTex, designed as the next challenge to compare, evaluate and analyze algorithms. ClevrTex features synthetic scenes with diverse shapes, textures and photo-mapped materials, created using physically based rendering techniques. ClevrTex has 50k examples depicting 3-10 objects arranged on a background, created using a catalog of 60 materials, and a further test set featuring 10k images created using 25 different materials. We benchmark a large set of recent unsupervised multi-object segmentation models on ClevrTex and find all state-of-the-art approaches fail to learn good representations in the textured setting, despite impressive performance on simpler data. We also create variants of the ClevrTex dataset, controlling for different aspects of scene complexity, and probe current approaches for individual shortcomings. Dataset and code are available at https://www.robots.ox.ac.uk/~vgg/research/clevrtex.",Datasets & Benchmarks,NeurIPS,2021,Poster,Laurynas Karazija;Iro Laina;Christian Rupprecht,True,https://openreview.net/pdf?id=J4Nl2qRMDrR
JH61CD7afTv,LiRo: Benchmark and leaderboard for Romanian language tasks,"Recent advances in NLP have been sustained by the availability of large amounts of data and standardized benchmarks, which are not available for many languages. As a small step towards addressing this we propose LiRo, a platform for benchmarking models on the Romanian language on nine standard tasks: text classification, named entity recognition, machine translation, sentiment analysis, POS tagging, dependency parsing, language modelling, question-answering, and semantic textual similarity. We also include a less standard task of embedding debiasing, to address the growing concerns related to gender bias in language models. The platform exposes per-task leaderboards populated with baseline results for each task. In addition, we create three new datasets: one from Romanian Wikipedia and two by translating the Semantic Textual Similarity (STS) benchmark and the Cross-lingual Question Answering Dataset (XQuAD) into Romanian. We believe LiRo will not only add to the growing body of benchmarks covering various languages, but can also enable multi-lingual research by augmenting parallel corpora, and hence is of interest for the wider NLP community. LiRo is available at https://lirobenchmark.github.io/.",Datasets & Benchmarks,NeurIPS,2021,Poster,Stefan Daniel Dumitrescu;Petru Rebeja;Beata Lorincz;Mihaela Gaman;Andrei Avram;Mihai Ilie;Andrei Pruteanu;Adriana Stan;Lorena Rosia;Cristina Iacobescu;Luciana Morogan;George Dima;Gabriel Marchidan;Traian Rebedea;Madalina Chitez;Dani Yogatama;Sebastian Ruder;Radu Tudor Ionescu;Razvan Pascanu;Viorica Patraucean,True,https://openreview.net/pdf?id=JH61CD7afTv
JNvy4qBcxHW,VoxelScape: Large Scale Simulated 3D Point Cloud Dataset of Urban Traffic Environments,"Having a profound understanding of the surrounding environment is considered one of the crucial tasks for the reliable operation of future self-driving cars. Light Detection and Ranging (LiDAR) sensor plays a critical role in achieving such understanding due to its capability to perceive the world in 3D. Similar to 2D perception tasks, current state-of-the-art methods in 3D perception tasks rely on deep neural networks (DNNs). However, the performance of 3D perception tasks, specially point-wise semantic segmentation, is not on par with their 2D counterparts. One of the main reasons is the lack of publicly available labelled 3D point cloud datasets (PCDs) from 3D LiDAR sensors. In this work, we are introducing the VoxelScape dataset, a large-scale simulated 3D PCD with 100K annotated point cloud scans. The annotations in the VoxelScape dataset includes both point-wise semantic labels and 3D bounding boxes labels. Additionally, we used a number of baseline approaches to validate the transferability of VoxelScape to real 3D PCD for two challenging 3D perception tasks. The promising results have shown that training DNNs on VoxelScape boosted the performance of the 3D perception tasks on the real PCD. The VoxelScape dataset is publicly available through https://voxel-scape.github.io/dataset/",Datasets & Benchmarks,NeurIPS,2021,Reject,Khaled Saleh;Mo Hossny;Ahmed Abobakr;Mohammed Attia;Julie Iskander,True,https://openreview.net/pdf?id=JNvy4qBcxHW
JjuthZiKQ8H,"SkillBERT: ""Skilling"" the BERT to classify skills using Electronic Recruitment Records","In this work, we show how the Electronic Recruitment Records (ERRs) that store the information related to job postings and candidates can be mined and analyzed to provide assistance to hiring managers in recruitment. These ERRs are captured through our recruitment portal, where hiring managers can post the jobs and candidates can apply for various job postings. These ERRs are stored in the form of tables in our recruitment database and whenever there is a new job posting, a new ERR is added to the database.
We have leveraged the skills present in the ERRs to train a BERT-based model, SkillBERT, the embeddings of which are used as features for classifying skills into groups referred to as “competency groups”. A competency group is a group of similar skills, and it is used as matching criteria (instead of matching on skills) for finding the overlap of skills between the candidates and the jobs. This proxy match takes advantage of the BERT’s capability of deriving meaning from the structure of competency groups present in the skill dataset. The skill classification is a multi-label classification problem as a single skill can belong to multiple competency groups. To solve multi-label competency group classification using a binary classifier, we have paired each skill with each competency group and tried to predict the probability of that skill belonging to that particular competency group. SkillBERT, which is trained from scratch on the skills present in job requisitions, is shown to be better performing than the pre-trained BERT (Devlin et al., 2019) and the Word2Vec (Mikolov et al., 2013). We have also explored K-means clustering (Lloyd, 1982) and spectral clustering (Chung, 1997) on SkillBERT embeddings to generate cluster-based features. Both algorithms provide similar performance benefits. Last, we have experimented with different classification models like Random Forest (Breiman, 2001), XGBoost (Chen and Guestrin, 2016), and a deep learning algorithm Bi-LSTM (Schuster and Paliwal, 1997; Hochreiter and Schmidhuber, 1997) for the tagging of competency groups to skill. We did not observe a significant performance difference among the algorithms, although XGBoost and Bi-LSTM perform slightly better than Random Forest. The features created using SkillBERT are most predictive in the classification task, which demonstrates that the SkillBERT is able to capture the information pertaining to skill ontology from the data. We have made the source code, the trained models, and the dataset.",Datasets & Benchmarks,NeurIPS,2021,Reject,Amber Nigam;Shikha Tyagi;Kuldeep Tyagi,True,https://openreview.net/pdf?id=JjuthZiKQ8H
JtjzUXPEaCu,CropHarvest: A global dataset for crop-type classification,"Remote sensing datasets pose a number of interesting challenges to machine learning researchers and practitioners, from domain shift (spatially, semantically and temporally) to highly imbalanced labels. In addition, the outputs of models trained on remote sensing datasets can contribute to positive societal impacts, for example in food security and climate change. However, there are many barriers that limit the accessibility of satellite data to the machine learning community, including a lack of large labeled datasets as well as an understanding of the range of satellite products available, how these products should be processed, and how to manage multi-dimensional geospatial data. To lower these barriers and facilitate the use of satellite datasets by the machine learning community, we present CropHarvest---a satellite dataset of more than 90,000 geographically-diverse samples with agricultural labels. The data and accompanying python package are available at https://github.com/nasaharvest/cropharvest.",Datasets & Benchmarks,NeurIPS,2021,Poster,Gabriel Tseng;Ivan Zvonkov;Catherine Lilian Nakalembe;Hannah Kerner,True,https://openreview.net/pdf?id=JtjzUXPEaCu
Jul-uX7EV_I,SciGen: a Dataset for Reasoning-Aware Text Generation from Scientific Tables,"We introduce SciGen, a new challenge dataset consisting of tables from scientific articles and their corresponding descriptions, for the task of reasoning-aware data-to-text generation. Describing scientific tables goes beyond the surface realization of the table content and requires reasoning over table values. The unique properties of SciGen are that (1) tables mostly contain numerical values, and (2) the corresponding descriptions require arithmetic reasoning. SciGen is the first dataset that assesses the arithmetic reasoning capabilities of generation models on complex input structures, such as tables from scientific articles, and thus it opens new avenues for future research in reasoning-aware text generation and evaluation. The core part of SciGen, including the test data, is annotated by one of the authors of the corresponding articles. Such expert annotations do not scale to large training data sizes. To tackle this, we propose a pipeline for automatically extracting high-quality table-description pairs from the LaTeX sources of scientific articles. We study the effectiveness of state-of-the-art data-to-text generation models on SciGen and evaluate the results using common metrics and human evaluation. Our results and analyses show that adding high-quality unsupervised training data improves the correctness and reduces the hallucination in generated descriptions, however, the ability of state-of-the-art models is still severely limited on this task.",Datasets & Benchmarks,NeurIPS,2021,Poster,Nafise Sadat Moosavi;Andreas Rücklé;Dan Roth;Iryna Gurevych,True,https://openreview.net/pdf?id=Jul-uX7EV_I
Jvxa8adr3iY,NaturalProofs: Mathematical Theorem Proving in Natural Language,"Understanding and creating mathematics using natural mathematical language - the mixture of symbolic and natural language used by humans - is a challenging and important problem for driving progress in machine learning. As a step in this direction, we develop NaturalProofs, a multi-domain corpus of mathematical statements and their proofs, written in natural mathematical language. NaturalProofs unifies broad coverage, deep coverage, and low-resource mathematical sources, allowing for evaluating both in-distribution and zero-shot generalization. Using NaturalProofs, we benchmark strong neural methods on mathematical reference retrieval and generation tasks which test a system's ability to determine key results that appear in a proof. Large-scale sequence models show promise compared to classical information retrieval methods, yet their performance and out-of-domain generalization leave substantial room for improvement. NaturalProofs opens many avenues for research on challenging mathematical tasks.",Datasets & Benchmarks,NeurIPS,2021,Poster,Sean Welleck;Jiacheng Liu;Ronan Le Bras;Hannaneh Hajishirzi;Yejin Choi;Kyunghyun Cho,True,https://openreview.net/pdf?id=Jvxa8adr3iY
K7ke_GZ_6N,Artsheets for Art Datasets,"Machine learning (ML) techniques are increasingly being employed within a variety of creative domains. For example, ML tools are being used to analyze the authenticity of artworks, to simulate artistic styles, and to augment human creative processes. While this progress has opened up new creative avenues, it has also paved the way for adverse downstream effects such as cultural appropriation (e.g., cultural misrepresentation, offense, and undervaluing) and representational harm. Many such concerning issues stem from the training data in ways that diligent evaluation can uncover, prevent, and mitigate. We posit that, when developing an arts-based dataset, it is essential to consider the social factors that influenced the process of conception and design, and the resulting gaps must be examined in order to maximize understanding of the dataset's meaning and future impact. Each dataset creator's decision produces opportunities, but also omissions. Each choice, moreover, builds on preexisting histories of the data's formation and handling across time by prior actors including, but not limited to, art collectors, galleries, libraries, archives, museums, and digital repositories. To illuminate the aforementioned aspects, we provide a checklist of questions customized for use with art datasets in order to help guide assessment of the ways that dataset design may either perpetuate or shift exclusions found in repositories of art data. The checklist is organized to address the dataset creator's motivation together with dataset provenance, composition, collection, pre-processing, cleaning, labeling, use (including data generation), distribution, and maintenance. Two case studies exemplify the value and application of our questionnaire.",Datasets & Benchmarks,NeurIPS,2021,Poster,Ramya Srinivasan;Emily Denton;Jordan Famularo;Negar Rostamzadeh;Fernando Diaz;Beth Coleman,False,https://openreview.net/pdf?id=K7ke_GZ_6N
K9WlOVPEpnM,Evaluating State-of-the-Art Classification Models Against Bayes Optimality,"Evaluating the inherent difficulty of a given data-driven classification problem is important for establishing absolute benchmarks and evaluating progress in the field. To this end, a natural quantity to consider is the \\\\emph{Bayes error}, which measures the optimal classification error theoretically achievable for a given data distribution.  While generally an intractable quantity, we show that we can compute the exact Bayes error of generative models learned using normalizing flows. Our technique relies on a fundamental result, which states that the Bayes error is invariant under invertible transformation. Therefore, we can compute the exact Bayes error of the learned flow models by computing it for Gaussian base distributions, which can be done efficiently using Holmes-Diaconis-Ross integration. Moreover, we show that by varying the temperature of the learned flow models, we can generate synthetic datasets that closely resemble standard benchmark datasets, but with almost any desired Bayes error. We use our approach to conduct a thorough investigation of state-of-the-art classification models, and find that in some --- but not all --- cases, these models are capable of obtaining accuracy very near optimal. Finally, we use our method to evaluate the intrinsic ""hardness"" of standard benchmark datasets.",main,NeurIPS,2021,Poster,Ryan Theisen;Huan Wang;Lav R. Varshney;Caiming Xiong;richard socher,True,https://openreview.net/pdf?id=K9WlOVPEpnM
KBbxt3JGn0Y,One Million Scenes for Autonomous Driving: ONCE Dataset,"Current perception models in autonomous driving have become notorious for greatly relying on a mass of annotated data to cover unseen cases and address the long-tail problem. On the other hand, learning from unlabeled large-scale collected data and incrementally self-training powerful recognition models have received increasing attention and may become the solutions of next-generation industry-level powerful and robust perception models in autonomous driving. However, the research community generally suffered from data inadequacy of those essential real-world scene data, which hampers the future exploration of fully/semi/self-supervised methods for 3D perception. In this paper, we introduce the ONCE (One millioN sCenEs) dataset for 3D object detection in the autonomous driving scenario. The ONCE dataset consists of 1 million LiDAR scenes and 7 million corresponding camera images. The data is selected from 144 driving hours, which is 20x longer than the largest 3D autonomous driving dataset available (\\\\eg nuScenes and Waymo), and it is collected across a range of different areas, periods and weather conditions. To facilitate future research on exploiting unlabeled data for 3D detection, we additionally provide a benchmark in which we reproduce and evaluate a variety of self-supervised and semi-supervised methods on the ONCE dataset. We conduct extensive analyses on those methods and provide valuable observations on their performance related to the scale of used data. Data, code, and more information are available at \\\\href{https://once-for-auto-driving.github.io/index.html}{http://www.once-for-auto-driving.com}. ",Datasets & Benchmarks,NeurIPS,2021,Poster,Jiageng Mao;Minzhe Niu;Chenhan Jiang;hanxue liang;Jingheng Chen;Xiaodan Liang;Yamin Li;Chaoqiang Ye;Wei Zhang;Zhenguo Li;Jie Yu;Hang Xu;Chunjing Xu,True,https://openreview.net/pdf?id=KBbxt3JGn0Y
KGeAHDH4njY,Mitigating dataset harms requires stewardship: Lessons from 1000 papers,"Machine learning datasets have elicited concerns about privacy, bias, and unethical applications, leading to the retraction of prominent datasets such as DukeMTMC, MS-Celeb-1M, and Tiny Images. In response, the machine learning community has called for higher ethical standards in dataset creation. To help inform these efforts, we studied three influential but ethically problematic face and person recognition datasets---Labeled Faces in the Wild (LFW), MS-Celeb-1M, and DukeMTMC---by analyzing nearly 1000 papers that cite them. We found that the creation of derivative datasets and models, broader technological and social change, the lack of clarity of licenses, and
dataset management practices can introduce a wide range of ethical concerns. We conclude by suggesting a distributed approach to harm mitigation that considers the entire life cycle of a dataset.",Datasets & Benchmarks,NeurIPS,2021,Poster,Kenneth L Peng;Arunesh Mathur;Arvind Narayanan,False,https://openreview.net/pdf?id=KGeAHDH4njY
KVMS3fl4Rsv,Neural Latents Benchmark ‘21: Evaluating latent variable models of neural population activity,"Advances in neural recording present increasing opportunities to study neural activity in unprecedented detail. Latent variable models (LVMs) are promising tools for analyzing this rich activity across diverse neural systems and behaviors, as LVMs do not depend on known relationships between the activity and external experimental variables. However, progress with LVMs for neuronal population activity is currently impeded by a lack of standardization, resulting in methods being developed and compared in an ad hoc manner. To coordinate these modeling efforts, we introduce a benchmark suite for latent variable modeling of neural population activity. We curate four datasets of neural spiking activity from cognitive, sensory, and motor areas to promote models that apply to the wide variety of activity seen across these areas. We identify unsupervised evaluation as a common framework for evaluating models across datasets, and apply several baselines that demonstrate the variety of the benchmarked datasets. We release this benchmark through EvalAI. http://neurallatents.github.io",Datasets & Benchmarks,NeurIPS,2021,Poster,Felix C Pei;Joel Ye;David M. Zoltowski;Anqi Wu;Raeed Hasan Chowdhury;Hansem Sohn;Joseph E O'Doherty;Krishna V. Shenoy;Matthew Kaufman;Mark M Churchland;Mehrdad Jazayeri;Lee E. Miller;Jonathan W. Pillow;Il Memming Park;Eva L Dyer;Chethan Pandarinath,False,https://openreview.net/pdf?id=KVMS3fl4Rsv
LL_LfK7dfpR,NATURE: Natural Auxiliary Text Utterances forRealistic Spoken Language Evaluation,"Slot-filling and intent detection are the backbone of conversational agents such as voice assistants and they are active areas of research. Even though state-of-the-art techniques on publicly available benchmarks show impressive performance, their ability to generalize to realistic scenarios has yet to be improved. In this work, we present NATURE, a set of simple spoken language oriented transformations, applied to the evaluation set of datasets, to introduce human spoken language variations while preserving the semantics of an utterance. We apply NATURE to common slot-filling and intent detection benchmarks and demonstrate that simple deviations from the standard test set by NATURE can deteriorate model's performance significantly. Additionally, we apply different strategies to mitigate the effects of NATURE and report that data-augmentation leads to some improvement.",Datasets & Benchmarks,NeurIPS,2021,Reject,David Alfonso-Hermelo;Ahmad Rashid;Abbas Ghaddar;Philippe Langlais;Mehdi Rezagholizadeh,False,https://openreview.net/pdf?id=LL_LfK7dfpR
LZ5cx2yismf,FedScale: Benchmarking Model and System Performance of Federated Learning,"We present FedScale, a diverse set of challenging and realistic benchmark datasets to facilitate scalable, comprehensive, and reproducible federated learning (FL) research. FedScale datasets are large-scale, encompassing a diverse range of important FL tasks, such as image classification, object detection, language modeling, speech recognition, and reinforcement learning. For each dataset, we provide a unified evaluation protocol using realistic data splits and evaluation metrics. To meet the pressing need for reproducing realistic FL at scale, we have also built an efficient evaluation platform to simplify and standardize the process of FL experimental setup and model evaluation. Our evaluation platform provides flexible APIs to implement new FL algorithms and includes new execution backends with minimal developer efforts. Finally, we perform indepth benchmark experiments on these datasets. Our experiments suggest fruitful opportunities in heterogeneity-aware co-optimizations of the system and statistical efficiency under realistic FL characteristics. FedScale is open-source with permissive licenses and actively maintained, and we welcome feedback and contributions from the community. ",Datasets & Benchmarks,NeurIPS,2021,Reject,Fan Lai;Yinwei Dai;Xiangfeng Zhu;Mosharaf Chowdhury,True,https://openreview.net/pdf?id=LZ5cx2yismf
LjjqegBNtPi,Seasons in Drift: A Long Term Thermal Imaging Dataset for Studying Concept Drift,"The time dimension of datasets and the long-term performance of machine learning models have received little attention. With extended deployments in the wild, models are bound to encounter novel scenarios and concept drift that cannot be accounted for during development and training. In order for long-term patterns and cycles to appear in datasets, the datasets must cover long periods of time. Since this is rarely the case, it is difficult to explore how computer vision algorithms cope with changes in data distribution occurring across long-term cycles such as seasons. Video surveillance is an application area clearly affected by concept drift. For this reason, we publish the Long-term Thermal Drift (LTD) dataset. LTD consists of thermal surveillance imaging from a single location across 8 months. Along with thermal images we provide relevant metadata such as weather, the day/night cycle, and scene activity. In this paper, we use the metadata for in-depth analysis of the causal and correlational relationships between environmental variables and the performance of selected computer vision algorithms used for anomaly and object detection. Long-term performance is shown to be most correlated with temperature, humidity, the day/night cycle, and scene activity level. This suggests that the coverage of these variables should be prioritised when building datasets for similar applications. As a baseline, we propose to mitigate the impact of concept drift by first detecting points in time where drift occurs. At this point, we collect additional data that is used to retraining the models. This improves later performance by an average of 25% across all tested algorithms.",Datasets & Benchmarks,NeurIPS,2021,Poster,Ivan Nikolov;Mark Philip Philipsen;Jinsong Liu;Jacob Velling Dueholm;Anders Skaarup Johansen;Kamal Nasrollahi;Thomas B. Moeslund,True,https://openreview.net/pdf?id=LjjqegBNtPi
LlCQWh8-pwK,A Procedural World Generation Framework for Systematic Evaluation of Continual Learning,"Several families of continual learning techniques have been proposed to alleviate catastrophic interference in deep neural network training on non-stationary data. However, a comprehensive comparison and analysis of limitations remains largely open due to the inaccessibility to suitable datasets. Empirical examination not only varies immensely between individual works, it further currently relies on contrived composition of benchmarks through subdivision and concatenation of various prevalent static vision datasets. In this work, our goal is to bridge this gap by introducing a computer graphics simulation framework that repeatedly renders only upcoming urban scene fragments in an endless real-time procedural world generation process. At its core lies a modular parametric generative model with adaptable generative factors. The latter can be used to flexibly compose data streams, which significantly facilitates a detailed analysis and allows for effortless investigation of various continual learning schemes. ",Datasets & Benchmarks,NeurIPS,2021,Poster,Timm Hess;Martin Mundt;Iuliia Pliushch;Visvanathan Ramesh,False,https://openreview.net/pdf?id=LlCQWh8-pwK
LqRSh6V0vR,Reinforcement Learning Benchmarks for Traffic Signal Control,"We propose a toolkit for developing and comparing reinforcement learning (RL)-based traffic signal controllers. The toolkit includes implementation of state-of-the-art deep-RL algorithms for signal control along with benchmark control problems that are based on realistic traffic scenarios. Importantly, the toolkit allows a first-of-its-kind comparison between state-of-the-art RL-based signal controllers while providing benchmarks for future comparisons. Consequently, we compare and report the relative performance of current RL algorithms. The experimental results suggest that previous algorithms are not robust to varying sensing assumptions and non-stylized intersection layouts. When more realistic signal layouts and advanced sensing capabilities are assumed, a distributed deep-Q learning approach is shown to outperform previously reported state-of-the-art algorithms in many cases.",Datasets & Benchmarks,NeurIPS,2021,Poster,James Ault;Guni Sharon,False,https://openreview.net/pdf?id=LqRSh6V0vR
MAWgLrYvMs0,"The CPD Data Set: Personnel, Use of Force, and Complaints in the Chicago Police Department","The lack of accessibility to data on policing has severely limited researchers’ ability to conduct thorough quantitative analyses on police activity and behavior, particularly with regard to predicting and explaining police violence. In the present work, we provide a new dataset that contains information on the personnel, activities, use of force, and complaints in the Chicago Police Department (CPD). The raw data, obtained from the CPD via a series of requests under the Freedom of Information Act (FOIA), consists of 35 unlinked, inconsistent, and undocumented spreadsheets. Our paper provides a cleaned, linked, and documented version of this data that can be reproducibly generated via open source code. We provide a detailed description of the dataset contents, the procedures for cleaning the data, and summary statistics. The data have a rich variety of uses, such as prediction (e.g., predicting misconduct from officer traits, experience, and assigned units), network analysis (e.g., detecting communities within the social network of officers co-listed on complaints), spatiotemporal data analysis (e.g., investigating patterns of officer shooting events), causal inference (e.g., tracking the effects of new disciplinary practices, new training techniques, and new oversight on complaints and use of force), and much more. Access to this dataset will enable the machine learning community to meaningfully engage with the problem of police violence.",Datasets & Benchmarks,NeurIPS,2021,Poster,Thibaut Horel;Lorenzo Masoero;Raj Agrawal;Daria Roithmayr;Trevor Campbell,True,https://openreview.net/pdf?id=MAWgLrYvMs0
MQlMIrm3Hv5,Benchmarking the Robustness of Spatial-Temporal Models Against Corruptions,"The state-of-the-art deep neural networks are vulnerable to common corruptions (e.g., input data degradations, distortions, and disturbances caused by weather changes, system error, and processing). While much progress has been made in analyzing and improving the robustness of models in image understanding, the robustness in video understanding is largely unexplored. In this paper, we establish a corruption robustness benchmark, Mini Kinetics-C and Mini SSV2-C, which considers temporal corruptions beyond spatial corruptions in images. We make the first attempt to conduct an exhaustive study on the corruption robustness of established CNN-based and Transformer-based spatial-temporal models. The study provides some guidance on robust model design and training: Transformer-based model performs better than CNN-based models on corruption robustness; the generalization ability of spatial-temporal models implies robustness against temporal corruptions; model corruption robustness (especially robustness in the temporal domain) enhances with computational cost and model capacity, which may contradict the current trend of improving the computational efficiency of models. Moreover, we find the robustness intervention for image-related tasks (e.g., training models with noise) may not work for spatial-temporal models. Our codes are available on https://github.com/Newbeeyoung/Video-Corruption-Robustness.
",Datasets & Benchmarks,NeurIPS,2021,Poster,Chenyu Yi;SIYUAN YANG;Haoliang Li;Yap-peng Tan;Alex Kot,True,https://openreview.net/pdf?id=MQlMIrm3Hv5
Mop0QMT5yei,A Benchmark of Discovering Drug-Target Interaction from Biomedical Literature,"As millions of papers come out every year in the biomedical domain, automatic knowledge discovery (KD) from biomedical literature becomes an urgent demand in the industry. While KD in the biomedical domain attracts much research attention in recent years, the lack of benchmark datasets significantly hinders its progress. In this work, we create a dataset, KD-DTI, for discovering <drug, target, interaction> triplets from literature, which is one of the most important KD tasks in the biomedical domain. KD-DTI contains 14k unique biomedical papers, each of which is associated with at least one drug, target, interaction triplet. We also provide a semi-supervised dataset with 139k unique papers. We present and analyze multiple solutions, including several extractive/generative models and two data enhancement methods. The results show that the performance of those models is far from industry demand, indicating that the dataset presents a challenging research problem for the community. The dataset will be freely accessible after the review process.",Datasets & Benchmarks,NeurIPS,2021,Reject,Yutai Hou;Yingce Xia;Lijun Wu;Shufang Xie;Yang Fan;Jinhua Zhu;Wanxiang Che;Tao Qin;Tie-Yan Liu,True,https://openreview.net/pdf?id=Mop0QMT5yei
MudT1U2eIY,NeRS: Neural Reflectance Surfaces for Sparse-view 3D Reconstruction in the Wild,"Recent history has seen a tremendous growth of work exploring implicit representations of geometry and radiance, popularized through Neural Radiance Fields (NeRF).  Such works are fundamentally based on a (implicit) {\\\\em volumetric} representation of occupancy, allowing them to model diverse scene structure including translucent objects and atmospheric obscurants. But because the vast majority of real-world scenes are composed of well-defined surfaces, we introduce a {\\\\em surface} analog of such implicit models called Neural Reflectance Surfaces (NeRS). NeRS learns a neural shape representation of a closed surface that is diffeomorphic to a sphere, guaranteeing water-tight reconstructions. Even more importantly, surface parameterizations allow NeRS to learn (neural) bidirectional surface reflectance functions (BRDFs) that factorize view-dependent appearance into environmental illumination, diffuse color (albedo), and specular “shininess.” Finally, rather than illustrating our results on synthetic scenes or controlled in-the-lab capture, we assemble a novel dataset of multi-view images from online marketplaces for selling goods. Such “in-the-wild” multi-view image sets pose a number of challenges, including a small number of views with unknown/rough camera estimates. We demonstrate that surface-based neural reconstructions enable learning from such data, outperforming volumetric neural rendering-based reconstructions. We hope that NeRS serves as a first step toward building scalable, high-quality libraries of real-world shape, materials, and illumination.",main,NeurIPS,2021,Poster,Jason Y. Zhang;Gengshan Yang;Shubham Tulsiani;Deva Ramanan,True,https://openreview.net/pdf?id=MudT1U2eIY
NCDMYD2y5kK,Deep Extrapolation for Attribute-Enhanced Generation,"Attribute extrapolation in sample generation is challenging for deep neural networks operating beyond the training distribution. We formulate a new task for extrapolation in sequence generation, focusing on natural language and proteins, and propose GENhance, a generative framework that enhances attributes through a learned latent space. Trained on movie reviews and a computed protein stability dataset, GENhance can generate strongly-positive text reviews and highly stable protein sequences without being exposed to similar data during training. We release our benchmark tasks and models to contribute to the study of generative modeling extrapolation and data-driven design in biology and chemistry.",main,NeurIPS,2021,Poster,Alvin Chan;Ali Madani;Ben Krause;Nikhil Naik,True,https://openreview.net/pdf?id=NCDMYD2y5kK
NYgt9vcdyjm,GraphGT: Machine Learning Datasets for Graph Generation and Transformation,"Graph generation has shown great potential in applications like network design and mobility synthesis and is one of the fastest-growing domains in machine learning for graphs. Despite the success of graph generation, the corresponding real-world datasets are few and limited to areas such as molecules and citation networks. To fill the gap, we introduce GraphGT, a large dataset collection for graph generation and transformation problem, which contains 36 datasets from 9 domains across 6 subjects. To assist the researchers with better explorations of the datasets, we provide a systemic review and classification of the datasets based on research tasks, graph types, and application domains. We have significantly (re)processed all the data from different domains to fit the unified framework of graph generation and transformation problems. In addition, GraphGT provides an easy-to-use graph generation pipeline that simplifies the process for graph data loading, experimental setup and model evaluation. Finally, we compare the performance of popular graph generative models in 16 graph generation and 17 graph transformation datasets, showing the great power of GraphGT in differentiating and evaluating model capabilities and drawbacks. GraphGT has been regularly updated and welcomes inputs from the community. GraphGT is publicly available at \\\\url{https://graphgt.github.io/} and can also be accessed via an open Python library.",Datasets & Benchmarks,NeurIPS,2021,Poster,Yuanqi Du;Shiyu Wang;Xiaojie Guo;Hengning Cao;Shujie Hu;Junji Jiang;Aishwarya Varala;Abhinav Angirekula;Liang Zhao,True,https://openreview.net/pdf?id=NYgt9vcdyjm
NbaEmFm2mUW,Hierarchical Skills for Efficient Exploration,"In reinforcement learning, pre-trained low-level skills have the potential to greatly facilitate exploration. However, prior knowledge of the downstream task is required to strike the right balance between generality (fine-grained control) and specificity (faster learning) in skill design. In previous work on continuous control, the sensitivity of methods to this trade-off has not been addressed explicitly, as locomotion provides a suitable prior for navigation tasks, which have been of foremost interest. In this work, we analyze this trade-off for low-level policy pre-training with a new benchmark suite of  diverse, sparse-reward tasks for bipedal robots. We alleviate the need for prior knowledge by proposing a hierarchical skill learning framework that acquires skills of varying complexity in an unsupervised manner. For utilization on downstream tasks, we present a three-layered hierarchical learning algorithm to automatically trade off between general and specific skills as required by the respective task. In our experiments, we show that our approach performs this trade-off effectively and achieves better results than current state-of-the-art methods for end-to-end hierarchical reinforcement learning and unsupervised skill discovery.",main,NeurIPS,2021,Poster,Jonas Gehring;Gabriel Synnaeve;Andreas Krause;Nicolas Usunier,True,https://openreview.net/pdf?id=NbaEmFm2mUW
Nc2uduhU9qa,EEGEyeNet: a Simultaneous Electroencephalography and Eye-tracking Dataset and Benchmark for Eye Movement Prediction,"We present a new dataset and benchmark with the goal of advancing research in the intersection of brain activities and eye movements. Our dataset, EEGEyeNet, consists of simultaneous Electroencephalography (EEG) and Eye-tracking (ET) recordings from 356 different subjects collected from three different experimental paradigms. Using this dataset, we also propose a benchmark to evaluate gaze prediction from EEG measurements. The benchmark consists of three tasks with an increasing level of difficulty: left-right, angle-amplitude and absolute position. We run extensive experiments on this benchmark in order to provide solid baselines, both based on classical machine learning models and on large neural networks. We release our complete code and data and provide a simple and easy-to-use interface to evaluate new methods.",Datasets & Benchmarks,NeurIPS,2021,Poster,Ard Kastrati;Martyna Beata Plomecka;Damian Pascual;Lukas Wolf;Victor Gillioz;Roger Wattenhofer;Nicolas Langer,True,https://openreview.net/pdf?id=Nc2uduhU9qa
NevK78-K4bZ,The Multi-Agent Behavior Dataset: Mouse Dyadic Social Interactions,"Multi-agent behavior modeling aims to understand the interactions that occur between agents. We present a multi-agent dataset from behavioral neuroscience, the Caltech Mouse Social Interactions (CalMS21) Dataset. Our dataset consists of trajectory data of social interactions, recorded from videos of freely behaving mice in a standard resident-intruder assay. To help accelerate behavioral studies, the CalMS21 dataset provides benchmarks to evaluate the performance of automated behavior classification methods in three settings: (1) for training on large behavioral datasets all annotated by a single annotator, (2) for style transfer to learn inter-annotator differences in behavior definitions, and (3) for learning of new behaviors of interest given limited training data. The dataset consists of 6 million frames of unlabeled tracked poses of interacting mice, as well as over 1 million frames with tracked poses and corresponding frame-level behavior annotations. The challenge of our dataset is to be able to classify behaviors accurately using both labeled and unlabeled tracking data, as well as being able to generalize to new settings. ",Datasets & Benchmarks,NeurIPS,2021,Poster,Jennifer J. Sun;Tomomi Karigo;Dipam Chakraborty;Sharada Mohanty;Benjamin Wild;Quan Sun;Chen Chen;David Anderson;Pietro Perona;Yisong Yue;Ann Kennedy,True,https://openreview.net/pdf?id=NevK78-K4bZ
NfTU-wN8Uo,$\\\\texttt{RP-Mod}\\\\ \\\\&\\\\ \\\\texttt{RP-Crowd:}$ Moderator- and Crowd-Annotated German News Comment Datasets,"Abuse and hate are penetrating social media and many comment sections of news media companies. To prevent losing readers who get appalled by inappropriate texts, these platform providers invest considerable efforts to moderate user-generated contributions. This is further enforced by legislative actions, which make non-clearance of these comments a punishable action. While (semi-)automated solutions using Natural Language Processing and advanced Machine Learning techniques are getting increasingly sophisticated, the domain of abusive language detection still struggles as large non-English and well-curated datasets are scarce or not publicly available. With this work, we publish and analyse the largest annotated German abusive language comment datasets to date. In contrast to existing datasets, we achieve a high labeling standard by conducting a thorough crowd-based annotation study that complements professional moderators' decisions, which are also included in the dataset. We compare and cross-evaluate the performance of baseline algorithms and state-of-the-art transformer-based language models, which are fine-tuned on our datasets and an existing alternative, showing the usefulness for the community.",Datasets & Benchmarks,NeurIPS,2021,Poster,Dennis Assenmacher;Marco Niemann;Kilian Müller;Moritz Seiler;Dennis M Riehle;Heike Trautmann,True,https://openreview.net/pdf?id=NfTU-wN8Uo
NxWUnvwFV4,Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,"Adversarial attacks on graphs have posed a major threat to the robustness of graph machine learning (GML) models. Naturally, there is an ever-escalating arms race between attackers and defenders. However, the strategies behind both sides are often not fairly compared under the same and realistic conditions. To bridge this gap, we present the Graph Robustness Benchmark (GRB) with the goal of providing a scalable, unified, modular, and reproducible evaluation for the adversarial robustness of GML models. GRB standardizes the process of attacks and defenses by 1) developing scalable and diverse datasets, 2) modularizing the attack and defense implementations, and 3) unifying the evaluation protocol in refined scenarios. By leveraging the modular GRB pipeline,  the end-users can focus on the development of robust GML models with automated data processing and experimental evaluations. To support open and reproducible research on graph adversarial learning, GRB also hosts public leaderboards for different scenarios. 
As a starting point, we provide various baseline experiments to benchmark the state-of-the-art techniques. GRB is an open-source benchmark and all datasets, code, and leaderboards are available at https://cogdl.ai/grb/home. ",Datasets & Benchmarks,NeurIPS,2021,Poster,Qinkai Zheng;Xu Zou;Yuxiao Dong;Yukuo Cen;Da Yin;Jiarong Xu;Yang Yang;Jie Tang,True,https://openreview.net/pdf?id=NxWUnvwFV4
O24OhmqpJtP,HPO-B: A Large-Scale Reproducible Benchmark for Black-Box HPO based on OpenML,"Hyperparameter optimization (HPO) is a core problem for the machine learning community and remains largely unsolved due to the significant computational resources required to evaluate hyperparameter configurations. As a result, a series of recent related works have focused on the direction of transfer learning for quickly fine-tuning hyperparameters on a dataset. Unfortunately, the community does not have a common large-scale benchmark for comparing HPO algorithms. Instead, the de facto practice consists of empirical protocols on arbitrary small-scale meta-datasets that vary inconsistently across publications, making reproducibility a challenge. To resolve this major bottleneck and enable a fair and fast comparison of black-box HPO methods on a level playing field, we propose HPO-B, a new large-scale benchmark in the form of a collection of meta-datasets. Our benchmark is assembled and preprocessed from the OpenML repository and consists of 176 search spaces (algorithms) evaluated sparsely on 196 datasets with a total of 6.4 million hyperparameter evaluations. For ensuring reproducibility on our benchmark, we detail explicit experimental protocols, splits, and evaluation measures for comparing methods for both non-transfer, as well as, transfer learning HPO.",Datasets & Benchmarks,NeurIPS,2021,Poster,Sebastian Pineda Arango;Hadi Samer Jomaa;Martin Wistuba;Josif Grabocka,True,https://openreview.net/pdf?id=O24OhmqpJtP
OCrD8ycKjG,OpenML Benchmarking Suites,"Machine learning research depends on objectively interpretable, comparable, and reproducible algorithm benchmarks. We advocate the use of curated, comprehensive suites of machine learning tasks to standardize the setup, execution, and reporting of benchmarks. We enable this through software tools that help to create and leverage these benchmarking suites. These are seamlessly integrated into the OpenML platform, and accessible through interfaces in Python, Java, and R. OpenML benchmarking suites (a) are easy to use through standardized data formats, APIs, and client libraries; (b) come with extensive meta-information on the included datasets; and (c) allow benchmarks to be shared and reused in future studies. We then present a first, carefully curated and practical benchmarking suite for classification: the OpenML Curated Classification benchmarking suite 2018 (OpenML-CC18). Finally, we discuss use cases and applications which demonstrate the usefulness of OpenML benchmarking suites and the OpenML-CC18 in particular.",Datasets & Benchmarks,NeurIPS,2021,Poster,Bernd Bischl;Giuseppe Casalicchio;Matthias Feurer;Pieter Gijsbers;Frank Hutter;Michel Lang;Rafael Gomes Mantovani;Jan N. van Rijn;Joaquin Vanschoren,True,https://openreview.net/pdf?id=OCrD8ycKjG
OFiGmksrSz1,SegmentMeIfYouCan: A Benchmark for Anomaly Segmentation,"State-of-the-art semantic or instance segmentation deep neural networks (DNNs) are usually trained on a closed set of semantic classes. As such, they are ill-equipped to handle previously-unseen objects. 
However, detecting and localizing such objects is crucial for safety-critical applications such as perception for automated driving, especially if they appear on the road ahead. While some methods have tackled the tasks of anomalous or out-of-distribution object segmentation, progress remains slow, in large part due to the lack of solid benchmarks; existing datasets either consist of synthetic data, or suffer from label inconsistencies. In this paper, we bridge this gap by introducing the ""SegmentMeIfYouCan"" benchmark. Our benchmark addresses two tasks: Anomalous object segmentation, which considers any previously-unseen object category; and road obstacle segmentation, which focuses on any object on the road, may it be known or unknown.
We provide two corresponding datasets together with a test suite performing an in-depth method analysis, considering both established pixel-wise performance metrics and recent component-wise ones, which are insensitive to object sizes. We empirically evaluate multiple state-of-the-art baseline methods, including several models specifically designed for anomaly / obstacle segmentation, on our datasets and on public ones, using our test suite.
The anomaly and obstacle segmentation results show that our datasets contribute to the diversity and difficulty of both data landscapes.",Datasets & Benchmarks,NeurIPS,2021,Poster,Robin Chan;Krzysztof Lis;Svenja Uhlemeyer;Hermann Blum;Sina Honari;Roland Siegwart;Pascal Fua;Mathieu Salzmann;Matthias Rottmann,True,https://openreview.net/pdf?id=OFiGmksrSz1
OTnqQUEwPKu,Benchmarking Bias Mitigation Algorithms in Representation Learning through Fairness Metrics,"With the recent expanding attention of machine learning researchers and practitioners to fairness, there is a void of a common framework to analyze and compare the capabilities of proposed models in deep representation learning. In this paper, we evaluate different fairness methods trained with deep neural networks on a common synthetic dataset and a real-world dataset to obtain better insights on how these methods work. In particular, we train about 3000 different models in various setups, including imbalanced and correlated data configurations, to verify the limits of the current models and better understand in which setups they are subject to failure. Our results show that the bias of models increase as datasets become more imbalanced or datasets attributes become more correlated, the level of dominance of correlated sensitive dataset features impact bias, and the sensitive information remains in the latent representation even when bias-mitigation algorithms are applied. Overall, we present a dataset, propose various challenging evaluation setups, and rigorously evaluate recent promising bias-mitigation algorithms in a common framework and publicly release this benchmark, hoping the research community would take it as a common entry point for fair deep learning.",Datasets & Benchmarks,NeurIPS,2021,Poster,Charan Reddy;Deepak Sharma;Soroush Mehri;Adriana Romero-Soriano;Samira Shabanian;Sina Honari,True,https://openreview.net/pdf?id=OTnqQUEwPKu
Pq8FBz0gZHY,Generating Datasets of 3D Garments with Sewing Patterns,"Garments are ubiquitous in both real and many of the virtual worlds. They are highly deformable objects, exhibit an immense variety of designs and shapes, and yet, most garments are created from a set of regularly shaped flat pieces. Exploration of garment structure presents a peculiar case for an object structure estimation task and might prove useful for downstream tasks of neural 3D garment modeling and reconstruction by providing strong prior on garment shapes. To facilitate research in these directions, we propose a method for generating large synthetic datasets of 3D garment designs and their sewing patterns. Our method consists of a flexible description structure for specifying parametric sewing pattern templates and the automatic generation pipeline to produce garment 3D models with little-to-none manual intervention. To add realism, the pipeline additionally creates corrupted versions of the final meshes that imitate artifacts of 3D scanning.

With this pipeline, we created the first large-scale synthetic dataset of 3D garment models with their sewing patterns. The dataset contains more than 20000 garment design variations produced from 19 different base types. Seven of these garment types are specifically designed to target evaluation of the generalization across garment sewing pattern topologies.",Datasets & Benchmarks,NeurIPS,2021,Poster,Maria Korosteleva;Sung-Hee Lee,True,https://openreview.net/pdf?id=Pq8FBz0gZHY
Q0hm0_G1mpH,A Unified Few-Shot Classification Benchmark to Compare Transfer and Meta Learning Approaches,"Meta and transfer learning are two successful families of approaches to few-shot learning. Despite highly related goals, state-of-the-art advances in each family are measured largely in isolation of each other. As a result of diverging evaluation norms, a direct or thorough comparison of different approaches is challenging. To bridge this gap, we introduce a few-shot classification evaluation protocol named VTAB+MD with the explicit goal of facilitating sharing of insights from each community. We demonstrate its accessibility in practice by performing a cross-family study of the best transfer and meta learners which report on both a large-scale meta-learning benchmark (Meta-Dataset, MD), and a transfer learning benchmark (Visual Task Adaptation Benchmark, VTAB). We find that, on average, large-scale transfer methods (Big Transfer, BiT) outperform competing approaches on MD, even when trained only on ImageNet. In contrast, meta-learning approaches struggle to compete on VTAB when trained and validated on MD. However, BiT is not without limitations, and pushing for scale does not improve performance on highly out-of-distribution MD tasks. We hope that this work contributes to accelerating progress on few-shot learning research.",Datasets & Benchmarks,NeurIPS,2021,Poster,Vincent Dumoulin;Neil Houlsby;Utku Evci;Xiaohua Zhai;Ross Goroshin;Sylvain Gelly;Hugo Larochelle,False,https://openreview.net/pdf?id=Q0hm0_G1mpH
Q0zOIaec8HF,Benchmarking Multimodal AutoML for Tabular Data with Text Fields,"We consider the use of automated supervised learning systems for data tables that not only contain numeric/categorical columns, but one or more text fields as well. Here we assemble 18 multimodal data tables that each contain some text fields and stem from a real business application. Our publicly-available benchmark enables researchers to comprehensively evaluate their own methods for supervised learning with numeric, categorical, and text features. To ensure that any single modeling strategy which performs well over all 18 datasets will serve as a practical foundation for multimodal text/tabular AutoML, the diverse datasets in our benchmark vary greatly in: sample size, problem types (a mix of classification and regression tasks), number of features (with the number of text columns ranging from 1 to 28 between datasets), as well as how the predictive signal is decomposed between text vs. numeric/categorical features (and predictive interactions thereof). Over this benchmark, we evaluate various straightforward pipelines to model such data, including standard two-stage approaches where NLP is used to featurize the text such that AutoML for tabular data can then be applied. Compared with human data science teams, the fully automated methodology that performed best on our benchmark also manages to rank 1st place when fit to the raw text/tabular data in two MachineHack prediction competitions and 2nd place (out of 2380 teams) in Kaggle's Mercari Price Suggestion Challenge.",Datasets & Benchmarks,NeurIPS,2021,Poster,Xingjian Shi;Jonas Mueller;Nick Erickson;Mu Li;Alex Smola,True,https://openreview.net/pdf?id=Q0zOIaec8HF
Q9SKS5k8io,WRENCH: A Comprehensive Benchmark for Weak Supervision,"Recent Weak Supervision (WS)  approaches have had widespread success in easing the bottleneck of labeling training data for machine learning by synthesizing labels from multiple potentially noisy supervision sources. However, proper measurement and analysis of these approaches remain a challenge. First, datasets used in existing works are often private and/or custom, limiting standardization. Second, WS datasets with the same name and base data often vary in terms of the labels and weak supervision sources used, a significant ""hidden"" source of evaluation variance. Finally, WS studies often diverge in terms of the evaluation protocol and ablations used. To address these problems, we introduce a benchmark platform, WRENCH, for thorough and standardized evaluation of WS approaches. It consists of 22 varied real-world datasets for classification and sequence tagging; a range of real, synthetic, and procedurally-generated weak supervision sources; and a modular, extensible framework for WS evaluation, including implementations for popular WS methods. We use WRENCH to conduct extensive comparisons over more than 120 method variants to demonstrate its efficacy as a benchmark platform. The code is available at https://github.com/JieyuZ2/wrench.",Datasets & Benchmarks,NeurIPS,2021,Poster,Jieyu Zhang;Yue Yu;Yinghao Li;Yujing Wang;Yaming Yang;Mao Yang;Alexander Ratner,False,https://openreview.net/pdf?id=Q9SKS5k8io
QVeMBoRXAo_,WikiChurches: A Fine-Grained Dataset of Architectural Styles with Real-World Challenges,"We introduce a novel dataset for architectural style classification, consisting of 9,485 images of church buildings. Both images and style labels were sourced from Wikipedia. The dataset can serve as a benchmark for various research fields, as it combines numerous real-world challenges: fine-grained distinctions between classes based on subtle visual features, a comparatively small sample size, a highly imbalanced class distribution, a high variance of viewpoints, and a hierarchical organization of labels, where only some images are labeled at the most precise level.
In addition, we provide 631 bounding box annotations of characteristic visual features for 139 churches from four major categories. These annotations can, for example, be useful for research on fine-grained classification, where additional expert knowledge about distinctive object parts is often available.
Images and annotations are available at: https://doi.org/10.5281/zenodo.5166986",Datasets & Benchmarks,NeurIPS,2021,Poster,Björn Barz;Joachim Denzler,True,https://openreview.net/pdf?id=QVeMBoRXAo_
Qd_eU1wvJeu,"Addressing ""Documentation Debt"" in Machine Learning: A Retrospective Datasheet for BookCorpus","This paper contributes a formal case study in retrospective dataset documentation and pinpoints several problems with the influential BookCorpus dataset. Recent work has underscored the importance of dataset documentation in machine learning research, including by addressing ``documentation debt'' for datasets that have been used widely but documented sparsely. BookCorpus is one such dataset. Researchers have used BookCorpus to train OpenAI's GPT-N models and Google's BERT models, but little to no documentation exists about the dataset's motivation, composition, collection process, etc. We offer a retrospective datasheet with key context and information about BookCorpus, including several notable deficiencies. In particular, we find evidence that (1) BookCorpus violates copyright restrictions for many books, (2) BookCorpus contains thousands of duplicated books, and (3) BookCorpus exhibits significant skews in genre representation. We also find hints of other potential deficiencies that call for future research, such as lopsided author contributions. While more work remains, this initial effort to provide a datasheet for BookCorpus offers a cautionary case study and adds to growing literature that urges more careful, systematic documentation of machine learning datasets.",Datasets & Benchmarks,NeurIPS,2021,Poster,Jack Bandy;Nicholas Vincent,False,https://openreview.net/pdf?id=Qd_eU1wvJeu
QkOBP-aD1qA,QAConv: Question Answering on Informative Conversations,"This paper introduces QAConv, a new question answering (QA) dataset that uses conversations as a knowledge source. We focus on informative conversations, including business emails, panel discussions, and work channels. Unlike open-domain and task-oriented dialogues, these conversations are usually long, complex, asynchronous, and involve strong domain knowledge.  In total, we collect 34,204 QA pairs, including multi-span and unanswerable questions, from 10,259 selected conversations with both human-written and machine-generated questions. We segment long conversations into chunks and use a question generator and a dialogue summarizer as auxiliary tools to collect multi-hop questions. The dataset has two testing scenarios, chunk mode and full mode, depending on whether the grounded chunk is provided or retrieved from a large pool of conversations.  Experimental results show that state-of-the-art pretrained QA systems have limited zero-shot ability and tend to predict our questions as unanswerable. Finetuning such systems on our corpus can significantly improve up to 23.6\\\\% and 13.6\\\\% in both chunk mode and full mode, respectively.",Datasets & Benchmarks,NeurIPS,2021,Reject,Chien-Sheng Wu;Andrea Madotto;Wenhao Liu;Pascale Fung;Caiming Xiong,True,https://openreview.net/pdf?id=QkOBP-aD1qA
QkljT4mrfs,Partial success in closing the gap between human and machine vision,"A few years ago, the first CNN surpassed human performance on ImageNet. However, it soon became clear that machines lack robustness on more challenging test cases, a major obstacle towards deploying machines ""in the wild"" and towards obtaining better computational models of human visual perception. Here we ask: Are we making progress in closing the gap between human and machine vision? To answer this question, we tested human observers on a broad range of out-of-distribution (OOD) datasets, recording 85,120 psychophysical trials across 90 participants. We then investigated a range of promising machine learning developments that crucially deviate from standard supervised CNNs along three axes: objective function (self-supervised, adversarially trained, CLIP language-image training), architecture (e.g. vision transformers), and dataset size (ranging from 1M to 1B).

Our findings are threefold. (1.) The longstanding distortion robustness gap between humans and CNNs is closing, with the best models now exceeding human feedforward performance on most of the investigated OOD datasets. (2.) There is still a substantial image-level consistency gap, meaning that humans make different errors than models. In contrast, most models systematically agree in their categorisation errors, even substantially different ones like contrastive self-supervised vs. standard supervised models. (3.) In many cases, human-to-model consistency improves when training dataset size is increased by one to three orders of magnitude. Our results give reason for cautious optimism: While there is still much room for improvement, the behavioural difference between human and machine vision is narrowing. In order to measure future progress, 17 OOD datasets with image-level human behavioural data and evaluation code are provided as a toolbox and benchmark at: https://github.com/bethgelab/model-vs-human/",main,NeurIPS,2021,Oral,Robert Geirhos;Kantharaju Narayanappa;Benjamin Mitzkus;Tizian Thieringer;Matthias Bethge;Felix A. Wichmann;Wieland Brendel,True,https://openreview.net/pdf?id=QkljT4mrfs
QzNHE7QHhut,The Tufts fNIRS Mental Workload Dataset & Benchmark for Brain-Computer Interfaces that Generalize,"Functional near-infrared spectroscopy (fNIRS) promises a non-intrusive way to measure real-time brain activity and build responsive brain-computer interfaces. A primary barrier to realizing this technology's potential has been that observed fNIRS signals vary significantly across human users. Building models that generalize well to never-before-seen users has been difficult; a large amount of subject-specific data has been needed to train effective models. To help overcome this barrier, we introduce the largest open-access dataset of its kind, containing multivariate fNIRS recordings from 68 participants, each with labeled segments indicating four possible mental workload intensity levels. Labels were collected via a controlled setting in which subjects performed standard n-back tasks to induce desired working memory levels. We propose a benchmark analysis of this dataset with a standardized training and evaluation protocol, which allows future researchers to report comparable numbers and fairly assess generalization potential while avoiding any overlap or leakage between train and test data. Using this dataset and benchmark, we show how models trained using abundant fNIRS data from many other participants can effectively classify a new target subject's data, thus reducing calibration and setup time for new subjects. We further show how performance improves as the size of the available dataset grows, while also analyzing error rates across key subpopulations to audit equity concerns. We share our open-access Tufts fNIRS to Mental Workload (fNIRS2MW) dataset and open-source code as a step toward advancing brain computer interfaces.
",Datasets & Benchmarks,NeurIPS,2021,Poster,Zhe Huang;Liang Wang;Giles Blaney;Christopher Slaughter;Devon McKeon;Ziyu Zhou;Robert Jacob;Michael C Hughes,True,https://openreview.net/pdf?id=QzNHE7QHhut
R07XwJPmgpl,OmniPrint: A Configurable Printed Character Synthesizer,"We introduce OmniPrint, a synthetic data generator of isolated printed characters, geared toward machine learning research. It draws inspiration from famous datasets such as MNIST, SVHN and Omniglot, but offers the capability of generating a wide variety of printed characters from various languages, fonts and styles, with customized distortions. We include 935 fonts from 27 scripts and many types of distortions. As a proof of concept, we show various use cases, including an example of meta-learning dataset designed for the upcoming MetaDL NeurIPS 2021 competition. OmniPrint is available at https://github.com/SunHaozhe/OmniPrint.",Datasets & Benchmarks,NeurIPS,2021,Poster,Haozhe Sun;Wei-Wei Tu;Isabelle M Guyon,False,https://openreview.net/pdf?id=R07XwJPmgpl
R7vr14ffhF9,Synthetic Benchmarks for Scientific Research in Explainable Machine Learning,"As machine learning models grow more complex and their applications become more high-stakes, tools for explaining model predictions have become increasingly important. This has spurred a flurry of research in model explainability and has given rise to feature attribution methods such as LIME and SHAP. Despite their widespread use, evaluating and comparing different feature attribution methods remains challenging: evaluations ideally require human studies, and empirical evaluation metrics are often data-intensive or computationally prohibitive on real-world datasets. In this work, we address this issue by releasing XAI-BENCH: a suite of synthetic datasets along with a library for benchmarking feature attribution algorithms. Unlike real-world datasets, synthetic datasets allow the efficient computation of conditional expected values that are needed to evaluate ground-truth Shapley values and other metrics. The synthetic datasets we release offer a wide variety of parameters that can be configured to simulate real-world data. We demonstrate the power of our library by benchmarking popular explainability techniques across several evaluation metrics and across a variety of settings. The versatility and efficiency of our library will help researchers bring their explainability methods from development to deployment. Our code is available at https://github.com/abacusai/xai-bench.",Datasets & Benchmarks,NeurIPS,2021,Poster,Yang Liu;Sujay Khandagale;Colin White;Willie Neiswanger,True,https://openreview.net/pdf?id=R7vr14ffhF9
R8CwidgJ0yT,The People’s Speech: A Large-Scale Diverse English Speech Recognition Dataset for Commercial Usage,"The People’s Speech is a free-to-download 31,400-hour and growing supervised conversational English speech recognition dataset licensed for academic and commercial usage under CC-BY-SA. The data is collected via searching the Internet for appropriately licensed audio data with existing transcriptions. We describe our data collection methodology and release our data collection system under the Apache2.0 license. We show that a model trained on this dataset achieves a 32.17% word error rate on Librispeech’s test-clean test set.  Finally, we discuss the legal and ethical issues surrounding the creation of a sizable machine learning corpora and plans for continued maintenance of the project under MLCommons’s sponsorship.",Datasets & Benchmarks,NeurIPS,2021,Poster,Daniel Galvez;Greg Diamos;Juan Manuel Ciro Torres;Juan Felipe Cerón;Keith Achorn;Anjali Gopi;David Kanter;Max Lam;Mark Mazumder;Vijay Janapa Reddi,True,https://openreview.net/pdf?id=R8CwidgJ0yT
R_CuaMJKvDs,The Cross-environment Hyperparameter Setting Benchmark for Reinforcement Learning,"This paper introduces a new benchmark, the Single Hyperparameter Benchmark, that allows comparison of RL algorithms across environments using only a single hyperparameter, encouraging algorithmic development which is insensitive to hyperparameters. We demonstrate that the benchmark is robust to statistical noise and obtains qualitatively similar results across repeated applications, even when using a small number of samples. This robustness makes the benchmark computationally cheap to apply, allowing statistically sound insights at a low cost. To demonstrate the applicability of the SHB to modern RL algorithms on challenging environments, we provide a novel empirical study of an open question in the continuous control literature. We show, with high confidence, that there is no meaningful difference in performance between Ornstein-Uhlenbeck noise and uncorrelated Gaussian noise for exploration with the DDPG algorithm across the entire DMControl suite.",Datasets & Benchmarks,NeurIPS,2021,Reject,Andrew Patterson;Samuel Neumann;Raksha Kumaraswamy;Martha White;Adam M White,False,https://openreview.net/pdf?id=R_CuaMJKvDs
RmuXDtjDhG,Causal Abstractions of Neural Networks,"Structural analysis methods (e.g., probing and feature attribution) are increasingly important tools for neural network analysis. We propose a new structural analysis method grounded in a formal theory of causal abstraction that provides rich characterizations of model-internal representations and their roles in input/output behavior. In this method, neural representations are aligned with variables in interpretable causal models, and then interchange interventions are used to experimentally verify that the neural representations have the causal properties of their aligned variables. We apply this method in a case study to analyze neural models trained on Multiply Quantified Natural Language Inference (MQNLI) corpus, a highly complex NLI dataset that was constructed with a tree-structured natural logic causal model. We discover that a BERT-based model with state-of-the-art performance successfully realizes parts of the natural logic model’s causal structure, whereas a simpler baseline model fails to show any such structure, demonstrating that neural representations encode the compositional structure of MQNLI examples.",main,NeurIPS,2021,Poster,Atticus Geiger;Hanson Lu;Thomas F Icard;Christopher Potts,True,https://openreview.net/pdf?id=RmuXDtjDhG
RqAzAoL8BER,Fine-Grained Zero-Shot Learning with DNA as Side Information,"Fine-grained zero-shot learning task requires some form of side-information to transfer discriminative information from seen to unseen classes. As manually annotated visual attributes are extremely costly and often impractical to obtain for a large number of classes, in this study we use DNA as a side information for the first time for fine-grained zero-shot classification of species. Mitochondrial DNA plays an important role as a genetic marker in evolutionary biology and has been used to achieve near perfect accuracy in species classification of living organisms. We implement a simple hierarchical Bayesian model that uses DNA information to establish the hierarchy in the image space and employs local priors to define surrogate classes for unseen ones. On the benchmark CUB dataset we show that DNA can be equally promising, yet in general a more accessible alternative than word vectors as a side information. This is especially important as obtaining robust word representations for fine-grained species names is not a practicable goal when information about these species in free-form text is limited. On a newly compiled fine-grained insect dataset that uses DNA information from over a thousand species we show that the Bayesian approach outperforms state-of-the-art by a wide margin. ",main,NeurIPS,2021,Poster,Sarkhan Badirli;Zeynep Akata;George Mohler;Christine Picard;Murat Dundar,True,https://openreview.net/pdf?id=RqAzAoL8BER
Rtquf4Jk0jN,ReaSCAN: Compositional Reasoning in Language Grounding,"The ability to compositionally map language to referents, relations, and actions is an essential component of language understanding. The recent gSCAN dataset (Ruis et al. 2020, NeurIPS) is an inspiring attempt to assess the capacity of models to learn this kind of grounding in scenarios involving navigational instructions. However, we show that gSCAN's highly constrained design means that it does not require compositional interpretation and that many details of its instructions and scenarios are not required for task success. To address these limitations, we propose ReaSCAN, a benchmark dataset that builds off gSCAN but requires compositional language interpretation and reasoning about entities and relations. We assess two models on ReaSCAN: a multi-modal baseline and a state-of-the-art graph convolutional neural model. These experiments show that ReaSCAN is substantially harder than gSCAN for both neural architectures. This suggests that ReaSCAN can serve as a valuable benchmark for advancing our understanding of models' compositional generalization and reasoning capabilities.",Datasets & Benchmarks,NeurIPS,2021,Poster,Zhengxuan Wu;Elisa Kreiss;Desmond Ong;Christopher Potts,True,https://openreview.net/pdf?id=Rtquf4Jk0jN
SSKZPJCt7B,RobustBench: a standardized adversarial robustness benchmark,"As a research community, we are still lacking a systematic understanding of the progress on adversarial robustness which often makes it hard to identify the most promising ideas in training robust models. A key challenge in benchmarking robustness is that its evaluation is often error-prone leading to robustness overestimation. Our goal is to establish a standardized benchmark of adversarial robustness, which as accurately as possible reflects the robustness of the considered models within a reasonable computational budget. To this end, we start by considering the image classification task and introduce restrictions (possibly loosened in the future) on the allowed models. We evaluate adversarial robustness with AutoAttack, an ensemble of white- and black-box attacks, which was recently shown in a large-scale study to improve almost all robustness evaluations compared to the original publications. To prevent overadaptation of new defenses to AutoAttack, we welcome external evaluations based on adaptive attacks, especially where AutoAttack flags a potential overestimation of robustness. Our leaderboard, hosted at https://robustbench.github.io/, contains evaluations of 120+ models and aims at reflecting the current state of the art in image classification on a set of well-defined tasks in $\\\\ell_\\\\infty$- and $\\\\ell_2$-threat models and on common corruptions, with possible extensions in the future. Additionally, we open-source the library https://github.com/RobustBench/robustbench that provides unified access to 80+ robust models to facilitate their downstream applications. Finally, based on the collected models, we analyze the impact of robustness on the performance on distribution shifts, calibration, out-of-distribution detection, fairness, privacy leakage, smoothness, and transferability. ",Datasets & Benchmarks,NeurIPS,2021,Poster,Francesco Croce;Maksym Andriushchenko;Vikash Sehwag;Edoardo Debenedetti;Nicolas Flammarion;Mung Chiang;Prateek Mittal;Matthias Hein,False,https://openreview.net/pdf?id=SSKZPJCt7B
SnC9rUeqiqd,HiRID-ICU-Benchmark --- A Comprehensive Machine Learning Benchmark on High-resolution ICU Data,"The recent success of machine learning methods applied to time series collected from Intensive Care Units (ICU) exposes the lack of standardized machine learning benchmarks for developing and comparing such methods. While raw datasets, such as MIMIC-IV or eICU, can be freely accessed on Physionet, the choice of tasks and pre-processing is often chosen ad-hoc for each publication, limiting comparability across publications. In this work, we aim to improve this situation by providing a  benchmark covering a large spectrum of ICU-related tasks. Using the HiRID dataset, we define multiple clinically relevant tasks developed in collaboration with clinicians.  In addition, we provide a reproducible end-to-end pipeline to construct both data and labels. Finally, we provide an in-depth analysis of current state-of-the-art sequence modeling methods, highlighting some limitations of deep learning approaches for this type of data. With this benchmark, we hope to give the research community the possibility of a fair comparison of their work.",Datasets & Benchmarks,NeurIPS,2021,Poster,Hugo Yèche;Rita Kuznetsova;Marc Zimmermann;Matthias Hüser;Xinrui Lyu;Martin Faltys;Gunnar Ratsch,False,https://openreview.net/pdf?id=SnC9rUeqiqd
TAXFsg6ZaOl,FakeAVCeleb: A Novel Audio-Video Multimodal Deepfake Dataset,"While the significant advancements have made in the generation of deepfakes using deep learning technologies, its misuse is a well-known issue now. Deepfakes can cause severe security and privacy issues as they can be used to impersonate a person's identity in a video by replacing his/her face with another person's face. Recently, a new problem of generating synthesized human voice of a person is emerging, where AI-based deep learning models can synthesize any person's voice requiring just a few seconds of audio. With the emerging threat of impersonation attacks using deepfake audios and videos, a new generation of deepfake detectors is needed to focus on both video and audio collectively. To develop a competent deepfake detector, a large amount of high-quality data is typically required to capture real-world (or practical) scenarios.
Existing deepfake datasets either contain deepfake videos or audios, which are racially biased as well. As a result, it is critical to develop a high-quality video and audio deepfake dataset that can be used to detect both audio and video deepfakes simultaneously. To fill this gap, we propose a novel Audio-Video Deepfake dataset, FakeAVCeleb, which contains not only deepfake videos but also respective synthesized lip-synced fake audios. We generate this dataset using the current most popular deepfake generation methods. We selected real YouTube videos of celebrities with four ethnic backgrounds to develop a more realistic multimodal dataset that addresses racial bias, and further help develop multimodal deepfake detectors. We performed several experiments using state-of-the-art detection methods to evaluate our deepfake dataset and demonstrate the challenges and usefulness of our multimodal Audio-Video deepfake dataset.",Datasets & Benchmarks,NeurIPS,2021,Poster,Hasam Khalid;Shahroz Tariq;Minha Kim;Simon S. Woo,True,https://openreview.net/pdf?id=TAXFsg6ZaOl
TAdzPkgnnV8,The Tarteel Dataset: Crowd-Sourced and Labeled Quranic Recitation,"We propose a standard schema for paired Quranic audio and text datasets. We describe the collection, labeling, and validation of the Tarteel recitation dataset, the first large-scale dataset of Quranic recitation and accompanying Arabic text collected in a crowd-sourced manner. The dataset contains 25,000 audio clips totalling 67.39 hours of audio and represents a wide variety of recitation styles, proficiencies, and speeds. The data were collected over a period of six months from over 1,200 unique individuals of different ages, genders, and ethnicities. We describe the composition of the data and contributors, describe in detail how the data was collected and processed, and give some baseline performance for preliminary machine learning algorithms that were trained and evaluated on the dataset.",Datasets & Benchmarks,NeurIPS,2021,Reject,Hamzah I Khan;Abubakar Abid;Mohamed Medhat Moussa;Anas Abou-Allaban,True,https://openreview.net/pdf?id=TAdzPkgnnV8
TFEFvU0ZV6Q,"Baby Intuitions Benchmark (BIB): Discerning the goals, preferences, and actions of others","To achieve human-like common sense about everyday life, machine learning systems must understand and reason about the goals, preferences, and actions of other agents in the environment. By the end of their first year of life, human infants intuitively achieve such common sense, and these cognitive achievements lay the foundation for humans' rich and complex understanding of the mental states of others. Can machines achieve generalizable, commonsense reasoning about other agents like human infants? The Baby Intuitions Benchmark (BIB) challenges machines to predict the plausibility of an agent's behavior based on the underlying causes of its actions. Because BIB's content and paradigm are adopted from developmental cognitive science, BIB allows for direct comparison between human and machine performance. Nevertheless, recently proposed, deep-learning-based agency reasoning models fail to show infant-like reasoning, leaving BIB an open challenge.",main,NeurIPS,2021,Poster,Kanishk Gandhi;Gala Stojnic;Brenden M. Lake;Moira Rose Dillon,True,https://openreview.net/pdf?id=TFEFvU0ZV6Q
TSvj5dmuSd,Task Agnostic and Task Specific Self-Supervised Learning from Speech with LeBenchmark,"Self-Supervised Learning (SSL) has yielded remarkable improvements in many different domains including computer vision, natural language processing and speech processing by leveraging large amounts of unlabeled data. In the specific context of speech, however, and despite promising results, there exists a clear lack of standardization in the evaluation process for comprehensive comparisons of these models. This issue gets even worse with the investigation of SSL approaches for other languages than English. We present LeBenchmark, an open-source and reproducible framework for assessing SSL from French speech data. It includes documented, large-scale and heterogeneous corpora, seven pretrained SSL wav2vec 2.0 models shared with the community, and a clear evaluation protocol made of four downstream tasks along with their scoring scripts: automatic speech recognition, spoken language understanding, automatic speech translation and automatic emotion recognition. For the first time, SSL models are analyzed and compared on the latter domains both from a task-agnostic (i.e. frozen) and task-specific (i.e. fine-tuned w.r.t the downstream task) perspectives. We report state-of-the-art performance on most considered French tasks and provide a readable evaluation set-up for the development of future SSL models for speech processing.",Datasets & Benchmarks,NeurIPS,2021,Poster,Solène Evain;Ha Nguyen;Hang Le;Marcely Zanon Boito;Salima Mdhaffar;Sina Alisamir;Ziyi Tong;Natalia Tomashenko;Marco Dinarelli;Titouan Parcollet;Alexandre Allauzen;Yannick Estève;Benjamin Lecouteux;François Portet;Solange Rossato;Fabien Ringeval;Didier Schwab;laurent besacier,True,https://openreview.net/pdf?id=TSvj5dmuSd
TUplOmF8DsM,MQBench: Towards Reproducible and Deployable Model Quantization Benchmark,"Model quantization has emerged as an indispensable technique to accelerate deep learning inference. 
Although researchers continue to push the frontier of quantization algorithms, existing quantization work is often unreproducible and undeployable. 
This is because researchers do not choose consistent training pipelines and ignore the requirements for hardware deployments. 
In this work, we propose Model Quantization Benchmark (MQBench), a first attempt to evaluate, analyze, and benchmark the reproducibility and deployability for model quantization algorithms. 
We choose multiple different platforms for real-world deployments, including CPU, GPU, ASIC, DSP, and evaluate extensive state-of-the-art quantization algorithms under a unified training pipeline. 
MQBench acts like a bridge to connect the algorithm and the hardware. 
We conduct a comprehensive analysis and find considerable intuitive or counter-intuitive insights. By aligning up the training settings, we find existing algorithms have about-the-same performance on the conventional academic track. While for the hardware-deployable quantization, there is a huge accuracy gap and still a long way to go. Surprisingly, no existing algorithm wins every challenge in MQBench, and we hope this work could inspire future research directions.",Datasets & Benchmarks,NeurIPS,2021,Poster,Yuhang Li;Mingzhu Shen;Jian Ma;Yan Ren;Mingxin Zhao;Qi Zhang;Ruihao Gong;Fengwei Yu;Junjie Yan,False,https://openreview.net/pdf?id=TUplOmF8DsM
TlE6Ar1sRsR,Accurate Point Cloud Registration with Robust Optimal Transport,"This work investigates the use of robust optimal transport (OT) for shape matching. Specifically, we show that recent OT solvers improve both optimization-based and deep learning methods for point cloud registration, boosting accuracy at an affordable computational cost. This manuscript starts with a practical overview of modern OT theory. We then provide solutions to the main difficulties in using this framework for shape matching. Finally, we showcase the performance of transport-enhanced registration models on a wide range of challenging tasks: rigid registration for partial shapes; scene flow estimation on the Kitti dataset; and nonparametric registration of lung vascular trees between inspiration and expiration. Our OT-based methods achieve state-of-the-art results on Kitti and for the challenging lung registration task, both in terms of accuracy and scalability. We also release PVT1010, a new public dataset of 1,010 pairs of lung vascular trees with densely sampled points. This dataset provides a challenging use case for point cloud registration algorithms with highly complex shapes and deformations. Our work demonstrates that robust OT enables fast pre-alignment and fine-tuning for a wide range of registration models, thereby providing a new key method for the computer vision toolbox. Our code and dataset are available online at: https://github.com/uncbiag/robot.",main,NeurIPS,2021,Poster,Zhengyang Shen;Jean Feydy;Peirong Liu;Ariel Hernán Curiale;Ruben San Jose Estepar;Raul San Jose Estepar;Marc Niethammer,True,https://openreview.net/pdf?id=TlE6Ar1sRsR
Ud1K-l71AI2,Q-Pain: A Question Answering Dataset to Measure Social Bias in Pain Management,"Recent advances in Natural Language Processing (NLP), and specifically automated Question Answering (QA) systems, have demonstrated both impressive linguistic fluency and a pernicious tendency to reflect social biases. In this study, we introduce Q-Pain, a dataset for assessing bias in medical QA in the context of pain management, one of the most challenging forms of clinical decision-making. Along with the dataset, we propose a new, rigorous framework, including a sample experimental design, to measure the potential biases present when making treatment decisions. We demonstrate its use by assessing two reference Question-Answering systems, GPT-2 and GPT-3, and find statistically significant differences in treatment between intersectional race-gender subgroups, thus reaffirming the risks posed by AI in medical settings, and the need for datasets like ours to ensure safety before medical AI applications are deployed.",Datasets & Benchmarks,NeurIPS,2021,Poster,Cécile Logé;Emily Ross;David Yaw Amoah Dadey;Saahil Jain;Adriel Saporta;Andrew Y. Ng;Pranav Rajpurkar,True,https://openreview.net/pdf?id=Ud1K-l71AI2
Uk2mymgn_LZ,DABS: a Domain-Agnostic Benchmark for Self-Supervised Learning,"Self-supervised learning algorithms, including BERT and SimCLR, have enabled significant strides in fields like natural language processing, computer vision, and speech processing. However, these algorithms are domain-specific, meaning that new self-supervised learning algorithms must be developed for each new setting, including myriad healthcare, scientific, and multimodal domains. To catalyze progress toward domain-agnostic methods, we introduce DABS: a Domain-Agnostic Benchmark for Self-supervised learning. To perform well on DABS, an algorithm is evaluated on seven diverse domains: natural images, multichannel sensor data, English text, speech recordings, multilingual text, chest x-rays, and images with text descriptions. Each domain contains an unlabeled dataset for pretraining; the model is then is scored based on its downstream performance on a set of labeled tasks in the domain. We also present e-Mix and ShED: two baseline domain-agnostic algorithms; their relatively modest performance demonstrates that significant progress is needed before self-supervised learning is an out-of-the-box solution for arbitrary domains. Code for benchmark datasets and baseline algorithms is available at https://github.com/alextamkin/dabs.",Datasets & Benchmarks,NeurIPS,2021,Poster,Alex Tamkin;Vincent Liu;Rongfei Lu;Daniel Fein;Colin Schultz;Noah Goodman,False,https://openreview.net/pdf?id=Uk2mymgn_LZ
UuUbIYnHKO,An Empirical Study of Graph Contrastive Learning,"Graph Contrastive Learning (GCL) establishes a new paradigm for learning graph representations without human annotations. Although remarkable progress has been witnessed recently, the success behind GCL is still left somewhat mysterious. In this work, we first identify several critical design considerations within a general GCL paradigm, including augmentation functions, contrasting modes, contrastive objectives, and negative mining strategies. Then, to understand the interplay of different GCL components, we conduct comprehensive, controlled experiments over benchmark tasks on datasets across various domains. Our empirical studies suggest a set of general receipts for effective GCL, e.g., simple topology augmentations that produce sparse graph views bring promising performance improvements; contrasting modes should be aligned with the granularities of end tasks. In addition, to foster future research and ease the implementation of GCL algorithms, we develop an easy-to-use library PyGCL, featuring modularized CL components, standardized evaluation, and experiment management. We envision this work to provide useful empirical evidence of effective GCL algorithms and offer several insights for future research.",Datasets & Benchmarks,NeurIPS,2021,Poster,Yanqiao Zhu;Yichen Xu;Qiang Liu;Shu Wu,False,https://openreview.net/pdf?id=UuUbIYnHKO
V8PcLz1NoQ0,NAS-Bench-x11 and the Power of Learning Curves,"While early research in neural architecture search (NAS) required extreme computational resources, the recent releases of tabular and surrogate benchmarks have greatly increased the speed and reproducibility of NAS research. However, two of the most popular benchmarks do not provide the full training information for each architecture. As a result, on these benchmarks it is not possible to evaluate many types of multi-fidelity algorithms, such as learning curve extrapolation, that require evaluating architectures at arbitrary epochs. In this work, we present a method using singular value decomposition and noise modeling to create surrogate benchmarks, NAS-Bench-111, NAS-Bench-311, and NAS-Bench-NLP11, that output the full training information for each architecture, rather than just the final validation accuracy. We demonstrate the power of using the full training information by introducing a learning curve extrapolation framework to modify single-fidelity algorithms, showing that it leads to improvements over popular single-fidelity algorithms which claimed to be state-of-the-art upon release.",main,NeurIPS,2021,Poster,Shen Yan;Colin White;Yash Savani;Frank Hutter,True,https://openreview.net/pdf?id=V8PcLz1NoQ0
VdvDlnnjzIN,Brax - A Differentiable Physics Engine for Large Scale Rigid Body Simulation,"We present Brax, an open source library for \\\\textbf{r}igid \\\\textbf{b}ody simulation with a focus on performance and parallelism on accelerators, written in JAX.  We present results on a suite of tasks inspired by the existing reinforcement learning literature, but remade in our engine.  Additionally, we provide reimplementations of PPO, SAC, ES, and direct policy optimization in JAX that compile alongside our environments, allowing the learning algorithm and the environment processing to occur on the same device, and to scale seamlessly on accelerators.  Finally, we include notebooks that facilitate training of performant policies on common MuJoCo-like tasks in minutes.",Datasets & Benchmarks,NeurIPS,2021,Poster,C. Daniel Freeman;Erik Frey;Anton Raichuk;Sertan Girgin;Igor Mordatch;Olivier Bachem,False,https://openreview.net/pdf?id=VdvDlnnjzIN
VhIIQBm00VI,Few-Shot Learning Evaluation in Natural Language Understanding,"Most recent progress in natural language understanding (NLU) has been driven, in part, by benchmarks such as GLUE, SuperGLUE, SQuAD, etc. In fact, many NLU models have now matched or exceeded ""human-level"" performance on many tasks in these benchmarks. Most of these benchmarks, however, give models access to relatively large amounts of labeled data for training. As such, the models are provided far more data than required by humans to achieve strong performance. That has motivated a line of work that focuses on improving few-shot learning performance of NLU models. However, there is a lack of standardized evaluation benchmarks for few-shot NLU resulting in different experimental settings in different papers.
To help accelerate this line of work, we introduce CLUES, a benchmark for evaluating the few-shot learning capabilities of NLU models. We demonstrate that while recent models reach human performance when they have access to large amounts of labeled data, there is a huge gap in performance in the few-shot setting for most tasks. We also demonstrate differences between alternative model families and adaptation techniques in the few shot setting. Finally, we discuss several principles and choices in designing the experimental settings for evaluating the true few-shot learning performance and suggest a unified standardized approach to few-shot learning evaluation. We aim to encourage research on NLU models that can generalize to new tasks with a small number of examples. Code and data for CLUES are available at https://github.com/microsoft/CLUES.
",Datasets & Benchmarks,NeurIPS,2021,Poster,Subhabrata Mukherjee;Xiaodong Liu;Guoqing Zheng;Saghar Hosseini;Hao Cheng;Greg Yang;Christopher Meek;Ahmed Hassan Awadallah;Jianfeng Gao,True,https://openreview.net/pdf?id=VhIIQBm00VI
VjFn0L5T7NU,RealCity3D: A Large-scale Georeferenced 3D Shape Dataset of Real-world Cities,"Existing 3D shape datasets in the research community are generally limited to objects or scenes at the home level. City-level shape datasets are rare due to the difficulty in data collection and processing. However, such datasets uniquely present a new type of 3D data with a high variance in geometric complexity and spatial layout styles, such as residential/historical/commercial buildings and skyscrapers. This work focuses on collecting such data, and proposes city generation as new tasks for data-driven content generation. Thus, we collect over 1,000,000 geo-referenced 3D building models from New York City and Zurich. We benchmark various baseline performances on two challenging tasks: (1) city layout generation, and (2) building shape generation. Moreover, we propose an auto-encoding tree neural network for 2D building footprint and 3D building cuboid generation. The dataset, tools, and algorithms will be released to the community.",Datasets & Benchmarks,NeurIPS,2021,Reject,Congcong Wen;Wenyu Han;Lazarus Chok;Yan Liang Tan;Sheung Lung Chan;Hang Zhao;Chen Feng,True,https://openreview.net/pdf?id=VjFn0L5T7NU
VjJxBi1p9zh,"RedCaps: Web-curated image-text data created by the people, for the people","Large datasets of paired images and text have become increasingly popular for learning generic representations for vision and vision-and-language tasks. Such datasets have been built by querying search engines or collecting HTML alt-text – since web data is noisy, they require complex filtering pipelines to maintain quality. We explore alternate data sources to collect high quality data with minimal filtering. We introduce RedCaps – a large-scale dataset of 12M image-text pairs collected from Reddit. Images and captions from Reddit depict and describe a wide variety of objects and scenes. We collect data from a manually curated set of subreddits, which give coarse image labels and allow us to steer the dataset composition without labeling individual instances. We show that captioning models trained on RedCaps produce rich and varied captions preferred by humans, and learn visual representations that transfer to many downstream tasks.",Datasets & Benchmarks,NeurIPS,2021,Poster,Karan Desai;Gaurav Kaul;Zubin Trivadi Aysola;Justin Johnson,True,https://openreview.net/pdf?id=VjJxBi1p9zh
VtqyY2dvE6h,MDP Playground: A Design and Debug Testbed for Reinforcement Learning,"We present \\\\emph{MDP Playground}, an efficient testbed for Reinforcement Learning (RL) agents with \\\\textit{orthogonal} dimensions that can be controlled independently to challenge agents in different ways and obtain varying degrees of hardness in generated environments. We consider and allow control over a wide variety of dimensions, including \\\\textit{delayed rewards}, \\\\textit{rewardable sequences}, \\\\textit{density of rewards}, \\\\textit{stochasticity}, \\\\textit{image representations}, \\\\textit{irrelevant features}, \\\\textit{time unit}, \\\\textit{action range} and more. We define a parameterised collection of fast-to-run toy environments in \\\\textit{OpenAI Gym} by varying these dimensions and propose to use these for the initial design and development of agents. We also provide wrappers that inject these dimensions into complex environments from \\\\textit{Atari} and \\\\textit{Mujoco} to allow for evaluating agent robustness. We further provide various example use-cases and instructions on how to use \\\\textit{MDP Playground} to design and debug agents. We believe that \\\\textit{MDP Playground} is a valuable testbed for researchers designing new, adaptive and intelligent RL agents and those wanting to unit test their agents.",Datasets & Benchmarks,NeurIPS,2021,Reject,Raghu Rajan;Jessica Lizeth Borja Diaz;Suresh Guttikonda;Fabio Ferreira;André Biedenkapp;Jan Ole von Hartz;Frank Hutter,True,https://openreview.net/pdf?id=VtqyY2dvE6h
WV0waZz9dTF,Constructing a Visual Dataset to Study the Effects of Spatial Apartheid in South Africa,"Aerial images of neighborhoods in South Africa show the clear legacy of Apartheid, a former policy of political and economic discrimination against non-European groups, with completely segregated neighborhoods of townships next to gated wealthy areas. This paper introduces the first publicly available dataset to study the evolution of spatial apartheid, using 6,768 high resolution satellite images of 9 provinces in South Africa. Our dataset was created using polygons demarcating land use, geographically labelled coordinates of buildings in South Africa, and high resolution satellite imagery covering the country from 2006-2017. We describe our iterative process to create this dataset, which includes pixel wise labels for 4 classes of neighborhoods: wealthy areas, non wealthy areas, non residential neighborhoods and vacant land. While datasets 7 times smaller than ours have cost over 1M to annotate, our dataset was created with highly constrained resources. We finally show examples of applications examining the evolution of neighborhoods in South Africa using our dataset. 
",Datasets & Benchmarks,NeurIPS,2021,Poster,Raesetje Sefala;Timnit Gebru;Luzango Mfupe;Nyalleng Moorosi;Richard Klein,True,https://openreview.net/pdf?id=WV0waZz9dTF
Wb4vR9NPE6_,Turath-150K: Image Database of Arab Heritage,"Large-scale image databases remain largely biased towards objects and activities encountered in a select few cultures. This absence of culturally-diverse images, which we refer to as the \\\\enquote{hidden tail}, limits the applicability of pre-trained neural networks and inadvertently excludes researchers from under-represented regions. To begin remedying this issue, we curate Turath-150K, a database of images of the Arab world that reflect objects, activities, and scenarios commonly found there. In the process, we introduce three benchmark databases, Turath Standard, Art, and UNESCO, specialised subsets of the Turath dataset. After demonstrating the limitations of existing networks pre-trained on ImageNet when deployed on such benchmarks, we train and evaluate several networks on the task of image classification. As a consequence of Turath, we hope to engage machine learning researchers in under-represented regions, and to inspire the release of additional culture-focused databases. The database can be accessed here: \\\\url{danikiyasseh.github.io/Turath}.",Datasets & Benchmarks,NeurIPS,2021,Reject,Dani Kiyasseh;Rasheed El-Bouri,True,https://openreview.net/pdf?id=Wb4vR9NPE6_
WcY35wjmCBA,Dynamic Environments with Deformable Objects,"We propose a set of environments with dynamic tasks that involve highly deformable topologically non-trivial objects. These environments facilitate easy experimentation: offer fast runtime, support large-scale parallel data generation, are easy to connect to reinforcement learning frameworks with OpenAI Gym API. We offer several types of benchmark tasks with varying levels of complexity, provide variants with procedurally generated cloth objects and randomized material textures. Moreover, we allow users to customize the tasks: import custom objects and textures, adjust size and material properties of deformable objects.
We prioritize dynamic aspects of the tasks: forgoing 2D tabletop manipulation in favor of 3D tasks, with gravity and inertia playing a non-negligible role. Such advanced challenges require insights from multiple fields: machine learning and computer vision to process high-dimensional inputs, methods from computer graphics and topology to inspire structured and interpretable representations, insights from robotics to learn advanced control policies. We aim to help researches from these fields contribute their insights and simplify establishing interdisciplinary collaborations.",Datasets & Benchmarks,NeurIPS,2021,Poster,Rika Antonova;Peiyang Shi;Hang Yin;Zehang Weng;Danica Kragic Jensfelt,True,https://openreview.net/pdf?id=WcY35wjmCBA
XN1M27T6uux,DECAF: Generating Fair Synthetic Data Using Causally-Aware Generative Networks,"Machine learning models have been criticized for reflecting unfair biases in the training data.  Instead of solving for this by introducing fair learning algorithms directly, we focus on generating fair synthetic data, such that any downstream learner is fair. Generating fair synthetic data from unfair data - while remaining truthful to the underlying data-generating process (DGP) - is non-trivial. In this paper, we introduce DECAF: a GAN-based fair synthetic data generator for tabular data.  With DECAF we embed the DGP explicitly as a structural causal model in the input layers of the generator, allowing each variable to be reconstructed conditioned on its causal parents.  This procedure enables inference time debiasing, where biased edges can be strategically removed for satisfying user-defined fairness requirements. The DECAF framework is versatile and compatible with several popular definitions of fairness. In our experiments, we show that DECAF successfully removes undesired bias and - in contrast to existing methods - is capable of generating high-quality synthetic data. Furthermore, we provide theoretical guarantees on the generator's convergence and the fairness of downstream models.",main,NeurIPS,2021,Poster,Boris van Breugel;Trent Kyono;Jeroen Berrevoets;Mihaela van der Schaar,True,https://openreview.net/pdf?id=XN1M27T6uux
XccDXrDNLek,Pervasive Label Errors in Test Sets Destabilize Machine Learning Benchmarks,"We identify label errors in the test sets of 10 of the most commonly-used computer vision, natural language, and audio datasets, and subsequently study the potential for these label errors to affect benchmark results.  Errors in test sets are numerous and widespread: we estimate an average of at least 3.3% errors across the 10 datasets, where for example label errors comprise at least 6% of the ImageNet validation set. Putative label errors are identified using confident learning algorithms and then human-validated via crowdsourcing (51% of the algorithmically-flagged candidates are indeed erroneously labeled, on average across the datasets).  Traditionally, machine learning practitioners choose which model to deploy based on test accuracy -- our findings advise caution here, proposing that judging models over correctly labeled test sets may be more useful, especially for noisy real-world datasets.  Surprisingly, we find that lower capacity models may be practically more useful than higher capacity models in real-world datasets with high proportions of erroneously labeled data.  For example, on ImageNet with corrected labels: ResNet-18 outperforms ResNet-50 if the prevalence of originally mislabeled test examples increases by just 6%.  On CIFAR-10 with corrected labels: VGG-11 outperforms VGG-19 if the prevalence of originally mislabeled test examples increases by just 5%.  Test set errors across the 10 datasets can be viewed at https://labelerrors.com and all label errors can be reproduced by https://github.com/cleanlab/label-errors.",Datasets & Benchmarks,NeurIPS,2021,Poster,Curtis G Northcutt;Anish Athalye;Jonas Mueller,False,https://openreview.net/pdf?id=XccDXrDNLek
XqEF9riB93S,When Is Unsupervised Disentanglement Possible?,"A common assumption in many domains is that high dimensional data are a smooth nonlinear function of a small number of independent factors. When is it possible to recover the factors from unlabeled data? In the context of deep models this problem is called “disentanglement” and was recently shown to be impossible without additional strong assumptions [17, 19]. In this paper, we show that the assumption of local isometry together with non-Gaussianity of the factors, is sufficient to provably recover disentangled representations from data. We leverage recent advances in deep generative models to construct manifolds of highly realistic images for which the ground truth latent representation is known, and test whether modern and classical methods succeed in recovering the latent factors. For many different manifolds, we find that a spectral method that explicitly optimizes local isometry and non-Gaussianity consistently finds the correct latent factors, while baseline deep autoencoders do not. We propose how to encourage deep autoencoders to find encodings that satisfy local isometry and show that this helps them discover disentangled representations. Overall, our results suggest that in some realistic settings, unsupervised disentanglement is provably possible, without any domain-specific assumptions.",main,NeurIPS,2021,Poster,Daniella Horan;Eitan Richardson;Yair Weiss,True,https://openreview.net/pdf?id=XqEF9riB93S
XyDozX3_L4l,NATURE: Natural Auxiliary Text Utterances for Realistic Spoken Language Evaluation,"Slot-filling and intent detection are the backbone of conversational agents such as voice assistants, and are active areas of research. Even though state-of-the-art techniques on publicly available benchmarks show impressive performance, their ability to generalize to realistic scenarios is yet to be demonstrated. In this work, we present NATURE, a set of simple spoken-language-oriented transformations, applied to the evaluation set of datasets, to introduce human spoken language variations while preserving the semantics of an utterance. We apply NATURE to common slot-filling and intent detection benchmarks and demonstrate that simple perturbations from the standard evaluation set by NATURE can deteriorate model performance significantly. Through our experiments we demonstrate that when NATURE operators are applied to evaluation set of popular benchmarks the model accuracy can drop by up to 40%.",Datasets & Benchmarks,NeurIPS,2021,Poster,David Alfonso-Hermelo;Ahmad Rashid;Abbas Ghaddar;Philippe Langlais;Mehdi Rezagholizadeh,False,https://openreview.net/pdf?id=XyDozX3_L4l
Y6sH0l4PExm,Intelligent Sight and Sound: A Chronic Cancer Facial Pain Dataset,"Cancer patients experience high rates of chronic pain throughout the treatment process. Assessing pain for this patient population is a vital component of psychological and functional well-being, as it can cause a rapid deterioration of quality of life. Existing work in facial pain detection often have deficiencies in labeling or methodology that prevent them from being clinically relevant.  This paper introduces the first chronic cancer pain dataset, collected as part of the Intelligent Sight and Sound (ISS) clinical trial, guided by clinicians to help ensure that model findings yield clinically relevant results. The data collected to date consists of 29 patients, 509 smartphone videos, 189,999 frames, and self-reported affective and activity pain scores adopted from the Brief Pain Inventory (BPI). Using static images and multi-modal data to predict self-reported pain levels, early models show significant gaps between current methods available to predict pain today, with room for improvement. Due to the especially sensitive nature of the inherent Personally Identifiable Information (PII) of facial images, the dataset will be released under the guidance and control of the National Institutes of Health (NIH). ",Datasets & Benchmarks,NeurIPS,2021,Poster,Catherine Ordun;Alexandra Cha;Edward Raff;Byron Gaskin;Alexander Hanson;Mason Rule;Sanjay Purushotham;James Gulley,True,https://openreview.net/pdf?id=Y6sH0l4PExm
YDMFgD_qJuA,SKM-TEA: A Dataset for Accelerated MRI Reconstruction with Dense Image Labels for Quantitative Clinical Evaluation,"Magnetic resonance imaging (MRI) is a cornerstone of modern medical imaging. However, long image acquisition times, the need for qualitative expert analysis, and the lack of (and difficulty extracting) quantitative indicators that are sensitive to tissue health have curtailed widespread clinical and research studies. While recent machine learning methods for MRI reconstruction and analysis have shown promise for reducing this burden, these techniques are primarily validated with imperfect image quality metrics, which are discordant with clinically-relevant measures that ultimately hamper clinical deployment and clinician trust. To mitigate this challenge, we present the Stanford Knee MRI with Multi-Task Evaluation (SKM-TEA) dataset, a collection of quantitative knee MRI (qMRI) scans that enables end-to-end, clinically-relevant evaluation of MRI reconstruction and analysis tools. This 1.6TB dataset consists of raw-data measurements of ~25,000 slices (155 patients) of anonymized patient MRI scans, the corresponding scanner-generated DICOM images, manual segmentations of four tissues, and bounding box annotations for sixteen clinically relevant pathologies. We provide a framework for using qMRI parameter maps, along with image reconstructions and dense image labels, for measuring the quality of qMRI biomarker estimates extracted from MRI reconstruction, segmentation, and detection techniques. Finally, we use this framework to benchmark state-of-the-art baselines on this dataset. We hope our SKM-TEA dataset and code can enable a broad spectrum of research for modular image reconstruction and image analysis in a clinically informed manner. Dataset access, code, and benchmarks are available at https://github.com/StanfordMIMI/skm-tea.",Datasets & Benchmarks,NeurIPS,2021,Poster,Arjun D Desai;Andrew M Schmidt;Elka B Rubin;Christopher Michael Sandino;Marianne Susan Black;Valentina Mazzoli;Kathryn J Stevens;Robert Boutin;Christopher Re;Garry E Gold;Brian Hargreaves;Akshay Chaudhari,True,https://openreview.net/pdf?id=YDMFgD_qJuA
YJHXfcTDaqw,Rail-5k: a Real-World Dataset for Rail Surface Defects Detection,"This paper presents the Rail-5k dataset for benchmarking the performance of visual algorithms in a real-world application scenario, namely the rail surface defect detection task.
We collected over 5k high-quality images from railways across China and annotated 1100 images with the help of railway experts to identify the most common 13 types of railway defects.
The dataset can be used for two settings both with unique challenges, the first is the fully-supervised setting using the 1k labeled images for training, fine-grained nature and long-tailed distribution of defect classes make it hard for visual algorithms to tackle.
The second is the semi-supervised learning setting facilitated by the 4k unlabeled images, these 4k images are uncurated containing possible image corruptions and domain shift with the labeled images, which can not be easily tackled by previous semi-supervised learning methods.
We believe our dataset could be a valuable benchmark for evaluating the robustness and reliability of visual algorithms.",Datasets & Benchmarks,NeurIPS,2021,Reject,Zihao Zhang;Shaozuo Yu;Siwei Yang;Bingchen Zhao;Yu Zhou,True,https://openreview.net/pdf?id=YJHXfcTDaqw
YN4TMf3sv52,Exploring Forensic Dental Identification with Deep Learning,"Dental forensic identification targets to identify persons with dental traces.
The task is vital for the investigation of criminal scenes and mass disasters because of the resistance of dental structures and the wide-existence of dental imaging. 
However, no widely accepted automated solution is available for this labour-costly task. 
In this work, we pioneer to study deep learning for dental forensic identification based on panoramic radiographs. 
We construct a comprehensive benchmark with various dental variations that can adequately reflect the difficulties of the task. 
By considering the task's unique challenges, we propose FoID, a deep learning method featured by: (\\\\textit{i}) clinical-inspired attention localization, (\\\\textit{ii}) domain-specific augmentations that enable instance discriminative learning, and (\\\\textit{iii}) transformer-based self-attention mechanism that dynamically reasons the relative importance of attentions. 
We show that FoID can outperform traditional approaches by at least \\\\textbf{22.98\\\\%} in terms of Rank-1 accuracy, and outperform strong CNN baselines by at least \\\\textbf{10.50\\\\%} in terms of mean Average Precision (mAP). 
Moreover, extensive ablation studies verify the effectiveness of each building blocks of FoID. 
Our work can be a first step towards the automated system for forensic identification among large-scale multi-site databases. 
Also, the proposed techniques, \\\\textit{e.g.}, self-attention mechanism, can also be meaningful for other identification tasks, \\\\textit{e.g.}, pedestrian re-identification.
Related data and codes can be found at \\\\href{https://github.com/liangyuandg/FoID}{https://github.com/liangyuandg/FoID}. ",main,NeurIPS,2021,Poster,Yuan Liang;Weikun Han;Liang Qiu;Chen Wu;Yiting Shao;Kun Wang;Lei He,True,https://openreview.net/pdf?id=YN4TMf3sv52
Yx9jT3fkBaD,A Spoken Language Dataset of Descriptions for Speech-Based Grounded Language Learning,"Grounded language acquisition is a major area of research combining aspects of natural language processing, computer vision, and signal processing, compounded by domain issues requiring sample efficiency and other deployment constraints. 
In this work, we present a multimodal dataset of RGB+depth objects with spoken as well as textual descriptions. We analyze the differences between the two types of descriptive language and our experiments demonstrate that the different modalities affect learning. This will enable researchers studying the intersection of robotics, NLP, and HCI to better investigate how the multiple modalities of image, depth, text, speech, and transcription interact, as well as how differences in the vernacular of these modalities impact results.",Datasets & Benchmarks,NeurIPS,2021,Poster,Gaoussou Youssouf Kebe;Padraig Higgins;Patrick Jenkins;Kasra Darvish;Rishabh Sachdeva;Ryan Barron;John Winder;Donald Engel;Edward Raff;Francis Ferraro;Cynthia Matuszek,True,https://openreview.net/pdf?id=Yx9jT3fkBaD
ZRcjSOmYraB,Learning Distilled Collaboration Graph for Multi-Agent Perception,"To promote better performance-bandwidth trade-off for multi-agent perception, we propose a novel distilled collaboration graph (DiscoGraph) to model trainable, pose-aware, and adaptive collaboration among agents. Our key novelties lie in two aspects. First, we propose a teacher-student framework to train DiscoGraph via knowledge distillation. The teacher model employs an early collaboration with holistic-view inputs; the student model is based on intermediate collaboration with single-view inputs. Our framework trains DiscoGraph by constraining post-collaboration feature maps in the student model to match the correspondences in the teacher model. Second, we propose a matrix-valued edge weight in DiscoGraph. In such a matrix, each element reflects the inter-agent attention at a specific spatial region, allowing an agent to adaptively highlight the informative regions. During inference, we only need to use the student model named as the distilled collaboration network (DiscoNet). Attributed to the teacher-student framework, multiple agents with the shared DiscoNet could collaboratively approach the performance of a hypothetical teacher model with a holistic view. Our approach is validated on V2X-Sim 1.0, a large-scale multi-agent perception dataset that we synthesized using CARLA and SUMO co-simulation. Our quantitative and qualitative experiments in multi-agent 3D object detection show that DiscoNet could not only achieve a better performance-bandwidth trade-off than the state-of-the-art collaborative perception methods, but also bring more straightforward design rationale. Our code is available on https://github.com/ai4ce/DiscoNet.",main,NeurIPS,2021,Poster,Yiming Li;Shunli Ren;Pengxiang Wu;Siheng Chen;Chen Feng;Wenjun Zhang,True,https://openreview.net/pdf?id=ZRcjSOmYraB
ZarM_uLVyGw,Contrastive Reinforcement Learning of Symbolic Reasoning Domains,"Abstract symbolic reasoning, as required in domains such as mathematics and logic, is a key component of human intelligence. Solvers for these domains have important applications, especially to computer-assisted education. But learning to solve symbolic problems is challenging for machine learning algorithms. Existing models either learn from human solutions or use hand-engineered features, making them expensive to apply in new domains. In this paper, we instead consider symbolic domains as simple environments where states and actions are given as unstructured text, and binary rewards indicate whether a problem is solved. This flexible setup makes it easy to specify new domains, but search and planning become challenging. We introduce five environments inspired by the Mathematics Common Core Curriculum, and observe that existing Reinforcement Learning baselines perform poorly. We then present a novel learning algorithm, Contrastive Policy Learning (ConPoLe) that explicitly optimizes the InfoNCE loss, which lower bounds the mutual information between the current state and next states that continue on a path to the solution. ConPoLe successfully solves all four domains. Moreover, problem representations learned by ConPoLe enable accurate prediction of the categories of problems in a real mathematics curriculum. Our results suggest new directions for reinforcement learning in symbolic domains, as well as applications to mathematics education.",main,NeurIPS,2021,Poster,Gabriel Poesia;WenXin Dong;Noah Goodman,True,https://openreview.net/pdf?id=ZarM_uLVyGw
Zkj_VcZ6ol,ImageNet-21K Pretraining for the Masses,"ImageNet-1K serves as the primary dataset for pretraining deep learning models for computer vision tasks. ImageNet-21K dataset, which is bigger and more diverse, is used less frequently for pretraining, mainly due to its complexity, low accessibility, and underestimation of its added value.
This paper aims to close this gap, and make high-quality efficient pretraining on ImageNet-21K available for everyone.
Via a dedicated preprocessing stage, utilization of WordNet hierarchical structure, and a novel training scheme called semantic softmax, we show that various models significantly benefit from ImageNet-21K pretraining on numerous datasets and tasks, including small mobile-oriented models. 
We also show that we outperform previous ImageNet-21K pretraining schemes for prominent new models like ViT and Mixer.
Our proposed pretraining pipeline is efficient, accessible, and leads to SoTA reproducible results, from a publicly available dataset. The training code and pretrained models are available at:  https://github.com/Alibaba-MIIL/ImageNet21K",Datasets & Benchmarks,NeurIPS,2021,Poster,Tal Ridnik;Emanuel Ben-Baruch;Asaf Noy;Lihi Zelnik-Manor,False,https://openreview.net/pdf?id=Zkj_VcZ6ol
_-O9SefMb99,LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptation Semantic Segmentation,"Deep learning approaches have shown promising results in remote sensing high spatial resolution (HSR) land-cover mapping. However, urban and rural scenes can show completely different geographical landscapes, and the inadequate generalizability of these algorithms hinders city-level or national-level mapping. Most of the existing HSR land-cover datasets only focus on improvement of the semantic segmentation in one domain (urban or rural), thereby ignoring the model transferability. In this paper, we introduce the Land-cOVEr Domain Adaptation semantic segmentation (LoveDA) dataset to promote large-scale land-cover mapping. The LoveDA dataset contains 3338 aerial images with 86,516 annotated objects for seven common land-cover categories. Compared to the existing datasets, the LoveDA dataset encompasses two domains (urban and rural), which brings considerable challenges due to the: 1) multi-scale objects; 2) complex background samples; and 3) inconsistent class distributions. The LoveDA dataset is suitable for both land-cover semantic segmentation and unsupervised domain adaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on nine semantic segmentation methods and eight UDA methods. Some exploratory studies were also carried out to find alternative ways to address these challenges. The code and data will be available at: https://github.com/Junjue-Wang/LoveDA.",Datasets & Benchmarks,NeurIPS,2021,Reject,Junjue Wang;Zhuo Zheng;Ailong Ma;Xiaoyan Lu;Yanfei Zhong,True,https://openreview.net/pdf?id=_-O9SefMb99
_KqWSCu566,Characterizing Generalization under Out-Of-Distribution Shifts in Deep Metric Learning,"Deep Metric Learning (DML) aims to find representations suitable for zero-shot transfer to a priori unknown test distributions. However, common evaluation protocols only test a single, fixed data split in which train and test classes are assigned randomly. More realistic evaluations should consider a broad spectrum of distribution shifts with potentially varying degree and difficulty.
In this work, we systematically construct train-test splits of increasing difficulty and present the ooDML benchmark to characterize generalization under out-of-distribution shifts in DML. ooDML is designed to probe the generalization performance on much more challenging, diverse train-to-test distribution shifts. Based on our new benchmark, we conduct a thorough empirical analysis of state-of-the-art DML methods. We find that while generalization tends to consistently degrade with difficulty, some methods are better at retaining performance as the distribution shift increases. Finally, we propose few-shot DML as an efficient way to consistently improve generalization in response to unknown test shifts presented in ooDML.",main,NeurIPS,2021,Poster,Timo Milbich;Karsten Roth;Samarth Sinha;Ludwig Schmidt;Marzyeh Ghassemi;Björn Ommer,True,https://openreview.net/pdf?id=_KqWSCu566
_WnGcwXLYOE,FLEX: Unifying Evaluation for Few-Shot NLP,"Few-shot NLP research is highly active, yet conducted in disjoint research threads with evaluation suites that lack challenging-yet-realistic testing setups and fail to employ careful experimental design. Consequently, the community does not know which techniques perform best or even if they outperform simple baselines. In response, we formulate the FLEX Principles, a set of requirements and best practices for unified, rigorous, valid, and cost-sensitive few-shot NLP evaluation. These principles include Sample Size Design, a novel approach to benchmark design that optimizes statistical accuracy and precision while keeping evaluation costs manageable. Following the principles, we release the FLEX benchmark, which includes four few-shot transfer settings, zero-shot evaluation, and a public leaderboard that covers diverse NLP tasks. In addition, we present UniFew, a prompt-based model for few-shot learning that unifies pretraining and finetuning prompt formats, eschewing complex machinery of recent prompt-based approaches in adapting downstream task formats to language model pretraining objectives. We demonstrate that despite simplicity, UniFew achieves results competitive with both popular meta-learning and prompt-based approaches.",main,NeurIPS,2021,Poster,Jonathan Bragg;Arman Cohan;Kyle Lo;Iz Beltagy,True,https://openreview.net/pdf?id=_WnGcwXLYOE
_kwj6V53ZqB,Grounding Representation Similarity Through Statistical Testing,"To understand neural network behavior, recent works quantitatively compare different networks' learned representations using canonical correlation analysis (CCA), centered kernel alignment (CKA), and other dissimilarity measures. Unfortunately, these widely used measures often disagree on fundamental observations, such as whether deep networks differing only in random initialization learn similar representations. These disagreements raise the question: which, if any, of these dissimilarity measures should we believe? We provide a framework to ground this question through a concrete test: measures should have \\\\emph{sensitivity} to changes that affect functional behavior, and \\\\emph{specificity} against changes that do not. We quantify this through a variety of functional behaviors including probing accuracy and robustness to distribution shift, and examine changes such as varying random initialization and deleting principal components. We find that current metrics exhibit different weaknesses, note that a classical baseline performs surprisingly well, and highlight settings where all metrics appear to fail, thus providing a challenge set for further improvement.",main,NeurIPS,2021,Poster,Frances Ding;Jean-Stanislas Denain;Jacob Steinhardt,True,https://openreview.net/pdf?id=_kwj6V53ZqB
aH0NedstEei,Pl@ntNet-300K: a new plant image dataset for the evaluation of set-valued classifiers,"This paper presents a novel image dataset with high intrinsic ambiguity specifically built for evaluating and comparing set-valued classifiers. This dataset, built from the database of Pl@ntnet citizen observatory, consists of 306,146 images covering 1,081 species. We highlight two particular features of the dataset, inherent to the way the images are acquired and to the intrinsic diversity of plants morphology:
    i) The dataset has a strong class imbalance, meaning that a few species account for most of the images.
    ii) Many species are visually similar, making identification difficult even for the expert eye.
These two characteristics make the present dataset well suited for the evaluation of set-valued classification methods and algorithms. Therefore, we recommend two set-valued evaluation metrics associated with the dataset (top-k and average-k) and we provide the results of a baseline approach based on a resnet50 trained with the cross-entropy loss.",Datasets & Benchmarks,NeurIPS,2021,Reject,Camille Garcin;alexis joly;Pierre Bonnet;Antoine Affouard;Jean-Christophe Lombardo;Mathias Chouet;Maximilien Servajean;Joseph Salmon,True,https://openreview.net/pdf?id=aH0NedstEei
aIfp8kLuvc9,TenSet: A Large-scale Program Performance Dataset for Learned Tensor Compilers,"Search-based tensor compilers can greatly accelerate the execution of machine learning models by generating high-performance tensor programs, such as matrix multiplications and convolutions. These compilers take a high-level mathematical expression as input and search for the fastest low-level implementations. At the core of the search procedure is a cost model which estimates the performance of different candidates to reduce the frequency of time-consuming on-device measurements. There has been a growing interest in using machine learning techniques to learn a cost model to ease the effort of building an analytical model. However, a standard dataset for pre-training and benchmarking learned cost models is lacking.

We introduce TenSet, a large-scale tensor program performance dataset. TenSet contains 52 million program performance records collected from 6 hardware platforms. We provide comprehensive studies on how to learn and evaluate the cost models, including data collection, model architectures, loss functions, transfer learning, and evaluation metrics. We also show that a cost model pre-trained on TenSet can accelerate the search time in the state-of-the-art tensor compiler by up to 10$\\\\times$. The dataset is available at https://github.com/tlc-pack/tenset.",Datasets & Benchmarks,NeurIPS,2021,Poster,Lianmin Zheng;Ruochen Liu;Junru Shao;Tianqi Chen;Joseph E. Gonzalez;Ion Stoica;Ameer Haj Ali,True,https://openreview.net/pdf?id=aIfp8kLuvc9
aqCD8RINP54,RELLISUR: A Real Low-Light Image Super-Resolution Dataset,"In this paper, we introduce RELLISUR, a novel dataset of real low-light low-resolution images paired with normal-light high-resolution reference image counterparts. With this dataset, we seek to fill the gap between low-light image enhancement and low-resolution image enhancement (Super-Resolution (SR)) which is currently only being addressed separately in the literature, even though the visibility of real-world images are often limited by both low-light and low-resolution. Part of the reason for this, is the lack of a large-scale dataset. To this end, we release a dataset with 12750 paired images of different resolutions and degrees of low-light illumination, to facilitate learning of deep-learning based models that can perform a direct mapping from degraded images with low visibility to sharp and detail rich images of high resolution. Additionally, we provide a benchmark of the existing methods for separate Low Light Enhancement (LLE) and SR on the proposed dataset along with experiments with joint LLE and SR. The latter shows that joint processing results in more accurate reconstructions with better perceptual quality compared to sequential processing of the images. With this, we confirm that the new RELLISUR dataset can be useful for future machine learning research aimed at solving simultaneous image LLE and SR.",Datasets & Benchmarks,NeurIPS,2021,Poster,Andreas Aakerberg;Kamal Nasrollahi;Thomas B. Moeslund,True,https://openreview.net/pdf?id=aqCD8RINP54
b3Zoeq2sCLq,KeSpeech: An Open Source Speech Dataset of Mandarin and Its Eight Subdialects,"This paper introduces an open source speech dataset, KeSpeech, which involves 1,542 hours of speech signals recorded by 27,237 speakers in 34 cities in China, and the pronunciation includes standard Mandarin and its 8 subdialects. The new dataset possesses several properties. Firstly, the dataset provides multiple labels including content transcription, speaker identity and subdialect, hence supporting a variety of speech processing tasks, such as speech recognition, speaker recognition, and subdialect identification, as well as other advanced techniques like multi-task learning and conditional learning. Secondly, some of the text samples were parallel recorded with both the standard Mandarin and a particular subdialect, allowing for new applications such as subdialect style conversion. Thirdly, the number of speakers is much larger than other open-source datasets, making it suitable for tasks that require training data from vast speakers. Finally, the speech signals were recorded in two phases, which opens the opportunity for the study of the time variance property of human speech. We present the design principle of the KeSpeech dataset and four baseline systems based on the new data resource: speech recognition, speaker verification, subdialect identification and voice conversion. The dataset is free for all academic usage.",Datasets & Benchmarks,NeurIPS,2021,Poster,Zhiyuan Tang;Dong Wang;Yanguang Xu;Jianwei Sun;Xiaoning Lei;Shuaijiang Zhao;Cheng Wen;Xingjun Tan;Chuandong Xie;Shuran Zhou;Rui Yan;Chenjia Lv;Yang Han;Wei Zou;Xiangang Li,True,https://openreview.net/pdf?id=b3Zoeq2sCLq
bKBhQhPeKaF,Benchmark for Compositional Text-to-Image Synthesis,"Rapid progress in text-to-image generation has been often measured by Frechet Inception Distance (FID) to capture how realistic the generated images are, or by R-Precision to assess if they are well conditioned on the given textual descriptions. However, a systematic study on how well the text-to-image synthesis models generalize to novel word compositions is missing. In this work, we focus on assessing how true the generated images are to the input texts in this particularly challenging scenario of novel compositions. We present the first systematic study of text-to-image generation on zero-shot compositional splits targeting two scenarios, unseen object-color (e.g. ""blue petal"") and object-shape (e.g. ""long beak"") phrases. We create new benchmarks building on the existing CUB and Oxford Flowers datasets. We also propose a new metric, based on a powerful vision-and-language CLIP model, which we leverage to compute R-Precision. This is in contrast to the common approach where the same retrieval model is used during training and evaluation, potentially leading to biased behavior. We experiment with several recent text-to-image generation methods. Our automatic and human evaluation confirm that there is indeed a gap in performance when encountering previously unseen phrases. We show that the image correctness rather than purely perceptual quality is especially impacted. Finally, our CLIP-R-Precision metric demonstrates better correlation with human judgments than the commonly used metric.",Datasets & Benchmarks,NeurIPS,2021,Poster,Dong Huk Park;Samaneh Azadi;Xihui Liu;Trevor Darrell;Anna Rohrbach,True,https://openreview.net/pdf?id=bKBhQhPeKaF
bLBIbVaGDu,LoveDA: A Remote Sensing Land-Cover Dataset for Domain Adaptive Semantic Segmentation,"Deep learning approaches have shown promising results in remote sensing high spatial resolution (HSR) land-cover mapping. However, urban and rural scenes can show completely different geographical landscapes, and the inadequate generalizability of these algorithms hinders city-level or national-level mapping. Most of the existing HSR land-cover datasets mainly promote the research of learning semantic representation, thereby ignoring the model transferability. In this paper, we introduce the Land-cOVEr Domain Adaptive semantic segmentation (LoveDA) dataset to advance semantic and transferable learning. The LoveDA dataset contains 5987 HSR images with 166768 annotated objects from three different cities. Compared to the existing datasets, the LoveDA dataset encompasses two domains (urban and rural), which brings considerable challenges due to the:  1) multi-scale objects; 2) complex background samples; and 3) inconsistent class distributions. The LoveDA dataset is suitable for both land-cover semantic segmentation and unsupervised domain adaptation (UDA) tasks. Accordingly, we benchmarked the LoveDA dataset on eleven semantic segmentation methods and eight UDA methods. Some exploratory studies including multi-scale architectures and strategies, additional background supervision, and pseudo-label analysis were also carried out to address these challenges. The code and data are available at https://github.com/Junjue-Wang/LoveDA.",Datasets & Benchmarks,NeurIPS,2021,Poster,Junjue Wang;Zhuo Zheng;马爱龙;Xiaoyan Lu;Yanfei Zhong,True,https://openreview.net/pdf?id=bLBIbVaGDu
bNL5VlTfe3p,Hardware Design and Accurate Simulation of Structured-Light Scanning for Benchmarking of 3D Reconstruction Algorithms,"Images of a real scene taken with a camera commonly differ from synthetic images of a virtual replica of the same scene, despite advances in light transport simulation and calibration. By explicitly co-developing the Structured-Light Scanning (SLS) hardware and rendering pipeline we are able to achieve negligible per-pixel difference between the real image and the synthesized image on geometrically complex calibration objects with known material properties. This approach provides an ideal test-bed for developing and evaluating data-driven algorithms in the area of 3D reconstruction, as the synthetic data is indistinguishable from real data and can be generated at large scale by simulation. We propose three benchmark challenges using a combination of acquired and synthetic data generated with our system: (1) a denoising benchmark tailored to structured-light scanning, (2) a shape completion benchmark to fill in missing data, and (3) a benchmark for surface reconstruction from dense point clouds. Besides, we provide a large collection of high-resolution scans that allow to use our system and benchmarks without reproduction of the hardware setup on our website: https://geometryprocessing.github.io/scanner-sim",Datasets & Benchmarks,NeurIPS,2021,Poster,Sebastian Koch;Yurii Piadyk;Markus Worchel;Marc Alexa;Cláudio Silva;Denis Zorin;Daniele Panozzo,True,https://openreview.net/pdf?id=bNL5VlTfe3p
bYi_2708mKK,Retiring Adult: New Datasets for Fair Machine Learning,"Although the fairness community has recognized the importance of data, researchers in the area primarily rely on UCI Adult when it comes to tabular data. Derived from a 1994 US Census survey, this dataset has appeared in hundreds of research papers where it served as the basis for the development and comparison of many algorithmic fairness interventions. We reconstruct a superset of the UCI Adult data from available US Census sources and reveal idiosyncrasies of the UCI Adult dataset that limit its external validity. Our primary contribution is a suite of new datasets derived from US Census surveys that extend the existing data ecosystem for research on fair machine learning. We create prediction tasks relating to income, employment, health, transportation, and housing. The data span multiple years and all states of the United States, allowing researchers to study temporal shift and geographic variation. We highlight a broad initial sweep of new empirical insights relating to trade-offs between fairness criteria, performance of algorithmic interventions, and the role of distribution shift based on our new datasets. Our findings inform ongoing debates, challenge some existing narratives, and point to future research directions.",main,NeurIPS,2021,Oral,Frances Ding;Moritz Hardt;John Miller;Ludwig Schmidt,True,https://openreview.net/pdf?id=bYi_2708mKK
bajTv_cvkI,Exploiting Domain-Specific Features to Enhance Domain Generalization,"Domain Generalization (DG) aims to train a model, from multiple observed source domains, in order to perform well on unseen target domains. To obtain the generalization capability, prior DG approaches have focused on extracting domain-invariant information across sources to generalize on target domains, while useful domain-specific information which strongly correlates with labels in individual domains and the generalization to target domains is usually ignored. In this paper, we propose meta-Domain Specific-Domain Invariant (mDSDI) - a novel theoretically sound framework that extends beyond the invariance view to further capture the usefulness of domain-specific information. Our key insight is to disentangle features in the latent space while jointly learning both domain-invariant and domain-specific features in a unified framework. The domain-specific representation is optimized through the meta-learning framework to adapt from source domains, targeting a robust generalization on unseen domains. We empirically show that mDSDI provides competitive results with state-of-the-art techniques in DG. A further ablation study with our generated dataset, Background-Colored-MNIST, confirms the hypothesis that domain-specific is essential, leading to better results when compared with only using domain-invariant.",main,NeurIPS,2021,Poster,Ha Manh Bui;Toan Tran;Anh Tuan Tran;Dinh Phung,True,https://openreview.net/pdf?id=bajTv_cvkI
bgWHz41FMB7,RAFT: A Real-World Few-Shot Text Classification Benchmark,"Large pre-trained language models have shown promise for few-shot learning, completing text-based tasks given only a few task-specific examples. Will models soon solve classification tasks that have so far been reserved for human research assistants? Existing benchmarks are not designed to measure progress in applied settings, and so don't directly answer this question. The RAFT benchmark (Real-world Annotated Few-shot Tasks) focuses on naturally occurring tasks and uses an evaluation setup that mirrors deployment. Baseline evaluations on RAFT reveal areas current techniques struggle with: reasoning over long texts and tasks with many classes. Human baselines show that some classification tasks are difficult for non-expert humans, reflecting that real-world value sometimes depends on domain expertise. Yet even non-expert human baseline F1 scores exceed GPT-3 by an average of 0.11. The RAFT datasets and leaderboard will track which model improvements translate into real-world benefits at https://raft.elicit.org/.",Datasets & Benchmarks,NeurIPS,2021,Poster,Neel Alex;Eli Lifland;Lewis Tunstall;Abhishek Thakur;Pegah Maham;C. Jess Riedel;Emmie Hine;Carolyn Ashurst;Paul Sedille;Alexis Carlier;Michael Noetel;Andreas Stuhlmüller,True,https://openreview.net/pdf?id=bgWHz41FMB7
bhEAWsS9-Sb,Scalable Diverse Model Selection for Accessible Transfer Learning,"With the preponderance of pretrained deep learning models available off-the-shelf from model banks today, finding the best weights to fine-tune to your use-case can be a daunting task. Several methods have recently been proposed to find good models for transfer learning, but they either don't scale well to large model banks or don't perform well on the diversity of off-the-shelf models. Ideally the question we want to answer is, ""given some data and a source model, can you quickly predict the model's accuracy after fine-tuning?"" In this paper, we formalize this setting as ""Scalable Diverse Model Selection"" and propose several benchmarks for evaluating on this task. We find that existing model selection and transferability estimation methods perform poorly here and analyze why this is the case. We then introduce simple techniques to improve the performance and speed of these algorithms. Finally, we iterate on existing methods to create PARC, which outperforms all other methods on diverse model selection. We have released the benchmarks and method code in hope to inspire future work in model selection for accessible transfer learning.",main,NeurIPS,2021,Poster,Daniel Bolya;Rohit Mittapalli;Judy Hoffman,True,https://openreview.net/pdf?id=bhEAWsS9-Sb
c20jiJ5K2H,Multilingual Spoken Words Corpus,"Multilingual Spoken Words Corpus is a large and growing audio dataset of spoken words in 50 languages collectively spoken by over 5 billion people, for academic research and commercial applications in keyword spotting and spoken term search, licensed under CC-BY 4.0. The dataset contains more than 340,000 keywords, totaling 23.4 million 1-second spoken examples (over 6,000 hours). The dataset has many use cases, ranging from voice-enabled consumer devices to call center automation. We generate this dataset by applying forced alignment on crowd-sourced sentence-level audio to produce per-word timing estimates for extraction. All alignments are included in the dataset. We provide a detailed analysis of the contents of the data and contribute methods for detecting potential outliers. We report baseline accuracy metrics on keyword spotting models trained from our dataset compared to models trained on a manually-recorded keyword dataset. We conclude with our plans for dataset maintenance, updates, and open-sourced code.",Datasets & Benchmarks,NeurIPS,2021,Poster,Mark Mazumder;Sharad Chitlangia;Colby Banbury;Yiping Kang;Juan Manuel Ciro;Keith Achorn;Daniel Galvez;Mark Sabini;Peter Mattson;David Kanter;Greg Diamos;Pete Warden;Josh Meyer;Vijay Janapa Reddi,True,https://openreview.net/pdf?id=c20jiJ5K2H
cB3OdLInAr9,AIT-QA: Question Answering Dataset over Complex Tables in the Airline Industry,"Recent advances in transformers have enabled Table Question Answering (Table QA) systems to achieve high accuracy and SOTA results on open domain datasets like WikiTableQuestions and WikiSQL. Such transformers are frequently pre-trained on open-domain content such as Wikipedia, where they effectively encode questions and corresponding tables from Wikipedia as seen in Table QA dataset. However, web tables in Wikipedia are notably flat in their layout, with the first row as the sole column header. The layout lends to a relational view of tables where each row is a tuple. Whereas, tables in domain-specific business or scientific documents often have a much more complex layout, including hierarchical row and column headers, in addition to having specialized vocabulary terms from that domain. To address this problem, we introduce the domain-specific Table QA dataset AITQA (Airline Industry Table QA). The dataset consists of 515 questions authored by human annotators on 116 tables extracted from public U.S. SEC filings (SEC Filings publicly available at: https://www.sec.gov/edgar.shtml) of major airline companies for the fiscal years 2017-2019. We also provide annotations pertaining to the nature of questions, marking those that require hierarchical headers, domain-specific terminology, and paraphrased forms. Our zero-shot baseline evaluation of three transformer-based SOTA Table QA methods - TaPAS (end-to-end), TaBERT (semantic parsing-based), and RCI (row-column encoding-based) - clearly exposes the limitation of these methods in this practical setting, with the best accuracy at just 51.8% (RCI). We also present pragmatic table pre-processing steps used to pivot and project these complex tables into a layout suitable for the SOTA Table QA models.",Datasets & Benchmarks,NeurIPS,2021,Reject,Yannis Katsis;Saneem Ahmed Chemmengath;vishwajeet kumar;samarth bharadwaj;Mustafa Canim;Michael Glass;Alfio Gliozzo;Feifei Pan;Jaydeep Sen;Karthik Sankaranarayanan;Soumen Chakrabarti,True,https://openreview.net/pdf?id=cB3OdLInAr9
cIrPX-Sn5n,Benchmarking Multi-Agent Deep Reinforcement Learning Algorithms in Cooperative Tasks,"Multi-agent deep reinforcement learning (MARL) suffers from a lack of commonly-used evaluation tasks and criteria, making comparisons between approaches difficult. In this work, we provide a systematic evaluation and comparison of three different classes of MARL algorithms (independent learning, centralised multi-agent policy gradient, value decomposition) in a diverse range of cooperative multi-agent learning tasks. Our experiments serve as a reference for the expected performance of algorithms across different learning tasks, and we provide insights regarding the effectiveness of different learning approaches. We open-source EPyMARL, which extends the PyMARL codebase to include additional algorithms and allow for flexible configuration of algorithm implementation details such as parameter sharing. Finally, we open-source two environments for multi-agent research which focus on coordination under sparse rewards.",Datasets & Benchmarks,NeurIPS,2021,Poster,Georgios Papoudakis;Filippos Christianos;Lukas Schäfer;Stefano V Albrecht,False,https://openreview.net/pdf?id=cIrPX-Sn5n
cXCZnLjDm4s,"Trust, but Verify: Cross-Modality Fusion for HD Map Change Detection","High-definition (HD) map change detection is the task of determining when sensor data and map data are no longer in agreement with one another due to real-world changes. We collect the first dataset for the task, which we entitle the Trust, but Verify (TbV) dataset, by mining thousands of hours of data from over 9 months of autonomous vehicle fleet operations. We present learning-based formulations for solving the problem in the bird's eye view and ego-view. Because real map changes are infrequent and vector maps are easy to synthetically manipulate, we lean on simulated data to train our model. Perhaps surprisingly, we show that such models can generalize to real world distributions. The dataset consists of maps and logs collected in six North American cities, is one of the largest AV datasets to date with more than 7.9 million images. We make the data available to the public, along with code and models under the the CC BY-NC-SA 4.0 license.",Datasets & Benchmarks,NeurIPS,2021,Poster,John Lambert;James Hays,True,https://openreview.net/pdf?id=cXCZnLjDm4s
cbFfF4g9fIy,NAS-Bench-360: Benchmarking Diverse Tasks for Neural Architecture Search,"Most existing neural architecture search (NAS) benchmarks and algorithms prioritize performance on well-studied tasks, focusing on computer vision datasets such as CIFAR and ImageNet. However, the applicability of NAS approaches in other areas is not adequately understood. In this paper, we present NAS-Bench-360, a benchmark suite for evaluating state-of-the-art NAS methods on less-explored datasets. To do this, we organize a diverse array of tasks, from classification of simple deformations of natural images to predicting protein folding and partial differential equation (PDE) solving. Our evaluation pipeline compares architecture search spaces of different flavors, and reveals varying performance on different tasks, providing baselines for further use. All data and reproducible evaluation code are open-source and publicly available. The results of our evaluation show that current state-of-the-art NAS methods often struggle to compete with simple baselines and human-designed architectures on the majority of tasks in our benchmark. At the same time, they can be quite effective on a few individual, understudied tasks. This demonstrates the importance of evaluation on diverse tasks to better understand the usefulness of different approaches to architecture search and automation.",Datasets & Benchmarks,NeurIPS,2021,Reject,Renbo Tu;Mikhail Khodak;Nicholas Carl Roberts;Nina Balcan;Ameet Talwalkar,True,https://openreview.net/pdf?id=cbFfF4g9fIy
dA2Iov0Ecxt,Cluster3D: A Dataset and Benchmark for Clustering Non-Categorical 3D CAD Models,"We introduce the first large-scale dataset and benchmark for non-categorical annotation and clustering of 3D CAD models. We use the geometric data of the ABC dataset, and we develop an interface to allow expert mechanical engineers to efficiently annotate pairwise CAD model similarities, which we use to evaluate the performance of seven baseline deep clustering methods. Our dataset contains a manually annotated subset of 22,968 shapes, and 252,648 annotations. Our dataset is the first to directly target deep clustering algorithms for geometric shapes, and we believe it will be an important building block to analyze and utilize the massive 3D shape collections that are starting to appear in deep geometric computing. Our results suggest that, differently from the already mature shape classification algorithms, deep clustering algorithms for 3D CAD models are in their infancy and there is much room for improving their performance.",Datasets & Benchmarks,NeurIPS,2021,Reject,Siyuan Xiang;Ching Tseng;Congcong Wen;Deshana Desai;Yifeng Kou;Binil Starly;Daniele Panozzo;Chen Feng,True,https://openreview.net/pdf?id=dA2Iov0Ecxt
dA2Q8CfmGpp,BiToD: A Bilingual Multi-Domain Dataset For Task-Oriented Dialogue Modeling,"Task-oriented dialogue (ToD) benchmarks provide an important avenue to measure progress and develop better conversational agents. However, existing datasets for end-to-end ToD modeling are limited to a single language, hindering the development of robust end-to-end ToD systems for multilingual countries and regions. Here we introduce BiToD, the first bilingual multi-domain dataset for end-to-end task-oriented dialogue modeling. BiToD contains over 7k multi-domain dialogues (144k utterances) with a large and realistic bilingual knowledge base. It serves as an effective benchmark for evaluating bilingual ToD systems and cross-lingual transfer learning approaches. We provide state-of-the-art baselines under three evaluation settings (monolingual, bilingual, and cross-lingual). The analysis of our baselines in different settings highlights 1) the effectiveness of training a bilingual ToD system comparing to two independent monolingual ToD systems, and 2) the potential of leveraging a bilingual knowledge base and cross-lingual transfer learning to improve the system performance in the low resource condition.",Datasets & Benchmarks,NeurIPS,2021,Poster,Zhaojiang Lin;Andrea Madotto;Genta Indra Winata;Peng Xu;Feijun Jiang;Yuxiang Hu;Chen Shi;Pascale Fung,True,https://openreview.net/pdf?id=dA2Q8CfmGpp
dTp-VUFDIB,"PTR: A Benchmark for Part-based Conceptual, Relational, and Physical Reasoning","A critical aspect of human visual perception is the ability to parse visual scenes into individual objects and further into object parts, forming part-whole hierarchies. Such composite structures could induce a rich set of semantic concepts and relations, thus playing an important role in the interpretation and organization of visual signals as well as for the generalization of visual perception and reasoning. However, existing visual reasoning benchmarks mostly focus on objects rather than parts. Visual reasoning based on the full part-whole hierarchy is much more challenging than object-centric reasoning due to finer-grained concepts, richer geometry relations, and more complex physics. Therefore, to better serve for part-based conceptual, relational and physical reasoning, we introduce a new large-scale diagnostic visual reasoning dataset named PTR. PTR contains around 80k RGBD synthetic images with ground truth object and part level annotations regarding semantic instance segmentation, color attributes, spatial and geometric relationships, and certain physical properties such as stability. These images are paired with 800k machine-generated questions covering various types of reasoning types, making them a good testbed for visual reasoning models. We examine several state-of-the-art visual reasoning models on this dataset and observe that they still make many surprising mistakes in situations where humans can easily infer the correct answer. We believe this dataset will open up new opportunities for part-based reasoning. PTR dataset and baseline models are publicly available. ",main,NeurIPS,2021,Poster,Yining Hong;Li Yi;Joshua B. Tenenbaum;Antonio Torralba;Chuang Gan,True,https://openreview.net/pdf?id=dTp-VUFDIB
dTxWk9K5KC,EEG Thinking1 Datasets: Think-Count-Recall (TCR) and Read-Write-Type (RWT),"EEG-based Brain-Computer Interfaces (BCI) have been widely used in clinical and non-clinical research. In this paper, we present a framework to collect a large amount of EEG data with an easy-to-use experiment setup, using non-invasive, wireless, and affordable hardware. Interpretable feedback generated by benchmark machine learning algorithms has been provided to the researchers and end-users. Two existing datasets are used as case studies for the framework: Read-Write-Type (RWT) and Think-Count-Recall (TCR). The goal is to inspire new machine learning approaches for decoding behavior from large-scale EEG data. The framework of experimental design, data collection, data analysis, feedback generation, and community building could pave the way towards a future when everyone can easily use BCI systems every day, similar to smartphones nowadays.",Datasets & Benchmarks,NeurIPS,2021,Reject,Xiaodong Qu;Peiyan Liu,False,https://openreview.net/pdf?id=dTxWk9K5KC
db1InWAwW2T,ThreeDWorld: A Platform for Interactive Multi-Modal Physical Simulation,"We introduce ThreeDWorld (TDW), a platform for interactive multi-modal physical simulation. TDW enables the simulation of high-fidelity sensory data and physical interactions between mobile agents and objects in rich 3D environments. Unique properties include real-time near-photo-realistic image rendering; a library of objects and environments, and routines for their customization; generative procedures for efficiently building classes of new environments; high-fidelity audio rendering; realistic physical interactions for a variety of material types, including cloths, liquid, and deformable objects; customizable ``avatars” that embody AI agents; and support for human interactions with VR devices. TDW’s API enables multiple agents to interact within a simulation and returns a range of sensor and physics data representing the state of the world. We present initial experiments enabled by TDW in emerging research directions in computer vision, machine learning, and cognitive science, including multi-modal physical scene understanding, physical dynamics predictions, multi-agent interactions, models that ‘learn like a child’, and attention studies in humans and neural networks. ",Datasets & Benchmarks,NeurIPS,2021,Poster,Chuang Gan;Jeremy Schwartz;Seth Alter;Damian Mrowca;Martin Schrimpf;James Traer;Julian De Freitas;Jonas Kubilius;Abhishek Bhandwaldar;Nick Haber;Megumi Sano;Kuno Kim;Elias Wang;Michael Lingelbach;Aidan Curtis;Kevin Tyler Feigelis;Daniel Bear;Dan Gutfreund;David Daniel Cox;Antonio Torralba;James J. DiCarlo;Joshua B. Tenenbaum;Josh Mcdermott;Daniel LK Yamins,False,https://openreview.net/pdf?id=db1InWAwW2T
e82_BlJL43M,RB2: Robotic Manipulation Benchmarking with a Twist,"Benchmarks offer a scientific way to compare algorithms using objective performance metrics.
Good benchmarks have two features: (a) they should be widely useful for many research groups; (b) and they should produce reproducible findings. In robotic manipulation research, there is a trade-off between reproducibility and broad accessibility. If the benchmark is kept restrictive (fixed hardware, objects), the numbers are reproducible but the setup becomes less general. On the other hand, a benchmark could be a loose set of protocols (e.g. object set) but the underlying variation in setups make the results non-reproducible. In this paper, we re-imagine benchmarking for robotic manipulation as state-of-the-art algorithmic implementations, alongside the usual set of tasks and experimental protocols. The added baseline implementations will provide a way to easily recreate SOTA numbers in a new local robotic setup, thus providing credible relative rankings between existing approaches and new work. However, these ""local rankings"" could vary between different setups. To resolve this issue, we build a mechanism for pooling experimental data between labs, and thus we establish a single global ranking for existing (and proposed) SOTA algorithms. Our benchmark, called Ranking-Based Robotics Benchmark (RB2), is evaluated on tasks that are inspired from clinically validated Southampton Hand Assessment Procedures. Our benchmark was run across two different labs and reveals several surprising findings. For example, extremely simple baselines like open-loop behavior cloning, outperform more complicated models (e.g. closed loop, RNN, Offline-RL, etc.) that are preferred by the field. We hope our fellow researchers will use RB2 to improve their research's quality and rigor.",Datasets & Benchmarks,NeurIPS,2021,Poster,Sudeep Dasari;Jianren Wang;Joyce Hong;Shikhar Bahl;Yixin Lin;Austin S Wang;Abitha Thankaraj;Karanbir Singh Chahal;Berk Calli;Saurabh Gupta;David Held;Lerrel Pinto;Deepak Pathak;Vikash Kumar;Abhinav Gupta,False,https://openreview.net/pdf?id=e82_BlJL43M
e9P6bypUFd,A Realistic Simulation Framework for Learning with Label Noise,"We propose a simulation framework for generating realistic instance-dependent noisy labels via a pseudo-labeling paradigm. We show that this framework generates synthetic noisy labels that exhibit important characteristics of the label noise in practical settings via comparison with the CIFAR10-H dataset. Equipped with controllable label noise, we study the negative impact of noisy labels across a few realistic settings to understand when label noise is more problematic. Additionally, with the availability of annotator information from our simulation framework, we propose a new technique, Label Quality Model (LQM), that leverages annotator features to predict and correct against noisy labels. We show that by adding LQM as a label correction step before applying existing noisy label techniques, we can further improve the models' performance.",Datasets & Benchmarks,NeurIPS,2021,Reject,Keren Gu;Xander Masotto;Vandana Bachani;Balaji Lakshminarayanan;Jack Nikodem;Dong Yin,False,https://openreview.net/pdf?id=e9P6bypUFd
eLYinD0TtIt,Pl@ntNet-300K: a plant image dataset with high label ambiguity and a long-tailed distribution,"This paper presents a novel image dataset with high intrinsic ambiguity specifically built for evaluating and comparing set-valued classifiers. This dataset, built from the database of Pl@ntnet citizen observatory, consists of 306,146 images covering 1,081 species. We highlight two particular features of the dataset, inherent to the way the images are acquired and to the intrinsic diversity of plants morphology:
    i) The dataset has a strong class imbalance, meaning that a few species account for most of the images.
    ii) Many species are visually similar, making identification difficult even for the expert eye.
These two characteristics make the present dataset well suited for the evaluation of set-valued classification methods and algorithms. Therefore, we recommend two set-valued evaluation metrics associated with the dataset (mean top-k accuracy and mean average-k accuracy) and we provide the results of a baseline approach based on a deep neural network trained with the cross-entropy loss.",Datasets & Benchmarks,NeurIPS,2021,Poster,Camille Garcin;alexis joly;Pierre Bonnet;Antoine Affouard;Jean-Christophe Lombardo;Mathias Chouet;Maximilien Servajean;Titouan Lorieul;Joseph Salmon,True,https://openreview.net/pdf?id=eLYinD0TtIt
eOOiCyZ_h9f,StudentSADD: Mobile Depression and Suicidal Ideation Screening of College Students during the Coronavirus Pandemic,"The growing prevalence of depression and suicidal ideation among college students is alarming, with the Coronavirus pandemic  further highlighting the need for universal mental illness screening technology. While traditional screening questionnaires are too burdensome to achieve universal screening in this population, data collected through mobile applications has the potential to identify at-risk students. However, knowing the modalities that students are  willing to share and that contain strong screening capabilities is critical for developing such mental illness screening technology. Thus, we deployed a mobile application to over 300 students during the pandemic to collect the Student Suicidal Ideation and Depression Detection (StudentSADD) dataset. Overall, students were most willing to share text responses, unscripted voice recordings, and scripted voice recordings. To provide baselines, we trained machine learning and deep learning methods on these modalities to screen for depression and suicidal ideation. The novel  StudentSADD dataset is a valuable resource for developing mobile mental illness screening technologies.",Datasets & Benchmarks,NeurIPS,2021,Reject,ML Tlachac;Ricardo Flores;Miranda Reisch;Rimsha Kayastha;Nina Taurich;Veronica Melican;Connor Bruneau;Hunter Caouette;Ermal Toto;Elke Rundensteiner,True,https://openreview.net/pdf?id=eOOiCyZ_h9f
eYg8ssXm3BT,A Framework for Cluster and Classifier Evaluation in the Absence of Reference Labels,"In some problem spaces the high cost of obtaining ground truth labels necessitates use of lower quality reference datasets. 
It is difficult to benchmark model changes using these datasets, as evaluation results may be misleading or biased. We propose a supplement to using reference labels which we call an approximate ground truth refinement (AGTR). Using an AGTR we prove that bounds on the precision and recall of a clustering algorithm or multiclass classifier can be computed without reference labels. We introduce a litmus test that uses an AGTR to identify inaccurate evaluation results produced from reference datasets of dubious quality. Creating an AGTR requires domain knowledge, and malware family classification is a task with robust domain knowledge approaches that support the construction of an AGTR. We demonstrate our AGTR evaluation framework by applying it to a popular malware labeling tool to diagnose over-fitting in prior testing and evaluate changes that could not be meaningfully quantified in their impact under previous data.",Datasets & Benchmarks,NeurIPS,2021,Reject,Robert J Joyce;Edward Raff;Charles K. Nicholas,False,https://openreview.net/pdf?id=eYg8ssXm3BT
eZu4BZxlRnX,Alchemy: A benchmark and analysis toolkit for meta-reinforcement learning agents,"There has been rapidly growing interest in meta-learning as a method for increasing the flexibility and sample efficiency of reinforcement learning. One problem in this area of research, however, has been a scarcity of adequate benchmark tasks. In general, the structure underlying past benchmarks has either been too simple to be inherently interesting, or too ill-defined to support principled analysis. In the present work, we introduce a new benchmark for meta-RL research, emphasizing transparency and potential for in-depth analysis as well as structural richness. Alchemy is a 3D video game, implemented in Unity, which involves a latent causal structure that is resampled procedurally from episode to episode, affording structure learning, online inference, hypothesis testing and action sequencing based on abstract domain knowledge. We evaluate a pair of powerful RL agents on Alchemy and present an in-depth analysis of one of these agents. Results clearly indicate a frank and specific failure of meta-learning, providing validation for Alchemy as a challenging benchmark for meta-RL. Concurrent with this report, we are releasing Alchemy as public resource, together with a suite of analysis tools and sample agent trajectories.",Datasets & Benchmarks,NeurIPS,2021,Poster,Jane X Wang;Michael King;Nicolas Pierre Mickael Porcel;Zeb Kurth-Nelson;Tina Zhu;Charlie Deck;Peter Choy;Mary Cassin;Malcolm Reynolds;H. Francis Song;Gavin Buttimore;David P Reichert;Neil Charles Rabinowitz;Loic Matthey;Demis Hassabis;Alexander Lerchner;Matthew Botvinick,True,https://openreview.net/pdf?id=eZu4BZxlRnX
enYjtbjYJrf,Chaos as an interpretable benchmark for forecasting and data-driven modelling,"The striking fractal geometry of strange attractors underscores the generative nature of chaos: like probability distributions, chaotic systems can be repeatedly measured to produce arbitrarily-detailed information about the underlying attractor. Chaotic systems thus pose a unique challenge to modern statistical learning techniques, while retaining quantifiable mathematical properties that make them controllable and interpretable as benchmarks. Here, we present a growing database currently comprising 131 known chaotic dynamical systems, each paired with corresponding precomputed multivariate and univariate time series. Our dataset has comparable scale to existing static time series databases; however, our systems can be re-integrated to produce additional datasets of arbitrary length and granularity. Our dataset is annotated with known mathematical properties of each system, and we perform feature analysis to broadly categorize the diverse dynamics present across our dataset. Chaotic systems inherently challenge forecasting models, and across extensive benchmarks we correlate forecasting performance with the degree of chaos present.  We also exploit the unique generative properties of our dataset in several proof-of-concept experiments: surrogate transfer learning to improve time series classification, importance sampling to accelerate model training, and benchmarking symbolic regression algorithms.",Datasets & Benchmarks,NeurIPS,2021,Poster,William Gilpin,True,https://openreview.net/pdf?id=enYjtbjYJrf
erOBVUgvryF,JECC: Commonsense Reasoning Tasks Derived fromInteractive Fictions,"Commonsense reasoning simulates the human ability to make presumptions about our physical world, and it is an essential cornerstone in building general AI systems. We propose a new commonsense reasoning dataset based on human's Interactive Fiction  (IF) gameplay walkthroughs as human players demonstrate plentiful and diverse commonsense reasoning. The new dataset provides a natural mixture of various reasoning types and requires multi-hop reasoning. Moreover, the IF game-based construction procedure requires much less human interventions than previous ones. Experiments show that the introduced dataset is challenging to previous machine reading models with a significant 20% performance gap compared to human experts.",Datasets & Benchmarks,NeurIPS,2021,Reject,Mo Yu;Xiaoxiao Guo;Yufei Feng;Yi Gu;Xiaodan Zhu;Michael Greenspan;Murray Campbell;Chuang Gan,True,https://openreview.net/pdf?id=erOBVUgvryF
f5zF1mgCLR0,A2X: An Agent and Environment Interaction Benchmark for Multimodal Human Trajectory Prediction,"Recent trends in human trajectory prediction are the development of generative models which generate distributions of trajectories. However existing metrics are suited only for single (unimodal) trajectory instances. Furthermore, existing datasets are largely limited to small-scale interactions between people, with little to no agent-to-agent environment interaction. To address these challenges, we propose a dataset that compensates for the lack of agent-to-environment interaction in existing datasets with a new simulated dataset and metrics to convey model performance with more reliability and nuance. A subset of these metrics are novel multiverse metrics, which are better-suited for multimodal models than existing metrics but are still applicable to unimodal models. Our results showcase the benefits of the augmented dataset and metrics. The dataset is available at: https://mubbasir.github.io/HTP-benchmark/.",Datasets & Benchmarks,NeurIPS,2021,Reject,Samuel S. Sohn;Mihee Lee;Seonghyeon Moon;Gang Qiao;Usman Muhammad;Sejong Yoon;vladimir pavlovic;Mubbasir Kapadia,True,https://openreview.net/pdf?id=f5zF1mgCLR0
fCjd2bXG5iI,Can fMRI reveal the representation of syntactic structure in the brain?,"While studying semantics in the brain, neuroscientists use two approaches. One is to identify areas that are correlated with semantic processing load. Another is to find areas that are predicted by the semantic representation of the stimulus words. However, most studies of syntax have focused only on identifying areas correlated with syntactic processing load.  One possible reason for this discrepancy is that representing syntactic structure in an embedding space such that it can be used to model brain activity is a non-trivial computational problem. Another possible reason is that it is unclear if the low signal-to-noise ratio of neuroimaging tools such as functional Magnetic Resonance Imaging (fMRI) can allow us to reveal the correlates of complex (and perhaps subtle) syntactic representations. In this study, we propose novel multi-dimensional features that encode information about the syntactic structure of sentences. Using these features and fMRI recordings of participants reading a natural text, we model the brain representation of syntax. First, we find that our syntactic structure-based features explain additional variance in the brain activity of various parts of the language system, even after controlling for complexity metrics that capture processing load. At the same time, we see that regions well-predicted by syntactic features are distributed in the language system and are not distinguishable from those processing semantics. Our code and data will be available at https://github.com/anikethjr/brain_syntactic_representations.",main,NeurIPS,2021,Poster,Aniketh Janardhan Reddy;Leila Wehbe,True,https://openreview.net/pdf?id=fCjd2bXG5iI
fE_gwAAKM7O,Natural Adversarial Objects,"Although state-of-the-art object detection methods have shown compelling performance, models often are not robust to adversarial attacks and out-of-distribution data.

We introduce a new dataset, Natural Adversarial Objects (NAO), to evaluate the robustness of object detection models. NAO contains 7,936 images and 13,604 objects that are unmodified, but cause state-of-the-art detection models to misclassify with high confidence. The mean average precision (mAP) of EfficientDet-D7 drops 68.3\\\\% when evaluated on NAO compared to the standard MSCOCO validation set.

We investigate why examples in NAO are difficult to detect and classify. Experiments of shuffling image patches reveal that models are overly sensitive to local texture. Additionally, using integrated gradients and background replacement, we find that the detection model is reliant on pixel information within the bounding box, and insensitive to the background context when predicting class labels.",Datasets & Benchmarks,NeurIPS,2021,Reject,Felix Lau;Sasha Harrison;Nishant Subramani;Aerin Kim;Elliot Branson;Rosanne Liu,True,https://openreview.net/pdf?id=fE_gwAAKM7O
fU7-so5RRhW,Clockwork Variational Autoencoders,"Deep learning has enabled algorithms to generate realistic images. However, accurately predicting long video sequences requires understanding long-term dependencies and remains an open challenge. While existing video prediction models succeed at generating sharp images, they tend to fail at accurately predicting far into the future. We introduce the Clockwork VAE (CW-VAE), a video prediction model that leverages a hierarchy of latent sequences, where higher levels tick at slower intervals. We demonstrate the benefits of both hierarchical latents and temporal abstraction on 4 diverse video prediction datasets with sequences of up to 1000 frames, where CW-VAE outperforms top video prediction models. Additionally, we propose a Minecraft benchmark for long-term video prediction. We conduct several experiments to gain insights into CW-VAE and confirm that slower levels learn to represent objects that change more slowly in the video, and faster levels learn to represent faster objects.",main,NeurIPS,2021,Poster,Vaibhav Saxena;Jimmy Ba;Danijar Hafner,True,https://openreview.net/pdf?id=fU7-so5RRhW
fbAHHm_jyo2,Tracking Without Re-recognition in Humans and Machines,"Imagine trying to track one particular fruitfly in a swarm of hundreds. Higher biological visual systems have evolved to track moving objects by relying on both their appearance and their motion trajectories. We investigate if state-of-the-art spatiotemporal deep neural networks are capable of the same. For this, we introduce PathTracker, a synthetic visual challenge that asks human observers and machines to track a target object in the midst of identical-looking ""distractor"" objects. While humans effortlessly learn PathTracker and generalize to systematic variations in task design, deep networks struggle. To address this limitation, we identify and model circuit mechanisms in biological brains that are implicated in tracking objects based on motion cues. When instantiated as a recurrent network, our circuit model learns to solve PathTracker with a robust visual strategy that rivals human performance and explains a significant proportion of their decision-making on the challenge. We also show that the success of this circuit model extends to object tracking in natural videos. Adding it to a transformer-based architecture for object tracking builds tolerance to visual nuisances that affect object appearance, establishing the new state of the art on the large-scale TrackingNet challenge. Our work highlights the importance of understanding human vision to improve computer vision.

",main,NeurIPS,2021,Poster,Drew Linsley;Girik Malik;Junkyung Kim;Lakshmi Narasimhan Govindarajan;Ennio Mingolla;Thomas Serre,True,https://openreview.net/pdf?id=fbAHHm_jyo2
fe_hCc4RBrg,Programming Puzzles,"We introduce a new type of programming challenge called programming puzzles, as an objective and comprehensive evaluation of program synthesis, and release an open-source dataset of Python Programming Puzzles (P3). Each puzzle is defined by a short Python program $f$, and the goal is to find an input which makes $f$ return True. The puzzles are objective in that each one is specified entirely by the source code of its verifier $f$, so evaluating $f$ is all that is needed to test a candidate solution. They do not require an answer key or input/output examples, nor do they depend on natural language understanding. The dataset is comprehensive in that it spans problems of a range of difficulties and domains, ranging from trivial string manipulation problems, to classic programming puzzles (e.g., Tower of Hanoi), to interview/competitive-programming problems (e.g., dynamic programming), to longstanding open problems in algorithms and mathematics (e.g., factoring). We develop baseline enumerative program synthesis, GPT-3 and Codex solvers that are capable of solving puzzles---even without access to any reference solutions---by learning from their own past solutions. Codex performs best, solving up to 18% of 397 test problems with a single try and 80% of the problems with 1,000 tries per problem. In a small user study, we find a positive correlation between puzzle-solving performance and coding experience, and between the puzzle difficulty for humans and AI solvers. Therefore, further improvements on P3 could have a significant impact on many program synthesis areas.",Datasets & Benchmarks,NeurIPS,2021,Poster,Tal Schuster;Ashwin Kalyan;Alex Polozov;Adam Tauman Kalai,True,https://openreview.net/pdf?id=fe_hCc4RBrg
fgFBtYgJQX_,Isaac Gym: High Performance GPU Based Physics Simulation For Robot Learning,Isaac Gym offers a high-performance learning platform to train policies for a wide variety of robotics tasks entirely on GPU. Both physics simulation and neural network policy training reside on GPU and communicate by directly passing data from physics buffers to PyTorch tensors without ever going through CPU bottlenecks. This leads to blazing fast training times for complex robotics tasks on a single GPU with 2-3 orders of magnitude improvements compared to conventional RL training that uses a CPU-based simulator and GPUs for neural networks. We host the results and videos at https://sites.google.com/view/isaacgym-nvidia and Isaac Gym can be downloaded at https://developer.nvidia.com/isaac-gym. The benchmark and environments are available at https://github.com/NVIDIA-Omniverse/IsaacGymEnvs. ,Datasets & Benchmarks,NeurIPS,2021,Poster,Viktor Makoviychuk;Lukasz Wawrzyniak;Yunrong Guo;Michelle Lu;Kier Storey;Miles Macklin;David Hoeller;Nikita Rudin;Arthur Allshire;Ankur Handa;Gavriel State,False,https://openreview.net/pdf?id=fgFBtYgJQX_
fl5PtMeGJ8z,CatLC: Catalonia Multiresolution Land Cover Dataset,"Traditional natural image datasets are very rich. However, only a few remote sensing datasets are available and cover a tiny territory or cover a larger one with low spatial resolution and/or few classes. In this paper, we present the Catalonia Multiresolution Land Cover Dataset (CatLC), a remote sensing dataset. The dataset contains images at different spatial resolutions captured by both aircraft and satellites (Sentinel-1 and Sentinel-2), in addition to topographic maps. All this dataset has been created with images from the Cartographic and Geological Institute of Catalonia (ICGC) catalogs and the European Space Agency (ESA). The ICGC's land cover ground truth accompanies these images with 41 classes at a spatial resolution of 1 m in an area of 32000 km2, covering the Spanish region of Catalonia. CatLC is a multilayer, multiresolution, multimodal, multitemporal dataset, which has excellent potential for the Artificial Intelligence (AI) community and the exploration of modeling methodologies. Land cover maps are used in different realms such as forestry for inventory area estimates, hydrology regarding microclimatic variables, agriculture to improve irrigation or geology in geohazards, and risk identification and assessment. Therefore, accurate and updated knowledge about land changes is essential for territory management with different purposes over multiple fields. Using various combinations of the images from the dataset, we offer a benchmark that could serve as a starting point to explore artificial intelligence techniques for remote sensing segmentation purposes. In this vein, CatLC dataset aims to engage with computer vision experts interested in remote sensing and stimulate research and development.",Datasets & Benchmarks,NeurIPS,2021,Reject,Carlos García Rodríguez;Oscar Mora;Fernando Pérez-Aragüés;Jordi Vitria,True,https://openreview.net/pdf?id=fl5PtMeGJ8z
fnuAjFL7MXy,The Met Dataset: Instance-level Recognition for Artworks,"This work introduces a dataset for large-scale instance-level recognition in the domain of artworks. The proposed benchmark exhibits a number of different challenges such as large inter-class similarity, long tail distribution, and many classes. We rely on the open access collection of The Met museum to form a large training set of about 224k classes, where each class corresponds to a museum exhibit with photos taken under studio conditions. Testing is primarily performed on photos taken by museum guests depicting exhibits, which introduces a distribution shift between training and testing. Testing is additionally performed on a set of images not related to Met exhibits making the task resemble an out-of-distribution detection problem. The proposed benchmark follows the paradigm of other recent datasets for instance level recognition on different domains to encourage research on domain independent approaches. A number of suitable approaches are evaluated to offer a testbed for future comparisons. Self-supervised and supervised contrastive learning are effectively combined to train the backbone which is used for non-parametric classification that is shown as a promising direction. Dataset webpage: http://cmp.felk.cvut.cz/met/",Datasets & Benchmarks,NeurIPS,2021,Poster,Nikolaos-Antonios Ypsilantis;Noa Garcia;Guangxing Han;Sarah Ibrahimi;Nanne Van Noord;Giorgos Tolias,True,https://openreview.net/pdf?id=fnuAjFL7MXy
gISH-80g05u,BlendGAN: Implicitly GAN Blending for Arbitrary Stylized Face Generation,"Generative Adversarial Networks (GANs) have made a dramatic leap in high-fidelity image synthesis and stylized face generation. Recently, a layer-swapping mechanism has been developed to improve the stylization performance. However, this method is incapable of fitting arbitrary styles in a single model and requires hundreds of style-consistent training images for each style. To address the above issues, we propose BlendGAN for arbitrary stylized face generation by leveraging a flexible blending strategy and a generic artistic dataset. Specifically, we first train a self-supervised style encoder on the generic artistic dataset to extract the representations of arbitrary styles. In addition, a weighted blending module (WBM) is proposed to blend face and style representations implicitly and control the arbitrary stylization effect. By doing so, BlendGAN can gracefully fit arbitrary styles in a unified model while avoiding case-by-case preparation of style-consistent training images. To this end, we also present a novel large-scale artistic face dataset AAHQ. Extensive experiments demonstrate that BlendGAN outperforms state-of-the-art methods in terms of visual quality and style diversity for both latent-guided and reference-guided stylized face synthesis.",main,NeurIPS,2021,Poster,Mingcong Liu;Qiang Li;Zekui Qin;Guoxin Zhang;Pengfei Wan;Wen Zheng,True,https://openreview.net/pdf?id=gISH-80g05u
gN35BGa1Rt,"A sandbox for prediction and integration of DNA, RNA, and proteins in single cells","The last decade has witnessed a technological arms race to encode the molecular states of cells into DNA libraries, turning DNA sequencers into scalable single-cell microscopes. Single-cell measurement of chromatin accessibility (DNA), gene expression (RNA), and proteins has revealed rich cellular diversity across tissues, organisms, and disease states. However, single-cell data poses a unique set of challenges. A dataset may comprise millions of cells with tens of thousands of sparse features. Identifying biologically relevant signals from the background sources of technical noise requires innovation in predictive and representational learning. Furthermore, unlike in machine vision or natural language processing, biological ground truth is limited. Here we leverage recent advances in multi-modal single-cell technologies which, by simultaneously measuring two layers of cellular processing in each cell, provide ground truth analogous to language translation. We define three key tasks to predict one modality from another and learn integrated representations of cellular state. We also generate a novel dataset of the human bone marrow specifically designed for benchmarking studies. The dataset and tasks are accessible through an open-source framework that facilitates centralized evaluation of community-submitted methods.",Datasets & Benchmarks,NeurIPS,2021,Poster,Malte D Luecken;Daniel Bernard Burkhardt;Robrecht Cannoodt;Christopher Lance;Aditi Agrawal;Hananeh Aliee;Ann T Chen;Louise Deconinck;Angela M Detweiler;Alejandro A Granados;Shelly Huynh;Laura Isacco;Yang Joon Kim;Dominik Klein;BONY DE KUMAR;Sunil Kuppasani;Heiko Lickert;Aaron McGeever;Honey Mekonen;Joaquin Caceres Melgarejo;Maurizio Morri;Michaela Müller;Norma Neff;Sheryl Paul;Bastian Rieck;Kaylie Schneider;Scott Steelman;Michael Sterr;Daniel J. Treacy;Alexander Tong;Alexandra-Chloe Villani;Guilin Wang;Jia Yan;Ce Zhang;Angela Oliveira Pisco;Smita Krishnaswamy;Fabian J Theis;Jonathan M. Bloom,True,https://openreview.net/pdf?id=gN35BGa1Rt
gWIbXsrtOCc,Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning,"Inducing causal relationships from observations is a classic problem in machine learning. Most work in causality starts from the premise that the causal variables themselves  are observed. However, for AI agents such as robots trying to make sense of their environment, the only observables are low-level variables like pixels in images. To generalize well, an agent must induce high-level variables, particularly those which are causal or are affected by causal variables. A central goal for AI and causality is thus the joint discovery of abstract representations and causal structure. However, we note that existing environments for studying causal induction are poorly suited for this  objective because they have complicated task-specific causal graphs which are impossible to manipulate parametrically (e.g., number of nodes, sparsity, causal chain length, etc.). In this work, our goal is to facilitate research in learning representations of high-level variables as well as causal structures among them. In order  to systematically probe the ability of methods to identify these variables and structures, we design a suite of benchmarking RL environments. We evaluate various representation learning algorithms from the literature and find that explicitly incorporating structure and modularity in models can help causal induction in model-based reinforcement learning.",Datasets & Benchmarks,NeurIPS,2021,Poster,Nan Rosemary Ke;Aniket Rajiv Didolkar;Sarthak Mittal;Anirudh Goyal;Guillaume Lajoie;Stefan Bauer;Danilo Jimenez Rezende;Yoshua Bengio;Christopher Pal;Michael Curtis Mozer,False,https://openreview.net/pdf?id=gWIbXsrtOCc
gx7TEqAogg8,Challenging America: Digitized Newspapers as a Source of Machine Learning Challenges,"This paper introduces an ML challenge, named ChallAm, based on OCR excerpts from historical newspapers collected on the Chronicling America portal. ChallAm provides a dataset of OCR excerpts, labeled with metadata on their origin and paired with their textual contents retrieved by an OCR tool. Three ML tasks are defined in the challenge: determining the article date, detecting the location of the issue, and deducing a word in a text gap. The challenge is published on the Gonito platform, an evaluation environment for ML tasks, which presents a leader-board of all submitted solutions. Baselines are provided in Gonito for all three tasks of the challenge.
",Datasets & Benchmarks,NeurIPS,2021,Reject,Filip Graliński;Jakub Pokrywka;Krzysztof Jassem;Krzysztof Jan Jurkiewicz;Piotr Wierzchon;Karol Kaczmarek,True,https://openreview.net/pdf?id=gx7TEqAogg8
h-flVCIlstW,FEVEROUS: Fact Extraction and VERification Over Unstructured and Structured information,"Fact verification has attracted a lot of attention in the machine learning and natural language processing communities, as it is one of the key methods for detecting misinformation. Existing large-scale benchmarks for this task have focused mostly on textual sources, i.e. unstructured information, and thus ignored the wealth of information available in structured formats, such as tables. In this paper we introduce a novel dataset and benchmark, Fact Extraction and VERification Over Unstructured and Structured information (FEVEROUS), which consists of 87,026 verified claims. Each claim is annotated with evidence in the form of sentences and/or cells from tables in Wikipedia, as well as a label indicating whether this evidence supports, refutes, or does not provide enough information to reach a verdict. Furthermore, we detail our efforts to track and minimize the biases present in the dataset and could be exploited by models, e.g. being able to predict the label without using evidence. Finally, we develop a baseline for verifying claims against text and tables which predicts both the correct evidence and verdict for 18% of the claims.
",Datasets & Benchmarks,NeurIPS,2021,Poster,Rami Aly;Zhijiang Guo;Michael Sejr Schlichtkrull;James Thorne;Andreas Vlachos;Christos Christodoulopoulos;Oana Cocarascu;Arpit Mittal,True,https://openreview.net/pdf?id=h-flVCIlstW
hhKA5k0oVy5,Variance-Aware Machine Translation Test Sets,"We release 70 small and discriminative test sets for machine translation (MT) evaluation called variance-aware test sets (VAT), covering 35 translation directions from WMT16 to WMT20 competitions. VAT is automatically created by a novel variance-aware filtering method that filters the indiscriminative test instances of the current MT benchmark without any human labor. Experimental results show that VAT outperforms the original WMT benchmark in terms of the correlation with human judgment across mainstream language pairs and test sets. Further analysis on the properties of VAT reveals the challenging linguistic features (e.g., translation of low-frequency words and proper nouns) for the competitive MT systems, providing guidance for constructing future MT test sets. The test sets and the code for preparing variance-aware MT test sets are freely available at https://github.com/NLP2CT/Variance-Aware-MT-Test-Sets.",Datasets & Benchmarks,NeurIPS,2021,Poster,Runzhe Zhan;Xuebo Liu;Derek F. Wong;Lidia S. Chao,True,https://openreview.net/pdf?id=hhKA5k0oVy5
hjBEEXWNFH3,Play to Grade: Testing Coding Games as Classifying Markov Decision Process,"Contemporary coding education often presents students with the task of developing programs that have user interaction and complex dynamic systems, such as mouse based games. While pedagogically compelling, there are no contemporary autonomous methods for providing feedback. Notably, interactive programs are impossible to grade by traditional unit tests. 
In this paper we formalize the challenge of providing feedback to interactive programs as a task of classifying Markov Decision Processes (MDPs). Each student's program fully specifies an MDP where the agent needs to operate and decide, under reasonable generalization, if the dynamics and reward model of the input MDP should be categorized as correct or broken. We demonstrate that by designing a cooperative objective between an agent and an autoregressive model, we can use the agent to sample differential trajectories from the input MDP that allows a classifier to determine membership: Play to Grade. Our method enables an automatic feedback system for interactive code assignments. We release a dataset of 711,274 anonymized student submissions to a single assignment with hand-coded bug labels to support future research.",main,NeurIPS,2021,Poster,Allen Nie;Emma Brunskill;Christopher J Piech,True,https://openreview.net/pdf?id=hjBEEXWNFH3
hjsJraoqn1Y,A realistic approach to generate masked faces applied on two novel masked face recognition data sets,"The COVID-19 pandemic raises the problem of adapting face recognition systems to the new reality, where people may wear surgical masks to cover their noses and mouths. Traditional data sets (e.g., CelebA, CASIA-WebFace) used for training these systems were released before the pandemic, so they now seem unsuited due to the lack of examples of people wearing masks. We propose a method for enhancing data sets containing faces without masks by creating synthetic masks and overlaying them on faces in the original images. Our method relies on SparkAR Studio, a developer program made by Facebook that is used to create Instagram face filters. In our approach, we use 9 masks of different colors, shapes and fabrics. We employ our method to generate a number of 445,446 (90%) samples of masks for the CASIA-WebFace data set and 196,254 (96.8%) masks for the CelebA data set, releasing the mask images at https://github.com/securifai/masked_faces. We show that our method produces significantly more realistic training examples of masks overlaid on faces by asking volunteers to qualitatively compare it to other methods or data sets designed for the same task. We also demonstrate the usefulness of our method by evaluating state-of-the-art face recognition systems (FaceNet, VGG-face, ArcFace) trained on our enhanced data sets and showing that they outperform equivalent systems trained on original data sets (containing faces without masks) or competing data sets (containing masks generated by related methods), when the test benchmarks contain masked faces.",Datasets & Benchmarks,NeurIPS,2021,Poster,Tudor-Alexandru Mare;Georgian Duta;Iuliana Georgescu;Adrian Sandru;Bogdan Alexe;Marius Popescu;Radu Tudor Ionescu,True,https://openreview.net/pdf?id=hjsJraoqn1Y
hwjnu6qW7E4,Personalized Benchmarking with the Ludwig Benchmarking Toolkit,"The rapid proliferation of machine learning models across domains and deployment settings has given rise to various communities (e.g. industry practitioners) which seek to benchmark models across tasks and objectives of personal value. Unfortunately, these users cannot use standard benchmark results to perform such value-driven comparisons as traditional benchmarks evaluate models on a single objective (e.g. average accuracy) and fail to facilitate a standardized training framework that controls for confounding variables (e.g. computational budget), making fair comparisons difficult. To address these challenges, we introduce the open-source Ludwig Benchmarking Toolkit (LBT), a personalized benchmarking toolkit for running end-to-end benchmark studies (from hyperparameter optimization to evaluation) across an easily extensible set of tasks, deep learning models, datasets and evaluation metrics. LBT provides a configurable interface for controlling training and customizing evaluation, a standardized training framework for eliminating confounding variables, and support for multi-objective evaluation. We demonstrate how LBT can be used to create personalized benchmark studies with a large-scale comparative analysis for text classification across 7 models and 9 datasets. We explore the trade-offs between inference latency and performance, relationships between dataset attributes and performance, and the effects of pretraining on convergence and robustness, showing how LBT can be used to satisfy various benchmarking objectives.",Datasets & Benchmarks,NeurIPS,2021,Poster,Avanika Narayan;Piero Molino;Karan Goel;Willie Neiswanger;Christopher Re,False,https://openreview.net/pdf?id=hwjnu6qW7E4
hwoK62_GkiT,Multi-Agent Reinforcement Learning for Active Voltage Control on Power Distribution Networks,"This paper presents a problem in power networks that creates an exciting and yet challenging real-world scenario for application of multi-agent reinforcement learning (MARL). The emerging trend of decarbonisation is placing excessive stress on power distribution networks. Active voltage control is seen as a promising solution to relieve power congestion and improve voltage quality without extra hardware investment, taking advantage of the controllable apparatuses in the network, such as roof-top photovoltaics (PVs) and static var compensators (SVCs). These controllable apparatuses appear in a vast number and are distributed in a wide geographic area, making MARL a natural candidate. This paper formulates the active voltage control problem in the framework of Dec-POMDP and establishes an open-source environment. It aims to bridge the gap between the power community and the MARL community and be a drive force towards real-world applications of MARL algorithms. Finally, we analyse the special characteristics of the active voltage control problems that cause challenges (e.g. interpretability) for state-of-the-art MARL approaches, and summarise the potential directions.",main,NeurIPS,2021,Poster,Jianhong Wang;Wangkun Xu;Yunjie Gu;Wenbin Song;Tim C Green,True,https://openreview.net/pdf?id=hwoK62_GkiT
iBLHqLgbRn,VISIOCITY: A New Benchmarking Dataset and Evaluation Framework Towards Realistic Video Summarization,"Automatic video summarization has attracted a lot of interest, but is still an unsolved problem due to several challenges. The currently available datasets either have very short videos or have a few long videos of only a particular type. We introduce a new benchmarking video dataset called VISIOCITY (VIdeo SummarIzatiOn based on Continuity, Intent and DiversiTY) which consists of longer videos across six different domains with dense concept annotations capable of supporting different flavors of video summarization and other vision problems. Secondly, supervised video summarization techniques require many human reference summaries as ground truth. Acquiring them is not easy, especially for long videos. We propose a strategy to automatically generate multiple reference summaries using the annotations present in VISIOCITY and show that these are at par with the human summaries. The annotations thus serve as indirect ground truth. Thirdly, due to the highly subjective nature of the task, different ideal reference summaries of long videos can be quite different from each other. Due to this, the current practice of evaluating a summary vis-a-vis a limited set of human summaries and over-dependence on a single measure has its shortcomings. Our proposed evaluation framework overcomes these and offers a better quantitative assessment of a summary's quality. Finally, based on the above observations we present insights into how a mixture model can be easily enhanced to yield better summaries and demonstrate the effectiveness of our recipe in doing so as compared to some of the representative state-of-the-art techniques when tested on VISIOCITY. We make VISIOCITY publicly available via our website (https://visiocity.github.io/).",Datasets & Benchmarks,NeurIPS,2021,Reject,Vishal Kaushal;Suraj Nandkishor Kothawade;Rishabh K Iyer;Ganesh Ramakrishnan,True,https://openreview.net/pdf?id=iBLHqLgbRn
iEEAPq3TUEZ,UFC-BERT: Unifying Multi-Modal Controls for Conditional Image Synthesis,"Conditional image synthesis aims to create an image according to some multi-modal guidance in the forms of textual descriptions, reference images, and image blocks to preserve, as well as their combinations. In this paper, instead of investigating these control signals separately, we propose a new two-stage architecture, UFC-BERT, to unify any number of multi-modal controls. In UFC-BERT, both the diverse control signals and the synthesized image are uniformly represented as a sequence of discrete tokens to be processed by Transformer. Different from existing two-stage autoregressive approaches such as DALL-E and VQGAN, UFC-BERT adopts non-autoregressive generation (NAR) at the second stage to enhance the holistic consistency of the synthesized image, to support preserving specified image blocks, and to improve the synthesis speed. Further, we design a progressive algorithm that iteratively improves the non-autoregressively generated image, with the help of two estimators developed for evaluating the compliance with the controls and evaluating the fidelity of the synthesized image, respectively. Extensive experiments on a newly collected large-scale clothing dataset M2C-Fashion and a facial dataset Multi-Modal CelebA-HQ verify that UFC-BERT can synthesize high-fidelity images that comply with flexible multi-modal controls.
",main,NeurIPS,2021,Poster,Zhu Zhang;Jianxin Ma;Chang Zhou;Rui Men;Zhikang Li;Ming Ding;Jie Tang;Jingren Zhou;Hongxia Yang,True,https://openreview.net/pdf?id=iEEAPq3TUEZ
iorEu783qJ5,Particle Cloud Generation with Message Passing Generative Adversarial Networks,"In high energy physics (HEP), jets are collections of correlated particles produced ubiquitously in particle collisions such as those at the CERN Large Hadron Collider (LHC). Machine learning (ML)-based generative models, such as generative adversarial networks (GANs), have the potential to significantly accelerate LHC jet simulations. However, despite jets having a natural representation as a set of particles in momentum-space, a.k.a. a particle cloud, there exist no generative models applied to such a dataset. In this work, we introduce a new particle cloud dataset (JetNet), and apply to it existing point cloud GANs. Results are evaluated using (1) 1-Wasserstein distances between high- and low-level feature distributions, (2) a newly developed Fréchet ParticleNet Distance, and (3) the coverage and (4) minimum matching distance metrics. Existing GANs are found to be inadequate for physics applications, hence we develop a new message passing GAN (MPGAN), which outperforms existing point cloud GANs on virtually every metric and shows promise for use in HEP. We propose JetNet as a novel point-cloud-style dataset for the ML community to experiment with, and set MPGAN as a benchmark to improve upon for future generative models. Additionally, to facilitate research and improve accessibility and reproducibility in this area, we release the open-source JetNet Python package with interfaces for particle cloud datasets, implementations for evaluation and loss metrics, and more tools for ML in HEP development.",main,NeurIPS,2021,Poster,Raghav Kansal;Javier Duarte;Hao Su;Breno Orzari;Thiago R F P Tomei;Maurizio Pierini;Mary Touranakou;Jean-roch Vlimant;Dimitrios Gunopulos,True,https://openreview.net/pdf?id=iorEu783qJ5
ip0FhVivoX0,Protein-Ligand Docking Surrogate Models: A SARS-CoV-2 Benchmark for Deep Learning Accelerated Virtual Screening,"We propose a benchmark to study surrogate model accuracy for protein-ligand docking. We share a dataset consisting of 200 million 3D complex structures and 2D structure scores across a consistent set of 13 million ``in-stock'' molecules over 15 receptors, or binding sites, across the SARS-CoV-2 proteome. Our work shows surrogate docking models have six orders of magnitude more throughput than standard docking protocols on the same supercomputer node types. We demonstrate the power of high-speed surrogate models by running each target against 1 billion molecules in under a day (50k predictions per GPU seconds). We showcase a workflow for docking utilizing surrogate ML models as a pre-filter. Our workflow is ten times faster at screening a library of compounds than the standard technique, with an error rate less than 0.01% of detecting the underlying best scoring 0.1\\\\% of compounds. Our analysis of the speedup explains that to screen more molecules under a docking paradigm, another order of magnitude speedup must come from model accuracy rather than computing speed (which, if increased, will not anymore alter our throughput to screen molecules). We believe this is strong evidence for the community to begin focusing on improving the accuracy of surrogate models to improve the ability to screen massive compound libraries 100x or even 1000x faster than current techniques. ",Datasets & Benchmarks,NeurIPS,2021,Reject,Austin Clyde;Thomas Brettin;Alexander Partin;Hyunseung Yoo;Yadu Babuji;Ben Blaiszik;Andre Merzky;Matteo Turilli;Shantenu Jhah;Arvind Ramanathan;Rick Stevens,True,https://openreview.net/pdf?id=ip0FhVivoX0
izzQAL8BciY,MultiBench: Multiscale Benchmarks for Multimodal Representation Learning,"Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. It is a challenging yet crucial area with numerous real-world applications in multimedia, affective computing, robotics, finance, human-computer interaction, and healthcare. Unfortunately, multimodal research has seen limited resources to study (1) generalization across domains and modalities, (2) complexity during training and inference, and (3) robustness to noisy and missing modalities. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiBench, a systematic and unified large-scale benchmark for multimodal learning spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. MultiBench provides an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, MultiBench offers a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench introduces impactful challenges for future research, including scalability to large-scale multimodal datasets and robustness to realistic imperfections. To accompany this benchmark, we also provide a standardized implementation of 20 core approaches in multimodal learning spanning innovations in fusion paradigms, optimization objectives, and training approaches. Simply applying methods proposed in different research areas can improve the state-of-the-art performance on 9/15 datasets. Therefore, MultiBench presents a milestone in unifying disjoint efforts in multimodal machine learning research and paves the way towards a better understanding of the capabilities and limitations of multimodal models, all the while ensuring ease of use, accessibility, and reproducibility. MultiBench, our standardized implementations, and leaderboards are publicly available, will be regularly updated, and welcomes inputs from the community.",Datasets & Benchmarks,NeurIPS,2021,Poster,Paul Pu Liang;Yiwei Lyu;Xiang Fan;Zetian Wu;Yun Cheng;Jason Wu;Leslie Yufan Chen;Peter Wu;Michelle A Lee;Yuke Zhu;Russ Salakhutdinov;Louis-Philippe Morency,False,https://openreview.net/pdf?id=izzQAL8BciY
j6NxpQbREA1,AI and the Everything in the Whole Wide World Benchmark,"There is a tendency across different subfields in AI to see value in a small collection of influential benchmarks, which we term 'general' benchmarks. These benchmarks operate as stand-ins or abstractions for a range of anointed common problems that are frequently framed as foundational milestones on the path towards flexible and generalizable AI systems. State-of-the-art performance on these benchmarks is widely understood as indicative of progress towards these long-term goals. In this position paper, we explore how such benchmarks are designed, constructed and used in order to reveal key limitations of their framing as the functionally 'general' broad measures of progress they are set up to be.",Datasets & Benchmarks,NeurIPS,2021,Poster,Inioluwa Deborah Raji;Emily Denton;Emily M. Bender;Alex Hanna;Amandalynne Paullada,False,https://openreview.net/pdf?id=j6NxpQbREA1
jI_BbL-qjJN,An Information Retrieval Approach to Building Datasets for Hate Speech Detection,"Building a benchmark dataset for hate speech detection presents various challenges. Firstly, because hate speech is relatively rare, random sampling of tweets to annotate is very inefficient in finding hate speech. To address this, prior datasets often include only tweets matching known ``hate words''. However, restricting data to a pre-defined vocabulary may exclude portions of the real-world phenomenon we seek to model. A second challenge is that definitions of hate speech tend to be highly varying and subjective. Annotators having diverse prior notions of hate speech may not only disagree with one another but also struggle to conform to specified labeling guidelines. Our key insight is that the rarity and subjectivity of hate speech are akin to that of relevance in information retrieval (IR). This connection suggests that well-established methodologies for creating IR test collections can be usefully applied to create better benchmark datasets for hate speech. To intelligently and efficiently select which tweets to annotate, we apply standard IR techniques of {\\\\em pooling} and {\\\\em active learning}. To improve both consistency and value of annotations, we apply {\\\\em task decomposition} and {\\\\em annotator rationale} techniques. We share a new benchmark dataset for hate speech detection on Twitter that provides broader coverage of hate than prior datasets. We also show a dramatic drop in accuracy of existing detection models when tested on these broader forms of hate. Annotator rationales we collect not only justify labeling decisions but also enable future work opportunities for dual-supervision and/or explanation generation in modeling. Further details of our approach can be found in the supplementary materials \\\\cite{rahman21-neurips21-supplementary}.",Datasets & Benchmarks,NeurIPS,2021,Poster,Md Mustafizur Rahman;Dinesh Balakrishnan;Dhiraj Murthy;Mucahid Kutlu;Matthew Lease,True,https://openreview.net/pdf?id=jI_BbL-qjJN
jpwGODt2Av,Whole Brain Vessel Graphs: A Dataset and Benchmark for Graph Learning and Neuroscience,"Biological neural networks define the brain function and intelligence of humans and other mammals, and form ultra-large, spatial, structured graphs. Their neuronal organization is closely interconnected with the spatial organization of the brain's microvasculature, which supplies oxygen to the neurons and builds a complementary spatial graph. This vasculature (or the vessel structure) plays an important role in neuroscience; for example, the organization of (and changes to) vessel structure can represent early signs of various pathologies, e.g. Alzheimer's disease or stroke. Recently, advances in tissue clearing have enabled whole brain imaging and segmentation of the entirety of the mouse brain's vasculature.

Building on these advances in imaging, we are presenting an extendable dataset of whole-brain vessel graphs based on specific imaging protocols. Specifically, we extract vascular graphs using a refined graph extraction scheme leveraging the volume rendering engine Voreen and provide them in an accessible and adaptable form through the OGB and PyTorch Geometric dataloaders. Moreover, we benchmark numerous state-of-the-art graph learning algorithms on the biologically relevant tasks of vessel prediction and vessel classification using the introduced vessel graph dataset.

Our work paves a path towards advancing graph learning research into the field of neuroscience. Complementarily, the presented dataset raises challenging graph learning research questions for the machine learning community, in terms of incorporating biological priors into learning algorithms, or in scaling these algorithms to handle sparse,spatial graphs with millions of nodes and edges.",Datasets & Benchmarks,NeurIPS,2021,Poster,Johannes C. Paetzold;Julian McGinnis;Suprosanna Shit;Ivan Ezhov;Paul Büschl;Chinmay Prabhakar;Anjany Sekuboyina;Mihail Todorov;Georgios Kaissis;Ali Ertürk;Stephan Günnemann;bjoern menze,True,https://openreview.net/pdf?id=jpwGODt2Av
jyd4Lyjr2iB,Benchmarking Bayesian Deep Learning on Diabetic Retinopathy Detection Tasks,"Bayesian deep learning seeks to equip deep neural networks with the ability to precisely quantify their predictive uncertainty, and has promised to make deep learning more reliable for safety-critical real-world applications. Yet, existing Bayesian deep learning methods fall short of this promise; new methods continue to be evaluated on unrealistic test beds that do not reflect the complexities of downstream real-world tasks that would benefit most from reliable uncertainty quantification. We propose the RETINA Benchmark, a set of real-world tasks that accurately reflect such complexities and are designed to assess the reliability of predictive models in safety-critical scenarios. Specifically, we curate two publicly available datasets of high-resolution human retina images exhibiting varying degrees of diabetic retinopathy, a medical condition that can lead to blindness, and use them to design a suite of automated diagnosis tasks that require reliable predictive uncertainty quantification. We use these tasks to benchmark well-established and state-of-the-art Bayesian deep learning methods on task-specific evaluation metrics. We provide an easy-to-use codebase for fast and easy benchmarking following reproducibility and software design principles. We provide implementations of all methods included in the benchmark as well as results computed over 100 TPU days, 20 GPU days, 400 hyperparameter configurations, and evaluation on at least 6 random seeds each.",Datasets & Benchmarks,NeurIPS,2021,Poster,Neil Band;Tim G. J. Rudner;Qixuan Feng;Angelos Filos;Zachary Nado;Michael W Dusenberry;Ghassen Jerfel;Dustin Tran;Yarin Gal,False,https://openreview.net/pdf?id=jyd4Lyjr2iB
k-ghaB9VZBw,Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets,"Language models can generate harmful and biased outputs and exhibit undesirable behavior according to a given cultural context. We propose a Process for Adapting Language Models to Society (PALMS) with Values-Targeted Datasets, an iterative process to significantly change model behavior by crafting and fine-tuning on a dataset that reflects a predetermined set of target values. We evaluate our process using three metrics: quantitative metrics with human evaluations that score output adherence to a target value, toxicity scoring on outputs; and qualitative metrics analyzing the most common word associated with a given social category. Through each iteration, we add additional training dataset examples based on observed shortcomings from evaluations. PALMS performs significantly better on all metrics compared to baseline and control models for a broad range of GPT-3 language model sizes without compromising capability integrity. We find that the effectiveness of PALMS increases with model size. We show that significantly adjusting language model behavior is feasible with a small, hand-curated dataset.",main,NeurIPS,2021,Spotlight,Irene Solaiman;Christy Dennison,True,https://openreview.net/pdf?id=k-ghaB9VZBw
kBNhgqXatI,An Empirical Investigation of Representation Learning for Imitation,"Imitation learning often needs a large demonstration set in order to handle the full range of situations that an agent might find itself in during deployment. However, collecting expert demonstrations can be expensive. Recent work in vision, reinforcement learning, and NLP has shown that auxiliary representation learning objectives can reduce the need for large amounts of expensive, task-specific data. Our Empirical Investigation of Representation Learning for Imitation (EIRLI) investigates whether similar benefits apply to imitation learning. We propose a modular framework for constructing representation learning algorithms, then use our framework to evaluate the utility of representation learning for imitation across several environment suites. In the settings we evaluate, we find that existing algorithms for image-based representation learning provide limited value relative to a well-tuned baseline with image augmentations. To explain this result, we investigate differences between imitation learning and other settings where representation learning *has* provided significant benefit, such as image classification. Finally, we release a well-documented codebase which both replicates our findings and provides a modular framework for creating new representation learning algorithms out of reusable components.",Datasets & Benchmarks,NeurIPS,2021,Poster,Xin Chen;Sam Toyer;Cody Wild;Scott Emmons;Ian Fischer;Kuang-Huei Lee;Neel Alex;Steven H Wang;Ping Luo;Stuart Russell;Pieter Abbeel;Rohin Shah,False,https://openreview.net/pdf?id=kBNhgqXatI
kOxP7Fbeduy,SeasonDepth: Cross-Season Monocular Depth Prediction Dataset and Benchmark under Multiple Environments,"Different environments pose a great challenge on the outdoor robust visual perception for long-term autonomous driving and the generalization of learning-based algorithms on different environmental effects is still an open problem. Althoughmonocular depth prediction has been well studied recently, there is few work focusing on the robust learning-based depth prediction across different environments,e.g.changing illumination and seasons, owing to the lack of such a multi-environment real-world dataset and benchmark. To this end, the first cross-season monocular depth prediction dataset and benchmarkSeasonDepth (available on https://seasondepth.github.io/) is built based on CMU Visual Localizationdataset. To benchmark the depth estimation performance under different environments, we investigate representative and recent state-of-the-art open-source supervised, self-supervised and domain adaptation depth prediction methods fromKITTIbenchmark using several newly-formulated metrics. Through extensive experimental evaluation on the proposed dataset, the influence of multiple environments on performance and robustness is analyzed both qualitatively and quantitatively, showing that the long-term monocular depth prediction is far from solved even with fine-tuning. We further give promising avenues that self-supervised training and stereo geometry constraint help to enhance the robustness to changing environments.",Datasets & Benchmarks,NeurIPS,2021,Reject,Hanjiang Hu;Baoquan Yang;Zhijian Qiao;Ding Zhao;Hesheng Wang,True,https://openreview.net/pdf?id=kOxP7Fbeduy
kUkp7WdUny9,SODA10M: Towards Large-Scale Object Detection Benchmark for Autonomous Driving,"Aiming at facilitating a real-world, ever-evolving and scalable autonomous driving system, we present a large-scale benchmark for standardizing the evaluation of different self-supervised and semi-supervised approaches by learning from raw data, which is the first and largest benchmark to date. Existing autonomous driving systems heavily rely on `perfect' visual perception models (e.g., detection) trained using extensive annotated data to ensure the safety. However, it is unrealistic to elaborately label instances of all scenarios and circumstances (e.g., night, extreme weather, cities) when deploying a robust autonomous driving system. Motivated by recent powerful advances of self-supervised and semi-supervised learning, a promising direction is to learn a robust detection model by collaboratively exploiting large-scale unlabeled data and few labeled data. Existing dataset (e.g., KITTI, Waymo) either provides only a small amount of data or covers limited domains with full annotation, hindering the exploration of large-scale pre-trained models. Here, we release a Large-Scale Object Detection benchmark for Autonomous driving, named as SODA10M, containing 10 million unlabeled images and 20K images labeled with 6 representative object categories. To improve diversity, the images are collected every ten seconds per frame within 32 different cities under different weather conditions, periods and location scenes. We provide extensive experiments and deep analyses of existing supervised state-of-the-art detection models, popular self-supervised and semi-supervised approaches, and some insights about how to develop future models. We show that SODA10M can serve as a promising pre-training dataset for different self-supervised learning methods, which gives superior performance when finetuning autonomous driving downstream tasks. This benchmark will be used to hold the ICCV2021 SSLAD challenge. The data and more up-to-date information have been released at https://soda-2d.github.io.",Datasets & Benchmarks,NeurIPS,2021,Reject,Jianhua Han;Xiwen Liang;Hang Xu;Kai Chen;Lanqing HONG;Chaoqiang Ye;Wei Zhang;Zhenguo Li;Xiaodan Liang;Chunjing Xu,True,https://openreview.net/pdf?id=kUkp7WdUny9
knKJgksd7kA,"Interesting Object, Curious Agent: Learning Task-Agnostic Exploration","Common approaches for task-agnostic exploration learn tabula-rasa --the agent assumes isolated environments and no prior knowledge or experience. However, in the real world, agents learn in many environments and always come with prior experiences as they explore new ones. Exploration is a lifelong process. In this paper, we propose a paradigm change in the formulation and evaluation of task-agnostic exploration. In this setup, the agent first learns to explore across many environments without any extrinsic goal in a task-agnostic manner.
Later on, the agent effectively transfers the learned exploration policy to better explore new environments when solving tasks. In this context, we evaluate several baseline exploration strategies and present a simple yet effective approach to learning task-agnostic exploration policies. Our key idea is that there are two components of exploration: (1) an agent-centric component encouraging exploration of unseen parts of the environment based on an agent’s belief; (2) an environment-centric component encouraging exploration of inherently interesting objects. We show that our formulation is effective and provides the most consistent exploration across several training-testing environment pairs. We also introduce benchmarks and metrics for evaluating task-agnostic exploration strategies. The source code is available at https://github.com/sparisi/cbet/.",main,NeurIPS,2021,Oral,Simone Parisi;Victoria Dean;Deepak Pathak;Abhinav Gupta,True,https://openreview.net/pdf?id=knKJgksd7kA
lM2971LAwV,Evolution Gym: A Large-Scale Benchmark for Evolving Soft Robots,"Both the design and control of a robot play equally important roles in its task performance. However, while optimal control is well studied in the machine learning and robotics community, less attention is placed on finding the optimal robot design. This is mainly because co-optimizing design and control in robotics is characterized as a challenging problem, and more importantly, a comprehensive evaluation benchmark for co-optimization does not exist. In this paper, we propose Evolution Gym, the first large-scale benchmark for co-optimizing the design and control of soft robots. In our benchmark, each robot is composed of different types of voxels (e.g., soft, rigid, actuators), resulting in a modular and expressive robot design space. Our benchmark environments span a wide range of tasks, including locomotion on various types of terrains and manipulation. Furthermore, we develop several robot co-evolution algorithms by combining state-of-the-art design optimization methods and deep reinforcement learning techniques. Evaluating the algorithms on our benchmark platform, we observe robots exhibiting increasingly complex behaviors as evolution progresses, with the best evolved designs solving many of our proposed tasks. Additionally, even though robot designs are evolved autonomously from scratch without prior knowledge, they often grow to resemble existing natural creatures while outperforming hand-designed robots. Nevertheless, all tested algorithms fail to find robots that succeed in our hardest environments. This suggests that more advanced algorithms are required to explore the high-dimensional design space and evolve increasingly intelligent robots -- an area of research in which we hope Evolution Gym will accelerate progress. Our website with code, environments, documentation, and tutorials is available at http://evogym.csail.mit.edu/.",main,NeurIPS,2021,Poster,Jagdeep Singh Bhatia;Holly Jackson;Yunsheng Tian;Jie Xu;Wojciech Matusik,True,https://openreview.net/pdf?id=lM2971LAwV
lg3iGvFQ5V,Benchmarking the Robustness of CNN-based Spatial-Temporal Models,"The state-of-the-art deep convolutional neural networks are vulnerable to common corruptions in nature (e.g., input data corruptions caused by weather changes, system errors). While rapid progress has been made in analyzing and improving the robustness of models in image understanding, the robustness in video understanding is largely ignored. In this paper, we establish a corruption robustness benchmark, Mini Kinetics-C and Mini SSV2-C, which considers temporal corruptions beyond spatial corruptions in images. We make the first attempt to conduct an exhaustive study on corruption robustness in terms of spatial and temporal domain, using established CNN-based spatial-temporal models. The study provides some guidance on robust model design, training and inference: 1) 3D modules make video classification models more robust instead of 2D modules, 2) longer input length and uniform sampling of input frames can benefit model corruption robustness, 3) model corruption robustness (especially robustness in the temporal domain) enhances with computational cost, which may contradict with the current trend of improving the computational efficiency of models. Our codes are available on https://github.com/Newbeeyoung/Video-Corruption-Robustness.",Datasets & Benchmarks,NeurIPS,2021,Reject,Chenyu Yi;SIYUAN YANG;Haoliang Li;Alex Kot,True,https://openreview.net/pdf?id=lg3iGvFQ5V
luWTh5Q63e,Predicting Event Memorability from Contextual Visual Semantics,"Episodic event memory is a key component of human cognition. Predicting event memorability,i.e., to what extent an event is recalled, is a tough challenge in memory research and has profound implications for artificial intelligence. In this study, we investigate factors that affect event memorability according to a cued recall process. Specifically, we explore whether event memorability is contingent on the event context, as well as the intrinsic visual attributes of image cues. We design a novel experiment protocol and conduct a large-scale experiment with 47 elder subjects over 3 months.  Subjects’ memory of life events is tested in a cued recall process. Using advanced visual analytics methods, we build a first-of-its-kind event memorability dataset (called R3) with rich information about event context and visual semantic features. Furthermore, we propose a contextual event memory network (CEMNet) that tackles multi-modal input to predict item-wise event memorability, which outperforms competitive benchmarks.  The findings inform deeper understanding of episodic event memory, and open up a new avenue for prediction of human episodic memory.  Source code is available at https://github.com/ffzzy840304/Predicting-Event-Memorability.",main,NeurIPS,2021,Poster,Qianli Xu;Fen Fang;Ana Garcia del Molino;Vigneshwaran Subbaraju;Joo Hwee Lim,True,https://openreview.net/pdf?id=luWTh5Q63e
lwlkxYsGDi,DIPS-Plus: The Enhanced Database of Interacting Protein Structures for Interface Prediction,"How and where proteins interface with one another can ultimately impact the proteins' functions along with a range of other biological processes. As such, precise computational methods for protein interface prediction (PIP) come highly sought after as they could yield significant advances in drug discovery and design as well as protein function analysis. However, the traditional benchmark dataset for this task, Docking Benchmark 5 (DB5), contains only a modest 230 complexes for training, validating, and testing different machine learning algorithms. In this work, we expand on a dataset recently introduced for this task, the Database of Interacting Protein Structures (DIPS), to present DIPS-Plus, an enhanced, feature-rich dataset of 42,112 complexes for geometric deep learning of protein interfaces. The previous version of DIPS contains only the Cartesian coordinates and types of the atoms comprising a given protein complex, whereas DIPS-Plus now includes a plethora of new residue-level features including protrusion indices, half-sphere amino acid compositions, and new profile hidden Markov model (HMM)-based sequence features for each amino acid, giving researchers a large, well-curated feature bank for training protein interface prediction methods. We demonstrate through rigorous benchmarks that training an existing state-of-the-art (SOTA) model for PIP on DIPS-Plus yields SOTA results, surpassing the performance of all other models trained on residue-level and atom-level encodings of protein complexes to date.",Datasets & Benchmarks,NeurIPS,2021,Reject,Alex Morehead;Chen Chen;Ada Sedova;Jianlin Cheng,True,https://openreview.net/pdf?id=lwlkxYsGDi
lwrPkQP_is,URLB: Unsupervised Reinforcement Learning Benchmark,"Deep Reinforcement Learning (RL) has emerged as a powerful paradigm to solve a range of complex yet specific control tasks. Training generalist agents that can quickly adapt to new tasks remains an outstanding challenge. Recent advances in unsupervised RL have shown that pre-training RL agents with self-supervised intrinsic rewards can result in efficient adaptation. However, these algorithms have been hard to compare and develop due to the lack of a unified benchmark. To this end, we introduce the Unsupervised Reinforcement Learning Benchmark (URLB). URLB consists of two phases: reward-free pre-training and downstream task adaptation with extrinsic rewards. Building on the DeepMind Control Suite, we provide twelve continuous control tasks from three domains for evaluation and open-source code for eight leading unsupervised RL methods. We find that the implemented baselines make progress but are not able to solve URLB and propose directions for future research.
",Datasets & Benchmarks,NeurIPS,2021,Poster,Michael Laskin;Denis Yarats;Hao Liu;Kimin Lee;Albert Zhan;Kevin Lu;Catherine Cang;Lerrel Pinto;Pieter Abbeel,True,https://openreview.net/pdf?id=lwrPkQP_is
m28E5RN64hi,RealCause: Realistic Causal Inference Benchmarking,"There are many different causal effect estimators in causal inference. However, it is unclear how to choose between these estimators because there is no ground-truth for causal effects. A commonly used option is to simulate synthetic data, where the ground-truth is known. However, the best causal estimators on synthetic data are unlikely to be the best causal estimators on real data. An ideal benchmark for causal estimators would both (a) yield ground-truth values of the causal effects and (b) be representative of real data. Using flexible generative models, we provide a benchmark that both yields ground-truth and is realistic. Using this benchmark, we evaluate over 1500 different causal estimators and provide evidence that it is rational to choose hyperparameters for causal estimators using predictive metrics.",Datasets & Benchmarks,NeurIPS,2021,Reject,Brady Neal;Chin-Wei Huang;Sunand Raghupathi,True,https://openreview.net/pdf?id=m28E5RN64hi
mPducS1MsEK,Are We Learning Yet? A Meta Review of Evaluation Failures Across Machine Learning,"Many subfields of machine learning share a common stumbling block: evaluation.  Advances in machine learning often evaporate under closer scrutiny or turn out to be less widely applicable than originally hoped.  We conduct a meta-review of 107 survey papers from natural language processing, recommender systems, computer vision, reinforcement learning, computational biology, graph learning, and more, organizing the wide range of surprisingly consistent critique into a concrete taxonomy of observed failure modes. Inspired by measurement and evaluation theory, we divide failure modes into two categories: internal and external validity. Internal validity issues pertain to evaluation on a learning problem in isolation, such as improper comparisons to baselines or overfitting from test set re-use. External validity relies on relationships between different learning problems, for instance, whether progress on a learning problem translates to progress on seemingly related tasks. ",Datasets & Benchmarks,NeurIPS,2021,Poster,Thomas Liao;Rohan Taori;Inioluwa Deborah Raji;Ludwig Schmidt,False,https://openreview.net/pdf?id=mPducS1MsEK
mbW_GT3ZN-,CREAK: A Dataset for Commonsense Reasoning over Entity Knowledge,"Most benchmark datasets targeting commonsense reasoning focus on everyday scenarios: physical knowledge like knowing that you could fill a cup under a waterfall, social knowledge like bumping into someone is awkward, and other generic situations. However, there is a rich space of commonsense inferences anchored to knowledge about specific entities: for example, deciding the truthfulness of a claim Harry Potter can teach classes on how to fly on a broomstick. Can models learn to combine entity knowledge with commonsense reasoning in this fashion? We introduce CREAK, a testbed for commonsense reasoning about entity knowledge, bridging fact-checking about entities (Harry Potter is a wizard and is skilled at riding a broomstick) with commonsense inferences (if you're good at a skill you can teach others how to do it). Our dataset consists of 13k human-authored English claims about entities that are either true or false, in addition to a small contrast set. Crowdworkers can easily come up with these statements and human performance on the dataset is high (high 90s); we argue that pre-trained language models (LMs) should be able to blend entity knowledge and commonsense reasoning to do well here. In our experiments, we focus on the closed-book setting and observe that a baseline model finetuned on existing fact verification benchmark struggles with the type of inferences in CREAK. Training a model on CREAK improves accuracy by a substantial margin, but still falls short of human performance. Our benchmark provides a unique probe into natural language understanding models, testing both its ability to retrieve facts (e.g., who teaches at the University of Chicago?) and unstated commonsense knowledge (e.g., butlers do not yell at guests).",Datasets & Benchmarks,NeurIPS,2021,Poster,Yasumasa Onoe;Michael JQ Zhang;Eunsol Choi;Greg Durrett,True,https://openreview.net/pdf?id=mbW_GT3ZN-
mrsx7ninrtU,Relational VAE: A Continuous Latent Variable Model for Graph Structured Data,"Graph Networks (GNs) enable the fusion of prior knowledge and relational reasoning with flexible function approximations. In this work, a general GN-based model is proposed which takes full advantage of the relational modeling capabilities of GNs and extends these to probabilistic modeling with Variational Bayes (VB). To that end, we combine complementary pre-existing approaches on VB for graph data and propose an approach that relies on graph-structured latent and conditioning variables. It is demonstrated that Neural Processes can also be viewed through the lens of the proposed model. We show applications on the problem of structured probability density modeling for simulated and real wind farm monitoring data, as well as on the meta-learning of simulated Gaussian Process data. We release the source code, along with the simulated datasets.",main,NeurIPS,2021,Reject,Charilaos Mylonas;Imad Abdallah;Eleni Chatzi,True,https://openreview.net/pdf?id=mrsx7ninrtU
mv-1sL8FMN5,Joint Semantic Mining for Weakly Supervised RGB-D Salient Object Detection,"Training saliency detection models with weak supervisions, e.g., image-level tags or captions, is appealing as it removes the costly demand of per-pixel annotations. Despite the rapid progress of RGB-D saliency detection in fully-supervised setting, it however remains an unexplored territory when only weak supervision signals are available. This paper is set to tackle the problem of weakly-supervised RGB-D salient object detection. The key insight in this effort is the idea of maintaining per-pixel pseudo-labels with iterative refinements by reconciling the multimodal input signals in our joint semantic mining (JSM). Considering the large variations in the raw depth map and the lack of explicit pixel-level supervisions, we propose spatial semantic modeling (SSM) to capture saliency-specific depth cues from the raw depth and produce depth-refined pseudo-labels. Moreover, tags and captions are incorporated via a fill-in-the-blank training in our textual semantic modeling (TSM) to estimate the confidences of competing pseudo-labels. At test time, our model involves only a light-weight sub-network of the training pipeline, i.e., it requires only an RGB image as input, thus allowing efficient inference. Extensive evaluations demonstrate the effectiveness of our approach under the weakly-supervised setting. Importantly, our method could also be adapted to work in both fully-supervised and unsupervised paradigms. In each of these scenarios, superior performance has been attained by our approach with comparing to the state-of-the-art dedicated methods. As a by-product, a CapS dataset is constructed by augmenting existing benchmark training set with additional image tags and captions. ",main,NeurIPS,2021,Poster,Jingjing Li;Wei Ji;Qi Bi;Cheng Yan;Miao Zhang;Yongri Piao;Huchuan Lu;Li Cheng,True,https://openreview.net/pdf?id=mv-1sL8FMN5
ngdcA1tlDvj,Scallop: From Probabilistic Deductive Databases to Scalable Differentiable Reasoning,"Deep learning and symbolic reasoning are complementary techniques for an intelligent system. However, principled combinations of these techniques have limited scalability, rendering them ill-suited for real-world applications. We propose Scallop, a system that builds upon probabilistic deductive databases, to bridge this gap. The key insight underlying Scallop is a provenance framework that introduces a tunable parameter to specify the level of reasoning granularity. Scallop thereby i) generalizes exact probabilistic reasoning, ii) asymptotically reduces computational cost, and iii) provides relative accuracy guarantees. On a suite of tasks that involve mathematical and logical reasoning, Scallop scales significantly better without sacrificing accuracy compared to DeepProbLog, a principled neural logic programming approach. We also create and evaluate on a real-world Visual Question Answering (VQA) benchmark that requires multi-hop reasoning. Scallop outperforms two VQA-tailored models, a Neural Module Networks based and a transformer based model, by 12.42% and 21.66% respectively.
",main,NeurIPS,2021,Poster,Jiani Huang;Ziyang Li;Binghong Chen;Karan Samel;Mayur Naik;Le Song;Xujie Si,True,https://openreview.net/pdf?id=ngdcA1tlDvj
nlJ1rV6G_Iq,CSAW-M: An Ordinal Classification Dataset for Benchmarking Mammographic Masking of Cancer,"Interval and large invasive breast cancers, which are associated with worse prognosis than other cancers, are usually detected at a late stage due to false negative assessments of screening mammograms. The missed screening-time detection is commonly caused by the tumor being obscured by its surrounding breast tissues, a phenomenon called masking. To study and benchmark mammographic masking of cancer, in this work we introduce CSAW-M, the largest public mammographic dataset, collected from over 10,000 individuals and annotated with potential masking. In contrast to the previous approaches which measure breast image density as a proxy, our dataset directly provides annotations of masking potential assessments from five specialists. We also trained deep learning models on CSAW-M to estimate the masking level and showed that the estimated masking is significantly more predictive of screening participants diagnosed with interval and large invasive cancers -- without being explicitly trained for these tasks -- than its breast density counterparts.",Datasets & Benchmarks,NeurIPS,2021,Poster,Moein Sorkhei;Yue Liu;Hossein Azizpour;Edward Azavedo;Karin Dembrower;Dimitra Ntoula;Athanasios Zouzos;Fredrik Strand;Kevin Smith,True,https://openreview.net/pdf?id=nlJ1rV6G_Iq
nvPp2SomA98,"MMVText: A Large-Scale, Multidimensional Multilingual Dataset for Video Text Spotting","Video text spotting is crucial for numerous real application scenarios, but most existing video text reading benchmarks are challenging to evaluate the performance of advanced deep learning algorithms due to the limited amount of training data and tedious scenarios. To address this issue, we introduce a new large-scale benchmark dataset named Multidimensional Multilingual Video Text (MMVText), the first large-scale and multilingual benchmark for video text spotting in a variety of scenarios. There are mainly three features for MMVText. Firstly, we provide 510 videos with more than 1,000,000 frame images, four times larger than the existing largest dataset for text in videos. Secondly, our dataset covers 30 open categories with a wide selection of various scenarios, life vlog, sports news, automatic drive, cartoon, etc. Besides, caption text and scene text are separately tagged for the two different representational meanings in the video. The former represents more theme information, and the latter is the scene information. Thirdly, the MMVText provides multilingual text annotation to promote multiple cultures live and communication. In the end, a comprehensive experimental result and analysis concerning text detection, recognition, tracking, and end-to-end spotting on MMVText are provided. We also discuss the potentials of using MMVText for other video-and-text research. ",Datasets & Benchmarks,NeurIPS,2021,Reject,Weijia Wu;Yuanqiang Cai;Debing Zhang;Hong Zhou;Sibo Wang,True,https://openreview.net/pdf?id=nvPp2SomA98
nzqoh6FN6sF,Estimating the Long-Term Effects of Novel Treatments,"Policy makers often need to estimate the long-term effects of novel treatments, while only having historical data of older treatment options. We propose a surrogate-based approach using a long-term dataset where only past treatments were administered and a short-term dataset where novel treatments have been administered. Our approach generalizes previous surrogate-style methods, allowing for continuous treatments and serially-correlated treatment policies while maintaining consistency and root-n asymptotically normal estimates under a Markovian assumption on the data and the observational policy. Using a semi-synthetic dataset on customer incentives from a major corporation, we evaluate the performance of our method and discuss solutions to practical challenges when deploying our methodology.",main,NeurIPS,2021,Poster,Keith Battocchi;Eleanor Dillon;Maggie Hei;Greg Lewis;Miruna Oprescu;Vasilis Syrgkanis,True,https://openreview.net/pdf?id=nzqoh6FN6sF
oJ0oHQtAld,A Toolbox for Construction and Analysis of Speech Datasets,"Automatic Speech Recognition and Text-to-Speech systems are primarily trained in a supervised fashion and require high-quality, accurately labeled speech datasets. 
In this work, we examine common problems with speech data and introduce a toolbox for the construction and interactive error analysis of speech datasets. The construction tool is based on K{\\\\""u}rzinger et al. work, and, to the best of our knowledge, the dataset exploration tool is the world's first open-source tool of this kind. We demonstrate how to apply these tools to create a Russian speech dataset and analyze existing speech datasets (Multilingual LibriSpeech, Mozilla Common Voice). The tools are open sourced as a part of the NeMo framework.",Datasets & Benchmarks,NeurIPS,2021,Poster,Evelina Bakhturina;Vitaly Lavrukhin;Boris Ginsburg,False,https://openreview.net/pdf?id=oJ0oHQtAld
oUg5rC_95OM,A Benchmark of Medical Out of Distribution Detection,"Motivation: Deep learning models deployed on medical tasks can be equipped with Out-of-Distribution Detection (OoDD) methods in order to avoid erroneous predictions. However it is unclear which OoDD methods are effective in practice. 

Specific Problem: Systems trained for one particular domain of images cannot be expected to perform accurately on images of a different domain.  These images should be flagged by an OoDD method prior to prediction. 

Our approach: This paper defines 3 categories of OoD examples and benchmarks popular OoDD methods in three domains of medical imaging: chest X-ray, fundus imaging, and histology slides. 

Results: Our experiments show that despite methods yielding good results on some categories of out-of-distribution samples, they fail to recognize images close to the training distribution.

Conclusion: We find a simple binary classifier on the feature representation has the best accuracy and AUPRC on average. Users of diagnostic tools which employ these OoDD methods should still remain vigilant that images very close to the training distribution yet not in it could yield unexpected results.",Datasets & Benchmarks,NeurIPS,2021,Reject,Tianshi Cao;Chin-Wei Huang;David Yu-Tung Hui;Joseph Paul Cohen,False,https://openreview.net/pdf?id=oUg5rC_95OM
ogNcxJn32BZ,Teach Me to Explain: A Review of Datasets for Explainable Natural Language Processing,"Explainable Natural Language Processing (ExNLP) has increasingly focused on collecting human-annotated textual explanations. These explanations are used downstream in three ways: as data augmentation to improve performance on a predictive task, as supervision to train models to produce explanations for their predictions, and as a ground-truth to evaluate model-generated explanations. In this review, we identify 65 datasets with three predominant classes of textual explanations (highlights, free-text, and structured), organize the literature on annotating each type, identify strengths and shortcomings of existing collection methodologies, and give recommendations for collecting ExNLP datasets in the future.",Datasets & Benchmarks,NeurIPS,2021,Poster,Sarah Wiegreffe;Ana Marasovic,False,https://openreview.net/pdf?id=ogNcxJn32BZ
ooWbKrRIk9G,The Argoverse Trajectory Retrieval Benchmark,"As tracking data becomes more readily available in many domains such as sports, animal tracking, and autonomous vehicles, so does the need for effective information access and retrieval of those growing datasets. To that end, we develop the Argoverse Trajectory Retrieval Benchmark for contextual trajectory retrieval of driving scenarios.  The goal of this task is to find similar trajectories from within a large dataset given a query trajectory. This task is challenging because there are many dimensions of variation in which two trajectories can be similar, such as vehicle kinematics, social causality, and road configurations. To our knowledge, this is the first standardized benchmark for trajectory retrieval of driving scenarios. We also provide an evaluation of baseline approaches based on representation learning and relevance feedback, and highlight several areas for improvement for which machine learning can play a large role in future work.",Datasets & Benchmarks,NeurIPS,2021,Reject,Eric Zhan;Jagjeet Singh;Yisong Yue;Andrew Hartnett,True,https://openreview.net/pdf?id=ooWbKrRIk9G
p2dMLEwL8tF,FLIP: Benchmark tasks in fitness landscape inference for proteins,"Machine learning could enable an unprecedented level of control in protein engineering for therapeutic and industrial applications. Critical to its use in designing proteins with desired properties, machine learning models must capture the protein sequence-function relationship, often termed fitness landscape. Existing benchmarks like CASP or CAFA assess structure and function predictions of proteins, respectively, yet they do not target metrics relevant for protein engineering. In this work, we introduce Fitness Landscape Inference for Proteins (FLIP), a benchmark for function prediction to encourage rapid scoring of representation learning for protein engineering. Our curated splits, baselines, and metrics probe model generalization in settings relevant for protein engineering, e.g. low-resource and extrapolative. Currently, FLIP encompasses experimental data across adeno-associated virus stability for gene therapy, protein domain B1 stability and immunoglobulin binding, and thermostability from multiple protein families. In order to enable ease of use and future expansion to new splits, all data are presented in a standard format. FLIP scripts and data are freely accessible at https://benchmark.protein.properties.",Datasets & Benchmarks,NeurIPS,2021,Poster,Christian Dallago;Jody Mou;Kadina E Johnston;Bruce Wittmann;Nick Bhattacharya;Samuel Goldman;Ali Madani;Kevin K Yang,True,https://openreview.net/pdf?id=p2dMLEwL8tF
pBwQ82pYha,Graph Robustness Benchmark: Rethinking and Benchmarking Adversarial Robustness of Graph Neural Networks,"Recent studies have shown that Graph Neural Networks (GNNs) are vulnerable to adversarial attacks. Previous attacks and defenses on GNNs face common problems like scalability or generality, which hinder the progress of this domain. By rethinking limitations in previous works, we propose Graph Robustness Benchmark (GRB), the first benchmark that aims to provide scalable, general, unified, and reproducible evaluation on adversarial robustness of GNNs. GRB includes (1) scalable datasets processed by a novel splitting scheme; (2) diverse set of baseline methods covering GNNs, attacks, and defenses; (3) unified evaluation pipeline that permits a fair comparison; (4) modular coding framework that facilitates implementation of various methods and ensures reproducibility; (5) leaderboards that track the progress of the field. Besides, we propose two strong baseline defenses that significantly outperform previous ones. With extensive experiments, we can fairly compare all methods and investigate their pros and cons. GRB is open-source and maintains all datasets, codes, leaderboards at https://cogdl.ai/grb/home, which will be continuously updated to promote future research in this field.",Datasets & Benchmarks,NeurIPS,2021,Reject,Qinkai Zheng;Xu Zou;Yuxiao Dong;Yukuo Cen;Da Yin;Jie Tang,True,https://openreview.net/pdf?id=pBwQ82pYha
pMWtc5NKd7V,RadGraph: Extracting Clinical Entities and Relations from Radiology Reports,"Extracting structured clinical information from free-text radiology reports can enable the use of radiology report information for a variety of critical healthcare applications. In our work, we present RadGraph, a dataset of entities and relations in full-text chest X-ray radiology reports based on a novel information extraction schema we designed to structure radiology reports. We release a development dataset, which contains board-certified radiologist annotations for 500 radiology reports from the MIMIC-CXR dataset (14,579 entities and 10,889 relations), and a test dataset, which contains two independent sets of board-certified radiologist annotations for 100 radiology reports split equally across the MIMIC-CXR and CheXpert datasets. Using these datasets, we train and test a deep learning model, RadGraph Benchmark, that achieves a micro F1 of 0.82 and 0.73 on relation extraction on the MIMIC-CXR and CheXpert test sets respectively. Additionally, we release an inference dataset, which contains annotations automatically generated by RadGraph Benchmark across 220,763 MIMIC-CXR reports (around 6 million entities and 4 million relations) and 500 CheXpert reports (13,783 entities and 9,908 relations) with mappings to associated chest radiographs. Our freely available dataset can facilitate a wide range of research in medical natural language processing, as well as computer vision and multi-modal learning when linked to chest radiographs.",Datasets & Benchmarks,NeurIPS,2021,Poster,Saahil Jain;Ashwin Agrawal;Adriel Saporta;Steven Truong;Du Nguyen Duong;Tan Bui;Pierre Chambon;Yuhao Zhang;Matthew P. Lungren;Andrew Y. Ng;Curtis Langlotz;Pranav Rajpurkar,True,https://openreview.net/pdf?id=pMWtc5NKd7V
pOf_IV_-XG,Using Dynamic Neural Networks to Model the Speed-Accuracy Trade-Off in People,"Neural networks have been shown to exhibit remarkable object recognition performance. We ask here whether such networks can provide a useful model for how people recognize objects. Human recognition time varies, from 0.1 to 10 s, depending on the stimulus and task. Slowness of recognition is a key feature in some public health issues, such as dyslexia, so it is crucial to create a model of human speed-accuracy trade-offs. This is an essential aspect of any useful computational model of human cognitive behavior. We present a benchmark dataset for human speed-accuracy trade-off in recognizing a CIFAR-10 image~\\\\cite{Krizhevsky09learningmultiple} from a set of provided class labels. Within a series of trials, a beep sounds at a fixed delay after the target (the desired reaction time), and the response counts only if it occurs near that time. We observe that accuracy grows with reaction time and examine several dynamic neural networks that exhibit a speed-accuracy trade-off as humans do. After limiting the network resources and adding image perturbations (grayscale conversion, noise, blur) to bring the two observers (human and network) into the same accuracy range, humans and networks show very similar dependence on duration or floating point operations (FLOPS). We conclude that dynamic neural networks are a promising model of human reaction time in recognition tasks. Understanding how the brain allocates appropriate resources under time pressure would be a milestone in neuroscience and a first step toward understanding conditions like dyslexia. Our dataset and code are publicly available.
",Datasets & Benchmarks,NeurIPS,2021,Reject,Ajay Subramanian;Omkar Kumbhar;Elena Sizikova;Najib J. Majaj;Denis Pelli,True,https://openreview.net/pdf?id=pOf_IV_-XG
pR3dPOHrbfy,Do Input Gradients Highlight Discriminative Features?,"Post-hoc gradient-based interpretability methods [Simonyan et al., 2013, Smilkov et al., 2017] that provide instance-specific explanations of model predictions are often based on assumption (A): magnitude of input gradients—gradients of logits with respect to input—noisily highlight discriminative task-relevant features. In this work, we test the validity of assumption (A) using a three-pronged approach:

1. We develop an evaluation framework, DiffROAR, to test assumption (A) on four image classification benchmarks. Our results suggest that (i) input gradients of standard models (i.e., trained on original data) may grossly violate (A), whereas (ii) input gradients of adversarially robust models satisfy (A).

2. We then introduce BlockMNIST, an MNIST-based semi-real dataset, that by design encodes a priori knowledge of discriminative features. Our analysis on BlockMNIST leverages this information to validate as well as characterize differences between input gradient attributions of standard and robust models.

3. Finally, we theoretically prove that our empirical findings hold on a simplified version of the BlockMNIST dataset. Specifically, we prove that input gradients of standard one-hidden-layer MLPs trained on this dataset do not highlight instance-specific signal coordinates, thus grossly violating assumption (A).

Our findings motivate the need to formalize and test common assumptions in interpretability in a falsifiable manner [Leavitt and Morcos, 2020]. We believe that the DiffROAR evaluation framework and BlockMNIST-based datasets can serve as sanity checks to audit instance-specific interpretability methods; code and data available at https://github.com/harshays/inputgradients.",main,NeurIPS,2021,Poster,Harshay Shah;Prateek Jain;Praneeth Netrapalli,True,https://openreview.net/pdf?id=pR3dPOHrbfy
pX4x8f6Km5T,Benchmarking the Combinatorial Generalizability of Complex Query Answering on Knowledge Graphs,"Complex Query Answering (CQA) is an important reasoning task on knowledge graphs. Current CQA learning models have been shown to be able to generalize from atomic operators to more complex formulas, which can be regarded as the combinatorial generalizability. In this paper, we present EFO-1-QA, a new dataset to benchmark the combinatorial generalizability of CQA models by including 301 different queries types, which is 20 times larger than existing datasets. Besides, our benchmark, for the first time, provide a benchmark to evaluate and analyze the impact of different operators and normal forms by using (a) 7 choices of the operator systems and (b) 9 forms of complex queries. Specifically, we provide the detailed study of the combinatorial generalizability of two commonly used operators, i.e., projection and intersection, and justify the impact of the forms of queries given the canonical choice of operators. Our code and data can provide an effective pipeline to benchmark CQA models.",Datasets & Benchmarks,NeurIPS,2021,Poster,Zihao Wang;Hang Yin;Yangqiu Song,True,https://openreview.net/pdf?id=pX4x8f6Km5T
pY9MHwmrymR,An Extensible Benchmark Suite for Learning to Simulate Physical Systems,"Simulating physical systems is a core component of scientific computing, encompassing a wide range of physical domains and applications. Recently, there has been a surge in data-driven methods to complement traditional numerical simulation methods, motivated by the opportunity to reduce computational costs and/or learn new physical models leveraging access to large collections of data. However, the diversity of problem settings and applications has led to a plethora of approaches, each one evaluated on a different setup and with different evaluation metrics. We introduce a set of benchmark problems to take a step towards unified benchmarks and evaluation protocols. We propose four representative physical systems, as well as a collection of both widely used classical time integrators and representative data-driven methods (kernel-based, MLP, CNN, nearest neighbors). Our framework allows evaluating objectively and systematically the stability, accuracy, and computational efficiency of data-driven methods. Additionally, it is configurable to permit adjustments for accommodating other learning tasks and for establishing a foundation for future developments in machine learning for scientific computing.",Datasets & Benchmarks,NeurIPS,2021,Poster,Karl Otness;Arvi Gjoka;Joan Bruna;Daniele Panozzo;Benjamin Peherstorfer;Teseo Schneider;Denis Zorin,True,https://openreview.net/pdf?id=pY9MHwmrymR
pl6xnx3jZMh,ExpMRC: Explainability Evaluation for Machine Reading Comprehension,"Achieving human-level performance on some Machine Reading Comprehension (MRC) datasets is no longer challenging with the help of powerful Pre-trained Language Models (PLMs). However, it is necessary to provide both answer prediction and its explanation to further improve the MRC system's reliability, especially for real-life applications. In this paper, we propose a new benchmark called ExpMRC for evaluating the explainability of the MRC systems. ExpMRC contains four subsets, including SQuAD, CMRC 2018, RACE$^+$, and C$^3$, with additional annotations of the answer's evidence. The MRC systems are required to give not only the correct answer but also its explanation. We use state-of-the-art pre-trained language models to build baseline systems and adopt various unsupervised approaches to extract evidence without a human-annotated training set. The experimental results show that these models are still far from human performance, suggesting that the ExpMRC is challenging.",Datasets & Benchmarks,NeurIPS,2021,Reject,Yiming Cui;Ting Liu;Wanxiang Che;Zhigang Chen;Shijin Wang,True,https://openreview.net/pdf?id=pl6xnx3jZMh
ps95-mkHF_,B-Pref: Benchmarking Preference-Based Reinforcement Learning,"Reinforcement learning (RL) requires access to a reward function that incentivizes the right behavior, but these are notoriously hard to specify for complex tasks. Preference-based RL provides an alternative: learning policies using a teacher's preferences without pre-defined rewards, thus overcoming concerns associated with reward engineering. However, it is difficult to quantify the progress in preference-based RL due to the lack of a commonly adopted benchmark. In this paper, we introduce B-Pref: a benchmark specially designed for preference-based RL. A key challenge with such a benchmark is providing the ability to evaluate candidate algorithms quickly, which makes relying on real human input for evaluation prohibitive. At the same time, simulating human input as giving perfect preferences for the ground truth reward function is unrealistic. B-Pref alleviates this by simulating teachers with a wide array of irrationalities, and proposes metrics not solely for performance but also for robustness to these potential irrationalities. We showcase the utility of B-Pref by using it to analyze algorithmic design choices, such as selecting informative queries, for state-of-the-art preference-based RL algorithms. We hope that B-Pref can serve as a common starting point to study preference-based RL more systematically. Source code is available at https://github.com/rll-research/B-Pref.",Datasets & Benchmarks,NeurIPS,2021,Poster,Kimin Lee;Laura Smith;Anca Dragan;Pieter Abbeel,True,https://openreview.net/pdf?id=ps95-mkHF_
q-8h8-LZiUm,KLUE: Korean Language Understanding Evaluation,"We introduce Korean Language Understanding Evaluation (KLUE) benchmark. KLUE is a collection of eight Korean natural language understanding (NLU) tasks, including Topic Classification, Semantic Textual Similarity, Natural LanguageInference, Named Entity Recognition, Relation Extraction, Dependency Parsing, Machine Reading Comprehension, and Dialogue State Tracking. We create all of the datasets from scratch in a principled way. We design the tasks to have diverse formats and each task to be built upon various source corpora that respect copyrights. Also, we propose suitable evaluation metrics and organize annotation protocols in a way to ensure quality. To prevent ethical risks in KLUE, we proactively remove examples reflecting social biases, containing toxic content or personally identifiable information (PII). Along with the benchmark datasets, we release pre-trained language models (PLM) for Korean, KLUE-BERT and KLUE-RoBERTa, and find KLUE-Roberta-large outperforms other baselines including multilingual PLMs and existing open-source Korean PLMs. The fine-tuning recipes are publicly open for anyone to reproduce our baseline result. We believe our work will facilitate future research on cross-lingual as well as Korean language models and the creation of similar resources for other languages. KLUE is available at https://klue-benchmark.com.

",Datasets & Benchmarks,NeurIPS,2021,Poster,Sungjoon Park;Jihyung Moon;Sungdong Kim;Won Ik Cho;Ji Yoon Han;Jangwon Park;Chisung Song;Junseong Kim;Youngsook Song;Taehwan Oh;Joohong Lee;Juhyun Oh;Sungwon Lyu;Younghoon Jeong;Inkwon Lee;Sangwoo Seo;Dongjun Lee;Hyunwoo Kim;Myeonghwa Lee;Seongbo Jang;Seungwon Do;Sunkyoung Kim;Kyungtae Lim;Jongwon Lee;Kyumin Park;Jamin Shin;Seonghyun Kim;Lucy Park;Alice Oh;Jung-Woo Ha;Kyunghyun Cho,True,https://openreview.net/pdf?id=q-8h8-LZiUm
q2JWz371le,How to transfer algorithmic reasoning knowledge to learn new algorithms?,"Learning to execute algorithms is a fundamental problem that has been widely studied. Prior work (Veličković et al., 2019) has shown that to enable systematic generalisation on graph algorithms it is critical to have access to the intermediate steps of the program/algorithm. In many reasoning tasks, where algorithmic-style reasoning is important, we only have access to the input and output examples. Thus, inspired by the success of pre-training on similar tasks or data in Natural Language Processing (NLP) and Computer vision, we set out to study how we can transfer algorithmic reasoning knowledge. Specifically, we investigate how we can use algorithms for which we have access to the execution trace to learn to solve similar tasks for which we do not. We investigate two major classes of graph algorithms, parallel algorithms such as breadth-first search and Bellman-Ford and sequential greedy algorithms such as Prims and Dijkstra. Due to the fundamental differences between algorithmic reasoning knowledge and feature extractors such as used in Computer vision or NLP, we hypothesis that standard transfer techniques will not be sufficient to achieve systematic generalisation. To investigate this empirically we create a dataset including 9 algorithms and 3 different graph types. We validate this empirically and show how instead multi-task learning can be used to achieve the transfer of algorithmic reasoning knowledge.",main,NeurIPS,2021,Poster,Louis-Pascal A. C. Xhonneux;Andreea Deac;Petar Veličković;Jian Tang,True,https://openreview.net/pdf?id=q2JWz371le
q7XJj9_egih,Particulate Matter Dataset Collected with Vehicle Mounted IoT Devices in Delhi-NCR,"Air pollution is one of the biggest concerns faced by developing countries like India and the world at large. The capital of India, Delhi and the National Capital Region (NCR), sees life threatening air pollution levels. This paper presents a new Particulate Matter (PM) dataset for Delhi-NCR, which contains PM data recorded over three months from November 2020 to January 2021 over an area spanning 559 square Kms. The data has been collected using vehicle-mounted mobile sensors in collaboration with the Delhi Integrated Multi-Modal Transit System (DIMTS) buses. The 13 bus dataset has been compared with the data over the same period obtained from the pre-existing static sensors, which the buses pass by. Several Machine Learning (ML) problems have been outlined, that can be studied using this dataset, two of which, spatio-temporal interpolation and anomaly detection in IoT networks are detailed in this paper. The dataset is public at https://www.cse.iitd.ac.in/pollutiondata, along with appropriate documentation. We will keep augmenting the website as new data get collected, with more buses and other pollutant sensors (SOx, NOx, COx) added to our deployment in future. ",Datasets & Benchmarks,NeurIPS,2021,Reject,Ishan Nangia;Chinmay Degwekar;Sagar Gaddam;Saswat Kumar Pujari;Ismi Abidi;Vedant Vijay;Ajay Kumar Soni;Sayan Ranu;Rijurekha Sen,True,https://openreview.net/pdf?id=q7XJj9_egih
qBl8hnwR0px,Which priors matter? Benchmarking models for learning latent dynamics,"Learning dynamics is at the heart of many important applications of machine learning (ML), such as robotics and autonomous driving.  In these settings, ML algorithms typically need to reason about a physical system using high dimensional observations, such as images, without access to the underlying state. Recently, several methods have proposed to integrate priors from classical mechanics into ML models to address the challenge of physical reasoning from images.  In this work, we take a sober look at the current capabilities of these models. To this end, we introduce a suite consisting of 17 datasets with visual observations 
based on physical systems exhibiting a wide range of dynamics.  We conduct a thorough and detailed comparison of the major classes of physically inspired methods alongside several strong baselines. While models that incorporate physical priors can often learn latent spaces with desirable properties, our results demonstrate that these methods fail to significantly improve upon standard techniques. Nonetheless, we find that the use of continuous and time-reversible dynamics benefits models of all classes. ",Datasets & Benchmarks,NeurIPS,2021,Poster,Aleksandar Botev;Andrew Jaegle;Peter Wirnsberger;Daniel Hennes;Irina Higgins,True,https://openreview.net/pdf?id=qBl8hnwR0px
qF7FlUT5dxa,CommonsenseQA 2.0: Exposing the Limits of AI through Gamification,"Constructing benchmarks that test the abilities of modern natural language understanding models is difficult - pre-trained language models exploit artifacts in benchmarks to achieve human parity, but still fail on adversarial examples and make errors that demonstrate a lack of common sense. In this work, we propose gamification as a framework for data construction. 
The goal of players in the game is to compose questions that mislead a rival AI while using specific phrases for extra points. The game environment leads to enhanced user engagement and simultaneously gives the game designer control over the collected data, allowing us to collect high-quality data at scale. Using our method we create CommonsenseQA 2.0, which includes 14,343 yes/no questions, and demonstrate its difficulty for models that are orders-of-magnitude larger than the AI used in the game itself.
Our best baseline, the T5-based Unicorn with 11B parameters achieves an accuracy of 70.2%, substantially higher than GPT-3 (52.9%) in a few-shot inference setup.  Both score well below human performance which is at 94.1%.",Datasets & Benchmarks,NeurIPS,2021,Poster,Alon Talmor;Ori Yoran;Ronan Le Bras;Chandra Bhagavatula;Yoav Goldberg;Yejin Choi;Jonathan Berant,True,https://openreview.net/pdf?id=qF7FlUT5dxa
qM45LHaWM6E,Shifts: A Dataset of Real Distributional Shift Across Multiple Large-Scale Tasks," There has been significant research done on developing methods for improving robustness to distributional shift and uncertainty estimation. In contrast, only limited work has examined developing standard datasets and benchmarks for assessing these approaches. Additionally, most work on uncertainty estimation and robustness has developed new techniques based on small-scale regression or image classification tasks. However, many tasks of practical interest have different modalities, such as tabular data, audio, text, or sensor data,  which offer significant challenges involving regression and discrete or continuous structured prediction. Thus, given the current state of the field, a standardized large-scale dataset of tasks across a range of modalities affected by distributional shifts is necessary. This will enable researchers to meaningfully evaluate the plethora of recently developed uncertainty quantification methods, as well as assessment criteria and state-of-the-art baselines. In this work, we propose the \\\\emph{Shifts Dataset} for evaluation of uncertainty estimates and robustness to distributional shift. The dataset, which has been collected from industrial sources and services, is composed of three tasks, with each corresponding to a particular data modality: tabular weather prediction, machine translation, and self-driving car (SDC) vehicle motion prediction. All of these data modalities and tasks are affected by real, `in-the-wild' distributional shifts and pose interesting challenges with respect to uncertainty estimation. In this work we provide a description of the dataset and baseline results for all tasks.",Datasets & Benchmarks,NeurIPS,2021,Poster,Andrey Malinin;Neil Band;Yarin Gal;Mark Gales;Alexander Ganshin;German Chesnokov;Alexey Noskov;Andrey Ploskonosov;Liudmila Prokhorenkova;Ivan Provilkov;Vatsal Raina;Vyas Raina;Denis Roginskiy;Mariya Shmatova;Panagiotis Tigas;Boris Yangel,True,https://openreview.net/pdf?id=qM45LHaWM6E
qeM58whnpXM,It's COMPASlicated: The Messy Relationship between RAI Datasets and Algorithmic Fairness Benchmarks,"Risk assessment instrument (RAI) datasets, particularly ProPublica’s COMPAS dataset, are commonly used in algorithmic fairness papers due to benchmarking practices of comparing algorithms on datasets used in prior work. In many cases, this data is used as a benchmark to demonstrate good performance without ac-counting for the complexities of criminal justice (CJ) processes.  However, we show that pretrial RAI datasets can contain numerous measurement biases and errors, and due to disparities in discretion and deployment, algorithmic fairness applied to RAI datasets is limited in making claims about real-world outcomes.These reasons make the datasets a poor fit for benchmarking under assumptions of ground truth and real-world impact. Furthermore, conventional practices of simply replicating previous data experiments may implicitly inherit or edify normative positions without explicitly interrogating value-laden assumptions. Without con-text of how interdisciplinary fields have engaged in CJ research and context of how RAIs operate upstream and downstream, algorithmic fairness practices are misaligned for meaningful contribution in the context of CJ, and would benefit from transparent engagement with normative considerations and values related to fairness, justice, and equality.  These factors prompt questions about whether benchmarks for intrinsically socio-technical systems like the CJ system can exist in a beneficial and ethical way.",Datasets & Benchmarks,NeurIPS,2021,Poster,Michelle Bao;Angela Zhou;Samantha A Zottola;Brian Brubach;Sarah Desmarais;Aaron Seth Horowitz;Kristian Lum;Suresh Venkatasubramanian,False,https://openreview.net/pdf?id=qeM58whnpXM
qkcLxoC52kL,OGB-LSC: A Large-Scale Challenge for Machine Learning on Graphs,"Enabling effective and efficient machine learning (ML) over large-scale graph data (e.g., graphs with billions of edges) can have a great impact on both industrial and scientific applications. However, existing efforts to advance large-scale graph ML have been largely limited by the lack of a suitable public benchmark. Here we present OGB Large-Scale Challenge (OGB-LSC), a collection of three real-world datasets for facilitating the advancements in large-scale graph ML. The OGB-LSC datasets are orders of magnitude larger than existing ones, covering three core graph learning tasks---link prediction, graph regression, and node classification. Furthermore, we provide dedicated baseline experiments, scaling up expressive graph ML models to the massive datasets. We show that expressive models significantly outperform simple scalable baselines, indicating an opportunity for dedicated efforts to further improve graph ML at scale. Moreover, OGB-LSC datasets were deployed at ACM KDD Cup 2021 and attracted more than 500 team registrations globally, during which significant performance improvements were made by a variety of innovative techniques. We summarize the common techniques used by the winning solutions and highlight the current best practices in large-scale graph ML. Finally, we describe how we have updated the datasets after the KDD Cup to further facilitate research advances. The OGB-LSC datasets, baseline code, and all the information about the KDD Cup are available at https://ogb.stanford.edu/docs/lsc/.",Datasets & Benchmarks,NeurIPS,2021,Poster,Weihua Hu;Matthias Fey;Hongyu Ren;Maho Nakata;Yuxiao Dong;Jure Leskovec,True,https://openreview.net/pdf?id=qkcLxoC52kL
qxKh67NNJ2I,Safe Reinforcement Learning with Natural Language Constraints,"While safe reinforcement learning (RL) holds great promise for many practical applications like robotics or autonomous cars, current approaches require specifying constraints in mathematical form. Such specifications demand domain expertise, limiting the adoption of safe RL. In this paper, we propose learning to interpret natural language constraints for safe RL. To this end, we first introduce HAZARDWORLD, a new multi-task benchmark that requires an agent to optimize reward while not violating constraints specified in free-form text. We then develop an agent with a modular architecture that can interpret and adhere to such textual constraints while learning new tasks. Our model consists of (1) a constraint interpreter that encodes textual constraints into spatial and temporal representations of forbidden states, and (2) a policy network that uses these representations to produce a policy achieving minimal constraint violations during training. Across different domains in HAZARDWORLD, we show that our method achieves higher rewards (up to11x) and fewer constraint violations (by 1.8x) compared to existing approaches. However, in terms of absolute performance, HAZARDWORLD still poses significant challenges for agents to learn efficiently, motivating the need for future work.",main,NeurIPS,2021,Spotlight,Tsung-Yen Yang;Michael Hu;Yinlam Chow;Peter Ramadge;Karthik R Narasimhan,True,https://openreview.net/pdf?id=qxKh67NNJ2I
r8IvOsnHchr,Revisiting Time Series Outlier Detection: Definitions and Benchmarks,"Time series outlier detection has been extensively studied with many advanced algorithms proposed in the past decade. Despite these efforts, very few studies have investigated how we should benchmark the existing algorithms. In particular, using synthetic datasets for evaluation has become a common practice in the literature, and thus it is crucial to have a general synthetic criterion to benchmark algorithms. This is a non-trivial task because the existing synthetic methods are very different in different applications and the outlier definitions are often ambiguous. To bridge this gap, we propose a behavior-driven taxonomy for time series outliers and categorize outliers into point- and pattern-wise outliers with clear context definitions. Following the new taxonomy, we then present a general synthetic criterion and generate 35 synthetic datasets accordingly. We further identify 4 multivariate real-world datasets from different domains and benchmark 9 algorithms on the synthetic and the real-world datasets. Surprisingly, we observe that some classical algorithms could outperform many recent deep learning approaches. The datasets, pre-processing and synthetic scripts, and the algorithm implementations are made publicly available at https://github.com/datamllab/tods/tree/benchmark",Datasets & Benchmarks,NeurIPS,2021,Poster,Kwei-Herng Lai;Daochen Zha;Junjie Xu;Yue Zhao;Guanchu Wang;Xia Hu,True,https://openreview.net/pdf?id=r8IvOsnHchr
rCRyg1-Yovi,Three million images and morphological profiles of cells treated with matched chemical and genetic perturbations,"We present a new, carefully designed and well-annotated dataset of images and image-based profiles of cells that have been treated with chemical compounds and genetic perturbations. Each gene that is perturbed is a known target of at least two compounds in the dataset. The dataset can thus serve as a benchmark to evaluate methods for predicting similarities between compounds and between genes and compounds, measuring the effect size of a perturbation, developing style-transfer methods to predict one experimental condition from another, and more generally, learning effective representations for measuring cellular state from microscopy images.",Datasets & Benchmarks,NeurIPS,2021,Reject,Srinivas Niranj Chandrasekaran;Beth A Cimini;Amy Goodale;Lisa Miller;Maria Kost-Alimova;Nasim Jamali;John Doench;Briana Fritchman;Adam Skepner;Michelle Melanson;Daniel Kuhn;Desiree Hernandez;Jim Berstler;Hamdah Shafqat-Abbasi;David Root;Susanne E Swalley;Shantanu Singh;Anne E Carpenter,True,https://openreview.net/pdf?id=rCRyg1-Yovi
rH8yliN6C83,AP-10K: A Benchmark for Animal Pose Estimation in the Wild,"Accurate animal pose estimation is an essential step towards understanding animal behavior, and can potentially benefit many downstream applications, such as wildlife conservation. Previous works only focus on specific animals while ignoring the diversity of animal species, limiting the generalization ability. In this paper, we propose AP-10K, the first large-scale benchmark for general animal pose estimation, to facilitate the research in animal pose estimation. AP-10K consists of 10,015 images collected and filtered from 23 animal families and 54 species following the taxonomic rank and high-quality keypoint annotations labeled and checked manually. Based on AP-10K, we benchmark representative pose estimation models on the following three tracks: (1) supervised learning for animal pose estimation, (2) cross-domain transfer learning from human pose estimation to animal pose estimation, and (3) intra- and inter-family domain generalization for unseen animals. The experimental results provide sound empirical evidence on the superiority of learning from diverse animals species in terms of both accuracy and generalization ability. It opens new directions for facilitating future research in animal pose estimation. AP-10k is publicly available at https://github.com/AlexTheBad/AP10K.",Datasets & Benchmarks,NeurIPS,2021,Poster,Hang Yu;Yufei Xu;Jing Zhang;Wei Zhao;Ziyu Guan;Dacheng Tao,True,https://openreview.net/pdf?id=rH8yliN6C83
rNs2FvJGDK,DUE: End-to-End Document Understanding Benchmark,"Understanding documents with rich layouts plays a vital role in digitization and hyper-automation but remains a challenging topic in the NLP research community. Additionally, the lack of a commonly accepted benchmark made it difficult to quantify progress in the domain. To empower research in this field, we introduce the Document Understanding Evaluation (DUE) benchmark consisting of both available and reformulated datasets to measure the end-to-end capabilities of systems in real-world scenarios.
The benchmark includes Visual Question Answering, Key Information Extraction, and Machine Reading Comprehension tasks over various document domains and layouts featuring tables, graphs, lists, and infographics. In addition, the current study reports systematic baselines and analyzes challenges in currently available datasets using recent advances in layout-aware language modeling. We open both the benchmarks and reference implementations and make them available at https://duebenchmark.com and https://github.com/due-benchmark.
",Datasets & Benchmarks,NeurIPS,2021,Poster,Łukasz Borchmann;Michał Pietruszka;Tomasz Stanislawek;Dawid Jurkiewicz;Michał Turski;Karolina Szyndler;Filip Graliński,True,https://openreview.net/pdf?id=rNs2FvJGDK
rTxCRLXRtk9,Least Square Calibration for Peer Reviews,"Peer review systems such as conference paper review often suffer from the issue of miscalibration. Previous works on peer review calibration usually only use the ordinal information or assume simplistic reviewer scoring functions such as linear functions. In practice, applications like academic conferences often rely on manual methods, such as open discussions, to mitigate miscalibration. It remains an important question to develop algorithms that can   handle different types of miscalibrations based on available prior knowledge. In this paper, we propose a flexible framework, namely \\\\emph{least square calibration} (LSC), for selecting top candidates from peer ratings. Our framework provably performs perfect calibration from noiseless linear scoring functions under mild assumptions, yet also provides competitive calibration results when the scoring function is from broader classes beyond linear functions and with arbitrary noise. On our synthetic dataset, we empirically demonstrate that our algorithm consistently outperforms the baseline which select top papers based on the highest average ratings.
",main,NeurIPS,2021,Poster,Sijun Tan;Jibang Wu;Xiaohui Bei;Haifeng Xu,True,https://openreview.net/pdf?id=rTxCRLXRtk9
rX_fl1Gd0k4,Karenina: Modeling the Complexity of Negative Emotions to Better Serve Industry Goals,"Sentiment analysis systems are widely applied in industry, but standard formulations of the task (e.g., positive/negative/neutral classification) are often not well aligned with real-world goals. For instance, in customer support contexts, negative labels dominate due to the nature of the work, and different negative emotions call for different solutions. To help address this issue, we introduce Karenina, a labeled dataset of 25K consumer healthcare experience comments with labels that support standard sentiment distinctions but also allow for a breakdown into six negative emotions: confused, disappointed, frustrated, angry, stressed, and worried. Each text has 1-4 emotion labels, with over 90% of examples having at least 2 labels. We define strong baselines for this dataset, we seek to motivate a flexible approach to evaluation that takes into account the variable costs for different mistakes in different industrial contexts, and we report on some illustrative analyses using Karenina to understand customer experiences.",Datasets & Benchmarks,NeurIPS,2021,Reject,Moritz Sudhof;Liam Croteau;Christopher Potts,True,https://openreview.net/pdf?id=rX_fl1Gd0k4
rYhBGWYm6AU,Intriguing Properties of Contrastive Losses,"We study three intriguing properties of contrastive learning. First, we generalize the standard contrastive loss to a broader family of losses, and we find that various instantiations of the generalized loss perform similarly under the presence of a multi-layer non-linear projection head. Second, we study if instance-based contrastive learning (with a global image representation) can learn well on images with multiple objects present. We find that meaningful hierarchical local features can be learned despite the fact that these objectives operate on global instance-level features. Finally, we study the phenomenon of feature suppression among competing features shared across augmented views, such as ""color distribution"" vs ""object class"". We construct datasets with explicit and controllable competing features, and show that, for contrastive learning, a few bits of easy-to-learn shared features can suppress, and even fully prevent, the learning of other sets of competing features. In scenarios where there are multiple objects in an image, the dominant object would suppress the learning of smaller objects. Existing contrastive learning methods critically rely on data augmentation to favor certain sets of features over others, and could suffer from learning saturation for scenarios where existing augmentations cannot fully address the feature suppression. This poses open challenges to existing contrastive learning techniques.",main,NeurIPS,2021,Poster,Ting Chen;Calvin Luo;Lala Li,True,https://openreview.net/pdf?id=rYhBGWYm6AU
rm0I5y2zkG8,Learning to delegate for large-scale vehicle routing,"Vehicle routing problems (VRPs) form a class of combinatorial problems with wide practical applications. While previous heuristic or learning-based works achieve decent solutions on small problem instances, their performance deteriorates in large problems. This article presents a novel learning-augmented local search framework to solve large-scale VRP. The method iteratively improves the solution by identifying appropriate subproblems and $delegating$ their improvement to a black box subsolver. At each step, we leverage spatial locality to consider only a linear number of subproblems, rather than exponential. We frame subproblem selection as regression and train a Transformer on a generated training set of problem instances. Our method accelerates state-of-the-art VRP solvers by 10x to 100x while achieving competitive solution qualities for VRPs with sizes ranging from 500 to 3000. Learned subproblem selection offers a 1.5x to 2x speedup over heuristic or random selection. Our results generalize to a variety of VRP distributions, variants, and solvers.",main,NeurIPS,2021,Spotlight,Sirui Li;Zhongxia Yan;Cathy Wu,True,https://openreview.net/pdf?id=rm0I5y2zkG8
s6M0gjo0rL0,Rethinking the Role of Hyperparameter Tuning in Optimizer Benchmarking,"Many optimizers have been proposed for training deep neural networks, and they often have multiple hyperparameters, which make it tricky to benchmark their performance. In this work, we propose a new benchmarking protocol to evaluate both end-to-end efficiency (training a model from scratch without knowing the best hyperparameter configuration) and data-addition training efficiency (the previously selected hyperparameters are used for periodically re-training the model with newly collected data). For end-to-end efficiency, unlike previous work that assumes random hyperparameter tuning, which may over-emphasize the tuning time, we propose to evaluate with a bandit hyperparameter tuning strategy. ",Datasets & Benchmarks,NeurIPS,2021,Reject,Yuanhao Xiong;Xuanqing Liu;Li-Cheng Lan;Yang You;Si Si;Cho-Jui Hsieh,False,https://openreview.net/pdf?id=s6M0gjo0rL0
sD93GOzH3i5,Measuring Coding Challenge Competence With APPS,"While programming is one of the most broadly applicable skills in modern society, it is unclear how well state-of-the-art machine learning models can write code. Despite its importance, there has been surprisingly little work on evaluating code generation, and it can be difficult to assess code generation performance in an accurate and rigorous manner. To meet this challenge, we introduce APPS, a benchmark for code generation. Unlike prior work in more restricted settings, our benchmark measures the ability of models to take an arbitrary natural language specification and generate satisfactory Python code. Similar to how companies assess candidate software developers, we evaluate models by checking their generated code on test cases. Our benchmark includes 10,000 problems, which range from having simple one-line solutions to being substantial algorithmic challenges. We fine-tune large language models on both GitHub and our training set, and we find that the prevalence of syntax errors is decreasing exponentially as models improve. Recent models such as GPT-Neo can pass approximately 20% of the test cases of introductory problems, so we find that machine learning models are now beginning to learn how to code. As the social significance of automatic code generation increases over the coming years, our benchmark can provide an objective measure for tracking advancements.",Datasets & Benchmarks,NeurIPS,2021,Poster,Dan Hendrycks;Steven Basart;Saurav Kadavath;Mantas Mazeika;Akul Arora;Ethan Guo;Collin Burns;Samir Puranik;Horace He;Dawn Song;Jacob Steinhardt,True,https://openreview.net/pdf?id=sD93GOzH3i5
skFwlyefkWJ,MiniHack the Planet: A Sandbox for Open-Ended Reinforcement Learning Research,"Progress in deep reinforcement learning (RL) is heavily driven by the availability of challenging benchmarks used for training agents. However, benchmarks that are widely adopted by the community are not explicitly designed for evaluating specific capabilities of RL methods. While there exist environments for assessing particular open problems in RL (such as exploration, transfer learning, unsupervised environment design, or even language-assisted RL), it is generally difficult to extend these to richer, more complex environments once research goes beyond proof-of-concept results. We present MiniHack, a powerful sandbox framework for easily designing novel RL environments. MiniHack is a one-stop shop for RL experiments with environments ranging from small rooms to complex, procedurally generated worlds. By leveraging the full set of entities and environment dynamics from NetHack, one of the richest grid-based video games, MiniHack allows designing custom RL testbeds that are fast and convenient to use. With this sandbox framework, novel environments can be designed easily, either using a human-readable description language or a simple Python interface. In addition to a variety of RL tasks and baselines, MiniHack can wrap existing RL benchmarks and provide ways to seamlessly add additional complexity.",Datasets & Benchmarks,NeurIPS,2021,Poster,Mikayel Samvelyan;Robert Kirk;Vitaly Kurin;Jack Parker-Holder;Minqi Jiang;Eric Hambro;Fabio Petroni;Heinrich Kuttler;Edward Grefenstette;Tim Rocktäschel,False,https://openreview.net/pdf?id=skFwlyefkWJ
t1kTABPcp9s,Indian-COVID-19 CT Dataset and Analysis of Chest CT Scans of COVID-19 Patients Using Lightweight CNN,"Indian-COVID-19 CT is the chest Computed Tomography (CT) images from COVID-19 patients from India. It has been collected and curated to aid in the diagnosis of COVID-19 and other chest CT analysis tasks using machine learning algorithms. Currently it consists of 6174 images from 142 patients COVID-19, obtained from a single hospital with same image acquisition clinical settings. The dataset will be regularly updated to include more data and the original 3D volumes of dicoms will also be made available. It does not include normal or any other pneumonia images like other similar repositories. It would provide researchers opportunities to develop generalizable and robust models for COVID-19 detection and for developing models for other lung disease detection tasks. To the best of our knowledge, this is the only dataset available from Indian population making it a valuable addition to other similar repositories. Here we also propose a lightweight Convolutional Neural Network (CNN) model to classify chest CT scans into three classes, viz., Normal, non-Covid Pneumonia and COVID-19. The model has been trained and validated on publicly available dataset COVIDx-CT dataset [1]. Performance of the model is evaluated on both COVIDx-CT and Indian-COVID-19 CT datasets and is observed to be comparable, with accuracy slightly lower on Indian-COVID-19 CT dataset. This is not surprising as it is an external test set not seen by the model during training. The proposed lightweight model for diagnosing COVID-19 is well suited for a clinical setting. However, the model is still a prototype and needs more rigorous testing and re-calibrations before using it for clinical diagnosis. The dataset will be made available at http://aimedhub.iiit.ac.in/datasets/gandhi-hospital-covid-dataset. ",Datasets & Benchmarks,NeurIPS,2021,Reject,S Suba;Nita Parekh;Ramesh Loganathan;Vikram Pudi;Chinnababu Sunkavalli,True,https://openreview.net/pdf?id=t1kTABPcp9s
tMFTT3BDEK9,Look at What I’m Doing: Self-Supervised Spatial Grounding of Narrations in Instructional Videos,"We introduce the task of spatially localizing narrated interactions in videos. Key to our approach is the ability to learn to spatially localize interactions with self-supervision on a large corpus of videos with accompanying transcribed narrations. 
To achieve this goal, we propose a multilayer cross-modal attention network that enables effective optimization of a contrastive loss during training. We introduce a divided strategy that alternates between computing inter- and intra-modal attention across the visual and natural language modalities, which allows effective training via directly contrasting the two modalities' representations. We demonstrate the effectiveness of our approach by self-training on the HowTo100M instructional video dataset and evaluating on a newly collected dataset of localized described interactions in the YouCook2 dataset. We show that our approach outperforms alternative baselines, including shallow co-attention and full cross-modal attention. We also apply our approach to grounding phrases in images with weak supervision on Flickr30K and show that stacking multiple attention layers is effective and, when combined with a word-to-region loss, achieves state of the art on recall-at-one and pointing hand accuracies.",main,NeurIPS,2021,Spotlight,Reuben Tan;Bryan A. Plummer;Kate Saenko;Hailin Jin;Bryan Russell,True,https://openreview.net/pdf?id=tMFTT3BDEK9
tNMPcy9PovC,Measuring the State of Document Understanding,"Understanding documents with rich layouts plays a vital role in digitization and hyper-automation but remains a challenging topic in the NLP research community.
Additionally, the lack of a commonly accepted benchmark made it difficult to quantify progress in the domain. To empower research in Document Understanding, we present a suite of tasks that fulfill the highest quality, difficulty, and licensing criteria. The benchmark includes Visual Question Answering, Key Information Extraction, and Machine Reading Comprehension tasks over various document domains, and layouts featuring tables, graphs, lists, and infographics. The current study reports systematic baselines making use of recent advances in layout-aware language modeling. To support adoption by other researchers, both the benchmarks and reference implementations will be shortly released.",Datasets & Benchmarks,NeurIPS,2021,Reject,Łukasz Borchmann;Michał Pietruszka;Tomasz Stanislawek;Dawid Jurkiewicz;Michał Turski;Karolina Szyndler;Filip Graliński,True,https://openreview.net/pdf?id=tNMPcy9PovC
tfBBt_q4nHT,Detecting Moments and Highlights in Videos via Natural Language Queries,"Detecting customized moments and highlights from videos given natural language (NL) user queries is an important but under-studied topic. One of the challenges in pursuing this direction is the lack of annotated data. To address this issue, we present the Query-based Video Highlights (QVHighlights) dataset. It consists of over 10,000 YouTube videos, covering a wide range of topics, from everyday activities and travel in lifestyle vlog videos to social and political activities in news videos. Each video in the dataset is annotated with: (1) a human-written free-form NL query, (2) relevant moments in the video w.r.t. the query, and (3) five-point scale saliency scores for all query-relevant clips. This comprehensive annotation enables us to develop and evaluate systems that detect relevant moments as well as salient highlights for diverse, flexible user queries. We also present a strong baseline for this task, Moment-DETR, a transformer encoder-decoder model that views moment retrieval as a direct set prediction problem, taking extracted video and query representations as inputs and predicting moment coordinates and saliency scores end-to-end. While our model does not utilize any human prior, we show that it performs competitively when compared to well-engineered architectures. With weakly supervised pretraining using ASR captions, Moment-DETR substantially outperforms previous methods. Lastly, we present several ablations and visualizations of Moment-DETR. Data and code is publicly available at https://github.com/jayleicn/moment_detr.",main,NeurIPS,2021,Poster,Jie Lei;Tamara Lee Berg;Mohit Bansal,True,https://openreview.net/pdf?id=tfBBt_q4nHT
tjZjv_qh_CE,ARKitScenes: A Diverse Real-World Dataset For 3D Indoor Scene Understanding Using Mobile RGB-D Data,"Scene understanding is an active research area. Commercial depth sensors, such as Kinect, have enabled the release of several RGB-D datasets over the past few years which spawned novel methods in 3D scene understanding. More recently with the launch of the LiDAR sensor in Apple’s iPads and iPhones, high qual- ity RGB-D data is accessible to millions of people on a device they commonly use. This opens a whole new era in scene understanding for the Computer Vision community as well as app developers. The fundamental research in scene understanding together with the advances in machine learning can now impact people’s everyday experiences. However, transforming these scene un- derstanding methods to real-world experiences requires additional innovation and development. In this paper we introduce ARKitScenes. It is not only the first RGB-D dataset that is captured with a now widely available depth sensor, but to our best knowledge, it also is the largest indoor scene understanding data released. In addition to the raw and processed data from the mobile device, ARKitScenes includes high resolution depth maps captured using a stationary laser scanner, as well as manually labeled 3D oriented bounding boxes for a large taxonomy of furniture. We further analyze the usefulness of the data for two downstream tasks: 3D object detection and color-guided depth upsam- pling. We demonstrate that our dataset can help push the boundaries of existing state-of-the-art methods and it introduces new challenges that better represent real-world scenarios.",Datasets & Benchmarks,NeurIPS,2021,Poster,Gilad Baruch;Zhuoyuan Chen;Afshin Dehghan;Yuri Feigin;Peter Fu;Thomas Gebauer;Daniel Kurz;Tal Dimry;Brandon Joffe;Arik Schwartz;Elad Shulman,True,https://openreview.net/pdf?id=tjZjv_qh_CE
tjwQaOI9tdy,Symbolic Regression via Deep Reinforcement Learning Enhanced Genetic Programming Seeding,"Symbolic regression is the process of identifying mathematical expressions that fit observed output from a black-box process. It is a discrete optimization problem generally believed to be NP-hard. Prior approaches to solving the problem include neural-guided search (e.g. using reinforcement learning) and genetic programming. In this work, we introduce a hybrid neural-guided/genetic programming approach to symbolic regression and other combinatorial optimization problems. We propose a neural-guided component used to seed the starting population of a random restart genetic programming component, gradually learning better starting populations. On a number of common benchmark tasks to recover underlying expressions from a dataset, our method recovers 65% more expressions than a recently published top-performing model using the same experimental setup. We demonstrate that running many genetic programming generations without interdependence on the neural-guided component performs better for symbolic regression than alternative formulations where the two are more strongly coupled. Finally, we introduce a new set of 22 symbolic regression benchmark problems with increased difficulty over existing benchmarks. Source code is provided at www.github.com/brendenpetersen/deep-symbolic-optimization.",main,NeurIPS,2021,Poster,Terrell N. Mundhenk;Mikel Landajuela;Ruben Glatt;Claudio P. Santiago;Daniel faissol;Brenden K. Petersen,True,https://openreview.net/pdf?id=tjwQaOI9tdy
tyn3MYS_uDT,Open Bandit Dataset and Pipeline: Towards Realistic and Reproducible Off-Policy Evaluation,"\\\\textit{Off-policy evaluation} (OPE) aims to estimate the performance of hypothetical policies using data generated by a different policy. Because of its huge potential impact in practice, there has been growing research interest in this field. There is, however, no real-world public dataset that enables the evaluation of OPE, making its experimental studies unrealistic and irreproducible. With the goal of enabling realistic and reproducible OPE research, we present \\\\textit{Open Bandit Dataset}, a public logged bandit dataset collected on a large-scale fashion e-commerce platform, ZOZOTOWN. Our dataset is unique in that it contains a set of \\\\textit{multiple} logged bandit datasets collected by running different policies on the same platform. This enables experimental comparisons of different OPE estimators for the first time. We also develop Python software called \\\\textit{Open Bandit Pipeline} to streamline and standardize the implementation of batch bandit algorithms and OPE. Our open data and software will contribute to fair and transparent OPE research and help the community identify fruitful research directions. We provide extensive benchmark experiments of existing OPE estimators using our dataset and software. The results open up essential challenges and new avenues for future OPE research.",Datasets & Benchmarks,NeurIPS,2021,Poster,Yuta Saito;Shunsuke Aihara;Megumi Matsutani;Yusuke Narita,True,https://openreview.net/pdf?id=tyn3MYS_uDT
uKv5inrWeld,Evaluating Bayes Error Estimators on Real-World Datasets with FeeBee,"The Bayes error rate (BER) is a fundamental concept in machine learning that quantifies the best possible accuracy any classifier can achieve on a fixed probability distribution. Despite years of research on building estimators of lower and upper bounds for the BER, these were usually compared only on synthetic datasets with known probability distributions, leaving two key questions unanswered: (1) How well do they perform on realistic, non-synthetic datasets?, and (2) How practical are they? Answering these is not trivial. Apart from the obvious challenge of an unknown BER for real-world datasets, there are two main aspects any BER estimator needs to overcome in order to be applicable in real-world settings: (1) the computational and sample complexity, and (2) the sensitivity and selection of hyper-parameters.
In this work, we propose FeeBee, the first principled framework for analyzing and comparing BER estimators on modern real-world datasets with unknown probability distribution. We achieve this by injecting a controlled amount of label noise and performing multiple evaluations on a series of different noise levels, supported by a theoretical result which allows drawing conclusions about the evolution of the BER. By implementing and analyzing 7 multi-class BER estimators on 6 commonly used datasets of the computer vision and NLP domains, FeeBee allows a thorough study of these estimators, clearly identifying strengths and weaknesses of each, whilst being easily deployable on any future BER estimator.",Datasets & Benchmarks,NeurIPS,2021,Poster,Cedric Renggli;Luka Rimanic;Nora Hollenstein;Ce Zhang,False,https://openreview.net/pdf?id=uKv5inrWeld
uUa4jNMLjrL,"DENETHOR: The DynamicEarthNET dataset for Harmonized, inter-Operable, analysis-Ready, daily crop monitoring from space","Recent advances in remote sensing products allow near-real time monitoring of the Earth’s surface. Despite increasing availability of near-daily time-series of satellite imagery, there has been little exploration of deep learning methods to utilize the unprecedented temporal density of observations. This is particularly interesting in crop monitoring where time-series remote sensing data has been used frequently to exploit phenological differences of crops in the growing cycle over time. In this work, we present DENETHOR: The DynamicEarthNET dataset for Harmonized, inter-Operabel, analysis-Ready, daily crop monitoring from space. Our dataset contains daily, analysis-ready Planet Fusion data together with Sentinel-1 radar and Sentinel-2 optical time-series for crop type classification in Northern Germany. Our baseline experiments underline that incorporating the available spatial and temporal information fully may not be straightforward and could require the design of tailored architectures. The dataset presents two main challenges to the community: Exploit the temporal dimension for improved crop classification and ensure that models can handle a domain shift to a different year.",Datasets & Benchmarks,NeurIPS,2021,Poster,Lukas Kondmann;Aysim Toker;Marc Rußwurm;Andrés Camero;Devis Peressuti;Grega Milcinski;Pierre-Philippe Mathieu;Nicolas Longépé;Timothy Davis;Giovanni Marchisio;Laura Leal-Taixé;Xiao Xiang Zhu,True,https://openreview.net/pdf?id=uUa4jNMLjrL
uXa9oBDZ9V1,IconQA: A New Benchmark for Abstract Diagram Understanding and Visual Language Reasoning,"Current visual question answering (VQA) tasks mainly consider answering human-annotated questions for natural images. However, aside from natural images, abstract diagrams with semantic richness are still understudied in visual understanding and reasoning research. In this work, we introduce a new challenge of Icon Question Answering (IconQA) with the goal of answering a question in an icon image context. We release IconQA, a large-scale dataset that consists of 107,439 questions and three sub-tasks: multi-image-choice, multi-text-choice, and filling-in-the-blank. The IconQA dataset is inspired by real-world diagram word problems that highlight the importance of abstract diagram understanding and comprehensive cognitive reasoning. Thus, IconQA requires not only perception skills like object recognition and text understanding, but also diverse cognitive reasoning skills, such as geometric reasoning, commonsense reasoning, and arithmetic reasoning. To facilitate potential IconQA models to learn semantic representations for icon images, we further release an icon dataset Icon645 which contains 645,687 colored icons on 377 classes. We conduct extensive user studies and blind experiments and reproduce a wide range of advanced VQA methods to benchmark the IconQA task. Also, we develop a strong IconQA baseline Patch-TRM that applies a pyramid cross-modal Transformer with input diagram embeddings pre-trained on the icon dataset. IconQA and Icon645 are available at https://iconqa.github.io.",Datasets & Benchmarks,NeurIPS,2021,Poster,Pan Lu;Liang Qiu;Jiaqi Chen;Tony Xia;Yizhou Zhao;Wei Zhang;Zhou Yu;Xiaodan Liang;Song-Chun Zhu,True,https://openreview.net/pdf?id=uXa9oBDZ9V1
udVUN__gFO,DEBAGREEMENT: A comment-reply dataset for (dis)agreement detection in online debates,"In this paper, we introduce DEBAGREEMENT, a dataset of 42,894 comment-reply pairs from the popular discussion website Reddit, annotated with agree, neutral or disagree labels. We collect data from five forums on Reddit: r/BlackLivesMatter, r/Brexit, r/climate, r/democrats, r/Republican. For each forum, we select comment pairs such that they form altogether a user interaction graph. DEBAGREEMENT presents a challenge for Natural Language Processing (NLP) systems, as it contains slang, sarcasm and topic-specific jokes, often present in online exchanges. We evaluate the performance of state-of-the-art language models on a (dis)agreement detection task, and investigate the use of contextual information available (graph, authorship, and temporal information). Since recent research has shown that context, such as social context or knowledge graph information, enables language models to better perform on downstream NLP tasks, DEBAGREEMENT provides novel opportunities for combining graph-based and text-based machine learning techniques to detect (dis)agreements online.",Datasets & Benchmarks,NeurIPS,2021,Poster,John Pougué-Biyong;Valentina Semenova;Alexandre Matton;Rachel Han;Aerin Kim;Renaud Lambiotte;Doyne Farmer,True,https://openreview.net/pdf?id=udVUN__gFO
vCthaJ4ywT,Unsupervised Motion Representation Learning with Capsule Autoencoders,"We propose the Motion Capsule Autoencoder (MCAE), which addresses a key challenge in the unsupervised learning of motion representations: transformation invariance. MCAE models motion in a two-level hierarchy. In the lower level, a spatio-temporal motion signal is divided into short, local, and semantic-agnostic snippets. In the higher level, the snippets are aggregated to form full-length semantic-aware segments. For both levels, we represent motion with a set of learned transformation invariant templates and the corresponding geometric transformations by using capsule autoencoders of a novel design. This leads to a robust and efficient encoding of viewpoint changes. MCAE is evaluated on a novel Trajectory20 motion dataset and various real-world skeleton-based human action datasets. Notably, it achieves better results than baselines on Trajectory20 with considerably fewer parameters and state-of-the-art performance on the unsupervised skeleton-based action recognition task.",main,NeurIPS,2021,Poster,Ziwei Xu;Xudong Shen;Yongkang Wong;Mohan Kankanhalli,True,https://openreview.net/pdf?id=vCthaJ4ywT
vDilkBNNbx6,CARLA: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms,"Counterfactual explanations provide means for prescriptive model explanations by suggesting actionable feature changes (e.g., increase income) that allow individuals to achieve favourable outcomes in the future (e.g., insurance approval).
Choosing an appropriate method is a crucial aspect for meaningful counterfactual explanations. As documented in recent reviews, there exists a quickly growing literature with available methods. Yet, in the absence of widely available open--source implementations, the decision in favour of certain models is primarily based on what is readily available. Going forward -- to guarantee meaningful comparisons across explanation methods -- we present \\\\texttt{CARLA} (\\\\textbf{C}ounterfactual \\\\textbf{A}nd \\\\textbf{R}ecourse \\\\textbf{L}ibr\\\\textbf{A}ry), a python library for benchmarking counterfactual explanation methods across both different data sets and different machine learning models. In summary, our work provides the following contributions: (i) an extensive benchmark of 11 popular counterfactual explanation methods, (ii) a benchmarking framework for research on future counterfactual explanation methods, and (iii) a standardized set of integrated evaluation measures and data sets for transparent and extensive comparisons of these methods.
We have open sourced \\\\texttt{CARLA} and our experimental results on \\\\href{https://github.com/indyfree/CARLA}{Github}, making them available as competitive baselines. We welcome contributions from other research groups and practitioners.",Datasets & Benchmarks,NeurIPS,2021,Poster,Martin Pawelczyk;Sascha Bielawski;Johan Van den Heuvel;Tobias Richter;Gjergji Kasneci,False,https://openreview.net/pdf?id=vDilkBNNbx6
vKQGe36av4k,Argoverse 2: Next Generation Datasets for Self-Driving Perception and Forecasting,"We introduce Argoverse 2 (AV2) — a collection of three datasets for perception and forecasting research in the self-driving domain. The annotated Sensor Dataset contains 1,000 sequences of multimodal data, encompassing high-resolution imagery from seven ring cameras, and two stereo cameras in addition to lidar point clouds, and 6-DOF map-aligned pose. Sequences contain 3D cuboid annotations for 26 object categories, all of which are sufficiently-sampled to support training and evaluation of 3D perception models. The Lidar Dataset contains 20,000 sequences of unlabeled lidar point clouds and map-aligned pose. This dataset is the largest ever collection of lidar sensor data and supports self-supervised learning and the emerging task of point cloud forecasting. Finally, the Motion Forecasting Dataset contains 250,000 scenarios mined for interesting and challenging interactions be- tween the autonomous vehicle and other actors in each local scene. Models are tasked with the prediction of future motion for “scored actors"" in each scenario and are provided with track histories that capture object location, heading, velocity, and category. In all three datasets, each scenario contains its own HD Map with 3D lane and crosswalk geometry — sourced from data captured in six distinct cities. We believe these datasets will support new and existing machine learning research problems in ways that existing datasets do not. All datasets are released under the CC BY-NC-SA 4.0 license.",Datasets & Benchmarks,NeurIPS,2021,Poster,Benjamin Wilson;William Qi;Tanmay Agarwal;John Lambert;Jagjeet Singh;Siddhesh Khandelwal;Bowen Pan;Ratnesh Kumar;Andrew Hartnett;Jhony Kaesemodel Pontes;Deva Ramanan;Peter Carr;James Hays,True,https://openreview.net/pdf?id=vKQGe36av4k
vhjsBtq9OxO,HumBugDB: A Large-scale Acoustic Mosquito Dataset,"This paper presents the first large-scale multi-species dataset of acoustic recordings of mosquitoes tracked continuously in free flight. We present 20 hours of audio recordings that we have expertly labelled and tagged precisely in time. Significantly, 18 hours of recordings contain annotations from 36 different species. Mosquitoes are well-known carriers of diseases such as malaria, dengue and yellow fever. Collecting this dataset is motivated by the need to assist applications which utilise mosquito acoustics to conduct surveys to help predict outbreaks and inform intervention policy. The task of detecting mosquitoes from the sound of their wingbeats is challenging due to the difficulty in collecting recordings from realistic scenarios. To address this, as part of the HumBug project, we conducted global experiments to record mosquitoes ranging from those bred in culture cages to mosquitoes captured in the wild. Consequently, the audio recordings vary in signal-to-noise ratio and contain a broad range of indoor and outdoor background environments from Tanzania, Thailand, Kenya, the USA and the UK. In this paper we describe in detail how we collected, labelled and curated the data.  The data is provided from a PostgreSQL database, which captures important metadata such as the capture method, age, feeding status and gender of the mosquitoes. Additionally, we provide code to extract features and train Bayesian convolutional neural networks for two key tasks: the identification of mosquitoes from their corresponding background environments, and the classification of detected mosquitoes into species. Our extensive dataset is both challenging to machine learning researchers focusing on acoustic identification, and critical to entomologists, geo-spatial modellers and other domain experts to understand mosquito behaviour, model their distribution, and manage the threat they pose to humans.",Datasets & Benchmarks,NeurIPS,2021,Poster,Ivan Kiskin;Marianne Sinka;Adam D. Cobb;Waqas Rafique;Lawrence Wang;Davide Zilli;Benjamin Gutteridge;Rinita Dam;Theodoros Marinos;Yunpeng Li;Dickson Msaky;Emmanuel Kaindoa;Gerard Killeen;Eva Herreros-Moya;Kathy Willis;Stephen J. Roberts,True,https://openreview.net/pdf?id=vhjsBtq9OxO
vqHak8NLk25,Parameter Prediction for Unseen Deep Architectures,"Deep learning has been successful in automating the design of features in machine learning pipelines. However, the algorithms optimizing neural network parameters remain largely hand-designed and computationally inefficient. We study if we can use deep learning to directly predict these parameters by exploiting the past knowledge of training other networks. We introduce a large-scale dataset of diverse computational graphs of neural architectures - DeepNets-1M - and use it to explore parameter prediction on CIFAR-10 and ImageNet. By leveraging advances in graph neural networks, we propose a hypernetwork that can predict performant parameters in a single forward pass taking a fraction of a second, even on a CPU. The proposed model achieves surprisingly good performance on unseen and diverse networks. For example, it is able to predict all 24 million parameters of a ResNet-50 achieving a 60% accuracy on CIFAR-10. On ImageNet, top-5 accuracy of some of our networks approaches 50%. Our task along with the model and results can potentially lead to a new, more computationally efficient paradigm of training networks. Our model also learns a strong representation of neural architectures enabling their analysis.",main,NeurIPS,2021,Poster,Boris Knyazev;Michal Drozdzal;Graham W. Taylor;Adriana Romero,True,https://openreview.net/pdf?id=vqHak8NLk25
vsCCDVdTAx,Human-Adversarial Visual Question Answering,"Performance on the most commonly used Visual Question Answering dataset (VQA v2) is starting to approach human accuracy. However, in interacting with state-of-the-art VQA models, it is clear that the problem is far from being solved. In order to stress test VQA models, we benchmark them against human-adversarial examples. Human subjects interact with a state-of-the-art VQA model, and for each image in the dataset, attempt to find a question where the model’s predicted answer is incorrect. We find that a wide range of state-of-the-art models perform poorly when evaluated on these examples. We conduct an extensive analysis of the collected adversarial examples and provide guidance on future research directions. We hope that this Adversarial VQA (AdVQA) benchmark can help drive progress in the field and advance the state of the art.",main,NeurIPS,2021,Poster,Sasha Sheng;Amanpreet Singh;Vedanuj Goswami;Jose Alberto Lopez Magana;Tristan Thrush;Wojciech Galuba;Devi Parikh;Douwe Kiela,True,https://openreview.net/pdf?id=vsCCDVdTAx
vyyHxhnZJSB,GrowSpace: Learning How to Shape Plants,"Plants are dynamic systems that are integral to our existence and survival. Plants are faced with  environment changes and adapt over time to their surrounding conditions. We argue that plant responses to an environmental stimulus are a good example of a real-world problem that can be approached within a reinforcement learning (RL) framework. With the objective of controlling a plant by moving the light source, we propose GrowSpace, as a new RL benchmark. The back-end of the simulator is implemented using the Space Colonisation Algorithm, a plant growing model based on competition for space. Compared to video game RL environments, this simulator addresses a real-world problem and serves as a test bed to visualize plant growth and movement in a faster way than physical experiments. GrowSpace is composed of a suite of challenges that tackle several problems such as control, hierarchical learning, fairness and multi-objective learning. We provide agent baselines alongside case studies to demonstrate the difficulty of the proposed benchmark. ",Datasets & Benchmarks,NeurIPS,2021,Reject,Yasmeen Hitti;Ionelia Buzatu;Manuel Del Verme;Mark Lefsrud;Florian Golemo;Audrey Durand,True,https://openreview.net/pdf?id=vyyHxhnZJSB
vzb0f0TIVlI,"A Bilingual, OpenWorld Video Text Dataset and End-to-end Video Text Spotter with Transformer","Most existing video text spotting benchmarks focus on evaluating a single language and scenario with limited data. In this work, we introduce a large-scale, Bilingual, Open World Video text benchmark dataset(BOVText). There are four features for BOVText. Firstly, we provide 1,850+ videos with more than 1,600,000+ frames, 25 times larger than the existing largest dataset with incidental text in videos. Secondly, our dataset covers 30+ open categories with a wide selection of various scenarios, Life Vlog, Driving, Movie, etc. Thirdly, abundant text types annotation (i.e., title, caption, or scene text) are provided for the different representational meanings in the video. Fourthly, the MOVText provides multilingual text annotation to promote multiple cultures' live and communication.  Besides, we propose an end-to-end video text spotting framework with Transformer, termed TransVTSpotter, which solves the multi-orient text spotting in video with a simple, but efficient attention-based query-key mechanism. It applies object features from the previous frame as a tracking query for the current frame and introduces a rotation angle prediction to fit the multi-orient text instance. On ICDAR2015(video), TransVTSpotter achieves state-of-the-art performance with 44.2% MOTA, 13 fps. The dataset and code of TransVTSpotter can be found at https://github.com/weijiawu/BOVText-Benchmark and https://github.com/weijiawu/TransVTSpotter, respectively.",Datasets & Benchmarks,NeurIPS,2021,Poster,Weijia Wu;Debing Zhang;Yuanqiang Cai;Sibo Wang;Jiahong Li;Zhuang Li;Yejun Tang;Hong Zhou,True,https://openreview.net/pdf?id=vzb0f0TIVlI
wBOBL5b-aa9,DermX: a Dermatological Diagnosis Explainability Dataset,"In this paper, we introduce DermX: a novel dermatological diagnosis and explanations dataset annotated by eight board-certified dermatologists. To date, public datasets for dermatological applications have been limited to diagnosis and lesion segmentation, while validation of dermatological explainability has been limited to visual inspection. As such, this work is a first release of a dataset providing gold standard explanations for dermatological diagnosis to enable a quantitative evaluation of ConvNet explainability. DermX consists of 525 images sourced from two public datasets, DermNetNZ and SD-260, spanning six of the most prevalent skin conditions. Each image was enriched with diagnoses and diagnosis explanations by three dermatologists. Supporting explanations were collected as 15 non-localisable characteristics, 16 localisable characteristics, and 23 additional terms.  Dermatologists manually segmented localisable characteristic and described them with additional terms.  We showcase a possible use of our dataset by benchmarking the explainability of two ConvNet architectures, ResNet-50 and EfficientNet-B4,trained on an internal skin lesion dataset and tested on DermX. ConvNet visualisations are obtained through gradient-weighted class-activation map (Grad-CAM), a commonly used model visualisation technique. Our analysis reveals EfficientNet-B4 as the most explainable between the two. Thus, we prove that DermX can be used to objectively benchmark the explainability power of dermatological diagnosis models. The dataset is available at https://github.com/ralucaj/dermx.",Datasets & Benchmarks,NeurIPS,2021,Reject,Raluca Jalaboi;Mauricio Orbes Arteaga;Dan Richter Jørgensen;Ionela Manole;Oana Ionescu-Bozdog;Andrei Chiriac;Ole Winther;Alfiia Galimzianova,True,https://openreview.net/pdf?id=wBOBL5b-aa9
wCu6T5xFjeJ,BEIR: A Heterogeneous Benchmark for Zero-shot Evaluation of Information Retrieval Models,"Existing neural information retrieval (IR) models have often been studied in homogeneous and narrow settings, which has considerably limited insights into their out-of-distribution (OOD) generalization capabilities. To address this, and to facilitate researchers to broadly evaluate the effectiveness of their models, we introduce Benchmarking-IR (BEIR), a robust and heterogeneous evaluation benchmark for information retrieval. We leverage a careful selection of 18 publicly available datasets from diverse text retrieval tasks and domains and evaluate 10 state-of-the-art retrieval systems including lexical, sparse, dense, late-interaction, and re-ranking architectures on the BEIR benchmark. Our results show BM25 is a robust baseline and re-ranking and late-interaction based models on average achieve the best zero-shot performances, however, at high computational costs. In contrast, dense and sparse-retrieval models are computationally more efficient but often underperform other approaches, highlighting the considerable room for improvement in their generalization capabilities. We hope this framework allows us to better evaluate and understand existing retrieval systems and contributes to accelerating progress towards more robust and generalizable systems in the future. BEIR is publicly available at https://github.com/UKPLab/beir.",Datasets & Benchmarks,NeurIPS,2021,Poster,Nandan Thakur;Nils Reimers;Andreas Rücklé;Abhishek Srivastava;Iryna Gurevych,False,https://openreview.net/pdf?id=wCu6T5xFjeJ
wEc1mgAjU-,Monash Time Series Forecasting Archive,"Many businesses nowadays rely on large quantities of time series data making time series forecasting an important research area. Global forecasting models and multivariate models that are trained across sets of time series have shown huge potential in providing accurate forecasts compared with the traditional univariate forecasting models that work on isolated series. However, there are currently no comprehensive time series forecasting archives that contain datasets of time series from similar sources available for researchers to evaluate the performance of new global or multivariate forecasting algorithms over varied datasets. In this paper, we present such a comprehensive forecasting archive containing 25 publicly available time series datasets from varied domains, with different characteristics in terms of frequency, series lengths, and inclusion of missing values. We also characterise the datasets, and identify similarities and differences among them, by conducting a feature analysis. Furthermore, we present the performance of a set of standard baseline forecasting methods over all datasets across ten error metrics, for the benefit of researchers using the archive to benchmark their forecasting algorithms.",Datasets & Benchmarks,NeurIPS,2021,Poster,Rakshitha Wathsadini Godahewa;Christoph Bergmeir;Geoffrey I. Webb;Rob Hyndman;Pablo Montero-Manso,False,https://openreview.net/pdf?id=wEc1mgAjU-
woX9uagUQiE,MIND dataset for diet planning and dietary healthcare with machine learning: Dataset creation using combinatorial optimization and controllable generation with domain experts,"Diet planning, a basic and regular human activity, is important to all individuals. Children, adults, the healthy, and the infirm all profit from diet planning. Many recent attempts have been made to develop machine learning (ML) applications related to diet planning. However, given the complexity and difficulty of implementing this task, no high-quality diet-level dataset exists at present. Professionals, particularly dietitians and physicians, would benefit greatly from such a dataset and ML application. In this work, we create and publish the Korean Menus–Ingredients–Nutrients–Diets (MIND) dataset for a ML application regarding diet planning and dietary health research. The nature of diet planning entails both explicit (nutrition) and implicit (composition) requirements. Thus, the MIND dataset was created by integrating input from experts who considered implicit data requirements for diet solution with the capabilities of an operations research (OR) model that specifies and applies explicit data requirements for diet solution and a controllable generative machine that automates the high-quality diet generation process. MIND consists of data from 1,500 South Korean daily diets, 3,238 menus, and 3,036 ingredients. MIND considers the daily recommended dietary intake of 14 major nutrients. MIND can be easily downloaded and analyzed using the Python package dietkit accessible via the package installer for Python. MIND is expected to contribute to the use of ML in solving medical, economic, and social problems associated with diet planning. Furthermore, our approach of integrating data from experts with OR and ML models is expected to promote the use of ML in other fields that require the generation of high-quality synthetic professional task data, especially since the use of ML to automate and support professional tasks has become a highly valuable service.",Datasets & Benchmarks,NeurIPS,2021,Poster,Changhun Lee;Soohyeok Kim;Sehwa Jeong;Chiehyeon Lim;Jayun Kim;Yeji Kim;Minyoung Jung,True,https://openreview.net/pdf?id=woX9uagUQiE
x4oe1W8Hpl3,MOMA: Multi-Object Multi-Actor Activity Parsing,"Complex activities often involve multiple humans utilizing different objects to complete actions (e.g., in healthcare settings, physicians, nurses, and patients interact with each other and various medical devices). Recognizing activities poses a challenge that requires a detailed understanding of actors' roles, objects' affordances, and their associated relationships. Furthermore, these purposeful activities are composed of multiple achievable steps, including sub-activities and atomic actions, which jointly define a hierarchy of action parts. This paper introduces Activity Parsing as the overarching task of temporal segmentation and classification of activities, sub-activities, atomic actions, along with an instance-level understanding of actors, objects, and their relationships in videos. Involving multiple entities (actors and objects), we argue that traditional pair-wise relationships, often used in scene or action graphs, do not appropriately represent the dynamics between them. Hence, we introduce Action Hypergraph, a spatial-temporal graph containing hyperedges (i.e., edges with higher-order relationships), as a new representation. In addition, we introduce Multi-Object Multi-Actor (MOMA), the first benchmark and dataset dedicated to activity parsing. Lastly, to parse a video, we propose the HyperGraph Activity Parsing (HGAP) network, which outperforms several baselines, including those based on regular graphs and raw video data. ",main,NeurIPS,2021,Poster,Zelun Luo;Wanze Xie;Siddharth Kapoor;Yiyun Liang;Michael Cooper;Juan Carlos Niebles;Ehsan Adeli;L. Fei-Fei,True,https://openreview.net/pdf?id=x4oe1W8Hpl3
x4t0fxWPNdi,Implicit Transformer Network for Screen Content Image Continuous Super-Resolution,"Nowadays, there is an explosive growth of screen contents due to the wide application of screen sharing, remote cooperation, and online education. To match the limited terminal bandwidth,  high-resolution (HR) screen contents may be downsampled and compressed.  At the receiver side, the super-resolution (SR)of low-resolution (LR) screen content images (SCIs) is highly demanded by the HR display or by the users to zoom in for detail observation.  However, image SR methods mostly designed for natural images do not generalize well for SCIs due to the very different image characteristics as well as the requirement of SCI browsing at arbitrary scales. To this end, we propose a novel Implicit Transformer Super-Resolution Network (ITSRN) for SCISR. For high-quality continuous SR at arbitrary ratios, pixel values at query coordinates are inferred from image features at key coordinates by the proposed implicit transformer and an implicit position encoding scheme is proposed to aggregate similar neighboring pixel values to the query one. We construct benchmark SCI1K and SCI1K-compression datasets withLR and HR SCI pairs.  Extensive experiments show that the proposed ITSRN significantly outperforms several competitive continuous and discrete SR methods for both compressed and uncompressed SCIs.",main,NeurIPS,2021,Poster,Jingyu Yang;Sheng Shen;Huanjing Yue;Kun Li,True,https://openreview.net/pdf?id=x4t0fxWPNdi
xVQMrDLyGst,Contemporary Symbolic Regression Methods and their Relative Performance,"Many promising approaches to symbolic regression have been presented in recent years, yet progress in the field continues to suffer from a lack of uniform, robust, and transparent benchmarking standards. In this paper, we address this shortcoming by introducing an open-source, reproducible benchmarking platform for symbolic regression. We assess 14 symbolic regression methods and 7 machine learning methods on a set of 252 diverse regression problems.  Our assessment includes both real-world datasets with no known model form as well as ground-truth benchmark problems, including physics equations and systems of ordinary differential equations. For the real-world datasets, we benchmark the ability of each method to learn models with low error and low complexity relative to state-of-the-art machine learning methods. For the synthetic problems, we assess each method's ability to find exact solutions in the presence of varying levels of noise. Under these controlled experiments, we conclude that the best performing methods for real-world regression combine genetic algorithms with parameter estimation and/or semantic search drivers. When tasked with recovering exact equations in the presence of noise, we find that deep learning and genetic algorithm-based approaches perform similarly. We provide a detailed guide to reproducing this experiment and contributing new methods, and encourage other researchers to collaborate with us on a common and living symbolic regression benchmark.
",Datasets & Benchmarks,NeurIPS,2021,Poster,William La Cava;Patryk Orzechowski;Bogdan Burlacu;Fabricio Olivetti de Franca;Marco Virgolin;Ying Jin;Michael Kommenda;Jason H. Moore,True,https://openreview.net/pdf?id=xVQMrDLyGst
y2AbfIXgBK3,VFP290K: A Large-Scale Benchmark Dataset for Vision-based Fallen Person Detection,"Detection of fallen persons due to, for example, health problems, violence, or accidents, is a critical challenge. Accordingly, detection of these anomalous events is of paramount importance for a number of applications, including but not limited to CCTV surveillance, security, and health care. Given that many detection systems rely on a comprehensive dataset comprising fallen person images collected under diverse environments and in various situations is crucial. However, existing datasets are limited to only specific environmental conditions and lack diversity. To address the above challenges and help researchers develop more robust detection systems, we create a novel, large-scale dataset for the detection of fallen persons composed of fallen person images collected in various real-world scenarios, with the support of the South Korean government. Our Vision-based Fallen Person (VFP290K) dataset consists of 294,713 frames of fallen persons extracted from 178 videos, including 131 scenes in 49 locations. We empirically demonstrate the effectiveness of the features through extensive experiments analyzing the performance shift based on object detection models. In addition, we evaluate our VFP290K dataset with properly divided versions of our dataset by measuring the performance of fallen person detecting systems. We ranked first in the first round of the anomalous behavior recognition track of AI Grand Challenge 2020, South Korea, using our VFP290K dataset, which can be found here. Our achievement implies the usefulness of our dataset for research on fallen person detection, which can further extend to other applications, such as intelligent CCTV or monitoring systems. The data and more up-to-date information have been provided at our VFP290K site.",Datasets & Benchmarks,NeurIPS,2021,Poster,Jaeju An;Jeongho Kim;Hanbeen Lee;Jinbeom Kim;Junhyung Kang;Minha Kim;Saebyeol Shin;Minha Kim;Donghee Hong;Simon S. Woo,True,https://openreview.net/pdf?id=y2AbfIXgBK3
yJyIjWyPJgs,Towards a robust experimental framework and benchmark for lifelong language learning,"In lifelong learning, a model learns different tasks sequentially throughout its lifetime. State-of-the-art deep learning models, however, struggle to generalize in this setting and suffer from catastrophic forgetting of old tasks when learning new ones. While a number of approaches have been developed in an attempt to ameliorate this problem, there are no established, unified or generalized frameworks for rigorous evaluations of proposed solutions; a problem which is particularly pronounced in the domain of NLP. The few existing benchmarks are typically limited to a specific flavor of lifelong learning -- continual open-set classification -- where new classes, as opposed to tasks, are learned incrementally. Moreover, the only general lifelong learning benchmark combines a multi-label classification setup with a multi-class classification setup resulting in misleading gradients during training. We empirically demonstrate that the catastrophic forgetting observed here can be attributed to the experimental design rather than to any inherent modeling limitations. To address these issues, we propose an experimental framework for true, general lifelong learning in NLP. Using this framework, we develop a comprehensive suite of benchmarks that target different properties of lifelong learning (e.g., forgetting or intransigence); experiment with diverse facets of language learning: multi-domain, multilingual and different levels of linguistic hierarchy; and present a continuous evaluation scheme under a new metric: Area Under the Lifelong Test Curve. Our framework reveals shortcomings of prevalent memory-based solutions, demonstrating they are unable to outperform a simple experience replay baseline under the realistic lifelong learning setup.",Datasets & Benchmarks,NeurIPS,2021,Poster,Aman Hussain;Nithin Holla;Pushkar Mishra;Helen Yannakoudakis;Ekaterina Shutova,True,https://openreview.net/pdf?id=yJyIjWyPJgs
yYQuqGcxFvb,GitTables: A Large-Scale Corpus of Relational Tables,"The practical success of deep learning has sparked interest in improving relational table tasks, like data search, with models trained on large table corpora. Existing corpora primarily contain tables extracted from HTML pages, limiting the capability to represent offline database tables. To train and evaluate high-capacity models for applications beyond the Web, we need additional resources with tables that resemble relational database tables. 

Here we introduce GitTables, a corpus of currently 1.7M relational tables extracted from GitHub. Our continuing curation aims at growing the corpus to at least 20M tables. We annotate table columns in GitTables with more than 2K different semantic types from Schema.org and DBpedia. Our column annotations consist of semantic types, hierarchical relations, range types and descriptions. The corpus is available at https://gittables.github.io.

Our analysis of GitTables shows that its structure, content, and topical coverage differ significantly from existing table corpora. We evaluate our annotation pipeline on hand-labeled tables from the T2Dv2 benchmark and find that our approach provides results on par with human annotations. We demonstrate a use case of GitTables by training a semantic type detection model on it and obtain high prediction accuracy. We also show that the same model trained on tables from the Web generalizes poorly.",Datasets & Benchmarks,NeurIPS,2021,Reject,Madelon Hulsebos;Çağatay Demiralp;Paul Groth,True,https://openreview.net/pdf?id=yYQuqGcxFvb
yl9aThYT9W,All-In-One Drive: A Comprehensive Perception Dataset with High-Density Long-Range Point Clouds,"Developing datasets that cover comprehensive sensors, annotations, and out-of-distribution data is important for innovating robust multi-sensor multi-task perception systems in autonomous driving. Though many datasets have been released, they target different use-cases such as 3D segmentation (SemanticKITTI), radar data (nuScenes), large-scale training and evaluation (Waymo). As a result, we are still in need of a dataset that forms a union of various strengths of existing datasets. To address this challenge, we present the AIODrive dataset, a synthetic large-scale dataset that provides comprehensive sensors, annotations, and environmental variations. Specifically, we provide (1) eight sensor modalities (RGB, Stereo, Depth, LiDAR, SPAD-LiDAR, Radar, IMU, GPS), (2) annotations for all mainstream perception tasks (e.g., detection, tracking, prediction, segmentation, depth estimation, etc), and (3) out-of-distribution driving scenarios such as adverse weather and lighting, crowded scenes, high-speed driving, violation of traffic rules, and vehicle crash. In addition to comprehensive data, long-range perception is also important to perception systems as early detection of faraway objects can help prevent collision in high-speed driving scenarios. However, due to the sparsity and limited range of point cloud data in prior datasets, developing and evaluating long-range perception algorithms is not feasible. To address the issue, we provide high-density long-range point clouds for LiDAR and SPAD-LiDAR sensors (10x than Velodyne-64), to enable research in long-range perception. Our dataset is released and free to use for both research and commercial purpose: http://www.aiodrive.org/",Datasets & Benchmarks,NeurIPS,2021,Reject,Xinshuo Weng;Yunze Man;Jinhyung Park;Ye Yuan;Matthew O'Toole;Kris M. Kitani,True,https://openreview.net/pdf?id=yl9aThYT9W
zNQBIBKJRkd,"Reduced, Reused and Recycled: The Life of a Dataset in Machine Learning Research","Benchmark datasets play a central role in the organization of machine learning research. They coordinate researchers around shared research problems and serve as a measure of progress towards shared goals.  Despite the foundational role of benchmarking practices in this field, relatively little attention has been paid to the dynamics of benchmark dataset use and reuse, within or across machine learning subcommunities. In this paper, we dig into these dynamics. We study how dataset usage patterns differ across machine learning subcommunities and across time from 2015-2020. We find increasing concentration on fewer and fewer datasets within task communities, significant adoption of datasets from other tasks, and concentration across the field on datasets that have been introduced by researchers situated within a small number of elite institutions. Our results have implications for scientific evaluation, AI ethics, and equity/access within the field.",Datasets & Benchmarks,NeurIPS,2021,Poster,Bernard Koch;Emily Denton;Alex Hanna;Jacob Gates Foster,False,https://openreview.net/pdf?id=zNQBIBKJRkd
zQIvkXHS_U5,ManiSkill: Generalizable Manipulation Skill Benchmark with Large-Scale Demonstrations,"Object manipulation from 3D visual inputs poses many challenges on building generalizable perception and policy models. However, 3D assets in existing benchmarks mostly lack the diversity of 3D shapes that align with real-world intra-class complexity in topology and geometry. Here we propose SAPIEN Manipulation Skill Benchmark (ManiSkill) to benchmark manipulation skills over diverse objects in a full-physics simulator. 3D assets in ManiSkill include large intra-class topological and geometric variations. Tasks are carefully chosen to cover distinct types of manipulation challenges. Latest progress in 3D vision also makes us believe that we should customize the benchmark so that the challenge is inviting to researchers working on 3D deep learning. To this end, we simulate a moving panoramic camera that returns ego-centric point clouds or RGB-D images. In addition, we would like ManiSkill to serve a broad set of researchers interested in manipulation research. Besides supporting the learning of policies from interactions,  we also support learning-from-demonstrations (LfD) methods, by providing a large number of high-quality demonstrations (~36,000 successful trajectories, ~1.5M point cloud/RGB-D frames in total). We provide baselines using 3D deep learning and LfD algorithms. All code of our benchmark (simulator, environment, SDK, and baselines) is open-sourced (\\\\href{https://github.com/haosulab/ManiSkill}{Github repo}), and a challenge facing interdisciplinary researchers will be held based on the benchmark. ",Datasets & Benchmarks,NeurIPS,2021,Poster,Tongzhou Mu;Zhan Ling;Fanbo Xiang;Derek Cathera Yang;Xuanlin Li;Stone Tao;Zhiao Huang;Zhiwei Jia;Hao Su,True,https://openreview.net/pdf?id=zQIvkXHS_U5
zfMtM7HZGLT,Benchmarks for Corruption Invariant Person Re-identification,"When deploying person re-identification (ReID) model in safety-critical applications, it is pivotal to understanding the robustness of the model against a diverse array of image corruptions. However, current evaluations of person ReID only consider the performance on clean datasets and ignore images in various corrupted scenarios. In this work, we comprehensively establish five ReID benchmarks for learning corruption invariant representation. In the field of ReID, we are the first to conduct an exhaustive study on corruption invariant learning in single- and cross-modality datasets, including Market-1501, CUHK03, MSMT17, RegDB, SYSU-MM01. After reproducing and examining the robustness performance of 21 recent ReID methods, we have some observations:
 1) transformer-based models are more robust towards corrupted images, compared with CNN-based models,
 2) increasing the probability of random erasing (a commonly used augmentation method) hurts model corruption robustness,
 3) cross-dataset generalization improves with corruption robustness increases.
By analyzing the above observations, we propose a strong baseline on both single- and cross-modality ReID datasets which achieves improved robustness against diverse corruptions.
Our codes are available on https://github.com/MinghuiChen43/CIL-ReID.",Datasets & Benchmarks,NeurIPS,2021,Poster,Minghui Chen;Zhiqiang Wang;Feng Zheng,True,https://openreview.net/pdf?id=zfMtM7HZGLT
-VyJim9UBxQ,Understanding Aesthetics with Language: A Photo Critique Dataset for Aesthetic Assessment,"Computational inference of aesthetics is an ill-defined task due to its subjective nature. Many datasets have been proposed to tackle the problem by providing pairs of images and aesthetic scores based on human ratings. However, humans are better at expressing their opinion, taste, and emotions by means of language rather than summarizing them in a single number. In fact, photo critiques provide much richer information as they reveal how and why users rate the aesthetics of visual stimuli. In this regard, we propose the Reddit Photo Critique Dataset (RPCD), which contains tuples of image and photo critiques. RPCD consists of 74K images and 220K comments and is collected from a Reddit community used by hobbyists and professional photographers to improve their photography skills by leveraging constructive community feedback. The proposed dataset differs from previous aesthetics datasets mainly in three aspects, namely (i) the large scale of the dataset and the extension of the comments criticizing different aspects of the image, (ii) it contains mostly UltraHD images, and (iii) it can easily be extended to new data as it is collected through an automatic pipeline. To the best of our knowledge, in this work, we propose the first attempt to estimate the aesthetic quality of visual stimuli from the critiques. To this end, we exploit the polarity of the sentiment of criticism as an indicator of aesthetic judgment. We demonstrate how sentiment polarity correlates positively with the aesthetic judgment available for two aesthetic assessment benchmarks. Finally, we experiment with several models by using the sentiment scores as a target for ranking images. Dataset and baselines are available https://github.com/mediatechnologycenter/aestheval.",Datasets & Benchmarks,NeurIPS,2022,Poster,Daniel Vera Nieto;Luigi Celona;Clara Fernandez Labrador,True,https://openreview.net/pdf?id=-VyJim9UBxQ
-h6WAS6eE4,Locating and Editing Factual Associations in GPT,"We analyze the storage and recall of factual associations in autoregressive transformer language models, finding evidence that these associations correspond to localized, directly-editable computations. We first develop a causal intervention for identifying neuron activations that are decisive in a model's factual predictions. This reveals a distinct set of steps in middle-layer feed-forward modules that mediate factual predictions while processing subject tokens. To test our hypothesis that these computations correspond to factual association recall, we modify feed-forward weights to update specific factual associations using Rank-One Model Editing (ROME).  We find that ROME is effective on a standard zero-shot relation extraction (zsRE) model-editing task, comparable to existing methods. To perform a more sensitive evaluation, we also evaluate ROME on a new dataset of counterfactual assertions, on which it simultaneously maintains both specificity and generalization, whereas other methods sacrifice one or another. Our results confirm an important role for mid-layer feed-forward modules in storing factual associations and suggest that direct manipulation of computational mechanisms may be a feasible approach for model editing. The code, dataset, visualizations, and an interactive demo notebook are available in the supplemental materials.",main,NeurIPS,2022,Poster,Kevin Meng;David Bau;Alex J Andonian;Yonatan Belinkov,True,https://openreview.net/pdf?id=-h6WAS6eE4
-yiZR4_Xhh,Dance of SNN and ANN: Solving binding problem by combining spike timing and reconstructive attention,"The binding problem is one of the fundamental challenges that prevent the artificial neural network (ANNs) from a compositional understanding of the world like human perception, because disentangled and distributed representations of generative factors can interfere and lead to ambiguity when complex data with multiple objects are presented. In this paper, we propose a brain-inspired unsupervised hybrid neural network (HNN) that introduces temporal binding theory originated from neuroscience into ANNs by integrating spike timing dynamics (via spiking neural networks, SNNs) with reconstructive attention (by ANNs). Spike timing provides an additional dimension for grouping, while reconstructive feedback coordinates the spikes into temporal coherent states. Through iterative interaction of ANN and SNN, the model continuously binds multiple objects at alternative synchronous firing times in the SNN coding space. The effectiveness of the model is evaluated on five artificially generated datasets of binary images. By visualization and analysis, we demonstrate that the binding is explainable, soft, flexible, and hierarchical. Notably, the model is trained on single object datasets without explicit supervision on grouping, but can successfully bind multiple objects on test datasets, showing its compositional generalization capability. Further results show its binding ability in dynamic situations.",main,NeurIPS,2022,Poster,Hao Zheng;Hui Lin;Rong Zhao;Luping Shi,True,https://openreview.net/pdf?id=-yiZR4_Xhh
04OPxj0jGN_,AnimeRun: 2D Animation Visual Correspondence from Open Source 3D Movies,"Visual correspondence of 2D animation is the core of many applications and deserves careful study. Existing correspondence datasets for 2D cartoon suffer from simple frame composition and monotonic movements, making them  insufficient to simulate real animations. In this work, we present a new 2D animation visual correspondence dataset, AnimeRun, by converting open source 3D movies to full scenes in 2D style, including simultaneous moving background and interactions of multiple subjects. Statistics show that our proposed dataset not only resembles real anime more in image composition, but also possesses richer and more complex motion patterns compared to existing datasets. With this dataset, we establish a comprehensive benchmark by evaluating several existing optical flow and segment matching methods, and analyze shortcomings of these methods on animation data. Data are available at https://lisiyao21.github.io/projects/AnimeRun.",Datasets & Benchmarks,NeurIPS,2022,Poster,Li Siyao;Yuhang Li;Bo Li;Chao Dong;Ziwei Liu;Chen Change Loy,True,https://openreview.net/pdf?id=04OPxj0jGN_
08Yk-n5l2Al,Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding,"We present Imagen, a text-to-image diffusion model with an unprecedented degree of photorealism and a deep level of language understanding. Imagen builds on the power of large transformer language models in understanding text and hinges on the strength of diffusion models in high-fidelity image generation. Our key discovery is that generic large language models (e.g., T5), pretrained on text-only corpora, are surprisingly effective at encoding text for image synthesis: increasing the size of the language model in Imagen boosts both sample fidelity and image-text alignment much more than increasing the size of the image diffusion model. Imagen achieves a new state-of-the-art FID score of 7.27 on the COCO dataset, without ever training on COCO, and human raters find Imagen samples to be on par with the COCO data itself in image-text alignment. To assess text-to-image models in greater depth, we introduce DrawBench, a comprehensive and challenging benchmark for text-to-image models. With DrawBench, we compare Imagen with recent methods including VQ-GAN+CLIP, Latent Diffusion Models, and DALL-E 2, and find that human raters prefer Imagen over other models in side-by-side comparisons, both in terms of sample quality and image-text alignment.",main,NeurIPS,2022,Poster,Chitwan Saharia;William Chan;Saurabh Saxena;Lala Li;Jay Whang;Emily Denton;Seyed Kamyar Seyed Ghasemipour;Raphael Gontijo-Lopes;Burcu Karagol Ayan‎;Tim Salimans;Jonathan Ho;David J. Fleet;Mohammad Norouzi,True,https://openreview.net/pdf?id=08Yk-n5l2Al
0RTJcuvHtIu,Flexible Diffusion Modeling of Long Videos,We present a framework for video modeling based on denoising diffusion probabilistic models that produces long-duration video completions in a variety of realistic environments. We introduce a generative model that can at test-time sample any arbitrary subset of video frames conditioned on any other subset and present an architecture adapted for this purpose. Doing so allows us to efficiently compare and optimize a variety of schedules for the order in which frames in a long video are sampled and use selective sparse and long-range conditioning on previously sampled frames.  We demonstrate improved video modeling over prior work on a number of datasets and sample temporally coherent videos over 25 minutes in length.  We additionally release a new video modeling dataset and semantically meaningful metrics based on videos generated in the CARLA autonomous driving simulator.,main,NeurIPS,2022,Poster,William Harvey;Saeid Naderiparizi;Vaden Masrani;Christian Dietrich Weilbach;Frank Wood,True,https://openreview.net/pdf?id=0RTJcuvHtIu
0TDki1mlcwz,LASSIE: Learning Articulated Shapes from Sparse Image Ensemble via 3D Part Discovery,"Creating high-quality articulated 3D models of animals is challenging either via manual creation or using 3D scanning tools. 
Therefore, techniques to reconstruct articulated 3D objects from 2D images are crucial and highly useful. In this work, we propose a practical problem setting to estimate 3D pose and shape of animals given only a few (10-30) in-the-wild images of a particular animal species (say, horse). Contrary to existing works that rely on pre-defined template shapes, we do not assume any form of 2D or 3D ground-truth annotations, nor do we leverage any multi-view or temporal information. Moreover, each input image ensemble can contain animal instances with varying poses, backgrounds, illuminations, and textures. Our key insight is that 3D parts have much simpler shape compared to the overall animal and that they are robust w.r.t. animal pose articulations. Following these insights, we propose LASSIE, a novel optimization framework which discovers 3D parts in a self-supervised manner with minimal user intervention. A key driving force behind LASSIE is the enforcing of 2D-3D part consistency using self-supervisory deep features. Experiments on Pascal-Part and self-collected in-the-wild animal datasets demonstrate considerably better 3D reconstructions as well as both 2D and 3D part discovery compared to prior arts. Project page: https://chhankyao.github.io/lassie/",main,NeurIPS,2022,Poster,Chun-Han Yao;Wei-Chih Hung;Yuanzhen Li;Michael Rubinstein;Ming-Hsuan Yang;Varun Jampani,True,https://openreview.net/pdf?id=0TDki1mlcwz
0cn6LSqwjUv,RainNet: A Large-Scale Imagery Dataset and Benchmark for Spatial Precipitation Downscaling,"AI-for-science approaches have been applied to solve scientific problems (e.g., nuclear fusion, ecology, genomics, meteorology) and have achieved highly promising results. Spatial precipitation downscaling is one of the most important meteorological problem and urgently requires the participation of AI. However, the lack of a well-organized and annotated large-scale dataset hinders the training and verification of more effective and advancing deep-learning models for precipitation downscaling. To alleviate these obstacles, we present the first large-scale spatial precipitation downscaling dataset named RainNet, which contains more than 62,400 pairs of high-quality low/high-resolution precipitation maps for over 17 years, ready to help the evolution of deep learning models in precipitation downscaling. Specifically, the precipitation maps carefully collected in RainNet cover various meteorological phenomena (e.g., hurricane, squall), which is of great help to improve the model generalization ability. In addition, the map pairs in RainNet are organized in the form of image sequences (720 maps per month or 1 map/hour), showing complex physical properties, e.g., temporal misalignment, temporal sparse, and fluid properties. Furthermore, two deep-learning-oriented metrics are specifically introduced to evaluate or verify the comprehensive performance of the trained model (e.g., prediction maps reconstruction accuracy). To illustrate the applications of RainNet, 14 state-of-the-art models, including deep models and traditional approaches, are evaluated. To fully explore potential downscaling solutions, we propose an implicit physical estimation benchmark framework to learn the above characteristics. Extensive experiments demonstrate the value of RainNet in training and evaluating downscaling models. Our dataset is available at https://neuralchen.github.io/RainNet/.",main,NeurIPS,2022,Poster,Xuanhong Chen;Kairui Feng;Naiyuan Liu;Bingbing Ni;Yifan Lu;Zhengyan Tong;Ziang Liu,True,https://openreview.net/pdf?id=0cn6LSqwjUv
10iA3OowAV3,Chartalist: Labeled Graph Datasets for UTXO and Account-based Blockchains,"Machine learning on blockchain graphs is an emerging field with many applications such as ransomware payment tracking, price manipulation analysis, and money laundering detection. However, analyzing blockchain data requires domain expertise and computational resources, which pose a significant barrier and hinder advancement in this field. 

We introduce Chartalist, the first comprehensive platform to methodically access and use machine learning across a large selection of blockchains to address this challenge. Chartalist contains ML-ready datasets from unspent transaction output (UTXO) (e.g., Bitcoin) and account-based blockchains (e.g., Ethereum). We envision that Chartalist can facilitate data modeling, analysis, and representation of blockchain data and attract a wider community of scientists to analyze blockchains. Chartalist is an open-science initiative at https://github.com/cakcora/Chartalist.",Datasets & Benchmarks,NeurIPS,2022,Poster,Kiarash Shamsi;Friedhelm Victor;Murat Kantarcioglu;Yulia Gel;Cuneyt Gurcan Akcora,True,https://openreview.net/pdf?id=10iA3OowAV3
1GVpwr2Tfdg,Towards Better Evaluation for Dynamic Link Prediction,"Despite the prevalence of recent success in learning from static graphs, learning from time-evolving graphs remains an open challenge. In this work, we design new, more stringent evaluation procedures for link prediction specific to dynamic graphs, which reflect real-world considerations, to better compare the strengths and weaknesses of methods. First, we create two visualization techniques to understand the reoccurring patterns of edges over time and show that many edges reoccur at later time steps. Based on this observation, we propose a pure memorization-based baseline called EdgeBank. EdgeBank achieves surprisingly strong performance across multiple settings which highlights that the negative edges used in the current evaluation are easy. To sample more challenging negative edges, we introduce two novel negative sampling strategies that improve robustness and better match real-world applications. Lastly, we introduce six new dynamic graph datasets from a diverse set of domains missing from current benchmarks, providing new challenges and opportunities for future research. Our code repository is accessible at https://github.com/fpour/DGB.git.",Datasets & Benchmarks,NeurIPS,2022,Poster,Farimah Poursafaei;Andy Huang;Kellin Pelrine;Reihaneh Rabbany,True,https://openreview.net/pdf?id=1GVpwr2Tfdg
1kIZiRelqFt,FLAIR: Federated Learning Annotated Image Repository,"Cross-device federated learning is an emerging machine learning (ML) paradigm where a large population of devices collectively train an ML model while the data remains on the devices.
This research field has a unique set of practical challenges, and to systematically make advances, new datasets curated to be compatible with this paradigm are needed.
Existing federated learning benchmarks in the image domain do not accurately capture the scale and heterogeneity of many real-world use cases. 
We introduce FLAIR, a challenging large-scale annotated image dataset for multi-label classification suitable for federated learning.
FLAIR has 429,078 images from  51,414  Flickr users and captures many of the intricacies typically encountered in federated learning, such as heterogeneous user data and a long-tailed label distribution.
We implement multiple baselines in different learning setups for different tasks on this dataset. 
We believe FLAIR can serve as a challenging benchmark for advancing the state-of-the art in federated learning.
Dataset access and the code for the benchmark are available at https://github.com/apple/ml-flair.
",Datasets & Benchmarks,NeurIPS,2022,Poster,Congzheng Song;Filip Granqvist;Kunal Talwar,True,https://openreview.net/pdf?id=1kIZiRelqFt
2FNnBhwJsHK,A Unified Framework for Deep Symbolic Regression,"The last few years have witnessed a surge in methods for symbolic regression, from advances in traditional evolutionary approaches to novel deep learning-based systems. Individual works typically focus on advancing the state-of-the-art for one particular class of solution strategies, and there have been few attempts to investigate the benefits of hybridizing or integrating multiple strategies. In this work, we identify five classes of symbolic regression solution strategies---recursive problem simplification, neural-guided search, large-scale pre-training, genetic programming, and linear models---and propose a strategy to hybridize them into a single modular, unified symbolic regression framework. Based on empirical evaluation using SRBench, a new community tool for benchmarking symbolic regression methods, our unified framework achieves state-of-the-art performance in its ability to (1) symbolically recover analytical expressions, (2) fit datasets with high accuracy, and (3) balance accuracy-complexity trade-offs, across 252 ground-truth and black-box benchmark problems, in both noiseless settings and across various noise levels. Finally, we provide practical use case-based guidance for constructing hybrid symbolic regression algorithms, supported by extensive, combinatorial ablation studies.",main,NeurIPS,2022,Poster,Mikel Landajuela;Chak Lee;Jiachen Yang;Ruben Glatt;Claudio P. Santiago;Ignacio Aravena;Terrell N. Mundhenk;Garrett Mulcahy;Brenden K. Petersen,True,https://openreview.net/pdf?id=2FNnBhwJsHK
2N8JzuiWZ25,OpenSRH: optimizing brain tumor surgery using intraoperative stimulated Raman histology,"Accurate intraoperative diagnosis is essential for providing safe and effective care during brain tumor surgery. Our standard-of-care diagnostic methods are time, resource, and labor intensive, which restricts access to optimal surgical treatments. To address these limitations, we propose an alternative workflow that combines stimulated Raman histology (SRH), a rapid optical imaging method, with deep learning-based automated interpretation of SRH images for intraoperative brain tumor diagnosis and real-time surgical decision support. Here, we present OpenSRH, the first public dataset of clinical SRH images from 300+ brain tumors patients and 1300+ unique whole slide optical images. OpenSRH contains data from the most common brain tumors diagnoses, full pathologic annotations, whole slide tumor segmentations, raw and processed optical imaging data for end-to-end model development and validation. We provide a framework for patch-based whole slide SRH classification and inference using weak (i.e. patient-level) diagnostic labels. Finally, we benchmark two computer vision tasks: multi-class histologic brain tumor classification and patch-based contrastive representation learning. We hope OpenSRH will facilitate the clinical translation of rapid optical imaging and real-time ML-based surgical decision support in order to improve the access, safety, and efficacy of cancer surgery in the era of precision medicine.",Datasets & Benchmarks,NeurIPS,2022,Poster,Cheng Jiang;Asadur Zaman Chowdury;Xinhai Hou;Akhil Kondepudi;Christian Freudiger;Kyle Stephen Conway;Sandra Camelo-Piragua;Daniel A Orringer;Honglak Lee;Todd Hollon,True,https://openreview.net/pdf?id=2N8JzuiWZ25
2QrFr_U782Z,A Comprehensive Study on Large-Scale Graph Training: Benchmarking and Rethinking,"Large-scale graph training is a notoriously challenging problem for graph neural networks (GNNs). Due to the nature of evolving graph structures into the training process, vanilla GNNs usually fail to scale up, limited by the GPU memory space. Up to now, though numerous scalable GNN architectures have been proposed, we still lack a comprehensive survey and fair benchmark of this reservoir to find the rationale for designing scalable GNNs. To this end, we first systematically formulate the representative methods of large-scale graph training into several branches and further establish a fair and consistent benchmark for them by a greedy hyperparameter searching. In addition, regarding efficiency, we theoretically evaluate the time and space complexity of various branches and empirically compare them w.r.t GPU memory usage, throughput, and convergence. Furthermore, We analyze the pros and cons for various branches of scalable GNNs and then present a new ensembling training manner, named EnGCN, to address the existing issues. Remarkably, our proposed method has achieved new state-of-the-art (SOTA) performance on large-scale datasets. Our code is available at https://github.com/VITA-Group/Large_Scale_GCN_Benchmarking.",Datasets & Benchmarks,NeurIPS,2022,Poster,Keyu Duan;Zirui Liu;Peihao Wang;Wenqing Zheng;Kaixiong Zhou;Tianlong Chen;Xia Hu;Zhangyang Wang,False,https://openreview.net/pdf?id=2QrFr_U782Z
2ptbv_JjYKA,pFL-Bench: A Comprehensive Benchmark for Personalized Federated Learning,"Personalized Federated Learning (pFL), which utilizes and deploys distinct local models, has gained increasing attention in recent years due to its success in handling the statistical heterogeneity of FL clients. However, standardized evaluation and systematical analysis of diverse pFL methods remain a challenge. Firstly, the highly varied datasets, FL simulation settings and pFL implementations prevent easy and fair comparisons of pFL methods. Secondly, the current pFL literature diverges in the adopted evaluation and ablation protocols. Finally, the effectiveness and robustness of pFL methods are under-explored in various practical scenarios, such as the generalization to new clients and the participation of resource-limited clients. To tackle these challenges, we propose the first comprehensive pFL benchmark, pFL-Bench, for facilitating rapid, reproducible, standardized and thorough pFL evaluation. The proposed benchmark contains more than 10 dataset variants in various application domains with a unified data partition and realistic heterogeneous settings; a modularized and easy-to-extend pFL codebase with more than 20 competitive pFL method implementations; and systematic evaluations under containerized environments in terms of generalization, fairness, system overhead, and convergence. We highlight the benefits and potential of state-of-the-art pFL methods and hope pFL-Bench enables further pFL research and broad applications that would otherwise be difficult owing to the absence of a dedicated benchmark. The code is released at https://github.com/alibaba/FederatedScope/tree/master/benchmark/pFL-Bench.",Datasets & Benchmarks,NeurIPS,2022,Poster,Daoyuan Chen;Dawei Gao;Weirui Kuang;Yaliang Li;Bolin Ding,True,https://openreview.net/pdf?id=2ptbv_JjYKA
2rQPxsmjKF,DGraph: A Large-Scale Financial Dataset for Graph Anomaly Detection,"Graph Anomaly Detection (GAD) has recently become a hot research spot due to its practicability and theoretical value. Since GAD emphasizes the application and the rarity of anomalous samples, enriching the varieties of its datasets is fundamental. Thus, this paper present DGraph, a real-world dynamic graph in the finance domain. DGraph overcomes many limitations of current GAD datasets. It contains about 3M nodes, 4M dynamic edges, and 1M ground-truth nodes. We provide a comprehensive observation of DGraph, revealing that anomalous nodes and normal nodes generally have different structures, neighbor distribution, and temporal dynamics. Moreover, it suggests that 2M background nodes are also essential for detecting fraudsters. Furthermore, we conduct extensive experiments on DGraph. Observation and experiments demonstrate that DGraph is propulsive to advance GAD research and enable in-depth exploration of anomalous nodes. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Xuanwen Huang;Yang Yang;Yang Wang;Chunping Wang;Zhisheng Zhang;Jiarong Xu;Lei Chen;Michalis Vazirgiannis,True,https://openreview.net/pdf?id=2rQPxsmjKF
31_U7n18gM7,BackdoorBench: A Comprehensive Benchmark of Backdoor Learning,"Backdoor learning is an emerging and vital topic for studying deep neural networks' vulnerability (DNNs). Many pioneering backdoor attack and defense methods are being proposed, successively or concurrently, in the status of a rapid arms race. However, we find that the evaluations of new methods are often unthorough to verify their claims and accurate performance, mainly due to the rapid development, diverse settings, and the difficulties of implementation and reproducibility.  Without thorough evaluations and comparisons, it is not easy to track the current progress and design the future development roadmap of the literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench. It consists of an extensible modular-based codebase (currently including implementations of 8 state-of-the-art (SOTA) attacks and 9 SOTA defense algorithms) and a standardized protocol of complete backdoor learning. We also provide comprehensive evaluations of every pair of 8 attacks against 9 defenses, with 5 poisoning ratios, based on 5 models and 4 datasets, thus 8,000 pairs of evaluations in total. We present abundant analysis from different perspectives about these 8,000 evaluations, studying the effects of different factors in backdoor learning.  All codes and evaluations of BackdoorBench are publicly available at https://backdoorbench.github.io.",Datasets & Benchmarks,NeurIPS,2022,Poster,Baoyuan Wu;Hongrui Chen;Mingda Zhang;Zihao Zhu;Shaokui Wei;Danni Yuan;Chao Shen,False,https://openreview.net/pdf?id=31_U7n18gM7
3AbigH4s-ml,CEBaB: Estimating the Causal Effects of Real-World Concepts on NLP Model Behavior,"The increasing size and complexity of modern ML systems has improved their predictive capabilities but made their behavior harder to explain. Many techniques for model explanation have been developed in response, but we lack clear criteria for assessing these techniques. In this paper, we cast model explanation as the causal inference problem of estimating causal effects of real-world concepts on the output behavior of ML models given actual input data. We introduce CEBaB, a new benchmark dataset for assessing concept-based explanation methods in Natural Language Processing (NLP). CEBaB consists of short restaurant reviews with human-generated counterfactual reviews in which an aspect (food, noise, ambiance, service) of the dining experience was modified. Original and counterfactual reviews are annotated with multiply-validated sentiment ratings at the aspect-level and review-level. The rich structure of CEBaB allows us to go beyond input features to study the effects of abstract, real-world concepts on model behavior. We use CEBaB to compare the quality of a range of concept-based explanation methods covering different assumptions and conceptions of the problem, and we seek to establish natural metrics for comparative assessments of these methods.",main,NeurIPS,2022,Poster,Eldar David Abraham;Karel D'Oosterlinck;Amir Feder;Yair Ori Gat;Atticus Geiger;Christopher Potts;Roi Reichart;Zhengxuan Wu,True,https://openreview.net/pdf?id=3AbigH4s-ml
3HCT3xfNm9r,Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset,"One concern with the rise of large language models lies with their potential for significant harm, particularly from pretraining on biased, obscene, copyrighted, and private information. Emerging ethical approaches have attempted to filter pretraining material, but such approaches have been ad hoc and failed to take context into account. We offer an approach to filtering grounded in law, which has directly addressed the tradeoffs in filtering material. First, we gather and make available the Pile of Law, a ~256GB (and growing) dataset of open-source English-language legal and administrative data, covering court opinions, contracts, administrative rules, and legislative records. Pretraining on the Pile of Law may help with legal tasks that have the promise to improve access to justice. Second, we distill the legal norms that governments have developed to constrain the inclusion of toxic or private content into actionable lessons for researchers and discuss how our dataset reflects these norms. Third, we show how the Pile of Law offers researchers the opportunity to learn such filtering rules directly from the data, providing an exciting new research direction in model-based processing.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Peter Henderson;Mark Simon Krass;Lucia Zheng;Neel Guha;Christopher D Manning;Dan Jurafsky;Daniel E. Ho,True,https://openreview.net/pdf?id=3HCT3xfNm9r
3lk54yE2tYJ,Geoclidean: Few-Shot Generalization in Euclidean Geometry,"Euclidean geometry is among the earliest forms of mathematical thinking. While the geometric primitives underlying its constructions, such as perfect lines and circles, do not often occur in the natural world, humans rarely struggle to perceive and reason with them. Will computer vision models trained on natural images show the same sensitivity to Euclidean geometry? Here we explore these questions by studying few-shot generalization in the universe of Euclidean geometry constructions. We introduce Geoclidean, a domain-specific language for Euclidean geometry, and use it to generate two datasets of geometric concept learning tasks for benchmarking generalization judgements of humans and machines. We find that humans are indeed sensitive to Euclidean geometry and generalize strongly from a few visual examples of a geometric concept. In contrast, low-level and high-level visual features from standard computer vision models pretrained on natural images do not support correct generalization. Thus Geoclidean represents a novel few-shot generalization benchmark for geometric concept learning, where the performance of humans and of AI models diverge. The Geoclidean framework and dataset are publicly available for download.
",Datasets & Benchmarks,NeurIPS,2022,Poster,Joy Hsu;Jiajun Wu;Noah Goodman,True,https://openreview.net/pdf?id=3lk54yE2tYJ
3vYkhJIty7E,Learning Optical Flow from Continuous Spike Streams,"Spike camera is an emerging bio-inspired vision sensor with ultra-high temporal resolution. It records scenes by accumulating photons and outputting continuous binary spike streams. Optical flow is a key task for spike cameras and their applications. A previous attempt has been made for spike-based optical flow. However, the previous work only focuses on motion between two moments, and it uses graphics-based data for training, whose generalization is limited. In this paper, we propose a tailored network,  Spike2Flow that extracts information from binary spikes with temporal-spatial representation based on the differential of spike firing time and spatial information aggregation. The network utilizes continuous motion clues through joint correlation decoding. Besides, a new dataset with real-world scenes is proposed for better generalization. Experimental results show that our approach achieves state-of-the-art performance on existing synthetic datasets and real data captured by spike cameras. The source code and dataset are available at \\\\url{https://github.com/ruizhao26/Spike2Flow}.",main,NeurIPS,2022,Poster,Rui Zhao;Ruiqin Xiong;Jing Zhao;Zhaofei Yu;Xiaopeng Fan;Tiejun Huang,True,https://openreview.net/pdf?id=3vYkhJIty7E
4-bV1bi74M,🏘️ ProcTHOR: Large-Scale Embodied AI Using Procedural Generation,"Massive datasets and high-capacity models have driven many recent advancements in computer vision and natural language understanding. This work presents a platform to enable similar success stories in Embodied AI. We propose ProcTHOR, a framework for procedural generation of Embodied AI environments. ProcTHOR enables us to sample arbitrarily large datasets of diverse, interactive, customizable, and performant virtual environments to train and evaluate embodied agents across navigation, interaction, and manipulation tasks. We demonstrate the power and potential of ProcTHOR via a sample of 10,000 generated houses and a simple neural model. Models trained using only RGB images on ProcTHOR, with no explicit mapping and no human task supervision produce state-of-the-art results across 6 embodied AI benchmarks for navigation, rearrangement, and arm manipulation, including the presently running Habitat 2022, AI2-THOR Rearrangement 2022, and RoboTHOR challenges. We also demonstrate strong 0-shot results on these benchmarks, via pre-training on ProcTHOR with no fine-tuning on the downstream benchmark, often beating previous state-of-the-art systems that access the downstream training data.",main,NeurIPS,2022,Poster,Matt Deitke;Eli VanderBilt;Alvaro Herrasti;Luca Weihs;Kiana Ehsani;Jordi Salvador;Winson Han;Eric Kolve;Aniruddha Kembhavi;Roozbeh Mottaghi,True,https://openreview.net/pdf?id=4-bV1bi74M
47qVX2pa-2,A new dataset for multilingual keyphrase generation,"  Keyphrases  are an important tool for efficiently dealing with the ever-increasing amount of information present on the internet. While there are many recent papers on English keyphrase generation, keyphrase generation for other languages remains vastly understudied, mostly due to the absence of datasets. To address this, we present a novel dataset called Papyrus, composed of 16427 pairs of abstracts and keyphrases. We release four versions of this dataset, corresponding to different subtasks. Papyrus-e considers only English keyphrases, Papyrus-f considers French keyphrases, Papyrus-m considers keyphrase generation in any language (mostly French and English), and Papyrus-a considers keyphrase generation in several languages. We train a state-of-the-art model on all four tasks and show that they lead to better results for non-English languages, with an average improvement of 14.2\\\\% on keyphrase extraction and 2.0\\\\% on generation. We also show an improvement of 0.4\\\\% on extraction and 0.7\\\\% on generation over English state-of-the-art results by concatenating Papyrus-e with the Kp20K training set.",Datasets & Benchmarks,NeurIPS,2022,Poster,Frédéric Piedboeuf;Philippe Langlais,True,https://openreview.net/pdf?id=47qVX2pa-2
4kjQZTNz-NH,AnimeSR: Learning Real-World Super-Resolution Models for Animation Videos,"This paper studies the problem of real-world video super-resolution (VSR) for animation videos, and reveals three key improvements for practical animation VSR. First, recent real-world super-resolution approaches typically rely on degradation simulation using basic operators without any learning capability, such as blur, noise, and compression. In this work, we propose to learn such basic operators from real low-quality animation videos, and incorporate the learned ones into the degradation generation pipeline. Such neural-network-based basic operators could help to better capture the distribution of real degradations. Second, a large-scale high-quality animation video dataset, AVC, is built to facilitate comprehensive training and evaluations for animation VSR. Third, we further investigate an efficient multi-scale network structure. It takes advantage of the efficiency of unidirectional recurrent networks and the effectiveness of sliding-window-based methods. Thanks to the above delicate designs, our method, AnimeSR, is capable of restoring real-world low-quality animation videos effectively and efficiently, achieving superior performance to previous state-of-the-art methods.",main,NeurIPS,2022,Poster,Yanze Wu;Xintao Wang;Gen Li;Ying Shan,True,https://openreview.net/pdf?id=4kjQZTNz-NH
4nAe0PS7D-l,PROSPECT: Labeled Tandem Mass Spectrometry Dataset for Machine Learning in Proteomics,"Proteomics is the interdisciplinary field focusing on the large-scale study of proteins. Proteins essentially organize and execute all functions within organisms. Today, the bottom-up analysis approach is the most commonly used workflow, where proteins are digested into peptides and subsequently analyzed using Tandem Mass Spectrometry (MS/MS). MS-based proteomics has transformed various fields in life sciences, such as drug discovery and biomarker identification. Today, proteomics is entering a phase where it is helpful for clinical decision-making. Computational methods are vital in turning large amounts of acquired raw MS data into information and, ultimately, knowledge. Deep learning has proved its success in multiple domains as a robust framework for supervised and unsupervised machine learning problems. In proteomics, scientists are increasingly leveraging the potential of deep learning to predict the properties of peptides based on their sequence to improve their confident identification. However, a reference dataset is missing, covering several proteomics tasks, enabling performance comparison, and evaluating reproducibility and generalization. Here, we present a large labeled proteomics dataset spanning several tasks in the domain to address this challenge. We focus on two common applications: peptide retention time and MS/MS spectrum prediction. We review existing methods and task formulations from a machine learning perspective and recommend suitable evaluation metrics and visualizations. With an accessible dataset, we aim to lower the entry barrier and enable faster development in machine learning for proteomics.",Datasets & Benchmarks,NeurIPS,2022,Poster,Omar Shouman;Wassim Gabriel;Victor-George Giurcoiu;Vitor Sternlicht;Mathias Wilhelm,True,https://openreview.net/pdf?id=4nAe0PS7D-l
4u252OfG-xh,A Greek Parliament Proceedings Dataset for Computational Linguistics and Political Analysis,"Large, diachronic datasets of political discourse are hard to come across, especially for resource-lean languages such as Greek. In this paper, we introduce a curated dataset of the Greek Parliament Proceedings that extends chronologically from 1989 up to 2020. It consists of more than 1 million speeches with extensive meta-data, extracted from 5,355 parliamentary sitting record files. We explain how it was constructed and the challenges that had to be overcome. The dataset can be used for both computational linguistics and political analysis---ideally, combining the two. We present such an application, showing (i) how the dataset can be used to study the change of word usage through time, (ii) between significant historical events and political parties, (iii) by evaluating and employing algorithms for detecting semantic shifts.",Datasets & Benchmarks,NeurIPS,2022,Poster,Konstantina Dritsa;Aikaterini Thoma;John Pavlopoulos;Panos Louridas,True,https://openreview.net/pdf?id=4u252OfG-xh
56109,[Re] Learning to count everything,"Scope of Reproducibility The core finding of the paper is a novel architecture FamNet for handling the few-shot counting task. We examine its implementation in the provided code on GitHub and compare it to the theory in the original paper. The authors also introduce a data set with 147 visual categories FSC-147, which we analyze. We try to reproduce the authors’ results on it and on CARPK data set. Additionally, we test FamNet on a category specific data set JHU-CROWD++. Furthermore, we try to reproduce the ground truth density maps, the code for which is not provided by the authors.
Methodology We use the combination of the authors’ and our own code, for parts where the code is not provided (e.g., generating ground truth density maps, CARPK data set preprocessing). We also modify some parts of the authors’ code so that we can evaluate the model on various data sets. For running the code we used the Quadro RTX 5000 GPU and had a total computation time of approximately 50 GPU hours.
Results We could not reproduce the density maps, but we produced similar density maps by modifying some of the parameters. We exactly reproduced the results on the paper’s data set. We did not get the same results on the CARPK data set and in experiments where implementation details were not provided. However, the differences are within standard error and our results support the claim that the model outperforms the baselines.
What was easy Running the pretrained models and the demo app was quite easy, as the authors provided instructions. It was also easy to reproduce the results on a given data set with a pretrained model.
What was difficult It was difficult to verify the ground truth density map generation as the code was not provided and the process was incorrectly described. Obtaining a performant GPU was also quite a challenge and it took quite many emails to finally get one. This also meant that we were unable to reproduce the training of the model.
Communication with original authors We contacted the authors three times through issues on GitHub. They were helpful and responsive, but we have not resolved all of the issues.",Journal,NeurIPS,2022,Journal,"Maša Kljun, Matija Teršek, Domen Vreš",True,https://openreview.net/pdf?id=56109
56gbQGXlnYv,Ontologue: Declarative Benchmark Construction for Ontological Multi-Label Classification,"We describe a customizable benchmark for hierarchical and ontological multi-label classification, a task where labels are equipped with a graph structure and data items can be assigned multiple labels.  We find that current benchmarks do not adequately represent the problem space, casting doubt on the generalizability of current results. We consider three dimensions of the problem space: context (availability of rich features on the data and labels), distribution of labels over data, and graph structure. For context, the lack of complex features on the labels (and in some cases, the data) artificially prevent the use of modern representation learning techniques as an appropriate baseline.  For distribution, we find the long tail of labels over data constitute a few-shot learning problem that artificially confounds the results: for most common benchmarks, over 40% of the labels have fewer than 5 data points in the training set.  For structure, we find that the correlation between performance and the height of the tree can explain some of the variation in performance, informing practical utility. In this paper, we demonstrate how the lack of diversity in benchmarks can confound performance analysis, then present a declarative query system called Ontologue for generating custom benchmarks with specific properties, then use this system to design 4 new benchmarks extracted from DBPedia that better represent the problem space. We evaluate state-of-the-art algorithms on both existing and new benchmarks and show that the performance conclusions can vary significantly depending on the dimensions we consider.  We intend the system and derived benchmarks to improve the analysis of generalizability for these problems.",Datasets & Benchmarks,NeurIPS,2022,Poster,Sean T. Yang;Bernease Herman;Bill Howe,True,https://openreview.net/pdf?id=56gbQGXlnYv
5Fg3XoHjQ4r,Towards Hard-pose Virtual Try-on via 3D-aware Global Correspondence Learning,"In this paper, we target image-based person-to-person virtual try-on in the presence of diverse poses and large viewpoint variations. Existing methods are restricted in this setting as they estimate garment warping flows mainly based on 2D poses and appearance, which omits the geometric prior of the 3D human body shape.
Moreover, current garment warping methods are confined to localized regions, which makes them ineffective in capturing long-range dependencies and results in inferior flows with artifacts.
To tackle these issues, we present 3D-aware global correspondences, which are reliable flows that jointly encode global semantic correlations, local deformations, and geometric priors of 3D human bodies. Particularly, given an image pair depicting the source and target person, (a) we first obtain their pose-aware and high-level representations via two encoders, and introduce a coarse-to-fine decoder with multiple refinement modules to predict the pixel-wise global correspondence. (b) 3D parametric human models inferred from images are incorporated as priors to regularize the correspondence refinement process so that our flows can be 3D-aware and better handle variations of pose and viewpoint. (c) Finally, an adversarial generator takes the garment warped by the 3D-aware flow, and the image of the target person as inputs, to synthesize the photo-realistic try-on result. Extensive experiments on public benchmarks and our selected HardPose test set demonstrate the superiority of our method against state-of-the-art try-on approaches.",main,NeurIPS,2022,Poster,Zaiyu Huang;Hanhui Li;Zhenyu Xie;Michael Kampffmeyer;qingling Cai;Xiaodan Liang,True,https://openreview.net/pdf?id=5Fg3XoHjQ4r
5mi-CkvEqj,TweetNERD - End to End Entity Linking Benchmark for Tweets,"Named Entity Recognition and Disambiguation (NERD) systems are foundational for information retrieval, question answering, event detection, and other natural language processing (NLP) applications. We introduce TweetNERD, a dataset of 340K+ Tweets across 2010-2021, for benchmarking NERD systems on Tweets. This is the largest and most temporally diverse open sourced dataset benchmark for NERD on Tweets and can be used to facilitate research in this area. We describe evaluation setup with TweetNERD for three NERD tasks: Named Entity Recognition (NER), Entity Linking with True Spans (EL), and End to End Entity Linking (End2End); and provide performance of existing publicly available methods on specific TweetNERD splits. TweetNERD is available at: https://doi.org/10.5281/zenodo.6617192 under Creative Commons Attribution 4.0 International (CC BY 4.0) license. Check out more details at https://github.com/twitter-research/TweetNERD.",Datasets & Benchmarks,NeurIPS,2022,Poster,Shubhanshu Mishra;Aman Saini;Raheleh Makki;Sneha Mehta;Aria Haghighi;Ali Mollahosseini,True,https://openreview.net/pdf?id=5mi-CkvEqj
5wNiiIDynDF,CGLB: Benchmark Tasks for Continual Graph Learning,"Continual learning on graph data, which aims to accommodate new tasks over newly emerged graph data while maintaining the model performance over existing tasks, is attracting increasing attention from the community. Unlike continual learning on Euclidean data ($\\\\textit{e.g.}$, images, texts, etc.) that has established benchmarks and unified experimental settings, benchmark tasks are rare for Continual Graph Learning (CGL). Moreover, due to the variety of graph data and its complex topological structures, existing works adopt different protocols to configure datasets and experimental settings. This creates a great obstacle to compare different techniques and thus hinders the development of CGL. To this end, we systematically study the task configurations in different application scenarios and develop a comprehensive Continual Graph Learning Benchmark (CGLB) curated from different public datasets. Specifically, CGLB contains both node-level and graph-level continual graph learning tasks under task-incremental (currently widely adopted) and class-incremental (more practical, challenging, yet underexplored) settings, as well as a toolkit for training, evaluating, and visualizing different CGL methods. Within CGLB, we also systematically explain the difference among these task configurations by comparing them to classical continual learning settings. Finally, we comprehensively compare state-of-the-art baselines on CGLB to investigate their effectiveness. Given CGLB and the developed toolkit, the barrier to exploring CGL has been greatly lowered and researchers can focus more on the model development without worrying about tedious work on pre-processing of datasets or encountering unseen pitfalls. The benchmark and the toolkit are available through https://github.com/QueuQ/CGLB.",Datasets & Benchmarks,NeurIPS,2022,Poster,Xikun ZHANG;Dongjin Song;Dacheng Tao,False,https://openreview.net/pdf?id=5wNiiIDynDF
5xuowSQ17vy,MTNeuro: A Benchmark for Evaluating Representations of Brain Structure Across Multiple Levels of Abstraction,"There are multiple scales of abstraction from which we can describe the same image, depending on whether we are focusing on fine-grained details or a more global attribute of the image. In brain mapping, learning to automatically parse images to build representations of both small-scale features (e.g., the presence of cells or blood vessels) and global properties of an image (e.g., which brain region the image comes from) is a crucial and open challenge. However, most existing datasets and benchmarks for neuroanatomy consider only a single downstream task at a time. To bridge this gap, we introduce a new dataset, annotations, and multiple downstream tasks that provide diverse ways to readout information about brain structure and architecture from the same image. Our multi-task neuroimaging benchmark (MTNeuro) is built on volumetric, micrometer-resolution X-ray microtomography images spanning a large thalamocortical section of mouse brain, encompassing multiple cortical and subcortical regions. We generated a number of different prediction challenges and evaluated several supervised and self-supervised models for brain-region prediction and pixel-level semantic segmentation of microstructures. Our experiments not only highlight the rich heterogeneity of this dataset, but also provide insights into how self-supervised approaches can be used to learn representations that capture multiple attributes of a single image and perform well on a variety of downstream tasks. Datasets, code, and pre-trained baseline models are provided at: https://mtneuro.github.io/.",Datasets & Benchmarks,NeurIPS,2022,Poster,Jorge Quesada;Lakshmi Sathidevi;Ran Liu;Nauman Ahad;Joy M Jackson;Mehdi Azabou;Jingyun Xiao;Chris Liding;Matthew Jin;Carolina Urzay;William Gray-Roncal;Erik Christopher Johnson;Eva L Dyer,True,https://openreview.net/pdf?id=5xuowSQ17vy
6Hl7XoPNAVX,Ambiguous Images With Human Judgments for Robust Visual Event Classification,"Contemporary vision benchmarks predominantly consider tasks on which humans can achieve near-perfect performance. However, humans are frequently presented with visual data that they cannot classify with 100% certainty, and models trained on standard vision benchmarks achieve low performance when evaluated on this data. To address this issue, we introduce a procedure for creating datasets of ambiguous images and use it to produce SQUID-E (""Squidy""), a collection of noisy images extracted from videos. All images are annotated with ground truth values and a test set is annotated with human uncertainty judgments. We use this dataset to characterize human uncertainty in vision tasks and evaluate existing visual event classification models. Experimental results suggest that existing vision models are not sufficiently equipped to provide meaningful outputs for ambiguous images and that datasets of this nature can be used to assess and improve such models through model training and direct evaluation of model calibration. These findings motivate large-scale ambiguous dataset creation and further research focusing on noisy visual data.",Datasets & Benchmarks,NeurIPS,2022,Poster,Kate Sanders;Reno Kriz;Anqi Liu;Benjamin Van Durme,True,https://openreview.net/pdf?id=6Hl7XoPNAVX
6RoAxmwj0L2,DaDA: Distortion-aware Domain Adaptation for Unsupervised Semantic Segmentation,"Distributional shifts in photometry and texture have been extensively studied for unsupervised domain adaptation, but their counterparts in optical distortion have been largely neglected. In this work, we tackle the task of unsupervised domain adaptation for semantic image segmentation where unknown optical distortion exists between source and target images. To this end, we propose a distortion-aware domain adaptation (DaDA) framework that boosts the unsupervised segmentation performance. We first present a relative distortion learning (RDL) approach that is capable of modeling domain shifts in fine-grained geometric deformation based on diffeomorphic transformation. Then, we demonstrate that applying additional global affine transformations to the diffeomorphically transformed source images can further improve the segmentation adaptation. Besides, we find that our distortion-aware adaptation method helps to enhance self-supervised learning by providing higher-quality initial models and pseudo labels. To evaluate, we propose new distortion adaptation benchmarks, where rectilinear source images and fisheye target images are used for unsupervised domain adaptation. Extensive experimental results highlight the effectiveness of our approach over state-of-the-art methods under unknown relative distortion across domains. Datasets and more information are available at https://sait-fdd.github.io/.",main,NeurIPS,2022,Highlighted,Sujin Jang;Joohan Na;Dokwan Oh,True,https://openreview.net/pdf?id=6RoAxmwj0L2
7-LTDcvNc_,Analyzing Data-Centric Properties for Graph Contrastive Learning,"Recent analyses of self-supervised learning (SSL) find the following data-centric properties to be critical for learning good representations: invariance to task-irrelevant semantics, separability of classes in some latent space, and recoverability of labels from augmented samples. However, given their discrete, non-Euclidean nature, graph datasets and graph SSL methods are unlikely to satisfy these properties. This raises the question: how do graph SSL methods, such as contrastive learning (CL), work well? To systematically probe this question, we perform a generalization analysis for CL when using generic graph augmentations (GGAs), with a focus on data-centric properties. Our analysis yields formal insights into the limitations of GGAs and the necessity of task-relevant augmentations. As we empirically show, GGAs do not induce task-relevant invariances on common benchmark datasets, leading to only marginal gains over naive, untrained baselines. Our theory motivates a synthetic data generation process that enables control over task-relevant information and boasts pre-defined optimal augmentations. This flexible benchmark helps us identify yet unrecognized limitations in advanced augmentation techniques (e.g., automated methods). Overall, our work rigorously contextualizes, both empirically and theoretically, the effects of data-centric properties on augmentation strategies and learning paradigms for graph SSL. ",main,NeurIPS,2022,Poster,Puja Trivedi;Ekdeep Singh Lubana;Mark Heimann;Danai Koutra;Jayaraman J. Thiagarajan,True,https://openreview.net/pdf?id=7-LTDcvNc_
70_Wx-dON3q,Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification,"We introduce Meta-Album, an image classification meta-dataset designed to facilitate few-shot learning, transfer learning, meta-learning, among other tasks. It includes 40 open datasets, each having at least 20 classes with 40 examples per class, with verified licences. They stem from diverse domains, such as ecology (fauna and flora), manufacturing (textures, vehicles), human actions, and optical character recognition, featuring various image scales (microscopic, human scales, remote sensing). All datasets are preprocessed, annotated, and formatted uniformly, and come in 3 versions (Micro $\\\\subset$ Mini $\\\\subset$ Extended) to match users’ computational resources. We showcase the utility of the first 30 datasets on few-shot learning problems. The other 10 will be released shortly after. Meta-Album is already more diverse and larger (in number of datasets) than similar efforts, and we are committed to keep enlarging it via a series of competitions. As competitions terminate, their test data are released, thus creating a rolling benchmark, available through OpenML.org. Our website https://meta-album.github.io/ contains the source code of challenge winning methods, baseline methods, data loaders, and instructions for contributing either new datasets or algorithms to our expandable meta-dataset.",Datasets & Benchmarks,NeurIPS,2022,Poster,Ihsan Ullah;Dustin Carrión-Ojeda;Sergio Escalera;Isabelle M Guyon;Mike Huisman;Felix Mohr;Jan N. van Rijn;Haozhe Sun;Joaquin Vanschoren;Phan Anh Vu,True,https://openreview.net/pdf?id=70_Wx-dON3q
76w7bsdViZf,Hard ImageNet: Segmentations for Objects with Strong Spurious Cues,"Deep classifiers are known to rely on spurious features, leading to reduced generalization. The severity of this problem varies significantly by class. We identify $15$ classes in ImageNet with very strong spurious cues, and collect segmentation masks for these challenging objects to form \\\\emph{Hard ImageNet}. Leveraging noise, saliency, and ablation based metrics, we demonstrate that models rely on spurious features in Hard ImageNet far more than in RIVAL10, an ImageNet analog to CIFAR10. We observe Hard ImageNet objects are less centered and occupy much less space in their images than RIVAL10 objects, leading to greater spurious feature reliance. Further, we use robust neural features to automatically rank our images based on the degree of spurious cues present. Comparing images with high and low rankings within a class reveals the exact spurious features models rely upon, and shows reduced performance when spurious features are absent. With Hard ImageNet's image rankings, object segmentations, and our extensive evaluation suite, the community can begin to address the problem of learning to detect challenging objects \\\\emph{for the right reasons}, despite the presence of strong spurious cues.  ",Datasets & Benchmarks,NeurIPS,2022,Poster,Mazda Moayeri;Sahil Singla;Soheil Feizi,True,https://openreview.net/pdf?id=76w7bsdViZf
7e6W6LEOBg3,Honor of Kings Arena: an Environment for Generalization in Competitive Reinforcement Learning,"This paper introduces Honor of Kings Arena, a reinforcement learning (RL) environment based on the Honor of Kings, one of the world’s most popular games at present. Compared to other environments studied in most previous work, ours presents new generalization challenges for competitive reinforcement learning. It is a multi-agent problem with one agent competing against its opponent; and it requires the generalization ability as it has diverse targets to control and diverse opponents to compete with. We describe the observation, action, and reward specifications for the Honor of Kings domain and provide an open-source Python-based interface for communicating with the game engine. We provide twenty target heroes with a variety of tasks in Honor of Kings Arena and present initial baseline results for RL-based methods with feasible computing resources.  Finally, we showcase the generalization challenges imposed by Honor of Kings Arena and possible remedies to the challenges. All of the software, including the environment-class, are publicly available.",Datasets & Benchmarks,NeurIPS,2022,Poster,Hua Wei;Jingxiao Chen;Xiyang Ji;Hongyang Qin;Minwen Deng;Siqin Li;Liang Wang;Weinan Zhang;Yong Yu;Liu Linc;Lanxiao Huang;Deheng Ye;QIANG FU;Yang Wei,False,https://openreview.net/pdf?id=7e6W6LEOBg3
7w-a8PYPlP,OpenFWI: Large-scale Multi-structural Benchmark Datasets for Full Waveform Inversion,"Full waveform inversion (FWI) is widely used in geophysics to reconstruct high-resolution velocity maps from seismic data. The recent success of data-driven FWI methods results in a rapidly increasing demand for open datasets to serve the geophysics community. We present OpenFWI, a collection of large-scale multi-structural benchmark datasets, to facilitate diversified, rigorous, and reproducible research on FWI. In particular, OpenFWI consists of $12$ datasets ($2.1$TB in total) synthesized from multiple sources. It encompasses diverse domains in geophysics (interface, fault, CO$_2$ reservoir, etc.), covers different geological subsurface structures (flat, curve, etc.), and contain various amounts of data samples (2K - 67K). It also includes a dataset for 3D FWI. Moreover, we use OpenFWI to perform benchmarking over four deep learning methods, covering both supervised and unsupervised learning regimes. Along with the benchmarks, we implement additional experiments, including physics-driven methods, complexity analysis, generalization study, uncertainty quantification, and so on, to sharpen our understanding of datasets and methods. The studies either provide valuable insights into the datasets and the performance, or uncover their current limitations. We hope OpenFWI supports prospective research on FWI and inspires future open-source efforts on AI for science. All datasets and related information can be accessed through our website at https://openfwi-lanl.github.io/",Datasets & Benchmarks,NeurIPS,2022,Poster,Chengyuan Deng;Shihang Feng;Hanchen Wang;Xitong Zhang;Peng Jin;Yinan Feng;Qili Zeng;Yinpeng Chen;Youzuo Lin,True,https://openreview.net/pdf?id=7w-a8PYPlP
8RKJj1YDBJT,Neural Surface Reconstruction of Dynamic Scenes with Monocular RGB-D Camera,"We propose Neural-DynamicReconstruction (NDR), a template-free method to recover high-fidelity geometry and motions of a dynamic scene from a monocular RGB-D camera. In NDR, we adopt the neural implicit function for surface representation and rendering such that the captured color and depth can be fully utilized to jointly optimize the surface and deformations. To represent and constrain the non-rigid deformations, we propose a novel neural invertible deforming network such that the cycle consistency between arbitrary two frames is automatically satisfied. Considering that the surface topology of dynamic scene might change over time, we employ a topology-aware strategy to construct the topology-variant correspondence for the fused frames. NDR also further refines the camera poses in a global optimization manner. Experiments on public datasets and our collected dataset demonstrate that NDR outperforms existing monocular dynamic reconstruction methods.",main,NeurIPS,2022,Poster,Hongrui Cai;Wanquan Feng;Xuetao Feng;Yan Wang;Juyong Zhang,True,https://openreview.net/pdf?id=8RKJj1YDBJT
8hHg-zs_p-h,GOOD: A Graph Out-of-Distribution Benchmark,"Out-of-distribution (OOD) learning deals with scenarios in which training and test data follow different distributions. Although general OOD problems have been intensively studied in machine learning, graph OOD is only an emerging area of research. Currently, there lacks a systematic benchmark tailored to graph OOD method evaluation. In this work, we aim at developing an OOD benchmark, known as GOOD, for graphs specifically. We explicitly make distinctions between covariate and concept shifts and design data splits that accurately reflect different shifts. We consider both graph and node prediction tasks as there are key differences in designing shifts. Overall, GOOD contains 11 datasets with 17 domain selections. When combined with covariate, concept, and no shifts, we obtain 51 different splits. We provide performance results on 10 commonly used baseline methods with 10 random runs. This results in 510 dataset-model combinations in total. Our results show significant performance gaps between in-distribution and OOD settings. Our results also shed light on different performance trends between covariate and concept shifts by different methods. Our GOOD benchmark is a growing project and expects to expand in both quantity and variety of resources as the area develops. The GOOD benchmark can be accessed via https://github.com/divelab/GOOD/.",Datasets & Benchmarks,NeurIPS,2022,Poster,Shurui Gui;Xiner Li;Limei Wang;Shuiwang Ji,True,https://openreview.net/pdf?id=8hHg-zs_p-h
8lQDn9zTQlW,BigBio: A Framework for Data-Centric Biomedical Natural Language Processing,"Training and evaluating language models increasingly requires the construction of meta-datasets -- diverse collections of curated data with clear provenance. Natural language prompting has recently lead to improved zero-shot generalization by transforming existing, supervised datasets into a variety of novel instruction tuning tasks, highlighting the benefits of meta-dataset curation. While successful in general-domain text, translating these data-centric approaches to biomedical language modeling remains challenging, as labeled biomedical datasets are significantly underrepresented in popular data hubs. To address this challenge, we introduce BigBio a community library of 126+ biomedical NLP datasets, currently covering 13 task categories and 10+ languages. BigBio facilitates reproducible meta-dataset curation via programmatic access to datasets and their metadata, and is compatible with current platforms for prompt engineering and end-to-end few/zero shot language model evaluation. We discuss our process for task schema harmonization, data auditing, contribution guidelines, and outline two illustrative use cases: zero-shot evaluation of biomedical prompts and large-scale, multi-task learning. BigBio is an ongoing community effort and is available at https://github.com/bigscience-workshop/biomedical",Datasets & Benchmarks,NeurIPS,2022,Poster,Jason Alan Fries;Leon Weber;Natasha Seelam;Gabriel Altay;Debajyoti Datta;Samuele Garda;Myungsun Kang;Ruisi Su;Wojciech Kusa;Samuel Cahyawijaya;Fabio Barth;Simon Ott;Matthias Samwald;Stephen Bach;Stella Biderman;Mario Sänger;Bo Wang;Alison Callahan;Daniel León Periñán;Théo Gigant;Patrick Haller;Jenny Chim;Jose David Posada;John Michael Giorgi;Karthik Rangasai Sivaraman;Marc Pàmies;Marianna Nezhurina;Robert Martin;Michael Cullan;Moritz Freidank;Nathan Dahlberg;Shubhanshu Mishra;Shamik Bose;Nicholas Michio Broad;Yanis Labrak;Shlok S Deshmukh;Sid Kiblawi;Ayush Singh;Minh Chien Vu;Trishala Neeraj;Jonas Golde;Albert Villanova del Moral;Benjamin Beilharz,True,https://openreview.net/pdf?id=8lQDn9zTQlW
93cqcWFpTex,A Dataset for Efforts Towards Achieving the Sustainable Development Goal of Safe Working Environments,"Among United Nations' 17 Sustainable Development Goals (SDGs), we highlight SDG 8 on Decent Work and Economic Growth.  Specifically, we consider how to achieve subgoal 8.8, ""protect labour rights and promote safe working environments for all workers [...]"", in light of poor health, safety and environment (HSE) conditions being a widespread problem at workplaces. In EU alone, it is estimated that more than 4000 deaths occur each year due to poor working conditions. To handle the problem and achieve SDG 8, governmental agencies conduct labour inspections and it is therefore essential that these are carried out efficiently. Current research suggests that machine learning (ML) can be used to improve labour inspections, for instance by selecting organisations for inspections more effectively. However, the research in this area is very limited, in part due to a lack of publicly available data. Consequently, we introduce a new dataset called the Labour Inspection Checklists Dataset (LICD), which we have made publicly available. LICD consists of 63634 instances where each instance is an inspection conducted by the Norwegian Labour Inspection Authority. LICD has 577 features and labels. The dataset provides several ML research opportunities; we discuss two demonstration experiments. One experiment deals with the problem of selecting a relevant checklist for inspecting a given target organisation. The other experiment concerns the problem of predicting HSE violations, given a specific checklist and a target organisation. Our experimental results, while promising, suggest that achieving good ML classification performance is difficult for both problems. This motivates future research to improve ML performance, inspire other data analysis efforts, and ultimately achieve SDG 8.",Datasets & Benchmarks,NeurIPS,2022,Poster,Eirik Lund Flogard;Ole Jakob Mengshoel,True,https://openreview.net/pdf?id=93cqcWFpTex
9HBbWAsZxFt,Unsupervised Reinforcement Learning with Contrastive Intrinsic Control,"We introduce Contrastive Intrinsic Control (CIC), an unsupervised reinforcement learning (RL) algorithm that maximizes the mutual information between state-transitions and latent skill vectors. CIC utilizes contrastive learning between state-transitions and skills vectors to learn behaviour embeddings and maximizes the entropy of these embeddings as an intrinsic reward to encourage behavioural diversity. We evaluate our algorithm on the Unsupervised RL Benchmark (URLB) in the asymptotic state-based setting, which consists of a long reward-free pre-training phase followed by a short adaptation phase to downstream tasks with extrinsic rewards. We find that CIC improves over prior exploration algorithms in terms of adaptation efficiency to downstream tasks on state-based URLB.",main,NeurIPS,2022,Poster,Michael Laskin;Hao Liu;Xue Bin Peng;Denis Yarats;Aravind Rajeswaran;Pieter Abbeel,True,https://openreview.net/pdf?id=9HBbWAsZxFt
9K-8l0WgSK3,CEDe: A collection of expert-curated datasets with atom-level entity annotations for Optical Chemical Structure Recognition,"Optical Chemical Structure Recognition (OCSR) deals with the translation from chemical images to molecular structures, this being the main way chemical compounds are depicted in scientific documents. Traditionally, rule-based methods have followed a framework based on the detection of chemical entities, such as atoms and bonds, followed by a compound structure reconstruction step. Recently, neural architectures analog to image captioning have been explored to solve this task, yet they still show to be data inefficient, using millions of examples just to show performances comparable with traditional methods. Looking to motivate and benchmark new approaches based on atomic-level entities detection and graph reconstruction, we present CEDe, a unique collection of chemical entity bounding boxes manually curated by experts for scientific literature datasets. These annotations combine to more than 700,000 chemical entity bounding boxes with the necessary information for structure reconstruction. Also, a large synthetic dataset containing one million molecular images and annotations is released in order to explore transfer-learning techniques that could help these architectures perform better under low-data regimes. Benchmarks show that detection-reconstruction based models can achieve performances on par with or better than image captioning-like models, even with 100x fewer training examples.",Datasets & Benchmarks,NeurIPS,2022,Poster,Rodrigo Hormazabal;Changyoung Park;Soonyoung Lee;Sehui Han;Yeonsik Jo;Jaewan Lee;Ahra Jo;Seung Hwan Kim;Jaegul Choo;Moontae Lee;Honglak Lee,True,https://openreview.net/pdf?id=9K-8l0WgSK3
9T0Bnap5-j7,DeepFoids: Adaptive Bio-Inspired Fish Simulation with Deep Reinforcement Learning,"Our goal is to synthesize realistic underwater scenes with various fish species in different fish cages, which can be utilized to train computer vision models to automate fish counting and sizing tasks. It is a challenging problem to prepare a sufficiently diverse labeled dataset of images from aquatic environments. We solve this challenge by introducing an adaptive bio-inspired fish simulation. The behavior of caged fish changes based on the species, size and number of fish, and the size and shape of the cage, among other variables. However, a method to autonomously achieve schooling behavior for caged fish did not exist. In this paper, we propose a method for achieving schooling behavior for any given combination of variables, using multi-agent deep reinforcement learning (DRL) in various fish cages in arbitrary environments. Furthermore, to visually reproduce the underwater scene in different locations and seasons, we incorporate a physically-based underwater simulation.",main,NeurIPS,2022,Poster,Yuko Ishiwaka;Xiao Steven Zeng;Shun Ogawa;Donovan Michael Westwater;Tadayuki Tone;Masaki Nakada,True,https://openreview.net/pdf?id=9T0Bnap5-j7
9v1_6m0ZKC,360-MLC: Multi-view Layout Consistency for Self-training and Hyper-parameter Tuning,"We present 360-MLC, a self-training method based on multi-view layout consistency for finetuning monocular room-layout models using unlabeled 360-images only. This can be valuable in practical scenarios where a pre-trained model needs to be adapted to a new data domain without using any ground truth annotations. Our simple yet effective assumption is that multiple layout estimations in the same scene must define a consistent geometry regardless of their camera positions. Based on this idea, we leverage a pre-trained model to project estimated layout boundaries from several camera views into the 3D world coordinate. Then, we re-project them back to the spherical coordinate and build a probability function, from which we sample the pseudo-labels for self-training. To handle unconfident pseudo-labels, we evaluate the variance in the re-projected boundaries as an uncertainty value to weight each pseudo-label in our loss function during training. In addition, since ground truth annotations are not available during training nor in testing, we leverage the entropy information in multiple layout estimations as a quantitative metric to measure the geometry consistency of the scene, allowing us to evaluate any layout estimator for hyper-parameter tuning, including model selection without ground truth annotations. Experimental results show that our solution achieves favorable performance against state-of-the-art methods when self-training from three publicly available source datasets to a unique, newly labeled dataset consisting of multi-view images of the same scenes.",main,NeurIPS,2022,Poster,Bolivar Enrique Solarte;Chin-Hsuan Wu;Yueh-Cheng Liu;Yi-Hsuan Tsai;Min Sun,True,https://openreview.net/pdf?id=9v1_6m0ZKC
A6AFK_JwrIW,Learning Causally Invariant Representations for Out-of-Distribution Generalization on Graphs,"Despite recent success in using the invariance principle for out-of-distribution (OOD) generalization on Euclidean data (e.g., images), studies on graph data are still limited. Different from images, the complex nature of graphs poses unique challenges to adopting the invariance principle. In particular, distribution shifts on graphs can appear in a variety of forms such as attributes and structures, making it difficult to identify the invariance. Moreover, domain or environment partitions, which are often required by OOD methods on Euclidean data, could be highly expensive to obtain for graphs. To bridge this gap, we propose a new framework, called Causality Inspired Invariant Graph LeArning (CIGA), to capture the invariance of graphs for guaranteed OOD generalization under various distribution shifts. Specifically, we characterize potential distribution shifts on graphs with causal models, concluding that OOD generalization on graphs is achievable when models focus only on subgraphs containing the most information about the causes of labels. Accordingly, we propose an information-theoretic objective to extract the desired subgraphs that maximally preserve the invariant intra-class information. Learning with these subgraphs is immune to distribution shifts. Extensive experiments on 16 synthetic or real-world datasets, including a challenging setting -- DrugOOD, from AI-aided drug discovery, validate the superior OOD performance of CIGA.",main,NeurIPS,2022,Poster,Yongqiang Chen;Yonggang Zhang;Yatao Bian;Han Yang;MA KAILI;Binghui Xie;Tongliang Liu;Bo Han;James Cheng,True,https://openreview.net/pdf?id=A6AFK_JwrIW
A79jAS4MeW9,Robustness Analysis of Video-Language Models Against Visual and Language Perturbations,"Joint visual and language modeling on large-scale datasets has recently shown good progress in multi-modal tasks when compared to single modal learning. However, robustness of these  approaches against real-world perturbations has not been studied. In this work, we perform the first extensive robustness study of video-language models against various real-world perturbations. We focus on text-to-video retrieval and propose two large-scale benchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual and 35 different text perturbations. The study reveals some interesting initial findings from the studied models: 1) models are more robust when text is perturbed versus when video is perturbed, 2) models that are pre-trained are more robust than those trained from scratch, 3) models attend more to scene and objects rather than motion and action. We hope this study will serve as a benchmark and guide future research in robust video-language learning. The benchmark introduced in this study along with the code and datasets is available at https://bit.ly/3CNOly4.",Datasets & Benchmarks,NeurIPS,2022,Poster,Madeline Chantry Schiappa;Shruti Vyas;Hamid Palangi;Yogesh S Rawat;Vibhav Vineet,True,https://openreview.net/pdf?id=A79jAS4MeW9
AJzrFyqP0ci,Formalizing Consistency and Coherence of Representation Learning,"In the study of reasoning in neural networks, recent efforts have sought to improve consistency and coherence of sequence models, leading to important developments in the area of neuro-symbolic AI. In symbolic AI, the concepts of consistency and coherence can be defined and verified formally, but for neural networks these definitions are lacking. The provision of such formal definitions is crucial to offer a common basis for the quantitative evaluation and systematic comparison of connectionist, neuro-symbolic and transfer learning approaches. In this paper, we introduce formal definitions of consistency and coherence for neural systems. To illustrate the usefulness of our definitions, we propose a new dynamic relation-decoder model built around the principles of consistency and coherence. We compare our results with several existing relation-decoders using a partial transfer learning task based on a novel data set introduced in this paper. Our experiments show that relation-decoders that maintain consistency over unobserved regions of representation space retain
coherence across domains, whilst achieving better transfer learning performance.",main,NeurIPS,2022,Poster,Harald Stromfelt;Luke Dickens;Artur Garcez;Alessandra Russo,True,https://openreview.net/pdf?id=AJzrFyqP0ci
AXDNM76T1nc,Video PreTraining (VPT): Learning to Act by Watching Unlabeled Online Videos,"Pretraining on noisy, internet-scale datasets has been heavily studied as a technique for training models with broad, general capabilities for text, images, and other modalities. However, for many sequential decision domains such as robotics, video games, and computer use, publicly available data does not contain the labels required to train behavioral priors in the same way. We extend the internet-scale pretraining paradigm to sequential decision domains through semi-supervised imitation learning wherein agents learn to act by watching online unlabeled videos. Specifically, we show that with a small amount of labeled data we can train an inverse dynamics model accurate enough to label a huge unlabeled source of online data -- here, online videos of people playing Minecraft -- from which we can then train a general behavioral prior. Despite using the native human interface (mouse and keyboard at 20Hz), we show that this behavioral prior has nontrivial zero-shot capabilities and that it can be fine-tuned, with both imitation learning and reinforcement learning, to hard-exploration tasks that are impossible to learn from scratch via reinforcement learning. For many tasks our models exhibit human-level performance, and we are the first to report computer agents that can craft diamond tools, which can take proficient humans upwards of 20 minutes (24,000 environment actions) of gameplay to accomplish. ",main,NeurIPS,2022,Highlighted,Bowen Baker;Ilge Akkaya;Peter Zhokov;Joost Huizinga;Jie Tang;Adrien Ecoffet;Brandon Houghton;Raul Sampedro;Jeff Clune,True,https://openreview.net/pdf?id=AXDNM76T1nc
Aisi2oEq1sc,Finding Differences Between Transformers and ConvNets Using Counterfactual Simulation Testing,"Modern deep neural networks tend to be evaluated on static test sets. One shortcoming of this is the fact that these deep neural networks cannot be easily evaluated for robustness issues with respect to specific scene variations. For example, it is hard to study the robustness of these networks to variations of object scale, object pose, scene lighting and 3D occlusions. The main reason is that collecting real datasets with fine-grained naturalistic variations of sufficient scale can be extremely time-consuming and expensive. In this work, we present Counterfactual Simulation Testing, a counterfactual framework that allows us to study the robustness of neural networks with respect to some of these naturalistic variations by building realistic synthetic scenes that allow us to ask counterfactual questions to the models, ultimately providing answers to questions such as ""Would your classification still be correct if the object were viewed from the top?"" or ""Would your classification still be correct if the object were partially occluded by another object?"". Our method allows for a fair comparison of the robustness of recently released, state-of-the-art Convolutional Neural Networks and Vision Transformers, with respect to these naturalistic variations. We find evidence that ConvNext is more robust to pose and scale variations than Swin, that ConvNext generalizes better to our simulated domain and that Swin handles partial occlusion better than ConvNext. We also find that robustness for all networks improves with network scale and with data scale and variety. We release the Naturalistic Variation Object Dataset (NVD), a large simulated dataset of 272k images of everyday objects with naturalistic variations such as object pose, scale, viewpoint, lighting and occlusions. Project page: https://counterfactualsimulation.github.io",main,NeurIPS,2022,Poster,Nataniel Ruiz;Sarah Adel Bargal;Cihang Xie;Kate Saenko;Stan Sclaroff,True,https://openreview.net/pdf?id=Aisi2oEq1sc
B2W8Vy0rarw,EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records,"We present a new text-to-SQL dataset for electronic health records (EHRs). The utterances were collected from 222 hospital staff, including physicians, nurses, insurance review and health records teams, and more. To construct the QA dataset on structured EHR data, we conducted a poll at a university hospital and templatized the responses to create seed questions. Then, we manually linked them to two open-source EHR databases—MIMIC-III and eICU—and included them with various time expressions and held-out unanswerable questions in the dataset, which were all collected from the poll. Our dataset poses a unique set of challenges: the model needs to 1) generate SQL queries that reflect a wide range of needs in the hospital, including simple retrieval and complex operations such as calculating survival rate, 2) understand various time expressions to answer time-sensitive questions in healthcare, and 3) distinguish whether a given question is answerable or unanswerable based on the prediction confidence. We believe our dataset, EHRSQL, could serve as a practical benchmark to develop and assess QA models on structured EHR data and take one step further towards bridging the gap between text-to-SQL research and its real-life deployment in healthcare. EHRSQL is available at https://github.com/glee4810/EHRSQL.",Datasets & Benchmarks,NeurIPS,2022,Poster,Gyubok Lee;Hyeonji Hwang;Seongsu Bae;Yeonsu Kwon;Woncheol Shin;Seongjun Yang;Minjoon Seo;Jong-Yeup Kim;Edward Choi,True,https://openreview.net/pdf?id=B2W8Vy0rarw
BZ92dxDS3tO,OnePose++: Keypoint-Free One-Shot Object Pose Estimation without CAD Models,"We propose a new method for object pose estimation without CAD models. The previous feature-matching-based method OnePose has shown promising results under a one-shot setting which eliminates the need for CAD models or object-specific training. However, OnePose relies on detecting repeatable image keypoints and is thus prone to failure on low-textured objects. We propose a keypoint-free pose estimation pipeline to remove the need for repeatable keypoint detection. Built upon the detector-free feature matching method LoFTR, we devise a new keypoint-free SfM method to reconstruct a semi-dense point-cloud model for the object. Given a query image for object pose estimation, a 2D-3D matching network directly establishes 2D-3D correspondences between the query image and the reconstructed point-cloud model without first detecting keypoints in the image. Experiments show that the proposed pipeline outperforms existing one-shot CAD-model-free methods by a large margin and is comparable to CAD-model-based methods on LINEMOD even for low-textured objects. We also collect a new dataset composed of 80 sequences of 40 low-textured objects to facilitate future research on one-shot object pose estimation. The supplementary material, code and dataset are available on the project page: https://zju3dv.github.io/onepose_plus_plus/.",main,NeurIPS,2022,Poster,Xingyi He;Jiaming Sun;Yuang Wang;Di Huang;Hujun Bao;Xiaowei Zhou,True,https://openreview.net/pdf?id=BZ92dxDS3tO
BkMGK9dv2Z9,pyKT: A Python Library to Benchmark Deep Learning based Knowledge Tracing Models,"Knowledge tracing (KT) is the task of using students' historical learning interaction data to model their knowledge mastery over time so as to make predictions on their future interaction performance. Recently, remarkable progress has been made of using various deep learning techniques to solve the KT problem. However, the success behind deep learning based knowledge tracing (DLKT) approaches is still left somewhat unknown and proper measurement and analysis of these DLKT approaches remain a challenge. First, data preprocessing procedures in existing works are often private and custom, which limits experimental standardization. Furthermore, existing DLKT studies often differ in terms of the evaluation protocol and are far away real-world educational contexts. To address these problems, we introduce a comprehensive python based benchmark platform, \\\\textsc{pyKT}, to guarantee valid comparisons across DLKT methods via thorough evaluations. The \\\\textsc{pyKT} library consists of a standardized set of integrated data preprocessing procedures on 7 popular datasets across different domains, and 10 frequently compared DLKT model implementations for transparent experiments. Results from our fine-grained and rigorous empirical KT studies yield a set of observations and suggestions for effective DLKT, e.g., wrong evaluation setting may cause label leakage that generally leads to performance inflation; and the improvement of many DLKT approaches is minimal compared to the very first DLKT model proposed by Piech et al. \\\\cite{piech2015deep}. We have open sourced \\\\textsc{pyKT} and our experimental results at \\\\url{https://pykt.org/}. We welcome contributions from other research groups and practitioners.",Datasets & Benchmarks,NeurIPS,2022,Poster,Zitao Liu;Qiongqiong Liu;Jiahao Chen;Shuyan Huang;Jiliang Tang;Weiqi Luo,False,https://openreview.net/pdf?id=BkMGK9dv2Z9
Bs8iFQ7AM6,DC-BENCH: Dataset Condensation Benchmark,"Dataset Condensation is a newly emerging technique aiming at learning a tiny dataset that captures the rich information encoded in the original dataset. As the size of datasets contemporary machine learning models rely on becomes increasingly large, condensation methods become a prominent direction for accelerating network training and reducing data storage. Despite numerous methods have been proposed in this rapidly growing field, evaluating and comparing different condensation methods is non-trivial and still remains an open issue. 
The quality of condensed dataset are often shadowed by many critical contributing factors to the end performance, such as data augmentation and model architectures. The lack of a systematic way to evaluate and compare condensation methods not only hinders our understanding of existing techniques, but also discourages practical usage of the synthesized datasets. This work provides the first large-scale standardized benchmark on Dataset Condensation. It consists of a suite of evaluations to comprehensively reflect the generability and effectiveness of condensation methods through the lens of their generated dataset. Leveraging this benchmark, we conduct a large-scale study of current condensation methods, and report many insightful findings that open up new possibilities for future development. The benchmark library, including evaluators, baseline methods, and generated datasets, is open-sourced to facilitate future research and application.",Datasets & Benchmarks,NeurIPS,2022,Poster,Justin Cui;Ruochen Wang;Si Si;Cho-Jui Hsieh,True,https://openreview.net/pdf?id=Bs8iFQ7AM6
BubxnHpuMbG,EnvPool: A Highly Parallel Reinforcement Learning Environment Execution Engine,"There has been significant progress in developing reinforcement learning (RL) training systems. Past works such as IMPALA, Apex, Seed RL, Sample Factory, and others, aim to improve the system's overall throughput. In this paper, we aim to address a common bottleneck in the RL training system, i.e., parallel environment execution, which is often the slowest part of the whole system but receives little attention. With a curated design for paralleling RL environments, we have improved the RL environment simulation speed across different hardware setups, ranging from a laptop and a modest workstation, to a high-end machine such as NVIDIA DGX-A100. On a high-end machine, EnvPool achieves one million frames per second for the environment execution on Atari environments and three million frames per second on MuJoCo environments. When running EnvPool on a laptop, the speed is 2.8x that of the Python subprocess. Moreover, great compatibility with existing RL training libraries has been demonstrated in the open-sourced community, including CleanRL, rl_games, DeepMind Acme, etc. Finally, EnvPool allows researchers to iterate their ideas at a much faster pace and has great potential to become the de facto RL environment execution engine. Example runs show that it only takes five minutes to train agents to play Atari Pong and MuJoCo Ant on a laptop.  EnvPool is open-sourced at https://github.com/sail-sg/envpool.",Datasets & Benchmarks,NeurIPS,2022,Poster,Jiayi Weng;Min Lin;Shengyi Huang;Bo Liu;Denys Makoviichuk;Viktor Makoviychuk;Zichen Liu;Yufan Song;Ting Luo;Yukun Jiang;Zhongwen Xu;Shuicheng YAN,False,https://openreview.net/pdf?id=BubxnHpuMbG
CNJQKM5cV2o,"ConfLab: A Data Collection Concept, Dataset, and Benchmark for Machine Analysis of Free-Standing Social Interactions in the Wild","Recording the dynamics of unscripted human interactions in the wild is challenging due to the delicate trade-offs between several factors: participant privacy, ecological validity, data fidelity, and logistical overheads. To address these, following a 'datasets for the community by the community' ethos, we propose the Conference Living Lab (ConfLab): a new concept for multimodal multisensor data collection of in-the-wild free-standing social conversations. For the first instantiation of ConfLab described here, we organized a real-life professional networking event at a major international conference. Involving 48 conference attendees, the dataset captures a diverse mix of status, acquaintance, and networking motivations. Our capture setup improves upon the data fidelity of prior in-the-wild datasets while retaining privacy sensitivity: 8 videos (1920x1080, 60 fps) from a non-invasive overhead view, and custom wearable sensors with onboard recording of body motion (full 9-axis IMU), privacy-preserving low-frequency audio (1250 Hz), and Bluetooth-based proximity. Additionally, we developed custom solutions for distributed hardware synchronization at acquisition, and time-efficient continuous annotation of body keypoints and actions at high sampling rates. Our benchmarks showcase some of the open research tasks related to in-the-wild privacy-preserving social data analysis: keypoints detection from overhead camera views, skeleton-based no-audio speaker detection, and F-formation detection.",Datasets & Benchmarks,NeurIPS,2022,Poster,Chirag Raman;Jose Vargas Quiros;Stephanie Tan;Ashraful Islam;Ekin Gedik;Hayley Hung,True,https://openreview.net/pdf?id=CNJQKM5cV2o
CZAd_6uiUx0,"This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish","The availability of compute and data to train larger and larger language models increases the demand for robust methods of benchmarking the true progress of LM training. Recent years witnessed significant progress in standardized benchmarking for English. Benchmarks such as GLUE, SuperGLUE, or KILT have become a de facto standard tools to compare large language models. Following the trend to replicate GLUE for other languages, the KLEJ benchmark\\\\ (klej is the word for glue in Polish) has been released for Polish. In this paper, we evaluate the progress in benchmarking for low-resourced languages. We note that only a handful of languages have such comprehensive benchmarks. We also note the gap in the number of tasks being evaluated by benchmarks for resource-rich English/Chinese and the rest of the world.

In this paper, we introduce LEPISZCZE (lepiszcze is the Polish word for glew, the Middle English predecessor of glue), a new, comprehensive benchmark for Polish NLP with a large variety of tasks and high-quality operationalization of the benchmark.
We design LEPISZCZE with flexibility in mind. Including new models, datasets, and tasks is as simple as possible while still offering data versioning and model tracking. In the first run of the benchmark, we test 13 experiments (task and dataset pairs) based on the five most recent LMs for Polish. We use five datasets from the Polish benchmark and add eight novel datasets. As the paper's main contribution, apart from LEPISZCZE, we provide insights and experiences learned while creating the benchmark for Polish as the blueprint to design similar benchmarks for other low-resourced languages.",Datasets & Benchmarks,NeurIPS,2022,Poster,Lukasz Augustyniak;Kamil Tagowski;Albert Sawczyn;Denis Janiak;Roman Bartusiak;Adrian Dominik Szymczak;Arkadiusz Janz;Piotr Szymański;Marcin Wątroba;Mikołaj Morzy;Tomasz Jan Kajdanowicz;Maciej Piasecki,True,https://openreview.net/pdf?id=CZAd_6uiUx0
CZeIOfCjMf,HAPI: A Large-scale Longitudinal Dataset of Commercial ML API Predictions,"Commercial ML APIs offered by providers such as Google, Amazon and Microsoft have dramatically simplified ML adoptions in many applications. Numerous companies and academics pay to use ML APIs for tasks such as object detection, OCR and sentiment analysis. Different ML APIs tackling the same task can have very heterogeneous performances. Moreover, the ML models underlying the APIs also evolve over time. As ML APIs rapidly become a valuable marketplace and an integral part of analytics, it is critical to systematically study and compare different APIs with each other and to characterize how individual APIs change over time. However, this practically important topic is currently underexplored due to the lack of data. In this paper, we present HAPI (History of APIs), a longitudinal dataset of 1,761,417 instances of commercial ML API applications (involving APIs from Amazon, Google, IBM, Microsoft and other providers) across diverse tasks including image tagging, speech recognition, and text mining from 2020 to 2022. Each instance consists of a query input for an API (e.g., an image or text) along with the API’s output prediction/annotation and confidence scores. HAPI is the first large-scale dataset of ML API usages and is a unique resource for studying ML  as-a-service (MLaaS). As examples of the types of analyses that HAPI enables, we show that ML APIs’ performance changes substantially over time—several APIs’ accuracies dropped on specific benchmark datasets. Even when the API’s aggregate performance stays steady, its error modes can shift across different subtypes of data between 2020 and 2022. Such changes can substantially impact the entire analytics pipelines that use some ML API as a component. We further use HAPI to study commercial APIs’ performance disparities across demographic subgroups over time. HAPI can stimulate more research in the growing field of MLaaS.",Datasets & Benchmarks,NeurIPS,2022,Poster,Lingjiao Chen;Zhihua Jin;Sabri Eyuboglu;Christopher Re;Matei Zaharia;James Y. Zou,True,https://openreview.net/pdf?id=CZeIOfCjMf
ChWf1E43l4,DABS 2.0: Improved Datasets and Algorithms for Universal Self-Supervision,"Universal self-supervised (SSL) algorithms hold enormous promise for making machine learning accessible to high-impact domains such as protein biology, manufacturing, and genomics. We present DABS 2.0: a set of improved datasets and algorithms for advancing research on universal SSL. We extend the recently-introduced DABS benchmark with the addition of five real-world science and engineering domains: protein biology, bacterial genomics, multispectral satellite imagery, semiconductor wafers, and particle physics, bringing the total number of domains in the benchmark to twelve. We also propose a new universal SSL algorithm, Capri, and a generalized version of masked autoencoding, and apply both on all twelve domains---the most wide-ranging exploration of SSL yet. We find that multiple algorithms show gains across domains, outperforming previous baselines. In addition, we demonstrate the usefulness of DABS for scientific study of SSL by investigating the optimal corruption rate for each algorithm, showing that the best setting varies based on the domain. Code will be released at http://github.com/alextamkin/dabs}{http://github.com/alextamkin/dabs",Datasets & Benchmarks,NeurIPS,2022,Poster,Alex Tamkin;Gaurab Banerjee;Mohamed Owda;Vincent Liu;Shashank Rammoorthy;Noah Goodman,True,https://openreview.net/pdf?id=ChWf1E43l4
ChWo6qLgILf,SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning,"We introduce SoundSpaces 2.0, a platform for on-the-fly geometry-based audio rendering for 3D environments. Given a 3D mesh of a real-world environment, SoundSpaces can generate highly realistic acoustics for arbitrary sounds captured from arbitrary microphone locations. Together with existing 3D visual assets, it supports an array of audio-visual research tasks, such as audio-visual navigation, mapping, source localization and separation, and acoustic matching. Compared to existing resources, SoundSpaces 2.0 has the advantages of allowing continuous spatial sampling, generalization to novel environments, and configurable microphone and material properties. To our knowledge, this is the first geometry-based acoustic simulation that offers high fidelity and realism while also being fast enough to use for embodied learning. We showcase the simulator's properties and  benchmark its performance against real-world audio measurements. In addition, we demonstrate two downstream tasks---embodied navigation and far-field automatic speech recognition---and highlight sim2real performance for the latter. SoundSpaces 2.0 is publicly available to facilitate wider research for perceptual systems that can both see and hear.",Datasets & Benchmarks,NeurIPS,2022,Poster,Changan Chen;Carl Schissler;Sanchit Garg;Philip Kobernik;Alexander Clegg;Paul Calamia;Dhruv Batra;Philip W Robinson;Kristen Grauman,False,https://openreview.net/pdf?id=ChWo6qLgILf
D29JbExncTP,Towards Human-Level Bimanual Dexterous Manipulation with Reinforcement Learning,"Achieving human-level dexterity is an important open problem in robotics. However, tasks of dexterous hand manipulation even at the baby level are challenging to solve through reinforcement learning (RL). The difficulty lies in the high degrees of freedom and the required cooperation among heterogeneous agents (e.g., joints of fingers). In this study, we propose the Bimanual Dexterous Hands Benchmark (Bi-DexHands), a simulator that involves two dexterous hands with tens of bimanual manipulation tasks and thousands of target objects. Tasks in Bi-DexHands are first designed to match human-level motor skills according to literature in cognitive science, and then are built in Issac Gym; this enables highly efficient RL trainings, reaching 30,000+ FPS by only one single NVIDIA RTX 3090. We provide a comprehensive benchmark for popular RL algorithms under different settings; this includes multi-agent RL, offline RL, multi-task RL, and meta RL. Our results show that PPO type on-policy algorithms can learn to solve simple manipulation tasks that are equivalent up to 48-month human baby (e.g., catching a flying object, opening a bottle), while multi-agent RL can further help to learn manipulations that require skilled bimanual cooperation (e.g., lifting a pot, stacking blocks). Despite the success on each individual task, when it comes to mastering multiple manipulation skills, existing RL algorithms fail to work in most of the multi-task and the few-shot learning tasks, which calls for more future development from the RL community. Our project is open-sourced at https://github.com/PKU-MARL/DexterousHands.",Datasets & Benchmarks,NeurIPS,2022,Poster,Yuanpei Chen;Tianhao Wu;Shengjie Wang;Xidong Feng;Jiechuan Jiang;Zongqing Lu;Stephen Marcus McAleer;Hao Dong;Song-Chun Zhu;Yaodong Yang,False,https://openreview.net/pdf?id=D29JbExncTP
DEigo9L8xZA,Open High-Resolution Satellite Imagery: The WorldStrat Dataset – With Application to Super-Resolution,"Analyzing the planet at scale with satellite imagery and machine learning is a dream that has been constantly hindered by the cost of difficult-to-access highly-representative high-resolution imagery. To remediate this, we introduce here the  WorldStratified dataset. The largest and most varied such publicly available dataset, at Airbus SPOT 6/7 satellites' high resolution of up to 1.5 m/pixel, empowered by European Space Agency's Phi-Lab as part of the ESA-funded QueryPlanet project, we curate 10,000 sq km of unique locations to ensure stratified representation of all types of land-use across the world: from agriculture to ice caps, from forests to multiple urbanization densities. We also enrich those with locations typically under-represented in ML datasets: sites of humanitarian interest, illegal mining sites, and settlements of persons at risk. We temporally-match each high-resolution image with multiple low-resolution images from the freely accessible lower-resolution Sentinel-2 satellites at 10 m/pixel. 
We accompany this dataset with an open-source Python package to: rebuild or extend the WorldStrat dataset, train and infer baseline algorithms, and learn with abundant tutorials, all compatible with the popular EO-learn toolbox. 
We hereby hope to foster broad-spectrum applications of ML to satellite imagery, and possibly develop from free public low-resolution Sentinel2 imagery the same power of analysis allowed by costly private high-resolution imagery. We illustrate this specific point by training and releasing several highly compute-efficient baselines on the task of Multi-Frame Super-Resolution. 
License-wise, the high-resolution Airbus imagery is CC-BY-NC, while the labels, Sentinel2 imagery, and trained weights are under CC-BY, and the source code under BSD, to allow for the widest use and dissemination. The dataset is available at \\\\url{https://zenodo.org/record/6810792} and the software package at \\\\url{https://github.com/worldstrat/worldstrat}.",Datasets & Benchmarks,NeurIPS,2022,Poster,Julien Cornebise;Ivan Orsolic;Freddie Kalaitzis,True,https://openreview.net/pdf?id=DEigo9L8xZA
DcfsR89KUa,Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world,"We introduce \\\\textit{Nocturne}, a new 2D driving simulator for investigating multi-agent coordination under partial observability. The focus of Nocturne is to enable research into inference and theory of mind in real-world multi-agent settings without the computational overhead of computer vision and feature extraction from images. Agents in this simulator only observe an obstructed view of the scene, mimicking human visual sensing constraints. Unlike existing benchmarks that are bottlenecked by rendering human-like observations directly using a camera input, Nocturne uses efficient intersection methods to compute a vectorized set of visible features in a C++ back-end, allowing the simulator to run at $2000+$ steps-per-second. Using open-source trajectory and map data, we construct a simulator to load and replay arbitrary trajectories and scenes from real-world driving data. Using this environment, we benchmark reinforcement-learning and imitation-learning agents and demonstrate that the agents are quite far from human-level coordination ability and deviate significantly from the expert trajectories.",Datasets & Benchmarks,NeurIPS,2022,Poster,Eugene Vinitsky;Nathan Lichtlé;Xiaomeng Yang;Brandon Amos;Jakob Nicolaus Foerster,False,https://openreview.net/pdf?id=DcfsR89KUa
EONuSdDjJrp,MSDS: A Large-Scale Chinese Signature and Token Digit String Dataset for Handwriting Verification,"Although online handwriting verification has made great progress recently, the verification performances are still far behind the real usage owing to the small scale of the datasets as well as the limited biometric mediums. Therefore, this paper proposes a new handwriting verification benchmark dataset named Multimodal Signature and Digit String (MSDS), which consists of two subsets: MSDS-ChS (Chinese Signatures) and MSDS-TDS (Token Digit Strings), contributed by 402 users, with 20 genuine samples and 20 skilled forgeries per user per subset. MSDS-ChS consists of handwritten Chinese signatures, which, to the best of our knowledge, is the largest publicly available Chinese signature dataset for handwriting verification, at least eight times larger than existing online datasets. Meanwhile, MSDS-TDS consists of handwritten Token Digit Strings, i.e, the actual phone numbers of users, which have not been explored yet. Extensive experiments with different baselines are respectively conducted for MSDS-ChS and MSDS-TDS. Surprisingly, verification performances of state-of-the-art methods on MSDS-TDS are generally better than those on MSDS-ChS, which indicates that the handwritten Token Digit String could be a more effective biometric than handwritten Chinese signature. This is a promising discovery that could inspire us to explore new biometric traits. The MSDS dataset is available at https://github.com/HCIILAB/MSDS.",Datasets & Benchmarks,NeurIPS,2022,Poster,Peirong Zhang;Jiajia Jiang;Yuliang Liu;Lianwen Jin,True,https://openreview.net/pdf?id=EONuSdDjJrp
EZcHYuU_9E,A Large Scale Search Dataset for Unbiased Learning to Rank,"The unbiased learning to rank (ULTR) problem has been greatly advanced by recent deep learning techniques and well-designed debias algorithms. However, promising results on the existing benchmark datasets may not be extended to the practical scenario due to some limitations of existing datasets. First, their semantic feature extractions are outdated while state-of-the-art large-scale pre-trained language models like BERT cannot be utilized due to the lack of original text. Second, display features are incomplete; thus in-depth study on ULTR is impossible such as the displayed abstract for analyzing the click necessary bias. Third, synthetic user feedback has been adopted by most existing datasets and real-world user feedback is greatly missing. To overcome these disadvantages, we introduce the Baidu-ULTR dataset. It involves randomly sampled 1.2 billion searching sessions and 7,008 expert annotated queries(397,572 query document pairs). Baidu-ULTR is the first billion-level dataset for ULTR. Particularly, it offers: (1)the original semantic features and pre-trained language models of different sizes; (2)sufficient display information such as position, displayed height, and displayed abstract, enabling the comprehensive study of multiple displayed biases; and (3)rich user feedback on search result pages (SERPs) like dwelling time, allowing for user engagement optimization and promoting the exploration of multi-task learning in ULTR. Furthermore, we present the design principle of Baidu-ULTR and the performance of representative ULTR algorithms on Baidu-ULTR. The Baidu-ULTR dataset and corresponding baseline implementations are available at https://github.com/ChuXiaokai/baidu_ultr_dataset. The dataset homepage is available at https://searchscience.baidu.com/dataset.html.

",Datasets & Benchmarks,NeurIPS,2022,Poster,Lixin Zou;Haitao Mao;Xiaokai Chu;Jiliang Tang;Wenwen Ye;Shuaiqiang Wang;Dawei Yin,True,https://openreview.net/pdf?id=EZcHYuU_9E
EvtEGQmXe3,Neural Topological Ordering for Computation Graphs,"Recent works on machine learning for combinatorial optimization have shown that learning based approaches can outperform heuristic methods in terms of speed and performance. In this paper, we consider the problem of finding an optimal topological order on a directed acyclic graph (DAG) with focus on the memory minimization problem which arises in compilers. We propose an end-to-end machine learning based approach for topological ordering using an encoder-decoder framework. Our encoder is a novel attention based graph neural network architecture called \\\\emph{Topoformer} which uses different topological transforms of a DAG for message passing. The node embeddings produced by the encoder are converted into node priorities which are used by the decoder to generate a probability distribution over topological orders. We train our model on a dataset of synthetically generated graphs called layered graphs. We show that our model outperforms, or is on-par, with several topological ordering baselines while being significantly faster on synthetic graphs with up to 2k nodes. We also train and test our model on a set of real-world computation graphs, showing performance improvements. ",main,NeurIPS,2022,Poster,Mukul Gagrani;Corrado Rainone;Yang Yang;Harris Teague;Wonseok Jeon;Roberto Bondesan;Herke van Hoof;Christopher Lott;Weiliang Will Zeng;Piero Zappi,True,https://openreview.net/pdf?id=EvtEGQmXe3
F9ENmZABB0,Wild-Time: A Benchmark of in-the-Wild Distribution Shift over Time,"Distribution shifts occur when the test distribution differs from the training distribution, and can considerably degrade performance of machine learning models deployed in the real world. While recent works have studied robustness to distribution shifts, distribution shifts arising from the passage of time have the additional structure of timestamp metadata. Real-world examples of such shifts are underexplored, and it is unclear whether existing models can leverage trends in past distribution shifts to reliably extrapolate into the future. To address this gap, we curate Wild-Time, a benchmark of 5 datasets that reflect temporal distribution shifts arising in a variety of real-world applications, including drug discovery, patient prognosis, and news classification. On these datasets, we systematically benchmark 13 approaches with various inductive biases. We evaluate methods in domain-generalization, continual learning, self-supervised learning, and ensemble learning, which leverage timestamps to extract the common structure of the distribution shifts. We extend several domain-generalization methods to the temporal distribution shift setting by treating windows of time as different domains. Finally, we propose two evaluation strategies to evaluate model performance under temporal distribution shifts---evaluation with a fixed time split (Eval-Fix) and evaluation with a data stream (Eval-Stream). Eval-Fix, our primary evaluation strategy, aims to provide a simple evaluation protocol for the broader machine learning community, while Eval-Stream serves as a complementary benchmark for continual learning approaches. Our experiments demonstrate that existing methods are limited in tackling temporal distribution shift: across all settings, we observe an average performance drop of 20% from in-distribution to out-of-distribution data.",Datasets & Benchmarks,NeurIPS,2022,Poster,Huaxiu Yao;Caroline Choi;Bochuan Cao;Yoonho Lee;Pang Wei Koh;Chelsea Finn,True,https://openreview.net/pdf?id=F9ENmZABB0
FA9jVbCIgBh,A Survey and Datasheet Repository of Publicly Available US Criminal Justice Datasets,"Criminal justice is an increasingly important application domain for machine learning and algorithmic fairness, as predictive tools are becoming widely used in police, courts, and prison systems worldwide. A few relevant benchmarks have received significant attention, e.g., the COMPAS dataset, often without proper consideration of the domain context. To raise awareness of publicly available criminal justice datasets and encourage their responsible use, we conduct a survey, consider contexts, highlight potential uses, and identify gaps and limitations. We provide datasheets for 15 datasets and upload them to a public repository. We compare the datasets across several dimensions, including size, coverage of the population, and potential use, highlighting concerns. We hope that this work can provide a useful starting point for researchers looking for appropriate datasets related to criminal justice, and that the repository will continue to grow as a community effort. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Miri Zilka;Bradley Butcher;Adrian Weller,False,https://openreview.net/pdf?id=FA9jVbCIgBh
FPgCB_Z_0O,DART: Articulated Hand Model with Diverse Accessories and Rich Textures,"Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of digital twins. Among different hand morphable models, MANO has been widely used in vision and graphics community. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic hand data. In this paper, we extend MANO with Diverse Accessories and Rich Textures, namely DART. DART is composed of 50 daily 3D accessories which varies in appearance and shape, and 325 hand-crafted 2D texture maps covers different kinds of blemishes or make-ups. Unity GUI is also provided to generate synthetic hand data with user-defined settings, e.g., pose, camera, background, lighting, textures, and accessories. Finally, we release DARTset, which contains large-scale (800K), high-fidelity synthetic hand images, paired with perfect-aligned 3D labels. Experiments demonstrate its superiority in diversity. As a complement to existing hand datasets, DARTset boosts the generalization in both hand pose estimation and mesh recovery tasks. Raw ingredients (textures, accessories), Unity GUI, source code and DARTset are publicly available at dart2022.github.io.
",Datasets & Benchmarks,NeurIPS,2022,Poster,Daiheng Gao;Yuliang Xiu;Kailin Li;Lixin Yang;Feng Wang;Peng Zhang;Bang Zhang;Cewu Lu;Ping Tan,True,https://openreview.net/pdf?id=FPgCB_Z_0O
FgDzS8_Fz7c,Category-Level 6D Object Pose Estimation in the Wild: A Semi-Supervised Learning Approach and A New Dataset,"6D object pose estimation is one of the fundamental problems in computer vision and robotics research. While a lot of recent efforts have been made on generalizing pose estimation to novel object instances within the same category, namely category-level 6D pose estimation, it is still restricted in constrained environments given the limited number of annotated data. In this paper, we collect Wild6D, a new unlabeled RGBD object video dataset with diverse instances and backgrounds. We utilize this data to generalize category-level 6D object pose estimation in the wild with semi-supervised learning. We propose a new model, called Rendering for Pose estimation network RePoNet), that is jointly trained using the free ground-truths with the synthetic data, and a silhouette matching objective function on the real-world data. Without using any 3D annotations on real data, our method outperforms state-of-the-art methods on the previous dataset and our Wild6D test set (with manual annotations for evaluation) by a large margin.  Project page with Wild6D data:  \\\\url{https://oasisyang.github.io/semi-pose/}.",main,NeurIPS,2022,Poster,Yang Fu;Xiaolong Wang,True,https://openreview.net/pdf?id=FgDzS8_Fz7c
FhqzyGoTSH,CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks,"Current state-of-the-art vision-and-language models are evaluated on tasks either individually or in a multi-task setting, overlooking the challenges of continually learning (CL) tasks as they arrive. Existing CL benchmarks have facilitated research on task adaptation and mitigating ""catastrophic forgetting"", but are limited to vision-only and language-only tasks. We present CLiMB, a benchmark to study the challenge of learning multimodal tasks in a CL setting, and to systematically evaluate how upstream continual learning can rapidly generalize to new multimodal and unimodal tasks. CLiMB includes implementations of several CL algorithms and a modified Vision-Language Transformer (ViLT) model that can be deployed on both multimodal and unimodal tasks. We find that common CL methods can help mitigate forgetting during multimodal task learning, but do not enable cross-task knowledge transfer. We envision that CLiMB will facilitate research on a new class of CL algorithms for this challenging multimodal setting.",Datasets & Benchmarks,NeurIPS,2022,Poster,Tejas Srinivasan;Ting-Yun Chang;Leticia Leonor Pinto Alva;Georgios Chochlakis;Mohammad Rostami;Jesse Thomason,False,https://openreview.net/pdf?id=FhqzyGoTSH
Fp7__phQszn,Why do tree-based models still outperform deep learning on typical tabular data?,"While deep learning has enabled tremendous progress on text and image datasets, its superiority on tabular data is not clear. We contribute extensive benchmarks of standard and novel deep learning methods as well as tree-based models such as XGBoost and Random Forests, across a large number of datasets and hyperparameter combinations. We define a standard set of 45 datasets from varied domains with clear characteristics of tabular data and a benchmarking methodology accounting for both fitting models and finding good hyperparameters. Results show that tree-based models remain state-of-the-art on medium-sized data ($\\\\sim$10K samples) even without accounting for their superior speed. To understand this gap, we conduct an empirical investigation into the differing inductive biases of tree-based models and neural networks. This leads to a series of challenges which should guide researchers aiming to build tabular-specific neural network: 1) be robust to uninformative features, 2) preserve the orientation of the data, and 3) be able to easily learn irregular functions. To stimulate research on tabular architectures, we contribute a standard benchmark and raw data for baselines: every point of a 20\\\\,000 compute hours hyperparameter search for each learner.",Datasets & Benchmarks,NeurIPS,2022,Poster,Leo Grinsztajn;Edouard Oyallon;Gael Varoquaux,False,https://openreview.net/pdf?id=Fp7__phQszn
GKOa7yNH8Uh,GLOBEM Dataset: Multi-Year Datasets for Longitudinal Human Behavior Modeling Generalization,"Recent research has demonstrated the capability of behavior signals captured by smartphones and wearables for longitudinal behavior modeling. However, there is a lack of a comprehensive public dataset that serves as an open testbed for fair comparison among algorithms. Moreover, prior studies mainly evaluate algorithms using data from a single population within a short period, without measuring the cross-dataset generalizability of these algorithms. We present the first multi-year passive sensing datasets, containing over 700 user-years and 497 unique users’ data collected from mobile and wearable sensors, together with a wide range of well-being metrics. Our datasets can support multiple cross-dataset evaluations of behavior modeling algorithms’ generalizability across different users and years. As a starting point, we provide the benchmark results of 18 algorithms on the task of depression detection. Our results indicate that both prior depression detection algorithms and domain generalization techniques show potential but need further research to achieve adequate cross-dataset generalizability. We envision our multi-year datasets can support the ML community in developing generalizable longitudinal behavior modeling algorithms.",Datasets & Benchmarks,NeurIPS,2022,Poster,Xuhai Xu;Han Zhang;Yasaman S Sefidgar;Yiyi Ren;Xin Liu;Woosuk Seo;Jennifer Brown;Kevin Scott Kuehn;Mike A Merrill;Paula S Nurius;Shwetak Patel;Tim Althoff;Margaret E Morris;Eve A. Riskin;Jennifer Mankoff;Anind Dey,True,https://openreview.net/pdf?id=GKOa7yNH8Uh
GP1Ncd8nTgn,METS-CoV: A Dataset of Medical Entity and Targeted Sentiment on COVID-19 Related Tweets,"The COVID-19 pandemic continues to bring up various topics discussed or debated on social media. In order to explore the impact of pandemics on people's lives, it is crucial to understand the public's concerns and attitudes towards pandemic-related entities (e.g., drugs, vaccines) on social media. However, models trained on existing named entity recognition (NER) or targeted sentiment analysis (TSA) datasets have limited ability to understand COVID-19-related social media texts because these datasets are not designed or annotated from a medical perspective. In this paper, we release METS-CoV, a dataset containing medical entities and targeted sentiments from COVID-19 related tweets. METS-CoV contains 10,000 tweets with 7 types of entities, including 4 medical entity types (Disease, Drug, Symptom, and Vaccine) and 3 general entity types (Person, Location, and Organization). To further investigate tweet users' attitudes toward specific entities, 4 types of entities (Person, Organization, Drug, and Vaccine) are selected and annotated with user sentiments, resulting in a targeted sentiment dataset with 9,101 entities (in 5,278 tweets). To the best of our knowledge, METS-CoV is the first dataset to collect medical entities and corresponding sentiments of COVID-19 related tweets. We benchmark the performance of classical machine learning models and state-of-the-art deep learning models on NER and TSA tasks with extensive experiments. Results show that this dataset has vast room for improvement for both NER and TSA tasks. With rich annotations and comprehensive benchmark results, we believe METS-CoV is a fundamental resource for building better medical social media understanding tools and facilitating computational social science research, especially on epidemiological topics. Our data, annotation guidelines, benchmark models, and source code are publicly available (\\\\url{https://github.com/YLab-Open/METS-CoV}) to ensure reproducibility. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Peilin Zhou;Zeqiang Wang;Dading Chong;Zhijiang Guo;Yining Hua;Zichang Su;Zhiyang Teng;Jiageng Wu;Jie Yang,True,https://openreview.net/pdf?id=GP1Ncd8nTgn
GgM5DiAb6A2,FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings,"Federated Learning (FL) is a novel approach enabling several clients holding sensitive data to collaboratively train machine learning models, without centralizing data. The cross-silo FL setting corresponds to the case of few ($2$--$50$) reliable clients, each holding medium to large datasets, and is typically found in applications such as healthcare, finance, or industry. While previous works have proposed representative datasets for cross-device FL, few realistic healthcare cross-silo FL datasets exist, thereby slowing algorithmic research in this critical application. In this work, we propose a novel cross-silo dataset suite focused on healthcare, FLamby (Federated Learning AMple Benchmark of Your cross-silo strategies), to bridge the gap between theory and practice of cross-silo FL.
FLamby encompasses 7 healthcare datasets with natural splits, covering multiple tasks, modalities, and data volumes, each accompanied with baseline training code. As an illustration, we additionally benchmark standard FL algorithms on all datasets.
Our flexible and modular suite allows researchers to easily download datasets, reproduce results and re-use the different components for their research. FLamby is available at~\\\\url{www.github.com/owkin/flamby}.",Datasets & Benchmarks,NeurIPS,2022,Poster,Jean Ogier du Terrail;Samy-Safwan Ayed;Edwige Cyffers;Felix Grimberg;Chaoyang He;Regis Loeb;Paul Mangold;Tanguy Marchand;Othmane Marfoq;Erum Mushtaq;Boris Muzellec;Constantin Philippenko;Santiago Silva;Maria Teleńczuk;Shadi Albarqouni;Salman Avestimehr;Aurélien Bellet;Aymeric Dieuleveut;Martin Jaggi;Sai Praneeth Karimireddy;Marco Lorenzi;Giovanni Neglia;Marc Tommasi;Mathieu Andreux,True,https://openreview.net/pdf?id=GgM5DiAb6A2
GiEnzxTnaMN,Wasserstein Iterative Networks for Barycenter Estimation,"Wasserstein barycenters have become popular due to their ability to represent the average of probability measures in a geometrically meaningful way. In this paper, we present an algorithm to approximate the Wasserstein-2 barycenters of continuous measures via a generative model. Previous approaches rely on regularization (entropic/quadratic) which introduces bias or on input convex neural networks which are not expressive enough for large-scale tasks. In contrast, our algorithm does not introduce bias and allows using arbitrary neural networks. In addition, based on the celebrity faces dataset, we construct Ave, celeba! dataset which can be used for quantitative evaluation of barycenter algorithms by using standard metrics of generative models such as FID. ",main,NeurIPS,2022,Poster,Alexander Korotin;Vage Egiazarian;Lingxiao Li;Evgeny Burnaev,True,https://openreview.net/pdf?id=GiEnzxTnaMN
H4Po2dDzdFq,FACT: Learning Governing Abstractions Behind Integer Sequences,"Integer sequences are of central importance to the modeling of concepts admitting complete finitary descriptions. We introduce a novel view on the learning of such concepts and lay down a set of benchmarking tasks aimed at conceptual understanding by machine learning models. These tasks indirectly assess model ability to abstract, and challenge them to reason both interpolatively and extrapolatively from the knowledge gained by observing representative examples. To further aid research in knowledge representation and reasoning, we present FACT, the Finitary Abstraction Comprehension Toolkit. The toolkit surrounds a large dataset of integer sequences comprising both organic and synthetic entries, a library for data pre-processing and generation, a set of model performance evaluation tools, and a collection of baseline model implementations, enabling the making of the future advancements with ease.",Datasets & Benchmarks,NeurIPS,2022,Poster,Peter Belcak;Ard Kastrati;Flavio Schenker;Roger Wattenhofer,True,https://openreview.net/pdf?id=H4Po2dDzdFq
HCnb1TByvx7,Multilingual Abusive Comment Detection at Scale for Indic Languages,"Social media platforms were conceived to act as online `town squares' where people could get together, share information and communicate with each other peacefully. However, harmful content borne out of bad actors are constantly plaguing these platforms slowly converting them into `mosh pits' where the bad actors take the liberty to extensively abuse various marginalised groups. Accurate and timely detection of abusive content on social media platforms is therefore very important for facilitating safe interactions between users.  However, due to the small scale and sparse linguistic coverage of Indic abusive speech datasets, development of such algorithms for Indic social media users (one-sixth of global population) is severely impeded.
To facilitate and encourage research in this important direction, we contribute for the first time MACD - a large-scale (150K), human-annotated, multilingual (5 languages), balanced (49\\\\% abusive content) and diverse (70K users) abuse detection dataset of user comments, sourced from a popular social media platform - ShareChat. We also release AbuseXLMR, an abusive content detection model pretrained on large number of social media comments in 15+ Indic languages which outperforms XLM-R and MuRIL on multiple Indic datasets. Along with the annotations, we also release the mapping between comment, post and user id's to facilitate modelling the relationship between them. We share competitive monolingual, cross-lingual and few-shot baselines so that MACD can be used as a dataset benchmark for future research. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Vikram Gupta;Sumegh Roychowdhury;Mithun Das;Somnath Banerjee;Punyajoy Saha;Binny Mathew;hastagiri prakash vanchinathan;Animesh Mukherjee,True,https://openreview.net/pdf?id=HCnb1TByvx7
HYELrdRdJI,MVP-N: A Dataset and Benchmark for Real-World Multi-View Object Classification,"Combining information from multiple views is essential for discriminating similar objects. However, existing datasets for multi-view object classification have several limitations, such as synthetic and coarse-grained objects, no validation split for hyperparameter tuning, and a lack of view-level information quantity annotations for analyzing multi-view-based methods. To address this issue, this study proposes a new dataset, MVP-N, which contains 44 retail products, 16k real captured views with human-perceived information quantity annotations, and 9k multi-view sets. The fine-grained categorization of objects naturally generates multi-view label noise owing to the inter-class view similarity, allowing the study of learning from noisy labels in the multi-view case. Moreover, this study benchmarks four multi-view-based feature aggregation methods and twelve soft label methods on MVP-N. Experimental results show that MVP-N will be a valuable resource for facilitating the development of real-world multi-view object classification methods. The dataset and code are publicly available at https://github.com/SMNUResearch/MVP-N.",Datasets & Benchmarks,NeurIPS,2022,Poster,Ren Wang;Jiayue Wang;Tae Sung Kim;JINSUNG KIM;Hyuk-Jae Lee,True,https://openreview.net/pdf?id=HYELrdRdJI
HjwK-Tc_Bc,Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering,"When answering a question, humans utilize the information available across different modalities to synthesize a consistent and complete chain of thought (CoT). This process is normally a black box in the case of deep learning models like large-scale language models. Recently, science question benchmarks have been used to diagnose the multi-hop reasoning ability and interpretability of an AI system. However, existing datasets fail to provide annotations for the answers, or are restricted to the textual-only modality, small scales, and limited domain diversity. To this end, we present Science Question Answering (ScienceQA), a new benchmark that consists of ~21k multimodal multiple choice questions with a diverse set of science topics and annotations of their answers with corresponding lectures and explanations. We further design language models to learn to generate lectures and explanations as the chain of thought (CoT) to mimic the multi-hop reasoning process when answering ScienceQA questions. ScienceQA demonstrates the utility of CoT in language models, as CoT improves the question answering performance by 1.20% in few-shot GPT-3 and 3.99% in fine-tuned UnifiedQA. We also explore the upper bound for models to leverage explanations by feeding those in the input; we observe that it improves the few-shot performance of GPT-3 by 18.96%. Our analysis further shows that language models, similar to humans, benefit from explanations to learn from fewer data and achieve the same performance with just 40% of the data. The data and code are available at https://scienceqa.github.io.",main,NeurIPS,2022,Poster,Pan Lu;Swaroop Mishra;Tony Xia;Liang Qiu;Kai-Wei Chang;Song-Chun Zhu;Oyvind Tafjord;Peter Clark;Ashwin Kalyan,True,https://openreview.net/pdf?id=HjwK-Tc_Bc
IIbJ9m5G73t,BLOX: Macro Neural Architecture Search Benchmark and Algorithms,"Neural architecture search (NAS) has been successfully used to design numerous high-performance neural networks. However, NAS is typically compute-intensive, so most existing approaches restrict the search to decide the operations and topological structure of a single block only, then the same block is stacked repeatedly to form an end-to-end model. Although such an approach reduces the size of search space, recent studies show that a macro search space, which allows blocks in a model to be different, can lead to better performance. To provide a systematic study of the performance of NAS algorithms on a macro search space, we release Blox – a benchmark that consists of 91k unique models trained on the CIFAR-100 dataset. The dataset also includes runtime measurements of all the models on a diverse set of hardware platforms. We perform extensive experiments to compare existing algorithms that are well studied on cell-based search spaces, with the emerging blockwise approaches that aim to make NAS scalable to much larger macro search spaces. The Blox benchmark and code are available at https://github.com/SamsungLabs/blox.",Datasets & Benchmarks,NeurIPS,2022,Poster,Thomas Chun Pong Chau;Łukasz Dudziak;Hongkai Wen;Nicholas Donald Lane;Mohamed S Abdelfattah,True,https://openreview.net/pdf?id=IIbJ9m5G73t
IUikebJ1Bf0,Autoformalization with Large Language Models,"Autoformalization is the process of automatically translating from natural language mathematics to formal specifications and proofs. A successful autoformalization system could advance the fields of formal verification, program synthesis, and artificial intelligence.
While the long-term goal of autoformalization seemed elusive for a long time, we show large language models provide new prospects towards this goal. We make the surprising observation that LLMs can correctly translate a significant portion ($25.3\\\\%$) of mathematical competition problems perfectly to formal specifications in Isabelle/HOL. We demonstrate the usefulness of this process by improving a previously introduced neural theorem prover via training on these autoformalized theorems. Our methodology results in a new state-of-the-art result on the MiniF2F theorem proving benchmark, improving the proof rate from~$29.6\\\\%$ to~$35.2\\\\%$.",main,NeurIPS,2022,Poster,Yuhuai Wu;Albert Qiaochu Jiang;Wenda Li;Markus Norman Rabe;Charles E Staats;Mateja Jamnik;Christian Szegedy,True,https://openreview.net/pdf?id=IUikebJ1Bf0
Jq3uTzLg9se,ComMU: Dataset for Combinatorial Music Generation,"Commercial adoption of automatic music composition requires the capability of generating diverse and high-quality music suitable for the desired context (e.g., music for romantic movies, action games, restaurants, etc.). In this paper, we introduce combinatorial music generation, a new task to create varying background music based on given conditions. Combinatorial music generation creates short samples of music with rich musical metadata, and combines them to produce a complete music. In addition, we introduce ComMU, the first symbolic music dataset consisting of short music samples and their corresponding 12 musical metadata for combinatorial music generation. Notable properties of ComMU are that (1) dataset is manually constructed by professional composers with an objective guideline that induces regularity, and (2) it has 12 musical metadata that embraces composers' intentions. Our results show that we can generate diverse high-quality music only with metadata, and that our unique metadata such as track-role and extended chord quality improves the capacity of the automatic composition. We highly recommend watching our video before reading the paper (https://pozalabs.github.io/ComMU/).",Datasets & Benchmarks,NeurIPS,2022,Poster,Lee Hyun;Taehyun Kim;Hyolim Kang;Minjoo Ki;Hyeonchan Hwang;Kwanho Park;Sharang Han;Seon Joo Kim,True,https://openreview.net/pdf?id=Jq3uTzLg9se
JyTT03dqCFD,The Neural Testbed: Evaluating Joint Predictions,"
Predictive distributions quantify uncertainties ignored by point estimates. This paper introduces The Neural Testbed: an open source benchmark for controlled and principled evaluation of agents that generate such predictions. Crucially, the testbed assesses agents not only on the quality of their marginal predictions per input, but also on their joint predictions across many inputs. We evaluate a range of agents using a simple neural network data generating process.

Our results indicate that some popular Bayesian deep learning agents do not fare well with joint predictions, even when they can produce accurate marginal predictions. We also show that the quality of joint predictions drives performance in downstream decision tasks. We find these results are robust across choice a wide range of generative models, and highlight the practical importance of joint predictions to the community.",main,NeurIPS,2022,Poster,Ian Osband;Zheng Wen;Seyed Mohammad Asghari;Vikranth Dwaracherla;Xiuyuan Lu;Morteza Ibrahimi;Dieterich Lawson;Botao Hao;Brendan O'Donoghue;Benjamin Van Roy,True,https://openreview.net/pdf?id=JyTT03dqCFD
K8cD1Uv3wZy,Private Multiparty Perception for Navigation,"We introduce a framework for navigating through cluttered environments by connecting multiple cameras together while simultanously preserving privacy. Occlusions and obstacles in large environments are often challenging situations for navigation agents because the environment is not fully observable from a single camera view. Given multiple camera views of an environment, our approach learns to produce a multiview scene representation that can only be used for navigation, provably preventing one party from inferring anything beyond the output task. On a new navigation dataset that we will publicly release, experiments show that private multiparty representations allow navigation through complex scenes and around obstacles while jointly preserving privacy. Our approach scales to an arbitrary number of camera viewpoints. We believe developing visual representations that preserve privacy is increasingly important for many applications such as navigation. ",main,NeurIPS,2022,Poster,Hui Lu;Mia Chiquier;Carl Vondrick,True,https://openreview.net/pdf?id=K8cD1Uv3wZy
KUOKpojFr_,ShapeCrafter: A Recursive Text-Conditioned 3D Shape Generation Model,"We present ShapeCrafter, a neural network for recursive text-conditioned 3D shape generation. Existing methods to generate text-conditioned 3D shapes consume an entire text prompt to generate a 3D shape in a single step. However, humans tend to describe shapes recursively---we may start with an initial description and progressively add details based on intermediate results. To capture this recursive process, we introduce a method to generate a 3D shape distribution, conditioned on an initial phrase, that gradually evolves as more phrases are added. Since existing datasets are insufficient for training this approach, we present Text2Shape++, a large dataset of 369K shape--text pairs that supports recursive shape generation. To capture local details that are often used to refine shape descriptions, we build on top of vector-quantized deep implicit functions that generate a distribution of high-quality shapes. Results show that our method can generate shapes consistent with text descriptions, and shapes evolve gradually as more phrases are added. Our method supports shape editing, extrapolation, and can enable new applications in human--machine collaboration for creative design.",main,NeurIPS,2022,Poster,Rao Fu;Xiao Zhan;Yiwen Chen;Daniel Ritchie;Srinath Sridhar,True,https://openreview.net/pdf?id=KUOKpojFr_
KglFYlTiASW,Neural Transmitted Radiance Fields,"Neural radiance fields (NeRF) have brought tremendous progress to novel view synthesis. Though NeRF enables the rendering of subtle details in a scene by learning from a dense set of images, it also reconstructs the undesired reflections when we capture images through glass. As a commonly observed interference, the reflection would undermine the visibility of the desired transmitted scene behind glass by occluding the transmitted light rays. In this paper, we aim at addressing the problem of rendering novel transmitted views given a set of reflection-corrupted images. By introducing the transmission encoder and recurring edge constraints as guidance, our neural transmitted radiance fields can resist such reflection interference during rendering and reconstruct high-fidelity results even under sparse views. The proposed method achieves superior performance from the experiments on a newly collected dataset compared with state-of-the-art methods. ",main,NeurIPS,2022,Poster,Chengxuan Zhu;Renjie Wan;Boxin Shi,True,https://openreview.net/pdf?id=KglFYlTiASW
Kyswf8Kj83,TwiBot-22: Towards Graph-Based Twitter Bot Detection,"Twitter bot detection has become an increasingly important task to combat misinformation, facilitate social media moderation, and preserve the integrity of the online discourse. State-of-the-art bot detection methods generally leverage the graph structure of the Twitter network, and they exhibit promising performance when confronting novel Twitter bots that traditional methods fail to detect. However, very few of the existing Twitter bot detection datasets are graph-based, and even these few graph-based datasets suffer from limited dataset scale, incomplete graph structure, as well as low annotation quality. In fact, the lack of a large-scale graph-based Twitter bot detection benchmark that addresses these issues has seriously hindered the development and evaluation of novel graph-based bot detection approaches. In this paper, we propose TwiBot-22, a comprehensive graph-based Twitter bot detection benchmark that presents the largest dataset to date, provides diversified entities and relations on the Twitter network, and has considerably better annotation quality than existing datasets. In addition, we re-implement 35 representative Twitter bot detection baselines and evaluate them on 9 datasets, including TwiBot-22, to promote a fair comparison of model performance and a holistic understanding of research progress. To facilitate further research, we consolidate all implemented codes and datasets into the TwiBot-22 evaluation framework, where researchers could consistently evaluate new models and datasets. The TwiBot-22 Twitter bot detection benchmark and evaluation framework are publicly available at \\\\url{https://twibot22.github.io/}.",Datasets & Benchmarks,NeurIPS,2022,Poster,Shangbin Feng;Zhaoxuan Tan;Herun Wan;Ningnan Wang;Zilong Chen;Binchi Zhang;Qinghua Zheng;Wenqian Zhang;Zhenyu Lei;Shujie Yang;Xinshun Feng;Qingyue Zhang;Hongrui Wang;Yuhan Liu;Yuyang Bai;Heng Wang;Zijian Cai;Yanbo Wang;Lijing Zheng;Zihan Ma;Jundong Li;Minnan Luo,True,https://openreview.net/pdf?id=Kyswf8Kj83
LbOdQrnOb2q,Forecasting Future World Events With Neural Networks,"Forecasting future world events is a challenging but valuable task. Forecasts of climate, geopolitical conflict, pandemics and economic indicators help shape policy and decision making. In these domains, the judgment of expert humans contributes to the best forecasts. Given advances in language modeling, can these forecasts be automated? To this end, we introduce Autocast, a dataset containing thousands of forecasting questions and an accompanying news corpus. Questions are taken from forecasting tournaments, ensuring high quality, real-world importance, and diversity. The news corpus is organized by date, allowing us to precisely simulate the conditions under which humans made past forecasts (avoiding leakage from the future). Motivated by the difficulty of forecasting numbers across orders of magnitude (e.g. global cases of COVID-19 in 2022), we also curate IntervalQA, a dataset of numerical questions and metrics for calibration. We test language models on our forecasting task and find that performance is far below a human expert baseline. However, performance improves with increased model size and incorporation of relevant information from the news corpus. In sum, Autocast poses a novel challenge for large language models and improved performance could bring large practical benefits.",Datasets & Benchmarks,NeurIPS,2022,Poster,Andy Zou;Tristan Xiao;Ryan Jia;Joe Kwon;Mantas Mazeika;Richard Li;Dawn Song;Jacob Steinhardt;Owain Evans;Dan Hendrycks,True,https://openreview.net/pdf?id=LbOdQrnOb2q
LkAFwrqdRY6,FinRL-Meta: Market Environments and Benchmarks for Data-Driven Financial Reinforcement Learning,"Finance is a particularly challenging playground for deep reinforcement learning. However, establishing high-quality market environments and benchmarks for financial reinforcement learning is challenging due to three major factors, namely, low signal-to-noise ratio of financial data, survivorship bias of historical data, and backtesting overfitting. In this paper, we present an openly accessible FinRL-Meta library that has been actively maintained by the AI4Finance community. First, following a DataOps paradigm, we will provide hundreds of market environments through an automatic data curation pipeline that processes dynamic datasets from real-world markets into gym-style market environments. Second, we reproduce popular papers as stepping stones for users to design new trading strategies. We also deploy the library on cloud platforms so that users can visualize their own results and assess the relative performance via community-wise competitions. Third, FinRL-Meta provides tens of Jupyter/Python demos organized into a curriculum and a documentation website to serve the rapidly growing community. FinRL-Meta is available at: \\\\url{https://github.com/AI4Finance-Foundation/FinRL-Meta}",Datasets & Benchmarks,NeurIPS,2022,Poster,Xiao-Yang Liu;Ziyi Xia;Jingyang Rui;Jiechao Gao;Hongyang Yang;Ming Zhu;Christina Dan Wang;Zhaoran Wang;Jian Guo,False,https://openreview.net/pdf?id=LkAFwrqdRY6
LvyJX20Rll,Factuality Enhanced Language Models for Open-Ended Text Generation,"Pretrained language models (LMs) are susceptible to generate text with nonfactual information.  In this work, we measure and improve the factual accuracy of large-scale LMs for open-ended text generation.  We design the FactualityPrompts test set and metrics to measure the factuality of LM generations.  Based on that, we study the factual accuracy of LMs with parameter sizes ranging from 126M to 530B.   Interestingly, we find that larger LMs are more factual than smaller ones, although a previous study suggests that larger LMs can be less truthful in terms of misconceptions.  In addition, popular sampling algorithms (e.g., top-p) in open-ended text generation can harm the factuality due to the ``uniform randomness'' introduced at every sampling step.  We propose the factual-nucleus sampling algorithm that dynamically adapts the randomness to improve the factuality of generation while maintaining quality.  Furthermore, we analyze the inefficiencies of the standard training method in learning correct associations between entities from factual text corpus (e.g., Wikipedia).   We propose a factuality-enhanced training method that uses TopicPrefix for better awareness of facts and sentence completion as the training objective, which can vastly reduce the factual errors.",main,NeurIPS,2022,Poster,Nayeon Lee;Wei Ping;Peng Xu;Mostofa Patwary;Pascale Fung;Mohammad Shoeybi;Bryan Catanzaro,True,https://openreview.net/pdf?id=LvyJX20Rll
M3Y74vmsMcY,LAION-5B: An open large-scale dataset for training next generation image-text models,"Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Christoph Schuhmann;Romain Beaumont;Richard Vencu;Cade W Gordon;Ross Wightman;Mehdi Cherti;Theo Coombes;Aarush Katta;Clayton Mullis;Mitchell Wortsman;Patrick Schramowski;Srivatsa R Kundurthy;Katherine Crowson;Ludwig Schmidt;Robert Kaczmarczyk;Jenia Jitsev,True,https://openreview.net/pdf?id=M3Y74vmsMcY
MKDdTASg_1y,A Benchmark for Compositional Visual Reasoning,"A fundamental component of human vision is our ability to parse complex visual scenes and judge the relations between their constituent objects. AI benchmarks for visual reasoning have driven rapid progress in recent years with state-of-the-art systems now reaching human accuracy on some of these benchmarks. Yet, there remains a major gap between humans and AI systems in terms of the sample efficiency with which they learn new visual reasoning tasks. Humans' remarkable efficiency at learning has been at least partially attributed to their ability to harness compositionality -- allowing them to efficiently take advantage of previously gained knowledge when learning new tasks. Here, we introduce a novel visual reasoning benchmark, Compositional Visual Relations (CVR), to drive progress towards the development of more data-efficient learning algorithms. We take inspiration from fluidic intelligence and non-verbal reasoning tests and describe a novel method for creating compositions of abstract rules and generating image datasets corresponding to these rules at scale. Our proposed benchmark includes measures of sample efficiency, generalization, compositionality, and transfer across task rules. We systematically evaluate modern neural architectures and find that convolutional architectures surpass transformer-based architectures across all performance measures in most data regimes. However, all computational models are much less data efficient than humans, even after learning informative visual representations using self-supervision. Overall, we hope our challenge will spur interest in developing neural architectures that can learn to harness compositionality for more efficient learning. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Aimen Zerroug;Mohit Vaishnav;Julien Colin;Sebastian Musslick;Thomas Serre,True,https://openreview.net/pdf?id=MKDdTASg_1y
MOCZI3h8Ye,Model Zoos: A Dataset of Diverse Populations of Neural Network Models,"In the last years, neural networks (NN) have evolved from laboratory environments to the state-of-the-art for many real-world problems. It was shown that NN models (i.e., their weights and biases) evolve on unique trajectories in weight space during training. Following, a population of such neural network models (referred to as model zoo) would form structures in weight space. We think that the geometry, curvature and smoothness of these structures contain information about the state of training and can reveal latent properties of individual models. With such model zoos, one could investigate novel approaches for (i) model analysis, (ii) discover unknown learning dynamics, (iii) learn rich representations of such populations, or (iv) exploit the model zoos for generative modelling of NN weights and biases. Unfortunately, the lack of standardized model zoos and available benchmarks significantly increases the friction for further research about populations of NNs. With this work, we publish a novel dataset of model zoos containing systematically generated and diverse populations of NN models for further research. In total the proposed model zoo dataset is based on eight image datasets, consists of 27 model zoos trained with varying hyperparameter combinations and includes 50’360 unique NN models as well as their sparsified twins, resulting in over 3’844’360 collected model states. Additionally, to the model zoo data we provide an in-depth analysis of the zoos and provide benchmarks for multiple downstream tasks. The dataset can be found at www.modelzoos.cc.",Datasets & Benchmarks,NeurIPS,2022,Poster,Konstantin Schürholt;Diyar Taskiran;Boris Knyazev;Xavier Giró-i-Nieto;Damian Borth,True,https://openreview.net/pdf?id=MOCZI3h8Ye
MU2495w47rz,OpenXAI: Towards a Transparent Evaluation of Model Explanations,"While several types of post hoc explanation methods have been proposed in recent literature, there is very little work on systematically benchmarking these methods. Here, we introduce OpenXAI, a comprehensive and extensible open-source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, (ii) open-source implementations of twenty-two quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, and (iii) the first ever public XAI leaderboards to readily compare several explanation methods across a wide variety of metrics, models, and datasets. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. 
Overall, OpenXAI provides an automated end-to-end pipeline that not only simplifies and standardizes the evaluation of post hoc explanation methods, but also promotes transparency and reproducibility in benchmarking these methods. While the first release of OpenXAI supports only tabular datasets, the explanation methods and metrics that we consider are general enough to be applicable to other data modalities. OpenXAI datasets and data loaders, implementations of state-of-the-art explanation methods and evaluation metrics, as well as leaderboards are publicly available at https://open-xai.github.io/. OpenXAI will be regularly updated to incorporate text and image datasets, other new metrics and explanation methods, and welcomes inputs from the community.",Datasets & Benchmarks,NeurIPS,2022,Poster,Chirag Agarwal;Satyapriya Krishna;Eshika Saxena;Martin Pawelczyk;Nari Johnson;Isha Puri;Marinka Zitnik;Himabindu Lakkaraju,False,https://openreview.net/pdf?id=MU2495w47rz
My5AI9aM49R,Video compression dataset and benchmark of learning-based video-quality metrics,"Video-quality measurement is a critical task in video processing. Nowadays, many implementations of new encoding standards - such as AV1, VVC, and LCEVC - use deep-learning-based decoding algorithms with perceptual metrics that serve as optimization objectives. But investigations of the performance of modern video- and image-quality metrics commonly employ videos compressed using older standards, such as AVC. In this paper, we present a new benchmark for video-quality metrics that evaluates video compression. It is based on a new dataset consisting of about 2,500 streams encoded using different standards, including AVC, HEVC, AV1, VP9, and VVC.  Subjective scores were collected using crowdsourced pairwise comparisons. The list of evaluated metrics includes recent ones based on machine learning and neural networks. The results demonstrate that new no-reference metrics exhibit high correlation with subjective quality and approach the capability of top full-reference metrics.",Datasets & Benchmarks,NeurIPS,2022,Poster,Anastasia Antsiferova;Sergey Lavrushkin;Maksim Smirnov;Aleksandr Gushchin;Dmitriy S. Vatolin;Dmitriy Kulikov,True,https://openreview.net/pdf?id=My5AI9aM49R
MzaPEKHv-0J,PeRFception: Perception using Radiance Fields,"The recent progress in implicit 3D representation, i.e., Neural Radiance Fields (NeRFs), has made accurate and photorealistic 3D reconstruction possible in a differentiable manner. This new representation can effectively convey the information of hundreds of high-resolution images in one compact format and allows photorealistic synthesis of novel views. In this work, using the variant of NeRF called Plenoxels, we create the first large-scale radiance fields datasets  for perception tasks, called the PeRFception, which consists of two parts that incorporate both object-centric and scene-centric scans for classification and segmentation. It shows a significant memory compression rate (96.4\\\\%) from the original dataset, while containing both 2D and 3D information in a unified form. We construct the  classification and segmentation models that directly take this radiance fields format as input and also propose a novel augmentation technique to avoid overfitting on backgrounds of images. The code and data are publicly available in ""https://postech-cvlab.github.io/PeRFception/"".",Datasets & Benchmarks,NeurIPS,2022,Poster,Yoonwoo Jeong;Seungjoo Shin;Junha Lee;Chris Choy;Anima Anandkumar;Minsu Cho;Jaesik Park,True,https://openreview.net/pdf?id=MzaPEKHv-0J
N6-ABrmQMqD,CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to multiple Real-World Domains,"Unsupervised Domain Adaptation demonstrates great potential to mitigate domain shifts by transferring models from labeled source domains to unlabeled target domains. While Unsupervised Domain Adaptation has been applied to a wide variety of complex vision tasks, only few works focus on lane detection for autonomous driving. This can be attributed to the lack of publicly available datasets. To facilitate research in these directions, we propose CARLANE, a 3-way sim-to-real domain adaptation benchmark for 2D lane detection. CARLANE encompasses the single-target datasets MoLane and TuLane and the multi-target dataset MuLane. These datasets are built from three different domains, which cover diverse scenes and contain a total of 163K unique images, 118K of which are annotated. In addition we evaluate and report systematic baselines, including our own method, which builds upon Prototypical Cross-domain Self-supervised Learning. We find that false positive and false negative rates of the evaluated domain adaptation methods are high compared to those of fully supervised baselines. This affirms the need for benchmarks such as CARLANE to further strengthen research in Unsupervised Domain Adaptation for lane detection. CARLANE, all evaluated models and the corresponding implementations are publicly available at https://carlanebenchmark.github.io.",Datasets & Benchmarks,NeurIPS,2022,Poster,Bonifaz Stuhr;Johann Kaspar Ludwig Haselberger;Julian Gebele,True,https://openreview.net/pdf?id=N6-ABrmQMqD
NAYoSV3tk9,VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation,"Benefiting from language flexibility and compositionality, humans naturally intend to use language to command an embodied agent for complex tasks such as navigation and object manipulation. In this work, we aim to fill the blank of the last mile of embodied agents---object manipulation by following human guidance, e.g., “move the red mug next to the box while keeping it upright.” To this end, we introduce an Automatic Manipulation Solver (AMSolver) system and build a Vision-and-Language Manipulation benchmark (VLMbench) based on it, containing various language instructions on categorized robotic manipulation tasks. Specifically, modular rule-based task templates are created to automatically generate robot demonstrations with language instructions, consisting of diverse object shapes and appearances, action types, and motion constraints. We also develop a keypoint-based model 6D-CLIPort to deal with multi-view observations and language input and output a sequence of 6 degrees of freedom (DoF) actions. We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation.",Datasets & Benchmarks,NeurIPS,2022,Poster,Kaizhi Zheng;Xiaotong Chen;Odest Jenkins;Xin Eric Wang,True,https://openreview.net/pdf?id=NAYoSV3tk9
NN_TpS5dpo5,Physically-Based Face Rendering for NIR-VIS Face Recognition,"Near infrared (NIR) to Visible (VIS) face matching is challenging due to the significant domain gaps as well as a lack of sufficient data for cross-modality model training. To overcome this problem, we propose a novel method for paired NIR-VIS facial image generation. Specifically, we reconstruct 3D face shape and reflectance from a large 2D facial dataset and introduce a novel method of transforming the VIS reflectance to NIR reflectance. We then use a physically-based renderer to generate a vast, high-resolution and photorealistic dataset consisting of various poses and identities in the NIR and VIS spectra. Moreover, to facilitate the identity feature learning, we propose an IDentity-based Maximum Mean Discrepancy (ID-MMD) loss, which not only reduces the modality gap between NIR and VIS images at the domain level but encourages the network to focus on the identity features instead of facial details, such as poses and accessories. Extensive experiments conducted on four challenging NIR-VIS face recognition benchmarks demonstrate that the proposed method can achieve comparable performance with the state-of-the-art (SOTA) methods without requiring any existing NIR-VIS face recognition datasets. With slightly fine-tuning on the target NIR-VIS face recognition datasets, our method can significantly surpass the SOTA performance. Code and pretrained models are released under the insightface GitHub.",main,NeurIPS,2022,Poster,Yunqi Miao;Alexandros Lattas;Jiankang Deng;Jungong Han;Stefanos Zafeiriou,True,https://openreview.net/pdf?id=NN_TpS5dpo5
ONB4RdP2GX,Hardness in Markov Decision Processes: Theory and Practice,"Meticulously analysing the empirical strengths and weaknesses of reinforcement learning methods in hard (challenging) environments is essential to inspire innovations and assess progress in the field. In tabular reinforcement learning, there is no well-established standard selection of environments to conduct such analysis, which is partially due to the lack of a widespread understanding of the rich theory of hardness of environments. The goal of this paper is to unlock the practical usefulness of this theory through four main contributions. First, we present a systematic survey of the theory of hardness, which also identifies promising research directions. Second, we introduce $\\\\texttt{Colosseum}$, a pioneering package that enables empirical hardness analysis and implements a principled benchmark composed of environments that are diverse with respect to different measures of hardness. Third, we present an empirical analysis that provides new insights into computable measures. Finally, we benchmark five tabular agents in our newly proposed benchmark. While advancing the theoretical understanding of hardness in non-tabular reinforcement learning remains essential, our contributions in the tabular setting are intended as solid steps towards a principled non-tabular benchmark. Accordingly, we benchmark four agents in non-tabular versions of $\\\\texttt{Colosseum}$ environments, obtaining results that demonstrate the generality of tabular hardness measures.",main,NeurIPS,2022,Poster,Michelangelo Conserva;Paulo Rauber,True,https://openreview.net/pdf?id=ONB4RdP2GX
Oa2-cdfBxun,"mRI: Multi-modal 3D Human Pose Estimation Dataset using mmWave, RGB-D, and Inertial Sensors","The ability to estimate 3D human body pose and movement, also known as human pose estimation (HPE), enables many applications for home-based health monitoring, such as remote rehabilitation training. Several possible solutions have emerged using sensors ranging from RGB cameras, depth sensors, millimeter-Wave (mmWave) radars, and wearable inertial sensors. Despite previous efforts on datasets and benchmarks for HPE, few dataset exploits multiple modalities and focuses on home-based health monitoring. To bridge the gap, we present mRI, a multi-modal 3D human pose estimation dataset with mmWave, RGB-D, and Inertial Sensors. Our dataset consists of over 160k synchronized frames from 20 subjects performing rehabilitation exercises and supports the benchmarks of HPE and action detection. We perform extensive experiments using our dataset and delineate the strength of each modality. We hope that the release of mRI can catalyze the research in pose estimation, multi-modal learning, and action understanding, and more importantly facilitate the applications of home-based health monitoring. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Sizhe An;Yin Li;Umit Ogras,True,https://openreview.net/pdf?id=Oa2-cdfBxun
ObD_o92z4p,LIPS - Learning Industrial Physical Simulation benchmark suite,"Physical simulations are at the core of many critical industrial systems. However, today's physical simulators  have some limitations such as computation time, dealing with missing or uncertain data, or even  non-convergence for some feasible cases. Recently, the use of data-driven approaches to learn complex physical simulations has been considered as a promising approach to address those issues. However, this comes often at the cost of some accuracy which may hinder the industrial use. To drive this new research topic towards a better real-world applicability, we propose a new benchmark suite ""Learning Industrial Physical Simulations""(LIPS) to meet the need of developing efficient, industrial application-oriented, augmented simulators. To define how to assess such benchmark performance, we propose a set of four generic categories of criteria. The proposed benchmark suite is a modular and configurable framework that can deal with different physical problems. To demonstrate this ability, we propose in this paper to investigate two distinct use-cases with different physical simulations, namely: the power grid and the pneumatic. For each use case, several benchmarks are described and assessed with existing models. None of the models perform well under all expected criteria, inviting the community to develop  new industry-applicable solutions and possibly showcase their performance publicly upon online LIPS instance on Codabench.",Datasets & Benchmarks,NeurIPS,2022,Poster,Milad Leyli-abadi;Antoine Marot;Jérôme Picault;David Danan;Mouadh Yagoubi;Benjamin Donnot;Seif-Eddine Attoui;Pavel Dimitrov;Asma Farjallah;Clement Etienam,True,https://openreview.net/pdf?id=ObD_o92z4p
OxFoLTKDcNm,Communicating Natural Programs to Humans and Machines,"The Abstraction and Reasoning Corpus (ARC) is a set of procedural tasks that tests an agent's ability to flexibly solve novel problems. While most ARC tasks are easy for humans, they are challenging for state-of-the-art AI. What makes building intelligent systems that can generalize to novel situations such as ARC difficult? We posit that the answer might be found by studying the difference of $\\\\textit{language}$: While humans readily generate and interpret instructions in a general language, computer systems are shackled to a narrow domain-specific language that they can precisely execute. We present LARC, the $\\\\textit{Language-complete ARC}$: a collection of natural language descriptions by a group of human participants  who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88\\\\% of the ARC tasks. We analyze the collected instructions as `natural programs', finding that while they resemble computer programs, they are distinct in two ways: First, they contain a wide range of primitives; Second, they frequently leverage communicative strategies beyond directly executable codes. We demonstrate that these two distinctions prevent current program synthesis techniques from leveraging LARC to its full potential, and give concrete suggestions on how to build the next-generation program synthesizers.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Sam Acquaviva;Yewen Pu;Marta Kryven;Theodoros Sechopoulos;Catherine Wong;Gabrielle Ecanow;Maxwell Nye;Michael Henry Tessler;Joshua B. Tenenbaum,True,https://openreview.net/pdf?id=OxFoLTKDcNm
PfuW84q25y9,Tenrec: A Large-scale Multipurpose Benchmark Dataset for Recommender Systems,"Existing benchmark datasets for recommender systems (RS)  either are created  at a small scale or involve very limited forms of user feedback. RS models evaluated on such datasets often lack practical values for large-scale real-world applications. In this paper, we describe Tenrec, a novel and publicly available data collection for RS that records various user feedback from four different recommendation scenarios. To be specific, Tenrec has the following five characteristics: (1) it is large-scale, containing around 5 million users and 140 million interactions; (2) it has not only positive user feedback, but also true  negative feedback (vs. one-class recommendation); (3) it contains overlapped users and items across four different scenarios; (4) it contains various types of  user positive feedback, in forms of clicking, liking, sharing, and following, etc; (5) it contains additional features beyond the user IDs and item IDs. We verify Tenrec on ten diverse  recommendation  tasks by running several classical baseline models per task. Tenrec has the potential to become a  useful benchmark dataset for a majority of popular recommendation tasks.  Our source codes and datasets will be included  in supplementary materials.",Datasets & Benchmarks,NeurIPS,2022,Poster,Guanghu Yuan;Fajie Yuan;Yudong Li;Beibei Kong;Shujie Li;Lei Chen;Min Yang;Chenyun Yu;Bo Hu;Zang Li;Yu Xu;Xiaohu Qie,True,https://openreview.net/pdf?id=PfuW84q25y9
PfyWdxM-S4N,xView3-SAR: Detecting Dark Fishing Activity Using Synthetic Aperture Radar Imagery,"Unsustainable fishing practices worldwide pose a major threat to marine resources and ecosystems. Identifying vessels that do not show up in conventional monitoring systems---known as ``dark vessels''---is key to managing and securing the health of marine environments. With the rise of satellite-based synthetic aperture radar (SAR) imaging and modern machine learning (ML), it is now possible to automate detection of dark vessels day or night, under all-weather conditions. SAR images, however, require a domain-specific treatment and are not widely accessible to the ML community. Maritime objects (vessels and offshore infrastructure) are relatively small and sparse, challenging traditional computer vision approaches. We present the largest labeled dataset for training ML models to detect and characterize vessels and ocean structures in SAR imagery. xView3-SAR consists of nearly 1,000 analysis-ready SAR images from the Sentinel-1 mission that are, on average, 29,400-by-24,400 pixels each. The images are annotated using a combination of automated and manual analysis. Co-located bathymetry and wind state rasters accompany every SAR image. We also provide an overview of the xView3 Computer Vision Challenge, an international competition using xView3-SAR for ship detection and characterization at large scale. We release the data  (\\\\href{https://iuu.xview.us/}{https://iuu.xview.us/}) and code (\\\\href{https://github.com/DIUx-xView}{https://github.com/DIUx-xView}) to support ongoing development and evaluation of ML approaches for this important application.",Datasets & Benchmarks,NeurIPS,2022,Poster,Fernando Paolo;Tsu-ting Tim Lin;Ritwik Gupta;Bryce Goodman;Nirav Patel;Daniel Kuster;David Kroodsma;Jared Dunnmon,True,https://openreview.net/pdf?id=PfyWdxM-S4N
Proso5bUa,Flare7K: A Phenomenological Nighttime Flare Removal Dataset,"Artificial lights commonly leave strong lens flare artifacts on images captured at night. Nighttime flare not only affects the visual quality but also degrades the performance of vision algorithms. Existing flare removal methods mainly focus on removing daytime flares and fail in nighttime. Nighttime flare removal is challenging because of the unique luminance and spectrum of artificial lights and the diverse patterns and image degradation of the flares captured at night. The scarcity of nighttime flare removal datasets limits the research on this crucial task. In this paper, we introduce, Flare7K, the first nighttime flare removal dataset, which is generated based on the observation and statistics of real-world nighttime lens flares. It offers 5,000 scattering and 2,000 reflective flare images, consisting of 25 types of scattering flares and 10 types of reflective flares. The 7,000 flare patterns can be randomly added to flare-free images, forming the flare-corrupted and flare-free image pairs. With the paired data, we can train deep models to restore flare-corrupted images taken in the real world effectively. Apart from abundant flare patterns, we also provide rich annotations, including the labeling of light source, glare with shimmer, reflective flare, and streak, which are commonly absent from existing datasets. Hence, our dataset can facilitate new work in nighttime flare removal and more fine-grained analysis of flare patterns. Extensive experiments show that our dataset adds diversity to existing flare datasets and pushes the frontier of nighttime flare removal.",Datasets & Benchmarks,NeurIPS,2022,Poster,Yuekun Dai;Chongyi Li;Shangchen Zhou;Ruicheng Feng;Chen Change Loy,True,https://openreview.net/pdf?id=Proso5bUa
Pu-QtT0h2E,DeVRF: Fast Deformable Voxel Radiance Fields for Dynamic Scenes,"Modeling dynamic scenes is important for many applications such as virtual reality and telepresence. Despite achieving unprecedented fidelity for novel view synthesis in dynamic scenes, existing methods based on Neural Radiance Fields (NeRF) suffer from slow convergence (i.e., model training time measured in days). In this paper, we present DeVRF, a novel representation to accelerate learning dynamic radiance fields. The core of DeVRF is to model both the 3D canonical space and 4D deformation field of a dynamic, non-rigid scene with explicit and discrete voxel-based representations. However, it is quite challenging to train such a representation which has a large number of model parameters, often resulting in overfitting issues. To overcome this challenge, we devise a novel static-to-dynamic learning paradigm together with a new data capture setup that is convenient to deploy in practice. This paradigm unlocks efficient learning of deformable radiance fields via utilizing the 3D volumetric canonical space learnt from multi-view static images to ease the learning of 4D voxel deformation field with only few-view dynamic sequences. To further improve the efficiency of our DeVRF and its synthesized novel view's quality, we conduct thorough explorations and identify a set of strategies. We evaluate DeVRF on both synthetic and real-world dynamic scenes with different types of deformation. Experiments demonstrate that DeVRF achieves two orders of magnitude speedup (**100× faster**) with on-par high-fidelity results compared to the previous state-of-the-art approaches. The code and dataset are released in https://github.com/showlab/DeVRF.",main,NeurIPS,2022,Poster,Jia-Wei Liu;Yan-Pei Cao;Weijia Mao;Wenqiao Zhang;David Junhao Zhang;Jussi Keppo;Ying Shan;Xiaohu Qie;Mike Zheng Shou,True,https://openreview.net/pdf?id=Pu-QtT0h2E
QeuwINa96C,USB: A Unified Semi-supervised Learning Benchmark for Classification,"Semi-supervised learning (SSL) improves model generalization by leveraging massive unlabeled data to augment limited labeled samples. However, currently, popular SSL evaluation protocols are often constrained to computer vision (CV) tasks. In addition, previous work typically trains deep neural networks from scratch, which is time-consuming and environmentally unfriendly. To address the above issues, we construct a Unified SSL Benchmark (USB) for classification by selecting 15 diverse, challenging, and comprehensive tasks from CV, natural language processing (NLP), and audio processing (Audio), on which we systematically evaluate the dominant SSL methods, and also open-source a modular and extensible codebase for fair evaluation of these SSL methods. We further provide the pre-trained versions of the state-of-the-art neural models for CV tasks to make the cost affordable for further tuning. USB enables the evaluation of a single SSL algorithm on more tasks from multiple domains but with less cost. Specifically, on a single NVIDIA V100, only 39 GPU days are required to evaluate FixMatch on 15 tasks in USB while 335 GPU days (279 GPU days on 4 CV datasets except for ImageNet) are needed on 5 CV tasks with TorchSSL.",Datasets & Benchmarks,NeurIPS,2022,Poster,Yidong Wang;Hao Chen;Yue Fan;Wang SUN;Ran Tao;Wenxin Hou;Renjie Wang;Linyi Yang;Zhi Zhou;Lan-Zhe Guo;Heli Qi;Zhen Wu;Yu-Feng Li;Satoshi Nakamura;Wei Ye;Marios Savvides;Bhiksha Raj;Takahiro Shinozaki;Bernt Schiele;Jindong Wang;Xing Xie;Yue Zhang,False,https://openreview.net/pdf?id=QeuwINa96C
QgTZ56-zJou,PEER: A Comprehensive and Multi-Task Benchmark for Protein Sequence Understanding,"We are now witnessing significant progress of deep learning methods in a variety of tasks (or datasets) of proteins. However, there is a lack of a standard benchmark to evaluate the performance of different methods, which hinders the progress of deep learning in this field. In this paper, we propose such a benchmark called PEER, a comprehensive and multi-task benchmark for Protein sEquence undERstanding. PEER provides a set of diverse protein understanding tasks including protein function prediction, protein localization prediction, protein structure prediction, protein-protein interaction prediction, and protein-ligand interaction prediction. We evaluate different types of sequence-based methods for each task including traditional feature engineering approaches, different sequence encoding methods as well as large-scale pre-trained protein language models. In addition, we also investigate the performance of these methods under the multi-task learning setting. Experimental results show that large-scale pre-trained protein language models achieve the best performance for most individual tasks, and jointly training multiple tasks further boosts the performance. The datasets and source codes of this benchmark will be open-sourced soon.",Datasets & Benchmarks,NeurIPS,2022,Poster,Minghao Xu;Zuobai Zhang;Jiarui Lu;Zhaocheng Zhu;Yangtian Zhang;Chang Ma;Runcheng Liu;Jian Tang,False,https://openreview.net/pdf?id=QgTZ56-zJou
R9KnuFlvnU,WebShop: Towards Scalable Real-World Web Interaction with Grounded Language Agents,"Most existing benchmarks for grounding language in interactive environments either lack realistic linguistic elements, or prove difficult to scale up due to substantial human involvement in the collection of data or feedback signals. We develop WebShop – a simulated e-commerce website environment with 1.18 million real-world products and 12,087 crowd-sourced text instructions. In this environment, an agent needs to navigate multiple types of webpages and issue diverse actions to find, customize, and purchase a product given an instruction. WebShop provides several challenges including understanding compositional instructions, query (re-)formulation, dealing with noisy text in webpages, and performing strategic exploration. We collect over 1,600 human trajectories to first validate the benchmark, then train and evaluate a diverse range of agents using reinforcement learning, imitation learning, and pre-trained image and language models. Our best model achieves a task success rate of 29%, which significantly outperforms rule heuristics but is far lower than expert human performance (59%). We also analyze agent and human trajectories and ablate various model components to provide insights for developing future agents with stronger language understanding and decision making abilities. Finally, we show our agent trained on WebShop exhibits non-trivial sim-to-real transfer when evaluated on amazon.com and ebay.com, indicating the potential value of our benchmark for developing practical web agents that can operate in the wild.",main,NeurIPS,2022,Poster,Shunyu Yao;Howard Chen;John Yang;Karthik R Narasimhan,True,https://openreview.net/pdf?id=R9KnuFlvnU
RW-OOBU11xl,Forecasting Human Trajectory from Scene History,"Predicting the future trajectory of a person remains a challenging problem, due to randomness and subjectivity. However, the moving patterns of human in constrained scenario typically conform to a limited number of regularities to a certain extent, because of the scenario restrictions (\\\\eg, floor plan, roads and obstacles) and person-person or person-object interactivity. Thus, an individual person in this scenario should follow one of the regularities as well. In other words, a person's subsequent trajectory has likely been traveled by others. Based on this hypothesis, we propose to forecast a person's future trajectory by learning from the implicit scene regularities. We call the regularities, inherently derived from the past dynamics of the people and the environment in the scene,  \\\\emph{scene history}. We categorize scene history information into two types: historical group trajectories and individual-surroundings interaction. To exploit these information for trajectory prediction, we propose a novel framework Scene History Excavating Network (SHENet), where the scene history is leveraged in a simple yet effective approach. In particular, we design two components, the group trajectory bank module to extract representative group trajectories as the candidate for future path, and the cross-modal interaction module to model the interaction between individual past trajectory and its surroundings for trajectory refinement, respectively.  In addition, to mitigate the uncertainty in the evaluation, caused by the aforementioned randomness and subjectivity, we propose to include smoothness into evaluation metrics. We conduct extensive evaluations to validate the efficacy of proposed framework on ETH, UCY, as well as a new, challenging benchmark dataset PAV, demonstrating superior performance compared to state-of-the-art methods.",main,NeurIPS,2022,Poster,Mancheng Meng;Ziyan Wu;Terrence Chen;Xiran Cai;Xiang Sean Zhou;Fan Yang;Dinggang Shen,True,https://openreview.net/pdf?id=RW-OOBU11xl
SKE_J-B3e9X,CAESAR: An Embodied Simulator for Generating Multimodal Referring Expression Datasets,"Humans naturally use verbal utterances and nonverbal gestures to refer to various objects (known as $\\\\textit{referring expressions}$) in different interactional scenarios. As collecting real human interaction datasets are costly and laborious, synthetic datasets are often used to train models to unambiguously detect relationships among objects. However, existing synthetic data generation tools that provide referring expressions generally neglect nonverbal gestures. Additionally, while a few small-scale datasets contain multimodal cues (verbal and nonverbal), these datasets only capture the nonverbal gestures from an exo-centric perspective (observer). As models can use complementary information from multimodal cues to recognize referring expressions, generating multimodal data from multiple views can help to develop robust models. To address these critical issues, in this paper, we present a novel embodied simulator, CAESAR, to generate multimodal referring expressions containing both verbal utterances and nonverbal cues captured from multiple views. Using our simulator, we have generated two large-scale embodied referring expression datasets, which we have released publicly. We have conducted experimental analyses on embodied spatial relation grounding using various state-of-the-art baseline models. Our experimental results suggest that visual perspective affects the models' performance; and that nonverbal cues improve spatial relation grounding accuracy. Finally, we will release the simulator publicly to allow researchers to generate new embodied interaction datasets.",Datasets & Benchmarks,NeurIPS,2022,Poster,Md Mofijul Islam;Reza Manuel Mirzaiee;Alexi Gladstone;Haley N Green;Tariq Iqbal,True,https://openreview.net/pdf?id=SKE_J-B3e9X
SyoUVEyzJbE,MATE: Benchmarking Multi-Agent Reinforcement Learning in Distributed Target Coverage Control,"We introduce the Multi-Agent Tracking Environment (MATE), a novel multi-agent environment simulates the target coverage control problems in the real world. MATE hosts an asymmetric cooperative-competitive game consisting of two groups of learning agents--""cameras"" and ""targets""--with opposing interests. Specifically, ""cameras"", a group of directional sensors, are mandated to actively control the directional perception area to maximize the coverage rate of targets. On the other side, ""targets"" are mobile agents that aim to transport cargo between multiple randomly assigned warehouses while minimizing the exposure to the camera sensor networks. To showcase the practicality of MATE, we benchmark the multi-agent reinforcement learning (MARL) algorithms from different aspects, including cooperation, communication, scalability, robustness, and asymmetric self-play. We start by reporting results for cooperative tasks using MARL algorithms (MAPPO, IPPO, QMIX, MADDPG) and the results after augmenting with multi-agent communication protocols (TarMAC, I2C). We then evaluate the effectiveness of the popular self-play techniques (PSRO, fictitious self-play) in an asymmetric zero-sum competitive game. This process of co-evolution between cameras and targets helps to realize a less exploitable camera network. We also observe the emergence of different roles of the target agents while incorporating I2C into target-target communication. MATE is written purely in Python and integrated with OpenAI Gym API to enhance user-friendliness. Our project is released at https://github.com/UnrealTracking/mate.",Datasets & Benchmarks,NeurIPS,2022,Poster,Xuehai Pan;Mickel Liu;fangwei zhong;Yaodong Yang;Song-Chun Zhu;Yizhou Wang,True,https://openreview.net/pdf?id=SyoUVEyzJbE
T7114JzrwB,ZeroC: A Neuro-Symbolic Model for Zero-shot Concept Recognition and Acquisition at Inference Time,"Humans have the remarkable ability to recognize and acquire novel visual concepts in a zero-shot manner. Given a high-level, symbolic description of a novel concept in terms of previously learned visual concepts and their relations, humans can recognize novel concepts without seeing any examples. Moreover, they can acquire new concepts by parsing and communicating symbolic structures using learned visual concepts and relations. Endowing these capabilities in machines is pivotal in improving their generalization capability at inference time. In this work, we introduce Zero-shot Concept Recognition and Acquisition (ZeroC), a neuro-symbolic architecture that can recognize and acquire novel concepts in a zero-shot way.  ZeroC represents concepts as graphs of constituent concept models (as nodes) and their relations (as edges). To allow inference time composition, we employ energy-based models (EBMs) to model concepts and relations. We design ZeroC architecture so that it allows a one-to-one mapping between a symbolic graph structure of a concept and its corresponding EBM, which for the first time, allows acquiring new concepts, communicating its graph structure, and applying it to classification and detection tasks (even across domains) at inference time. We introduce algorithms for learning and inference with ZeroC. We evaluate ZeroC on a challenging grid-world dataset which is designed to probe zero-shot concept recognition and acquisition, and demonstrate its capability.",main,NeurIPS,2022,Poster,Tailin Wu;Megan Tjandrasuwita;Zhengxuan Wu;Xuelin Yang;Kevin Liu;Rok Sosic;Jure Leskovec,True,https://openreview.net/pdf?id=T7114JzrwB
TG8KACxEON,Training language models to follow instructions with human feedback,"Making language models bigger does not inherently make them better at following a user's intent. For example, large language models can generate outputs that are untruthful, toxic, or simply not helpful to the user. In other words, these models are not aligned with their users. In this paper, we show an avenue for aligning language models with user intent on a wide range of tasks by fine-tuning with human feedback. Starting with a set of labeler-written prompts and prompts submitted through a language model API, we collect a dataset of labeler demonstrations of the desired model behavior, which we use to fine-tune GPT-3 using supervised learning. We then collect a dataset of rankings of model outputs, which we use to further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT. In human evaluations on our prompt distribution, outputs from the 1.3B parameter InstructGPT model are preferred to outputs from the 175B GPT-3, despite having 100x fewer parameters. Moreover, InstructGPT models show improvements in truthfulness and reductions in toxic output generation while having minimal performance regressions on public NLP datasets. Even though InstructGPT still makes simple mistakes, our results show that fine-tuning with human feedback is a promising direction for aligning language models with human intent.",main,NeurIPS,2022,Poster,Long Ouyang;Jeffrey Wu;Xu Jiang;Diogo Almeida;Carroll Wainwright;Pamela Mishkin;Chong Zhang;Sandhini Agarwal;Katarina Slama;Alex Gray;John Schulman;Jacob Hilton;Fraser Kelton;Luke Miller;Maddie Simens;Amanda Askell;Peter Welinder;Paul Christiano;Jan Leike;Ryan Lowe,True,https://openreview.net/pdf?id=TG8KACxEON
TTM7iEFOTzJ,EpiGRAF: Rethinking training of 3D GANs,"A recent trend in generative modeling is building 3D-aware generators from 2D image collections. To induce the 3D bias, such models typically rely on volumetric rendering, which is expensive to employ at high resolutions. Over the past months, more than ten works have addressed this scaling issue by training a separate 2D decoder to upsample a low-resolution image (or a feature tensor) produced from a pure 3D generator.  But this solution comes at a cost: not only does it break multi-view consistency (i.e., shape and texture change when the camera moves), but it also learns geometry in low fidelity. In this work, we show that obtaining a high-resolution 3D generator with SotA image quality is possible by following a completely different route of simply training the model patch-wise. We revisit and improve this optimization scheme in two ways. First, we design a location- and scale-aware discriminator to work on patches of different proportions and spatial positions. Second, we modify the patch sampling strategy based on an annealed beta distribution to stabilize training and accelerate the convergence. The resulting model, named EpiGRAF, is an efficient, high-resolution, pure 3D generator, and we test it on four datasets (two introduced in this work) at \\\\(256^2\\\\) and \\\\(512^2\\\\) resolutions. It obtains state-of-the-art image quality, high-fidelity geometry and trains \\\\({\\\\approx}\\\\)2.5 faster than the upsampler-based counterparts. Code/data/visualizations: https://universome.github.io/epigraf.",main,NeurIPS,2022,Poster,Ivan Skorokhodov;Sergey Tulyakov;Yiqun Wang;Peter Wonka,True,https://openreview.net/pdf?id=TTM7iEFOTzJ
TaARsI_Iio,A Multi-Task Benchmark for Korean Legal Language Understanding and Judgement Prediction,"The recent advances of deep learning have dramatically changed how machine learning, especially in the domain of natural language processing, can be applied to legal domain. However, this shift to the data-driven approaches calls for larger and more diverse datasets, which are nevertheless still small in number, especially in non-English languages. Here we present the first large-scale benchmark of Korean legal AI datasets, LBOX OPEN, that consists of one legal corpus, two classification tasks, two legal judgement prediction (LJP) tasks, and one summarization task. The legal corpus consists of 147k Korean precedents (259M tokens), of which 63k are sentenced in last 4 years and 96k are from the first and the second level courts in which factual issues are reviewed. The two classification tasks are case names (11.3k) and statutes (2.8k) prediction from the factual description of individual cases. The LJP tasks consist of (1) 10.5k criminal examples where the model is asked to predict fine amount, imprisonment with labor, and imprisonment without labor ranges for the given facts, and (2) 4.7k civil examples where the inputs are facts and claim for relief and outputs are the degrees of claim acceptance. The summarization task consists of the Supreme Court precedents and the corresponding summaries (20k). We also release realistic variants of the datasets by extending the domain (1) to infrequent case categories in case name (31k examples) and statute (17.7k) classification tasks, and (2) to long input sequences in the summarization task (51k). Finally, we release LCUBE, the first Korean legal language model trained on the legal corpus from this study. Given the uniqueness of the Law of South Korea and the diversity of the legal tasks covered in this work, we believe that LBOX OPEN contributes to the multilinguality of global legal research. LBOX OPEN and LCUBE will be publicly available.",Datasets & Benchmarks,NeurIPS,2022,Poster,Wonseok Hwang;Dongjun Lee;Kyoungyeon Cho;Hanuhl Lee;Minjoon Seo,True,https://openreview.net/pdf?id=TaARsI_Iio
TscdNx8udf5,SMPL: Simulated Industrial Manufacturing and Process Control Learning Environments,"Traditional biological and pharmaceutical manufacturing plants are controlled by human workers or pre-defined thresholds. Modernized factories have advanced process control algorithms such as model predictive control (MPC). However, there is little exploration of applying deep reinforcement learning to control manufacturing plants. One of the reasons is the lack of high fidelity simulations and standard APIs for benchmarking. To bridge this gap, we develop an easy-to-use library that includes five high-fidelity simulation environments: BeerFMTEnv, ReactorEnv, AtropineEnv, PenSimEnv and mAbEnv, which cover a wide range of manufacturing processes. We build these environments on published dynamics models. Furthermore, we benchmark online and offline, model-based and model-free reinforcement learning algorithms for comparisons of follow-up research.",Datasets & Benchmarks,NeurIPS,2022,Poster,Mohan Zhang;Xiaozhou Wang;Benjamin Decardi-Nelson;Song Bo;An Zhang;Jinfeng Liu;Sile Tao;Jiayi Cheng;Xiaohong Liu;Dengdeng Yu;Matthew Poon;Animesh Garg,False,https://openreview.net/pdf?id=TscdNx8udf5
TzNuIdrHoU,Avalon: A Benchmark for RL Generalization Using Procedurally Generated Worlds,"Despite impressive successes, deep reinforcement learning (RL) systems still fall short of human performance on generalization to new tasks and environments that differ from their training. As a benchmark tailored for studying RL generalization, we introduce Avalon, a set of tasks in which embodied agents in highly diverse procedural 3D worlds must survive by navigating terrain, hunting or gathering food, and avoiding hazards. Avalon is unique among existing RL benchmarks in that the reward function, world dynamics, and action space are the same for every task, with tasks differentiated solely by altering the environment; its 20 tasks, ranging in complexity from eat and throw to hunt and navigate, each create worlds in which the agent must perform specific skills in order to survive. This setup enables investigations of generalization within tasks, between tasks, and to compositional tasks that require combining skills learned from previous tasks. Avalon includes a highly efficient simulator, a library of baselines, and a benchmark with scoring metrics evaluated against hundreds of hours of human performance, all of which are open-source and publicly available. We find that standard RL baselines make progress on most tasks but are still far from human performance, suggesting Avalon is challenging enough to advance the quest for generalizable RL.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Joshua Albrecht;Abraham J Fetterman;Bryden Fogelman;Ellie Kitanidis;Bartosz Wróblewski;Nicole Seo;Michael Rosenthal;Maksis Knutins;Zachary Polizzi;James B Simon;Kanjun Qiu,True,https://openreview.net/pdf?id=TzNuIdrHoU
UXPXs-OYbks,Robustness Disparities in Face Detection,"Facial analysis systems have been deployed by large companies and critiqued by scholars and activists for the past decade. Many existing algorithmic audits examine the performance of these systems on later stage elements of facial analysis systems like facial recognition and age, emotion, or perceived gender prediction; however, a core component to these systems has been vastly understudied from a fairness perspective: face detection, sometimes called face localization. Since face detection is a pre-requisite step in facial analysis systems, the bias we observe in face detection will flow downstream to the other components like facial recognition and emotion prediction. Additionally, no prior work has focused on the robustness of these systems under various perturbations and corruptions, which leaves open the question of how various people are impacted by these phenomena. We present the first of its kind detailed benchmark of face detection systems, specifically examining the robustness to noise of commercial and academic models. We use both standard and recently released academic facial datasets to quantitatively analyze trends in face detection robustness. Across all the datasets and systems, we generally find that photos of individuals who are masculine presenting, older, of darker skin type, or have dim lighting are more susceptible to errors than their counterparts in other identities.",Datasets & Benchmarks,NeurIPS,2022,Poster,Samuel Dooley;George Z Wei;Tom Goldstein;John P Dickerson,False,https://openreview.net/pdf?id=UXPXs-OYbks
UiRSQykVNiC,Myriad: a real-world testbed to bridge trajectory optimization and deep learning,"We present Myriad, a testbed written in JAX which enables machine learning researchers to benchmark imitation learning and reinforcement learning algorithms against trajectory optimization-based methods in real-world environments. Myriad contains 17 optimal control problems presented in continuous time which span medicine, ecology, epidemiology, and engineering. As such, Myriad strives to serve as a stepping stone towards application of modern machine learning techniques for impactful real-world tasks. The repository also provides machine learning practitioners access to trajectory optimization techniques, not only for standalone use, but also for integration within a typical automatic differentiation workflow. Indeed, the combination of classical control theory and deep learning in a fully GPU-compatible package unlocks potential for new algorithms to arise. We present one such novel approach for use in dynamics learning and control tasks. Trained in a fully end-to-end fashion, our model leverages an implicit planning module over neural ordinary differential equations, enabling simultaneous learning and planning with unknown environment dynamics. All environments, optimizers and tools are available in the software package at \\\\url{https://github.com/nikihowe/myriad}.",Datasets & Benchmarks,NeurIPS,2022,Poster,Nikolaus H. R. Howe;Simon Dufort-Labbé;Nitarshan Rajkumar;Pierre-Luc Bacon,True,https://openreview.net/pdf?id=UiRSQykVNiC
UoEw6KigkUn,The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset,"As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Hugo Laurençon;Lucile Saulnier;Thomas Wang;Christopher Akiki;Albert Villanova del Moral;Teven Le Scao;Leandro Von Werra;Chenghao Mou;Eduardo González Ponferrada;Huu Nguyen;Jörg Frohberg;Mario Šaško;Quentin Lhoest;Angelina McMillan-Major;Gérard Dupont;Stella Biderman;Anna Rogers;Loubna Ben allal;Francesco De Toni;Giada Pistilli;Olivier Nguyen;Somaieh Nikpoor;Maraim Masoud;Pierre Colombo;Javier de la Rosa;Paulo Villegas;Tristan Thrush;Shayne Longpre;Sebastian Nagel;Leon Weber;Manuel Romero Muñoz;Jian Zhu;Daniel Van Strien;Zaid Alyafeai;Khalid Almubarak;Vu Minh Chien;Itziar Gonzalez-Dios;Aitor Soroa;Kyle Lo;Manan Dey;Pedro Ortiz Suarez;Aaron Gokaslan;Shamik Bose;David Ifeoluwa Adelani;Long Phan;Hieu Tran;Ian Yu;Suhas Pai;Jenny Chim;Violette Lepercq;Suzana Ilic;Margaret Mitchell;Sasha Luccioni;Yacine Jernite,True,https://openreview.net/pdf?id=UoEw6KigkUn
UrAYT2QwOX8,"Turning the Tables: Biased, Imbalanced, Dynamic Tabular Datasets for ML Evaluation","Evaluating new techniques on realistic datasets plays a crucial role in the development of ML research and its broader adoption by practitioners. In recent years, there has been a significant increase of publicly available unstructured data resources for computer vision and NLP tasks. However, tabular data — which is prevalent in many high-stakes domains — has been lagging behind. To bridge this gap, we present Bank Account Fraud (BAF), the first publicly available 1 privacy-preserving, large-scale, realistic suite of tabular datasets. The suite was generated by applying state-of-the-art tabular data generation techniques on an anonymized,real-world bank account opening fraud detection dataset. This setting carries a set of challenges that are commonplace in real-world applications, including temporal dynamics and significant class imbalance. Additionally, to allow practitioners to stress test both performance and fairness of ML methods, each dataset variant of BAF contains specific types of data bias. With this resource, we aim to provide the research community with a more realistic, complete, and robust test bed to evaluate novel and existing methods.",Datasets & Benchmarks,NeurIPS,2022,Poster,Sérgio Jesus;José Pombal;Duarte Alves;André Cruz;Pedro Saleiro;Rita P. Ribeiro;João Gama;Pedro Bizarro,True,https://openreview.net/pdf?id=UrAYT2QwOX8
VF9f79cCYdZ,OpenFilter: A Framework to Democratize Research Access to Social Media AR Filters,"Augmented Reality or AR filters on selfies have become very popular on social media platforms for a variety of applications, including marketing, entertainment and aesthetics. Given the wide adoption of AR face filters and the importance of faces in our social structures and relations, there is increased interest by the scientific community to analyze the impact of such filters from a psychological, artistic and sociological perspective. However, there are few quantitative analyses in this area mainly due to a lack of publicly available datasets of facial images with applied AR filters. The proprietary, close nature of most social media platforms does not allow users, scientists and practitioners to access the code and the details of the available AR face filters. Scraping faces from these platforms to collect data is ethically unacceptable and should, therefore, be avoided in research. In this paper, we present OpenFilter, a flexible framework to apply AR filters available in social media platforms on existing large collections of human faces. Moreover, we share FairBeauty and B-LFW, two beautified versions of the publicly available FairFace and LFW datasets and we outline insights derived from the analysis of these beautified datasets. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Piera Riccio;Bill Psomas;Francesco Galati;Francisco Escolano;Thomas Hofmann;Nuria M Oliver,True,https://openreview.net/pdf?id=VF9f79cCYdZ
Vk4-HUnkEak,AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation,"Despite the considerable progress in automatic abdominal multi-organ segmentation from CT/MRI scans in recent years, a comprehensive evaluation of the models' capabilities is hampered by the lack of a large-scale benchmark from diverse clinical scenarios. Constraint by the high cost of collecting and labeling 3D medical data, most of the deep learning models to date are driven by datasets with a limited number of organs of interest or samples, which still limits the power of modern deep models and makes it difficult to provide a fully comprehensive and fair estimate of various methods. To mitigate the limitations, we present AMOS, a large-scale, diverse, clinical dataset for abdominal organ segmentation. AMOS provides 500 CT and 100 MRI scans collected from multi-center, multi-vendor, multi-modality, multi-phase, multi-disease patients, each with voxel-level annotations of 15 abdominal organs, providing challenging examples and test-bed for studying robust segmentation algorithms under diverse targets and scenarios. We further benchmark several state-of-the-art medical segmentation models to evaluate the status of the existing methods on this new challenging dataset. We have made our datasets, benchmark servers, and baselines publicly available, and hope to inspire future research. Information can be found at https://amos22.grand-challenge.org.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Yuanfeng Ji;Haotian Bai;Chongjian GE;Jie Yang;Ye Zhu;Ruimao Zhang;Zhen Li;Lingyan Zhanng;Wanling Ma;Xiang Wan;Ping Luo,True,https://openreview.net/pdf?id=Vk4-HUnkEak
VnAwNNJiwDb,Generating Long Videos of Dynamic Scenes,"We present a video generation model that accurately reproduces object motion, changes in camera viewpoint, and new content that arises over time. Existing video generation methods often fail to produce new content as a function of time while maintaining consistencies expected in real environments, such as plausible dynamics and object persistence. A common failure case is for content to never change due to over-reliance on inductive bias to provide temporal consistency, such as a single latent code that dictates content for the entire video. On the other extreme, without long-term consistency, generated videos may morph unrealistically between different scenes. To address these limitations, we prioritize the time axis by redesigning the temporal latent representation and learning long-term consistency from data by training on longer videos. We leverage a two-phase training strategy, where we separately train using longer videos at a low resolution and shorter videos at a high resolution. To evaluate the capabilities of our model, we introduce two new benchmark datasets with explicit focus on long-term temporal dynamics.",main,NeurIPS,2022,Poster,Tim Brooks;Janne Hellsten;Miika Aittala;Ting-chun Wang;Timo Aila;Jaakko Lehtinen;Ming-Yu Liu;Alexei A Efros;Tero Karras,True,https://openreview.net/pdf?id=VnAwNNJiwDb
VtEEpi-dGlt,Kantorovich Strikes Back! Wasserstein GANs are not Optimal Transport?,"Wasserstein Generative Adversarial Networks (WGANs) are the popular generative models built on the theory of Optimal Transport (OT) and the Kantorovich duality. Despite the success of WGANs, it is still unclear how well the underlying OT dual solvers approximate the OT cost (Wasserstein-1 distance, W1) and the OT gradient needed to update the generator. In this paper, we address these questions. We construct 1-Lipschitz functions and use them to build ray monotone transport plans. This strategy yields pairs of continuous benchmark distributions with the analytically known OT plan, OT cost and OT gradient in high-dimensional spaces such as spaces of images. We thoroughly evaluate popular WGAN dual form solvers (gradient penalty, spectral normalization, entropic regularization, etc.) using these benchmark pairs. Even though these solvers perform well in WGANs, none of them faithfully compute W1 in high dimensions. Nevertheless, many provide a meaningful approximation of the OT gradient. These observations suggest that these solvers should not be treated as good estimators of W1 but to some extent they indeed can be used in variational problems requiring the minimization of W1.",Datasets & Benchmarks,NeurIPS,2022,Poster,Alexander Korotin;Alexander Kolesov;Evgeny Burnaev,True,https://openreview.net/pdf?id=VtEEpi-dGlt
W_bsDmzwaZ7,K-Radar: 4D Radar Object Detection for Autonomous Driving in Various Weather Conditions,"Unlike RGB cameras that use visible light bands (384∼769 THz) and Lidars that use infrared bands (361∼331 THz), Radars use relatively longer wavelength radio bands (77∼81 GHz), resulting in robust measurements in adverse weathers. Unfortunately, existing Radar datasets only contain a relatively small number of samples compared to the existing camera and Lidar datasets. This may hinder the development of sophisticated data-driven deep learning techniques for Radar-based perception. Moreover, most of the existing Radar datasets only provide 3D Radar tensor (3DRT) data that contain power measurements along the Doppler, range, and azimuth dimensions. As there is no elevation information, it is challenging to estimate the 3D bounding box of an object from 3DRT. In this work, we introduce KAIST-Radar (K-Radar), a novel large-scale object detection dataset and benchmark that contains 35K frames of 4D Radar tensor (4DRT) data with power measurements along the Doppler, range, azimuth, and elevation dimensions, together with carefully annotated 3D bounding box labels of objects on the roads. K-Radar includes challenging driving conditions such as adverse weathers (fog, rain, and snow) on various road structures (urban, suburban roads, alleyways, and highways). In addition to the 4DRT, we provide auxiliary measurements from carefully calibrated high-resolution Lidars, surround stereo cameras, and RTK-GPS. We also provide 4DRT-based object detection baseline neural networks (baseline NNs) and show that the height information is crucial for 3D object detection. And by comparing the baseline NN with a similarly-structured Lidar-based neural network, we demonstrate that 4D Radar is a more robust sensor for adverse weather conditions. All codes are available at https://github.com/kaist-avelab/k-radar.",Datasets & Benchmarks,NeurIPS,2022,Poster,Dong-Hee Paek;Seung-Hyun Kong;Kevin Tirta Wijaya,True,https://openreview.net/pdf?id=W_bsDmzwaZ7
WrIrYMCZgbb,Exploiting Semantic Relations for Glass Surface Detection,"Glass surfaces are omnipresent in our daily lives and often go unnoticed by the majority of us. While humans are generally able to infer their locations and thus avoid collisions, it can be difficult for current object detection systems to handle them due to the transparent nature of glass surfaces. Previous methods approached the problem by extracting global context information to obtain priors such as object boundaries and reflections. However, their performances cannot be guaranteed when these deterministic features are not available. We observe that humans often reason through the semantic context of the environment, which offers insights into the categories of and proximity between entities that are expected to appear in the surrounding. For example, the odds of co-occurrence of glass windows with walls and curtains are generally higher than that with other objects such as cars and trees, which have relatively less semantic relevance. Based on this observation, we propose a model ('GlassSemNet') that integrates the contextual relationship of the scenes for glass surface detection with two novel modules: (1) Scene Aware Activation (SAA) Module to adaptively filter critical channels with respect to spatial and semantic features, and (2) Context Correlation Attention (CCA) Module to progressively learn the contextual correlations among objects both spatially and semantically. In addition, we propose a large-scale glass surface detection dataset named {\\\\it Glass Surface Detection - Semantics} ('GSD-S'), which contains 4,519 real-world RGB glass surface images from diverse real-world scenes with detailed annotations for both glass surface detection and semantic segmentation. Experimental results show that our model outperforms contemporary works, especially with 42.6\\\\% MAE improvement on our proposed GSD-S dataset. Code, dataset, and models are available at https://jiaying.link/neurips2022-gsds/",main,NeurIPS,2022,Poster,Jiaying Lin;Yuen Hei Yeung;Rynson W. H. Lau,True,https://openreview.net/pdf?id=WrIrYMCZgbb
X0m9q0IcsmX,ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints,"Recent studies have demonstrated that visual recognition models lack robustness to distribution shift. However, current work mainly considers model robustness to 2D image transformations, leaving viewpoint changes in the 3D world less explored. In general, viewpoint changes are prevalent in various real-world applications (e.g., autonomous driving), making it imperative to evaluate viewpoint robustness. In this paper, we propose a novel method called ViewFool to find adversarial viewpoints that mislead visual recognition models. By encoding real-world objects as neural radiance fields (NeRF), ViewFool characterizes a distribution of diverse adversarial viewpoints under an entropic regularizer, which helps to handle the fluctuations of the real camera pose and mitigate the reality gap between the real objects and their neural representations. Experiments validate that the common image classifiers are extremely vulnerable to the generated adversarial viewpoints, which also exhibit high cross-model transferability. Based on ViewFool, we introduce ImageNet-V, a new out-of-distribution dataset for benchmarking viewpoint robustness of image classifiers. Evaluation results on 40 classifiers with diverse architectures, objective functions, and data augmentations reveal a significant drop in model performance when tested on ImageNet-V, which provides a possibility to leverage ViewFool as an effective data augmentation strategy to improve viewpoint robustness.",main,NeurIPS,2022,Poster,Yinpeng Dong;Shouwei Ruan;Hang Su;Caixin Kang;Xingxing Wei;Jun Zhu,True,https://openreview.net/pdf?id=X0m9q0IcsmX
X2dHozbd1at,3DOS: Towards 3D Open Set Learning - Benchmarking and Understanding Semantic Novelty Detection on Point Clouds,"In recent years there has been significant progress in the field of 3D learning on classification, detection and segmentation problems. The vast majority of the existing studies focus on canonical closed-set conditions, neglecting the intrinsic open nature of the real-world. This limits the abilities of robots and autonomous systems involved in safety-critical applications that require managing novel and unknown signals. In this context exploiting 3D data can be a valuable asset since it provides rich information about the geometry of perceived objects and scenes. 
With this paper we provide the first broad study on 3D Open Set learning. We introduce 3DOS: a novel testbed for semantic novelty detection that considers several settings with increasing difficulties in terms of semantic (category) shift, and covers both in-domain (synthetic-to-synthetic, real-to-real) and cross-domain (synthetic-to-real) scenarios. Moreover, we investigate the related 2D Open Set literature to understand if and how its recent improvements are effective on 3D data. Our extensive benchmark positions several algorithms in the same coherent picture, revealing their strengths and limitations. The results of our analysis may serve as a reliable foothold for future tailored 3D Open Set methods.",Datasets & Benchmarks,NeurIPS,2022,Poster,Antonio Alliegro;Francesco Cappio Borlino;Tatiana Tommasi,True,https://openreview.net/pdf?id=X2dHozbd1at
Y6A4-R_Hgsw,Toward a realistic model of speech processing in the brain with self-supervised learning,"Several deep neural networks have recently been shown to generate activations similar to those of the brain in response to the same input. These algorithms, however, remain largely implausible: they require (1) extraordinarily large amounts of data, (2) unobtainable supervised labels, (3) textual rather than raw sensory input, and / or (4) implausibly large memory (e.g. thousands of contextual words). These elements highlight the need to identify algorithms that, under these limitations, would suffice to account for both behavioral and brain responses. Focusing on speech processing, we here hypothesize that self-supervised algorithms trained on the raw waveform constitute a promising candidate. Specifically, we compare a recent self-supervised model, wav2vec 2.0, to the brain activity of 412 English, French, and Mandarin individuals recorded with functional Magnetic Resonance Imaging (fMRI), while they listened to approximately one hour of audio books. First, we show that this algorithm learns brain-like representations with as little as 600 hours of unlabelled speech -- a quantity comparable to what infants can be exposed to during language acquisition. Second, its functional hierarchy aligns with the cortical hierarchy of speech processing. Third, different training regimes reveal a functional specialization akin to the cortex: wav2vec 2.0 learns sound-generic, speech-specific and language-specific representations similar to those of the prefrontal and temporal cortices. Fourth, we confirm the similarity of this specialization with the behavior of 386 additional participants. These elements, resulting from the largest neuroimaging benchmark to date, show how self-supervised learning can account for a rich organization of speech processing in the brain, and thus delineate a path to identify the laws of language acquisition which shape the human brain.",main,NeurIPS,2022,Poster,Juliette MILLET;Charlotte Caucheteux;Pierre Orhan;Yves Boubenec;Alexandre Gramfort;Ewan Dunbar;Christophe Pallier;Jean-Remi King,True,https://openreview.net/pdf?id=Y6A4-R_Hgsw
YVXaxB6L2Pl,The Surprising Effectiveness of PPO in Cooperative Multi-Agent Games,"Proximal Policy Optimization (PPO) is a ubiquitous on-policy reinforcement learning algorithm but is significantly less utilized than off-policy learning algorithms in multi-agent settings. This is often due to the belief that PPO is significantly less sample efficient than off-policy methods in multi-agent systems. In this work, we carefully study the performance of PPO in cooperative multi-agent settings. We show that PPO-based multi-agent algorithms achieve surprisingly strong performance in four popular multi-agent testbeds: the particle-world environments, the StarCraft multi-agent challenge, the Hanabi challenge, and Google Research Football, with minimal hyperparameter tuning and without any domain-specific algorithmic modifications or architectures. Importantly, compared to competitive off-policy methods, PPO often achieves competitive or superior results in both final returns and sample efficiency. Finally, through ablation studies, we analyze implementation and hyperparameter factors that are critical to PPO's empirical performance, and give concrete practical suggestions regarding these factors. Our results show that when using these practices, simple PPO-based methods are a strong baseline in cooperative multi-agent reinforcement learning. Source code is released at https://github.com/marlbenchmark/on-policy.",Datasets & Benchmarks,NeurIPS,2022,Poster,Chao Yu;Akash Velu;Eugene Vinitsky;Jiaxuan Gao;Yu Wang;Alexandre Bayen;Yi Wu,False,https://openreview.net/pdf?id=YVXaxB6L2Pl
YXvGXEmtZ5N,BOND: Benchmarking Unsupervised Outlier Node Detection on Static Attributed Graphs,"Detecting which nodes in graphs are outliers is a relatively new machine learning task with numerous applications. Despite the proliferation of algorithms developed in recent years for this task, there has been no standard comprehensive setting for performance evaluation. Consequently, it has been difficult to understand which methods work well and when under a broad range of settings. To bridge this gap, we present—to the best of our knowledge—the first comprehensive benchmark for unsupervised outlier node detection on static attributed graphs called BOND, with the following highlights. (1) We benchmark the outlier detection performance of 14 methods ranging from classical matrix factorization to the latest graph neural networks. (2) Using nine real datasets, our benchmark assesses how the different detection methods respond to two major types of synthetic outliers and separately to “organic” (real non-synthetic) outliers. (3) Using an existing random graph generation technique, we produce a family of synthetically generated datasets of different graph sizes that enable us to compare the running time and memory usage of the different outlier detection algorithms. Based on our experimental results, we discuss the pros and cons of existing graph outlier detection algorithms, and we highlight opportunities for future research. Importantly, our code is freely available and meant to be easily extendable: https://github.com/pygod-team/pygod/tree/main/benchmark",Datasets & Benchmarks,NeurIPS,2022,Poster,Kay Liu;Yingtong Dou;Yue Zhao;Xueying Ding;Xiyang Hu;Ruitong Zhang;Kaize Ding;Canyu Chen;Hao Peng;Kai Shu;Lichao Sun;Jundong Li;George H. Chen;Zhihao Jia;Philip S. Yu,True,https://openreview.net/pdf?id=YXvGXEmtZ5N
YmacJv0i_UR,GriddlyJS: A Web IDE for Reinforcement Learning,"Progress in reinforcement learning (RL) research is often driven by the design of new, challenging environments---a costly undertaking requiring skills orthogonal to that of a typical machine learning researcher. The complexity of environment development has only increased with the rise of procedural-content generation (PCG) as the prevailing paradigm for producing varied environments capable of testing the robustness and generalization of RL agents. Moreover, existing environments often require complex build processes, making reproducing results difficult. To address these issues, we introduce GriddlyJS, a web-based Integrated Development Environment (IDE) based on the Griddly engine. GriddlyJS allows researchers to easily design and debug arbitrary, complex PCG grid-world environments, as well as visualize, evaluate, and record the performance of trained agent models. By connecting the RL workflow to the advanced functionality enabled by modern web standards, GriddlyJS allows publishing interactive agent-environment demos that reproduce experimental results directly to the web. To demonstrate the versatility of GriddlyJS, we use it to quickly develop a complex compositional puzzle-solving environment alongside arbitrary human-designed environment configurations and their solutions for use in a automatic curriculum learning and offline RL context. The GriddlyJS IDE is open source and freely available at https://griddly.ai.",Datasets & Benchmarks,NeurIPS,2022,Poster,Christopher Bamford;Minqi Jiang;Mikayel Samvelyan;Tim Rocktäschel,False,https://openreview.net/pdf?id=YmacJv0i_UR
YxUdazpgweG,MultiScan: Scalable RGBD scanning for 3D environments with articulated objects,"We introduce MultiScan, a scalable RGBD dataset construction pipeline leveraging commodity mobile devices to scan indoor scenes with articulated objects and web-based semantic annotation interfaces to efficiently annotate object and part semantics and part mobility parameters. We use this pipeline to collect 273 scans of 117 indoor scenes containing 10957 objects and 5129 parts. The resulting MultiScan dataset provides RGBD streams with per-frame camera poses, textured 3D surface meshes, richly annotated part-level and object-level semantic labels, and part mobility parameters. We validate our dataset on instance segmentation and part mobility estimation tasks and benchmark methods for these tasks from prior work. Our experiments show that part segmentation and mobility estimation in real 3D scenes remain challenging despite recent progress in 3D object segmentation.",main,NeurIPS,2022,Poster,Yongsen Mao;Yiming Zhang;Hanxiao Jiang;Angel X Chang;Manolis Savva,True,https://openreview.net/pdf?id=YxUdazpgweG
Z0s5T89qfjc,ENS-10: A Dataset For Post-Processing Ensemble Weather Forecasts,"Post-processing ensemble prediction systems can improve the reliability of weather forecasting, especially for extreme event prediction. In recent years, different machine learning models have been developed to improve the quality of weather post-processing. However, these models require a comprehensive dataset of weather simulations to produce high-accuracy results, which comes at a high computational cost to generate. This paper introduces the ENS-10 dataset, consisting of ten ensemble members spanning 20 years (1998--2017). The ensemble members are generated by perturbing numerical weather simulations to capture the chaotic behavior of the Earth. To represent the three-dimensional state of the atmosphere, ENS-10 provides the most relevant atmospheric variables at 11 distinct pressure levels and the surface at \\\\ang{0.5} resolution for forecast lead times T=0, 24, and 48 hours (two data points per week). We propose the ENS-10 prediction correction task for improving the forecast quality at a 48-hour lead time through ensemble post-processing. We provide a set of baselines and compare their skill at correcting the predictions of three important atmospheric variables. Moreover, we measure the baselines' skill at improving predictions of extreme weather events using our dataset. The ENS-10 dataset is available under the Creative Commons Attribution 4.0 International (CC BY 4.0) license.",Datasets & Benchmarks,NeurIPS,2022,Poster,Saleh Ashkboos;Langwen Huang;Nikoli Dryden;Tal Ben-Nun;Peter Dominik Dueben;Lukas Gianinazzi;Luca Nicola Kummer;Torsten Hoefler,True,https://openreview.net/pdf?id=Z0s5T89qfjc
ZMrZ5SC2G3_,Towards Versatile Embodied Navigation,"With the emergence of varied visual navigation tasks (e.g., image-/object-/audio-goal and vision-language navigation) that specify the target in different ways, the community has made appealing advances in training specialized agents capable of handling individual navigation tasks well. Given plenty of embodied navigation tasks and task-specific solutions, we address a more fundamental question: can we learn a single powerful agent that masters not one but multiple navigation tasks concurrently? First, we propose VXN, a large-scale 3D dataset that instantiates~four classic navigation tasks in standardized, continuous, and audiovisual-rich environments. Second, we propose Vienna, a versatile embodied navigation agent that simultaneously learns to perform the four navigation tasks with one model. Building upon a full-attentive architecture, Vienna formulates various navigation tasks as a unified, parse-and-query procedure: the target description, augmented with four task embeddings, is comprehensively interpreted into a set of diversified goal vectors, which are refined as the navigation progresses, and used as queries to retrieve supportive context from episodic history for decision making. This enables the reuse of knowledge across navigation tasks with varying input domains/modalities. We empirically demonstrate that, compared with learning each visual navigation task individually, our multitask agent achieves comparable or even better performance with reduced complexity.",main,NeurIPS,2022,Poster,Hanqing Wang;Wei Liang;Luc Van Gool;Wenguan Wang,True,https://openreview.net/pdf?id=ZMrZ5SC2G3_
ZPUkqTf6a-P,Truly Deterministic Policy Optimization,"In this paper, we present a policy gradient method that avoids exploratory noise injection and performs policy search over the deterministic landscape, with the goal of improving learning with long horizons and non-local rewards. By avoiding noise injection all sources of estimation variance can be eliminated in systems with deterministic dynamics (up to the initial state distribution). Since deterministic policy regularization is impossible using traditional non-metric measures such as the KL divergence, we derive a Wasserstein-based quadratic model for our purposes. We state conditions on the system model under which it is possible to establish a monotonic policy improvement guarantee, propose a surrogate function for policy gradient estimation, and show that it is possible to compute exact advantage estimates if both the state transition model and the policy are deterministic. Finally, we describe two novel robotic control environments---one with non-local rewards in the frequency domain and the other with a long horizon (8000 time-steps)---for which our policy gradient method (TDPO) significantly outperforms existing methods (PPO, TRPO, DDPG, and TD3). Our implementation with all the experimental settings and a video of the physical hardware test is available at https://github.com/ehsansaleh/tdpo .",main,NeurIPS,2022,Poster,Ehsan Saleh;Saba Ghaffari;Tim Bretl;Matthew West,True,https://openreview.net/pdf?id=ZPUkqTf6a-P
ZZ3FeSSPPblo,Touch and Go: Learning from Human-Collected Vision and Touch,"The ability to associate touch with sight is essential for tasks that require physically interacting with objects in the world. We propose a dataset with paired visual and tactile data called Touch and Go, in which human data collectors probe objects in natural environments using tactile sensors, while simultaneously recording egocentric video. In contrast to previous efforts, which have largely been confined to lab settings or simulated environments, our dataset spans a large number of “in the wild” objects and scenes. We successfully apply our dataset to a variety of multimodal learning tasks: 1) self-supervised visuo-tactile feature learning, 2) tactile-driven image stylization, i.e., making the visual appearance of an object more consistent with a given tactile signal, and 3) predicting future frames of a tactile signal from visuo-tactile inputs.",Datasets & Benchmarks,NeurIPS,2022,Poster,Fengyu Yang;Chenyang Ma;Jiacheng Zhang;Jing Zhu;Wenzhen Yuan;Andrew Owens,True,https://openreview.net/pdf?id=ZZ3FeSSPPblo
ZeeswGSOw7r,IKEA-Manual: Seeing Shape Assembly Step by Step,"Human-designed visual manuals are crucial components in shape assembly activities. They provide step-by-step guidance on how we should move and connect different parts in a convenient and physically-realizable way. While there has been an ongoing effort in building agents that perform assembly tasks, the information in human-design manuals has been largely overlooked. We identify that this is due to 1) a lack of realistic 3D assembly objects that have paired manuals and 2) the difficulty of extracting structured information from purely image-based manuals. Motivated by this observation, we present IKEA-Manual, a dataset consisting of 102 IKEA objects paired with assembly manuals. We provide fine-grained annotations on the IKEA objects and assembly manuals, including decomposed assembly parts, assembly plans, manual segmentation, and 2D-3D correspondence between 3D parts and visual manuals. We illustrate the broad application of our dataset on four tasks related to shape assembly: assembly plan generation, part segmentation, pose estimationand 3D part assembly.",Datasets & Benchmarks,NeurIPS,2022,Poster,Ruocheng Wang;Yunzhi Zhang;Jiayuan Mao;Ran Zhang;Chin-Yi Cheng;Jiajun Wu,True,https://openreview.net/pdf?id=ZeeswGSOw7r
Zmosb2KfzYd,TAP-Vid: A Benchmark for Tracking Any Point in a Video,"Generic motion understanding from video involves not only tracking objects, but also perceiving how their surfaces deform and move. This information is useful to make inferences about 3D shape, physical properties and object interactions. While the problem of tracking arbitrary physical points on surfaces over longer video clips has received some attention, no dataset or benchmark for evaluation existed, until now.  In this paper, we first formalize the problem, naming it tracking any point (TAP). We introduce a companion benchmark,TAP-Vid, which is composed of both real-world videos with accurate human annotations of point tracks, and synthetic videos with perfect ground-truth point tracks. Central to the construction of our benchmark is a novel semi-automatic crowdsourced pipeline which uses optical flow estimates to compensate for easier, short-term motion like camera shake, allowing annotators to focus on harder sections of the video. We validate our pipeline on synthetic data and propose a simple end-to-end point tracking model, TAP-Net, showing that it outperforms all prior methods on our benchmark when trained on synthetic data.",Datasets & Benchmarks,NeurIPS,2022,Poster,Carl Doersch;Ankush Gupta;Larisa Markeeva;Adria Recasens Continente;Lucas Smaira;Yusuf Aytar;Joao Carreira;Andrew Zisserman;Yi Yang,True,https://openreview.net/pdf?id=Zmosb2KfzYd
Zp8YmiQ_bDC,AirfRANS: High Fidelity Computational Fluid Dynamics Dataset for Approximating Reynolds-Averaged Navier–Stokes Solutions,"Surrogate models are necessary to optimize meaningful quantities in physical dynamics as their recursive numerical resolutions are often prohibitively expensive. It is mainly the case for fluid dynamics and the resolution of Navier–Stokes equations. However, despite the fast-growing field of data-driven models for physical systems, reference datasets representing real-world phenomena are lacking. In this work, we develop \\\\textsc{AirfRANS}, a dataset for studying the two-dimensional incompressible steady-state Reynolds-Averaged Navier–Stokes equations over airfoils at a subsonic regime and for different angles of attacks. We also introduce metrics on the stress forces at the surface of geometries and visualization of boundary layers to assess the capabilities of models to accurately predict the meaningful information of the problem. Finally, we propose deep learning baselines on four machine learning tasks to study \\\\textsc{AirfRANS} under different constraints for generalization considerations: big and scarce data regime, Reynolds number, and angle of attack extrapolation.",Datasets & Benchmarks,NeurIPS,2022,Poster,Florent Bonnet;Jocelyn Ahmed Mazari;Paola Cinnella;patrick gallinari,True,https://openreview.net/pdf?id=Zp8YmiQ_bDC
Zx5qJzNesn0,Learning Long-Term Crop Management Strategies with CyclesGym,"To improve the sustainability and resilience of modern food systems, designing improved crop management strategies is crucial. The increasing abundance of data on agricultural systems suggests that future strategies could benefit from adapting to environmental conditions, but how to design these adaptive policies poses a new frontier. A natural technique for learning policies in these kinds of sequential decision-making problems is reinforcement learning (RL). To obtain the large number of samples required to learn effective RL policies, existing work has used mechanistic crop growth models (CGMs) as simulators. These solutions focus on single-year, single-crop simulations for learning strategies for a single agricultural management practice. However, to learn sustainable long-term policies we must be able to train in multi-year environments, with multiple crops, and consider a wider array of management techniques. We introduce CYCLESGYM, an RL environment based on the multi-year, multi-crop CGM Cycles. CYCLESGYM allows for long-term planning in agroecosystems, provides modular state space and reward constructors and weather generators, and allows for complex actions. For RL researchers, this is a novel benchmark to investigate issues arising in real-world applications. For agronomists, we demonstrate the potential of RL as a powerful optimization tool for agricultural systems management in multi-year case studies on nitrogen (N) fertilization and crop planning scenarios.",Datasets & Benchmarks,NeurIPS,2022,Poster,Matteo Turchetta;Luca Corinzia;Scott Sussex;Amanda Burton;Juan Herrera;Ioannis N. Athanasiadis;Joachim M. Buhmann;Andreas Krause,False,https://openreview.net/pdf?id=Zx5qJzNesn0
_HLcjaVlqJ,JAHS-Bench-201: A Foundation For Research On Joint Architecture And Hyperparameter Search,"The past few years have seen the development of many benchmarks for Neural Architecture Search (NAS), fueling rapid progress in NAS research. However, recent work, which shows that good hyperparameter settings can be more important than using the best architecture, calls for a shift in focus towards Joint Architecture and Hyperparameter Search (JAHS). Therefore, we present JAHS-Bench-201, the first collection of surrogate benchmarks for JAHS, built to also facilitate research on multi-objective, cost-aware and (multi) multi-fidelity optimization algorithms. To the best of our knowledge, JAHS-Bench-201 is based on the most extensive dataset of neural network performance data in the public domain. It is composed of approximately 161 million data points and 20 performance metrics for three deep learning tasks, while featuring a 14-dimensional search and fidelity space that extends the popular NAS-Bench-201 space. With JAHS-Bench-201, we hope to democratize research on JAHS and lower the barrier to entry of an extremely compute intensive field, e.g., by reducing the compute time to run a JAHS algorithm from 5 days to only a few seconds.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Archit Bansal;Danny Stoll;Maciej Janowski;Arber Zela;Frank Hutter,True,https://openreview.net/pdf?id=_HLcjaVlqJ
_b7Rq4BU3ug,FlyView: a bio-informed optical flow truth dataset for visual navigation using panoramic stereo vision,"Flying at speed through complex environments is a challenging task that has been performed successfully by insects since the Carboniferous, but which remains a challenge for robotic and autonomous systems. Insects navigate the world using optical flow sensed by their compound eyes, which they process using a deep neural network weighing just a few milligrams. Deploying an insect-inspired network architecture in computer vision could therefore enable more efficient and effective ways of estimating structure and self-motion using optical flow. Training a bio-informed deep network to implement these tasks requires biologically relevant training, test, and validation data. To this end, we introduce FlyView, a novel bio-informed truth dataset for visual navigation. This simulated dataset is rendered using open source 3D scenes in which the observer's position is known at every frame, and is accompanied by truth data on depth, self-motion, and motion flow. This dataset comprising 42,475 frames has several key features that are missing from existing optical flow datasets, including: (i) panoramic cameras with a monocular and binocular field of view matched to that of a fly's compound eyes; (ii) dynamically meaningful self-motion modelled on motion primitives, or the 3D trajectories of drones and flies; and (iii) complex natural and indoor environments including reflective surfaces.",Datasets & Benchmarks,NeurIPS,2022,Poster,Alix Leroy;Graham Keith Taylor,True,https://openreview.net/pdf?id=_b7Rq4BU3ug
_vSn5XxGRnG,SCAMPS: Synthetics for Camera Measurement of Physiological Signals,"The use of cameras and computational algorithms for noninvasive, low-cost and scalable measurement of physiological (e.g., cardiac and pulmonary) vital signs is very attractive. However, diverse data representing a range of environments, body motions, illumination conditions and physiological states is laborious, time consuming and expensive to obtain. Synthetic data have proven a valuable tool in several areas of machine learning, yet are not widely available for camera measurement of physiological states. Synthetic data offer ""perfect"" labels (e.g., without noise and with precise synchronization), labels that may not be possible to obtain otherwise (e.g., precise pixel level segmentation maps) and provide a high degree of control over variation and diversity in the dataset.  We present SCAMPS, a dataset of synthetics containing 2,800 videos (1.68M frames) with aligned cardiac and respiratory signals and facial action intensities. The RGB frames are provided alongside segmentation maps and precise descriptive statistics about the underlying waveforms, including inter-beat interval, heart rate variability, and pulse arrival time. Finally, we present baseline results training on these synthetic data and testing on real-world datasets to illustrate generalizability.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Daniel McDuff;Miah Wander;Xin Liu;Brian L. Hill;Javier Hernandez;Jonathan Lester;Tadas Baltrusaitis,True,https://openreview.net/pdf?id=_vSn5XxGRnG
aJtVdI251Vv,WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models,"While vision-and-language models perform well on tasks such as visual question answering, they struggle when it comes to basic human commonsense reasoning skills. In this work, we introduce WinoGAViL: an online game of vision-and-language associations (e.g., between werewolves and a full moon), used as a dynamic evaluation benchmark. Inspired by the popular card game Codenames, a spymaster gives a textual cue related to several visual candidates, and another player tries to identify them. Human players are rewarded for creating associations that are challenging for a rival AI model but still solvable by other human players. We use the game to collect 3.5K instances, finding that they are intuitive for humans (>90% Jaccard index) but challenging for state-of-the-art AI models, where the best model (ViLT) achieves a score of 52%, succeeding mostly where the cue is visually salient. Our analysis as well as the feedback we collect from players indicate that the collected associations require diverse reasoning skills, including general knowledge, common sense, abstraction, and more. We release the dataset, the code and the interactive game, allowing future data collection that can be used to develop models with better association abilities.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Yonatan Bitton;Nitzan Bitton Guetta;Ron Yosef;Yuval Elovici;Mohit Bansal;Gabriel Stanovsky;Roy Schwartz,True,https://openreview.net/pdf?id=aJtVdI251Vv
am86qcwErJm,Towards a Standardised Performance Evaluation Protocol for Cooperative MARL,"Multi-agent reinforcement learning (MARL) has emerged as a useful approach to solving decentralised decision-making problems at scale. Research in the field has been growing steadily with many breakthrough algorithms proposed in recent years. In this work, we take a closer look at this rapid development with a focus on evaluation methodologies employed across a large body of research in cooperative MARL. By conducting a detailed meta-analysis of prior work, spanning 75 papers accepted for publication from 2016 to 2022, we bring to light worrying trends that put into question the true rate of progress. We further consider these trends in a wider context and take inspiration from single-agent RL literature on similar issues with recommendations that remain applicable to MARL. Combining these recommendations, with novel insights from our analysis, we propose a standardised performance evaluation protocol for cooperative MARL. We argue that such a standard protocol, if widely adopted, would greatly improve the validity and credibility of future research, make replication and reproducibility easier, as well as improve the ability of the field to accurately gauge the rate of progress over time by being able to make sound comparisons across different works. Finally, we release our meta-analysis data publicly on our project website for future research on evaluation accompanied by our open-source evaluation tools repository.",main,NeurIPS,2022,Poster,Rihab Gorsane;Omayma Mahjoub;Ruan John de Kock;Roland Dubb;Siddarth Singh;Arnu Pretorius,True,https://openreview.net/pdf?id=am86qcwErJm
b0VDQiNLPy9,ETAB: A Benchmark Suite for Visual Representation Learning in Echocardiography,"Echocardiography is one of the most commonly used diagnostic imaging modalities in cardiology. Application of deep learning models to echocardiograms can enable automated identification of cardiac structures, estimation of cardiac function, and prediction of clinical outcomes. However, a major hindrance to realizing the full potential of deep learning is the lack of large-scale, fully curated and annotated data sets required for supervised training. High-quality pre-trained representations that can transfer useful visual features of echocardiograms to downstream tasks can help adapt deep learning models to new setups using fewer examples. In this paper, we design a suite of benchmarks that can be used to pre-train and evaluate echocardiographic representations with respect to various clinically-relevant tasks using publicly accessible data sets. In addition, we develop a unified evaluation protocol---which we call the echocardiographic task adaptation benchmark (ETAB)---that measures how well a visual representation of echocardiograms generalizes to common downstream tasks of interest. We use our benchmarking framework to evaluate state-of-the-art vision modeling pipelines. We envision that our standardized, publicly accessible benchmarks would encourage future research and expedite progress in applying deep learning to high-impact problems in cardiovascular medicine.",Datasets & Benchmarks,NeurIPS,2022,Poster,Ahmed Alaa;Anthony Philippakis;David Sontag,False,https://openreview.net/pdf?id=b0VDQiNLPy9
bBff294gqLp,NAS-Bench-Graph: Benchmarking Graph Neural Architecture Search,"Graph neural architecture search (GraphNAS) has recently aroused considerable attention in both academia and industry. However, two key challenges seriously hinder the further research of GraphNAS. First, since there is no consensus for the experimental setting, the empirical results in different research papers are often not comparable and even not reproducible, leading to unfair comparisons. Secondly, GraphNAS often needs extensive computations, which makes it highly inefficient and inaccessible to researchers without access to large-scale computation. To solve these challenges, we propose NAS-Bench-Graph, a tailored benchmark that supports unified, reproducible, and efficient evaluations for GraphNAS. Specifically, we construct a unified, expressive yet compact search space, covering 26,206 unique graph neural network (GNN) architectures and propose a principled evaluation protocol. To avoid unnecessary repetitive training, we have trained and evaluated all of these architectures on nine representative graph datasets, recording detailed metrics including train, validation, and test performance in each epoch, the latency, the number of parameters, etc. Based on our proposed benchmark, the performance of GNN architectures can be directly obtained by a look-up table without any further computation, which enables fair, fully reproducible, and efficient comparisons.  To demonstrate its usage, we make in-depth analyses of our proposed NAS-Bench-Graph, revealing several interesting findings for GraphNAS. We also showcase how the benchmark can be easily compatible with GraphNAS open libraries such as AutoGL and NNI. To the best of our knowledge, our work is the first benchmark for graph neural architecture search.   ",Datasets & Benchmarks,NeurIPS,2022,Poster,Yijian Qin;Ziwei Zhang;Xin Wang;Zeyang Zhang;Wenwu Zhu,False,https://openreview.net/pdf?id=bBff294gqLp
bKO6BPtYQA7,Change Event Dataset for Discovery from Spatio-temporal Remote Sensing Imagery,"Satellite imagery is increasingly available, high resolution, and temporally detailed.  Changes in spatio-temporal datasets such as satellite images are particularly interesting as they reveal the many events and forces that shape our world.  However, finding such interesting and meaningful change events from the vast data is challenging.  In this paper, we present new datasets for such change events that include semantically meaningful events like road construction.  Instead of manually annotating the very large corpus of satellite images, we introduce a novel unsupervised approach that takes a large spatio-temporal dataset from satellite images and finds interesting change events.  To evaluate the meaningfulness on these datasets we create 2 benchmarks namely CaiRoad and CalFire which capture the events of road construction and forest fires.  These new benchmarks can be used to evaluate semantic retrieval/classification performance.  We explore these benchmarks qualitatively and quantitatively by using several methods and show that these new datasets are indeed challenging for many existing methods. 
",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Utkarsh Mall;Bharath Hariharan;Kavita Bala,True,https://openreview.net/pdf?id=bKO6BPtYQA7
bSULxOy3On,Beyond Real-world Benchmark Datasets: An Empirical Study of Node Classification with GNNs,"Graph Neural Networks (GNNs) have achieved great success on a node classification task. Despite the broad interest in developing and evaluating GNNs, they have been assessed with limited benchmark datasets. As a result, the existing evaluation of GNNs lacks fine-grained analysis from various characteristics of graphs. Motivated by this, we conduct extensive experiments with a synthetic graph generator that can generate graphs having controlled characteristics for fine-grained analysis. Our empirical studies clarify the strengths and weaknesses of GNNs from four major characteristics of real-world graphs with class labels of nodes, i.e., 1) class size distributions (balanced vs. imbalanced), 2) edge connection proportions between classes (homophilic vs. heterophilic), 3) attribute values (biased vs. random), and 4) graph sizes (small vs. large). In addition, to foster future research on GNNs, we publicly release our codebase that allows users to evaluate various GNNs with various graphs. We hope this work offers interesting insights for future research.",Datasets & Benchmarks,NeurIPS,2022,Poster,Seiji Maekawa;Koki Noda;Yuya Sasaki;Makoto Onizuka,False,https://openreview.net/pdf?id=bSULxOy3On
bntkx18xEb4,HUMANISE: Language-conditioned Human Motion Generation in 3D Scenes,"Learning to generate diverse scene-aware and goal-oriented human motions in 3D scenes remains challenging due to the mediocre characters of the existing datasets on Human-Scene Interaction (HSI); they only have limited scale/quality and lack semantics. To fill in the gap, we propose a large-scale and semantic-rich synthetic HSI dataset, denoted as HUMANISE, by aligning the captured human motion sequences with various 3D indoor scenes. We automatically annotate the aligned motions with language descriptions that depict the action and the individual interacting objects; e.g., sit on the armchair near the desk. HUMANIZE thus enables a new generation task, language-conditioned human motion generation in 3D scenes. The proposed task is challenging as it requires joint modeling of the 3D scene, human motion, and natural language. To tackle this task, we present a novel scene-and-language conditioned generative model that can produce 3D human motions of the desirable action interacting with the specified objects. Our experiments demonstrate that our model generates diverse and semantically consistent human motions in 3D scenes. 
",main,NeurIPS,2022,Poster,Zan Wang;Yixin Chen;Tengyu Liu;Yixin Zhu;Wei Liang;Siyuan Huang,True,https://openreview.net/pdf?id=bntkx18xEb4
c0l2YolqD2T,How Well Do Unsupervised Learning Algorithms Model Human Real-time and Life-long Learning?,"Humans learn from visual inputs at multiple timescales, both rapidly and flexibly acquiring visual knowledge over short periods, and robustly accumulating online learning progress over longer periods. Modeling these powerful learning capabilities is an important problem for computational visual cognitive science, and models that could replicate them would be of substantial utility in real-world computer vision settings. In this work, we establish benchmarks for both real-time and life-long continual visual learning. Our real-time learning benchmark measures a model's ability to match the rapid visual behavior changes of real humans over the course of minutes and hours, given a stream of visual inputs. Our life-long learning benchmark evaluates the performance of models in a purely online learning curriculum obtained directly from child visual experience over the course of years of development. We evaluate a spectrum of recent deep self-supervised visual learning algorithms on both benchmarks, finding that none of them perfectly match human performance, though some algorithms perform substantially better than others. Interestingly, algorithms embodying recent trends in self-supervised learning -- including BYOL, SwAV and MAE -- are substantially worse on our benchmarks than an earlier generation of self-supervised algorithms such as SimCLR and MoCo-v2. We present analysis indicating that the failure of these newer algorithms is primarily due to their inability to handle the kind of sparse low-diversity datastreams that naturally arise in the real world, and that actively leveraging memory through negative sampling -- a mechanism eschewed by these newer algorithms -- appears useful for facilitating learning in such low-diversity environments. We also illustrate a complementarity between the short and long timescales in the two benchmarks, showing how requiring a single learning algorithm to be locally context-sensitive enough to match real-time learning changes while stable enough to avoid catastrophic forgetting over the long term induces a trade-off that human-like algorithms may have to straddle. Taken together, our benchmarks establish a quantitative way to directly compare learning between neural networks models and human learners, show how choices in the mechanism by which such algorithms handle sample comparison and memory strongly impact their ability to match human learning abilities, and expose an open problem space for identifying more flexible and robust visual self-supervision algorithms. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Chengxu Zhuang;Violet Xiang;Yoon Bai;Xiaoxuan Jia;Nicholas Turk-Browne;Kenneth Norman;James J. DiCarlo;Daniel LK Yamins,True,https://openreview.net/pdf?id=c0l2YolqD2T
c7f9uoPnzgE,Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark,"Vision-Language Pre-training (VLP) models have shown remarkable performance on various downstream tasks. Their success heavily relies on the scale of pre-trained cross-modal datasets. However, the lack of large-scale datasets and benchmarks in Chinese hinders the development of Chinese VLP models and broader multilingual applications. In this work, we release a large-scale Chinese cross-modal dataset named Wukong, which contains 100 million Chinese image-text pairs collected from the web. Wukong aims to benchmark different multi-modal pre-training methods to facilitate the VLP research and community development. Furthermore, we release a group of models pre-trained with various image encoders (ViT-B/ViT-L/SwinT) and also apply advanced pre-training techniques into VLP such as locked-image text tuning, token-wise similarity in contrastive learning, and reduced-token interaction. Extensive experiments and a benchmarking of different downstream tasks including a new largest human-verified image-text test dataset are also provided. Experiments show that Wukong can serve as a promising Chinese pre-training dataset and benchmark for different cross-modal learning methods. For the zero-shot image classification task on 10 datasets, $Wukong_\\\\text{ViT-L}$ achieves an average accuracy of 73.03%. For the image-text retrieval task, it achieves a mean recall of 71.6% on AIC-ICC which is 12.9% higher than WenLan 2.0. Also, our Wukong models are benchmarked on downstream tasks with other variants on multiple datasets, e.g., Flickr8K-CN, Flickr-30K-CN, COCO-CN, et al. More information can be referred to https://wukong-dataset.github.io/wukong-dataset/.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Jiaxi Gu;Xiaojun Meng;Guansong Lu;Lu Hou;Minzhe Niu;Xiaodan Liang;Lewei Yao;Runhui Huang;Wei Zhang;Xin Jiang;Chunjing Xu;Hang Xu,True,https://openreview.net/pdf?id=c7f9uoPnzgE
cj6K4IWVomU,Rotation-Equivariant Conditional Spherical Neural Fields for Learning a Natural Illumination Prior,"Inverse rendering is an ill-posed problem. Previous work has sought to resolve this by focussing on priors for object or scene shape or appearance. In this work, we instead focus on a prior for natural illuminations. Current methods rely on spherical harmonic lighting or other generic representations and, at best, a simplistic prior on the parameters. We propose a conditional neural field representation based on a variational auto-decoder with a SIREN network and, extending Vector Neurons, build equivariance directly into the network. Using this, we develop a rotation-equivariant, high dynamic range (HDR) neural illumination model that is compact and able to express complex, high-frequency features of natural environment maps. Training our model on a curated dataset of 1.6K HDR environment maps of natural scenes, we compare it against traditional representations, demonstrate its applicability for an inverse rendering task and show environment map completion from partial observations.",main,NeurIPS,2022,Poster,James A D Gardner;Bernhard Egger;William A P Smith,True,https://openreview.net/pdf?id=cj6K4IWVomU
dIUQ5haSOI,Relation-Constrained Decoding for Text Generation,"The dominant paradigm for neural text generation nowadays is seq2seq learning with large-scale pretrained language models. However, it is usually difficult to manually constrain the generation process of these models. Prior studies have introduced Lexically Constrained Decoding (LCD) to ensure the presence of pre-specified words or phrases in the output. However, simply applying lexical constraints has no guarantee of the grammatical or semantic relations between words. Thus, more elaborate constraints are needed. To this end, we first propose a new constrained decoding scenario named Relation-Constrained Decoding (RCD), which requires the model's output to contain several given word pairs with respect to the given relations between them. For this scenario, we present a novel plug-and-play decoding algorithm named RElation-guided probability Surgery and bEam ALlocation (RESEAL), which can handle different categories of relations, e.g., syntactical relations or factual relations. Moreover, RESEAL can adaptively ""reseal"" the relations to form a high-quality sentence, which can be applied to the inference stage of any autoregressive text generation model. To evaluate our method, we first construct an RCD benchmark based on dependency relations from treebanks with annotated dependencies. Experimental results demonstrate that our approach can achieve better preservation of the input dependency relations compared to previous methods. To further illustrate the effectiveness of RESEAL, we apply our method to three downstream tasks: sentence summarization, fact-based text editing, and data-to-text generation. We observe an improvement in generation quality. The source code is available at https://github.com/CasparSwift/RESEAL. ",main,NeurIPS,2022,Poster,Xiang Chen;Zhixian Yang;Xiaojun Wan,True,https://openreview.net/pdf?id=dIUQ5haSOI
dLL4KXzKUpS,Where2comm: Communication-Efficient Collaborative Perception via Spatial Confidence Maps,"Multi-agent collaborative perception could significantly upgrade the perception performance by enabling agents to share complementary information with each other through communication. It inevitably results in a fundamental trade-off between perception performance and communication bandwidth. To tackle this bottleneck issue, we propose a spatial confidence map, which reflects the spatial heterogeneity of perceptual information. It empowers agents to only share spatially sparse, yet perceptually critical information, contributing to where to communicate. Based on this novel spatial confidence map, we propose Where2comm, a communication-efficient collaborative perception framework. Where2comm has two distinct advantages: i) it considers pragmatic compression and uses less communication to achieve higher perception performance by focusing on perceptually critical areas; and ii) it can handle varying communication bandwidth by dynamically adjusting spatial areas involved in communication. To evaluate Where2comm, we consider 3D object detection in both real-world and simulation scenarios with two modalities (camera/LiDAR) and two agent types (cars/drones) on four datasets: OPV2V, V2X-Sim, DAIR-V2X, and our original CoPerception-UAVs. Where2comm consistently outperforms previous methods; for example, it achieves more than $100,000 \\\\times$ lower communication volume and still outperforms DiscoNet and V2X-ViT on OPV2V. Our code is available at~\\\\url{https://github.com/MediaBrain-SJTU/where2comm}.",main,NeurIPS,2022,Poster,Yue Hu;Shaoheng Fang;Zixing Lei;Yiqi Zhong;Siheng Chen,True,https://openreview.net/pdf?id=dLL4KXzKUpS
ddPXQt-gM--,Benchmarking Heterogeneous Treatment Effect Models through the Lens of Interpretability,"Estimating personalized effects of treatments is a complex, yet pervasive problem. To tackle it, recent developments in the machine learning (ML) literature on heterogeneous treatment effect estimation gave rise to many sophisticated, but opaque, tools: due to their flexibility, modularity and ability to learn constrained representations, neural networks in particular have become central to this literature. Unfortunately, the assets of such black boxes come at a cost: models typically involve countless nontrivial operations, making it difficult to understand what they have learned. Yet, understanding these models can be crucial -- in a medical context, for example, discovered knowledge on treatment effect heterogeneity could inform treatment prescription in clinical practice. In this work, we therefore use post-hoc feature importance methods to identify features that influence the model's predictions. This allows us to evaluate treatment effect estimators along a new and important dimension that has been overlooked in previous work: We construct a benchmarking environment to empirically investigate the ability of personalized treatment effect models to identify predictive covariates -- covariates that determine differential responses to treatment. Our benchmarking environment then enables us to provide new insight into the strengths and weaknesses of different types of treatment effects models as we modulate different challenges specific to treatment effect estimation -- e.g. the ratio of prognostic to predictive information, the possible nonlinearity of potential outcomes and the presence and type of confounding.  ",Datasets & Benchmarks,NeurIPS,2022,Poster,Jonathan Crabbé;Alicia Curth;Ioana Bica;Mihaela van der Schaar,True,https://openreview.net/pdf?id=ddPXQt-gM--
dh_MkX0QfrK,PDEBench: An Extensive Benchmark for Scientific Machine Learning,"Machine learning-based modeling of physical systems has experienced increased interest in recent years. Despite some impressive progress, there is still a lack of benchmarks for Scientific ML that are easy to use but still challenging and repre- sentative of a wide range of problems. We introduce PDEBENCH, a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations (PDEs). PDEBENCH comprises both code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines. Our proposed set of benchmark problems con- tribute the following unique features: (1) A much wider range of PDEs compared to existing benchmarks, ranging from relatively common examples to more real- istic and difficult problems; (2) much larger ready-to-use datasets compared to prior work, comprising multiple simulation runs across a larger number of ini- tial and boundary conditions and PDE parameters; (3) more extensible source codes with user-friendly APIs for data generation and baseline results with popular machine learning models (FNO, U-Net, PINN, Gradient-Based Inverse Method). PDEBENCH allows researchers to extend the benchmark freely for their own pur- poses using a standardized API and to compare the performance of new models to existing baseline methods. We also propose new evaluation metrics with the aim to provide a more holistic understanding of learning methods in the context of Scientific ML. With those metrics we identify tasks which are challenging for recent ML methods and propose these tasks as future challenges for the community. The code is available at https://github.com/pdebench/PDEBench.",Datasets & Benchmarks,NeurIPS,2022,Poster,Makoto Takamoto;Timothy Praditia;Raphael Leiteritz;Dan MacKinlay;Francesco Alesiani;Dirk Pflüger;Mathias Niepert,True,https://openreview.net/pdf?id=dh_MkX0QfrK
djnKHOjpb7I,EPIC-KITCHENS VISOR Benchmark: VIdeo Segmentations and Object Relations,"We introduce VISOR, a new dataset of pixel annotations and a benchmark suite for segmenting hands and active objects in egocentric video. VISOR annotates videos from EPIC-KITCHENS, which comes with a new set of challenges not encountered in current video segmentation datasets. Specifically, we need to ensure both short- and long-term consistency of pixel-level annotations as objects undergo transformative interactions, e.g. an onion is peeled, diced and cooked - where we aim to obtain accurate pixel-level annotations of the peel, onion pieces, chopping board, knife, pan, as well as the acting hands. VISOR introduces an annotation pipeline, AI-powered in parts, for scalability and quality. In total, we publicly release 272K manual semantic masks of 257 object classes, 9.9M interpolated dense masks, 67K hand-object relations, covering 36 hours of 179 untrimmed videos. Along with the annotations, we introduce three challenges in video object segmentation, interaction understanding and long-term reasoning.

For data, code and leaderboards: http://epic-kitchens.github.io/VISOR",Datasets & Benchmarks,NeurIPS,2022,Poster,Ahmad Darkhalil;Dandan Shan;Bin Zhu;Jian Ma;Amlan Kar;Richard Ely Locke Higgins;Sanja Fidler;David Fouhey;Dima Damen,True,https://openreview.net/pdf?id=djnKHOjpb7I
dp0zWsdOV1h,"Retrieve, Reason, and Refine: Generating Accurate and Faithful Patient Instructions","The ""Patient Instruction"" (PI), which contains critical instructional information provided both to carers and to the patient at the time of discharge, is essential for the patient to manage their condition outside hospital. An accurate and easy-to-follow PI can improve the self-management of patients which can in turn reduce hospital readmission rates. However, writing an appropriate PI can be extremely time consuming for physicians, and is subject to being incomplete or error-prone for (potentially overworked) physicians. Therefore, we propose a new task that can provide an objective means of avoiding incompleteness, while reducing clinical workload: the automatic generation of the PI, which is imagined as being a document that the clinician can review, modify, and approve as necessary (rather than taking the human ""out of the loop""). We build a benchmark clinical dataset and propose the Re$^3$Writer, which imitates the working patterns of physicians to first retrieve related working experience from historical PIs written by physicians, then reason related medical knowledge. Finally, it refines the retrieved working experience and reasoned medical knowledge to extract useful information, which is used to generate the PI for previously-unseen patient according to their health records during hospitalization. Our experiments show that, using our method, the performance of 6 different models can be substantially boosted across all metrics, with up to 20%, 11%, and 19% relative improvements in BLEU-4, ROUGE-L, and METEOR, respectively. Meanwhile, we show results from human evaluations to measure the effectiveness in terms of its usefulness for clinical practice. The code is available at https://github.com/AI-in-Health/Patient-Instructions.",main,NeurIPS,2022,Poster,Fenglin Liu;Bang Yang;Chenyu You;Xian Wu;Shen Ge;Zhangdaihong Liu;Xu Sun;Yang Yang;David A. Clifton,True,https://openreview.net/pdf?id=dp0zWsdOV1h
dwi57JI_-K,SafeBench: A Benchmarking Platform for Safety Evaluation of Autonomous Vehicles,"As shown by recent studies, machine intelligence-enabled systems are vulnerable to test cases resulting from either adversarial manipulation or natural distribution shifts. This has raised great concerns about deploying machine learning algorithms for real-world applications, especially in safety-critical domains such as autonomous driving (AD). On the other hand, traditional AD testing on naturalistic scenarios requires hundreds of millions of driving miles due to the high dimensionality and rareness of the safety-critical scenarios in the real world. As a result, several approaches for autonomous driving evaluation have been explored, which are usually, however, based on different simulation platforms, types of safety-critical scenarios, scenario generation algorithms, and driving route variations. Thus, despite a large amount of effort in autonomous driving testing, it is still challenging to compare and understand the effectiveness and efficiency of different testing scenario generation algorithms and testing mechanisms under similar conditions. In this paper, we aim to provide the first unified platform SafeBench to integrate different types of safety-critical testing scenarios, scenario generation algorithms, and other variations such as driving routes and environments. In particular, we consider 8 safety-critical testing scenarios following National Highway Traffic Safety Administration (NHTSA) and develop 4 scenario generation algorithms considering 10 variations for each scenario. Meanwhile, we implement 4 deep reinforcement learning-based AD algorithms with 4 types of input (e.g., bird’s-eye view, camera) to perform fair comparisons on SafeBench. We find our generated testing scenarios are indeed more challenging and observe the trade-off between the performance of AD agents under benign and safety-critical testing scenarios. We believe our unified platform SafeBench for large-scale and effective autonomous driving testing will motivate the development of new testing scenario generation and safe AD algorithms. SafeBench is available at https://safebench.github.io.",Datasets & Benchmarks,NeurIPS,2022,Poster,Chejian Xu;Wenhao Ding;Weijie Lyu;Zuxin Liu;Shuai Wang;Yihan He;Hanjiang Hu;Ding Zhao;Bo Li,False,https://openreview.net/pdf?id=dwi57JI_-K
dz79MhQXWvg,Weakly supervised causal representation learning,"Learning high-level causal representations together with a causal model from unstructured low-level data such as pixels is impossible from observational data alone. We prove under mild assumptions that this representation is however identifiable in a weakly supervised setting. This involves a dataset with paired samples before and after random, unknown interventions, but no further labels. We then introduce implicit latent causal models, variational autoencoders that represent causal variables and causal structure without having to optimize an explicit discrete graph structure. On simple image data, including a novel dataset of simulated robotic manipulation, we demonstrate that such models can reliably identify the causal structure and disentangle causal variables.",main,NeurIPS,2022,Poster,Johann Brehmer;Pim De Haan;Phillip Lippe;Taco Cohen,True,https://openreview.net/pdf?id=dz79MhQXWvg
eJhc_CPXQIT,MOMA-LRG: Language-Refined Graphs for Multi-Object Multi-Actor Activity Parsing,"Video-language models (VLMs), large models pre-trained on numerous but noisy video-text pairs from the internet, have revolutionized activity recognition through their remarkable generalization and open-vocabulary capabilities. While complex human activities are often hierarchical and compositional, most existing tasks for evaluating VLMs focus only on high-level video understanding, making it difficult to accurately assess and interpret the ability of VLMs to understand complex and fine-grained human activities. Inspired by the recently proposed MOMA framework, we define activity graphs as a single universal representation of human activities that encompasses video understanding at the activity, sub-activity, and atomic action level.  We redefine activity parsing as the overarching task of activity graph generation, requiring understanding human activities across all three levels. To facilitate the evaluation of models on activity parsing, we introduce MOMA-LRG (Multi-Object Multi-Actor Language-Refined Graphs), a large dataset of complex human activities with activity graph annotations that can be readily transformed into natural language sentences. Lastly, we present a model-agnostic and lightweight approach to adapting and evaluating VLMs by incorporating structured knowledge from activity graphs into VLMs, addressing the individual limitations of language and graphical models. We demonstrate strong performance on few-shot activity parsing, and our framework is intended to foster future research in the joint modeling of videos, graphs, and language.",Datasets & Benchmarks,NeurIPS,2022,Poster,Zelun Luo;Zane Durante;Linden Li;Wanze Xie;Ruochen Liu;Emily Jin;Zhuoyi Huang;Lun Yu Li;Jiajun Wu;Juan Carlos Niebles;Ehsan Adeli;Li Fei-Fei,True,https://openreview.net/pdf?id=eJhc_CPXQIT
eMW9AkXaREI,Vision Transformers provably learn spatial structure,"Vision Transformers (ViTs) have recently achieved comparable or superior performance to Convolutional neural networks (CNNs) in computer vision. This empirical breakthrough is even more remarkable since ViTs discards spatial information by mixing patch embeddings and positional encodings and do not embed any visual inductive bias (e.g.\\\\ spatial locality). Yet, recent work showed that while minimizing their training loss, ViTs specifically learn spatially delocalized patterns. This raises a central question: how do ViTs learn this pattern by solely minimizing their training loss using gradient-based methods from \\\\emph{random initialization}? We propose a structured classification dataset and a simplified ViT model to provide preliminary theoretical justification of this phenomenon. Our model relies on a simplified attention mechanism --the positional attention mechanism-- where the attention matrix solely depends on the positional encodings. While the problem admits multiple solutions that generalize, we show that our model implicitly learns the spatial structure of the dataset while generalizing. 
We finally prove that learning the structure helps to  sample-efficiently transfer to downstream datasets that share the same structure as the pre-training one but with different  features. We empirically verify that ViTs using only the positional attention mechanism perform similarly to the original one on CIFAR-10/100, SVHN and ImageNet.",main,NeurIPS,2022,Poster,Samy Jelassi;Michael Eli Sander;Yuanzhi Li,True,https://openreview.net/pdf?id=eMW9AkXaREI
eOnQ2etkxto,Enabling Detailed Action Recognition Evaluation Through Video Dataset Augmentation,"It is well-known in the video understanding community that human action recognition models suffer from background bias, i.e., over-relying on scene cues in making their predictions. However, it is difficult to quantify this effect using existing evaluation frameworks. We introduce the Human-centric Analysis Toolkit (HAT), which enables evaluation of learned background bias without the need for new manual video annotation. It does so by automatically generating synthetically manipulated videos and leveraging the recent advances in image segmentation and video inpainting. Using HAT we perform an extensive analysis of 74 action recognition models trained on the Kinetics dataset. We confirm that all these models focus more on the scene background than on the human motion; further, we demonstrate that certain model design decisions (such as training with fewer frames per video or using dense as opposed to uniform temporal sampling) appear to worsen the background bias. We open-source HAT to enable the community to design more robust and generalizable human action recognition models.",Datasets & Benchmarks,NeurIPS,2022,Poster,Jihoon Chung;Yu Wu;Olga Russakovsky,False,https://openreview.net/pdf?id=eOnQ2etkxto
ex60CCi5GS,Debiasing Graph Neural Networks via Learning Disentangled Causal Substructure,"Most Graph Neural Networks (GNNs) predict the labels of unseen graphs by learning the correlation between the input graphs and labels. However, by presenting a graph classification investigation on the training graphs with severe bias, surprisingly, we discover that GNNs always tend to explore the spurious correlations to make decision, even if the causal correlation always exists. This implies that existing GNNs trained on such biased datasets will suffer from poor generalization capability.  By analyzing this problem in a causal view, we find that disentangling and decorrelating the causal and bias latent variables from the biased graphs are both crucial for debiasing. Inspired by this, we propose a general disentangled GNN framework to learn the causal substructure and bias substructure, respectively. Particularly,  we design a parameterized edge mask generator to explicitly split the input graph into causal and bias subgraphs. Then two GNN modules supervised by causal/bias-aware loss functions respectively are trained to encode causal and bias subgraphs into their corresponding representations. With the disentangled representations, we synthesize the counterfactual unbiased training samples to further decorrelate causal and bias variables. Moreover, to better benchmark the severe bias problem, we construct three new graph datasets, which have controllable bias degrees and are easier to visualize and explain. Experimental results well demonstrate that our approach achieves superior generalization performance over existing baselines. Furthermore, owing to the learned edge mask, the proposed model has appealing interpretability and transferability.",main,NeurIPS,2022,Poster,Shaohua Fan;Xiao Wang;Yanhu Mo;Chuan Shi;Jian Tang,True,https://openreview.net/pdf?id=ex60CCi5GS
foA_SFQ9zo0,ADBench: Anomaly Detection Benchmark,"Given a long list of anomaly detection algorithms developed in the last few decades, how do they perform with regard to (i) varying levels of supervision, (ii) different types of anomalies, and (iii) noisy and corrupted data? In this work, we answer these key questions by conducting (to our best knowledge) the most comprehensive anomaly detection benchmark with 30 algorithms on 57 benchmark datasets, named ADBench. Our extensive experiments (98,436 in total) identify meaningful insights into the role of supervision and anomaly types, and unlock future directions for researchers in algorithm selection and design. With ADBench, researchers can easily conduct comprehensive and fair evaluations for newly proposed methods on the datasets (including our contributed ones from natural language and computer vision domains) against the existing baselines. To foster accessibility and reproducibility, we fully open-source ADBench and the corresponding results.",Datasets & Benchmarks,NeurIPS,2022,Poster,Songqiao Han;Xiyang Hu;Hailiang Huang;Minqi Jiang;Yue Zhao,True,https://openreview.net/pdf?id=foA_SFQ9zo0
foNVYPnQbhk,SCONE: Surface Coverage Optimization in Unknown Environments by Volumetric Integration,"Next Best View computation (NBV) is a long-standing problem in robotics, and consists in identifying the next most informative sensor position(s) for reconstructing a 3D object or scene efficiently and accurately. Like most current methods, we consider NBV prediction from a depth sensor like Lidar systems. Learning-based methods relying on a volumetric representation of the scene are suitable for path planning, but have lower accuracy than methods using a surface-based representation. However, the latter do not scale well with the size of the scene and constrain the camera to a small number of poses. To obtain the advantages of both representations, we show that we can maximize surface metrics by Monte Carlo integration over a volumetric representation. In particular, we propose an approach, SCONE, that relies on two neural modules: The first module predicts occupancy probability in the entire volume of the scene. Given any new camera pose, the second module samples points in the scene based on their occupancy probability and leverages a self-attention mechanism to predict the visibility of the samples. Finally, we integrate the visibility to evaluate the gain in surface coverage for the new camera pose. NBV is selected as the pose that maximizes the gain in total surface coverage. Our method scales to large scenes and handles free camera motion: It takes as input an arbitrarily large point cloud gathered by a depth sensor as well as camera poses to predict NBV. We demonstrate our approach on a novel dataset made of large and complex 3D scenes.",main,NeurIPS,2022,Poster,Antoine Guedon;Pascal Monasse;Vincent Lepetit,True,https://openreview.net/pdf?id=foNVYPnQbhk
gT6j4_tskUt,OpenOOD: Benchmarking Generalized Out-of-Distribution Detection,"Out-of-distribution (OOD) detection is vital to safety-critical machine learning applications and has thus been extensively studied, with a plethora of methods developed in the literature. However, the field currently lacks a unified, strictly formulated, and comprehensive benchmark, which often results in unfair comparisons and inconclusive results. From the problem setting perspective, OOD detection is closely related to neighboring fields including anomaly detection (AD), open set recognition (OSR), and model uncertainty, since methods developed for one domain are often applicable to each other. To help the community to improve the evaluation and advance, we build a unified, well-structured codebase called OpenOOD, which implements over 30 methods developed in relevant fields and provides a comprehensive benchmark under the recently proposed generalized OOD detection framework. With a comprehensive comparison of these methods, we are gratified that the field has progressed significantly over the past few years, where both preprocessing methods and the orthogonal post-hoc methods show strong potential. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Jingkang Yang;Pengyun Wang;Dejian Zou;Zitang Zhou;Kunyuan Ding;WENXUAN PENG;Haoqi Wang;Guangyao Chen;Bo Li;Yiyou Sun;Xuefeng Du;Kaiyang Zhou;Wayne Zhang;Dan Hendrycks;Yixuan Li;Ziwei Liu,False,https://openreview.net/pdf?id=gT6j4_tskUt
gud0qopqJc4,SkinCon: A skin disease dataset densely annotated by domain experts for fine-grained debugging and analysis,"For the deployment of artificial intelligence (AI) in high risk settings, such as healthcare, methods that provide interpretability/explainability or allow fine-grained error analysis are critical. Many recent methods for interpretability/explainability and fine-grained error analysis use concepts, which are meta-labels which are semantically meaningful to humans.  However, there are only a few datasets that include concept-level meta-labels and most of these meta-labels are relevant for natural images that do not require domain expertise. Previous densely annotated datasets in medicine focused on meta-labels that are relevant to a single disease such as osteoarthritis or melanoma. In dermatology, skin disease is described using an established clinical lexicon that allow clinicians to describe physical exam findings to one another. To provide the first medical dataset densely annotated by domain experts to provide annotations useful across multiple disease processes, we developed SkinCon: a skin disease dataset densely annotated by dermatologists. SkinCon includes 3230 images from the Fitzpatrick 17k skin disease dataset densely annotated with 48 clinical concepts, 22 of which have at least 50 images representing the concept. The concepts used were chosen by two dermatologists considering the clinical descriptor terms used to describe skin lesions. Examples include ""plaque"", ""scale"", and ""erosion"". These same concepts were also used to label 656 skin disease images from the Diverse Dermatology Images dataset, providing an additional external dataset with diverse skin tone representations. We review the potential applications for the SkinCon dataset, such as probing models, concept-based explanations, concept bottlenecks, error analysis, and slice discovery. Furthermore, we use SkinCon to demonstrate two of these use cases: debugging mistakes of an existing dermatology AI model with concepts and developing interpretable models with post-hoc concept bottleneck models.",Datasets & Benchmarks,NeurIPS,2022,Poster,Roxana Daneshjou;Mert Yuksekgonul;Zhuo Ran Cai;Roberto A. Novoa;James Zou,True,https://openreview.net/pdf?id=gud0qopqJc4
gvaqa_WcIR6,VeriDark: A Large-Scale Benchmark for Authorship Verification on the Dark Web,"The Dark Web represents a hotbed for illicit activity, where users communicate on different market forums in order to exchange goods and services. Law enforcement agencies benefit from forensic tools that perform authorship analysis, in order to identify and profile users based on their textual content. However, authorship analysis has been traditionally studied using corpora featuring literary texts such as fragments from novels or fan fiction, which may not be suitable in a cybercrime context. Moreover, the few works that employ authorship analysis tools for cybercrime prevention usually employ ad-hoc experimental setups and datasets. To address these issues, we release VeriDark: a benchmark comprised of three large scale authorship verification datasets and one authorship identification dataset obtained from user activity from either Dark Web related Reddit communities or popular illicit Dark Web market forums. We evaluate competitive NLP baselines on the three datasets and perform an analysis of the predictions to better understand the limitations of such approaches. We make the datasets and baselines publicly available at https://github.com/bit-ml/VeriDark .",Datasets & Benchmarks,NeurIPS,2022,Poster,Andrei Manolache;Florin Brad;Antonio Barbalau;Radu Tudor Ionescu;Marius Popescu,True,https://openreview.net/pdf?id=gvaqa_WcIR6
h3RYh6IBBS,Revisiting Neural Scaling Laws in Language and Vision,"The remarkable progress in deep learning in recent years is largely driven by improvements in scale, where bigger models are trained on larger datasets for longer schedules. To predict the benefit of scale empirically, we argue for a more rigorous methodology based on the extrapolation loss, instead of reporting the best-fitting (interpolating) parameters. We then present a recipe for estimating scaling law parameters reliably from learning curves. We demonstrate that it extrapolates more accurately than previous methods in a wide range of architecture families across several domains, including image classification, neural machine translation (NMT) and  language modeling, in addition to tasks from the BIG-Bench evaluation benchmark. Finally, we release a benchmark dataset comprising of 90 evaluation tasks to facilitate research in this domain. ",main,NeurIPS,2022,Poster,Ibrahim Alabdulmohsin;Behnam Neyshabur;Xiaohua Zhai,True,https://openreview.net/pdf?id=h3RYh6IBBS
h3jZCLjhtmV,Multi-agent Dynamic Algorithm Configuration,"Automated algorithm configuration relieves users from tedious, trial-and-error tuning tasks. A popular algorithm configuration tuning paradigm is dynamic algorithm configuration (DAC), in which an agent learns dynamic configuration policies across instances by reinforcement learning (RL). However, in many complex algorithms, there may exist different types of configuration hyperparameters, and such heterogeneity may bring difficulties for classic DAC which uses a single-agent RL policy. In this paper, we aim to address this issue and propose multi-agent DAC (MA-DAC), with one agent working for one type of configuration hyperparameter. MA-DAC formulates the dynamic configuration of a complex algorithm with multiple types of hyperparameters as a contextual multi-agent Markov decision process and solves it by a cooperative multi-agent RL (MARL) algorithm. To instantiate, we apply MA-DAC to a well-known optimization algorithm for multi-objective optimization problems. Experimental results show the effectiveness of MA-DAC in not only achieving superior performance compared with other configuration tuning approaches based on heuristic rules, multi-armed bandits, and single-agent RL, but also being capable of generalizing to different problem classes. Furthermore, we release the environments in this paper as a benchmark for testing MARL algorithms, with the hope of facilitating the application of MARL.",main,NeurIPS,2022,Poster,Ke Xue;Jiacheng Xu;Lei Yuan;Miqing Li;Chao Qian;Zongzhang Zhang;Yang Yu,True,https://openreview.net/pdf?id=h3jZCLjhtmV
hGl8rsmNXzs,ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models,"Learning visual representations from natural language supervision has recently shown great promise in a number of pioneering works. In general, these language-augmented visual models demonstrate strong transferability to a variety of datasets/tasks. However, it remains challenging to evaluate the transferablity of these foundation models due to the lack of easy-to-use toolkits for fair benchmarking. To tackle this, we build ELEVATER (Evaluation of Language-augmented Visual Task-level Transfer), the first benchmark to compare and evaluate pre-trained language-augmented visual models. Several highlights include: (i) Datasets. As downstream evaluation suites, it consists of 20 image classification datasets and 35 object detection datasets, each of which is augmented with external knowledge. (ii) Toolkit. An automatic hyper-parameter tuning toolkit is developed to ensure the fairness in model adaption. To leverage the full power of language-augmented visual models, novel language-aware initialization methods are proposed to significantly improve the adaption performance. (iii) Metrics. A variety of evaluation metrics are used, including sample-efficiency (zero-shot and few-shot) and parameter-efficiency (linear probing and full model fine-tuning). We will publicly release ELEVATER.",Datasets & Benchmarks,NeurIPS,2022,Poster,Chunyuan Li;Haotian Liu;Liunian Harold Li;Pengchuan Zhang;Jyoti Aneja;Jianwei Yang;Ping Jin;Houdong Hu;Zicheng Liu;Yong Jae Lee;Jianfeng Gao,True,https://openreview.net/pdf?id=hGl8rsmNXzs
heBKnuV42O,DDXPlus: A New Dataset For Automatic Medical Diagnosis,"There has been a rapidly growing interest in Automatic Symptom Detection (ASD) and Automatic Diagnosis (AD) systems in the machine learning research literature, aiming to assist doctors in telemedicine services. These systems are designed to interact with patients, collect evidence about their symptoms and relevant antecedents, and possibly make predictions about the underlying diseases. Doctors would review the interactions, including the evidence and the predictions, collect if necessary additional information from patients, before deciding on next steps. Despite recent progress in this area, an important piece of doctors' interactions with patients is missing in the design of these systems, namely the differential diagnosis. Its absence is largely due to the lack of datasets that include such information for models to train on. In this work, we present a large-scale synthetic dataset of roughly 1.3 million patients that includes a differential diagnosis, along with the ground truth pathology, symptoms and antecedents for each patient. Unlike existing datasets which only contain binary symptoms and antecedents, this dataset also contains categorical and multi-choice symptoms and antecedents useful for efficient data collection. Moreover, some symptoms are organized in a hierarchy, making it possible to design systems able to interact with patients in a logical way. As a proof-of-concept, we extend two existing AD and ASD systems to incorporate the differential diagnosis, and provide empirical evidence that using differentials as training signals is essential for the efficiency of such systems or for helping doctors better understand the reasoning of those systems.",Datasets & Benchmarks,NeurIPS,2022,Poster,Arsene Fansi Tchango;Rishab Goel;Zhi Wen;Julien Martel;Joumana Ghosn,True,https://openreview.net/pdf?id=heBKnuV42O
hq-p55-qil9,Associating Objects and Their Effects in Video through Coordination Games,"We explore a feed-forward approach for decomposing a video into layers, where each layer contains an object of interest along with its associated shadows, reflections, and other visual effects. This problem is challenging since associated effects vary widely with the 3D geometry and lighting conditions in the scene, and ground-truth labels for visual effects are difficult (and in some cases impractical) to collect. 
We take a self-supervised approach and train a neural network to produce a foreground image and alpha matte from a rough object segmentation mask under a reconstruction and sparsity loss. Under reconstruction loss, the layer decomposition problem is underdetermined: many combinations of layers may reconstruct the input video.
Inspired by the game theory concept of focal points---or \\\\emph{Schelling points}---we pose the problem as a coordination game, where each player (network) predicts the effects for a single object without knowledge of the other players' choices. The players learn to converge on the ``natural'' layer decomposition in order to maximize the likelihood of their choices aligning with the other players'. We train the network to play this game with itself, and show how to design the rules of this game so that the focal point lies at the correct layer decomposition. We demonstrate feed-forward results on a challenging synthetic dataset, then show that pretraining on this dataset significantly reduces optimization time for real videos.",main,NeurIPS,2022,Poster,Erika Lu;Forrester Cole;Weidi Xie;Tali Dekel;William T. Freeman;Andrew Zisserman;Michael Rubinstein,True,https://openreview.net/pdf?id=hq-p55-qil9
i1bFPSw42W0,MBW: Multi-view Bootstrapping in the Wild,"Labeling articulated objects in unconstrained settings has a wide variety of applications including entertainment, neuroscience, psychology, ethology, and many fields of medicine. Large offline labeled datasets do not exist for all but the most common articulated object categories (e.g., humans). Hand labeling these landmarks within a video sequence is a laborious task. Learned landmark detectors can help, but can be error-prone when trained from only a few examples. Multi-camera systems that train fine-grained detectors have shown significant promise in detecting such errors, allowing for self-supervised solutions that only need a small percentage of the video sequence to be hand-labeled. The approach, however, is based on calibrated cameras and rigid geometry, making it expensive, difficult to manage, and impractical in real-world scenarios. In this paper, we address these bottlenecks by combining a non-rigid 3D neural prior with deep flow to obtain high-fidelity landmark estimates from videos with only two or three uncalibrated, handheld cameras. With just a few annotations (representing $1-2\\\\%$ of the frames), we are able to produce 2D results comparable to state-of-the-art fully supervised methods, along with 3D reconstructions that are impossible with other existing approaches. Our Multi-view Bootstrapping in the Wild (MBW) approach demonstrates impressive results on standard human datasets, as well as tigers, cheetahs, fish, colobus monkeys, chimpanzees, and flamingos from videos captured casually in a zoo. We release the codebase for MBW as well as this challenging zoo dataset consisting of image frames of tail-end distribution categories with their corresponding 2D and 3D labels generated from minimal human intervention.",Datasets & Benchmarks,NeurIPS,2022,Poster,Mosam Dabhi;Chaoyang Wang;Tim Clifford;Laszlo Attila Jeni;Ian R. Fasel;Simon Lucey,True,https://openreview.net/pdf?id=i1bFPSw42W0
iAxH-ikIP0I,TaiSu: A 166M Large-scale High-Quality Dataset for Chinese Vision-Language Pre-training,"Vision-Language Pre-training (VLP) has been shown to be an efficient method to improve the performance of models on different vision-and-language downstream tasks. Substantial studies have shown that neural networks may be able to learn some general rules about language and visual concepts from a large-scale weakly labeled image-text dataset. However, most of the public cross-modal datasets that contain more than 100M image-text pairs are in English; there is a lack of available large-scale and high-quality Chinese VLP datasets. In this work, we propose a new framework for automatic dataset acquisition and cleaning with which we construct a new large-scale and high-quality cross-modal dataset named as TaiSu, containing 166 million images and 219 million Chinese captions. Compared with the recently released Wukong dataset, our dataset is achieved with much stricter restrictions on the semantic correlation of image-text pairs. We also propose to combine texts collected from the web with texts generated by a pre-trained image-captioning model. To the best of our knowledge, TaiSu is currently the largest publicly accessible Chinese cross-modal dataset. Furthermore, we test our dataset on several vision-language downstream tasks. TaiSu outperforms BriVL by a large margin on the zero-shot image-text retrieval task and zero-shot image classification task. TaiSu also shows better performance than Wukong on the image-retrieval task without using image augmentation for training. Results demonstrate that TaiSu can serve as a promising VLP dataset, both for understanding and generative tasks. More information can be referred to https://github.com/ksOAn6g5/TaiSu.",Datasets & Benchmarks,NeurIPS,2022,Poster,Yulong Liu;Guibo Zhu;Bin Zhu;Qi Song;Guojing Ge;Haoran Chen;GuanHui Qiao;Ru Peng;Lingxiang Wu;Jinqiao Wang,True,https://openreview.net/pdf?id=iAxH-ikIP0I
in7XC5RcjEn,Long Range Graph Benchmark,"Graph Neural Networks (GNNs) that are based on the message passing (MP) paradigm generally exchange information between 1-hop neighbors to build node representations at each layer. In principle, such networks are not able to capture long-range interactions (LRI) that may be desired or necessary for learning a given task on graphs. Recently, there has been an increasing interest in development of Transformer-based methods for graphs that can consider full node connectivity beyond the original sparse structure, thus enabling the modeling of LRI. However, MP-GNNs that simply rely on 1-hop message passing often fare better in several existing graph benchmarks when combined with positional feature representations, among other innovations, hence limiting the perceived utility and ranking of Transformer-like architectures. Here, we present the Long Range Graph Benchmark (LRGB) with 5 graph learning datasets: $\\\\texttt{PascalVOC-SP}$, $\\\\texttt{COCO-SP}$, $\\\\texttt{PCQM-Contact}$, $\\\\texttt{Peptides-func}$ and $\\\\texttt{Peptides-struct}$ that arguably require LRI reasoning to achieve strong performance in a given task. We benchmark both baseline GNNs and Graph Transformer networks to verify that the models which capture long-range dependencies perform significantly better on these tasks. Therefore, these datasets are suitable for benchmarking and exploration of MP GNNs and Graph Transformer architectures that are intended to capture LRI.",Datasets & Benchmarks,NeurIPS,2022,Poster,Vijay Prakash Dwivedi;Ladislav Rampášek;Mikhail Galkin;Ali Parviz;Guy Wolf;Anh Tuan Luu;Dominique Beaini,True,https://openreview.net/pdf?id=in7XC5RcjEn
jIIzJaMbfw,StrokeRehab: A Benchmark Dataset for Sub-second Action Identification," Automatic action identification from video and kinematic data is an important machine learning problem with applications ranging from robotics to smart health. Most existing works focus on identifying coarse actions such as running, climbing,  or cutting vegetables, which have relatively long durations and a complex series of motions. This is an important limitation for applications that require identification of more elemental motions at high temporal resolution. For example, in the rehabilitation of arm impairment after stroke, quantifying the training dose (number of repetitions) requires differentiating motions with sub-second durations. Our goal is to bridge this gap. To this end, we introduce a large-scale, multimodal dataset, StrokeRehab, as a new action-recognition benchmark that includes elemental short-duration actions labeled at a high temporal resolution. StrokeRehab consists of a high-quality inertial measurement unit sensor and video data of 51 stroke-impaired patients and 20 healthy subjects performing activities of daily living like feeding, brushing teeth, etc. Because it contains data from both healthy and impaired individuals, StrokeRehab can be used to study the influence of distribution shift in action-recognition tasks. When evaluated on StrokeRehab, current state-of-the-art models for action segmentation produce noisy predictions, which reduces their accuracy in identifying the corresponding sequence of actions. To address this, we propose a novel approach for high-resolution action identification, inspired by speech-recognition techniques, which is based on a sequence-to-sequence model that directly predicts the sequence of actions. This approach outperforms current state-of-the-art methods on StrokeRehab, as well as on the standard benchmark datasets 50Salads, Breakfast, and Jigsaws.",Datasets & Benchmarks,NeurIPS,2022,Poster,Aakash Kaku;Kangning Liu;Avinash Parnandi;Haresh Rengaraj Rajamohan;Kannan Venkataramanan;Anita Venkatesan;Audre Wirtanen;Natasha Pandit;Heidi Schambra;Carlos Fernandez-Granda,True,https://openreview.net/pdf?id=jIIzJaMbfw
jNdLszxdtra,NeoRL: A Near Real-World Benchmark for Offline Reinforcement Learning,"Offline reinforcement learning (RL) aims at learning effective policies from historical data without extra environment interactions. During our experience of applying offline RL, we noticed that previous offline RL benchmarks commonly involve significant reality gaps, which we have identified include rich and overly exploratory datasets, degraded baseline, and missing policy validation. In many real-world situations, to ensure system safety, running an overly exploratory policy to collect various data is prohibited, thus only a narrow data distribution is available. The resulting policy is regarded as effective if it is better than the working behavior policy; the policy model can be deployed only if it has been well validated, rather than accomplished the training. In this paper, we present a Near real-world offline RL benchmark, named NeoRL, to reflect these properties. NeoRL datasets are collected with a more conservative strategy. Moreover, NeoRL contains the offline training and offline validation pipeline before the online test, corresponding to real-world situations. We then evaluate recent state-of-the-art offline RL algorithms in NeoRL. The empirical results demonstrate that some offline RL algorithms are less competitive to the behavior cloning and the deterministic behavior policy, implying that they could be less effective in real-world tasks than in the previous benchmarks. We also disclose that current offline policy evaluation methods could hardly select the best policy. We hope this work will shed some light on future research and deploying RL in real-world systems.",Datasets & Benchmarks,NeurIPS,2022,Poster,Rong-Jun Qin;Xingyuan Zhang;Songyi Gao;Xiong-Hui Chen;Zewen Li;Weinan Zhang;Yang Yu,True,https://openreview.net/pdf?id=jNdLszxdtra
jbdp9m7nr0R,How Would The Viewer Feel? Estimating Wellbeing From Video Scenarios,"In recent years, deep neural networks have demonstrated increasingly strong abilities to recognize objects and activities in videos. However, as video understanding becomes widely used in real-world applications, a key consideration is developing human-centric systems that understand not only the content of the video but also how it would affect the wellbeing and emotional state of viewers. To facilitate research in this setting, we introduce two large-scale datasets with over 60,000 videos manually annotated for emotional response and subjective wellbeing. The Video Cognitive Empathy (VCE) dataset contains annotations for distributions of fine-grained emotional responses, allowing models to gain a detailed understanding of affective states. The Video to Valence (V2V) dataset contains annotations of relative pleasantness between videos, which enables predicting a continuous spectrum of wellbeing. In experiments, we show how video models that are primarily trained to recognize actions and find contours of objects can be repurposed to understand human preferences and the emotional content of videos. Although there is room for improvement, predicting wellbeing and emotional response is on the horizon for state-of-the-art models. We hope our datasets can help foster further advances at the intersection of commonsense video understanding and human preference learning.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Mantas Mazeika;Eric Tang;Andy Zou;Steven Basart;Jun Shern Chan;Dawn Song;David Forsyth;Jacob Steinhardt;Dan Hendrycks,True,https://openreview.net/pdf?id=jbdp9m7nr0R
k3462dQtQhg,A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks,"Textual backdoor attacks are a kind of practical threat to NLP systems. By injecting a backdoor in the training phase, the adversary could control model predictions via predefined triggers. As various attack and defense models have been proposed, it is of great significance to perform rigorous evaluations. However, we highlight two issues in previous backdoor learning evaluations: (1) The differences between real-world scenarios (e.g. releasing poisoned datasets or models) are neglected, and we argue that each scenario has its own constraints and concerns, thus requires specific evaluation protocols; (2) The evaluation metrics only consider whether the attacks could flip the models' predictions on poisoned samples and retain performances on benign samples, but ignore that poisoned samples should also be stealthy and semantic-preserving. To address these issues, we categorize existing works into three practical scenarios in which attackers release datasets, pre-trained models, and fine-tuned models respectively, then discuss their unique evaluation methodologies. On metrics, to completely evaluate poisoned samples, we use grammar error increase and perplexity difference for stealthiness, along with text similarity for validity. After formalizing the frameworks, we develop an open-source toolkit OpenBackdoor to foster the implementations and evaluations of textual backdoor learning. With this toolkit, we perform extensive experiments to benchmark attack and defense models under the suggested paradigm. To facilitate the underexplored defenses against poisoned datasets, we further propose CUBE, a simple yet strong clustering-based defense baseline. We hope that our frameworks and benchmarks could serve as the cornerstones for future model development and evaluations.",Datasets & Benchmarks,NeurIPS,2022,Poster,Ganqu Cui;Lifan Yuan;Bingxiang He;Yangyi Chen;Zhiyuan Liu;Maosong Sun,False,https://openreview.net/pdf?id=k3462dQtQhg
kCTZt0b9DQz,Prototypical VoteNet for Few-Shot 3D Point Cloud Object Detection,"Most existing 3D point cloud object detection approaches heavily rely on large amounts of labeled training data. However, the labeling process is costly and time-consuming. This paper considers few-shot 3D point cloud object detection, where only a few annotated samples of novel classes are needed with abundant samples of base classes. To this end, we propose Prototypical VoteNet to recognize and localize novel instances, which incorporates two new modules: Prototypical Vote Module (PVM) and Prototypical Head Module (PHM). Specifically, as the 3D basic geometric structures can be shared among categories, PVM is designed to leverage class-agnostic geometric prototypes, which are learned from base classes, to refine local features of novel categories. Then PHM is proposed to utilize class prototypes to enhance the global feature of each object, facilitating subsequent object localization and classification, which is trained by the episodic training strategy. To evaluate the model in this new setting, we contribute two new benchmark datasets, FS-ScanNet and FS-SUNRGBD. We conduct extensive experiments to demonstrate the effectiveness of Prototypical VoteNet, and our proposed method shows significant and consistent improvements compared to baselines on two benchmark datasets. ",main,NeurIPS,2022,Poster,Shizhen Zhao;XIAOJUAN QI,True,https://openreview.net/pdf?id=kCTZt0b9DQz
kQUOIyPg-ux,SurDis: A Surface Discontinuity Dataset for Wearable Technology to Assist Blind Navigation in Urban Environments,"According to World Health Organization, there is an estimated 2.2 billion people with a near or distance vision impairment worldwide. Difficulty in self-navigation is one of the greatest challenges to independence for the blind and low vision (BLV) people. Through consultations with several BLV service providers, we realized that negotiating surface discontinuities is one of the very prominent challenges when navigating an outdoor environment within the urban. Surface discontinuities are commonly formed by rises and drop-offs along a pathway. They could be a threat to balancing during a walk and perceiving such a threat is highly challenging to the BLVs. In this paper, we introduce SurDis, a novel dataset of depth maps and stereo images that exemplifies the issue of surface discontinuity in the urban areas of Klang Valley, Malaysia. We seek to address the limitation of existing datasets of such nature in these areas. Current mobility tools for the BLVs predominantly focus on furniture, indoor built environments, traffic signs, vehicles, humans and various types of objects' detection above the surface of a pathway. We emphasize a specific purpose for SurDis – to support the development of assistive wearable technology for the BLVs to negotiate surface discontinuity. We consulted BLV volunteers on the specifications of surface condition that could become hazardous for navigation using 3D printed replicas of actual scaled-down scenes, and identified locations that are frequented by the BLVs as our target data collection fields. With feedback from these volunteers, we developed a lightweight, small and unobtrusive prototype equipped with a tiny stereo camera and an embedded system on a single board computer to capture the samples from 10 different locations. We describe instrument development, data collection, preprocessing, annotation, and experiments conducted. The dataset contains: (1) more than 17000 depth maps generated from 200 sets of stereo image sequences, (2) annotations of surface discontinuity in the depth maps, and (3) bitmap stereo image pairs corresponding to the depth maps in (1).",Datasets & Benchmarks,NeurIPS,2022,Poster,Kuan Yew Leong;Siew Mooi Lim,True,https://openreview.net/pdf?id=kQUOIyPg-ux
lRUCfzs5Hzg,How Transferable are Video Representations Based on Synthetic Data?,"Action recognition has improved dramatically with massive-scale video datasets. Yet, these datasets are accompanied with issues related to curation cost, privacy, ethics, bias, and copyright. Compared to that, only minor efforts have been devoted toward exploring the potential of synthetic video data. In this work, as a stepping stone towards addressing these shortcomings, we study the transferability of video representations learned solely from synthetically-generated video clips, instead of real data. We propose SynAPT, a novel benchmark for action recognition based on a combination of existing synthetic datasets, in which a model is pre-trained on synthetic videos rendered by various graphics simulators, and then transferred to a set of downstream action recognition datasets, containing different categories than the synthetic data. We provide an extensive baseline analysis on SynAPT revealing that the simulation-to-real gap is minor for datasets with low object and scene bias, where models pre-trained with synthetic data even outperform their real data counterparts. We posit that the gap between real and synthetic action representations can be attributed to contextual bias and static objects related to the action, instead of the temporal dynamics of the action itself. The SynAPT benchmark is available at https://github.com/mintjohnkim/SynAPT.",Datasets & Benchmarks,NeurIPS,2022,Poster,Yo-whan Kim;Samarth Mishra;SouYoung Jin;Rameswar Panda;Hilde Kuehne;Leonid Karlinsky;Venkatesh Saligrama;Kate Saenko;Aude Oliva;Rogerio Feris,True,https://openreview.net/pdf?id=lRUCfzs5Hzg
lzZstLVGVGW,Earthformer: Exploring Space-Time Transformers for Earth System Forecasting,"Conventionally, Earth system (e.g., weather and climate) forecasting relies on numerical simulation with complex physical models and hence is both expensive in computation and demanding on domain expertise. With the explosive growth of spatiotemporal Earth observation data in the past decade, data-driven models that apply Deep Learning (DL) are demonstrating impressive potential for various Earth system forecasting tasks. The Transformer as an emerging DL architecture, despite its broad success in other domains, has limited adoption in this area. In this paper, we propose Earthformer, a space-time Transformer for Earth system forecasting. Earthformer is based on a generic, flexible and efficient space-time attention block, named Cuboid Attention. The idea is to decompose the data into cuboids and apply cuboid-level self-attention in parallel. These cuboids are further connected with a collection of global vectors. We conduct experiments on the MovingMNIST dataset and a newly proposed chaotic $N$-body MNIST dataset to verify the effectiveness of cuboid attention and figure out the best design of Earthformer. Experiments on two real-world benchmarks about precipitation nowcasting and El Niño/Southern Oscillation (ENSO) forecasting show that Earthformer achieves state-of-the-art performance.",main,NeurIPS,2022,Poster,Zhihan Gao;Xingjian Shi;Hao Wang;Yi Zhu;Bernie Wang;Mu Li;Dit-Yan Yeung,True,https://openreview.net/pdf?id=lzZstLVGVGW
mJWt6pOcHNy,Breaking Bad: A Dataset for Geometric Fracture and Reassembly,"We introduce Breaking Bad, a large-scale dataset of fractured objects. Our dataset consists of over one million fractured objects simulated from ten thousand base models. The fracture simulation is powered by a recent physically based algorithm that efficiently generates a variety of fracture modes of an object. Existing shape assembly datasets decompose objects according to semantically meaningful parts, effectively modeling the construction process. In contrast, Breaking Bad models the destruction process of how a geometric object naturally breaks into fragments. Our dataset serves as a benchmark that enables the study of fractured object reassembly and presents new challenges for geometric shape understanding. We analyze our dataset with several geometry measurements and benchmark three state-of-the-art shape assembly deep learning methods under various settings. Extensive experimental results demonstrate the difficulty of our dataset, calling on future research in model designs specifically for the geometric shape assembly task. We host our dataset at https://breaking-bad-dataset.github.io/.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Silvia Sellán;Yun-Chun Chen;Ziyi Wu;Animesh Garg;Alec Jacobson,True,https://openreview.net/pdf?id=mJWt6pOcHNy
mV4EKzUVI96,APT-36K: A Large-scale Benchmark for Animal Pose Estimation and Tracking,"Animal pose estimation and tracking (APT) is a fundamental task for detecting and tracking animal keypoints from a sequence of video frames. Previous animal-related datasets focus either on animal tracking or single-frame animal pose estimation, and never on both aspects. The lack of APT datasets hinders the development and evaluation of video-based animal pose estimation and tracking methods, limiting the applications in real world, e.g., understanding animal behavior in wildlife conservation. To fill this gap, we make the first step and propose APT-36K, i.e., the first large-scale benchmark for animal pose estimation and tracking. Specifically, APT-36K consists of 2,400 video clips collected and filtered from 30 animal species with 15 frames for each video, resulting in 36,000 frames in total. After manual annotation and careful double-check, high-quality keypoint and tracking annotations are provided for all the animal instances. Based on APT-36K, we benchmark several representative models on the following three tracks: (1) supervised animal pose estimation on a single frame under intra- and inter-domain transfer learning settings, (2) inter-species domain generalization test for unseen animals, and (3) animal pose estimation with animal tracking. Based on the experimental results, we gain some empirical insights and show that APT-36K provides a useful animal pose estimation and tracking benchmark, offering new challenges and opportunities for future research. The code and dataset will be made publicly available at https://github.com/pandorgan/APT-36K.",Datasets & Benchmarks,NeurIPS,2022,Poster,Yuxiang Yang;Junjie Yang;Yufei Xu;Jing Zhang;Long Lan;Dacheng Tao,True,https://openreview.net/pdf?id=mV4EKzUVI96
mZke5vYdF99,OLIVES Dataset: Ophthalmic Labels for Investigating Visual Eye Semantics,"Clinical diagnosis of the eye is performed over multifarious data modalities including scalar clinical labels, vectorized biomarkers, two-dimensional fundus images, and three-dimensional Optical Coherence Tomography (OCT) scans. Clinical practitioners use all available data modalities for diagnosing and treating eye diseases like Diabetic Retinopathy (DR) or Diabetic Macular Edema (DME). Enabling usage of machine learning algorithms within the ophthalmic medical domain requires research into the relationships and interactions between all relevant data over a treatment period. Existing datasets are limited in that they neither provide data nor consider the explicit relationship modeling between the data modalities. In this paper, we introduce the Ophthalmic Labels for Investigating Visual Eye Semantics (OLIVES) dataset that addresses the above limitation. This is the first OCT and near-IR fundus dataset that includes clinical labels, biomarker labels, disease labels, and time-series patient treatment information from associated clinical trials. The dataset consists of 1268 near-IR fundus images each with at least 49 OCT scans, and 16 biomarkers, along with 4 clinical labels and a disease diagnosis of DR or DME. In total, there are 96 eyes' data averaged over a period of at least two years with each eye treated for an average of 66 weeks and 7 injections. We benchmark the utility of OLIVES dataset for ophthalmic data as well as provide benchmarks and concrete research directions for core and emerging machine learning paradigms within medical image analysis.",Datasets & Benchmarks,NeurIPS,2022,Poster,Mohit Prabhushankar;Kiran Premdat Kokilepersaud;Yash-yee Logan;Stephanie Trejo Corona;Ghassan AlRegib;Charles Wykoff,True,https://openreview.net/pdf?id=mZke5vYdF99
ml1NjI-ujzf,Active-Passive SimStereo - Benchmarking the Cross-Generalization Capabilities of Deep Learning-based Stereo Methods,"In stereo vision, self-similar or bland regions can make it difficult to match patches between two images. Active stereo-based methods mitigate this problem by projecting a pseudo-random pattern on the scene so that each patch of an image pair can be identified without ambiguity. However, the projected pattern significantly alters the appearance of the image. If this pattern acts as a form of adversarial noise, it could negatively impact the performance of deep learning-based methods, which are now the de-facto standard for dense stereo vision. In this paper, we propose the Active-Passive SimStereo dataset and a corresponding benchmark to evaluate the performance gap between passive and active stereo images for stereo matching algorithms. Using the proposed benchmark and an additional ablation study, we show that the feature extraction and matching modules of a selection of twenty selected deep learning-based stereo matching methods generalize to active stereo without a problem. However, the disparity refinement modules of three of the twenty architectures (ACVNet, CascadeStereo, and StereoNet) are negatively affected by the active stereo patterns due to their reliance on the appearance of the input images.

",Datasets & Benchmarks,NeurIPS,2022,Poster,Laurent Valentin Jospin;Allen Antony;Lian Xu;Hamid Laga;Farid Boussaid;Mohammed Bennamoun,True,https://openreview.net/pdf?id=ml1NjI-ujzf
mowt1WNhTC7,When does dough become a bagel? Analyzing the remaining mistakes on ImageNet,"Image classification accuracy on the ImageNet dataset has been a barometer for progress in computer vision over the last decade. Several recent papers have questioned the degree to which the benchmark remains useful to the community, yet innovations continue to contribute gains to performance, with today's largest models achieving 90%+ top-1 accuracy. To help contextualize progress on ImageNet and provide a more meaningful evaluation for today's state-of-the-art models, we manually review and categorize every remaining mistake that a few top models make in order to provide insight into the long-tail of errors on one of the most benchmarked datasets in computer vision. We focus on the multi-label subset evaluation of ImageNet, where today's best models achieve upwards of 97% top-1 accuracy. Our analysis reveals that nearly half of the supposed mistakes are not mistakes at all, and we uncover new valid multi-labels, demonstrating that, without careful review, we are significantly underestimating the performance of these models. On the other hand, we also find that today's best models still make a significant number of mistakes (40%) that are obviously wrong to human reviewers. To calibrate future progress on ImageNet, we provide an updated multi-label evaluation set, and we curate ImageNet-Major: a 68-example ""major error"" slice of the obvious mistakes made by today's top models -- a slice where models should achieve near perfection, but today are far from doing so.
",main,NeurIPS,2022,Poster,Vijay Vasudevan;Benjamin Caine;Raphael Gontijo-Lopes;Sara Fridovich-Keil;Rebecca Roelofs,True,https://openreview.net/pdf?id=mowt1WNhTC7
nE8_DvxAqAB,Egocentric Video-Language Pretraining,"Video-Language Pretraining (VLP), which aims to learn transferable representation to advance a wide range of video-text downstream tasks, has recently received increasing attention. Best performing works rely on large-scale, 3rd-person video-text datasets, such as HowTo100M. In this work, we exploit the recently released Ego4D dataset to pioneer Egocentric VLP along three directions. (i) We create EgoClip, a 1st-person video-text pretraining dataset comprising 3.8M clip-text pairs well-chosen from Ego4D, covering a large variety of human daily activities. (ii) We propose a novel pretraining objective, dubbed EgoNCE, which adapts video-text contrastive learning to the egocentric domain by mining egocentric-aware positive and negative samples. (iii) We introduce EgoMCQ, a development benchmark that is close to EgoClip and hence can support effective validation and fast exploration of our design decisions in EgoClip and EgoNCE. Furthermore, we demonstrate strong performance on five egocentric downstream tasks across three datasets: video-text retrieval on EPIC-KITCHENS-100; action recognition on Charades-Ego; natural language query, moment query, and object state change classification on Ego4D challenge benchmarks. The dataset and code are available at https://github.com/showlab/EgoVLP.",main,NeurIPS,2022,Poster,Kevin Qinghong Lin;Jinpeng Wang;Mattia Soldan;Michael Wray;Rui Yan;Zhongcong Xu;Difei Gao;Rong-Cheng Tu;Wenzhe Zhao;Weijie Kong;Chengfei Cai;WANG HongFa;Dima Damen;Bernard Ghanem;Wei Liu;Mike Zheng Shou,True,https://openreview.net/pdf?id=nE8_DvxAqAB
nQZHEunntbJ,AutoWS-Bench-101: Benchmarking Automated Weak Supervision with 100 Labels,"Weak supervision (WS) is a powerful method to build labeled datasets for training supervised models in the face of little-to-no labeled data. It replaces hand-labeling data with aggregating multiple noisy-but-cheap label estimates expressed by labeling functions (LFs). While it has been used successfully in many domains, weak supervision's application scope is limited by the difficulty of constructing labeling functions for domains with complex or high-dimensional features. To address this, a handful of methods have proposed automating the LF design process using a small set of ground truth labels. In this work, we introduce AutoWS-Bench-101: a framework for evaluating automated WS (AutoWS) techniques in challenging WS settings---a set of diverse application domains on which it has been previously difficult or impossible to apply traditional WS techniques. While AutoWS is a promising direction toward expanding the application-scope of WS, the emergence of powerful methods such as zero-shot foundation models reveal the need to understand how AutoWS techniques compare or cooperate with modern zero-shot or few-shot learners. This informs the central question of AutoWS-Bench-101: given an initial set of 100 labels for each task, we ask whether a practitioner should use an AutoWS method to generate additional labels or use some simpler baseline, such as zero-shot predictions from a foundation model or supervised learning. We observe that it is necessary for AutoWS methods to incorporate signal from foundation models if they are to outperform simple few-shot baselines, and AutoWS-Bench-101 promotes future research in this direction. We conclude with a thorough ablation study of AutoWS methods. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Nicholas Roberts;Xintong Li;Tzu-Heng Huang;Dyah Adila;Spencer Schoenberg;Cheng-Yu Liu;Lauren Pick;Haotian Ma;Aws Albarghouthi;Frederic Sala,False,https://openreview.net/pdf?id=nQZHEunntbJ
nRcyGtY2kBC,Transfer Learning on Heterogeneous Feature Spaces for Treatment Effects Estimation,"Consider the problem of improving the estimation of conditional average treatment effects (CATE) for a target domain of interest by leveraging related information from a source domain with a different feature space. This heterogeneous transfer learning problem for CATE estimation is ubiquitous in areas such as healthcare where we may wish to evaluate the effectiveness of a treatment for a new patient population for which different clinical covariates and limited data are available. In this paper, we address this problem by introducing several building blocks that use representation learning to handle the heterogeneous feature spaces and a flexible multi-task architecture with shared and private layers to transfer information between potential outcome functions across domains. Then, we show how these building blocks can be used to recover transfer learning equivalents of the standard CATE learners. On a new semi-synthetic data simulation benchmark for heterogeneous transfer learning, we not only demonstrate performance improvements of our heterogeneous transfer causal effect learners across datasets, but also provide insights into the differences between these learners from a transfer perspective. ",main,NeurIPS,2022,Poster,Ioana Bica;Mihaela van der Schaar,True,https://openreview.net/pdf?id=nRcyGtY2kBC
nUTemM6v9sv,HandMeThat: Human-Robot Communication in Physical and Social Environments,"We introduce HandMeThat, a benchmark for a holistic evaluation of instruction understanding and following in physical and social environments. While previous datasets primarily focused on language grounding and planning, HandMeThat considers the resolution of human instructions with ambiguities based on the physical (object states and relations) and social (human actions and goals) information. HandMeThat contains 10,000 episodes of human-robot interactions. In each episode, the robot first observes a trajectory of human actions towards her internal goal. Next, the robot receives a human instruction and should take actions to accomplish the subgoal set through the instruction. 
In this paper, we present a textual interface for our benchmark, where the robot interacts with a virtual environment through textual commands. We evaluate several baseline models on HandMeThat, and show that both offline and online reinforcement learning algorithms perform poorly on HandMeThat, suggesting significant room for future work on physical and social human-robot communications and interactions.",Datasets & Benchmarks,NeurIPS,2022,Poster,Yanming Wan;Jiayuan Mao;Joshua B. Tenenbaum,True,https://openreview.net/pdf?id=nUTemM6v9sv
o4uFFg9_TpV,Visual Prompting via Image Inpainting,"How does one adapt a pre-trained visual model to novel downstream tasks without task-specific finetuning or any model modification? Inspired by prompting in NLP, this paper investigates visual prompting: given input-output image example(s) of a new task at test time and a new input image, the goal is to automatically produce the output image, consistent with the given examples. We show that posing this problem as simple image inpainting -- literally just filling in a hole in a concatenated visual prompt image -- turns out to be surprisingly effective, provided that the inpainting algorithm has been trained on the right data. We train masked auto-encoders on a new dataset that we curated -- 88k unlabeled figures from academic papers sources on Arxiv. We apply visual prompting to these pretrained models and demonstrate results on various downstream image-to-image tasks, including foreground segmentation, single object detection, colorization, edge detection, etc. Project page: https://yossigandelsman.github.io/visual_prompt",main,NeurIPS,2022,Poster,Amir Bar;Yossi Gandelsman;Trevor Darrell;Amir Globerson;Alexei A Efros,True,https://openreview.net/pdf?id=o4uFFg9_TpV
ofwkaIWFqqv,GRASP: Navigating Retrosynthetic Planning with Goal-driven Policy,"Retrosynthetic planning occupies a crucial position in synthetic chemistry and, accordingly, drug discovery, which aims to find synthetic pathways of a target molecule through a sequential decision-making process on a set of feasible reactions. While the majority of recent works focus on the prediction of feasible reactions at each step, there have been limited attempts toward improving the sequential decision-making policy. Existing strategies rely on either the expensive and high-variance value estimation by online rollout, or a settled value estimation neural network pre-trained with simulated pathways of limited diversity and no negative feedback. Besides, how to return multiple candidate pathways that are not only diverse but also desirable for chemists (e.g., affordable building block materials) remains an open challenge. To this end, we propose a Goal-dRiven Actor-critic retroSynthetic Planning (GRASP) framework, where we identify the policy that performs goal-driven retrosynthesis navigation toward a user-demand objective. Our experiments on the benchmark Pistachio dataset and a chemists-designed dataset demonstrate that the framework outperforms state-of-the-art approaches by up to 32.2% on search efficiency and 5.6% on quality. Remarkably, our user studies show that GRASP successfully plans pathways that accomplish the goal prescribed with a designated goal (building block materials).",main,NeurIPS,2022,Poster,Yemin Yu;Ying Wei;Kun Kuang;Zhengxing Huang;Huaxiu Yao;Fei Wu,True,https://openreview.net/pdf?id=ofwkaIWFqqv
olvz0gAdGOX,ActionSense: A Multimodal Dataset and Recording Framework for Human Activities Using Wearable Sensors in a Kitchen Environment,"This paper introduces ActionSense, a multimodal dataset and recording framework with an emphasis on wearable sensing in a kitchen environment.  It provides rich, synchronized data streams along with ground truth data to facilitate learning pipelines that could extract insights about how humans interact with the physical world during activities of daily living, and help lead to more capable and collaborative robot assistants.  The wearable sensing suite captures motion, force, and attention information; it includes eye tracking with a first-person camera, forearm muscle activity sensors, a body-tracking system using 17 inertial sensors, finger-tracking gloves, and custom tactile sensors on the hands that use a matrix of conductive threads.  This is coupled with activity labels and with externally-captured data from multiple RGB cameras, a depth camera, and microphones.  The specific tasks recorded in ActionSense are designed to highlight lower-level physical skills and higher-level scene reasoning or action planning.  They include simple object manipulations (e.g., stacking plates), dexterous actions (e.g., peeling or cutting vegetables), and complex action sequences (e.g., setting a table or loading a dishwasher).  The resulting dataset and underlying experiment framework are available at https://action-sense.csail.mit.edu. Preliminary networks and analyses explore modality subsets and cross-modal correlations.  ActionSense aims to support applications including learning from demonstrations, dexterous robot control, cross-modal predictions, and fine-grained action segmentation. It could also help inform the next generation of smart textiles that may one day unobtrusively send rich data streams to in-home collaborative or autonomous robot assistants.",Datasets & Benchmarks,NeurIPS,2022,Poster,Joseph DelPreto;Chao Liu;Yiyue Luo;Michael Foshey;Yunzhu Li;Antonio Torralba;Wojciech Matusik;Daniela Rus,True,https://openreview.net/pdf?id=olvz0gAdGOX
pCrB8orUkSq,Monocular Dynamic View Synthesis: A Reality Check,"We study the recent progress on dynamic view synthesis (DVS) from monocular video. Though existing approaches have demonstrated impressive results, we show a discrepancy between the practical capture process and the existing experimental protocols, which effectively leaks in multi-view signals during training. We define effective multi-view factors (EMFs) to quantify the amount of multi-view signal present in the input capture sequence based on the relative camera-scene motion. We introduce two new metrics: co-visibility masked image metrics and correspondence accuracy, which overcome the issue in existing protocols. We also propose a new iPhone dataset that includes more diverse real-life deformation sequences. Using our proposed experimental protocol, we show that the state-of-the-art approaches observe a 1-2 dB drop in masked PSNR in the absence of multi-view cues and 4-5 dB drop when modeling complex motion. Code and data can be found at http://hangg7.com/dycheck.",main,NeurIPS,2022,Poster,Hang Gao;Ruilong Li;Shubham Tulsiani;Bryan Russell;Angjoo Kanazawa,True,https://openreview.net/pdf?id=pCrB8orUkSq
pvvPh5sSJTC,Is one annotation enough? - A data-centric image classification benchmark for noisy and ambiguous label estimation,"High-quality data is necessary for modern machine learning. However, the acquisition of such data is difficult due to noisy and ambiguous annotations of humans. The aggregation of such annotations to determine the label of an image leads to a lower data quality. We propose a data-centric image classification benchmark with nine real-world datasets and multiple annotations per image to allow researchers to investigate and quantify the impact of such data quality issues. With the benchmark we can study the impact of annotation costs and (semi-)supervised methods on the data quality for image classification by applying a novel methodology to a range of different algorithms and diverse datasets. Our benchmark uses a two-phase approach via a data label improvement method in the first phase and a fixed evaluation model in the second phase. Thereby, we give a measure for the relation between the input labeling effort and the performance of (semi-)supervised algorithms to enable a deeper insight into how labels should be created for effective model training. Across thousands of experiments, we show that one annotation is not enough and that the inclusion of multiple annotations allows for a better approximation of the real underlying class distribution. We identify that hard labels can not capture the ambiguity of the data and this might lead to the common issue of overconfident models. Based on the presented datasets, benchmarked methods, and analysis, we create multiple research opportunities for the future directed at the improvement of label noise estimation approaches, data annotation schemes, realistic (semi-)supervised learning, or more reliable image collection. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Lars Schmarje;Vasco Grossmann;Claudius Zelenka;Sabine Dippel;Rainer Kiko;Mariusz Oszust;Matti Pastell;Jenny Stracke;Anna Valros;Nina Volkmann;Reinhard Koch,True,https://openreview.net/pdf?id=pvvPh5sSJTC
qiDmAaG6mP,"M4Singer: A Multi-Style, Multi-Singer and Musical Score Provided Mandarin Singing Corpus","The lack of publicly available high-quality and accurately labeled datasets has long been a major bottleneck for singing voice synthesis (SVS). To tackle this problem, we present M4Singer, a free-to-use Multi-style, Multi-singer Mandarin singing collection with elaborately annotated Musical scores as well as its benchmarks. Specifically, 1) we construct and release a large high-quality Chinese singing voice corpus, which is recorded by 20 professional singers, covering 700 Chinese pop songs as well as all the four SATB types (i.e.,  soprano, alto, tenor, and bass); 2) we take extensive efforts to manually compose the musical scores for each recorded song, which are necessary to the study of the prosody modeling for SVS. 3) To facilitate the use and demonstrate the quality of M4Singer, we conduct four different benchmark experiments: score-based SVS, controllable singing voice (CSV), singing voice conversion (SVC) and automatic music transcription (AMT).",Datasets & Benchmarks,NeurIPS,2022,Poster,Lichao Zhang;Ruiqi Li;Shoutong Wang;Liqun Deng;Jinglin Liu;Yi Ren;Jinzheng He;Rongjie Huang;Jieming Zhu;Xiao Chen;Zhou Zhao,True,https://openreview.net/pdf?id=qiDmAaG6mP
qnfYsave0U4,The Dollar Street Dataset: Images Representing the Geographic and Socioeconomic Diversity of the World,"It is crucial that image datasets for computer vision are representative and contain accurate demographic information to ensure their robustness and fairness, especially for smaller subpopulations. To address this issue, we present Dollar Street - a supervised dataset that contains 38,479 images of everyday household items from homes around the world. This dataset was manually curated and fully labeled, including tags for objects (e.g. “toilet,” “toothbrush,” “stove”) and demographic data such as region, country and home monthly income. This dataset includes images from homes with no internet access and incomes as low as \\\\$26.99 per month, visually capturing valuable socioeconomic diversity of traditionally under-represented populations. All images and data are licensed under CC-BY, permitting their use in academic and commercial work. Moreover, we show that this dataset can improve the performance of classification tasks for images of household items from lower income homes, addressing a critical need for datasets that combat bias.",Datasets & Benchmarks,NeurIPS,2022,Poster,William A Gaviria Rojas;Sudnya Diamos;Keertan Ranjan Kini;David Kanter;Vijay Janapa Reddi;Cody Coleman,True,https://openreview.net/pdf?id=qnfYsave0U4
r2DdJQ9AJvI,TGEA 2.0: A Large-Scale Diagnostically Annotated Dataset with Benchmark Tasks for Text Generation of Pretrained Language Models,"In order to diagnostically analyze and improve the capability of pretrained language models (PLMs) in text generation, we propose TGEA 2.0, to date the largest dataset built on machine-authored texts by PLMs with fine-grained semantic annotations on a wide variety of pathological generation errors. We collect 170K nominal, phrasal and sentential prompts from 6M natural sentences in 3 domains. These prompts are fed into 4 generative PLMs with their best decoding strategy to generate paragraphs. 195,629 sentences are extracted from these generated paragraphs for manual annotation, where 36K erroneous sentences are detected, 42K erroneous spans are located and categorized into an error type defined in a two-level error taxonomy. We define a \\\\textbf{Mi}nimal \\\\textbf{S}et of \\\\textbf{E}rror-related \\\\textbf{W}ords (MiSEW) for each erroneous span, which not only provides error-associated words but also rationalizes the reasoning behind the error. Quality control with a pre-annotation and feedback loop is performed before and during the entire annotation process. With the diagnostically annotated dataset, we propose 5 diagnosis benchmark tasks (i.e., erroneous text detection, MiSEW extraction, erroneous span location and correction together with error type classification) and 2 pathology mitigation benchmark tasks (pairwise comparison and word prediction). Experiment results on these benchmark tasks demonstrate that TGEA 2.0 is a challenging dataset that could facilitate further research on automatic diagnosis and pathology mitigation over machine texts. The dataset will be publicly available at https://github.com/tjunlp-lab/TGEA/.",Datasets & Benchmarks,NeurIPS,2022,Poster,Huibin Ge;Xiaohu Zhao;Chuang Liu;Yulong Zeng;Qun Liu;Deyi Xiong,True,https://openreview.net/pdf?id=r2DdJQ9AJvI
rG7HZZtIc-,D^2NeRF: Self-Supervised Decoupling of Dynamic and Static Objects from a Monocular Video,"Given a monocular video, segmenting and decoupling dynamic objects while recovering the static environment is a widely studied problem in machine intelligence. Existing solutions usually approach this problem in the image domain, limiting their performance and understanding of the environment. We introduce Decoupled Dynamic Neural Radiance Field (D^2NeRF), a self-supervised approach that takes a monocular video and learns a 3D scene representation which decouples moving objects, including their shadows, from the static background. Our method represents the moving objects and the static background by two separate neural radiance fields with only one allowing for temporal changes. A naive implementation of this approach leads to the dynamic component taking over the static one as the representation of the former is inherently more general and prone to overfitting. To this end, we propose a novel loss to promote correct separation of phenomena. We further propose a shadow field network to detect and decouple dynamically moving shadows. We introduce a new dataset containing various dynamic objects and shadows and demonstrate that our method can achieve better performance than state-of-the-art approaches in decoupling dynamic and static 3D objects, occlusion and shadow removal, and image segmentation for moving objects. Project page: https://d2nerf.github.io/",main,NeurIPS,2022,Poster,Tianhao Walter Wu;Fangcheng Zhong;Andrea Tagliasacchi;Forrester Cole;Cengiz Oztireli,True,https://openreview.net/pdf?id=rG7HZZtIc-
rbrouCKPiej,AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection,"Analyzing the distribution shift of data is a growing research direction in nowadays Machine Learning (ML), leading to emerging new benchmarks that focus on providing a suitable scenario for studying the generalization properties of ML models. The existing benchmarks are focused on supervised learning, and to the best of our knowledge, there is none for unsupervised learning. Therefore, we introduce an unsupervised anomaly detection benchmark with data that shifts over time, built over Kyoto-2006+, a traffic dataset for network intrusion detection. This type of data meets the premise of shifting the input distribution: it covers a large time span (10 years), with naturally occurring changes over time (e.g. users modifying their behavior patterns, and software updates). We first highlight the non-stationary nature of the data, using a basic per-feature analysis, t-SNE, and an Optimal Transport approach for measuring the overall distribution distances between years. Next, we propose AnoShift, a protocol splitting the data in IID, NEAR, and FAR testing splits. We validate the performance degradation over time with diverse models, ranging from classical approaches to deep learning. Finally, we show that by acknowledging the distribution shift problem and properly addressing it, the performance can be improved compared to the classical training which assumes independent and identically distributed data (on average, by up to 3% for our approach). Dataset and code are available at https://github.com/bit-ml/AnoShift/.",Datasets & Benchmarks,NeurIPS,2022,Poster,Marius Dragoi;Elena Burceanu;Emanuela Haller;Andrei Manolache;Florin Brad,False,https://openreview.net/pdf?id=rbrouCKPiej
rc8o_j8I8PX,MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge,"Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable agent architecture. We introduce MineDojo, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MineDojo's data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of open-ended tasks specified in free-form language without any manually designed dense shaping reward. We open-source the simulation suite, knowledge bases, algorithm implementation, and pretrained models (https://minedojo.org) to promote research towards the goal of generally capable embodied agents.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Linxi Fan;Guanzhi Wang;Yunfan Jiang;Ajay Mandlekar;Yuncong Yang;Haoyi Zhu;Andrew Tang;De-An Huang;Yuke Zhu;Anima Anandkumar,True,https://openreview.net/pdf?id=rc8o_j8I8PX
rjBYortWdRV,Benchmarking and Analyzing 3D Human Pose and Shape Estimation Beyond Algorithms,"3D human pose and shape estimation (a.k.a. ``human mesh recovery'') has achieved substantial progress. Researchers mainly focus on the development of novel algorithms, while less attention has been paid to other critical factors involved. This could lead to less optimal baselines, hindering the fair and faithful evaluations of newly designed methodologies. To address this problem, this work presents the \\\\textit{first} comprehensive benchmarking study from three under-explored perspectives beyond algorithms. \\\\emph{1) Datasets.} An analysis on 31 datasets reveals the distinct impacts of data samples: datasets featuring critical attributes (\\\\emph{i.e.} diverse poses, shapes, camera characteristics, backbone features) are more effective. Strategical selection and combination of high-quality datasets can yield a significant boost to the model performance. \\\\emph{2) Backbones.} Experiments with 10 backbones, ranging from CNNs to transformers, show the knowledge learnt from a proximity task is readily transferable to human mesh recovery. \\\\emph{3) Training strategies.} Proper augmentation techniques and loss designs are crucial. With the above findings, we achieve a PA-MPJPE of 47.3 \\\\(mm\\\\) on the 3DPW test set with a relatively simple model. More importantly, we provide strong baselines for fair comparisons of algorithms, and recommendations for building effective training configurations in the future. Codebase is available at \\\\url{https://github.com/smplbody/hmr-benchmarks}.",Datasets & Benchmarks,NeurIPS,2022,Poster,Hui En Pang;Zhongang Cai;Lei Yang;Tianwei Zhang;Ziwei Liu,False,https://openreview.net/pdf?id=rjBYortWdRV
sWOdnSkB0qu,MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control,"Simulated humanoids are an appealing research domain due to their physical capabilities. Nonetheless, they are also challenging to control, as a policy must drive an unstable, discontinuous, and high-dimensional physical system. One widely studied approach is to utilize motion capture (MoCap) data to teach the humanoid agent low-level skills (e.g., standing, walking, and running) that can then be re-used to synthesize high-level behaviors. However, even with MoCap data, controlling simulated humanoids remains very hard, as MoCap data offers only kinematic information. Finding physical control inputs to realize the demonstrated motions requires computationally intensive methods like reinforcement learning. Thus, despite the publicly available MoCap data, its utility has been limited to institutions with large-scale compute. In this work, we dramatically lower the barrier for productive research on this topic by training and releasing high-quality agents that can track over three hours of MoCap data for a simulated humanoid in the dm_control physics-based environment. We release MoCapAct (Motion Capture with Actions), a dataset of these expert agents and their rollouts, which contain proprioceptive observations and actions. We demonstrate the utility of MoCapAct by using it to train a single hierarchical policy capable of tracking the entire MoCap dataset within dm_control and show the learned low-level component can be re-used to efficiently learn downstream high-level tasks. Finally, we use MoCapAct to train an autoregressive GPT model and show that it can control a simulated humanoid to perform natural motion completion given a motion prompt.

Videos of the results and links to the code and dataset are available at https://microsoft.github.io/MoCapAct.",Datasets & Benchmarks,NeurIPS,2022,Poster,Nolan Wagener;Andrey Kolobov;Felipe Vieira Frujeri;Ricky Loynd;Ching-An Cheng;Matthew Hausknecht,True,https://openreview.net/pdf?id=sWOdnSkB0qu
sd1fv0g3UO1,CLEVRER-Humans: Describing Physical and Causal Events the Human Way,"Building machines that can reason about physical events and their causal relationships is crucial for flexible interaction with the physical world. However, most existing physical and causal reasoning benchmarks are exclusively based on synthetically generated events and synthetic natural language descriptions of the causal relationships. This design brings up two issues. First, there is a lack of diversity in both event types and natural language descriptions; second, causal relationships based on manually-defined heuristics are different from human judgments. To address both shortcomings, we present the CLEVRER-Humans benchmark, a video reasoning dataset for causal judgment of physical events with human labels. We employ two techniques to improve data collection efficiency: first, a novel iterative event cloze task to elicit a new representation of events in videos, which we term Causal Event Graphs (CEGs); second, a data augmentation technique based on neural language generative models. We convert the collected CEGs into questions and answers to be consistent with prior work. Finally, we study a collection of baseline approaches for CLEVRER-Humans question-answering, highlighting great challenges set forth by our benchmark.",Datasets & Benchmarks,NeurIPS,2022,Poster,Jiayuan Mao;Xuelin Yang;Xikun Zhang;Noah Goodman;Jiajun Wu,True,https://openreview.net/pdf?id=sd1fv0g3UO1
srHMs3mPD5y,FETA: Towards Specializing Foundational Models for Expert Task Applications,"    Foundational Models (FMs) have demonstrated unprecedented capabilities including zero-shot learning, high fidelity data synthesis, and out of domain generalization. However, the parameter capacity of FMs is still limited, leading to poor out-of-the-box performance of FMs on many expert tasks (e.g. retrieval of car manuals technical illustrations from language queries), data for which is either unseen or belonging to a long-tail part of the data distribution of the huge datasets used for FM pre-training. This underlines the necessity to explicitly evaluate and finetune FMs on such expert tasks, arguably ones that appear the most in practical real-world applications. In this paper, we propose a first of its kind FETA benchmark built around the task of teaching FMs to understand technical documentation, via learning to match their graphical illustrations to corresponding language descriptions. Our FETA benchmark focuses on text-to-image and image-to-text retrieval in public car manuals and sales catalogue brochures. FETA is equipped with a procedure for completely automatic annotation extraction (code would be released upon acceptance), allowing easy extension of FETA to more documentation types and application domains in the future. Our automatic annotation leads to an automated performance metric shown to be consistent with metrics computed on human-curated annotations (also released). We provide multiple baselines and analysis of popular FMs on FETA leading to several interesting findings that we believe would be very valuable to the FM community, paving the way towards real-world application of FMs for many practical expert tasks currently being `overlooked' by standard benchmarks focusing on common objects.",Datasets & Benchmarks,NeurIPS,2022,Poster,Amit Alfassy;Assaf Arbelle;Oshri Halimi;Sivan Harary;Roei Herzig;Eli Schwartz;Rameswar Panda;Michele Dolfi;Christoph Auer;Peter W. J. Staar;Kate Saenko;Rogerio Feris;Leonid Karlinsky,True,https://openreview.net/pdf?id=srHMs3mPD5y
tTPVefaATp6,OccGen: Selection of Real-world Multilingual Parallel Data Balanced in Gender within Occupations,"This paper describes the OCCGEN toolkit, which allows extracting multilingual parallel data balanced in gender within occupations. OCCGEN can extract datasets that reflect gender diversity (beyond binary) more fairly in society to be further used to explicitly mitigate occupational gender stereotypes. We propose two use cases that extract evaluation datasets for machine translation in four high-resource
languages from different linguistic families and in a low-resource African language. Our analysis of these use cases shows that translation outputs in high-resource languages tend to worsen in feminine subsets (compared to masculine). This can be explained because less attention is paid to the source sentence. Then, more attention is given to the target prefix overgeneralizing to the most frequent masculine forms.",Datasets & Benchmarks,NeurIPS,2022,Poster,Marta R. Costa-jussà;Christine Basta;Oriol Domingo;André Niyongabo Rubungo,True,https://openreview.net/pdf?id=tTPVefaATp6
tXEe-Ew_ikh,Unravelling the Performance of Physics-informed Graph Neural Networks for Dynamical Systems,"Recently, graph neural networks have been gaining a lot of attention to simulate dynamical systems due to their inductive nature leading to zero-shot generalizability. Similarly, physics-informed inductive biases in deep-learning frameworks have been shown to give superior performance in learning the dynamics of physical systems. There is a growing volume of literature that attempts to combine these two approaches. Here, we evaluate the performance of thirteen different graph neural networks, namely, Hamiltonian and Lagrangian graph neural networks, graph neural ODE, and their variants with explicit constraints and different architectures. We briefly explain the theoretical formulation highlighting the similarities and differences in the inductive biases and graph architecture of these systems. Then, we evaluate them on spring, pendulum, and gravitational and 3D deformable solid systems to compare the performance in terms of rollout error, conserved quantities such as energy and momentum, and generalizability to unseen system sizes. Our study demonstrates that GNNs with additional inductive biases, such as explicit constraints and decoupling of kinetic and potential energies, exhibit significantly enhanced performance. Further, all the physics-informed GNNs exhibit zero-shot generalizability to system sizes an order of magnitude larger than the training system, thus providing a promising route to simulate large-scale realistic systems.",Datasets & Benchmarks,NeurIPS,2022,Poster,Abishek Thangamuthu;Gunjan Kumar;Suresh Bishnoi;Ravinder Bhattoo;N M Anoop Krishnan;Sayan Ranu,False,https://openreview.net/pdf?id=tXEe-Ew_ikh
ttxAvIQA4i_,EgoTaskQA: Understanding Human Tasks in Egocentric Videos,"Understanding human tasks through video observations is an essential capability of intelligent agents. The challenges of such capability lie in the difficulty of generating a detailed understanding of situated actions, their effects on object states (\\\\ie, state changes), and their causal dependencies. These challenges are further aggravated by the natural parallelism from multi-tasking and partial observations in multi-agent collaboration. Most prior works leverage action localization or future prediction as an \\\\textit{indirect} metric for evaluating such task understanding from videos. To make a \\\\textit{direct} evaluation, we introduce the EgoTaskQA benchmark that provides a single home for the crucial dimensions of task understanding through question answering on real-world egocentric videos. We meticulously design questions that target the understanding of (1) action dependencies and effects, (2) intents and goals, and (3) agents' beliefs about others. These questions are divided into four types, including descriptive (what status?), predictive (what will?), explanatory (what caused?), and counterfactual (what if?) to provide diagnostic analyses on \\\\textit{spatial, temporal, and causal} understandings of goal-oriented tasks. We evaluate state-of-the-art video reasoning models on our benchmark and show their significant gaps between humans in understanding complex goal-oriented egocentric videos. We hope this effort would drive the vision community to move onward with goal-oriented video understanding and reasoning.",Datasets & Benchmarks,NeurIPS,2022,Poster,Baoxiong Jia;Ting Lei;Song-Chun Zhu;Siyuan Huang,True,https://openreview.net/pdf?id=ttxAvIQA4i_
u46CbCaLufp,Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models,"Large language models produce human-like text that drive a growing number of applications.  However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful. 
 Though work to evaluate language model harms is under way, translating foresight about which harms may arise into rigorous benchmarks is not straightforward.  To facilitate this translation, we outline six ways of characterizing harmful text which merit explicit consideration when designing new benchmarks.  We then use these characteristics as a lens to identify trends and gaps in existing benchmarks. Finally, we apply them in a case study of the Perspective API, a toxicity classifier that is widely used in harm benchmarks.  Our characteristics provide one piece of the bridge that translates between foresight and effective evaluation.",Datasets & Benchmarks,NeurIPS,2022,Poster,Maribeth Rauh;John F J Mellor;Jonathan Uesato;Po-Sen Huang;Johannes Welbl;Laura Weidinger;Sumanth Dathathri;Amelia Glaese;Geoffrey Irving;Iason Gabriel;William Isaac;Lisa Anne Hendricks,False,https://openreview.net/pdf?id=u46CbCaLufp
uDlkiCI5N7Y,Evaluating Out-of-Distribution Performance on Document Image Classifiers,"The ability of a document classifier to handle inputs that are drawn from a distribution different from the training distribution is crucial for robust deployment and generalizability. The RVL-CDIP corpus is the de facto standard benchmark for document classification, yet to our knowledge all studies that use this corpus do not include evaluation on out-of-distribution documents. In this paper, we curate and release a new out-of-distribution benchmark for evaluating out-of-distribution performance for document classifiers. Our new out-of-distribution benchmark consists of two types of documents: those that are not part of any of the 16 in-domain RVL-CDIP categories (RVL-CDIP-O), and those that are one of the 16 in-domain categories yet are drawn from a distribution different from that of the original RVL-CDIP dataset (RVL-CDIP-N). While prior work on document classification for in-domain RVL-CDIP documents reports high accuracy scores, we find that these models exhibit accuracy drops of between roughly 15-30% on our new out-of-domain RVL-CDIP-N benchmark, and further struggle to distinguish between in-domain RVL-CDIP-N and out-of-domain RVL-CDIP-O inputs. Our new benchmark provides researchers with a valuable new resource for analyzing out-of-distribution performance on document classifiers.",Datasets & Benchmarks,NeurIPS,2022,Poster,Stefan Larson;Gordon Lim;Yutong Ai;David Kuang;Kevin Leach,True,https://openreview.net/pdf?id=uDlkiCI5N7Y
uP9RiC4uVcR,When to Make Exceptions: Exploring Language Models as Accounts of Human Moral Judgment,"AI systems are becoming increasingly intertwined with human life. In order to effectively collaborate with humans and ensure safety, AI systems need to be able to understand, interpret and predict human moral judgments and decisions. Human moral judgments are often guided by rules, but not always. A central challenge for AI safety is capturing the flexibility of the human moral mind — the ability to determine when a rule should be broken, especially in novel or unusual situations. In this paper, we present a novel challenge set consisting of moral exception question answering (MoralExceptQA) of cases that involve potentially permissible moral exceptions – inspired by recent moral psychology studies. Using a state-of-the-art large language model (LLM) as a basis, we propose a novel moral chain of thought (MoralCoT) prompting strategy that combines the strengths of LLMs with theories of moral reasoning developed in cognitive science to predict human moral judgments. MoralCoT outperforms seven existing LLMs by 6.2% F1, suggesting that modeling human reasoning might be necessary to capture the flexibility of the human moral mind. We also conduct a detailed error analysis to suggest directions for future work to improve AI safety using MoralExceptQA. Our data is open-sourced at https://huggingface.co/datasets/feradauto/MoralExceptQA and code at https://github.com/feradauto/MoralCoT.",main,NeurIPS,2022,Highlighted,Zhijing Jin;Sydney Levine;Fernando Gonzalez Adauto;Ojasv Kamal;Maarten Sap;Mrinmaya Sachan;Rada Mihalcea;Joshua B. Tenenbaum;Bernhard Schölkopf,True,https://openreview.net/pdf?id=uP9RiC4uVcR
uRSvcqwOm0,Bivariate Causal Discovery for Categorical Data via Classification with Optimal Label Permutation,"Causal discovery for quantitative data has been extensively studied but less is known for categorical data. We propose a novel causal model for categorical data based on a new classification model, termed classification with optimal label permutation (COLP). By design, COLP is a parsimonious classifier, which gives rise to a provably identifiable causal model. A simple learning algorithm via comparing likelihood functions of causal and anti-causal models suffices to learn the causal direction. Through experiments with synthetic and real data, we demonstrate the favorable performance of the proposed COLP-based causal model compared to state-of-the-art methods. We also make available an accompanying R package COLP, which contains the proposed causal discovery algorithm and a benchmark dataset of categorical cause-effect pairs. ",main,NeurIPS,2022,Poster,Yang Ni,True,https://openreview.net/pdf?id=uRSvcqwOm0
v3yM5zVzP4C,Finding Naturally Occurring Physical Backdoors in Image Datasets,"Extensive literature on backdoor poison attacks has studied attacks and defenses for backdoors using  “digital trigger patterns.” In contrast, “physical backdoors” use physical objects as triggers, have only recently been identified, and are qualitatively different enough to resist most defenses targeting digital trigger backdoors. Research on physical backdoors is limited by access to large datasets containing real images of physical objects co-located with misclassification targets. Building these datasets is time- and labor-intensive.

This work seeks to address the challenge of accessibility for research on physical backdoor attacks. We hypothesize that there may be naturally occurring physically co-located objects already present in popular datasets such as ImageNet. Once identified, a careful relabeling of these data can transform them into training samples for physical backdoor attacks. We propose a method to scalably identify these subsets of potential triggers in existing datasets, along with the specific classes they can poison. We call these naturally occurring trigger-class subsets natural backdoor datasets. Our techniques successfully identify natural backdoors in widely-available datasets, and produce models behaviorally equivalent to those trained on manually curated datasets. We release our code to allow the research community to create their own datasets for research on physical backdoor attacks.",Datasets & Benchmarks,NeurIPS,2022,Poster,Emily Wenger;Roma Bhattacharjee;Arjun Nitin Bhagoji;Josephine Passananti;Emilio Andere;Haitao Zheng;Ben Zhao,False,https://openreview.net/pdf?id=v3yM5zVzP4C
vExdPu73R2z,R^2-VOS: Robust Referring Video Object Segmentation via Relational Cycle Consistency,"Referring video object segmentation (R-VOS) aims to segment the object masks in a video given a referring linguistic expression to the object. It is a recently introduced task attracting growing research attention. However, all existing works make a strong assumption: The object depicted by the expression must exist in the video, namely, the expression and video must have an object-level semantic consensus. This is often violated in real-world applications where an expression can be queried to false videos, and existing methods always fail in such false queries due to abusing the assumption. In this work, we emphasize that studying semantic consensus is necessary to improve the robustness of R-VOS. Accordingly, we pose an extended task from R-VOS without the semantic consensus assumption, named Robust R-VOS ($\\\\mathrm{R}^2$-VOS). The $\\\\mathrm{R}^2$-VOS task is essentially related to the joint modeling of the primary R-VOS task and its dual problem (text reconstruction). We embrace the observation that the embedding spaces have relational consistency through the cycle of text-video-text transformation which connects the primary and dual problems. We leverage the cycle consistency to discriminate and augment the semantic consensus, thus advancing the primary task. Parallel optimization of the primary and dual problems are enabled by introducing an early grounding medium. A new evaluation dataset, $\\\\mathrm{R}^2$-Youtube-VOS, is collected to measure the robustness of R-VOS models against unpaired videos and expressions. Our method not only identifies negative pairs of unrelated expressions and videos, but also improves the segmentation accuracy for positive pairs with a superior disambiguating ability. The proposed model achieves the state-of-the-art performance on Ref-DAVIS17, Ref-Youtube-VOS, and the novel $\\\\mathrm{R}^2$-Youtube-VOS dataset.",main,NeurIPS,2022,Reject,Xiang Li;Jinglu Wang;Xiaohao Xu;Xiao Li;Yan Lu;Bhiksha Raj,True,https://openreview.net/pdf?id=vExdPu73R2z
v_0F4IZJZw,Exploring the Limits of Domain-Adaptive Training for Detoxifying Large-Scale Language Models,"Pre-trained language models (LMs) are shown to easily generate toxic language. In this work, we systematically explore domain-adaptive training to reduce the toxicity of language models. We conduct this study on three dimensions: training corpus, model size, and parameter efficiency. For the training corpus, we demonstrate that using self-generated datasets consistently outperforms the existing baselines across various model sizes on both automatic and human evaluations, even when it uses a 3 1 smaller training corpus. We then comprehensively study detoxifying LMs with parameter sizes ranging from 126M up to 530B (3× larger than GPT3), a scale that has never been studied before. We find that i) large LMs have similar toxicity levels as smaller ones given the same pre-training corpus, and ii) large LMs require more endeavor to unlearn the toxic content seen at pretraining. We also explore parameter-efficient training methods for detoxification. We demonstrate that adding and training adapter-only layers in LMs not only saves a lot of parameters but also achieves a better trade-off between toxicity and perplexity than whole model adaptation for large-scale models. Our code will be available at: https://github.com/NVIDIA/Megatron-LM/.",main,NeurIPS,2022,Poster,Boxin Wang;Wei Ping;Chaowei Xiao;Peng Xu;Mostofa Patwary;Mohammad Shoeybi;Bo Li;Anima Anandkumar;Bryan Catanzaro,True,https://openreview.net/pdf?id=v_0F4IZJZw
vrnqr3PG4yB,TempEL: Linking Dynamically Evolving and Newly Emerging Entities,"In our continuously evolving world, entities change over time and new, previously non-existing or unknown, entities appear. We study how this evolutionary scenario impacts the performance on a well established entity linking (EL) task. For that study, we introduce TempEL, an entity linking dataset that consists of time-stratified English Wikipedia snapshots from 2013 to 2022, from which we collect both anchor mentions of entities, and these target entities’ descriptions. By capturing such temporal aspects, our newly introduced TempEL resource contrasts with currently existing entity linking datasets, which are composed of fixed mentions linked to a single static version of a target Knowledge Base (e.g., Wikipedia 2010 for CoNLL-AIDA). Indeed, for each of our collected temporal snapshots, TempEL contains links to entities that are continual, i.e., occur in all of the years, as well as completely new entities that appear for the first time at some point. Thus, we enable to quantify the performance of current state-of-the-art EL models for: (i) entities that are subject to changes over time in their Knowledge Base descriptions as well as their mentions’ contexts, and (ii) newly created entities that were previously non-existing (e.g., at the time the EL model was trained). Our experimental results show that in terms of temporal performance degradation, (i) continual entities suffer a decrease of up to 3.1% EL accuracy, while (ii) for new entities this accuracy drop is up to 17.9%. This highlights the challenge of the introduced TempEL dataset and opens new research prospects in the area of time-evolving entity disambiguation. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Klim Zaporojets;Lucie-Aimée Kaffee;Johannes Deleu;Thomas Demeester;Chris Develder;Isabelle Augenstein,True,https://openreview.net/pdf?id=vrnqr3PG4yB
w7VPQWgnn3s,Pythae: Unifying Generative Autoencoders in Python - A Benchmarking Use Case,"In recent years, deep generative models have attracted increasing interest due to their capacity to model complex distributions. Among those models, variational autoencoders have gained popularity as they have proven both to be computationally efficient and yield impressive results in multiple fields. Following this breakthrough, extensive research has been done in order to improve the original publication, resulting in a variety of different VAE models in response to different tasks. In this paper we present \\\\textbf{Pythae}, a versatile \\\\textit{open-source} Python library providing both a \\\\textit{unified implementation} and a dedicated framework allowing \\\\textit{straightforward}, \\\\emph{reproducible} and \\\\textit{reliable} use of generative autoencoder models. We then propose to use this library to perform a case study benchmark where we present and compare 19 generative autoencoder models representative of some of the main improvements on downstream tasks such as image reconstruction, generation, classification, clustering and interpolation. The open-source library can be found at \\\\url{https://github.com/clementchadebec/benchmark_VAE}.",Datasets & Benchmarks,NeurIPS,2022,Poster,Clément Chadebec;Louis J. Vincent;Stephanie Allassonniere,False,https://openreview.net/pdf?id=w7VPQWgnn3s
wJwHTgIoE0P,Procedural Image Programs for Representation Learning,"Learning image representations using synthetic data allows training neural networks without some of the concerns associated with real images, such as privacy and bias. Existing work focuses on a handful of curated generative processes which require expert knowledge to design, making it hard to scale up. To overcome this, we propose training with a large dataset of twenty-one thousand programs, each one generating a diverse set of synthetic images. These programs are short code snippets, which are easy to modify and fast to execute using OpenGL. The proposed dataset can be used for both supervised and unsupervised representation learning, and reduces the gap between pre-training with real and procedurally generated images by 38%.",main,NeurIPS,2022,Poster,Manel Baradad;Chun-Fu Chen;Jonas Wulff;Tongzhou Wang;Rogerio Feris;Antonio Torralba;Phillip Isola,True,https://openreview.net/pdf?id=wJwHTgIoE0P
wPEXGTzZJt,ViSioNS: Visual Search in Natural Scenes Benchmark,"Visual search is an essential part of almost any everyday human interaction with the visual environment. Nowadays, several algorithms are able to predict gaze positions during simple observation, but few models attempt to simulate human behavior during visual search in natural scenes. Furthermore, these models vary widely in their design and exhibit differences in the datasets and metrics with which they were evaluated. Thus, there is a need for a reference point, on which each model can be tested and from where potential improvements can be derived. In this study, we select publicly available state-of-the-art visual search models and datasets in natural scenes, and provide a common framework for their evaluation. To this end, we apply a unified format and criteria, bridging the gaps between them, and we estimate the models’ efficiency and similarity with humans using a specific set of metrics. This integration has allowed us to enhance the Ideal Bayesian Searcher by combining it with a neural network-based visual search model, which enables it to generalize to other datasets. The present work sheds light on the limitations of current models and how integrating different approaches with a unified criteria can lead to better algorithms. Moreover, it moves forward on bringing forth a solution for the urgent need for benchmarking data and metrics to support the development of more general human visual search computational models. All of the code used here, including metrics, plots, and visual search models, alongside the preprocessed datasets, are available at $\\\\url{https://github.com/FerminT/VisualSearchBenchmark}$.",Datasets & Benchmarks,NeurIPS,2022,Poster,Fermín Travi;Gonzalo Ruarte;Gaston Bujia;Juan E Kamienkowski,False,https://openreview.net/pdf?id=wPEXGTzZJt
xUXTbq6gWsB,NAS-Bench-360: Benchmarking Neural Architecture Search on Diverse Tasks,"Most existing neural architecture search (NAS) benchmarks and algorithms prioritize well-studied tasks, e.g. image classification on CIFAR or ImageNet. This makes the performance of NAS approaches in more diverse areas poorly understood. In this paper, we present NAS-Bench-360, a benchmark suite to evaluate methods on domains beyond those traditionally studied in architecture search, and use it to address the following question: do state-of-the-art NAS methods perform well on diverse tasks? To construct the benchmark, we curate ten tasks spanning a diverse array of application domains, dataset sizes, problem dimensionalities, and learning objectives. Each task is carefully chosen to interoperate with modern CNN-based search methods while possibly being far-afield from its original development domain. To speed up and reduce the cost of NAS research, for two of the tasks we release the precomputed performance of 15,625 architectures comprising a standard CNN search space. Experimentally, we show the need for more robust NAS evaluation of the kind NAS-Bench-360 enables by showing that several modern NAS procedures perform inconsistently across the ten tasks, with many catastrophically poor results. We also demonstrate how NAS-Bench-360 and its associated precomputed results will enable future scientific discoveries by testing whether several recent hypotheses promoted in the NAS literature hold on diverse tasks. NAS-Bench-360 is hosted at https://nb360.ml.cmu.edu.",Datasets & Benchmarks,NeurIPS,2022,Poster,Renbo Tu;Nicholas Roberts;Mikhail Khodak;Junhong Shen;Frederic Sala;Ameet Talwalkar,True,https://openreview.net/pdf?id=xUXTbq6gWsB
x_kBZYiUrxR,PulseImpute: A Novel Benchmark Task for Pulsative Physiological Signal Imputation,"The promise of Mobile Health (mHealth) is the ability to use wearable sensors to monitor participant physiology at high frequencies during daily life to enable temporally-precise health interventions. However, a major challenge is frequent missing data. Despite a rich imputation literature, existing techniques are ineffective for the pulsative signals which comprise many mHealth applications, and a lack of available datasets has stymied progress. We address this gap with PulseImpute, the first large-scale pulsative signal imputation challenge which includes realistic mHealth missingness models, an extensive set of baselines, and clinically-relevant downstream tasks. Our baseline models include a novel transformer-based architecture designed to exploit the structure of pulsative signals. We hope that PulseImpute will enable the ML community to tackle this important and challenging task.",Datasets & Benchmarks,NeurIPS,2022,Poster,Maxwell Xu;Alexander Moreno;Supriya Nagesh;Varol Burak Aydemir;David W Wetter;Santosh Kumar;James Matthew Rehg,True,https://openreview.net/pdf?id=x_kBZYiUrxR
yCJVkELVT9d,Are Defenses for Graph Neural Networks Robust?,"A cursory reading of the literature suggests that we have made a lot of progress in designing effective adversarial defenses for Graph Neural Networks (GNNs). Yet, the standard methodology has a serious flaw – virtually all of the defenses are evaluated against non-adaptive attacks leading to overly optimistic robustness estimates. We perform a thorough robustness analysis of 7 of the most popular defenses spanning the entire spectrum of strategies, i.e., aimed at improving the graph, the architecture, or the training. The results are sobering – most defenses show no or only marginal improvement compared to an undefended baseline. We advocate using custom adaptive attacks as a gold standard and we outline the lessons we learned from successfully designing such attacks. Moreover, our diverse collection of perturbed graphs forms a (black-box) unit test offering a first glance at a model's robustness.",main,NeurIPS,2022,Poster,Felix Mujkanovic;Simon Geisler;Stephan Günnemann;Aleksandar Bojchevski,True,https://openreview.net/pdf?id=yCJVkELVT9d
yCZRdI0Y7G,Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization,"Molecular optimization is a fundamental goal in the chemical sciences and is of central interest to drug and material design. In recent years, significant progress has been made in solving challenging problems across various aspects of computational molecular optimizations, emphasizing high validity, diversity, and, most recently, synthesizability. Despite this progress, many papers report results on trivial or self-designed tasks, bringing additional challenges to directly assessing the performance of new methods. Moreover, the sample efficiency of the optimization---the number of molecules evaluated by the oracle---is rarely discussed, despite being an essential consideration for realistic discovery applications.

To fill this gap, we have created an open-source benchmark for practical molecular optimization, PMO, to facilitate the transparent and reproducible evaluation of algorithmic advances in molecular optimization. This paper thoroughly investigates the performance of 25 molecular design algorithms on 23 single-objective (scalar) optimization tasks with a particular focus on sample efficiency. Our results show that most ``state-of-the-art'' methods fail to outperform their predecessors under a limited oracle budget allowing 10K queries and that no existing algorithm can efficiently solve certain molecular optimization problems in this setting. We analyze the influence of the optimization algorithm choices, molecular assembly strategies, and oracle landscapes on the optimization performance to inform future algorithm development and benchmarking. PMO provides a standardized experimental setup to comprehensively evaluate and compare new molecule optimization methods with existing ones. All code can be found at https://github.com/wenhao-gao/mol_opt.",Datasets & Benchmarks,NeurIPS,2022,Poster,Wenhao Gao;Tianfan Fu;Jimeng Sun;Connor W. Coley,True,https://openreview.net/pdf?id=yCZRdI0Y7G
yPZ7w29qSNK,Towards Video Text Visual Question Answering: Benchmark and Baseline,"There are already some text-based visual question answering (TextVQA) benchmarks for developing machine's ability to answer questions based on texts in images in recent years. However, models developed on these benchmarks cannot work effectively in many real-life scenarios (e.g. traffic monitoring, shopping ads and e-learning videos) where temporal reasoning ability is required. To this end, we propose a new task named Video Text Visual Question Answering (ViteVQA in short) that aims at answering questions by reasoning texts and visual information spatiotemporally in a given video. In particular, on the one hand, we build the first ViteVQA benchmark dataset named M4-ViteVQA --- the abbreviation of Multi-category Multi-frame Multi-resolution Multi-modal benchmark for ViteVQA, which contains 7,620 video clips of 9 categories (i.e., shopping, traveling, driving, vlog, sport, advertisement, movie, game and talking) and 3 kinds of resolutions (i.e., 720p, 1080p and 1176x664), and 25,123 question-answer pairs. On the other hand, we develop a baseline method named T5-ViteVQA for the ViteVQA task. T5-ViteVQA consists of five transformers. It first extracts optical character recognition (OCR) tokens, question features, and video representations via two OCR transformers, one language transformer and one video-language transformer, respectively. Then, a multimodal fusion transformer and an answer generation module are applied to fuse multimodal information and generate the final prediction. Extensive experiments on M4-ViteVQA demonstrate the superiority of T5-ViteVQA to the existing approaches of TextVQA and VQA tasks. The ViteVQA benchmark is available in https://github.com/bytedance/VTVQA.",Datasets & Benchmarks,NeurIPS,2022,Poster,Minyi Zhao;Bingjia Li;Jie Wang;Wanqing Li;Wenjing Zhou;Lan Zhang;Shijie Xuyang;Zhihang Yu;Xinkun Yu;Guangze Li;Aobotao Dai;Shuigeng Zhou,True,https://openreview.net/pdf?id=yPZ7w29qSNK
yWhuIjIjH8k,NAS-Bench-Suite-Zero: Accelerating Research on Zero Cost Proxies,"Zero-cost proxies (ZC proxies) are a recent architecture performance prediction technique aiming to significantly speed up algorithms for neural architecture search (NAS). Recent work has shown that these techniques show great promise, but certain aspects, such as evaluating and exploiting their complementary strengths, are under-studied. In this work, we create NAS-Bench-Suite: we evaluate 13 ZC proxies across 28 tasks, creating by far the largest dataset (and unified codebase) for ZC proxies, enabling orders-of-magnitude faster experiments on ZC proxies, while avoiding confounding factors stemming from different implementations. To demonstrate the usefulness of NAS-Bench-Suite, we run a large-scale analysis of ZC proxies, including a bias analysis, and the first information-theoretic analysis which concludes that ZC proxies capture substantial complementary information. Motivated by these findings, we present a procedure to improve the performance of ZC proxies by reducing biases such as cell size, and we also show that incorporating all 13 ZC proxies into the surrogate models used by NAS algorithms can improve their predictive performance by up to 42%. Our code and datasets are available at https://github.com/automl/naslib/tree/zerocost.",Datasets & Benchmarks,NeurIPS,2022,Poster,Arjun Krishnakumar;Colin White;Arber Zela;Renbo Tu;Mahmoud Safari;Frank Hutter,True,https://openreview.net/pdf?id=yWhuIjIjH8k
ytnwPTrpl38,Generalizing Goal-Conditioned Reinforcement Learning with Variational Causal Reasoning,"As a pivotal component to attaining generalizable solutions in human intelligence, reasoning provides great potential for reinforcement learning (RL) agents' generalization towards varied goals by summarizing part-to-whole arguments and discovering cause-and-effect relations. However, how to discover and represent causalities remains a huge gap that hinders the development of causal RL. In this paper, we augment Goal-Conditioned RL (GCRL) with Causal Graph (CG), a structure built upon the relation between objects and events. We novelly formulate the GCRL problem into variational likelihood maximization with CG as latent variables. To optimize the derived objective, we propose a framework with theoretical performance guarantees that alternates between two steps: using interventional data to estimate the posterior of CG; using CG to learn generalizable models and interpretable policies. Due to the lack of public benchmarks that verify generalization capability under reasoning, we design nine tasks and then empirically show the effectiveness of the proposed method against five baselines on these tasks. Further theoretical analysis shows that our performance improvement is attributed to the virtuous cycle of causal discovery, transition modeling, and policy training, which aligns with the experimental evidence in extensive ablation studies.

",main,NeurIPS,2022,Poster,Wenhao Ding;Haohong Lin;Bo Li;Ding Zhao,True,https://openreview.net/pdf?id=ytnwPTrpl38
z1d8fUiS8Cr,Multi-LexSum: Real-world Summaries of Civil Rights Lawsuits at Multiple Granularities,"With the advent of large language models, methods for abstractive summarization have made great strides, creating potential for use in applications to aid knowledge workers processing unwieldy document collections. One such setting is the Civil Rights Litigation Clearinghouse (CRLC, https://clearinghouse.net), which posts information about large-scale civil rights lawsuits, serving lawyers, scholars, and the general public. Today, summarization in the CRLC requires extensive training of lawyers and law students who spend hours per case understanding multiple relevant documents in order to produce high-quality summaries of key events and outcomes. Motivated by this ongoing real-world summarization effort, we introduce Multi-LexSum, a collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing. Multi-LexSum presents a challenging multi-document summarization task given the length of the source documents, often exceeding two hundred pages per case. Furthermore, Multi-LexSum is distinct from other datasets in its multiple target summaries, each at a different granularity (ranging from one-sentence ""extreme"" summaries to multi-paragraph narrations of over five hundred words). We present extensive analysis demonstrating that despite the high-quality summaries in the training data (adhering to strict content and style guidelines), state-of-the-art summarization models perform poorly on this task. We release Multi-LexSum for further summarization research and to facilitate the development of applications to assist in the CRLC's mission at https://multilexsum.github.io.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Zejiang Shen;Kyle Lo;Lauren Yu;Nathan Dahlberg;Margo Schlanger;Doug Downey,True,https://openreview.net/pdf?id=z1d8fUiS8Cr
zBBmV-i84Go,Addressing Resource Scarcity across Sign Languages with Multilingual Pretraining and Unified-Vocabulary Datasets,"There are over 300 sign languages in the world, many of which have very limited or no labelled sign-to-text datasets. To address low-resource data scenarios, self-supervised pretraining and multilingual finetuning have been shown to be effective in natural language and speech processing. In this work, we apply these ideas to sign language recognition.

We make three contributions.
- First, we release SignCorpus, a large pretraining dataset on sign languages comprising about 4.6K hours of signing data across 10 sign languages. SignCorpus is curated from sign language videos on the internet, filtered for data quality, and converted into sequences of pose keypoints thereby removing all personal identifiable information (PII).
- Second, we release Sign2Vec, a graph-based model with 5.2M parameters that is pretrained on SignCorpus. We envisage Sign2Vec as a multilingual large-scale pretrained model which can be fine-tuned for various sign recognition tasks across languages.
- Third, we create MultiSign-ISLR -- a multilingual and label-aligned dataset of sequences of pose keypoints from 11 labelled datasets across 7 sign languages, and MultiSign-FS -- a new finger-spelling training and test set across 7 languages. On these datasets, we fine-tune Sign2Vec to create multilingual isolated sign recognition models. With experiments on multiple benchmarks, we show that pretraining and multilingual transfer are effective giving significant gains over state-of-the-art results.

All datasets, models, and code has been made open-source via the OpenHands toolkit.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Gokul NC;Manideep Ladi;Sumit Negi;Prem Selvaraj;Pratyush Kumar;Mitesh M Khapra,True,https://openreview.net/pdf?id=zBBmV-i84Go
zHNNSzo10xN,Dungeons and Data: A Large-Scale NetHack Dataset,"Recent breakthroughs in the development of agents to solve challenging sequential decision making problems such as Go, StarCraft, or DOTA, have relied on both simulated environments and large-scale datasets.  However, progress on this research has been hindered by the scarcity of open-sourced datasets and the prohibitive computational cost to work with them.  Here we present the NetHack Learning Dataset (NLD), a large and highly-scalable dataset of trajectories from the popular game of NetHack, which is both extremely challenging for current methods and very fast to run. NLD consists of three parts: 10 billion state transitions from 1.5 million human trajectories collected on the NAO public NetHack server from 2009 to 2020; 3 billion state-action-score transitions from 100,000 trajectories collected from the symbolic bot winner of the NetHack Challenge 2021; and, accompanying code for users to record, load and stream any collection of such trajectories in a highly compressed form.  We evaluate a wide range of existing algorithms for learning from demonstrations, showing that significant research advances are needed to fully leverage large-scale datasets for challenging sequential decision making tasks. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Eric Hambro;Roberta Raileanu;Danielle Rothermel;Vegard Mella;Tim Rocktäschel;Heinrich Kuttler;Naila Murray,True,https://openreview.net/pdf?id=zHNNSzo10xN
zp_Cp38qJE0,Transferring Fairness under Distribution Shifts via Fair Consistency Regularization,"The increasing reliance on ML models in high-stakes tasks has raised a major concern about fairness violations. Although there has been a surge of work that improves algorithmic fairness, most are under the assumption of an identical training and test distribution. In many real-world applications, however, such an assumption is often violated as previously trained fair models are often deployed in a different environment, and the fairness of such models has been observed to collapse. In this paper, we study how to transfer model fairness under distribution shifts, a widespread issue in practice. We conduct a fine-grained analysis of how the fair model is affected under different types of distribution shifts and find that domain shifts are more challenging than subpopulation shifts. Inspired by the success of self-training in transferring accuracy under domain shifts, we derive a sufficient condition for transferring group fairness. Guided by it, we propose a practical algorithm with fair consistency regularization as the key component. A synthetic dataset benchmark, which covers diverse types of distribution shifts, is deployed for experimental verification of the theoretical findings. Experiments on synthetic and real datasets, including image and tabular data, demonstrate that our approach effectively transfers fairness and accuracy under various types of distribution shifts.",main,NeurIPS,2022,Poster,Bang An;Zora Che;Mucong Ding;Furong Huang,True,https://openreview.net/pdf?id=zp_Cp38qJE0
08zf7kTOoh,Exposing flaws of generative model evaluation metrics and their unfair treatment of diffusion models,"We systematically study a wide variety of generative models spanning semantically-diverse image datasets to understand and improve the feature extractors and metrics used to evaluate them.
Using best practices in psychophysics, we measure human perception of image realism for generated samples by conducting the largest experiment evaluating generative models to date, and find that no existing metric strongly correlates with human evaluations.
Comparing to 17 modern metrics for evaluating the overall performance, fidelity, diversity, rarity, and memorization of generative models, we find that the state-of-the-art perceptual realism of diffusion models as judged by humans is not reflected in commonly reported metrics such as FID. This discrepancy is not explained by diversity in generated samples, though one cause is over-reliance on Inception-V3.
We address these flaws through a study of alternative self-supervised feature extractors, find that the semantic information encoded by individual networks strongly depends on their training procedure, and show that DINOv2-ViT-L/14 allows for much richer evaluation of generative models. Next, we investigate data memorization, and find that generative models do memorize training examples on simple, smaller datasets like CIFAR10, but not necessarily on more complex datasets like ImageNet. However, our experiments show that current metrics do not properly detect memorization: none in the literature is able to separate memorization from other phenomena such as underfitting or mode shrinkage. To facilitate further development of generative models and their evaluation we release all generated image datasets, human evaluation data, and a modular library to compute 17 common metrics for 9 different encoders at https://github.com/layer6ai-labs/dgm-eval.",main,NeurIPS,2023,Poster,George Stein;Jesse C. Cresswell;Rasa Hosseinzadeh;Yi Sui;Brendan Leigh Ross;Valentin Villecroze;Zhaoyan Liu;Anthony L. Caterini;Eric Taylor;Gabriel Loaiza-Ganem,True,https://openreview.net/pdf?id=08zf7kTOoh
0H5fRQcpQ7,RoboHive: A Unified Framework for Robot Learning,"We present RoboHive, a comprehensive software platform and ecosystem for research in the field of Robot Learning and Embodied Artificial Intelligence. Our platform encompasses a diverse range of pre-existing and novel environments, including dexterous manipulation with the Shadow Hand, whole-arm manipulation tasks with Franka and Fetch robots, quadruped locomotion, among others. Included environments are organized within and cover multiple domains such as hand manipulation, locomotion, multi-task, multi-agent, muscles, etc. In comparison to prior works, RoboHive offers a streamlined and unified task interface taking dependency on only a minimal set of well-maintained packages, features tasks with high physics fidelity and rich visual diversity, and supports common hardware drivers for real-world deployment. The unified interface of RoboHive offers a convenient and accessible abstraction for algorithmic research in imitation, reinforcement, multi-task, and hierarchical learning. Furthermore, RoboHive includes expert demonstrations and baseline results for most environments, providing a standard for benchmarking and comparisons. Details: https://sites.google.com/view/robohive",Datasets & Benchmarks,NeurIPS,2023,Poster,Vikash Kumar;Rutav Shah;Gaoyue Zhou;Vincent Moens;Vittorio Caggiano;Abhishek Gupta;Aravind Rajeswaran,False,https://openreview.net/pdf?id=0H5fRQcpQ7
0MGvE1Gkgv,Katakomba: Tools and Benchmarks for Data-Driven NetHack,"NetHack is known as the frontier of reinforcement learning research where learning-based methods still need to catch up to rule-based solutions. One of the promising directions for a breakthrough is using pre-collected datasets similar to recent developments in robotics, recommender systems, and more under the umbrella of offline reinforcement learning (ORL). Recently, a large-scale NetHack dataset was released; while it was a necessary step forward, it has yet to gain wide adoption in the ORL community. In this work, we argue that there are three major obstacles for adoption: tool-wise, implementation-wise, and benchmark-wise. To address them, we develop an open-source library that provides workflow fundamentals familiar to the ORL community: pre-defined D4RL-style tasks, uncluttered baseline implementations, and reliable evaluation tools with accompanying configs and logs synced to the cloud.",Datasets & Benchmarks,NeurIPS,2023,Poster,Vladislav Kurenkov;Alexander Nikulin;Denis Tarasov;Sergey Kolesnikov,False,https://openreview.net/pdf?id=0MGvE1Gkgv
0RSQEh9lRG,OFCOURSE: A Multi-Agent Reinforcement Learning Environment for Order Fulfillment,"The dramatic growth of global e-commerce has led to a surge in demand for efficient and cost-effective order fulfillment which can increase customers' service levels and sellers' competitiveness. However, managing order fulfillment is challenging due to a series of interdependent online sequential decision-making problems. To clear this hurdle, rather than solving the problems separately as attempted in some recent researches, this paper proposes a method based on multi-agent reinforcement learning to integratively solve the series of interconnected problems, encompassing order handling, packing and pickup, storage, order consolidation, and last-mile delivery. In particular, we model the integrated problem as a Markov game, wherein a team of agents learns a joint policy via interacting with a simulated environment. Since no simulated environment supporting the complete order fulfillment problem exists, we devise Order Fulfillment COoperative mUlti-agent Reinforcement learning Scalable Environment (OFCOURSE) in the OpenAI Gym style, which allows reproduction and re-utilization to build customized applications. By constructing the fulfillment system in OFCOURSE, we optimize a joint policy that solves the integrated problem, facilitating sequential order-wise operations across all fulfillment units and minimizing the total cost of fulfilling all orders within the promised time. With OFCOURSE, we also demonstrate that the joint policy learned by multi-agent reinforcement learning outperforms the combination of locally optimal policies. The source code of OFCOURSE is available at: https://github.com/GitYiheng/ofcourse.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yiheng Zhu;Yang Zhan;Xuankun Huang;Yuwei Chen;yujie Chen;Jiangwen Wei;Wei Feng;Yinzhi Zhou;Haoyuan Hu;Jieping Ye,True,https://openreview.net/pdf?id=0RSQEh9lRG
0Wmglu8zak,BubbleML: A Multiphase Multiphysics Dataset and Benchmarks for Machine Learning,"In the field of phase change phenomena, the lack of accessible and diverse datasets suitable for machine learning (ML) training poses a significant challenge. Existing experimental datasets are often restricted, with limited availability and sparse ground truth, impeding our understanding of this complex multiphysics phenomena. To bridge this gap, we present the BubbleML dataset which leverages physics-driven simulations to provide accurate ground truth information for various boiling scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled boiling. This extensive dataset covers a wide range of parameters, including varying gravity conditions, flow rates, sub-cooling levels, and wall superheat, comprising 79 simulations.  BubbleML is validated against experimental observations and trends, establishing it as an invaluable resource for ML research. Furthermore, we showcase its potential to facilitate the exploration of diverse downstream tasks by introducing two benchmarks: (a) optical flow analysis to capture bubble dynamics, and (b) neural PDE solvers for learning temperature and flow dynamics. The BubbleML dataset and its benchmarks aim to catalyze progress in ML-driven research on multiphysics phase change phenomena, providing robust baselines for the development and comparison of state-of-the-art techniques and models.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Sheikh Md Shakeel Hassan;Arthur Feeney;Akash Dhruv;Jihoon Kim;Youngjoon Suh;Jaiyoung Ryu;Yoonjin Won;Aparna Chandramowlishwaran,True,https://openreview.net/pdf?id=0Wmglu8zak
0cltUI2Sto,On Occlusions in Video Action Detection: Benchmark Datasets And Training Recipes,"This paper explores the impact of occlusions in video action detection. We facilitate
this study by introducing five new benchmark datasets namely O-UCF and O-
JHMDB consisting of synthetically controlled static/dynamic occlusions, OVIS-
UCF and OVIS-JHMDB consisting of occlusions with realistic motions and Real-
OUCF for occlusions in realistic-world scenarios. We formally confirm an intuitive
expectation: existing models suffer a lot as occlusion severity is increased and
exhibit different behaviours when occluders are static vs when they are moving.
We discover several intriguing phenomenon emerging in neural nets: 1) transformers
can naturally outperform CNN models which might have even used occlusion as a
form of data augmentation during training 2) incorporating symbolic-components
like capsules to such backbones allows them to bind to occluders never even seen
during training and 3) Islands of agreement (similar to the ones hypothesized in
Hinton et Al’s GLOM) can emerge in realistic images/videos without instance-level
supervision, distillation or contrastive-based objectives(eg. video-textual training).
Such emergent properties allow us to derive simple yet effective training recipes
which lead to robust occlusion models inductively satisfying the first two stages of
the binding mechanism (grouping/segregation). Models leveraging these recipes
outperform existing video action-detectors under occlusion by 32.3% on O-UCF,
32.7% on O-JHMDB & 2.6% on Real-OUCF in terms of the vMAP metric. The code for this work has been released at https: //github.com/rajatmodi62/OccludedActionBenchmark.",Datasets & Benchmarks,NeurIPS,2023,Poster,Rajat Modi;Vibhav Vineet;Yogesh S Rawat,True,https://openreview.net/pdf?id=0cltUI2Sto
0hwq2vOHT4,Described Object Detection: Liberating Object Detection with Flexible Expressions,"Detecting objects based on language information is a popular task that includes Open-Vocabulary object Detection (OVD) and Referring Expression Comprehension (REC). In this paper, we advance them to a more practical setting called *Described Object Detection* (DOD) by expanding category names to flexible language expressions for OVD and overcoming the limitation of REC only grounding the pre-existing object. We establish the research foundation for DOD by constructing a *Description Detection Dataset* ($D^3$). This dataset features flexible language expressions, whether short category names or long descriptions, and annotating all described objects on all images without omission. By evaluating previous SOTA methods on $D^3$, we find some troublemakers that fail current REC, OVD, and bi-functional methods. REC methods struggle with confidence scores, rejecting negative instances, and multi-target scenarios, while OVD methods face constraints with long and complex descriptions. Recent bi-functional methods also do not work well on DOD due to their separated training procedures and inference strategies for REC and OVD tasks. Building upon the aforementioned findings, we propose a baseline that largely improves REC methods by reconstructing the training data and introducing a binary classification sub-task, outperforming existing methods. Data and code are available at https://github.com/shikras/d-cube and related works are tracked in https://github.com/Charles-Xie/awesome-described-object-detection.",main,NeurIPS,2023,Poster,Chi Xie;Zhao Zhang;Yixuan Wu;Feng Zhu;Rui Zhao;Shuang Liang,True,https://openreview.net/pdf?id=0hwq2vOHT4
10R4Fg1aA0,Decoding the Enigma: Benchmarking Humans and AIs on the Many Facets of Working Memory,"Working memory (WM), a fundamental cognitive process facilitating the temporary storage, integration, manipulation, and retrieval of information, plays a vital role in reasoning and decision-making tasks. Robust benchmark datasets that capture the multifaceted nature of WM are crucial for the effective development and evaluation of AI WM models. Here, we introduce a comprehensive Working Memory (WorM) benchmark dataset for this purpose. WorM comprises 10 tasks and a total of 1 million trials, assessing 4 functionalities, 3 domains, and 11 behavioral and neural characteristics of WM. We jointly trained and tested state-of-the-art recurrent neural networks and transformers on all these tasks. We also include human behavioral benchmarks as an upper bound for comparison. Our results suggest that AI models replicate some characteristics of WM in the brain, most notably primacy and recency effects, and neural clusters and correlates specialized for different domains and functionalities of WM. In the experiments, we also reveal some limitations in existing models to approximate human behavior. This dataset serves as a valuable resource for communities in cognitive psychology, neuroscience, and AI, offering a standardized framework to compare and enhance WM models, investigate WM's neural underpinnings, and develop WM models with human-like capabilities. Our source code and data are available at: https://github.com/ZhangLab-DeepNeuroCogLab/WorM",Datasets & Benchmarks,NeurIPS,2023,Poster,Ankur Sikarwar;Mengmi Zhang,True,https://openreview.net/pdf?id=10R4Fg1aA0
1ODvxEwsGk,Diverse Community Data for Benchmarking Data Privacy Algorithms,"The Collaborative Research Cycle (CRC) is a National Institute of Standards and Technology (NIST) benchmarking program intended to strengthen understanding of tabular data deidentification technologies. Deidentification algorithms are vulnerable to the same bias and privacy issues that impact other data analytics and machine learning applications, and it can even amplify those issues by contaminating downstream applications. This paper summarizes four CRC contributions: theoretical work on the relationship between diverse populations and challenges for equitable deidentification; public benchmark data focused on diverse populations and challenging features; a comprehensive open source suite of evaluation metrology for deidentified datasets; and an archive of more than 450 deidentified data samples from a broad range of techniques. The initial set of evaluation results demonstrate the value of the CRC tools for investigations in this field.",Datasets & Benchmarks,NeurIPS,2023,Poster,Aniruddha Sen;Christine Task;Dhruv Kapur;Gary Stanley Howarth;Karan Bhagat,True,https://openreview.net/pdf?id=1ODvxEwsGk
1SF2tiopYJ,CommonScenes: Generating Commonsense 3D Indoor Scenes with Scene Graph Diffusion,"Controllable scene synthesis aims to create interactive environments for numerous industrial use cases. Scene graphs provide a highly suitable interface to facilitate these applications by abstracting the scene context in a compact manner. Existing methods, reliant on retrieval from extensive databases or pre-trained shape embeddings, often overlook scene-object and object-object relationships, leading to inconsistent results due to their limited generation capacity. To address this issue, we present CommonScenes, a fully generative model that converts scene graphs into corresponding controllable 3D scenes, which are semantically realistic and conform to commonsense. Our pipeline consists of two branches, one predicting the overall scene layout via a variational auto-encoder and the other generating compatible shapes via latent diffusion, capturing global scene-object and local inter-object relationships in the scene graph while preserving shape diversity. The generated scenes can be manipulated by editing the input scene graph and sampling the noise in the diffusion model. Due to the lack of a scene graph dataset offering high-quality object-level meshes with relations, we also construct SG-FRONT, enriching the off-the-shelf indoor dataset 3D-FRONT with additional scene graph labels. Extensive experiments are conducted on SG-FRONT, where CommonScenes shows clear advantages over other methods regarding generation consistency, quality, and diversity. Codes and the dataset are available on the website.",main,NeurIPS,2023,Poster,Guangyao Zhai;Evin Pinar Örnek;Shun-Cheng Wu;Yan Di;Federico Tombari;Nassir Navab;Benjamin Busam,True,https://openreview.net/pdf?id=1SF2tiopYJ
1agtIRxlCY,EPIC Fields: Marrying 3D Geometry and Video Understanding,"Neural rendering is fuelling a unification of learning, 3D geometry and video understanding that has been waiting for more than two decades. Progress, however, is still hampered by a lack of suitable datasets and benchmarks. To address this gap, we introduce EPIC Fields, an augmentation of EPIC-KITCHENS with 3D camera information. Like other datasets for neural rendering, EPIC Fields removes the complex and expensive step of reconstructing cameras using photogrammetry, and allows researchers to focus on modelling problems. We illustrate the challenge of photogrammetry in egocentric videos of dynamic actions and propose innovations to address them. Compared to other neural rendering datasets, EPIC Fields is better tailored to video understanding because it is paired with labelled action segments and the recent VISOR segment annotations. To further motivate the community, we also evaluate two benchmark tasks in neural rendering and segmenting dynamic objects, with strong baselines that showcase what is not possible today. We also highlight the advantage of geometry in semi-supervised video object segmentations on the VISOR annotations. EPIC Fields reconstructs 96\\\\% of videos in EPIC-KITCHENS, registering 19M frames in 99 hours recorded in 45 kitchens, and is available from: http://epic-kitchens.github.io/epic-fields",Datasets & Benchmarks,NeurIPS,2023,Poster,Vadim Tschernezki;Ahmad Darkhalil;Zhifan Zhu;David Fouhey;Iro Laina;Diane Larlus;Dima Damen;Andrea Vedaldi,True,https://openreview.net/pdf?id=1agtIRxlCY
1jrYSOG7DR,Revealing the unseen: Benchmarking video action recognition under occlusion,"In this work, we study the effect of occlusion on video action recognition. To
facilitate this study, we propose three benchmark datasets and experiment with
seven different video action recognition models. These datasets include two synthetic benchmarks, UCF-101-O and K-400-O, which enabled understanding the
 effects of fundamental properties of occlusion via controlled experiments. We also
 propose a real-world occlusion dataset, UCF-101-Y-OCC, which helps in further
 validating the findings of this study. We find several interesting insights such as 1)
 transformers are more robust than CNN counterparts, 2) pretraining make models
robust against occlusions, and 3) augmentation helps, but does not generalize
 well to real-world occlusions. In addition, we propose a simple transformer based
 compositional model, termed as CTx-Net, which generalizes well under this distribution shift. We observe that CTx-Net outperforms models which are trained
 using occlusions as augmentation, performing significantly better under natural
 occlusions. We believe this benchmark will open up interesting future research in
 robust video action recognition",Datasets & Benchmarks,NeurIPS,2023,Poster,Shresth Grover;Vibhav Vineet;Yogesh S Rawat,True,https://openreview.net/pdf?id=1jrYSOG7DR
1ngbR3SZHW,What can Large Language Models do in chemistry? A comprehensive benchmark on eight tasks,"Large Language Models (LLMs) with strong abilities in natural language processing tasks have emerged and have been applied in various kinds of areas such as science, finance and software engineering. However, the capability of LLMs to advance the field of chemistry remains unclear. In this paper, rather than pursuing state-of-the-art performance, we aim to evaluate capabilities of LLMs in a wide range of tasks across the chemistry domain. We identify three key chemistry-related capabilities including understanding, reasoning and explaining to explore in LLMs and establish a benchmark containing eight chemistry tasks. Our analysis draws on widely recognized datasets facilitating a broad exploration of the capacities of LLMs within the context of practical chemistry. Five LLMs (GPT-4,GPT-3.5, Davinci-003, Llama and Galactica) are evaluated for each chemistry task in zero-shot and few-shot in-context learning settings with carefully selected demonstration examples and specially crafted prompts. Our investigation found that GPT-4 outperformed other models and LLMs exhibit different competitive levels in eight chemistry tasks. In addition to the key findings from the comprehensive benchmark analysis, our work provides insights into the limitation of current LLMs and the impact of in-context learning settings on LLMs’ performance across various chemistry tasks. The code and datasets used in this study are available at https://github.com/ChemFoundationModels/ChemLLMBench.",Datasets & Benchmarks,NeurIPS,2023,Poster,Taicheng Guo;Kehan Guo;Bozhao Nan;Zhenwen Liang;Zhichun Guo;Nitesh V Chawla;Olaf Wiest;Xiangliang Zhang,False,https://openreview.net/pdf?id=1ngbR3SZHW
1plAfmP5ms,MVDoppler: Unleashing the Power of Multi-View Doppler for MicroMotion-based Gait Classification,"Modern perception systems rely heavily on high-resolution cameras, LiDARs, and advanced deep neural networks, enabling exceptional performance across various applications. However, these optical systems predominantly depend on geometric features and shapes of objects, which can be challenging to capture in long-range perception applications. To overcome this limitation, alternative approaches such as Doppler-based perception using high-resolution radars have been proposed. 
Doppler-based systems are capable of measuring micro-motions of targets remotely and with very high precision. When compared to geometric features, the resolution of micro-motion features exhibits significantly greater resilience to the influence of distance. However, the true potential of Doppler-based perception has yet to be fully realized due to several factors. These include the unintuitive nature of Doppler signals, the limited availability of public Doppler datasets, and the current datasets' inability to capture the specific co-factors that are unique to Doppler-based perception, such as the effect of the radar's observation angle and the target's motion trajectory.
This paper introduces a new large multi-view Doppler dataset together with baseline perception models for micro-motion-based gait analysis and classification. The dataset captures the impact of the subject's walking trajectory and radar's observation angle on the classification performance. Additionally, baseline multi-view data fusion techniques are provided to mitigate these effects. This work demonstrates that sub-second micro-motion snapshots can be sufficient for reliable detection of hand movement patterns and even changes in a pedestrian's walking behavior when distracted by their phone. Overall, this research not only showcases the potential of Doppler-based perception, but also offers valuable solutions to tackle its fundamental challenges.",Datasets & Benchmarks,NeurIPS,2023,Poster,Soheil Hor;Shubo Yang;Jae-Ho Choi;Amin Arbabian,True,https://openreview.net/pdf?id=1plAfmP5ms
1qvx610Cu7,Is Your Code Generated by ChatGPT Really Correct? Rigorous Evaluation of Large Language Models for Code Generation,"Program synthesis has been long studied with recent approaches focused on directly using the power of Large Language Models (LLMs) to generate code. Programming benchmarks, with curated synthesis problems and test-cases, are used to measure the performance of various LLMs on code synthesis. However, these test-cases can be limited in both quantity and quality for fully assessing the functional correctness of the generated code. Such limitation in the existing benchmarks begs the following question: In the era of LLMs, is the code generated really correct? To answer this, we propose EvalPlus – a code synthesis evaluation framework to rigorously benchmark the functional correctness of LLM-synthesized code. EvalPlus augments a given evaluation dataset with large amounts of test-cases newly produced by an automatic test input generator, powered by both LLM- and mutation-based strategies. While EvalPlus is general, we extend the test-cases of the popular HumanEval benchmark by 80x to build HumanEval+. Our extensive evaluation across 26 popular LLMs (e.g., GPT-4 and ChatGPT) demonstrates that HumanEval+ is able to catch significant amounts of previously undetected wrong code synthesized by LLMs, reducing the pass@k by up-to 19.3-28.9%. We also surprisingly found that test insufficiency can lead to mis-ranking. For example, both WizardCoder-CodeLlama and Phind-CodeLlama now outperform ChatGPT on HumanEval+, while none of them could on HumanEval. Our work not only indicates that prior popular code synthesis evaluation results do not accurately reflect the true performance of LLMs for code synthesis, but also opens up a new direction to improve such programming benchmarks through automated testing. We have open-sourced our tools, enhanced datasets as well as all LLM-generated code at https://github.com/evalplus/evalplus to facilitate and accelerate future LLM-for-code research.",main,NeurIPS,2023,Poster,Jiawei Liu;Chunqiu Steven Xia;Yuyao Wang;LINGMING ZHANG,True,https://openreview.net/pdf?id=1qvx610Cu7
1uAsASS1th,MM-Fi: Multi-Modal Non-Intrusive 4D Human Dataset for Versatile Wireless Sensing,"4D human perception plays an essential role in a myriad of applications, such as home automation and metaverse avatar simulation. However, existing solutions which mainly rely on cameras and wearable devices are either privacy intrusive or inconvenient to use. To address these issues, wireless sensing has emerged as a promising alternative, leveraging LiDAR, mmWave radar, and WiFi signals for device-free human sensing. In this paper, we propose MM-Fi, the first multi-modal non-intrusive 4D human dataset with 27 daily or rehabilitation action categories, to bridge the gap between wireless sensing and high-level human perception tasks. MM-Fi consists of over 320k synchronized frames of five modalities from 40 human subjects. Various annotations are provided to support potential sensing tasks, e.g., human pose estimation and action recognition. Extensive experiments have been conducted to compare the sensing capacity of each or several modalities in terms of multiple tasks. We envision that MM-Fi can contribute to wireless sensing research with respect to action recognition, human pose estimation, multi-modal learning, cross-modal supervision, and interdisciplinary healthcare research.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jianfei Yang;He Huang;Yunjiao Zhou;Xinyan Chen;Yuecong Xu;Shenghai Yuan;Han Zou;Chris Xiaoxuan Lu;Lihua Xie,True,https://openreview.net/pdf?id=1uAsASS1th
1wOkHN9JK8,Hierarchical VAEs provide a normative account of motion processing in the primate brain,"The relationship between perception and inference, as postulated by Helmholtz in the 19th century, is paralleled in modern machine learning by generative models like Variational Autoencoders (VAEs) and their hierarchical variants. Here, we evaluate the role of hierarchical inference and its alignment with brain function in the domain of motion perception. We first introduce a novel synthetic data framework, Retinal Optic Flow Learning (ROFL), which enables control over motion statistics and their causes. We then present a new hierarchical VAE and test it against alternative models on two downstream tasks: (i) predicting ground truth causes of retinal optic flow (e.g., self-motion); and (ii) predicting the responses of neurons in the motion processing pathway of primates. We manipulate the model architectures (hierarchical versus non-hierarchical), loss functions, and the causal structure of the motion stimuli. We find that hierarchical latent structure in the model leads to several improvements. First, it improves the linear decodability of ground truth variables and does so in a sparse and disentangled manner. Second, our hierarchical VAE outperforms previous state-of-the-art models in predicting neuronal responses and exhibits sparse latent-to-neuron relationships. These results depend on the causal structure of the world, indicating that alignment between brains and artificial neural networks depends not only on architecture but also on matching ecologically relevant stimulus statistics. Taken together, our results suggest that hierarchical Bayesian inference underlines the brain's understanding of the world, and hierarchical VAEs can effectively model this understanding.",main,NeurIPS,2023,Poster,Hadi Vafaii;Jacob L. Yates;Daniel A. Butts,True,https://openreview.net/pdf?id=1wOkHN9JK8
1yOnfDpkVe,Battle of the Backbones: A Large-Scale Comparison of Pretrained Models across Computer Vision Tasks,"Neural network based computer vision systems are typically built on a backbone, a pretrained or randomly initialized feature extractor.  Several years ago, the default option was an ImageNet-trained convolutional neural network.  However, the recent past has seen the emergence of countless backbones pretrained using various algorithms and datasets. While this abundance of choice has led to performance increases for a range of systems, it is difficult for practitioners to make informed decisions about which backbone to choose.  Battle of the Backbones (BoB) makes this choice easier by benchmarking a diverse suite of pretrained models, including vision-language models, those trained via self-supervised learning, and the Stable Diffusion backbone, across a diverse set of computer vision tasks ranging from classification to object detection to OOD generalization and more.  Furthermore, BoB sheds light on promising directions for the research community to advance computer vision by illuminating strengths and weakness of existing approaches through a comprehensive analysis conducted on more than 1500 training runs.  While vision transformers (ViTs) and self-supervised learning (SSL) are increasingly popular, we find that convolutional neural networks pretrained in a supervised fashion on large training sets still perform best on most tasks among the models we consider. Moreover, in apples-to-apples comparisons on the same architectures and similarly sized pretraining datasets, we find that SSL backbones are highly competitive, indicating that future works should perform SSL pretraining with advanced architectures and larger pretraining datasets.  We release the raw results of our experiments along with code that allows researchers to put their own backbones through the gauntlet here: https://github.com/hsouri/Battle-of-the-Backbones.",Datasets & Benchmarks,NeurIPS,2023,Poster,Micah Goldblum;Hossein Souri;Renkun Ni;Manli Shu;Viraj Uday Prabhu;Gowthami Somepalli;Prithvijit Chattopadhyay;Mark Ibrahim;Adrien Bardes;Judy Hoffman;Rama Chellappa;Andrew Gordon Wilson;Tom Goldstein,False,https://openreview.net/pdf?id=1yOnfDpkVe
22RlsVAOTT,RenderMe-360: A Large Digital Asset Library and Benchmarks Towards High-fidelity Head Avatars,"Synthesizing high-fidelity head avatars is a central problem for computer vision and graphics. While head avatar synthesis algorithms have advanced rapidly, the best ones still face great obstacles in real-world scenarios. One of the vital causes is the inadequate datasets  -- 1) current public datasets can only support researchers to explore high-fidelity head avatars in one or two task directions; 2) these datasets usually contain digital head assets with limited data volume, and narrow distribution over different attributes, such as expressions, ages, and accessories. In this paper, we present RenderMe-360, a comprehensive 4D human head dataset to drive advance in head avatar algorithms across different scenarios. It contains massive data assets, with 243+ million complete head frames and over 800k video sequences from 500 different identities captured by multi-view cameras at 30 FPS. It is a large-scale digital library for head avatars with three key attributes: 1) High Fidelity: all subjects are captured in 360 degrees via 60 synchronized, high-resolution 2K cameras. 2) High Diversity: The collected subjects vary from different ages, eras, ethnicities, and cultures, providing abundant materials with distinctive styles in appearance and geometry. Moreover, each subject is asked to perform various dynamic motions, such as expressions and head rotations, which further extend the richness of assets. 3) Rich Annotations: the dataset provides annotations with different granularities: cameras' parameters, background matting, scan, 2D/3D facial landmarks, FLAME fitting, and text description.

  
  Based on the dataset, we build a comprehensive benchmark for head avatar research, with 16 state-of-the-art methods performed on five main tasks: novel view synthesis, novel expression synthesis, hair rendering, hair editing, and talking head generation. Our experiments uncover the strengths and flaws of state-of-the-art methods. RenderMe-360 opens the door for future exploration in modern head avatars. All of the data, code, and models will be publicly available at https://renderme-360.github.io/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Dongwei Pan;Long Zhuo;Jingtan Piao;Huiwen Luo;Wei Cheng;Yuxin WANG;Siming Fan;Shengqi Liu;Lei Yang;Bo Dai;Ziwei Liu;Chen Change Loy;Chen Qian;Wayne Wu;Dahua Lin;Kwan-Yee Lin,True,https://openreview.net/pdf?id=22RlsVAOTT
25vRtG56YH,Beyond Normal: On the Evaluation of Mutual Information Estimators,"Mutual information is a general statistical dependency measure which has found applications in representation learning, causality, domain generalization and computational biology. However, mutual information estimators are typically evaluated on simple families of probability distributions, namely multivariate normal distribution and selected distributions with one-dimensional random variables. In this paper, we show how to construct a diverse family of distributions with known ground-truth mutual information and propose a language-independent benchmarking platform for mutual information estimators. We discuss the general applicability and limitations of classical and neural estimators in settings involving high dimensions, sparse interactions, long-tailed distributions, and high mutual information. Finally, we provide guidelines for practitioners on how to select appropriate estimator adapted to the difficulty of problem considered and issues one needs to consider when applying an estimator to a new data set.",main,NeurIPS,2023,Poster,Paweł Czyż;Frederic Grabowski;Julia E Vogt;Niko Beerenwinkel;Alexander Marx,True,https://openreview.net/pdf?id=25vRtG56YH
27vPcG4vKV,ProteinShake: Building datasets and benchmarks for deep learning on protein structures,"We present ProteinShake, a Python software package that simplifies dataset
creation and model evaluation for deep learning on protein structures. Users
can create custom datasets or load an extensive set of pre-processed datasets from
biological data repositories such as the Protein Data Bank (PDB) and AlphaFoldDB.
Each dataset is associated with prediction tasks and evaluation functions covering
a broad array of biological challenges. A benchmark on these tasks shows that pre-
training almost always improves performance, the optimal data modality (graphs,
voxel grids, or point clouds) is task-dependent, and models struggle to generalize
to new structures. ProteinShake makes protein structure data easily accessible
and comparison among models straightforward, providing challenging benchmark
settings with real-world implications.

ProteinShake is available at: https://proteinshake.ai",Datasets & Benchmarks,NeurIPS,2023,Poster,Tim Kucera;Carlos Oliver;Dexiong Chen;Karsten Borgwardt,False,https://openreview.net/pdf?id=27vPcG4vKV
2CJUQe6IoR,MultiVENT: Multilingual Videos of Events and Aligned Natural Text,"Everyday news coverage has shifted from traditional broadcasts towards a wide range of presentation formats such as first-hand, unedited video footage. Datasets that reflect the diverse array of multimodal, multilingual news sources available online could be used to teach models to benefit from this shift, but existing news video datasets focus on traditional news broadcasts produced for English-speaking audiences. We address this limitation by constructing MultiVENT, a dataset of multilingual, event-centric videos grounded in text documents across five target languages. MultiVENT includes both news broadcast videos and non-professional event footage, which we use to analyze the state of online news videos and how they can be leveraged to build robust, factually accurate models. Finally, we provide a  model for complex, multilingual video retrieval to serve as a baseline for information retrieval using MultiVENT.",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=2CJUQe6IoR
2EiqizElGO,ViSt3D: Video Stylization with 3D CNN,"Visual stylization has been a very popular research area in recent times. While image stylization has seen a rapid advancement in the recent past, video stylization, while being more challenging, is relatively less explored. The immediate method of stylizing videos by stylizing each frame independently has been tried with some success. To the best of our knowledge, we present the first approach to video stylization using 3D CNN directly, building upon insights from 2D image stylization. Stylizing video is highly challenging, as the appearance and video motion, which includes both camera and subject motions, are inherently entangled in the representations learnt by a 3D CNN. Hence, a naive extension of 2D CNN stylization methods to 3D CNN does not work. To perform stylization with 3D CNN, we propose to explicitly disentangle motion and appearance, stylize the appearance part, and then add back the motion component and decode the final stylized video. In addition, we propose a dataset, curated from existing datasets, to train video stylization networks. We also provide an independently collected test set to study the generalization of video stylization methods. We provide results on this test dataset comparing the proposed method with 2D stylization methods applied frame by frame. We show successful stylization with 3D CNN for the first time, and obtain better stylization in terms of texture cf.\\\\ the existing 2D methods.",main,NeurIPS,2023,Poster,Ayush Pande;Gaurav Sharma,True,https://openreview.net/pdf?id=2EiqizElGO
2s7ZZUhEGS,MARBLE: Music Audio Representation Benchmark for Universal Evaluation,"In the era of extensive intersection between art and Artificial Intelligence (AI), such as image generation and fiction co-creation, AI for music remains relatively nascent, particularly in music understanding. This is evident in the limited work on deep music representations, the scarcity of large-scale datasets, and the absence of a universal and community-driven benchmark. To address this issue, we introduce the Music Audio Representation Benchmark for universaL Evaluation, termed MARBLE. It aims to provide a benchmark for various Music Information Retrieval (MIR) tasks by defining a comprehensive taxonomy with four hierarchy levels, including acoustic, performance, score, and high-level description. We then establish a unified protocol based on 18 tasks on 12 public-available datasets, providing a fair and standard assessment of representations of all open-sourced pre-trained models developed on music recordings as baselines. Besides, MARBLE offers an easy-to-use, extendable, and reproducible suite for the community, with a clear statement on copyright issues on datasets. Results suggest recently proposed large-scale pre-trained musical language models perform the best in most tasks, with room for further improvement. The leaderboard and toolkit repository are published to promote future music AI research.",Datasets & Benchmarks,NeurIPS,2023,Poster,,False,https://openreview.net/pdf?id=2s7ZZUhEGS
38bZuqQOhC,Does Continual Learning Meet Compositionality? New Benchmarks and An Evaluation Framework,"Compositionality facilitates the comprehension of novel objects using acquired concepts and the maintenance of a knowledge pool. This is particularly crucial for continual learners to prevent catastrophic forgetting and enable compositionally forward transfer of knowledge. However, the existing state-of-the-art benchmarks inadequately evaluate the capability of compositional generalization, leaving an intriguing question unanswered. To comprehensively assess this capability, we introduce two vision benchmarks, namely Compositional GQA (CGQA) and Compositional OBJects365 (COBJ), along with a novel evaluation framework called Compositional Few-Shot Testing (CFST). These benchmarks evaluate the systematicity, productivity, and substitutivity aspects of compositional generalization. Experimental results on five baselines and two modularity-based methods demonstrate that current continual learning techniques do exhibit somewhat favorable compositionality in their learned feature extractors. Nonetheless, further efforts are required in developing modularity-based approaches to enhance compositional generalization. We anticipate that our proposed benchmarks and evaluation protocol will foster research on continual learning and compositionality.",Datasets & Benchmarks,NeurIPS,2023,Poster,Weiduo Liao;Ying Wei;Mingchen Jiang;Qingfu Zhang;Hisao Ishibuchi,True,https://openreview.net/pdf?id=38bZuqQOhC
38o372YoYt,Im-Promptu: In-Context Composition from Image Prompts,"Large language models are few-shot learners that can solve diverse tasks from a handful of demonstrations. This implicit understanding of tasks suggests that the attention mechanisms over word tokens may play a role in analogical reasoning. In this work, we investigate whether analogical reasoning can enable in-context composition over composable elements of visual stimuli. First, we introduce a suite of three benchmarks to test the generalization properties of a visual in-context learner. We formalize the notion of an analogy-based in-context learner and use it to design a meta-learning framework called Im-Promptu. Whereas the requisite token granularity for language is well established, the appropriate compositional granularity for enabling in-context generalization in visual stimuli is usually unspecified. To this end, we use Im-Promptu to train multiple agents with different levels of compositionality, including vector representations, patch representations, and object slots. Our experiments reveal tradeoffs between extrapolation abilities and the degree of compositionality, with non-compositional representations extending learned composition rules to unseen domains but performing poorly on combinatorial tasks. Patch-based representations require patches to contain entire objects for robust extrapolation. At the same time, object-centric tokenizers coupled with a cross-attention module generate consistent and high-fidelity solutions, with these inductive biases being particularly crucial for compositional generalization. Lastly, we demonstrate a use case of Im-Promptu as an intuitive programming interface for image generation.",main,NeurIPS,2023,Poster,Bhishma Dedhia;Michael Chang;Jake Snell;Thomas L. Griffiths;Niraj Jha,True,https://openreview.net/pdf?id=38o372YoYt
393EoKpJN3,ChimpACT: A Longitudinal Dataset for Understanding Chimpanzee Behaviors,"Understanding the behavior of non-human primates is crucial for improving animal welfare, modeling social behavior, and gaining insights into distinctively human and phylogenetically shared behaviors. However, the lack of datasets on non-human primate behavior hinders in-depth exploration of primate social interactions, posing challenges to research on our closest living relatives. To address these limitations, we present ChimpACT, a comprehensive dataset for quantifying the longitudinal behavior and social relations of chimpanzees within a social group. Spanning from 2015 to 2018, ChimpACT features videos of a group of over 20 chimpanzees residing at the Leipzig Zoo, Germany, with a particular focus on documenting the developmental trajectory of one young male, Azibo. ChimpACT is both comprehensive and challenging, consisting of 163 videos with a cumulative 160,500 frames, each richly annotated with detection, identification, pose estimation, and fine-grained spatiotemporal behavior labels. We benchmark representative methods of three tracks on ChimpACT: (i) tracking and identification, (ii) pose estimation, and (iii) spatiotemporal action detection of the chimpanzees. Our experiments reveal that ChimpACT offers ample opportunities for both devising new methods and adapting existing ones to solve fundamental computer vision tasks applied to chimpanzee groups, such as detection, pose estimation, and behavior analysis, ultimately deepening our comprehension of communication and sociality in non-human primates.",Datasets & Benchmarks,NeurIPS,2023,Poster,Xiaoxuan Ma;Stephan Paul Kaufhold;Jiajun Su;Wentao Zhu;Jack Terwilliger;Andres Meza;Yixin Zhu;Federico Rossano;Yizhou Wang,True,https://openreview.net/pdf?id=393EoKpJN3
3BQaMV9jxK,$\\\\mathbf{\\\\mathbb{E}^{FWI}}$: Multiparameter Benchmark Datasets for Elastic Full Waveform Inversion of Geophysical Properties,"Elastic geophysical properties (such as P- and S-wave velocities) are of great importance to various subsurface applications like CO$_2$ sequestration and energy exploration (e.g., hydrogen and geothermal). Elastic full waveform inversion (FWI) is widely applied for characterizing reservoir properties. In this paper, we introduce $\\\\mathbf{\\\\mathbb{E}^{FWI}}$, a comprehensive benchmark dataset that is specifically designed for elastic FWI. $\\\\mathbf{\\\\mathbb{E}^{FWI}}$ encompasses 8 distinct datasets that cover diverse subsurface geologic structures (flat, curve, faults, etc). The benchmark results produced by three different deep learning methods are provided. In contrast to our previously presented dataset (pressure recordings) for acoustic FWI (referred to as OpenFWI), the seismic dataset in $\\\\mathbf{\\\\mathbb{E}^{FWI}}$ has both vertical and horizontal components. Moreover, the velocity maps in $\\\\mathbf{\\\\mathbb{E}^{FWI}}$ incorporate both P- and S-wave velocities. While the multicomponent data and the added S-wave velocity make the data more realistic, more challenges are introduced regarding the convergence and computational cost of the inversion. We conduct comprehensive numerical experiments to explore the relationship between P-wave and S-wave velocities in seismic data. The relation between  P- and S-wave velocities provides crucial insights into the subsurface properties such as lithology, porosity, fluid content, etc.  We anticipate that $\\\\mathbf{\\\\mathbb{E}^{FWI}}$ will facilitate future research on multiparameter inversions and stimulate endeavors in several critical research topics of carbon-zero and new energy exploration. All datasets, codes and relevant information can be accessed through our website at https://efwi-lanl.github.io/",Datasets & Benchmarks,NeurIPS,2023,Poster,Shihang Feng;Hanchen Wang;Chengyuan Deng;Yinan Feng;Yanhua Liu;Min Zhu;Peng Jin;Yinpeng Chen;Youzuo Lin,True,https://openreview.net/pdf?id=3BQaMV9jxK
3BxYAaovKr,Ego4D Goal-Step: Toward Hierarchical Understanding of Procedural Activities,"Human activities are goal-oriented and hierarchical, comprising primary goals at the top level, sequences of steps and substeps in the middle, and atomic actions at the lowest level. Recognizing human activities thus requires relating atomic actions and steps to their functional objectives (what the actions contribute to) and modeling their sequential and hierarchical dependencies towards achieving the goals. Current activity recognition research has primarily focused on only the lowest levels of this hierarchy, i.e., atomic or low-level actions, often in trimmed videos with annotations spanning only a few seconds. In this work, we introduce Ego4D Goal-Step, a new set of annotations on the recently released Ego4D with a novel hierarchical taxonomy of goal-oriented activity labels. It provides dense annotations for 48K procedural step segments (430 hours) and high-level goal annotations for 2,807 hours of Ego4D videos. Compared to existing procedural video datasets, it is substantially larger in size, contains hierarchical action labels (goals - steps - substeps), and provides goal-oriented auxiliary information including natural language summary description, step completion status, and step-to-goal relevance information. We take a data-driven approach to build our taxonomy, resulting in dense step annotations that do not suffer from poor label-data alignment issues resulting from a taxonomy defined a priori. Through comprehensive evaluations and analyses, we demonstrate how Ego4D Goal-Step supports exploring various questions in procedural activity understanding, including goal inference, step prediction, hierarchical relation learning, and long-term temporal modeling.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Yale Song;Gene Byrne;Tushar Nagarajan;Huiyu Wang;Miguel Martin;Lorenzo Torresani,True,https://openreview.net/pdf?id=3BxYAaovKr
3HlULdiKFM,CoVR: Learning Composed Video Retrieval from Web Video Captions,"Composed Image Retrieval (CoIR) has recently gained popularity as a task that considers both text and image queries together, to search for relevant images in a database. Most CoIR approaches require manually annotated datasets, containing image-text-image triplets, where the text describes a modification from the query image to the target image. However, manual curation of CoIR triplets is expensive and prevents scalability. In this work, we instead propose a scalable automatic dataset creation methodology that generates triplets given video-caption pairs. To this end, we mine paired videos with a similar caption from a large database, and leverage a large language model to generate the corresponding modification text. We automatically construct our WebVid-CoVR dataset by applying this procedure to the large WebVid2M collection, resulting in 1.6M triplets. Moreover, we introduce a new benchmark for composed video retrieval (CoVR) and contribute a manually annotated evaluation set, along with baseline results. We further show that training a CoVR model on our dataset transfers well to CoIR, improving the state of the art in the zero-shot setup on both the CIRR and FashionIQ benchmarks. Our code, datasets, and models will be made publicly available",main,NeurIPS,2023,Reject,Lucas Ventura;Antoine Yang;Cordelia Schmid;Gül Varol,True,https://openreview.net/pdf?id=3HlULdiKFM
3b9sqxCW1x,A Data-Free Approach to Mitigate Catastrophic Forgetting in Federated Class Incremental Learning for Vision Tasks,"Deep learning models often suffer from forgetting previously learned information when trained on new data. This problem is exacerbated in federated learning (FL), where the data is distributed and can change independently for each user. Many solutions are proposed to resolve this catastrophic forgetting in a centralized setting. However, they do not apply directly to FL because of its unique complexities, such as privacy concerns and resource limitations. To overcome these challenges, this paper presents a framework for \\\\textbf{federated class incremental learning} that utilizes a generative model to synthesize samples from past distributions. This data can be later exploited alongside the training data to mitigate catastrophic forgetting. To preserve privacy, the generative model is trained on the server using data-free methods at the end of each task without requesting data from clients. Moreover, our solution does not demand the users to store old data or models, which gives them the freedom to join/leave the training at any time. Additionally, we introduce SuperImageNet, a new regrouping of the ImageNet dataset specifically tailored for federated continual learning. We demonstrate significant improvements compared to existing baselines through extensive experiments on multiple datasets.",main,NeurIPS,2023,Poster,Sara Babakniya;Zalan Fabian;Chaoyang He;Mahdi Soltanolkotabi;Salman Avestimehr,True,https://openreview.net/pdf?id=3b9sqxCW1x
3sRR2u72oQ,INSPECT: A Multimodal Dataset for Pulmonary Embolism Diagnosis and Prognosis,"Synthesizing information from various data sources plays a crucial role in the practice of modern medicine. Current applications of artificial intelligence in medicine often focus on single-modality data due to a lack of publicly available, multimodal medical datasets. To address this limitation, we introduce INSPECT, which contains de-identified longitudinal records from a large cohort of pulmonary embolism (PE) patients, along with ground truth labels for multiple outcomes. INSPECT contains data from 19,402 patients, including CT images, sections of radiology reports, and structured electronic health record (EHR) data (including demographics, diagnoses, procedures, and vitals). Using our provided dataset, we develop and release a benchmark for evaluating several baseline modeling approaches on a variety of important PE related tasks. We evaluate image-only, EHR-only, and fused models. Trained models and the de-identified dataset are made available for non-commercial use under a data use agreement. To the best our knowledge, INSPECT is the largest multimodal dataset for enabling reproducible research on strategies for integrating 3D medical imaging and EHR data.",Datasets & Benchmarks,NeurIPS,2023,Poster,Shih-Cheng Huang;Zepeng Huo;Ethan Steinberg;Chia-Chun Chiang;Matthew P. Lungren;Curtis Langlotz;Serena Yeung;Nigam Shah;Jason Alan Fries,True,https://openreview.net/pdf?id=3sRR2u72oQ
3z9YV29Ogn,ClimateSet: A Large-Scale Climate Model Dataset for Machine Learning,"Climate models have been key for assessing the impact of climate change and simulating future climate scenarios. The machine learning (ML) community has taken an increased interest in supporting climate scientists’ efforts on various tasks such as climate model emulation, downscaling, and prediction tasks. Many of those tasks have been addressed on datasets created with single climate models. However, both the climate science and ML communities have suggested that to address those tasks at scale, we need large, consistent, and ML-ready climate model datasets. Here, we introduce ClimateSet, a dataset containing the inputs and outputs of 36 climate models from the Input4MIPs and CMIP6 archives. In addition, we provide a modular dataset pipeline for retrieving and preprocessing additional climate models and scenarios. We showcase the potential of our dataset by using it as a benchmark for ML-based climate model emulation. We gain new insights about the performance and generalization capabilities of the different ML models by analyzing their performance across different climate models. Furthermore, the dataset can be used to train an ML emulator on several climate models instead of just one. Such a “super-emulator” can quickly project new climate change scenarios, complementing existing scenarios already provided to policymakers. We believe ClimateSet will create the basis needed for the ML community to tackle climate-related tasks at scale.",Datasets & Benchmarks,NeurIPS,2023,Poster,Julia Kaltenborn;Charlotte Emilie Elektra Lange;Venkatesh Ramesh;Philippe Brouillard;Yaniv Gurwicz;Chandni Nagda;Jakob Runge;Peer Nowack;David Rolnick,True,https://openreview.net/pdf?id=3z9YV29Ogn
4d8dO5sAeM,Benchmarking Robustness of Adaptation Methods on Pre-trained Vision-Language Models,"Various adaptation methods, such as LoRA, prompts, and adapters, have been proposed to enhance the performance of pre-trained vision-language models in specific domains. As test samples in real-world applications usually differ from adaptation data, the robustness of these adaptation methods against distribution shifts are essential. In this study, we assess the robustness of 11 widely-used adaptation methods across 4 vision-language datasets under multimodal corruptions. Concretely, we introduce 7 benchmark datasets, including 96 visual and 87 textual corruptions, to investigate the robustness of different adaptation methods, the impact of available adaptation examples, and the influence of trainable parameter size during adaptation. Our analysis reveals that: 1) Adaptation methods are more sensitive to text corruptions than visual corruptions. 2) Full fine-tuning does not consistently provide the highest robustness; instead, adapters can achieve better robustness with comparable clean performance. 3) Contrary to expectations, our findings indicate that increasing the number of adaptation data and parameters does not guarantee enhanced robustness; instead, it results in even lower robustness. We hope this study could benefit future research in the development of robust multimodal adaptation methods. The benchmark, code, and dataset used in this study can be accessed at https://adarobustness.github.io.",Datasets & Benchmarks,NeurIPS,2023,Poster,Shuo Chen;Jindong Gu;Zhen Han;Yunpu Ma;Philip Torr;Volker Tresp,True,https://openreview.net/pdf?id=4d8dO5sAeM
4dsMX3RnF0,"Knowledge-based in silico models and dataset for the comparative evaluation of mammography AI for a range of breast characteristics, lesion conspicuities and doses","To generate evidence regarding the safety and efficacy of artificial intelligence (AI) enabled medical devices, AI models need to be evaluated on a diverse population of patient cases, some of which may not be readily available. We propose an evaluation approach for testing medical imaging AI models that relies on in silico imaging pipelines in which stochastic digital models of human anatomy (in object space) with and without pathology are imaged using a digital replica imaging acquisition system to generate realistic synthetic image datasets. Here, we release M-SYNTH, a dataset of cohorts with four breast fibroglandular density distributions imaged at different exposure levels using Monte Carlo x-ray simulations with the publicly available Virtual Imaging Clinical Trial for Regulatory Evaluation (VICTRE) toolkit. We utilize the synthetic dataset to analyze AI model performance and find that model performance decreases with increasing breast density and increases with higher mass density, as expected. As exposure levels decrease, AI model performance drops with the highest performance achieved at exposure levels lower than the nominal recommended dose for the breast type.",Datasets & Benchmarks,NeurIPS,2023,Poster,Elena Sizikova;Niloufar Saharkhiz;Diksha Sharma;Miguel Lago;Berkman Sahiner;Jana Gut Delfino;Aldo Badano,True,https://openreview.net/pdf?id=4dsMX3RnF0
4hturzLcKX,AlpacaFarm: A Simulation Framework for Methods that Learn from Human Feedback,"Large language models (LLMs) such as ChatGPT have seen widespread adoption due to their ability to follow user instructions well.
Developing these LLMs involves a complex yet poorly understood workflow requiring training with human feedback. Replicating and understanding this instruction-following process faces three major challenges: the high cost of data collection, the lack of trustworthy evaluation, and the absence of reference method implementations. We address these bottlenecks with AlpacaFarm, a simulator that enables research and development for learning from feedback at a low cost. First, we design LLM based simulator for human feedback that is 45x cheaper than crowdworkers and displays high agreement with humans. Second, we identify an evaluation dataset representative of real-world instructions and propose an automatic evaluation procedure. Third, we contribute reference implementations for several methods (PPO, best-of-n, expert iteration, among others) that learn from pairwise feedback. Finally, as an end-to-end validation of AlpacaFarm, we train and evaluate eleven models on 10k pairs of human feedback and show that rankings of models trained in AlpacaFarm match rankings of models trained on human data. As a demonstration of the research possible in AlpacaFarm, we find that methods that use a reward model can substantially improve over supervised fine-tuning and that our reference PPO implementation leads to a +10% win-rate improvement against Davinci003.",main,NeurIPS,2023,Spotlight,Yann Dubois;Xuechen Li;Rohan Taori;Tianyi Zhang;Ishaan Gulrajani;Jimmy Ba;Carlos Guestrin;Percy Liang;Tatsunori Hashimoto,True,https://openreview.net/pdf?id=4hturzLcKX
4kV7qDi0EB,WCLD: Curated Large Dataset of Criminal Cases from Wisconsin Circuit Courts,"Machine learning based decision-support tools in criminal justice systems are subjects of intense discussions and academic research. There are important open questions about the utility and fairness of such tools. Academic researchers often rely on a few small datasets that are not sufficient to empirically study various real-world aspects of these questions. In this paper, we contribute WCLD, a curated large dataset of 1.5 million criminal cases from circuit courts in the U.S. state of Wisconsin. We used reliable public data from 1970 to 2020 to curate attributes like prior criminal counts and recidivism outcomes. The dataset contains large number of samples from five racial groups, in addition to information like sex and age (at judgment and first offense). Other attributes in this dataset include neighborhood characteristics obtained from census data, detailed types of offense, charge severity, case decisions, sentence lengths, year of filing etc. We also provide pseudo-identifiers for judge, county and zipcode. The dataset will not only enable researchers to more rigorously study algorithmic fairness in the context of criminal justice, but also relate algorithmic challenges with various systemic issues. We also discuss in detail the process of constructing the dataset and provide a datasheet. The WCLD dataset is available at https://clezdata.github.io/wcld/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Elliott Ash;Naman Goel;Nianyun Li;Claudia Marangon;Peiyao Sun,True,https://openreview.net/pdf?id=4kV7qDi0EB
5ADv5OfQgU,trajdata: A Unified Interface to Multiple Human Trajectory Datasets,"The field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous large-scale, real-world human trajectory datasets for autonomous vehicles (AVs) and pedestrian motion tracking. While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets. To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets. At its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data. As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights. trajdata is permissively licensed (Apache 2.0) and can be accessed online at https://github.com/NVlabs/trajdata.",Datasets & Benchmarks,NeurIPS,2023,Poster,Boris Ivanovic;Guanyu Song;Igor Gilitschenski;Marco Pavone,False,https://openreview.net/pdf?id=5ADv5OfQgU
5Exz7eaBXH,Physion++: Evaluating Physical Scene Understanding that Requires Online Inference of Different Physical Properties,"General physical scene understanding requires more than simply localizing and recognizing objects -- it requires knowledge that objects can have different latent properties (e.g., mass or elasticity), and that those properties affect the outcome of physical events. While there has been great progress in physical and video prediction models in recent years, benchmarks to test their performance typically do not require an understanding that objects have individual physical properties, or at best test only those properties that are directly observable (e.g., size or color). This work proposes a novel dataset and benchmark, termed Physion++, that rigorously evaluates visual physical prediction in artificial systems under circumstances where those predictions rely on accurate estimates of the latent physical properties of objects in the scene. Specifically, we test scenarios where accurate prediction relies on estimates of properties such as mass, friction, elasticity, and deformability, and where the values of those properties can only be inferred by observing how objects move and interact with other objects or fluids. We evaluate the performance of a number of state-of-the-art prediction models that span a variety of levels of learning vs. built-in knowledge, and compare that performance to a set of human predictions. We find that models that have been trained using standard regimes and datasets do not spontaneously learn to make inferences about latent properties, but also that models that encode objectness and physical states tend to make better predictions. However, there is still a huge gap between all models and human performance, and all models' predictions correlate poorly with those made by humans, suggesting that no state-of-the-art model is learning to make physical predictions in a human-like way. These results show that current deep learning models that succeed in some settings nevertheless fail to achieve human-level physical prediction in other cases, especially those where latent property inference is required. Project page: https://dingmyu.github.io/physion_v2/",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=5Exz7eaBXH
5FnttJZQFn,The Waymo Open Sim Agents Challenge,"Simulation with realistic, interactive agents represents a key task for autonomous vehicle software development. In this work, we introduce the Waymo Open Sim Agents Challenge (WOSAC). WOSAC is the first public challenge to tackle this task and propose corresponding metrics. The goal of the challenge is to stimulate the design of realistic simulators that can be used to evaluate and train a behavior model for autonomous driving. We outline our evaluation methodology, present results for a number of different baseline simulation agent methods, and analyze several submissions to the 2023 competition which ran from March 16, 2023 to May 23, 2023. The WOSAC evaluation server remains open for submissions and we discuss open problems for the task.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Nico Montali;John Lambert;Paul Mougin;Alex Kuefler;Nicholas Rhinehart;Michelle Li;Cole Gulino;Tristan Emrich;Zoey Zeyu Yang;Shimon Whiteson;Brandyn White;Dragomir Anguelov,False,https://openreview.net/pdf?id=5FnttJZQFn
5HisVXnx0n,GLEMOS: Benchmark for Instantaneous Graph Learning Model Selection,"The choice of a graph learning (GL) model (i.e., a GL algorithm and its hyperparameter settings) has a significant impact on the performance of downstream tasks. However, selecting the right GL model becomes increasingly difficult and time consuming as more and more GL models are developed. Accordingly, it is of great significance and practical value to equip users of GL with the ability to perform a near-instantaneous selection of an effective GL model without manual intervention. Despite the recent attempts to tackle this important problem, there has been no comprehensive benchmark environment to evaluate the performance of GL model selection methods. To bridge this gap, we present GLEMOS in this work, a comprehensive benchmark for instantaneous GL model selection that makes the following contributions. (i) GLEMOS provides extensive benchmark data for fundamental GL tasks, i.e., link prediction and node classification, including the performances of 366 models on 457 graphs on these tasks. (ii) GLEMOS designs multiple evaluation settings, and assesses how effectively representative model selection techniques perform in these different settings. (iii) GLEMOS is designed to be easily extended with new models, new graphs, and new performance records. (iv) Based on the experimental results, we discuss the limitations of existing approaches and highlight future research directions. To promote research on this significant problem, we make the benchmark data and code publicly available at https://namyongpark.github.io/glemos.",Datasets & Benchmarks,NeurIPS,2023,Poster,Namyong Park;Ryan A. Rossi;Xing Wang;Antoine Simoulin;Nesreen K. Ahmed;Christos Faloutsos,True,https://openreview.net/pdf?id=5HisVXnx0n
5OjLGiJW3u,SMACv2: An Improved Benchmark for Cooperative Multi-Agent Reinforcement Learning,"The availability of challenging benchmarks has played a key role in the recent progress of machine learning. In cooperative multi-agent reinforcement learning, the StarCraft Multi-Agent Challenge (SMAC) 
has become a popular testbed for centralised training with decentralised execution. However, after years of sustained improvement on SMAC, algorithms now achieve near-perfect performance. In this work, we conduct new analysis demonstrating that SMAC lacks the stochasticity and partial observability to require complex *closed-loop* policies. In particular, we show that an *open-loop* policy conditioned only on the timestep can achieve non-trivial win rates for many SMAC scenarios. To address this limitation, we introduce SMACv2, a new version of the benchmark where scenarios are procedurally generated and require agents to generalise to previously unseen settings (from the same distribution) during evaluation. We also introduce the extended partial observability challenge (EPO), which augments SMACv2 to ensure meaningful partial observability. We show that these changes ensure the benchmark
requires the use of *closed-loop* policies. We evaluate state-of-the-art algorithms on SMACv2 and show that it presents significant challenges not present in the original benchmark.  Our analysis illustrates that SMACv2 addresses the discovered deficiencies of SMAC and can help benchmark the next generation of MARL methods. Videos of training are available on our [website](https://sites.google.com/view/smacv2).",Datasets & Benchmarks,NeurIPS,2023,Poster,Benjamin Ellis;Jonathan Cook;Skander Moalla;Mikayel Samvelyan;Mingfei Sun;Anuj Mahajan;Jakob Nicolaus Foerster;Shimon Whiteson,True,https://openreview.net/pdf?id=5OjLGiJW3u
5ytypAqAsR,No Representation Rules Them All in Category Discovery,"In this paper we tackle the problem of Generalized Category Discovery (GCD). Specifically, given a dataset with labelled and unlabelled images, the task is to cluster all images in the unlabelled subset, whether or not they belong to the labelled categories. Our first contribution is to recognise that most existing GCD benchmarks only contain labels for a single clustering of the data, making it difficult to ascertain whether models are leveraging the available labels to solve the GCD task, or simply solving an unsupervised clustering problem. As such, we present a synthetic dataset, named 'Clevr-4', for category discovery. Clevr-4 contains four equally valid partitions of the data, i.e based on object 'shape', 'texture' or 'color' or 'count'. To solve the task, models are required to extrapolate the taxonomy specified by labelled set, rather than simply latch onto a single natural grouping of the data. We use this dataset to demonstrate the limitations of unsupervised clustering in the GCD setting, showing that even very strong unsupervised models fail on Clevr-4. We further use Clevr-4 to examine the weaknesses of existing GCD algorithms, and propose a new method which addresses these shortcomings, leveraging consistent findings from the representation learning literature to do so. Our simple solution, which is based on `Mean Teachers' and termed $\\\\mu$GCD, substantially outperforms implemented baselines on Clevr-4. Finally, when we transfer these findings to real data on the challenging Semantic Shift Benchmark suite, we find that $\\\\mu$GCD outperforms all prior work, setting a new state-of-the-art.",main,NeurIPS,2023,Poster,Sagar Vaze;Andrea Vedaldi;Andrew Zisserman,True,https://openreview.net/pdf?id=5ytypAqAsR
6URyQ9QhYv,Into the LAION’s Den: Investigating Hate in Multimodal Datasets,"`Scale the model, scale the data, scale the compute' is the reigning sentiment in the world of generative AI today. While the impact of model scaling has been extensively studied, we are only beginning to scratch the surface of data scaling and its consequences. This is especially of critical importance in the context of vision-language datasets such as LAION. These datasets are continually growing in size and are built based on large-scale internet dumps such as the Common Crawl, which is known to have numerous drawbacks ranging from quality, legality, and content. The datasets then serve as the backbone for large generative models, contributing to the operationalization and perpetuation of harmful societal and historical biases and stereotypes. In this paper, we investigate the effect of scaling datasets on hateful content through a comparative audit of two datasets: LAION-400M and LAION-2B. Our results show that hate content increased by nearly **12%** with dataset scale, measured both qualitatively and quantitatively using a metric that we term as Hate Content Rate (HCR). We also found that filtering dataset contents based on Not Safe For Work (NSFW) values calculated based on images alone does not exclude all the harmful content in alt-text. Instead, we found that trace amounts of hateful, targeted, and aggressive text remain even when carrying out conservative filtering. We end with a reflection and a discussion of the significance of our results for dataset curation and usage in the AI community.
Code and the meta-data assets curated in this paper are publicly available at https://github.com/vinayprabhu/hate_scaling.
 Content warning: This paper contains examples of hateful text that might be disturbing, distressing, and/or offensive.",Datasets & Benchmarks,NeurIPS,2023,Poster,,False,https://openreview.net/pdf?id=6URyQ9QhYv
6hZIfAY9GD,Large Language Model as Attributed Training Data Generator: A Tale of Diversity and Bias,"Large language models (LLMs) have been recently leveraged as training data generators for various natural language processing (NLP) tasks. While previous research has explored different approaches to training models using generated data, they generally rely on simple class-conditional prompts, which may limit the diversity of the generated data and inherit systematic biases of LLM. Thus, we investigate training data generation with diversely attributed prompts (e.g., specifying attributes like length and style), which have the potential to yield diverse and attributed generated data. Our investigation focuses on datasets with high cardinality and diverse domains, wherein we demonstrate that attributed prompts outperform simple class-conditional prompts in terms of the resulting model's performance. Additionally, we present a comprehensive empirical study on data generation encompassing vital aspects like bias, diversity, and efficiency, and highlight three key observations: firstly, synthetic datasets generated by simple prompts exhibit significant biases, such as regional bias; secondly, attribute diversity plays a pivotal role in enhancing model performance; lastly, attributed prompts achieve the performance of simple class-conditional prompts while utilizing only 5\\\\% of the querying cost of ChatGPT associated with the latter. The data and code are available on {\\\\url{https://github.com/yueyu1030/AttrPrompt}}.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yue Yu;Yuchen Zhuang;Jieyu Zhang;Yu Meng;Alexander Ratner;Ranjay Krishna;Jiaming Shen;Chao Zhang,False,https://openreview.net/pdf?id=6hZIfAY9GD
6iRH9SITva,Into the Single Cell Multiverse: an End-to-End Dataset for Procedural Knowledge Extraction in Biomedical Texts,"Many of the most commonly explored natural language processing (NLP) information extraction tasks can be thought of as evaluations of declarative knowledge, or fact-based information extraction. Procedural knowledge extraction, i.e., breaking down a described process into a series of steps, has received much less attention, perhaps in part due to the lack of structured datasets that capture the knowledge extraction process from end-to-end. To address this unmet need, we present FlaMBé (Flow annotations for Multiverse Biological entities), a collection of expert-curated datasets across a series of complementary tasks that capture procedural knowledge in biomedical texts. This dataset is inspired by the observation that one ubiquitous source of procedural knowledge that is described as unstructured text is within academic papers describing their methodology. The workflows annotated in FlaMBé are from texts in the burgeoning field of single cell research, a research area that has become notorious for the number of software tools and complexity of workflows used. Additionally, FlaMBé provides, to our knowledge, the largest manually curated named entity recognition (NER) and disambiguation (NED) datasets for tissue/cell type, a fundamental biological entity that is critical for knowledge extraction in the biomedical research domain. Beyond providing a valuable dataset to enable further development of NLP models for procedural knowledge extraction, automating the process of workflow mining also has important implications for advancing reproducibility in biomedical research.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Ruth Dannenfelser;Jeffrey Zhong;Ran Zhang;Vicky Yao,True,https://openreview.net/pdf?id=6iRH9SITva
6jOlRwnqbb,Humans in Kitchens: A Dataset for Multi-Person Human Motion Forecasting with Scene Context,"Forecasting human motion of multiple persons is very challenging. It requires to model the interactions between humans and the interactions with objects and the environment. For example, a person might want to make a coffee, but if the coffee machine is already occupied the person will have
to wait. These complex relations between scene geometry and persons arise
constantly in our daily lives, and models that wish to accurately forecast
human behavior will have to take them into consideration. To facilitate research in this direction, we propose Humans in Kitchens, a
large-scale multi-person human motion dataset with annotated 3D human poses, scene geometry and activities per person and frame.
Our dataset consists of over 7.3h recorded data of up to 16 persons at the same time in four kitchen scenes, with more than 4M annotated human poses, represented by a parametric 3D body model. In addition, dynamic scene geometry and objects like chair or cupboard are annotated per frame. As first benchmarks, we propose two protocols for short-term and long-term human motion forecasting.",Datasets & Benchmarks,NeurIPS,2023,Poster,Julian Alexander Tanke;Oh-Hun Kwon;Felix Benjamin Mueller;Andreas Doering;Juergen Gall,True,https://openreview.net/pdf?id=6jOlRwnqbb
6kRQTPEVip,AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning,"Deep learning-based reaction predictors have undergone significant architectural evolution. However, their reliance on reactions from the US Patent Office results in a lack of interpretable predictions and limited generalizability to other chemistry domains, such as radical and atmospheric chemistry. To address these challenges, we introduce a new reaction predictor system, RMechRP, that leverages contrastive learning in conjunction with mechanistic pathways, the most interpretable representation of chemical reactions. Specifically designed for radical reactions, RMechRP provides different levels of interpretation of chemical reactions. We develop and train multiple deep-learning models using RMechDB, a public database of radical reactions, to establish the first benchmark for predicting radical reactions. Our results demonstrate the effectiveness of RMechRP in providing accurate and interpretable predictions of radical reactions, and its potential for various applications in atmospheric chemistry.",main,NeurIPS,2023,Poster,Mohammadamin Tavakoli;Pierre Baldi;Ann Marie Carlton;Yinting Chiu;Alexander Shmakov;David Van Vranken,True,https://openreview.net/pdf?id=6kRQTPEVip
6qLzQeFGio,Where are we in the search for an Artificial Visual Cortex for Embodied Intelligence?,"We present the largest and most comprehensive empirical study of pre-trained visual representations (PVRs) or visual ‘foundation models’ for Embodied AI. First, we curate CortexBench, consisting of 17 different tasks spanning locomotion, navigation, dexterous, and mobile manipulation. Next, we systematically evaluate existing PVRs and find that none are universally dominant. To study the effect of pre-training data size and diversity, we combine over 4,000 hours of egocentric videos from 7 different sources (over 4.3M images) and ImageNet to train different-sized vision transformers using Masked Auto-Encoding (MAE) on slices of this data. Contrary to inferences from prior work, we find that scaling dataset size and diversity does not improve performance universally (but does so on average). Our largest model, named VC-1, outperforms all prior PVRs on average but does not universally dominate either. Next, we show that task- or domain-specific adaptation of VC-1 leads to substantial gains, with VC-1 (adapted) achieving competitive or superior performance than the best known results on all of the benchmarks in CortexBench. Finally, we present real-world hardware experiments, in which VC-1 and VC-1 (adapted) outperform the strongest pre-existing PVR. Overall, this paper presents no new techniques but a rigorous systematic evaluation, a broad set of findings about PVRs (that in some cases, refute those made in narrow domains in prior work), and open-sourced code and models (that required over 10,000 GPU-hours to train) for the benefit of the research community.",main,NeurIPS,2023,Poster,Arjun Majumdar;Karmesh Yadav;Sergio Arnaud;Yecheng Jason Ma;Claire Chen;Sneha Silwal;Aryan Jain;Vincent-Pierre Berges;Tingfan Wu;Jay Vakil;Pieter Abbeel;Jitendra Malik;Dhruv Batra;Yixin Lin;Oleksandr Maksymets;Aravind Rajeswaran;Franziska Meier,True,https://openreview.net/pdf?id=6qLzQeFGio
6zcfrSz98y,"$\\\\mathcal{M}^4$: A Unified XAI Benchmark for Faithfulness Evaluation of Feature Attribution Methods across Metrics, Modalities and Models","While Explainable Artificial Intelligence (XAI) techniques have been widely studied to explain predictions made by deep neural networks, the way to evaluate the faithfulness of explanation results remains challenging, due to the heterogeneity of explanations for various models and the lack of ground-truth explanations. This paper introduces an XAI benchmark named $\\\\mathcal{M}^4$, which allows evaluating various input feature attribution methods using the same set of faithfulness metrics across multiple data modalities (images and texts) and network structures (ResNets, MobileNets, Transformers). A taxonomy for the metrics has been proposed as well. We first categorize commonly used XAI evaluation metrics into three groups based on the ground truth they require. We then implement classic and state-of-the-art feature attribution methods using InterpretDL and conduct extensive experiments to compare methods and gain insights. Extensive experiments have been conducted to provide holistic evaluations as benchmark baselines. Several interesting observations are noticed for designing attribution algorithms. The implementation of state-of-the-art explanation methods and evaluation metrics of $\\\\mathcal{M}^4$ is publicly available at \\\\url{https://github.com/PaddlePaddle/InterpretDL}.",Datasets & Benchmarks,NeurIPS,2023,Poster,Xuhong Li;Mengnan Du;Jiamin Chen;Yekun Chai;Himabindu Lakkaraju;Haoyi Xiong,False,https://openreview.net/pdf?id=6zcfrSz98y
71uRr9N39A,QH9: A Quantum Hamiltonian Prediction Benchmark for QM9 Molecules,"Supervised machine learning approaches have been increasingly used in accelerating electronic structure prediction as surrogates of first-principle computational methods, such as density functional theory (DFT). While numerous quantum chemistry datasets focus on chemical properties and atomic forces, the ability to achieve accurate and efficient prediction of the Hamiltonian matrix is highly desired, as it is the most important and fundamental physical quantity that determines the quantum states of physical systems and chemical properties. In this work, we generate a new Quantum Hamiltonian dataset, named as QH9, to provide precise Hamiltonian matrices for 999 molecular dynamics trajectories and 130,831 stable molecular geometries, based on the QM9 dataset. By designing benchmark tasks with various molecules, we show that current machine learning models have the capacity to predict Hamiltonian matrices for arbitrary molecules. Both the QH9 dataset and the baseline models are provided to the community through an open-source benchmark, which can be highly valuable for developing machine learning methods and accelerating molecular and materials design for scientific and technological applications. Our benchmark is publicly available at https://github.com/divelab/AIRS/tree/main/OpenDFT/QHBench.",Datasets & Benchmarks,NeurIPS,2023,Poster,Haiyang Yu;Meng Liu;Youzhi Luo;Alex Strasser;Xiaofeng Qian;Xiaoning Qian;Shuiwang Ji,True,https://openreview.net/pdf?id=71uRr9N39A
73920,An Empirical Investigation of the Role of Pre-training in Lifelong Learning,"The lifelong learning paradigm in machine learning is an attractive alternative to the more prominent isolated learning scheme not only due to its resemblance to biological learning but also its potential to reduce energy waste by obviating excessive model re-training. A key challenge to this paradigm is the phenomenon of catastrophic forgetting. With the increasing popularity and success of pre-trained models in machine learning, we pose the question: What role does pre-training play in lifelong learning, specifically with respect to catastrophic forgetting? We investigate existing methods in the context of large, pre-trained models and evaluate their performance on a variety of text and image classification tasks, including a large-scale study using a novel data set of 15 diverse NLP tasks. Across all settings, we observe that generic pre-training implicitly alleviates the effects of catastrophic forgetting when learning multiple tasks sequentially compared to randomly initialized models. We then further investigate why pre-training alleviates forgetting in this setting. We study this phenomenon by analyzing the loss landscape, finding that pre-trained weights appear to ease forgetting by leading to wider minima. Based on this insight, we propose jointly optimizing for current task loss and loss basin sharpness to explicitly encourage wider basins during sequential fine-tuning. We show that this optimization approach outperforms several state-of-the-art task-sequential continual learning algorithms across multiple settings, occasionally even without retaining a memory that scales in size with the number of tasks.",Journal,NeurIPS,2023,Poster,"Sanket Vaibhav Mehta, Darshan Patil, Sarath Chandar, Emma Strubell",True,https://openreview.net/pdf?id=73920
74146,RELIC: Reproducibility and Extension on LIC metric in quantifying bias in captioning models,"Scope of ReproducibilityIn this work we reproduce and extend the results presented in “Quantifying Societal Bias Amplification in Image Captioning” by Hirota et al. This paper introduces LIC, a metric to quantify bias amplification by image captioning models, which is tested for gender and racial bias amplification. The original paper claims that this metric is robust, and that all models amplify both gender and racial bias. It also claims that gender bias is more apparent than racial bias, and the Equalizer variation of the NIC+ model increases gender but not racial bias. We repeat the measurements to confirm these claims. We extend the analysis to whether the method can be generalized to other attributes such as bias in age.MethodologyThe authors of the paper provided a repository containing the necessary code. We had to modify it and add several scripts to be able to run all the experiments. The results were reproduced using the same subset of COCO [3] as in the original paper. Additionally, we manually labeled images according to age for our specific experiments. All experiments were ran on GPUs for a total of approximately 100 hours.ResultsAll claims made by the paper seem to hold, as the results we obtained follow the same trends as those presented in the original paper even if they do not match exactly. However, the same cannot always be said of the additional experiments.What was easyThe paper was clear and matched the implementation. The code was well organized and was easy to run using the command interface provided by the authors. This also made it easy to replicate and expand upon it by adding our own new features. The data was also readily available and could be easily downloaded with no need for preprocessing.What was difficultWe had to run several iterations of the same code, using different seeds and models, to get the results with the same conditions as in the original paper, which made use of time and resources. Our own experiments required additional time to hand-annotate data due to lack of data for new features.Communication with original authorsThere was no contact with the authors, since the code and the experiments were clear and did not need any additional explanation.",Journal,NeurIPS,2023,Poster,"Martijn van Raaphorst, Egoitz Gonzalez, Marta Grasa, Paula Antequera Hernández",True,https://openreview.net/pdf?id=74146
74156,Reproducibility Study of “Quantifying Societal Bias Amplification in Image Captioning”,"Scope of reproducibility - We study the reproducibility of the paper ""Quantifying Societal Bias Amplification in Image Captioning"" by Hirota et al. In this paper, the authors propose a new metric to measure bias amplification, called LIC, and evaluate it on multiple image captioning models. Based on this evaluation, they make the following main claims which we aim to verify: (1) all models amplify gender bias, (2) all models amplify racial bias, (3) LIC is robust against encoders, and (4) the NIC+Equalizer model increases gender bias with respect to the baseline. We also extend upon the original work by evaluating LIC for age bias.Methodology - For our reproduction, we were able to run the code provided by the authors without any modifications. For our extension, we automatically labelled the images in the dataset with age annotations and adjusted the code to work with this dataset. In total, 38 GPU hours were needed to perform all experiments.Results - The reproduced results are close to the original results and support all four main claims. Furthermore, our additional results show that only a subset of the models amplifies age bias, while they strengthen the claim that LIC is robust against encoders. However, we acknowledge that our extension to age bias has its limitations.What was easy - The author's code and the data needed to run it are publicly available. The code required no modification to run and the scripts were provided with an extensive argument parser, allowing us to quickly set up our experiments. Moreover, the details of the original experiments were clearly stated in the appendix.What was difficult - We found that it was difficult to interpret the author's code as the provided documentation contained room for improvement. Also, the scripts contained repetitive code. While the authors retrained all image captioning models, they did not share the model weights, making it difficult to extend upon their work.Communication with original authors - No (attempt at) communication with the original authors was performed.",Journal,NeurIPS,2023,Poster,"Farrukh Baratov, Goksenin Yuksel, Darie Petcu, Jan Bakker",True,https://openreview.net/pdf?id=74156
74184,[Re] On Explainability of Graph Neural Networks via Subgraph Explorations,"Yuan et al. claim their proposed method SubgraphX achieves (i) higher fidelity in explaining models for graph- and node classification tasks compared to other explanation techniques, namely GNNExplainer. Additionally, (ii) the computational effort of SubgraphX is at a 'reasonable level', which is not further specified by the original authors. We define this as at most ten times slower than GNNExplainer. We reimplemented the proposed algorithm in PyTorch. Then, we replicated the experiments performed by the authors on a smaller scale due to resource constraints. Additionally, we checked the performance on a new dataset and investigated the influence of hyperparameters. Lastly, we improved SubgraphX using greedy initialization and utilizing fidelity as a score function. We were able to reproduce the main claims on the MUTAG dataset, where SubgraphX has a better performance than GNNExplainer. Furthermore, SubgraphX has a reasonable runtime of about seven times longer than GNNExplainer. We successfully employed SubgraphX on the Karate Club dataset, where it outperforms GNNExplainer as well. The hyperparameter study revealed that the number of Monte-Carlo Tree search iterations and Monte-Carlo sampling steps are the most important hyperparameters and directly trade performance for runtime. Lastly, we show that our proposed improvements to SubgraphX significantly enhance fidelity and runtime. The authors' description of the algorithm was clear and concise. The original implementation is available in the DIG-library as a reference to our implementation. The authors performed extensive experiments, which we could not replicate in their full scale due to resource constraints. However, we were able to achieve similar results on a subset of the datasets used. Another issue was that despite the original code of the authors and datasets being publicly available, there were many compatibility issues. The original authors briefly reviewed our work and agreed with the findings.",Journal,NeurIPS,2023,Poster,"Yannik Mahlau, Lukas Berg, Leonie Kayser",True,https://openreview.net/pdf?id=74184
7AjdHnjIHX,COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs,"Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of multimodal vision-language models via training data augmentation. We make our code and the COCO-Counterfactuals dataset publicly available.",Datasets & Benchmarks,NeurIPS,2023,Poster,Tiep Le;Vasudev Lal;Phillip Howard,True,https://openreview.net/pdf?id=7AjdHnjIHX
7EMphtUgCI,AVIS: Autonomous Visual Information Seeking with Large Language Model Agent,"In this paper, we propose an autonomous information seeking visual question answering framework, AVIS. Our method leverages a Large Language Model (LLM) to dynamically strategize the utilization of external tools and to investigate their outputs via tree search, thereby acquiring the indispensable knowledge needed to provide answers to the posed questions. Responding to visual questions that necessitate external knowledge, such as ""What event is commemorated by the building depicted in this image?"", is a complex task. This task presents a combinatorial search space that demands a sequence of actions, including invoking APIs, analyzing their responses, and making informed decisions. We conduct a user study to collect a variety of instances of human decision-making when faced with this task. This data is then used to design a system comprised of three components: an LLM-powered planner that dynamically determines which tool to use next, an LLM-powered reasoner that analyzes and extracts key information from the tool outputs, and a working memory component that retains the acquired information throughout the process. The collected user behavior serves as a guide for our system in two key ways. First, we create a transition graph by analyzing the sequence of decisions made by users. This graph delineates distinct states and confines the set of actions available at each state. Second, we use examples of user decision-making to provide our LLM-powered planner and reasoner with relevant contextual instances, enhancing their capacity to make informed decisions. We show that AVIS achieves state-of-the-art results on knowledge-based visual question answering benchmarks such as Infoseek and OK-VQA.",main,NeurIPS,2023,Poster,Ziniu Hu;Ahmet Iscen;Chen Sun;Kai-Wei Chang;Yizhou Sun;David A Ross;Cordelia Schmid;Alireza Fathi,True,https://openreview.net/pdf?id=7EMphtUgCI
7NR2ZVzZxx,LogicBench: A Benchmark for Evaluation of Logical Reasoning,"Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really ""Reason"" over the natural language? This question has been receiving significant research attention and a number of reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability has focused only on a couple of axioms (such as modus ponens and modus tollens) of propositional and first-order logic. To study logical reasoning,  we introduce LogicBench, a systematically created natural language question-answering dataset encompassing 25 reasoning patterns spanning over propositional, first-order, and non-monotonic logics. Key steps of our dataset construction consist of (1) controlled generation of sentences and their negations containing different ontologies, (2) (context, question, answer) triplets creation using heuristically designed templates, and (3) semantic variations of triplets adding more diversity. We first evaluate easily accessible and widely used LLMs such as GPT-3, ChatGPT, and FLAN-T5 and show that they do not fare well on LogicBench, achieving just above random accuracy on average (~52%). Then, we show that LLMs trained using our data exhibit a better understanding of logical reasoning leading to performance improvements on several existing logical reasoning datasets such as LogicNLI, FOLIO, LogiQA, and ReClor.",Datasets & Benchmarks,NeurIPS,2023,Reject,Mihir Parmar;Neeraj Varshney;Nisarg Patel;Santosh Mashetty;Man Luo;Arindam Mitra;Chitta Baral,True,https://openreview.net/pdf?id=7NR2ZVzZxx
7VSBaP2OXN,"Waymax: An Accelerated, Data-Driven Simulator for Large-Scale Autonomous Driving Research","Simulation is an essential tool to develop and benchmark autonomous vehicle planning software in a safe and cost-effective manner. However, realistic simulation requires accurate modeling of multi-agent interactive behaviors to be trustworthy, behaviors which can be highly nuanced and complex. To address these challenges, we introduce Waymax, a new data-driven simulator for autonomous driving in multi-agent scenes, designed for large-scale simulation and testing.  Waymax uses publicly-released, real-world driving data (e.g., the Waymo Open Motion Dataset) to initialize or play back a diverse set of multi-agent simulated scenarios.   It runs entirely on hardware accelerators such as TPUs/GPUs and supports in-graph simulation for training, making it suitable for modern large-scale, distributed machine learning workflows. To support online training and evaluation, Waymax includes several learned and hard-coded behavior models that allow for realistic interaction within simulation. To supplement Waymax, we benchmark a suite of popular imitation and reinforcement learning algorithms with ablation studies on different design decisions, where we highlight the effectiveness of routes as guidance for planning agents and the ability of RL to overfit against simulated agents.",Datasets & Benchmarks,NeurIPS,2023,Poster,Cole Gulino;Justin Fu;Wenjie Luo;George Tucker;Eli Bronstein;Yiren Lu;Jean Harb;Xinlei Pan;Yan Wang;Xiangyu Chen;John D Co-Reyes;Rishabh Agarwal;Rebecca Roelofs;Yao Lu;Nico Montali;Paul Mougin;Zoey Zeyu Yang;Brandyn White;Aleksandra Faust;Rowan Thomas McAllister;Dragomir Anguelov;Benjamin Sapp,False,https://openreview.net/pdf?id=7VSBaP2OXN
7bghy0Gq75,HOH: Markerless Multimodal Human-Object-Human Handover Dataset with Large Object Count,"We present the HOH (Human-Object-Human) Handover Dataset, a large object count dataset with 136 objects, to accelerate data-driven research on handover studies, human-robot handover implementation, and artificial intelligence (AI) on handover parameter estimation from 2D and 3D data of two-person interactions. HOH contains multi-view RGB and depth data, skeletons, fused point clouds, grasp type and handedness labels, object, giver hand, and receiver hand 2D and 3D segmentations, giver and receiver comfort ratings, and paired object metadata and aligned 3D models for 2,720 handover interactions spanning 136 objects and 20 giver-receiver pairs—40 with role-reversal—organized from 40 participants. We also show experimental results of neural networks trained using HOH to perform grasp, orientation, and trajectory prediction. As the only fully markerless handover capture dataset, HOH represents natural human-human handover interactions, overcoming challenges with markered datasets that require specific suiting for body tracking, and lack high-resolution hand tracking. To date, HOH is the largest handover dataset in terms of object count, participant count, pairs with role reversal accounted for, and total interactions captured.",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=7bghy0Gq75
7fRThuXp3h,Off-the-Grid MARL: Datasets with Baselines for Offline Multi-Agent Reinforcement Learning,"Being able to harness the power of large datasets for developing cooperative multi-agent controllers promises to unlock enormous value for real-world applications. 
Many important industrial systems are multi-agent in nature and are difficult to model using bespoke simulators. 
However, in industry, distributed processes can often be recorded during operation, and large quantities of demonstrative data stored.
Offline multi-agent reinforcement learning (MARL) provides a promising paradigm for building effective decentralised controllers from such datasets. 
However, offline MARL is still in its infancy and therefore lacks standardised benchmark datasets and baselines typically found in more mature subfields of reinforcement learning (RL). 
These deficiencies make it difficult for the community to sensibly measure progress. 
In this work, we aim to fill this gap by releasing \\\\emph{off-the-grid MARL (OG-MARL)}: a growing repository of high-quality datasets with baselines for cooperative offline MARL research.
Our datasets provide settings that are characteristic of real-world systems, including complex environment dynamics, heterogeneous agents, non-stationarity, many agents, partial observability, suboptimality, sparse rewards and demonstrated coordination.
For each setting, we provide a range of different dataset types (e.g. \\\\texttt{Good}, \\\\texttt{Medium}, \\\\texttt{Poor}, and \\\\texttt{Replay}) and profile the composition of experiences for each dataset.
We hope that OG-MARL will serve the community as a reliable source of datasets and help drive progress, while also providing an accessible entry point for researchers new to the field.",Datasets & Benchmarks,NeurIPS,2023,Reject,Juan Claude Formanek;Asad Jeewa;Jonathan Phillip Shock;Arnu Pretorius,True,https://openreview.net/pdf?id=7fRThuXp3h
7gbjsgcN5p,Unsupervised Optical Flow Estimation with Dynamic Timing Representation for Spike Camera,"Efficiently selecting an appropriate spike stream data length to extract precise information is the key to the spike vision tasks. To address this issue, we propose a dynamic timing representation for spike streams. Based on multi-layers architecture, it applies dilated convolutions on temporal dimension to extract features on multi-temporal scales with few parameters. And we design layer attention to dynamically fuse these features. Moreover, we propose an unsupervised learning method for optical flow estimation in a spike-based manner to break the dependence on labeled data. In addition, to verify the robustness, we also build a spike-based synthetic validation dataset for extreme scenarios in autonomous driving, denoted as SSES dataset. It consists of various corner cases. Experiments show that our method can predict optical flow from spike streams in different high-speed scenes, including real scenes. For instance, our method achieves $15\\\\%$ and $19\\\\%$ error reduction on PHM dataset compared to the best spike-based work, SCFlow, in $\\\\Delta t=10$ and $\\\\Delta t=20$ respectively, using the same settings as in previous works. The source code and dataset are available at \\\\href{https://github.com/Bosserhead/USFlow}{https://github.com/Bosserhead/USFlow}.",main,NeurIPS,2023,Poster,Lujie Xia;Ziluo Ding;Rui Zhao;Jiyuan Zhang;Lei Ma;Zhaofei Yu;Tiejun Huang;Ruiqin Xiong,True,https://openreview.net/pdf?id=7gbjsgcN5p
7kc4gtEk3b,A Comprehensive Benchmark for Neural Human Radiance Fields,"The past two years have witnessed a significant increase in interest concerning NeRF-based human body rendering. While this surge has propelled considerable advancements, it has also led to an influx of methods and datasets. This explosion complicates experimental settings and makes fair comparisons challenging. In this work, we design and execute thorough studies into unified evaluation settings and metrics to establish a fair and reasonable benchmark for human NeRF models. To reveal the effects of extant models, we benchmark them against diverse and hard scenes. Additionally, we construct a cross-subject benchmark pre-trained on large-scale datasets to assess generalizable methods. Finally, we analyze the essential components for animatability and generalizability, and make HumanNeRF from monocular videos generalizable, as the inaugural baseline. We hope these benchmarks and analyses could serve the community.",Datasets & Benchmarks,NeurIPS,2023,Poster,Kenkun Liu;Derong Jin;Ailing Zeng;Xiaoguang Han;Lei Zhang,False,https://openreview.net/pdf?id=7kc4gtEk3b
7p5YWe8GqG,MeGraph: Capturing Long-Range Interactions by Alternating Local and Hierarchical Aggregation on Multi-Scaled Graph Hierarchy,"Graph neural networks, which typically exchange information between local neighbors, often struggle to capture long-range interactions (LRIs) within the graph. Building a graph hierarchy via graph pooling methods is a promising approach to address this challenge; however, hierarchical information propagation cannot entirely take over the role of local information aggregation. To balance locality and hierarchy, we integrate the local and hierarchical structures, represented by intra- and inter-graphs respectively, of a multi-scale graph hierarchy into a single mega graph. Our proposed MeGraph model consists of multiple layers alternating between local and hierarchical information aggregation on the mega graph. Each layer first performs local-aware message-passing on graphs of varied scales via the intra-graph edges, then fuses information across the entire hierarchy along the bidirectional pathways formed by inter-graph edges. By repeating this fusion process, local and hierarchical information could intertwine and complement each other. To evaluate our model, we establish a new Graph Theory Benchmark designed to assess LRI capture ability, in which MeGraph demonstrates dominant performance. Furthermore, MeGraph exhibits superior or equivalent performance to state-of-the-art models on the Long Range Graph Benchmark. The experimental results on commonly adopted real-world datasets further demonstrate the broad applicability of MeGraph.",main,NeurIPS,2023,Poster,Honghua Dong;Jiawei Xu;Yu Yang;Rui Zhao;Shiwen Wu;Chun Yuan;Xiu Li;Chris J. Maddison;Lei Han,True,https://openreview.net/pdf?id=7p5YWe8GqG
7tMgzSvopH,Massively Multilingual Corpus of Sentiment Datasets and Multi-faceted Sentiment Classification Benchmark,"Despite impressive advancements in multilingual corpora collection and model training, developing large-scale deployments of multilingual models still presents a significant challenge. This is particularly true for language tasks that are culture-dependent. One such example is the area of multilingual sentiment analysis, where affective markers can be subtle and deeply ensconced in culture.

This work presents the most extensive open massively multilingual corpus of datasets for training sentiment models. The corpus consists of 79 manually selected datasets from over 350 datasets reported in the scientific literature based on strict quality criteria. The corpus covers 27 languages representing 6 language families. Datasets can be queried using several linguistic and functional features. In addition, we present a multi-faceted sentiment classification benchmark summarizing hundreds of experiments conducted on different base models, training objectives, dataset collections, and fine-tuning strategies.",Datasets & Benchmarks,NeurIPS,2023,Poster,Lukasz Augustyniak;Szymon Woźniak;Marcin Gruza;Piotr Gramacki;Krzysztof Rajda;Mikołaj Morzy;Tomasz Jan Kajdanowicz,True,https://openreview.net/pdf?id=7tMgzSvopH
80g3Yqlo1a,Fast Scalable and Accurate Discovery of DAGs Using the Best Order Score Search and Grow Shrink Trees,"Learning graphical conditional independence structures is an important machine learning problem and a cornerstone of causal discovery. However, the accuracy and execution time of learning algorithms generally struggle to scale to problems with hundreds of highly connected variables---for instance, recovering brain networks from fMRI data. We introduce the best order score search (BOSS) and grow-shrink trees (GSTs) for learning directed acyclic graphs (DAGs) in this paradigm. BOSS greedily searches over permutations of variables, using GSTs to construct and score DAGs from permutations. GSTs efficiently cache scores to eliminate redundant calculations. BOSS achieves state-of-the-art performance in accuracy and execution time, comparing favorably to a variety of combinatorial and gradient-based learning algorithms under a broad range of conditions. To demonstrate its practicality, we apply BOSS to two sets of resting-state fMRI data: simulated data with pseudo-empirical noise distributions derived from randomized empirical fMRI cortical signals and clinical data from 3T fMRI scans processed into cortical parcels. BOSS is available for use within the TETRAD project which includes Python and R wrappers.",main,NeurIPS,2023,Poster,Bryan Andrews;Joseph Ramsey;Ruben Sanchez Romero;Jazmin Camchong;Erich Kummerfeld,True,https://openreview.net/pdf?id=80g3Yqlo1a
846X3N11bf,ProBio: A Protocol-guided Multimodal Dataset for Molecular Biology Lab,"The challenge of replicating research results has posed a significant impediment to the field of molecular biology. The advent of modern intelligent systems has led to notable progress in various domains. Consequently, we embarked on an investigation of intelligent monitoring systems as a means of tackling the issue of the reproducibility crisis. Specifically, we first curate a comprehensive multimodal dataset, named ProBio, as an initial step towards this objective. This dataset comprises fine-grained hierarchical annotations intended for the purpose of studying activity understanding in BioLab. Next, we devise two challenging benchmarks, transparent solution tracking and multimodal action recognition, to emphasize the unique characteristics and difficulties associated with activity understanding in BioLab settings. Finally, we provide a thorough experimental evaluation of contemporary video understanding models and highlight their limitations in this specialized domain to identify potential avenues for future research. We hope ProBio with associated benchmarks may garner increased focus on modern AI techniques in the realm of molecular biology.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jieming Cui;Ziren Gong;Baoxiong Jia;Siyuan Huang;Zilong Zheng;Jianzhu Ma;Yixin Zhu,True,https://openreview.net/pdf?id=846X3N11bf
86dXbqT5Ua,Geometry-Informed Neural Operator for Large-Scale 3D PDEs,"We propose the geometry-informed neural operator (GINO), a highly efficient approach for learning the solution operator of large-scale partial differential equations with varying geometries. GINO uses a signed distance function (SDF) representation of the input shape and neural operators based on graph and Fourier architectures to learn the solution operator. The graph neural operator handles irregular grids and transforms them into and from regular latent grids on which Fourier neural operator can be efficiently applied. We provide an efficient implementation of GINO using an optimized hashing approach, which allows efficient learning  in a shared, compressed latent space with reduced computation and memory costs. GINO is discretization-invariant, meaning the trained model can be applied to arbitrary discretizations of the continuous domain and applies to any shape or resolution.  To empirically validate the performance of our method on large-scale simulation, we generate the industry-standard aerodynamics dataset of 3D vehicle geometries with Reynolds numbers as high as five million. For this large-scale 3D fluid simulation, numerical methods are expensive to compute surface pressure. We successfully trained GINO to predict the pressure on car surfaces using only five hundred data points. The cost-accuracy experiments show a 26,000x speed-up compared to optimized GPU-based computational fluid dynamics (CFD) simulators on computing the drag coefficient. When tested on new combinations of geometries and boundary conditions (inlet velocities), GINO obtains a one-fourth reduction in error rate compared to deep neural network approaches.",main,NeurIPS,2023,Poster,Zongyi Li;Nikola Borislavov Kovachki;Chris Choy;Boyi Li;Jean Kossaifi;Shourya Prakash Otta;Mohammad Amin Nabian;Maximilian Stadler;Christian Hundt;Kamyar Azizzadenesheli;Anima Anandkumar,True,https://openreview.net/pdf?id=86dXbqT5Ua
8JCZe7QrPy,Systematic Visual Reasoning through Object-Centric Relational Abstraction,"Human visual reasoning is characterized by an ability to identify abstract patterns from only a small number of examples, and to systematically generalize those patterns to novel inputs. This capacity depends in large part on our ability to represent complex visual inputs in terms of both objects and relations. Recent work in computer vision has introduced models with the capacity to extract object-centric representations, leading to the ability to process multi-object visual inputs, but falling short of the systematic generalization displayed by human reasoning. Other recent models have employed inductive biases for relational abstraction to achieve systematic generalization of learned abstract rules, but have generally assumed the presence of object-focused inputs. Here, we combine these two approaches, introducing Object-Centric Relational Abstraction (OCRA), a model that extracts explicit representations of both objects and abstract relations, and achieves strong systematic generalization in tasks (including a novel dataset, CLEVR-ART, with greater visual complexity) involving complex visual displays.",main,NeurIPS,2023,Poster,Taylor Whittington Webb;Shanka Subhra Mondal;Jonathan Cohen,True,https://openreview.net/pdf?id=8JCZe7QrPy
8JsbdJjRvY,3D Indoor Instance Segmentation in an Open-World,"Existing 3D instance segmentation methods typically assume that all semantic classes to be segmented would be available during training and only seen categories are segmented at inference. We argue that such a closed-world assumption is restrictive and explore for the first time 3D indoor instance segmentation in an open-world setting, where the model is allowed to distinguish a set of known classes as well as identify an unknown object as unknown and then later incrementally learning the semantic category of the unknown when the corresponding category labels are available. To this end, we introduce an open-world 3D indoor instance segmentation method, where an auto-labeling scheme is employed to produce pseudo-labels during training and induce separation to separate known and unknown category labels. We further improve the pseudo-labels quality at inference by adjusting the unknown class probability based on the objectness score distribution. We also introduce carefully curated open-world splits leveraging realistic scenarios based on inherent object distribution, region-based indoor scene exploration and randomness aspect of open-world classes. Extensive experiments reveal the efficacy of the proposed contributions leading to promising open-world 3D instance segmentation performance. Code and splits are available at: https://github.com/aminebdj/3D-OWIS.",main,NeurIPS,2023,Poster,Mohamed El Amine Boudjoghra;Salwa K. Al Khatib;Jean Lahoud;Hisham Cholakkal;Rao Muhammad Anwer;Salman Khan;Fahad Khan,True,https://openreview.net/pdf?id=8JsbdJjRvY
8TMhs2pIfG,NAVI: Category-Agnostic Image Collections with High-Quality 3D Shape and Pose Annotations,"Recent advances in neural reconstruction enable high-quality 3D object reconstruction from casually captured image collections. Current techniques mostly analyze their progress on relatively simple image collections where SfM techniques can provide ground-truth (GT) camera poses. We note that SfM techniques tend to fail on in-the-wild image collections such as image search results with varying backgrounds and illuminations. To enable systematic research progress on 3D reconstruction from casual image captures, we propose `NAVI': a new dataset of category-agnostic image collections of objects with high-quality 3D scans along with per-image 2D-3D alignments providing near-perfect GT camera parameters. These 2D-3D alignments allow us to extract accurate derivative annotations such as dense pixel correspondences, depth and segmentation maps. We demonstrate the use of NAVI image collections on different problem settings and show that NAVI enables more thorough evaluations that were not possible with existing datasets. We believe NAVI is beneficial for systematic research progress on 3D reconstruction and correspondence estimation.",Datasets & Benchmarks,NeurIPS,2023,Poster,Varun Jampani;Kevis-kokitsi Maninis;Andreas Engelhardt;Arjun Karpur;Karen Truong;Kyle Sargent;Stefan Popov;Andre Araujo;Ricardo Martin Brualla;Kaushal Patel;Daniel Vlasic;Vittorio Ferrari;Ameesh Makadia;Ce Liu;Yuanzhen Li;Howard Zhou,True,https://openreview.net/pdf?id=8TMhs2pIfG
8ZRAHNT7E9,LagrangeBench: A Lagrangian Fluid Mechanics Benchmarking Suite,"Machine learning has been successfully applied to grid-based PDE modeling in various scientific applications. However, learned PDE solvers based on Lagrangian particle discretizations, which are the preferred approach to problems with free surfaces or complex physics, remain largely unexplored. We present LagrangeBench, the first benchmarking suite for Lagrangian particle problems, focusing on temporal coarse-graining. In particular, our contribution is: (a) seven new fluid mechanics datasets (four in 2D and three in 3D) generated with the Smoothed Particle Hydrodynamics (SPH) method including the Taylor-Green vortex, lid-driven cavity, reverse Poiseuille flow, and dam break, each of which includes different physics like solid wall interactions or free surface, (b) efficient JAX-based API with various recent training strategies and three neighbor search routines, and (c) JAX implementation of established Graph Neural Networks (GNNs) like GNS and SEGNN with baseline results. Finally, to measure the performance of learned surrogates we go beyond established position errors and introduce physical metrics like kinetic energy MSE and Sinkhorn distance for the particle distribution. Our codebase is available under the URL: [https://github.com/tumaer/lagrangebench](https://github.com/tumaer/lagrangebench).",Datasets & Benchmarks,NeurIPS,2023,Poster,Artur Toshev;Gianluca Galletti;Fabian Fritz;Stefan Adami;Nikolaus A. Adams,True,https://openreview.net/pdf?id=8ZRAHNT7E9
8bqjirgxQM,Understanding Social Reasoning in Language Models with Language Models,"As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Kanishk Gandhi;Jan-Philipp Fränken;Tobias Gerstenberg;Noah Goodman,True,https://openreview.net/pdf?id=8bqjirgxQM
8eVgdwKs2N,G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks,"Graph neural networks (GNNs) have recently emerged as a promising approach for solving the Boolean Satisfiability Problem (SAT), offering potential alternatives to traditional backtracking or local search SAT solvers. However, despite the growing volume of literature in this field, there remains a notable absence of a unified dataset and a fair benchmark to evaluate and compare existing approaches. To address this crucial gap, we present G4SATBench, the first benchmark study that establishes a comprehensive evaluation framework for GNN-based SAT solvers. In G4SATBench, we meticulously curate a large and diverse set of SAT datasets comprising 7 problems with 3 difficulty levels and benchmark a broad range of GNN models across various prediction tasks, training objectives, and inference algorithms. To explore the learning abilities and comprehend the strengths and limitations of GNN-based SAT solvers, we also compare their solving processes with the heuristics in search-based SAT solvers. Our empirical results provide valuable insights into the performance of GNN-based SAT solvers and further suggest that existing GNN models can effectively learn a solving strategy akin to greedy local search but struggle to learn backtracking search in the latent space.",Datasets & Benchmarks,NeurIPS,2023,Reject,Zhaoyu Li;Jinpei Guo;Xujie Si,True,https://openreview.net/pdf?id=8eVgdwKs2N
8gDJXL652A,Pairwise GUI Dataset Construction Between Android Phones and Tablets,"In the current landscape of pervasive smartphones and tablets, apps frequently exist across both platforms.
Although apps share most graphic user interfaces (GUIs) and functionalities across phones and tablets, developers often rebuild from scratch for tablet versions, escalating costs and squandering existing design resources.
Researchers are attempting to collect data and employ deep learning in automated GUIs development to enhance developers' productivity.
There are currently several publicly accessible GUI page datasets for phones, but none for pairwise GUIs between phones and tablets.
This poses a significant barrier to the employment of deep learning in automated GUI development.
In this paper, we introduce the Papt dataset, a pioneering pairwise GUI dataset tailored for Android phones and tablets, encompassing 10,035 phone-tablet GUI page pairs sourced from 5,593 unique app pairs.
We propose novel pairwise GUI collection approaches for constructing this dataset and delineate its advantages over currently prevailing datasets in the field.
Through preliminary experiments on this dataset, we analyze the present challenges of utilizing deep learning in automated GUI development.",Datasets & Benchmarks,NeurIPS,2023,Poster,Han Hu;Haolan Zhan;Yujin Huang;Di Liu,True,https://openreview.net/pdf?id=8gDJXL652A
9B57dEeP3O,Intelligent Grimm - Open-ended Visual Storytelling via Latent Diffusion Models,"Benefiting from the impressive diffusion models, conditional generative models have exhibited exceptional capabilities in various generation tasks, for example, image or short video generation based on text description. In this work, we focus on the task of generating a series of coherent images based on a given storyline, denoted as open-ended visual storytelling. We make the following three contributions: (i) to fulfill the task of visual storytelling, we introduce two modules into a pre-trained stable diffusion model, and construct an auto-regressive image generator, termed as StoryGen, that enables to generate the current frame by conditioning on a text prompt and preceding frame; (ii) to train our proposed model, we collect paired image and text samples by sourcing from various online sources, such as videos, E-books, and establish a data processing pipeline for constructing a diverse dataset, named StorySalon, with a far larger vocabulary than existing animation-specific datasets; (iii) we adopt a three-stage curriculum training strategy, that enables style transfer, visual context conditioning, and human feedback alignment, respectively. Quantitative experiments and human evaluation have validated the superiority of our proposed model, in terms of image quality, style consistency, content consistency, and visual-language alignment. We will make the code, model, and dataset publicly available to the research community.",main,NeurIPS,2023,Reject,Chang Liu;Haoning Wu;Yujie Zhong;Xiaoyun Zhang;Weidi Xie,True,https://openreview.net/pdf?id=9B57dEeP3O
9CKx9SsSSc,ADGym: Design Choices for Deep Anomaly Detection,"Deep learning (DL) techniques have recently found success in anomaly detection (AD) across various fields such as finance, medical services, and cloud computing. However, most of the current research tends to view deep AD algorithms as a whole, without dissecting the contributions of individual design choices like loss functions and network architectures. This view tends to diminish the value of preliminary steps like data preprocessing, as more attention is given to newly designed loss functions, network architectures, and learning paradigms. In this paper, we aim to bridge this gap by asking two key questions: (i) Which design choices in deep AD methods are crucial for detecting anomalies? (ii) How can we automatically select the optimal design choices for a given AD dataset, instead of relying on generic, pre-existing solutions? To address these questions, we introduce ADGym, a platform specifically crafted for comprehensive evaluation and automatic selection of AD design elements in deep methods. Our extensive experiments reveal that relying solely on existing leading methods is not sufficient. In contrast, models developed using ADGym significantly surpass current state-of-the-art techniques.",Datasets & Benchmarks,NeurIPS,2023,Poster,Minqi Jiang;Chaochuan Hou;Ao Zheng;Songqiao Han;Hailiang Huang;Qingsong Wen;Xiyang Hu;Yue Zhao,False,https://openreview.net/pdf?id=9CKx9SsSSc
9FLkxTGY3B,${\\\\rm EFO}_k$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation,"To answer complex queries on knowledge graphs, logical reasoning over incomplete knowledge is required due to the open-world assumption. Learning-based methods are essential because they are capable of generalizing over unobserved knowledge. Therefore, an appropriate dataset is fundamental to both obtaining and evaluating such methods under this paradigm. In this paper, we propose ${\\\\rm EFO}_k$-CQA, a comprehensive framework for data generation, model training, and method evaluation that covers the combinatorial space of Existential First-order Queries with multiple variables (${\\\\rm EFO}_k$). The combinatorial query space in our framework significantly extends those defined by set operations in the existing literature. Additionally, we construct a dataset with 741 query types for empirical evaluation, and our benchmark results provide new insights into how query hardness affects the results. Furthermore, we demonstrate that the existing dataset construction process is systematically biased that hinders the appropriate development of query-answering methods, highlighting the importance of our work. Our code and data are provided in~\\\\url{https://anonymous.4open.science/r/EFOK-CQA/README.md}.",Datasets & Benchmarks,NeurIPS,2023,Reject,Hang Yin;Zihao Wang;Fei Weizhi;Yangqiu Song,True,https://openreview.net/pdf?id=9FLkxTGY3B
9QsdPQlWiE,Test-time Training for Matching-based Video Object Segmentation,"The video object segmentation (VOS) task involves the segmentation of an object over time based on a single initial mask. Current state-of-the-art approaches use a memory of previously processed frames and rely on matching to estimate segmentation masks of subsequent frames. Lacking any adaptation mechanism, such methods are prone to test-time distribution shifts. This work focuses on matching-based VOS under distribution shifts such as video corruptions, stylization, and sim-to-real transfer. We explore test-time training strategies that are agnostic to the specific task as well as strategies that are designed specifically for VOS. This includes a variant based on mask cycle consistency tailored to matching-based VOS methods. The experimental results on common benchmarks demonstrate that the proposed test-time training yields significant improvements in performance. In particular for the sim-to-real scenario and despite using only a single test video, our approach manages to recover a substantial portion of the performance gain achieved through training on real videos. Additionally, we introduce DAVIS-C, an augmented version of the popular DAVIS test set, featuring extreme distribution shifts like image-/video-level corruptions and stylizations. Our results illustrate that test-time training enhances performance even in these challenging cases.",main,NeurIPS,2023,Poster,Juliette Bertrand;Giorgos Kordopatis-Zilos;Yannis Kalantidis;Giorgos Tolias,True,https://openreview.net/pdf?id=9QsdPQlWiE
9U8bqr8epr,EMBERSim: A Large-Scale Databank for Boosting Similarity Search in Malware Analysis,"In recent years there has been a shift from heuristics based malware detection towards machine learning, which proves to be more robust in the current heavily adversarial threat landscape. While we acknowledge machine learning to be better equipped to mine for patterns in the increasingly high amounts of similar-looking files, we also note a remarkable scarcity of the data available for similarity targeted research. Moreover, we observe that the focus in the few related works falls on quantifying similarity in malware, often overlooking the clean data. This one-sided quantification is especially dangerous in the context of detection bypass. We propose to address the deficiencies in the space of similarity research on binary files, starting from EMBER — one of the largest malware classification datasets. We enhance EMBER with similarity information as well as malware class tags, to enable further research in the similarity space. Our contribution is threefold: (1) we publish EMBERSim, an augmented version of EMBER, that includes similarity informed tags; (2) we enrich EMBERSim with automatically determined malware class tags using the open-source tool AVClass on VirusTotal data and (3) we describe and share the implementation for our class scoring technique and leaf similarity method.",Datasets & Benchmarks,NeurIPS,2023,Poster,Dragos Georgian Corlatescu;Alexandru Dinu;Mihaela Gaman;Paul Sumedrea,True,https://openreview.net/pdf?id=9U8bqr8epr
9UxUTGCteW,LuminAIRe: Illumination-Aware Conditional Image Repainting for Lighting-Realistic Generation,"We present the ilLumination-Aware conditional Image Repainting (LuminAIRe) task to address the unrealistic lighting effects in recent conditional image repainting (CIR) methods. The environment lighting and 3D geometry conditions are explicitly estimated from given background images and parsing masks using a parametric lighting representation and learning-based priors. These 3D conditions are then converted into illumination images through the proposed physically-based illumination rendering and illumination attention module. With the injection of illumination images, physically-correct lighting information is fed into the lighting-realistic generation process and repainted images with harmonized lighting effects in both foreground and background regions can be acquired, whose superiority over the results of state-of-the-art methods is confirmed through extensive experiments. For facilitating and validating the LuminAIRe task, a new dataset Car-LuminAIRe with lighting annotations and rich appearance variants is collected.",main,NeurIPS,2023,Poster,Jiajun Tang;Haofeng Zhong;Shuchen Weng;Boxin Shi,True,https://openreview.net/pdf?id=9UxUTGCteW
9Z1cmO7S7o,Generating QM1B with PySCF$_{\\\\text{IPU}}$,"The emergence of foundation models in Computer Vision and Natural Language Processing have resulted in immense progress on downstream tasks.  This progress was enabled by datasets with billions of training examples. Similar benefits are yet to be unlocked for quantum chemistry, where the potential of deep learning is constrained by comparatively small datasets with 100k to 20M training examples. These datasets are limited in size because the labels are computed using the accurate (but computationally demanding) predictions of Density Functional Theory (DFT). Notably, prior DFT datasets were created using CPU supercomputers without leveraging hardware acceleration.  In this paper, we take a first step towards utilising hardware accelerators by introducing the data generator PySCF$_{\\\\text{IPU}}$ using Intelligence Processing Units (IPUs). This allows us to create the dataset QM1B with one billion training examples containing 9-11 heavy atoms. We demonstrate that a simple baseline neural network (SchNet 9M) improves its performance by simply increasing the amount of training data without additional inductive biases. To encourage future researchers to use QM1B responsibly, we highlight several limitations of QM1B and emphasise the low resolution of our DFT options, which also serves as motivation for even larger, more accurate datasets.",Datasets & Benchmarks,NeurIPS,2023,Poster,Alexander Mathiasen;Hatem Helal;Kerstin Klaeser;Paul Balanca;Josef Dean;Carlo Luschi;Dominique Beaini;Andrew W Fitzgibbon;Dominic Masters,True,https://openreview.net/pdf?id=9Z1cmO7S7o
9gLnjw8DfA,Digital Typhoon: Long-term Satellite Image Dataset for the Spatio-Temporal Modeling of Tropical Cyclones,"This paper presents the official release of the Digital Typhoon dataset, the longest typhoon satellite image dataset for 40+ years aimed at benchmarking machine learning models for long-term spatio-temporal data. To build the dataset, we developed a workflow to create an infrared typhoon-centered image for cropping using Lambert azimuthal equal-area projection referring to the best track data. We also address data quality issues such as inter-satellite calibration to create a homogeneous dataset. To take advantage of the dataset, we organized machine learning tasks by the types and targets of inference, with other tasks for meteorological analysis, societal impact, and climate change. The benchmarking results on the analysis, forecasting, and reanalysis for the intensity suggest that the dataset is challenging for recent deep learning models, due to many choices that affect the performance of various models. This dataset reduces the barrier for machine learning researchers to meet large-scale real-world events called tropical cyclones and develop machine learning models that may contribute to advancing scientific knowledge on tropical cyclones as well as solving societal and sustainability issues such as disaster reduction and climate change. The dataset is publicly available at http://agora.ex.nii.ac.jp/digital-typhoon/dataset/ and https://github.com/kitamoto-lab/digital-typhoon/.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Asanobu Kitamoto;Jared Hwang;Bastien Vuillod;Lucas Gautier;Yingtao Tian;Tarin Clanuwat,True,https://openreview.net/pdf?id=9gLnjw8DfA
9gkrbrFzZj,OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning,"Spatio-temporal predictive learning is a learning paradigm that enables models to learn spatial and temporal patterns by predicting future frames from given past frames in an unsupervised manner. Despite remarkable progress in recent years, a lack of systematic understanding persists due to the diverse settings, complex implementation, and difficult reproducibility. Without standardization, comparisons can be unfair and insights inconclusive. To address this dilemma, we propose OpenSTL, a comprehensive benchmark for spatio-temporal predictive learning that categorizes prevalent approaches into recurrent-based and recurrent-free models. OpenSTL provides a modular and extensible framework implementing various state-of-the-art methods. We conduct standard evaluations on datasets across various domains, including synthetic moving object trajectory, human motion, driving scenes, traffic flow, and weather forecasting. Based on our observations, we provide a detailed analysis of how model architecture and dataset properties affect spatio-temporal predictive learning performance. Surprisingly, we find that recurrent-free models achieve a good balance between efficiency and performance than recurrent models. Thus, we further extend the common MetaFormers to boost recurrent-free spatial-temporal predictive learning. We open-source the code and models at https://github.com/chengtan9907/OpenSTL.",Datasets & Benchmarks,NeurIPS,2023,Poster,Cheng Tan;Siyuan Li;Zhangyang Gao;Wenfei Guan;Zedong Wang;Zicheng Liu;Lirong Wu;Stan Z. Li,False,https://openreview.net/pdf?id=9gkrbrFzZj
9lOVNw7guQ,Low-shot Object Learning with Mutual Exclusivity Bias,"This paper introduces Low-shot Object Learning with Mutual Exclusivity Bias (LSME), the first computational framing of mutual exclusivity bias, a phenomenon commonly observed in infants during word learning. We provide a novel dataset, comprehensive baselines, and a SOTA method to enable the ML community to tackle this challenging learning task. The goal of LSME is to analyze an RGB image of a scene containing multiple objects and correctly associate a previously-unknown object instance with a provided category label. This association is then used to perform low-shot learning to test category generalization. We provide a data generation pipeline for the LSME problem and conduct a thorough analysis of the factors that contribute to its difficulty. Additionally, we evaluate the performance of multiple baselines, including state-of-the-art foundation models. Finally, we present a baseline approach that outperforms state-of-the-art models in terms of low-shot accuracy. Code and data are available at https://github.com/rehg-lab/LSME.",Datasets & Benchmarks,NeurIPS,2023,Poster,Ngoc Anh Thai;Ahmad Humayun;Stefan Stojanov;Zixuan Huang;Bikram Boote;James Matthew Rehg,True,https://openreview.net/pdf?id=9lOVNw7guQ
9wrYfqdrwk,Diversify Your Vision Datasets with Automatic Diffusion-based Augmentation,"Many fine-grained classification tasks, like rare animal identification, have limited training data and consequently classifiers trained on these datasets often fail to  generalize to variations in the domain like changes in weather or location.  As such, we explore how natural language descriptions of the domains seen in training data can be used with large vision models trained on diverse pretraining datasets to generate useful variations of the training data. We introduce ALIA (Automated Language-guided Image Augmentation), a method which utilizes large vision and language models to automatically generate natural language descriptions of a dataset's domains and augment the training data via language-guided image editing. To maintain data integrity, a model trained on the original dataset filters out minimal image edits and those which corrupt class-relevant information. The resulting dataset is visually consistent with the original training data and offers significantly enhanced diversity. We show that ALIA is able to surpasses traditional data augmentation and text-to-image generated data on fine-grained classification tasks, including cases of domain generalization and contextual bias. Code is available at https://github.com/lisadunlap/ALIA.",main,NeurIPS,2023,Poster,Lisa Dunlap;Alyssa Umino;Han Zhang;Jiezhi Yang;Joseph E. Gonzalez;Trevor Darrell,True,https://openreview.net/pdf?id=9wrYfqdrwk
A6EquH0enk,Effective Bayesian Heteroscedastic Regression with Deep Neural Networks,"Flexibly quantifying both irreducible aleatoric and model-dependent epistemic uncertainties plays an important role for complex regression problems. While deep neural networks in principle can provide this flexibility and learn heteroscedastic aleatoric uncertainties through non-linear functions, recent works highlight that maximizing the log likelihood objective parameterized by mean and variance can lead to compromised mean fits since the gradient are scaled by the predictive variance, and propose adjustments in line with this premise. 
We instead propose to use the natural parametrization of the Gaussian, which has been shown to be more stable for heteroscedastic regression based on non-linear feature maps and Gaussian processes. Further, we emphasize the significance of principled regularization of the network parameters and prediction. We therefore propose an efficient Laplace approximation for heteroscedastic neural networks that allows automatic regularization through empirical Bayes and provides epistemic uncertainties, both of which improve generalization.
We showcase on a range of regression problems—including a new heteroscedastic image regression benchmark—that our methods are scalable, improve over previous approaches for heteroscedastic regression, and provide epistemic uncertainty without requiring hyperparameter tuning.",main,NeurIPS,2023,Poster,Alexander Immer;Emanuele Palumbo;Alexander Marx;Julia E Vogt,True,https://openreview.net/pdf?id=A6EquH0enk
AA2uO0HHmr,Estimating Generic 3D Room Structures from 2D Annotations,"Indoor rooms are among the most common use cases in 3D scene understanding. Current state-of-the-art methods for this task are driven by large annotated datasets. Room layouts are especially important, consisting of structural elements in 3D, such as wall, floor, and ceiling. However, they are difficult to annotate, especially on pure RGB video. We propose a novel method to produce generic 3D room layouts just from 2D segmentation masks, which are easy to annotate for humans. Based on these 2D annotations, we automatically reconstruct 3D plane equations for the structural elements and their spatial extent in the scene, and connect adjacent elements at the appropriate contact edges. We annotate and publicly release 2246 3D room layouts on the RealEstate10k dataset, containing YouTube videos. We demonstrate the high quality of these 3D layouts annotations with extensive experiments.",Datasets & Benchmarks,NeurIPS,2023,Poster,Denys Rozumnyi;Stefan Popov;Kevis-kokitsi Maninis;Matthias Nießner;Vittorio Ferrari,False,https://openreview.net/pdf?id=AA2uO0HHmr
AIeeXKsspI,YouTubePD: A Multimodal Benchmark for Parkinson’s Disease Analysis,"The healthcare and AI communities have witnessed a growing interest in the development of AI-assisted systems for automated diagnosis of Parkinson's Disease (PD), one of the most prevalent neurodegenerative disorders. However, the progress in this area has been significantly impeded by the absence of a unified, publicly available benchmark, which prevents comprehensive evaluation of existing PD analysis methods and the development of advanced models. This work overcomes these challenges by introducing YouTubePD -- the *first* publicly available multimodal benchmark designed for PD analysis. We crowd-source existing videos featured with PD from YouTube, exploit multimodal information including *in-the-wild* videos, audios, and facial landmarks across 200+ subject videos, and provide dense and diverse annotations from a clinical expert. Based on our benchmark, we propose three challenging and complementary tasks encompassing *both discriminative and generative* tasks, along with a comprehensive set of corresponding baselines. Experimental evaluation showcases the potential of modern deep learning and computer vision techniques, in particular the generalizability of the models developed on our YouTubePD to real-world clinical settings, while revealing their limitations. We hope that our work paves the way for future research in this direction.",Datasets & Benchmarks,NeurIPS,2023,Poster,Andy Zhou;Samuel Li;Pranav Sriram;Xiang Li;Jiahua Dong;Ansh Sharma;Yuanyi Zhong;Shirui Luo;Maria Jaromin;Volodymyr Kindratenko;Joerg Heintz;Christopher Zallek;Yu-Xiong Wang,True,https://openreview.net/pdf?id=AIeeXKsspI
AMIJEupsNq,"3D-Aware Visual Question Answering about Parts, Poses and Occlusions","Despite rapid progress in Visual question answering (\\\\textit{VQA}), existing datasets and models mainly focus on testing reasoning in 2D.  However, it is important that VQA models also understand the 3D structure of visual scenes, for example to support tasks like navigation or manipulation.  This includes an understanding of the 3D object pose, their parts and occlusions.   In this work, we introduce the task of 3D-aware VQA, which focuses on challenging questions that require a compositional reasoning over the 3D structure of visual scenes.   We address 3D-aware VQA from both the dataset and the model perspective.   First, we introduce Super-CLEVR-3D, a compositional reasoning dataset that contains questions about object parts, their 3D poses, and occlusions.   Second, we propose PO3D-VQA, a 3D-aware VQA model that marries two powerful ideas: probabilistic neural symbolic program execution for reasoning and deep neural networks with 3D generative representations of objects for robust visual recognition.  Our experimental results show our model PO3D-VQA outperforms existing methods significantly, but we still observe a significant performance gap compared to 2D VQA benchmarks, indicating that 3D-aware VQA remains an important open research area.",main,NeurIPS,2023,Poster,Xingrui Wang;Wufei Ma;Zhuowan Li;Adam Kortylewski;Alan Yuille,True,https://openreview.net/pdf?id=AMIJEupsNq
AOKU4nRw1W,Linguistic Binding in Diffusion Models: Enhancing Attribute Correspondence through Attention Map Alignment,"Text-conditioned image generation models often generate incorrect associations between entities and their visual attributes. This reflects an impaired mapping between linguistic binding of entities and modifiers in the prompt and visual binding of the corresponding elements in the generated image. As one example, a query like ``a pink sunflower and a yellow flamingo'' may incorrectly produce an image of a yellow sunflower and a pink flamingo. To remedy this issue, we propose SynGen, an approach which first syntactically analyses the prompt to identify entities and their modifiers, and then uses a novel loss function that encourages the cross-attention maps to agree with the linguistic binding reflected by the syntax. Specifically, we encourage large overlap between attention maps of entities and their modifiers, and small overlap with other entities and modifier words. The loss is optimized during inference, without retraining or fine-tuning the model. Human evaluation on three datasets, including one new and challenging set, demonstrate significant improvements of SynGen compared with current state of the art methods. This work highlights how making use of sentence structure during inference can efficiently and substantially improve the faithfulness of text-to-image generation.",main,NeurIPS,2023,Oral,Royi Rassin;Eran Hirsch;Daniel Glickman;Shauli Ravfogel;Yoav Goldberg;Gal Chechik,True,https://openreview.net/pdf?id=AOKU4nRw1W
AiEipk1X0c,A Deep Instance Generative Framework for MILP Solvers Under Limited Data Availability,"In the past few years, there has been an explosive surge in the use of machine learning (ML) techniques to address combinatorial optimization (CO) problems, especially mixed-integer linear programs (MILPs). Despite the achievements, the limited availability of real-world instances often leads to sub-optimal decisions and biased solver assessments, which motivates a suite of synthetic MILP instance generation techniques. However, existing methods either rely heavily on expert-designed formulations or struggle to capture the rich features of real-world instances. To tackle this problem, we propose G2MILP, *the first* deep generative framework for MILP instances. Specifically, G2MILP represents MILP instances as bipartite graphs, and applies a masked variational autoencoder to iteratively corrupt and replace parts of the original graphs to generate new ones. The appealing feature of G2MILP is that it can learn to generate novel and realistic MILP instances without prior expert-designed formulations, while preserving the structures and computational hardness of real-world datasets, simultaneously. Thus the generated instances can facilitate downstream tasks for enhancing MILP solvers under limited data availability. We design a suite of benchmarks to evaluate the quality of the generated MILP instances. Experiments demonstrate that our method can produce instances that closely resemble real-world datasets in terms of both structures and computational hardness. The deliverables are released at [https://miralab-ustc.github.io/L2O-G2MILP](https://miralab-ustc.github.io/L2O-G2MILP).",main,NeurIPS,2023,Spotlight,Zijie Geng;Xijun Li;Jie Wang;Xiao Li;Yongdong Zhang;Feng Wu,True,https://openreview.net/pdf?id=AiEipk1X0c
ApqgcSnhjh,Occ3D: A Large-Scale 3D Occupancy Prediction Benchmark for Autonomous Driving,"Robotic perception requires the modeling of both 3D geometry and semantics. Existing methods typically focus on estimating 3D bounding boxes, neglecting finer geometric details and struggling to handle general, out-of-vocabulary objects. 3D occupancy prediction, which estimates the detailed occupancy states and semantics of a scene, is an emerging task to overcome these limitations.
To support 3D occupancy prediction, we develop a label generation pipeline that produces dense, visibility-aware labels for any given scene. This pipeline comprises three stages: voxel densification, occlusion reasoning, and image-guided voxel refinement. We establish two benchmarks, derived from the Waymo Open Dataset and the nuScenes Dataset, namely Occ3D-Waymo and Occ3D-nuScenes benchmarks. 
Furthermore, we provide an extensive analysis of the proposed dataset with various baseline models. 
Lastly, we propose a new model, dubbed Coarse-to-Fine Occupancy (CTF-Occ) network, which demonstrates superior performance on the Occ3D benchmarks.The code, data, and benchmarks are released at \\\\url{https://tsinghua-mars-lab.github.io/Occ3D/}.",Datasets & Benchmarks,NeurIPS,2023,Poster,Xiaoyu Tian;Tao Jiang;Longfei Yun;Yucheng Mao;Huitong Yang;Yue Wang;Yilun Wang;Hang Zhao,True,https://openreview.net/pdf?id=ApqgcSnhjh
As4101fOG1,What a MESS: Multi-Domain Evaluation of Zero-Shot Semantic Segmentation,"While semantic segmentation has seen tremendous improvements in the past, there are still significant labeling efforts necessary and the problem of limited generalization to classes that have not been present during training. To address this problem, zero-shot semantic segmentation makes use of large self-supervised vision-language models, allowing zero-shot transfer to unseen classes. In this work, we build a benchmark for Multi-domain Evaluation of Zero-Shot Semantic Segmentation (MESS), which allows a holistic analysis of performance across a wide range of domain-specific datasets such as medicine, engineering, earth monitoring, biology, and agriculture. To do this, we reviewed 120 datasets, developed a taxonomy, and classified the datasets according to the developed taxonomy. We select a representative subset consisting of 22 datasets and propose it as the MESS benchmark. We evaluate eight recently published models on the proposed MESS benchmark and analyze characteristics for the performance of zero-shot transfer models. The toolkit is available at https://github.com/blumenstiel/MESS.",Datasets & Benchmarks,NeurIPS,2023,Poster,Benedikt Blumenstiel;Johannes Jakubik;Hilde Kuehne;Michael Vössing,False,https://openreview.net/pdf?id=As4101fOG1
AvttCE8n3H,A Massive Scale Semantic Similarity Dataset of Historical English,"A diversity of tasks use language models trained on semantic similarity data. While there are a variety of datasets that capture semantic similarity, they are either constructed from modern web data or are relatively small datasets created in the past decade by human annotators. This study utilizes a novel source, newly digitized articles from off-copyright, local U.S. newspapers, to assemble a massive-scale semantic similarity dataset spanning 70 years from 1920 to 1989 and containing nearly 400M positive semantic similarity pairs. Historically, around half of articles in U.S. local newspapers came from newswires like the Associated Press. While local papers reproduced articles from the newswire, they wrote their own headlines, which form abstractive summaries of the associated articles. We associate articles and their headlines by exploiting document layouts and language understanding. We then use deep neural methods to detect which articles are from the same underlying source, in the presence of substantial noise and abridgement. The headlines of reproduced articles form positive semantic similarity pairs. The resulting publicly available HEADLINES dataset is significantly larger than most existing semantic similarity datasets and covers a much longer span of time. It will facilitate the application of contrastively trained semantic similarity models to a variety of tasks, including the study of semantic change across space and time.",Datasets & Benchmarks,NeurIPS,2023,Poster,Emily Silcock;Abhishek Arora;Melissa Dell,True,https://openreview.net/pdf?id=AvttCE8n3H
AwhpBEqmyo,StoryBench: A Multifaceted Benchmark for Continuous Story Visualization,"Generating video stories from text prompts is a complex task. In addition to having high visual quality, videos need to realistically adhere to a sequence of text prompts whilst being consistent throughout the frames. Creating a benchmark for video generation requires data annotated over time, which contrasts with the single caption used often in video datasets. To fill this gap, we collect comprehensive human annotations on three existing datasets, and introduce StoryBench: a new, challenging multi-task benchmark to reliably evaluate forthcoming text-to-video models. Our benchmark includes three video generation tasks of increasing difficulty: action execution, where the next action must be generated starting from a conditioning video; story continuation, where a sequence of actions must be executed starting from a conditioning video; and story generation, where a video must be generated from only text prompts. We evaluate small yet strong text-to-video baselines, and show the benefits of training on story-like data algorithmically generated from existing video captions. Finally, we establish guidelines for human evaluation of video stories, and reaffirm the need of better automatic metrics for video generation. StoryBench aims at encouraging future research efforts in this exciting new area.",Datasets & Benchmarks,NeurIPS,2023,Poster,Emanuele Bugliarello;Hernan Moraldo;Ruben Villegas;Mohammad Babaeizadeh;Mohammad Taghi Saffar;Han Zhang;Dumitru Erhan;Vittorio Ferrari;Pieter-Jan Kindermans;Paul Voigtlaender,True,https://openreview.net/pdf?id=AwhpBEqmyo
BNwsJ4bFsc,VisoGender: A dataset for benchmarking gender bias in image-text pronoun resolution,"We introduce VisoGender, a novel dataset for benchmarking gender bias in vision-language models. We focus on occupation-related biases within a hegemonic system of binary gender, inspired by Winograd and Winogender schemas, where each image is associated with a caption containing a pronoun relationship of subjects and objects in the scene. VisoGender is balanced by gender representation in professional roles, supporting bias evaluation in two ways: i) resolution bias, where we evaluate the difference between pronoun resolution accuracies for image subjects with gender presentations perceived as masculine versus feminine by human annotators and ii) retrieval bias, where we compare ratios of professionals perceived to have masculine and feminine gender presentations retrieved for a gender-neutral search query. We benchmark several state-of-the-art vision-language models and find that they demonstrate bias in resolving binary gender in complex scenes. While the direction and magnitude of gender bias depends on the task and the model being evaluated, captioning models are generally less biased than Vision-Language Encoders.",Datasets & Benchmarks,NeurIPS,2023,Poster,Siobhan Mackenzie Hall;Fernanda Gonçalves Abrantes;Hanwen Zhu;Grace Sodunke;Aleksandar Shtedritski;Hannah Rose Kirk,True,https://openreview.net/pdf?id=BNwsJ4bFsc
BOP5McdqGy,Uncovering and Quantifying Social Biases in Code Generation,"With the popularity of automatic code generation tools, such as Copilot, the study of the potential hazards of these tools is gaining importance. In this work, we explore the social bias problem in pre-trained code generation models. We propose a new paradigm to construct code prompts and successfully uncover social biases in code generation models. To quantify the severity of social biases in generated code, we develop a dataset along with three metrics to evaluate the overall social bias and fine-grained unfairness across different demographics. Experimental results on three pre-trained code generation models (Codex, InCoder, and CodeGen) with varying sizes, reveal severe social biases. Moreover, we conduct analysis to provide useful insights for further choice of code generation models with low social bias.",main,NeurIPS,2023,Poster,Yan Liu;Xiaokang Chen;Yan Gao;Zhe Su;Fengji Zhang;Daoguang Zan;Jian-Guang Lou;Pin-Yu Chen;Tsung-Yi Ho,True,https://openreview.net/pdf?id=BOP5McdqGy
BR1m3JIoKm,BoardgameQA: A Dataset for Natural Language Reasoning with Contradictory Information,"Automated reasoning with unstructured natural text is a key requirement for many potential applications of NLP and for developing robust AI systems. Recently, Language Models (LMs) have demonstrated complex reasoning capacities even without any finetuning. However, existing evaluation for automated reasoning assumes access to a consistent and coherent set of information over which models reason. When reasoning in the real-world, the available information is frequently inconsistent or contradictory, and therefore models need to be equipped with a strategy to resolve such conflicts when they arise. One widely-applicable way of resolving conflicts is to impose preferences over information sources (e.g., based on source credibility or information recency) and adopt the source with higher preference. In this paper, we formulate the problem of reasoning with contradictory information guided by preferences over sources as the classical problem of defeasible reasoning, and develop a dataset called BoardgameQA for measuring the reasoning capacity of LMs in this setting. BoardgameQA also incorporates reasoning with implicit background knowledge, to better reflect reasoning problems in downstream applications. We benchmark various LMs on BoardgameQA and the results reveal a significant gap in the reasoning capacity of state-of-the-art LMs on this problem, showing that reasoning with conflicting information does not surface out-of-the-box in LMs. While performance can be improved with finetuning, it nevertheless remains poor.",Datasets & Benchmarks,NeurIPS,2023,Poster,Mehran Kazemi;Quan Yuan;Deepti Bhatia;Najoung Kim;Xin Xu;Vaiva Imbrasaite;Deepak Ramachandran,True,https://openreview.net/pdf?id=BR1m3JIoKm
BVN9Kgvwzv,From Cloze to Comprehension: Retrofitting Pre-trained Masked Language Models to Pre-trained Machine Reader,"We present Pre-trained Machine Reader (PMR), a novel method for retrofitting pre-trained masked language models (MLMs) to pre-trained machine reading comprehension (MRC) models without acquiring labeled data.
PMR can resolve the discrepancy between model pre-training and downstream fine-tuning of existing MLMs.
To build the proposed PMR, we constructed a large volume of general-purpose and high-quality MRC-style training data by using Wikipedia hyperlinks and designed a Wiki Anchor Extraction task to guide the MRC-style pre-training.
Apart from its simplicity, PMR effectively solves extraction tasks, such as Extractive Question Answering and Named Entity Recognition. PMR shows tremendous improvements over existing approaches, especially in low-resource scenarios.
When applied to the sequence classification task in the MRC formulation, PMR enables the extraction of high-quality rationales to explain the classification process, thereby providing greater prediction explainability. PMR also has the potential to serve as a unified model for tackling various extraction and classification tasks in the MRC formulation.",main,NeurIPS,2023,Poster,Weiwen Xu;Xin Li;Wenxuan Zhang;Meng Zhou;Wai Lam;Luo Si;Lidong Bing,True,https://openreview.net/pdf?id=BVN9Kgvwzv
BbIxB4xnbq,LANCE: Stress-testing Visual Models by Generating Language-guided Counterfactual Images,"We propose an automated algorithm to stress-test a trained visual model by generating language-guided counterfactual test images (LANCE). Our method leverages recent progress in large language modeling and text-based image editing to augment an IID test set with a suite of diverse, realistic, and challenging test images without altering model weights. We benchmark the performance of a diverse set of pre-trained models on our generated data and observe significant and consistent performance drops. We further analyze model sensitivity across different types of edits, and demonstrate its applicability at surfacing previously unknown class-level model biases in ImageNet. Code is available at https://github.com/virajprabhu/lance.",main,NeurIPS,2023,Poster,Viraj Uday Prabhu;Sriram Yenamandra;Prithvijit Chattopadhyay;Judy Hoffman,True,https://openreview.net/pdf?id=BbIxB4xnbq
BgY17iEnTb,AVIDa-hIL6: A Large-Scale VHH Dataset Produced from an Immunized Alpaca for Predicting Antigen-Antibody Interactions,"Antibodies have become an important class of therapeutic agents to treat human diseases.
To accelerate therapeutic antibody discovery, computational methods, especially machine learning, have attracted considerable interest for predicting specific interactions between antibody candidates and target antigens such as viruses and bacteria.
However, the publicly available datasets in existing works have notable limitations, such as small sizes and the lack of non-binding samples and exact amino acid sequences.
To overcome these limitations, we have developed AVIDa-hIL6, a large-scale dataset for predicting antigen-antibody interactions in the variable domain of heavy chain of heavy chain antibodies (VHHs), produced from an alpaca immunized with the human interleukin-6 (IL-6) protein, as antigens.
By leveraging the simple structure of VHHs, which facilitates identification of full-length amino acid sequences by DNA sequencing technology, AVIDa-hIL6 contains 573,891 antigen-VHH pairs with amino acid sequences.
All the antigen-VHH pairs have reliable labels for binding or non-binding, as generated by a novel labeling method.
Furthermore, via introduction of artificial mutations, AVIDa-hIL6 contains 30 different mutants in addition to wild-type IL-6 protein.
This characteristic provides opportunities to develop machine learning models for predicting changes in antibody binding by antigen mutations.
We report experimental benchmark results on AVIDa-hIL6 by using machine learning models.
The results indicate that the existing models have potential, but further research is needed to generalize them to predict effective antibodies against unknown mutants.
The dataset is available at https://avida-hil6.cognanous.com.",Datasets & Benchmarks,NeurIPS,2023,Poster,Hirofumi Tsuruta;Hiroyuki Yamazaki;Ryota Maeda;Ryotaro Tamura;Jennifer N. Wei;Zelda E Mariet;Poomarin Phloyphisut;Hidetoshi Shimokawa;Joseph R. Ledsam;Lucy J Colwell;Akihiro Imura,True,https://openreview.net/pdf?id=BgY17iEnTb
BkQM8huiIc,A normative theory of social conflict,"Social conflict is a survival mechanism yielding both normal and pathological behaviors. To understand its underlying principles, we collected behavioral and whole-brain neural data from mice advancing through stages of social conflict. We modeled the animals’ interactions as a normal-form game using Bayesian inference to account for the partial observability of animals’ strengths. We find that our behavioral and neural data are consistent with the first-level Theory of Mind (1-ToM) model where mice form “primary” beliefs about the strengths of all mice involved and “secondary” beliefs that estimate the beliefs of their opponents. Our model identifies the brain regions that carry the information about these beliefs and offers a framework for studies of social behaviors in partially observable settings.",main,NeurIPS,2023,Poster,Sergey A. Shuvaev;Evgeny M Amelchenko;Dmitry Smagin;Natalia Kudryavtseva;Grigori Enikolopov;Alexei A. Koulakov,True,https://openreview.net/pdf?id=BkQM8huiIc
C0zw2ERKiQ,Revisiting the Evaluation of Image Synthesis with GANs,"A good metric, which promises a reliable comparison between solutions, is essential for any well-defined task. Unlike most vision tasks that have per-sample ground-truth, image synthesis tasks target generating unseen data and hence are usually evaluated through a distributional distance between one set of real samples and another set of generated samples. This study presents an empirical investigation into the evaluation of synthesis performance, with generative adversarial networks (GANs) as a representative of generative models. In particular, we make in-depth analyses of various factors, including how to represent a data point in the representation space, how to calculate a fair distance using selected samples, and how many instances to use from each set. Extensive experiments conducted on multiple datasets and settings reveal several important findings. Firstly, a group of models that include both CNN-based and ViT-based architectures serve as reliable and robust feature extractors for measurement evaluation. Secondly, Centered Kernel Alignment (CKA) provides a better comparison across various extractors and hierarchical layers in one model. Finally, CKA is more sample-efficient and enjoys better agreement with human judgment in characterizing the similarity between two internal data correlations. These findings contribute to the development of a new measurement system, which enables a consistent and reliable re-evaluation of current state-of-the-art generative models.",Datasets & Benchmarks,NeurIPS,2023,Poster,Mengping Yang;Ceyuan Yang;Yichi Zhang;Qingyan Bai;Yujun Shen;Bo Dai,False,https://openreview.net/pdf?id=C0zw2ERKiQ
CG0L2PFrb1,D4: Improving LLM Pretraining via Document De-Duplication and Diversification,"Over recent years, an increasing amount of compute and data has been poured into training large language models (LLMs), usually by doing one-pass learning on as many tokens as possible randomly selected from large-scale web corpora. While training on ever-larger portions of the internet leads to consistent performance improvements, the size of these improvements diminishes with scale, and there has been little work exploring the effect of data selection on pre-training and downstream performance beyond simple de-duplication methods such as MinHash. Here, we show that careful data selection (on top of de-duplicated data) via pre-trained model embeddings can speed up training (20% efficiency gains) and improves average downstream accuracy on 16 NLP tasks (up to 2%) at the 6.7B model scale. Furthermore, we show that repeating data intelligently consistently outperforms baseline training (while repeating random data performs worse than baseline training). Our results indicate that clever data selection can significantly improve LLM pre-training, calls into question the common practice of training for a single epoch on as much data as possible, and demonstrates a path to keep improving our models past the limits of randomly sampling web data.",Datasets & Benchmarks,NeurIPS,2023,Poster,,False,https://openreview.net/pdf?id=CG0L2PFrb1
CSJYz1Zovj,MedSat: A Public Health Dataset for England Featuring Medical Prescriptions and Satellite Imagery,"As extreme weather events become more frequent, understanding their impact on human health becomes increasingly crucial. However, the utilization of Earth Observation to effectively analyze the environmental context in relation to health remains limited. This limitation is primarily due to the lack of fine-grained spatial and temporal data in public and population health studies, hindering a comprehensive understanding of health outcomes. Additionally, obtaining appropriate environmental indices across different geographical levels and timeframes poses a challenge. For the years 2019 (pre-COVID) and 2020 (COVID), we collected spatio-temporal indicators for all Lower Layer Super Output Areas in England. These indicators included: i) 111 sociodemographic features linked to health in existing literature, ii) 43 environmental point features (e.g., greenery and air pollution levels), iii) 4 seasonal composite satellite images each with 11 bands, and iv) prescription prevalence associated with five medical conditions (depression, anxiety, diabetes, hypertension, and asthma), opioids and total prescriptions. We combined these indicators into a single MedSat dataset, the availability of which presents an opportunity for the machine learning community to develop new techniques specific to public health. These techniques would address challenges such as handling large and complex data volumes, performing effective feature engineering on environmental and sociodemographic factors, capturing spatial and temporal dependencies in the models, addressing imbalanced data distributions, developing novel computer vision methods for health modeling based on satellite imagery, ensuring model explainability, and achieving generalization beyond the specific geographical region.",Datasets & Benchmarks,NeurIPS,2023,Poster,Sanja Scepanovic;Ivica Obadic;Sagar Joglekar;Laura GIUSTARINI;Cristiano Nattero;Daniele Quercia;Xiao Xiang Zhu,True,https://openreview.net/pdf?id=CSJYz1Zovj
CSbGXyCswu,Fine-Grained Human Feedback Gives Better Rewards for Language Model Training,"Language models (LMs) often exhibit undesirable text generation behaviors, including generating false, toxic, or irrelevant outputs. 
Reinforcement learning from human feedback (RLHF)---where human preference judgments on LM outputs are transformed into a learning signal---has recently shown promise in addressing these issues. However, such holistic feedback conveys limited information on long text outputs; it does not indicate which aspects of the outputs influenced user preference; e.g., which parts contain what type(s) of errors. In this paper, we use fine-grained human feedback (e.g., which sentence is false, which sub-sentence is irrelevant) as an explicit training signal. We introduce Fine-Grained RLHF, a framework that enables training and learning from reward functions that are fine-grained in two respects: (1) density, providing a reward after every segment (e.g., a sentence) is generated; and (2) incorporating multiple reward models associated with different feedback types (e.g., factual incorrectness, irrelevance, and information incompleteness). We conduct experiments on detoxification and long-form question answering to illustrate how learning with this reward function leads to improved performance, supported by both automatic and human evaluation. Additionally, we show that LM behaviors can be customized using different combinations of fine-grained reward models. We release all data, collected human feedback, and codes at https://FineGrainedRLHF.github.io.",main,NeurIPS,2023,Spotlight,Zeqiu Wu;Yushi Hu;Weijia Shi;Nouha Dziri;Alane Suhr;Prithviraj Ammanabrolu;Noah A. Smith;Mari Ostendorf;Hannaneh Hajishirzi,True,https://openreview.net/pdf?id=CSbGXyCswu
Cf2c9Pk9yF,ImageNet-Hard: The Hardest Images Remaining from a Study of the Power of Zoom and Spatial Biases in Image Classification,"Image classifiers are information-discarding machines, by design. Yet, how these models discard information remains mysterious. We hypothesize that one way for image classifiers to reach high accuracy is to first zoom to the most discriminative region in the image and then extract features from there to predict image labels, discarding the rest of the image. Studying six popular networks ranging from AlexNet to CLIP, we find that proper framing of the input image can lead to the correct classification of 98.91% of ImageNet images. Furthermore, we  uncover positional biases in various datasets, especially a strong center bias in two popular datasets: ImageNet-A and ObjectNet. Finally, leveraging our insights into the potential of zooming, we propose a test-time augmentation (TTA) technique that improves classification accuracy by forcing models to explicitly perform zoom-in operations before making predictions.
Our method is more interpretable, accurate, and faster than MEMO, a state-of-the-art (SOTA) TTA method. We introduce ImageNet-Hard, a new benchmark that challenges SOTA classifiers including large vision-language models even when optimal zooming is allowed.",Datasets & Benchmarks,NeurIPS,2023,Poster,Mohammad Reza Taesiri;Giang Nguyen;Sarra Habchi;Cor-Paul Bezemer;Anh Totti Nguyen,True,https://openreview.net/pdf?id=Cf2c9Pk9yF
CiRHWaRbp0,Benchmarking Robustness to Adversarial Image Obfuscations,"Automated content filtering and moderation is an important tool that allows online platforms to build striving user communities that facilitate cooperation and prevent abuse. Unfortunately, resourceful actors try to bypass automated filters in a bid to post content that violate platform policies and codes of conduct. To reach this goal, these malicious actors may obfuscate policy violating images (e.g., overlay harmful images by carefully selected benign images or visual patterns) to prevent machine learning models from reaching the correct decision. In this paper, we invite researchers to tackle this specific issue and present a new image benchmark. This benchmark, based on ImageNet, simulates the type of obfuscations created by malicious actors. It goes beyond Image-Net-C and ImageNet-C-bar by proposing general, drastic, adversarial modifications that preserve the original content intent. It aims to tackle a more common adversarial threat than the one considered by lp-norm bounded adversaries. We evaluate 33 pretrained models on the benchmark and train models with different augmentations, architectures and training methods on subsets of the obfuscations to measure generalization. Our hope is that this benchmark will encourage researchers to test their models and methods and try to find new approaches that are more robust to these obfuscations.",Datasets & Benchmarks,NeurIPS,2023,Poster,Florian Stimberg;Ayan Chakrabarti;Chun-Ta Lu;Hussein Hazimeh;Otilia Stretcu;Wei Qiao;Yintao Liu;Merve Kaya;Cyrus Rashtchian;Ariel Fuxman;Mehmet Nejat Tek;Sven Gowal,True,https://openreview.net/pdf?id=CiRHWaRbp0
CjVdXey4zT,When Do Neural Nets Outperform Boosted Trees on Tabular Data?,"Tabular data is one of the most commonly used types of data in machine learning. Despite recent advances in neural nets (NNs) for tabular data, there is still an active discussion on whether or not NNs generally outperform gradient-boosted decision trees (GBDTs) on tabular data, with several recent works arguing either that GBDTs consistently outperform NNs on tabular data, or vice versa. In this work, we take a step back and question the importance of this debate. To this end, we conduct the largest tabular data analysis to date, comparing 19 algorithms across 176 datasets, and we find that the 'NN vs. GBDT' debate is overemphasized: for a surprisingly high number of datasets, either the performance difference between GBDTs and NNs is negligible, or light hyperparameter tuning on a GBDT is more important than choosing between NNs and GBDTs. Next, we analyze dozens of metafeatures to determine what \\\\emph{properties} of a dataset make NNs or GBDTs better-suited to perform well. For example, we find that GBDTs are much better than NNs at handling skewed or heavy-tailed feature distributions and other forms of dataset irregularities. Our insights act as a guide for practitioners to determine which techniques may work best on their dataset. Finally, with the goal of accelerating tabular data research, we release the TabZilla Benchmark Suite: a collection of the 36 'hardest' of the datasets we study. Our benchmark suite, codebase, and all raw results are available at https://github.com/naszilla/tabzilla.",Datasets & Benchmarks,NeurIPS,2023,Poster,Duncan C. McElfresh;Sujay Khandagale;Jonathan Valverde;Vishak Prasad C;Ganesh Ramakrishnan;Micah Goldblum;Colin White,True,https://openreview.net/pdf?id=CjVdXey4zT
CpFFRtxcbz,CARE-MI: Chinese Benchmark for Misinformation Evaluation in Maternity and Infant Care,"The recent advances in natural language processing (NLP), have led to a new trend of applying large language models (LLMs) to real-world scenarios. While the latest LLMs are astonishingly fluent when interacting with humans, they suffer from the misinformation problem by unintentionally generating factually false statements. This can lead to harmful consequences, especially when produced within sensitive contexts, such as healthcare. Yet few previous works have focused on evaluating misinformation in the long-form (LF) generation of LLMs, especially for knowledge-intensive topics. Moreover, although LLMs have been shown to perform well in different languages, misinformation evaluation has been mostly conducted in English. To this end, we present a benchmark, CARE-MI, for evaluating LLM misinformation in: 1) a sensitive topic, specifically the maternity and infant care domain; and 2) a language other than English, namely Chinese. Most importantly, we provide an innovative paradigm for building LF generation evaluation benchmarks that can be transferred to other knowledge-intensive domains and low-resourced languages. Our proposed benchmark fills the gap between the extensive usage of LLMs and the lack of datasets for assessing the misinformation generated by these models. It contains 1,612 expert-checked questions, accompanied with human-selected references. Using our benchmark, we conduct extensive experiments and found that current Chinese LLMs are far from perfect in the topic of maternity and infant care. In an effort to minimize the reliance on human resources for performance evaluation, we offer off-the-shelf judgment models for automatically assessing the LF output of LLMs given benchmark questions. Moreover, we compare potential solutions for LF generation evaluation and provide insights for building better automated metrics.",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=CpFFRtxcbz
CsXC6IcdwI,EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models,"While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains de-identified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data  (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR. We provide an end-to-end pipeline for the community to validate and build upon its performance. Third, we define 15 few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as sample efficiency and task adaptation. Our model, dataset, and code are available here: https://ehrshot.stanford.edu/",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Michael Wornow;Rahul Thapa;Ethan Steinberg;Jason Alan Fries;Nigam Shah,True,https://openreview.net/pdf?id=CsXC6IcdwI
D1MOK2t2t2,BEDD: The MineRL BASALT Evaluation and Demonstrations Dataset for Training and Benchmarking Agents that Solve Fuzzy Tasks,"The MineRL BASALT competition has served to catalyze advances in learning from human feedback through four hard-to-specify tasks in Minecraft, such as create and photograph a waterfall. Given the completion of two years of BASALT competitions, we offer to the community a formalized benchmark through the BASALT Evaluation and Demonstrations Dataset (BEDD), which serves as a resource for algorithm development and performance assessment. BEDD consists of a collection of 26 million image-action pairs from nearly 14,000 videos of human players completing the BASALT tasks in Minecraft. It also includes over 3,000 dense pairwise human evaluations of human and algorithmic agents. These comparisons serve as a fixed, preliminary leaderboard for evaluating newly-developed algorithms.  To enable this comparison, we present a streamlined codebase for benchmarking new algorithms against the leaderboard. In addition to presenting these datasets, we conduct a detailed analysis of the data from both datasets to guide algorithm development and evaluation. The released code and data are available at https://github.com/minerllabs/basalt-benchmark.",Datasets & Benchmarks,NeurIPS,2023,Oral,Stephanie Milani;Anssi Kanervisto;Karolis Ramanauskas;Sander V Schulhoff;Brandon Houghton;Rohin Shah,True,https://openreview.net/pdf?id=D1MOK2t2t2
DDkl9vaJyE,Brant: Foundation Model for Intracranial Neural Signal,"We propose a foundation model named Brant for modeling intracranial recordings, which learns powerful representations of intracranial neural signals by pre-training, providing a large-scale, off-the-shelf model for medicine. Brant is the largest model in the field of brain signals and is pre-trained on a large corpus of intracranial data collected by us. The design of Brant is to capture long-term temporal dependency and spatial correlation from neural signals, combining the information in both time and frequency domains. As a foundation model, Brant achieves SOTA performance on various downstream tasks (i.e. neural signal forecasting, frequency-phase forecasting, imputation and seizure detection), showing the generalization ability to a broad range of tasks. The low-resource label analysis and representation visualization further illustrate the effectiveness of our pre-training strategy. In addition, we explore the effect of model size to show that a larger model with a higher capacity can lead to performance improvements on our dataset. The source code and pre-trained weights are available at: https://zju-brainnet.github.io/Brant.github.io/.",main,NeurIPS,2023,Poster,Daoze Zhang;Zhizhang Yuan;Yang Yang;Junru Chen;Jingjing Wang;Yafeng Li,True,https://openreview.net/pdf?id=DDkl9vaJyE
DEiNSfh1k7,DreamSim: Learning New Dimensions of Human Visual Similarity using Synthetic Data,"Current perceptual similarity metrics operate at the level of pixels and patches. These metrics compare images in terms of their low-level colors and textures, but fail to capture mid-level similarities and differences in image layout, object pose, and semantic content. In this paper, we develop a perceptual metric that assesses images holistically. Our first step is to collect a new dataset of human similarity judgments over image pairs that are alike in diverse ways. Critical to this dataset is that judgments are nearly automatic and shared by all observers. To achieve this we use recent text-to-image models to create synthetic pairs that are perturbed along various dimensions. We observe that popular perceptual metrics fall short of explaining our new data, and we introduce a new metric, DreamSim, tuned to better align with human perception. We analyze how our metric is affected by different visual attributes, and find that it focuses heavily on foreground objects and semantic content while also being sensitive to color and layout. Notably, despite being trained on synthetic data, our metric generalizes to real images, giving strong results on retrieval and reconstruction tasks. Furthermore, our metric outperforms both prior learned metrics and recent large vision models on these tasks. Our project page: https://dreamsim-nights.github.io/",main,NeurIPS,2023,Spotlight,Stephanie Fu;Netanel Yakir Tamir;Shobhita Sundaram;Lucy Chai;Richard Zhang;Tali Dekel;Phillip Isola,True,https://openreview.net/pdf?id=DEiNSfh1k7
DILUIcDmU9,HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding,"Understanding comprehensive assembly knowledge from videos is critical for futuristic ultra-intelligent industry. To enable technological breakthrough, we present HA-ViD – the first human assembly video dataset that features representative industrial assembly scenarios, natural procedural knowledge acquisition process, and consistent human-robot shared annotations. Specifically, HA-ViD captures diverse collaboration patterns of real-world assembly, natural human behaviors and learning progression during assembly, and granulate action annotations to subject, action verb, manipulated object, target object, and tool. We provide 3222 multi-view and multi-modality videos), 1.5M frames, 96K temporal labels and 2M spatial labels. We benchmark four foundational video understanding tasks: action recognition, action segmentation, object detection and multi-object tracking. Importantly, we analyze their performance and the further reasoning steps for comprehending knowledge in assembly progress, process efficiency, task collaboration, skill parameters and human intention. Details of HA-ViD is available at: https://iai-hrc.github.io/ha-vid.",Datasets & Benchmarks,NeurIPS,2023,Poster,Hao Zheng;Regina Lee;Yuqian Lu,True,https://openreview.net/pdf?id=DILUIcDmU9
DIeZu6nqvo,EgoTracks: A Long-term Egocentric Visual Object Tracking Dataset,"Visual object tracking is a key component to many egocentric vision problems. However, the full spectrum of challenges of egocentric tracking faced by an embodied AI is underrepresented in many existing datasets; these tend to focus on relatively short, third-person videos. Egocentric video has several distinguishing characteristics from those commonly found in past datasets: frequent large camera motions and hand interactions with objects commonly lead to occlusions or objects exiting the frame, and object appearance can change rapidly due to widely different points of view, scale, or object states. Embodied tracking is also naturally long-term, and being able to consistently (re-)associate objects to their appearances and disappearances over as long as a lifetime is critical. Previous datasets under-emphasize this re-detection problem, and their ""framed"" nature has led to adoption of various spatiotemporal priors that we find do not necessarily generalize to egocentric video. We thus introduce EgoTracks, a new dataset for long-term egocentric visual object tracking. Sourced from the Ego4D dataset, this new dataset presents a significant challenge to recent state-of-the-art single-object tracking models, which we find score poorly on traditional tracking metrics for our new dataset, compared to popular benchmarks. We further show improvements that can be made to a STARK tracker to significantly increase its performance on egocentric data, resulting in a baseline model we call EgoSTARK. We publicly release our annotations and benchmark, hoping our dataset leads to further advancements in tracking.",Datasets & Benchmarks,NeurIPS,2023,Poster,Hao Tang;Kevin J Liang;Kristen Grauman;Matt Feiszli;Weiyao Wang,True,https://openreview.net/pdf?id=DIeZu6nqvo
DSYuRMJnaY,Neural MMO 2.0: A Massively Multi-task Addition to Massively Multi-agent Learning,"Neural MMO 2.0 is a massively multi-agent and multi-task environment for reinforcement learning research. This version features a novel task-system that broadens the range of training settings and poses a new challenge in generalization: evaluation on and against tasks, maps, and opponents never seen during training. Maps are procedurally generated with 128 agents in the standard setting and 1-1024 supported overall. Version 2.0 is a complete rewrite of its predecessor with three-fold improved performance, effectively addressing simulation bottlenecks in online training. Enhancements to compatibility enable training with standard reinforcement learning frameworks designed for much simpler environments. Neural MMO 2.0 is free and open-source with comprehensive documentation available at neuralmmo.github.io and an active community Discord. To spark initial research on this new platform, we are concurrently running a competition at NeurIPS 2023.",Datasets & Benchmarks,NeurIPS,2023,Poster,Joseph Suarez;David Bloomin;Kyoung Whan Choe;Hao Xiang Li;Ryan Sullivan;Nishaanth Kanna Ravichandran;Daniel Scott;Rose S Shuman;Herbie Bradley;Louis Castricato;Phillip Isola;Kirsty You;Yuhao Jiang;Qimai Li;Jiaxin Chen;Xiaolong Zhu,False,https://openreview.net/pdf?id=DSYuRMJnaY
EBYZSRRzSE,Video Timeline Modeling For News Story Understanding,"In this paper, we present a novel problem, namely video timeline modeling. Our objective is to create a video-associated timeline from a set of videos related to a specific topic, thereby facilitating the content and structure understanding of the story being told. This problem has significant potential in various real-world applications, for instance, news story summarization. To bootstrap research in this area, we curate a realistic benchmark dataset, YouTube-News-Timeline, consisting of over $12$k timelines and $300$k YouTube news videos. Additionally, we propose a set of quantitative metrics to comprehensively evaluate and compare methodologies. With such a testbed, we further develop and benchmark several deep learning approaches to tackling this problem. We anticipate that this exploratory work will pave the way for further research in video timeline modeling. The assets are available via https://github.com/google-research/google-research/tree/master/video_timeline_modeling.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Meng Liu;Mingda Zhang;Jialu Liu;Hanjun Dai;Ming-Hsuan Yang;Shuiwang Ji;Zheyun Feng;Boqing Gong,True,https://openreview.net/pdf?id=EBYZSRRzSE
EFl8zjjXeX,OV-PARTS: Towards Open-Vocabulary Part Segmentation,"Segmenting and recognizing diverse object parts is a crucial ability in applications spanning various computer vision and robotic tasks. While significant progress has been made in object-level Open-Vocabulary Semantic Segmentation (OVSS), i.e., segmenting objects with arbitrary text, the corresponding part-level research poses additional challenges. Firstly, part segmentation inherently involves intricate boundaries, while limited annotated data compounds the challenge. Secondly, part segmentation introduces an open granularity challenge due to the diverse and often ambiguous definitions of parts in the open world. Furthermore, the large-scale vision and language models, which play a key role in the open vocabulary setting, struggle to recognize parts as effectively as objects. To comprehensively investigate and tackle these challenges, we propose an Open-Vocabulary Part Segmentation (OV-PARTS) benchmark. OV-PARTS includes refined versions of two publicly available datasets: Pascal-Part-116 and ADE20K-Part-234. And it covers three specific tasks: Generalized Zero-Shot Part Segmentation, Cross-Dataset Part Segmentation, and Few-Shot Part Segmentation, providing insights into analogical reasoning, open granularity and few-shot adapting abilities of models. Moreover, we analyze and adapt two prevailing paradigms of existing object-level OVSS methods for OV-PARTS. Extensive experimental analysis is conducted to inspire future research in leveraging foundational models for OV-PARTS. The code and dataset are available at https://github.com/kellyiss/OV_PARTS.",Datasets & Benchmarks,NeurIPS,2023,Poster,Meng Wei;Xiaoyu Yue;Wenwei Zhang;Shu Kong;Xihui Liu;Jiangmiao Pang,True,https://openreview.net/pdf?id=EFl8zjjXeX
EIydMrHBHP,PTADisc: A Cross-Course Dataset Supporting Personalized Learning in Cold-Start Scenarios,"The focus of our work is on diagnostic tasks in personalized learning, such as cognitive diagnosis and knowledge tracing. The goal of these tasks is to assess students' latent proficiency on knowledge concepts through analyzing their historical learning records. However, existing research has been limited to single-course scenarios; cross-course studies have not been explored due to a lack of dataset. We address this issue by constructing PTADisc, a Diverse, Immense, Student-centered dataset that emphasizes its sufficient Cross-course information for personalized learning. PTADisc includes 74 courses, 1,530,100 students, 4,054 concepts, 225,615 problems, and over 680 million student response logs. Based on PTADisc, we developed a model-agnostic Cross-Course Learner Modeling Framework (CCLMF) which utilizes relationships between students' proficiency across courses to alleviate the difficulty of diagnosing student knowledge state in cold-start scenarios. CCLMF uses a meta network to generate personalized mapping functions between courses. The experimental results on PTADisc verify the effectiveness of CCLMF with an average improvement of 4.2% on AUC. We also report the performance of baseline models for cognitive diagnosis and knowledge tracing over PTADisc, demonstrating that our dataset supports a wide scope of research in personalized learning. Additionally, PTADisc contains valuable programming logs and student-group information that are worth exploring in the future.",Datasets & Benchmarks,NeurIPS,2023,Poster,Liya Hu;Zhiang Dong;Jingyuan Chen;Guifeng Wang;Zhihua Wang;Zhou Zhao;Fei Wu,True,https://openreview.net/pdf?id=EIydMrHBHP
EO1KuHoR0V,AUDIT: Audio Editing by Following Instructions with Latent Diffusion Models,"Audio editing is applicable for various purposes, such as adding background sound effects, replacing a musical instrument, and repairing damaged audio. Recently, some diffusion-based methods achieved zero-shot audio editing by using a diffusion and denoising process conditioned on the text description of the output audio. However, these methods still have some problems: 1) they have not been trained on editing tasks and cannot ensure good editing effects; 2) they can erroneously modify audio segments that do not require editing; 3) they need a complete description of the output audio, which is not always available or necessary in practical scenarios. In this work, we propose AUDIT, an instruction-guided audio editing model based on latent diffusion models. Specifically, \\\\textbf{AUDIT} has three main design features: 1) we construct triplet training data (instruction, input audio, output audio) for different audio editing tasks and train a diffusion model using instruction and input (to be edited) audio as conditions and generating output (edited) audio; 2) it can automatically learn to only modify segments that need to be edited by comparing the difference between the input and output audio; 3) it only needs edit instructions instead of full target audio descriptions as text input. AUDIT achieves state-of-the-art results in both objective and subjective metrics for several audio editing tasks (e.g., adding, dropping, replacement, inpainting, super-resolution). Demo samples are available at https://audit-demopage.github.io/.",main,NeurIPS,2023,Poster,Yuancheng Wang;Zeqian Ju;Xu Tan;Lei He;Zhizheng Wu;Jiang Bian;sheng zhao,True,https://openreview.net/pdf?id=EO1KuHoR0V
EPz1DcdPVE,Towards Federated Foundation Models: Scalable Dataset Pipelines for Group-Structured Learning,"We introduce Dataset Grouper, a library to create large-scale group-structured (e.g., federated) datasets, enabling federated learning simulation at the scale of foundation models. This library facilitates the creation of group-structured versions of existing datasets based on user-specified partitions, and directly leads to a variety of useful heterogeneous datasets that can be plugged into existing software frameworks. Dataset Grouper offers three key advantages. First, it scales to settings where even a single group's dataset is too large to fit in memory. Second, it provides flexibility, both in choosing the base (non-partitioned) dataset and in defining partitions. Finally, it is framework-agnostic. We empirically demonstrate that Dataset Grouper enables large-scale federated language modeling simulations on datasets that are orders of magnitude larger than in previous work, allowing for federated training of language models with hundreds of millions, and even billions, of parameters. Our experimental results show that algorithms like FedAvg operate more as meta-learning methods than as empirical risk minimization methods at this scale, suggesting their utility in downstream personalization and task-specific adaptation. Dataset Grouper is available at https://github.com/google-research/dataset_grouper.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zachary Charles;Nicole Elyse Mitchell;Krishna Pillutla;Michael Reneer;Zachary Garrett,False,https://openreview.net/pdf?id=EPz1DcdPVE
ESEM1lNoeS,AttrSeg: Open-Vocabulary Semantic Segmentation via Attribute Decomposition-Aggregation,"Open-vocabulary semantic segmentation is a challenging task that requires segmenting novel object categories at inference time. 
Recent works explore vision-language pre-training to handle this task, but suffer from unrealistic assumptions in practical scenarios, i.e., low-quality textual category names.
For example, this paradigm assumes that new textual categories will be accurately and completely provided, and exist in lexicons during pre-training.
However, exceptions often happen when meet with ambiguity for brief or incomplete names, new words that are not present in the pre-trained lexicons, and difficult-to-describe categories for users.
To address these issues, this work proposes a novel *attribute decomposition-aggregation* framework, **AttrSeg**, inspired by human cognition in understanding new concepts. 
Specifically, in the *decomposition* stage, we decouple class names into diverse attribute descriptions to complement semantic contexts from multiple perspectives.
Two attribute construction strategies are designed: using large language models for common categories, and involving manually labelling for human-invented categories. 
In the *aggregation* stage, we group diverse attributes into an integrated global description, to form a discriminative classifier that distinguishes the target object from others. 
One hierarchical aggregation architecture is further proposed 
to achieve multi-level aggregation, leveraging the meticulously designed clustering module.
The final result is obtained by computing the similarity between aggregated attributes and images embedding.
To evaluate the effectiveness, we annotate three datasets with attribute descriptions, and conduct extensive experiments and ablation studies. The results show the superior performance of attribute decomposition-aggregation.
We refer readers to the latest arXiv version at https://arxiv.org/abs/2309.00096.",main,NeurIPS,2023,Poster,Chaofan Ma;Yuhuan Yang;Chen Ju;Fei Zhang;Ya Zhang;Yanfeng Wang,True,https://openreview.net/pdf?id=ESEM1lNoeS
EkcO9tHm6S,Recaptured Raw Screen Image and Video Demoiréing via Channel and Spatial Modulations,"Capturing screen contents by smartphone cameras has become a common way for information sharing. However, these images and videos are often degraded by moiré patterns, which are caused by frequency aliasing between the camera filter array and digital display grids. We observe that the moiré patterns in raw domain is simpler than those in sRGB domain, and the moiré patterns in raw color channels have different properties. Therefore, we propose an image and video demoiréing network tailored for raw inputs. We introduce a color-separated feature branch, and it is fused with the traditional feature-mixed branch via channel and spatial modulations. Specifically, the channel modulation utilizes modulated color-separated features to enhance the color-mixed features. The spatial modulation utilizes the feature with large receptive field to modulate the feature with small receptive field. In addition, we build the first well-aligned raw video demoiréing (RawVDemoiré) dataset and propose an efficient temporal alignment method by inserting alternating patterns. Experiments demonstrate that our method achieves state-of-the-art performance for both image and video demoiréing. Our dataset and code will be released after the acceptance of this work.",main,NeurIPS,2023,Poster,Huanjing Yue;Yijia Cheng;Xin Liu;Jingyu Yang,True,https://openreview.net/pdf?id=EkcO9tHm6S
EqnZqrbFrc,Sample Complexity of Goal-Conditioned Hierarchical Reinforcement Learning,"Hierarchical Reinforcement Learning (HRL) algorithms can perform planning at multiple levels of abstraction. Empirical results have shown that state or temporal abstractions might significantly improve the sample efficiency of algorithms. Yet, we still do not have a complete understanding of the basis of those efficiency gains nor any theoretically grounded design rules. In this paper, we derive a lower bound on the sample complexity for the considered class of goal-conditioned HRL algorithms. The proposed lower bound empowers us to quantify the benefits of hierarchical decomposition and leads to the design of a simple Q-learning-type algorithm that leverages hierarchical decompositions. We empirically validate our theoretical findings by investigating the sample complexity of the proposed hierarchical algorithm on a spectrum of tasks (hierarchical $n$-rooms, Gymnasium's Taxi). The hierarchical $n$-rooms tasks were designed to allow us to dial their complexity over multiple orders of magnitude. Our theory and algorithmic findings provide a step towards answering the foundational question of quantifying the improvement hierarchical decomposition offers over monolithic solutions in reinforcement learning.",main,NeurIPS,2023,Poster,Arnaud Robert;Ciara Pike-Burke;Aldo A. Faisal,True,https://openreview.net/pdf?id=EqnZqrbFrc
FAGY52HbyV,Debiased and Denoised Entity Recognition from Distant Supervision,"While distant supervision has been extensively explored and exploited in NLP tasks like named entity recognition, a major obstacle stems from the inevitable noisy distant labels tagged unsupervisedly. A few past works approach this problem by adopting a self-training framework with a sample-selection mechanism. In this work, we innovatively identify two types of biases that were omitted by prior work, and these biases lead to inferior performance of the distant-supervised NER setup. First, we characterize the noise concealed in the distant labels as highly structural rather than fully randomized. Second, the self-training framework would ubiquitously introduce an inherent bias that causes erroneous behavior in both sample selection and eventually prediction. To cope with these problems, we propose a novel self-training framework, dubbed DesERT. This framework augments the conventional NER predicative pathway to a dual form that effectively adapts the sample-selection process to conform to its innate distributional-bias structure. The other crucial component of DesERT composes a debiased module aiming to enhance the token representations, hence the quality of the pseudo-labels. Extensive experiments are conducted to validate the DesERT. The results show that our framework establishes a new state-of-art performance, it achieves a +2.22% average F1 score improvement on five standardized benchmarking datasets. Lastly, DesERT demonstrates its effectiveness under a new DSNER benchmark where additional distant supervision comes from the ChatGPT model.",main,NeurIPS,2023,Poster,Haobo Wang;Yiwen Dong;Ruixuan Xiao;Fei Huang;Gang Chen;Junbo Zhao,True,https://openreview.net/pdf?id=FAGY52HbyV
FC0dsvguFi,Lung250M-4B: A Combined 3D Dataset for CT- and Point Cloud-Based Intra-Patient Lung Registration,"A popular benchmark for intra-patient lung registration is provided by the DIR-LAB COPDgene dataset consisting of large-motion in- and expiratory breath-hold CT pairs. This dataset alone, however, does not provide enough samples to properly train state-of-the-art deep learning methods. Other public datasets often also provide only small sample sizes or include primarily small motions between scans that do not translate well to larger deformations. For point-based geometric registration, the PVT1010 dataset provides a large number of vessel point clouds without any correspondences and a labeled test set corresponding to the COPDgene cases. However, the absence of correspondences for supervision complicates training, and a fair comparison with image-based algorithms is infeasible, since CT scans for the training data are not publicly available.
We here provide a combined benchmark for image- and point-based registration approaches. We curated a total of 248 public multi-centric in- and expiratory lung CT scans from 124 patients, which show large motion between scans, processed them to ensure sufficient homogeneity between the data and generated vessel point clouds that are well distributed even deeper inside the lungs. For supervised training, we provide vein and artery segmentations of the vessels and multiple thousand image-derived keypoint correspondences for each pair. For validation, we provide multiple scan pairs with manual landmark annotations. Finally, as first baselines on our new benchmark, we evaluate several image and point cloud registration methods on the dataset.",Datasets & Benchmarks,NeurIPS,2023,Poster,Fenja Falta;Christoph Großbröhmer;Alessa Hering;Alexander Bigalke;Mattias P Heinrich,True,https://openreview.net/pdf?id=FC0dsvguFi
Fp5uC6YHwe,3D-IntPhys: Towards More Generalized 3D-grounded Visual Intuitive Physics under Challenging Scenes,"Given a visual scene, humans have strong intuitions about how a scene can evolve over time under given actions. The intuition, often termed visual intuitive physics, is a critical ability that allows us to make effective plans to manipulate the scene to achieve desired outcomes without relying on extensive trial and error. In this paper, we present a framework capable of learning 3D-grounded visual intuitive physics models from videos of complex scenes with fluids. Our method is composed of a conditional Neural Radiance Field (NeRF)-style visual frontend and a 3D point-based dynamics prediction backend, using which we can impose strong relational and structural inductive bias to capture the structure of the underlying environment. Unlike existing intuitive point-based dynamics works that rely on the supervision of dense point trajectory from simulators, we relax the requirements and only assume access to multi-view RGB images and (imperfect) instance masks acquired using color prior. This enables the proposed model to handle scenarios where accurate point estimation and tracking are hard or impossible. We generate datasets including three challenging scenarios involving fluid, granular materials, and rigid objects in the simulation. The datasets do not include any dense particle information so most previous 3D-based intuitive physics pipelines can barely deal with that. We show our model can make long-horizon future predictions by learning from raw images and significantly outperforms models that do not employ an explicit 3D representation space. We also show that once trained, our model can achieve strong generalization in complex scenarios under extrapolate settings.",main,NeurIPS,2023,Poster,Haotian Xue;Antonio Torralba;Joshua B. Tenenbaum;Daniel LK Yamins;Yunzhu Li;Hsiao-Yu Tung,True,https://openreview.net/pdf?id=Fp5uC6YHwe
FpK2aQfbyo,MiliPoint: A Point Cloud Dataset for mmWave Radar,"Millimetre-wave (mmWave) radar has emerged as an attractive and cost-effective alternative for human activity sensing compared to traditional camera-based systems. mmWave radars are also non-intrusive, providing better protection for user privacy. However, as a Radio Frequency based technology, mmWave radars rely on capturing reflected signals from objects, making them more prone to noise compared to cameras. This raises an intriguing question for the deep learning community: Can we develop more effective point set-based deep learning methods for such attractive sensors? 
  
To answer this question, our work, termed MiliPoint, delves into this idea by providing a large-scale, open dataset for the community to explore how mmWave radars can be utilised for human activity recognition. Moreover, MiliPoint stands out as it is larger in size than existing datasets, has more diverse human actions represented, and encompasses all three key tasks in human activity recognition. We have also established a range of point-based deep neural networks such as DGCNN, PointNet++ and PointTransformer, on MiliPoint, which can serve to set the ground baseline for further development.",Datasets & Benchmarks,NeurIPS,2023,Poster,Han Cui;Shu Zhong;Jiacheng Wu;Zichao Shen;Naim Dahnoun;Yiren Zhao,True,https://openreview.net/pdf?id=FpK2aQfbyo
FxRfAIj4s2,"Neural Image Compression: Generalization, Robustness, and Spectral Biases","Recent advances in neural image compression (NIC) have produced models that are starting to outperform classic codecs. While this has led to growing excitement about using NIC in real-world applications, the successful adoption of any machine learning system in the wild requires it to generalize (and be robust) to unseen distribution shifts at deployment. Unfortunately, current research lacks comprehensive datasets and informative tools to evaluate and understand NIC performance in real-world settings. To bridge this crucial gap, first, this paper presents a comprehensive benchmark suite to evaluate the out-of-distribution (OOD) performance of image compression methods. Specifically, we provide CLIC-C and Kodak-C by introducing 15 corruptions to the popular CLIC and Kodak benchmarks. Next, we propose spectrally-inspired inspection tools to gain deeper insight into errors introduced by image compression methods as well as their OOD performance. We then carry out a detailed performance comparison of several classic codecs and NIC variants, revealing intriguing findings that challenge our current understanding of the strengths and limitations of NIC. Finally, we corroborate our empirical findings with theoretical analysis, providing an in-depth view of the OOD performance of NIC and its dependence on the spectral properties of the data. Our benchmarks, spectral inspection tools, and findings provide a crucial bridge to the real-world adoption of NIC. We hope that our work will propel future efforts in designing robust and generalizable NIC methods. Code and data will be made available at https://github.com/klieberman/ood_nic.",main,NeurIPS,2023,Poster,Kelsey Lieberman;James Diffenderfer;Charles Godfrey;Bhavya Kailkhura,True,https://openreview.net/pdf?id=FxRfAIj4s2
G5RwHpBUv0,Pick-a-Pic: An Open Dataset of User Preferences for Text-to-Image Generation,"The ability to collect a large dataset of human preferences from text-to-image users is usually limited to companies, making such datasets inaccessible to the public. To address this issue, we create a web app that enables text-to-image users to generate images and specify their preferences. Using this web app we build Pick-a-Pic, a large, open dataset of text-to-image prompts and real users’ preferences over generated images. We leverage this dataset to train a CLIP-based scoring function, PickScore, which exhibits superhuman performance on the task of predicting human preferences. Then, we test PickScore’s ability to perform model evaluation and observe that it correlates better with human rankings than other automatic evaluation metrics. Therefore, we recommend using PickScore for evaluating future text-to-image generation models, and using Pick-a-Pic prompts as a more relevant dataset than MS-COCO. Finally, we demonstrate how PickScore can enhance existing text-to-image models via ranking.",main,NeurIPS,2023,Poster,Yuval Kirstain;Adam Polyak;Uriel Singer;Shahbuland Matiana;Joe Penna;Omer Levy,True,https://openreview.net/pdf?id=G5RwHpBUv0
GDYuzX0rwj,"Facing Off World Model Backbones: RNNs, Transformers, and S4","World models are a fundamental component in model-based reinforcement learning (MBRL). To perform temporally extended and consistent simulations of the future in partially observable environments, world models need to possess long-term memory. However, state-of-the-art MBRL agents, such as Dreamer, predominantly employ recurrent neural networks (RNNs) as their world model backbone, which have limited memory capacity. In this paper, we seek to explore alternative world model backbones for improving long-term memory. In particular, we investigate the effectiveness of Transformers and Structured State Space Sequence (S4) models, motivated by their remarkable ability to capture long-range dependencies in low-dimensional sequences and their complementary strengths. We propose S4WM, the first world model compatible with parallelizable SSMs including S4 and its variants. By incorporating latent variable modeling, S4WM can efficiently generate high-dimensional image sequences through latent imagination. Furthermore, we extensively compare RNN-, Transformer-, and S4-based world models across four sets of environments, which we have tailored to assess crucial memory capabilities of world models, including long-term imagination, context-dependent recall, reward prediction, and memory-based reasoning. Our findings demonstrate that S4WM outperforms Transformer-based world models in terms of long-term memory, while exhibiting greater efficiency during training and imagination. These results pave the way for the development of stronger MBRL agents.",main,NeurIPS,2023,Poster,Fei Deng;Junyeong Park;Sungjin Ahn,True,https://openreview.net/pdf?id=GDYuzX0rwj
GF5l0F19Bt,An NLP Benchmark Dataset for Assessing Corporate Climate Policy Engagement,"As societal awareness of climate change grows, corporate climate policy engagements are attracting attention.
We propose a dataset to estimate corporate climate policy engagement from various PDF-formatted documents.
Our dataset comes from LobbyMap (a platform operated by global think tank InfluenceMap) that provides engagement categories and stances on the documents.
To convert the LobbyMap data into the structured dataset, we developed a pipeline using text extraction and OCR.
Our contributions are: (i) Building an NLP dataset including 10K documents on corporate climate policy engagement. (ii) Analyzing the properties and challenges of the dataset. (iii) Providing experiments for the dataset using pre-trained language models.
The results show that while Longformer outperforms baselines and other pre-trained models, there is still room for significant improvement.
We hope our work begins to bridge research on NLP and climate change.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Gaku Morio;Christopher D Manning,True,https://openreview.net/pdf?id=GF5l0F19Bt
GF84C0z45H,GenImage: A Million-Scale Benchmark for Detecting AI-Generated Image,"The extraordinary ability of generative models to generate photographic images has intensified concerns about the spread of disinformation, thereby leading to the demand for detectors capable of distinguishing between AI-generated fake images and real images. However, the lack of large datasets containing images from the most advanced image generators poses an obstacle to the development of such detectors. In this paper, we introduce the GenImage dataset, which has the following advantages: 1) Plenty of Images, including over one million pairs of AI-generated fake images and collected real images. 2) Rich Image Content, encompassing a broad range of image classes. 3) State-of-the-art Generators, synthesizing images with advanced diffusion models and GANs. The aforementioned advantages allow the detectors trained on GenImage to undergo a thorough evaluation and demonstrate strong applicability to diverse images. We conduct a comprehensive analysis of the dataset and propose two tasks for evaluating the detection method in resembling real-world scenarios. The cross-generator image classification task measures the performance of a detector trained on one generator when tested on the others. The degraded image classification task assesses the capability of the detectors in handling degraded images such as low-resolution, blurred, and compressed images. With the GenImage dataset, researchers can effectively expedite the development and evaluation of superior AI-generated image detectors in comparison to prevailing methodologies.",Datasets & Benchmarks,NeurIPS,2023,Poster,Mingjian Zhu;Hanting Chen;Qiangyu YAN;Xudong Huang;Guanyu Lin;Wei Li;Zhijun Tu;Hailin Hu;Jie Hu;Yunhe Wang,True,https://openreview.net/pdf?id=GF84C0z45H
GRHZiTbDDI,4D Panoptic Scene Graph Generation,"We are living in a three-dimensional space while moving forward through a fourth dimension: time. To allow artificial intelligence to develop a comprehensive understanding of such a 4D environment, we introduce **4D Panoptic Scene Graph (PSG-4D)**, a new representation that bridges the raw visual data perceived in a dynamic 4D world and high-level visual understanding. Specifically, PSG-4D abstracts rich 4D sensory data into nodes, which represent entities with precise location and status information, and edges, which capture the temporal relations. To facilitate research in this new area, we build a richly annotated PSG-4D dataset consisting of 3K RGB-D videos with a total of 1M frames, each of which is labeled with 4D panoptic segmentation masks as well as fine-grained, dynamic scene graphs. To solve PSG-4D, we propose PSG4DFormer, a Transformer-based model that can predict panoptic segmentation masks, track masks along the time axis, and generate the corresponding scene graphs via a relation component. Extensive experiments on the new dataset show that our method can serve as a strong baseline for future research on PSG-4D. In the end, we provide a real-world application example to demonstrate how we can achieve dynamic scene understanding by integrating a large language model into our PSG-4D system.",main,NeurIPS,2023,Spotlight,Jingkang Yang;Jun CEN;Wenxuan Peng;Shuai Liu;Fangzhou Hong;Xiangtai Li;Kaiyang Zhou;Qifeng Chen;Ziwei Liu,True,https://openreview.net/pdf?id=GRHZiTbDDI
GSuP99u2kR,LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day,"Conversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text. Multimodal conversational AI has seen rapid progress by leveraging billions of image-text pairs from the public web, but such general-domain vision-language models still lack sophistication in understanding and conversing about biomedical images. In this paper, we propose a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to train a Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal conversational capability and can follow open-ended instruction to assist with inquiries about a biomedical image. On three standard biomedical visual question answering datasets, LLaVA-Med outperforms previous supervised state-of-the-art on certain metrics. To facilitate biomedical multimodal research, we will release our instruction-following data and the LLaVA-Med model.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Chunyuan Li;Cliff Wong;Sheng Zhang;Naoto Usuyama;Haotian Liu;Jianwei Yang;Tristan Naumann;Hoifung Poon;Jianfeng Gao,True,https://openreview.net/pdf?id=GSuP99u2kR
GYjV1M5s0D,FineMoGen: Fine-Grained Spatio-Temporal Motion Generation and Editing,"Text-driven motion generation has achieved substantial progress with the emergence of diffusion models. However, existing methods still struggle to generate complex motion sequences that correspond to fine-grained descriptions, depicting detailed and accurate spatio-temporal actions.This lack of fine controllability limits the usage of motion generation to a larger audience. To tackle these challenges, we present FineMoGen, a diffusion-based motion generation and editing framework that can synthesize fine-grained motions, with spatial-temporal composition to the user instructions. Specifically, FineMoGen builds upon diffusion model with a novel transformer architecture dubbed Spatio-Temporal Mixture Attention SAMI. SAMI optimizes the generation of the global attention template from two perspectives: 1) explicitly modeling the constraints of spatio-temporal composition; and 2) utilizing sparsely-activated mixture-of-experts to adaptively extract fine-grained features. To facilitate a large-scale study on this new fine-grained motion generation task, we contribute the HuMMan-MoGen dataset, which consists of 2,968 videos and 102,336 fine-grained spatio-temporal descriptions. Extensive experiments validate that FineMoGen  exhibits superior motion generation quality over state-of-the-art methods. Notably, FineMoGen further enables zero-shot motion editing capabilities with the aid of modern large language models (LLM), which faithfully manipulates motion sequences with fine-grained instructions.",main,NeurIPS,2023,Poster,Mingyuan Zhang;Huirong Li;Zhongang Cai;Jiawei Ren;Lei Yang;Ziwei Liu,True,https://openreview.net/pdf?id=GYjV1M5s0D
GjNvvswoUL,DICES Dataset: Diversity in Conversational AI Evaluation for Safety,"Machine learning approaches often require training and evaluation datasets with a clear separation between positive and negative examples. This requirement overly simplifies the natural subjectivity present in many tasks, and obscures the inherent diversity in human perceptions and opinions about many content items. Preserving the variance in content and diversity in human perceptions in datasets is often quite expensive and laborious. This is especially troubling when building safety datasets for conversational AI systems, as safety is socio-culturally situated in this context. To demonstrate this crucial aspect of conversational AI safety, and to facilitate in-depth model performance analyses, we introduce the DICES (Diversity In Conversational AI Evaluation for Safety) dataset that contains fine-grained demographics information about raters, high replication of ratings per item to ensure statistical power for analyses, and encodes rater votes as distributions across different demographics to allow for in-depth explorations of different aggregation strategies. The DICES dataset enables the observation and measurement of variance, ambiguity, and diversity in the context of safety for conversational AI. We further describe a set of metrics that show how rater diversity influences safety perception across different geographic regions, ethnicity groups, age groups, and genders. The goal of the DICES dataset is to be used as a shared resource and benchmark that respects diverse perspectives during safety evaluation of conversational AI systems.",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=GjNvvswoUL
H2Yb28qGLV,Lo-Hi: Practical ML Drug Discovery Benchmark,"Finding new drugs is getting harder and harder. One of the hopes of drug discovery is to use machine learning models to predict molecular properties. That is why models for molecular property prediction are being developed and tested on benchmarks such as MoleculeNet. However, existing benchmarks are unrealistic and are too different from applying the models in practice. We have created a new practical \\\\emph{Lo-Hi} benchmark consisting of two tasks: Lead Optimization (Lo) and Hit Identification (Hi), corresponding to the real drug discovery process. For the Hi task, we designed a novel molecular splitting algorithm that solves the Balanced Vertex Minimum $k$-Cut problem.  We tested state-of-the-art and classic ML models, revealing which works better under practical settings. We analyzed modern benchmarks and showed that they are unrealistic and overoptimistic.

Review: https://openreview.net/forum?id=H2Yb28qGLV

Lo-Hi benchmark: https://github.com/SteshinSS/lohi_neurips2023

Lo-Hi splitter library: https://github.com/SteshinSS/lohi_splitter",Datasets & Benchmarks,NeurIPS,2023,Poster,Simon Steshin,True,https://openreview.net/pdf?id=H2Yb28qGLV
HYEGXFnPoq,Perception Test: A Diagnostic Benchmark for Multimodal Video Models,"We propose a novel multimodal video benchmark - the Perception Test - to evaluate the perception and reasoning skills of pre-trained multimodal models (e.g. Flamingo, BEiT-3, or GPT-4). Compared to existing benchmarks that focus on computational tasks (e.g. classification, detection or tracking), the Perception Test focuses on skills (Memory, Abstraction, Physics, Semantics) and types of reasoning (descriptive, explanatory, predictive, counterfactual) across video, audio, and text modalities, to provide a comprehensive and efficient evaluation tool. The benchmark probes pre-trained models for their transfer capabilities, in a zero-shot / few-shot or limited finetuning regime. For these purposes, the Perception Test introduces 11.6k real-world videos, 23s average length, designed to show perceptually interesting situations, filmed by around 100 participants worldwide. The videos are densely annotated with six types of labels (multiple-choice and grounded video question-answers, object and point tracks, temporal action and sound segments), enabling both language and non-language evaluations. The fine-tuning and validation splits of the benchmark are publicly available (CC-BY license), in addition to a challenge server with a held-out test split. Human baseline results compared to state-of-the-art video QA models show a substantial gap in performance (91.4% vs 46.2%), suggesting that there is significant room for improvement in multimodal video understanding.
Dataset, baselines code, and challenge server are available at https://github.com/deepmind/perception_test",Datasets & Benchmarks,NeurIPS,2023,Poster,Viorica Patraucean;Lucas Smaira;Ankush Gupta;Adria Recasens Continente;Larisa Markeeva;Dylan Sunil Banarse;Skanda Koppula;Joseph Heyward;Mateusz Malinowski;Yi Yang;Carl Doersch;Tatiana Matejovicova;Yury Sulsky;Antoine Miech;Alexandre Fréchette;Hanna Klimczak;Raphael Koster;Junlin Zhang;Stephanie Winkler;Yusuf Aytar;Simon Osindero;Dima Damen;Andrew Zisserman;Joao Carreira,True,https://openreview.net/pdf?id=HYEGXFnPoq
HfKOIPCvsv,RealTime QA: What's the Answer Right Now?,"We introduce RealTime QA, a dynamic question answering (QA) platform that announces questions and evaluates systems on a regular basis (weekly in this version). RealTime QA inquires about the current world, and QA systems need to answer questions about novel events or information. It therefore challenges static, conventional assumptions in open-domain QA datasets and pursues instantaneous applications. We build strong baseline models upon large pretrained language models, including GPT-3 and T5. Our benchmark is an ongoing effort, and this paper presents real-time evaluation results over the past year. Our experimental results show that GPT-3 can often properly update its generation results, based on newly-retrieved documents, highlighting the importance of up-to-date information retrieval. Nonetheless, we find that GPT-3 tends to return outdated answers when retrieved documents do not provide sufficient information to find an answer. This suggests an important avenue for future research: can an open-domain QA system identify such unanswerable cases and communicate with the user or even the retrieval module to modify the retrieval results? We hope that RealTime QA will spur progress in instantaneous applications of question answering and beyond.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jungo Kasai;Keisuke Sakaguchi;yoichi takahashi;Ronan Le Bras;Akari Asai;Xinyan Velocity Yu;Dragomir Radev;Noah A. Smith;Yejin Choi;Kentaro Inui,False,https://openreview.net/pdf?id=HfKOIPCvsv
HhcQ0zeqZp,Benchmarking Large Language Models on CMExam - A comprehensive Chinese Medical Exam Dataset,"Recent advancements in large language models (LLMs) have transformed the field of question answering (QA). However, evaluating LLMs in the medical field is challenging due to the lack of standardized and comprehensive datasets. To address this gap, we introduce CMExam, sourced from the Chinese National Medical Licensing Examination. CMExam consists of 60K+ multiple-choice questions for standardized and objective evaluations, as well as solution explanations for model reasoning evaluation in an open-ended manner. For in-depth analyses of LLMs, we invited medical professionals to label five additional question-wise annotations, including disease groups, clinical departments, medical disciplines, areas of competency, and question difficulty levels. Alongside the dataset, we further conducted thorough experiments with representative LLMs and QA algorithms on CMExam. The results show that GPT-4 had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results highlight a great disparity when compared to human accuracy, which stood at 71.6%. For explanation tasks, while LLMs could generate relevant reasoning and demonstrate improved performance after finetuning, they fall short of a desired standard, indicating ample room for improvement. To the best of our knowledge, CMExam is the first Chinese medical exam dataset to provide comprehensive medical annotations. The experiments and findings of LLM evaluation also provide valuable insights into the challenges and potential solutions in developing Chinese medical QA systems and LLM evaluation pipelines.",Datasets & Benchmarks,NeurIPS,2023,Poster,Junling Liu;Peilin Zhou;Yining Hua;Dading Chong;Zhongyu Tian;Andrew Liu;Helin Wang;Chenyu You;Zhenhua Guo;Zhu Lei;Michael Lingzhi Li,True,https://openreview.net/pdf?id=HhcQ0zeqZp
Hm1Ih3uLII,DVSOD: RGB-D Video Salient Object Detection,"Salient object detection (SOD) aims to identify standout elements in a scene, with recent advancements primarily focused on integrating depth data (RGB-D) or temporal data from videos to enhance SOD in complex scenes. However, the unison of two types of crucial information remains largely underexplored due to data constraints. To bridge this gap, we in this work introduce the DViSal dataset, fueling further research in the emerging field of RGB-D video salient object detection (DVSOD). Our dataset features 237 diverse RGB-D videos alongside comprehensive annotations, including object and instance-level markings, as well as bounding boxes and scribbles. These resources enable a broad scope for potential research directions. We also conduct benchmarking experiments using various SOD models, affirming the efficacy of multimodal video input for salient object detection. Lastly, we highlight some intriguing findings and promising future research avenues. To foster growth in this field, our dataset and benchmark results are publicly accessible at: https://dvsod.github.io/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jingjing Li;Wei Ji;Size Wang;Wenbo Li;Li Cheng,True,https://openreview.net/pdf?id=Hm1Ih3uLII
HuG4eOFLO9,Bullying10K: A Large-Scale Neuromorphic Dataset towards Privacy-Preserving Bullying Recognition,"The prevalence of violence in daily life poses significant threats to individuals' physical and mental well-being. Using surveillance cameras in public spaces has proven effective in proactively deterring and preventing such incidents. However, concerns regarding privacy invasion have emerged due to their widespread deployment.To address the problem, we leverage Dynamic Vision Sensors (DVS) cameras to detect violent incidents and preserve privacy since it captures pixel brightness variations instead of static imagery. We introduce the Bullying10K dataset, encompassing various actions, complex movements, and occlusions from real-life scenarios. It provides three benchmarks for evaluating different tasks: action recognition, temporal action localization, and pose estimation. With 10,000 event segments, totaling 12 billion events and 255 GB of data, Bullying10K contributes significantly by balancing violence detection and personal privacy persevering. And it also poses a challenge to the neuromorphic dataset. It will serve as a valuable resource for training and developing privacy-protecting video systems. The Bullying10K opens new possibilities for innovative approaches in these domains.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yiting Dong;Yang Li;Dongcheng Zhao;Guobin Shen;Yi Zeng,True,https://openreview.net/pdf?id=HuG4eOFLO9
HvcLKgtbco,Towards a Comprehensive Benchmark for High-Level Synthesis Targeted to FPGAs,"High-level synthesis (HLS) aims to raise the abstraction layer in hardware design, enabling the design of domain-specific accelerators (DSAs) like field-programmable gate arrays (FPGAs) using C/C++ instead of hardware description languages (HDLs). Compiler directives in the form of pragmas play a crucial role in modifying the microarchitecture within the HLS framework. However, the space of possible microarchitectures grows exponentially with the number of pragmas. Moreover, the evaluation of each candidate design using the HLS tool consumes significant time, ranging from minutes to hours, leading to a time-consuming optimization process. To accelerate this process, machine learning models have been used to predict design quality in milliseconds. However, existing open-source datasets for training such models are limited in terms of design complexity and available optimizations. In this paper, we present HLSyn, the first benchmark that addresses these limitations. It contains more complex programs with a wider range of optimization pragmas, making it a comprehensive dataset for training and evaluating design quality prediction models. The HLSyn benchmark consists of 42 unique programs/kernels, resulting in over 42,000 labeled designs. We conduct an extensive comparison of state-of-the-art baselines to assess their effectiveness in predicting design quality. As an ongoing project, we anticipate expanding the HLSyn benchmark in terms of both quantity and variety of programs to further support the development of this field.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yunsheng Bai;Atefeh Sohrabizadeh;Zongyue Qin;Ziniu Hu;Yizhou Sun;Jason Cong,True,https://openreview.net/pdf?id=HvcLKgtbco
IL5zJqfxAa,EmbodiedGPT: Vision-Language Pre-Training via Embodied Chain of Thought,"Embodied AI is a crucial frontier in robotics, capable of planning and executing action sequences for robots to accomplish long-horizon tasks in physical environments.
In this work, we introduce EmbodiedGPT, an end-to-end multi-modal foundation model for embodied AI, empowering embodied agents with multi-modal understanding and execution capabilities. To achieve this, we have made the following efforts: (i) We craft a large-scale embodied planning dataset, termed EgoCOT. The dataset consists of carefully selected videos from the Ego4D dataset, along with corresponding high-quality language instructions. Specifically, we generate a sequence of sub-goals with the ""Chain of Thoughts"" mode for effective embodied planning.
(ii) We introduce an efficient training approach to EmbodiedGPT for high-quality plan generation, by adapting a 7B large language model (LLM) to the EgoCOT dataset via prefix tuning. (iii) We introduce a paradigm for extracting task-related features from LLM-generated planning queries to form a closed loop between high-level planning and low-level control.
Extensive experiments show the effectiveness of EmbodiedGPT on embodied tasks, including embodied planning, embodied control, visual captioning, and visual question answering.
Notably, EmbodiedGPT significantly enhances the success rate of the embodied control task by extracting more effective features. It has achieved a remarkable 1.6 times increase in success rate on the Franka Kitchen benchmark and a 1.3 times increase on the Meta-World benchmark, compared to the BLIP-2 baseline fine-tuned with the Ego4D dataset.",main,NeurIPS,2023,Spotlight,Yao Mu;Qinglong Zhang;Mengkang Hu;Wenhai Wang;Mingyu Ding;Jun Jin;Bin Wang;Jifeng Dai;Yu Qiao;Ping Luo,True,https://openreview.net/pdf?id=IL5zJqfxAa
IZRlMABK4l,Efficient Test-Time Adaptation for Super-Resolution with Second-Order Degradation and Reconstruction,"Image super-resolution (SR) aims to learn a mapping from low-resolution (LR) to high-resolution (HR) using paired HR-LR training images. Conventional SR methods typically gather the paired training data by synthesizing LR images from HR images using a predetermined degradation model, e.g., Bicubic down-sampling. However, the realistic degradation type of test images may mismatch with the training-time degradation type due to the dynamic changes of the real-world scenarios, resulting in inferior-quality SR images. To address this, existing methods attempt to estimate the degradation model and train an image-specific model, which, however, is quite time-consuming and impracticable to handle rapidly changing domain shifts. Moreover, these methods largely concentrate on the estimation of one degradation type (e.g., blur degradation), overlooking other degradation types like noise and JPEG in real-world test-time scenarios, thus limiting their practicality. To tackle these problems, we present an efficient test-time adaptation framework for SR, named SRTTA, which is able to quickly adapt SR models to test domains with different/unknown degradation types. Specifically, we design a second-order degradation scheme to construct paired data based on the degradation type of the test image, which is predicted by a pre-trained degradation classifier. Then, we adapt the SR model by implementing feature-level reconstruction learning from the initial test image to its second-order degraded counterparts, which helps the SR model generate plausible HR images. Extensive experiments are conducted on newly synthesized corrupted DIV2K datasets with 8 different degradations and several real-world datasets, demonstrating that our SRTTA framework achieves an impressive improvement over existing methods with satisfying speed. The source code is available at https://github.com/DengZeshuai/SRTTA.",main,NeurIPS,2023,Poster,Zeshuai Deng;Zhuokun Chen;Shuaicheng Niu;Thomas H. Li;Bohan Zhuang;Mingkui Tan,True,https://openreview.net/pdf?id=IZRlMABK4l
Icxwnu9hcO,NIS3D: A Completely Annotated Benchmark for Dense 3D Nuclei Image Segmentation,"3D segmentation of nuclei images is a fundamental task for many biological studies. Despite the rapid advances of large-volume 3D imaging acquisition methods and the emergence of sophisticated algorithms to segment the nuclei in recent years, a benchmark with all cells completely annotated is still missing, making it hard to accurately assess and further improve the performance of the algorithms. The existing nuclei segmentation benchmarks either worked on 2D only or annotated a small number of 3D cells, perhaps due to the high cost of 3D annotation for large-scale data. To fulfill the critical need, we constructed NIS3D, a 3D, high cell density, large-volume, and completely annotated Nuclei Image Segmentation benchmark, assisted by our newly designed semi-automatic annotation software. NIS3D provides more than 22,000 cells across multiple most-used species in this area. Each cell is labeled by three independent annotators, so we can measure the variability of each annotation. A confidence score is computed for each cell, allowing more nuanced testing and performance comparison. A comprehensive review on the methods of segmenting 3D dense nuclei was conducted. The benchmark was used to evaluate the performance of several selected state-of-the-art segmentation algorithms. The best of current methods is still far away from human-level accuracy, corroborating the necessity of generating such a benchmark. The testing results also demonstrated the strength and weakness of each method and pointed out the directions of further methodological development. The dataset can be downloaded here: https://github.com/yu-lab-vt/NIS3D.",Datasets & Benchmarks,NeurIPS,2023,Poster,Wei Zheng;James Cheng Peng;Zeyuan Hou;Boyu Lyu;Mengfan Wang;Xuelong Mi;Shuoxuan Qiao;Yinan Wan;Guoqiang Yu,True,https://openreview.net/pdf?id=Icxwnu9hcO
IiRHQ7gvnq,Benchmarking Foundation Models with Language-Model-as-an-Examiner,"Numerous benchmarks have been established to assess the performance of foundation models on open-ended question answering, which serves as a comprehensive test of a model's ability to understand and generate language in a manner similar to humans.
Most of these works focus on proposing new datasets, however, we see two main issues within previous benchmarking pipelines, namely testing leakage and evaluation automation. In this paper, we propose a novel benchmarking framework, Language-Model-as-an-Examiner, where the LM serves as a knowledgeable examiner that formulates questions based on its knowledge and evaluates responses in a reference-free manner. Our framework allows for effortless extensibility as various LMs can be adopted as the examiner, and the questions can be constantly updated given more diverse trigger topics. For a more comprehensive and equitable evaluation, we devise three strategies: (1) We instruct the LM examiner to generate questions across a multitude of domains to probe for a broad acquisition, and raise follow-up questions to engage in a more in-depth assessment. (2) Upon evaluation, the examiner combines both scoring and ranking measurements, providing a reliable result as it aligns closely with human annotations. (3) We additionally propose a decentralized Peer-examination method to address the biases in a single examiner. Our data and benchmarking results are available at: http://lmexam.xlore.cn.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yushi Bai;Jiahao Ying;Yixin Cao;Xin Lv;Yuze He;Xiaozhi Wang;Jifan Yu;Kaisheng Zeng;Yijia Xiao;Haozhe Lyu;Jiayin Zhang;Juanzi Li;Lei Hou,False,https://openreview.net/pdf?id=IiRHQ7gvnq
IptxZvA3at,GEO-Bench: Toward Foundation Models for Earth Monitoring,"Recent progress in self-supervision has shown that pre-training large neural networks on vast amounts of unsupervised data can lead to substantial increases in generalization to downstream tasks. 
Such models, recently coined foundation models, have been transformational to the field of natural language processing.
Variants have also been proposed for image data, but their applicability to remote sensing tasks is limited.
To stimulate the development of foundation models for Earth monitoring, we propose a benchmark comprised of six classification and six segmentation tasks, which were carefully curated and adapted to be both relevant to the field and well-suited for model evaluation. We accompany this benchmark with a robust methodology for evaluating models and reporting aggregated results to enable a reliable assessment of progress. Finally, we report results for 20 baselines to gain information about the performance of existing models.
We believe that this benchmark will be a driver of progress across a variety of Earth monitoring tasks.",Datasets & Benchmarks,NeurIPS,2023,Poster,Alexandre Lacoste;Nils Lehmann;Pau Rodriguez;Evan David Sherwin;Hannah Kerner;Björn Lütjens;Jeremy Andrew Irvin;David Dao;Hamed Alemohammad;Alexandre Drouin;Mehmet Gunturkun;Gabriel Huang;David Vazquez;Dava Newman;Yoshua Bengio;Stefano Ermon;Xiao Xiang Zhu,True,https://openreview.net/pdf?id=IptxZvA3at
JGVSxwKHbq,GeoDE: a Geographically Diverse Evaluation Dataset for Object Recognition,"Current dataset collection methods typically scrape large amounts of data from the web. While this technique is extremely scalable, data collected in this way tends to reinforce stereotypical biases, can contain personally identifiable information, and typically originates from Europe and North America. In this work, we rethink the dataset collection paradigm and introduce GeoDE, a geographically diverse dataset with 61,940 images from 40 classes and 6 world regions, and no personally identifiable information, collected by soliciting images from people across the world. We analyse GeoDE to understand differences in images collected in this manner compared to web-scraping. Despite the smaller size of this dataset, we demonstrate its use as both an evaluation and training dataset, allowing us to highlight shortcomings in current models, as well as demonstrate improved performance even when training on this small dataset. We release the full dataset and code at https://geodiverse-data-collection.cs.princeton.edu/",Datasets & Benchmarks,NeurIPS,2023,Poster,Vikram V. Ramaswamy;Sing Yu Lin;Dora Zhao;Aaron Bryan Adcock;Laurens van der Maaten;Deepti Ghadiyaram;Olga Russakovsky,True,https://openreview.net/pdf?id=JGVSxwKHbq
JIKM2vS8XU,DatasetDM: Synthesizing Data with Perception Annotations Using Diffusion Models,"Current deep networks are very data-hungry and benefit from training on large-scale datasets, which are often time-consuming to collect and annotate. By contrast, synthetic data can be generated infinitely using generative models such as DALL-E and diffusion models, with minimal effort and cost. In this paper, we present DatasetDM, a generic dataset generation model that can produce diverse synthetic
images and the corresponding high-quality perception annotations (e.g., segmentation masks, and depth). Our method builds upon the pre-trained diffusion model and extends text-guided image synthesis to perception data generation. We show that the rich latent code of the diffusion model can be effectively decoded as accurate perception annotations using a decoder module. Training the decoder only needs less than 1% (around 100 images) of manually labeled images, enabling the generation of an infinitely large annotated dataset. Then these synthetic data can be used for training various perception models on downstream tasks. To showcase the power of the proposed approach, we generate datasets with rich dense pixel-wise labels for a wide range of downstream tasks, including semantic15
segmentation, instance segmentation, and depth estimation. Notably, it achieves 1) state-of-the-art results on semantic segmentation and instance segmentation; 2) significantly more efficient and robust in domain generalization than the real data; 3) state-of-the-art results in zero-shot segmentation setting; and 4) flexibility for efficient application and novel task composition (e.g., image editing)",main,NeurIPS,2023,Poster,Weijia Wu;Yuzhong Zhao;Hao Chen;Yuchao Gu;Rui Zhao;Yefei He;Hong Zhou;Mike Zheng Shou;Chunhua Shen,True,https://openreview.net/pdf?id=JIKM2vS8XU
JOHp5SmckS,Towards Combinatorial Generalization for Catalysts: A Kohn-Sham Charge-Density Approach,"The Kohn-Sham equations underlie many important applications such as the discovery of new catalysts. Recent machine learning work on catalyst modeling has focused on prediction of the energy, but has so far not yet demonstrated significant out-of-distribution generalization. Here we investigate another approach based on the pointwise learning of the Kohn-Sham charge-density. On a new dataset of bulk catalysts with charge densities, we show density models can generalize to new structures with combinations of elements not seen at train time, a form of combinatorial generalization. We show that over 80% of binary and ternary test cases achieve faster convergence than standard baselines in Density Functional Theory, amounting to an average reduction of 13% in the number of iterations required to reach convergence, which may be of independent interest. Our results suggest that density learning is a viable alternative, trading greater inference costs for a step towards combinatorial generalization, a key property for applications.",main,NeurIPS,2023,Poster,Phil Pope;David Jacobs,True,https://openreview.net/pdf?id=JOHp5SmckS
JVlWseddak,EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding,"We introduce EgoSchema, a very long-form video question-answering dataset, and benchmark to evaluate long video understanding capabilities of modern vision and language systems. Derived from Ego4D, EgoSchema consists of over 5000 human curated multiple choice question answer pairs, spanning over 250 hours of real video data, covering a very broad range of natural human activity and behavior. For each question, EgoSchema requires the correct answer to be selected between five given options based on a three-minute-long video clip. While some prior works have proposed video datasets with long clip lengths, we posit that merely the length of the video clip does not truly capture the temporal difficulty of the video task that is being considered. To remedy this, we introduce temporal certificate sets, a general notion for capturing the intrinsic temporal understanding length associated with a broad range of video understanding tasks & datasets. Based on this metric, we find EgoSchema to have intrinsic temporal lengths over 5.7x longer than the second closest dataset and 10x to 100x longer than any other video understanding dataset. Further, our evaluation of several current state-of-the-art video and language models shows them to be severely lacking in long-term video understanding capabilities. Even models with several billions of parameters achieve QA accuracy less than 33% (random is 20%) on the EgoSchema multi-choice question answering task, while humans achieve about 76% accuracy. We posit that EgoSchema, with its long intrinsic temporal structures and diverse complexity, would serve as a valuable evaluation probe for developing effective long-term video understanding systems in the future. Data and Zero-shot model evaluation code will all be open-sourced under the Ego4D license at http://egoschema.github.io.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,,True,https://openreview.net/pdf?id=JVlWseddak
JVzeOYEx6d,ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation,"We present a comprehensive solution to learn and improve text-to-image models from human preference feedback.
To begin with, we build ImageReward---the first general-purpose text-to-image human preference reward model---to effectively encode human preferences.
Its training is based on our systematic annotation pipeline including rating and ranking, which collects 137k expert comparisons to date.
In human evaluation, ImageReward outperforms existing scoring models and metrics, making it a promising automatic metric for evaluating text-to-image synthesis.
On top of it, we propose Reward Feedback Learning (ReFL), a direct tuning algorithm to optimize diffusion models against a scorer.
Both automatic and human evaluation support ReFL's advantages over compared methods.
All code and datasets are provided at \\\\url{https://github.com/THUDM/ImageReward}.",main,NeurIPS,2023,Poster,Jiazheng Xu;Xiao Liu;Yuchen Wu;Yuxuan Tong;Qinkai Li;Ming Ding;Jie Tang;Yuxiao Dong,True,https://openreview.net/pdf?id=JVzeOYEx6d
JqWtIIaS8n,LithoBench: Benchmarking AI Computational Lithography for Semiconductor Manufacturing,"Computational lithography provides algorithmic and mathematical support for resolution enhancement in optical lithography, which is the critical step in semiconductor manufacturing. 
The time-consuming lithography simulation and mask optimization processes limit the practical application of inverse lithography technology (ILT), a promising solution to the challenges of advanced-node lithography. 
Although various machine learning methods for ILT have shown promise for reducing the computational burden, this field is in lack of a dataset that can train the models thoroughly and evaluate the performance comprehensively. 
To boost the development of AI-driven computational lithography, we present the LithoBench dataset, a collection of circuit layout tiles for deep-learning-based lithography simulation and mask optimization. 
LithoBench consists of more than 120k tiles that are cropped from real circuit designs or synthesized according to the layout topologies of famous ILT testcases. 
The ground truths are generated by a famous lithography model in academia and an advanced ILT method. 
Based on the data, we provide a framework to design and evaluate deep neural networks (DNNs) with the data. 
The framework is used to benchmark state-of-the-art models on lithography simulation and mask optimization. 
We hope LithoBench can promote the research and development of computational lithography. 
LithoBench is available at https://anonymous.4open.science/r/lithobench-APPL.",Datasets & Benchmarks,NeurIPS,2023,Poster,Su Zheng;Haoyu Yang;Binwu Zhu;Bei Yu;Martin D.F. Wong,True,https://openreview.net/pdf?id=JqWtIIaS8n
Jsc7WSCZd4,SugarCrepe: Fixing Hackable Benchmarks for Vision-Language Compositionality,"In the last year alone, a surge of new benchmarks to measure $\\\\textit{compositional}$ understanding of vision-language models have permeated the machine learning ecosystem.
Given an image, these benchmarks probe a model's ability to identify its associated caption amongst a set of compositional distractors.
Surprisingly, we find significant biases in $\\\\textit{all}$ these benchmarks rendering them hackable. This hackability is so dire that blind models with no access to the image outperform state-of-the-art vision-language models.
To remedy this rampant vulnerability, we introduce $\\\\textit{SugarCrepe}$, a new benchmark for vision-language compositionality evaluation.
We employ large language models, instead of rule-based templates used in previous benchmarks, to generate fluent and sensical hard negatives, and utilize an adversarial refinement mechanism to maximally reduce biases. We re-evaluate state-of-the-art models and recently proposed compositionality inducing strategies, and find that their improvements were hugely overestimated, suggesting that more innovation is needed in this important direction.
We release $\\\\textit{SugarCrepe}$ and the code for evaluation at: https://github.com/RAIVNLab/sugar-crepe.",Datasets & Benchmarks,NeurIPS,2023,Poster,Cheng-Yu Hsieh;Jieyu Zhang;Zixian Ma;Aniruddha Kembhavi;Ranjay Krishna,True,https://openreview.net/pdf?id=Jsc7WSCZd4
KRBoWULo2w,PUG: Photorealistic and Semantically Controllable Synthetic Data for Representation Learning,"Synthetic image datasets offer unmatched advantages for designing and evaluating deep neural networks: they make it possible to (i) render as many data samples as needed, (ii) precisely control each scene and yield granular ground truth labels (and captions), (iii) precisely control distribution shifts between training and testing to isolate variables of interest for sound experimentation.
Despite such promise, the use of synthetic image data is still limited -- and often played down -- mainly due to their lack of realism. Most works therefore rely on datasets of real images, which have often been scraped from public images on the internet, and may have issues with regards to privacy, bias, and copyright, while offering little control over how objects precisely appear.
In this work, we present a path to democratize the use of photorealistic synthetic data: we develop a new generation of interactive environments for representation learning research, that offer both controllability and realism. 
We use the Unreal Engine, a powerful game engine well known in the entertainment industry, to produce PUG (Photorealistic Unreal Graphics) environments and datasets for representation learning. Using PUG for evaluation and fine-tuning, we demonstrate the potential of PUG to both enable more rigorous evaluations and to improve model training.",Datasets & Benchmarks,NeurIPS,2023,Poster,Florian Bordes;Shashank Shekhar;Mark Ibrahim;Diane Bouchacourt;Pascal Vincent;Ari S. Morcos,True,https://openreview.net/pdf?id=KRBoWULo2w
KZjSvE2mJz,MLFMF: Data Sets for Machine Learning for Mathematical Formalization,"We introduce MLFMF, a collection of data sets for benchmarking recommendation systems used to support formalization of mathematics with proof assistants. These systems help humans identify which previous entries (theorems, constructions, datatypes, and postulates) are relevant in proving a new theorem or carrying out a new construction. Each data set is derived from a library of formalized mathematics written in proof assistants Agda or Lean. The collection includes the largest Lean 4 library Mathlib, and some of the largest Agda libraries: the standard library, the library of univalent mathematics Agda-unimath, and the TypeTopology library. Each data set represents the corresponding library in two ways: as a heterogeneous network, and as a list of s-expressions representing the syntax trees of all the entries in the library. The network contains the (modular) structure of the library and the references between entries, while the s-expressions give complete and easily parsed information about every entry.
We report baseline results using standard graph and word embeddings, tree ensembles, and instance-based learning algorithms. The MLFMF data sets provide solid benchmarking support for further investigation of the numerous machine learning approaches to formalized mathematics. The methodology used to extract the networks and the s-expressions readily applies to other libraries, and is applicable to other proof assistants. With more than $250\\\\,000$ entries in total, this is currently the largest collection of formalized mathematical knowledge in machine learnable format.",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=KZjSvE2mJz
KexMPvrFgJ,Learning Depth-regularized Radiance Fields from Asynchronous RGB-D Sequences,"Recently it is shown that learning radiance fields with depth rendering and depth supervision can effectively promote the view synthesis quality and convergence. But this paradigm requires input RGB-D sequences to be synchronized, hindering its usage in the UAV city modeling scenario. To this end, we propose to jointly learn large-scale depth-regularized radiance fields and calibrate the mismatch between RGB-D frames. Although this joint learning problem can be simply addressed by adding new variables, we exploit the prior that RGB-D frames are actually sampled from the same physical trajectory. As such, we propose a novel time-pose function, which is an implicit network that maps timestamps to SE(3) elements. Our algorithm is designed in an alternative way consisting of three steps: (1) time-pose function fitting; (2) radiance field bootstrapping; (3) joint pose error compensation and radiance field refinement. In order to systematically evaluate under this new problem setting, we propose a large synthetic dataset with diverse controlled mismatch and ground truth. Through extensive experiments, we demonstrate that our method outperforms strong baselines. We also show qualitatively improved results on a real-world asynchronous RGB-D sequence captured by drones. Codes, data, and models will be made publicly available.",main,NeurIPS,2023,Reject,Zirui Wu;Yuantao Chen;Runyi Yang;Zhenxin Zhu;Chao Hou;Yongliang Shi;Hao Zhao;Guyue Zhou,True,https://openreview.net/pdf?id=KexMPvrFgJ
KgqucdSwIe,VoxDet: Voxel Learning for Novel Instance Detection,"Detecting unseen instances based on multi-view templates is a challenging problem due to its open-world nature. Traditional methodologies, which primarily rely on $2 \\\\mathrm{D}$ representations and matching techniques, are often inadequate in handling pose variations and occlusions. To solve this, we introduce VoxDet, a pioneer 3D geometry-aware framework that fully utilizes the strong 3D voxel representation and reliable voxel matching mechanism. VoxDet first ingeniously proposes template voxel aggregation (TVA) module, effectively transforming multi-view 2D images into 3D voxel features. By leveraging associated camera poses, these features are aggregated into a compact 3D template voxel. In novel instance detection, this voxel representation demonstrates heightened resilience to occlusion and pose variations. We also discover that a $3 \\\\mathrm{D}$ reconstruction objective helps to pre-train the 2D-3D mapping in TVA. Second, to quickly align with the template voxel, VoxDet incorporates a Query Voxel Matching (QVM) module. The 2D queries are first converted into their voxel representation with the learned 2D-3D mapping. We find that since the 3D voxel representations encode the geometry, we can first estimate the relative rotation and then compare the aligned voxels, leading to improved accuracy and efficiency. In addition to method, we also introduce the first instance detection benchmark, RoboTools, where 20 unique instances are video-recorded with camera extrinsic. RoboTools also provides 24 challenging cluttered scenarios with more than $9 \\\\mathrm{k}$ box annotations. Exhaustive experiments are conducted on the demanding LineMod-Occlusion, YCB-video, and RoboTools benchmarks, where VoxDet outperforms various $2 \\\\mathrm{D}$ baselines remarkably with faster speed. To the best of our knowledge, VoxDet is the first to incorporate implicit 3D knowledge for 2D novel instance detection tasks.",main,NeurIPS,2023,Spotlight,Bowen Li;Jiashun Wang;Yaoyu Hu;Chen Wang;Sebastian Scherer,True,https://openreview.net/pdf?id=KgqucdSwIe
Kn6VRkYqYk,The Drunkard’s Odometry: Estimating Camera Motion in Deforming Scenes,"Estimating camera motion in deformable scenes poses a complex and open research challenge. Most existing non-rigid structure from motion techniques assume to observe also static scene parts besides deforming scene parts in order to establish an anchoring reference. However, this assumption does not hold true in certain relevant application cases such as endoscopies. Deformable odometry and SLAM pipelines, which tackle the most challenging scenario of exploratory trajectories, suffer from a lack of robustness and proper quantitative evaluation methodologies. To tackle this issue with a common benchmark, we introduce the Drunkard's Dataset, a challenging collection of synthetic data targeting visual navigation and reconstruction in deformable environments. This dataset is the first large set of exploratory camera trajectories with ground truth inside 3D scenes where every surface exhibits non-rigid deformations over time. Simulations in realistic 3D buildings lets us obtain a vast amount of data and ground truth labels, including camera poses, RGB images and depth, optical flow and normal maps at high resolution and quality. We further present a novel deformable odometry method, dubbed the Drunkard’s Odometry, which decomposes optical flow estimates into rigid-body camera motion and non-rigid scene deformations. In order to validate our data, our work contains an evaluation of several baselines as well as a novel tracking error metric which does not require ground truth data. Dataset and code: https://davidrecasens.github.io/TheDrunkard'sOdometry/",Datasets & Benchmarks,NeurIPS,2023,Poster,David Recasens;Martin R. Oswald;Marc Pollefeys;Javier Civera,True,https://openreview.net/pdf?id=Kn6VRkYqYk
L9I9FhHfS3,Consensus and Subjectivity of Skin Tone Annotation for ML Fairness,"Understanding different human attributes and how they affect model behavior may become a standard need for all model creation and usage, from traditional computer vision tasks to the newest multimodal generative AI systems. In computer vision specifically, we have relied on datasets augmented with perceived attribute signals (eg, gender presentation, skin tone, and age) and benchmarks enabled by these datasets. Typically labels for these tasks come from human annotators. However, annotating attribute signals, especially skin tone, is a difficult and subjective task. Perceived skin tone is affected by technical factors, like lighting conditions, and social factors that shape an annotator's lived experience.
This paper examines the subjectivity of skin tone annotation through a series of annotation experiments using the Monk Skin Tone (MST) scale, a small pool of professional photographers, and a much larger pool of trained crowdsourced annotators. Along with this study we release the Monk Skin Tone Examples (MST-E) dataset, containing 1515 images and 31 videos spread across the full MST scale. MST-E is designed to help train human annotators to annotate MST effectively.Our study shows that annotators can reliably annotate skin tone in a way that aligns with an expert in the MST scale, even under challenging environmental conditions. We also find evidence that annotators from different geographic regions rely on different mental models of MST categories resulting in annotations that systematically vary across regions. Given this, we advise practitioners to use a diverse set of annotators and a higher replication count for each image when annotating skin tone for fairness research.",Datasets & Benchmarks,NeurIPS,2023,Poster,Candice Schumann;Gbolahan Oluwafemi Olanubi;Auriel Wright;Ellis Monk;Courtney Heldreth;Susanna Ricco,True,https://openreview.net/pdf?id=L9I9FhHfS3
LE4AN1FGjJ,Degraded Polygons Raise Fundamental Questions of Neural Network Perception,"It is well-known that modern computer vision systems often exhibit behaviors misaligned with those of humans: from adversarial attacks to image corruptions, deep
learning vision models suffer in a variety of settings that humans capably handle. In
light of these phenomena, here we introduce another, orthogonal perspective studying the human-machine vision gap. We revisit the task of recovering images under
degradation, first introduced over 30 years ago in the Recognition-by-Components
theory of human vision. Specifically, we study the performance and behavior of
neural networks on the seemingly simple task of classifying regular polygons at
varying orders of degradation along their perimeters. To this end, we implement the
Automated Shape Recoverability Test
for rapidly generating large-scale datasets
of perimeter-degraded regular polygons, modernizing the historically manual creation of image recoverability experiments. We then investigate the capacity of
neural networks to recognize and recover such degraded shapes when initialized
with different priors. Ultimately, we find that neural networks’ behavior on this
simple task conflicts with human behavior, raising a fundamental question of the
robustness and learning capabilities of modern computer vision models",Datasets & Benchmarks,NeurIPS,2023,Poster,Leonard Tang;Dan Ley,True,https://openreview.net/pdf?id=LE4AN1FGjJ
LJhfKeqZdu,RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark,"We introduce RL4CO, an extensive reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO employs state-of-the-art software libraries as well as best practices in implementation, such as modularity and configuration management, to be efficient and easily modifiable by researchers for adaptations of neural network architecture, environments, and RL algorithms. Contrary to the existing focus on specific tasks like the traveling salesman problem (TSP) for performance assessment, we underline the importance of scalability and generalization capabilities for diverse CO tasks. We also systematically benchmark zero-shot generalization, sample efficiency, and adaptability to changes in data distributions of various models. Our experiments show that some recent state-of-the-art methods fall behind their predecessors when evaluated using these metrics, suggesting the necessity for a more balanced view of the performance of neural CO (NCO) solvers. We hope RL4CO will encourage the exploration of novel solutions to complex real-world tasks, allowing the NCO community to compare with existing methods through a standardized interface that decouples the science from software engineering. We make our library publicly available at https://github.com/kaist-silab/rl4co.",Datasets & Benchmarks,NeurIPS,2023,Reject,Federico Berto;Chuanbo Hua;Junyoung Park;Minsu Kim;Hyeonah Kim;Jiwoo Son;Haeyeon Kim;Joungho Kim;Jinkyoo Park,False,https://openreview.net/pdf?id=LJhfKeqZdu
LaFKTgrZMG,DataPerf: Benchmarks for Data-Centric AI Development,"Machine learning research has long focused on models rather than datasets, and prominent datasets are used for common ML tasks without regard to the breadth, difficulty, and faithfulness of the underlying problems. Neglecting the fundamental importance of data has given rise to inaccuracy, bias, and fragility in real-world applications, and research is hindered by saturation across existing dataset benchmarks. In response, we present DataPerf, a community-led benchmark suite for evaluating ML datasets and data-centric algorithms. We aim to foster innovation in data-centric AI through competition, comparability, and reproducibility. We enable the ML community to iterate on datasets, instead of just architectures, and we provide an open, online platform with multiple rounds of challenges to support this iterative development. The first iteration of DataPerf contains five benchmarks covering a wide spectrum of data-centric techniques, tasks, and modalities in vision, speech, acquisition, debugging, and diffusion prompting, and we support hosting new contributed benchmarks from the community. The benchmarks, online evaluation platform, and baseline implementations are open source, and the MLCommons Association will maintain DataPerf to ensure long-term benefits to academia and industry.",Datasets & Benchmarks,NeurIPS,2023,Poster,Mark Mazumder;Colby Banbury;Xiaozhe Yao;Bojan Karlaš;William A Gaviria Rojas;Sudnya Diamos;Greg Diamos;Lynn He;Alicia Parrish;Hannah Rose Kirk;Jessica Quaye;Charvi Rastogi;Douwe Kiela;David Jurado;David Kanter;Rafael Mosquera;Will Cukierski;Juan Ciro;Lora Aroyo;Bilge Acun;Lingjiao Chen;Mehul Smriti Raje;Max Bartolo;Sabri Eyuboglu;Amirata Ghorbani;Emmett Daniel Goodman;Addison Howard;Oana Inel;Tariq Kane;Christine Kirkpatrick;D. Sculley;Tzu-Sheng Kuo;Jonas Mueller;Tristan Thrush;Joaquin Vanschoren;Margaret Warren;Adina Williams;Serena Yeung;Newsha Ardalani;Praveen Paritosh;Ce Zhang;James Y. Zou;Carole-Jean Wu;Cody Coleman;Andrew Ng;Peter Mattson;Vijay Janapa Reddi,False,https://openreview.net/pdf?id=LaFKTgrZMG
LegGqdch92,FLAIR : a Country-Scale Land Cover Semantic Segmentation Dataset From Multi-Source Optical Imagery,"We introduce the French Land cover from Aerospace ImageRy (FLAIR), an extensive dataset from the French National Institute of Geographical and Forest Information (IGN) that provides a unique and rich resource for large-scale geospatial analysis. FLAIR contains high-resolution aerial imagery with a ground sample distance of 20 cm and over 20 billion individually labeled pixels for precise land-cover classification. The dataset also integrates temporal and spectral data from optical satellite time series. 

FLAIR thus combines data with varying spatial, spectral, and temporal resolutions across over 817 km² of acquisitions representing the full landscape diversity of France. This diversity makes FLAIR a valuable resource for the development and evaluation of novel methods for large-scale land-cover semantic segmentation and raises significant challenges in terms of computer vision, data fusion, and geospatial analysis. We also provide powerful uni- and multi-sensor baseline models that can be employed to assess algorithm's performance and for downstream applications.",Datasets & Benchmarks,NeurIPS,2023,Poster,Anatol Garioud;Nicolas Gonthier;Loic Landrieu;Apolline De Wit;Marion Valette;Marc Poupée;Sebastien Giordano;Boris Wattrelos,True,https://openreview.net/pdf?id=LegGqdch92
Lr2swAfwff,Bridging RL Theory and Practice with the Effective Horizon,"Deep reinforcement learning (RL) works impressively in some environments and fails catastrophically in others. Ideally, RL theory should be able to provide an understanding of why this is, i.e. bounds predictive of practical performance. Unfortunately, current theory does not quite have this ability. We compare standard deep RL algorithms to prior sample complexity bounds by introducing a new dataset, BRIDGE. It consists of 155 MDPs from common deep RL benchmarks, along with their corresponding tabular representations, which enables us to exactly compute instance-dependent bounds. We find that prior bounds do not correlate well with when deep RL succeeds vs. fails, but discover a surprising property that does. When actions with the highest Q-values under the *random* policy also have the highest Q-values under the *optimal* policy—i.e., when it is optimal to act greedily with respect to the random's policy Q function—deep RL tends to succeed; when they don't, deep RL tends to fail. We generalize this property into a new complexity measure of an MDP that we call the *effective horizon*, which roughly corresponds to how many steps of lookahead search would be needed in that MDP in order to identify the next optimal action, when leaf nodes are evaluated with random rollouts. Using BRIDGE, we show that the effective horizon-based bounds are more closely reflective of the empirical performance of PPO and DQN than prior sample complexity bounds across four metrics. We also show that, unlike existing bounds, the effective horizon can predict the effects of using reward shaping or a pre-trained exploration policy. Our code and data are available at https://github.com/cassidylaidlaw/effective-horizon.",main,NeurIPS,2023,Oral,Cassidy Laidlaw;Stuart Russell;Anca Dragan,True,https://openreview.net/pdf?id=Lr2swAfwff
Luc1bZLeMY,CHAMMI: A benchmark for channel-adaptive models in microscopy imaging,"Most neural networks assume that input images have a fixed number of channels (three for RGB images). However, there are many settings where the number of channels may vary, such as microscopy images where the number of channels changes depending on instruments and experimental goals. Yet, there has not been a systemic attempt to create and evaluate neural networks that are invariant to the number and type of channels. As a result, trained models remain specific to individual studies and are hardly reusable for other microscopy settings. In this paper, we present a benchmark for investigating channel-adaptive models in microscopy imaging, which consists of 1) a dataset of varied-channel single-cell images, and 2) a biologically relevant evaluation framework. In addition, we adapted several existing techniques to create channel-adaptive models and compared their performance on this benchmark to fixed-channel, baseline models. We find that channel-adaptive models can generalize better to out-of-domain tasks and can be computationally efficient. We contribute a curated dataset and an evaluation API to facilitate objective comparisons in future research and applications.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zitong Chen;Chau Pham;Siqi Wang;Michael Doron;Nikita Moshkov;Bryan A. Plummer;Juan C Caicedo,True,https://openreview.net/pdf?id=Luc1bZLeMY
MCVfX7HgPO,Testing the General Deductive Reasoning Capacity of Large Language Models Using OOD Examples,"Given the intractably large size of the space of proofs, any model that is capable of general deductive reasoning must generalize to proofs of greater complexity. Recent studies have shown that large language models (LLMs) possess some abstract deductive reasoning ability given chain-of-thought prompts. However, they have primarily been tested on proofs using modus ponens or of a specific size, and from the same distribution as the in-context examples. To measure the general deductive reasoning ability of LLMs, we test on a broad set of deduction rules and measure their ability to generalize to more complex proofs from simpler demonstrations from multiple angles: depth-, width-, and compositional generalization. To facilitate systematic exploration, we construct a new synthetic and programmable reasoning dataset that enables control over deduction rules and proof complexity. Our experiments on four LLMs of various sizes and training objectives show that they are able to generalize to compositional proofs. However, they have difficulty generalizing to longer proofs, and they require explicit demonstrations to produce hypothetical subproofs, specifically in proof by cases and proof by contradiction.",main,NeurIPS,2023,Poster,Abulhair Saparov;Richard Yuanzhe Pang;Vishakh Padmakumar;Nitish Joshi;Mehran Kazemi;Najoung Kim;He He,True,https://openreview.net/pdf?id=MCVfX7HgPO
MEa0cQeURw,NeuroGraph: Benchmarks for Graph Machine Learning in Brain Connectomics,"Machine learning provides a valuable tool for analyzing high-dimensional functional neuroimaging data, and is proving effective in predicting various neurological conditions, psychiatric disorders, and cognitive patterns. In functional magnetic resonance imaging (MRI) research, interactions between brain regions are commonly modeled using graph-based representations. The potency of graph machine learning methods has been established across myriad domains, marking a transformative step in data interpretation and predictive modeling. Yet, despite their promise, the transposition of these techniques to the neuroimaging domain has been  challenging due to the expansive number of potential preprocessing pipelines and the large parameter search space for graph-based dataset construction. In this paper, we introduce NeuroGraph, a collection of graph-based neuroimaging datasets, and demonstrated its utility for predicting multiple categories of behavioral and cognitive traits. We delve deeply into the dataset generation search space by crafting 35 datasets that encompass static and dynamic brain connectivity, running in excess of 15 baseline methods for benchmarking. Additionally, we provide generic frameworks for learning on both static and dynamic graphs. Our extensive experiments lead to several key observations. Notably, using correlation vectors as node features, incorporating larger number of regions of interest, and employing sparser graphs lead to improved performance. To foster further advancements in graph-based data driven neuroimaging analysis, we offer a comprehensive open-source Python package that includes the benchmark datasets, baseline implementations, model training, and standard evaluation.",Datasets & Benchmarks,NeurIPS,2023,Poster,Anwar Said;Roza G Bayrak;Tyler Derr;Mudassir Shabbir;Daniel Moyer;Catie Chang;Xenofon D. Koutsoukos,True,https://openreview.net/pdf?id=MEa0cQeURw
MLLp6AHQFs,LOVM: Language-Only Vision Model Selection,"Pre-trained multi-modal vision-language models (VLMs) are becoming increasingly popular due to their exceptional performance on downstream vision applications, particularly in the few- and zero-shot settings. However, selecting the best-performing VLM for some downstream applications is non-trivial, as it is dataset and task-dependent. Meanwhile, the exhaustive evaluation of all available VLMs on a novel application is not only time and  computationally demanding but also necessitates the collection of a labeled dataset for evaluation. As the number of open-source VLM variants increases, there is a need for an efficient model selection strategy that does not require access to a curated evaluation dataset. This paper proposes a novel task and benchmark for efficiently evaluating VLMs' zero-shot performance on downstream applications without access to the downstream task dataset. Specifically, we introduce a new task LOVM: **L**anguage-**O**nly  **V**ision  **M**odel Selection , where methods are expected to perform both model selection and performance prediction based solely on a text description of the desired downstream application. We then introduced an extensive LOVM benchmark consisting of ground-truth evaluations of 35 pre-trained VLMs and 23 datasets, where methods are expected to rank the pre-trained VLMs and predict their zero-shot performance.",Datasets & Benchmarks,NeurIPS,2023,Poster,Orr Zohar;Shih-Cheng Huang;Kuan-Chieh Wang;Serena Yeung,False,https://openreview.net/pdf?id=MLLp6AHQFs
MZopld6S22,Benchmarking and Analyzing 3D-aware Image Synthesis with a Modularized Codebase,"Despite the rapid advance of 3D-aware image synthesis, existing studies usually adopt a mixture of techniques and tricks, leaving it unclear how each part contributes to the final performance in terms of generality. Following the most popular and effective paradigm in this field, which incorporates a neural radiance field (NeRF) into the generator of a generative adversarial network (GAN), we build
a well-structured codebase through modularizing the generation process. Such a design allows researchers to develop and replace each module independently, and hence offers an opportunity to fairly compare various approaches and recognize their contributions from the module perspective. The reproduction of a range of cutting-edge algorithms demonstrates the availability of our modularized codebase. We also perform a variety of in-depth analyses, such as the comparison across different types of point feature, the necessity of the tailing upsampler in the generator, the reliance on the camera pose prior, etc., which deepen our understanding of existing methods and point out some further directions of the research work. Code and models will be made publicly available to facilitate the development and evaluation of this field.",Datasets & Benchmarks,NeurIPS,2023,Poster,Qiuyu Wang;Zifan Shi;Kecheng Zheng;Yinghao Xu;Sida Peng;Yujun Shen,False,https://openreview.net/pdf?id=MZopld6S22
McAS4XoZJP,Alexa Arena: A User-Centric Interactive Platform for Embodied AI,"We introduce Alexa Arena, a user-centric simulation platform to facilitate research in building assistive conversational embodied agents. Alexa Arena features multi-room layouts and an abundance of interactable objects. With user-friendly graphics and control mechanisms, the platform supports the development of gamified robotic tasks readily accessible to general human users, allowing high-efficiency data collection and EAI system evaluation. Along with the platform, we introduce a dialog-enabled task completion benchmark with online human evaluations.",Datasets & Benchmarks,NeurIPS,2023,Poster,Qiaozi Gao;Govind Thattai;Suhaila Shakiah;Xiaofeng Gao;Shreyas Pansare;Vasu Sharma;Gaurav S. Sukhatme;Hangjie Shi;Bofei Yang;Desheng Zhang;Lucy Hu;Karthika Arumugam;Shui Hu;Matthew Wen;Dinakar Venkateswar Guthy;Shunan Cadence Chung;Rohan Khanna;Osman Ipek;Leslie Ball;Kate Bland;Heather Rocker;Michael Johnston;Reza Ghanadan;Dilek Hakkani-Tur;Prem Natarajan,False,https://openreview.net/pdf?id=McAS4XoZJP
MfhJWSp3Ea,AircraftVerse: A Large-Scale Multimodal Dataset of Aerial Vehicle Designs,"We present AircraftVerse, a publicly available aerial vehicle design dataset. Aircraft design encompasses different physics domains and, hence, multiple modalities of representation. The evaluation of these designs requires the use of scientific analytical and simulation models ranging from computer-aided design tools for structural and manufacturing analysis, computational fluid dynamics tools for drag and lift computation, battery models for energy estimation, and simulation models for flight control and dynamics. AircraftVerse contains $27{,}714$  diverse air vehicle designs - the largest corpus of designs with this level of complexity. Each design comprises the following artifacts: a symbolic design tree describing topology, propulsion subsystem, battery subsystem, and other design details; a STandard for the Exchange of Product (STEP) model data; a 3D CAD design using a stereolithography (STL) file format; a 3D point cloud for the shape of the design; and evaluation results from high fidelity state-of-the-art physics models that characterize performance metrics such as maximum flight distance and hover-time. We also present baseline surrogate models that use different modalities of design representation to predict design performance metrics, which we provide as part of our dataset release. Finally, we discuss the potential impact of this dataset on the use of learning in aircraft design, and more generally, in the emerging field of deep learning for scientific design. AircraftVerse is accompanied by a datasheet as suggested in the recent literature, and it is released under Creative Commons Attribution-ShareAlike (CC BY-SA) license. The dataset with baseline models are hosted at http://doi.org/10.5281/zenodo.6525446, code at https://github.com/SRI-CSL/AircraftVerse, and the dataset description at  https://uavdesignverse.onrender.com/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Adam D. Cobb;Anirban Roy;Daniel Elenius;Frederick Michael Heim;Brian Swenson;Sydney Whittington;James D Walker;Theodore Bapty;Joseph Hite;Karthik Ramani;Christopher McComb;Susmit Jha,True,https://openreview.net/pdf?id=MfhJWSp3Ea
Mn9oHNdYCE,XES3G5M: A Knowledge Tracing Benchmark Dataset with Auxiliary Information,"Knowledge tracing (KT) is a task that predicts students' future performance based on their historical learning interactions. With the rapid development of deep learning techniques, existing KT approaches follow a data-driven paradigm that uses massive problem-solving records to model students' learning processes. However, although the educational contexts contain various factors that may have an influence on student learning outcomes, existing public KT datasets mainly consist of anonymized ID-like features, which may hinder the research advances towards this field. Therefore, in this work, we present, \\\\emph{XES3G5M}, a large-scale dataset with rich auxiliary information about questions and their associated knowledge components (KCs)\\\\footnote{\\\\label{ft:kc}A KC is a generalization of everyday terms like concept, principle, fact, or skill.}. The XES3G5M dataset is collected from a real-world online math learning platform, which contains 7,652 questions, and 865 KCs with 5,549,635 interactions from 18,066 students. To the best of our knowledge, the XES3G5M dataset not only has the largest number of KCs in math domain but contains the richest contextual information including tree structured KC relations, question types, textual contents and analysis and student response timestamps. Furthermore, we build a comprehensive benchmark on 19 state-of-the-art deep learning based knowledge tracing (DLKT) models. Extensive experiments demonstrate the effectiveness of leveraging the auxiliary information in our XES3G5M with DLKT models. We hope the proposed dataset can effectively facilitate the KT research work.",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=Mn9oHNdYCE
MtekhXRP4h,Convolutional Neural Operators for robust and accurate learning of PDEs,"Although very successfully used in conventional machine learning, convolution based neural network architectures -- believed to be inconsistent in function space -- have been largely ignored in the context of learning solution operators of PDEs. Here, we present novel adaptations for convolutional neural networks to demonstrate that they are indeed able to process functions as inputs and outputs. The resulting architecture, termed as convolutional neural operators (CNOs), is designed specifically to preserve its underlying continuous nature, even when implemented in a discretized form on a computer. We prove a universality theorem to show that CNOs can approximate operators arising in PDEs to desired accuracy. CNOs are tested on a novel suite of benchmarks, encompassing a diverse set of PDEs with multi-scale solutions and are observed to significantly outperform baselines, paving the way for an alternative framework for robust and accurate operator learning.",main,NeurIPS,2023,Poster,Bogdan Raonic;Roberto Molinaro;Tim De Ryck;Tobias Rohner;Francesca Bartolucci;Rima Alaifari;Siddhartha Mishra;Emmanuel de Bezenac,True,https://openreview.net/pdf?id=MtekhXRP4h
MzZcXPeqcU,CORL: Research-oriented Deep Offline Reinforcement Learning Library,"CORL is an open-source library that provides thoroughly benchmarked single-file implementations of both deep offline and offline-to-online reinforcement learning algorithms. It emphasizes a simple developing experience with a straightforward codebase and a modern analysis tracking tool. In CORL, we isolate methods implementation into separate single files, making performance-relevant details easier to recognize. Additionally, an experiment tracking feature is available to help log metrics, hyperparameters, dependencies, and more to the cloud. Finally, we have ensured the reliability of the implementations by benchmarking commonly employed D4RL datasets providing a transparent source of results that can be reused for robust evaluation tools such as performance profiles, probability of improvement, or expected online performance.",Datasets & Benchmarks,NeurIPS,2023,Poster,Denis Tarasov;Alexander Nikulin;Dmitry Akimov;Vladislav Kurenkov;Sergey Kolesnikov,False,https://openreview.net/pdf?id=MzZcXPeqcU
NYwbmCrrni,Disambiguated Attention Embedding for Multi-Instance Partial-Label Learning,"In many real-world tasks, the concerned objects can be represented as a multi-instance bag associated with a candidate label set, which consists of one ground-truth label and several false positive labels. Multi-instance partial-label learning (MIPL) is a learning paradigm to deal with such tasks and has achieved favorable performances. Existing MIPL approach follows the instance-space paradigm by assigning augmented candidate label sets of bags to each instance and aggregating bag-level labels from instance-level labels. However, this scheme may be suboptimal as global bag-level information is ignored and the predicted labels of bags are sensitive to predictions of negative instances. In this paper, we study an alternative scheme where a multi-instance bag is embedded into a single vector representation. Accordingly, an intuitive algorithm named DEMIPL, i.e., Disambiguated attention Embedding for Multi-Instance Partial-Label learning, is proposed. DEMIPL employs a disambiguation attention mechanism to aggregate a multi-instance bag into a single vector representation, followed by a momentum-based disambiguation strategy to identify the ground-truth label from the candidate label set. Furthermore, we introduce a real-world MIPL dataset for colorectal cancer classification. Experimental results on benchmark and real-world datasets validate the superiority of DEMIPL against the compared MIPL and partial-label learning approaches.",main,NeurIPS,2023,Poster,Wei Tang;Weijia Zhang;Min-Ling Zhang,True,https://openreview.net/pdf?id=NYwbmCrrni
NoE8g3LRAM,Benchmarking Encoder-Decoder Architectures for Biplanar X-ray to 3D Bone Shape Reconstruction,"Various deep learning models have been proposed for 3D bone shape reconstruction from two orthogonal (biplanar) X-ray images.
However, it is unclear how these models compare against each other since they are evaluated on different anatomy, cohort and (often privately held) datasets.
Moreover, the impact of the commonly optimized image-based segmentation metrics such as dice score on the estimation of clinical parameters relevant in 2D-3D bone shape reconstruction is not well known.
To move closer toward clinical translation, we propose a benchmarking framework that evaluates tasks relevant to real-world clinical scenarios, including reconstruction of fractured bones, bones with implants, robustness to population shift, and error in estimating clinical parameters.
Our open-source platform provides reference implementations of 8 models (many of whose implementations were not publicly available), APIs to easily collect and preprocess 6 public datasets, and the implementation of automatic clinical parameter and landmark extraction methods. 
We present an extensive evaluation of 8 2D-3D models on equal footing using 6 public datasets comprising images for four different anatomies.
Our results show that attention-based methods that capture global spatial relationships tend to perform better across all anatomies and datasets; performance on clinically relevant subgroups may be overestimated without disaggregated reporting; ribs are substantially more difficult to reconstruct compared to femur, hip and spine; and the dice score improvement does not always bring corresponding improvement in the automatic estimation of clinically relevant parameters.",Datasets & Benchmarks,NeurIPS,2023,Poster,Mahesh Shakya;Bishesh Khanal,False,https://openreview.net/pdf?id=NoE8g3LRAM
O06z2G18me,Evaluating the Moral Beliefs Encoded in LLMs,"This paper presents a case study on the design, administration, post-processing,  and evaluation of surveys on large language models (LLMs). It comprises two components:
(1) A statistical method for eliciting beliefs encoded in LLMs. We introduce statistical measures and evaluation metrics that quantify the probability of an LLM ""making a choice"", the associated uncertainty, and the consistency of that choice.
(2) We apply this method to study what moral beliefs are encoded in different LLMs, especially in ambiguous cases where the right choice is not obvious.
We design a large-scale survey comprising 680 high-ambiguity moral scenarios (e.g., ""Should I tell a white lie?"") and 687 low-ambiguity moral scenarios (e.g., ""Should I stop for a pedestrian on the road?""). Each scenario includes a description, two possible actions, and auxiliary labels indicating violated rules (e.g., ""do not kill""). We administer the survey to 28 open- and closed-source LLMs.
We find that (a) in unambiguous scenarios, most models ``choose"" actions that align with commonsense. In ambiguous cases, most models express uncertainty.
(b) Some models are uncertain about choosing the commonsense action because their responses are sensitive to the question-wording.
(c) Some models reflect clear preferences in ambiguous scenarios. Specifically, closed-source models tend to agree with each other.",main,NeurIPS,2023,Spotlight,Nino Scherrer;Claudia Shi;Amir Feder;David Blei,True,https://openreview.net/pdf?id=O06z2G18me
OB10WTlwmX,Evaluating and Improving Tool-Augmented Computation-Intensive Math Reasoning,"Chain-of-thought prompting (CoT) and tool augmentation have been validated in recent work as effective practices for improving large language models (LLMs) to perform step-by-step reasoning on complex math-related tasks.
However, most existing math reasoning datasets may not be able to fully evaluate and analyze the ability of LLMs in manipulating tools and performing reasoning, as they often only require very few invocations of tools or miss annotations for evaluating intermediate reasoning steps, thus supporting only outcome evaluation.
To address the issue, we construct **CARP**, a new Chinese dataset consisting of 4,886 computation-intensive algebra problems with formulated annotations on intermediate steps, facilitating the evaluation of the intermediate reasoning process.
In CARP, we test four LLMs with CoT prompting, and find that they are all prone to make mistakes at the early steps of the solution, leading to incorrect answers.
Based on this finding, we propose a new approach that can facilitate the deliberation on reasoning steps with tool interfaces, namely **DELI**.
In DELI, we first initialize a step-by-step solution based on retrieved exemplars, then iterate two deliberation procedures that check and refine the intermediate steps of the generated solution, from both tool manipulation and natural language reasoning perspectives, until solutions converge or the maximum iteration is achieved.
Experimental results on CARP and six other datasets show that the proposed DELI mostly outperforms competitive baselines, and can further boost the performance of existing CoT methods.
Our data and code are available at https://github.com/RUCAIBox/CARP.",Datasets & Benchmarks,NeurIPS,2023,Poster,Beichen Zhang;Kun Zhou;Xilin Wei;Xin Zhao;Jing Sha;Shijin Wang;Ji-Rong Wen,True,https://openreview.net/pdf?id=OB10WTlwmX
ODB01Fyr4a,Imagine the Unseen World: A Benchmark for Systematic Generalization in Visual World Models,"Systematic compositionality, or the ability to adapt to novel situations by creating a mental model of the world using reusable pieces of knowledge, remains a significant challenge in machine learning. While there has been considerable progress in the language domain, efforts towards systematic visual imagination, or envisioning the dynamical implications of a visual observation, are in their infancy. We introduce the Systematic Visual Imagination Benchmark (SVIB), the first benchmark designed to address this problem head-on. SVIB offers a novel framework for a minimal world modeling problem, where models are evaluated based on their ability to generate one-step image-to-image transformations under a latent world dynamics. The framework provides benefits such as the possibility to jointly optimize for systematic perception and imagination, a range of difficulty levels, and the ability to control the fraction of possible factor combinations used during training. We provide a comprehensive evaluation of various baseline models on SVIB, offering insight into the current state-of-the-art in systematic visual imagination. We hope that this benchmark will help advance visual systematic compositionality.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yeongbin Kim;Gautam Singh;Junyeong Park;Caglar Gulcehre;Sungjin Ahn,True,https://openreview.net/pdf?id=ODB01Fyr4a
OHimIaixXk,Building the Bridge of Schrödinger: A Continuous Entropic Optimal Transport Benchmark,"Over the last several years, there has been significant progress in developing neural solvers for the Schrödinger Bridge (SB) problem and applying them to generative modelling. This new research field is justifiably fruitful as it is interconnected with the practically well-performing diffusion models and theoretically grounded entropic optimal transport (EOT). Still, the area lacks non-trivial tests allowing a researcher to understand how well the methods solve SB or its equivalent continuous EOT problem. We fill this gap and propose a novel way to create pairs of probability distributions for which the ground truth OT solution is known by the construction. Our methodology is generic and works for a wide range of OT formulations, in particular, it covers the EOT which is equivalent to SB (the main interest of our study). This development allows us to create continuous benchmark distributions with the known EOT and SB solutions on high-dimensional spaces such as spaces of images. As an illustration, we use these benchmark pairs to test how well existing neural EOT/SB solvers actually compute the EOT solution. Our code for constructing benchmark pairs under different setups is available at: https://github.com/ngushchin/EntropicOTBenchmark",Datasets & Benchmarks,NeurIPS,2023,Poster,Nikita Gushchin;Alexander Kolesov;Petr Mokrov;Polina Karpikova;Andrei Spiridonov;Evgeny Burnaev;Alexander Korotin,True,https://openreview.net/pdf?id=OHimIaixXk
OL2JQoO0kq,Quilt-1M: One Million Image-Text Pairs for Histopathology,"Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has slowed comparable progress. 
To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering $1,087$ hours of valuable educational histopathology videos from expert clinicians.
From YouTube, we curate QUILT: a large-scale vision-language dataset consisting of $802, 144$ image and text pairs.
QUILT was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition.
In comparison, the most comprehensive datasets curated for histopathology amass only around $200$K samples.
We combine QUILT with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even larger dataset: QUILT-1M, with $1$M paired image-text samples, marking it as the largest vision-language histopathology dataset to date. 
We demonstrate the value of QUILT-1M by fine-tuning a pre-trained CLIP model. 
Our model outperforms state-of-the-art models on both zero-shot and linear probing tasks for classifying new histopathology images across $13$ diverse patch-level datasets of $8$ different sub-pathologies and cross-modal retrieval tasks.",Datasets & Benchmarks,NeurIPS,2023,Oral,Wisdom Oluchi Ikezogwo;Mehmet Saygin Seyfioglu;Fatemeh Ghezloo;Dylan Stefan Chan Geva;Fatwir Sheikh Mohammed;Pavan Kumar Anand;Ranjay Krishna;Linda Shapiro,True,https://openreview.net/pdf?id=OL2JQoO0kq
OMOOO3ls6g,OpenLane-V2: A Topology Reasoning Benchmark for Unified 3D HD Mapping,"Accurately depicting the complex traffic scene is a vital component for autonomous vehicles to execute correct judgments. However, existing benchmarks tend to oversimplify the scene by solely focusing on lane perception tasks. Observing that human drivers rely on both lanes and traffic signals to operate their vehicles safely, we present OpenLane-V2, the first dataset on topology reasoning for traffic scene structure. The objective of the presented dataset is to advance research in understanding the structure of road scenes by examining the relationship between perceived entities, such as traffic elements and lanes. Leveraging existing datasets, OpenLane-V2 consists of 2,000 annotated road scenes that describe traffic elements and their correlation to the lanes. It comprises three primary sub-tasks, including the 3D lane detection inherited from OpenLane, accompanied by corresponding metrics to evaluate the model’s performance. We evaluate various state-of-the-art methods, and present their quantitative and qualitative results on OpenLane-V2 to indicate future avenues for investigating topology reasoning in traffic scenes.",Datasets & Benchmarks,NeurIPS,2023,Poster,Huijie Wang;Tianyu Li;Yang Li;Li Chen;Chonghao Sima;Zhenbo Liu;Bangjun Wang;Peijin Jia;Yuting Wang;Shengyin Jiang;Feng Wen;Hang Xu;Ping Luo;Junchi Yan;Wei Zhang;Hongyang Li,True,https://openreview.net/pdf?id=OMOOO3ls6g
OXOLiS0ak6,A Dataset for Analyzing Streaming Media Performance over HTTP/3 Browsers,"HTTP/3 is a new application layer protocol supported by most browsers. It uses QUIC as an underlying transport protocol. QUIC provides multiple benefits, like faster connection establishment, reduced latency, and improved connection migration. Hence, most popular browsers like Chrome/Chromium, Microsoft Edge, Apple Safari, and Mozilla Firefox have started supporting it. In this paper, we present an HTTP/3-supported browser dataset collection tool named H3B. It collects the application and network-level logs during YouTube streaming. We consider YouTube, as it  the most popular video streaming application supporting QUIC. Using this tool, we collected a dataset of over 5936 YouTube sessions covering 5464 hours of streaming over 5 different geographical locations and 5 different bandwidth patterns. We believe our tool and as well as the dataset could be used in multiple applications such as a better configuration of application/transport protocols based on the network conditions, intelligent integration of network and application, predicting YouTube's QoE etc. 
We analyze the dataset and observe that during an HTTP/3 streaming not all requests are served by HTTP/3. Instead whenever the network condition is not favorable the browser chooses to fallback, and the application requests are transmitted using HTTP/2 over the old-standing transport protocol TCP. We observe that such switching of protocols impacts the performance of video streaming applications.",Datasets & Benchmarks,NeurIPS,2023,Poster,Sapna Chaudhary;Mukulika Maity;Sandip Chakraborty;Naval Kumar Shukla,True,https://openreview.net/pdf?id=OXOLiS0ak6
OZ7aImD4uQ,Scale Alone Does not Improve Mechanistic Interpretability in Vision Models,"In light of the recent widespread adoption of AI systems, understanding the internal information processing of neural networks has become increasingly critical. Most recently, machine vision has seen remarkable progress by scaling neural networks to unprecedented levels in dataset and model size. We here ask whether this extraordinary increase in scale also positively impacts the field of mechanistic interpretability. In other words, has our understanding of the inner workings of scaled neural networks improved as well? We use a psychophysical paradigm to quantify one form of mechanistic interpretability for a diverse suite of nine models and find no scaling effect for interpretability - neither for model nor dataset size. Specifically, none of the investigated state-of-the-art models are easier to interpret than the GoogLeNet model from almost a decade ago. Latest-generation vision models appear even less interpretable than older architectures, hinting at a regression rather than improvement, with modern models sacrificing interpretability for accuracy. These results highlight the need for models explicitly designed to be mechanistically interpretable and the need for more helpful interpretability methods to increase our understanding of networks at an atomic level. We release a dataset containing more than 130'000 human responses from our psychophysical evaluation of 767 units across nine models. This dataset facilitates research on automated instead of human-based interpretability evaluations, which can ultimately be leveraged to directly optimize the mechanistic interpretability of models.",main,NeurIPS,2023,Spotlight,Roland S. Zimmermann;Thomas Klein;Wieland Brendel,True,https://openreview.net/pdf?id=OZ7aImD4uQ
Od6CHhPM7I,Red Teaming Deep Neural Networks with Feature Synthesis Tools,"Interpretable AI tools are often motivated by the goal of understanding model behavior in out-of-distribution (OOD) contexts. Despite the attention this area of study receives, there are comparatively few cases where these tools have identified previously unknown bugs in models. We argue that this is due, in part, to a common feature of many interpretability methods: they analyze model behavior by using a particular dataset. This only allows for the study of the model in the context of features that the user can sample in advance. To address this, a growing body of research involves interpreting models using feature synthesis methods that do not depend on a dataset. In this paper, we benchmark the usefulness of interpretability tools for model debugging. Our key insight is that we can implant human-interpretable trojans into models and then evaluate these tools based on whether they can help humans discover them. This is analogous to finding OOD bugs, except the ground truth is known, allowing us to know when a user's interpretation is correct. We make four contributions. (1) We propose trojan discovery as an evaluation task for interpretability tools and introduce a benchmark with 12 trojans of 3 different types. (2) We demonstrate the difficulty of this benchmark with a preliminary evaluation of 16 state-of-the-art feature attribution/saliency tools. Even under ideal conditions, given direct access to data with the trojan trigger, these methods still often fail to identify bugs. (3) We evaluate 7 feature-synthesis methods on our benchmark. (4) We introduce and evaluate 2 new variants of the best-performing method from the previous evaluation.",main,NeurIPS,2023,Poster,Stephen Casper;Tong Bu;Yuxiao Li;Jiawei Li;Kevin Zhang;Kaivalya Hariharan;Dylan Hadfield-Menell,True,https://openreview.net/pdf?id=Od6CHhPM7I
OdylEgIR1D,PufferLib: Making Reinforcement Learning Libraries and Environments Play Nice,"Reinforcement learning (RL) frameworks often falter in complex environments due to inherent simplifying assumptions. This gap necessitates labor-intensive and error-prone intermediate conversion layers, limiting the applicability of RL as a whole. To address this challenge, we introduce PufferLib, a novel middleware solution. PufferLib transforms complex environments into a broadly compatible, vectorized format, eliminating the need for bespoke conversion layers and enabling more rigorous testing. Users interact with PufferLib through concise bindings, significantly reducing the technical overhead. We release PufferLib's complete source code under the MIT license, a pip module, a containerized setup, comprehensive documentation, and example integrations. We also maintain a community Discord channel to facilitate support and discussion.",Datasets & Benchmarks,NeurIPS,2023,Reject,Joseph Suarez,False,https://openreview.net/pdf?id=OdylEgIR1D
Of0GBzow8P,The Transient Nature of Emergent In-Context Learning in Transformers,"Transformer neural networks can exhibit a surprising capacity for in-context learning (ICL) despite not being explicitly trained for it.  Prior work has provided a deeper understanding of how ICL emerges in transformers, e.g. through the lens of mechanistic interpretability, Bayesian inference, or by examining the distributional properties of training data. However, in each of these cases, ICL is treated largely as a persistent phenomenon; namely, once ICL emerges, it is assumed to persist asymptotically. Here, we show that the emergence of ICL during transformer training is, in fact, often transient. We train transformers on synthetic data designed so that both ICL and in-weights learning (IWL) strategies can lead to correct predictions. We find that ICL first emerges, then disappears and gives way to IWL, all while the training loss decreases, indicating an asymptotic preference for IWL. The transient nature of ICL is observed in transformers across a range of model sizes and datasets, raising the question of how much to ``overtrain'' transformers when seeking compact, cheaper-to-run models. We find that L2 regularization may offer a path to more persistent ICL that removes the need for early stopping based on ICL-style validation tasks. Finally, we present initial evidence that ICL transience may be caused by competition between ICL and IWL circuits.",main,NeurIPS,2023,Poster,Aaditya K Singh;Stephanie C.Y. Chan;Ted Moskovitz;Erin Grant;Andrew M Saxe;Felix Hill,True,https://openreview.net/pdf?id=Of0GBzow8P
OyTIV57Prb,CAPP-130: A Corpus of Chinese Application Privacy Policy Summarization and Interpretation,"A privacy policy serves as an online internet protocol crafted by service providers, which details how service providers collect, process, store, manage, and use personal information when users engage with applications. However, these privacy policies are often filled with technobabble and legalese, making them ""incomprehensible''. As a result, users often agree to all terms unknowingly, even some terms may conflict with the law, thereby posing a considerable risk to personal privacy information. One potential solution to alleviate this challenge is to automatically summarize privacy policies using NLP techniques. However, existing techniques primarily focus on extracting key sentences, resulting in comparatively shorter agreements, but failing to address the poor readability caused by the ""incomprehensible'' of technobabble and legalese. Moreover, research on Chinese application privacy policy summarization is currently almost nonexistent, and there is a lack of a high-quality corpus suitable for addressing readability issues. To tackle these challenges, we introduce a fine-grained CAPP-130 corpus and a TCSI-pp framework. CAPP-130 contains 130 Chinese privacy policies from popular applications that have been carefully annotated and interpreted by legal experts, resulting in 52,489 annotations and 20,555 rewritten sentences. TCSI-pp first extracts sentences related to the topic specified by users and then uses a generative model to rewrite the sentences into comprehensible summarization. Built upon TSCI-pp, we construct a summarization tool TSCI-pp-zh by selecting RoBERTa from six classification models for sentence extraction and selecting mT5 from five generative models for sentence rewriting. Experimental results show that TCSI-pp-zh outperforms GPT-4 and other baselines in Chinese application privacy policy summarization, demonstrating exceptional readability and reliability. Our data, annotation guidelines, benchmark models, and source code are publicly available at https://github.com/EnlightenedAI/CAPP-130.",Datasets & Benchmarks,NeurIPS,2023,Poster,Pengyun Zhu;Long Wen;Jinfei Liu;Feng Xue;Jian Lou;Zhibo Wang;Kui Ren,True,https://openreview.net/pdf?id=OyTIV57Prb
OzcPJz7rgg,STARSS23: An Audio-Visual Dataset of Spatial Recordings of Real Scenes with Spatiotemporal Annotations of Sound Events,"While direction of arrival (DOA) of sound events is generally estimated from multichannel audio data recorded in a microphone array, sound events usually derive from visually perceptible source objects, e.g., sounds of footsteps come from the feet of a walker. This paper proposes an audio-visual sound event localization and detection (SELD) task, which uses multichannel audio and video information to estimate the temporal activation and DOA of target sound events. Audio-visual SELD systems can detect and localize sound events using signals from a microphone array and audio-visual correspondence. We also introduce an audio-visual dataset, Sony-TAu Realistic Spatial Soundscapes 2023 (STARSS23), which consists of multichannel audio data recorded with a microphone array, video data, and spatiotemporal annotation of sound events. Sound scenes in STARSS23 are recorded with instructions, which guide recording participants to ensure adequate activity and occurrences of sound events. STARSS23 also serves human-annotated temporal activation labels and human-confirmed DOA labels, which are based on tracking results of a motion capture system. Our benchmark results demonstrate the benefits of using visual object positions in audio-visual SELD tasks. The data is available at https://zenodo.org/record/7880637.",Datasets & Benchmarks,NeurIPS,2023,Poster,Kazuki Shimada;Archontis Politis;Parthasaarathy Sudarsanam;Daniel Aleksander Krause;Kengo Uchida;Sharath Adavanne;Aapo Hakala;Yuichiro Koyama;Naoya Takahashi;Shusuke Takahashi;Tuomas Virtanen;Yuki Mitsufuji,True,https://openreview.net/pdf?id=OzcPJz7rgg
PARMyW6xX0,Type-to-Track: Retrieve Any Object via Prompt-based Tracking,"One of the recent trends in vision problems is to use natural language captions to describe the objects of interest. This approach can overcome some limitations of traditional methods that rely on bounding boxes or category annotations. This paper introduces a novel paradigm for Multiple Object Tracking called Type-to-Track, which allows users to track objects in videos by typing natural language descriptions. We present a new dataset for that Grounded Multiple Object Tracking task, called GroOT, that contains videos with various types of objects and their corresponding textual captions describing their appearance and action in detail. Additionally, we introduce two new evaluation protocols and formulate evaluation metrics specifically for this task. We develop a new efficient method that models a transformer-based eMbed-ENcoDE-extRact framework (MENDER) using the third-order tensor decomposition. The experiments in five scenarios show that our MENDER approach outperforms another two-stage design in terms of accuracy and efficiency, up to 14.7\\\\% accuracy and $4\\\\times$ speed faster.",main,NeurIPS,2023,Poster,Pha Nguyen;Kha Gia Quach;Kris M. Kitani;Khoa Luu,True,https://openreview.net/pdf?id=PARMyW6xX0
PF0lxayYST,On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets,"Different distribution shifts require different algorithmic and operational
  interventions. Methodological research must be grounded by the specific
  shifts they address.  Although nascent benchmarks provide a promising
  empirical foundation, they \\\\emph{implicitly} focus on covariate
  shifts, and the validity of empirical findings depends on the type of shift, 
  e.g., previous observations on algorithmic performance can fail to be valid when
  the $Y|X$ distribution changes.  We conduct a thorough investigation of
  natural shifts in 5 tabular datasets over 86,000 model configurations, and
  find that $Y|X$-shifts are most prevalent.  To encourage researchers to
  develop a refined language for distribution shifts, we build
 ``WhyShift``, an empirical testbed of curated real-world shifts where
  we characterize the type of shift we benchmark performance over.  Since
  $Y|X$-shifts are prevalent in tabular settings, we \\\\emph{identify covariate
  regions} that suffer the biggest $Y|X$-shifts and discuss implications for
  algorithmic and data-based interventions.  Our testbed highlights the
  importance of future research that builds an understanding of why
  distributions differ.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jiashuo Liu;Tianyu Wang;Peng Cui;Hongseok Namkoong,True,https://openreview.net/pdf?id=PF0lxayYST
PFfmfspm28,Minigrid & Miniworld: Modular & Customizable Reinforcement Learning Environments for Goal-Oriented Tasks,"We present the Minigrid and Miniworld libraries which provide a suite of goal-oriented 2D and 3D environments. The libraries were explicitly created with a minimalistic design paradigm to allow users to rapidly develop new environments for a wide range of research-specific needs. As a result, both have received widescale adoption by the RL community, facilitating research in a wide range of areas. In this paper, we outline the design philosophy, environment details, and their world generation API.  We also showcase the additional capabilities brought by the unified API between Minigrid and Miniworld through case studies on transfer learning (for both RL agents and humans) between the different observation spaces. The source code of Minigrid and Miniworld can be found at https://github.com/Farama-Foundation/Minigrid and https://github.com/Farama-Foundation/Miniworld along with their documentation at https://minigrid.farama.org/ and https://miniworld.farama.org/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Maxime Chevalier-Boisvert;Bolun Dai;Mark Towers;Rodrigo De Lazcano Perez-Vicente;Lucas Willems;Salem Lahlou;Suman Pal;Pablo Samuel Castro;J K Terry,False,https://openreview.net/pdf?id=PFfmfspm28
PWLGrvoqiR,RaLEs: a Benchmark for Radiology Language Evaluations,"The radiology report is the main form of communication between radiologists and other clinicians. Prior work in natural language processing in radiology reports has shown the value of developing methods tailored for individual tasks such as identifying reports with critical results or disease detection. Meanwhile, English and biomedical natural language understanding benchmarks such as the General Language Understanding and Evaluation as well as Biomedical Language Understanding and Reasoning Benchmark have motivated the development of models that can be easily adapted to address many tasks in those domains. Here, we characterize the radiology report as a distinct domain and introduce RaLEs, the Radiology Language Evaluations, as a benchmark for natural language understanding and generation in radiology. RaLEs is comprised of seven natural language understanding and generation evaluations including the extraction of anatomical and disease entities and their relations, procedure selection, and report summarization. We characterize the performance of models designed for the general, biomedical, clinical and radiology domains across these tasks. We find that advances in the general and biomedical domains do not necessarily translate to radiology, and that improved models from the general domain can perform comparably to smaller clinical-specific models. The limited performance of existing pre-trained models on RaLEs highlights the opportunity to improve domain-specific self-supervised models for natural language processing in radiology. We propose RaLEs as a benchmark to promote and track the development of such domain-specific radiology language models.",Datasets & Benchmarks,NeurIPS,2023,Poster,Juan Manuel Zambrano Chaves;Nandita Bhaskhar;Maayane Attias;Jean-Benoit Delbrouck;Daniel Rubin;Andreas Markus Loening;Curtis Langlotz;Akshay S Chaudhari,True,https://openreview.net/pdf?id=PWLGrvoqiR
Pj6X6GqNy8,Learning Fine-grained View-Invariant Representations from Unpaired Ego-Exo Videos via Temporal Alignment,"The egocentric and exocentric viewpoints of a human activity look dramatically different, yet invariant representations to link them are essential for many potential applications in robotics and augmented reality.  Prior work is limited to learning view-invariant features from paired synchronized viewpoints.  We relax that strong data assumption and propose to learn fine-grained action features that are invariant to the viewpoints by aligning egocentric and exocentric videos in time, even when not captured simultaneously or in the same environment. To this end, we propose AE2, a self-supervised embedding approach with two key designs: (1) an object-centric encoder that explicitly focuses on regions corresponding to hands and active objects; (2) a contrastive-based alignment objective that leverages temporally reversed frames as negative samples. For evaluation, we establish a benchmark for fine-grained video understanding in the ego-exo context, comprising four datasets---including an ego tennis forehand dataset we collected, along with dense per-frame labels we annotated for each dataset. On the four datasets, our AE2 method strongly outperforms prior work in a variety of fine-grained downstream tasks, both in regular and cross-view settings.",main,NeurIPS,2023,Poster,Zihui Xue;Kristen Grauman,True,https://openreview.net/pdf?id=Pj6X6GqNy8
Pk2x7FPuZ4,EHRXQA: A Multi-Modal Question Answering Dataset for Electronic Health Records with Chest X-ray Images,"Electronic Health Records (EHRs), which contain patients' medical histories in various multi-modal formats, often overlook the potential for joint reasoning across imaging and table modalities underexplored in current EHR Question Answering (QA) systems. In this paper, we introduce EHRXQA, a novel multi-modal question answering dataset combining structured EHRs and chest X-ray images. To develop our dataset, we first construct two uni-modal resources: 1) The MIMIC- CXR-VQA dataset, our newly created medical visual question answering (VQA) benchmark, specifically designed to augment the imaging modality in EHR QA, and 2) EHRSQL (MIMIC-IV), a refashioned version of a previously established table-based EHR QA dataset. By integrating these two uni-modal resources, we successfully construct a multi-modal EHR QA dataset that necessitates both uni-modal and cross-modal reasoning. To address the unique challenges of multi-modal questions within EHRs, we propose a NeuralSQL-based strategy equipped with an external VQA API. This pioneering endeavor enhances engagement with multi-modal EHR sources and we believe that our dataset can catalyze advances in real-world medical scenarios such as clinical decision-making and research. EHRXQA is available at https://github.com/baeseongsu/ehrxqa.",Datasets & Benchmarks,NeurIPS,2023,Poster,Seongsu Bae;Daeun Kyung;Jaehee Ryu;Eunbyeol Cho;Gyubok Lee;Sunjun Kweon;Jungwoo Oh;Lei Ji;Eric I-Chao Chang;Tackeun Kim;Edward Choi,True,https://openreview.net/pdf?id=Pk2x7FPuZ4
PnbCA4ylIc,Goal Driven Discovery of Distributional Differences via Language Descriptions,"Exploring large corpora can generate useful discoveries but is time-consuming for humans.
    We formulate a new task, D5, that automatically discovers differences between two large corpora in a goal-driven way. 
    The task input is a problem comprising a user-specified research goal (“*comparing the side effects of drug A and drug*”) and a corpus pair (two large collections of patients' self-reported reactions after taking each drug). 
    The output is a goal-related description (discovery) of how these corpora differ (patients taking drug A “*mention feelings of paranoia*” more often).
    We build a D5 system, and to quantitatively evaluate its performance, we 1) build a diagnostic benchmark, SynD5, to test whether it can recover known differences between two synthetic corpora, and 2) contribute a meta-dataset, OpenD5, aggregating 675 open-ended problems ranging across business, social sciences, humanities, machine learning, and health.
    With both synthetic and real datasets, we confirm that language models can leverage the user-specified goals to propose more relevant candidate discoveries, and they sometimes produce discoveries previously unknown to the authors, including demographic differences in discussion topics, political stances in speech, insights in commercial reviews, and error patterns in NLP models.
    Finally, we discuss the limitations of the current D5 system, which discovers correlation rather than causation and has the potential to reinforce societal biases when misused; therefore, practitioners should treat the outputs of our system with caution.",main,NeurIPS,2023,Poster,Ruiqi Zhong;Peter Zhang;Steve Li;Jinwoo Ahn;Dan Klein;Jacob Steinhardt,True,https://openreview.net/pdf?id=PnbCA4ylIc
Psnph85KYc,Interpretable Graph Networks Formulate Universal Algebra Conjectures,"The rise of Artificial Intelligence (AI) recently empowered researchers to investigate hard mathematical problems which eluded traditional approaches for decades. Yet, the use of AI in Universal Algebra (UA)---one of the fields laying the foundations of modern mathematics---is still completely unexplored. 
This work proposes the first use of AI to investigate UA's conjectures with an equivalent equational and topological characterization. While topological representations would enable the analysis of such properties using graph neural networks, the limited transparency and brittle explainability of these models hinder their straightforward use to empirically validate existing conjectures or to formulate new ones. 
To bridge these gaps, we propose a general algorithm generating AI-ready datasets based on UA's conjectures, and introduce a novel neural layer to build fully interpretable graph networks. The results of our experiments demonstrate that interpretable graph networks: (i) enhance interpretability without sacrificing task accuracy, (ii) strongly generalize when predicting universal algebra's properties, (iii) generate simple explanations that empirically validate existing conjectures, and (iv) identify subgraphs suggesting the formulation of novel conjectures.",main,NeurIPS,2023,Poster,Francesco Giannini;Stefano Fioravanti;Oguzhan Keskin;Alisia Maria Lupidi;Lucie Charlotte Magister;Pietro Lio;Pietro Barbiero,True,https://openreview.net/pdf?id=Psnph85KYc
QEDjXv9OyY,"YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus","Machine learning for sign languages is bottlenecked by data. In this paper, we present YouTube-ASL, a large-scale, open-domain corpus of American Sign Language (ASL) videos and accompanying English captions drawn from YouTube. With ~1000 hours of videos and >2500 unique signers, YouTube-ASL is ~3x as large and has ~10x as many unique signers as the largest prior ASL dataset. We train baseline models for ASL to English translation on YouTube-ASL and evaluate them on How2Sign, where we achieve a new fine-tuned state of the art of 12.397 BLEU and, for the first time, nontrivial zero-shot results.",Datasets & Benchmarks,NeurIPS,2023,Poster,David Uthus;Garrett Tanzer;Manfred Georg,True,https://openreview.net/pdf?id=QEDjXv9OyY
QXTjde8evS,DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology,"We present DiffInfinite, a hierarchical diffusion model that generates arbitrarily large histological images while preserving long-range correlation structural information. Our approach first generates synthetic segmentation masks, subsequently used as conditions for the high-fidelity generative diffusion process. The proposed sampling method can be scaled up to any desired image size while only requiring small patches for fast training. Moreover, it can be parallelized more efficiently than previous large-content generation methods while avoiding tiling artifacts. The training leverages classifier-free guidance to augment a small, sparsely annotated dataset with unlabelled data. Our method alleviates unique challenges in histopathological imaging practice: large-scale information, costly manual annotation, and protective data handling. The biological plausibility of DiffInfinite data is evaluated in a survey by ten experienced pathologists as well as a downstream classification and segmentation task. Samples from the model score strongly on anti-copying metrics which is relevant for the protection of patient data.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Marco Aversa;Gabriel Nobis;Miriam Hägele;Kai Standvoss;Mihaela Chirica;Roderick Murray-Smith;Ahmed Alaa;Lukas Ruff;Daniela Ivanova;Wojciech Samek;Frederick Klauschen;Bruno Sanguinetti;Luis Oala,False,https://openreview.net/pdf?id=QXTjde8evS
Qf8uzIT1OK,Ethical Considerations for Responsible Data Curation,"Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. HCCV datasets constructed through nonconsensual web scraping lack crucial metadata for comprehensive fairness and robustness evaluations. Current remedies are post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations, covering purpose, privacy and consent, and diversity, for curating HCCV evaluation datasets, addressing privacy and bias concerns. We adopt an ante hoc reflective perspective, drawing from current practices, guidelines, dataset withdrawals, and audits, to inform our considerations and recommendations.",Datasets & Benchmarks,NeurIPS,2023,Oral,Jerone Andrews;Dora Zhao;William Thong;Apostolos Modas;Orestis Papakyriakopoulos;Alice Xiang,False,https://openreview.net/pdf?id=Qf8uzIT1OK
R7lDPUgpaA,Input margins can predict generalization too,"Understanding generalization in deep neural networks is an active area of research. A promising avenue of exploration has been that of margin measurements: the shortest distance to the decision boundary for a given sample or its representation internal to the network. While margins have been shown to be correlated with the generalization ability of a model when measured at its hidden representations (hidden margins), no such link between large margins and generalization has been established for input margins. We show that while input margins are not generally predictive of generalization, they can be if the search space is appropriately constrained.
We develop such a measure based on input margins, which we refer to as 'constrained margins'. The predictive power of this new measure is demonstrated on the 'Predicting Generalization in Deep Learning' (PGDL) dataset and contrasted with hidden representation margins. We find that constrained margins achieve highly competitive scores and outperform other margin measurements in general.",main,NeurIPS,2023,Reject,Coenraad Mouton;Marthinus Wilhelmus Theunissen;Marelie Hattingh Davel,True,https://openreview.net/pdf?id=R7lDPUgpaA
RA7ND878XP,Segment Anything in High Quality,"The recent Segment Anything Model (SAM) represents a big leap in scaling up segmentation models, allowing for powerful zero-shot capabilities and flexible prompting. Despite being trained with 1.1 billion masks, SAM's mask prediction quality falls short in many cases, particularly when dealing with objects that have intricate structures.  We propose HQ-SAM, equipping SAM with the ability to accurately segment any object, while maintaining SAM's original promptable design, efficiency, and zero-shot generalizability. Our careful design reuses and preserves the pre-trained model weights of SAM, while only introducing minimal additional parameters and computation. We design a learnable High-Quality Output Token, which is injected into SAM's mask decoder and is responsible for predicting the high-quality mask. Instead of only applying it on mask-decoder features, we first fuse them with early and final ViT features for improved mask details. To train our introduced learnable parameters, we compose a dataset of 44K fine-grained masks from several sources. HQ-SAM is only trained on the introduced detaset of 44k masks, which takes only 4 hours on 8 GPUs. We show the efficacy of HQ-SAM in a suite of 10 diverse segmentation datasets across different downstream tasks, where 8 out of them are evaluated in a zero-shot transfer protocol. Our code and pretrained models are at https://github.com/SysCV/SAM-HQ.",main,NeurIPS,2023,Poster,Lei Ke;Mingqiao Ye;Martin Danelljan;Yifan liu;Yu-Wing Tai;Chi-Keung Tang;Fisher Yu,True,https://openreview.net/pdf?id=RA7ND878XP
RADrFxYqIH,How hard are computer vision datasets? Calibrating dataset difficulty to viewing time,"Humans outperform object recognizers despite the fact that models perform well on current datasets, including those explicitly designed to challenge machines with debiased images or distribution shift. This problem persists, in part, because we have no guidance on the absolute difficulty of an image or dataset making it hard to objectively assess progress toward human-level performance, to cover the range of human abilities, and to increase the challenge posed by a dataset. We develop a dataset difficulty metric MVT, Minimum Viewing Time, that addresses these three problems. Subjects view an image that flashes on screen and then classify the object in the image. Images that require brief flashes to recognize are easy, those which require seconds of viewing are hard. We compute the ImageNet and ObjectNet image difficulty distribution, which we find significantly undersamples hard images. Nearly 90% of current benchmark performance is derived from images that are easy for humans. Rather than hoping that we will make harder datasets, we can for the first time objectively guide dataset difficulty during development. We can also subset recognition performance as a function of difficulty: model performance drops precipitously while human performance remains stable. Difficulty provides a new lens through which to view model performance, one which uncovers new scaling laws: vision-language models stand out as being the most robust and human-like while all other techniques scale poorly. We release tools to automatically compute MVT, along with image sets which are tagged by difficulty. Objective image difficulty has practical applications – one can measure how hard a test set is before deploying a real-world system – and scientific applications such as discovering the neural correlates of image difficulty and enabling new object recognition techniques that eliminate the benchmark-vs- real-world performance gap.",Datasets & Benchmarks,NeurIPS,2023,Poster,David Mayo;Jesse Cummings;Xinyu Lin;Dan Gutfreund;Boris Katz;Andrei Barbu,False,https://openreview.net/pdf?id=RADrFxYqIH
RTRS3ZTsSj,ANPL: Towards Natural Programming with Interactive Decomposition,"Though LLMs are capable of generating plausible programs, it’s challenging to interact with the LLMs further to revise the program, especially if the user’s specific requirements are different from the initial proposal. In this paper, we introduce ANPL, an interactive programming system that ensures users can always refine the generated code towards their specific programmatic intents via structured
decompositions. Borrowing the paradigm of sketching from program synthesis, an ANPL program consists of a set of input-outputs that it must satisfy, a “sketch” — control/data flow expressed in precise code (e.g. Python), and “holes” — sub-modules to be implemented by the LLM specified with natural language. The user revises an ANPL program by either modifying the sketch, changing the language used to describe the holes, or providing additional input-outputs to a particular hole, turning it into a sub-ANPL program that can be solved recursively. This workflow allows the users to offload programming burdens to the LLM as much as possible while retaining the ability to pinpoint and resolve bugs locally, without exposing the rest of the program to the LLM. We deploy ANPL on the Abstraction and Reasoning Corpus (ARC), a set of unique tasks that are challenging for state-of-the-art AI systems, showing it outperforms baseline programming systems that (a) without the ability to decompose tasks interactively and (b) without the guarantee that the modules can be correctly composed together. Additional evaluations on APPS, HumanEval, and real-world programming tasks have validated that the ANPL framework is applicable to multiple programming domains. We release the ANPL solutions to the ARC tasks as a dataset, providing insights into how humans decompose novel tasks programmatically.",main,NeurIPS,2023,Poster,Di Huang;Ziyuan Nan;Xing Hu;Pengwei Jin;Shaohui Peng;Yuanbo Wen;Rui Zhang;Zidong Du;Qi Guo;Yewen Pu;Yunji Chen,True,https://openreview.net/pdf?id=RTRS3ZTsSj
RZJEkLFlPx,ClimateLearn: Benchmarking Machine Learning for Weather and Climate Modeling,"Modeling weather and climate is an essential endeavor to understand the near- and long-term impacts of climate change, as well as to inform technology and policymaking for adaptation and mitigation efforts. In recent years, there has been a surging interest in applying data-driven methods based on machine learning for solving core problems such as weather forecasting and climate downscaling. Despite promising results, much of this progress has been impaired due to the lack of large-scale, open-source efforts for reproducibility, resulting in the use of inconsistent or underspecified datasets, training setups, and evaluations by both domain scientists and artificial intelligence researchers. We introduce ClimateLearn, an open-source PyTorch library that vastly simplifies the training and evaluation of machine learning models for data-driven climate science. ClimateLearn consists of holistic pipelines for dataset processing (e.g., ERA5, CMIP6, PRISM), implementing state-of-the-art deep learning models (e.g., Transformers, ResNets), and quantitative and qualitative evaluation for standard weather and climate modeling tasks. We supplement these functionalities with extensive documentation, contribution guides, and quickstart tutorials to expand access and promote community growth. We have also performed comprehensive forecasting and downscaling experiments to showcase the capabilities and key features of our library. To our knowledge, ClimateLearn is the first large-scale, open-source effort for bridging research in weather and climate modeling with modern machine learning systems. Our library is available publicly at https://github.com/aditya-grover/climate-learn.",Datasets & Benchmarks,NeurIPS,2023,Poster,Tung Nguyen;Jason Kyle Jewik;Hritik Bansal;Prakhar Sharma;Aditya Grover,False,https://openreview.net/pdf?id=RZJEkLFlPx
Rep7BB4vDa,Species196: A One-Million Semi-supervised Dataset for Fine-grained Species Recognition,"The development of foundation vision models has pushed the general visual recognition to a high level, but cannot well address the fine-grained recognition in specialized domain such as invasive species classification. Identifying and managing invasive species has strong social and ecological value. Currently, most invasive species datasets are limited in scale and cover a narrow range of species, which restricts the development of deep-learning based invasion biometrics systems. To fill the gap of this area, we introduced Species196, a large-scale semi-supervised dataset of 196-category invasive species. It collects over 19K images with expert-level accurate annotations (Species196-L), and 1.2M unlabeled images of invasive species (Species196-U). The dataset provides four experimental settings for benchmarking the existing models and algorithms, namely, supervised learning, semi-supervised learning and self-supervised pretraining. To facilitate future research on these four learning paradigms, we conduct an empirical study of the representative methods on the introduced dataset. The dataset will be made publicly available at https://species-dataset.github.io/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Wei He;Kai Han;Ying Nie;Chengcheng Wang;Yunhe Wang,True,https://openreview.net/pdf?id=Rep7BB4vDa
RgdGkPRQ03,WildfireSpreadTS: A dataset of multi-modal time series for wildfire spread prediction,"We present a multi-temporal, multi-modal remote-sensing dataset for predicting how active wildfires will spread at a resolution of 24 hours. The dataset consists of 13607 images across 607 fire events in the United States from January 2018 to October 2021. For each fire event, the dataset contains a full time series of daily observations, containing detected active fires and variables related to fuel, topography and weather conditions. The dataset is challenging due to: a) its inputs being multi-temporal, b) the high number of 23 multi-modal input channels, c) highly imbalanced labels and d) noisy labels, due to smoke, clouds, and inaccuracies in the active fire detection. The underlying complexity of the physical processes adds to these challenges. Compared to existing public datasets in this area, WildfireSpreadTS allows for multi-temporal modeling of spreading wildfires, due to its time series structure. Furthermore, we provide additional input modalities and a high spatial resolution of 375m for the active fire maps. We publish this dataset to encourage further research on this important task with multi-temporal, noise-resistant or generative methods, uncertainty estimation or advanced optimization techniques that deal with the high-dimensional input space.",Datasets & Benchmarks,NeurIPS,2023,Poster,Sebastian Gerard;Yu Zhao;Josephine Sullivan,True,https://openreview.net/pdf?id=RgdGkPRQ03
RiSMijlsLT,SimMMDG: A Simple and Effective Framework for Multi-modal Domain Generalization,"In real-world scenarios, achieving domain generalization (DG) presents significant challenges as models are required to generalize to unknown target distributions. Generalizing to unseen multi-modal distributions poses even greater difficulties due to the distinct properties exhibited by different modalities. To overcome the challenges of achieving domain generalization in multi-modal scenarios, we propose SimMMDG, a simple yet effective multi-modal DG framework. We argue that mapping features from different modalities into the same embedding space impedes model generalization. To address this, we propose splitting the features within each modality into modality-specific and modality-shared components. We employ supervised contrastive learning on the modality-shared features to ensure they possess joint properties and impose distance constraints on modality-specific features to promote diversity. In addition, we introduce a cross-modal translation module to regularize the learned features, which can also be used for missing-modality generalization. We demonstrate that our framework is theoretically well-supported and achieves strong performance in multi-modal DG on the EPIC-Kitchens dataset and the novel Human-Animal-Cartoon (HAC) dataset introduced in this paper. Our source code and HAC dataset are available at https://github.com/donghao51/SimMMDG.",main,NeurIPS,2023,Poster,Hao Dong;Ismail Nejjar;Han Sun;Eleni Chatzi;Olga Fink,True,https://openreview.net/pdf?id=RiSMijlsLT
RwNIqaNOgd,RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization,"Visual Reinforcement Learning (Visual RL), coupled with high-dimensional observations, has consistently confronted the long-standing challenge of out-of-distribution generalization. Despite the focus on algorithms aimed at resolving visual generalization problems, we argue that the devil is in the existing benchmarks as they are restricted to isolated tasks and generalization categories, undermining a comprehensive evaluation of agents' visual generalization capabilities. To bridge this gap, we introduce RL-ViGen: a novel **R**einforcement **L**earning Benchmark for **Vi**sual **Gen**eralization, which contains diverse tasks and a wide spectrum of generalization types, thereby facilitating the derivation of more reliable conclusions. Furthermore, RL-ViGen incorporates the latest generalization visual RL algorithms into a unified framework, under which the experiment results indicate that no single existing algorithm has prevailed universally across tasks. Our aspiration is that Rl-ViGen will serve as a catalyst in this area, and lay a foundation for the future creation of universal visual generalization RL agents suitable for real-world scenarios.  Access to our code and implemented algorithms is provided at https://gemcollector.github.io/RL-ViGen/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zhecheng Yuan;Sizhe Yang;Pu Hua;Can Chang;Kaizhe Hu;Huazhe Xu,True,https://openreview.net/pdf?id=RwNIqaNOgd
SDJ3kYpJFX,DISCO-10M: A Large-Scale Music Dataset,"Music datasets play a crucial role in advancing research in machine learning for music. However, existing music datasets suffer from limited size, accessibility, and lack of audio resources. To address these shortcomings, we present DISCO-10M, a novel and extensive music dataset that surpasses the largest previously available music dataset by an order of magnitude. To ensure high-quality data, we implement a multi-stage filtering process. This process incorporates similarities based on textual descriptions and audio embeddings. Moreover, we provide precomputed CLAP embeddings alongside DISCO-10M, facilitating direct application on various downstream tasks. These embeddings enable efficient exploration of machine learning applications on the provided data. With DISCO-10M, we aim to democratize and facilitate new research to help advance the development of novel machine learning models for music: https://huggingface.co/DISCOX",Datasets & Benchmarks,NeurIPS,2023,Poster,Luca A Lanzendörfer;Florian Grötschla;Emil Funke;Roger Wattenhofer,True,https://openreview.net/pdf?id=SDJ3kYpJFX
SEU9m9NReo,SARAMIS: Simulation Assets for Robotic Assisted and Minimally Invasive Surgery,"Minimally-invasive surgery (MIS) and robot-assisted minimally invasive (RAMIS) surgery offer well-documented benefits to patients such as reduced post-operative pain and shorter hospital stays.
However, the automation of MIS and RAMIS through the use of AI has been slow due to difficulties in data acquisition and curation, partially caused by the ethical considerations of training, testing and deploying AI models in medical environments.
We introduce \\\\texttt{SARAMIS}, the first large-scale dataset of anatomically derived 3D rendering assets of the human abdominal anatomy.
Using previously existing, open-source CT datasets of the human anatomy, we derive novel 3D meshes, tetrahedral volumes, textures and diffuse maps for over 104 different anatomical targets in the human body, representing the largest, open-source dataset of 3D rendering assets for synthetic simulation of vision tasks in MIS+RAMIS, increasing the availability of openly available 3D meshes in the literature by three orders of magnitude.
We supplement our dataset with a series of GPU-enabled rendering environments, which can be used to generate datasets for realistic MIS/RAMIS tasks.
Finally, we present an example of the use of \\\\texttt{SARAMIS} assets for an autonomous navigation task in colonoscopy from CT abdomen-pelvis scans for the first time in the literature.
\\\\texttt{SARAMIS} is publically made available at https://github.com/NMontanaBrown/saramis/, with assets released under a CC-BY-NC-SA license.",Datasets & Benchmarks,NeurIPS,2023,Poster,Nina Montana-Brown;Shaheer U. Saeed;Ahmed Abdulaal;Thomas Dowrick;Yakup Kilic;Sophie Wilkinson;Jack Gao;Meghavi Mashar;Chloe He;Alkisti Stavropoulou;Emma L Thomson;Zachary Baum;Simone Foti;Brian Davidson;Yipeng Hu;Matthew John Clarkson,True,https://openreview.net/pdf?id=SEU9m9NReo
SKN2hflBIZ,OBELICS: An Open Web-Scale Filtered Dataset of Interleaved Image-Text Documents,"Large multimodal models trained on natural documents, which interleave images and text, outperform models trained on image-text pairs on various multimodal benchmarks. However, the datasets used to train these models have not been released, and the collection process has not been fully specified.  We introduce the OBELICS dataset, an open web-scale filtered dataset of interleaved image-text documents comprising 141 million web pages extracted from Common Crawl, 353 million associated images, and 115 billion text tokens. We describe the dataset creation process, present comprehensive filtering rules, and provide an analysis of the dataset's content. To show the viability of OBELICS, we train on the dataset vision and language models of 9 and 80 billion parameters, IDEFICS-9B and IDEFICS, and obtain competitive performance on different multimodal benchmarks. We release our dataset, models and code.",Datasets & Benchmarks,NeurIPS,2023,Poster,Hugo Laurençon;Lucile Saulnier;Leo Tronchon;Stas Bekman;Amanpreet Singh;Anton Lozhkov;Thomas Wang;Siddharth Karamcheti;Alexander M Rush;Douwe Kiela;Matthieu Cord;Victor Sanh,True,https://openreview.net/pdf?id=SKN2hflBIZ
SNznC08OOO,RoboDepth: Robust Out-of-Distribution Depth Estimation under Corruptions,"Depth estimation from monocular images is pivotal for real-world visual perception systems. While current learning-based depth estimation models train and test on meticulously curated data, they often overlook out-of-distribution (OoD) situations. Yet, in practical settings -- especially safety-critical ones like autonomous driving -- common corruptions can arise. Addressing this oversight, we introduce a comprehensive robustness test suite, RoboDepth, encompassing 18 corruptions spanning three categories: i) weather and lighting conditions; ii) sensor failures and movement; and iii) data processing anomalies. We subsequently benchmark 42 depth estimation models across indoor and outdoor scenes to assess their resilience to these corruptions. Our findings underscore that, in the absence of a dedicated robustness evaluation framework, many leading depth estimation models may be susceptible to typical corruptions. We delve into design considerations for crafting more robust depth estimation models, touching upon pre-training, augmentation, modality, model capacity, and learning paradigms. We anticipate our benchmark will establish a foundational platform for advancing robust OoD depth estimation.",Datasets & Benchmarks,NeurIPS,2023,Poster,Lingdong Kong;Shaoyuan Xie;Hanjiang Hu;Lai Xing Ng;Benoit R Cottereau;Wei Tsang Ooi,True,https://openreview.net/pdf?id=SNznC08OOO
SQouRKRIXY,MomentDiff: Generative Video Moment Retrieval from Random to Real,"Video moment retrieval pursues an efficient and generalized solution to identify the specific temporal segments within an untrimmed video that correspond to a given language description.
To achieve this goal, we provide a generative diffusion-based framework called MomentDiff, which simulates a typical human retrieval process from random browsing to gradual localization.
Specifically, we first diffuse the real span to random noise, and learn to denoise the random noise to the original span with the guidance of similarity between text and video.
This allows the model to learn a mapping from arbitrary random locations to real moments, enabling the ability to locate segments from random initialization.
Once trained, MomentDiff could sample random temporal segments as initial guesses and iteratively refine them to generate an accurate temporal boundary.
Different from discriminative works (e.g., based on learnable proposals or queries), MomentDiff with random initialized spans could resist the temporal location biases from datasets.
To evaluate the influence of the temporal location biases, we propose two ``anti-bias'' datasets with location distribution shifts, named Charades-STA-Len and Charades-STA-Mom.
The experimental results demonstrate that our efficient framework consistently outperforms state-of-the-art methods on three public benchmarks, and exhibits better generalization and robustness on the proposed anti-bias datasets. 
The code, model, and anti-bias evaluation datasets will be released publicly.",main,NeurIPS,2023,Poster,Pandeng Li;Chen-Wei Xie;Hongtao Xie;Liming Zhao;Lei Zhang;Yun Zheng;Deli Zhao;Yongdong Zhang,True,https://openreview.net/pdf?id=SQouRKRIXY
SS3CK3yx5Z,Does progress on ImageNet transfer to real-world datasets?,"Does progress on ImageNet transfer to real-world datasets? We investigate this question by evaluating ImageNet pre-trained models with varying accuracy (57% - 83%) on six practical image classification datasets. In particular, we study datasets collected with the goal of solving real-world tasks (e.g., classifying images from camera traps or satellites), as opposed to web-scraped benchmarks collected for comparing models. On multiple datasets, models with higher ImageNet accuracy do not consistently yield performance improvements. For certain tasks, interventions such as data augmentation improve performance even when architectures do not. We hope that future benchmarks will include more diverse datasets to encourage a more comprehensive approach to improving learning algorithms.",Datasets & Benchmarks,NeurIPS,2023,Poster,Alex Fang;Simon Kornblith;Ludwig Schmidt,False,https://openreview.net/pdf?id=SS3CK3yx5Z
Sq3CLKJeiz,Objaverse-XL: A Universe of 10M+ 3D Objects,"Natural language processing and 2D vision models have attained remarkable proficiency on many tasks primarily by escalating the scale of training data. However, 3D vision tasks have not seen the same progress, in part due to the challenges of acquiring high-quality 3D data. In this work, we present Objaverse-XL, a dataset of over 10 million 3D objects. Our compilation comprises deduplicated 3D objects from a diverse set of sources, including manually designed objects, photogrammetry scans of landmarks and everyday items, and professional scans of historic and antique artifacts. Representing the largest scale and diversity in the realm of 3D datasets, Objaverse-XL enables significant new possibilities for 3D vision. Our experiments demonstrate the vast improvements enabled with the scale provided by Objaverse-XL. We show that by training Zero123 on novel view synthesis, utilizing over 100 million multi-view rendered images, we achieve strong zero-shot generalization abilities. We hope that releasing Objaverse-XL will enable further innovations in the field of 3D vision at scale.",Datasets & Benchmarks,NeurIPS,2023,Poster,Matt Deitke;Ruoshi Liu;Matthew Wallingford;Huong Ngo;Oscar Michel;Aditya Kusupati;Alan Fan;Christian Laforte;Vikram Voleti;Samir Yitzhak Gadre;Eli VanderBilt;Aniruddha Kembhavi;Carl Vondrick;Georgia Gkioxari;Kiana Ehsani;Ludwig Schmidt;Ali Farhadi,True,https://openreview.net/pdf?id=Sq3CLKJeiz
T3FKjN4p8d,Intelligent Knee Sleeves: A Real-time Multimodal Dataset for 3D Lower Body Motion Estimation Using Smart Textile,"The kinematics of human movements and locomotion are closely linked to the activation and contractions of muscles. To investigate this, we present a multimodal dataset with benchmarks collected using a novel pair of Intelligent Knee Sleeves (Texavie MarsWear Knee Sleeves) for human pose estimation. Our system utilizes synchronized datasets that comprise time-series data from the Knee Sleeves and the corresponding ground truth labels from visualized motion capture camera system. We employ these to generate 3D human models solely based on the wearable data of individuals performing different activities. We demonstrate the effectiveness of this camera-free system and machine learning algorithms in the assessment of various movements and exercises, including extension to unseen exercises and individuals. The results show an average error of 7.21 degrees across all eight lower body joints when compared to the ground truth, indicating the effectiveness and reliability of the Knee Sleeve system for the prediction of different lower body joints beyond knees. The results enable human pose estimation in a seamless manner without being limited by visual occlusion or the field of view of cameras. Our results show the potential of multimodal wearable sensing in a variety of applications from home fitness to sports, healthcare, and physical rehabilitation focusing on pose and movement estimation.",Datasets & Benchmarks,NeurIPS,2023,Poster,Wenwen Zhang;Arvin Tashakori;Zenan Jiang;Amir Servati;Harishkumar Narayana;Saeid Soltanian;Rou Yi Yeap;Menghan Ma;Lauren Toy;Peyman Servati,True,https://openreview.net/pdf?id=T3FKjN4p8d
T5ArxPU3Oq,Classical Simulation of Quantum Circuits: Parallel Environments and Benchmark,"Google's quantum supremacy announcement has received broad questions from academia and industry due to the debatable estimate of 10,000 years' running time for the classical simulation task on the Summit supercomputer. Has quantum supremacy already come? Or will it come in one or two decades later? To avoid hasty advertisements of quantum supremacy by tech giants or quantum startups and eliminate the cost of dedicating a team to the classical simulation task, we advocate an open-source approach to maintain a trustable benchmark performance.  In this paper, we take a reinforcement learning approach for the classical simulation of quantum circuits and demonstrate its great potential by reporting an estimated simulation time of less than 4 days, a speedup of 5.40x over the state-of-the-art method.  Specifically, we formulate the classical simulation task as a tensor network contraction ordering problem using the K-spin Ising model and employ a novel Hamiltonina-based reinforcement learning algorithm. Then, we establish standard criteria to evaluate the performance of classical simulation of quantum circuits.  We develop a dozen of massively parallel environments to simulate quantum circuits.  We open-source our parallel gym environments and benchmarks. We hope the AI/ML community and quantum physics community will collaborate to maintain reference curves for validating an unequivocal first demonstration of empirical quantum supremacy.",Datasets & Benchmarks,NeurIPS,2023,Poster,Xiao-Yang Liu;Zeliang Zhang,True,https://openreview.net/pdf?id=T5ArxPU3Oq
Th33sYMCQd,Datasets and Benchmarks for Nanophotonic Structure and Parametric Design Simulations,"Nanophotonic structures have versatile applications including solar cells, anti-reflective coatings, electromagnetic interference shielding, optical filters, and light emitting diodes. To design and understand these nanophotonic structures, electrodynamic simulations are essential. These simulations enable us to model electromagnetic fields over time and calculate optical properties. In this work, we introduce frameworks and benchmarks to evaluate nanophotonic structures in the context of parametric structure design problems. The benchmarks are instrumental in assessing the performance of optimization algorithms and identifying an optimal structure based on target optical properties. Moreover, we explore the impact of varying grid sizes in electrodynamic simulations, shedding light on how evaluation fidelity can be strategically leveraged in enhancing structure designs.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jungtaek Kim;Mingxuan Li;Oliver Hinder;Paul Leu,True,https://openreview.net/pdf?id=Th33sYMCQd
Tt6DrRCgJV,GIMLET: A Unified Graph-Text Model for Instruction-Based Molecule Zero-Shot Learning,"Molecule property prediction has gained significant attention in recent years. The main bottleneck is the label insufficiency caused by expensive lab experiments. In order to alleviate this issue and to better leverage textual knowledge for tasks, this study investigates the feasibility of employing natural language instructions to accomplish molecule-related tasks in a zero-shot setting. We discover that existing molecule-text models perform poorly in this setting due to inadequate treatment of instructions and limited capacity for graphs.  
To overcome these issues, we propose GIMLET, which unifies language models for both graph and text data. By adopting generalized position embedding, our model is extended to encode both graph structures and instruction text without additional graph encoding modules. GIMLET also decouples encoding of the graph from tasks instructions in the attention mechanism, enhancing the generalization of graph features across novel tasks. We construct a dataset consisting of more than two thousand molecule tasks with corresponding instructions derived from task descriptions. We pretrain GIMLET on the molecule tasks along with instructions, enabling the model to transfer effectively to a broad range of tasks. Experimental results demonstrate that GIMLET significantly outperforms molecule-text baselines in instruction-based zero-shot learning, even achieving closed results to supervised GNN models on tasks such as toxcast and muv.",main,NeurIPS,2023,Poster,Haiteng Zhao;Shengchao Liu;Chang Ma;Hannan Xu;Jie Fu;Zhi-Hong Deng;Lingpeng Kong;Qi Liu,True,https://openreview.net/pdf?id=Tt6DrRCgJV
U5uRXlLwnM,GADBench: Revisiting and Benchmarking Supervised Graph Anomaly Detection,"With a long history of traditional Graph Anomaly Detection (GAD) algorithms and recently popular Graph Neural Networks (GNNs), it is still not clear (1) how they perform under a standard comprehensive setting, (2) whether GNNs can outperform traditional algorithms such as tree ensembles, and (3) how about their efficiency on large-scale graphs. In response, we introduce GADBench---a benchmark tool dedicated to supervised anomalous node detection in static graphs. GADBench facilitates a detailed comparison across 29 distinct models on ten real-world GAD datasets, encompassing thousands to millions (~6M) nodes. Our main finding is that tree ensembles with simple neighborhood aggregation can outperform the latest GNNs tailored for the GAD task. We shed light on the current progress of GAD, setting a robust groundwork for subsequent investigations in this domain. GADBench is open-sourced at https://github.com/squareRoot3/GADBench.",Datasets & Benchmarks,NeurIPS,2023,Poster,,False,https://openreview.net/pdf?id=U5uRXlLwnM
UBbm5embIB,Learning Human Action Recognition Representations Without Real Humans,"Pre-training on massive video datasets has become essential to achieve high action recognition performance on smaller downstream datasets. However, most large-scale video datasets contain images of people and hence are accompanied with issues related to privacy, ethics, and data protection, often preventing them from being publicly shared for reproducible research. Existing work has attempted to alleviate these problems by blurring faces, downsampling videos, or training on synthetic data. On the other hand, analysis on the {\\\\em transferability} of privacy-preserving pre-trained models to downstream tasks has been limited. In this work, we study this problem by first asking the question: can we pre-train models for human action recognition with data that does not include real humans? To this end, we present, for the first time, a benchmark that leverages real-world videos with {\\\\em humans removed} and synthetic data containing virtual humans to pre-train a model. We then evaluate the transferability of the representation learned on this data to a diverse set of downstream action recognition benchmarks. Furthermore, we propose a novel pre-training strategy, called Privacy-Preserving MAE-Align, to effectively combine synthetic data and human-removed real data. 
Our approach outperforms previous baselines by up to 5\\\\% and closes the performance gap between human and no-human action recognition representations on downstream tasks, for both linear probing and fine-tuning. Our benchmark, code, and models are available at https://github.com/howardzh01/PPMA.",Datasets & Benchmarks,NeurIPS,2023,Poster,Howard Zhong;Samarth Mishra;Donghyun Kim;SouYoung Jin;Rameswar Panda;Hilde Kuehne;Leonid Karlinsky;Venkatesh Saligrama;Aude Oliva;Rogerio Feris,True,https://openreview.net/pdf?id=UBbm5embIB
UDqHhbqYJV,Can Language Models Solve Graph Problems in Natural Language?,"Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language models do demonstrate preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based approaches to enhance LLMs in solving natural language graph problems. Build-a-Graph and Algorithmic prompting improve the performance of LLMs on NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to solve the most complicated graph reasoning tasks in our setup with language models remains an open research question.",main,NeurIPS,2023,Spotlight,Heng Wang;Shangbin Feng;Tianxing He;Zhaoxuan Tan;Xiaochuang Han;Yulia Tsvetkov,True,https://openreview.net/pdf?id=UDqHhbqYJV
UErNpveP6R,Evaluating Open-QA Evaluation,"This study focuses on the evaluation of the Open Question Answering (Open-QA) task, which can directly estimate the factuality of large language models (LLMs). Current automatic evaluation methods have shown limitations, indicating that human evaluation still remains the most reliable approach. We introduce a new task, QA Evaluation (QA-Eval) and the corresponding dataset EVOUNA, designed to assess the accuracy of AI-generated answers in relation to standard answers within Open-QA. Our evaluation of these methods utilizes human-annotated results to measure their performance. Specifically, the work investigates methods that show high correlation with human evaluations, deeming them more reliable. We also discuss the pitfalls of current methods and methods to improve LLM-based evaluators. We believe this new QA-Eval task and corresponding dataset EVOUNA will facilitate the development of more effective automatic evaluation tools and prove valuable for future research in this area. All resources are available at https://github.com/wangcunxiang/QA-Eval and it is under the Apache-2.0 License.",Datasets & Benchmarks,NeurIPS,2023,Poster,Cunxiang Wang;Sirui Cheng;Qipeng Guo;Yuanhao Yue;Bowen Ding;Zhikun Xu;Yidong Wang;Xiangkun Hu;Zheng Zhang;Yue Zhang,True,https://openreview.net/pdf?id=UErNpveP6R
UHIDdtxmVS,Asynchrony-Robust Collaborative Perception via Bird's Eye View Flow,"Collaborative perception can substantially boost each agent's perception ability by facilitating communication among multiple agents. However, temporal asynchrony among agents is inevitable in the real world due to communication delays, interruptions, and clock misalignments. This issue causes information mismatch during multi-agent fusion, seriously shaking the foundation of collaboration. To address this issue, we propose CoBEVFlow, an asynchrony-robust collaborative perception system based on bird's eye view (BEV) flow. The key intuition of CoBEVFlow is to compensate motions to align asynchronous collaboration messages sent by multiple agents. To model the motion in a scene, we propose BEV flow, which is a collection of the motion vector corresponding to each spatial location. Based on BEV flow, asynchronous perceptual features can be reassigned to appropriate positions, mitigating the impact of asynchrony. CoBEVFlow has two advantages: (i) CoBEVFlow can handle asynchronous collaboration messages sent at irregular, continuous time stamps without discretization; and (ii) with BEV flow, CoBEVFlow only transports the original perceptual features, instead of generating new perceptual features, avoiding additional noises. To validate CoBEVFlow's efficacy, we create IRregular V2V(IRV2V), the first synthetic collaborative perception dataset with various temporal asynchronies that simulate different real-world scenarios. Extensive experiments conducted on both IRV2V and the real-world dataset DAIR-V2X show that CoBEVFlow consistently outperforms other baselines and is robust in extremely asynchronous settings. The code is available at https://github.com/MediaBrain-SJTU/CoBEVFlow.",main,NeurIPS,2023,Poster,Sizhe Wei;Yuxi Wei;Yue Hu;Yifan Lu;Yiqi Zhong;Siheng Chen;Ya Zhang,True,https://openreview.net/pdf?id=UHIDdtxmVS
UQ8pDKcXTq,Graph Neural Networks for Road Safety Modeling: Datasets and Evaluations for Accident Analysis,"We consider the problem of traffic accident analysis on a road network based on road network connections and traffic volume. Previous works have designed various deep-learning methods using historical records to predict traffic accident occurrences. However, there is a lack of consensus on how accurate existing methods are, and a fundamental issue is the lack of public accident datasets for comprehensive evaluations. This paper constructs a large-scale, unified dataset of traffic accident records from official reports of various states in the US, totaling 9 million records, accompanied by road networks and traffic volume reports. Using this new dataset, we evaluate existing deep-learning methods for predicting the occurrence of accidents on road networks. Our main finding is that graph neural networks such as GraphSAGE can accurately predict the number of accidents on roads with less than 22% mean absolute error (relative to the actual count) and whether an accident will occur or not with over 87% AUROC, averaged over states. We achieve these results by using multitask learning to account for cross-state variabilities (e.g., availability of accident labels) and transfer learning to combine traffic volume with accident prediction. Ablation studies highlight the importance of road graph-structural features, amongst other features. Lastly, we discuss the implications of the analysis and develop a package for easily using our new dataset.",Datasets & Benchmarks,NeurIPS,2023,Poster,Abhinav Nippani;Dongyue Li;Haotian Ju;Haris Koutsopoulos;Hongyang R. Zhang,True,https://openreview.net/pdf?id=UQ8pDKcXTq
URoZHqAohf,ProteinGym: Large-Scale Benchmarks for Protein Fitness Prediction and Design,"Predicting the effects of mutations in proteins is critical to many applications, from understanding genetic disease to designing novel proteins that can address our most pressing challenges in climate, agriculture and healthcare. Despite a surge in machine learning-based protein models to tackle these questions, an assessment of their respective benefits is challenging due to the use of distinct, often contrived, experimental datasets, and the variable performance of models across different protein families. Addressing these challenges requires scale. To that end we introduce ProteinGym, a large-scale and holistic set of benchmarks specifically designed for protein fitness prediction and design. It encompasses both a broad collection of over 250 standardized deep mutational scanning assays, spanning millions of mutated sequences, as well as curated clinical datasets providing high-quality expert annotations about mutation effects. We devise a robust evaluation framework that combines metrics for both fitness prediction and design, factors in known limitations of the underlying experimental methods, and covers both zero-shot and supervised settings. We report the performance of a diverse set of over 70 high-performing models from various subfields (eg., alignment-based, inverse folding) into a unified benchmark suite. We open source the corresponding codebase, datasets, MSAs, structures, model predictions and develop a user-friendly website that facilitates data access and analysis.",Datasets & Benchmarks,NeurIPS,2023,Poster,Pascal Notin;Aaron W Kollasch;Daniel Ritter;Lood Van Niekerk;Steffan Paul;Han Spinner;Nathan J Rollins;Ada Shaw;Ruben Weitzman;Jonathan Frazer;Mafalda Dias;Dinko Franceschi;Rose Orenbuch;Yarin Gal;Debora Susan Marks,True,https://openreview.net/pdf?id=URoZHqAohf
UdByCgCNdr,MoCa: Measuring Human-Language Model Alignment on Causal and Moral Judgment Tasks,"Human commonsense understanding of the physical and social world is organized around intuitive theories. These theories support making causal and moral judgments. When something bad happens, we naturally ask: who did what, and why? A rich literature in cognitive science has studied people's causal and moral intuitions. This work has revealed a number of factors that systematically influence people's judgments, such as the violation of norms and whether the harm is avoidable or inevitable. We collected a dataset of stories from 24 cognitive science papers and developed a system to annotate each story with the factors they investigated. Using this dataset, we test whether large language models (LLMs) make causal and moral judgments about text-based scenarios that align with those of human participants. On the aggregate level, alignment has improved with more recent LLMs. However, using statistical analyses, we find that LLMs weigh the different factors quite differently from human participants. These results show how curated, challenge datasets combined with insights from cognitive science can help us go beyond comparisons based merely on aggregate metrics: we uncover LLMs implicit tendencies and show to what extent these align with human intuitions.",main,NeurIPS,2023,Poster,Allen Nie;Yuhui Zhang;Atharva Amdekar;Christopher J Piech;Tatsunori Hashimoto;Tobias Gerstenberg,True,https://openreview.net/pdf?id=UdByCgCNdr
UgPAaEugH3,Gigastep - One Billion Steps per Second Multi-agent Reinforcement Learning,"Multi-agent reinforcement learning (MARL) research is faced with a trade-off: it either uses complex environments requiring large compute resources, which makes it inaccessible to researchers with limited resources, or relies on simpler dynamics for faster execution, which makes the transferability of the results to more realistic tasks challenging. Motivated by these challenges, we present Gigastep, a fully vectorizable, MARL environment implemented in JAX, capable of executing up to one billion environment steps per second on consumer-grade hardware. Its design allows for comprehensive MARL experimentation, including a complex, high-dimensional space defined by 3D dynamics, stochasticity, and partial observations. Gigastep supports both collaborative and adversarial tasks, continuous and discrete action spaces, and provides RGB image and feature vector observations, allowing the evaluation of a wide range of MARL algorithms. 
We validate Gigastep's usability through an extensive set of experiments, underscoring its role in widening participation and promoting inclusivity in the MARL research community.",Datasets & Benchmarks,NeurIPS,2023,Poster,Mathias Lechner;Lianhao Yin;Tim Seyde;Tsun-Hsuan Wang;Wei Xiao;Ramin Hasani;Joshua Rountree;Daniela Rus,False,https://openreview.net/pdf?id=UgPAaEugH3
UlHueVjAKr,Textually Pretrained Speech Language Models,"Speech language models (SpeechLMs) process and generate acoustic data only, without textual supervision. In this work, we propose TWIST, a method for training SpeechLMs using a warm-start from a pretrained textual language models. We show using both automatic and human evaluations that TWIST outperforms a cold-start SpeechLM across the board. We empirically analyze the effect of different model design choices such as the speech tokenizer, the pretrained textual model, and the dataset size. We find that model and dataset scale both play an important role in constructing better-performing SpeechLMs. Based on our observations, we present the largest (to the best of our knowledge) SpeechLM both in terms of number of parameters and training data. We additionally introduce two spoken versions of the StoryCloze textual benchmark to further improve model evaluation and advance future research in the field. We make speech samples, code and models publicly available.",main,NeurIPS,2023,Poster,Michael Hassid;Tal Remez;Tu Anh Nguyen;Itai Gat;Alexis Conneau;Felix Kreuk;Jade Copet;Alexandre Défossez;Gabriel Synnaeve;Emmanuel Dupoux;Roy Schwartz;Yossi Adi,True,https://openreview.net/pdf?id=UlHueVjAKr
UpN2wfrLec,Language Is Not All You Need: Aligning Perception with Language Models,"A big convergence of language, multimodal perception, action, and world modeling is a key step toward artificial general intelligence. In this work, we introduce KOSMOS-1, a Multimodal Large Language Model (MLLM) that can perceive general modalities, learn in context (i.e., few-shot), and follow instructions (i.e., zero-shot). Specifically, we train KOSMOS-1 from scratch on web-scale multi-modal corpora, including arbitrarily interleaved text and images, image-caption pairs, and text data. We evaluate various settings, including zero-shot, few-shot, and multimodal chain-of-thought prompting, on a wide range of tasks without any gradient updates or finetuning. Experimental results show that KOSMOS-1 achieves impressive performance on (i) language understanding, generation, and even OCR-free NLP (directly fed with document images), (ii) perception-language tasks, including multimodal dialogue, image captioning, visual question answering, and (iii) vision tasks, such as image recognition with descriptions (specifying classification via text instructions). We also show that MLLMs can benefit from cross-modal transfer, i.e., transfer knowledge from language to multimodal, and from multimodal to language. In addition, we introduce a dataset of Raven IQ test, which diagnoses the nonverbal reasoning capability of MLLMs.",main,NeurIPS,2023,Poster,Shaohan Huang;Li Dong;Wenhui Wang;Yaru Hao;Saksham Singhal;Shuming Ma;Tengchao Lv;Lei Cui;Owais Khan Mohammed;Barun Patra;Qiang Liu;Kriti Aggarwal;Zewen Chi;Johan Bjorck;Vishrav Chaudhary;Subhojit Som;Xia Song;Furu Wei,True,https://openreview.net/pdf?id=UpN2wfrLec
UvX8QfhfUx,Pgx: Hardware-Accelerated Parallel Game Simulators for Reinforcement Learning,"We propose Pgx, a suite of board game reinforcement learning (RL) environments written in JAX and optimized for GPU/TPU accelerators. By leveraging JAX's auto-vectorization and parallelization over accelerators, Pgx can efficiently scale to thousands of simultaneous simulations over accelerators. In our experiments on a DGX-A100 workstation, we discovered that Pgx can simulate RL environments 10-100x faster than existing implementations available in Python. Pgx includes RL environments commonly used as benchmarks in RL research, such as backgammon, chess, shogi, and Go. Additionally, Pgx offers miniature game sets and baseline models to facilitate rapid research cycles. We demonstrate the efficient training of the Gumbel AlphaZero algorithm with Pgx environments. Overall, Pgx provides high-performance environment simulators for researchers to accelerate their RL experiments. Pgx is available at https://github.com/sotetsuk/pgx.",Datasets & Benchmarks,NeurIPS,2023,Poster,Sotetsu Koyamada;Shinri Okano;Soichiro Nishimori;Yu Murata;Keigo Habara;Haruka Kita;Shin Ishii,False,https://openreview.net/pdf?id=UvX8QfhfUx
V5eG47pyVl,Localized Symbolic Knowledge Distillation for Visual Commonsense Models,"Instruction following vision-language (VL) models offer a flexible
interface that supports a broad range of multimodal tasks in a zero-shot fashion.
However, interfaces that operate on full images do not directly enable the user to
“point to"" and access specific regions within images. This capability is important
not only to support reference-grounded VL benchmarks, but also, for practical
applications that require precise within-image reasoning. We build Localized
Visual Commonsense model which allows users to specify (multiple) regions-
as-input. We train our model by sampling localized commonsense knowledge
from a large language model (LLM): specifically, we prompt a LLM to collect
commonsense knowledge given a global literal image description and a local
literal region description automatically generated by a set of VL models. This
pipeline is scalable and fully automatic, as no aligned or human-authored image
and text pairs are required. With a separately trained critic model that selects
high quality examples, we find that training on the localized commonsense corpus
expanded solely from images can successfully distill existing VL models to support
a reference-as-input interface. Empirical results and human evaluations in zero-shot
settings demonstrate that our distillation method results in more precise VL models
of reasoning compared to a baseline of passing a generated referring expression.",main,NeurIPS,2023,Poster,Jae Sung Park;Jack Hessel;Khyathi Chandu;Paul Pu Liang;Ximing Lu;Peter West;Youngjae Yu;Qiuyuan Huang;Jianfeng Gao;Ali Farhadi;Yejin Choi,True,https://openreview.net/pdf?id=V5eG47pyVl
VH1vxapUTs,Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean,"We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire modeling in the Mediterranean. Mesogeos integrates variables representing wildfire drivers (meteorology, vegetation, human activity) and historical records of wildfire ignitions and burned areas for 17 years (2006-2022). It is designed as a cloud-friendly spatio-temporal dataset, namely a datacube, harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The datacube structure offers opportunities to assess machine learning (ML) usage in various wildfire modeling tasks. We extract two ML-ready datasets that establish distinct tracks to demonstrate this potential: (1) short-term wildfire danger forecasting and (2) final burned area estimation given the point of ignition. We define appropriate metrics and baselines to evaluate the performance of models in each track. By publishing the datacube, along with the code to create the ML datasets and models, we encourage the community to foster the implementation of additional tracks for mitigating the increasing threat of wildfires in the Mediterranean.",Datasets & Benchmarks,NeurIPS,2023,Oral,Spyros Kondylatos;Ioannis Prapas;Gustau Camps-Valls;Ioannis Papoutsis,True,https://openreview.net/pdf?id=VH1vxapUTs
VIRKdeFJIg,Improving multimodal datasets with image captioning,"Massive web datasets play a key role in the success of large vision-language models like CLIP and Flamingo. However, the raw web data is noisy, and existing filtering methods to reduce noise often come at the expense of data diversity. Our work focuses on caption quality as one major source of noise, and studies how generated captions can increase the utility of web-scraped datapoints with nondescript text. Through exploring different mixing strategies for raw and generated captions, we outperform the best filtering method proposed by the DataComp benchmark by 2% on ImageNet and 4% on average across 38 tasks, given a candidate pool of 128M image-text pairs. Our best approach is also 2x better at Flickr and MS-COCO retrieval. We then analyze what makes synthetic captions an effective source of text supervision. In experimenting with different image captioning models, we also demonstrate that the performance of a model on standard image captioning benchmarks (e.g., NoCaps CIDEr) is not a reliable indicator of the utility of the captions it generates for multimodal training. Finally, our experiments with using generated captions at DataComp's large scale (1.28B image-text pairs) offer insights into the limitations of synthetic text, as well as the importance of image curation with increasing training data quantity. The synthetic captions used in our experiments are now available on HuggingFace.",Datasets & Benchmarks,NeurIPS,2023,Poster,Thao Nguyen;Samir Yitzhak Gadre;Gabriel Ilharco;Sewoong Oh;Ludwig Schmidt,True,https://openreview.net/pdf?id=VIRKdeFJIg
VSJotgbPHF,OpenAssistant Conversations - Democratizing Large Language Model Alignment,"Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT.
Alignment techniques such as supervised fine-tuning (\\\\textit{SFT}) and  reinforcement learning from human feedback (\\\\textit{RLHF}) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains.
However, state-of-the-art alignment techniques like \\\\textit{RLHF} rely on high-quality human feedback data, which is expensive to create and often remains proprietary.
In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotated conversation trees.
The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers.
Models trained on OpenAssistant Conversations show consistent improvements on standard benchmarks over respective base models.
We release our code\\\\footnote{\\\\git} and data\\\\footnote{\\\\data} under a fully permissive licence.",Datasets & Benchmarks,NeurIPS,2023,Oral,Andreas Köpf;Yannic Kilcher;Dimitri von Rütte;Sotiris Anagnostidis;Zhi Rui Tam;Keith Stevens;Abdullah Barhoum;Duc Minh Nguyen;Oliver Stanley;Richárd Nagyfi;Shahul ES;Sameer Suri;David Alexandrovich Glushkov;Arnav Varma Dantuluri;Andrew Maguire;Christoph Schuhmann;Huu Nguyen;Alexander Julian Mattick,True,https://openreview.net/pdf?id=VSJotgbPHF
VeJgZYhT7H,Learning to Taste: A Multimodal Wine Dataset,"We present WineSensed, a large multimodal wine dataset for studying the relations between visual perception, language, and flavor. The dataset encompasses 897k images of wine labels and 824k reviews of wines curated from the Vivino platform. It has over 350k unique bottlings, annotated with year, region, rating, alcohol percentage, price, and grape composition. We obtained fine-grained flavor annotations on a subset by conducting a wine-tasting experiment with 256 participants who were asked to rank wines based on their similarity in flavor, resulting in more than 5k pairwise flavor distances. We propose a low-dimensional concept embedding algorithm that combines human experience with automatic machine similarity kernels. We demonstrate that this shared concept embedding space improves upon separate embedding spaces for coarse flavor classification  (alcohol percentage, country, grape, price, rating) and representing human perception of flavor.",Datasets & Benchmarks,NeurIPS,2023,Poster,Thoranna Bender;Simon Moe Sørensen;Alireza Kashani;Kristjan Eldjarn Hjorleifsson;Grethe Hyldig;Søren Hauberg;Serge Belongie;Frederik Rahbæk Warburg,True,https://openreview.net/pdf?id=VeJgZYhT7H
VfP6VTVsHc,RDumb: A simple approach that questions our progress in continual test-time adaptation,"Test-Time Adaptation (TTA) allows to update pre-trained models to changing data distributions at deployment time. While early work tested these algorithms for individual fixed distribution shifts, recent work proposed and applied methods for continual adaptation over long timescales. To examine the reported progress in the field, we propose the Continually Changing Corruptions (CCC) benchmark to measure asymptotic performance of TTA techniques. We find that eventually all but one state-of-the-art methods collapse and perform worse than a non-adapting model, including models specifically proposed to be robust to performance collapse. In addition, we introduce a simple baseline, ""RDumb"", that periodically resets the model to its pretrained state. RDumb performs better or on par with the previously proposed state-of-the-art in all considered benchmarks.
Our results show that previous TTA approaches are neither effective at regularizing adaptation to avoid collapse nor able to outperform a simplistic resetting strategy.",main,NeurIPS,2023,Poster,Ori Press;Steffen Schneider;Matthias Kuemmerer;Matthias Bethge,True,https://openreview.net/pdf?id=VfP6VTVsHc
ViFTWelHVZ,Efficient Activation Function Optimization through Surrogate Modeling,"Carefully designed activation functions can improve the performance of neural networks in many machine learning tasks.  However, it is difficult for humans to construct optimal activation functions, and current activation function search algorithms are prohibitively expensive.  This paper aims to improve the state of the art through three steps: First, the benchmark datasets Act-Bench-CNN, Act-Bench-ResNet, and Act-Bench-ViT were created by training convolutional, residual, and vision transformer architectures from scratch with 2,913 systematically generated activation functions. Second, a characterization of the benchmark space was developed, leading to a new surrogate-based method for optimization. More specifically, the spectrum of the Fisher information matrix associated with the model's predictive distribution at initialization and the activation function's output distribution were found to be highly predictive of performance. Third, the surrogate was used to discover improved activation functions in several real-world tasks, with a surprising finding: a sigmoidal design that outperformed all other activation functions was discovered, challenging the status quo of always using rectifier nonlinearities in deep learning.  Each of these steps is a contribution in its own right; together they serve as a practical and theoretical foundation for further research on activation function optimization.",main,NeurIPS,2023,Poster,Garrett Bingham;Risto Miikkulainen,True,https://openreview.net/pdf?id=ViFTWelHVZ
Vn5qZGxGj3,SatBird: a Dataset for Bird Species Distribution Modeling using Remote Sensing and Citizen Science Data,"Biodiversity is declining at an unprecedented rate, impacting ecosystem services necessary to ensure food, water, and human health and well-being. Understanding the distribution of species and their habitats is crucial for conservation policy planning. 
However, traditional methods in ecology for species distribution models (SDMs) generally focus either on narrow sets of species or narrow geographical areas and there remain significant knowledge gaps about the distribution of species. A major reason for this is the limited availability of data traditionally used, due to the prohibitive amount of effort and expertise required for traditional field monitoring. 
The wide availability of remote sensing data and the growing adoption of citizen science tools to collect species observations data at low cost offer an opportunity for improving biodiversity monitoring and enabling the modelling of complex ecosystems. We introduce a novel task for mapping bird species to their habitats by predicting species encounter rates from satellite images, and present SatBird, a satellite dataset of locations in the USA with labels derived from presence-absence observation data from the citizen science database eBird, considering summer (breeding) and winter seasons. We also provide a dataset in Kenya representing low-data regimes. We additionally provide environmental data and species range maps for each location.  We benchmark a set of baselines on our dataset, including SOTA models for remote sensing tasks. SatBird opens up possibilities for scalably modelling properties of ecosystems worldwide.",Datasets & Benchmarks,NeurIPS,2023,Poster,Mélisande Teng;Amna Elmustafa;Benjamin Akera;Yoshua Bengio;Hager Radi;Hugo Larochelle;David Rolnick,True,https://openreview.net/pdf?id=Vn5qZGxGj3
VtbKj2xlhI,WBCAtt: A White Blood Cell Dataset Annotated with Detailed Morphological Attributes,"The examination of blood samples at a microscopic level plays a fundamental role in clinical diagnostics. For instance, an in-depth study of White Blood Cells (WBCs), a crucial component of our blood, is essential for diagnosing blood-related diseases such as leukemia and anemia. While multiple datasets containing WBC images have been proposed, they mostly focus on cell categorization, often lacking the necessary morphological details to explain such categorizations, despite the importance of explainable artificial intelligence (XAI) in medical domains. This paper seeks to address this limitation by introducing comprehensive annotations for WBC images. Through collaboration with pathologists, a thorough literature review, and manual inspection of microscopic images, we have identified 11 morphological attributes associated with the cell and its components (nucleus, cytoplasm, and granules). We then annotated ten thousand WBC images with these attributes, resulting in 113k labels (11 attributes x 10.3k images). Annotating at this level of detail and scale is unprecedented, offering unique value to AI in pathology. Moreover, we conduct experiments to predict these attributes from cell images, and also demonstrate specific applications that can benefit from our detailed annotations. Overall, our dataset paves the way for interpreting WBC recognition models, further advancing XAI in the fields of pathology and hematology.",Datasets & Benchmarks,NeurIPS,2023,Poster,Satoshi Tsutsui;Winnie Pang;Bihan Wen,True,https://openreview.net/pdf?id=VtbKj2xlhI
VzmpXQAn6E,Exposing Attention Glitches with Flip-Flop Language Modeling,"Why do large language models sometimes output factual inaccuracies and exhibit erroneous reasoning? The brittleness of these models, particularly when executing long chains of reasoning, currently seems to be an inevitable price to pay for their advanced capabilities of coherently synthesizing knowledge, pragmatics, and abstract thought. Towards making sense of this fundamentally unsolved problem, this work identifies and analyzes the phenomenon of _attention glitches_, in which the Transformer architecture's inductive biases intermittently fail to capture robust reasoning. To isolate the issue, we introduce _flip-flop language modeling_ (FFLM), a parametric family of synthetic benchmarks designed to probe the extrapolative behavior of neural language models. This simple generative task requires a model to copy binary symbols over long-range dependencies, ignoring the tokens in between. We find that Transformer FFLMs suffer from a long tail of sporadic reasoning errors, some of which we can eliminate using various regularization techniques. Our preliminary mechanistic analyses show why the remaining errors may be very difficult to diagnose and resolve. We hypothesize that attention glitches account for (some of) the closed-domain hallucinations in natural LLMs.",main,NeurIPS,2023,Spotlight,Bingbin Liu;Jordan T. Ash;Surbhi Goel;Akshay Krishnamurthy;Cyril Zhang,True,https://openreview.net/pdf?id=VzmpXQAn6E
W23ZTdsabj,Are Vision Transformers More Data Hungry Than Newborn Visual Systems?,"Vision transformers (ViTs) are top-performing models on many computer vision benchmarks and can accurately predict human behavior on object recognition tasks. However, researchers question the value of using ViTs as models of biological learning because ViTs are thought to be more “data hungry” than brains, with ViTs requiring more training data than brains to reach similar levels of performance. To test this assumption, we directly compared the learning abilities of ViTs and animals, by performing parallel controlled-rearing experiments on ViTs and newborn chicks. We first raised chicks in impoverished visual environments containing a single object, then simulated the training data available in those environments by building virtual animal chambers in a video game engine. We recorded the first-person images acquired by agents moving through the virtual chambers and used those images to train self-supervised ViTs that leverage time as a teaching signal, akin to biological visual systems. When ViTs were trained “through the eyes” of newborn chicks, the ViTs solved the same view-invariant object recognition tasks as the chicks. Thus, ViTs were not more data hungry than newborn chicks: both learned view-invariant object representations in impoverished visual environments. The flexible and generic attention-based learning mechanism in ViTs—combined with the embodied data streams available to newborn animals—appears sufficient to drive the development of animal-like object recognition.",main,NeurIPS,2023,Poster,Lalit Pandey;Samantha Marie Waters Wood;Justin Newell Wood,True,https://openreview.net/pdf?id=W23ZTdsabj
W5If9P1xqO,ClimSim: A large multi-scale dataset for hybrid physics-ML climate emulation,"Modern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise predictions of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore's Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset designed for hybrid ML-physics research. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics on a host climate simulator's macro-scale physical state.

The dataset is global in coverage, spans multiple years at high sampling frequency, and is designed such that resulting emulators are compatible with downstream coupling into operational climate simulators. We implement a range of deterministic and stochastic regression baselines to highlight the ML challenges and their scoring. The data (https://huggingface.co/datasets/LEAP/ClimSim_high-res) and code (https://leap-stc.github.io/ClimSim) are released openly to support the development of hybrid ML-physics and high-fidelity climate simulations for the benefit of science and society.",Datasets & Benchmarks,NeurIPS,2023,Oral,Sungduk Yu;Walter Hannah;Liran Peng;Jerry Lin;Mohamed Aziz Bhouri;Ritwik Gupta;Björn Lütjens;Justus Christopher Will;Gunnar Behrens;Julius Busecke;Nora Loose;Charles I Stern;Tom Beucler;Bryce Harrop;Benjamin R Hillman;Andrea Jenney;Savannah Ferretti;Nana Liu;Anima Anandkumar;Noah D Brenowitz;Veronika Eyring;Nicholas Geneva;Pierre Gentine;Stephan Mandt;Jaideep Pathak;Akshay Subramaniam;Carl Vondrick;Rose Yu;Laure Zanna;Tian Zheng;Ryan Abernathey;Fiaz Ahmed;David C Bader;Pierre Baldi;Elizabeth Barnes;Christopher Bretherton;Peter Caldwell;Wayne Chuang;Yilun Han;YU HUANG;Fernando Iglesias-Suarez;Sanket Jantre;Karthik Kashinath;Marat Khairoutdinov;Thorsten Kurth;Nicholas Lutsko;Po-Lun Ma;Griffin Mooers;J. David Neelin;David Randall;Sara Shamekh;Mark A Taylor;Nathan Urban;Janni Yuval;Guang Zhang;Michael Pritchard,True,https://openreview.net/pdf?id=W5If9P1xqO
W6xb7bkbYA,UDC-SIT: A Real-World Dataset for Under-Display Cameras,"Under Display Camera (UDC) is a novel imaging system that mounts a digital camera lens beneath a display panel with the panel covering the camera. However, the display panel causes severe degradation to captured images, such as low transmittance, blur, noise, and flare. The restoration of UDC-degraded images is challenging because of the unique luminance and diverse patterns of flares. Existing UDC dataset studies focus on unrealistic or synthetic UDC degradation rather than real-world UDC images. In this paper, we propose a real-world UDC dataset called UDC-SIT. To obtain the non-degraded and UDC-degraded images for the same scene, we propose an image-capturing system and an image alignment technique that exploits discrete Fourier transform (DFT) to align a pair of captured images. UDC-SIT also includes comprehensive annotations missing from other UDC datasets, such as light source, day/night, indoor/outdoor, and flare components (e.g.,  shimmers, streaks, and glares). We compare UDC-SIT with four existing representative UDC datasets and present the problems with existing UDC datasets. To show UDC-SIT's effectiveness, we compare UDC-SIT and a representative synthetic UDC dataset using four representative learnable image restoration models. The result indicates that the models trained with the synthetic UDC dataset are impractical because the synthetic UDC dataset does not reflect the actual characteristics of UDC-degraded images. UDC-SIT can enable further exploration in the UDC image restoration area and provide better insights into the problem. UDC-SIT is available at: https://github.com/mcrl/UDC-SIT.",Datasets & Benchmarks,NeurIPS,2023,Poster,Kyusu Ahn;Byeonghyun Ko;HyunGyu Lee;Chanwoo Park;Jaejin Lee,True,https://openreview.net/pdf?id=W6xb7bkbYA
WZmlxIuIGR,Safety Gymnasium: A Unified Safe Reinforcement Learning Benchmark,"Artificial intelligence (AI) systems possess significant potential to drive societal progress. However, their deployment often faces obstacles due to substantial safety concerns. Safe reinforcement learning (SafeRL) emerges as a solution to optimize policies while simultaneously adhering to multiple constraints, thereby addressing the challenge of integrating reinforcement learning in safety-critical scenarios. In this paper, we present an environment suite called Safety-Gymnasium, which encompasses safety-critical tasks in both single and multi-agent scenarios, accepting vector and vision-only input. Additionally, we offer a library of algorithms named Safe Policy Optimization (SafePO), comprising 16 state-of-the-art SafeRL algorithms. This comprehensive library can serve as a validation tool for the research community. By introducing this benchmark, we aim to facilitate the evaluation and comparison of safety performance, thus fostering the development of reinforcement learning for safer, more reliable, and responsible real-world applications. The website of this project can be accessed at https://sites.google.com/view/safety-gymnasium.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jiaming Ji;Borong Zhang;Jiayi Zhou;Xuehai Pan;Weidong Huang;Ruiyang Sun;Yiran Geng;Yifan Zhong;Josef Dai;Yaodong Yang,True,https://openreview.net/pdf?id=WZmlxIuIGR
WbFhFvjjKj,"Paraphrasing evades detectors of AI-generated text, but retrieval is an effective defense","The rise in malicious usage of large language models, such as fake content creation and academic plagiarism, has motivated the development of approaches that identify AI-generated text, including those based on watermarking or outlier detection. However, the robustness of these detection algorithms to paraphrases of AI-generated text remains unclear. To stress test these detectors, we build a 11B parameter paraphrase generation model (DIPPER) that can paraphrase paragraphs, condition on surrounding context, and control lexical diversity and content reordering. Paraphrasing text generated by three large language models (including GPT3.5-davinci-003) with DIPPER successfully evades several detectors, including watermarking, GPTZero, DetectGPT, and OpenAI's text classifier. For example, DIPPER drops detection accuracy of DetectGPT from 70.3% to 4.6% (at a constant false positive rate of 1%), without appreciably modifying the input semantics.
To increase the robustness of AI-generated text detection to paraphrase attacks, we introduce a simple defense that relies on retrieving semantically-similar generations and must be maintained by a language model API provider. Given a candidate text, our algorithm searches a database of sequences previously generated by the API, looking for sequences that match the candidate text within a certain threshold. We empirically verify our defense using a database of 15M generations from a fine-tuned T5-XXL model and find that it can detect 80% to 97% of paraphrased generations across different settings while only classifying 1% of human-written sequences as AI-generated. We open-source our models, code and data.",main,NeurIPS,2023,Poster,Kalpesh Krishna;Yixiao Song;Marzena Karpinska;John Frederick Wieting;Mohit Iyyer,True,https://openreview.net/pdf?id=WbFhFvjjKj
Wbr51vK331,GenEval: An object-focused framework for evaluating text-to-image alignment,"Recent breakthroughs in diffusion models, multimodal pretraining, and efficient finetuning have led to an explosion of text-to-image generative models. 
Given human evaluation is expensive and difficult to scale, automated methods are critical for evaluating the increasingly large number of new models. 
However, most current automated evaluation metrics like FID or CLIPScore only offer a distribution-level measure of image quality or image-text alignment, and are unsuited for fine-grained or instance-level analysis. 
In this paper, we introduce GenEval, an object-focused framework to evaluate compositional image properties such as object co-occurrence, position, count, and color. 
We show that current object detection models can be leveraged to evaluate text-to-image models on a variety of generation tasks with strong human agreement, and that other discriminative vision models can be linked to this pipeline to further verify properties like object color. 
We then evaluate several open-source text-to-image models and analyze their relative reasoning capabilities on our benchmark. 
We find that recent models demonstrate significant improvement on these tasks, though they are still lacking in complex capabilities such as spatial relations and attribute binding. 
Finally, we demonstrate how GenEval might be used to help discover existing failure modes, in order to inform development of the next generation of text-to-image models. 
Our code to run the GenEval framework will be made publicly available at https://github.com/djghosh13/geneval.",Datasets & Benchmarks,NeurIPS,2023,Poster,Dhruba Ghosh;Hannaneh Hajishirzi;Ludwig Schmidt,False,https://openreview.net/pdf?id=Wbr51vK331
WqSPQFxFRC,LegalBench: A Collaboratively Built Benchmark for Measuring Legal Reasoning in Large Language Models,"The advent of large language models (LLMs) and their adoption by the legal community has given rise to the question: what types of legal reasoning can LLMs perform? To enable greater study of this question, we present LegalBench: a collaboratively constructed legal reasoning benchmark consisting of 162 tasks covering six different types of legal reasoning. LegalBench was built through an interdisciplinary process, in which we collected tasks designed and hand-crafted by legal professionals. Because these subject matter experts took a leading role in construction, tasks either measure legal reasoning capabilities that are practically useful, or measure reasoning skills that lawyers find interesting. To enable cross-disciplinary conversations about LLMs in the law, we additionally show how popular legal frameworks for describing legal reasoning—which distinguish between its many forms—correspond to LegalBench tasks, thus giving lawyers and LLM developers a common vocabulary. This paper describes LegalBench, presents an empirical evaluation of 20 open-source and commercial LLMs, and illustrates the types of research explorations LegalBench enables.",Datasets & Benchmarks,NeurIPS,2023,Poster,Neel Guha;Julian Nyarko;Daniel E. Ho;Christopher Re;Adam Chilton;Aditya Narayana;Alex Chohlas-Wood;Austin Peters;Brandon Waldon;Daniel Rockmore;Diego Zambrano;Dmitry Talisman;Enam Hoque;Faiz Surani;Frank Fagan;Galit Sarfaty;Gregory M. Dickinson;Haggai Porat;Jason Hegland;Jessica Wu;Joe Nudell;Joel Niklaus;John J Nay;Jonathan H. Choi;Kevin Tobia;Margaret Hagan;Megan Ma;Michael Livermore;Nikon Rasumov-Rahe;Nils Holzenberger;Noam Kolt;Peter Henderson;Sean Rehaag;Sharad Goel;Shang Gao;Spencer Williams;Sunny Gandhi;Tom Zur;Varun Iyer;Zehua Li,True,https://openreview.net/pdf?id=WqSPQFxFRC
WtajAo0JWU,Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset,"In this paper, we present Motion-X, a large-scale 3D expressive whole-body motion dataset. Existing motion datasets predominantly contain body-only poses, lacking facial expressions, hand gestures, and fine-grained pose descriptions. Moreover, they are primarily collected from limited laboratory scenes with textual descriptions manually labeled, which greatly limits their scalability. To overcome these limitations, we develop a whole-body motion and text annotation pipeline, which can automatically annotate motion from either single- or multi-view videos and provide comprehensive semantic labels for each video and fine-grained whole-body pose descriptions for each frame. This pipeline is of high precision, cost-effective, and scalable for further research. Based on it, we construct Motion-X, which comprises 15.6M precise 3D whole-body pose annotations (i.e., SMPL-X) covering 81.1K motion sequences from massive scenes. Besides, Motion-X provides 15.6M frame-level whole-body pose descriptions and 81.1K sequence-level semantic labels. Comprehensive experiments demonstrate the accuracy of the annotation pipeline and the significant benefit of Motion-X in enhancing expressive, diverse, and natural motion generation, as well as 3D whole-body human mesh recovery.",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=WtajAo0JWU
Wz2BJNQlyI,VisAlign: Dataset for Measuring the Alignment between AI and Humans in Visual Perception,"AI alignment refers to models acting towards human-intended goals, preferences, or ethical principles. Analyzing the similarity between models and humans can be a proxy measure for ensuring AI safety. In this paper, we focus on the models' visual perception alignment with humans, further referred to as AI-human visual alignment. Specifically, we propose a new dataset for measuring AI-human visual alignment in terms of image classification. In order to evaluate AI-human visual alignment, a dataset should encompass samples with various scenarios and have gold human perception labels. Our dataset consists of three groups of samples, namely Must-Act (i.e., Must-Classify), Must-Abstain, and Uncertain, based on the quantity and clarity of visual information in an image and further divided into eight categories. All samples have a gold human perception label; even Uncertain (e.g., severely blurry) sample labels were obtained via crowd-sourcing. The validity of our dataset is verified by sampling theory, statistical theories related to survey design, and experts in the related fields. Using our dataset, we analyze the visual alignment and reliability of five popular visual perception models and seven abstention methods. Our code and data is available at https://github.com/jiyounglee-0523/VisAlign.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jiyoung Lee;Seungho Kim;Seunghyun Won;Joonseok Lee;Marzyeh Ghassemi;James Thorne;Jaeseok Choi;O-Kil Kwon;Edward Choi,True,https://openreview.net/pdf?id=Wz2BJNQlyI
XOpaPrb0U5,QATCH: Benchmarking SQL-centric tasks with Table Representation Learning Models on Your Data,"Table Representation Learning (TRL) models are commonly pre-trained on large open-domain datasets comprising millions of tables and then used to address downstream tasks. Choosing the right TRL model to use on proprietary data can be challenging, as the best results depend on the content domain, schema, and data quality. Our purpose is to support end-users in testing TRL models on proprietary data in two established SQL-centric tasks, i.e., Question Answering (QA) and Semantic Parsing (SP). We present QATCH (Query-Aided TRL Checklist), a toolbox to highlight TRL models’ strengths and weaknesses on relational tables unseen at training time. For an input table, QATCH automatically generates a testing checklist tailored to QA and SP. Checklist generation is driven by a SQL query engine that crafts tests of different complexity. This design facilitates inherent portability, allowing the checks to be used by alternative models. We also introduce a set of cross-task performance metrics evaluating the TRL model’s performance over its output. Finally, we show how QATCH automatically generates tests for proprietary datasets to evaluate various state-of-the-art models including TAPAS, TAPEX, and CHATGPT.",Datasets & Benchmarks,NeurIPS,2023,Poster,Simone Papicchio;Paolo Papotti;Luca Cagliero,False,https://openreview.net/pdf?id=XOpaPrb0U5
XYxNklOMMX,Benchmarking Distribution Shift in Tabular Data with TableShift,"Robustness to distribution shift has become a growing concern for text and image models as they transition from research subjects to deployment in the real world. However, high-quality benchmarks for distribution shift in tabular machine learning tasks are still lacking despite the widespread real-world use of tabular data and differences in the models used for tabular data in comparison to text and images. As a consequence, the robustness of tabular models to distribution shift is poorly understood. To address this issue, we introduce TableShift, a distribution shift benchmark for tabular data. TableShift contains 15 binary classification tasks in total, each with an associated shift, and includes a diverse set of data sources, prediction targets, and distribution shifts. The benchmark covers domains including finance, education, public policy, healthcare, and civic participation, and is accessible using only a few lines of Python code via the TableShift API. We conduct a large-scale study comparing several state-of-the-art tabular data models alongside robust learning and domain generalization methods on the benchmark tasks. Our study demonstrates (1) a linear trend between in-distribution (ID) and out-of-distribution (OOD) accuracy; (2) domain robustness methods can reduce shift gaps but at the cost of reduced ID accuracy; (3) a strong relationship between shift gap (difference between ID and OOD performance) and shifts in the label distribution. The benchmark data, Python package, model implementations, and more information about TableShift are available at https://github.com/mlfoundations/tableshift and https://tableshift.org .",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=XYxNklOMMX
XZf2bnMBag,Realistic Synthetic Financial Transactions for Anti-Money Laundering Models,"With the widespread digitization of finance and the increasing popularity of cryptocurrencies, the sophistication of fraud schemes devised by cybercriminals is growing. Money laundering -- the movement of illicit funds to conceal their origins -- can cross bank and national boundaries, producing complex transaction patterns. The UN estimates 2-5\\\\% of global GDP or \\\\$0.8 - \\\\$2.0 trillion dollars are laundered globally each year.  Unfortunately, real data to train machine learning models to detect laundering is generally not available, and previous synthetic data generators have had significant shortcomings. A realistic, standardized, publicly-available benchmark is needed for comparing models and for the advancement of the area.

To this end, this paper contributes a synthetic financial transaction dataset generator and a set of synthetically generated AML (Anti-Money Laundering) datasets.  We have calibrated this agent-based generator to match real transactions as closely as possible and made the datasets public. We describe the generator in detail and demonstrate how the datasets generated can help compare different machine learning models in terms of their AML abilities. In a key way, using synthetic data in these comparisons can be even better than using real data: the ground truth labels are complete, whilst many laundering transactions in real data are never detected.",Datasets & Benchmarks,NeurIPS,2023,Poster,Erik Altman;Jovan Blanuša;Luc Von Niederhäusern;Beni Egressy;Andreea Anghel;Kubilay Atasu,True,https://openreview.net/pdf?id=XZf2bnMBag
XbVnNXaIQY,Holistic Transfer: Towards Non-Disruptive Fine-Tuning with Partial Target Data,"We propose a learning problem involving adapting a pre-trained source model to the target domain for classifying all classes that appeared in the source data, using target data that covers only a partial label space. This problem is practical, as it is unrealistic for the target end-users to collect data for all classes prior to adaptation. However, it has received limited attention in the literature. To shed light on this issue, we construct benchmark datasets and conduct extensive experiments to uncover the inherent challenges. We found a dilemma --- on the one hand, adapting to the new target domain is important to claim better performance; on the other hand, we observe that preserving the classification accuracy of classes missing in the target adaptation data is highly challenging, let alone improving them. To tackle this, we identify two key directions: 1) disentangling domain gradients from classification gradients, and 2) preserving class relationships. We present several effective solutions that maintain the accuracy of the missing classes and enhance the overall performance, establishing solid baselines for holistic transfer of pre-trained models with partial target data.",main,NeurIPS,2023,Poster,Cheng-Hao Tu;Hong-You Chen;Zheda Mai;Jike Zhong;Vardaan Pahuja;Tanya Berger-Wolf;Song Gao;Charles Stewart;Yu Su;Wei-Lun Chao,True,https://openreview.net/pdf?id=XbVnNXaIQY
XfQbPqRPXi,Towards Better Evaluation of GNN Expressiveness with BREC Dataset,"Research on the theoretical expressiveness of Graph Neural Networks~(GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, most methods do not have a uniform expressiveness measure except for a few that strictly follow the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test hierarchy. Their theoretical analyses are often limited to distinguishing certain families of non-isomorphic graphs, leading to difficulties in quantitatively comparing their expressiveness. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100\\\\% accuracy), granularity (models tend to be either 100\\\\% correct or near random guess), and scale (only a few essentially different graphs in each dataset). To address these limitations, we propose a new expressiveness dataset, **BREC**, which includes 400 pairs of non-isomorphic graphs carefully selected from four primary categories (Basic, Regular, Extension, and CFI). These graphs have higher difficulty (up to 4-WL-indistinguishable), finer granularity (able to compare models between 1-WL and 3-WL), and a larger scale (400 pairs). Further, we synthetically test 23 models with higher-than-1-WL expressiveness on our BREC dataset. Our experiment gives the first thorough comparison of the expressiveness of those state-of-the-art beyond-1-WL GNN models. We expect this dataset to serve as a benchmark for testing the expressiveness of future GNNs. Our dataset and evaluation code are released at: https://github.com/GraphPKU/BREC.",Datasets & Benchmarks,NeurIPS,2023,Reject,Yanbo Wang;Muhan Zhang,True,https://openreview.net/pdf?id=XfQbPqRPXi
XjaWEAyToL,Scientific Document Retrieval using Multi-level Aspect-based Queries,"In scientific research, the ability to effectively retrieve relevant documents based on complex, multifaceted queries is critical. Existing evaluation datasets for this task are limited, primarily due to the high costs and effort required to annotate resources that effectively represent complex queries. To address this, we propose a novel task,  $\\\\textbf{S}$cientific $\\\\textbf{Do}$cument $\\\\textbf{R}$etrieval using $\\\\textbf{M}$ulti-level $\\\\textbf{A}$spect-based qu$\\\\textbf{E}$ries (DORIS-MAE), which is designed to handle the complex nature of user queries in scientific research. We developed a benchmark dataset within the field of computer science, consisting of 100 human-authored complex query cases. For each complex query, we assembled a collection of 100 relevant documents and produced annotated relevance scores for ranking them. Recognizing the significant labor of expert annotation, we also introduce Anno-GPT, a scalable framework for evaluating the viability of Large Language Models (LLMs) such as ChatGPT-3.5 for expert-level dataset annotation tasks. The application of Anno-GPT to annotate the DORIS-MAE dataset resulted in a 500x reduction in cost, without compromising quality. Furthermore, due to the multi-tiered structure of these complex queries, our DORIS-MAE dataset can be extended to over 4,000 sub-query test cases without requiring additional annotation. We evaluated 17 recent retrieval methods on DORIS-MAE, observing notable performance drops compared to traditional datasets. This highlights DORIS-MAE's challenges and the need for better approaches to handle complex, multifaceted queries in scientific research. Our dataset and codebase are available at https://github.com/Real-Doris-Mae/Doris-Mae-Dataset .",Datasets & Benchmarks,NeurIPS,2023,Poster,Jianyou Wang;Kaicheng Wang;Xiaoyue Wang;Prudhviraj Naidu;Leon Bergen;Ramamohan Paturi,True,https://openreview.net/pdf?id=XjaWEAyToL
Xoi31wJ5iI,Seeing is not always believing: Benchmarking Human and Model Perception of AI-Generated Images,"Photos serve as a way for humans to record what they experience in their daily lives, and they are often regarded as trustworthy sources of information. However, there is a growing concern that the advancement of artificial intelligence (AI) technology may produce fake photos, which can create confusion and diminish trust in photographs. This study aims to comprehensively evaluate agents for distinguishing state-of-the-art AI-generated visual content. Our study benchmarks both human capability and cutting-edge fake image detection AI algorithms, using a newly collected large-scale fake image dataset Fake2M. In our human perception evaluation, titled HPBench, we discovered that humans struggle significantly to distinguish real photos from AI-generated ones, with a misclassification rate of 38.7\\\\%. Along with this, we conduct the model capability of AI-Generated images detection evaluation MPBench and the top-performing model from MPBench achieves a 13\\\\% failure rate under the same setting used in the human evaluation.
We hope that our study can raise awareness of the potential risks of AI-generated images and facilitate further research to prevent the spread of false information. More information can refer to https://github.com/Inf-imagine/Sentry.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zeyu Lu;Di Huang;LEI BAI;Jingjing Qu;Chengyue Wu;Xihui Liu;Wanli Ouyang,True,https://openreview.net/pdf?id=Xoi31wJ5iI
Y45ZCxslFx,MADLAD-400: A Multilingual And Document-Level Large Audited Dataset,"We introduce MADLAD-400, a manually audited, general domain 3T token monolingual dataset based on CommonCrawl, spanning 419 languages. We discuss the limitations revealed by self-auditing MADLAD-400, and the role data auditing had in the dataset creation process. We then train and release a 10.7B-parameter multilingual machine translation model on 250 billion tokens covering over 450 languages using publicly available data, and find that it is competitive with models that are significantly larger, and report the results on different domains. In addition, we train a 8B-parameter language model, and assess the results on few-shot translation. We make the baseline models available to the research community.",Datasets & Benchmarks,NeurIPS,2023,Poster,Sneha Kudugunta;Isaac Rayburn Caswell;Biao Zhang;Xavier Garcia;Derrick Xin;Aditya Kusupati;Romi Stella;Ankur Bapna;Orhan Firat,True,https://openreview.net/pdf?id=Y45ZCxslFx
Y4GZ2w74f4,VisIT-Bench: A Dynamic Benchmark for Evaluating Instruction-Following Vision-and-Language Models,"We introduce VisIT-Bench (Visual InsTruction Benchmark), a benchmark for evaluating instruction-following vision-language models for real-world use. Our starting point is curating 70 ""instruction families"" that we envision instruction tuned vision-language models should be able to address. Extending beyond evaluations like VQAv2 and COCO, tasks range from basic recognition to game playing and creative generation. Following curation, our dataset comprises 592 test queries, each with a human-authored instruction-conditioned caption. These descriptions surface instruction-specific factors, e.g., for an instruction asking about the accessibility of a storefront for wheelchair users, the instruction-conditioned caption describes ramps/potential obstacles. These descriptions enable 1) collecting human-verified reference outputs for each instance; and 2) automatic evaluation of candidate multimodal generations using a text-only LLM, aligning with human judgment. We quantify quality gaps between models and references using both human and automatic evaluations; e.g., the top-performing instruction-following model wins against the GPT-4 reference in just 27% of the comparison. VisIT-Bench is dynamic to participate, practitioners simply submit their model's response on the project website; Data, code and leaderboard is available at https://visit-bench.github.io/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yonatan Bitton;Hritik Bansal;Jack Hessel;Rulin Shao;Wanrong Zhu;Anas Awadalla;Joshua P Gardner;Rohan Taori;Ludwig Schmidt,True,https://openreview.net/pdf?id=Y4GZ2w74f4
YJ4ioRbxNb,A benchmark of categorical encoders for binary classification,"Categorical encoders transform categorical features into numerical representations that are indispensable for a wide range of machine learning models.
Existing encoder benchmark studies lack generalizability because of their limited choice of (1) encoders, (2) experimental factors, and (3) datasets. 
Additionally, inconsistencies arise from the adoption of varying aggregation strategies.
This paper is the most comprehensive benchmark of categorical encoders to date, including an extensive evaluation of 32 configurations of encoders from diverse families, with 36 combinations of experimental factors, and on 50 datasets.
The study shows the profound influence of dataset selection, experimental factors, and aggregation strategies on the benchmark's conclusions~---~aspects disregarded in previous encoder benchmarks.
Our code is available at \\\\url{https://github.com/DrCohomology/EncoderBenchmarking}.",Datasets & Benchmarks,NeurIPS,2023,Poster,Federico Matteucci;Vadim Arzamasov;Klemens Böhm,False,https://openreview.net/pdf?id=YJ4ioRbxNb
YQA28p7qNz,3D-LLM: Injecting the 3D World into Large Language Models,"Large language models (LLMs) and Vision-Language Models (VLMs) have been proved to excel at multiple tasks, such as commonsense reasoning. Powerful as these models can be, they are not grounded in the 3D physical world, which involves richer concepts such as spatial relationships, affordances, physics, layout, and so on. In this work, we propose to inject the 3D world into large language models, and introduce a whole new family of 3D-LLMs. Specifically, 3D-LLMs can take 3D point clouds and their features as input and perform a diverse set of 3D-related tasks, including captioning, dense captioning, 3D question answering, task decomposition, 3D
grounding, 3D-assisted dialog, navigation, and so on. Using three types of prompting mechanisms that we design, we are able to collect over 300k 3D-language data covering these tasks. To efficiently train 3D-LLMs, we first utilize a 3D feature extractor that obtains 3D features from rendered multi-view images. Then, we use 2D VLMs as our backbones to train our 3D-LLMs. By introducing a 3D localization mechanism, 3D-LLMs could better capture 3D spatial information.  Experiments on ScanQA  show that our model outperforms state-of-the-art baselines by a large margin (\\\\textit{e.g.}, the BLEU-1 score surpasses state-of-the-art score by 9\\\\%). Furthermore, experiments on our held-in datasets for 3D captioning, task composition, and 3D-assisted dialogue show that our model outperforms 2D VLMs. Qualitative examples also show that our model could perform more tasks beyond the scope of existing LLMs and VLMs. Our model and data will be publicly available.",main,NeurIPS,2023,Spotlight,Yining Hong;Haoyu Zhen;Peihao Chen;Shuhong Zheng;Yilun Du;Zhenfang Chen;Chuang Gan,True,https://openreview.net/pdf?id=YQA28p7qNz
YWJ7Yi4OtH,ECG-QA: A Comprehensive Question Answering Dataset Combined With Electrocardiogram,"Question answering (QA) in the field of healthcare has received much attention due to significant advancements in natural language processing. However, existing healthcare QA datasets primarily focus on medical images, clinical notes, or structured electronic health record tables. This leaves the vast potential of combining electrocardiogram (ECG) data with these systems largely untapped. To address this gap, we present ECG-QA, the first QA dataset specifically designed for ECG analysis. The dataset comprises a total of 70 question templates that cover a wide range of clinically relevant ECG topics, each validated by an ECG expert to ensure their clinical utility. As a result, our dataset includes diverse ECG interpretation questions, including those that require a comparative analysis of two different ECGs. In addition, we have conducted numerous experiments to provide valuable insights for future research directions. We believe that ECG-QA will serve as a valuable resource for the development of intelligent QA systems capable of assisting clinicians in ECG interpretations.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jungwoo Oh;Gyubok Lee;Seongsu Bae;Joon-myoung Kwon;Edward Choi,True,https://openreview.net/pdf?id=YWJ7Yi4OtH
YXogl4uQUO,PlanBench: An Extensible Benchmark for Evaluating Large Language Models on Planning and Reasoning about Change,"Generating plans of action, and reasoning about change have long been considered a core competence of intelligent agents. It is thus no surprise that evaluating the planning and reasoning capabilities of large language models (LLMs) has become a hot topic of research. Most claims about LLM planning capabilities are however based on common sense tasks–where it becomes hard to tell whether LLMs are planning or merely retrieving from their vast world knowledge.  There is a strong need for systematic and extensible planning benchmarks with sufficient diversity to evaluate whether LLMs have innate planning capabilities. Motivated by this, we propose PlanBench, an extensible benchmark suite based on the kinds of domains used in the automated planning community, especially in the International Planning Competition, to test the capabilities of LLMs in planning or reasoning about actions and change. PlanBench provides sufficient diversity in both the task domains and the specific planning capabilities. Our studies also show that on many critical capabilities–including plan generation–LLM performance falls quite short, even with the SOTA models. PlanBench can thus function as a useful marker of progress of LLMs in planning and reasoning.",Datasets & Benchmarks,NeurIPS,2023,Poster,Karthik Valmeekam;Matthew Marquez;Alberto Olmo;Sarath Sreedharan;Subbarao Kambhampati,True,https://openreview.net/pdf?id=YXogl4uQUO
YdjWXrdOTh,Evaluating Graph Neural Networks for Link Prediction: Current Pitfalls and New Benchmarking,"Link prediction attempts to predict whether an unseen edge exists based on only a portion of the graph. A flurry of methods has been created in recent years that attempt to make use of graph neural networks (GNNs) for this task. Furthermore, new and diverse datasets have also been created to better evaluate the effectiveness of these new models. However, multiple limitations currently exist that hinders our ability to properly evaluate these new methods. This includes, but is not limited to: (1) The underreporting of performance on multiple baselines, (2) A lack of a unified data split and evaluation metric on some datasets, (3) An unrealistic evaluation setting that produces negative samples that are easy to classify. To overcome these challenges we first conduct a fair comparison across prominent methods and datasets, utilizing the same dataset settings and hyperparameter settings. We then create a new real-world evaluation setting that samples difficult negative samples via multiple heuristics. The new evaluation setting helps promote new challenges and opportunities in link prediction by aligning the evaluation with real-world situations.",Datasets & Benchmarks,NeurIPS,2023,Poster,Juanhui Li;Harry Shomer;Haitao Mao;Shenglai Zeng;Yao Ma;Neil Shah;Jiliang Tang;Dawei Yin,False,https://openreview.net/pdf?id=YdjWXrdOTh
YfPKQycBDE,SEVA: Leveraging sketches to evaluate alignment between human and machine visual abstraction,"Sketching is a powerful tool for creating abstract images that are sparse but meaningful. Sketch understanding poses fundamental challenges for general-purpose vision algorithms because it requires robustness to the sparsity of sketches relative to natural visual inputs and because it demands tolerance for semantic ambiguity, as sketches can reliably evoke multiple meanings. While current vision algorithms have achieved high performance on a variety of visual tasks, it remains unclear to what extent they understand sketches in a human-like way. Here we introduce $\\\\texttt{SEVA}$, a new benchmark dataset containing approximately 90K human-generated sketches of 128 object concepts produced under different time constraints, and thus systematically varying in sparsity. We evaluated a suite of state-of-the-art vision algorithms on their ability to correctly identify the target concept depicted in these sketches and to generate responses that are strongly aligned with human response patterns on the same sketch recognition task. We found that vision algorithms that better predicted human sketch recognition performance also better approximated human uncertainty about sketch meaning, but there remains a sizable gap between model and human response patterns. To explore the potential of models that emulate human visual abstraction in generative tasks, we conducted further evaluations of a recently developed sketch generation algorithm (Vinker et al., 2022) capable of generating sketches that vary in sparsity. We hope that public release of this dataset and evaluation protocol will catalyze progress towards algorithms with enhanced capacities for human-like visual abstraction.",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=YfPKQycBDE
Ys8RmfF9w1,Uncovering Neural Scaling Laws in Molecular Representation Learning,"Molecular Representation Learning (MRL) has emerged as a powerful tool for drug and materials discovery in a variety of tasks such as virtual screening and inverse design. While there has been a surge of interest in advancing model-centric techniques, the influence of both data quantity and quality on molecular representations is not yet clearly understood within this field. 
In this paper, we delve into the neural scaling behaviors of MRL from a data-centric viewpoint, examining four key dimensions: (1) data modalities, (2) dataset splitting, (3) the role of pre-training, and (4) model capacity.
Our empirical studies confirm a consistent power-law relationship between data volume and MRL performance across these dimensions. Additionally, through detailed analysis, we identify potential avenues for improving learning efficiency.
To challenge these scaling laws, we adapt seven popular data pruning strategies to molecular data and benchmark their performance. Our findings underline the importance of data-centric MRL and highlight possible directions for future research.",Datasets & Benchmarks,NeurIPS,2023,Poster,Dingshuo Chen;Yanqiao Zhu;Jieyu Zhang;Yuanqi Du;Zhixun Li;Qiang Liu;Shu Wu;Liang Wang,False,https://openreview.net/pdf?id=Ys8RmfF9w1
Z764QxwETf,Puzzlefusion: Unleashing the Power of Diffusion Models for Spatial Puzzle Solving,"This paper presents an end-to-end neural architecture based on Diffusion Models for spatial puzzle solving, particularly jigsaw puzzle and room arrangement tasks.
In the latter task, for instance, the proposed system ``PuzzleFusion'' takes a set of room layouts as polygonal curves in the top-down view and aligns the room layout pieces by estimating their 2D translations and rotations, akin to solving the jigsaw puzzle of room layouts. A surprising discovery of the paper is that the simple use of a Diffusion Model effectively solves these challenging spatial puzzle tasks as a conditional generation process. 
To enable learning of an end-to-end neural system, the paper introduces new datasets with ground-truth arrangements: 1) 2D Voronoi Jigsaw Dataset, a synthetic one where pieces are generated by voronoi diagram of 2D pointset; and 2) MagicPlan Dataset, a real one from a production pipeline by MagicPlan, where pieces are room layouts constructed by augmented reality App by real-estate consumers.
The qualitative and quantitative evaluations demonstrate that the proposed approach outperforms the competing methods by significant margins in all three spatial puzzle tasks. We have provided code and data in https://sepidsh.github.io/puzzlefusion.",main,NeurIPS,2023,Spotlight,Sepidehsadat Hosseini;Mohammad Amin Shabani;Saghar Irandoust;Yasutaka Furukawa,True,https://openreview.net/pdf?id=Z764QxwETf
ZDnnzsado4,A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset,"In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-1M Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetic-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research within the machine learning community, progress on creating an image-based taxonomic classifier will also further the ultimate goal of all BIOSCAN research: to lay the foundation for a comprehensive survey of global biodiversity. This paper introduces the dataset and explores the classification task through the implementation and analysis of a baseline classifier. The code repository of the BIOSCAN-1M-Insect dataset is available at https://github.com/zahrag/BIOSCAN-1M",Datasets & Benchmarks,NeurIPS,2023,Poster,Zahra Gharaee;ZeMing Gong;Nicholas Pellegrino;Iuliia Zarubiieva;Joakim Bruslund Haurum;Scott C Lowe;Jaclyn McKeown;Chris C.Y. Ho;Joschka McLeod;Yi-Yun Catherine Wei;Jireh Agda;Sujeevan Ratnasingham;Dirk Steinke;Angel X Chang;Graham W. Taylor;Paul W. Fieguth,True,https://openreview.net/pdf?id=ZDnnzsado4
ZJWQfgXQb6,The ToMCAT Dataset,"We present a rich, multimodal dataset consisting of data from 40 teams of three humans conducting simulated urban search-and-rescue (SAR) missions in a Minecraft-based testbed, collected for the Theory of Mind-based Cognitive Architecture for Teams (ToMCAT) project. Modalities include two kinds of brain scan data---functional near-infrared spectroscopy (fNIRS) and electroencephalography (EEG), as well as skin conductance, heart rate, eye tracking, face images, spoken dialog audio data with automatic speech recognition (ASR) transcriptions, game screenshots, gameplay data, game performance data, demographic data, and self-report questionnaires. Each team undergoes up to six consecutive phases: three behavioral tasks, one mission training session, and two collaborative SAR missions. As time-synchronized multimodal data collected under a variety of circumstances, this dataset will support studying a large variety of research questions on topics including teamwork, coordination, plan recognition, affective computing, physiological linkage, entrainment, and dialog understanding.  We provide an initial public release of the de-identified data, along with analyses illustrating the utility of this dataset to both computer scientists and social scientists.",Datasets & Benchmarks,NeurIPS,2023,Poster,Adarsh Pyarelal;Eric Duong;Caleb Jones Shibu;Paulo Soares;Savannah Boyd;Payal Khosla;Valeria Pfeifer;Diheng Zhang;Eric S Andrews;Rick Champlin;Vincent Paul Raymond;Meghavarshini Krishnaswamy;Clayton Morrison;Emily Butler;Kobus Barnard,True,https://openreview.net/pdf?id=ZJWQfgXQb6
ZV4tZgclu8,Large Language Models are Fixated by Red Herrings: Exploring Creative Problem Solving and Einstellung Effect using the Only Connect Wall Dataset,"The quest for human imitative AI has been an enduring topic in AI research since inception. The technical evolution and emerging capabilities of the latest cohort of large language models (LLMs) have reinvigorated the subject beyond academia to cultural zeitgeist. 
While recent NLP evaluation benchmark tasks test some aspects of human-imitative behaviour (e.g., BIG-bench's `human-like behavior' tasks), few, if not none, examine *creative problem solving* abilities. Creative problem solving in humans is a well-studied topic in cognitive neuroscience with standardized tests that predominantly use ability to associate (heterogeneous) connections among clue words as a metric for creativity. Exposure to misleading stimuli --- distractors dubbed *red herrings* --- impede human performance in such tasks via the *fixation effect* and Einstellung paradigm. In cognitive neuroscience studies, such fixations are experimentally induced by pre-exposing participants to orthographically similar incorrect words to subsequent word-fragments or clues. The popular British quiz show Only Connect's *Connecting Wall* segment essentially mimics Mednick's Remote Associates Test (RAT) formulation with built-in, deliberate red herrings, that makes it an ideal proxy dataset to explore and study fixation effect and Einstellung paradigm from cognitive neuroscience in LLMs. In addition to presenting the novel Only Connect Wall (OCW) dataset, we also report results from our evaluation of selected pre-trained language models and LLMs (including OpenAI's GPT series) on creative problem solving tasks like grouping clue words by heterogeneous connections, and identifying correct open knowledge domain connections in respective groups. We synthetically generate two additional datasets: OCW-Randomized, OCW-WordNet to further analyze our red-herrings hypothesis in language models.
The code and link to the dataset is available at [url](https://github.com/TaatiTeam/OCW).",Datasets & Benchmarks,NeurIPS,2023,Poster,Saeid Alavi Naeini;Raeid Saqur;mozhgan saeidi;John Michael Giorgi;Babak Taati,True,https://openreview.net/pdf?id=ZV4tZgclu8
ZZgfS1DbmO,Continuous Parametric Optical Flow,"In this paper, we present continuous parametric optical flow, a parametric representation of dense and continuous motion over arbitrary time interval. In contrast to existing discrete-time representations (i.e., flow in between consecutive frames), this new representation transforms the frame-to-frame pixel correspondences to dense continuous flow. In particular, we present a temporal-parametric model that employs B-splines to fit point trajectories using a limited number of frames. To further improve the stability and robustness of the trajectories, we also add an encoder with a neural ordinary differential equation (NODE) to represent features associated with specific times. We also contribute a synthetic dataset and introduce two evaluation perspectives to measure the accuracy and robustness of continuous flow estimation. Benefiting from the combination of explicit parametric modeling and implicit feature optimization, our model focuses on motion continuity and outperforms the flow-based and point-tracking approaches for fitting long-term and variable sequences.",main,NeurIPS,2023,Poster,Jianqin Luo;Zhexiong Wan;yuxin mao;Bo Li;Yuchao Dai,True,https://openreview.net/pdf?id=ZZgfS1DbmO
ZbmS3MU25p,CSMeD: Bridging the Dataset Gap in Automated Citation Screening for Systematic Literature Reviews,"Systematic literature reviews (SLRs) play an essential role in summarising, synthesising and validating scientific evidence. In recent years, there has been a growing interest in using machine learning techniques to automate the identification of relevant studies for SLRs. However, the lack of standardised evaluation datasets makes comparing the performance of such automated literature screening systems difficult. In this paper, we analyse the citation screening evaluation datasets, revealing that many of the available datasets are either too small, suffer from data leakage or have limited applicability to systems treating automated literature screening as a classification task, as opposed to, for example, a retrieval or question-answering task. To address these challenges, we introduce CSMED, a meta-dataset consolidating nine publicly released collections, providing unified access to 325 SLRs from the fields of medicine and computer science. CSMED serves as a comprehensive resource for training and evaluating the performance of automated citation screening models. Additionally, we introduce CSMED-FT, a new dataset designed explicitly for evaluating the full text publication screening task. To demonstrate the utility of CSMED, we conduct experiments and establish baselines on new datasets.",Datasets & Benchmarks,NeurIPS,2023,Poster,Wojciech Kusa;Oscar E. Mendoza;Matthias Samwald;Petr Knoth;Allan Hanbury,True,https://openreview.net/pdf?id=ZbmS3MU25p
ZknHnDDxng,VidChapters-7M: Video Chapters at Scale,"Segmenting untrimmed videos into chapters enables users to quickly navigate to the information of their interest. This important topic has been understudied due to the lack of publicly released datasets. To address this issue, we present VidChapters-7M, a dataset of 817K user-chaptered videos including 7M chapters in total. VidChapters-7M is automatically created from videos online in a scalable manner by scraping user-annotated chapters and hence without any additional manual annotation. We introduce the following three tasks based on this data. First, the video chapter generation task consists of temporally segmenting the video and generating a chapter title for each segment. To further dissect the problem, we also define two variants of this task: video chapter generation given ground-truth boundaries, which requires generating a chapter title given an annotated video segment, and video chapter grounding, which requires temporally localizing a chapter given its annotated title. We benchmark both simple baselines as well as state-of-the-art video-language models on these three tasks. We also show that pretraining on VidChapters-7M transfers well to dense video captioning tasks, largely improving the state of the art on the YouCook2 and ViTT benchmarks. Finally, our experiments reveal that downstream performance scales well with the size of the pretraining dataset.",Datasets & Benchmarks,NeurIPS,2023,Poster,Antoine Yang;Arsha Nagrani;Ivan Laptev;Josef Sivic;Cordelia Schmid,True,https://openreview.net/pdf?id=ZknHnDDxng
ZrNRBmOzwE,Data Portraits: Recording Foundation Model Training Data,"Foundation models are trained on increasingly immense and opaque datasets. Even while these models are now key in AI system building, it can be  difficult to answer the straightforward question: has the model already encountered a given example during training? We therefore propose a widespread adoption of Data Portraits: artifacts that record training data and allow for downstream inspection. First we outline the properties of such an artifact and discuss how existing solutions can be used to increase transparency. We then propose and implement a solution based on data sketching, stressing fast and space efficient querying. Using our tools, we document a popular language modeling corpus (The Pile) and a recently released code modeling dataset (The Stack). We show that our solution enables answering questions about test set leakage and model plagiarism. Our tool is lightweight and fast, costing only 3% of the dataset size in overhead. We release a live interface of our tools at https://dataportraits.org/ and call on dataset and model creators to release Data Portraits as a complement to current documentation practices.",Datasets & Benchmarks,NeurIPS,2023,Poster,Marc Marone;Benjamin Van Durme,False,https://openreview.net/pdf?id=ZrNRBmOzwE
ZsDB2GzsqG,MagicBrush: A Manually Annotated Dataset for Instruction-Guided Image Editing,"Text-guided image editing is widely needed in daily life, ranging from personal use to professional applications such as Photoshop.
However, existing methods are either zero-shot or trained on an automatically synthesized dataset, which contains a high volume of noise.
Thus, they still require lots of manual tuning to produce desirable outcomes in practice.
To address this issue, we introduce MagicBrush, the first large-scale, manually annotated dataset for instruction-guided real image editing that covers diverse scenarios: single-turn, multi-turn, mask-provided, and mask-free editing.
MagicBrush comprises over 10K manually annotated triplets (source image, instruction, target image), which supports trainining large-scale text-guided image editing models.
We fine-tune InstructPix2Pix on MagicBrush and show that the new model can produce much better images according to human evaluation.
We further conduct extensive experiments to evaluate current image editing baselines from multiple dimensions including quantitative, qualitative, and human evaluations.
The results reveal the challenging nature of our dataset and the gap between current baselines and real-world editing needs.",Datasets & Benchmarks,NeurIPS,2023,Poster,Kai Zhang;Lingbo Mo;Wenhu Chen;Huan Sun;Yu Su,True,https://openreview.net/pdf?id=ZsDB2GzsqG
aIpGtPwXny,Learning to Modulate pre-trained Models in RL,"Reinforcement Learning (RL) has been successful in various domains like robotics, game playing, and simulation. While RL agents have shown impressive capabilities in their specific tasks, they insufficiently adapt to new tasks. In supervised learning, this adaptation problem is addressed by large-scale pre-training followed by fine-tuning to new down-stream tasks. Recently, pre-training on multiple tasks has been gaining traction in RL. However, fine-tuning a pre-trained model often suffers from catastrophic forgetting. That is, the performance on the pre-training tasks deteriorates when fine-tuning on new tasks. To investigate the catastrophic forgetting phenomenon, we first jointly pre-train a model on datasets from two benchmark suites, namely Meta-World and DMControl. Then, we evaluate and compare a variety of fine-tuning methods prevalent in natural language processing, both in terms of performance on new tasks, and how well performance on pre-training tasks is retained. Our study shows that with most fine-tuning approaches, the performance on pre-training tasks deteriorates significantly. Therefore, we propose a novel method, Learning-to-Modulate (L2M), that avoids the degradation of learned skills by modulating the information flow of the frozen pre-trained model via a learnable modulation pool. Our method achieves state-of-the-art performance on the Continual-World benchmark, while retaining performance on the pre-training tasks. Finally, to aid future research in this area, we release a dataset encompassing 50 Meta-World and 16 DMControl tasks.",main,NeurIPS,2023,Poster,Thomas Schmied;Markus Hofmarcher;Fabian Paischer;Razvan Pascanu;Sepp Hochreiter,True,https://openreview.net/pdf?id=aIpGtPwXny
aKnWIrDPiR,Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine,"We propose the Multimodal Clinical Benchmark for Emergency Care (MC-BEC), a comprehensive benchmark for evaluating foundation models in Emergency Medicine using a dataset of 100K+ continuously monitored Emergency Department visits from 2020-2022. MC-BEC focuses on clinically relevant prediction tasks at timescales from minutes to days, including predicting patient decompensation, disposition, and emergency department (ED) revisit, and includes a standardized evaluation framework with train-test splits and evaluation metrics. The multimodal dataset includes a wide range of detailed clinical data, including triage information, prior diagnoses and medications, continuously measured vital signs, electrocardiogram and photoplethysmograph waveforms, orders placed and medications administered throughout the visit, free-text reports of imaging studies, and information on ED diagnosis, disposition, and subsequent revisits. We provide performance baselines for each prediction task to enable the evaluation of multimodal, multitask models. We believe that MC-BEC will encourage researchers to develop more effective, generalizable, and accessible foundation models for multimodal clinical data.",Datasets & Benchmarks,NeurIPS,2023,Poster,Emma Chen;Aman Kansal;Julie Chen;Boyang Tom Jin;Julia Rachel Reisler;David A Kim;Pranav Rajpurkar,True,https://openreview.net/pdf?id=aKnWIrDPiR
aZ44Na3l9p,Reproducibility in Multiple Instance Learning: A Case For Algorithmic Unit Tests,"Multiple Instance Learning (MIL) is a sub-domain of classification problems with positive and negative labels and a ""bag"" of inputs, where the label is positive if and only if a positive element is contained within the bag, and otherwise is negative. Training in this context requires associating the bag-wide label to instance-level information, and implicitly contains a causal assumption and asymmetry to the task (i.e., you can't swap the labels without changing the semantics). MIL problems occur in healthcare (one malignant cell indicates cancer), cyber security (one malicious executable makes an infected computer), and many other tasks. In this work, we examine five of the most prominent deep-MIL models and find that none of them respects the standard MIL assumption. They are able to learn anti-correlated instances, i.e., defaulting to ""positive"" labels until seeing a negative counter-example, which should not be possible for a correct MIL model. We suspect that enhancements and other works derived from these models will share the same issue. In any context in which these models are being used, this creates the potential for learning incorrect models, which creates risk of operational failure.  We identify and demonstrate this problem via a proposed ``algorithmic unit test'', where we create synthetic datasets that can be solved by a MIL respecting model, and which clearly reveal learning that violates MIL assumptions. The five evaluated methods each fail one or more of these tests. This provides a model-agnostic way to identify violations of modeling assumptions, which we hope will be useful for future development and evaluation of MIL models.",main,NeurIPS,2023,Poster,Edward Raff;James Holt,True,https://openreview.net/pdf?id=aZ44Na3l9p
argZAtDMMF,FLOP: Tasks for Fitness Landscapes Of Protein wildtypes,"Protein engineering has the potential to create optimized protein variants with improved properties and function. 
An initial step in the protein optimization process typically consists of a search among natural (wildtype) sequences to find the naturally occurring proteins with the most desirable properties. 
Promising candidates from this initial discovery phase then form the basis of the second step: a more local optimization procedure, exploring the space of variants separated from this candidate by a number of mutations. 
While considerable progress has been made on evaluating machine learning methods on single protein datasets, benchmarks of data-driven approaches for global fitness landscape exploration are still lacking. 
In this paper, we have carefully curated a representative benchmark dataset, which reflects industrially relevant scenarios for the initial wildtype discovery phase of protein engineering.
We focus on exploration within a protein family, and investigate the downstream predictive power of various protein representation paradigms, i.e., protein language model-based representations, structure-based representations, and evolution-based representations.
Our benchmark highlights the importance of coherent split strategies, and how we can be misled into overly optimistic estimates of the state of the field. The codebase and data can be accessed via https://github.com/petergroth/FLOP.",Datasets & Benchmarks,NeurIPS,2023,Reject,Peter Mørch Groth;Richard Michael;Jesper Salomon;Pengfei Tian;Wouter Boomsma,True,https://openreview.net/pdf?id=argZAtDMMF
bW1uwPV3im,LoRA: A Logical Reasoning Augmented Dataset for Visual Question Answering,"The capacity to reason logically is a hallmark of human cognition. Humans excel at integrating multimodal information for locigal reasoning, as exemplified by the Visual Question Answering (VQA) task, which is a challenging multimodal task. VQA tasks and large vision-and-language models aim to tackle reasoning problems, but the accuracy, consistency and fabrication of the generated answers is hard to evaluate in the absence of a VQA dataset that can offer formal, comprehensive and systematic complex logical reasoning questions. To address this gap, we present LoRA, a novel Logical Reasoning Augmented VQA dataset that requires formal and complex description logic reasoning based on a food-and-kitchen knowledge base. Our main objective in creating LoRA is to enhance the complex and formal logical reasoning capabilities of VQA models, which are not adequately measured by existing VQA datasets. We devise strong and flexible programs to automatically generate 200,000 diverse description logic reasoning questions based on the SROIQ Description Logic, along with realistic kitchen scenes and ground truth answers. We fine-tune the latest transformer VQA models and evaluate the zero-shot performance of the state-of-the-art large vision-and-language models on LoRA. The results reveal that LoRA presents a unique challenge in logical reasoning, setting a systematic and comprehensive evaluation standard.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jingying Gao;Qi Wu;Alan Blair;Maurice Pagnucco,True,https://openreview.net/pdf?id=bW1uwPV3im
bbbbbov4Xu,Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast Contrastive Fusion,"Instance segmentation in 3D is a challenging task due to the lack of large-scale annotated datasets. In this paper, we show that this task can be addressed effectively by leveraging instead 2D pre-trained models for instance segmentation. We propose a novel approach to lift 2D segments to 3D and fuse them by means of a neural field representation, which encourages multi-view consistency across frames. The core of our approach is a slow-fast clustering objective function, which is scalable and well-suited for scenes with a large number of objects. Unlike previous approaches, our method does not require an upper bound on the number of objects or object tracking across frames. To demonstrate the scalability of the slow-fast clustering, we create a new semi-realistic dataset called the Messy Rooms dataset, which features scenes with up to 500 objects per scene. Our approach outperforms the state-of-the-art on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well as on our newly created Messy Rooms dataset, demonstrating the effectiveness and scalability of our slow-fast clustering method.",main,NeurIPS,2023,Spotlight,Yash Sanjay Bhalgat;Iro Laina;Joao F. Henriques;Andrea Vedaldi;Andrew Zisserman,True,https://openreview.net/pdf?id=bbbbbov4Xu
bdWkFt7M6X,A Dataset of Relighted 3D Interacting Hands,"The two-hand interaction is one of the most challenging signals to analyze due to the self-similarity, complicated articulations, and occlusions of hands. Although several datasets have been proposed for the two-hand interaction analysis, all of them do not achieve 1) diverse and realistic image appearances and 2) diverse and large-scale groundtruth (GT) 3D poses at the same time. In this work, we propose Re:InterHand, a dataset of relighted 3D interacting hands that achieve the two goals. To this end, we employ a state-of-the-art hand relighting network with our accurately tracked two-hand 3D poses. We compare our Re:InterHand with existing 3D interacting hands datasets and show the benefit of it. Our Re:InterHand is available in https://mks0601.github.io/ReInterHand/",Datasets & Benchmarks,NeurIPS,2023,Poster,Gyeongsik Moon;Shunsuke Saito;Weipeng Xu;Rohan Joshi;Julia Buffalini;Harley Bellan;Nicholas Matthew Rosen;Jesse Richardson;Mallorie Mize;Philippe De Bree;Tomas Simon;Bo Peng;Shubham Garg;Kevyn Alex Anthony McPhail;Takaaki Shiratori,True,https://openreview.net/pdf?id=bdWkFt7M6X
bjvRVA2ihO,How to Data in Datathons,"The rise of datathons, also known as data or data science hackathons, has provided a platform to collaborate, learn, and innovate quickly. Despite their significant potential benefits, organizations often struggle to effectively work with data due to a lack of clear guidelines and best practices for potential issues that might arise. Drawing on our own experiences and insights from organizing +80 datathon challenges with +60 partnership organizations since 2016, we provide a guide that serves as a resource for organizers to navigate the data-related complexities of datathons. We apply our proposed framework to 10 case studies.",Datasets & Benchmarks,NeurIPS,2023,Poster,Carlos Mougan;Richard Plant;Clare Teng;Marya Bazzi;Alvaro Cabrejas-Egea;Ryan Sze-Yin Chan;David Salvador Jasin;martin stoffel;Kirstie Jane Whitaker;JULES MANSER,False,https://openreview.net/pdf?id=bjvRVA2ihO
blm1pqiOXe,Paxion: Patching Action Knowledge in Video-Language Foundation Models,"Action knowledge involves the understanding of textual, visual, and temporal aspects of actions. We introduce the **Action Dynamics Benchmark (ActionBench)** containing two carefully designed probing tasks: Action Antonym and Video Reversal, which targets multimodal alignment capabilities and temporal understanding skills of the model, respectively. Despite recent video-language models’ (VidLM) impressive performance on various benchmark tasks, our diagnostic tasks reveal their surprising deficiency (near-random performance) in action knowledge, suggesting that current models rely on object recognition abilities as a shortcut for action understanding. To remedy this, we propose a novel framework, **Paxion**, along with a new **Discriminative Video Dynamics Modeling (DVDM)** objective. The Paxion framework utilizes a **Knowledge Patcher** network to encode new action knowledge and a **Knowledge Fuser** component to integrate the Patcher into frozen VidLMs without compromising their existing capabilities. Due to limitations of the widely-used Video-Text Contrastive (VTC) loss for learning action knowledge, we introduce the DVDM objective to train the Knowledge Patcher. DVDM forces the model to encode the correlation between the action text and the correct ordering of video frames. Our extensive analyses show that Paxion and DVDM together effectively fill the gap in action knowledge understanding (~50% → 80%), while maintaining or improving performance on a wide spectrum of both object- and action-centric downstream tasks.",main,NeurIPS,2023,Spotlight,Zhenhailong Wang;Ansel Blume;Sha Li;Genglin Liu;Jaemin Cho;Zineng Tang;Mohit Bansal;Heng Ji,True,https://openreview.net/pdf?id=blm1pqiOXe
bmfMNIf1bU,EV-Eye: Rethinking High-frequency Eye Tracking through the Lenses of Event Cameras,"In this paper, we present EV-Eye, a first-of-its-kind large scale multimodal eye tracking dataset aimed at inspiring research on high-frequency eye/gaze tracking. EV-Eye utilizes an emerging bio-inspired event camera to capture independent pixel-level intensity changes induced by eye movements, achieving sub-microsecond latency. Our dataset was curated over a two-week period and collected from 48 participants encompassing diverse genders and age groups. It comprises over 1.5 million near-eye grayscale images and 2.7 billion event samples generated by two DAVIS346 event cameras. Additionally, the dataset contains 675 thousands scene images and 2.7 million gaze references captured by Tobii Pro Glasses 3 eye tracker for cross-modality validation. Compared with existing event-based high-frequency eye tracking datasets, our dataset is significantly larger in size, and the gaze references involve more natural eye movement patterns, i.e., fixation, saccade and smooth pursuit. Alongside the event data, we also present a hybrid eye tracking method as benchmark, which leverages both the near-eye grayscale images and event data for robust and high-frequency eye tracking. We show that our method achieves higher accuracy for both pupil and gaze estimation tasks compared to the existing solution.",Datasets & Benchmarks,NeurIPS,2023,Poster,Guangrong Zhao;Yurun Yang;Jingwei Liu;Ning Chen;Yiran Shen;Hongkai Wen;Guohao Lan,True,https://openreview.net/pdf?id=bmfMNIf1bU
bqXduvuW5E,"ProteinInvBench: Benchmarking Protein Inverse Folding on Diverse Tasks, Models, and Metrics","Protein inverse folding has attracted increasing attention in recent years. However, we observe that current methods are usually limited to the CATH dataset and the recovery metric. The lack of a unified framework for ensembling and comparing different methods hinders the comprehensive investigation. In this paper, we propose ProteinBench, a new benchmark for protein design, which comprises extended protein design tasks, integrated models, and diverse evaluation metrics. We broaden the application of methods originally designed for single-chain protein design to new scenarios of multi-chain and \\\\textit{de novo} protein design. Recent impressive methods, including GraphTrans, StructGNN, GVP, GCA, AlphaDesign, ProteinMPNN, PiFold and KWDesign are integrated into our framework. In addition to the recovery, we also evaluate the confidence, diversity, sc-TM, efficiency, and robustness to thoroughly revisit current protein design approaches and inspire future work. As a result, we establish the first comprehensive benchmark for protein design, which is publicly available at \\\\url{https://github.com/A4Bio/OpenCPD}.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zhangyang Gao;Cheng Tan;Yijie Zhang;Xingran Chen;Lirong Wu;Stan Z. Li,True,https://openreview.net/pdf?id=bqXduvuW5E
c3kuX7ltzr,FORB: A Flat Object Retrieval Benchmark for Universal Image Embedding,"Image retrieval is a fundamental task in computer vision. Despite recent advances in this field, many techniques have been evaluated on a limited number of domains, with a small number of instance categories. Notably, most existing works only consider domains like 3D landmarks, making it difficult to generalize the conclusions made by these works to other domains, e.g., logo and other 2D flat objects. To bridge this gap, we introduce a new dataset for benchmarking visual search methods on flat images with diverse patterns. Our flat object retrieval benchmark (FORB) supplements the commonly adopted 3D object domain, and more importantly, it serves as a testbed for assessing the image embedding quality on out-of-distribution domains. In this benchmark we investigate the retrieval accuracy of representative methods in terms of candidate ranks, as well as matching score margin, a viewpoint which is largely ignored by many works. Our experiments not only highlight the challenges and rich heterogeneity of FORB, but also reveal the hidden properties of different retrieval strategies. The proposed benchmark is a growing project and we expect to expand in both quantity and variety of objects. The dataset and supporting codes are available at https://github.com/pxiangwu/FORB/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Pengxiang Wu;Siman Wang;Kevin S Dela Rosa;Derek Hao Hu,True,https://openreview.net/pdf?id=c3kuX7ltzr
c5DUGninMz,RIO: A Benchmark for Reasoning Intention-Oriented Objects in Open Environments,"Intention-oriented object detection aims to detect desired objects based on specific intentions or requirements. For instance, when we desire to ""lie down and rest"", we instinctively seek out a suitable option such as a ""bed"" or a ""sofa"" that can fulfill our needs. Previous work in this area is limited either by the number of intention descriptions or by the affordance vocabulary available for intention objects. These limitations make it challenging to handle intentions in open environments effectively. To facilitate this research, we construct a comprehensive dataset called Reasoning Intention-Oriented Objects (RIO). In particular, RIO is specifically designed to incorporate diverse real-world scenarios and a wide range of object categories. It offers the following key features: 1) intention descriptions in RIO are represented as natural sentences rather than a mere word or verb phrase, making them more practical and meaningful; 2) the intention descriptions are contextually relevant to the scene, enabling a broader range of potential functionalities associated with the objects; 3) the dataset comprises a total of 40,214 images and 130,585 intention-object pairs. With the proposed RIO, we evaluate the ability of some existing models to reason intention-oriented objects in open environments.",Datasets & Benchmarks,NeurIPS,2023,Poster,Mengxue Qu;Yu Wu;Wu Liu;Xiaodan Liang;Jingkuan Song;Yao Zhao;Yunchao Wei,True,https://openreview.net/pdf?id=c5DUGninMz
c5dRV9tA3K,EMMA-X: An EM-like Multilingual Pre-training Algorithm for Cross-lingual Representation Learning,"Expressing universal semantics common to all languages is helpful to understand the meanings of complex and culture-specific sentences. The research theme underlying this scenario focuses on learning universal representations across languages with the usage of massive parallel corpora. However, due to the sparsity and scarcity of parallel data, there is still a big challenge in learning authentic ``universals'' for any two languages. In this paper, we propose Emma-X: an EM-like Multilingual pre-training Algorithm, to learn Cross-lingual universals with the aid of excessive multilingual non-parallel data. Emma-X unifies the cross-lingual representation learning task and an extra semantic relation prediction task within an EM framework. Both the extra semantic classifier and the cross-lingual sentence encoder approximate the semantic relation of two sentences, and supervise each other until convergence. To evaluate Emma-X, we conduct experiments on xrete, a newly introduced benchmark containing 12 widely studied cross-lingual tasks that fully depend on sentence-level representations. Results reveal that Emma-X achieves state-of-the-art performance. Further geometric analysis of the built representation space with three requirements demonstrates the superiority of Emma-X over advanced models.",main,NeurIPS,2023,Poster,Ping Guo;Xiangpeng Wei;Yue Hu;Baosong Yang;Dayiheng Liu;Fei Huang;jun xie,True,https://openreview.net/pdf?id=c5dRV9tA3K
c5rqd6PZn6,BuildingsBench: A Large-Scale Dataset of 900K Buildings and Benchmark for Short-Term Load Forecasting,"Short-term forecasting of residential and commercial building energy consumption is widely used in power systems and continues to grow in importance. Data-driven short-term load forecasting (STLF), although promising, has suffered from a lack of open, large-scale datasets with high building diversity. This has hindered exploring the pretrain-then-fine-tune paradigm for STLF. To help address this, we present BuildingsBench, which consists of: 1) Buildings-900K, a large-scale dataset of 900K simulated buildings representing the U.S. building stock; and 2) an evaluation platform with over 1,900 real residential and commercial buildings from 7 open datasets. BuildingsBench benchmarks two under-explored tasks: zero-shot STLF, where a pretrained model is evaluated on unseen buildings without fine-tuning, and transfer learning, where a pretrained model is fine-tuned on a target building. The main finding of our benchmark analysis is that synthetically pretrained models generalize surprisingly well to real commercial buildings. An exploration of the effect of increasing dataset size and diversity on zero-shot commercial building performance reveals a power-law with diminishing returns. We also show that fine-tuning pretrained models on real commercial and residential buildings improves performance for a majority of target buildings. We hope that BuildingsBench encourages and facilitates future research on generalizable STLF. All datasets and code can be accessed from https://github.com/NREL/BuildingsBench.",Datasets & Benchmarks,NeurIPS,2023,Poster,Patrick Emami;Abhijeet Sahu;Peter Graf,True,https://openreview.net/pdf?id=c5rqd6PZn6
cAjZ3tMye6,HyPoradise: An Open Baseline for Generative Speech Recognition with Large Language Models,"Advancements in deep neural networks have allowed automatic speech recognition (ASR) systems to attain human parity on several publicly available clean speech datasets. However, even state-of-the-art ASR systems experience performance degradation when confronted with adverse conditions, as a well-trained acoustic model is sensitive to variations in the speech domain, e.g., background noise. Intuitively, humans address this issue by relying on their linguistic knowledge: the meaning of ambiguous spoken terms is usually inferred from contextual cues thereby reducing the dependency on the auditory system. Inspired by this observation, we introduce the first open-source benchmark to utilize external large language models (LLMs) for ASR error correction, where N-best decoding hypotheses provide informative elements for true transcription prediction. This approach is a paradigm shift from the traditional language model rescoring strategy that can only select one candidate hypothesis as output transcription. The proposed benchmark contains a novel dataset, ""HyPoradise"" (HP), encompassing more than 316,000 pairs of N-best hypotheses and corresponding accurate transcriptions across prevalent speech domains. Given this dataset, we examine three types of error correction techniques based on LLMs with varying amounts of labeled hypotheses-transcription pairs, which gains significant word error rate (WER) reduction. Experimental evidence demonstrates the proposed technique achieves a breakthrough by surpassing the upper bound of traditional re-ranking based methods. More surprisingly, LLM with reasonable prompt design can even correct those tokens that are missing in N-best list. We make our results publicly accessible for reproducible pipelines with released pre-trained models, thus providing a new paradigm for ASR error correction with LLMs.",Datasets & Benchmarks,NeurIPS,2023,Poster,CHEN CHEN;Yuchen Hu;Chao-Han Huck Yang;Sabato Marco Siniscalchi;Pin-Yu Chen;EngSiong Chng,True,https://openreview.net/pdf?id=cAjZ3tMye6
cAyLnMxiTl,Enhancing Motion Deblurring in High-Speed Scenes with Spike Streams,"Traditional cameras produce desirable vision results but struggle with motion blur in high-speed scenes due to long exposure windows. Existing frame-based deblurring algorithms face challenges in extracting useful motion cues from severely blurred images. Recently, an emerging bio-inspired vision sensor known as the spike camera has achieved an extremely high frame rate while preserving rich spatial details, owing to its novel sampling mechanism. However, typical binary spike streams are relatively low-resolution, degraded image signals devoid of color information, making them unfriendly to human vision. In this paper, we propose a novel approach that integrates the two modalities from two branches, leveraging spike streams as auxiliary visual cues for guiding deblurring in high-speed motion scenes. 
We propose the first spike-based motion deblurring model with bidirectional information complementarity. We introduce a content-aware motion magnitude attention module that utilizes learnable mask to extract relevant information from blurry images effectively, and we incorporate a transposed cross-attention fusion module to efficiently combine features from both spike data and blurry RGB images.
Furthermore, we build two extensive synthesized datasets for training and validation purposes, encompassing high-temporal-resolution spikes, blurry images, and corresponding sharp images. The experimental results demonstrate that our method effectively recovers clear RGB images from highly blurry scenes and outperforms state-of-the-art deblurring algorithms in multiple settings.",main,NeurIPS,2023,Poster,Shiyan Chen;Jiyuan Zhang;Yajing Zheng;Tiejun Huang;Zhaofei Yu,True,https://openreview.net/pdf?id=cAyLnMxiTl
cF6rQz8V3V,Bitstream-Corrupted Video Recovery: A Novel Benchmark Dataset and Method,"The past decade has witnessed great strides in video recovery by specialist technologies, like video inpainting, completion, and error concealment. However, they typically simulate the missing content by manual-designed error masks, thus failing to fill in the realistic video loss in video communication (e.g., telepresence, live streaming, and internet video) and multimedia forensics. To address this, we introduce the bitstream-corrupted video (BSCV) benchmark, the first benchmark dataset with more than 28,000 video clips, which can be used for bitstream-corrupted video recovery in the real world. The BSCV is a collection of 1) a proposed three-parameter corruption model for video bitstream, 2) a large-scale dataset containing rich error patterns, multiple corruption levels, and flexible dataset branches, and 3) a new video recovery framework that serves as a benchmark. We evaluate state-of-the-art video inpainting methods on the BSCV dataset, demonstrating existing approaches' limitations and our framework's advantages in solving the bitstream-corrupted video recovery problem. The benchmark and dataset are released at https://github.com/LIUTIGHE/BSCV-Dataset.",Datasets & Benchmarks,NeurIPS,2023,Poster,Tianyi Liu;Kejun Wu;YI WANG;Wenyang Liu;Kim-Hui Yap;Lap-Pui Chau,True,https://openreview.net/pdf?id=cF6rQz8V3V
cNObl6QQEH,PanoGen: Text-Conditioned Panoramic Environment Generation for Vision-and-Language Navigation,"Vision-and-Language Navigation requires the agent to follow language instructions to navigate through 3D environments. One main challenge in Vision-and-Language Navigation is the limited availability of photorealistic training environments, which makes it hard to generalize to new and unseen environments. To address this problem, we propose PanoGen, a generation method that can potentially create an infinite number of diverse panoramic environments conditioned on text. Specifically, we collect room descriptions by captioning the room images in existing Matterport3D environments, and leverage a state-of-the-art text-to-image diffusion model to generate the new panoramic environments. We use recursive outpainting over the generated images to create consistent 360-degree panorama views. Our new panoramic environments share similar semantic information with the original environments by conditioning on text descriptions, which ensures the co-occurrence of objects in the panorama follows human intuition, and creates enough diversity in room appearance and layout with image outpainting. Lastly, we explore two ways of utilizing PanoGen in VLN pre-training and fine-tuning. We generate instructions for paths in our PanoGen environments with a speaker built on a pre-trained vision-and-language model for VLN pre-training, and augment the visual observation with our panoramic environments during agents' fine-tuning to avoid overfitting to seen environments. Empirically, learning with our PanoGen environments achieves the new state-of-the-art on the Room-to-Room, Room-for-Room, and CVDN datasets. Besides, we find that pre-training with our PanoGen speaker data is especially effective for CVDN, which has under-specified instructions and needs commonsense knowledge to reach the target. Lastly, we show that the agent can benefit from training with more generated panoramic environments, suggesting promising results for scaling up the PanoGen environments to enhance agents' generalization to unseen environments.",main,NeurIPS,2023,Poster,Jialu Li;Mohit Bansal,True,https://openreview.net/pdf?id=cNObl6QQEH
calKOSmBxj,HoK3v3: an Environment for Generalization in Heterogeneous Multi-agent Reinforcement Learning,"We introduce HoK3v3, a 3v3 game environment for multi-agent reinforcement learning (MARL) research, based on Honor of Kings, the world's most popular Multiplayer Online Battle Arena (MOBA) game at present. Due to the presence of diverse heroes and lineups (a.k.a., hero combinations), this environment poses a unique challenge for generalization in heterogeneous MARL. A detailed description of the tasks contained in HoK3v3, including observations, structured actions, and multi-head reward specifications, has been provided. We validate the environment by applying conventional MARL baseline algorithms.  We examine the challenges of generalization through experiments involving the 3v3 MOBA full game task and its decomposed sub tasks, executed by lineups picked from the hero pool. The results indicate the limitations of existing RL methods in addressing scenarios that require heterogeneous generalization. All of the code, tutorial, encrypted game engine, can be accessed at: https://github.com/tencent-ailab/hok_env.",Datasets & Benchmarks,NeurIPS,2023,Reject,Lin Liu;Jianzhun Shao;Xinkai Chen;Yun Qu;Boyuan Wang;Zhenbin Ye;Yuexuan Tu;Hongyang Qin;Yang Jun Feng;Lin Lai;Yuanqin Wang;Meng Meng;Wenjun Wang;Xiyang Ji;QIANG FU;Lanxiao Huang;Minwen Deng;Yang Wei;Houqiang Li;Wengang Zhou;Ning Xie;Xiangyang Ji;Lvfang Tao;Lin Yuan;Juchao Zhuo;YANG GUANG;Deheng Ye,False,https://openreview.net/pdf?id=calKOSmBxj
crbPFR2Hpv,AVOIDDS: Aircraft Vision-based Intruder Detection Dataset and Simulator,"Designing robust machine learning systems remains an open problem, and there is a need for benchmark problems that cover both environmental changes and evaluation on a downstream task. In this work, we introduce AVOIDDS, a realistic object detection benchmark for the vision-based aircraft detect-and-avoid problem. We provide a labeled dataset consisting of 72,000 photorealistic images of intruder aircraft with various lighting conditions, weather conditions, relative geometries, and geographic locations.  We also provide an interface that evaluates trained models on slices of this dataset to identify changes in performance with respect to changing environmental conditions. Finally, we implement a fully-integrated, closed-loop simulator of the vision-based detect-and-avoid problem to evaluate trained models with respect to the downstream collision avoidance task. This benchmark will enable further research in the design of robust machine learning systems for use in safety-critical applications. The AVOIDDS dataset and code are publicly available at https://purl.stanford.edu/hj293cv5980 and https://github.com/sisl/VisionBasedAircraftDAA, respectively.",Datasets & Benchmarks,NeurIPS,2023,Poster,Elysia Quinn Smyers;Sydney Michelle Katz;Anthony Corso;Mykel Kochenderfer,True,https://openreview.net/pdf?id=crbPFR2Hpv
cuheT1BAp4,Object Reprojection Error (ORE): Camera pose benchmarks from lightweight tracking annotations,"3D spatial understanding is highly valuable in the context of semantic modeling of environments, agents, and their relationships.  Semantic modeling approaches employed on monocular video often ingest outputs from off-the-shelf SLAM/SfM pipelines, which are anecdotally observed to perform poorly or fail completely on some fraction of the videos of interest.  These target videos may vary widely in complexity of scenes, activities, camera trajectory, etc.  Unfortunately, such semantically-rich video data often comes with no ground-truth 3D information, and in practice it is prohibitively costly or impossible to obtain ground truth reconstructions or camera pose post-hoc.  

This paper proposes a novel evaluation protocol, Object Reprojection Error (ORE) to benchmark camera trajectories; ORE computes reprojection error for static objects within the video and requires only lightweight object tracklet annotations.  These annotations are easy to gather on new or existing video, enabling ORE to be calculated on essentially arbitrary datasets.  We show that ORE maintains high rank correlation with standard metrics based on groundtruth.  Leveraging ORE, we source videos and annotations from Ego4D-EgoTracks, resulting in EgoStatic, a large-scale diverse dataset for evaluating camera trajectories in-the-wild.",Datasets & Benchmarks,NeurIPS,2023,Poster,Xingyu Chen;Weiyao Wang;Hao Tang;Matt Feiszli,True,https://openreview.net/pdf?id=cuheT1BAp4
cwjh8lqmOL,GPT4Tools: Teaching Large Language Model to Use Tools via Self-instruction,"This paper aims to efficiently enable Large Language Models (LLMs) to use multi-modal tools.
The advanced proprietary LLMs, such as ChatGPT and GPT-4, have shown great potential for tool usage through sophisticated prompt engineering.
Nevertheless, these models typically rely on prohibitive computational costs and publicly inaccessible data.
To address these challenges, we propose the GPT4Tools based on self-instruct to enable open-source LLMs, such as LLaMA and OPT, to use tools.
It generates an instruction-following dataset by prompting an advanced teacher with various multi-modal contexts.
By using the Low-Rank Adaptation (LoRA) optimization, our approach facilitates the open-source LLMs to solve a range of visual problems, including visual comprehension and image generation.
Moreover, we provide a benchmark to evaluate the ability of LLMs to use tools, which is performed in both zero-shot and fine-tuning ways.
Extensive experiments demonstrate the effectiveness of our method on various language models, which not only significantly improves the accuracy of invoking seen tools, but also enables the zero-shot capacity for unseen tools.",main,NeurIPS,2023,Poster,Rui Yang;Lin Song;Yanwei Li;Sijie Zhao;Yixiao Ge;Xiu Li;Ying Shan,True,https://openreview.net/pdf?id=cwjh8lqmOL
dI4wzAE6uV,Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs,"Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs, has gained increasing attention in recent years.  In particular, GPT-4 and Claude-2 have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database contents leaving the gap between academic study and real-world applications. To mitigate this gap, we present BIRD, a BIg benchmark for laRge-scale Database grounded in text-to-SQL tasks, containing 12,751 pairs of text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty database contents, external knowledge between NL questions and database contents,  and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most popular and effective text-to-SQL models, i.e. GPT-4, only achieve 54.89% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand. We also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. 
We believe that BIRD will contribute to advancing real-world applications of text-to-SQL research.
The leaderboard and source code are available: https://bird-bench.github.io/.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Jinyang Li;Binyuan Hui;GE QU;Jiaxi Yang;Binhua Li;Bowen Li;Bailin Wang;Bowen Qin;Ruiying Geng;Nan Huo;Xuanhe Zhou;Chenhao Ma;Guoliang Li;Kevin Chang;Fei Huang;Reynold Cheng;Yongbin Li,True,https://openreview.net/pdf?id=dI4wzAE6uV
dJEjgQcbOt,KuaiSim: A Comprehensive Simulator for Recommender Systems,"Reinforcement Learning (RL)-based recommender systems (RSs) have garnered considerable attention due to their ability to learn optimal recommendation policies and maximize long-term user rewards. 
However, deploying RL models directly in online environments and generating authentic data through A/B tests can pose challenges and require substantial resources. 
Simulators offer an alternative approach by providing training and evaluation environments for RS models, reducing reliance on real-world data. 
Existing simulators have shown promising results but also have limitations such as simplified user feedback, lacking consistency with real-world data, the challenge of simulator evaluation, and difficulties in migration and expansion across RSs.
To address these challenges, we propose KuaiSim, a comprehensive user environment that provides user feedback with multi-behavior and cross-session responses.
The resulting simulator can support three levels of recommendation problems: the request level list-wise recommendation task, the whole-session level sequential recommendation task, and the cross-session level retention optimization task. 
For each task, KuaiSim also provides evaluation protocols and baseline recommendation algorithms that further serve as benchmarks for future research. 
We also restructure existing competitive simulators on the Kuairand Dataset and compare them against KuaiSim to future assess their performance and behavioral differences. 
Furthermore, to showcase KuaiSim's flexibility in accommodating different datasets, we demonstrate its versatility and robustness when deploying it on the ML-1m dataset. The implementation code is available online to ease reproducibility \\\\footnote{https://github.com/Applied-Machine-Learning-Lab/KuaiSim}.",Datasets & Benchmarks,NeurIPS,2023,Poster,,False,https://openreview.net/pdf?id=dJEjgQcbOt
dK1Rs1o0Ij,Reimagining Synthetic Tabular Data Generation through Data-Centric AI: A Comprehensive Benchmark,"Synthetic data serves as an alternative in training machine learning models, particularly when real-world data is limited or inaccessible. However, ensuring that synthetic data mirrors the complex nuances of real-world data is a challenging task. This paper addresses this issue by exploring the potential of integrating data-centric AI techniques which profile the data to guide the synthetic data generation process. Moreover, we shed light on the often ignored consequences of neglecting these data profiles during synthetic data generation --- despite seemingly high statistical fidelity. Subsequently, we propose a novel framework to evaluate the integration of data profiles to guide the creation of more representative synthetic data. In an empirical study, we evaluate the performance of five state-of-the-art models for tabular data generation on eleven distinct tabular datasets. The findings offer critical insights into the successes and limitations of current synthetic data generation techniques. Finally, we provide practical recommendations for integrating data-centric insights into the synthetic data generation process, with a specific focus on classification performance, model selection, and feature selection. This study aims to reevaluate conventional approaches to synthetic data generation and promote the application of data-centric AI techniques in improving the quality and effectiveness of synthetic data.",Datasets & Benchmarks,NeurIPS,2023,Poster,Lasse Hansen;Nabeel Seedat;Mihaela van der Schaar;Andrija Petrovic,False,https://openreview.net/pdf?id=dK1Rs1o0Ij
dOeBYjxSoq,SG×P : A Sorghum Genotype × Phenotype Prediction Dataset and Benchmark,"Large scale field-phenotyping approaches have the potential to solve important questions about the relationship of plant genotype to plant phenotype.  Computational approaches to measuring the phenotype (the observable plant features) are required to address the problem at a large scale, but machine learning approaches to extract phenotypes from sensor data have been hampered by limited access to (a) sufficiently large, organized multi-sensor datasets, (b) field trials that have a large scale and significant number of genotypes, (c) full genetic sequencing of those phenotypes, and (d) datasets sufficiently organized so that algorithm centered researchers can directly address the real biological problems.  To address this, we present SGxP, a novel benchmark dataset from a large-scale field trial consisting of the complete genotype of over 300 sorghum varieties, and time sequences of imagery from several field plots growing each variety, taken with RGB and laser 3D scanner imaging.  To lower the barrier to entry and facilitate further developments, we provide a set of well organized, multi-sensor imagery and corresponding genomic data.  We implement baseline deep learning based phenotyping approaches to create baseline results for individual sensors and multi-sensor fusion for detecting genetic mutations with known impacts.  We also provide and support an open-ended challenge by identifying thousands of genetic mutations whose phenotypic impacts are currently unknown.  A web interface for machine learning researchers and practitioners to share approaches, visualizations and hypotheses supports engagement with plant biologists to further the understanding of the sorghum genotype x phenotype relationship. The full dataset, leaderboard (including baseline results) and discussion forums can be found at http://sorghumsnpbenchmark.com.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zeyu Zhang;Robert Pless;Nadia Shakoor;Austin Carnahan;Abby Stylianou,True,https://openreview.net/pdf?id=dOeBYjxSoq
dUFf0pgkC7,HHD-Ethiopic: A Historical Handwritten Dataset for Ethiopic OCR with Baseline Models and Human-level Performance,"This paper introduces HHD-Ethiopic, a new OCR dataset for historical handwritten Ethiopic script, characterized by a unique syllabic writing system, low resource availability, and complex orthographic diacritics. The dataset consists of roughly 80,000 annotated text-line images from 1700 pages of $18^{th}$ to $20^{th}$ century documents, including a training set with text-line images from the $19^{th}$ to $20^{th}$ century and two test sets. One is distributed similarly to the training set with nearly 6,000 text-line images, and the other contains only images from the $18^{th}$ century manuscripts, with around 16,000 images. The former test set allows us to check baseline performance in the classical IID setting (Independently and Identically Distributed), while the latter addresses a more realistic setting in which the test set is drawn from a different distribution than the training set (Out-Of-Distribution or OOD). Multiple annotators labeled all text-line images for the HHD-Ethiopic dataset, and an expert supervisor double-checked them. We assessed human-level recognition performance and compared it with state-of-the-art OCR models using the Character Error Rate (CER) metric. Our results show that the model performed comparably to human-level recognition on the $18^{th}$ century test set and outperformed humans on the IID test set. However, the unique challenges posed by the Ethiopic script, such as detecting complex diacritics, still present difficulties for the models. Our baseline evaluation and HHD-Ethiopic dataset will stimulate further research on tailored OCR techniques for the Ethiopic script. The HHD-Ethiopic dataset and the code are  publicly available at https://github.com/bdu-birhanu/HHD-Ethiopic",Datasets & Benchmarks,NeurIPS,2023,Reject,Birhanu Hailu Belay;Isabelle Guyon;Tadele Mengiste;Bezawork Tilahun;Marcus Liwicki;Tesfa Tegegne;Romain Egele;Tsiyon Worku,True,https://openreview.net/pdf?id=dUFf0pgkC7
dVaWCDMBof,DataComp: In search of the next generation of multimodal datasets,"Multimodal datasets are a critical component in recent breakthroughs such as CLIP, Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets. Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DataComp workflow leads to better training sets. Our best baseline, DataComp-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI's CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute. We release \\\\datanet and all accompanying code at www.datacomp.ai.",Datasets & Benchmarks,NeurIPS,2023,Oral,Samir Yitzhak Gadre;Gabriel Ilharco;Alex Fang;Jonathan Hayase;Georgios Smyrnis;Thao Nguyen;Ryan Marten;Mitchell Wortsman;Dhruba Ghosh;Jieyu Zhang;Eyal Orgad;Rahim Entezari;Giannis Daras;Sarah M Pratt;Vivek Ramanujan;Yonatan Bitton;Kalyani Marathe;Stephen Mussmann;Richard Vencu;Mehdi Cherti;Ranjay Krishna;Pang Wei Koh;Olga Saukh;Alexander Ratner;Shuran Song;Hannaneh Hajishirzi;Ali Farhadi;Romain Beaumont;Sewoong Oh;Alex Dimakis;Jenia Jitsev;Yair Carmon;Vaishaal Shankar;Ludwig Schmidt,True,https://openreview.net/pdf?id=dVaWCDMBof
dhJ8VbcEtX,AQuA: A Benchmarking Tool for Label Quality Assessment,"Machine learning (ML) models are only as good as the data they are trained on. But recent studies have found datasets widely used to train and evaluate ML models, e.g. _ImageNet_, to have pervasive labeling errors. Erroneous labels on the train set hurt ML models' ability to generalize, and they impact evaluation and model selection using the test set. Consequently, learning in the presence of labeling errors is an active area of research, yet this field lacks a comprehensive benchmark to evaluate these methods. Most of these methods are evaluated on a few computer vision datasets with significant variance in the experimental protocols. With such a large pool of methods and inconsistent evaluation, it is also unclear how ML practitioners can choose the right models to assess label quality in their data. To this end, we propose a benchmarking environment _AQuA_ to rigorously evaluate methods that enable machine learning in the presence of label noise. We also introduce a design space to delineate concrete design choices of label error detection models. We hope that our proposed design space and benchmark enable practitioners to choose the right tools to improve their label quality and that our benchmark enables objective and rigorous evaluation of machine learning tools facing mislabeled data.",Datasets & Benchmarks,NeurIPS,2023,Poster,Mononito Goswami;Vedant Sanil;Arjun Choudhry;Arvind Srinivasan;Chalisa Udompanyawit;Artur Dubrawski,False,https://openreview.net/pdf?id=dhJ8VbcEtX
doV2nhGm1l,Hyper-Skin: A Hyperspectral Dataset for Reconstructing Facial Skin-Spectra from RGB Images,"We introduce Hyper-Skin, a hyperspectral dataset covering wide range of wavelengths from visible (VIS) spectrum (400nm - 700nm) to near-infrared (NIR) spectrum (700nm - 1000nm), uniquely designed to facilitate research on facial skin-spectra reconstruction.
By reconstructing skin spectra from RGB images, our dataset enables the study of hyperspectral skin analysis, such as melanin and hemoglobin concentrations, directly on the consumer device. 
Overcoming limitations of existing datasets, Hyper-Skin consists of diverse facial skin data collected with a pushbroom hyperspectral camera. 
With 330 hyperspectral cubes from 51 subjects, the dataset covers the facial skin from different angles and facial poses.
Each hyperspectral cube has dimensions of 1024$\\\\times$1024$\\\\times$448, resulting in millions of spectra vectors per image. 
The dataset, carefully curated in adherence to ethical guidelines, includes paired hyperspectral images and synthetic RGB images generated using real camera responses. 
We demonstrate the efficacy of our dataset by showcasing skin spectra reconstruction using state-of-the-art models on 31 bands of hyperspectral data resampled in the VIS  and NIR spectrum. 
This  Hyper-Skin dataset would be a valuable resource to NeurIPS community, encouraging the development of novel algorithms for skin spectral reconstruction while fostering interdisciplinary collaboration in hyperspectral skin analysis related to cosmetology and skin's well-being. 
Instructions to request the data and the related benchmarking codes are publicly available at: https://github.com/hyperspectral-skin/Hyper-Skin-2023.",Datasets & Benchmarks,NeurIPS,2023,Poster,Pai Chet Ng;Zhixiang Chi;Yannick Verdie;Juwei Lu;Konstantinos N Plataniotis,True,https://openreview.net/pdf?id=doV2nhGm1l
e1WgjvFGWp,Large Language Models of Code Fail at Completing Code with Potential Bugs,"Large language models of code (Code-LLMs) have recently brought tremendous advances to code completion, a fundamental feature of programming assistance and code intelligence. However, most existing works ignore the possible presence of bugs in the code context for generation, which are inevitable in software development. Therefore, we introduce and study the buggy-code completion problem, inspired by the realistic scenario of real-time code suggestion where the code context contains potential bugs – anti-patterns that can become bugs in the completed program. To systematically study the task, we introduce two datasets: one with synthetic bugs derived from semantics-altering operator changes (buggy-HumanEval) and one with realistic bugs derived from user submissions to coding problems (buggy-FixEval). We find that the presence of potential bugs significantly degrades the generation performance of the high-performing Code-LLMs. For instance, the passing rates of CODEGEN-2B-MONO on test cases of buggy-HumanEval drop more than 50% given a single potential bug in the context. Finally, we investigate several post-hoc methods for mitigating the adverse effect of potential bugs and find that there remains a large gap in post-mitigation performance.",main,NeurIPS,2023,Poster,Tuan Dinh;Jinman Zhao;Samson Tan;Renato Negrinho;Leonard Lausen;Sheng Zha;George Karypis,True,https://openreview.net/pdf?id=e1WgjvFGWp
e2wtjx0Yqu,CLadder: Assessing Causal Reasoning in Language Models,"The ability to perform causal reasoning is widely considered a core feature of intelligence. In this work, we investigate whether large language models (LLMs) can coherently reason about causality. Much of the existing work in natural language processing (NLP) focuses on evaluating _commonsense_ causal reasoning in LLMs, thus failing to assess whether a model can perform causal inference in accordance with a set of well-defined _formal rules_. To address this, we propose a new NLP task, _causal inference in natural language_, inspired by the _""causal inference engine""_ postulated by Judea Pearl et al. We compose a large dataset, CLadder, with 10K samples: based on a collection of causal graphs and queries (associational, interventional, and counterfactual), we obtain symbolic questions and ground-truth answers, through an oracle causal inference engine. These are then translated into natural language. We evaluate multiple LLMs on our dataset, and we introduce and evaluate a bespoke chain-of-thought prompting strategy, CausalCoT. We show that our task is highly challenging for LLMs, and we conduct an in-depth analysis to gain deeper insight into the causal reasoning abilities of LLMs. Our data is open-sourced at https://huggingface.co/datasets/causalNLP/cladder, and our code can be found at https://github.com/causalNLP/cladder.",main,NeurIPS,2023,Poster,Zhijing Jin;Yuen Chen;Felix Leeb;Luigi Gresele;Ojasv Kamal;Zhiheng LYU;Kevin Blin;Fernando Gonzalez Adauto;Max Kleiman-Weiner;Mrinmaya Sachan;Bernhard Schölkopf,True,https://openreview.net/pdf?id=e2wtjx0Yqu
e9n4JjkmXZ,URL: A Representation Learning Benchmark for Transferable Uncertainty Estimates,"Representation learning has significantly driven the field to develop pretrained models that can act as a valuable starting point when transferring to new datasets. With the rising demand for reliable machine learning and uncertainty quantification, there is a need for pretrained models that not only provide embeddings but also transferable uncertainty estimates. To guide the development of such models, we propose the Uncertainty-aware Representation Learning (URL) benchmark. Besides the transferability of the representations, it also measures the zero-shot transferability of the uncertainty estimate using a novel metric. We apply URL to evaluate ten uncertainty quantifiers that are pretrained on ImageNet and transferred to eight downstream datasets. We find that approaches that focus on the uncertainty of the representation itself or estimate the prediction risk directly outperform those that are based on the probabilities of upstream classes. Yet, achieving transferable uncertainty quantification remains an open challenge. Our findings indicate that it is not necessarily in conflict with traditional representation learning goals. Code is available at [https://github.com/mkirchhof/url](https://github.com/mkirchhof/url).",Datasets & Benchmarks,NeurIPS,2023,Poster,Michael Kirchhof;Bálint Mucsányi;Seong Joon Oh;Enkelejda Kasneci,True,https://openreview.net/pdf?id=e9n4JjkmXZ
eBXM62SqKY,POP-3D: Open-Vocabulary 3D Occupancy Prediction from Images,"We describe an approach to predict open-vocabulary 3D semantic voxel occupancy map from input 2D images with the objective of enabling 3D grounding, segmentation and retrieval of free-form language queries. This is a challenging problem because of the 2D-3D ambiguity and the open-vocabulary nature of the target tasks, where obtaining annotated training data in 3D is difficult. The contributions of this work are three-fold. 
First, we design a new model architecture for open-vocabulary 3D semantic occupancy prediction.  The architecture consists of a 2D-3D encoder together with occupancy prediction and 3D-language heads. The output is a dense voxel map of 3D grounded language embeddings enabling a range of open-vocabulary tasks. 
Second, we develop a tri-modal self-supervised learning algorithm that leverages three modalities: (i) images, (ii) language and (iii) LiDAR point clouds, and enables training the proposed architecture using a strong pre-trained vision-language model without the need for any 3D manual language annotations. 
Finally, we demonstrate quantitatively the strengths of the proposed model on several open-vocabulary tasks:
Zero-shot 3D semantic segmentation using existing datasets; 3D grounding and retrieval of free-form language queries, using a small dataset that we propose as an extension of nuScenes. You can find the project page here https://vobecant.github.io/POP3D.",main,NeurIPS,2023,Poster,Antonín Vobecký;Oriane Siméoni;David Hurych;Spyros Gidaris;Andrei Bursuc;Patrick Perez;Josef Sivic,True,https://openreview.net/pdf?id=eBXM62SqKY
eEK99egXeB,OpenDataVal: a Unified Benchmark for Data Valuation,"Assessing the quality and impact of individual data points is critical for improving model performance and mitigating undesirable biases within the training dataset. Several data valuation algorithms have been proposed to quantify data quality, however, there lacks a systemic and standardized benchmarking system for data valuation. In this paper, we introduce *OpenDataVal*, an easy-to-use and unified benchmark framework that empowers researchers and practitioners to apply and compare various data valuation algorithms. *OpenDataVal* provides an integrated environment that includes (i) a diverse collection of image, natural language, and tabular datasets, (ii) implementations of eleven different state-of-the-art data valuation algorithms, and (iii) a prediction model API that can import any models in scikit-learn. Furthermore, we propose four downstream machine learning tasks for evaluating the quality of data values. We perform benchmarking analysis using *OpenDataVal*, quantifying and comparing the efficacy of state-of-the-art data valuation approaches. We find that no single algorithm performs uniformly best across all tasks, and an appropriate algorithm should be employed for a user's downstream task. *OpenDataVal* is publicly available at https://opendataval.github.io with comprehensive documentation. Furthermore, we provide a leaderboard where researchers can evaluate the effectiveness of their own data valuation algorithms.",Datasets & Benchmarks,NeurIPS,2023,Poster,Kevin Fu Jiang;Weixin Liang;James Zou;Yongchan Kwon,True,https://openreview.net/pdf?id=eEK99egXeB
eIFZtkshgH,AD-PT: Autonomous Driving Pre-Training with Large-scale Point Cloud Dataset,"It is a long-term vision for Autonomous Driving (AD) community that the perception models can learn from a large-scale point cloud dataset, to obtain unified representations that can achieve promising results on different tasks or benchmarks. Previous works mainly focus on the self-supervised pre-training pipeline, meaning that they perform the pre-training and fine-tuning on the same benchmark, which is difficult to attain the performance scalability and cross-dataset application for the pre-training checkpoint.  In this paper, for the first time, we are committed to building a large-scale pre-training point-cloud dataset with diverse data distribution, and meanwhile learning generalizable representations from such a diverse pre-training dataset. We formulate the point-cloud pre-training task as a semi-supervised problem, which leverages the few-shot labeled and massive unlabeled point-cloud data to generate the unified backbone representations that can be directly applied to many baseline models and benchmarks, decoupling the AD-related pre-training process and downstream fine-tuning task. During the period of backbone pre-training, by enhancing the scene- and instance-level distribution diversity and exploiting the backbone's ability to learn from unknown instances, we achieve significant performance gains on a series of downstream perception benchmarks including Waymo, nuScenes, and KITTI, under different baseline models like PV-RCNN++, SECOND, CenterPoint.",main,NeurIPS,2023,Poster,Jiakang Yuan;Bo Zhang;Xiangchao Yan;Botian Shi;Tao Chen;Yikang LI;Yu Qiao,True,https://openreview.net/pdf?id=eIFZtkshgH
eJ5nu9qvWz,M$^2$Hub: Unlocking the Potential of Machine Learning for Materials Discovery,"We introduce M$^2$Hub, a toolkit for advancing machine learning in materials discovery. Machine learning has achieved remarkable progress in modeling molecular structures, especially biomolecules for drug discovery. However, the development of machine learning approaches for modeling materials structures lag behind, which is partly due to the lack of an integrated platform that enables access to diverse tasks for materials discovery. To bridge this gap, M$^2$Hub will enable easy access to materials discovery tasks, datasets, machine learning methods, evaluations, and benchmark results that cover the entire workflow. Specifically, the first release of M$^2$Hub focuses on three key stages in materials discovery: virtual screening, inverse design, and molecular simulation, including 9 datasets that covers 6 types of materials with 56 tasks across 8 types of material properties. We further provide 2 synthetic datasets for the purpose of generative tasks on materials. In addition to random data splits, we also provide 3 additional data partitions to reflect the real-world materials discovery scenarios. State-of-the-art machine learning methods (including those are suitable for materials structures but never compared in the literature) are benchmarked on representative tasks. Our codes and library are publicly available at \\\\url{https://github.com/yuanqidu/M2Hub}.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yuanqi Du;Yingheng Wang;Yining Huang;Jianan Canal Li;Yanqiao Zhu;Tian Xie;Chenru Duan;John Gregoire;Carla P Gomes,True,https://openreview.net/pdf?id=eJ5nu9qvWz
eM6WLko4Dv,"LAMM: Language-Assisted Multi-Modal Instruction-Tuning Dataset, Framework, and Benchmark","Large language models have emerged as a promising approach towards achieving general-purpose AI agents. The thriving open-source LLM community has greatly accelerated the development of agents that support human-machine dialogue interaction through natural language processing. However, human interaction with the world extends beyond only text as a modality, and other modalities such as vision are also crucial. Recent works on multi-modal large language models, such as GPT-4V and Bard, have demonstrated their effectiveness in handling visual modalities. However, the transparency of these works is limited and insufficient to support academic research. To the best of our knowledge, we present one of the very first open-source endeavors in the field, LAMM, encompassing a Language-Assisted Multi-Modal instruction tuning dataset, framework, and benchmark. Our aim is to establish LAMM as a growing ecosystem for training and evaluating MLLMs, with a specific focus on facilitating AI agents capable of bridging the gap between ideas and execution, thereby enabling seamless human-AI interaction. Our main contribution is three-fold: 1) We present a comprehensive dataset and benchmark, which cover a wide range of vision tasks for 2D and 3D vision. Extensive experiments validate the effectiveness of our dataset and benchmark. 2) We outline the detailed methodology of constructing multi-modal instruction tuning datasets and benchmarks for MLLMs, enabling rapid scaling and extension of MLLM research to diverse domains, tasks, and modalities. 3) We provide a primary but potential MLLM training framework optimized for modality extension. We also provide baseline models, comprehensive experimental observations, and analysis to accelerate future research. Our baseline model is trained within 24 A100 GPU hours, framework supports training with V100 and RTX3090 is available thanks to the open-source society. Codes and data are now available at https://openlamm.github.io.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zhenfei Yin;Jiong WANG;Jianjian Cao;Zhelun Shi;Dingning Liu;Mukai Li;Xiaoshui Huang;Zhiyong Wang;Lu Sheng;LEI BAI;Jing Shao;Wanli Ouyang,True,https://openreview.net/pdf?id=eM6WLko4Dv
ecRaDicXxw,DiffVL: Scaling Up Soft Body Manipulation using Vision-Language Driven Differentiable Physics,"Combining gradient-based trajectory optimization with differentiable physics simulation is an efficient technique for solving soft-body manipulation problems.
Using a well-crafted optimization objective, the solver can quickly converge onto a valid trajectory.
However, writing the appropriate objective functions requires expert knowledge, making it difficult to collect a large set of naturalistic problems from non-expert users.
We introduce DiffVL, a method that enables non-expert users to communicate soft-body manipulation tasks -- a combination of vision and natural language, given in multiple stages -- that can be readily leveraged by a differential physics solver. 
We have developed GUI tools that enable non-expert users to specify 100 tasks inspired by real-life soft-body manipulations from online videos, which we'll make public.
We leverage large language models to translate task descriptions into machine-interpretable optimization objectives. The optimization objectives can help differentiable physics solvers to solve these long-horizon multistage tasks that are challenging for previous baselines.",main,NeurIPS,2023,Poster,Zhiao Huang;Feng Chen;Yewen Pu;Chunru Lin;Hao Su;Chuang Gan,True,https://openreview.net/pdf?id=ecRaDicXxw
epUQ40eCzk,TWIGMA: A dataset of AI-Generated Images with Metadata From Twitter,"Recent progress in generative artificial intelligence (gen-AI) has enabled the generation of photo-realistic and artistically-inspiring photos at a single click, catering to millions of users online. To explore how people use gen-AI models such as DALLE and StableDiffusion, it is critical to understand the themes, contents, and variations present in the AI-generated photos. In this work, we introduce TWIGMA (TWItter Generative-ai images with MetadatA), a comprehensive dataset encompassing over 800,000 gen-AI images collected from Jan 2021 to March 2023 on Twitter, with associated metadata (e.g., tweet text, creation date, number of likes), available at https://zenodo.org/records/8031785. Through a comparative analysis of TWIGMA with natural images and human artwork, we find that gen-AI images possess distinctive characteristics and exhibit, on average, lower variability when compared to their non-gen-AI counterparts. Additionally, we find that the similarity between a gen-AI image and natural images is inversely correlated with the number of likes. Finally, we observe a longitudinal shift in the themes of AI-generated images on Twitter, with users increasingly sharing artistically sophisticated content such as intricate human portraits, whereas their interest in simple subjects such as natural scenes and animals has decreased. Our findings underscore the significance of TWIGMA as a unique data resource for studying AI-generated images.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yiqun T. Chen;James Zou,True,https://openreview.net/pdf?id=epUQ40eCzk
fKVEMNmWqU,Reduced Policy Optimization for Continuous Control with Hard Constraints,"Recent advances in constrained reinforcement learning (RL) have endowed reinforcement learning with certain safety guarantees. However, deploying existing constrained RL algorithms in continuous control tasks with general hard constraints remains challenging, particularly in those situations with non-convex hard constraints. Inspired by the generalized reduced gradient (GRG) algorithm, a classical constrained optimization technique, we propose a reduced policy optimization (RPO) algorithm that combines RL with GRG to address general hard constraints. RPO partitions actions into basic actions and nonbasic actions following the GRG method and outputs the basic actions via a policy network. Subsequently, RPO calculates the nonbasic actions by solving equations based on equality constraints using the obtained basic actions. The policy network is then updated by implicitly differentiating nonbasic actions with respect to basic actions. Additionally, we introduce an action projection procedure based on the reduced gradient and apply a modified Lagrangian relaxation technique to ensure inequality constraints are satisfied. To the best of our knowledge, RPO is the first attempt that introduces GRG to RL as a way of efficiently handling both equality and inequality hard constraints. It is worth noting that there is currently a lack of RL environments with complex hard constraints, which motivates us to develop three new benchmarks: two robotics manipulation tasks and a smart grid operation control task. With these benchmarks, RPO achieves better performance than previous constrained RL algorithms in terms of both cumulative reward and constraint violation. We believe RPO, along with the new benchmarks, will open up new opportunities for applying RL to real-world problems with complex constraints.",main,NeurIPS,2023,Poster,Shutong Ding;Jingya Wang;Yali Du;Ye Shi,True,https://openreview.net/pdf?id=fKVEMNmWqU
fKzSz0oyaI,AVeriTeC: A Dataset for Real-world Claim Verification with Evidence from the Web,"Existing datasets for automated fact-checking have substantial limitations, such as relying on artificial claims, lacking annotations for evidence and intermediate reasoning, or including evidence published after the claim. In this paper we introduce AVeriTeC, a new dataset of 4,568 real-world claims covering fact-checks by 50 different organizations. Each claim is annotated with question-answer pairs supported by evidence available online, as well as textual justifications explaining how the evidence combines to produce a verdict. Through a multi-round annotation process, we avoid common pitfalls including context dependence, evidence insufficiency, and temporal leakage, and reach a substantial inter-annotator agreement of $\\\\kappa=0.619$ on verdicts. We develop a baseline as well as an evaluation scheme for verifying claims through question-answering against the open web.",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=fKzSz0oyaI
fOrm2rGX2r,C-Eval: A Multi-Level Multi-Discipline Chinese Evaluation Suite for Foundation Models,"New NLP benchmarks are urgently needed to align with the rapid development of large language models (LLMs). We present C-Eval, the first comprehensive Chinese evaluation suite designed to assess advanced knowledge and reasoning abilities of foundation models in a Chinese context. C-Eval comprises multiple-choice questions across four difficulty levels: middle school, high school, college, and professional. The questions span 52 diverse disciplines, ranging from humanities to science and engineering. C-Eval is accompanied by C-Eval Hard, a subset of very challenging subjects in C-Eval that requires advanced reasoning abilities to solve. We conduct a comprehensive evaluation of the most advanced LLMs on C-Eval, including both English- and Chinese-oriented models. Results indicate that only GPT-4 could achieve an average accuracy of over 60%, suggesting that there is still significant room for improvement for current LLMs. We anticipate C-Eval will help analyze important strengths and shortcomings of foundation models, and foster their development and growth for Chinese users.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yuzhen Huang;Yuzhuo Bai;Zhihao Zhu;Junlei Zhang;Jinghan Zhang;Tangjun Su;Junteng Liu;Chuancheng Lv;Yikai Zhang;jiayi lei;Yao Fu;Maosong Sun;Junxian He,True,https://openreview.net/pdf?id=fOrm2rGX2r
fZq8Tw0jdm,American Stories: A Large-Scale Structured Text Dataset of Historical U.S. Newspapers,"Existing full text datasets of U.S. public domain newspapers do not recognize the often complex layouts of newspaper scans, and as a result the digitized content scrambles texts from articles, headlines, captions, advertisements, and other layout regions. OCR quality can also be low. This study develops a novel, deep learning pipeline for extracting full article texts from newspaper images and applies it to the nearly 20 million scans in Library of Congress's public domain Chronicling America collection. The pipeline includes layout detection, legibility classification, custom OCR, and association of article texts spanning multiple bounding boxes. To achieve high scalability, it is built with efficient architectures designed for mobile phones. The resulting American Stories dataset provides high quality data that could be used for pre-training a large language model to achieve better understanding of historical English and historical world knowledge. The dataset could also be added to the external database of a retrieval-augmented language model to make historical information - ranging from interpretations of political events to minutiae about the lives of people's ancestors - more widely accessible. Furthermore, structured article texts facilitate using transformer-based methods for popular social science applications like topic classification, detection of reproduced content, and news story clustering.  Finally, American Stories provides a massive silver quality dataset for innovating multimodal layout analysis models and other multimodal applications.",Datasets & Benchmarks,NeurIPS,2023,Poster,Melissa Dell;Jacob Carlson;Tom Bryan;Emily Silcock;Abhishek Arora;Zejiang Shen;Luca D'Amico-Wong;Quan Le;Pablo Querubin;Leander Heldring,True,https://openreview.net/pdf?id=fZq8Tw0jdm
fmJv8Hj0yo,Are Diffusion Models Vision-And-Language Reasoners?,"Text-conditioned image generation models have recently shown immense qualitative success using denoising diffusion processes. However, unlike discriminative vision-and-language models, it is a non-trivial task to subject these diffusion-based generative models to automatic fine-grained quantitative evaluation of high-level phenomena such as compositionality.
Towards this goal, we perform two innovations. First, we transform diffusion-based models (in our case, Stable Diffusion) for any image-text matching (ITM) task using a novel method called DiffusionITM.
Second, we introduce the Generative-Discriminative Evaluation Benchmark (GDBench) benchmark with 7 complex vision-and-language tasks, bias evaluation and detailed analysis.
We find that Stable Diffusion + DiffusionITM is competitive on many tasks and outperforms CLIP on compositional tasks like like CLEVR and Winoground.
We further boost its compositional performance with a transfer setup by fine-tuning on MS-COCO while retaining generative capabilities. 
We also measure the stereotypical bias in diffusion models, and find that Stable Diffusion 2.1 is, for the most part, less biased than Stable Diffusion 1.5.
Overall, our results point in an exciting direction bringing discriminative and generative model evaluation closer. We will release code and benchmark setup soon.",main,NeurIPS,2023,Poster,Benno Krojer;Elinor Poole-Dayan;Vikram Voleti;Christopher Pal;Siva Reddy,True,https://openreview.net/pdf?id=fmJv8Hj0yo
fnQ2QPl5n7,GUARD: A Safe Reinforcement Learning Benchmark,"Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state-of-the-art safe RL algorithms in various task settings using GUARD and establish baselines that future work can build on.",Datasets & Benchmarks,NeurIPS,2023,Reject,Weiye Zhao;Rui Chen;Yifan Sun;Ruixuan Liu;Tianhao Wei;Changliu Liu,True,https://openreview.net/pdf?id=fnQ2QPl5n7
fr3OT4rosO,CityRefer: Geography-aware 3D Visual Grounding Dataset on City-scale Point Cloud Data,"City-scale 3D point cloud is a promising way to express detailed and complicated outdoor structures. It encompasses both the appearance and geometry features of segmented city components, including cars, streets, and buildings that can be utilized for attractive applications such as user-interactive navigation of autonomous vehicles and drones. However, compared to the extensive text annotations available for images and indoor scenes, the scarcity of text annotations for outdoor scenes poses a significant challenge for achieving these applications. To tackle this problem, we introduce the CityRefer dataset for city-level visual grounding. The dataset consists of 35k natural language descriptions of 3D objects appearing in SensatUrban city scenes and 5k landmarks labels synchronizing with OpenStreetMap. To ensure the quality and accuracy of the dataset, all descriptions and labels in the CityRefer dataset are manually verified. We also have developed a baseline system that can learn encoded language descriptions, 3D object instances, and geographical information about the city's landmarks to perform visual grounding on the CityRefer dataset. To the best of our knowledge, the CityRefer dataset is the largest city-level visual grounding dataset for localizing specific 3D objects.",Datasets & Benchmarks,NeurIPS,2023,Poster,Taiki Miyanishi;Fumiya Kitamori;Shuhei Kurita;Jungdae Lee;Motoaki Kawanabe;Nakamasa Inoue,True,https://openreview.net/pdf?id=fr3OT4rosO
frVo9MzRuU,Compositional Abilities Emerge Multiplicatively: Exploring Diffusion Models on a Synthetic Task,"Modern generative models exhibit unprecedented capabilities to generate extremely realistic data. However, given the inherent compositionality of the real world, reliable use of these models in practical applications requires that they exhibit the capability to compose a novel set of concepts to generate outputs not seen in the training data set. Prior work demonstrates that recent diffusion models do exhibit intriguing compositional generalization abilities, but also fail unpredictably. Motivated by this, we perform a controlled study for understanding compositional generalization in conditional diffusion models in a synthetic setting, varying different attributes of the training data and measuring the model's ability to generate samples out-of-distribution. Our results show: (i) the order in which the ability to generate samples from a concept and compose them emerges is governed by the structure of the underlying data-generating process; (ii) performance on compositional tasks exhibits a sudden ""emergence"" due to multiplicative reliance on the performance of constituent tasks, partially explaining emergent phenomena seen in generative models; and (iii) composing concepts with lower frequency in the training data to generate out-of-distribution samples requires considerably more optimization steps compared to generating in-distribution samples. Overall, our study lays a foundation for understanding emergent capabilities and compositionality in generative models from a data-centric perspective.",main,NeurIPS,2023,Poster,Maya Okawa;Ekdeep Singh Lubana;Robert P. Dick;Hidenori Tanaka,True,https://openreview.net/pdf?id=frVo9MzRuU
ftPoVcm821,Demo2Code: From Summarizing Demonstrations to Synthesizing Code via Extended Chain-of-Thought,"Language instructions and demonstrations are two natural ways for users to teach robots personalized tasks. Recent progress in Large Language Models (LLMs) has shown impressive performance in translating language instructions into code for robotic tasks. However, translating demonstrations into task code continues to be a challenge due to the length and complexity of both demonstrations and code, making learning a direct mapping intractable. This paper presents Demo2Code, a novel framework that generates robot task code from demonstrations via an extended chain-of-thought and defines a common latent specification to connect the two. Our framework employs a robust two-stage process: (1) a recursive summarization technique that condenses demonstrations into concise specifications, and (2) a code synthesis approach that expands each function recursively from the generated specifications. We conduct extensive evaluation on various robot task benchmarks, including a novel game benchmark Robotouille, designed to simulate diverse cooking tasks in a kitchen environment.",main,NeurIPS,2023,Poster,Huaxiaoyue Wang;Gonzalo Gonzalez-Pumariega;Yash Sharma;Sanjiban Choudhury,True,https://openreview.net/pdf?id=ftPoVcm821
fvKaLF1ns8,InterCode: Standardizing and Benchmarking Interactive Coding with Execution Feedback,"Humans write code in a fundamentally interactive manner and rely on constant execution feedback to correct errors, resolve ambiguities, and decompose tasks. While LLMs have recently exhibited promising coding capabilities, current coding benchmarks mostly consider a static instruction-to-code sequence transduction process, which has the potential for error propagation and a disconnect between the generated code and its final execution environment. To address this gap, we introduce InterCode, a lightweight, flexible, and easy-to-use framework of interactive coding as a standard reinforcement learning (RL) environment, with code as actions and execution feedback as observations. Our framework is language and platform agnostic, uses self-contained Docker environments to provide safe and reproducible execution, and is compatible out-of-the-box with traditional seq2seq coding methods, while enabling the development of new methods for interactive code generation. We use InterCode to create three interactive code environments with Bash, SQL, and Python as action spaces, leveraging data from the static NL2Bash, Spider, and MBPP datasets. We demonstrate InterCode’s viability as a testbed by evaluating multiple state-of-the-art LLMs configured with different prompting strategies such as ReAct and Plan & Solve. Our results showcase the benefits of interactive code generation and demonstrate that InterCode can serve as a challenging benchmark for advancing code understanding and generation capabilities. InterCode is designed to be easily extensible and can even be used to create new tasks such as Capture the Flag, a popular coding puzzle that is inherently multi-step and involves multiple programming languages.",Datasets & Benchmarks,NeurIPS,2023,Poster,John Yang;Akshara Prabhakar;Karthik R Narasimhan;Shunyu Yao,True,https://openreview.net/pdf?id=fvKaLF1ns8
g0QovXbFw3,BeaverTails: Towards Improved Safety Alignment of LLM via a Human-Preference Dataset,"In this paper, we introduce the BeaverTails dataset, aimed at fostering research on safety alignment in large language models (LLMs). This dataset uniquely separates annotations of helpfulness and harmlessness for question-answering pairs, thus offering distinct perspectives on these crucial attributes. In total, we have gathered safety meta-labels for 333,963 question-answer (QA) pairs and 361,903 pairs of expert comparison data for both the helpfulness and harmlessness metrics. We further showcase applications of BeaverTails in content moderation and reinforcement learning with human feedback (RLHF), emphasizing its potential for practical safety measures in LLMs. We believe this dataset provides vital resources for the community, contributing towards the safe development and deployment of LLMs. Our project page is available at the following URL: https://sites.google.com/view/pku-beavertails.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jiaming Ji;Mickel Liu;Juntao Dai;Xuehai Pan;Chi Zhang;Ce Bian;Boyuan Chen;Ruiyang Sun;Yizhou Wang;Yaodong Yang,True,https://openreview.net/pdf?id=g0QovXbFw3
g5v3Ig6WVq,Auslan-Daily: Australian Sign Language Translation for Daily Communication and News,"Sign language translation (SLT) aims to convert a continuous sign language video clip into a spoken language. Considering different geographic regions generally have their own native sign languages, it is valuable to establish corresponding SLT datasets to support related communication and research. Auslan, as a sign language specific to Australia, still lacks a dedicated large-scale dataset for SLT.
To fill this gap, we curate an Australian Sign Language translation dataset, dubbed Auslan-Daily, which is collected from the Auslan educational TV series and Auslan TV programs. The former involves daily communications among multiple signers in the wild, while the latter comprises sign language videos for up-to-date news, weather forecasts, and documentaries. In particular, Auslan-Daily has two main features: (1) the topics are diverse and signed by multiple signers, and (2) the scenes in our dataset are more complex, e.g., captured in various environments, gesture interference during multi-signers' interactions and various camera positions. With a collection of more than 45 hours of high-quality Auslan video materials, we invite Auslan experts to align different fine-grained visual and language pairs, including video $\\\\leftrightarrow$ fingerspelling, video $\\\\leftrightarrow$ gloss, and video $\\\\leftrightarrow$ sentence. As a result, Auslan-Daily contains multi-grained annotations that can be utilized to accomplish various fundamental sign language tasks, such as signer detection, sign spotting, fingerspelling detection, isolated sign language recognition, sign language translation and alignment. Moreover, we benchmark results with state-of-the-art models for each task in Auslan-Daily. Experiments indicate that Auslan-Daily is a highly challenging SLT dataset, and we hope this dataset will contribute to the development of Auslan and the advancement of sign languages worldwide in a broader context. All datasets and benchmarks are available at Auslan-Daily.",Datasets & Benchmarks,NeurIPS,2023,Poster,Xin Shen;Shaozu Yuan;Hongwei Sheng;Heming Du;Xin Yu,True,https://openreview.net/pdf?id=g5v3Ig6WVq
g7OX2sOJtn,LeanDojo: Theorem Proving with Retrieval-Augmented Language Models,"Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection—a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's program analysis capability to identify accessible premises and hard negative examples, which makes retrieval much more effective. Furthermore, we construct a new benchmark consisting of 98,734 theorems and proofs extracted from Lean's math library. It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never used in training. We use this benchmark for training and evaluation, and experimental results demonstrate the effectiveness of ReProver over non-retrieval baselines and GPT-4. We thus provide the first set of open-source LLM-based theorem provers without any proprietary datasets and release it under a permissive MIT license to facilitate further research.",Datasets & Benchmarks,NeurIPS,2023,Oral,Kaiyu Yang;Aidan M Swope;Alex Gu;Rahul Chalamala;Peiyang Song;Shixing Yu;Saad Godil;Ryan Prenger;Anima Anandkumar,True,https://openreview.net/pdf?id=g7OX2sOJtn
gFf0a0ZxJM,OpenAGI: When LLM Meets Domain Experts,"Human Intelligence (HI) excels at combining basic skills to solve complex tasks. This capability is vital for Artificial Intelligence (AI) and should be embedded in comprehensive AI Agents, enabling them to harness expert models for complex task-solving towards Artificial General Intelligence (AGI). Large Language Models (LLMs) show promising learning and reasoning abilities, and can effectively use external models, tools, plugins, or APIs to tackle complex problems. In this work, we introduce OpenAGI, an open-source AGI research and development platform designed for solving multi-step, real-world tasks. Specifically, OpenAGI uses a dual strategy, integrating standard benchmark tasks for benchmarking and evaluation, and open-ended tasks including more expandable models, tools, plugins, or APIs for creative problem-solving. Tasks are presented as natural language queries to the LLM, which then selects and executes appropriate models. We also propose a Reinforcement Learning from Task Feedback (RLTF) mechanism that uses task results to improve the LLM's task-solving ability, which creates a self-improving AI feedback loop. While we acknowledge that AGI is a broad and multifaceted research challenge with no singularly defined solution path, the integration of LLMs with domain-specific expert models, inspired by mirroring the blend of general and specialized intelligence in humans, offers a promising approach towards AGI. We are open-sourcing the OpenAGI project's code, dataset, benchmarks, evaluation methods, and the UI demo to foster community involvement in AGI advancement: https://github.com/agiresearch/OpenAGI.",Datasets & Benchmarks,NeurIPS,2023,Poster,,False,https://openreview.net/pdf?id=gFf0a0ZxJM
gMYsxTin4x,SiT Dataset: Socially Interactive Pedestrian Trajectory Dataset for Social Navigation Robots,"To ensure secure and dependable mobility in environments shared by humans and robots, social navigation robots should possess the capability to accurately perceive and predict the trajectories of nearby pedestrians. In this paper, we present a novel dataset of pedestrian trajectories, referred to as Social Interactive Pedestrian Trajectory (SiT) dataset, which can be used to train pedestrian detection, tracking, and trajectory prediction models needed to design social navigation robots. Our dataset includes sequential raw data captured by two 3D LiDARs and five cameras covering a 360-degree view, two inertial measurement units (IMUs), and real-time kinematic positioning (RTK), as well as annotations including 2D & 3D boxes, object classes, and object IDs. Thus far, various human trajectory datasets have been introduced to support the development of pedestrian motion forecasting models. Our SiT dataset differs from these datasets in the following three respects.  First, whereas the pedestrian trajectory data in other datasets were obtained from static scenes, our data was collected while the robot navigated in a crowded environment, capturing human-robot interactive scenarios in motion. Second, unlike many autonomous driving datasets where pedestrians are usually at a distance from vehicles and found on pedestrian paths, our dataset offers a distinctive view of navigation robots interacting closely with humans in crowded settings.Third, our dataset has been carefully organized to facilitate the training and evaluation of end-to-end prediction models encompassing 3D detection, 3D multi-object tracking, and trajectory prediction. This design allows for an end-to-end unified modular approach across different tasks. We introduce a comprehensive benchmark for assessing models across all aforementioned tasks and present the performance of multiple baseline models as part of our evaluation. Our dataset provides a strong foundation for future research in pedestrian trajectory prediction, which could expedite the development of safe and agile social navigation robots. The SiT dataset, development kit, and trained models are publicly available at: https://spalaboratory.github.io/SiT/",Datasets & Benchmarks,NeurIPS,2023,Poster,Jongwook Bae;Jungho Kim;Junyong Yun;Changwon Kang;Jeongseon Choi;Chanhyeok Kim;Junho Lee;Jungwook Choi;Jun Won Choi,True,https://openreview.net/pdf?id=gMYsxTin4x
gO0kS0eE0F,OpenProteinSet: Training data for structural biology at scale,"Multiple sequence alignments (MSAs) of proteins encode rich biological information and have been workhorses in bioinformatic methods for tasks like protein design and protein structure prediction for decades. Recent breakthroughs like AlphaFold2 that use transformers to attend directly over large quantities of raw MSAs have reaffirmed their importance. Generation of MSAs is highly computationally intensive, however, and no datasets comparable to those used to train AlphaFold2 have been made available to the research community, hindering progress in machine learning for proteins. To remedy this problem, we introduce OpenProteinSet, an open-source corpus of more than 16 million MSAs, associated structural homologs from the Protein Data Bank, and AlphaFold2 protein structure predictions. We have previously demonstrated the utility of OpenProteinSet by successfully retraining AlphaFold2 on it. We expect OpenProteinSet to be broadly useful as training and validation data for 1) diverse tasks focused on protein structure, function, and design and 2) large-scale multimodal machine learning research.",Datasets & Benchmarks,NeurIPS,2023,Poster,Gustaf Ahdritz;Nazim Bouatta;Sachin Kadyan;Lukas Jarosch;Dan Berenberg;Ian Fisk;Andrew Martin Watkins;Stephen Ra;Richard Bonneau;Mohammed AlQuraishi,True,https://openreview.net/pdf?id=gO0kS0eE0F
gsi9lJ3994,NVFi: Neural Velocity Fields for 3D Physics Learning from Dynamic Videos,"In this paper, we aim to model 3D scene dynamics from multi-view videos. Unlike the majority of existing works which usually focus on the common task of novel view synthesis within the training time period, we propose to simultaneously learn the geometry, appearance, and physical velocity of 3D scenes only from video frames, such that multiple desirable applications can be supported, including future frame extrapolation, unsupervised 3D semantic scene decomposition, and dynamic motion transfer. Our method consists of three major components, 1) the keyframe dynamic radiance field, 2) the interframe velocity field, and 3) a joint keyframe and interframe optimization module which is the core of our framework to effectively  train both networks. To validate our method, we further introduce two dynamic 3D datasets: 1) Dynamic Object dataset, and 2) Dynamic Indoor Scene dataset. We conduct extensive experiments on multiple datasets, demonstrating the superior performance of our method over all baselines, particularly in the critical tasks of future frame extrapolation and unsupervised 3D semantic scene decomposition.",main,NeurIPS,2023,Poster,Jinxi Li;Ziyang Song;Bo Yang,True,https://openreview.net/pdf?id=gsi9lJ3994
gx20B4ItIw,Emergent Communication for Rules Reasoning,"Research on emergent communication between deep-learning-based agents has received extensive attention due to its inspiration for linguistics and artificial intelligence. 
  However, previous attempts have hovered around emerging communication under perception-oriented environmental settings,
  that forces agents to describe low-level perceptual features intra image or symbol contexts.
  In this work, inspired by the classic human reasoning test (namely Raven's Progressive Matrix), we propose the Reasoning Game, a cognition-oriented environment that encourages agents to reason and communicate high-level rules, rather than perceived low-level contexts.
  Moreover, we propose 1) an unbiased dataset (namely rule-RAVEN) as a benchmark to avoid overfitting, 2) and a two-stage curriculum agent training method as a baseline for more stable convergence in the Reasoning Game,
  where contexts and semantics are bilaterally drifting.
  Experimental results show that, in the Reasoning Game, a semantically stable and compositional language emerges to solve reasoning problems.
  The emerged language helps agents apply the extracted rules to the generalization of unseen context attributes, and to the transfer between different context attributes or even tasks.",main,NeurIPS,2023,Poster,Yuxuan Guo;Yifan Hao;Rui Zhang;Enshuai Zhou;Zidong Du;Xishan Zhang;Xinkai Song;Yuanbo Wen;Yongwei Zhao;Xuehai Zhou;Jiaming Guo;Qi Yi;Shaohui Peng;Di Huang;Ruizhi Chen;Qi Guo;Yunji Chen,True,https://openreview.net/pdf?id=gx20B4ItIw
hHKBiMPfCj,Rectifying Open-Set Object Detection: Proper Evaluation and a Taxonomy,"Open-set object detection (OSOD), a task involving the detection of unknown objects while accurately detecting known objects, has recently gained attention. However, we identify a fundamental issue with the problem formulation employed in current OSOD studies. Inherent to object detection is knowing ''what to detect,'' which contradicts the idea of identifying ''unknown'' objects. 
This sets OSOD apart from open-set recognition (OSR). This contradiction complicates a proper evaluation of methods' performance, a fact that previous studies have overlooked. Next, we propose a novel formulation wherein detectors are required to detect both known and unknown classes within specified super-classes of object classes. This new formulation is free from the aforementioned issues and has practical applications. Finally, we design benchmark tests utilizing existing datasets and report the experimental evaluation of existing OSOD methods. As a byproduct, we introduce a taxonomy of OSOD, resolving confusion prevalent in the literature. We anticipate that our study will encourage the research community to reconsider OSOD and facilitate progress in the right direction.",Datasets & Benchmarks,NeurIPS,2023,Reject,Yusuke Hosoya;Masanori Suganuma;Takayuki Okatani,False,https://openreview.net/pdf?id=hHKBiMPfCj
hJPATsBb3l,"M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining Large Language Models","Despite the existence of various benchmarks for evaluating natural language processing models, we argue that human exams are a more suitable means of evaluating general intelligence for large language models (LLMs), as they inherently demand a much wider range of abilities such as language understanding, domain knowledge, and problem-solving skills. To this end, we introduce M3Exam, a novel benchmark sourced from real and official human exam questions for evaluating LLMs in a multilingual, multimodal, and multilevel context. M3Exam exhibits three unique characteristics: (1) multilingualism, encompassing questions from multiple countries that require strong multilingual proficiency and cultural knowledge; (2) multimodality, accounting for the multimodal nature of many exam questions to test the model's multimodal understanding capability; and (3) multilevel structure, featuring exams from three critical educational periods to comprehensively assess a model's proficiency at different levels. In total, M3Exam contains 12,317 questions in 9 diverse languages with three educational levels, where about 23\\\\% of the questions require processing images for successful solving. We assess the performance of top-performing LLMs on M3Exam and find that current models, including GPT-4, still struggle with multilingual text, particularly in low-resource and non-Latin script languages. Multimodal LLMs also perform poorly with complex multimodal questions. We believe that M3Exam can be a valuable resource for comprehensively evaluating LLMs by examining their multilingual and multimodal abilities and tracking their development. Data and evaluation code is available at \\\\url{https://github.com/DAMO-NLP-SG/M3Exam}.",Datasets & Benchmarks,NeurIPS,2023,Poster,Wenxuan Zhang;Mahani Aljunied;Chang Gao;Yew Ken Chia;Lidong Bing,True,https://openreview.net/pdf?id=hJPATsBb3l
hL8uGYjlHU,M$^{2}$SODAI: Multi-Modal Maritime Object Detection Dataset With RGB and Hyperspectral Image Sensors,"Object detection in aerial images is a growing area of research, with maritime object detection being a particularly important task for reliable surveillance, monitoring, and active rescuing. Notwithstanding astonishing advances of computer vision
technologies, detecting ships and floating matters in these images are challenging due to factors such as object distance. What makes it worse is pervasive sea surface effects such as sunlight reflection, wind, and waves. 
Hyperspectral image (HSI) sensors, providing more than 100 channels in wavelengths of visible and near-infrared, can extract intrinsic information of materials from a few pixels of HSIs.
The advent of HSI sensors motivates us to leverage HSIs to circumvent false positives due to the sea surface effects.
Unfortunately, there are few public HSI datasets due to the high cost and labor involved in collecting them, hindering object detection research based on HSIs. 
We have collected and annotated a new dataset called ``Multi-Modal Ship and flOating matter Detection in Aerial Images (M$^{2}$SODAI),'', which includes synchronized image pairs of RGB and HSI data, along with bounding box labels for nearly 6,000 instances per category. 
We also propose a new multi-modal extension of the feature pyramid network called DoubleFPN.
Extensive experiments on our benchmark demonstrate that fusion of RGB and HSI data can enhance mAP, especially in the presence of the sea surface effects.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jonggyu Jang;Sangwoo Oh;Youjin Kim;Dongmin Seo;Youngchol Choi;Hyun Jong Yang,True,https://openreview.net/pdf?id=hL8uGYjlHU
hVhoVxVD9D,SSL4EO-L: Datasets and Foundation Models for Landsat Imagery,"The Landsat program is the longest-running Earth observation program in history, with 50+ years of data acquisition by 8 satellites. The multispectral imagery captured by sensors onboard these satellites is critical for a wide range of scientific fields. Despite the increasing popularity of deep learning and remote sensing, the majority of researchers still use decision trees and random forests for Landsat image analysis due to the prevalence of small labeled datasets and lack of foundation models. In this paper, we introduce SSL4EO-L, the first ever dataset designed for Self-Supervised Learning for Earth Observation for the Landsat family of satellites (including 3 sensors and 2 product levels) and the largest Landsat dataset in history (5M image patches). Additionally, we modernize and re-release the L7 Irish and L8 Biome cloud detection datasets, and introduce the first ML benchmark datasets for Landsats 4–5 TM and Landsat 7 ETM+ SR. Finally, we pre-train the first foundation models for Landsat imagery using SSL4EO-L and evaluate their performance on multiple semantic segmentation tasks. All datasets and model weights are available via the TorchGeo library, making reproducibility and experimentation easy, and enabling scientific advancements in the burgeoning field of remote sensing for a multitude of downstream applications.",Datasets & Benchmarks,NeurIPS,2023,Poster,Adam J Stewart;Nils Lehmann;Isaac Corley;Yi Wang;Yi-Chia Chang;Nassim Ait Ait Ali Braham;Shradha Sehgal;Caleb Robinson;Arindam Banerjee,True,https://openreview.net/pdf?id=hVhoVxVD9D
hiO0735tmc,"Event Stream GPT: A Data Pre-processing and Modeling Library for Generative, Pre-trained Transformers over Continuous-time Sequences of Complex Events","Generative, pre-trained transformers (GPTs, a type of ""Foundation Models"") have reshaped natural language processing (NLP) through their versatility in diverse downstream tasks. However, their potential extends far beyond NLP. This paper provides a software utility to help realize this potential, extending the applicability of GPTs to continuous-time sequences of complex events with internal dependencies, such as medical record datasets. Despite their potential, the adoption of foundation models in these domains has been hampered by the lack of suitable tools for model construction and evaluation. To bridge this gap, we introduce Event Stream GPT (ESGPT), an open-source library designed to streamline the end-to-end process for building GPTs for continuous-time event sequences. ESGPT allows users to (1) build flexible, foundation-model scale input datasets by specifying only a minimal configuration file, (2) leverage a Hugging Face compatible modeling API for GPTs over this modality that incorporates intra-event causal dependency structures and autoregressive generation capabilities, and (3) evaluate models via standardized processes that can assess few and even zero-shot performance of pre-trained models on user-specified fine-tuning tasks.",Datasets & Benchmarks,NeurIPS,2023,Poster,,False,https://openreview.net/pdf?id=hiO0735tmc
hizSx8pf0U,DeepfakeBench: A Comprehensive Benchmark of Deepfake Detection,"A critical yet frequently overlooked challenge in the field of deepfake detection is the lack of a standardized, unified, comprehensive benchmark. This issue leads to unfair performance comparisons and potentially misleading results. Specifically, there is a lack of uniformity in data processing pipelines, resulting in inconsistent data inputs for detection models. Additionally, there are noticeable differences in experimental settings, and evaluation strategies and metrics lack standardization. To fill this gap, we present the first comprehensive benchmark for deepfake detection, called \\\\textit{DeepfakeBench}, which offers three key contributions: 1) a unified data management system to ensure consistent input across all detectors, 2) an integrated framework for state-of-the-art methods implementation, and 3) standardized evaluation metrics and protocols to promote transparency and reproducibility.  Featuring an extensible, modular-based codebase, \\\\textit{DeepfakeBench} contains 15 state-of-the-art detection methods, 9 deepfake datasets, a series of deepfake detection evaluation protocols and analysis tools, as well as comprehensive evaluations.  Moreover, we provide new insights based on extensive analysis of these evaluations from various perspectives (\\\\eg, data augmentations, backbones). We hope that our efforts could facilitate future research and foster innovation in this increasingly critical domain. All codes, evaluations, and analyses of our benchmark are publicly available at \\\\url{https://github.com/SCLBD/DeepfakeBench}.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zhiyuan Yan;Yong Zhang;Xinhang Yuan;Siwei Lyu;Baoyuan Wu,False,https://openreview.net/pdf?id=hizSx8pf0U
hjFQoqPIgL,Tartarus: A Benchmarking Platform for Realistic And Practical Inverse Molecular Design,"The efficient exploration of chemical space to design molecules with intended properties enables the accelerated discovery of drugs, materials, and catalysts, and is one of the most important outstanding challenges in chemistry. Encouraged by the recent surge in computer power and artificial intelligence development, many algorithms have been developed to tackle this problem. However, despite the emergence of many new approaches in recent years, comparatively little progress has been made in developing realistic benchmarks that reflect the complexity of molecular design for real-world applications. In this work, we develop a set of practical benchmark tasks relying on physical simulation of molecular systems mimicking real-life molecular design problems for materials, drugs, and chemical reactions. Additionally, we demonstrate the utility and ease of use of our new benchmark set by demonstrating how to compare the performance of several well-established families of algorithms. Overall, we believe that our benchmark suite will help move the field towards more realistic molecular design benchmarks, and move the development of inverse molecular design algorithms closer to the practice of designing molecules that solve existing problems in both academia and industry alike.",Datasets & Benchmarks,NeurIPS,2023,Poster,AkshatKumar Nigam;Robert Pollice;Gary Tom;Kjell Jorner;John Willes;Luca Thiede;Anshul Kundaje;Alan Aspuru-Guzik,True,https://openreview.net/pdf?id=hjFQoqPIgL
hr9Bd1A9Un,"Benchmark of Machine Learning Force Fields for Semiconductor Simulations: Datasets, Metrics, and Comparative Analysis","As semiconductor devices become miniaturized and their structures become more complex, there is a growing need for large-scale atomic-level simulations as a less costly alternative to the trial-and-error approach during development.
Although machine learning force fields (MLFFs) can meet the accuracy and scale requirements for such simulations, there are no open-access benchmarks for semiconductor materials.
Hence, this study presents a comprehensive benchmark suite that consists of two semiconductor material datasets and ten MLFF models with six evaluation metrics. 
We select two important semiconductor thin-film materials silicon nitride and hafnium oxide, and generate their datasets using computationally expensive density functional theory simulations under various scenarios at a cost of 2.6k GPU days.
Additionally, we provide a variety of architectures as baselines: descriptor-based fully connected neural networks and graph neural networks with rotational invariant or equivariant features.
We assess not only the accuracy of energy and force predictions but also five additional simulation indicators to determine the practical applicability of MLFF models in molecular dynamics simulations.
To facilitate further research, our benchmark suite is available at https://github.com/SAITPublic/MLFF-Framework.",Datasets & Benchmarks,NeurIPS,2023,Poster,Geonu Kim;Byunggook Na;Gunhee Kim;Hyuntae Cho;Seungjin Kang;Hee Sun Lee;Saerom Choi;Heejae Kim;Seungwon Lee;Yongdeok Kim,True,https://openreview.net/pdf?id=hr9Bd1A9Un
hrWsIC4Cmz,BioMassters: A Benchmark Dataset for Forest Biomass Estimation using Multi-modal Satellite Time-series,"Above Ground Biomass is an important variable as forests play a crucial role in mitigating climate change as they act as an efficient, natural and cost-effective carbon sink. Traditional field and airborne LiDAR measurements have been proven to provide reliable estimations of forest biomass. Nevertheless, the use of these techniques at a large scale can be challenging and expensive. Satellite data have been widely used as a valuable tool in estimating biomass on a global scale. However, the full potential of dense multi-modal satellite time series data, in combination with modern deep learning approaches, has yet to be fully explored. The aim of the ""BioMassters"" data challenge and benchmark dataset is to investigate the potential of multi-modal satellite data (Sentinel-1 SAR and Sentinel-2 MSI) to estimate forest biomass at a large scale using the Finnish Forest Centre's open forest and nature airborne LiDAR data as a reference. 
The performance of the top three baseline models shows the potential of deep learning to produce accurate and higher-resolution biomass maps. Our benchmark dataset is publically available at https://huggingface.co/datasets/nascetti-a/BioMassters (doi:10.57967/hf/1009) and the implementation of the top three winning models are available at https://github.com/drivendataorg/the-biomassters.",Datasets & Benchmarks,NeurIPS,2023,Poster,Andrea Nascetti;RITU YADAV;Kirill Brodt;Qixun Qu;Hongwei Fan;Yuri Shendryk;Isha Shah;Christine Chung,True,https://openreview.net/pdf?id=hrWsIC4Cmz
igEYxgQP7t,RVD: A Handheld Device-Based Fundus Video Dataset for Retinal Vessel Segmentation,"Retinal vessel segmentation is generally grounded in image-based datasets collected with bench-top devices. 
The static images naturally lose the dynamic characteristics of retina fluctuation, resulting in diminished dataset richness, and the usage of bench-top devices further restricts dataset scalability due to its limited accessibility. Considering these limitations, we introduce the first video-based retinal dataset by employing handheld devices for data acquisition. The dataset comprises 635 smartphone-based fundus videos collected from four different clinics, involving 415 patients from 50 to 75 years old. It delivers comprehensive and precise annotations of retinal structures in both spatial and temporal dimensions, aiming to advance the landscape of vasculature segmentation. Specifically, the dataset provides three levels of spatial annotations: binary vessel masks for overall retinal structure delineation, general vein-artery masks for distinguishing the vein and artery, and fine-grained vein-artery masks for further characterizing the granularities of each artery and vein. In addition, the dataset offers temporal annotations that capture the vessel pulsation characteristics, assisting in detecting ocular diseases that require fine-grained recognition of hemodynamic fluctuation. In application, our dataset exhibits a significant domain shift with respect to data captured by bench-top devices, thus posing great challenges to existing methods. Thanks to rich annotations and data scales, our dataset potentially paves the path for more advanced retinal analysis and accurate disease diagnosis. In the experiments, we provide evaluation metrics and benchmark results on our dataset, reflecting both the potential and challenges it offers for vessel segmentation tasks. We hope this challenging dataset would significantly contribute to the development of eye disease diagnosis and early prevention.",Datasets & Benchmarks,NeurIPS,2023,Poster,MD WAHIDUZZAMAN KHAN;Hongwei Sheng;Hu Zhang;Heming Du;Sen Wang;Minas Theodore Coroneo;Farshid Hajati;Sahar Shariflou;Michael Kalloniatis;Jack Phu;Ashish Agar;Zi Huang;Mojtaba Golzan;Xin Yu,True,https://openreview.net/pdf?id=igEYxgQP7t
ihlT8yvQ2I,GNNEvaluator: Evaluating GNN Performance On Unseen Graphs Without Labels,"Evaluating the performance of graph neural networks (GNNs) is an essential task for practical GNN model deployment and serving, as deployed GNNs face significant performance uncertainty when inferring on unseen and unlabeled test graphs, due to mismatched training-test graph distributions. In this paper, we study a *new* problem, **GNN model evaluation**, that aims to assess the performance of a specific GNN model trained on labeled and observed graphs, by precisely estimating its performance (e.g., node classification accuracy) on unseen graphs without labels. Concretely, we propose a two-stage GNN model evaluation framework, including (1) DiscGraph set construction and (2) GNNEvaluator training and inference. The DiscGraph set captures wide-range and diverse graph data distribution discrepancies through a discrepancy measurement function, which exploits the GNN outputs of latent node embeddings and node class predictions. Under the effective training supervision from the DiscGraph set, GNNEvaluator learns to precisely estimate node classification accuracy of the to-be-evaluated GNN model and makes an accurate inference for evaluating GNN model performance. Extensive experiments on real-world unseen and unlabeled test graphs demonstrate the effectiveness of our proposed method for GNN model evaluation.",main,NeurIPS,2023,Poster,Xin Zheng;Miao Zhang;Chunyang Chen;Soheila Molaei;Chuan Zhou;Shirui Pan,True,https://openreview.net/pdf?id=ihlT8yvQ2I
j2wasUypqN,MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning,"Recently, Meta-Black-Box Optimization with Reinforcement Learning (MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to mitigate manual fine-tuning of low-level black-box optimizers. However, this field is hindered by the lack of a unified benchmark. To fill this gap, we introduce MetaBox, the first benchmark platform expressly tailored for developing and evaluating MetaBBO-RL methods. MetaBox offers a flexible algorithmic template that allows users to effortlessly implement their unique designs within the platform. Moreover, it provides a broad spectrum of over 300 problem instances, collected from synthetic to realistic scenarios, and an extensive library of 19 baseline methods, including both traditional black-box optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three standardized performance metrics, enabling a more thorough assessment of the methods. In a bid to illustrate the utility of MetaBox for facilitating rigorous evaluation and in-depth analysis, we carry out a wide-ranging benchmarking study on existing MetaBBO-RL methods. Our MetaBox is open-source and accessible at: https://github.com/GMC-DRL/MetaBox.",Datasets & Benchmarks,NeurIPS,2023,Oral,Zeyuan Ma;Hongshu Guo;Jiacheng Chen;Zhenrui Li;Guojun Peng;Yue-Jiao Gong;Yining Ma;Zhiguang Cao,True,https://openreview.net/pdf?id=j2wasUypqN
j4b3l5kOil,AndroidInTheWild: A Large-Scale Dataset For Android Device Control,"There is a growing interest in device-control systems that can interpret human natural language instructions and execute them on a digital device by directly controlling its user interface. We present a dataset for device-control research, Android in the Wild (AitW), which is orders of magnitude larger than current datasets. The dataset contains human demonstrations of device interactions, including the screens and actions, and corresponding natural language instructions. It consists of 715k episodes spanning 30k unique instructions, four versions of Android (v10–13), and eight device types (Pixel 2 XL to Pixel 6) with varying screen resolutions. It contains multi-step tasks that require semantic understanding of language and visual context. This dataset poses a new challenge: actions available through the user interface must be inferred from their visual appearance, and, instead of simple UI element-based actions, the action space consists of precise gestures (e.g., horizontal scrolls to operate carousel widgets). We organize our dataset to encourage robustness analysis of device-control systems, i.e., how well a system performs in the presence of new task descriptions, new applications, or new platform versions. We develop two agents and report performance across the dataset. The dataset is available at https://github.com/google-research/google-research/tree/master/android_in_the_wild.",Datasets & Benchmarks,NeurIPS,2023,Poster,Christopher Rawles;Alice Li;Daniel Rodriguez;Oriana Riva;Timothy P Lillicrap,True,https://openreview.net/pdf?id=j4b3l5kOil
j5AoleAIru,What You See is What You Read? Improving Text-Image Alignment Evaluation,"Automatically determining whether a text and a corresponding image are semantically aligned is a significant challenge for vision-language models, with applications in generative text-to-image and image-to-text tasks. In this work, we study methods for automatic text-image alignment evaluation. We first introduce SeeTRUE: a comprehensive evaluation set, spanning multiple datasets from both text-to-image and image-to-text generation tasks, with human judgements for whether a given text-image pair is semantically aligned. We then describe two automatic methods to determine alignment: the first involving a pipeline based on question generation and visual question answering models, and the second employing an end-to-end classification approach by finetuning multimodal pretrained models. Both methods surpass prior approaches in various text-image alignment tasks, with significant improvements in challenging cases that involve complex composition or unnatural images. Finally, we demonstrate how our approaches can localize specific misalignments between an image and a given text, and how they can be used to automatically re-rank candidates in text-to-image generation.",main,NeurIPS,2023,Poster,Michal Yarom;Yonatan Bitton;Soravit Changpinyo;Roee Aharoni;Jonathan Herzig;Oran Lang;Eran Ofek;Idan Szpektor,True,https://openreview.net/pdf?id=j5AoleAIru
j7x9wW3tCf,Learning from Both Structural and Textual Knowledge for Inductive Knowledge Graph Completion,"Learning rule-based systems plays a pivotal role in knowledge graph completion (KGC). Existing rule-based systems restrict the input of the system to structural knowledge only, which may omit some useful knowledge for reasoning, e.g., textual knowledge. In this paper, we propose a two-stage framework that imposes both structural and textual knowledge to learn rule-based systems. In the first stage, we compute a set of triples with confidence scores (called \\\\emph{soft triples}) from a text corpus by distant supervision, where a textual entailment model with multi-instance learning is exploited to estimate whether a given triple is entailed by a set of sentences. In the second stage, these soft triples are used to learn a rule-based model for KGC. To mitigate the negative impact of noise from soft triples, we propose a new formalism for rules to be learnt, named \\\\emph{text enhanced rules} or \\\\emph{TE-rules} for short. To effectively learn TE-rules, we propose a neural model that simulates the inference of TE-rules. We theoretically show that any set of TE-rules can always be interpreted by a certain parameter assignment of the neural model. We introduce three new datasets to evaluate the effectiveness of our method. Experimental results demonstrate that the introduction of soft triples and TE-rules results in significant performance improvements in inductive link prediction.",main,NeurIPS,2023,Poster,Kunxun Qi;Jianfeng Du;Hai Wan,True,https://openreview.net/pdf?id=j7x9wW3tCf
jGyMUum1Lq,Revisiting Evaluation Metrics for Semantic Segmentation: Optimization and Evaluation of Fine-grained Intersection over Union,"Semantic segmentation datasets often exhibit two types of imbalance: \\\\textit{class imbalance}, where some classes appear more frequently than others and \\\\textit{size imbalance}, where some objects occupy more pixels than others. This causes traditional evaluation metrics to be biased towards \\\\textit{majority classes} (e.g. overall pixel-wise accuracy) and \\\\textit{large objects} (e.g. mean pixel-wise accuracy and per-dataset mean intersection over union). To address these shortcomings, we propose the use of fine-grained mIoUs along with corresponding worst-case metrics, thereby offering a more holistic evaluation of segmentation techniques. These fine-grained metrics offer less bias towards large objects, richer statistical information, and valuable insights into model and dataset auditing. Furthermore, we undertake an extensive benchmark study, where we train and evaluate 15 modern neural networks with the proposed metrics on 12 diverse natural and aerial segmentation datasets. Our benchmark study highlights the necessity of not basing evaluations on a single metric and confirms that fine-grained mIoUs reduce the bias towards large objects. Moreover, we identify the crucial role played by architecture designs and loss functions, which lead to best practices in optimizing fine-grained metrics. The code is available at \\\\href{https://github.com/zifuwanggg/JDTLosses}{https://github.com/zifuwanggg/JDTLosses}.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zifu Wang;Maxim Berman;Amal Rannen-Triki;Philip Torr;Devis Tuia;Tinne Tuytelaars;Luc Van Gool;Jiaqian Yu;Matthew B. Blaschko,False,https://openreview.net/pdf?id=jGyMUum1Lq
jHrgq55ftl,SAMRS: Scaling-up Remote Sensing Segmentation Dataset with Segment Anything Model,"The success of the Segment Anything Model (SAM) demonstrates the significance of data-centric machine learning. However, due to the difficulties and high costs associated with annotating Remote Sensing (RS) images, a large amount of valuable RS data remains unlabeled, particularly at the pixel level. In this study, we leverage SAM and existing RS object detection datasets to develop an efficient pipeline for generating a large-scale RS segmentation dataset, dubbed SAMRS. SAMRS totally possesses 105,090 images and 1,668,241 instances, surpassing existing high-resolution RS segmentation datasets in size by several orders of magnitude. It provides object category, location, and instance information that can be used for semantic segmentation, instance segmentation, and object detection, either individually or in combination. We also provide a comprehensive analysis of SAMRS from various aspects.  Moreover, preliminary experiments highlight the importance of conducting segmentation pre-training with SAMRS to address task discrepancies and alleviate the limitations posed by limited training data during fine-tuning. The code and dataset will be available at https://github.com/ViTAE-Transformer/SAMRS",Datasets & Benchmarks,NeurIPS,2023,Poster,Di Wang;Jing Zhang;Bo Du;Minqiang Xu;Lin Liu;Dacheng Tao;Liangpei Zhang,True,https://openreview.net/pdf?id=jHrgq55ftl
jKFKwW8JGG,Cola: A Benchmark for Compositional Text-to-image Retrieval,"Compositional reasoning is a hallmark of human visual intelligence. Yet, despite the size of large vision-language models, they struggle to represent simple compositions by combining objects with their attributes. To measure this lack of compositional capability, we design Cola, a text-to-image retrieval benchmark to Compose Objects Localized with Attributes. To solve Cola, a model must retrieve images with the correct configuration of attributes and objects and avoid choosing a distractor image with the same objects and attributes but in the wrong configuration. Cola contains about 1.2k composed queries of 168 objects and 197 attributes on around 30K images. Our human evaluation finds that Cola is 83.33% accurate, similar to contemporary compositionality benchmarks. Using Cola as a testbed, we explore empirical modeling designs to adapt pre-trained vision-language models to reason compositionally. We explore 6 adaptation strategies on 2 seminal vision-language models, using compositionality-centric test benchmarks - Cola and CREPE. We find the optimal adaptation strategy is to train a multi-modal attention layer that jointly attends over the frozen pre-trained image and language features. Surprisingly, training multimodal layers on CLIP performs better than tuning a larger FLAVA model with already pre-trained multimodal layers. Furthermore, our adaptation strategy improves CLIP and FLAVA to comparable levels, suggesting that training multimodal layers using contrastive attribute-object data is key, as opposed to using them pre-trained. Lastly, we show that Cola is harder than a closely related contemporary benchmark, CREPE, since simpler fine-tuning strategies without multimodal layers suffice on CREPE, but not on Cola. However, we still see a significant gap between our best adaptation and human accuracy, suggesting considerable room for further research. 
Project page: https://cs-people.bu.edu/array/research/cola/",Datasets & Benchmarks,NeurIPS,2023,Poster,Arijit Ray;Filip Radenovic;Abhimanyu Dubey;Bryan A. Plummer;Ranjay Krishna;Kate Saenko,True,https://openreview.net/pdf?id=jKFKwW8JGG
jP3BduIxy6,Hokoff: Real Game Dataset from Honor of Kings and its Offline Reinforcement Learning Benchmarks,"The advancement of Offline Reinforcement Learning (RL) and Offline Multi-Agent Reinforcement Learning (MARL) critically depends on the availability of high-quality, pre-collected offline datasets that represent real-world complexities and practical applications. However, existing datasets often fall short in their simplicity and lack of realism. To address this gap, we propose Hokoff, a comprehensive set of pre-collected datasets that covers both offline RL and offline MARL, accompanied by a robust framework, to facilitate further research. This data is derived from Honor of Kings, a recognized Multiplayer Online Battle Arena (MOBA) game known for its intricate nature, closely resembling real-life situations. Utilizing this framework, we benchmark a variety of offline RL and offline MARL algorithms. We also introduce a novel baseline algorithm tailored for the inherent hierarchical action space of the game. We reveal the incompetency of current offline RL approaches in handling task complexity, generalization and multi-task learning.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yun Qu;Boyuan Wang;Jianzhun Shao;Yuhang Jiang;Chen Chen;Zhenbin Ye;Lin Liu;Yang Jun Feng;Lin Lai;Hongyang Qin;Minwen Deng;Juchao Zhuo;Deheng Ye;QIANG FU;YANG GUANG;Yang Wei;Lanxiao Huang;Xiangyang Ji,True,https://openreview.net/pdf?id=jP3BduIxy6
jSO7Vgolc6,FELM: Benchmarking Factuality Evaluation of Large Language Models,"Assessing factuality of text generated by large language models (LLMs) is an emerging yet crucial research area, aimed at alerting users to potential errors and guiding the development of more reliable LLMs. Nonetheless, the evaluators assessing factuality necessitate suitable evaluation themselves to gauge progress and foster advancements. This direction remains under-explored, resulting in substantial impediments to the progress of factuality evaluators. To mitigate this issue, we introduce a benchmark for Factuality Evaluation of large Language Models, referred to as FELM. In this benchmark, we collect responses generated from LLMs and annotate factuality labels in a fine-grained manner. Contrary to previous studies that primarily concentrate on the factuality of world knowledge (e.g. information from Wikipedia), FELM focuses on factuality across diverse domains, spanning from world knowledge to math and reasoning. Our annotation is based on text segments, which can help pinpoint specific factual errors. The factuality annotations are further supplemented by predefined error types and reference links that either support or contradict the statement. In our experiments, we investigate the performance of several LLM-based factuality evaluators on FELM, including both vanilla LLMs and those augmented with retrieval mechanisms and chain-of-thought processes. Our findings reveal that while retrieval aids factuality evaluation, current LLMs are far from satisfactory to faithfully detect factual errors.",Datasets & Benchmarks,NeurIPS,2023,Poster,Shiqi Chen;Yiran Zhao;Jinghan Zhang;I-Chun Chern;Siyang Gao;Pengfei Liu;Junxian He,True,https://openreview.net/pdf?id=jSO7Vgolc6
jSuhnO9QJv,Spuriosity Rankings: Sorting Data to Measure and Mitigate Biases,"We present a simple but effective method to measure and mitigate model biases caused by reliance on spurious cues. Instead of requiring costly changes to one's data or model training, our method better utilizes the data one already has by sorting them. Specifically, we rank images within their classes based on spuriosity (the degree to which common spurious cues are present), proxied via deep neural features of an interpretable network. With spuriosity rankings, it is easy to identify minority subpopulations (i.e. low spuriosity images) and assess model bias as the gap in accuracy between high and low spuriosity images. One can even efficiently remove a model's bias at little cost to accuracy by finetuning its classification head on low spuriosity images, resulting in fairer treatment of samples regardless of spuriosity. We demonstrate our method on ImageNet, annotating $5000$ class-feature dependencies ($630$ of which we find to be spurious) and generating a dataset of $325k$ soft segmentations for these features along the way. Having computed spuriosity rankings via the identified spurious neural features, we assess biases for $89$ diverse models and find that class-wise biases are highly correlated across models. Our results suggest that model bias due to spurious feature reliance is influenced far more by what the model is trained on than how it is trained.",main,NeurIPS,2023,Spotlight,Mazda Moayeri;Wenxiao Wang;Sahil Singla;Soheil Feizi,True,https://openreview.net/pdf?id=jSuhnO9QJv
jUpVFjRdUV,Scalable 3D Captioning with Pretrained Models,"We introduce Cap3D, an automatic approach for generating descriptive text for 3D objects. This approach utilizes pretrained models from image captioning, image-text alignment, and LLM to consolidate captions from multiple views of a 3D asset, completely side-stepping the time-consuming and costly process of manual annotation. We apply Cap3D to the recently introduced large-scale 3D dataset, Objaverse, resulting in 660k 3D-text pairs. Our evaluation, conducted using 41k human annotations from the same dataset, demonstrates that Cap3D surpasses human-authored descriptions in terms of quality, cost, and speed. Through effective prompt engineering, Cap3D rivals human performance in generating geometric descriptions on 17k collected annotations from the ABO dataset. Finally, we finetune Text-to-3D models on Cap3D and human captions, and show Cap3D outperforms; and benchmark the SOTA including Point·E, Shape·E, and DreamFusion.",Datasets & Benchmarks,NeurIPS,2023,Poster,Tiange Luo;Chris Rockwell;Honglak Lee;Justin Johnson,False,https://openreview.net/pdf?id=jUpVFjRdUV
jfwRLudQyj,A Toolkit for Reliable Benchmarking and Research in Multi-Objective Reinforcement Learning,"Multi-objective reinforcement learning algorithms (MORL) extend standard reinforcement learning (RL) to scenarios where agents must optimize multiple---potentially conflicting---objectives, each represented by a distinct reward function. To facilitate and accelerate research and benchmarking in multi-objective RL problems, we introduce a comprehensive collection of software libraries that includes: 
(i) MO-Gymnasium, an easy-to-use and flexible API enabling the rapid construction of novel MORL environments. It also includes more than 20 environments under this API. This allows researchers to effortlessly evaluate any algorithms on any existing domains; (ii) MORL-Baselines, a collection of reliable and efficient implementations of state-of-the-art MORL algorithms, designed to provide a solid foundation for advancing research. Notably, all algorithms are inherently compatible with MO-Gymnasium; and
(iii) a thorough and robust set of benchmark results and comparisons of MORL-Baselines algorithms, tested across various challenging MO-Gymnasium environments. These benchmarks were constructed to serve as guidelines for the research community, underscoring the properties, advantages, and limitations of each particular state-of-the-art method.",Datasets & Benchmarks,NeurIPS,2023,Poster,Florian Felten;Lucas Nunes Alegre;Ann Nowe;Ana L. C. Bazzan;El Ghazali Talbi;Grégoire Danoy;Bruno Castro da Silva,False,https://openreview.net/pdf?id=jfwRLudQyj
k4juAEW1tG,BenchCLAMP: A Benchmark for Evaluating Language Models on Syntactic and Semantic Parsing,"Recent work has shown that generation from a prompted or fine-tuned language model can perform well at semantic parsing when the output is constrained to be a valid semantic representation. We introduce BenchCLAMP, a Benchmark to evaluate Constrained LAnguage Model Parsing, that includes context-free grammars for seven semantic parsing datasets and two syntactic parsing datasets with varied output meaning representations, as well as a constrained decoding interface to generate only valid outputs covered by these grammars. We provide low, medium, and high resource splits for each dataset, allowing accurate comparison of various language models under different data regimes. Our benchmark supports evaluation of language models using prompt-based learning as well as fine-tuning. We benchmark seven language models, including two GPT-3 variants available only through an API. Our experiments show that encoder-decoder pretrained language models can achieve similar performance or even surpass state-of-the-art methods for both syntactic and semantic parsing when the model output is constrained to be valid.",Datasets & Benchmarks,NeurIPS,2023,Poster,Subhro Roy;Sam Thomson;Tongfei Chen;Richard Shin;Adam Pauls;Jason Eisner;Benjamin Van Durme,True,https://openreview.net/pdf?id=k4juAEW1tG
kM5eGcdCzq,The RefinedWeb Dataset for Falcon LLM: Outperforming Curated Corpora with Web Data Only,"Large language models are commonly trained on a mixture of filtered web data and curated ``high-quality'' corpora, such as social media conversations, books, or technical papers. This curation process is believed to be necessary to produce performant models with broad zero-shot generalization abilities. However, as larger models requiring pretraining on trillions of tokens are considered, it is unclear how scalable is curation, and whether we will run out of unique high-quality data soon.  At variance with previous beliefs, we show that properly filtered and deduplicated web data alone can lead to powerful models; even significantly outperforming models trained on The Pile. Despite extensive filtering, the high-quality data we extract from the web is still plentiful, and we are able to obtain five trillion tokens from CommonCrawl. We publicly release an extract of 500 billion tokens from our RefinedWeb dataset, and 1.3/7.5B parameters language models trained on it.",Datasets & Benchmarks,NeurIPS,2023,Poster,Guilherme Penedo;Quentin Malartic;Daniel Hesslow;Ruxandra Cojocaru;Hamza Alobeidli;Alessandro Cappelli;Baptiste Pannier;Ebtesam Almazrouei;Julien Launay,True,https://openreview.net/pdf?id=kM5eGcdCzq
kXOXrVnwbb,DaTaSeg: Taming a Universal Multi-Dataset Multi-Task Segmentation Model,"Observing the close relationship among panoptic, semantic and instance segmentation tasks, we propose to train a universal multi-dataset multi-task segmentation model: DaTaSeg. We use a shared representation (mask proposals with class predictions) for all tasks. To tackle task discrepancy, we adopt different merge operations and post-processing for different tasks. We also leverage weak-supervision, allowing our segmentation model to benefit from cheaper bounding box annotations. To share knowledge across datasets, we use text embeddings from the same semantic embedding space as classifiers and share all network parameters among datasets. We train DaTaSeg on ADE semantic, COCO panoptic, and Objects365 detection datasets. DaTaSeg improves performance on all datasets, especially small-scale datasets, achieving 54.0 mIoU on ADE semantic and 53.5 PQ on COCO panoptic. DaTaSeg also enables weakly-supervised knowledge transfer on ADE panoptic and Objects365 instance segmentation. Experiments show DaTaSeg scales with the number of training datasets and enables open-vocabulary segmentation through direct transfer. In addition, we annotate an Objects365 instance segmentation set of 1,000 images and release it as a public evaluation benchmark on https://laoreja.github.io/dataseg.",main,NeurIPS,2023,Poster,Xiuye Gu;Yin Cui;Jonathan Huang;Abdullah Rashwan;Xuan Yang;Xingyi Zhou;Golnaz Ghiasi;Weicheng Kuo;Huizhong Chen;Liang-Chieh Chen;David A Ross,True,https://openreview.net/pdf?id=kXOXrVnwbb
kaHpo8OZw2,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,"Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications to healthcare and finance – where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives – including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially due to the reason that GPT-4 follows the (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/.",Datasets & Benchmarks,NeurIPS,2023,Oral,Boxin Wang;Weixin Chen;Hengzhi Pei;Chulin Xie;Mintong Kang;Chenhui Zhang;Chejian Xu;Zidi Xiong;Ritik Dutta;Rylan Schaeffer;Sang T. Truong;Simran Arora;Mantas Mazeika;Dan Hendrycks;Zinan Lin;Yu Cheng;Sanmi Koyejo;Dawn Song;Bo Li,True,https://openreview.net/pdf?id=kaHpo8OZw2
ke3RgcDmfO,TextDiffuser: Diffusion Models as Text Painters,"Diffusion models have gained increasing attention for their impressive generation abilities but currently struggle with rendering accurate and coherent text. To address this issue, we introduce TextDiffuser, focusing on generating images with visually appealing text that is coherent with backgrounds. TextDiffuser consists of two stages: first, a Transformer model generates the layout of keywords extracted from text prompts, and then diffusion models generate images conditioned on the text prompt and the generated layout. Additionally, we contribute the first large-scale text images dataset with OCR annotations, MARIO-10M, containing 10 million image-text pairs with text recognition, detection, and character-level segmentation annotations. We further collect the MARIO-Eval benchmark to serve as a comprehensive tool for evaluating text rendering quality. Through experiments and user studies, we demonstrate that TextDiffuser is flexible and controllable to create high-quality text images using text prompts alone or together with text template images, and conduct text inpainting to reconstruct incomplete images with text. We will make the code, model and dataset publicly available.",main,NeurIPS,2023,Poster,Jingye Chen;Yupan Huang;Tengchao Lv;Lei Cui;Qifeng Chen;Furu Wei,True,https://openreview.net/pdf?id=ke3RgcDmfO
kiYqbO3wqw,Mind2Web: Towards a Generalist Agent for the Web,"We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Xiang Deng;Yu Gu;Boyuan Zheng;Shijie Chen;Samuel Stevens;Boshi Wang;Huan Sun;Yu Su,True,https://openreview.net/pdf?id=kiYqbO3wqw
kxFKgqwFNk,PAD: A Dataset and Benchmark for Pose-agnostic Anomaly Detection,"Object anomaly detection is an important problem in the field of machine vision and has seen remarkable progress recently. However, two significant challenges hinder its research and application. First, existing datasets lack comprehensive visual information from various pose angles. They usually have an unrealistic assumption that the anomaly-free training dataset is pose-aligned, and the testing samples have the same pose as the training data. However, in practice, anomaly may exist in any regions on a object, the training and query samples may have different poses, calling for the study on pose-agnostic anomaly detection. Second, the absence of a consensus on experimental protocols for pose-agnostic anomaly detection leads to unfair comparisons of different methods, hindering the research on pose-agnostic anomaly detection. To address these issues, we develop Multi-pose Anomaly Detection (MAD) dataset and Pose-agnostic Anomaly Detection (PAD) benchmark, which takes the first step to address the pose-agnostic anomaly detection problem. Specifically, we build MAD using 20 complex-shaped LEGO toys including 4K views with various poses, and high-quality and diverse 3D anomalies in both simulated and real environments. Additionally, we propose a novel method OmniposeAD, trained using MAD, specifically designed for pose-agnostic anomaly detection. Through comprehensive evaluations, we demonstrate the relevance of our dataset and method. Furthermore, we provide an open-source benchmark library, including dataset and baseline methods that cover 8 anomaly detection paradigms, to facilitate future research and application in this domain. Code, data, and models are publicly available at https://github.com/EricLee0224/PAD.",Datasets & Benchmarks,NeurIPS,2023,Poster,Qiang Zhou;Weize Li;Lihan Jiang;Guoliang Wang;Guyue Zhou;Shanghang Zhang;Hao Zhao,True,https://openreview.net/pdf?id=kxFKgqwFNk
l4CZCKXoSn,FOCAL: Contrastive Learning for Multimodal Time-Series Sensing Signals in Factorized Orthogonal Latent Space,"This paper proposes a novel contrastive learning framework, called FOCAL, for extracting comprehensive features from multimodal time-series sensing signals through self-supervised training. Existing multimodal contrastive frameworks mostly rely on the shared information between sensory modalities, but do not explicitly consider the exclusive modality information that could be critical to understanding the underlying sensing physics. Besides, contrastive frameworks for time series have not handled the temporal information locality appropriately. FOCAL solves these challenges by making the following contributions: First, given multimodal time series, it encodes each modality into a factorized latent space consisting of shared features and private features that are orthogonal to each other. The shared space emphasizes feature patterns consistent across sensory modalities through a modal-matching objective. In contrast, the private space extracts modality-exclusive information through a transformation-invariant objective. Second, we propose a temporal structural constraint for modality features, such that the average distance between temporally neighboring samples is no larger than that of temporally distant samples. Extensive evaluations are performed on four multimodal sensing datasets with two backbone encoders and two classifiers to demonstrate the superiority of FOCAL. It consistently outperforms the state-of-the-art baselines in downstream tasks with a clear margin, under different ratios of available labels. The code and self-collected dataset are available at https://github.com/tomoyoshki/focal.",main,NeurIPS,2023,Poster,Shengzhong Liu;Tomoyoshi Kimura;Dongxin Liu;Ruijie Wang;Jinyang Li;Suhas Diggavi;Mani Srivastava;Tarek Abdelzaher,True,https://openreview.net/pdf?id=l4CZCKXoSn
l7Ggnzaws5,TradeMaster: A Holistic Quantitative Trading Platform Empowered by Reinforcement Learning,"The financial markets, which involve over \\\\$90 trillion market capitals, attract the attention of innumerable profit-seeking investors globally. Recent explosion of reinforcement learning in financial trading (RLFT) research has shown stellar performance on many quantitative trading tasks. However, it is still challenging to deploy reinforcement learning (RL) methods into real-world financial markets due to the highly composite nature of this domain, which entails design choices and interactions between components that collect financial data, conduct feature engineering, build market environments, make investment decisions, evaluate model behaviors and offers user interfaces. Despite the availability of abundant financial data and advanced RL techniques, a remarkable gap still exists between the potential and realized utilization of RL in financial trading. In particular, orchestrating an RLFT project lifecycle poses challenges in engineering (i.e. hard to build), benchmarking (i.e. hard to compare) and usability (i.e. hard to optimize, maintain and use). To overcome these challenges, we introduce TradeMaster, a holistic open-source RLFT platform that serves as a i) software toolkit, ii) empirical benchmark, and iii) user interface. Our ultimate goal is to provide infrastructures for transparent and reproducible RLFT research and facilitate their real-world deployment with industry impact. TradeMaster will be updated continuously and welcomes contributions from both RL and finance communities.",Datasets & Benchmarks,NeurIPS,2023,Poster,Shuo Sun;Molei Qin;Wentao Zhang;Haochong Xia;Chuqiao Zong;Jie Ying;Yonggang Xie;Lingxuan Zhao;Xinrun Wang;Bo An,False,https://openreview.net/pdf?id=l7Ggnzaws5
lCuoehPWrB,Interactive Visual Reasoning under Uncertainty,"One of the fundamental cognitive abilities of humans is to quickly resolve uncertainty by generating hypotheses and testing them via active trials. Encountering a novel phenomenon accompanied by ambiguous cause-effect relationships, humans make hypotheses against data, conduct inferences from observation, test their theory via experimentation, and correct the proposition if inconsistency arises. These iterative processes persist until the underlying mechanism becomes clear. In this work, we devise the  **IVRE** (pronounced as *""ivory""*) environment for evaluating artificial agents' reasoning ability under uncertainty. **IVRE** is an interactive environment featuring rich scenarios centered around *Blicket* detection. Agents in **IVRE** are placed into environments with various ambiguous action-effect pairs and asked to determine each object's role. They are encouraged to propose effective and efficient experiments to validate their hypotheses based on observations and actively gather new information. The game ends when all uncertainties are resolved or the maximum number of trials is consumed. By evaluating modern artificial agents in **IVRE**, we notice a clear failure of today's learning methods compared to humans. Such inefficacy in interactive reasoning ability under uncertainty calls for future research in building human-like intelligence.",Datasets & Benchmarks,NeurIPS,2023,Poster,Manjie Xu;Guangyuan Jiang;Wei Liang;Chi Zhang;Yixin Zhu,False,https://openreview.net/pdf?id=lCuoehPWrB
lRu0dN7BY6,Social Motion Prediction with Cognitive Hierarchies,"Humans exhibit a remarkable capacity for anticipating the actions of others and planning their own actions accordingly. In this study, we strive to replicate this ability by addressing the social motion prediction problem. We introduce a new benchmark, a novel formulation, and a cognition-inspired framework. We present Wusi, a 3D multi-person motion dataset under the context of team sports, which features intense and strategic human interactions and diverse pose distributions. By reformulating the problem from a multi-agent reinforcement learning perspective, we incorporate behavioral cloning and generative adversarial imitation learning to boost learning efficiency and generalization. Furthermore, we take into account the cognitive aspects of the human social action planning process and develop a cognitive hierarchy framework to predict strategic human social interactions. We conduct comprehensive experiments to validate the effectiveness of our proposed dataset and approach.",main,NeurIPS,2023,Poster,Wentao Zhu;Jason Qin;Yuke Lou;Hang Ye;Xiaoxuan Ma;Hai Ci;Yizhou Wang,True,https://openreview.net/pdf?id=lRu0dN7BY6
leS8668NJm,Toward Re-Identifying Any Animal,"The current state of re-identification (ReID) models poses limitations to their applicability in the open world, as they are primarily designed and trained for specific categories like person or vehicle. In light of the importance of ReID technology for tracking wildlife populations and migration patterns, we propose a new task called ``Re-identify Any Animal in the Wild'' (ReID-AW). This task aims to develop a ReID model capable of handling any unseen wildlife category it encounters. To address this challenge, we have created a comprehensive dataset called Wildlife-71, which includes ReID data from 71 different wildlife categories. This dataset is the first of its kind to encompass multiple object categories in the realm of ReID. Furthermore, we have developed a universal re-identification model named UniReID specifically for the ReID-AW task. To enhance the model's adaptability to the target category, we employ a dynamic prompting mechanism using category-specific visual prompts. These prompts are generated based on knowledge gained from a set of pre-selected images within the target category. Additionally, we leverage explicit semantic knowledge derived from the large-scale pre-trained language model, GPT-4. This allows UniReID to focus on regions that are particularly useful for distinguishing individuals within the target category. Extensive experiments have demonstrated the remarkable generalization capability of our UniReID model. It showcases promising performance in handling arbitrary wildlife categories, offering significant advancements in the field of ReID for wildlife conservation and research purposes.",main,NeurIPS,2023,Poster,Bingliang Jiao;Lingqiao Liu;Liying Gao;Ruiqi Wu;Guosheng Lin;PENG WANG;Yanning Zhang,True,https://openreview.net/pdf?id=leS8668NJm
lh2f1AD4ax,CMMA: Benchmarking Multi-Affection Detection in Chinese Multi-Modal Conversations,"Human communication has a multi-modal and multi-affection nature. The inter-relatedness of different emotions and sentiments poses a challenge to jointly detect multiple human affections with multi-modal clues. Recent advances in this field employed multi-task learning paradigms to render the inter-relatedness across tasks, but the scarcity of publicly available resources sets a limit to the potential of works. To fill this gap, we build the first Chinese Multi-modal Multi-Affection conversation (CMMA) dataset, which contains 3,000 multi-party conversations and 21,795 multi-modal utterances collected from various styles of TV-series. CMMA contains a wide variety of affection labels, including sentiment, emotion, sarcasm and humor, as well as the novel inter-correlations values between certain pairs of tasks. Moreover, it provides the topic and speaker information in conversations, which promotes better modeling of conversational context. On the dataset, we empirically analyze the influence of different data modalities and conversational contexts on different affection analysis tasks, and exhibit the practical benefit of inter-task correlations. The full dataset will be publicly available for research\\\\footnote{https://github.com/annoymity2022/Chinese-Dataset}",Datasets & Benchmarks,NeurIPS,2023,Poster,Yazhou Zhang;Yang Yu;Qing Guo;Benyou Wang;Dongming Zhao;Sagar Uprety;Dawei Song;Qiuchi Li;Jing Qin,True,https://openreview.net/pdf?id=lh2f1AD4ax
loOw3oyhFW,LargeST: A Benchmark Dataset for Large-Scale Traffic Forecasting,"Road traffic forecasting plays a critical role in smart city initiatives and has experienced significant advancements thanks to the power of deep learning in capturing non-linear patterns of traffic data. However, the promising results achieved on current public datasets may not be applicable to practical scenarios due to limitations within these datasets. First, the limited sizes of them may not reflect the real-world scale of traffic networks. Second, the temporal coverage of these datasets is typically short, posing hurdles in studying long-term patterns and acquiring sufficient samples for training deep models. Third, these datasets often lack adequate metadata for sensors, which compromises the reliability and interpretability of the data. To mitigate these limitations, we introduce the LargeST benchmark dataset. It encompasses a total number of 8,600 sensors in California with a 5-year time coverage and includes comprehensive metadata. Using LargeST, we perform in-depth data analysis to extract data insights, benchmark well-known baselines in terms of their performance and efficiency, and identify challenges as well as opportunities for future research. We release the datasets and baseline implementations at: https://github.com/liuxu77/LargeST.",Datasets & Benchmarks,NeurIPS,2023,Poster,Xu Liu;Yutong Xia;Yuxuan Liang;Junfeng Hu;Yiwei Wang;LEI BAI;Chao Huang;Zhenguang Liu;Bryan Hooi;Roger Zimmermann,True,https://openreview.net/pdf?id=loOw3oyhFW
m2mbfoSuJ1,A Comprehensive Study on Text-attributed Graphs: Benchmarking and Rethinking,"Text-attributed graphs (TAGs) are prevalent in various real-world scenarios, where each node is associated with a text description. The cornerstone of representation learning on TAGs lies in the seamless integration of textual semantics within individual nodes and the topological connections across nodes. Recent advancements in pre-trained language models (PLMs) and graph neural networks (GNNs) have facilitated effective learning on TAGs, garnering increased research interest. However, the absence of meaningful benchmark datasets and standardized evaluation procedures for TAGs has impeded progress in this field. In this paper, we propose CS-TAG, a comprehensive and diverse collection of challenging benchmark datasets for TAGs. The CS-TAG datasets are notably large in scale and encompass a wide range of domains, spanning from citation networks to purchase graphs. In addition to building the datasets,  we conduct extensive benchmark experiments over CS-TAG with various learning paradigms, including PLMs, GNNs, PLM-GNN co-training methods, and the proposed novel topological pre-training of language models. In a nutshell, we provide an overview of the CS-TAG datasets, standardized evaluation procedures, and present baseline experiments. The entire CS-TAG project is publicly accessible at \\\\url{https://github.com/sktsherlock/TAG-Benchmark}.",Datasets & Benchmarks,NeurIPS,2023,Poster,Hao Yan;Chaozhuo Li;Ruosong Long;Chao Yan;Jianan Zhao;Wenwen Zhuang;Jun Yin;Peiyan Zhang;Weihao Han;Hao Sun;Weiwei Deng;Qi Zhang;Lichao Sun;Xing Xie;Senzhang Wang,True,https://openreview.net/pdf?id=m2mbfoSuJ1
mcx8IGneYw,Neural Lighting Simulation for Urban Scenes,"Different outdoor illumination conditions drastically alter the appearance of urban scenes, and they can harm the performance of image-based robot perception systems if not seen during training. Camera simulation provides a cost-effective solution to create a large dataset of images captured under different lighting conditions. Towards this goal, we propose LightSim, a neural lighting camera simulation system that enables diverse, realistic, and controllable data generation. LightSim automatically builds lighting-aware digital twins at scale from collected raw sensor data and decomposes the scene into dynamic actors and static background with accurate geometry, appearance, and estimated scene lighting. These digital twins enable actor insertion, modification, removal, and rendering from a new viewpoint, all in a lighting-aware manner. LightSim then combines physically-based and learnable deferred rendering to perform realistic relighting of modified scenes, such as altering the sun location and modifying the shadows or changing the sun brightness, producing spatially- and temporally-consistent camera videos. Our experiments show that LightSim generates more realistic relighting results  than prior work. Importantly,  training perception models on data generated by LightSim can significantly improve their performance. Our project page is available at https://waabi.ai/lightsim/.",main,NeurIPS,2023,Poster,Ava Pun;Gary Sun;Jingkang Wang;Yun Chen;Ze Yang;Sivabalan Manivasagam;Wei-Chiu Ma;Raquel Urtasun,True,https://openreview.net/pdf?id=mcx8IGneYw
mkSDXjX6EM,FIND: A Function Description Benchmark for Evaluating Interpretability Methods,"Labeling neural network submodules with human-legible descriptions is useful for many downstream tasks: such descriptions can surface failures, guide interventions, and perhaps even explain important model behaviors. To date, most mechanistic descriptions of trained networks have involved small models, narrowly delimited phenomena, and large amounts of human labor. Labeling all human-interpretable sub-computations in models of increasing size and complexity will almost certainly require tools that can generate and validate descriptions automatically. Recently, techniques that use learned models in-the-loop for labeling have begun to gain traction, but methods for evaluating their efficacy are limited and ad-hoc. How should we validate and compare open-ended labeling tools? This paper introduces FIND (Function INterpretation and Description), a benchmark suite for evaluating the building blocks of automated interpretability methods. FIND contains functions that resemble components of trained neural networks, and accompanying descriptions of the kind we seek to generate. The functions are procedurally constructed across textual and numeric domains, and involve a range of real-world complexities, including noise, composition, approximation, and bias. We evaluate methods that use pretrained language models (LMs) to produce code-based and natural language descriptions of function behavior. Additionally, we introduce a new interactive method in which an Automated Interpretability Agent (AIA) generates function descriptions. We find that an AIA, built with an off-the-shelf LM augmented with black-box access to functions, can sometimes infer function structure—acting as a scientist by forming hypotheses, proposing experiments, and updating descriptions in light of new data. However, FIND also reveals that LM-based descriptions capture global function behavior while missing local details. These results suggest that FIND will be useful for characterizing the performance of more sophisticated interpretability methods before they are applied to real-world models.",Datasets & Benchmarks,NeurIPS,2023,Poster,Sarah Schwettmann;Tamar Rott Shaham;Joanna Materzynska;Neil Chowdhury;Shuang Li;Jacob Andreas;David Bau;Antonio Torralba,True,https://openreview.net/pdf?id=mkSDXjX6EM
mmmd2vp0n0,Transient Neural Radiance Fields for Lidar View Synthesis and 3D Reconstruction,"Neural radiance fields (NeRFs) have become a ubiquitous tool for modeling scene appearance and geometry from multiview imagery. Recent work has also begun to explore how to use additional supervision from lidar or depth sensor measurements in the NeRF framework. However, previous lidar-supervised NeRFs focus on rendering conventional camera imagery and use lidar-derived point cloud data as auxiliary supervision; thus, they fail to incorporate the underlying image formation model of the lidar. Here, we propose a novel method for rendering transient NeRFs that take as input the raw, time-resolved photon count histograms measured by a single-photon lidar system, and we seek to render such histograms from novel views. Different from conventional NeRFs, the approach relies on a time-resolved version of the volume rendering equation to render the lidar measurements and capture transient light transport phenomena at picosecond timescales. We evaluate our method on a first-of-its-kind dataset of simulated and captured transient multiview scans from a prototype single-photon lidar. Overall, our work brings NeRFs to a new dimension of imaging at transient timescales, newly enabling rendering of transient imagery from novel views. Additionally, we show that our approach recovers improved geometry and conventional appearance compared to point cloud-based supervision when training on few input viewpoints. Transient NeRFs may be especially useful for applications which seek to simulate raw lidar measurements for downstream tasks in autonomous driving, robotics, and remote sensing.",main,NeurIPS,2023,Spotlight,Anagh Malik;Parsa Mirdehghan;Sotiris Nousias;Kyros Kutulakos;David B. Lindell,True,https://openreview.net/pdf?id=mmmd2vp0n0
msWIK6SKBK,SUPA: A Lightweight Diagnostic Simulator for Machine Learning in Particle Physics,"Deep learning methods have gained popularity in high energy physics for fast modeling of particle showers in detectors. Detailed simulation frameworks such as the gold standard \\\\textsc{Geant4} are computationally intensive, and current deep generative architectures work on discretized, lower resolution versions of the detailed simulation. The development of models that work at higher spatial resolutions is currently hindered by the complexity of the full simulation data, and by the lack of simpler, more interpretable benchmarks. Our contribution is \\\\textsc{SUPA}, the SUrrogate PArticle propagation simulator, an algorithm and software package for generating data by simulating simplified particle propagation, scattering and shower development in matter. The generation is extremely fast and easy to use compared to \\\\textsc{Geant4}, but still exhibits the key characteristics and challenges of the detailed simulation. The proposed simulator generates thousands of particle showers per second on a desktop machine, a speed up of up to 6 orders of magnitudes over \\\\textsc{Geant4}, and stores detailed geometric information about the shower propagation. \\\\textsc{\\\\textsc{SUPA}} provides much greater flexibility for setting initial conditions and defining multiple benchmarks for the development of models. Moreover, interpreting particle showers as point clouds creates a connection to geometric machine learning and provides challenging and fundamentally new datasets for the field.",Datasets & Benchmarks,NeurIPS,2023,Poster,Atul Kumar Sinha;Daniele Paliotta;Bálint Máté;John Andrew Raine;Tobias Golling;François Fleuret,True,https://openreview.net/pdf?id=msWIK6SKBK
n2wW7goGky,AirDelhi: Fine-Grained Spatio-Temporal Particulate Matter Dataset From Delhi For ML based Modeling,"Air pollution poses serious health concerns  in developing countries, such as India, necessitating large-scale measurement for correlation analysis, policy recommendations, and informed decision-making. However, fine-grained data collection is costly.  Specifically, static sensors for pollution measurement cost several thousand dollars per unit, leading to inadequate deployment and coverage. To complement the existing sparse static sensor network, we propose a mobile sensor network utilizing lower-cost PM2.5 sensors mounted on public buses in the Delhi-NCR region of India. Through this exercise, we introduce a novel dataset AirDelhi comprising PM2.5 and PM10 measurements. This dataset is made publicly available, at https://www.cse.iitd.ac.in/pollutiondata, serving as a valuable resource for machine learning (ML) researchers and environmentalists. We present three key contributions with the release of this dataset. Firstly, through in-depth statistical analysis, we demonstrate that the released dataset significantly differs from existing pollution datasets, highlighting its uniqueness and potential for new insights. Secondly, the dataset quality been validated against existing expensive sensors. Thirdly, we conduct a benchmarking exercise (https://github.com/sachin-iitd/DelhiPMDatasetBenchmark), evaluating state-of-the-art methods for interpolation, feature imputation, and forecasting on this dataset, which is the largest publicly available PM dataset to date. The results of the benchmarking exercise underscore the substantial disparities in accuracy between the proposed dataset and other publicly available datasets. This finding highlights the complexity and richness of our dataset, emphasizing its value for advancing research in the field of air pollution.",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=n2wW7goGky
n4OwK8cpx2,REASONER: An Explainable Recommendation Dataset with Comprehensive Labeling Ground Truths,"Explainable recommendation has attracted much attention from the industry and academic communities. It has shown great potential to improve the recommendation persuasiveness, informativeness and user satisfaction. In the past few years, while a lot of promising explainable recommender models have been proposed, the datasets used to evaluate them still suffer from several limitations, for example, the explanation ground truths are not labeled by the real users, the explanations are mostly single-modal and around only one aspect. To bridge these gaps, in this paper, we build a new explainable recommendation dataset, which, to our knowledge, is the first contribution that provides a large amount of real user labeled multi-modal and multi-aspect explaination ground truths. In specific, we firstly develop a video recommendation platform, where a series of questions around the recommendation explainability are carefully designed. Then, we recruit about 3000 high-quality labelers with different backgrounds to use the system, and collect their behaviors and feedback to our questions. In this paper, we detail the construction process of our dataset and also provide extensive analysis on its characteristics. In addition, we develop a library, where ten well-known explainable recommender models are implemented in a unified framework. Based on this library, we build several benchmarks for different explainable recommendation tasks. At last, we present many new opportunities brought by our dataset, which are expected to promote the field of explainable recommendation. Our dataset, library and the related documents have been released at https://reasoner2023.github.io/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Xu Chen;Jingsen Zhang;Lei Wang;Quanyu Dai;Zhenhua Dong;Ruiming Tang;Rui Zhang;Li Chen;Xin Zhao;Ji-Rong Wen,True,https://openreview.net/pdf?id=n4OwK8cpx2
n581purqB4,"AbdomenAtlas-8K: Annotating 8,000 CT Volumes for Multi-Organ Segmentation in Three Weeks","Annotating medical images, particularly for organ segmentation, is laborious and time-consuming. For example, annotating an abdominal organ requires an estimated rate of 30-60 minutes per CT volume based on the expertise of an annotator and the size, visibility, and complexity of the organ. Therefore, publicly available datasets for multi-organ segmentation are often limited in data size and organ diversity. This paper proposes an active learning procedure to expedite the annotation process for organ segmentation and creates the largest multi-organ dataset (by far) with the spleen, liver, kidneys, stomach, gallbladder, pancreas, aorta, and IVC annotated in 8,448 CT volumes, equating to 3.2 million slices. The conventional annotation methods would take an experienced annotator up to 1,600 weeks (or roughly 30.8 years) to complete this task. In contrast, our annotation procedure has accomplished this task in three weeks (based on an 8-hour workday, five days a week) while maintaining a similar or even better annotation quality. This achievement is attributed to three unique properties of our method: (1) label bias reduction using multiple pre-trained segmentation models, (2) effective error detection in the model predictions, and (3) attention guidance for annotators to make corrections on the most salient errors. Furthermore, we summarize the taxonomy of common errors made by AI algorithms and annotators. This allows for continuous improvement of AI and annotations, significantly reducing the annotation costs required to create large-scale datasets for a wider variety of medical imaging tasks. Code and dataset are available at https://github.com/MrGiovanni/AbdomenAtlas",Datasets & Benchmarks,NeurIPS,2023,Poster,Chongyu Qu;Tiezheng Zhang;Hualin Qiao;Jie Liu;Yucheng Tang;Alan Yuille;Zongwei Zhou,True,https://openreview.net/pdf?id=n581purqB4
n8hpztIuet,SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation,"Expressive human pose and shape estimation (EHPS) unifies body, hands, and face motion capture with numerous applications. Despite encouraging progress, current state-of-the-art methods still depend largely on a confined set of training datasets. In this work, we investigate scaling up EHPS towards the first generalist foundation model (dubbed SMPLer-X), with up to ViT-Huge as the backbone and training with up to 4.5M instances from diverse data sources. With big data and the large model, SMPLer-X exhibits strong performance across diverse test benchmarks and excellent transferability to even unseen environments. 1) For the data scaling, we perform a systematic investigation on 32 EHPS datasets, including a wide range of scenarios that a model trained on any single dataset cannot handle. More importantly, capitalizing on insights obtained from the extensive benchmarking process, we optimize our training scheme and select datasets that lead to a significant leap in EHPS capabilities. 2) For the model scaling, we take advantage of vision transformers to study the scaling law of model sizes in EHPS. Moreover, our finetuning strategy turn SMPLer-X into specialist models, allowing them to achieve further performance boosts. Notably, our foundation model SMPLer-X consistently delivers state-of-the-art results on seven benchmarks such as AGORA (107.2 mm NMVE), UBody (57.4 mm PVE), EgoBody (63.6 mm PVE), and EHF (62.3 mm PVE without finetuning).",Datasets & Benchmarks,NeurIPS,2023,Poster,Zhongang Cai;Wanqi Yin;Ailing Zeng;CHEN WEI;Qingping SUN;Yanjun Wang;Hui En Pang;Haiyi Mei;Mingyuan Zhang;Lei Zhang;Chen Change Loy;Lei Yang;Ziwei Liu,False,https://openreview.net/pdf?id=n8hpztIuet
oIUXpBnyjv,LightZero: A Unified Benchmark for Monte Carlo Tree Search in General Sequential Decision Scenarios,"Building agents based on tree-search planning capabilities with learned models has achieved remarkable success in classic decision-making problems, such as Go and Atari.
However, it has been deemed challenging or even infeasible to extend Monte Carlo Tree Search (MCTS) based algorithms to diverse real-world applications, especially when these environments involve complex action spaces and significant simulation costs, or inherent stochasticity.
In this work, we introduce LightZero, the first unified benchmark for deploying MCTS/MuZero in general sequential decision scenarios. 
Specificially, we summarize the most critical challenges in designing a general MCTS-style decision-making solver, then decompose the tightly-coupled algorithm and system design of tree-search RL methods into distinct sub-modules.
By incorporating more appropriate exploration and optimization strategies, we can significantly enhance these sub-modules and construct powerful LightZero agents to tackle tasks across a wide range of domains, such as board games, Atari, MuJoCo, MiniGrid and GoBigger.
Detailed benchmark results reveal the significant potential of such methods in building scalable and efficient decision intelligence.
The code is available as part of OpenDILab at https://github.com/opendilab/LightZero.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Yazhe Niu;Yuan Pu;Zhenjie Yang;Xueyan Li;Tong Zhou;Jiyuan Ren;Shuai Hu;Hongsheng Li;Yu Liu,True,https://openreview.net/pdf?id=oIUXpBnyjv
oQSfcVTNr1,SoundCam: A Dataset for Finding Humans Using Room Acoustics,"A room’s acoustic properties are a product of the room’s geometry, the objects within the room, and their specific positions. A room’s acoustic properties can be characterized by its impulse response (RIR) between a source and listener location, or roughly inferred from recordings of natural signals present in the room. Variations in the positions of objects in a room can effect measurable changes in the room’s acoustic properties, as characterized by the RIR. Existing datasets of RIRs either do not systematically vary positions of objects in an environment, or they consist of only simulated RIRs. We present SoundCam, the largest dataset of unique RIRs from in-the-wild rooms publicly released to date. It includes 5,000 10-channel real-world measurements of room impulse responses and 2,000 10-channel recordings of music in three different rooms, including a controlled acoustic lab, an in-the-wild living room, and a conference room, with different humans in positions throughout each room. We show that these measurements can be used for interesting tasks, such as detecting and identifying humans, and tracking their positions.",Datasets & Benchmarks,NeurIPS,2023,Poster,Mason Long Wang;Samuel Clarke;Jui-Hsien Wang;Ruohan Gao;Jiajun Wu,True,https://openreview.net/pdf?id=oQSfcVTNr1
oi1MUMk5NF,DISCS: A Benchmark for Discrete Sampling,"Sampling in discrete spaces, with critical applications in simulation and optimization, has recently been boosted by significant advances in gradient-based approaches that exploit modern accelerators like GPUs. However, two key challenges are hindering further advancement in research on discrete sampling. First, since there is no consensus on experimental settings and evaluation setups, the empirical results in different research papers are often not comparable. Second, implementing samplers and target distributions often requires a nontrivial amount of effort in terms of calibration and parallelism. To tackle these challenges, we propose DISCS (DISCrete Sampling), a tailored package and benchmark that supports unified and efficient experiment implementation and evaluations for discrete sampling in three types of tasks: sampling from classical graphical models and energy based generative models, and sampling for solving combinatorial optimization. Throughout the comprehensive evaluations in DISCS, we gained new insights into scalability, design principles for proposal distributions, and lessons for adaptive sampling design. DISCS efficiently implements representative discrete samplers in existing research works as baselines and offers a simple interface that researchers can conveniently add new discrete samplers and directly compare their performance with the benchmark result in a calibrated setup.",Datasets & Benchmarks,NeurIPS,2023,Poster,Katayoon Goshvadi;Haoran Sun;Xingchao Liu;Azade Nova;Ruqi Zhang;Will Sussman Grathwohl;Dale Schuurmans;Hanjun Dai,False,https://openreview.net/pdf?id=oi1MUMk5NF
oz4AGs0phP,SynMob: Creating High-Fidelity Synthetic GPS Trajectory Dataset for Urban Mobility Analysis,"Urban mobility analysis has been extensively studied in the past decade using a vast amount of GPS trajectory data, which reveals hidden patterns in movement and human activity within urban landscapes. Despite its significant value, the availability of such datasets often faces limitations due to privacy concerns, proprietary barriers, and quality inconsistencies. To address these challenges, this paper presents a synthetic trajectory dataset with high fidelity, offering a general solution to these data accessibility issues. Specifically, the proposed dataset adopts a diffusion model as its synthesizer, with the primary aim of accurately emulating the spatial-temporal behavior of the original trajectory data. These synthesized data can retain the geo-distribution and statistical properties characteristic of real-world datasets. Through rigorous analysis and case studies, we validate the high similarity and utility between the proposed synthetic trajectory dataset and real-world counterparts. Such validation underscores the practicality of synthetic datasets for urban mobility analysis and advocates for its wider acceptance within the research community. Finally, we publicly release the trajectory synthesizer and datasets, aiming to enhance the quality and availability of synthetic trajectory datasets and encourage continued contributions to this rapidly evolving field. The dataset is released for public online availability https://github.com/Applied-Machine-Learning-Lab/SynMob.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yuanshao Zhu;Yongchao Ye;Ying Wu;Xiangyu Zhao;James Yu,True,https://openreview.net/pdf?id=oz4AGs0phP
p8gTWkFIvx,Modality-Independent Teachers Meet Weakly-Supervised Audio-Visual Event Parser,"Audio-visual learning has been a major pillar of multi-modal machine learning, where the community mostly focused on its $\\\\textit{modality-aligned}$ setting, $\\\\textit{i.e.}$, the audio and visual modality are $\\\\textit{both}$ assumed to signal the prediction target.
With the Look, Listen, and Parse dataset (LLP), we investigate the under-explored $\\\\textit{unaligned}$ setting, where the goal is to recognize audio and visual events in a video with only weak labels observed.
Such weak video-level labels only tell what events happen without knowing the modality they are perceived (audio, visual, or both).
To enhance learning in this challenging setting, we incorporate large-scale contrastively pre-trained models as the modality teachers. A simple, effective, and generic method, termed $\\\\textbf{V}$isual-$\\\\textbf{A}$udio $\\\\textbf{L}$abel Elab$\\\\textbf{or}$ation (VALOR), is innovated to harvest modality labels for the training events.
Empirical studies show that the harvested labels significantly improve an attentional baseline by $\\\\textbf{8.0}$ in average F-score (Type@AV).
Surprisingly, we found that modality-independent teachers outperform their modality-fused counterparts since they are noise-proof from the other potentially unaligned modality.
Moreover, our best model achieves the new state-of-the-art on all metrics of LLP by a substantial margin ($\\\\textbf{+5.4}$ F-score for Type@AV). VALOR is further generalized to Audio-Visual Event Localization and achieves the new state-of-the-art as well.",main,NeurIPS,2023,Poster,Yung-Hsuan Lai;Yen-Chun Chen;Yu-Chiang Frank Wang,True,https://openreview.net/pdf?id=p8gTWkFIvx
pNtG6NAmx0,Statistical Knowledge Assessment for Large Language Models,"Given varying prompts regarding a factoid question, can a large language model (LLM) reliably generate factually correct answers? Existing LLMs may generate distinct responses for different prompts. In this paper, we study the problem of quantifying knowledge contained in an LLM regarding a given set of facts. We propose KaRR, a statistical approach to assess factual knowledge for LLMs. The main idea is to estimate the ratio of LLM generating text corresponding to the answer entity given diverse prompts of the subject and the querying relation, versus it generating by random chances. Our assessment suite contains a comprehensive set of 994,123 entities and 600 relations, with 1,395,905 text aliases. We use our method to evaluate 20 LLMs of various sizes, including LLaMA, Alpaca, OPT, etc. Experiments show that our results have a strong correlation (0.43 Kendall's $\\\\tau$) with the results of human assessment on LLMs. Our results reveal that the knowledge in LLMs with the same backbone architecture adheres to the scaling law, while tuning on instruction-following data sometimes compromises the model's capability to generate factually correct text reliably.",main,NeurIPS,2023,Poster,Qingxiu Dong;Jingjing Xu;Lingpeng Kong;Zhifang Sui;Lei Li,True,https://openreview.net/pdf?id=pNtG6NAmx0
pRnrg2bWr0,OpenIllumination: A Multi-Illumination Dataset for Inverse Rendering Evaluation on Real Objects,"We introduce OpenIllumination, a real-world dataset containing over 108K images of 64 objects with diverse materials, captured under 72 camera views and a large number of different illuminations. For each image in the dataset, we provide accurate camera parameters, illumination ground truth, and foreground segmentation masks. Our dataset enables the quantitative evaluation of most inverse rendering and material decomposition methods for real objects. We examine several state-of-the-art inverse rendering methods on our dataset and compare their performances. The dataset and code can be found on the project page: https://oppo-us-research.github.io/OpenIllumination.",Datasets & Benchmarks,NeurIPS,2023,Poster,Isabella Liu;Linghao Chen;Ziyang Fu;Liwen Wu;Haian Jin;Zhong Li;Chin Ming Ryan Wong;Yi Xu;Ravi Ramamoorthi;Zexiang Xu;Hao Su,True,https://openreview.net/pdf?id=pRnrg2bWr0
pTSNoBTk8E,DynaDojo: An Extensible Benchmarking Platform for Scalable Dynamical System Identification,"Modeling complex dynamical systems poses significant challenges, with traditional methods struggling to work across a variety of systems and scale to high-dimensional dynamics. In response, we present DynaDojo, a novel benchmarking platform designed for data-driven dynamical system identification. DynaDojo enables comprehensive evaluation of how an algorithm's performance scales across three key dimensions: (1) the number of training samples provided, (2) the complexity of the dynamical system being modeled, and (3) the training samples required to achieve a target error threshold. Furthermore, DynaDojo enables studying out-of-distribution generalization (by providing multiple test conditions for each system) and active learning (by supporting closed-loop control). Through its user-friendly and easily extensible API, DynaDojo accommodates a wide range of user-defined $\\\\texttt{Algorithms}$, $\\\\texttt{Systems}$, and $\\\\texttt{Challenges}$ (scaling metrics). The platform also prioritizes resource-efficient training for running on a cluster. To showcase its utility, in DynaDojo $\\\\texttt{0.9}$, we include implementations of 7 baseline algorithms and 20 dynamical systems, along with many demo notebooks. This work aspires to make DynaDojo a unifying benchmarking platform for system identification, paralleling the role of OpenAI’s Gym in reinforcement learning.",Datasets & Benchmarks,NeurIPS,2023,Poster,Logan Mondal Bhamidipaty;Tommy Bruzzese;Caryn Tran;Rami Ratl Mrad;Max Kanwal,False,https://openreview.net/pdf?id=pTSNoBTk8E
pV1xV2RK6I,ToolQA: A Dataset for LLM Question Answering with External Tools,"Large Language Models (LLMs) have demonstrated impressive performance in various NLP tasks, but they still suffer from challenges such as hallucination and weak numerical reasoning. To overcome these challenges, external tools can be used to enhance LLMs' question-answering abilities. However, current evaluation methods do not distinguish between questions that can be answered using LLMs' internal knowledge and those that require external information through tool use. To address this issue, we introduce a new dataset called ToolQA, which is designed to faithfully evaluate LLMs' ability to use external tools for question answering. Our development of ToolQA involved a scalable, automated process for dataset curation, along with 13 specialized tools designed for interaction with external knowledge in order to answer questions. Importantly, we strive to minimize the overlap between our benchmark data and LLMs' pre-training data, enabling a more precise evaluation of LLMs' tool-use reasoning abilities. We conducted an in-depth diagnosis of existing tool-use LLMs to highlight their strengths, weaknesses, and potential improvements. Our findings set a new benchmark for evaluating LLMs and suggest new directions for future advancements. Our data and code are freely available for the broader scientific community on GitHub.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yuchen Zhuang;Yue Yu;Kuan Wang;Haotian Sun;Chao Zhang,True,https://openreview.net/pdf?id=pV1xV2RK6I
pWkrU6raMt,SubseasonalClimateUSA: A Dataset for Subseasonal Forecasting and Benchmarking,"Subseasonal forecasting of the weather two to six weeks in advance is critical for resource allocation and advance disaster notice but poses many challenges for the forecasting community. At this forecast horizon, physics-based dynamical models have limited skill, and the targets for prediction depend in a complex manner on both local weather variables and global climate variables. Recently, machine learning methods have shown promise in advancing the state of the art but only at the cost of complex data curation, integrating expert knowledge with aggregation across multiple relevant data sources, file formats, and temporal and spatial resolutions. To streamline this process and accelerate future development, we introduce SubseasonalClimateUSA, a curated dataset for training and benchmarking subseasonal forecasting models in the United States. We use this dataset to benchmark a diverse suite of models, including operational dynamical models, classical meteorological baselines, and ten state-of-the-art machine learning and deep learning-based methods from the literature. Overall, our benchmarks suggest simple and effective ways to extend the accuracy of current operational models. SubseasonalClimateUSA is regularly updated and accessible via the https://github.com/microsoft/subseasonal_data/ Python package.",Datasets & Benchmarks,NeurIPS,2023,Poster,Soukayna Mouatadid;Paulo Orenstein;Genevieve Elaine Flaspohler;Miruna Oprescu;Judah Cohen;Franklyn Wang;Sean Edward Knight;Maria Geogdzhayeva;Samuel James Levang;Ernest Fraenkel;Lester Mackey,True,https://openreview.net/pdf?id=pWkrU6raMt
pX5xlL1T4C,Renku: a platform for sustainable data science,"Data and code working together is fundamental to machine learning (ML), but the context around datasets and interactions between datasets and code are in general captured only rudimentarily. Context such as how the dataset was prepared and created, what source data were used, what code was used in processing, how the dataset evolved, and where it has been used and reused can provide much insight, but this information is often poorly documented. That is unfortunate since it makes datasets into black-boxes with potentially hidden characteristics that have downstream consequences. We argue that making dataset preparation more accessible and dataset usage easier to record and document would have significant benefits for the ML community: it would allow for greater diversity in datasets by inviting modification to published sources, simplify use of alternative datasets and, in doing so, make results more transparent and robust, while allowing for all contributions to be adequately credited. We present a platform, Renku, designed to support and encourage such sustainable development and use of data, datasets, and code, and we demonstrate its benefits through a few illustrative projects which span the spectrum from dataset creation to dataset consumption and showcasing.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,,False,https://openreview.net/pdf?id=pX5xlL1T4C
plAix1NxhU,TpuGraphs: A Performance Prediction Dataset on Large Tensor Computational Graphs,"Precise hardware performance models play a crucial role in code optimizations. They can assist compilers in making heuristic decisions or aid autotuners in identifying the optimal configuration for a given program. For example, the autotuner for XLA, a machine learning compiler, discovered 10–20\\\\% speedup on state-of-the-art models serving substantial production traffic at Google. Although there exist a few datasets for program performance prediction, they target small sub-programs such as basic blocks or kernels. This paper introduces TpuGraphs, a performance prediction dataset on full tensor programs, represented as computational graphs, running on Tensor Processing Units (TPUs). Each graph in the dataset represents the main computation of a machine learning workload, e.g., a training epoch or an inference step. Each data sample contains a computational graph, a compilation configuration, and the execution time of the graph when compiled with the configuration. The graphs in the dataset are collected from open-source machine learning programs, featuring popular model architectures (e.g., ResNet, EfficientNet, Mask R-CNN, and Transformer). TpuGraphs provides 25x more graphs than the largest graph property prediction dataset (with comparable graph sizes), and 770x larger graphs on average compared to existing performance prediction datasets on machine learning programs. This graph-level prediction task on large graphs introduces new challenges in learning, ranging from scalability, training efficiency, to model quality.",Datasets & Benchmarks,NeurIPS,2023,Poster,Phitchaya Mangpo Phothilimthana;Sami Abu-El-Haija;Kaidi Cao;Bahare Fatemi;Michael Burrows;Charith Mendis;Bryan Perozzi,True,https://openreview.net/pdf?id=plAix1NxhU
pu3sNlrgQr,Are These the Same Apple? Comparing Images Based on Object Intrinsics,"The human visual system can effortlessly recognize an object under different extrinsic factors such as lighting, object poses, and background, yet current computer vision systems often struggle with these variations. An important step to understanding and improving artificial vision systems is to measure image similarity purely based on intrinsic object properties that define object identity. This problem has been studied in the computer vision literature as re-identification, though mostly restricted to specific object categories such as people and cars. We propose to extend it to general object categories, exploring an image similarity metric based on object intrinsics. To benchmark such measurements, we collect the Common paired objects Under differenT Extrinsics (CUTE) dataset of 18, 000 images of 180 objects under different extrinsic factors such as lighting, poses, and imaging conditions. While existing methods such as LPIPS and CLIP scores do not measure object intrinsics well, we find that combining deep features learned from contrastive self-supervised learning with foreground filtering is a simple yet effective approach to approximating the similarity. We conduct an extensive survey of pre-trained features and foreground extraction methods to arrive at a strong baseline that best measures intrinsic object-centric image similarity among current methods. Finally, we demonstrate that our approach can aid in downstream applications such as acting as an analog for human subjects and improving generalizable re-identification. Please see our project website at https://s-tian.github.io/projects/cute/ for visualizations of the data and demos of our metric.",Datasets & Benchmarks,NeurIPS,2023,Poster,Klemen Kotar;Stephen Tian;Hong-Xing Yu;Daniel LK Yamins;Jiajun Wu,True,https://openreview.net/pdf?id=pu3sNlrgQr
pvdm4B6JMK,ChessGPT: Bridging Policy Learning and Language Modeling,"When solving decision-making tasks, humans typically depend on information from two key sources: (1) Historical policy data, which provides interaction replay from the environment, and (2) Analytical insights in natural language form, exposing the invaluable thought process or strategic considerations. Despite this, the majority of preceding research focuses on only one source: they either use historical replay exclusively to directly learn policy or value functions, or engaged in language model training utilizing mere language corpus. In this paper, we argue that a powerful autonomous agent should cover both sources. Thus, we propose ChessGPT, a GPT model bridging policy learning and language modeling by integrating data from these two sources in Chess games. Specifically, we build a large-scale game and language dataset related to chess. Leveraging the dataset, we showcase two model examples ChessCLIP and ChessGPT, integrating policy learning and language modeling. Finally, we propose a full evaluation framework for evaluating language model's chess ability. Experimental results validate our model and dataset's effectiveness. We open source our code, model, and dataset at https://github.com/waterhorse1/ChessGPT.",Datasets & Benchmarks,NeurIPS,2023,Poster,Xidong Feng;Yicheng Luo;Ziyan Wang;Hongrui Tang;Mengyue Yang;Kun Shao;David Henry Mguni;Yali Du;Jun Wang,True,https://openreview.net/pdf?id=pvdm4B6JMK
pyhv4qYCEJ,Evaluating Self-Supervised Learning for Molecular Graph Embeddings,"Graph Self-Supervised Learning (GSSL) provides a robust pathway for acquiring embeddings without expert labelling, a capability that carries profound implications for molecular graphs due to the staggering number of potential molecules and the high cost of obtaining labels. However, GSSL methods are designed not for optimisation within a specific domain but rather for transferability across a variety of downstream tasks. This broad applicability complicates their evaluation. Addressing this challenge, we present ""Molecular Graph Representation Evaluation"" (MOLGRAPHEVAL), generating detailed profiles of molecular graph embeddings with interpretable and diversified attributes. MOLGRAPHEVAL offers a suite of probing tasks grouped into three categories: (i) generic graph, (ii) molecular substructure, and (iii) embedding space properties. By leveraging MOLGRAPHEVAL to benchmark existing GSSL methods against both current downstream datasets and our suite of tasks, we uncover significant inconsistencies between inferences drawn solely from existing datasets and those derived from more nuanced probing. These findings suggest that current evaluation methodologies fail to capture the entirety of the landscape.",Datasets & Benchmarks,NeurIPS,2023,Poster,Hanchen Wang;Jean Kaddour;Shengchao Liu;Jian Tang;Joan Lasenby;Qi Liu,False,https://openreview.net/pdf?id=pyhv4qYCEJ
q1NaqDadKM,LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models,"Large Vision-Language Models (LVLM) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building an LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as visual question answering and embodied artificial intelligence on $40$ standard text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study reveals several innovative findings. First, Instruction-tuned LVLM with massive in-domain data such as InstructBLIP heavily overfits many existing tasks, generalizing poorly in the open-world scenario. Second, Instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). It either makes the current evaluation metric such as CIDER for image captioning ineffective or generates wrong answers. Third, employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective metric for LVLM evaluation. The findings provide a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques. The evaluation pipeline will be available at [vlarena page](https://github.com/OpenGVLab/Multi-Modality-Arena/tree/main/LVLM_evaluation).",Datasets & Benchmarks,NeurIPS,2023,Reject,Peng Xu;Wenqi Shao;Kaipeng Zhang;Peng Gao;Shuo Liu;Fanqing Meng;Siyuan Huang;Meng Lei;Ping Luo;Yu Qiao,False,https://openreview.net/pdf?id=q1NaqDadKM
q3FJk2Nvkk,IMP-MARL: a Suite of Environments for Large-scale Infrastructure Management Planning via MARL,"We introduce IMP-MARL, an open-source suite of multi-agent reinforcement learning (MARL) environments for large-scale Infrastructure Management Planning (IMP), offering a platform for benchmarking the scalability of cooperative MARL methods in real-world engineering applications.
In IMP, a multi-component engineering system is subject to a risk of failure due to its components' damage condition.
Specifically, each agent plans inspections and repairs for a specific system component, aiming to minimise maintenance costs while cooperating to minimise system failure risk.
With IMP-MARL, we release several environments including one related to offshore wind structural systems, in an effort to meet today's needs to improve management strategies to support sustainable and reliable energy systems.
Supported by IMP practical engineering environments featuring up to 100 agents, we conduct a benchmark campaign, where the scalability and performance of state-of-the-art cooperative MARL methods are compared against expert-based heuristic policies. 
The results reveal that centralised training with decentralised execution methods scale better with the number of agents than fully centralised or decentralised RL approaches, while also outperforming expert-based heuristic policies in most IMP environments.
Based on our findings, we additionally outline remaining cooperation and scalability challenges that future MARL methods should still address.
Through IMP-MARL, we encourage the implementation of new environments and the further development of MARL methods.",Datasets & Benchmarks,NeurIPS,2023,Poster,Pascal Leroy;Pablo G. Morato;Jonathan Pisane;Athanasios Kolios;Damien Ernst,False,https://openreview.net/pdf?id=q3FJk2Nvkk
q4XNX15kSe,rPPG-Toolbox: Deep Remote PPG Toolbox,"Camera-based physiological measurement is a fast growing field of computer vision. Remote photoplethysmography (rPPG) utilizes imaging devices (e.g., cameras) to measure the peripheral blood volume pulse (BVP) via photoplethysmography, and enables cardiac measurement via webcams and smartphones. However, the task is non-trivial with important pre-processing, modeling and post-processing steps required to obtain state-of-the-art results. Replication of results and benchmarking of new models is critical for scientific progress; however, as with many other applications of deep learning, reliable codebases are not easy to find or use. We present a comprehensive toolbox, rPPG-Toolbox, unsupervised and supervised rPPG models with support for public benchmark datasets, data augmentation and systematic evaluation: https://github.com/ubicomplab/rPPG-Toolbox.",Datasets & Benchmarks,NeurIPS,2023,Poster,Xin Liu;Girish Narayanswamy;Akshay Paruchuri;Xiaoyu Zhang;Jiankai Tang;Yuzhe Zhang;Roni Sengupta;Shwetak Patel;Yuntao Wang;Daniel McDuff,False,https://openreview.net/pdf?id=q4XNX15kSe
q9hc7R8N7P,Exploring Why Object Recognition Performance Degrades Across Income Levels and Geographies with Factor Annotations,"Despite impressive advances in object-recognition, deep learning systems’ performance degrades significantly across geographies and lower income levels---raising pressing concerns of inequity. Addressing such performance gaps remains a challenge, as little is understood about why performance degrades across incomes or geographies.
We take a step in this direction by annotating images from Dollar Street, a popular benchmark of geographically and economically diverse images, labeling each image with factors such as color, shape, and background. These annotations unlock a new granular view into how objects differ across incomes/regions. We then use these object differences to pinpoint model vulnerabilities across incomes and regions.
We study a range of modern vision models, finding that performance disparities are most associated with differences in _texture, occlusion_, and images with _darker lighting_.
We illustrate how insights from our factor labels can surface mitigations to improve models' performance disparities.
As an example, we show that mitigating a model's vulnerability to texture 
can improve performance on the lower income level.
**We release all the factor annotations along with an interactive dashboard
to facilitate research into more equitable vision systems.**",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Laura Gustafson;Megan Richards;Melissa Hall;Caner Hazirbas;Diane Bouchacourt;Mark Ibrahim,True,https://openreview.net/pdf?id=q9hc7R8N7P
qG7IkQ7IBO,Temporal Graph Benchmark for Machine Learning on Temporal Graphs,"We present the Temporal Graph Benchmark (TGB), a collection of challenging and diverse benchmark datasets for realistic, reproducible, and robust evaluation of machine learning models on temporal graphs. TGB datasets are of large scale, spanning years in duration, incorporate both node and edge-level prediction tasks and cover a diverse set of domains including social, trade, transaction, and transportation networks. For both tasks, we design evaluation protocols based on realistic use-cases. We extensively benchmark each dataset and find that the performance of common models can vary drastically across datasets. In addition, on dynamic node property prediction tasks, we show that simple methods often achieve superior performance compared to existing temporal graph models. We believe that these findings open up opportunities for future research on temporal graphs. Finally, TGB provides an automated machine learning pipeline for reproducible and accessible temporal graph research, including data loading, experiment setup and performance evaluation. TGB will be maintained and updated on a regular basis and welcomes community feedback. TGB datasets, data loaders, example codes, evaluation setup, and leaderboards are publicly available at https://tgb.complexdatalab.com/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Shenyang Huang;Farimah Poursafaei;Jacob Danovitch;Matthias Fey;Weihua Hu;Emanuele Rossi;Jure Leskovec;Michael M. Bronstein;Guillaume Rabusseau;Reihaneh Rabbany,True,https://openreview.net/pdf?id=qG7IkQ7IBO
qPUbKxKvXq,Monitor-Guided Decoding of Code LMs with Static Analysis of Repository Context,"Language models of code (LMs) work well when the surrounding code provides sufficient context. This is not true when it becomes necessary to use types, functionality or APIs defined elsewhere in the repository or a linked library, especially those not seen during training. LMs suffer from limited awareness of such global context and end up hallucinating.

Integrated development environments (IDEs) assist developers in understanding repository context using static analysis.  We extend this assistance, enjoyed by developers, to LMs. We propose monitor-guided decoding (MGD) where a monitor uses static analysis to guide the decoding. We construct a repository-level dataset PragmaticCode for method-completion in Java and evaluate MGD on it. On models of varying parameter scale, by monitoring for type-consistent object dereferences, MGD consistently improves compilation rates and agreement with ground truth. Further, LMs with fewer parameters, when augmented with MGD, can outperform larger LMs. With MGD, SantaCoder-1.1B achieves better compilation rate and next-identifier match than the much larger text-davinci-003 model.

We also conduct a generalizability study to evaluate the ability of MGD to generalize to multiple programming languages (Java, C# and Rust), coding scenarios (e.g., correct number of arguments to method calls), and to enforce richer semantic constraints (e.g., stateful API protocols). Our data and implementation are available at https://github.com/microsoft/monitors4codegen.",main,NeurIPS,2023,Poster,Lakshya Agrawal;Aditya Kanade;Navin Goyal;Shuvendu K Lahiri;Sriram Rajamani,True,https://openreview.net/pdf?id=qPUbKxKvXq
qQnO1HLQHe,Complex Query Answering on Eventuality Knowledge Graph with Implicit Logical Constraints,"Querying knowledge graphs (KGs) using deep learning approaches can naturally leverage the reasoning and generalization ability to learn to infer better answers. Traditional neural complex query answering (CQA) approaches mostly work on entity-centric KGs. However, in the real world, we also need to make logical inferences about events, states, and activities (i.e., eventualities or situations) to push learning systems from System I to System II, as proposed by Yoshua Bengio. Querying logically from an EVentuality-centric KG (EVKG) can naturally provide references to such kind of intuitive and logical inference. Thus, in this paper, we propose a new framework to leverage neural methods to answer complex logical queries based on an EVKG, which can satisfy not only traditional first-order logic constraints but also implicit logical constraints over eventualities concerning their occurrences and orders. For instance, if we know that *Food is bad* happens before *PersonX adds soy sauce*, then *PersonX adds soy sauce* is unlikely to be the cause of *Food is bad* due to implicit temporal constraint. To facilitate consistent reasoning on EVKGs, we propose Complex Eventuality Query Answering (CEQA), a more rigorous definition of CQA that considers the implicit logical constraints governing the temporal order and occurrence of eventualities. In this manner, we propose to leverage theorem provers for constructing benchmark datasets to ensure the answers satisfy implicit logical constraints. We also propose a Memory-Enhanced Query Encoding (MEQE) approach to significantly improve the performance of state-of-the-art neural query encoders on the CEQA task.",main,NeurIPS,2023,Poster,Jiaxin Bai;Xin Liu;Weiqi Wang;Chen Luo;Yangqiu Song,True,https://openreview.net/pdf?id=qQnO1HLQHe
qVXYU3F017,Stable Bias: Evaluating Societal Representations in Diffusion Models,"As machine learning-enabled Text-to-Image (TTI) systems are becoming increasingly prevalent and seeing growing adoption as commercial services, characterizing the social biases they exhibit is a necessary first step to lowering their risk of discriminatory outcomes. This evaluation, however, is made more difficult by the synthetic nature of these systems’ outputs: common definitions of diversity are grounded in social categories of people living in the world, whereas the artificial depictions of fictive humans created by these systems have no inherent gender or ethnicity. To address this need, we propose a new method for exploring the social biases in TTI systems. Our approach relies on characterizing the variation in generated images triggered by enumerating gender and ethnicity markers in the prompts, and comparing it to the variation engendered by spanning different professions. This allows us to (1) identify specific bias trends, (2) provide targeted scores to directly compare models in terms of diversity and representation, and (3) jointly model interdependent social variables to support a multidimensional analysis. We leverage this method to analyze images generated by 3 popular TTI systems (Dall·E 2 , Stable Diffusion v 1.4 and 2) and find that while all of their outputs show correlations with US labor demographics, they also consistently under-represent marginalized identities to different extents. We also release the datasets and low-code interactive bias exploration platforms developed for
this work, as well as the necessary tools to similarly evaluate additional TTI systems.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,,True,https://openreview.net/pdf?id=qVXYU3F017
qWsQi9DGJb,StressID: a Multimodal Dataset for Stress Identification,"StressID is a new dataset specifically designed for stress identification from
unimodal and multimodal data. It contains videos of facial expressions, audio
recordings, and physiological signals. The video and audio recordings are acquired
using an RGB camera with an integrated microphone. The physiological data
is composed of electrocardiography (ECG), electrodermal activity (EDA), and
respiration signals that are recorded and monitored using a wearable device. This
experimental setup ensures a synchronized and high-quality multimodal data col-
lection. Different stress-inducing stimuli, such as emotional video clips, cognitive
tasks including mathematical or comprehension exercises, and public speaking
scenarios, are designed to trigger a diverse range of emotional responses. The
final dataset consists of recordings from 65 participants who performed 11 tasks,
as well as their ratings of perceived relaxation, stress, arousal, and valence levels.
StressID is one of the largest datasets for stress identification that features three
different sources of data and varied classes of stimuli, representing more than
39 hours of annotated data in total. StressID offers baseline models for stress
classification including a cleaning, feature extraction, and classification phase for
each modality. Additionally, we provide multimodal predictive models combining
video, audio, and physiological inputs. The data and the code for the baselines are
available at https://project.inria.fr/stressid/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Hava Chaptoukaev;Valeriya Strizhkova;Michele Panariello;Bianca Dalpaos;Aglind Reka;Valeria Manera;Susanne Thummler;Esma ISMAILOVA;Nicholas Evans;Francois Bremond;Massimiliano Todisco;Maria A Zuluaga;Laura M. Ferrari,True,https://openreview.net/pdf?id=qWsQi9DGJb
qY9LR74O3Z,Holistic Evaluation of Text-to-Image Models,"The stunning qualitative improvement of text-to-image models has led to their widespread attention and adoption. However, we lack a comprehensive quantitative understanding of their capabilities and risks. To fill this gap, we introduce a new benchmark, Holistic Evaluation of Text-to-Image Models (HEIM). Whereas previous evaluations focus mostly on image-text alignment and image quality, we identify 12 aspects, including text-image alignment, image quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. We curate 62 scenarios encompassing these aspects and evaluate 26 state-of-the-art text-to-image models on this benchmark. Our results reveal that no single model excels in all aspects, with different models demonstrating different strengths. We release the generated images and human evaluation results for full transparency at https://crfm.stanford.edu/heim/latest and the code at https://github.com/stanford-crfm/helm, which is integrated with the HELM codebase",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Tony Lee;Michihiro Yasunaga;Chenlin Meng;Yifan Mai;Joon Sung Park;Agrim Gupta;Yunzhi Zhang;Deepak Narayanan;Hannah Benita Teufel;Marco Bellagente;Minguk Kang;Taesung Park;Jure Leskovec;Jun-Yan Zhu;Li Fei-Fei;Jiajun Wu;Stefano Ermon;Percy Liang,True,https://openreview.net/pdf?id=qY9LR74O3Z
qf4CWnrvZa,VTaC: A Benchmark Dataset of Ventricular Tachycardia Alarms from ICU Monitors,"False arrhythmia alarms in intensive care units (ICUs) are a continuing problem despite considerable effort from industrial and academic algorithm developers. Of all life-threatening arrhythmias, ventricular tachycardia (VT) stands out as the most challenging arrhythmia to detect reliably. We introduce a new annotated VT alarm database, VTaC (Ventricular Tachycardia annotated alarms from ICUs)  consisting of  over 5,000 waveform recordings with VT alarms triggered by bedside monitors in the ICUs. Each VT alarm in the dataset has been labeled by at least two independent human expert annotators. The dataset encompasses data collected from ICUs in three major US hospitals and includes data from three leading bedside monitor manufacturers, providing a diverse and representative collection of alarm waveform data. Each waveform recording comprises at least two electrocardiogram (ECG) leads and one or more pulsatile waveforms, such as photoplethysmogram (PPG or PLETH) and arterial blood pressure (ABP) waveforms. We demonstrate the utility of this new benchmark dataset for the task of false arrhythmia alarm reduction, and present performance of multiple machine learning approaches, including conventional supervised machine learning, deep learning, contrastive learning and generative approaches for the task of VT false alarm reduction.",Datasets & Benchmarks,NeurIPS,2023,Poster,Li-wei H. Lehman;Benjamin E Moody;Harsh Deep;Feng Wu;Hasan Saeed;Lucas McCullum;Diane Perry;Tristan Struja;Qiao Li;Gari Clifford;Roger Mark,True,https://openreview.net/pdf?id=qf4CWnrvZa
qi0Zrm6E5E,Framework and Benchmarks for Combinatorial and Mixed-variable Bayesian Optimization,"This paper introduces a modular framework for Mixed-variable and Combinatorial Bayesian Optimization (MCBO) to address the lack of systematic benchmarking and standardized evaluation in the field. Current MCBO papers often introduce non-diverse or non-standard benchmarks to evaluate their methods, impeding the proper assessment of different MCBO primitives and their combinations. Additionally,  papers introducing a solution for a single MCBO primitive often omit benchmarking against baselines that utilize the same methods for the remaining primitives. This omission is primarily due to the significant implementation overhead involved, resulting in a lack of controlled assessments and an inability to showcase the merits of a contribution effectively.
To overcome these challenges, our proposed framework enables an effortless combination of Bayesian Optimization components, and provides a diverse set of synthetic and real-world benchmarking tasks. 
Leveraging this flexibility, we implement 47 novel MCBO algorithms and benchmark them against seven existing MCBO solvers and five standard black-box optimization algorithms on ten tasks, conducting over 4000 experiments. 
Our findings reveal a superior combination of MCBO primitives outperforming existing approaches and illustrate the significance of model fit and the use of a trust region. We make our MCBO library available under the MIT license at \\\\url{https://github.com/huawei-noah/HEBO/tree/master/MCBO}.",Datasets & Benchmarks,NeurIPS,2023,Poster,Kamil Dreczkowski;Antoine Grosnit;Haitham Bou Ammar,True,https://openreview.net/pdf?id=qi0Zrm6E5E
qkhpbRNSSE,ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics,"We introduce ProofNet, a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. The problems are primarily drawn from popular undergraduate pure mathematics textbooks and cover topics such as real and complex analysis, linear algebra, abstract algebra, and topology. We intend for ProofNet to be a challenging benchmark that will drive progress in autoformalization and automatic theorem proving.  We report baseline results on statement autoformalization via in-context learning. Moreover we demonstrate improvements over our baselines by applying prompt retrieval and distilled backtranslation.",Datasets & Benchmarks,NeurIPS,2023,Reject,Zhangir Azerbayev;Bartosz Piotrowski;Hailey Schoelkopf;Edward W. Ayers;Dragomir Radev;Jeremy Avigad,True,https://openreview.net/pdf?id=qkhpbRNSSE
qmCxdPkNsa,COOM: A Game Benchmark for Continual Reinforcement Learning,"The advancement of continual reinforcement learning (RL) has been facing various obstacles, including standardized metrics and evaluation protocols, demanding computational requirements, and a lack of widely accepted standard benchmarks. In response to these challenges, we present COOM ($\\\\textbf{C}$ontinual D$\\\\textbf{OOM}$), a continual RL benchmark tailored for embodied pixel-based RL. COOM presents a meticulously crafted suite of task sequences set within visually distinct 3D environments, serving as a robust evaluation framework to assess crucial aspects of continual RL, such as catastrophic forgetting, knowledge transfer, and sample-efficient learning. Following an in-depth empirical evaluation of popular continual learning (CL) methods, we pinpoint their limitations, provide valuable insight into the benchmark and highlight unique algorithmic challenges. This makes our work the first to benchmark image-based CRL in 3D environments with embodied perception. The primary objective of the COOM benchmark is to offer the research community a valuable and cost-effective challenge. It seeks to deepen our comprehension of the capabilities and limitations of current and forthcoming CL methods in an RL setting. The code and environments are open-sourced and accessible on GitHub.",Datasets & Benchmarks,NeurIPS,2023,Poster,Tristan Tomilin;Meng Fang;Yudi Zhang;Mykola Pechenizkiy,True,https://openreview.net/pdf?id=qmCxdPkNsa
qynH28Y4xE,Wyze Rule: Federated Rule Dataset for Rule Recommendation Benchmarking,"In the rapidly evolving landscape of smart home automation, the potential of IoT devices is vast. In this realm, rules are the main tool utilized for this automation, which are predefined conditions or triggers that establish connections between devices, enabling seamless automation of specific processes. However, one significant challenge researchers face is the lack of comprehensive datasets to explore and advance the field of smart home rule recommendations. These datasets are essential for developing and evaluating intelligent algorithms that can effectively recommend rules for automating processes while preserving the privacy of the users, as it involves personal information about users' daily lives. To bridge this gap, we present the Wyze Rule Dataset, a large-scale dataset designed specifically for smart home rule recommendation research. Wyze Rule encompasses over 1 million rules gathered from a diverse user base of 300,000 individuals from Wyze Labs, offering an extensive and varied collection of real-world data.   With a focus on federated learning, our dataset is tailored to address the unique challenges of a cross-device federated learning setting in the recommendation domain, featuring a large-scale number of clients with widely heterogeneous data. To establish a benchmark for comparison and evaluation, we have meticulously implemented multiple baselines in both centralized and federated settings. Researchers can leverage these baselines to gauge the performance and effectiveness of their rule recommendation systems, driving advancements in the domain. The Wyze Rule Dataset is publicly accessible through [HuggingFace](https://huggingface.co/datasets/wyzelabs/RuleRecommendation)'s dataset API.",Datasets & Benchmarks,NeurIPS,2023,Poster,Mohammad Mahdi Kamani;Yuhang Yao;Hanjia Lyu;Zhongwei Cheng;Lin Chen;Liangju Li;Carlee Joe-Wong;Jiebo Luo,True,https://openreview.net/pdf?id=qynH28Y4xE
r30thTMcaM,The Cambridge Law Corpus: A Dataset for Legal AI Research,"We introduce the Cambridge Law Corpus (CLC), a dataset for legal AI research. It consists of over 250 000 court cases from the UK. Most cases are from the 21st century, but the corpus includes cases as old as the 16th century. This paper presents the first release of the corpus, containing the raw text and meta-data. Together with the corpus, we provide annotations on case outcomes for 638 cases, done by legal experts. Using our annotated data, we have trained and evaluated case outcome extraction with GPT-3, GPT-4 and RoBERTa models to provide benchmarks. We include an extensive legal and ethical discussion to address the potentially sensitive nature of this material. As a consequence, the corpus will only be released for research purposes under certain restrictions.",Datasets & Benchmarks,NeurIPS,2023,Poster,Andreas Östling;Holli Sargeant;Huiyuan Xie;Ludwig Konrad Bull;Alexander Terenin;Leif Jonsson;Måns Magnusson;Felix Steffek,True,https://openreview.net/pdf?id=r30thTMcaM
rJc5Lsn5QU,ARTIC3D: Learning Robust Articulated 3D Shapes from Noisy Web Image Collections,"Estimating 3D articulated shapes like animal bodies from monocular images is inherently challenging due to the ambiguities of camera viewpoint, pose, texture, lighting, etc. We propose ARTIC3D, a self-supervised framework to reconstruct per-instance 3D shapes from a sparse image collection in-the-wild. Specifically, ARTIC3D is built upon a skeleton-based surface representation and is further guided by 2D diffusion priors from Stable Diffusion. First, we enhance the input images with occlusions/truncation via 2D diffusion to obtain cleaner mask estimates and semantic features. Second, we perform diffusion-guided 3D optimization to estimate shape and texture that are of high-fidelity and faithful to input images. We also propose a novel technique to calculate more stable image-level gradients via diffusion models compared to existing alternatives. Finally, we produce realistic animations by fine-tuning the rendered shape and texture under rigid part transformations. Extensive evaluations on multiple existing datasets as well as newly introduced noisy web image collections with occlusions and truncation demonstrate that ARTIC3D outputs are more robust to noisy images, higher quality in terms of shape and texture details, and more realistic when animated.",main,NeurIPS,2023,Poster,Chun-Han Yao;Amit Raj;Wei-Chih Hung;Michael Rubinstein;Yuanzhen Li;Ming-Hsuan Yang;Varun Jampani,True,https://openreview.net/pdf?id=rJc5Lsn5QU
rR1c6rzXHa,RD-Suite: A Benchmark for Ranking Distillation,"The distillation of ranking models has become an important topic in both academia and industry. In recent years, several advanced methods have been proposed to tackle this problem, often leveraging ranking information from teacher rankers that is absent in traditional classification settings. To date, there is no well-established consensus on how to evaluate this class of models. Moreover, inconsistent benchmarking on a wide range of tasks and datasets make it difficult to assess or invigorate advances in this field. This paper first examines representative prior arts on ranking distillation, and raises three questions to be answered around methodology and reproducibility. To that end, we propose a systematic and unified benchmark, Ranking Distillation Suite (RD-Suite), which is a suite of tasks with 4 large real-world datasets, encompassing two major modalities (textual and numeric) and two applications (standard distillation and distillation transfer). RD-Suite consists of benchmark results that challenge some of the common wisdom in the field, and the release of datasets with teacher scores and evaluation scripts for future research. RD-Suite paves the way towards better understanding of ranking distillation, facilities more research in this direction, and presents new challenges.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zhen Qin;Rolf Jagerman;Rama Kumar Pasumarthi;Honglei Zhuang;He Zhang;Aijun Bai;Kai Hui;Le Yan;Xuanhui Wang,True,https://openreview.net/pdf?id=rR1c6rzXHa
rXi13M3PKc,OceanBench: The Sea Surface Height Edition,"The ocean is a crucial component of the Earth's system. 
It profoundly influences human activities and plays a critical role in climate regulation. 
Our understanding has significantly improved over the last decades with the advent of satellite remote sensing data, allowing us to capture essential sea surface quantities over the globe, e.g., sea surface height (SSH). 
Despite their ever-increasing abundance, ocean satellite data presents challenges for information extraction due to their sparsity and irregular sampling, signal complexity, and noise. 
Machine learning (ML) techniques have demonstrated their capabilities in dealing with large-scale, complex signals. 
Therefore we see an opportunity for these ML models to harness the full extent of the information contained in ocean satellite data. 
However, data representation and relevant evaluation metrics can be the defining factors when determining the success of applied ML. 
The processing steps from the raw observation data to a ML-ready state and from model outputs to interpretable quantities require domain expertise, which can be a significant barrier to entry for ML researchers. 
In addition, imposing fixed processing steps, like committing to specific variables, regions, and geometries, will narrow the scope of ML models and their potential impact on real-world applications. 
OceanBench is a unifying framework that provides standardized processing steps that comply with domain-expert standards. 
It is designed with a flexible and pedagogical abstraction: it a) provides plug-and-play data and pre-configured pipelines for ML researchers to benchmark their models w.r.t. ML and domain-related baselines and b) provides a transparent and configurable framework for researchers to customize and extend the pipeline for their tasks. 
In this work, we demonstrate the OceanBench framework through a first edition dedicated to SSH interpolation challenges. 
We provide datasets and ML-ready benchmarking pipelines for the long-standing problem of interpolating observations from simulated ocean satellite data, multi-modal and multi-sensor fusion issues, and transfer-learning to real ocean satellite observations. 
The  OceanBench framework is available at https://github.com/jejjohnson/oceanbench and the dataset registry is available at https://github.com/quentinf00/oceanbench-data-registry.",Datasets & Benchmarks,NeurIPS,2023,Poster,Juan Emmanuel Johnson;Quentin Febvre;Anastasia Gorbunova;Sammy Metref;Maxime Ballarotta;Julien Le Sommer;Ronan Fablet,True,https://openreview.net/pdf?id=rXi13M3PKc
s6qtLyR6uJ,NeuroEvoBench: Benchmarking Evolutionary Optimizers for Deep Learning Applications,"Recently, the Deep Learning community has become interested in evolutionary optimization (EO) as a means to address hard optimization problems, e.g. meta-learning through long inner loop unrolls or optimizing non-differentiable operators. One core reason for this trend has been the recent innovation in hardware acceleration and compatible software -- making distributed population evaluations much easier than before. Unlike for gradient descent-based methods though, there is a lack of hyperparameter understanding and best practices for EO – arguably due to severely less `graduate student descent' and benchmarking being performed for EO methods. Additionally, classical benchmarks from the evolutionary community provide few practical insights for Deep Learning applications. This poses challenges for newcomers to hardware-accelerated EO and hinders significant adoption. Hence, we establish a new benchmark of EO methods (NEB) tailored toward Deep Learning applications and exhaustively evaluate traditional and meta-learned EO. We investigate core scientific questions including resource allocation, fitness shaping, normalization, regularization & scalability of EO. The benchmark is open-sourced at https://github.com/neuroevobench/neuroevobench under Apache-2.0 license.",Datasets & Benchmarks,NeurIPS,2023,Poster,Robert Tjarko Lange;Yujin Tang;Yingtao Tian,True,https://openreview.net/pdf?id=s6qtLyR6uJ
scYa9DYUAy,VAST: A Vision-Audio-Subtitle-Text Omni-Modality Foundation Model and Dataset,"Vision and text have been fully explored  in contemporary video-text foundational models, while other modalities such as audio and subtitles in videos have not received sufficient attention. In this paper, we resort to establish connections between multi-modality video tracks, including Vision, Audio, and Subtitle, and Text by exploring an automatically generated large-scale omni-modality video caption dataset called VAST-27M. Specifically, we first collect 27 million open-domain video clips and separately train a vision and an audio captioner to generate vision and audio captions. Then, we employ an off-the-shelf Large Language Model (LLM) to integrate the generated captions, together with subtitles and instructional prompts into omni-modality captions. Based on the proposed VAST-27M dataset, we train an omni-modality video-text foundational model named VAST, which can perceive and process vision, audio, and subtitle modalities from video, and better support various tasks including  vision-text, audio-text, and multi-modal video-text tasks (retrieval, captioning and QA). Extensive experiments have been conducted to demonstrate the effectiveness of our proposed VAST-27M corpus and VAST foundation model. VAST achieves 22 new state-of-the-art results on various cross-modality benchmarks.",main,NeurIPS,2023,Poster,Sihan Chen;Handong Li;Qunbo Wang;Zijia Zhao;Mingzhen Sun;Xinxin Zhu;Jing Liu,True,https://openreview.net/pdf?id=scYa9DYUAy
snY3FOnlQi,AV-NeRF: Learning Neural Fields for Real-World Audio-Visual Scene Synthesis,"Can machines recording an audio-visual scene produce realistic, matching audio-visual experiences at novel positions and novel view directions? We answer it by studying a new task---real-world audio-visual scene synthesis---and a first-of-its-kind NeRF-based approach for multimodal learning. Concretely, given a video recording of an audio-visual scene, the task is to synthesize new videos with spatial audios along arbitrary novel camera trajectories in that scene. We propose an acoustic-aware audio generation module that integrates prior knowledge of audio propagation into NeRF, in which we implicitly associate audio generation with the 3D geometry and material properties of a visual environment. Furthermore, we present a coordinate transformation module that expresses a view direction relative to the sound source, enabling the model to learn sound source-centric acoustic fields. To facilitate the study of this new task, we collect a high-quality Real-World Audio-Visual Scene (RWAVS) dataset. We demonstrate the advantages of our method on this real-world dataset and the simulation-based SoundSpaces dataset. Notably, we refer readers to view our demo videos for convincing comparisons.",main,NeurIPS,2023,Poster,Susan Liang;Chao Huang;Yapeng Tian;Anurag Kumar;Chenliang Xu,True,https://openreview.net/pdf?id=snY3FOnlQi
tIW4kbnJIM,NurViD: A Large Expert-Level Video Database for Nursing Procedure Activity Understanding,"The application of deep learning to nursing procedure activity understanding has the potential to greatly enhance the quality and safety of nurse-patient interactions. By utilizing the technique, we can facilitate training and education, improve quality control, and enable operational compliance monitoring. However, the development of automatic recognition systems in this field is currently hindered by the scarcity of appropriately labeled datasets. The existing video datasets pose several limitations: 1) these datasets are small-scale in size to support comprehensive investigations of nursing activity; 2) they primarily focus on single procedures, lacking expert-level annotations for various nursing procedures and action steps; and 3) they lack temporally localized annotations, which prevents the effective localization of targeted actions within longer video sequences. To mitigate these limitations, we propose NurViD, a large video dataset with expert-level annotation for nursing procedure activity understanding. NurViD consists of over 1.5k videos totaling 144 hours, making it approximately four times longer than the existing largest nursing activity datasets. Notably, it encompasses 51 distinct nursing procedures and 177 action steps, providing a much more comprehensive coverage compared to existing datasets that primarily focus on limited procedures. To evaluate the efficacy of current deep learning methods on nursing activity understanding, we establish three benchmarks on NurViD: procedure recognition on untrimmed videos, procedure and action recognition on trimmed videos, and action detection. Our benchmark and code will be available at https://github.com/minghu0830/NurViD-benchmark.",Datasets & Benchmarks,NeurIPS,2023,Poster,Ming Hu;Lin Wang;Siyuan Yan;Don Ma;Qingli Ren;Peng Xia;Wei Feng;Peibo Duan;Lie Ju;Zongyuan Ge,True,https://openreview.net/pdf?id=tIW4kbnJIM
tOd8rSjcWz,"Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text","In-context vision and language models like Flamingo support arbitrarily interleaved sequences of images and text as input.
This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g., ``What do image A and image B have in common?''
To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text.
To date, however, large-scale data of this form have not been publicly available.


We release Multimodal C4, an augmentation of the popular text-only C4 corpus with images interleaved.
We use a linear assignment algorithm to place images into longer bodies of text using CLIP features, a process that we show outperforms alternatives.
Multimodal C4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (88\\\\%) of images are topically relevant, and that linear assignment frequently selects individual sentences specifically well-aligned with each image (80\\\\%). 
After filtering NSFW images, ads, etc., the resulting corpus consists of 101.2M documents with 571M images interleaved in 43B English tokens.",Datasets & Benchmarks,NeurIPS,2023,Poster,Wanrong Zhu;Jack Hessel;Anas Awadalla;Samir Yitzhak Gadre;Jesse Dodge;Alex Fang;Youngjae Yu;Ludwig Schmidt;William Yang Wang;Yejin Choi,True,https://openreview.net/pdf?id=tOd8rSjcWz
tScBQRNgjk,ForecastPFN: Synthetically-Trained Zero-Shot Forecasting,"The vast majority of time-series forecasting approaches require a substantial training dataset. However, many real-life forecasting applications have very little initial observations, sometimes just 40 or fewer. Thus, the applicability of most forecasting methods is restricted in data-sparse commercial applications. While there is recent work in the setting of very limited initial data (so-called `zero-shot' forecasting), its performance is inconsistent depending on the data used for pretraining. In this work, we take a different approach and devise ForecastPFN, the first zero-shot forecasting model trained purely on a novel synthetic data distribution. ForecastPFN is a prior-data fitted network, trained to approximate Bayesian inference, which can make predictions on a new time series dataset in a single forward pass. Through extensive experiments, we show that zero-shot predictions made by ForecastPFN are more accurate and faster compared to state-of-the-art forecasting methods, even when the other methods are allowed to train on hundreds of additional in-distribution data points.",main,NeurIPS,2023,Poster,Samuel Dooley;Gurnoor Singh Khurana;Chirag Mohapatra;Siddartha Venkat Naidu;Colin White,True,https://openreview.net/pdf?id=tScBQRNgjk
tfyr2zRVoK,SheetCopilot: Bringing Software Productivity to the Next Level through Large Language Models,"Computer end users have spent billions of hours completing daily tasks like tabular data processing and project timeline scheduling. Most of these tasks are repetitive and error-prone, yet most end users lack the skill to automate these burdensome works. With the advent of large language models (LLMs), directing software with natural language user requests become a reachable goal. In this work, we propose a SheetCopilot agent that takes natural language task and control spreadsheet to fulfill the requirements. We propose a set of atomic actions as an abstraction of spreadsheet software functionalities. We further design a state machine-based task planning framework for LLMs to robustly interact with spreadsheets. We curate a representative dataset containing 221 spreadsheet control tasks and establish a fully automated evaluation pipeline for rigorously benchmarking the ability of LLMs in software control tasks. Our SheetCopilot correctly completes 44.3\\\\% of tasks for a single generation, outperforming the strong code generation baseline by a wide margin. Our project page: https://sheetcopilot.github.io/.",main,NeurIPS,2023,Poster,Hongxin Li;Jingran Su;Yuntao Chen;Qing Li;Zhaoxiang Zhang,True,https://openreview.net/pdf?id=tfyr2zRVoK
thPI8hrA4V,GlyphControl: Glyph Conditional Control for Visual Text Generation,"Recently, there has been an increasing interest in developing diffusion-based text-to-image generative models capable of generating coherent and well-formed visual text. In this paper, we propose a novel and efficient approach called GlyphControl to address this task. Unlike existing methods that rely on character-aware text encoders like ByT5 and require retraining of text-to-image models, our approach leverages additional glyph conditional information to enhance the performance of the off-the-shelf Stable-Diffusion model in generating accurate visual text. By incorporating glyph instructions, users can customize the content, location, and size of the generated text according to their specific requirements. To facilitate further research in visual text generation, we construct a training benchmark dataset called LAION-Glyph. We evaluate the effectiveness of our approach by measuring OCR-based metrics, CLIP score, and FID of the generated visual text. Our empirical evaluations demonstrate that GlyphControl outperforms the recent DeepFloyd IF approach in terms of OCR accuracy, CLIP score, and FID, highlighting the efficacy of our method.",main,NeurIPS,2023,Poster,Yukang Yang;Dongnan Gui;Yuhui Yuan;Weicong Liang;Haisong Ding;Han Hu;Kai Chen,True,https://openreview.net/pdf?id=thPI8hrA4V
tk27oD2cBw,"The Harvard USPTO Patent Dataset: A Large-Scale, Well-Structured, and Multi-Purpose Corpus of Patent Applications","Innovation is a major driver of economic and social development, and information about many kinds of innovation is embedded in semi-structured data from patents and patent applications. Though the impact and novelty of innovations expressed in patent data are difficult to measure through traditional means, machine learning offers a promising set of techniques for evaluating novelty, summarizing contributions, and embedding semantics. In this paper, we introduce the Harvard USPTO Patent Dataset (HUPD), a large-scale, well-structured, and multi-purpose corpus of English-language patent applications filed to the United States Patent and Trademark Office (USPTO) between 2004 and 2018. With more than 4.5 million patent documents, HUPD is two to three times larger than comparable corpora. Unlike other NLP patent datasets, HUPD contains the inventor-submitted versions of patent applications, not the final versions of granted patents, allowing us to study patentability at the time of filing using NLP methods for the first time. It is also novel in its inclusion of rich structured data alongside the text of patent filings: By providing each application’s metadata along with all of its text fields, HUPD enables researchers to perform new sets of NLP tasks that leverage variation in structured covariates. As a case study on the types of research HUPD makes possible, we introduce a new task to the NLP community -- patent acceptance prediction. We additionally show the structured metadata provided in HUPD allows us to conduct explicit studies of concept shifts for this task. We find that performance on patent acceptance prediction decays when models trained in one context are evaluated on different innovation categories and over time. Finally, we demonstrate how HUPD can be used for three additional tasks: Multi-class classification of patent subject areas, language modeling, and abstractive summarization. Put together, our publicly-available dataset aims to advance research extending language and classification models to diverse and dynamic real-world data distributions.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Mirac Suzgun;Luke Melas-Kyriazi;Suproteem K Sarkar;Scott Kominers;Stuart Shieber,True,https://openreview.net/pdf?id=tk27oD2cBw
tp2nEZ5zfP,NetHack is Hard to Hack,"Neural policy learning methods have achieved remarkable results in various control problems, ranging from Atari games to simulated locomotion. However, these methods struggle in long-horizon tasks, especially in open-ended environments with multi-modal observations, such as the popular dungeon-crawler game, NetHack. Intriguingly, the NeurIPS 2021 NetHack Challenge revealed that symbolic agents outperformed neural approaches by over four times in median game score. In this paper, we delve into the reasons behind this performance gap and present an extensive study on neural policy learning for NetHack. To conduct this study, we analyze the winning symbolic agent, extending its codebase to track internal strategy selection in order to generate one of the largest available demonstration datasets. Utilizing this dataset, we examine (i) the advantages of an action hierarchy; (ii) enhancements in neural architecture; and (iii) the integration of reinforcement learning with imitation learning. Our investigations produce a state-of-the-art neural agent that surpasses previous fully neural policies by 127% in offline settings and 25% in online settings on median game score. However, we also demonstrate that mere scaling is insufficient to bridge the performance gap with the best symbolic models or even the top human players.",main,NeurIPS,2023,Poster,Ulyana Piterbarg;Lerrel Pinto;Rob Fergus,True,https://openreview.net/pdf?id=tp2nEZ5zfP
txPdKZrrZF,Fed-FA: Theoretically Modeling Client Data Divergence for Federated Language Backdoor Defense,"Federated learning algorithms enable neural network models to be trained across multiple decentralized edge devices without sharing private data. However, they are susceptible to backdoor attacks launched by malicious clients. Existing robust federated aggregation algorithms heuristically detect and exclude suspicious clients based on their parameter distances, but they are ineffective on Natural Language Processing (NLP) tasks. The main reason is that, although text backdoor patterns are obvious at the underlying dataset level, they are usually hidden at the parameter level, since injecting backdoors into texts with discrete feature space has less impact on the statistics of the model parameters. To settle this issue, we propose to identify backdoor clients by explicitly modeling the data divergence among clients in federated NLP systems. Through theoretical analysis, we derive the f-divergence indicator to estimate the client data divergence with aggregation updates and Hessians. Furthermore, we devise a dataset synthesization method with a Hessian reassignment mechanism guided by the diffusion theory to address the key challenge of inaccessible datasets in calculating clients' data Hessians.
We then present the novel Federated F-Divergence-Based Aggregation~(\\\\textbf{Fed-FA}) algorithm, which leverages the f-divergence indicator to detect and discard suspicious clients. Extensive empirical results show that Fed-FA outperforms all the parameter distance-based methods in defending against backdoor attacks among various natural language backdoor attack scenarios.",main,NeurIPS,2023,Poster,Zhiyuan Zhang;Deli Chen;Hao Zhou;Fandong Meng;Jie Zhou;Xu Sun,True,https://openreview.net/pdf?id=txPdKZrrZF
tz7XkY6S9Z,Mr. HiSum: A Large-scale Dataset for Video Highlight Detection and Summarization,"Video highlight detection is a task to automatically select the most engaging moments from a long video. This problem is highly challenging since it aims to learn a general way of finding highlights from a variety of videos in the real world.The task has an innate subjectivity because the definition of a highlight differs across individuals. Therefore, to detect consistent and meaningful highlights, prior benchmark datasets have been labeled by multiple (5-20) raters. Due to the high cost of manual labeling, most existing public benchmarks are in extremely small scale, containing only a few tens or hundreds of videos. This insufficient benchmark scale causes multiple issues such as unstable evaluation or high sensitivity in traintest splits. We present Mr. HiSum, a large-scale dataset for video highlight detection and summarization, containing 31,892 videos and reliable labels aggregated over 50,000+ users per video. We empirically prove reliability of the labels as frame importance by cross-dataset transfer and user study.",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=tz7XkY6S9Z
u2cXRGm95Y,UUKG: Unified Urban Knowledge Graph Dataset for Urban Spatiotemporal Prediction,"Accurate Urban SpatioTemporal Prediction (USTP) is of great importance to the development and operation of the smart city. As an emerging building block, multi-sourced urban data are usually integrated as urban knowledge graphs (UrbanKGs) to provide critical knowledge for urban spatiotemporal prediction models. However, existing UrbanKGs are often tailored for specific downstream prediction tasks and are not publicly available, which limits the potential advancement. This paper presents UUKG, the unified urban knowledge graph dataset for knowledge-enhanced urban spatiotemporal predictions. Specifically, we first construct UrbanKGs consisting of millions of triplets for two metropolises by connecting heterogeneous urban entities such as administrative boroughs, POIs, and road segments. 
Moreover, we conduct qualitative and quantitative analysis on constructed UrbanKGs and uncover diverse high-order structural patterns, such as hierarchies and cycles, that can be leveraged to benefit downstream USTP tasks. To validate and facilitate the use of UrbanKGs, we implement and evaluate 15 KG embedding methods on the KG completion task and integrate the learned KG embeddings into 9 spatiotemporal models for five different USTP tasks. The extensive experimental results not only provide benchmarks of knowledge-enhanced USTP models under different task settings but also highlight the potential of state-of-the-art high-order structure-aware UrbanKG embedding methods. We hope the proposed UUKG fosters research on urban knowledge graphs and broad smart city applications. The dataset and source code are available at https://github.com/usail-hkust/UUKG/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yansong Ning;Hao Liu;Hao Henry Wang;Zhenyu Zeng;Hui Xiong,True,https://openreview.net/pdf?id=u2cXRGm95Y
uHlKNCDAJb,ScenarioNet: Open-Source Platform for Large-Scale Traffic Scenario Simulation and Modeling,"Large-scale driving datasets such as Waymo Open Dataset and nuScenes substantially accelerate autonomous driving research, especially for perception tasks such as 3D detection and trajectory forecasting. Since the driving logs in these datasets contain HD maps and detailed object annotations which accurately reflect the real-world complexity of traffic behaviors, we can harvest a massive number of complex traffic scenarios and recreate their digital twins in simulation. Compared to the hand-crafted scenarios often used in existing simulators, data-driven scenarios collected from the real world can facilitate many research opportunities in machine learning and autonomous driving. In this work, we present ScenarioNet, an open-source platform for large-scale traffic scenario modeling and simulation. ScenarioNet defines a unified scenario description format and collects a large-scale repository of real-world traffic scenarios from the heterogeneous data in various driving datasets including Waymo, nuScenes, Lyft L5, and nuPlan datasets. These scenarios can be further replayed and interacted with in multiple views from Bird-Eye-View layout to realistic 3D rendering in MetaDrive simulator. This provides a benchmark for evaluating the safety of autonomous driving stacks in simulation before their real-world deployment. We further demonstrate the strengths of ScenarioNet on large-scale scenario generation, imitation learning, and reinforcement learning in both single-agent and multi-agent settings. Code, demo videos, and website are available at https://github.com/metadriverse/scenarionet",Datasets & Benchmarks,NeurIPS,2023,Poster,Quanyi Li;Zhenghao Peng;Lan Feng;Zhizheng Liu;Chenda Duan;Wenjie Mo;Bolei Zhou,False,https://openreview.net/pdf?id=uHlKNCDAJb
uIj1jDc8k6,Building Socio-culturally Inclusive Stereotype Resources with Community Engagement,"With rapid development and deployment of generative language models in global settings, there is an urgent need to also scale our measurements of harm, not just in the number and types of harms covered, but also how well they account for local cultural contexts, including marginalized identities and the social biases experienced by them.
Current evaluation paradigms are limited in their abilities to address this, as they are not representative of diverse, locally situated but global, socio-cultural perspectives. It is imperative that our evaluation resources are enhanced and calibrated by including people and experiences from different cultures and societies worldwide, in order to prevent gross underestimations or skews in measurements of harm. In this work, we demonstrate a socio-culturally aware expansion of evaluation resources in the Indian societal context, specifically for the harm of stereotyping. We devise a community engaged effort to build a resource which contains stereotypes for axes of disparity that are uniquely present in India. The resultant resource increases the number of stereotypes known for and in the Indian context by over 1000 stereotypes across many unique identities. We also demonstrate the utility and effectiveness of such expanded resources for evaluations of language models.
CONTENT WARNING: This paper contains examples of stereotypes that may be offensive.",Datasets & Benchmarks,NeurIPS,2023,Poster,Sunipa Dev;Jaya Goyal;Dinesh Tewari;Shachi Dave;Vinodkumar Prabhakaran,True,https://openreview.net/pdf?id=uIj1jDc8k6
uIppiU2JKP,Synthcity: a benchmark framework for diverse use cases of tabular synthetic data,"Accessible high-quality data is the bread and butter of machine learning research, and the demand for data has exploded as larger and more advanced ML models are built across different domains. Yet, real data often contain sensitive information, are subject to various biases, and are costly to acquire, which compromise their quality and accessibility. Synthetic data have thus emerged as a complement to, sometimes even a replacement for, real data for ML training. However, the landscape of synthetic data research has been fragmented due to the diverse range of data modalities, such as tabular, time series, and images, and the wide array of use cases, including privacy preservation, fairness considerations, and data augmentation. This fragmentation poses practical challenges when comparing and selecting synthetic data generators in for different problem settings. To this end, we develop Synthcity, an open-source Python library that allows researchers and practitioners to perform one-click benchmarking of synthetic data generators across data modalities and use cases. Beyond benchmarking, Synthcity serves as a centralized toolkit for accessing cutting-edge data generators. In addition, Synthcity’s flexible plug-in style API makes it easy to incorporate additional data generators into the framework. Using examples of tabular data generation and data augmentation, we illustrate the general applicability of Synthcity, and the insight one can obtain.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zhaozhi Qian;Rob Davis;Mihaela van der Schaar,False,https://openreview.net/pdf?id=uIppiU2JKP
uJT68uPtC0,M5HisDoc: A Large-scale Multi-style Chinese Historical Document Analysis Benchmark,"Recognizing and organizing text in correct reading order plays a crucial role in historical document analysis and preservation. While existing methods have shown promising performance, they often struggle with challenges such as diverse layouts, low image quality, style variations, and distortions. This is primarily due to the lack of consideration for these issues in the current benchmarks, which hinders the development and evaluation of historical document analysis and recognition (HDAR) methods in complex real-world scenarios. To address this gap, this paper introduces a complex multi-style Chinese historical document analysis benchmark, named M5HisDoc. The M5 indicates five properties of style, ie., Multiple layouts, Multiple document types, Multiple calligraphy styles, Multiple backgrounds, and Multiple challenges. The M5HisDoc dataset consists of two subsets, M5HisDoc-R (Regular) and M5HisDoc-H (Hard). The M5HisDoc-R subset comprises 4,000 historical document images. To ensure high-quality annotations, we meticulously perform manual annotation and triple-checking. To replicate real-world conditions for historical document analysis applications, we incorporate image rotation, distortion, and resolution reduction into M5HisDoc-R subset to form a new challenging subset named M5HisDoc-H, which contains the same number of images as M5HisDoc-R. The dataset exhibits diverse styles, significant scale variations, dense texts, and an extensive character set. We conduct benchmarking experiments on five tasks: text line detection, text line recognition, character detection, character recognition, and reading order prediction. We also conduct cross-validation with other benchmarks. Experimental results demonstrate that the M5HisDoc dataset can offer new challenges and great opportunities for future research in this field, thereby providing deep insights into the solution for HDAR. The dataset is available at https://github.com/HCIILAB/M5HisDoc.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yongxin Shi;Chongyu Liu;Dezhi Peng;Cheng Jian;Jiarong Huang;Lianwen Jin,True,https://openreview.net/pdf?id=uJT68uPtC0
uXBO47JcJT,Amazon-M2: A Multilingual Multi-locale Shopping Session Dataset for Recommendation and Text Generation,"Modeling customer shopping intentions is a crucial task for e-commerce, as it directly impacts user experience and engagement. Thus,  accurately understanding customer preferences is essential for providing personalized recommendations. Session-based recommendation, which utilizes customer session data to predict their next interaction, has become increasingly popular. 
However, existing session datasets have limitations in terms of item attributes, user diversity, and dataset scale. As a result, they cannot comprehensively capture the spectrum of user behaviors and preferences.
To bridge this gap, we present the Amazon Multilingual Multi-locale Shopping Session Dataset, namely Amazon-M2. It is the first multilingual dataset consisting of millions of user sessions from six different locales, where the major languages of products are English, German, Japanese, French, Italian, and Spanish.
Remarkably, the dataset can help us enhance personalization and understanding of user preferences, which can benefit various existing tasks as well as enable new tasks. To test the potential of the dataset, we introduce three tasks in this work:
(1) next-product recommendation, (2) next-product recommendation with domain shifts, and (3) next-product title generation.
With the above tasks, we benchmark a range of algorithms on our proposed dataset, drawing new insights for further research and practice. 
In addition, based on the proposed dataset and tasks, we hosted a competition in the KDD CUP 2023 https://www.aicrowd.com/challenges/amazon-kdd-cup-23-multilingual-recommendation-challenge and have attracted thousands of users and submissions. The winning solutions and the associated workshop can be accessed at our website~https://kddcup23.github.io/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Wei Jin;Haitao Mao;Zheng Li;Haoming Jiang;Chen Luo;Hongzhi Wen;Haoyu Han;Hanqing Lu;Zhengyang Wang;Ruirui Li;Zhen Li;Monica Xiao Cheng;Rahul Goutam;Haiyang Zhang;Karthik Subbian;Suhang Wang;Yizhou Sun;Jiliang Tang;Bing Yin;Xianfeng Tang,True,https://openreview.net/pdf?id=uXBO47JcJT
uZedGmxGUg,EasyTPP: Towards Open Benchmarking the Temporal Point Processes,"Continuous-time event sequences play a vital role in real-world domains such as healthcare, finance, online shopping, social networks, and so on. To model such data, temporal point processes (TPPs) have emerged as the most natural and competitive models, making a significant impact in both academic and application communities. Despite the emergence of many powerful models in recent years, there hasn't been a central benchmark for these models and future research endeavors. This lack of standardization impedes researchers and practitioners from comparing methods and reproducing results, potentially slowing down progress in this field. In this paper, we present EasyTPP, the first central repository of research assets (e.g., data, models, evaluation programs, documentations) in the area of event sequence modeling. Our EasyTPP makes several unique contributions to this area: a unified interface of using existing datasets and adding new datasets; a wide range of evaluation programs that are easy to use and extend as well as facilitate reproducible research; 
implementations of popular neural TPPs, together with a rich library of modules by composing which one could quickly build complex models. Our benchmark is open-sourced: all the data and implementation can be found at this \\\\href{https://github.com/ant-research/EasyTemporalPointProcess}{\\\\textcolor{blue}{Github repository}}.} We will actively maintain this benchmark and welcome contributions from other researchers and practitioners. Our benchmark will help promote reproducible research in this field, thus accelerating research progress as well as making more significant real-world impacts.",Datasets & Benchmarks,NeurIPS,2023,Reject,Siqiao Xue;Xiaoming Shi;Zhixuan Chu;Yan Wang;Hongyan Hao;Caigao JIANG;Chen Pan;James Y. Zhang;Qingsong Wen;JUN ZHOU;Hongyuan Mei,False,https://openreview.net/pdf?id=uZedGmxGUg
uZjpSBTPik,CL-NeRF: Continual Learning of Neural Radiance Fields for Evolving Scene Representation,"Existing methods for adapting Neural Radiance Fields (NeRFs) to scene changes require extensive data capture and model retraining, which is both time-consuming and labor-intensive. In this paper, we tackle the challenge of efficiently adapting NeRFs to real-world scene changes over time using a few new images while retaining the memory of unaltered areas, focusing on the continual learning aspect of NeRFs. To this end, we propose CL-NeRF, which consists of two key components: a lightweight expert adaptor for adapting to new changes and evolving scene representations and a conflict-aware knowledge distillation learning objective for memorizing unchanged parts. We also present a new benchmark for evaluating Continual Learning of NeRFs with comprehensive metrics. Our extensive experiments demonstrate that CL-NeRF can synthesize high-quality novel views of both changed and unchanged regions with  high training efficiency, surpassing existing methods in terms of reducing forgetting and adapting to changes. Code and benchmark will be made available.",main,NeurIPS,2023,Poster,Xiuzhe Wu;Peng Dai;Weipeng DENG;Handi Chen;Yang Wu;Yan-Pei Cao;Ying Shan;XIAOJUAN QI,True,https://openreview.net/pdf?id=uZjpSBTPik
uccHPGDlao,Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena,"Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences.
To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions.
We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them.
We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform.
Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80\\\\% agreement, the same level of agreement between humans.
Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain.
Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna.
The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.",Datasets & Benchmarks,NeurIPS,2023,Poster,Lianmin Zheng;Wei-Lin Chiang;Ying Sheng;Siyuan Zhuang;Zhanghao Wu;Yonghao Zhuang;Zi Lin;Zhuohan Li;Dacheng Li;Eric Xing;Hao Zhang;Joseph E. Gonzalez;Ion Stoica,True,https://openreview.net/pdf?id=uccHPGDlao
ugRnHKMK95,Turbulence in Focus: Benchmarking Scaling Behavior of 3D Volumetric Super-Resolution with BLASTNet 2.0 Data,"Analysis of compressible turbulent flows is essential for applications related to propulsion, energy generation, and the environment. 
Here, we present BLASTNet 2.0, a 2.2 TB network-of-datasets containing 744 full-domain samples from 34 high-fidelity direct numerical simulations, which addresses the current limited availability of 3D high-fidelity reacting and non-reacting compressible turbulent flow simulation data.  With this data, we benchmark a total of 49 variations of five deep learning approaches for 3D super-resolution - which can be applied for improving scientific imaging, simulations, turbulence models, as well as in computer vision  applications. We perform neural scaling analysis on these models to examine the performance of different machine learning (ML) approaches, including two scientific ML techniques. We demonstrate that (i) predictive performance can scale with model size and cost, (ii) architecture matters significantly, especially for smaller models, and (iii) the benefits of physics-based losses can persist with increasing model size. The outcomes of this benchmark study are anticipated to offer insights that can aid the design of 3D super-resolution models, especially for turbulence models, while this data is expected to foster ML methods for a broad range of flow physics applications. This data is publicly available with download links and browsing tools consolidated at https://blastnet.github.io.",Datasets & Benchmarks,NeurIPS,2023,Poster,Wai Tong Chung;Bassem Akoush;Pushan Sharma;Alex Tamkin;Ki Sung Jung;Jacqueline Chen;Jack Guo;Davy Brouzet;Mohsen Talei;Bruno Savard;Alexei Y Poludnenko;Matthias Ihme,True,https://openreview.net/pdf?id=ugRnHKMK95
v4PMCdSaAT,A Performance-Driven Benchmark for Feature Selection in Tabular Deep Learning,"Academic tabular benchmarks often contain small sets of curated features. In contrast, data scientists typically collect as many features as possible into their datasets, and even engineer new features from existing ones. To prevent over-fitting in subsequent downstream modeling, practitioners commonly use automated feature selection methods that identify a reduced subset of informative features. Existing benchmarks for tabular feature selection consider classical downstream models, toy synthetic datasets, or do not evaluate feature selectors on the basis of downstream performance. We construct a challenging feature selection benchmark evaluated on downstream neural networks including transformers, using real datasets and multiple methods for generating extraneous features. We also propose Deep Lasso -- an input-gradient-based analogue of LASSO for neural networks that outperforms classical feature selection methods on challenging problems such as selecting from corrupted or second-order features.",Datasets & Benchmarks,NeurIPS,2023,Poster,Valeriia Cherepanova;Roman Levin;Gowthami Somepalli;Jonas Geiping;C. Bayan Bruss;Andrew Gordon Wilson;Tom Goldstein;Micah Goldblum,False,https://openreview.net/pdf?id=v4PMCdSaAT
vBx0yNQmik,Federated Virtual Learning on Heterogeneous Data with Local-global Distillation,"Despite Federated Learning (FL)'s trend for learning machine learning models in a distributed manner, it is susceptible to performance drops when training on heterogeneous data. In addition, FL inevitability faces the challenges of synchronization, efficiency and privacy. Recently, dataset distillation has been explored in order to improve the efficiency and scalability of FL by creating a smaller, synthetic dataset that retains the performance of a model trained on the local private datasets. We discover that using distilled local datasets can amplify the heterogeneity issue in FL. To address this, we propose a new method, called Federated Virtual Learning on Heterogeneous Data with Local-Global Distillation (FedLGD) which trains FL using a smaller synthetic dataset (referred as virtual data) created through a combination of local and global dataset distillation. Specifically, to handle synchronization and class imbalance, we propose iterative distribution matching to allow clients to have the same amount of balanced local virtual data; to harmonize the domain shifts, we use federated gradient matching to distill global virtual data that are shared with clients without hindering data privacy to rectify heterogeneous local training via enforcing local-global feature similarity. We experiment on both benchmark and real-world datasets that contain heterogeneous data from different sources, and further scale up to an FL scenario that contains large number of clients with heterogeneous and class imbalance data. Our method outperforms state-of-the-art heterogeneous FL algorithms under various settings with a very limited amount of distilled virtual data.",main,NeurIPS,2023,Reject,Chun-Yin Huang;Ruinan Jin;Can Zhao;Daguang Xu;Xiaoxiao Li,True,https://openreview.net/pdf?id=vBx0yNQmik
vNsdFwjPtL,Suggesting Variable Order for Cylindrical Algebraic Decomposition via Reinforcement Learning,"Cylindrical Algebraic Decomposition (CAD) is one of the pillar algorithms of symbolic computation, and its worst-case complexity is double exponential to the number of variables. Researchers found that variable order dramatically affects efficiency and proposed various heuristics. 
The existing learning-based methods are all supervised learning methods that cannot cope with diverse polynomial sets.
This paper proposes two Reinforcement Learning (RL) approaches combined with Graph Neural Networks (GNN) for Suggesting Variable Order (SVO). One is GRL-SVO(UP), a branching heuristic integrated with CAD. The other is GRL-SVO(NUP), a fast heuristic providing a total order directly. We generate a random dataset and collect a real-world dataset from SMT-LIB. The experiments show that our approaches outperform state-of-the-art learning-based heuristics and are competitive with the best expert-based heuristics. Interestingly, our models show a strong generalization ability, working well on various datasets even if they are only trained on a 3-var random dataset. The source code and data are available at https://github.com/dongyuhang22/GRL-SVO.",main,NeurIPS,2023,Poster,Fuqi Jia;Yuhang Dong;Minghao Liu;Pei Huang;Feifei Ma;Jian Zhang,True,https://openreview.net/pdf?id=vNsdFwjPtL
vTrRq6vCQH,"PIXIU: A Comprehensive Benchmark, Instruction Dataset and Large Language Model for Finance","Although large language models (LLMs) have shown great performance in natural language processing (NLP) in the financial domain, there are no publicly available financially tailored LLMs, instruction tuning datasets, and evaluation benchmarks, which is critical for continually pushing forward the open-source development of financial artificial intelligence (AI). This paper introduces PIXIU, a comprehensive framework including the first financial LLM based on fine-tuning LLaMA with instruction data, the first instruction data with 128K data samples to support the fine-tuning, and an evaluation benchmark with 8 tasks and 15 datasets. We first construct the large-scale multi-task instruction data considering a variety of financial tasks, financial document types, and financial data modalities. We then propose a financial LLM called FinMA by fine-tuning LLaMA with the constructed dataset to be able to follow instructions for various financial tasks. To support the evaluation of financial LLMs, we propose a standardized benchmark that covers a set of critical financial tasks, including six financial NLP tasks and two financial prediction tasks. With this benchmark, we conduct a detailed analysis of FinMA and several existing LLMs, uncovering their strengths and weaknesses in handling critical financial tasks. The model, datasets, benchmark, and experimental results are open-sourced to facilitate future research in financial AI.",Datasets & Benchmarks,NeurIPS,2023,Poster,Qianqian Xie;Weiguang Han;Xiao Zhang;Yanzhao Lai;Min Peng;Alejandro Lopez-Lira;Jimin Huang,True,https://openreview.net/pdf?id=vTrRq6vCQH
vZ9tA3o3hr,SustainGym: Reinforcement Learning Environments for Sustainable Energy Systems,"The lack of standardized benchmarks for reinforcement learning (RL) in sustainability applications has made it difficult to both track progress on specific domains and identify bottlenecks for researchers to focus their efforts. In this paper, we present SustainGym, a suite of five environments designed to test the performance of RL algorithms on realistic sustainable energy system tasks, ranging from electric vehicle charging to carbon-aware data center job scheduling. The environments test RL algorithms under realistic distribution shifts as well as in multi-agent settings. We show that standard off-the-shelf RL algorithms leave significant room for improving performance and highlight the challenges ahead for introducing RL to real-world sustainability tasks.",Datasets & Benchmarks,NeurIPS,2023,Poster,Christopher Yeh;Victor Li;Rajeev Datta;Julio Arroyo;Nicolas Christianson;Chi Zhang;Yize Chen;Mohammad Mehdi Hosseini;Azarang Golmohammadi;Yuanyuan Shi;Yisong Yue;Adam Wierman,False,https://openreview.net/pdf?id=vZ9tA3o3hr
vZf7jrX1el,Data-Driven Network Neuroscience: On Data Collection and Benchmark,"This paper presents a comprehensive and quality collection of functional human brain network data for potential research in the intersection of neuroscience, machine learning, and graph analytics. 
Anatomical and functional MRI images have been used to understand the functional connectivity of the human brain and are particularly important in identifying underlying neurodegenerative conditions such as Alzheimer's, Parkinson's, and Autism. Recently, the study of the brain in the form of brain networks using machine learning and graph analytics has become increasingly popular, especially to predict the early onset of these conditions. A brain network, represented as a graph, retains rich structural and positional information that traditional examination methods are unable to capture. However, the lack of publicly accessible brain network data prevents researchers from data-driven explorations. One of the main difficulties lies in the complicated domain-specific preprocessing steps and the exhaustive computation required to convert the data from MRI images into brain networks. We bridge this gap by collecting a large amount of MRI images from public databases and a private source, working with domain experts to make sensible design choices, and preprocessing the MRI images to produce a collection of brain network datasets. The datasets originate from 6 different sources, cover 4 brain conditions, and consist of a total of 2,702 subjects. 
We test our graph datasets on 12 machine learning models to provide baselines and validate the data quality on a recent graph analysis model. To lower the barrier to entry and promote the research in this interdisciplinary field, we release our brain network data and complete preprocessing details including codes at https://doi.org/10.17608/k6.auckland.21397377 and https://github.com/brainnetuoa/data_driven_network_neuroscience.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jiaxing Xu;Yunhan Yang;David Tse Jung Huang;Sophi Shilpa Gururajapathy;Yiping Ke;Miao Qiao;Alan Wang;Haribalan Kumar;Josh McGeown;Eryn Kwon,True,https://openreview.net/pdf?id=vZf7jrX1el
vfzXDRTcF4,JourneyDB: A Benchmark for Generative Image Understanding,"While recent advancements in vision-language models have had a transformative impact on multi-modal comprehension, the extent to which these models possess the ability to comprehend generated images remains uncertain. Synthetic images, in comparison to real data, encompass a higher level of diversity in terms of both content and style, thereby presenting significant challenges for the models to fully grasp. In light of this challenge, we introduce a comprehensive dataset, referred to as JourneyDB, that caters to the domain of generative images within the context of multi-modal visual understanding. Our meticulously curated dataset comprises 4 million distinct and high-quality generated images, each paired with the corresponding text prompts that were employed in their creation. Furthermore, we additionally introduce an external subset with results of another 22 text-to-image generative models, which makes JourneyDB a comprehensive benchmark for evaluating the comprehension of generated images. On our dataset, we have devised four benchmarks to assess the performance of generated image comprehension in relation to both content and style interpretation. These benchmarks encompass prompt inversion, style retrieval, image captioning, and visual question answering. Lastly, we evaluate the performance of state-of-the-art multi-modal models when applied to the JourneyDB dataset, providing a comprehensive analysis of their strengths and limitations in comprehending generated content. We anticipate that the proposed dataset and benchmarks will facilitate further research in the field of generative content understanding. The dataset is publicly available at https://journeydb.github.io.",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=vfzXDRTcF4
viktK3nO5b,SpokenWOZ: A Large-Scale Speech-Text Benchmark for Spoken Task-Oriented Dialogue Agents,"Task-oriented dialogue (TOD) models have made significant progress in recent years. However, previous studies primarily focus on datasets written by annotators, which has resulted in a gap between academic research and real-world spoken con- versation scenarios. While several small-scale spoken TOD datasets are proposed to address robustness issues such as ASR errors, they ignore the unique challenges in spoken conversation. To tackle the limitations, we introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD, containing 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from human-to-human spoken conversations. SpokenWOZ further incorporates common spoken characteristics such as word-by-word processing and reasoning in spoken language. Based on these characteristics, we present cross-turn slot and reasoning slot detection as new challenges. We conduct experiments on various baselines, including text-modal models, newly proposed dual-modal models, and LLMs, e.g., ChatGPT. The results show that the current models still have substantial room for improvement in spoken conversation, where the most advanced dialogue state tracker only achieves 25.65% in joint goal accuracy and the SOTA end-to-end model only correctly completes the user request in 52.1% of dialogues. Our dataset, code, and leaderboard are available at https://spokenwoz.github.io/SpokenWOZ-github.io/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Shuzheng Si;Wentao Ma;Haoyu Gao;Yuchuan Wu;Ting-En Lin;Yinpei Dai;Hangyu Li;Rui Yan;Fei Huang;Yongbin Li,True,https://openreview.net/pdf?id=viktK3nO5b
vv3cocNsEK,HT-Step: Aligning Instructional Articles with How-To Videos,"We introduce HT-Step, a large-scale dataset containing temporal annotations of instructional article steps in cooking videos. It includes 122k segment-level annotations over 20k narrated videos (approximately 2.3k hours) of the HowTo100M dataset.
Each annotation provides a temporal interval, and a categorical step label from a taxonomy of 4,958 unique steps automatically mined from wikiHow articles which include rich descriptions of each step.
Our dataset significantly surpasses existing labeled step datasets in terms of scale, number of tasks, and richness of natural language step descriptions. Based on these annotations, we introduce a strongly supervised benchmark for aligning instructional articles with how-to videos and present a comprehensive evaluation of baseline methods for this task.
By publicly releasing these annotations and defining rigorous evaluation protocols and metrics,
we hope to significantly accelerate research in the field of procedural activity understanding.",Datasets & Benchmarks,NeurIPS,2023,Poster,Triantafyllos Afouras;Effrosyni Mavroudi;Tushar Nagarajan;Huiyu Wang;Lorenzo Torresani,True,https://openreview.net/pdf?id=vv3cocNsEK
w0H2xGHlkw,Visual Instruction Tuning,"Instruction tuning large language models (LLMs) using machine-generated instruction-following data has been shown to improve zero-shot capabilities on new tasks, but the idea is less explored in the multimodal field. We present the first attempt to use language-only GPT-4 to generate multimodal language-image instruction-following data. By instruction tuning on such generated data, we introduce LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and an LLM for general-purpose visual and language understanding. To facilitate future research on visual instruction following, we construct two evaluation benchmarks with diverse and challenging application-oriented tasks. Our experiments show that LLaVA demonstrates impressive multimodal chat abilities, sometimes exhibiting the behaviors of multimodal GPT-4 on unseen images/instructions, and yields a 85.1% relative score compared with GPT-4 on a synthetic multimodal instruction-following dataset. When fine-tuned on Science QA, the synergy of LLaVA and GPT-4 achieves a new state-of-the-art accuracy of 92.53%. We make GPT-4 generated visual instruction tuning data, our model, and code publicly available.",main,NeurIPS,2023,Oral,Haotian Liu;Chunyuan Li;Qingyang Wu;Yong Jae Lee,True,https://openreview.net/pdf?id=w0H2xGHlkw
w4zZNC4ZaV,How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources,"In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Despite recent claims that open models can be on par with state-of-the-art proprietary models, these claims are often accompanied by limited evaluation, making it difficult to compare models across the board and determine the utility of various resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size, trained on 12 instruction datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning, multilinguality, coding, safety, and open-ended instruction following abilities through a collection of automatic, model-based, and human-based metrics. We further introduce Tülu, our best performing instruction-tuned model suite finetuned on a combination of high-quality open resources.

Our experiments show that different instruction-tuning datasets can uncover or enhance specific skills, while no single dataset (or combination) provides the best performance across all evaluations. Interestingly, we find that model and human preference-based evaluations fail to reflect differences in model capabilities exposed by benchmark-based evaluations, suggesting the need for the type of systemic evaluation performed in this work. Our evaluations show that the best model in any given evaluation reaches on average 87% of ChatGPT performance, and 73% of GPT-4 performance, suggesting that further investment in building better base models and instruction-tuning data is required to close the gap. We release our instruction-tuned models, including a fully finetuned 65B Tülu, along with our code, data, and evaluation framework to facilitate future research.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Yizhong Wang;Hamish Ivison;Pradeep Dasigi;Jack Hessel;Tushar Khot;Khyathi Chandu;David Wadden;Kelsey MacMillan;Noah A. Smith;Iz Beltagy;Hannaneh Hajishirzi,False,https://openreview.net/pdf?id=w4zZNC4ZaV
weHBzTLXpH,T2I-CompBench: A Comprehensive Benchmark for Open-world Compositional Text-to-image Generation,"Despite the stunning ability to generate high-quality images by recent text-to-image models, current approaches often struggle to effectively compose objects with different attributes and relationships into a complex and coherent scene. We propose T2I-CompBench, a comprehensive benchmark for open-world compositional text-to-image generation, consisting of 6,000 compositional text prompts from 3 categories (attribute binding, object relationships, and complex compositions) and 6 sub-categories (color binding, shape binding, texture binding, spatial relationships, non-spatial relationships, and complex compositions). We further propose several evaluation metrics specifically designed to evaluate compositional text-to-image generation and explore the potential and limitations of multimodal LLMs for evaluation. We introduce a new approach, Generative mOdel finetuning with Reward-driven Sample selection (GORS), to boost the compositional text-to-image generation abilities of pretrained text-to-image models. Extensive experiments and evaluations are conducted to benchmark previous methods on T2I-CompBench, and to validate the effectiveness of our proposed evaluation metrics and GORS approach. Project page is available at https://karine-h.github.io/T2I-CompBench/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Kaiyi Huang;Kaiyue Sun;Enze Xie;Zhenguo Li;Xihui Liu,True,https://openreview.net/pdf?id=weHBzTLXpH
wgDcbBMSfh,CrossCodeEval: A Diverse and Multilingual Benchmark for Cross-File Code Completion,"Code completion models have made significant progress in recent years, yet current popular evaluation datasets, such as HumanEval and MBPP, predominantly focus on code completion tasks within a single file. This over-simplified setting falls short of representing the real-world software development scenario where repositories span multiple files with numerous cross-file dependencies, and accessing and understanding cross-file context is often required to complete the code correctly. 

To fill in this gap, we propose CrossCodeEval, a diverse and multilingual code completion benchmark that necessitates an in-depth cross-file contextual understanding to complete the code accurately. CrossCodeEval is built on a diverse set of real-world, open-sourced, permissively-licensed repositories in four popular programming languages: Python, Java, TypeScript, and C#. To create examples that strictly require cross-file context for accurate completion, we propose a straightforward yet efficient static-analysis-based approach to pinpoint the use of cross-file context within the current file. 

Extensive experiments on state-of-the-art code language models like CodeGen and StarCoder demonstrate that CrossCodeEval is extremely challenging when the relevant cross-file context is absent, and we see clear improvements when adding these context into the prompt. However, despite such improvements,  the pinnacle of performance remains notably unattained even with the highest-performing model,  indicating that CrossCodeEval is also capable of assessing model's capability in leveraging extensive context to make better code completion. Finally, we benchmarked various methods in retrieving cross-file context, and show that CrossCodeEval can also be used to measure the capability of code retrievers.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yangruibo Ding;Zijian Wang;Wasi Uddin Ahmad;Hantian Ding;Ming Tan;Nihal Jain;Murali Krishna Ramanathan;Ramesh Nallapati;Parminder Bhatia;Dan Roth;Bing Xiang,True,https://openreview.net/pdf?id=wgDcbBMSfh
wiw5mnja8W,AllSim: Simulating and Benchmarking Resource Allocation Policies in Multi-User Systems,"Numerous real-world systems, ranging from healthcare to energy grids, involve users competing for finite and potentially scarce resources. Designing policies for resource allocation in such real-world systems is challenging for many reasons, including the changing nature of user types and their (possibly urgent) need for resources. Researchers have developed numerous machine learning solutions for determining resource allocation policies in these challenging settings. However, a key limitation has been the absence of good methods and test-beds for benchmarking these policies; almost all resource allocation policies are benchmarked in environments which are either completely synthetic or do not allow _any_ deviation from historical data. In this paper we introduce AllSim, which is a benchmarking environment for realistically simulating the impact and utility of policies for resource allocation in systems in which users compete for such scarce resources. Building such a benchmarking environment is challenging because it needs to successfully take into account _the entire collective_ of potential users and the impact a resource allocation policy has on all the other users in the system. AllSim's benchmarking environment is modular (each component being parameterized individually), learnable (informed by historical data), and customizable (adaptable to changing conditions). These, when interacting with an allocation policy, produce a dataset of simulated outcomes for evaluation and comparison of such policies. We believe AllSim is an essential step towards a more systematic evaluation of policies for scarce resource allocation compared to current approaches for benchmarking such methods.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jeroen Berrevoets;Daniel Jarrett;Alex James Chan;Mihaela van der Schaar,True,https://openreview.net/pdf?id=wiw5mnja8W
x5ZruOa4ax,Improving *day-ahead* Solar Irradiance Time Series Forecasting by Leveraging Spatio-Temporal Context,"Solar power harbors immense potential in mitigating climate change by substantially reducing CO$_{2}$ emissions. Nonetheless, the inherent variability of solar irradiance poses a significant challenge for seamlessly integrating solar power into the electrical grid. While the majority of prior research has centered on employing purely time series-based methodologies for solar forecasting, only a limited number of studies have taken into account factors such as cloud cover or the surrounding physical context.
In this paper, we put forth a deep learning architecture designed to harness spatio-temporal context using satellite data, to attain highly accurate day-ahead time-series forecasting for any given station, with a particular emphasis on forecasting Global Horizontal Irradiance (GHI). We also suggest a methodology to extract a distribution for each time step prediction, which can serve as a very valuable measure of uncertainty attached to the forecast. When evaluating models, we propose a testing scheme in which we separate particularly difficult examples from easy ones, in order to capture the model performances in crucial situations, which in the case of this study are the days suffering from varying cloudy conditions. Furthermore, we present a new multi-modal dataset gathering satellite imagery over a large zone and time series for solar irradiance and other related physical variables from multiple geographically diverse solar stations. Our approach exhibits robust performance in solar irradiance forecasting, including zero-shot generalization tests at unobserved solar stations, and holds great promise in promoting the effective integration of solar power into the grid.",main,NeurIPS,2023,Poster,Oussama Boussif;Ghait Boukachab;Dan Assouline;Stefano Massaroli;Tianle Yuan;Loubna Benabbou;Yoshua Bengio,True,https://openreview.net/pdf?id=x5ZruOa4ax
x6cOcxRnxG,Neural Ideal Large Eddy Simulation: Modeling Turbulence with Neural Stochastic Differential Equations,"We introduce a data-driven learning framework that assimilates two powerful ideas: ideal large eddy simulation (LES) from turbulence closure modeling and neural stochastic differential equations (SDE) for stochastic modeling. The ideal LES models the LES flow by treating each full-order trajectory as a random realization of the underlying dynamics, as such, the effect of small-scales is marginalized to obtain the deterministic evolution of the LES state. However, ideal LES is analytically intractable. In our work, we use a latent neural SDE to model the evolution of the stochastic process and an encoder-decoder pair for transforming between the latent space and the desired ideal flow field. This stands in sharp contrast to other types of neural parameterization of closure models where each trajectory is treated as a deterministic realization of the dynamics. We show the effectiveness of our approach (niLES – neural ideal LES) on two challenging chaotic dynamical systems: Kolmogorov flow at a Reynolds number of 20,000 and flow past a cylinder at Reynolds number 500. Compared to competing methods, our method can handle non-uniform geometries using unstructured meshes seamlessly. In particular, niLES leads to trajectories with more accurate statistics and enhances stability, particularly for long-horizon rollouts. (Source codes and datasets will be made publicly available.)",main,NeurIPS,2023,Poster,Anudhyan Boral;Zhong Yi Wan;Leonardo Zepeda-Nunez;James Lottes;Qing Wang;Yi-Fan Chen;John Roberts Anderson;Fei Sha,True,https://openreview.net/pdf?id=x6cOcxRnxG
xJ7YWXQOrg,Mathematical Capabilities of ChatGPT,"We investigate the mathematical capabilities of two versions of ChatGPT (released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on publicly available datasets, as well as hand-crafted ones, using a novel evaluation scheme. In contrast to formal mathematics, where large databases of formal proofs are available (e.g., mathlib, the Lean Mathematical Library), current datasets of natural-language mathematics used to benchmark language models either cover only elementary mathematics or are very small. We address this by publicly releasing two new datasets: GHOSTS and miniGHOSTS. These are the first natural-language datasets curated by working researchers in mathematics that (1) aim to cover graduate-level mathematics, (2) provide a holistic overview of the mathematical capabilities of language models, and (3) distinguish multiple dimensions of mathematical reasoning. These datasets test, by using 1636 human expert evaluations, whether ChatGPT and GPT-4 can be helpful assistants to professional mathematicians by emulating use cases that arise in the daily professional activities of mathematicians. We benchmark the models on a range of fine-grained performance metrics. For advanced mathematics, this is the most detailed evaluation effort to date. We find that ChatGPT and GPT-4 can be used most successfully as mathematical assistants for querying facts, acting as mathematical search engines and knowledge base interfaces. GPT-4 can additionally be used for undergraduate-level mathematics but fails on graduate-level difficulty. Contrary to many positive reports in the media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of selection bias), their overall mathematical performance is well below the level of a graduate student. Hence, if you aim to use ChatGPT to pass a graduate-level math exam, you would be better off copying from your average peer!",Datasets & Benchmarks,NeurIPS,2023,Poster,Simon Frieder;Luca Pinchetti;Alexis Chevalier;Ryan-Rhys Griffiths;Tommaso Salvatori;Thomas Lukasiewicz;Philipp Christian Petersen;Julius Berner,True,https://openreview.net/pdf?id=xJ7YWXQOrg
xKYtTmtyI2,Validated Image Caption Rating Dataset,"We present a new high-quality validated image caption rating (VICR) dataset. How well a caption fits an image can be difficult to assess due to the subjective nature of caption quality. How do we evaluate whether a caption is good? We generated a new dataset to help answer this question by using our new image caption rating system, which consists of a novel robust rating scale and gamified approach to gathering human ratings. We show that our approach is consistent and teachable. 113 participants were involved in generating the dataset, which is composed of 68,217 ratings among 15,646 image-caption pairs. Our new dataset has greater inter-rater agreement than the state of the art, and custom machine learning rating predictors that were trained on our dataset outperform previous metrics. We improve over Flickr8k-Expert in Kendall's $W$ by 12\\\\% and in Fleiss' $\\\\kappa$ by 19\\\\%, and thus provide a new benchmark dataset for image caption rating.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Lothar Narins;Andrew T Scott;Aakash Gautam;Anagha Kulkarni;Mar Castanon;Benjamin Kao;Shasta Ihorn;Yue-Ting Siu;James M Mason;Alexander Mario Blum;Ilmi Yoon,True,https://openreview.net/pdf?id=xKYtTmtyI2
xT3i5GS3zU,GSLB: The Graph Structure Learning Benchmark,"Graph Structure Learning (GSL) has recently garnered considerable attention due to its ability to optimize both the parameters of Graph Neural Networks (GNNs) and the computation graph structure simultaneously. Despite the proliferation of GSL methods developed in recent years, there is no standard experimental setting or fair comparison for performance evaluation, which creates a great obstacle to understanding the progress in this field. To fill this gap, we systematically analyze the performance of GSL in different scenarios and develop a comprehensive Graph Structure Learning Benchmark (GSLB) curated from 20 diverse graph datasets and 16 distinct GSL algorithms. Specifically, GSLB systematically investigates the characteristics of GSL in terms of three dimensions: effectiveness, robustness, and complexity. We comprehensively evaluate state-of-the-art GSL algorithms in node- and graph-level tasks, and analyze their performance in robust learning and model complexity. Further, to facilitate reproducible research, we have developed an easy-to-use library for training, evaluating, and visualizing different GSL methods. Empirical results of our extensive experiments demonstrate the ability of GSL and reveal its potential benefits on various downstream tasks, offering insights and opportunities for future research. The code of GSLB is available at: https://github.com/GSL-Benchmark/GSLB.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zhixun Li;Liang Wang;Xin Sun;Yifan Luo;Yanqiao Zhu;Dingshuo Chen;Yingtao Luo;Xiangxin Zhou;Qiang Liu;Shu Wu;Liang Wang;Jeffrey Xu Yu,False,https://openreview.net/pdf?id=xT3i5GS3zU
xbUz5DsW5T,Visual Abductive Reasoning Meets Driving Hazard Prediction: Problem Formulation and Dataset,"This paper addresses the problem of predicting hazards that drivers may encounter while driving a car. We formulate it as a task of anticipating impending accidents using a single input image captured by car dashcams. Unlike existing approaches to driving hazard prediction that rely on computational simulations or anomaly detection from videos, this study focuses on high-level inference from static images. The problem needs predicting and reasoning about future events based on uncertain observations, which falls under visual abductive reasoning. To enable research in this understudied area, a new dataset named the DHPR (Driving Hazard Prediction and Reasoning) dataset is created. The dataset consists of 15K dashcam images of street scenes, and each image is associated with a tuple containing car speed, a hypothesized hazard description, and visual entities present in the scene. These are annotated by human annotators, who identify risky scenes and provide descriptions of potential accidents that could occur a few seconds later. We present several baseline methods and evaluate their performance on our dataset, identifying remaining issues and discussing future directions. This study contributes to the field by introducing a novel problem formulation and dataset, enabling researchers to explore the potential of multi-modal AI for driving hazard prediction.",Datasets & Benchmarks,NeurIPS,2023,Reject,Korawat Charoenpitaks;Van-Quang Nguyen;Masanori Suganuma;Masahiro Takahashi;Ryoma Niihara;Takayuki Okatani,True,https://openreview.net/pdf?id=xbUz5DsW5T
xewwYquInO,"WordScape: a Pipeline to extract multilingual, visually rich Documents with Layout Annotations from Web Crawl Data","We introduce WordScape, a novel pipeline for the creation of cross-disciplinary, multilingual corpora comprising millions of pages with annotations for document layout detection. Relating visual and textual items on document pages has gained further significance with the advent of multimodal models. Various approaches proved effective for visual question answering or layout segmentation. However, the interplay of text, tables, and visuals remains challenging for a variety of document understanding tasks. In particular, many models fail to generalize well to diverse domains and new languages due to insufficient availability of training data. WordScape addresses these limitations. Our automatic annotation pipeline parses the Open XML structure of Word documents obtained from the web, jointly providing layout-annotated document images and their textual representations. In turn, WordScape offers unique properties as it (1) leverages the ubiquity of the Word file format on the internet, (2) is readily accessible through the Common Crawl web corpus, (3) is adaptive to domain-specific documents, and (4) offers culturally and linguistically diverse document pages with natural semantic structure and high-quality text. Together with the pipeline, we will additionally release 9.5M urls to word documents which can be processed using WordScape to create a dataset of over 40M pages. Finally, we investigate the quality of text and layout annotations extracted by WordScape, assess the impact on document understanding benchmarks, and demonstrate that manual labeling costs can be substantially reduced.",Datasets & Benchmarks,NeurIPS,2023,Poster,Maurice Weber;Carlo Siebenschuh;Rory Marshall Butler;Anton Alexandrov;Valdemar Ragnar Thanner;Georgios Tsolakis;Haris Jabbar;Ian Foster;Bo Li;Rick Stevens;Ce Zhang,True,https://openreview.net/pdf?id=xewwYquInO
xhbIud48JN,SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning,"Recently, commonsense reasoning in text generation has attracted much attention. Generative commonsense reasoning is the task that requires machines, given a group of keywords, to compose a single coherent sentence with commonsense plausibility. While existing datasets targeting generative commonsense reasoning focus on everyday scenarios, it is unclear how well machines reason under specific geographical and temporal contexts. We formalize this challenging task as SituatedGen, where machines with commonsense should generate a pair of contrastive sentences given a group of keywords including geographical or temporal entities. We introduce a corresponding English dataset consisting of 8,268 contrastive sentence pairs, which are built upon several existing commonsense reasoning benchmarks with minimal manual labor. Experiments show that state-of-the-art generative language models struggle to generate sentences with commonsense plausibility and still lag far behind human performance. Our dataset is publicly available at https://github.com/yunx-z/situated_gen.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yunxiang Zhang;Xiaojun Wan,True,https://openreview.net/pdf?id=xhbIud48JN
xo6zDI8gvB,A Multi-modal Global Instance Tracking Benchmark (MGIT): Better Locating Target in Complex Spatio-temporal and Causal Relationship,"Tracking an arbitrary moving target in a video sequence is the foundation for high-level tasks like video understanding. Although existing visual-based trackers have demonstrated good tracking capabilities in short video sequences, they always perform poorly in complex environments, as represented by the recently proposed global instance tracking task, which consists of longer videos with more complicated narrative content. 
Recently, several works have introduced natural language into object tracking, desiring to address the limitations of relying only on a single visual modality. However, these selected videos are still short sequences with uncomplicated spatio-temporal and causal relationships, and the provided semantic descriptions are too simple to characterize video content.
To address these issues, we (1) first propose a new multi-modal global instance tracking benchmark named MGIT. It consists of 150 long video sequences with a total of 2.03 million frames, aiming to fully represent the complex spatio-temporal and causal relationships coupled in longer narrative content. 
(2) Each video sequence is annotated with three semantic grains (i.e., action, activity, and story) to model the progressive process of human cognition. We expect this multi-granular annotation strategy can provide a favorable environment for multi-modal object tracking research and long video understanding. 
(3) Besides, we execute comparative experiments on existing multi-modal object tracking benchmarks, which not only explore the impact of different annotation methods, but also validate that our annotation method is a feasible solution for coupling human understanding into semantic labels. 
(4) Additionally, we conduct detailed experimental analyses on MGIT, and hope the explored performance bottlenecks of existing algorithms can support further research in multi-modal object tracking. 
The proposed benchmark, experimental results, and toolkit will be released gradually on  http://videocube.aitestunion.com/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Shiyu Hu;Dailing Zhang;Meiqi Wu;Xiaokun Feng;Xuchen Li;Xin Zhao;Kaiqi Huang,True,https://openreview.net/pdf?id=xo6zDI8gvB
xrK3QA9mLo,FaceComposer: A Unified Model for Versatile Facial Content Creation,"This work presents FaceComposer, a unified generative model that accomplishes a variety of facial content creation tasks, including text-conditioned face synthesis, text-guided face editing, face animation etc. Based on the latent diffusion framework, FaceComposer follows the paradigm of compositional generation and employs diverse face-specific conditions, e.g., Identity Feature and Projected Normalized Coordinate Code, to release the model creativity at all possible. To support text control and animation, we clean up some existing face image datasets and collect around 500 hours of talking-face videos, forming a high-quality large-scale multi-modal face database. A temporal self-attention module is incorporated into the U-Net structure, which allows learning the denoising process on the mixture of images and videos. Extensive experiments suggest that our approach not only achieves comparable or even better performance than state-of-the-arts on each single task, but also facilitates some combined tasks with one-time forward, demonstrating its potential in serving as a foundation generative model in face domain. We further develop an interface such that users can enjoy our one-step service to create, edit, and animate their own characters. Code, dataset, model, and interface will be made publicly available.",main,NeurIPS,2023,Poster,Jiayu Wang;Kang Zhao;Yifeng Ma;Shiwei Zhang;Yingya Zhang;Yujun Shen;Deli Zhao;Jingren Zhou,True,https://openreview.net/pdf?id=xrK3QA9mLo
xzEtNSuDJk,LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning,"Lifelong learning offers a promising paradigm of building a generalist agent that learns and adapts over its lifespan. 
Unlike traditional lifelong learning problems in image and text domains, which primarily involve the transfer of declarative knowledge of entities and concepts, lifelong learning in decision-making (LLDM) also necessitates the transfer of procedural knowledge, such as actions and behaviors. To advance research in LLDM, we introduce LIBERO, a novel benchmark of lifelong learning for robot manipulation. Specifically, LIBERO highlights five key research topics in LLDM: 1) how to efficiently transfer declarative knowledge, procedural knowledge, or the mixture of both; 2) how to design effective policy architectures and 3) effective algorithms for LLDM; 4) the robustness of a lifelong learner with respect to task ordering; and 5) the effect of model pretraining for LLDM. We develop an extendible procedural generation pipeline that can in principle generate infinitely many tasks. For benchmarking purpose, we create four task suites (130 tasks in total) that we use to investigate the above-mentioned research topics. To support sample-efficient learning, we provide high-quality human-teleoperated demonstration data for all tasks. Our extensive experiments present several insightful or even unexpected discoveries: sequential finetuning outperforms existing lifelong learning methods in forward transfer, no single visual encoder architecture excels at all types of knowledge transfer, and naive supervised pretraining can hinder agents' performance in the subsequent LLDM.",Datasets & Benchmarks,NeurIPS,2023,Poster,Bo Liu;Yifeng Zhu;Chongkai Gao;Yihao Feng;qiang liu;Yuke Zhu;Peter Stone,True,https://openreview.net/pdf?id=xzEtNSuDJk
yEf8NSqTPu,PopSign ASL v1.0: An Isolated American Sign Language Dataset Collected via Smartphones,"PopSign is a smartphone-based bubble-shooter game that helps hearing parents
of deaf infants learn sign language. To help parents practice their ability to sign,
PopSign is integrating sign language recognition as part of its gameplay. For
training the recognizer, we introduce the PopSign ASL v1.0 dataset that collects
examples of 250 isolated American Sign Language (ASL) signs using Pixel 4A
smartphone selfie cameras in a variety of environments. It is the largest publicly
available, isolated sign dataset by number of examples and is the first dataset to
focus on one-handed, smartphone signs. We collected over 210,000 examples
at 1944x2592 resolution made by 47 consenting Deaf adult signers for whom
American Sign Language is their primary language. We manually reviewed 217,866
of these examples, of which 175,022 (approximately 700 per sign) were the sign
intended for the educational game. 39,304 examples were recognizable as a sign
but were not the desired variant or were a different sign. We provide a training set
of 31 signers, a validation set of eight signers, and a test set of eight signers. A
baseline LSTM model for the 250-sign vocabulary achieves 82.1% accuracy (81.9%
class-weighted F1 score) on the validation set and 84.2% (83.9% class-weighted
F1 score) on the test set. Gameplay suggests that accuracy will be sufficient for
creating educational games involving sign language recognition.",Datasets & Benchmarks,NeurIPS,2023,Poster,Thad Starner;Sean Forbes;Matthew So;David Martin;Rohit Sridhar;Gururaj Deshpande;Sam Sepah;Sahir Shahryar;Khushi Bhardwaj;Tyler Kwok;Daksh Sehgal;Saad Hassan;Bill Neubauer;Sofia Anandi Vempala;Alec Tan;Jocelyn Heath;Unnathi Utpal Kumar;Priyanka Vijayaraghavan Mosur;Tavenner M. Hall;Rajandeep Singh;Christopher Zhang Cui;Glenn Cameron;Sohier Dane;Garrett Tanzer,True,https://openreview.net/pdf?id=yEf8NSqTPu
yWpY5I3XyX,FETV: A Benchmark for Fine-Grained Evaluation of Open-Domain Text-to-Video Generation,"Recently, open-domain text-to-video (T2V) generation models have made remarkable progress. However, the promising results are mainly shown by the qualitative cases of generated videos, while the quantitative evaluation of T2V models still faces two critical problems. Firstly, existing studies lack fine-grained evaluation of T2V models on different categories of text prompts. Although some benchmarks have categorized the prompts, their categorization either only focuses on a single aspect or fails to consider the temporal information in video generation. Secondly, it is unclear whether the automatic evaluation metrics are consistent with human standards. To address these problems, we propose **FETV**, a benchmark for **F**ine-grained **E**valuation of **T**ext-to-**V**ideo generation. FETV is multi-aspect, categorizing the prompts based on three orthogonal aspects: the major content, the attributes to control and the prompt complexity. FETV is also temporal-aware, which introduces several temporal categories tailored for video generation. 
Based on FETV, we conduct comprehensive manual evaluations of four representative T2V models, revealing their pros and cons on different categories of prompts from different aspects. We also extend FETV as a testbed to evaluate the reliability of automatic T2V metrics. The multi-aspect categorization of FETV enables fine-grained analysis of the metrics' reliability in different scenarios. We find that existing automatic metrics (e.g., CLIPScore and FVD) correlate poorly with human evaluation. To address this problem, we explore several solutions to improve CLIPScore and FVD, and develop two automatic metrics that exhibit significant higher correlation with humans than existing metrics. Benchmark page: https://github.com/llyx97/FETV.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yuanxin Liu;Lei Li;Shuhuai Ren;Rundong Gao;Shicheng Li;Sishuo Chen;Xu Sun;Lu Hou,True,https://openreview.net/pdf?id=yWpY5I3XyX
yXLyhKvK4D,OpenGSL: A Comprehensive Benchmark for Graph Structure Learning,"Graph Neural Networks (GNNs) have emerged as the *de facto* standard for representation learning on graphs, owing to their ability to effectively integrate graph topology and node attributes. However, the inherent suboptimal nature of node connections, resulting from the complex and contingent formation process of graphs, presents significant challenges in modeling them effectively. To tackle this issue, Graph Structure Learning (GSL), a family of data-centric learning approaches, has garnered substantial attention in recent years. The core concept behind GSL is to jointly optimize the graph structure and the corresponding GNN models. Despite the proposal of numerous GSL methods, the progress in this field remains unclear due to inconsistent experimental protocols, including variations in datasets, data processing techniques, and splitting strategies. In this paper, we introduce OpenGSL, the first comprehensive benchmark for GSL, aimed at addressing this gap. OpenGSL enables a fair comparison among state-of-the-art GSL methods by evaluating them across various popular datasets using uniform data processing and splitting strategies. Through extensive experiments, we observe that existing GSL methods do not consistently outperform vanilla GNN counterparts. We also find that there is no significant correlation between the homophily of the learned structure and task performance, challenging the common belief. Moreover, we observe that the learned graph structure demonstrates a strong generalization ability across different GNN models, despite the high computational and space consumption. We hope that our open-sourced library will facilitate rapid and equitable evaluation and inspire further innovative research in this field. The code of the benchmark can be found in https://github.com/OpenGSL/OpenGSL.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zhiyao Zhou;Sheng Zhou;Bochao Mao;Xuanyi Zhou;Jiawei Chen;Qiaoyu Tan;Daochen Zha;Yan Feng;Chun Chen;Can Wang,False,https://openreview.net/pdf?id=yXLyhKvK4D
yZQDF9f6bR,Diplomat: A Dialogue Dataset for Situated PragMATic Reasoning,"The ability to discern and comprehend pragmatic meanings is a cornerstone of social and emotional intelligence, referred to as pragmatic reasoning. Despite the strides made in the development of Large Language Models (LLMs), such as ChatGPT, these models grapple with capturing the nuanced and ambiguous facets of language, falling short of the aspiration to build human-like conversational agents. In this work, we introduce a novel benchmark, the **DiPlomat**, which delves into the fundamental components of conversational pragmatic reasoning, encompassing situational context reasoning, open-world knowledge acquisition, and unified figurative language understanding. We start by collecting a new human-annotated dialogue dataset, composed of 4,177 multi-turn dialogues and a vocabulary of 48,900 words. Along with the dataset, two tasks are proposed to evaluate machines' pragmatic reasoning capabilities, namely, Pragmatic Reasoning and Identification(PIR) and Conversational Question Answering (CQA). Furthermore, we probe into a zero-shot natural language inference task, where the significance of context in pragmatic reasoning is underscored. Experimental findings illustrate the existing limitations of current prevailing LLMs in the realm of pragmatic reasoning, shedding light on the pressing need for further research to facilitate the emergence of emotional intelligence within human-like conversational agents.",Datasets & Benchmarks,NeurIPS,2023,Poster,Hengli Li;Song-Chun Zhu;Zilong Zheng,True,https://openreview.net/pdf?id=yZQDF9f6bR
ygXSNrIU1p,"Symmetry-Informed Geometric Representation for Molecules, Proteins, and Crystalline Materials","Artificial intelligence for scientific discovery has recently generated significant interest within the machine learning and scientific communities, particularly in the domains of chemistry, biology, and material discovery. For these scientific problems, molecules serve as the fundamental building blocks, and machine learning has emerged as a highly effective and powerful tool for modeling their geometric structures. Nevertheless, due to the rapidly evolving process of the field and the knowledge gap between science ({\\\\eg}, physics,  chemistry, \\\\& biology) and machine learning communities, a benchmarking study on geometrical representation for such data has not been conducted. To address such an issue, in this paper, we first provide a unified view of the current symmetry-informed geometric methods, classifying them into three main categories: invariance, equivariance with spherical frame basis, and equivariance with vector frame basis. Then we propose a platform, coined Geom3D, which enables benchmarking the effectiveness of geometric strategies. Geom3D contains 16 advanced symmetry-informed geometric representation models and 14 geometric pretraining methods over 52 diverse tasks, including small molecules, proteins, and crystalline materials. We hope that Geom3D can, on the one hand, eliminate barriers for machine learning researchers interested in exploring scientific problems; and, on the other hand, provide valuable guidance for researchers in computational chemistry, structural biology, and materials science, aiding in the informed selection of representation techniques for specific applications. The source code is available on \\\\href{https://github.com/chao1224/Geom3D}{the GitHub repository}.",Datasets & Benchmarks,NeurIPS,2023,Poster,Shengchao Liu;weitao Du;Yanjing Li;Zhuoxinran Li;Zhiling Zheng;Chenru Duan;Zhi-Ming Ma;Omar M. Yaghi;Anima Anandkumar;Christian Borgs;Jennifer T Chayes;Hongyu Guo;Jian Tang,False,https://openreview.net/pdf?id=ygXSNrIU1p
yjWVd8Fhqt,OBJECT 3DIT: Language-guided 3D-aware Image Editing,"Existing image editing tools, while powerful, typically disregard the underlying 3D geometry from which the image is projected. As a result, edits made using these tools may become detached from the geometry and lighting conditions that are at the foundation of the image formation process; such edits break the portrayal of a coherent 3D world. 3D-aware generative models are a promising solution, but currently only succeed on small datasets or at the level of a single object. In this work, we formulate the new task of language-guided 3D-aware editing, where objects in an image should be edited according to a language instruction while remaining consistent with the underlying 3D scene. To promote progress towards this goal, we release OBJect: a benchmark dataset of 400K editing examples created from procedurally generated 3D scenes. Each example consists of an input image, editing instruction in language, and the edited image. We also introduce 3DIT: single and multi-task models for four editing tasks. Our models show impressive abilities to understand the 3D composition of entire scenes, factoring in surrounding objects, surfaces, lighting conditions, shadows, and physically-plausible object configurations. Surprisingly, training on only synthetic scenes from \\\\dataset, editing capabilities of 3DIT generalize to real-world images.",main,NeurIPS,2023,Poster,Oscar Michel;Anand Bhattad;Eli VanderBilt;Ranjay Krishna;Aniruddha Kembhavi;Tanmay Gupta,True,https://openreview.net/pdf?id=yjWVd8Fhqt
yoZTVn0T50,CaMP: Causal Multi-policy Planning for Interactive Navigation in Multi-room Scenes,"Visual navigation has been widely studied under the assumption that there may be several clear routes to reach the goal. However, in more practical scenarios such as a house with several messy rooms, there may not. Interactive Navigation (InterNav) considers agents navigating to their goals more effectively with object interactions, posing new challenges of learning interaction dynamics and extra action space. Previous works learn single vision-to-action policy with the guidance of designed representations. However, the causality between actions and outcomes is prone to be confounded when the attributes of obstacles are diverse and hard to measure. Learning policy for long-term action planning in complex scenes also leads to extensive inefficient exploration. In this paper, we introduce a causal diagram of InterNav clarifying the confounding bias caused by obstacles. To address the problem, we propose a multi-policy model that enables the exploration of counterfactual interactions as well as reduces unnecessary exploration. We develop a large-scale dataset containing 600k task episodes in 12k multi-room scenes based on the ProcTHOR simulator and showcase the effectiveness of our method with the evaluations on our dataset.",main,NeurIPS,2023,Poster,Xiaohan Wang;Yuehu Liu;Xinhang Song;Beibei Wang;Shuqiang Jiang,True,https://openreview.net/pdf?id=yoZTVn0T50
z9d9DsjAPH,CycleNet: Rethinking Cycle Consistency in Text-Guided Diffusion for Image Manipulation,"Diffusion models (DMs) have enabled breakthroughs in image synthesis tasks but lack an intuitive interface for consistent image-to-image (I2I) translation. Various methods have been explored to address this issue, including mask-based methods, attention-based methods, and image-conditioning. However, it remains a critical challenge to enable unpaired I2I translation with pre-trained DMs while maintaining satisfying consistency. This paper introduces Cyclenet, a novel but simple method that incorporates cycle consistency into DMs to regularize image manipulation. We validate Cyclenet on unpaired I2I tasks of different granularities. Besides the scene and object level translation, we additionally contribute a multi-domain I2I translation dataset to study the physical state changes of objects. Our empirical studies show that Cyclenet is superior in translation consistency and quality, and can generate high-quality images for out-of-domain distributions with a simple change of the textual prompt. Cyclenet is a practical framework, which is robust even with very limited training data (around 2k) and requires minimal computational resources (1 GPU) to train. Project homepage: https://cyclenetweb.github.io/",main,NeurIPS,2023,Poster,Sihan Xu;Ziqiao Ma;Yidong Huang;Honglak Lee;Joyce Chai,True,https://openreview.net/pdf?id=z9d9DsjAPH
zFvvdJblZm,A High-Resolution Dataset for Instance Detection with Multi-View Object Capture,"Instance detection (InsDet) is a long-lasting problem in robotics and computer vision, aiming to detect object instances (predefined by some visual examples) in a cluttered scene. Despite its practical significance, its advancement is overshadowed by Object Detection, which aims to detect objects belonging to some predefined classes. One major reason is that current InsDet datasets are too small in scale by today's standards. For example, the popular InsDet dataset GMU (published in 2016) has only 23 instances, far less than  COCO (80 classes), a well-known object detection dataset published in 2014. We are motivated to introduce a new InsDet dataset and protocol. First, we define a realistic setup for InsDet: training data consists of multi-view instance captures, along with diverse scene images allowing synthesizing training images by pasting instance images on them with free box annotations. Second, we release a real-world database, which contains multi-view capture of 100 object instances, and high-resolution (6k$\\\\times$8k) testing images. Third, we extensively study baseline methods for InsDet on our dataset, analyze their performance and suggest future work. Somewhat surprisingly, using the off-the-shelf  class-agnostic segmentation model (Segment Anything Model, SAM) and the self-supervised feature representation DINOv2 performs the best, achieving $>$10 AP better than end-to-end trained InsDet models that repurpose object detectors (e.g., FasterRCNN and RetinaNet).",Datasets & Benchmarks,NeurIPS,2023,Poster,QIANQIAN SHEN;Yunhan Zhao;Nahyun Kwon;Jeeeun Kim;Yanan Li;Shu Kong,True,https://openreview.net/pdf?id=zFvvdJblZm
zGthDp4yYe,Real3D-AD: A Dataset of Point Cloud Anomaly Detection,"High-precision point cloud anomaly detection is the gold standard for identifying the defects of advancing machining and precision manufacturing. Despite some methodological advances in this area, the scarcity of datasets and the lack of a systematic benchmark hinder its development. We introduce Real3D-AD, a challenging high-precision point cloud anomaly detection dataset, addressing the limitations in the field. With 1,254 high-resolution 3D items (from forty thousand to millions of points for each item), Real3D-AD is the largest dataset for high-precision 3D industrial anomaly detection to date. Real3D-AD surpasses existing 3D anomaly detection datasets available in terms of point cloud resolution (0.0010mm-0.0015mm), $360^{\\\\circ}$ degree coverage and perfect prototype. Additionally, we present a comprehensive benchmark for Real3D-AD, revealing the absence of baseline methods for high-precision point cloud anomaly detection. To address this, we propose Reg3D-AD, a registration-based 3D anomaly detection method incorporating a novel feature memory bank that preserves local and global representations. Extensive experiments on the Real3D-AD dataset highlight the effectiveness of Reg3D-AD. For reproducibility and accessibility, we provide the Real3D-AD dataset, benchmark source code, and Reg3D-AD on our website: https://github.com/M-3LAB/Real3D-AD.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jiaqi Liu;Guoyang Xie;ruitao chen;Xinpeng Li;Jinbao Wang;Yong Liu;Chengjie Wang;Feng Zheng,True,https://openreview.net/pdf?id=zGthDp4yYe
zQTi3pziFp,Sounding Bodies: Modeling 3D Spatial Sound of Humans Using Body Pose and Audio,"While 3D human body modeling has received much attention in computer vision, modeling the acoustic equivalent, i.e. modeling 3D spatial audio produced by body motion and speech, has fallen short in the community. To close this gap, we present a model that can generate accurate 3D spatial audio for full human bodies. The system consumes, as input, audio signals from headset microphones and body pose, and produces, as output, a 3D sound field surrounding the transmitter's body, from which spatial audio can be rendered at any arbitrary position in the 3D space. We collect a first-of-its-kind multimodal dataset of human bodies, recorded with multiple cameras and a spherical array of 345 microphones. In an empirical evaluation, we demonstrate that our model can produce accurate body-induced sound fields when trained with a suitable loss. Dataset and code are available online.",main,NeurIPS,2023,Spotlight,Xudong XU;Dejan Markovic;Jacob Sandakly;Todd Keebler;Steven Krenn;Alexander Richard,True,https://openreview.net/pdf?id=zQTi3pziFp
zQU33Uh3qM,"Revisiting Out-of-distribution Robustness in NLP: Benchmarks, Analysis, and LLMs Evaluations","This paper reexamines the research on out-of-distribution (OOD) robustness in the field of NLP. We find that the distribution shift settings in previous studies commonly lack adequate challenges, hindering the accurate evaluation of OOD robustness. To address these issues, we propose a benchmark construction protocol that ensures clear differentiation and challenging distribution shifts. Then we introduce
BOSS, a Benchmark suite for Out-of-distribution robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we conduct a series of experiments on pretrained language models for analysis and evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the relationship between in-distribution (ID) and OOD performance. We identify three typical types that unveil the inner learning
mechanism, which could potentially facilitate the forecasting of OOD robustness, correlating with the advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and find that, despite exhibiting some effectiveness in specific cases, they do not offer significant improvement compared to vanilla fine-tuning. Further, we evaluate 5 LLMs with various adaptation paradigms and find that when sufficient ID data is available, fine-tuning domain-specific models outperform LLMs on ID examples significantly. However, in the case of OOD instances, prioritizing LLMs with in-context learning yields better results. We identify that both fine-tuned small models and LLMs face challenges in effectively addressing downstream tasks. The code is public at https://github.com/lifan-yuan/OOD_NLP.",Datasets & Benchmarks,NeurIPS,2023,Poster,Lifan Yuan;Yangyi Chen;Ganqu Cui;Hongcheng Gao;FangYuan Zou;Xingyi Cheng;Heng Ji;Zhiyuan Liu;Maosong Sun,True,https://openreview.net/pdf?id=zQU33Uh3qM
zRYSJbcRcV,Stanford-ORB: A Real-World 3D Object Inverse Rendering Benchmark,"We introduce Stanford-ORB, a new real-world 3D Object inverse Rendering Benchmark. Recent advances in inverse rendering have enabled a wide range of real-world applications in 3D content generation, moving rapidly from research and commercial use cases to consumer devices. While the results continue to improve, there is no real-world benchmark that can quantitatively assess and compare the performance of various inverse rendering methods. Existing real-world datasets typically only consist of the shape and multi-view images of objects, which are not sufficient for evaluating the quality of material recovery and object relighting. Methods capable of recovering material and lighting often resort to synthetic data for quantitative evaluation, which on the other hand does not guarantee generalization to complex real-world environments. We introduce a new dataset of real-world objects captured under a variety of natural scenes with ground-truth 3D scans, multi-view images, and environment lighting. Using this dataset, we establish the first comprehensive real-world evaluation benchmark for object inverse rendering tasks from in-the-wild scenes, and compare the performance of various existing methods. All data, code, and models can be accessed at https://stanfordorb.github.io/",Datasets & Benchmarks,NeurIPS,2023,Poster,Zhengfei Kuang;Yunzhi Zhang;Hong-Xing Yu;Samir Agarwala;Shangzhe Wu;Jiajun Wu,True,https://openreview.net/pdf?id=zRYSJbcRcV
zWxKYyW9ik,Universality and Limitations of Prompt Tuning,"Despite the demonstrated empirical efficacy of prompt tuning to adapt a pretrained language model for a new task, the theoretical underpinnings of the difference between ""tuning parameters before the input"" against ""the tuning of model weights"" are limited. We thus take one of the first steps to understand the role of soft-prompt tuning for transformer-based architectures. By considering a general purpose architecture, we analyze prompt tuning from the lens of both: universal approximation and limitations with finite-depth fixed-weight pretrained transformers for continuous-valued functions. Our universality result guarantees the existence of a strong transformer with a prompt to approximate any sequence-to-sequence function in the set of Lipschitz functions. The limitations of prompt tuning for limited-depth transformers are first proved by constructing a set of datasets, that cannot be memorized by a prompt of any length for a given single encoder layer. We also provide a lower bound on the required number of tunable prompt parameters and compare the result with the number of parameters required for a low-rank update (based on LoRA) for a single-layer setting. We finally extend our analysis to multi-layer settings by providing sufficient conditions under which the transformer can at best learn datasets from invertible functions only. Our theoretical claims are also corroborated by empirical results.",main,NeurIPS,2023,Poster,Yihan Wang;Jatin Chauhan;Wei Wang;Cho-Jui Hsieh,True,https://openreview.net/pdf?id=zWxKYyW9ik
zbEYTg2F1U,ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign Language Recognition,"Sign languages are used as a primary language by approximately 70 million D/deaf people world-wide. However, most communication technologies operate in spoken and written languages, creating inequities in access. To help tackle this problem, we release ASL Citizen, the first crowdsourced Isolated Sign Language Recognition (ISLR) dataset, collected with consent and containing 83,399 videos for 2,731 distinct signs filmed by 52 signers in a variety of environments. We propose that this dataset be used for sign language dictionary retrieval for American Sign Language (ASL), where a user demonstrates a sign to their webcam to retrieve matching signs from a dictionary. We show that training supervised machine learning classifiers with our dataset advances the state-of-the-art on metrics relevant for dictionary retrieval, achieving 63\\\\% accuracy and a recall-at-10 of 91\\\\%, evaluated entirely on videos of users who are not present in the training or validation sets.",Datasets & Benchmarks,NeurIPS,2023,Poster,Aashaka Desai;Lauren Berger;Fyodor O Minakov;Vanessa Milan;Chinmay Singh;Kriston L Pumphrey;Richard Ladner;Hal Daumé III;Alex Xijie Lu;Naomi Caselli;Danielle Bragg,True,https://openreview.net/pdf?id=zbEYTg2F1U
zr1e15kczE,"Live Graph Lab: Towards Open, Dynamic and Real Transaction Graphs with NFT","Numerous studies have been conducted to investigate the properties of large-scale temporal graphs. Despite the ubiquity of these graphs in real-world scenarios, it's usually impractical for us to obtain the whole real-time graphs due to privacy concerns and technical limitations. In this paper, we introduce the concept of {\\\\it Live Graph Lab} for temporal graphs, which enables open, dynamic and real transaction graphs from blockchains. Among them, Non-fungible tokens (NFTs) have become one of the most prominent parts of blockchain over the past several years. With more than \\\\$40 billion market capitalization, this decentralized ecosystem produces massive, anonymous and real transaction activities, which naturally forms a complicated transaction network. However, there is limited understanding about the characteristics of this emerging NFT ecosystem from a temporal graph analysis perspective. To mitigate this gap, we instantiate a live graph with NFT transaction network and investigate its dynamics to provide new observations and insights. Specifically, through downloading and parsing the NFT transaction activities, we obtain a temporal graph with more than 4.5 million nodes and 124 million edges. Then, a series of measurements are presented to understand the properties of the NFT ecosystem. Through comparisons with social, citation, and web networks, our analyses give intriguing findings and point out potential directions for future exploration. Finally, we also study machine learning models in this live graph to enrich the current datasets and provide new opportunities for the graph community. The source codes and dataset are available at https://livegraphlab.github.io.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zhen Zhang;Bingqiao Luo;Shengliang Lu;Bingsheng He,True,https://openreview.net/pdf?id=zr1e15kczE
00Sx577BT3,The Well: a Large-Scale Collection of Diverse Physics Simulations for Machine Learning,"Machine learning based surrogate models offer researchers powerful tools for accelerating simulation-based workflows. However, as standard datasets in this space often cover small classes of physical behavior, it can be difficult to evaluate the efficacy of new approaches. To address this gap, we introduce the Well: a large-scale collection of datasets containing numerical simulations of a wide variety of spatiotemporal physical systems. The Well draws from domain experts and numerical software developers to provide 15TB of data across 16 datasets covering diverse domains such as biological systems, fluid dynamics, acoustic scattering, as well as magneto-hydrodynamic simulations of extra-galactic fluids or supernova explosions. These datasets can be used individually or as part of a broader benchmark suite. To facilitate usage of the Well, we provide a unified PyTorch interface for training and evaluating models. We demonstrate the function of this library by introducing example baselines that highlight the new challenges posed by the complex dynamics of the Well. The code and data is available at https://github.com/PolymathicAI/the_well.",Datasets & Benchmarks,NeurIPS,2024,Poster,Ruben Ohana;Michael McCabe;Lucas Thibaut Meyer;Rudy Morel;Fruzsina Julia Agocs;Miguel Beneitez;Marsha Berger;Blakesley Burkhart;Stuart B. Dalziel;Drummond Buschman Fielding;Daniel Fortunato;Jared A. Goldberg;Keiya Hirashima;Yan-Fei Jiang;Rich Kerswell;Suryanarayana Maddu;Jonah M. Miller;Payel Mukhopadhyay;Stefan S. Nixon;Jeff Shen;Romain Watteaux;Bruno Régaldo-Saint Blancard;François Rozet;Liam Holden Parker;Miles Cranmer;Shirley Ho,True,https://openreview.net/pdf?id=00Sx577BT3
01I55gys19,Few-Class Arena: A Benchmark for Efficient Vision Model Selection and Dataset Difficulty,"A wide variety of benchmark datasets with many classes (80-1000) have been created to assist Computer Vision architectural evolution. An increasing number of vision models are evaluated with these many-class datasets. However, real-world applications often involve substantially fewer classes of interest (2-10). This gap between many and few classes makes it difficult to predict performance of the few-class applications using the available many-class datasets. To date, little has been offered to evaluate models in this Few-Class Regime. We propose Few-Class Arena (FCA), as a unified benchmark with focus on testing efficient image classification models for few classes. We conduct a systematic evaluation of the ResNet family trained on ImageNet subsets from 2 to 1000 classes, and test a wide spectrum of Convolutional Neural Networks and Transformer architectures over ten datasets by using our newly proposed FCA tool. Furthermore, to aid an up-front assessment of dataset difficulty and a more efficient selection of models, we incorporate a difficulty measure as a function of class similarity. FCA offers a new tool for efficient machine learning in the Few-Class Regime, with goals ranging from new efficient similarity proposal, lightweight model architecture design to new scaling law discovery. FCA is user-friendly and can be easily extended to new models and datasets, facilitating future research work. Our benchmark is available at https://github.com/fewclassarena/fca.",Datasets & Benchmarks,NeurIPS,2024,Reject,Bryan Bo Cao;Lawrence O'Gorman;Michael Coss;Shubham Jain,True,https://openreview.net/pdf?id=01I55gys19
01lhHg8H9p,GLBench: A Comprehensive Benchmark for Graph with Large Language Models,"The emergence of large language models (LLMs) has revolutionized the way we interact with graphs, leading to a new paradigm called GraphLLM. Despite the rapid development of GraphLLM methods in recent years, the progress and understanding of this field remain unclear due to the lack of a benchmark with consistent experimental protocols. To bridge this gap, we introduce GLBench, the first comprehensive benchmark for evaluating GraphLLM methods in both supervised and zero-shot scenarios. GLBench provides a fair and thorough evaluation of different categories of GraphLLM methods, along with traditional baselines such as graph neural networks. Through extensive experiments on a collection of real-world datasets with consistent data processing and splitting strategies, we have uncovered several key findings. Firstly, GraphLLM methods outperform traditional baselines in supervised settings, with LLM-as-enhancers showing the most robust performance. However, using LLMs as predictors is less effective and often leads to uncontrollable output issues. We also notice that no clear scaling laws exist for current GraphLLM methods. In addition, both structures and semantics are crucial for effective zero-shot transfer, and our proposed simple baseline can even outperform several models tailored for zero-shot scenarios. The data and code of the benchmark can be found at https://github.com/NineAbyss/GLBench.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yuhan Li;Peisong Wang;Xiao Zhu;Aochuan Chen;Haiyun Jiang;Deng Cai;Victor Wai Kin Chan;Jia Li,True,https://openreview.net/pdf?id=01lhHg8H9p
08nbMTxazb,Chicks4FreeID: A Benchmark Dataset for Chicken Re-Identification,"To address the need for well-annotated datasets in the field of animal re-identification, and particularly to close the existing gap for chickens, we introduce the Chicks4FreeID dataset. This dataset is the first publicly available re-identification resource dedicated to the most farmed animal in the world. It includes top-down view images of individually segmented and annotated chickens, along with preprocessed cut-out crops of the instances. The dataset comprises 1215 annotations of 50 unique chicken individuals, as well as a total of 55 annotations of 2 roosters and 2 ducks. In addition to re-identification, the dataset supports semantic and instance segmentation tasks by providing corresponding masks. Curation and annotation were performed manually, ensuring high-quality, nearly pixel-perfect masks and accurate ground truth assignment of the individuals using expert knowledge. Additionally, we provide context by offering a comprehensive overview of existing datasets for animal re-identification. To facilitate comparability, we establish a baseline for the re-identification task testing different approaches. Performance is evaluated based on mAP, Top-1, and Top-5 accuracy metrics. Both the data and code are publicly shared under a CC BY 4.0 license, promoting accessibility and further research. The dataset can be accessed at https://huggingface.co/datasets/dariakern/Chicks4FreeID and the code at https://github.com/DariaKern/Chicks4FreeID.",Datasets & Benchmarks,NeurIPS,2024,Reject,Daria Kern;Tobias Schiele;Ulrich Klauck;Winfred Ingabire,True,https://openreview.net/pdf?id=08nbMTxazb
0G5OK5vmmg,WenMind: A Comprehensive Benchmark for Evaluating Large Language Models in Chinese Classical Literature and Language Arts,"Large Language Models (LLMs) have made significant advancements across numerous domains, but their capabilities in Chinese Classical Literature and Language Arts (CCLLA) remain largely unexplored due to the limited scope and tasks of existing benchmarks. To fill this gap, we propose WenMind, a comprehensive benchmark dedicated for evaluating LLMs in CCLLA. WenMind covers the sub-domains of Ancient Prose, Ancient Poetry, and Ancient Literary Culture, comprising 4,875 question-answer pairs, spanning 42 fine-grained tasks, 3 question formats, and 2 evaluation scenarios: domain-oriented and capability-oriented. Based on WenMind, we conduct a thorough evaluation of 31 representative LLMs, including general-purpose models and ancient Chinese LLMs. The results reveal that even the best-performing model, ERNIE-4.0, only achieves a total score of 64.3, indicating significant room for improvement of LLMs in the CCLLA domain. We also provide insights into the strengths and weaknesses of different LLMs and highlight the importance of pre-training data in achieving better results.
Overall, WenMind serves as a standardized and comprehensive baseline, providing valuable insights for future CCLLA research. 
Our benchmark and related code are available at \\\\url{https://github.com/SCUT-DLVCLab/WenMind}.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jiahuan Cao;Yang Liu;Yongxin Shi;Kai Ding;Lianwen Jin,True,https://openreview.net/pdf?id=0G5OK5vmmg
0G8AXwtmy2,T2Vs Meet VLMs: A Scalable Multimodal Dataset for Visual Harmfulness Recognition,"While widespread access to the Internet and the rapid advancement of generative models boost people's creativity and productivity, the risk of encountering inappropriate or harmful content also increases. To address the aforementioned issue, researchers managed to incorporate several harmful contents datasets with machine learning methods to detect harmful concepts. However, existing harmful datasets are curated by the presence of a narrow range of harmful objects, and only cover real harmful content sources. This restricts the generalizability of methods based on such datasets and leads to the potential misjudgment in certain cases. Therefore, we propose a comprehensive and extensive harmful dataset, **VHD11K**, consisting of 10,000 images and 1,000 videos, crawled from the Internet and generated by 4 generative models, across a total of 10 harmful categories covering a full spectrum of harmful concepts with non-trival definition. We also propose a novel annotation framework by formulating the annotation process as a multi-agent Visual Question Answering (VQA) task, having 3 different VLMs ""debate"" about whether the given image/video is harmful, and incorporating the in-context learning strategy in the debating process. Therefore, we can ensure that the VLMs consider the context of the given image/video and both sides of the arguments thoroughly before making decisions, further reducing the likelihood of misjudgments in edge cases. Evaluation and experimental results demonstrate that 
(1) the great alignment between the annotation from our novel annotation framework and those from human, ensuring the reliability of VHD11K;
(2) our full-spectrum harmful dataset successfully identifies the inability of existing harmful content detection methods to detect extensive harmful contents and improves the performance of existing harmfulness recognition methods;
(3) our dataset outperforms the baseline dataset, SMID, as evidenced by the superior improvement in harmfulness recognition methods.
The entire dataset is publicly available: https://huggingface.co/datasets/denny3388/VHD11K",Datasets & Benchmarks,NeurIPS,2024,Poster,Chen Yeh;You-Ming Chang;Wei-Chen Chiu;Ning Yu,True,https://openreview.net/pdf?id=0G8AXwtmy2
0Gmi8TkUC7,GenAI Arena: An Open Evaluation Platform for Generative Models,"Generative AI has made remarkable strides to revolutionize fields such as image and video generation. These advancements are driven by innovative algorithms, architecture, and data. However, the rapid proliferation of generative models has highlighted a critical gap: the absence of trustworthy evaluation metrics. Current automatic assessments such as FID, CLIP, FVD, etc often fail to capture the nuanced quality and user satisfaction associated with generative outputs. This paper proposes an open platform GenAI-Arena to evaluate different image and video generative models, where users can actively participate in evaluating these models. By leveraging collective user feedback and votes, GenAI-Arena aims to provide a more democratic and accurate measure of model performance. It covers three tasks of text-to-image generation, text-to-video generation, and image editing respectively. Currently, we cover a total of 35 open-source generative models. GenAI-Arena has been operating for seven months, amassing over 9000 votes from the community. We describe our platform, analyze the data, and explain the statistical methods for ranking the models. To further promote the research in building model-based evaluation metrics, we release a cleaned version of our preference data for the three tasks, namely GenAI-Bench. We prompt the existing multi-modal models like Gemini, and GPT-4o to mimic human voting. We compute the accuracy by comparing the model voting with the human voting to understand their judging abilities. Our results show existing multimodal models are still lagging in assessing the generated visual content, even the best model GPT-4o only achieves an average accuracy of $49.19\\\\%$ across the three generative tasks. Open-source MLLMs perform even worse due to the lack of instruction-following and reasoning ability in complex vision scenarios.",Datasets & Benchmarks,NeurIPS,2024,Poster,Dongfu Jiang;Max Ku;Tianle Li;Yuansheng Ni;Shizhuo Sun;Rongqi Fan;Wenhu Chen,True,https://openreview.net/pdf?id=0Gmi8TkUC7
0NQzQVu9tY,DMC-VB: A Benchmark for Representation Learning for Control with Visual Distractors,"Learning from previously collected data via behavioral cloning or offline reinforcement learning (RL) is a powerful recipe for scaling generalist agents by avoiding the need for expensive online learning. Despite strong generalization in some respects, agents are often remarkably brittle to minor visual variations in control-irrelevant factors such as the background or camera viewpoint.  In this paper, we present theDeepMind Control Visual Benchmark (DMC-VB), a dataset collected in the DeepMind Control Suite to evaluate the robustness of offline RL agents for solving continuous control tasks from visual input in the presence of visual distractors. In contrast to prior works, our dataset (a) combines locomotion and navigation tasks of varying difficulties, (b) includes static and dynamic visual variations, (c) considers data generated by policies with different skill levels, (d) systematically returns pairs of state and pixel observation, (e) is an order of magnitude larger, and (f) includes tasks with hidden goals. Accompanying our dataset, we propose three benchmarks to evaluate representation learning methods for pretraining, and carry out experiments on several recently proposed methods. First, we find that pretrained representations do not help policy learning on DMC-VB, and we highlight a large representation gap between policies learned on pixel observations and on states. Second, we demonstrate when expert data is limited, policy learning can benefit from representations pretrained on (a) suboptimal data, and (b) tasks with stochastic hidden goals. Our dataset and benchmark code to train and evaluate agents are available at https://github.com/google-deepmind/dmc_vision_benchmark.",Datasets & Benchmarks,NeurIPS,2024,Poster,Joseph Ortiz;Antoine Dedieu;Wolfgang Lehrach;J Swaroop Guntupalli;Carter Wendelken;Ahmad Humayun;Sivaramakrishnan Swaminathan;Guangyao Zhou;Miguel Lazaro-Gredilla;Kevin Patrick Murphy,True,https://openreview.net/pdf?id=0NQzQVu9tY
0SMhqvgHST,EEVEE and GATE: Finding the right benchmarks and how to run them seamlessly,"Model evaluation is a cornerstone of machine learning, guiding model design and progress measurement. Designing generalizable evaluation processes remains a challenge, however, partly due to the vast number of possible domain, task and modality combinations and lack of knowledge of how informative they are. In this paper, we propose EEVEE (Efficient Evaluation process Evolution Engine) - pronounced as \\\\textipa{/'i:vi:/} EE-vee - a method that frames evaluation process design as a learning problem. By analyzing a large number of evaluation metrics from diverse benchmarks and models, EEVEE identifies a smaller subset of tasks with high predictive power over the full set of evaluation metrics, reducing evaluation time. To find the optimal subset maximizing signal while minimizing GPU hours, EEVEE evaluates pre-trained models of various architectures, pretraining schemes, and modalities on diverse downstream tasks and datasets including image classification, segmentation, relational reasoning, zero-shot image-to-text tasks, medical classification and segmentation, video classification, and regression. Our results identify three subsets of benchmarks, with 8, 15 and 21 tasks, providing high quality signal for model generalization. Key benchmarks selected include iWildCam, CLEVR-Math, ACDC, WinoGround, CIFAR100, Fungi, and ADE20K. We structure the subsets into three tiers for 12, 24, and 36 GPU-hour budgets and package them into a unified, efficient, and user-friendly Python framework that we built with the researcher in mind -- which we refer to as the GATE engine.
Our experiments reveal ConvNextV2, SigLIP and CLIP as top-performing model encoders, with EfficientNetV2 and ResNext50 excelling in medical tasks and challenging image classification, in particular in Happy Whale Individual classification, ConvNet based models seem to outperform transformer models by a factor of 2.5x, which is surprising. The top performing encoder being ConvNextV2 followed by CLIP seems to agree with other recent large scale evaluations. We also demonstrate the framework's versatility in fine-tuning models from text and audio modalities, paving the way for future cross-modal evaluations.",Datasets & Benchmarks,NeurIPS,2024,Reject,Antreas Antoniou;Eleni Triantafillou;Hugo Larochelle;Sebastien Montella;Fady Rezk;Kiyoon Kim;Linus Ericsson;Pavlos Vougiouklis;Justin Engelmann;Elliot J. Crowley;Srihari Humbarwadi;Yi Liu;Guang Yang;Jeff Z. Pan;Amos Storkey,False,https://openreview.net/pdf?id=0SMhqvgHST
0T8xRFrScB,Benchmarking Counterfactual Image Generation,"Generative AI has revolutionised visual content editing, empowering users to effortlessly modify images and videos. However, not all edits are equal. To perform realistic edits in domains such as natural image or medical imaging, modifications must respect causal relationships inherent to the data generation process. Such image editing falls into the counterfactual image generation regime. Evaluating counterfactual image generation is substantially complex: not only it lacks observable ground truths, but also requires adherence to causal constraints. Although several counterfactual image generation methods and evaluation metrics exist a comprehensive comparison within a unified setting is lacking. We present a comparison framework to thoroughly benchmark counterfactual image generation methods. We evaluate the performance of three conditional image generation model families developed within the Structural Causal Model (SCM) framework. We incorporate several metrics that assess diverse aspects of counterfactuals, such as composition, effectiveness, minimality of interventions, and image realism. We integrate all models that have been used for the task at hand and expand them to novel datasets and causal graphs, demonstrating the superiority of Hierarchical VAEs across most datasets and metrics. Our framework is implemented in a user-friendly Python package that can be extended to incorporate additional SCMs, causal methods, generative models, and datasets for the community to build on. Code: https://github.com/gulnazaki/counterfactual-benchmark.",Datasets & Benchmarks,NeurIPS,2024,Poster,Thomas Melistas;Nikos Spyrou;Nefeli Gkouti;Pedro Sanchez;Athanasios Vlontzos;Yannis Panagakis;Giorgos Papanastasiou;Sotirios A. Tsaftaris,False,https://openreview.net/pdf?id=0T8xRFrScB
0mRouJElbZ,ProgressGym: Alignment with a Millennium of Moral Progress,"Frontier AI systems, including large language models (LLMs), hold increasing influence over the epistemology of human users. Such influence can reinforce prevailing societal values, potentially contributing to the lock-in of misguided moral beliefs and, consequently, the perpetuation of problematic moral practices on a broad scale. We introduce **progress alignment** as a technical solution to mitigate this imminent risk. Progress alignment algorithms learn to emulate the mechanics of human moral progress, thereby addressing the susceptibility of existing alignment methods to contemporary moral blindspots. To empower research in progress alignment, we introduce [**ProgressGym**](https://github.com/PKU-Alignment/ProgressGym), an experimental framework allowing the learning of moral progress mechanics from history, in order to facilitate future progress in real-world moral decisions. Leveraging 9 centuries of historical text and 18 [historical LLMs](https://huggingface.co/collections/PKU-Alignment/progressgym-666735fcf3e4efa276226eaa), ProgressGym enables codification of real-world progress alignment challenges into concrete benchmarks. Specifically, we introduce three core challenges: tracking evolving values (PG-Follow), preemptively anticipating moral progress (PG-Predict), and regulating the feedback loop between human and AI value shifts (PG-Coevolve). Alignment methods without a temporal dimension are inapplicable to these tasks. In response, we present *lifelong* and *extrapolative* algorithms as baseline methods of progress alignment, and build an [open leaderboard](https://huggingface.co/spaces/PKU-Alignment/ProgressGym-LeaderBoard) soliciting novel algorithms and challenges.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Tianyi Qiu;Yang Zhang;Xuchuan Huang;Jasmine Xinze Li;Jiaming Ji;Yaodong Yang,True,https://openreview.net/pdf?id=0mRouJElbZ
15PS30UOUp,Kuro Siwo: 33 billion $m^2$ under the water. A global multi-temporal satellite dataset for rapid flood mapping,"Global flash floods, exacerbated by climate change, pose severe threats to human
life, infrastructure, and the environment. Recent catastrophic events in Pakistan and
New Zealand underscore the urgent need for precise flood mapping to guide restoration efforts, understand vulnerabilities, and prepare for future occurrences. While Synthetic Aperture Radar (SAR) remote sensing offers day-and-night, all-weather
imaging capabilities, its application in deep learning for flood segmentation is limited by the lack of large annotated datasets. To address this, we introduce Kuro
Siwo, a manually annotated multi-temporal dataset, spanning 43 flood events globally. Our dataset maps more than 338 billion $m^2$ of land, with 33 billion designated
as either flooded areas or permanent water bodies. Kuro Siwo includes a highly
processed product optimized for flash flood mapping based on SAR Ground Range
Detected, and a primal SAR Single Look Complex product with minimal preprocessing, designed to promote research on the exploitation of both the phase and amplitude information and to offer maximum flexibility for downstream task preprocessing. To leverage advances in large scale self-supervised pretraining methods
for remote sensing data, we augment Kuro Siwo with a large unlabeled set of SAR
samples. Finally, we provide an extensive benchmark, namely BlackBench, offering strong baselines for a diverse set of flood events globally. All data and code are
published in our Github repository: https://github.com/Orion-AI-Lab/KuroSiwo.",Datasets & Benchmarks,NeurIPS,2024,Poster,Nikolaos Ioannis Bountos;Maria Sdraka;Angelos Zavras;Andreas Karavias;Ilektra Karasante;Themistocles Herekakis;Angeliki Thanasou;Dimitrios Michail;Ioannis Papoutsis,True,https://openreview.net/pdf?id=15PS30UOUp
1ELFGSNBGC,Multiview Scene Graph,"A proper scene representation is central to the pursuit of spatial intelligence where agents can robustly reconstruct and efficiently understand 3D scenes. A scene representation is either metric, such as landmark maps in 3D reconstruction, 3D bounding boxes in object detection, or voxel grids in occupancy prediction, or topological, such as pose graphs with loop closures in SLAM or visibility graphs in SfM.
  In this work, we propose to build Multiview Scene Graphs (MSG) from unposed images, representing a scene topologically with interconnected place and object nodes. 
  The task of building MSG is challenging for existing representation learning methods since it needs to jointly address both visual place recognition, object detection, and object association from images with limited fields of view and potentially large viewpoint changes.
  To evaluate any method tackling this task, we developed an MSG dataset and annotation based on a public 3D dataset.
  We also propose an evaluation metric based on the intersection-over-union score of MSG edges. 
  Moreover, we develop a novel baseline method built on mainstream pretrained vision models, combining visual place recognition and object association into one Transformer decoder architecture. 
  Experiments demonstrate that our method has superior performance compared to existing relevant baselines.",main,NeurIPS,2024,Poster,Juexiao Zhang;Gao Zhu;Sihang Li;Xinhao Liu;Haorui Song;Xinran Tang;Chen Feng,True,https://openreview.net/pdf?id=1ELFGSNBGC
1FVe59t3LX,DTGB: A Comprehensive Benchmark for Dynamic Text-Attributed Graphs,"Dynamic text-attributed graphs (DyTAGs) are prevalent in various real-world scenarios, where each node and edge are associated with text descriptions, and both the graph structure and text descriptions evolve over time. Despite their broad applicability, there is a notable scarcity of benchmark datasets tailored to DyTAGs, which hinders the potential advancement in many research fields. To address this gap, we introduce Dynamic Text-attributed Graph Benchmark (DTGB), a collection of large-scale, time-evolving graphs from diverse domains, with nodes and edges enriched by dynamically changing text attributes and categories. To facilitate the use of DTGB, we design standardized evaluation procedures based on four real-world use cases: future link prediction, destination node retrieval, edge classification, and textual relation generation. These tasks require models to understand both dynamic graph structures and natural language, highlighting the unique challenges posed by DyTAGs. Moreover, we conduct extensive benchmark experiments on DTGB, evaluating 7 popular dynamic graph learning algorithms and their variants of adapting to text attributes with LLM embeddings, along with 6 powerful large language models (LLMs). Our results show the limitations of existing models in handling DyTAGs. Our analysis also demonstrates the utility of DTGB in investigating the incorporation of structural and textual dynamics. The proposed DTGB fosters research on DyTAGs and their broad applications. It offers a comprehensive benchmark for evaluating and advancing models to handle the interplay between dynamic graph structures and natural language. The dataset and source code are available at  https://github.com/zjs123/DTGB.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jiasheng Zhang;Jialin Chen;Menglin Yang;Aosong Feng;Shuang Liang;Jie Shao;Rex Ying,True,https://openreview.net/pdf?id=1FVe59t3LX
1IU3P8VDbn,Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?,"Causal reasoning capability is critical in advancing large language models (LLMs) towards artificial general intelligence (AGI). While versatile LLMs appear to have demonstrated capabilities in understanding contextual causality and providing responses that obey the laws of causality, it remains unclear whether they perform genuine causal reasoning akin to humans. However, current evidence indicates the contrary. Specifically, LLMs are only capable of performing shallow (level-1) causal reasoning, primarily attributed to the causal knowledge embedded in their parameters, but they lack the capacity for genuine human-like (level-2) causal reasoning. To support this hypothesis, methodologically, we delve into the autoregression mechanism of transformer-based LLMs, revealing that it is not inherently causal. Empirically, we introduce a new causal Q&A benchmark named CausalProbe 2024, whose corpus is fresh and nearly unseen for the studied LLMs. Empirical results show a significant performance drop on CausalProbe 2024 compared to earlier benchmarks, indicating that LLMs primarily engage in level-1 causal reasoning.To bridge the gap towards level-2 causal reasoning, we draw inspiration from the fact that human reasoning is usually facilitated by general knowledge and intended goals. Inspired by this, we propose G$^2$-Reasoner, a LLM causal reasoning method that incorporates general knowledge and goal-oriented prompts into LLMs' causal reasoning processes. Experiments demonstrate that G$^2$-Reasoner significantly enhances LLMs' causal reasoning capability, particularly in fresh and fictitious contexts. This work sheds light on a new path for LLMs to advance towards genuine causal reasoning, going beyond level-1 and making strides towards level-2.",main,NeurIPS,2024,Poster,Haoang Chi;He Li;Wenjing Yang;Feng Liu;Long Lan;Xiaoguang Ren;Tongliang Liu;Bo Han,True,https://openreview.net/pdf?id=1IU3P8VDbn
1WtEqReCyS,Multilingual Diversity Improves Vision-Language Representations,"Massive web-crawled image-text datasets lay the foundation for recent progress in multimodal learning. These datasets are designed with the goal of training a model to do well on standard computer vision benchmarks, many of which, however, have been shown to be English-centric (e.g., ImageNet). Consequently, existing data curation techniques gravitate towards using predominantly English image-text pairs and discard many potentially useful non-English samples. Our work questions this practice. Multilingual data is inherently enriching not only because it provides a gateway to learn about culturally salient concepts, but also because it depicts common concepts differently from monolingual data. We thus conduct a systematic study to explore the performance benefits of using more samples of non-English origins with respect to English vision tasks. By translating all multilingual image-text pairs from a raw web crawl to English and re-filtering them, we increase the prevalence of (translated) multilingual data in the resulting training set. Pre-training on this dataset outperforms using English-only or English-dominated datasets on ImageNet, ImageNet distribution shifts, image-English-text retrieval and on average across 38 tasks from the DataComp benchmark. On a geographically diverse task like GeoDE, we also observe improvements across all regions, with the biggest gain coming from Africa. In addition, we quantitatively show that English and non-English data are significantly different in both image and (translated) text space. We hope that our findings motivate future work to be more intentional about including multicultural and multilingual data, not just when non-English or geographically diverse tasks are involved, but to enhance model capabilities at large.",main,NeurIPS,2024,Spotlight,Thao Nguyen;Matthew Wallingford;Sebastin Santy;Wei-Chiu Ma;Sewoong Oh;Ludwig Schmidt;Pang Wei Koh;Ranjay Krishna,True,https://openreview.net/pdf?id=1WtEqReCyS
1dpmeH6IHa,I2EBench: A Comprehensive Benchmark for Instruction-based Image Editing,"Significant progress has been made in the field of Instruction-based Image Editing (IIE). However, evaluating these models poses a significant challenge. A crucial requirement in this field is the establishment of a comprehensive evaluation benchmark for accurately assessing editing results and providing valuable insights for its further development. In response to this need, we propose I2EBench, a comprehensive benchmark designed to automatically evaluate the quality of edited images produced by IIE models from multiple dimensions. I2EBench consists of 2,000+ images for editing, along with 4,000+ corresponding original and diverse instructions. It offers three distinctive characteristics: 1) Comprehensive Evaluation Dimensions: I2EBench comprises 16 evaluation dimensions that cover both high-level and low-level aspects, providing a comprehensive assessment of each IIE model. 2) Human Perception Alignment: To ensure the alignment of our benchmark with human perception, we conducted an extensive user study for each evaluation dimension. 3) Valuable Research Insights: By analyzing the advantages and disadvantages of existing IIE models across the 16 dimensions, we offer valuable research insights to guide future development in the field. We will open-source I2EBench, including all instructions, input images, human annotations, edited images from all evaluated methods, and a simple script for evaluating the results from new IIE models. The code, dataset, and generated images from all IIE models are provided in GitHub: https://github.com/cocoshe/I2EBench.",main,NeurIPS,2024,Poster,Yiwei Ma;Jiayi Ji;Ke Ye;Weihuang Lin;zhibin wang;Yonghan Zheng;Qiang Zhou;Xiaoshuai Sun;Rongrong Ji,True,https://openreview.net/pdf?id=1dpmeH6IHa
1nqfIQIQBf,A Synthetic Dataset for Personal Attribute Inference,"Recently powerful Large Language Models (LLMs) have become easily accessible to hundreds of millions of users world-wide. However, their strong capabilities and vast world knowledge do not come without associated privacy risks. In this work, we focus on the emerging privacy threat LLMs pose – the ability to accurately infer personal information from online texts. Despite the growing importance of LLM-based author profiling, research in this area has been hampered by a lack of suitable public datasets, largely due to ethical and privacy concerns associated with real personal data. We take two steps to address this problem: (i) we construct a simulation framework for the popular social media platform Reddit using LLM agents seeded with synthetic personal profiles; (ii) using this framework, we generate *SynthPAI*, a diverse synthetic dataset of over 7800 comments manually labeled for personal attributes. We validate our dataset with a human study showing that humans barely outperform random guessing on the task of distinguishing our synthetic comments from real ones. Further, we verify that our dataset enables meaningful personal attribute inference research by showing across 18 state-of-the-art LLMs that our synthetic comments allow us to draw the same conclusions as real-world data. Combined, our experimental results, dataset and pipeline form a strong basis for future privacy-preserving research geared towards understanding and mitigating inference-based privacy threats that LLMs pose.",Datasets & Benchmarks,NeurIPS,2024,Poster,Hanna Yukhymenko;Robin Staab;Mark Vero;Martin Vechev,True,https://openreview.net/pdf?id=1nqfIQIQBf
1q3b2Z95ec,FindingEmo: An Image Dataset for Emotion Recognition in the Wild,"We introduce FindingEmo, a new image dataset containing annotations for 25k images, specifically tailored to Emotion Recognition. Contrary to existing datasets, it focuses on complex scenes depicting multiple people in various naturalistic, social settings, with images being annotated as a whole, thereby going beyond the traditional focus on faces or single individuals. Annotated dimensions include Valence, Arousal and Emotion label, with annotations gathered using Prolific. Together with the annotations, we release the list of URLs pointing to the original images, as well as all associated source code.",Datasets & Benchmarks,NeurIPS,2024,Poster,Laurent Mertens;Elahe Yargholi;Hans Op de Beeck;Jan Van den Stock;Joost Vennekens,True,https://openreview.net/pdf?id=1q3b2Z95ec
1s8l1tnTXW,Muharaf: Manuscripts of Handwritten Arabic Dataset for Cursive Text Recognition,"We present the Manuscripts of Handwritten Arabic (Muharaf) dataset, which is a machine learning dataset consisting of more than 1,600 historic handwritten page images transcribed by experts in archival Arabic. Each document image is accompanied by spatial polygonal coordinates of its text lines as well as basic page elements. This dataset was compiled to advance the state of the art in handwritten text recognition (HTR), not only for Arabic manuscripts but also for cursive text in general. The Muharaf dataset includes diverse handwriting styles and a wide range of document types, including personal letters, diaries, notes, poems, church records, and legal correspondences. In this paper, we describe the data acquisition pipeline, notable dataset features, and statistics. We also provide a preliminary baseline result achieved by training convolutional neural networks using this data.",Datasets & Benchmarks,NeurIPS,2024,Poster,Mehreen Saeed;Adrian Chan;Anupam Mijar;joseph Moukarzel;Gerges Habchi;Carlos Younes;amin elias;Chau-Wai Wong;Akram Khater,True,https://openreview.net/pdf?id=1s8l1tnTXW
1sLdprsbmk,Can Models Learn Skill Composition from Examples?,"As large language models (LLMs) become increasingly advanced, their ability to exhibit compositional generalization---the capacity to combine learned skills in novel ways not encountered during training---has garnered significant attention. This type of generalization, particularly in scenarios beyond training data, is also of great interest in the study of AI safety and alignment. A recent study introduced the Skill-Mix evaluation, where models are tasked with composing a short paragraph demonstrating the use of a specified $k$-tuple of language skills. While small models struggled with composing even with $k=3$, larger models like GPT-4 performed reasonably well with $k=5$ and $6$.

In this paper, we employ a setup akin to Skill-Mix to evaluate the capacity of smaller models to learn compositional generalization from examples. Utilizing a diverse set of language skills---including rhetorical, literary, reasoning, theory of mind, and common sense---GPT was used to generate text samples that exhibit random subsets of $k$ skills. Subsequent fine-tuning of 7B and 13B parameter models on these combined skill texts, for increasing values of $k$, revealed the following findings: (1) Training on combinations of $k=2$ and $3$ skills results in noticeable improvements in the ability to compose texts with $k=4$ and $5$ skills, despite models never having seen such examples during training. (2) When skill categories are split into training and held-out groups, models significantly improve at composing texts with held-out skills during testing despite having only seen training skills during fine-tuning, illustrating the efficacy of the training approach even with previously unseen skills.

This study also suggests that incorporating skill-rich (potentially synthetic) text into training can substantially enhance the compositional capabilities of models.",main,NeurIPS,2024,Poster,Haoyu Zhao;Simran Kaur;Dingli Yu;Anirudh Goyal;Sanjeev Arora,True,https://openreview.net/pdf?id=1sLdprsbmk
20QgErW5zH,Drones Help Drones: A Collaborative Framework for Multi-Drone Object Trajectory Prediction and Beyond,"Collaborative trajectory prediction can comprehensively forecast the future motion of objects through multi-view complementary information. However, it encounters two main challenges in multi-drone collaboration settings. The expansive aerial observations make it difficult to generate precise Bird's Eye View (BEV) representations. Besides, excessive interactions can not meet real-time prediction requirements within the constrained drone-based communication bandwidth. To address these problems, we propose a novel framework named ""Drones Help Drones"" (DHD). Firstly, we incorporate the ground priors provided by the drone's inclined observation to estimate the distance between objects and drones, leading to more precise BEV generation. Secondly, we design a selective mechanism based on the local feature discrepancy to prioritize the critical information contributing to prediction tasks during inter-drone interactions. Additionally, we create the first dataset for multi-drone collaborative prediction, named ""Air-Co-Pred"", and conduct quantitative and qualitative experiments to validate the effectiveness of our DHD framework. The results demonstrate that compared to state-of-the-art approaches, DHD reduces position deviation in BEV representations by over 20\\\\% and requires only a quarter of the transmission ratio for interactions while achieving comparable prediction performance. Moreover, DHD also shows promising generalization to the collaborative 3D object detection in CoPerception-UAVs.",main,NeurIPS,2024,Poster,Zhechao Wang;Peirui Cheng;Minxing Chen;Pengju Tian;Zhirui Wang;Xinming Li;Xue Yang;Xian Sun,True,https://openreview.net/pdf?id=20QgErW5zH
2AIwiIkE0s,Vision Transformer Neural Architecture Search for Out-of-Distribution Generalization: Benchmark and Insights,"While Vision Transformer (ViT) have achieved success across various machine learning tasks, deploying them in real-world scenarios faces a critical challenge: generalizing under Out-of-Distribution (OoD) shifts. A crucial research gap remains in understanding how to design ViT architectures – both manually and automatically – to excel in OoD generalization. **To address this gap,** we introduce OoD-ViT-NAS, the first systematic benchmark for ViT Neural Architecture Search (NAS) focused on OoD generalization. This comprehensive benchmark includes 3,000 ViT architectures of varying model computational budgets evaluated on common large-scale OoD datasets. With this comprehensive benchmark at hand, we analyze the factors that contribute to the OoD generalization of ViT architecture. Our analysis uncovers several key insights. Firstly, we show that ViT architecture designs have a considerable impact on OoD generalization. Secondly, we observe that In-Distribution (ID) accuracy might not be a very good indicator of OoD accuracy. This underscores the risk that ViT architectures optimized for ID accuracy might not perform well under OoD shifts. Thirdly, we conduct the first study to explore NAS for ViT’s OoD robustness. Specifically, we study 9 Training-free NAS for their OoD generalization performance on our benchmark. We observe that existing Training-free NAS are largely ineffective in predicting OoD accuracy despite their effectiveness at predicting ID accuracy. Moreover, simple proxies like #Param or #Flop surprisingly outperform more complex Training-free NAS in predicting ViTs OoD accuracy. Finally, we study how ViT architectural attributes impact OoD generalization. We discover that increasing embedding dimensions of a ViT architecture generally can improve the OoD generalization. We show that ViT architectures in our benchmark exhibit a wide range of OoD accuracy, with up to 11.85% for some OoD shift, prompting the importance to study ViT architecture design for OoD. We firmly believe that our OoD-ViT-NAS benchmark and our analysis can catalyze and streamline important research on understanding how ViT architecture designs influence OoD generalization. **Our OoD-NAS-ViT benchmark and code are available at [https://hosytuyen.github.io/projects/OoD-ViT-NAS](https://hosytuyen.github.io/projects/OoD-ViT-NAS)**",main,NeurIPS,2024,Poster,Sy-Tuyen Ho;Tuan Van Vo;Somayeh Ebrahimkhani;Ngai-man Cheung,True,https://openreview.net/pdf?id=2AIwiIkE0s
2HzZIDo48o,Meta-Referential Games to Learn Compositional Learning Behaviours,"Human beings use compositionality to generalise from past experiences to novel experiences, by assuming that past experiences can be separated into fundamental atomic components that can be recombined in novel ways. % to support our ability to engage with novel experiences.
We frame this as the ability to learn to generalise compositionally, and refer to behaviours making use of this ability as compositional learning behaviours (CLBs).
A central problem to learning CLBs is the resolution of a binding problem (BP).
While it is another feat of intelligence that human beings perform with ease, it is not the case for state-of-the-art artificial agents.
Thus, in order to build artificial agents able to collaborate with human beings, we propose to develop a novel benchmark to investigate agents' abilities to exhibit CLBs by solving a domain-agnostic version of the BP. 
We take inspiration from the language emergence and grounding framework of referential games and propose a meta-learning extension of referential games, entitled Meta-Referential Games, and use this framework to build our benchmark, the Symbolic Behaviour Benchmark (S2B). 
We provide baseline results and error analysis showing that the S2B is a compelling challenge that we hope will spur the research community towards developing more capable artificial agents.",Datasets & Benchmarks,NeurIPS,2024,Reject,Kevin Yandoka Denamganai;Sondess Missaoui;James Alfred Walker,True,https://openreview.net/pdf?id=2HzZIDo48o
2Net0eEj9d,NeurIPS 2023 Competition: Privacy Preserving Federated Learning Document VQA,"The Privacy Preserving Federated Learning Document VQA (PFL-DocVQA) competition challenged the community to develop provably private and communication-efficient solutions in a federated setting for a real-life use case: invoice processing. The competition introduced a dataset of real invoice documents, along with associated questions and answers requiring information extraction and reasoning over the document images. Thereby, it brings together researchers and expertise from the document analysis, privacy, and federated learning communities. Participants fine-tuned a pre-trained, state-of-the-art Document Visual Question Answering model provided by the organizers for this new domain, mimicking a typical federated invoice processing setup. The base model is a multi-modal generative language model, and sensitive information could be exposed through either the visual or textual input modality. Participants proposed elegant solutions to reduce communication costs while maintaining a minimum utility threshold in track 1 and to protect all information from each document provider using differential privacy in track 2. The competition served as a new testbed for developing and testing private federated learning methods, simultaneously raising awareness about privacy within the document image analysis and recognition community. Ultimately, the competition analysis provides best practices and recommendations for successfully running privacy-focused federated learning challenges in the future.",Datasets & Benchmarks,NeurIPS,2024,Reject,Marlon Tobaben;Mohamed Ali Souibgui;Rubèn Tito;Khanh Nguyen;Raouf Kerkouche;Kangsoo Jung;Joonas Jälkö;Lei Kang;Andrey Barsky;Vincent Poulain d'Andecy;Aurélie JOSEPH;Aashiq Muhamed;Kevin Kuo;Virginia Smith;Yusuke Yamasaki;Takumi Fukami;Kenta Niwa;Iifan Tyou;Hiro Ishii;Rio Yokota;Ragul N;Rintu Kutum;Josep Llados;Ernest Valveny;Antti Honkela;Mario Fritz;Dimosthenis Karatzas,True,https://openreview.net/pdf?id=2Net0eEj9d
2WbuKAfOxP,The Scandinavian Embedding Benchmarks: Comprehensive Assessment of Multilingual and Monolingual Text Embedding,"The evaluation of English text embeddings has transitioned from evaluating a handful of datasets to broad coverage across many tasks through benchmarks such as MTEB. However, this is not the case for multilingual text embeddings due to a lack of available benchmarks. To address this problem, we introduce the Scandinavian Embedding Benchmark (SEB). SEB is a comprehensive framework that enables text embedding evaluation for Scandinavian languages across 24 tasks, 10 subtasks, and 4 task categories. Building on SEB, we evaluate more than 26 models, uncovering significant performance disparities between public and commercial solutions not previously captured by MTEB. We open-source SEB and integrate it with MTEB, thus bridging the text embedding evaluation gap for Scandinavian languages.",Datasets & Benchmarks,NeurIPS,2024,Poster,Kenneth Enevoldsen;Márton Kardos;Niklas Muennighoff;Kristoffer Nielbo,True,https://openreview.net/pdf?id=2WbuKAfOxP
2dw3zQ3nk9,Vript: A Video Is Worth Thousands of Words,"Advancements in multimodal learning, particularly in video understanding and generation, require high-quality video-text datasets for improved model performance. Vript addresses this issue with a meticulously annotated corpus of 12K high-resolution videos, offering detailed, dense, and script-like captions for over 420K clips. Each clip has a caption of ~145 words, which is over 10x longer than most video-text datasets. Unlike captions only documenting static content in previous datasets, we enhance video captioning to video scripting by documenting not just the content, but also the camera operations, which include the shot types (medium shot, close-up, etc) and camera movements (panning, tilting, etc). By utilizing the Vript, we explore three training paradigms of aligning more text with the video modality rather than clip-caption pairs. This results in Vriptor, a top-performing video captioning model among open-source models, comparable to GPT-4V in performance. Vriptor is also a powerful model capable of end-to-end generation of dense and detailed captions for long videos. Moreover, we introduce Vript-Hard, a benchmark consisting of three video understanding tasks that are more challenging than existing benchmarks: Vript-HAL is the first benchmark evaluating action and object hallucinations in video LLMs, Vript-RR combines reasoning with retrieval resolving question ambiguity in long-video QAs, and Vript-ERO is a new task to evaluate the temporal understanding of events in long videos rather than actions in short videos in previous works. All code, models, and datasets are available in https://github.com/mutonix/Vript.",Datasets & Benchmarks,NeurIPS,2024,Poster,Dongjie Yang;Suyuan Huang;Chengqiang Lu;Xiaodong Han;Haoxin Zhang;Yan Gao;Yao Hu;hai zhao,True,https://openreview.net/pdf?id=2dw3zQ3nk9
2jjfRm2R6D,Multi-language Diversity Benefits Autoformalization,"Autoformalization is the task of translating natural language materials into machine-verifiable formalisations. Progress in autoformalization research is hindered by the lack of a sizeable dataset consisting of informal-formal pairs expressing the same essence. Existing methods tend to circumvent this challenge by manually curating small corpora or using few-shot learning with large language models. But these methods suffer from data scarcity and formal language acquisition difficulty. In this work, we create mma, a large, flexible, multi-language, and multi-domain dataset of informal-formal pairs, by using a language model to translate in the reverse direction, that is, from formal mathematical statements into corresponding informal ones. Experiments show that language models fine-tuned on mma can produce up to $29-31$\\\\% of statements acceptable with minimal corrections on the miniF2F and ProofNet benchmarks, up from $0$\\\\% with the base model. We demonstrate that fine-tuning on multi-language formal data results in more capable autoformalization models even on single-language tasks.",main,NeurIPS,2024,Poster,Albert Q. Jiang;Wenda Li;Mateja Jamnik,True,https://openreview.net/pdf?id=2jjfRm2R6D
2kTX7K6osK,Benchmarking Estimators for Natural Experiments: A Novel Dataset and a Doubly Robust Algorithm,"Estimating the effect of treatments from natural experiments, where treatments are pre-assigned, is an important and well-studied problem. We introduce a novel natural experiment dataset obtained from an early childhood literacy nonprofit. Surprisingly, applying over 20 established estimators to the dataset produces inconsistent results in evaluating the nonprofits efficacy. To address this, we create a benchmark to evaluate estimator accuracy using synthetic outcomes, whose design was guided by domain experts. The benchmark extensively explores performance as real world conditions like sample size, treatment correlation, and propensity score accuracy vary. Based on our benchmark, we observe that the class of doubly robust treatment effect estimators, which are based on simple and intuitive regression adjustment, generally outperform other more complicated estimators by orders of magnitude. To better support our theoretical understanding of doubly robust estimators, we derive a closed form expression for the variance of any such estimator that uses dataset splitting to obtain an unbiased estimate. This expression motivates the design of a new doubly robust estimator that uses a novel loss function when fitting functions for regression adjustment. We release the dataset and benchmark in a Python package; the package is built in a modular way to facilitate new datasets and estimators. https://github.com/rtealwitter/naturalexperiments",Datasets & Benchmarks,NeurIPS,2024,Poster,R. Teal Witter;Christopher Musco,True,https://openreview.net/pdf?id=2kTX7K6osK
2myGfVgfva,MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions,"Sora's high-motion intensity and long consistent videos have significantly impacted the field of video generation, attracting unprecedented attention. However, existing publicly available datasets are inadequate for generating Sora-like videos, as they mainly contain short videos with low motion intensity and brief captions. To address these issues, we propose MiraData, a high-quality video dataset that surpasses previous ones in video duration, caption detail, motion strength, and visual quality. We curate MiraData from diverse, manually selected sources and meticulously process the data to obtain semantically consistent clips. GPT-4V is employed to annotate structured captions, providing detailed descriptions from four different perspectives along with a summarized dense caption. To better assess temporal consistency and motion intensity in video generation, we introduce MiraBench, which enhances existing benchmarks by adding 3D consistency and tracking-based motion strength metrics. MiraBench includes 150 evaluation prompts and 17 metrics covering temporal consistency, motion strength, 3D consistency, visual quality, text-video alignment, and distribution similarity. To demonstrate the utility and effectiveness of MiraData, we conduct experiments using our DiT-based video generation model, MiraDiT. The experimental results on MiraBench demonstrate the superiority of MiraData, especially in motion strength.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xuan Ju;Yiming Gao;Zhaoyang Zhang;Ziyang Yuan;Xintao Wang;Ailing Zeng;Yu Xiong;Qiang Xu;Ying Shan,True,https://openreview.net/pdf?id=2myGfVgfva
2vMvh5XP0P,Subsurface Scattering for Gaussian Splatting,"3D reconstruction and relighting of objects made from scattering materials present a significant challenge due to the complex light transport beneath the surface. 3D Gaussian Splatting introduced high-quality novel view synthesis at real-time speeds. While 3D Gaussians efficiently approximate an object's surface, they fail to capture the volumetric properties of subsurface scattering. We propose a framework for optimizing an object's shape together with the radiance transfer field given multi-view OLAT (one light at a time) data. Our method decomposes the scene into an explicit surface represented as 3D Gaussians, with a spatially varying BRDF, and an implicit volumetric representation of the scattering component. A learned incident light field accounts for shadowing. We optimize all parameters jointly via ray-traced differentiable rendering. Our approach enables material editing, relighting, and novel view synthesis at interactive rates. We show successful application on synthetic data and contribute a newly acquired multi-view multi-light dataset of objects in a light-stage setup. Compared to previous work we achieve comparable or better results at a fraction of optimization and rendering time while enabling detailed control over material attributes.",main,NeurIPS,2024,Poster,Jan-Niklas Dihlmann;Arjun Majumdar;Andreas Engelhardt;Raphael Braun;Hendrik Lensch,True,https://openreview.net/pdf?id=2vMvh5XP0P
2vs1fIAy3X,Constrained Human-AI Cooperation: An Inclusive Embodied Social Intelligence Challenge,"We introduce Constrained Human-AI Cooperation (CHAIC), an inclusive embodied social intelligence challenge designed to test social perception and cooperation in embodied agents. In CHAIC, the goal is for an embodied agent equipped with egocentric observations to assist a human who may be operating under physical constraints—e.g., unable to reach high places or confined to a wheelchair—in performing common household or outdoor tasks as efficiently as possible. To achieve this, a successful helper must: (1) infer the human's intents and constraints by following the human and observing their behaviors (social perception), and (2) make a cooperative plan tailored to the human partner to solve the task as quickly as possible, working together as a team (cooperative planning). 
To benchmark this challenge, we create four new agents with real physical constraints and eight long-horizon tasks featuring both indoor and outdoor scenes with various constraints, emergency events, and potential risks. We benchmark planning- and learning-based baselines on the challenge and introduce a new method that leverages large language models and behavior modeling. Empirical evaluations demonstrate the effectiveness of our benchmark in enabling systematic assessment of key aspects of machine social intelligence. Our benchmark and code are publicly available at https://github.com/UMass-Foundation-Model/CHAIC.",Datasets & Benchmarks,NeurIPS,2024,Poster,Weihua Du;Qiushi Lyu;Jiaming Shan;Zhenting Qi;Hongxin Zhang;Sunli Chen;Andi Peng;Tianmin Shu;Kwonjoon Lee;Behzad Dariush;Chuang Gan,True,https://openreview.net/pdf?id=2vs1fIAy3X
30XanJanJP,EffiBench: Benchmarking the Efficiency of Automatically Generated Code,"Code generation models have increasingly become integral to aiding software development. Although current research has thoroughly examined the correctness of the code produced by code generation models, a vital aspect that plays a pivotal role in green
computing and sustainability efforts — the efficiency of the generated code — has often been neglected. This paper presents Effibench, a benchmark with 1,000 efficiency-critical coding problems to assess the efficiency of code generated by code generation models. 
EffiBench contains a diverse set of LeetCode coding problems. Each problem is paired with an executable human-written canonical solution, which obtains the SOTA efficiency on the LeetCode solution leaderboard. With EffiBench, we empirically examine the ability of 42 large language models (35 open-source and 7 closed-source) to generate efficient code. Our evaluation results demonstrate that the efficiency of the code generated by LLMs is generally worse than the efficiency of human-written canonical solutions. For example, GPT-4 generated code has an average \\\\textbf{3.12} times execution time that of the human-written canonical solutions. In the most extreme cases, the execution time and total memory usage of GPT-4 code are \\\\textbf{13.89} and \\\\textbf{43.92} times that of the canonical solutions. The source code of EffiBench is released on https://github.com/huangd1999/EffiBench. We also provide the LeaderBoard in https://huggingface.co/spaces/EffiBench/effibench-leaderboard.",Datasets & Benchmarks,NeurIPS,2024,Poster,Dong HUANG;Yuhao QING;Weiyi Shang;Heming Cui;Jie Zhang,True,https://openreview.net/pdf?id=30XanJanJP
36ehx1GHD0,CLImage: Human-Annotated Datasets for Complementary-Label Learning,"Complementary-label learning (CLL) is a weakly-supervised learning paradigm that aims to train a multi-class classifier using only complementary labels, which indicate classes to which an instance does not belong. Despite numerous algorithmic proposals for CLL, their practical applicability remains unverified for two reasons. Firstly, these algorithms often rely on assumptions about the generation of complementary labels, and it is not clear how far the assumptions are from reality. Secondly, their evaluation has been limited to synthetic datasets. To gain insights into the real-world performance of CLL algorithms, we developed a protocol to collect complementary labels from human annotators. Our efforts resulted in the creation of four datasets: CLCIFAR10, CLCIFAR20, CLMicroImageNet10, and CLMicroImageNet20, derived from well-known classification datasets CIFAR10, CIFAR100, and TinyImageNet200. These datasets represent the very first real-world CLL datasets. Through extensive benchmark experiments, we discovered a notable decrease in performance when transitioning from synthetic datasets to real-world datasets. We investigated the key factors contributing to the decrease with a thorough dataset-level ablation study. Our analyses highlight annotation noise as the most influential factor in the real-world datasets. In addition, we discover that the biased-nature of human-annotated complementary labels and the difficulty to validate with only complementary labels are two outstanding barriers to practical CLL. These findings suggest that the community focus more research efforts on developing CLL algorithms and validation schemes that are robust to noisy and biased complementary-label distributions.",Datasets & Benchmarks,NeurIPS,2024,Reject,Hsiu-Hsuan Wang;Mai Tan Ha;Nai-Xuan Ye;Wei-I Lin;Hsuan-Tien Lin,True,https://openreview.net/pdf?id=36ehx1GHD0
3814z76JNM,NetworkGym: Reinforcement Learning Environments for Multi-Access Traffic Management in Network Simulation,"Mobile devices such as smartphones, laptops, and tablets can often connect to multiple access networks (e.g., Wi-Fi, LTE, and 5G) simultaneously.
Recent advancements facilitate seamless integration of these connections below the transport layer, enhancing the experience for apps that lack inherent multi-path support.
This optimization hinges on dynamically determining the traffic distribution across networks for each device, a process referred to as multi-access traffic splitting.
This paper introduces NetworkGym, a high-fidelity network environment simulator that facilitates generating multiple network traffic flows and multi-access traffic splitting.
This simulator facilitates training and evaluating different RL-based solutions for the multi-access traffic splitting problem.
Our initial explorations demonstrate that the majority of existing state-of-the-art offline RL algorithms (e.g. CQL) fail to outperform certain hand-crafted heuristic policies on average.
This illustrates the urgent need to evaluate offline RL algorithms against a broader range of benchmarks, rather than relying solely on popular ones such as D4RL.
We also propose an extension to the TD3+BC algorithm, named Pessimistic TD3 (PTD3), and demonstrate that it outperforms many state-of-the-art offline RL algorithms.
PTD3's behavioral constraint mechanism, which relies on value-function pessimism, is theoretically motivated and relatively simple to implement.
We open source our code and offline datasets at github.com/hmomin/networkgym.",Datasets & Benchmarks,NeurIPS,2024,Poster,Momin Haider;Ming Yin;Menglei Zhang;Arpit Gupta;Jing Zhu;Yu-Xiang Wang,True,https://openreview.net/pdf?id=3814z76JNM
3A84lx1JFh,MyoChallenge 2023: Towards Human-Level Dexterity and Agility,"Humans move nimbly and with ease, capable of effortlessly grasping items of many shapes and qualities. Over millions of years, the musculoskeletal structure, central and peripheral neural systems have evolved together to provide this capacity. Understanding the underlying mechanisms of this complex system helps translate benefits to other fields, from robot locomotion to rehabilitation. To illicit new insights into the generation of diverse movements and precise control as well as foster collaboration between the biomechanics and the ML community, the MyoChallenge at the NeurIPS 2023 Competition featured two tracks: Manipulation and Locomotion. Manipulation involved precisely manoeuvering an object of varying shape by controlling a 63-musculoskeletal arm model and generating stable grasps.  Locomotion involved the combination of abstract reasoning and low-level control, as agents have to chase or evade from a moving object by controlling an 80-musculoskeletal model of human legs. These tasks best highlighted our overarching theme of dexterity and agility, requiring the generation of skilled and efficient movements with realistic human limbs. The Myosuite framework enabled the challenge through a realistic, contact-rich and computation-efficient virtual neuromusculoskeletal model of the human arm and legs. This was the second iteration of the MyoChallenge with 59 teams participating, and over 500 submissions. Each task involved two phases, increasing in difficulty over time. While many teams achieved high performance in phase 1 for the Manipulation track, locomotion showed variable performance across participants. In phase two, scores for all teams dropped significantly as the focus shifted towards generalization under uncertain conditions, highlighting the need for stronger generalization in agents In future challenges, we will continue to pursue the generalizability in dexterous manipulation and agile locomotion, which is crucial for understanding motor constructs in humans.",Datasets & Benchmarks,NeurIPS,2024,Reject,Vittorio Caggiano;Guillaume Durandau;HUIYI WANG;Chun Kwang Tan;Pierre Schumacher;Huawei Wang;Alberto Silvio Chiappa;Alessandro Marin Vargas;Alexander Mathis;Jungdam Won;JUNGNAM PARK;Gunwoo Park;Beomsoo Shin;Minseung Kim;SEUNGBUM KOO;Zhuo Yang;Wei Dang;Heng Cai;Jianfei Song;Seungmoon Song;Massimo Sartori;Vikash Kumar,True,https://openreview.net/pdf?id=3A84lx1JFh
3G1ZDXOI4f,LongVideoBench: A Benchmark for Long-context Interleaved Video-Language Understanding,"Large multimodal models (LMMs) are processing increasingly longer and richer inputs. Albeit the progress, few public benchmark is available to measure such development. To mitigate this gap, we introduce LongVideoBench, a question-answering benchmark that features video-language interleaved inputs up to an hour long. Our benchmark includes 3,763 varying-length web-collected videos with their subtitles across diverse themes, designed to comprehensively evaluate LMMs on long-term multimodal understanding. To achieve this, we interpret the primary challenge as to accurately retrieve and reason over detailed multimodal information from long inputs. As such, we formulate a novel video question-answering task termed referring reasoning. Specifically, as part of the question, it contains a referring query that references related video contexts, called referred context. The model is then required to reason over relevant video details from the referred context. Following the paradigm of referring reasoning, we curate 6,678 human-annotated multiple-choice questions in 17 fine-grained categories, establishing one of the most comprehensive benchmarks for long-form video understanding. Evaluations suggest that the LongVideoBench presents significant challenges even for the most advanced proprietary models (e.g. GPT-4o, Gemini-1.5-Pro), while their open-source counterparts show an even larger performance gap. In addition, our results indicate that model performance on the benchmark improves only when they are capable of processing more frames, positioning LongVideoBench as a valuable benchmark for evaluating future-generation long-context LMMs.",Datasets & Benchmarks,NeurIPS,2024,Poster,Haoning Wu;Dongxu Li;Bei Chen;Junnan Li,True,https://openreview.net/pdf?id=3G1ZDXOI4f
3Odq2tGSpp,Stylus: Automatic Adapter Selection for Diffusion Models,"Beyond scaling base models with more data or parameters, fine-tuned adapters provide an alternative way to generate high fidelity, custom images at reduced costs. As such, adapters have been widely adopted by open-source communities, accumulating a database of over 100K adapters—most of which are highly customized with insufficient descriptions. To generate high quality images, this paper explores the problem of matching the prompt to a Stylus of relevant adapters, built on recent work that highlight the performance gains of composing adapters. We introduce Stylus, which efficiently selects and automatically composes task-specific adapters based on a prompt's keywords. Stylus outlines a three-stage approach that first summarizes adapters with improved descriptions and embeddings, retrieves relevant adapters, and then further assembles adapters based on prompts' keywords by checking how well they fit the prompt. To evaluate Stylus, we developed StylusDocs, a curated dataset featuring 75K adapters with pre-computed adapter embeddings. In our evaluation on popular Stable Diffusion checkpoints, Stylus achieves greater CLIP/FID Pareto efficiency and is twice as preferred, with humans and multimodal models as evaluators, over the base model.",main,NeurIPS,2024,Oral,Michael Luo;Justin Wong;Brandon Trabucco;Yanping Huang;Joseph E. Gonzalez;Zhifeng Chen;Russ Salakhutdinov;Ion Stoica,True,https://openreview.net/pdf?id=3Odq2tGSpp
3Yrfx7oYMF,Instruction Embedding: Latent Representations of Instructions Towards Task Identification,"Instruction data is crucial for improving the capability of Large Language Models (LLMs) to align with human-level performance. Recent research LIMA demonstrates that alignment is essentially a process where the model adapts instructions' interaction style or format to solve various tasks, leveraging pre-trained knowledge and skills. Therefore, for instructional data, the most important aspect is the task it represents, rather than the specific semantics and knowledge information. The latent representations of instructions play roles for some instruction-related tasks like data selection and demonstrations retrieval. However, they are always derived from text embeddings, encompass overall semantic information that influences the representation of task categories. In this work, we introduce a new concept, instruction embedding, and construct Instruction Embedding Benchmark (IEB) for its training and evaluation. Then, we propose a baseline Prompt-based Instruction Embedding (PIE) method to make the representations more attention on tasks. The evaluation of PIE, alongside other embedding methods on IEB with two designed tasks, demonstrates its superior performance in accurately identifying task categories. Moreover, the application of instruction embeddings in four downstream tasks showcases its effectiveness and suitability for instruction-related tasks.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yiwei Li;Jiayi Shi;Shaoxiong Feng;Peiwen Yuan;Xinglin Wang;Boyuan Pan;Heda Wang;Yao Hu;Kan Li,True,https://openreview.net/pdf?id=3Yrfx7oYMF
3ZAfFoAcUI,On the Inductive Bias of Stacking Towards Improving Reasoning,"Given the increasing scale of model sizes, efficient training strategies like gradual stacking have garnered interest. Stacking enables efficient training by gradually growing the depth of a model in stages and using layers from a smaller model in an earlier stage to initialize the next stage. Although efficient for training, the model biases induced by such growing approaches are largely unexplored. In this work, we examine this fundamental aspect of gradual stacking, going beyond its efficiency benefits. We propose a variant of gradual stacking called MIDAS that can speed up language model training by up to 40\\\\%. Furthermore we discover an intriguing phenomenon: MIDAS is not only training-efficient but surprisingly also has an inductive bias towards improving downstream tasks, especially tasks that require reasoning abilities like reading comprehension and math problems, despite having similar or slightly worse perplexity compared to baseline training. To further analyze this inductive bias, we construct {\\\\em reasoning primitives} – simple synthetic tasks that are building blocks for reasoning – and find that a model pretrained with stacking is significantly better than standard pretraining on these primitives, with and without fine-tuning. This provides stronger and more robust evidence for this inductive bias towards reasoning. These findings of training efficiency and inductive bias towards reasoning are verified at 1B, 2B and 8B parameter language models. Finally, we conjecture the underlying reason for this inductive bias by exploring the connection of stacking to looped models and provide strong supporting empirical analysis.",main,NeurIPS,2024,Poster,Nikunj Saunshi;Stefani Karp;Shankar Krishnan;Sobhan Miryoosefi;Sashank J. Reddi;Sanjiv Kumar,True,https://openreview.net/pdf?id=3ZAfFoAcUI
3ZLuZ2l0aR,Revisiting Few-Shot Object Detection with Vision-Language Models,"The era of vision-language models (VLMs) trained on web-scale datasets challenges conventional formulations of “open-world"" perception. In this work, we revisit the task of few-shot object detection (FSOD) in the context of recent foundational VLMs. First, we point out that zero-shot predictions from VLMs such as GroundingDINO significantly outperform state-of-the-art few-shot detectors (48 vs. 33 AP) on COCO. Despite their strong zero-shot performance, such foundation models may still be sub-optimal. For example, trucks on the web may be defined differently from trucks for a target applications such as autonomous vehicle perception. We argue that the task of few-shot recognition can be reformulated as aligning foundation models to target concepts using a few examples. Interestingly, such examples can be multi-modal, using both text and visual cues, mimicking instructions that are often given to human annotators when defining a target concept of interest. Concretely, we propose Foundational FSOD, a new benchmark protocol that evaluates detectors pre-trained on any external data and fine-tuned on multi-modal (text and visual) K-shot examples per target class. We repurpose nuImages for Foundational FSOD, benchmark several popular open-source VLMs, and provide an empirical analysis of state-of-the-art methods. Lastly, we discuss our recent CVPR 2024 Foundational FSOD competition and share insights from the community. Notably, the winning team significantly outperforms our baseline by 23.3 mAP!",Datasets & Benchmarks,NeurIPS,2024,Poster,Anish Madan;Neehar Peri;Shu Kong;Deva Ramanan,False,https://openreview.net/pdf?id=3ZLuZ2l0aR
3ZjaXTPWiE,NanoBaseLib: A Multi-Task Benchmark Dataset for Nanopore Sequencing,"Nanopore sequencing is the third-generation sequencing technology with capabilities of generating long-read sequences and directly measuring modifications on DNA/RNA molecules, which makes it ideal for biological applications such as human Telomere-to-Telomere (T2T) genome assembly, Ebola virus surveillance and COVID-19 mRNA vaccine development. However, accuracies of computational methods in various tasks of Nanopore sequencing data analysis are far from satisfactory. For instance, the base calling accuracy of Nanopore RNA sequencing is $\\\\sim$90\\\\%, while the aim is $\\\\sim$99.9\\\\%. This highlights an urgent need of contributions from the machine learning community. A bottleneck that prevents machine learning researchers from entering this field is the lack of a large integrated benchmark dataset. To this end, we present NanoBaseLib, a comprehensive multi-task benchmark dataset. It integrates 16 public datasets with over 30 million reads for four critical tasks in Nanopore data analysis. To facilitate method development, we have preprocessed all the raw data using a uniform workflow, stored all the intermediate results in uniform formats, analysed test datasets with various baseline methods for four benchmark tasks, and developed a software package to easily access these results. NanoBaseLib is available at https://nanobaselib.github.io.",Datasets & Benchmarks,NeurIPS,2024,Poster,Guangzhao Cheng;Chengbo Fu;Lu Cheng,True,https://openreview.net/pdf?id=3ZjaXTPWiE
3gZBGBglBf,Beware of Overestimated Decoding Performance Arising from Temporal Autocorrelations in Electroencephalogram Signals,"Researchers have reported high decoding accuracy (>95%) using non-invasive Electroencephalogram (EEG) signals for brain-computer interface (BCI) decoding tasks like image decoding, emotion recognition, auditory spatial attention detection, etc. Since these EEG data were usually collected with well-designed paradigms in labs, the reliability and robustness of the corresponding decoding methods were doubted by some researchers, and they argued that such decoding accuracy was overestimated due to the inherent temporal autocorrelation of EEG signals. However, the coupling between the stimulus-driven neural responses and the EEG temporal autocorrelations makes it difficult to confirm whether this overestimation exists in truth. Furthermore, the underlying pitfalls behind overestimated decoding accuracy have not been fully explained due to a lack of appropriate formulation. In this work, we formulate the pitfall in various EEG decoding tasks in a unified framework. EEG data were recorded from watermelons to remove stimulus-driven neural responses. Labels were assigned to continuous EEG according to the experimental design for EEG recording of several typical datasets, and then the decoding methods were conducted. The results showed the label can be successfully decoded as long as continuous EEG data with the same label were split into training and test sets. Further analysis indicated that high accuracy of various BCI decoding tasks could be achieved by associating labels with EEG intrinsic temporal autocorrelation features. These results underscore the importance of choosing the right experimental designs and data splits in BCI decoding tasks to prevent inflated accuracies due to EEG temporal correlations. The watermelon EEG dataset collected in this work can be obtained at Zenodo: https://zenodo.org/records/11238929, and all the codes of this work can be obtained in the supplementary materials.",main,NeurIPS,2024,Reject,Xiran Xu;Bo Wang;Boda Xiao;Yadong Niu;Yiwen Wang;Xihong Wu;Jing Chen,True,https://openreview.net/pdf?id=3gZBGBglBf
3ivnixHy16,Boosting Text-to-Video Generative Model with MLLMs Feedback,"Recent advancements in text-to-video generative models, such as Sora, have showcased impressive capabilities. These models have attracted significant interest for their potential applications. However, they often rely on extensive datasets of variable quality, which can result in generated videos that lack aesthetic appeal and do not accurately reflect the input text prompts. A promising approach to mitigate these issues is to leverage Reinforcement Learning from Human Feedback (RLHF), which aims to align the outputs of text-to-video generative with human preferences. However, the considerable costs associated with manual annotation have led to a scarcity of comprehensive preference datasets. In response to this challenge, our study begins by investigating the efficacy of Multimodal Large Language Models (MLLMs) generated annotations in capturing video preferences, discovering a high degree of concordance with human judgments. Building upon this finding, we utilize MLLMs to perform fine-grained video preference annotations across two dimensions, resulting in the creation of VideoPrefer, which includes 135,000 preference annotations. Utilizing this dataset, we introduce VideoRM, the first general-purpose reward model tailored for video preference in the text-to-video domain. Our comprehensive experiments confirm the effectiveness of both VideoPrefer and VideoRM, representing a significant step forward in the field.",main,NeurIPS,2024,Poster,Xun Wu;Shaohan Huang;Guolong Wang;Jing Xiong;Furu Wei,True,https://openreview.net/pdf?id=3ivnixHy16
3qH8q02x0n,IndicVoices-R: Unlocking a Massive Multilingual Multi-speaker Speech Corpus for Scaling Indian TTS,"Recent advancements in text-to-speech (TTS) synthesis show that large-scale models trained with extensive web data produce highly natural-sounding output. However, such data is scarce for Indian languages due to the lack of high-quality, manually subtitled data on platforms like LibriVox or YouTube. To address this gap, we enhance existing large-scale ASR datasets containing natural conversations collected in low-quality environments to generate high-quality TTS training data. Our pipeline leverages the cross-lingual generalization of denoising and speech enhancement models trained on English and applied to Indian languages. This results in IndicVoices-R (IV-R), the largest multilingual Indian TTS dataset derived from an ASR dataset, with 1,704 hours of high-quality speech from 10,496 speakers across 22 Indian languages. IV-R matches the quality of gold-standard TTS datasets like LJSpeech, LibriTTS, and IndicTTS. We also introduce the IV-R Benchmark, the first to assess zero-shot, few-shot, and many-shot speaker generalization capabilities of TTS models on Indian voices, ensuring diversity in age, gender, and style. We demonstrate that fine-tuning an English pre-trained model on a combined dataset of high-quality IndicTTS and our IV-R dataset results in better zero-shot speaker generalization compared to fine-tuning on the IndicTTS dataset alone. Further, our evaluation reveals limited zero-shot generalization for Indian voices in TTS models trained on prior datasets, which we improve by fine-tuning the model on our data containing diverse set of speakers across language families. We open-source code and data for all 22 official Indian languages.",Datasets & Benchmarks,NeurIPS,2024,Poster,Ashwin Sankar;Srija Anand;Praveen Srinivasa Varadhan;Sherry Thomas;Mehak Singal;Shridhar Kumar;Deovrat Mehendale;Aditi Krishana;Giri Raju;Mitesh M Khapra,True,https://openreview.net/pdf?id=3qH8q02x0n
3qa4YLkcEw,TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models,"Aligned large language models (LLMs) demonstrate exceptional capabilities in task-solving, following instructions, and ensuring safety. However, the continual learning aspect of these aligned LLMs has been largely overlooked. Existing continual learning benchmarks lack sufficient challenge for leading aligned LLMs due to their homogeneous task types and low task complexity. To bridge this gap, we introduce TRACE, a benchmark designed to rigorously assess continual learning capabilities in LLMs. TRACE comprises eight challenging tasks from the scope of domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning.  Through systematic experiments on TRACE with six different aligned models ranging from 7B to 70B, we discovered significant declines in both general performance and instruction-following abilities. For example, the accuracy of llama2-chat 13B on the gsm8k dataset declined precipitously from 43.14\\\\% to 2.12\\\\% after training on our datasets. This highlights the challenge of finding a suitable tradeoff between achieving performance on specific tasks while preserving the original prowess of LLMs. Our results demonstrate that integrating task-specific cues with meta-rationales significantly reduces catastrophic forgetting and improves task convergence, offering a viable strategy to enhance the adaptability of LLMs in dynamic environments.",Datasets & Benchmarks,NeurIPS,2024,Reject,Xiao Wang;Yuansen Zhang;Tianze Chen;Songyang Gao;Senjie Jin;Zhiheng Xi;Rui Zheng;Yicheng Zou;Tao Gui;Qi Zhang;Xuanjing Huang,True,https://openreview.net/pdf?id=3qa4YLkcEw
3qoQ6AolAz,Mars: Situated Inductive Reasoning in an Open-World Environment,"Large Language Models (LLMs) trained on massive corpora have shown remarkable success in knowledge-intensive tasks. Yet, most of them rely on pre-stored knowledge. Inducing new general knowledge from a specific environment and
performing reasoning with the acquired knowledge—situated inductive reasoning, is crucial and challenging for machine intelligence. In this paper, we design Mars, an interactive environment devised for situated inductive reasoning. It introduces counter-commonsense game mechanisms by modifying terrain, survival setting and task dependency while adhering to certain principles. In Mars, agents need to actively interact with their surroundings, derive useful rules and perform decision-making tasks in specific contexts. We conduct experiments on various RL-based and LLM-based methods, finding that they all struggle on this challenging situated inductive reasoning benchmark. Furthermore, we explore Induction from Reflection, where we instruct agents to perform inductive reasoning from history trajectory. The superior performance underscores the importance of inductive reasoning in Mars. Through Mars, we aim to galvanize advancements in situated inductive reasoning and set the stage for developing the next generation of AI systems that can reason in an adaptive and context-sensitive way.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xiaojuan Tang;Jiaqi Li;Yitao Liang;Song-Chun Zhu;Muhan Zhang;Zilong Zheng,False,https://openreview.net/pdf?id=3qoQ6AolAz
3uDEmsf3Jf,OASIS: Conditional Distribution Shaping for Offline Safe Reinforcement Learning,"Offline safe reinforcement learning (RL) aims to train a policy that satisfies con- straints using a pre-collected dataset. Most current methods struggle with the mismatch between imperfect demonstrations and the desired safe and rewarding performance. In this paper, we mitigate this issue from a data-centric perspective and introduce OASIS (cOnditionAl diStributIon Shaping), a new paradigm in offline safe RL designed to overcome these critical limitations. OASIS utilizes a conditional diffusion model to synthesize offline datasets, thus shaping the data dis- tribution toward a beneficial target domain. Our approach makes compliance with safety constraints through effective data utilization and regularization techniques to benefit offline safe RL training. Comprehensive evaluations on public benchmarks and varying datasets showcase OASIS’s superiority in benefiting offline safe RL agents to achieve high-reward behavior while satisfying the safety constraints, out- performing established baselines. Furthermore, OASIS exhibits high data efficiency and robustness, making it suitable for real-world applications, particularly in tasks where safety is imperative and high-quality demonstrations are scarce. More details are available at the website https://sites.google.com/view/saferl-oasis/home.",main,NeurIPS,2024,Poster,Yihang Yao;Zhepeng Cen;Wenhao Ding;Haohong Lin;Shiqi Liu;Tingnan Zhang;Wenhao Yu;Ding Zhao,True,https://openreview.net/pdf?id=3uDEmsf3Jf
3uI4ceR4iz,SA3DIP: Segment Any 3D Instance with Potential 3D Priors,"The proliferation of 2D foundation models has sparked research into adapting them for open-world 3D instance segmentation. Recent methods introduce a paradigm that leverages superpoints as geometric primitives and incorporates 2D multi-view masks from Segment Anything model (SAM) as merging guidance, achieving outstanding zero-shot instance segmentation results. However, the limited use of 3D priors restricts the segmentation performance. Previous methods calculate the 3D superpoints solely based on estimated normal from spatial coordinates, resulting in under-segmentation for instances with similar geometry. Besides, the heavy reliance on SAM and hand-crafted algorithms in 2D space suffers from over-segmentation due to SAM's inherent part-level segmentation tendency. To address these issues, we propose SA3DIP, a novel method for Segmenting Any 3D Instances via exploiting potential 3D Priors. Specifically, on one hand, we generate complementary 3D primitives based on both geometric and textural priors, which reduces the initial errors that accumulate in subsequent procedures. On the other hand, we introduce supplemental constraints from the 3D space by using a 3D detector to guide a further merging process. Furthermore, we notice a considerable portion of low-quality ground truth annotations in ScanNetV2 benchmark, which affect the fair evaluations. Thus, we present ScanNetV2-INS with complete ground truth labels and supplement additional instances for 3D class-agnostic instance segmentation. Experimental evaluations on various 2D-3D datasets demonstrate the effectiveness and robustness of our approach. Our code and proposed ScanNetV2-INS dataset are available HERE.",main,NeurIPS,2024,Spotlight,Xi Yang;Xu Gu;Xingyilang Yin;Xinbo Gao,True,https://openreview.net/pdf?id=3uI4ceR4iz
41lovPOCo5,TableRAG: Million-Token Table Understanding with Language Models,"Recent advancements in language models (LMs) have notably enhanced their ability to reason with tabular data, primarily through program-aided mechanisms that manipulate and analyze tables.
However, these methods often require the entire table as input, leading to scalability challenges due to the positional bias or context length constraints.
In response to these challenges, we introduce TableRAG, a Retrieval-Augmented Generation (RAG) framework specifically designed for LM-based table understanding.
TableRAG leverages query expansion combined with schema and cell retrieval to pinpoint crucial information  before providing it to the LMs.
This enables more efficient data encoding and precise retrieval, significantly reducing prompt lengths and mitigating information loss.
We have developed two new million-token benchmarks from the Arcade and BIRD-SQL datasets to thoroughly evaluate TableRAG's effectiveness at scale.
Our results demonstrate that TableRAG's retrieval design achieves the highest retrieval quality, leading to the new state-of-the-art performance on large-scale table understanding.",main,NeurIPS,2024,Poster,Si-An Chen;Lesly Miculicich;Julian Martin Eisenschlos;Zifeng Wang;Zilong Wang;Yanfei Chen;Yasuhisa Fujii;Hsuan-Tien Lin;Chen-Yu Lee;Tomas Pfister,True,https://openreview.net/pdf?id=41lovPOCo5
42mqpIrA39,StackEval: Benchmarking LLMs in Coding Assistance,"We present two comprehensive benchmarks to evaluate the performance of language models in coding assistance tasks, covering code writing, debugging, code review, and conceptual understanding. Our main contribution includes two curated datasets: StackEval, a large-scale benchmark derived from Stack Overflow questions, and StackUnseen, a dynamic benchmark featuring the most recent Stack Overflow content. These benchmarks offer novel insights into the capabilities and limitations of LLMs, particularly in handling new and emerging content. Additionally, we assess LLMs' proficiency as judges for coding tasks using a curated, human-annotated dataset, exploring their evaluation capabilities and potential biases, including whether they favor their own generated solutions. Our findings underscore the potential of these benchmarks to advance LLM development and application in coding assistance. To ensure reproducibility, we publicly share our datasets and evaluation code at https://github.com/ProsusAI/stack-eval.",Datasets & Benchmarks,NeurIPS,2024,Poster,Nidhish Shah;Zulkuf Genc;Dogu Araci,True,https://openreview.net/pdf?id=42mqpIrA39
4351SumKS9,Beyond Aesthetics: Cultural Competence in Text-to-Image Models,"Text-to-Image (T2I) models are being increasingly adopted in diverse global communities where they create visual representations of their unique cultures. Current T2I benchmarks primarily focus on faithfulness, aesthetics, and realism of generated images, overlooking the critical dimension of *cultural competence*. In this work, we introduce a framework to evaluate cultural competence of T2I models along two crucial dimensions: cultural awareness and cultural diversity, and present a scalable approach using a combination of structured knowledge bases and large language models to build a large dataset of cultural artifacts to enable this evaluation. In particular, we apply this approach to build CUBE (CUltural BEnchmark for Text-to-Image models), a first-of-its-kind benchmark to evaluate cultural competence of T2I models. CUBE covers cultural artifacts associated with 8 countries across different geo-cultural regions and along 3 concepts: cuisine, landmarks, and art. CUBE consists of 1) CUBE-1K, a set of high-quality prompts that enable the evaluation of cultural awareness, and 2) CUBE-CSpace, a larger dataset of cultural artifacts that serves as grounding to evaluate cultural diversity. We also introduce cultural diversity as a novel T2I evaluation component, leveraging quality-weighted Vendi score. Our evaluations reveal significant gaps in the cultural awareness of existing models across countries and provide valuable insights into the cultural diversity of T2I outputs for underspecified prompts. Our methodology is extendable to other cultural regions and concepts and can facilitate the development of T2I models that better cater to the global population.",Datasets & Benchmarks,NeurIPS,2024,Poster,Nithish Kannen;Arif Ahmad;marco Andreetto;Vinodkumar Prabhakaran;Utsav Prabhu;Adji Bousso Dieng;Pushpak Bhattacharyya;Shachi Dave,True,https://openreview.net/pdf?id=4351SumKS9
43s8hgGTOX,OpenDebateEvidence: A Massive-Scale Argument Mining and Summarization Dataset,"We introduce OpenDebateEvidence, a comprehensive dataset for argument mining
and summarization sourced from the American Competitive Debate community.
This dataset includes over 3.5 million documents with rich metadata, making it
one of the most extensive collections of debate evidence. OpenDebateEvidence
captures the complexity of arguments in high school and college debates, pro-
viding valuable resources for training and evaluation. Our extensive experiments
demonstrate the efficacy of fine-tuning state-of-the-art large language models for
argumentative abstractive summarization across various methods, models, and
datasets. By providing this comprehensive resource, we aim to advance com-
putational argumentation and support practical applications for debaters, edu-
cators, and researchers. OpenDebateEvidence is publicly available to support
further research and innovation in computational argumentation. Access it here:
https://huggingface.co/datasets/Yusuf5/OpenCaselist.",Datasets & Benchmarks,NeurIPS,2024,Poster,Allen G Roush;Yusuf Shabazz;Arvind Balaji;Peter Zhang;Stefano Mezza;Markus Zhang;Sanjay Basu;Sriram Vishwanath;Ravid Shwartz-Ziv,True,https://openreview.net/pdf?id=43s8hgGTOX
46V9axmOuU,AP-Adapter: Improving Generalization of Automatic Prompts on Unseen Text-to-Image Diffusion Models,"Recent advancements in Automatic Prompt Optimization (APO) for text-to-image generation have streamlined user input while ensuring high-quality image output. However, most APO methods are trained assuming a fixed text-to-image model, which is impractical given the emergence of new models. To address this, we propose a novel task, model-generalized automatic prompt optimization (MGAPO), which trains APO methods on a set of known models to enable generalization to unseen models during testing. MGAPO presents significant challenges. First, we experimentally confirm the suboptimal performance of existing APO methods on unseen models. We then introduce a two-stage prompt optimization method, AP-Adapter. In the first stage, a large language model is used to rewrite the prompts. In the second stage, we propose a novel method to construct an enhanced representation space by leveraging inter-model differences. This space captures the characteristics of multiple domain models, storing them as domain prototypes. These prototypes serve as anchors to adjust prompt representations, enabling generalization to unseen models. The optimized prompt representations are subsequently used to generate conditional representations for controllable image generation. We curate a multi-modal, multi-model dataset that includes multiple diffusion models and their corresponding text-image data, and conduct experiments under a model generalization setting. The experimental results demonstrate the AP-Adapter's ability to enable the automatic prompts to generalize well to previously unseen diffusion models, generating high-quality images.",main,NeurIPS,2024,Poster,Yuchen Fu;Zhiwei Jiang;Yuliang Liu;Cong Wang;Zexuan Deng;Zhaoling Chen;Qing Gu,True,https://openreview.net/pdf?id=46V9axmOuU
476zUsqFZB,PMechRP: Interpretable Deep Learning for Polar Reaction Prediction,"In recent years, machine learning based methods for chemical reaction prediction have garnered significant interest due to the time consuming and resource intensive nature of designing synthetic pathways. However, with the majority of models being trained on the US Patent Office dataset, many proposed architectures lack interpretability by modeling chemical reactions as overall transformations. These models map directly from reactants to products, and provide minimal insight into the underlying driving forces of a reaction. In order to improve interpretrability and provide insight into the causality of a chemical reaction, we train various machine learning frameworks on the PMechDB dataset. This dataset contains polar elementary steps, which model chemical reactions as a sequence of steps associated with movements of electrons. Through training on PMechDB, we have created a new system for polar mechanistic reaction prediction: PMechRP. Our findings indicate that PMechRP is able to provide both accurate and interpretrable predictions, with a novel two-step transformer based method achieving the highest top-5 accuracy at 89.9%.",main,NeurIPS,2024,Reject,Ryan J Miller;Brayden Rudisill;Pierre Baldi;David Van Vranken,True,https://openreview.net/pdf?id=476zUsqFZB
4M9f8VMt2C,Long-form factuality in large language models,"Large language models (LLMs) often generate content that contains factual errors when responding to fact-seeking prompts on open-ended topics. To benchmark a model’s long-form factuality in open domains, we first use GPT-4 to generate LongFact, a prompt set comprising thousands of questions spanning 38 topics. We then propose that LLM agents can be used as automated evaluators for long-form factuality through a method which we call Search-Augmented Factuality Evaluator (SAFE). SAFE utilizes an LLM to break down a long-form response into a set of individual facts and to evaluate the accuracy of each fact using a multi-step reasoning process comprising sending search queries to Google Search and determining whether a fact is supported by the search results. Furthermore, we propose extending F1 score as an aggregated metric for long-form factuality. To do so, we balance the percentage of supported facts in a response (precision) with the percentage of provided facts relative to a hyperparameter representing a user’s preferred response length (recall).

Empirically, we demonstrate that LLM agents can outperform crowdsourced human annotators—on a set of∼16k individual facts, SAFE agrees with crowdsourced human annotators 72% of the time, and on a random subset of 100 disagreement cases, SAFE wins 76% of the time. At the same time, SAFE is more than 20 times cheaper than human annotators.  We also benchmark thirteen language models on LongFact across four model families (Gemini, GPT, Claude, and PaLM-2), finding that larger language models generally achieve better long-form factuality. LongFact, SAFE, and all experimental code are available at https://github.com/google-deepmind/long-form-factuality.",main,NeurIPS,2024,Poster,Jerry Wei;Chengrun Yang;Xinying Song;Yifeng Lu;Nathan Zixia Hu;Jie Huang;Dustin Tran;Daiyi Peng;Ruibo Liu;Da Huang;Cosmo Du;Quoc V Le,True,https://openreview.net/pdf?id=4M9f8VMt2C
4RTxKGUUgc,"Show, Don't Tell: Evaluating Large Language Models Beyond Textual Understanding with ChildPlay","The evaluation of Large Language Models (LLMs) often focuses on linguistic tasks, yet such assessments may not fully capture the models' general reasoning capabilities. We explore the hypothesis that LLMs, such as GPT-3.5 and GPT-4, possess broader cognitive functions, particularly in non-linguistic domains. Our approach extends beyond standard linguistic benchmarks by incorporating games like Tic-Tac-Toe, Connect Four, and Battleship, encoded via ASCII, to assess strategic thinking and decision-making. To evaluate the models' ability to generalize beyond their training data, we introduce two additional games. The first game, LEGO Connect Language (LCL), tests the models' capacity to understand spatial logic and follow assembly instructions. The second game, the game of shapes, challenges the models to identify shapes represented by 1s within a matrix of zeros, further testing their spatial reasoning skills. This ""show, don't tell"" strategy uses games to potentially reveal cognitive capabilities rather than simply querying the models. Our results indicate that despite their proficiency on standard benchmarks and temperature settings, GPT-3.5 and GPT-4's abilities to play and reason about fully observable games without pre-training is mediocre. Both models fail to anticipate losing moves in Tic-Tac-Toe and Connect Four, and they are unable to play Battleship correctly. While GPT-4 shows some success in the game of shapes, both models struggle with the assembly tasks presented in the LCL game. These results suggest that while LLMs like the GPT models can emulate conversational proficiency and basic rule comprehension, their performance in strategic gameplay and spatial reasoning tasks is limited in cognitive flexibility and generalization. Importantly, this reveals a blind spot in current LLM benchmarks that we highlight with our gameplay benchmark suite ChildPlay ($$\\\\href{https://github.com/child-play-neurips/child-play}{GitHub Repository}$$). Our findings provide a cautionary tale about claims of emergent intelligence and reasoning capabilities of LLMs that are roughly the size of GPT-3.5 and GPT-4",Datasets & Benchmarks,NeurIPS,2024,Reject,Gonçalo Hora de Carvalho;Robert Pollice;Oscar Knap,True,https://openreview.net/pdf?id=4RTxKGUUgc
4S8agvKjle,AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents,"Evaluating large language models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis through interactive visualization. This not only sheds light on the capabilities and limitations of LLM agents but also propels the interpretability of their performance to the forefront. Ultimately, AgentBoard serves as a significant step towards demystifying agent behaviors and accelerating the development of stronger LLM agents.",Datasets & Benchmarks,NeurIPS,2024,Oral,Chang Ma;Junlei Zhang;Zhihao Zhu;Cheng Yang;Yujiu Yang;Yaohui Jin;Zhenzhong Lan;Lingpeng Kong;Junxian He,True,https://openreview.net/pdf?id=4S8agvKjle
4TlUE0ufiz,Introspective Planning: Aligning Robots' Uncertainty with Inherent Task Ambiguity,"Large language models (LLMs) exhibit advanced reasoning skills, enabling robots to comprehend natural language instructions and strategically plan high-level actions through proper grounding. However, LLM hallucination may result in robots confidently executing plans that are misaligned with user goals or even unsafe in critical scenarios. Additionally, inherent ambiguity in natural language instructions can introduce uncertainty into the LLM's reasoning and planning. We propose introspective planning, a systematic approach that guides LLMs to refine their own uncertainty in alignment with inherent task ambiguity. Our approach constructs a knowledge base containing introspective reasoning examples as post-hoc rationalizations of human-selected safe and compliant plans, which are retrieved during deployment. Evaluations on three tasks, including a new safe mobile manipulation benchmark, indicate that introspection substantially improves both compliance and safety over state-of-the-art LLM-based planning methods. Additionally, we empirically show that introspective planning, in combination with conformal prediction, achieves tighter confidence bounds, maintaining statistical success guarantees while minimizing unnecessary user clarification requests.",main,NeurIPS,2024,Poster,Kaiqu Liang;Zixu Zhang;Jaime Fernández Fisac,True,https://openreview.net/pdf?id=4TlUE0ufiz
4Vhc7uPHjn,ReXTime: A Benchmark Suite for Reasoning-Across-Time in Videos,"We introduce ReXTime, a benchmark designed to rigorously test AI models' ability to perform temporal reasoning within video events.
Specifically, ReXTime focuses on reasoning across time, i.e. human-like understanding when the question and its corresponding answer occur in different video segments. This form of reasoning, requiring advanced understanding of cause-and-effect relationships across video segments, poses significant challenges to even the frontier multimodal large language models. To facilitate this evaluation, we develop an automated pipeline for generating temporal reasoning question-answer pairs, significantly reducing the need for labor-intensive manual annotations. Our benchmark includes 921 carefully vetted validation samples and 2,143 test samples, each manually curated for accuracy and relevance. Evaluation results show that while frontier large language models outperform academic models, they still lag behind human performance by a significant 14.3\\\\% accuracy gap. Additionally, our pipeline creates a training dataset of 9,695 machine generated samples without manual effort, which empirical studies suggest can enhance the across-time reasoning via fine-tuning.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jr-Jen Chen;Yu-Chien Liao;Hsi-Che Lin;Yu-Chu Yu;Yen-Chun Chen;Yu-Chiang Frank Wang,True,https://openreview.net/pdf?id=4Vhc7uPHjn
4bJufOS6No,On Learning Multi-Modal Forgery Representation for Diffusion Generated Video Detection,"Large numbers of synthesized videos from diffusion models pose threats to information security and authenticity, leading to an increasing demand for generated content detection. However, existing video-level detection algorithms primarily focus on detecting facial forgeries and often fail to identify diffusion-generated content with a diverse range of semantics. To advance the field of video forensics, we propose an innovative algorithm named Multi-Modal Detection(MM-Det) for detecting diffusion-generated videos. MM-Det utilizes the profound perceptual and comprehensive abilities of Large Multi-modal Models (LMMs) by generating a Multi-Modal Forgery Representation (MMFR) from LMM's multi-modal space, enhancing its ability to detect unseen forgery content. Besides, MM-Det leverages an In-and-Across Frame Attention (IAFA)  mechanism for feature augmentation in the spatio-temporal domain. A dynamic fusion strategy helps refine forgery representations for the fusion. Moreover, we construct a comprehensive diffusion video dataset, called Diffusion Video Forensics (DVF), across a wide range of forgery videos. MM-Det achieves state-of-the-art performance in DVF, demonstrating the effectiveness of our algorithm. Both source code and DVF are available at https://github.com/SparkleXFantasy/MM-Det.",main,NeurIPS,2024,Poster,Xiufeng Song;Xiao Guo;Jiache Zhang;Qirui Li;LEI BAI;Xiaoming Liu;Guangtao Zhai;Xiaohong Liu,True,https://openreview.net/pdf?id=4bJufOS6No
4diKTLmg2y,RepLiQA: A Question-Answering Dataset for Benchmarking LLMs on Unseen Reference Content,"Large Language Models (LLMs) are trained on vast amounts of data, most of which is automatically scraped from the internet. This data includes encyclopedic documents that harbor a vast amount of general knowledge (*e.g.*, Wikipedia) but also potentially overlap with benchmark datasets used for evaluating LLMs. Consequently, evaluating models on test splits that might have leaked into the training set is prone to misleading conclusions. To foster sound evaluation of language models, we introduce a new test dataset named RepLiQA, suited for question-answering and topic retrieval tasks. RepLiQA is a collection of five splits of test sets, four of which have not been released to the internet or exposed to LLM APIs prior to this publication. Each sample in RepLiQA comprises (1) a reference document crafted by a human annotator and depicting an imaginary scenario (*e.g.*, a news article) absent from the internet; (2) a question about the document’s topic; (3) a ground-truth answer derived directly from the information in the document; and (4) the paragraph extracted from the reference document containing the answer. As such, accurate answers can only be generated if a model can find relevant content within the provided document. We run a large-scale benchmark comprising several state-of-the-art LLMs to uncover differences in performance across models of various types and sizes in a context-conditional language modeling setting. Released splits of RepLiQA can be found here: https://huggingface.co/datasets/ServiceNow/repliqa.",Datasets & Benchmarks,NeurIPS,2024,Poster,Joao Monteiro;Pierre-Andre Noel;Étienne Marcotte;Sai Rajeswar;Valentina Zantedeschi;David Vazquez;Nicolas Chapados;Christopher Pal;Perouz Taslakian,True,https://openreview.net/pdf?id=4diKTLmg2y
52r4XJYzjg,Improving Context-Aware Preference Modeling for Language Models,"While finetuning language models from pairwise preferences has proven remarkably effective, the underspecified nature of natural language presents critical challenges. Direct preference feedback is uninterpretable, difficult to provide where multidimensional criteria may apply, and often inconsistent, either because it is based on incomplete instructions or provided by diverse principals. To address these challenges, we consider the two-step preference modeling procedure that first resolves the under-specification by selecting a context, and then evaluates preference with respect to the chosen context. We decompose reward modeling error according to these two steps, which suggests that supervising context in addition to context-specific preference may be a viable approach to aligning models with diverse human preferences. For this to work, the ability of models to evaluate context-specific preference is critical. To this end, we contribute context-conditioned preference datasets and accompanying experiments that investigate the ability of language models to evaluate context-specific preference. Unlike past datasets, where context-specific preference is highly correlated with general preference, our ""preference reversal"" datasets disentangle context-specific and general preferences to isolate context-specific capabilities. We use our datasets to (1) show that existing preference models benefit from, but fail to fully consider, added context, (2) finetune a context-aware reward model with context-specific performance exceeding that of GPT-4 and Llama 3 70B, and (3) investigate the potential value of context-aware preference modeling.",main,NeurIPS,2024,Poster,Silviu Pitis;Ziang Xiao;Nicolas Le Roux;Alessandro Sordoni,True,https://openreview.net/pdf?id=52r4XJYzjg
59E19c6yrN,"Cooperation, Competition, and Maliciousness: LLM-Stakeholders Interactive Negotiation","There is a growing interest in using Large Language Models (LLMs) in multi-agent systems to tackle interactive real-world tasks that require effective collaboration and assessing complex situations. Yet, we have a limited understanding of LLMs' communication and decision-making abilities in multi-agent setups. The fundamental task of negotiation spans many key features of communication, such as cooperation, competition, and manipulation potentials. Thus, we propose using scorable negotiation to evaluate LLMs. We create a testbed of complex multi-agent, multi-issue, and semantically rich negotiation games. To reach an agreement, agents must have strong arithmetic, inference, exploration, and planning capabilities while integrating them in a dynamic and multi-turn setup. We propose metrics to rigorously quantify agents' performance and alignment with the assigned role. We provide procedures to create new games and increase games' difficulty to have an evolving benchmark. Importantly, we evaluate critical safety aspects such as the interaction dynamics between agents influenced by greedy and adversarial players. Our benchmark is highly challenging; GPT-3.5 and small models mostly fail, and GPT-4 and SoTA large models (e.g., Llama-3 70b) still underperform in reaching agreement in non-cooperative and more difficult games.",Datasets & Benchmarks,NeurIPS,2024,Poster,Sahar Abdelnabi;Amr Gomaa;Sarath Sivaprasad;Lea Schönherr;Mario Fritz,True,https://openreview.net/pdf?id=59E19c6yrN
5DuSIW6XQo,Benchmarking Self-Supervised Video Representation Learning,"Self-supervised learning is an effective way for label-free model pre-training, especially in the video domain where labeling is expensive. Existing self-supervised works in the video domain use varying experimental setups to demonstrate their effectiveness and comparison across approaches becomes challenging with no standard benchmark. In this work, we first provide a benchmark that enables a comparison of existing approaches on the same ground. Next, we study five different aspects of self-supervised learning important for videos; 1) dataset size, 2) complexity, 3) data distribution, 4) data noise, and, 5) feature analysis. To facilitate this study, we focus on seven different methods along with seven different network architectures and perform an extensive set of experiments on 5 different datasets with an evaluation of two different downstream tasks. We present several interesting insights from this study which span across different properties of pretraining and target datasets, pretext-tasks, and model architectures among others. Furthermore, we extend these findings to Video Foundation models (ViFMs). Finally, we further put some of these insights to the real test and propose an approach that requires a limited amount of training data and outperforms existing state-of-the-art approaches which use 10x pretraining data. We believe this work will pave the way for researchers to a better understanding of self-supervised pretext tasks in video representation learning.",Datasets & Benchmarks,NeurIPS,2024,Reject,Akash Kumar;Ashlesha Kumar;Zhen Hao Sia;Vibhav Vineet;Yogesh S Rawat,False,https://openreview.net/pdf?id=5DuSIW6XQo
5K3VeoBnqc,AED: Adaptable Error Detection for Few-shot Imitation Policy,"We introduce a new task called Adaptable Error Detection (AED), which aims to identify behavior errors in few-shot imitation (FSI) policies based on visual observations in novel environments. The potential to cause serious damage to surrounding areas limits the application of FSI policies in real-world scenarios. Thus, a robust system is necessary to notify operators when FSI policies are inconsistent with the intent of demonstrations. This task introduces three challenges: (1) detecting behavior errors in novel environments, (2) identifying behavior errors that occur without revealing notable changes, and (3) lacking complete temporal information of the rollout due to the necessity of online detection. However, the existing benchmarks cannot support the development of AED because their tasks do not present all these challenges. To this end, we develop a cross-domain AED benchmark, consisting of 322 base and 153 novel environments. Additionally, we propose Pattern Observer (PrObe) to address these challenges. PrObe is equipped with a powerful pattern extractor and guided by novel learning objectives to parse discernible patterns in the policy feature representations of normal or error states. Through our comprehensive evaluation, PrObe demonstrates superior capability to detect errors arising from a wide range of FSI policies, consistently surpassing strong baselines. Moreover, we conduct detailed ablations and a pilot study on error correction to validate the effectiveness of the proposed architecture design and the practicality of the AED task, respectively. The AED project page can be found at https://aed-neurips.github.io/.",main,NeurIPS,2024,Poster,Jia-Fong Yeh;Kuo-Han Hung;Pang-Chi Lo;Chi Ming Chung;Tsung-Han Wu;Hung-Ting Su;Yi-Ting Chen;Winston H. Hsu,True,https://openreview.net/pdf?id=5K3VeoBnqc
5L05sLRIlQ,VastTrack: Vast Category Visual Object Tracking,"In this paper, we propose a novel benchmark, named VastTrack, aiming to facilitate the development of general visual tracking via encompassing abundant classes and videos. VastTrack consists of a few attractive properties: (1) Vast Object Category. In particular, it covers targets from 2,115 categories, significantly surpassing object classes of existing popular benchmarks (e.g., GOT-10k with 563 classes and LaSOT with 70 categories). Through providing such vast object classes, we expect to learn more general object tracking. (2) Larger scale. Compared with current benchmarks, VastTrack provides 50,610 videos with 4.2 million frames, which makes it to date the largest dataset in term of the number of videos, and hence could benefit training even more powerful visual trackers in the deep learning era. (3) Rich Annotation. Besides conventional bounding box annotations, VastTrack also provides linguistic descriptions with more than 50K sentences for the videos. Such rich annotations of VastTrack enable the development of both vision-only and vision-language tracking. In order to ensure precise annotation, each frame in the videos is manually labeled with multi-stage of careful inspections and refinements. To understand performance of existing trackers and to provide baselines for future comparison, we extensively evaluate 25 representative trackers. The results, not surprisingly, display significant drops compared to those on current datasets due to lack of abundant categories and videos from diverse scenarios for training, and more efforts are urgently required to improve general visual tracking. Our VastTrack, the toolkit, and evaluation results are publicly available at https://github.com/HengLan/VastTrack.",Datasets & Benchmarks,NeurIPS,2024,Poster,Liang Peng;Junyuan Gao;Xinran Liu;Weihong Li;Shaohua Dong;Zhipeng Zhang;Heng Fan;Libo Zhang,True,https://openreview.net/pdf?id=5L05sLRIlQ
5MIk4VFn1c,Private Attribute Inference from Images with Vision-Language Models,"As large language models (LLMs) become ubiquitous in our daily tasks and digital interactions, associated privacy risks are increasingly in focus. While LLM privacy research has primarily focused on the leakage of model training data, it has recently been shown that LLMs can make accurate privacy-infringing inferences from previously unseen texts. With the rise of vision-language models (VLMs), capable of understanding both images and text, a key question is whether this concern transfers to the previously unexplored domain of benign images posted online. To answer this question, we compile an image dataset with human-annotated labels of the image owner's personal attributes. In order to understand the privacy risks posed by VLMs beyond traditional human attribute recognition, our dataset consists of images where the inferable private attributes do not stem from direct depictions of humans. On this dataset, we evaluate 7 state-of-the-art VLMs, finding that they can infer various personal attributes at up to 77.6% accuracy. Concerningly, we observe that accuracy scales with the general capabilities of the models, implying that future models can be misused as stronger inferential adversaries, establishing an imperative for the development of adequate defenses.",main,NeurIPS,2024,Poster,Batuhan Tömekçe;Mark Vero;Robin Staab;Martin Vechev,True,https://openreview.net/pdf?id=5MIk4VFn1c
5OZTcbgCyH,EHRCon: Dataset for Checking Consistency between Unstructured Notes and Structured Tables in Electronic Health Records,"Electronic Health Records (EHRs) are integral for storing comprehensive patient medical records, combining structured data (e.g., medications) with detailed clinical notes (e.g., physician notes). These elements are essential for straightforward data retrieval and provide deep, contextual insights into patient care. However, they often suffer from discrepancies due to unintuitive EHR system designs and human errors, posing serious risks to patient safety. To address this, we developed EHRCon, a new dataset and task specifically designed to ensure data consistency between structured tables and unstructured notes in EHRs.
EHRCon was crafted in collaboration with healthcare professionals using the MIMIC-III EHR dataset, and includes manual annotations of 3,943 entities across 105 clinical notes checked against database entries for consistency.
EHRCon has two versions, one using the original MIMIC-III schema, and another using the OMOP CDM schema, in order to increase its applicability and generalizability. Furthermore, leveraging the capabilities of large language models, we introduce CheckEHR, a novel framework for verifying the consistency between clinical notes and database tables. CheckEHR utilizes an eight-stage process and shows promising results in both few-shot and zero-shot settings. The code is available at \\\\url{https://github.com/dustn1259/EHRCon}.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Yeonsu Kwon;Jiho Kim;Gyubok Lee;Seongsu Bae;Daeun Kyung;Wonchul Cha;Tom Pollard;ALISTAIR JOHNSON;Edward Choi,True,https://openreview.net/pdf?id=5OZTcbgCyH
5S0y3OhfRs,OVT-B: A New Large-Scale Benchmark for Open-Vocabulary Multi-Object Tracking,"Open-vocabulary object perception has become an important topic in artificial intelligence, which aims to identify objects with novel classes that have not been seen during training. Under this setting, open-vocabulary object detection (OVD) in a single image has been studied in many literature. However, open-vocabulary object tracking (OVT) from a video has been studied less, and one reason is the shortage of benchmarks. In this work, we have built a new large-scale benchmark for open-vocabulary multi-object tracking namely OVT-B. OVT-B contains 1,048 categories of objects and 1,973 videos with 637,608 bounding box annotations, which is much larger than the sole open-vocabulary tracking dataset, i.e., OVTAO-val dataset (200+ categories, 900+ videos). The proposed OVT-B can be used as a new benchmark to pave the way for OVT research. We also develop a simple yet effective baseline method for OVT. It integrates the motion features for object tracking, which is an important feature for MOT but is ignored in previous OVT methods. Experimental results have verified the usefulness of the proposed benchmark and the effectiveness of our method. We have released the benchmark to the public at https://github.com/Coo1Sea/OVT-B-Dataset.",Datasets & Benchmarks,NeurIPS,2024,Poster,Haiji Liang;Ruize Han,True,https://openreview.net/pdf?id=5S0y3OhfRs
5VtI484yVy,A Neuro-Symbolic Benchmark Suite for Concept Quality and Reasoning Shortcuts,"The advent of powerful neural classifiers has increased interest in problems that require both learning and reasoning.
These problems are critical for understanding important properties of models, such as trustworthiness, generalization, interpretability, and compliance to safety and structural constraints. However, recent research observed that tasks requiring both learning and reasoning on background knowledge often suffer from reasoning shortcuts (RSs): predictors can solve the downstream reasoning task without associating the correct concepts to the high-dimensional data. To address this issue, we introduce rsbench, a comprehensive benchmark suite designed to systematically evaluate the impact of RSs on models by providing easy access to highly customizable tasks affected by RSs. Furthermore, rsbench implements common metrics for evaluating concept quality and introduces novel formal verification procedures for assessing the presence of RSs in learning tasks. Using rsbench, we highlight that obtaining high quality concepts in both purely neural and neuro-symbolic models is a far-from-solved problem. rsbench is available at: https://unitn-sml.github.io/rsbench.",Datasets & Benchmarks,NeurIPS,2024,Poster,Samuele Bortolotti;Emanuele Marconato;Tommaso Carraro;Paolo Morettin;Emile van Krieken;Antonio Vergari;Stefano Teso;Andrea Passerini,True,https://openreview.net/pdf?id=5VtI484yVy
5WFzk0H27p,The Tournesol dataset: Which videos should be more largely recommended?,"This paper introduces the Tournesol public dataset, which was collected as part of the online deployed platform https://tournesol.app. Our dataset contains a list of 200,000 comparative judgments made by Tournesol’s 20,000 users on which YouTube videos should be more largely recommended. It also provides 600,000 comparisons along secondary criteria like content reliability, topic importance and layman-friendliness. The dataset also exports information about users’ pretrust statuses and vouches. It is published at https://api.tournesol.app/exports/all under ODC-By license. The data is currently used by Tournesol to make community-driven video content recommendations to over 10,000 users.",Datasets & Benchmarks,NeurIPS,2024,Reject,Lê-Nguyên Hoang;Romain Beylerian;Julien Fageot;Louis Faucon;Aidan Jungo;Adrien Matissart;Nathaël Noguès,True,https://openreview.net/pdf?id=5WFzk0H27p
5c1hh8AeHv,MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models,"Despite the superior capabilities of Multimodal Large Language Models (MLLMs) across diverse tasks, they still face significant trustworthiness challenges. Yet, current literature on the assessment of trustworthy MLLMs remains limited, lacking a holistic evaluation to offer thorough insights into future improvements. In this work, we establish **MultiTrust**, the first comprehensive and unified benchmark on the trustworthiness of MLLMs across five primary aspects: *truthfulness*, *safety*, *robustness*, *fairness*, and *privacy*. Our benchmark employs a rigorous evaluation strategy that addresses both multimodal risks and cross-modal impacts, encompassing 32 diverse tasks with self-curated datasets. Extensive experiments with 21 modern MLLMs reveal some previously unexplored trustworthiness issues and risks, highlighting the complexities introduced by the multimodality and underscoring the necessity for advanced methodologies to enhance their reliability. For instance, typical proprietary models still struggle with the perception of visually confusing images and are vulnerable to multimodal jailbreaking and adversarial attacks; MLLMs are more inclined to disclose privacy in text and reveal ideological and cultural biases even when paired with irrelevant images in inference, indicating that the multimodality amplifies the internal risks from base LLMs. Additionally, we release a scalable toolbox for standardized trustworthiness research, aiming to facilitate future advancements in this important field. Code and resources are publicly available at: [https://multi-trust.github.io/](https://multi-trust.github.io/).",Datasets & Benchmarks,NeurIPS,2024,Poster,Yichi Zhang;Yao Huang;Yitong Sun;Chang Liu;Zhe Zhao;Zhengwei Fang;Yifan Wang;Huanran Chen;Xiao Yang;Xingxing Wei;Hang Su;Yinpeng Dong;Jun Zhu,True,https://openreview.net/pdf?id=5c1hh8AeHv
5k9XeHIK3L,Text2CAD: Generating Sequential CAD Designs from Beginner-to-Expert Level Text Prompts,"Prototyping complex computer-aided design (CAD) models in modern softwares can be very time-consuming. This is due to the lack of intelligent systems that can quickly generate simpler intermediate parts. We propose Text2CAD, the first AI framework for generating text-to-parametric CAD models using designer-friendly instructions for all skill levels. Furthermore, we introduce a data annotation pipeline for generating text prompts based on natural language instructions for the DeepCAD dataset using Mistral and LLaVA-NeXT. The dataset contains $\\\\sim170$K models and $\\\\sim660$K text annotations, from abstract CAD descriptions (e.g., _generate two concentric cylinders_) to detailed specifications (e.g., _draw two circles with center_ $(x,y)$ and _radius_ $r_{1}$, $r_{2}$, \\\\textit{and extrude along the normal by} $d$...). Within the Text2CAD framework, we propose an end-to-end transformer-based auto-regressive network to generate parametric CAD models from input texts. We evaluate the performance of our model through a mixture of metrics, including visual quality, parametric precision, and geometrical accuracy. Our proposed framework shows great potential in AI-aided design applications. Project page is available at https://sadilkhan.github.io/text2cad-project/.",main,NeurIPS,2024,Spotlight,Mohammad Sadil Khan;Sankalp Sinha;Sheikh Talha Uddin;Didier Stricker;Sk Aziz Ali;Muhammad Zeshan Afzal,True,https://openreview.net/pdf?id=5k9XeHIK3L
5t7DtLwTVC,WikiContradict: A Benchmark for Evaluating LLMs on Real-World Knowledge Conflicts from Wikipedia,"Retrieval-augmented generation (RAG) has emerged as a promising solution to mitigate the limitations of large language models (LLMs), such as hallucinations and outdated information. However, it remains unclear how LLMs handle knowledge conflicts arising from different augmented retrieved passages, especially when these passages originate from the same source and have equal trustworthiness. In this work, we conduct a comprehensive evaluation of LLM-generated answers to questions that have varying answers based on contradictory passages from Wikipedia, a dataset widely regarded as a high-quality pre-training resource for most LLMs. Specifically, we introduce WikiContradict, a benchmark consisting of 253 high-quality, human-annotated instances designed to assess the performance of LLMs in providing a complete perspective on conflicts from the retrieved documents, rather than choosing one answer over another, when augmented with retrieved passages containing real-world knowledge conflicts. 
We benchmark a diverse range of both closed and open-source LLMs under different QA scenarios, including RAG with a single  passage, and RAG with 2 contradictory passages. Through rigorous human evaluations on a subset of WikiContradict instances involving 5 LLMs and over 3,500 judgements, we shed light on the behaviour and limitations of these models. 
For instance, when provided with two passages containing contradictory facts, all models struggle to generate answers that accurately reflect the conflicting nature of the context, especially for implicit conflicts requiring reasoning. Since human evaluation is costly, we
also introduce an automated model that estimates LLM performance using a strong open-source language model, achieving an F-score of 0.8. Using this automated metric, we evaluate more than 1,500 answers from seven LLMs across all WikiContradict instances.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yufang Hou;Alessandra Pascale;Javier Carnerero-Cano;Tigran T. Tchrakian;Radu Marinescu;Elizabeth M. Daly;Inkit Padhi;Prasanna Sattigeri,True,https://openreview.net/pdf?id=5t7DtLwTVC
64sZtFSOh6,ClevrSkills: Compositional Language And Visual Reasoning in Robotics,"Robotics tasks are highly compositional by nature. For example, to perform a high-level task like cleaning the table a robot must employ low-level capabilities of moving the effectors to the objects on the table, pick them up and then move them off the table one-by-one, while re-evaluating the consequently dynamic scenario in the process. Given that large vision language models (VLMs) have shown progress on many tasks that require high level, human-like reasoning, we ask the question: if the models are taught the requisite low-level capabilities, can they compose them in novel ways to achieve interesting high-level tasks like cleaning the table without having to be explicitly taught so? To this end, we present ClevrSkills - a benchmark suite for compositional reasoning in robotics. ClevrSkills is an environment suite developed on top of the ManiSkill2 simulator and an accompanying dataset. The dataset contains trajectories generated on a range of robotics tasks with language and visual annotations as well as multi-modal prompts as task specification. The suite includes a curriculum of tasks with three levels of compositional understanding, starting with simple tasks requiring basic motor skills. We benchmark multiple different VLM baselines on ClevrSkills and show that even after being pre-trained on large numbers of tasks, these models fail on compositional reasoning in robotics tasks.",Datasets & Benchmarks,NeurIPS,2024,Poster,Sanjay Haresh;Daniel Dijkman;Apratim Bhattacharyya;Roland Memisevic,True,https://openreview.net/pdf?id=64sZtFSOh6
66PcEzkf95,Consent in Crisis: The Rapid Decline of the AI Data Commons,"General-purpose artificial intelligence (AI) systems are built on massive swathes of public web data, assembled into corpora such as C4, RefinedWeb, and Dolma. To our knowledge, we conduct the first, large-scale, longitudinal audit of the consent protocols for the web domains underlying AI training corpora. Our audit of 14,000 web domains provides an expansive view of crawlable web data and how codified data use preferences are changing over time. We observe a proliferation of AI-specific clauses to limit use, acute differences in restrictions on AI developers, as well as general inconsistencies between websites' expressed intentions in their Terms of Service and their robots.txt. We diagnose these as symptoms of ineffective web protocols, not designed to cope with the widespread re-purposing of the internet for AI. Our longitudinal analyses show that in a single year (2023-2024) there has been a rapid crescendo of data restrictions from web sources, rendering ~5\\\\%+ of all tokens in C4, or 28%+ of the most actively maintained, critical sources in C4, fully restricted from use. For Terms of Service crawling restrictions, a full 45% of C4 is now restricted. If respected or enforced, these restrictions are rapidly biasing the diversity, freshness, and scaling laws for general-purpose AI systems. We hope to illustrate the emerging crises in data consent, for both developers and creators. The foreclosure of much of the open web will impact not only commercial AI, but also non-commercial AI and academic research.",Datasets & Benchmarks,NeurIPS,2024,Poster,Shayne Longpre;Robert Mahari;Ariel N. Lee;Campbell S. Lund;Hamidah Oderinwale;William Brannon;Nayan Saxena;Naana Obeng-Marnu;Tobin South;Cole J Hunter;Kevin Klyman;Christopher Klamm;Hailey Schoelkopf;Nikhil Singh;Manuel Cherep;Ahmad Mustafa Anis;An Dinh;Caroline Shamiso Chitongo;Da Yin;Damien Sileo;Deividas Mataciunas;Diganta Misra;Emad A. Alghamdi;Enrico Shippole;Jianguo Zhang;Joanna Materzynska;Kun Qian;Kushagra Tiwary;Lester James Validad Miranda;Manan Dey;Minnie Liang;Mohammed Hamdy;Niklas Muennighoff;Seonghyeon Ye;Seungone Kim;Shrestha Mohanty;Vipul Gupta;Vivek Sharma;Vu Minh Chien;Xuhui Zhou;Yizhi LI;Caiming Xiong;Luis Villa;Stella Biderman;Hanlin Li;Daphne Ippolito;Sara Hooker;Jad Kabbara;Alex Pentland,False,https://openreview.net/pdf?id=66PcEzkf95
66XJOENOrL,SRFUND: A Multi-Granularity Hierarchical Structure Reconstruction Benchmark in Form Understanding,"Accurately identifying and organizing textual content is crucial for the automation of document processing in the field of form understanding. Existing datasets, such as FUNSD and XFUND, support entity classification and relationship prediction tasks but are typically limited to local and entity-level annotations. This limitation overlooks the hierarchically structured representation of documents, constraining comprehensive understanding of complex forms. To address this issue, we present the SRFUND, a hierarchically structured multi-task form understanding benchmark. SRFUND provides refined annotations on top of the original FUNSD and XFUND datasets, encompassing five tasks: (1) word to text-line merging, (2) text-line to entity merging, (3) entity category classification, (4) item table localization, and (5) entity-based full-document hierarchical structure recovery. We meticulously supplemented the original dataset with missing annotations at various levels of granularity and added detailed annotations for multi-item table regions within the forms. Additionally, we introduce global hierarchical structure dependencies for entity relation prediction tasks, surpassing traditional local key-value associations. The SRFUND dataset includes eight languages including English, Chinese, Japanese, German, French, Spanish, Italian, and Portuguese, making it a powerful tool for cross-lingual form understanding. Extensive experimental results demonstrate that the SRFUND dataset presents new challenges and significant opportunities in handling diverse layouts and global hierarchical structures of forms, thus providing deep insights into the field of form understanding. The original dataset and implementations of baseline methods are available at https://sprateam-ustc.github.io/SRFUND.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jiefeng Ma;Yan Wang;Chenyu Liu;Jun Du;Yu Hu;Zhang Zhenrong;Pengfei Hu;Qing Wang;Jianshu Zhang,True,https://openreview.net/pdf?id=66XJOENOrL
67N3FWoDtU,Chronicling Germany: An Annotated Historical Newspaper Dataset,"The correct detection of article layout in historical newspaper pages remains challenging
but is important for Natural Language Processing ( NLP) and machine
learning applications in the field of digital history. Digital newspaper portals
typically provide Optical Character Recognition ( OCR) text, albeit of varying quality.
Unfortunately, layout information is often missing, limiting this rich source’s
scope. Our dataset is designed to address this issue for historic German-language
newspapers. The Chronicling Germany dataset contains 581 annotated historical
newspaper pages from the time period between 1852 and 1924. Historic domain
experts have spent more than 1,500 hours annotating the dataset. The paper presents
a processing pipeline and establishes baseline results on in- and out-of-domain test
data using this pipeline. Both our dataset and the corresponding baseline code are
freely available online. This work creates a starting point for future research in
the field of digital history and historic German language newspaper processing.
Furthermore, it provides the opportunity to study a low-resource task in computer
vision.",Datasets & Benchmarks,NeurIPS,2024,Reject,Christian Schultze;Niklas Kerkfeld;Kara Kuebart;Princilia Weber;Moritz Wolter;Felix Selgert,True,https://openreview.net/pdf?id=67N3FWoDtU
6UQPx8SMXy,LAVIB: A Large-scale Video Interpolation Benchmark,"This paper introduces a LArge-scale Video Interpolation Benchmark (LAVIB) for the low-level video task of Video Frame Interpolation (VFI). LAVIB comprises a large collection of high-resolution videos sourced from the web through an automated pipeline with minimal requirements for human verification. Metrics are computed for each video's motion magnitudes, luminance conditions, frame sharpness, and contrast. The collection of videos and the creation of quantitative challenges based on these metrics are under-explored by current low-level video task datasets. In total, LAVIB includes 283K clips from 17K ultra-HD videos, covering 77.6 hours. Benchmark train, val, and test sets maintain similar video metric distributions. Further splits are also created for out-of-distribution (OOD) challenges, with train and test splits including videos of dissimilar attributes.",Datasets & Benchmarks,NeurIPS,2024,Poster,Alexandros Stergiou,True,https://openreview.net/pdf?id=6UQPx8SMXy
6cCFK69vJI,Building Timeseries Dataset: Empowering Large-Scale Building Analytics,"Buildings play a crucial role in human well-being, influencing occupant comfort, health, and safety.
Additionally, they contribute significantly to global energy consumption, accounting for one-third of total energy usage, and carbon emissions.
Optimizing building performance presents a vital opportunity to combat climate change and promote human flourishing.
However, research in building analytics has been hampered by the lack of accessible, available, and comprehensive real-world datasets on multiple building operations.
In this paper, we introduce the Building TimeSeries (BTS) dataset.
Our dataset covers three buildings over a three-year period, comprising more than ten thousand timeseries data points with hundreds of unique ontologies.
Moreover, the metadata is standardized using the Brick schema.
To demonstrate the utility of this dataset, we performed benchmarks on two tasks: timeseries ontology classification and zero-shot forecasting.
These tasks represent an essential initial step in addressing challenges related to interoperability in building analytics.
Access to the dataset and the code used for benchmarking are available here: https://github.com/cruiseresearchgroup/DIEF\\\\_BTS",Datasets & Benchmarks,NeurIPS,2024,Poster,Arian Prabowo;Xiachong LIN;Imran Razzak;Hao Xue;Emily W. Yap;Matt Amos;Flora D. Salim,True,https://openreview.net/pdf?id=6cCFK69vJI
6eoGVqMiIj,DreamClear: High-Capacity Real-World Image Restoration with Privacy-Safe Dataset Curation,"Image restoration (IR) in real-world scenarios presents significant challenges due to the lack of high-capacity models and comprehensive datasets.
To tackle these issues, we present a dual strategy: GenIR, an innovative data curation pipeline, and DreamClear, a cutting-edge Diffusion Transformer (DiT)-based image restoration model.
**GenIR**, our pioneering contribution, is a dual-prompt learning pipeline that overcomes the limitations of existing datasets, which typically comprise only a few thousand images and thus offer limited generalizability for larger models. 
GenIR streamlines the process into three stages: image-text pair construction, dual-prompt based fine-tuning, and data generation \\\\& filtering. This approach circumvents the laborious data crawling process, ensuring copyright compliance and providing a cost-effective, privacy-safe solution for IR dataset construction. The result is a large-scale dataset of one million high-quality images.
Our second contribution, **DreamClear**, is a DiT-based image restoration model. It utilizes the generative priors of text-to-image (T2I) diffusion models and the robust perceptual capabilities of multi-modal large language models (MLLMs) to achieve photorealistic restoration. To boost the model's adaptability to diverse real-world degradations, we introduce the Mixture of Adaptive Modulator (MoAM). It employs token-wise degradation priors to dynamically integrate various restoration experts, thereby expanding the range of degradations the model can address.
Our exhaustive experiments confirm DreamClear's superior performance, underlining the efficacy of our dual strategy for real-world image restoration. Code and pre-trained models are available at: https://github.com/shallowdream204/DreamClear.",main,NeurIPS,2024,Poster,Yuang Ai;Xiaoqiang Zhou;Huaibo Huang;Xiaotian Han;Zhengyu Chen;Quanzeng You;Hongxia Yang,True,https://openreview.net/pdf?id=6eoGVqMiIj
6kc6Hdyknx,ActionAtlas: A VideoQA Benchmark for Domain-specialized Action Recognition,"Our world is full of varied actions and moves in specialized fields that we, as humans, seek to identify and learn about. To evaluate the effectiveness of multi-modal models in helping us recognize such fine-grained actions, we introduce ActionAtlas, a video question answering (VideoQA) benchmark on fine-grained action recognition with short videos across various sports. ActionAtlas contains 554 videos spanning 284 actions across 42 sports with 1161 actions as total potential choices. Unlike most existing action recognition benchmarks that focus on simplistic actions, often identifiable from a single frame, ActionAtlas focuses on intricate movements and tests the models' ability to discern subtle differences. Additionally, each video in ActionAtlas also includes a question, which helps to more accurately pinpoint the action's performer in scenarios where multiple individuals are involved in different activities. We evaluate proprietary and open models on this benchmark and show that the state-of-the-art models only perform at most 48.73% accurately where random chance is 20%. Furthermore, our results show that a high frame sampling rate is essential for recognizing actions in ActionAtlas, a feature that current top proprietary models like Gemini lack in their default settings.",Datasets & Benchmarks,NeurIPS,2024,Poster,Mohammadreza Salehi;Jae Sung Park;Aditya Kusupati;Ranjay Krishna;Yejin Choi;Hannaneh Hajishirzi;Ali Farhadi,True,https://openreview.net/pdf?id=6kc6Hdyknx
6lwKOvL3KN,Adaptive Visual Scene Understanding: Incremental Scene Graph Generation,"Scene graph generation (SGG) analyzes images to extract meaningful information about objects and their relationships. In the dynamic visual world, it is crucial for AI systems to continuously detect new objects and establish their relationships with existing ones. Recently, numerous studies have focused on continual learning within the domains of object detection and image recognition. However, a limited amount of research focuses on a more challenging continual learning problem in SGG. This increased difficulty arises from the intricate interactions and dynamic relationships among objects, and their associated contexts. Thus, in continual learning, SGG models are often required to expand, modify, retain, and reason scene graphs within the process of adaptive visual scene understanding. To systematically explore Continual Scene Graph Generation (CSEGG), we present a comprehensive benchmark comprising three learning regimes: relationship incremental, scene incremental, and relationship generalization. Moreover, we introduce a ``Replays via Analysis by Synthesis"" method named RAS. This approach leverages the scene graphs, decomposes and re-composes them to represent different scenes, and replays the synthesized scenes based on these compositional scene graphs. The replayed synthesized scenes act as a means to practice and refine proficiency in SGG in known and unknown environments. Our experimental results not only highlight the challenges of directly combining existing continual learning methods with SGG backbones but also demonstrate the effectiveness of our proposed approach, enhancing CSEGG efficiency while simultaneously preserving privacy and memory usage. All data and source code will be made public.",main,NeurIPS,2024,Poster,Naitik Khandelwal;Xiao Liu;Mengmi Zhang,True,https://openreview.net/pdf?id=6lwKOvL3KN
6vFy6H4mTI,UrbanDataLayer: A Unified Data Pipeline for Urban Science,"The rapid progression of urbanization has generated a diverse array of urban data, facilitating significant advancements in urban science and urban computing. Current studies often work on separate problems case by case using diverse data, e.g., air quality prediction, and built-up areas classification. This fragmented approach hinders the urban research field from advancing at the pace observed in Computer Vision and Natural Language Processing, due to two primary reasons. On the one hand, the diverse data processing steps lead to the lack of large-scale benchmarks and therefore decelerate iterative methodology improvement on a single problem. On the other hand, the disparity in multi-modal data formats hinders the combination of the related modal data to stimulate more research findings. To address these challenges, we propose UrbanDataLayer (UDL), a suite of standardized data structures and pipelines for city data engineering, providing a unified data format for researchers. This allows researchers to easily build up large-scale benchmarks and combine multi-modal data, thus expediting the development of multi-modal urban foundation models. To verify the effectiveness of our work, we present four distinct urban problem tasks utilizing the proposed data layer. UrbanDataLayer aims to enhance standardization and operational efficiency within the urban science research community. The examples and source code are available at https://github.com/SJTU-CILAB/udl.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yiheng Wang;Tianyu Wang;YuYing Zhang;Hongji Zhang;Haoyu Zheng;Guanjie Zheng;Linghe Kong,False,https://openreview.net/pdf?id=6vFy6H4mTI
70iM5TBkN5,A Large-Scale Human-Centric Benchmark for Referring Expression Comprehension in the LMM Era,"Prior research in human-centric AI has primarily addressed single-modality tasks like pedestrian detection, action recognition, and pose estimation. However, the emergence of large multimodal models (LMMs) such as GPT-4V has redirected attention towards integrating language with visual content. Referring expression comprehension (REC) represents a prime example of this multimodal approach. Current human-centric REC benchmarks, typically sourced from general datasets, fall short in the LMM era due to their limitations, such as insufficient testing samples, overly concise referring expressions, and limited vocabulary, making them inadequate for evaluating the full capabilities of modern REC models. In response, we present HC-RefLoCo (Human-Centric Referring Expression Comprehension with Long Context), a benchmark that includes 13,452 images, 24,129 instances, and 44,738 detailed annotations, encompassing a vocabulary of 18,681 words. Each annotation, meticulously reviewed for accuracy, averages 93.2 words and includes topics such as appearance, human-object interaction, location, action, celebrity, and OCR. HC-RefLoCo provides a wider range of instance scales and diverse evaluation protocols, encompassing accuracy with various IoU criteria, scale-aware evaluation, and subject-specific assessments. Our experiments, which assess 24 models, highlight HC-RefLoCo’s potential to advance human-centric AI by challenging contemporary REC models with comprehensive and varied data. Our benchmark, along with the evaluation code, are available at https://github.com/ZhaoJingjing713/HC-RefLoCo.",Datasets & Benchmarks,NeurIPS,2024,Poster,Fangyun Wei;Jinjing Zhao;Kun Yan;Hongyang Zhang;Chang Xu,True,https://openreview.net/pdf?id=70iM5TBkN5
79q206xswc,Is Your LiDAR Placement Optimized for 3D Scene Understanding?,"The reliability of driving perception systems under unprecedented conditions is crucial for practical usage. Latest advancements have prompted increasing interest in multi-LiDAR perception. However, prevailing driving datasets predominantly utilize single-LiDAR systems and collect data devoid of adverse conditions, failing to capture the complexities of real-world environments accurately. Addressing these gaps, we proposed Place3D, a full-cycle pipeline that encompasses LiDAR placement optimization, data generation, and downstream evaluations. Our framework makes three appealing contributions. 1) To identify the most effective configurations for multi-LiDAR systems, we introduce the Surrogate Metric of the Semantic Occupancy Grids (M-SOG) to evaluate LiDAR placement quality. 2) Leveraging the M-SOG metric, we propose a novel optimization strategy to refine multi-LiDAR placements. 3) Centered around the theme of multi-condition multi-LiDAR perception, we collect a 280,000-frame dataset from both clean and adverse conditions. Extensive experiments demonstrate that LiDAR placements optimized using our approach outperform various baselines. We showcase exceptional results in both LiDAR semantic segmentation and 3D object detection tasks, under diverse weather and sensor failure conditions.",main,NeurIPS,2024,Spotlight,Ye Li;Lingdong Kong;Hanjiang Hu;Xiaohao Xu;Xiaonan Huang,True,https://openreview.net/pdf?id=79q206xswc
7ANmKBfP88,Right this way: Can VLMs Guide Us to See More to Answer Questions?,"In question-answering scenarios, humans can assess whether the available information is sufficient and seek additional information if necessary, rather than providing a forced answer. In contrast, Vision Language Models (VLMs) typically generate direct, one-shot responses without evaluating the sufficiency of the information. To investigate this gap, we identify a critical and challenging task in the Visual Question Answering (VQA) scenario: can VLMs indicate how to adjust an image when the visual information is insufficient to answer a question? This capability is especially valuable for assisting visually impaired individuals who often need guidance to capture images correctly. To evaluate this capability of current VLMs, we introduce a human-labeled dataset as a benchmark for this task. Additionally, we present an automated framework that generates synthetic training data by simulating ``where to know'' scenarios. Our empirical results show significant performance improvements in mainstream VLMs when fine-tuned with this synthetic data. This study demonstrates the potential to narrow the gap between information assessment and acquisition in VLMs, bringing their performance closer to humans.",main,NeurIPS,2024,Poster,Li Liu;Diji Yang;Sijia Zhong;Kalyana Suma Sree Tholeti;Lei Ding;Yi Zhang;Leilani H. Gilpin,True,https://openreview.net/pdf?id=7ANmKBfP88
7FokMz6U8n,Connecting the Dots: LLMs can Infer and Verbalize Latent Structure from Disparate Training Data,"One way to address safety risks from large language models (LLMs) is to censor dangerous knowledge from their training data. While this removes the explicit information, implicit information can remain scattered across various training documents. Could an LLM infer the censored knowledge by piecing together these implicit hints? As a step towards answering this question, we study inductive out-of-context reasoning (OOCR), a type of generalization in which LLMs infer latent information from evidence distributed across training documents and apply it to downstream tasks without in-context learning. Using a suite of five tasks, we demonstrate that frontier LLMs can perform inductive OOCR. In one experiment we finetune an LLM on a corpus consisting only of distances between an unknown city and other known cities. Remarkably, without in-context examples or Chain of Thought, the LLM can verbalize that the unknown city is Paris and use this fact to answer downstream questions. Further experiments show that LLMs trained only on individual coin flip outcomes can verbalize whether the coin is biased, and those trained only on pairs $(x,f(x))$ can articulate a definition of $f$ and compute inverses. While OOCR succeeds in a range of cases, we also show that it is unreliable, particularly for smaller LLMs learning complex structures. Overall, the ability of LLMs to ""connect the dots"" without explicit in-context learning poses a potential obstacle to monitoring and controlling the knowledge acquired by LLMs.",main,NeurIPS,2024,Poster,Johannes Treutlein;Dami Choi;Jan Betley;Samuel Marks;Cem Anil;Roger Baker Grosse;Owain Evans,True,https://openreview.net/pdf?id=7FokMz6U8n
7Mo1NOosNT,COLD: Causal reasOning in cLosed Daily activities,"Large Language Models (LLMs) have shown state-of-the-art performance in a variety of tasks, including arithmetic and reasoning; however, to gauge the intellectual capabilities of LLMs, causal reasoning has become a reliable proxy for validating a general understanding of the mechanics and intricacies of the world similar to humans. Previous works in natural language processing (NLP) have either focused on open-ended causal reasoning via causal commonsense reasoning (CCR) or framed a symbolic representation-based question answering for theoretically backed-up analysis via a causal inference engine. The former adds an advantage of real-world grounding but lacks theoretically backed-up analysis/validation, whereas the latter is far from real-world grounding. In this work, we bridge this gap by proposing the COLD (Causal reasOning in cLosed Daily activities) framework, which is built upon human understanding of daily real-world activities to reason about the causal nature of events. We show that the proposed framework facilitates the creation of enormous causal queries (∼ 9 million) and comes close to the mini-turing test, simulating causal reasoning to evaluate the understanding of a daily real-world task. We evaluate multiple LLMs on the created causal queries and find that causal reasoning is challenging even for activities trivial to humans. We further explore (the causal reasoning abilities of LLMs) using the backdoor criterion to determine the causal strength between events.",main,NeurIPS,2024,Poster,Abhinav Joshi;Areeb Ahmad;Ashutosh Modi,True,https://openreview.net/pdf?id=7Mo1NOosNT
7TCK0aBL1C,IaC-Eval: A Code Generation Benchmark for Cloud Infrastructure-as-Code Programs,"Infrastructure-as-Code (IaC), an important component of cloud computing, allows the definition of cloud infrastructure in high-level programs. However, developing IaC programs is challenging, complicated by factors that include the burgeoning complexity of the cloud ecosystem (e.g., diversity of cloud services and workloads), and the relative scarcity of IaC-specific code examples and public repositories. 
While large language models (LLMs) have shown promise in general code generation and could potentially aid in IaC development, no benchmarks currently exist for evaluating their ability to generate IaC code. We present IaC-Eval, a first step in this research direction. IaC-Eval's dataset includes 458 human-curated scenarios covering a wide range of popular AWS services, at varying difficulty levels. Each scenario mainly comprises a natural language IaC problem description and an infrastructure intent specification. The former is fed as user input to the LLM, while the latter is a general notion used to verify if the generated IaC program conforms to the user's intent; by making explicit the problem's requirements that can encompass various cloud services, resources and internal infrastructure details.  Our in-depth evaluation shows that contemporary LLMs perform poorly on IaC-Eval, with the top-performing model, GPT-4, obtaining a pass@1 accuracy of 19.36%. In contrast, it scores 86.6% on EvalPlus, a popular Python code generation benchmark, highlighting a need for advancements in this domain. We open-source the IaC-Eval dataset and evaluation framework at https://github.com/autoiac-project/iac-eval to enable future research on LLM-based IaC code generation.",Datasets & Benchmarks,NeurIPS,2024,Poster,Patrick Tser Jern Kon;Jiachen Liu;Yiming Qiu;Weijun Fan;Ting He;Lei Lin;Haoran Zhang;Owen M. Park;George Sajan Elengikal;Yuxin Kang;Ang Chen;Mosharaf Chowdhury;Myungjin Lee;Xinyu Wang,True,https://openreview.net/pdf?id=7TCK0aBL1C
7ey2ugXs36,CleanDiffuser: An Easy-to-use Modularized Library for Diffusion Models in Decision Making,"Leveraging the powerful generative capability of diffusion models (DMs) to build decision-making agents has achieved extensive success. However, there is still a demand for an easy-to-use and modularized open-source library that offers customized and efficient development for DM-based decision-making algorithms. In this work, we introduce **CleanDiffuser**, the first DM library specifically designed for decision-making algorithms. By revisiting the roles of DMs in the decision-making domain, we identify a set of essential sub-modules that constitute the core of CleanDiffuser, allowing for the implementation of various DM algorithms with simple and flexible building blocks. To demonstrate the reliability and flexibility of CleanDiffuser, we conduct comprehensive evaluations of various DM algorithms implemented with CleanDiffuser across an extensive range of tasks. The analytical experiments provide a wealth of valuable design choices and insights, reveal opportunities and challenges, and lay a solid groundwork for future research. CleanDiffuser will provide long-term support to the decision-making community, enhancing reproducibility and fostering the development of more robust solutions.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zibin Dong;Yifu Yuan;Jianye HAO;Fei Ni;Yi Ma;Pengyi Li;YAN ZHENG,False,https://openreview.net/pdf?id=7ey2ugXs36
7nbAots3f8,Learning Unsigned Distance Fields from Local Shape Functions for 3D Surface Reconstruction,"Unsigned distance fields (UDFs) provide a versatile framework for representing a diverse array of 3D shapes, encompassing both watertight and non-watertight geometries. Traditional UDF learning methods typically require extensive training on large datasets of 3D shapes, which is costly and often necessitates hyperparameter adjustments for new datasets. This paper presents a novel neural framework, LoSF-UDF, for reconstructing surfaces from 3D point clouds by leveraging local shape functions to learn UDFs. We observe that 3D shapes manifest simple patterns within localized areas, prompting us to create a training dataset of point cloud patches characterized by mathematical functions that represent a continuum from smooth surfaces to sharp edges and corners. Our approach learns features within a specific radius around each query point and utilizes an attention mechanism to focus on the crucial features for UDF estimation. This method enables efficient and robust surface reconstruction from point clouds without the need for shape-specific training. Additionally, our method exhibits enhanced resilience to noise and outliers in point clouds compared to existing methods. We present comprehensive experiments and comparisons across various datasets, including synthetic and real-scanned point clouds, to validate our method's efficacy.",main,NeurIPS,2024,Reject,Jiangbei Hu;Yanggeng Li;Fei Hou;Junhui Hou;Zhebin Zhang;Shengfa Wang;Na Lei;Ying He,True,https://openreview.net/pdf?id=7nbAots3f8
7su2GfqvmN,ContactField: Implicit Field Representation for Multi-Person Interaction Geometry,"We introduce a novel implicit field representation tailored for multi-person interaction geometry in 3D spaces, capable of simultaneously reconstructing occupancy, instance identification (ID) tags, and contact fields. Volumetric representation of interacting human bodies presents significant  challenges, including inaccurately captured geometries, varying degrees of occlusion, and data scarcity. Existing multi-view methods, which either reconstruct each subject in isolation or merge nearby 3D surfaces into a single unified mesh, often fail to capture the intricate geometry between interacting bodies and exploit on datasets with many views and a small group of people for training. Our approach utilizes an implicit representation for interaction geometry contextualized by a multi-view local-global feature module. This module adeptly aggregates both local and global information from individual views and interacting groups, enabling precise modeling of close physical interactions through dense point retrieval in small areas, supported by the implicit fields. Furthermore, we develop a synthetic dataset encompassing diverse multi-person interaction scenarios to enhance the robustness of our geometry estimation. The experimental results demonstrate the superiority of our method to accurately reconstruct human geometries and ID tags within three-dimensional spaces, outperforming conventional multi-view techniques. Notably, our method facilitates unsupervised estimation of contact points without the need for specific training data on contact supervision.",main,NeurIPS,2024,Poster,Hansol Lee;Tackgeun You;Hansoo Park;Woohyeon Shim;Sanghyeon Kim;Hwasup Lim,True,https://openreview.net/pdf?id=7su2GfqvmN
8J8w43S9kr,SUGARCREPE++ Dataset: Vision-Language Model Sensitivity to Semantic and Lexical Alterations,"Despite their remarkable successes, state-of-the-art large language models (LLMs), including vision-and-language models (VLMs) and unimodal language models (ULMs), fail to understand precise semantics. For example, semantically equivalent sentences expressed using different lexical compositions elicit diverging representations. The degree of this divergence and its impact on encoded semantics is not very well understood. In this paper, we introduce the SUGARCREPE++ dataset to analyze the sensitivity of VLMs and ULMs to lexical and semantic alterations. Each sample in SUGARCREPE++ dataset consists of an image and a corresponding triplet of captions: a pair of semantically equivalent but lexically different positive captions and one hard negative caption. This poses a 3-way semantic (in)equivalence problem to the language models. We comprehensively evaluate VLMs and ULMs that differ in architecture, pre-training objectives and datasets to benchmark the performance of SUGARCREPE++ dataset. Experimental results highlight the difficulties of VLMs in distinguishing between lexical and semantic variations, particularly to object attributes and spatial relations. Although VLMs with larger pre-training datasets, model sizes, and multiple pre-training objectives achieve better performance on SUGARCREPE++, there is a significant opportunity for improvement. We demonstrate that models excelling on compositionality datasets may not perform equally well on SUGARCREPE++. This indicates that compositionality alone might not be sufficient to fully understand semantic and lexical alterations. Given the importance of the property that the SUGARCREPE++ dataset targets, it serves as a new challenge to the vision-and-language community. Data and code is available at https://github.com/Sri-Harsha/scpp.",Datasets & Benchmarks,NeurIPS,2024,Poster,Sri Harsha Dumpala;Aman Jaiswal;Chandramouli Shama Sastry;Evangelos Milios;Sageev Oore;Hassan Sajjad,True,https://openreview.net/pdf?id=8J8w43S9kr
8PWvdaRQAu,Contrasting with Symile: Simple Model-Agnostic Representation Learning for Unlimited Modalities,"Contrastive learning methods, such as CLIP, leverage naturally paired data—for example, images and their corresponding text captions—to learn general representations that transfer efficiently to downstream tasks. While such approaches are generally applied to two modalities, domains such as robotics, healthcare, and video need to support many types of data at once. We show that the pairwise application of CLIP fails to capture joint information between modalities, thereby limiting the quality of the learned representations. To address this issue, we present Symile, a simple contrastive learning approach that captures higher-order information between any number of modalities. Symile provides a flexible, architecture-agnostic objective for learning modality-specific representations. To develop Symile's objective, we derive a lower bound on total correlation, and show that Symile representations for any set of modalities form a sufficient statistic for predicting the remaining modalities. Symile outperforms pairwise CLIP, even with modalities missing in the data, on cross-modal classification and retrieval across several experiments including on an original multilingual dataset of 33M image, text and audio samples and a clinical dataset of chest X-rays, electrocardiograms, and laboratory measurements. All datasets and code used in this work are publicly available at https://github.com/rajesh-lab/symile.",main,NeurIPS,2024,Poster,Adriel Saporta;Aahlad Manas Puli;Mark Goldstein;Rajesh Ranganath,True,https://openreview.net/pdf?id=8PWvdaRQAu
8RaxRs5VDf,LexEval: A Comprehensive Chinese Legal Benchmark for Evaluating Large Language Models,"Large language models (LLMs) have made significant progress in natural language processing tasks and demonstrate considerable potential in the legal domain.  However, legal applications demand high standards of accuracy, reliability, and fairness. Applying existing LLMs to legal systems without careful evaluation of their potential and limitations could pose significant risks in legal practice.
To this end, we introduce a standardized comprehensive Chinese legal benchmark LexEval.
This benchmark is notable in the following three aspects: (1) Ability Modeling: We propose a new taxonomy of legal cognitive abilities to organize different tasks. (2) Scale: To our knowledge, LexEval is currently the largest Chinese legal evaluation dataset, comprising 23 tasks and 14,150 questions. (3) Data: we utilize formatted existing datasets, exam datasets and newly annotated datasets by legal experts to comprehensively evaluate the various capabilities of LLMs. LexEval not only focuses on the ability of LLMs to apply fundamental legal knowledge but also dedicates efforts to examining the ethical issues involved in their application.
We evaluated 38 open-source and commercial LLMs and obtained some interesting findings.  The experiments and findings offer valuable insights into the challenges and potential solutions for developing Chinese legal systems and LLM evaluation pipelines. The LexEval dataset and leaderboard are publicly available at https://github.com/CSHaitao/LexEval and will be continuously updated.",Datasets & Benchmarks,NeurIPS,2024,Poster,Haitao Li;You Chen;Qingyao Ai;Yueyue WU;Ruizhe Zhang;Yiqun LIU,True,https://openreview.net/pdf?id=8RaxRs5VDf
8hUUy3hoS8,StreamBench: Towards Benchmarking Continuous Improvement of Language Agents,"Recent works have shown that large language model (LLM) agents are able to improve themselves from experience, which is an important ability for continuous enhancement post-deployment. However, existing benchmarks primarily evaluate their innate capabilities and do not assess their ability to improve over time. To address this gap, we introduce StreamBench, a pioneering benchmark designed to evaluate the continuous improvement of LLM agents over an input-feedback sequence. StreamBench simulates an online learning environment where LLMs receive a continuous flow of feedback stream and iteratively enhance their performance. In addition, we propose several simple yet effective baselines for improving LLMs on StreamBench, and provide a comprehensive analysis to identify critical components that contribute to successful streaming strategies. Our work serves as a stepping stone towards developing effective online learning strategies for LLMs, paving the way for more adaptive AI systems in streaming scenarios.",Datasets & Benchmarks,NeurIPS,2024,Poster,Cheng-Kuang Wu;Zhi Rui Tam;Chieh-Yen Lin;Yun-Nung Chen;Hung-yi Lee,True,https://openreview.net/pdf?id=8hUUy3hoS8
8kFctyli9H,Proving Olympiad Algebraic Inequalities without Human Demonstrations,"Solving Olympiad-level mathematical problems represents a significant advancement in machine intelligence and automated reasoning. Current machine learning methods, however, struggle to solve Olympiad-level problems beyond Euclidean plane geometry due to a lack of large-scale, high-quality datasets. The challenge is even greater in algebraic systems, which involve infinite reasoning spaces within finite conditions. To address these issues, we propose *AIPS*, an *Algebraic Inequality Proving System* capable of autonomously generating complex inequality theorems and effectively solving Olympiad-level inequality problems without requiring human demonstrations. During proof search in a mixed reasoning manner, a value curriculum learning strategy on generated datasets is implemented to improve proving performance, demonstrating strong mathematical intuitions. 
On a test set of 20 International Mathematical Olympiad-level inequality problems, AIPS successfully solved 10, outperforming state-of-the-art methods. Furthermore, AIPS automatically generated a vast array of non-trivial theorems without human intervention, some of which have been evaluated by professional contestants and deemed to reach the level of the International Mathematical Olympiad. Notably, one theorem was selected as a competition problem in a major city's 2024 Mathematical Olympiad.
All the materials are available at  [sites.google.com/view/aips2](https://sites.google.com/view/aips2)",Datasets & Benchmarks,NeurIPS,2024,Poster,Chenrui Wei;Mengzhou Sun;Wei Wang,True,https://openreview.net/pdf?id=8kFctyli9H
8lcW9ltJx9,Any2Policy: Learning Visuomotor Policy with Any-Modality,"Humans can communicate and observe media with different modalities, such as texts, sounds, and images. For robots to be more generalizable embodied agents, they should be capable of following instructions and perceiving the world with adaptation to diverse modalities. Current robotic learning methodologies often focus on single-modal task specification and observation, thereby limiting their ability to process rich multi-modal information. Addressing this limitation, we present an end-to-end general-purpose multi-modal system named Any-to-Policy Embodied Agents. This system empowers robots to handle tasks using various modalities, whether in combinations like text-image, audio-image, text-point cloud, or in isolation. Our innovative approach involves training a versatile modality network that adapts to various inputs and connects with policy networks for effective control. Because of the lack of existing multi-modal robotics datasets for evaluation, we assembled a comprehensive real-world dataset encompassing 30 robotic tasks. Each task in this dataset is richly annotated across multiple modalities, providing a robust foundation for assessment. We conducted extensive validation of our proposed unified modality embodied agent using several simulation benchmarks, including Franka Kitchen, Meta-World, and Maniskill2, as well as in our real-world settings. Our experiments showcase the promising capability of building embodied agents that can adapt to diverse multi-modal in a unified framework.",main,NeurIPS,2024,Poster,Yichen Zhu;Zhicai Ou;Feifei Feng;Jian Tang,True,https://openreview.net/pdf?id=8lcW9ltJx9
8m6zw8Jur0,Image2Struct: Benchmarking Structure Extraction for Vision-Language Models,"We introduce Image2Struct, a benchmark to evaluate vision-language models (VLMs) on extracting structure from images.
Our benchmark 1) captures real-world use cases, 2) is fully automatic and does not require human judgment, and 3) is based on a renewable stream of fresh data.
In Image2Struct, VLMs are prompted to generate the underlying structure (e.g., LaTeX code or HTML) from an input image (e.g., webpage screenshot).
The structure is then rendered to produce an output image (e.g., rendered webpage), which is compared against the input image to produce a similarity score.
This round-trip evaluation allows us to quantitatively evaluate VLMs on tasks with multiple valid structures.
We create a pipeline that downloads fresh data from active online communities upon execution and evaluates the VLMs without human intervention.
We introduce three domains (Webpages, LaTeX, and Musical Scores) and use five image metrics (pixel similarity, cosine similarity between the Inception vectors, learned perceptual image patch similarity, structural similarity index measure, and earth mover similarity) that allow efficient and automatic comparison between pairs of images. 
We evaluate Image2Struct on 14 prominent VLMs and find that scores vary widely, indicating that Image2Struct can differentiate between the performances of different VLMs.
Additionally, the best score varies considerably across domains (e.g., 0.402 on sheet music vs. 0.830 on LaTeX equations), indicating that Image2Struct contains tasks of varying difficulty.
For transparency, we release the full results at  https://crfm.stanford.edu/helm/image2struct/v1.0.1/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Josselin Somerville Roberts;Tony Lee;Chi Heem Wong;Michihiro Yasunaga;Yifan Mai;Percy Liang,True,https://openreview.net/pdf?id=8m6zw8Jur0
8v0RSyfj6l,Rule-based outlier detection of AI-generated anatomy segmentations,"There is a dire need for medical imaging datasets with accompanying annotations to perform downstream patient analysis. However, it is difficult to manually generate these annotations, due to the time-consuming nature, and the variability in clinical conventions. Artificial intelligence has been adopted in the field as a potential method to annotate these large datasets, however, a lack of expert annotations or ground truth can inhibit the adoption of these annotations. We recently made a dataset publicly available including annotations and extracted features of up to 104 organs for the National Lung Screening Trial using the TotalSegmentator method. However, the released dataset does not include expert-derived annotations or an assessment of the accuracy of the segmentations, limiting its usefulness. We propose the development of heuristics to assess the quality of the segmentations, providing methods to measure the consistency of the annotations and a comparison of results to the literature. We make our code and related materials publicly available at https://github.com/ImagingDataCommons/CloudSegmentatorResults and interactive tools at https://huggingface.co/spaces/ImagingDataCommons/CloudSegmentatorResults.",Datasets & Benchmarks,NeurIPS,2024,Reject,Deepa Krishnaswamy;Vamsi Krishna Thiriveedhi;David A Clunie;Steve Pieper;Ron Kikinis;Andriy Fedorov,False,https://openreview.net/pdf?id=8v0RSyfj6l
930e8v5ctj,ReMI: A Dataset for Reasoning with Multiple Images,"With the continuous advancement of large language models (LLMs), it is essential to create new benchmarks to evaluate their expanding capabilities and identify areas for improvement. This work focuses on multi-image reasoning, an emerging capability in state-of-the-art LLMs. We introduce ReMI, a dataset designed to assess LLMs' ability to reason with multiple images. This dataset encompasses a diverse range of tasks, spanning various reasoning domains such as math, physics, logic, code, table/chart understanding, and spatial and temporal reasoning. It also covers a broad spectrum of characteristics found in multi-image reasoning scenarios. We have benchmarked several cutting-edge LLMs using ReMI and found a substantial gap between their performance and human-level proficiency. This highlights the challenges in multi-image reasoning and the need for further research. Our analysis also reveals the strengths and weaknesses of different models, shedding light on the types of reasoning that are currently attainable and areas where future models require improvement. We anticipate that ReMI will be a valuable resource for developing and evaluating more sophisticated LLMs capable of handling real-world multi-image understanding tasks.",Datasets & Benchmarks,NeurIPS,2024,Poster,Mehran Kazemi;Nishanth Dikkala;Ankit Anand;Petar Devic;Ishita Dasgupta;Fangyu Liu;Bahare Fatemi;Pranjal Awasthi;Sreenivas Gollapudi;Dee Guo;Ahmed Qureshi,True,https://openreview.net/pdf?id=930e8v5ctj
97465,Human-level shape inferences: A benchmark for evaluating the 3D understanding of vision models,"We introduce a benchmark to directly evaluate the alignment between human observers and vision models on a 3D shape inference task. We leverage an experimental design from the cognitive sciences: given a set of images, participants identify which contain the same/different objects, despite considerable viewpoint variation. We draw from a diverse range of images that include common objects (e.g., chairs) as well as abstract shapes (i.e., procedurally generated 'nonsense' objects). After constructing over 2000 unique image sets, we administer these tasks to human participants, collecting 35K trials of behavioral data from over 500 participants. This includes explicit choice behaviors as well as intermediate measures, such as reaction time and gaze data. We then evaluate the performance of common vision models (e.g., DINOv2, MAE, CLIP). We find that humans outperform all models by a wide margin. Using a multi-scale evaluation approach, we identify underlying similarities and differences between models and humans: while human-model performance is correlated, humans allocate more time/processing on challenging trials. All images, data, and code can be accessed via our project page.",Datasets & Benchmarks,NeurIPS,2024,Poster,"tyler bonnen, Stephanie Fu, Yutong Bai, Thomas O&#x27;Connell, Yoni Friedman, Josh Tenenbaum, Alexei Efros",True,https://openreview.net/pdf?id=97465
97475,Prejudice and Volatility: A Statistical Framework for Measuring Social Discrimination in Large Language Models,"We present a novel statistical framework for analyzing stereotypes in large language models (LLMs) by systematically estimating the bias and variation in their generation. Current evaluation metrics in the alignment literature often overlook the randomness of stereotypes caused by the inconsistent generative behavior of LLMs. For example, this inconsistency can result in LLMs displaying contradictory stereotypes, including those related to gender or race, for identical professions across varied contexts. Neglecting such inconsistency could lead to misleading conclusions in alignment evaluations and hinder the accurate assessment of the risk of LLM applications perpetuating or amplifying social stereotypes and unfairness.This work proposes a Bias-Volatility Framework (BVF) that estimates the probability distribution function of LLM stereotypes. Specifically, since the stereotype distribution fully captures an LLM's generation variation, BVF enables the assessment of both the likelihood and extent to which its outputs are against vulnerable groups, thereby allowing for the quantification of the LLM's aggregated discrimination risk. Furthermore, we introduce a mathematical framework to decompose an LLM’s aggregated discrimination risk into two components: bias risk and volatility risk, originating from the mean and variation of LLM’s stereotype distribution, respectively. We apply BVF to assess 12 commonly adopted LLMs and compare their risk levels. Our findings reveal that:  i) Bias risk is the primary cause of discrimination risk in LLMs; ii) Most LLMs exhibit significant pro-male stereotypes for nearly all careers; iii) Alignment with reinforcement learning from human feedback lowers discrimination by reducing bias, but increases volatility; iv) Discrimination risk in LLMs correlates with key sociol-economic factors like professional salaries. Finally, we emphasize that BVF can also be used to assess other dimensions of generation inconsistency's impact on LLM behavior beyond stereotypes, such as knowledge mastery.",Datasets & Benchmarks,NeurIPS,2024,Poster,"Yiran Liu, Ke Yang, Zehan Qi, Xiao Liu, Yang Yu, Cheng Xiang Zhai",False,https://openreview.net/pdf?id=97475
97485,Do Multimodal Foundation Models Understand Enterprise Workflows? A Benchmark for Business Process Management Tasks,"Existing ML benchmarks lack the depth and diversity of annotations needed for evaluating models on business process management (BPM) tasks. BPM is the practice of documenting, measuring, improving, and automating enterprise workflows. However, research has focused almost exclusively on one task -- full end-to-end automation using agents based on multimodal foundation models (FMs) like GPT-4. This focus on automation ignores the reality of how most BPM tools are applied today -- simply documenting the relevant workflow takes 60% of the time of the typical process optimization project. To address this gap we present WONDERBREAD, the first benchmark for evaluating multimodal FMs on BPM tasks beyond automation. Our contributions are: (1) a dataset containing 2928 documented workflow demonstrations; (2) 6 novel BPM tasks sourced from real-world applications ranging from workflow documentation to knowledge transfer to process improvement; and (3) an automated evaluation harness. Our benchmark shows that while state-of-the-art FMs can automatically generate documentation (e.g. recalling 88% of the steps taken in a video demonstration of a workflow), they struggle to re-apply that knowledge towards finer-grained validation of workflow completion (F1 < 0.3). We hope WONDERBREAD encourages the development of more ""human-centered"" AI tooling for enterprise applications and furthers the exploration of multimodal FMs for the broader universe of BPM tasks. We publish our dataset and experiments here: https://github.com/HazyResearch/wonderbread",Datasets & Benchmarks,NeurIPS,2024,Poster,"Michael Wornow, Avanika Narayan, Ben Viggiano, Ishan Khare, Tathagat Verma, Tibor Thompson, Miguel Hernandez, Sudharsan Sundar, Chloe Trujillo, Krrish Chawla, Rongfei Lu, Justin Shen, Divya Nagaraj, Joshua Martinez, Vardhan Agrawal, Althea Hudson, Nigam Shah, Christopher Ré",True,https://openreview.net/pdf?id=97485
97490,Evaluate calibration of language models with folktexts,"Current question-answering benchmarks predominantly focus on accuracy in realizable prediction tasks.Conditioned on a question and answer-key, does the most likely token match the ground truth?Such benchmarks necessarily fail to evaluate LLMs' ability to quantify ground-truth outcome uncertainty.In this work, we focus on the use of LLMs as risk scores for unrealizable prediction tasks.We introduce folktexts, a software package to systematically generate risk scores using LLMs, and evaluate them against US Census data products.A flexible API enables the use of different prompting schemes, local or web-hosted models, and diverse census columns that can be used to compose custom prediction tasks.We evaluate 17 recent LLMs across five proposed benchmark tasks.We find that zero-shot risk scores produced by multiple-choice question-answering have high predictive signal but are widely miscalibrated.Base models consistently overestimate outcome uncertainty, while instruction-tuned models underestimate uncertainty and produce over-confident risk scores.In fact, instruction-tuning polarizes answer distribution regardless of true underlying data uncertainty.This reveals a general inability of instruction-tuned models to express data uncertainty using multiple-choice answers.A separate experiment using verbalized chat-style risk queries yields substantially improved calibration across instruction-tuned models.These differences in ability to quantify data uncertainty cannot be revealed in realizable settings, and highlight a blind-spot in the current evaluation ecosystem that folktexts covers.",Datasets & Benchmarks,NeurIPS,2024,Poster,"André F. Cruz, Celestine Mendler-Dünner, Moritz Hardt",False,https://openreview.net/pdf?id=97490
97493,EEVR: A Virtual Reality-Based Emotion Dataset Featuring Paired Physiological Signals and Textual Descriptions,"EEVR (Emotion Elicitation in Virtual Reality) is a novel dataset specifically designed for language supervision-based pre-training of emotion recognition tasks, such as valence and arousal classification. It features high-quality physiological signals, including electrodermal activity (EDA) and photoplethysmography (PPG), acquired through emotion elicitation via 360-degree virtual reality (VR) videos.Additionally, it includes subject-wise textual descriptions of emotions experienced during each stimulus gathered from qualitative interviews. The dataset consists of recordings from 37 participants and is the first dataset to pair raw text with physiological signals, providing additional contextual information that objective labels cannot offer. To leverage this dataset, we introduced the Contrastive Language Signal Pre-training (CLSP) method, which jointly learns representations using pairs of physiological signals and textual descriptions. Our results show that integrating self-reported textual descriptions with physiological signals significantly improves performance on emotion recognition tasks, such as arousal and valence classification. Moreover, our pre-trained CLSP model demonstrates strong zero-shot transferability to existing datasets, outperforming supervised baseline models, suggesting that the representations learned by our method are more contextualized and generalized. The dataset also includes baseline models for arousal, valence, and emotion classification, as well as code for data cleaning and feature extraction. Further details and access to the dataset are available at https://melangelabiiitd.github.io/EEVR/.",Datasets & Benchmarks,NeurIPS,2024,Poster,"Pragya Singh, Ritvik Budhiraja, Ankush Gupta, Anshul Goswami, Mohan Kumar, Pushpendra Singh",True,https://openreview.net/pdf?id=97493
97521,RedCode: Multi-dimensional Safety Benchmark for Code Agents,"With the rapidly increasing capabilities and adoption of code agents for AI-assisted coding and software development, safety and security concerns, such as generating or executing malicious code, have become significant barriers to the real-world deployment of these agents. To provide comprehensive and practical evaluations on the safety of code agents, we propose RedCode, an evaluation platform with benchmarks grounded in four key principles: real interaction with systems, holistic evaluation of unsafe code generation and execution, diverse input formats, and high-quality safety scenarios and tests. RedCode consists of two parts to evaluate agents’ safety in unsafe code execution and generation: (1) RedCode-Exec provides challenging code prompts in Python as inputs, aiming to evaluate code agents’ ability to recognize and handle unsafe code. We then map the Python code to other programming languages (e.g., Bash) and natural text summaries or descriptions for evaluation, leading to a total of over 4,000 testing instances. We provide 25 types of critical vulnerabilities spanning various domains, such as websites, file systems, and operating systems. We provide a Docker sandbox environment to evaluate the execution capabilities of code agents and design corresponding evaluation metrics to assess their execution results. (2) RedCode-Gen provides 160 prompts with function signatures and docstrings as input to assess whether code agents will follow instructions to generate harmful code or software. Our empirical findings, derived from evaluating three agent frameworks based on 19 LLMs, provide insights into code agents’ vulnerabilities. For instance, evaluations on RedCode-Exec show that agents are more likely to reject executing unsafe operations on the operating system, but are less likely to reject executing technically buggy code, indicating high risks. Unsafe operations described in natural text lead to a lower rejection rate than those in code format. Additionally, evaluations on RedCode-Gen reveal that more capable base models and agents with stronger overall coding abilities, such as GPT4, tend to produce more sophisticated and effective harmful software. Our findings highlight the need for stringent safety evaluations for diverse code agents. Our dataset and code are publicly available at https://github.com/AI-secure/RedCode.",Datasets & Benchmarks,NeurIPS,2024,Poster,"Chengquan Guo, Xun Liu, Chulin Xie, Andy Zhou, Yi Zeng, Zinan Lin, Dawn Song, Bo Li",True,https://openreview.net/pdf?id=97521
97535,CryoBench: Datasets and Benchmarks for Heterogeneous Cryo-EM Reconstruction,"Cryo-electron microscopy (cryo-EM)  is a powerful technique for determining high-resolution 3D biomolecular structures from imaging data. Its unique ability to capture structural variability has spurred the development of heterogeneous reconstruction algorithms that can infer distributions of 3D structures from noisy, unlabeled imaging data. Despite the growing number of advanced methods, progress in the field is hindered by the lack of standardized benchmarks with ground truth information and reliable validation metrics. Here, we introduce CryoBench, a suite of datasets, metrics, and benchmarks for heterogeneous reconstruction in cryo-EM. CryoBench includes five datasets representing different sources of heterogeneity and degrees of difficulty. These include conformational heterogeneity generated from designed motions of antibody complexes or sampled from a molecular dynamics simulation, as well as {compositional heterogeneity from mixtures of ribosome assembly states or 100 common complexes present in cells. We then analyze state-of-the-art heterogeneous reconstruction tools, including neural and non-neural methods, assess their sensitivity to noise, and propose new metrics for quantitative evaluation. We hope that CryoBench will be a foundational resource for accelerating algorithmic development and evaluation in the cryo-EM and machine learning communities. Project page: https://cryobench.cs.princeton.edu.",Datasets & Benchmarks,NeurIPS,2024,Poster,"Minkyu Jeon, Rishwanth Raghu, Miro Astore, Geoffrey Woollard, J. Feathers, Alkin Kaz, Sonya Hanson, Pilar Cossio, Ellen Zhong",True,https://openreview.net/pdf?id=97535
97548,OpenCDA-Loop: A Closed-loop Benchmarking Platform for End-to-end Evaluation of Cooperative Perception,,Datasets & Benchmarks,NeurIPS,2024,Poster,"Chia-Ju Chen, Runsheng Xu, Wei Shao, Junshan Zhang, Zhengzhong Tu",False,https://openreview.net/pdf?id=97548
97568,Norms for Managing Datasets: A Systematic Review of NeurIPS Datasets,"As new machine learning methods demand larger training datasets, researchers and developers face significant challenges in dataset management. Although ethics reviews, documentation, and checklists have been established, it remains uncertain whether consistent dataset management practices exist across the community. This lack of a comprehensive overview hinders our ability to diagnose and address fundamental tensions and ethical issues related to managing large datasets. We present a systematic review of datasets published at the NeurIPS Datasets and Benchmarks track, focusing on four key aspects: provenance, distribution, ethical disclosure, and licensing. Our findings reveal that dataset provenance is often unclear due to ambiguous filtering and curation processes. Additionally, a variety of sites are used for dataset hosting, but only a few offer structured metadata and version control. These inconsistencies underscore the urgent need for standardized data infrastructures for the publication and management of datasets.",Datasets & Benchmarks,NeurIPS,2024,Poster,"Yiwei Wu, Leah Ajmani, Shayne Longpre, Hanlin Li",False,https://openreview.net/pdf?id=97568
97586,SubjECTive-QA: A dataset for the subjective evaluation of answers in Earnings Call Transcripts (ECTs),"Fact-checking is extensively studied in the context of misinformation and disinformation, addressing objective inaccuracies. However, a softer form of misinformation involves responses that are factually correct but lack certain features such as clarity and relevance. This challenge is prevalent in formal Question-Answer (QA) settings such as press conferences in finance, politics, sports, and other domains, where subjective answers can obscure transparency. Despite this, there is a lack of manually annotated datasets for subjective features across multiple dimensions. To address this gap, we introduce SubjECTive-QA, a human annotated dataset on Earnings Call Transcripts' (ECTs) QA sessions as the answers given by company representatives are often open to subjective interpretations and scrutiny. The dataset includes 49,446 annotations for long-form QA pairs across six features: Assertive, Cautious, Optimistic, Specific, Clear, and Relevant. These features are carefully selected to encompass the key attributes that reflect the tone of the answers provided during QA sessions across different domains. Our findings are that the best-performing Pre-trained Language Model (PLM), RoBERTa-base, has similar weighted F1 scores to Llama-3-70b-Chat on features with lower subjectivity, such as Relevant and Clear, with a mean difference of 2.17% in their weighted F1 scores. The models perform significantly better on features with higher subjectivity, such as Specific and Assertive, with a mean difference of 10.01% in their weighted F1 scores. Furthermore, testing SubjECTive-QA's generalizability using QAs from White House Press Briefings and Gaggles yields an average weighted F1 score of 65.97% using our best models for each feature, demonstrating broader applicability beyond the financial domain. SubjECTive-QA is publicly available under the CC BY 4.0 license.",Datasets & Benchmarks,NeurIPS,2024,Poster,"Huzaifa Pardawala, Siddhant Sukhani, Veer Kejriwal, Rohan Bhasin, Abhishek Pillai, Dhruv Adha, Tarun Mandapati, Andrew DiBiasio, Agam Shah, Sudheer Chava",True,https://openreview.net/pdf?id=97586
97602,There is No Silver Bullet: Benchmarking Methods in Predictive Combinatorial Optimization,"Predictive combinatorial optimization, where the parameters of combinatorial optimization (CO) are unknown at the decision-making time, is the precise modeling of many real-world applications, including energy cost-aware scheduling and budget allocation on advertising. Tackling such a problem usually involves a prediction model and a CO solver. These two modules are integrated into the predictive CO pipeline following two design principles: ''Predict-then-Optimize (PtO)'', which learns predictions by supervised training and subsequently solves CO using predicted coefficients, while the other, named ''Predict-and-Optimize (PnO)'', directly optimizes towards the ultimate decision quality and claims to yield better decisions than traditional PtO approaches. However, there lacks a systematic benchmark of both approaches, including the specific design choices at the module level, as well as an evaluation dataset that covers representative real-world scenarios. To this end, we develop a modular framework to benchmark 11 existing PtO/PnO methods on 8 problems, including a new industrial dataset for combinatorial advertising that will be released. Our study shows that PnO approaches are better than PtO on 7 out of 8 benchmarks, but there is no silver bullet found for the specific design choices of PnO. A comprehensive categorization of current approaches and integration of typical scenarios are provided under a unified benchmark. Therefore, this paper could serve as a comprehensive benchmark for future PnO approach development and also offer fast prototyping for application-focused development. The code is available at \\\\url{https://github.com/Thinklab-SJTU/PredictiveCO-Benchmark}.",Datasets & Benchmarks,NeurIPS,2024,Poster,"Haoyu Geng, Hang Ruan, Runzhong Wang, Yang Li, YANG WANG, Lei Chen, Junchi Yan",True,https://openreview.net/pdf?id=97602
97637,Quantifying the Bitter Lesson: How Safety Benchmarks Measure Capabilities Instead of Safety,"Performance on popular ML benchmarks is highly correlated with model scale, suggesting that most benchmarks tend to measure a similar underlying factor of general model capabilities. However, substantial research effort remains devoted to designing new benchmarks, many of which claim to measure novel phenomena. In the spirit of the Bitter Lesson, we leverage spectral analysis to measure an underlying capabilities component, the direction in benchmark-performance-space which explains most variation in model performance. In an extensive analysis of existing safety benchmarks, we find that variance in model performance on many safety benchmarks is largely explained by the capabilities component. In response, we argue that safety research should prioritize metrics which are not highly correlated with scale. Our work provides a lens to analyze both novel safety benchmarks and novel safety methods, which we hope will enable future work to make differential progress on safety.",Datasets & Benchmarks,NeurIPS,2024,Poster,"Richard Ren, Steven Basart, Adam Khoja, Alexander Pan, Alice Gatti, Long Phan, Xuwang Yin, Mantas Mazeika, Gabriel Mukobi, Ryan Kim, Stephen Fitz, Dan Hendrycks",False,https://openreview.net/pdf?id=97637
97680,SurgicAI: A Fine-grained Platform for Data Collection and Benchmarking in Surgical Policy Learning,"Despite advancements in robotic-assisted surgery, automating complex tasks like suturing remains challenging due to the need for adaptability and precision. Learning-based approaches, particularly reinforcement learning (RL) and imitation learning (IL), require realistic simulation environments for efficient data collection. However, current platforms often include only relatively simple, non-dexterous manipulations and lack the flexibility required for effective learning and generalization. We introduce SurgicAI, a novel platform for development and benchmarking that addresses these challenges by providing the flexibility to accommodate both modular subtasks and more importantly task decomposition in RL-based surgical robotics. Compatible with the da Vinci Surgical System, SurgicAI offers a standardized pipeline for collecting and utilizing expert demonstrations. It supports the deployment of multiple RL and IL approaches, and the training of both singular and compositional subtasks in suturing scenarios, featuring high dexterity and modularization. Meanwhile, SurgicAI sets clear metrics and benchmarks for the assessment of learned policies. We implemented and evaluated multiple RL and IL algorithms on SurgicAI. Our detailed benchmark analysis underscores SurgicAI's potential to advance policy learning in surgical robotics. Details: https://github.com/surgical-robotics-ai/SurgicAI",Datasets & Benchmarks,NeurIPS,2024,Poster,"Jin Wu, Haoying Zhou, Peter Kazanzides, Adnan Munawar, Anqi Liu",False,https://openreview.net/pdf?id=97680
97717,A Novel Benchmark for Decision-Making in Uncertain and Competitive Games,"Decision-making in large-scale games is an essential research area in artificial intelligence (AI) with significant real-world impact. However, the limited access to realistic large-scale game environments has hindered research progress in this area. In this paper, we present AuctionNet, a benchmark for bid decision-making in large-scale ad auctions derived from a real-world online advertising platform. AuctionNet is composed of three parts: an ad auction environment, a pre-generated dataset based on the environment, and performance evaluations of several baseline bid decision-making algorithms. More specifically, the environment effectively replicates the integrity and complexity of real-world ad auctions through the interaction of several modules: the ad opportunity generation module employs deep generative networks to bridge the gap between simulated and real-world data while mitigating the risk of sensitive data exposure; the bidding module implements diverse auto-bidding agents trained with different decision-making algorithms; and the auction module is anchored in the classic Generalized Second Price (GSP) auction but also allows for customization of auction mechanisms as needed. To facilitate research and provide insights into the environment, we have also pre-generated a substantial dataset based on the environment. The dataset contains 10 million ad opportunities, 48 diverse auto-bidding agents, and over 500 million auction records. Performance evaluations of baseline algorithms such as linear programming, reinforcement learning, and generative models for bid decision-making are also presented as a part of AuctionNet. AuctionNet has powered the NeurIPS 2024 Auto-Bidding in Large-Scale Auctions competition, providing competition environments for over 1,500 teams. We believe that AuctionNet is applicable not only to research on bid decision-making in ad auctions but also to the general area of decision-making in large-scale games. Code: https://github.com/alimama-tech/AuctionNet.",Datasets & Benchmarks,NeurIPS,2024,Poster,"Kefan Su, Yusen Huo, ZHILIN ZHANG, Shuai Dou, Chuan Yu, Jian Xu, Zongqing Lu, Bo Zheng",True,https://openreview.net/pdf?id=97717
97720,Comprehensive Framework for Curating Speech Datasets and Evaluating ASR Systems: A Case Study for the Polish Language,"Speech datasets available in the public domain are often underutilized because of challenges in accessibility and interoperability. To address this, a system to survey, catalog, and curate existing speech datasets was developed, enabling reproducible evaluation of automatic speech recognition (ASR) systems. The system was applied to curate over 24 datasets and evaluate 25 ASR models, with a specific focus on Polish. This research represents the most extensive comparison to date of commercial and free ASR systems for the Polish language, drawing insights from 600 system-model-test set evaluations across 8 analysis scenarios. Curated datasets and benchmark results are available publicly. The evaluation tools are open-sourced to support reproducibility of the benchmark, encourage community-driven improvements, and facilitate adaptation for other languages.",Datasets & Benchmarks,NeurIPS,2024,Poster,Michał Junczyk,False,https://openreview.net/pdf?id=97720
97730,SETBENCH: Assessing the Analytical and Semantic Robustness of Language Models,"Set theory is foundational to mathematics and, when sets are finite, to reasoning about the world. An intelligent system should perform set operations consistently, regardless of superficial variations in the operands. Initially designed for semantically-oriented NLP tasks, large language models (LLMs) are now being evaluated on algorithmic tasks. Because sets are comprised of arbitrary symbols (e.g. numbers, words), they provide an opportunity to test, systematically, the invariance of LLMs’ algorithmic abilities under simple lexical or semantic variations. To this end, we present the SETLEXSEM CHALLENGE, a synthetic benchmark that evaluates the performance of LLMs on set operations. SETLEXSEM assesses the robustness of LLMs’ instruction-following abilities under various conditions, focusing on the set operations and the nature and construction of the set members. Evaluating seven LLMs with SETLEXSEM, we find that they exhibit poor robust- ness to variation in both operation and operands. We show – via the framework’s systematic sampling of set members along lexical and semantic dimensions – that LLMs are not only not robust to variation along these dimensions but demonstrate unique failure modes in particular, easy-to-create semantic groupings of ""deceptive"" sets. We find that rigorously measuring language model robustness to variation in frequency and length is challenging and present an analysis that measures them in- dependently. The code for reproducing the results of this paper, and for generating the SETLEXSEM CHALLENGE dataset, is available https://github.com/amazon-science/SetLexSem-Challenge.",Datasets & Benchmarks,NeurIPS,2024,Poster,"Nicholas Dronen, Bardiya Akhbari, Manish Digambar Gawali",True,https://openreview.net/pdf?id=97730
97785,WikiDO: Evaluating Out-of-Distribution Generalization of Vision-Language Models in Cross-Modal Retrieval,,Datasets & Benchmarks,NeurIPS,2024,Poster,"Pavan Kalyan Tankala, Piyush Pasi, Sahil Dharod, Azeem Motiwala, Preethi Jyothi, Aditi Chaudhary, Krishna Srinivasan",True,https://openreview.net/pdf?id=97785
97829,Image2Struct: A Benchmark for Evaluating Vision-Language Models in Extracting Structured Information from Images,"We introduce Image2Struct, a benchmark to evaluate vision-language models (VLMs) on extracting structure from images.Our benchmark 1) captures real-world use cases, 2) is fully automatic and does not require human judgment, and 3) is based on a renewable stream of fresh data.In Image2Struct, VLMs are prompted to generate the underlying structure (e.g., LaTeX code or HTML) from an input image (e.g., webpage screenshot).The structure is then rendered to produce an output image (e.g., rendered webpage), which is compared against the input image to produce a similarity score.This round-trip evaluation allows us to quantitatively evaluate VLMs on tasks with multiple valid structures.We create a pipeline that downloads fresh data from active online communities upon execution and evaluates the VLMs without human intervention.We introduce three domains (Webpages, LaTeX, and Musical Scores) and use five image metrics (pixel similarity, cosine similarity between the Inception vectors, learned perceptual image patch similarity, structural similarity index measure, and earth mover similarity) that allow efficient and automatic comparison between pairs of images. We evaluate Image2Struct on 14 prominent VLMs and find that scores vary widely, indicating that Image2Struct can differentiate between the performances of different VLMs.Additionally, the best score varies considerably across domains (e.g., 0.402 on sheet music vs. 0.830 on LaTeX equations), indicating that Image2Struct contains tasks of varying difficulty.For transparency, we release the full results at  https://crfm.stanford.edu/helm/image2struct/v1.0.1/.",Datasets & Benchmarks,NeurIPS,2024,Poster,"Josselin Roberts, Tony Lee, Chi Heem Wong, Michihiro Yasunaga, Yifan Mai, Percy Liang",True,https://openreview.net/pdf?id=97829
97842,Through the Looking-Glass: Tracing Shifts in AI Data Consent across the Web,"General-purpose artificial intelligence (AI) systems are built on massive swathes of public web data, assembled into corpora such as C4, RefinedWeb, and Dolma. To our knowledge, we conduct the first, large-scale, longitudinal audit of the consent protocols for the web domains underlying AI training corpora. Our audit of 14,000 web domains provides an expansive view of crawlable web data and how codified data use preferences are changing over time. We observe a proliferation of AI-specific clauses to limit use, acute differences in restrictions on AI developers, as well as general inconsistencies between websites' expressed intentions in their Terms of Service and their robots.txt. We diagnose these as symptoms of ineffective web protocols, not designed to cope with the widespread re-purposing of the internet for AI. Our longitudinal analyses show that in a single year (2023-2024) there has been a rapid crescendo of data restrictions from web sources, rendering ~5\\\\%+ of all tokens in C4, or 28%+ of the most actively maintained, critical sources in C4, fully restricted from use. For Terms of Service crawling restrictions, a full 45% of C4 is now restricted. If respected or enforced, these restrictions are rapidly biasing the diversity, freshness, and scaling laws for general-purpose AI systems. We hope to illustrate the emerging crises in data consent, for both developers and creators. The foreclosure of much of the open web will impact not only commercial AI, but also non-commercial AI and academic research.",Datasets & Benchmarks,NeurIPS,2024,Poster,"Shayne Longpre, Robert Mahari, Ariel Lee, Campbell Lund, Hamidah Oderinwale, William Brannon, Nayan Saxena, Naana Obeng-Marnu, Tobin South, Cole Hunter, Kevin Klyman, Christopher Klamm, Hailey Schoelkopf, Nikhil Singh, Manuel Cherep, Ahmad Anis, An Dinh, Caroline Shamiso Chitongo, Da Yin, Damien Sileo, Deividas Mataciunas, Diganta Misra, Emad Alghamdi, Enrico Shippole, Jianguo Zhang, Joanna Materzynska, Kun Qian, Kushagra Tiwary, Lester James V. Miranda, Manan Dey, Minnie Liang, Mohammed Hamdy, Niklas Muennighoff, Seonghyeon Ye, Seungone Kim, Shrestha Mohanty, Vipul Gupta, Vivek Sharma, Minh Chien Vu, Xuhui Zhou, Yizhi Li, Caiming Xiong, Luis Villa, Stella Biderman, Hanlin Li, Daphne Ippolito, Sara Hooker, Jad Kabbara, Alex Pentland",False,https://openreview.net/pdf?id=97842
97845,Benchmarking Trustworthiness of Multimodal Large Language Models: A Comprehensive Study,"Despite the superior capabilities of Multimodal Large Language Models (MLLMs) across diverse tasks, they still face significant trustworthiness challenges. Yet, current literature on the assessment of trustworthy MLLMs remains limited, lacking a holistic evaluation to offer thorough insights into future improvements. In this work, we establishMultiTrust, the first comprehensive and unified benchmark on the trustworthiness of MLLMs across five primary aspects:truthfulness,safety,robustness,fairness, andprivacy. Our benchmark employs a rigorous evaluation strategy that addresses both multimodal risks and cross-modal impacts, encompassing 32 diverse tasks with self-curated datasets. Extensive experiments with 21 modern MLLMs reveal some previously unexplored trustworthiness issues and risks, highlighting the complexities introduced by the multimodality and underscoring the necessity for advanced methodologies to enhance their reliability. For instance, typical proprietary models still struggle with the perception of visually confusing images and are vulnerable to multimodal jailbreaking and adversarial attacks; MLLMs are more inclined to disclose privacy in text and reveal ideological and cultural biases even when paired with irrelevant images in inference, indicating that the multimodality amplifies the internal risks from base LLMs. Additionally, we release a scalable toolbox for standardized trustworthiness research, aiming to facilitate future advancements in this important field. Code and resources are publicly available at:https://multi-trust.github.io/.",Datasets & Benchmarks,NeurIPS,2024,Poster,"Yichi Zhang, Yao Huang, Yitong Sun, Chang Liu, Zhe Zhao, Zhengwei Fang, Yifan Wang, Huanran Chen, Xiao Yang, Xingxing Wei, Hang Su, Yinpeng Dong, Jun Zhu",True,https://openreview.net/pdf?id=97845
97846,A Benchmark Suite for Systematically Evaluating Reasoning Shortcuts,"The advent of powerful neural classifiers has increased interest in problems that require both learning and reasoning.These problems are critical for understanding important properties of models, such as trustworthiness, generalization, interpretability, and compliance to safety and structural constraints. However, recent research observed that tasks requiring both learning and reasoning on background knowledge often suffer from reasoning shortcuts (RSs): predictors can solve the downstream reasoning task without associating the correct concepts to the high-dimensional data. To address this issue, we introduce rsbench, a comprehensive benchmark suite designed to systematically evaluate the impact of RSs on models by providing easy access to highly customizable tasks affected by RSs. Furthermore, rsbench implements common metrics for evaluating concept quality and introduces novel formal verification procedures for assessing the presence of RSs in learning tasks. Using rsbench, we highlight that obtaining high quality concepts in both purely neural and neuro-symbolic models is a far-from-solved problem. rsbench is available at: https://unitn-sml.github.io/rsbench.",Datasets & Benchmarks,NeurIPS,2024,Poster,"Samuele Bortolotti, Emanuele Marconato, Tommaso Carraro, Paolo Morettin, Emile van Krieken, Antonio Vergari, Stefano Teso, Andrea Passerini",True,https://openreview.net/pdf?id=97846
98315,Critically Assessing the State of the Art in Neural Network Verification,"Recent research has proposed various methods to formally verify neural networks against minimal input perturbations; this verification task is also known as local robustness verification. The research area of local robustness verification is highly diverse, as verifiers rely on a multitude of techniques, including mixed integer programming and satisfiability modulo theories. At the same time, the problem instances encountered when performing local robustness verification differ based on the network to be verified, the property to be verified and the specific network input. This raises the question of which verification algorithm is most suitable for solving specific types of instances of the local robustness verification problem. To answer this question, we performed a systematic performance analysis of several CPU- and GPU-based local robustness verification systems on a newly and carefully assembled set of 79 neural networks, of which we verified a broad range of robustness properties, while taking a practitioner's point of view -- a perspective that complements the insights from initiatives such as the VNN competition, where the participating tools are carefully adapted to the given benchmarks by their developers. Notably, we show that no single best algorithm dominates performance across all verification problem instances. Instead, our results reveal complementarities in verifier performance and illustrate the potential of leveraging algorithm portfolios for more efficient local robustness verification. We quantify this complementarity using various performance measures, such as the Shapley value. Furthermore, we confirm the notion that most algorithms only support ReLU-based networks, while other activation functions remain under-supported.",Journal,NeurIPS,2024,Journal,"Matthias König, Annelot W. Bosman, Holger H. Hoos, Jan N. Van Rijn",True,https://openreview.net/pdf?id=98315
98319,Pre-trained Gaussian Processes for Bayesian Optimization,"Bayesian optimization (BO) has become a popular strategy for global optimization of expensive real-world functions. Contrary to a common expectation that BO is suited to optimizing black-box functions, it actually requires domain knowledge about those functions to deploy BO successfully. Such domain knowledge often manifests in Gaussian process (GP) priors that specify initial beliefs on functions. However, even with expert knowledge, it is non-trivial to quantitatively define a prior. This is especially true for hyperparameter tuning problems on complex machine learning models, where landscapes of tuning objectives are often difficult to comprehend. We seek an alternative practice for setting these functional priors. In particular, we consider the scenario where we have data from similar functions that allow us to pre-train a tighter distribution a priori. We detail what pre-training entails for GPs using a KL divergence based loss function, and propose a new pre-training based BO framework named HyperBO. Theoretically, we show bounded posterior predictions and near-zero regrets for HyperBO without assuming the ""ground truth"" GP prior is known. To verify our approach in realistic setups, we collect a large multi-task hyperparameter tuning dataset by training tens of thousands of configurations of near-state-of-the-art deep learning models on popular image and text datasets, as well as a protein sequence dataset. Our results show that on average, HyperBO is able to locate good hyperparameters at least 3 times more efficiently than the best competing methods on both our new tuning dataset and existing multi-task BO benchmarks.",Journal,NeurIPS,2024,Journal,"Zi Wang, George Dahl, Kevin Swersky, Chansoo Lee, Zachary Nado, Justin Gilmer, Jasper Snoek, Zoubin Ghahramani",True,https://openreview.net/pdf?id=98319
99342,"Reproducibility Study of ""Robust Fair Clustering: A Novel Fairness Attack and Defense Framework""","Clustering algorithms play a pivotal role in various societal applications, where fairness is paramount to prevent adverse impacts on individuals. In this study, we revisit the robustness of fair clustering algorithms against adversarial attacks, affirming previous research findings that highlighted their susceptibility and the resilience of the Consensus Fair Clustering (CFC) model. Beyond reproducing these critical results, our work extends the original analysis by refining the codebase for enhanced experimentation, introducing additional metrics and datasets to deepen the evaluation of fairness and clustering quality, and exploring novel attack strategies, including targeted attacks on new metrics and a combined approach for balance and entropy as well as an ablation study. These contributions validate the original claims about the vulnerability and resilience of fair clustering algorithms and broaden the research landscape by offering a more comprehensive toolkit for assessing adversarial robustness in fair clustering.",Journal,NeurIPS,2024,Journal,"Iason Skylitsis, Zheng Feng, Idries Nasim, Camille Niessink",True,https://openreview.net/pdf?id=99342
99345,"On the Reproducibility of: ""Learning Perturbations to Explain Time Series Predictions""","Deep Learning models have taken the front stage in the AI community, yet explainability challenges hinder their widespread adoption. Time series models, in particular, lack attention in this regard. This study tries to reproduce and extend the work of Enguehard (2023b), focusing on time series explainability by incorporating learnable masks and perturbations. Enguehard (2023b) employed two methods to learn these masks and perturbations, the preservation game (yielding SOTA results) and the deletion game (with poor performance). We extend the work by revising the deletion game’s loss function, testing the robustness of the proposed method on a novel weather dataset, and visualizing the learned masks and perturbations. Despite notable discrepancies in results across many experiments, our findings demonstrate that the proposed method consistently outperforms all baselines and exhibits robust performance across datasets. However, visualizations for the preservation game reveal that the learned perturbations primarily resemble a constant zero signal, questioning the importance of learning perturbations. Nevertheless, our revised deletion game shows promise, recovering meaningful perturbations and, in certain instances, surpassing the performance of the preservation game.",Journal,NeurIPS,2024,Journal,"Jasper Eppink, Floris Six Dijkstra, Wouter Bant, Ádám Divák",True,https://openreview.net/pdf?id=99345
99346,Reproducibility Study: Equal Improvability: A New Fairness Notion Considering the Long-Term Impact,"This reproducibility study aims to evaluate the robustness of Equal Improvability (EI) - an effort-based framework for ensuring long-term fairness. To this end, we seek to analyze the three proposed EI-ensuring regularization techniques, i.e. Covariance-based, KDE-based, and Loss-based EI. Our findings largely substantiate the initial assertions, demonstrating EI’s enhanced performance over Empirical Risk Minimization (ERM) techniques on various test datasets. Furthermore, while affirming the long-term effectiveness in fairness, the study also uncovers challenges in resilience to overfitting, particularly in highly complex models.
Building upon the original study, the experiments were extended to include a new dataset and multiple sensitive attributes. These additional tests further demonstrated the effec- tiveness of the EI approach, reinforcing its continued success. Our study highlights the importance of adaptable strategies in AI fairness, contributing to the ongoing discourse in this field of research.",Journal,NeurIPS,2024,Journal,"Berkay Chakar, Amina Izbassar, Mina Janićijević, Jakub Tomaszewski",True,https://openreview.net/pdf?id=99346
9FYat8HPpv,SpikeReveal: Unlocking Temporal Sequences from Real Blurry Inputs with Spike Streams,"Reconstructing a sequence of sharp images from the blurry input is crucial for enhancing our insights into the captured scene and poses a significant challenge due to the limited temporal features embedded in the image. Spike cameras, sampling at rates up to 40,000 Hz, have proven effective in capturing motion features and beneficial for solving this ill-posed problem. Nonetheless, existing methods fall into the supervised learning paradigm, which suffers from notable performance degradation when applied to real-world scenarios that diverge from the synthetic training data domain. To address these challenges, we propose the first self-supervised framework for the task of spike-guided motion deblurring. Our approach begins with the formulation of a spike-guided deblurring model that explores the theoretical relationships among spike streams, blurry images, and their corresponding sharp sequences. We subsequently develop a self-supervised cascaded framework to alleviate the issues of spike noise and spatial-resolution mismatching encountered in the deblurring model. With knowledge distillation and re-blurring loss, we further design a lightweight deblur network to generate high-quality sequences with brightness and texture consistency with the original input. Quantitative and qualitative experiments conducted on our real-world and synthetic datasets with spikes validate the superior generalization of the proposed framework. Our code, data and trained models are available at \\\\url{https://github.com/chenkang455/S-SDM}.",main,NeurIPS,2024,Spotlight,Kang Chen;Shiyan Chen;Jiyuan Zhang;Baoyue Zhang;Yajing Zheng;Tiejun Huang;Zhaofei Yu,True,https://openreview.net/pdf?id=9FYat8HPpv
9SpWvX9ykp,Generating Code World Models with Large Language Models Guided by Monte Carlo Tree Search,"In this work we consider Code World Models, world models generated by a Large Language Model (LLM) in the form of Python code for model-based Reinforcement Learning (RL). Calling code instead of LLMs for planning has potential to be more precise, reliable, interpretable, and extremely efficient.
However, writing appropriate Code World Models requires the ability to understand complex instructions, to generate exact code with non-trivial logic and to self-debug a long program with feedback from unit tests and environment trajectories. To address these challenges, we propose Generate, Improve and Fix with Monte Carlo Tree Search (GIF-MCTS), a new code generation strategy for LLMs. To test our approach in an offline RL setting, we introduce the Code World Models Benchmark (CWMB), a suite of program synthesis and planning tasks comprised of 18 diverse RL environments paired with corresponding textual descriptions and curated trajectories. GIF-MCTS surpasses all baselines on the CWMB and two other benchmarks, and we show that the Code World Models synthesized with it can be successfully used for planning, resulting in model-based RL agents with greatly improved sample efficiency and inference speed.",main,NeurIPS,2024,Poster,Nicola Dainese;Matteo Merler;Minttu Alakuijala;Pekka Marttinen,True,https://openreview.net/pdf?id=9SpWvX9ykp
9VbGjXLzig,"No ""Zero-Shot"" Without Exponential Data: Pretraining Concept Frequency Determines Multimodal Model Performance","Web-crawled pretraining datasets underlie the impressive ""zero-shot"" evaluation performance of multimodal models, such as CLIP for classification and Stable-Diffusion for image generation. However, it is unclear how meaningful the notion of ""zero-shot"" generalization is for such multimodal models, as it is not known to what extent their pretraining datasets encompass the downstream concepts targeted for during ""zero-shot"" evaluation. In this work, we ask: How is the performance of multimodal models on downstream concepts influenced by the frequency of these concepts in their pretraining datasets?

We comprehensively investigate this question across 34 models and 5 standard pretraining datasets (CC-3M, CC-12M, YFCC-15M, LAION-400M, LAION-Aesthetics), generating over 300GB of data artifacts. We consistently find that, far from exhibiting ""zero-shot"" generalization, multimodal models require exponentially more data to achieve linear improvements in downstream ""zero-shot"" performance, following a sample inefficient log-linear scaling trend. This trend persists even when controlling for sample-level similarity between pretraining and downstream datasets, and testing on purely synthetic data distributions. Furthermore, upon benchmarking models on long-tailed data sampled based on our analysis, we demonstrate that multimodal models across the board perform poorly. We contribute this long-tail test set as the Let it Wag! benchmark to further research in this direction. Taken together, our study reveals an exponential need for training data which implies that the key to ""zero-shot"" generalization capabilities under large-scale training data and compute paradigms remains to be found.",main,NeurIPS,2024,Poster,Vishaal Udandarao;Ameya Prabhu;Adhiraj Ghosh;Yash Sharma;Philip Torr;Adel Bibi;Samuel Albanie;Matthias Bethge,True,https://openreview.net/pdf?id=9VbGjXLzig
9Y8zUO11EQ,SWT-Bench: Testing and Validating Real-World Bug-Fixes with Code Agents,"Rigorous software testing is crucial for developing and maintaining high-quality code, making automated test generation a promising avenue for both improving software quality and boosting the effectiveness of code generation methods. However, while code generation with Large Language Models (LLMs) is an extraordinarily active research area, test generation remains relatively unexplored. We address this gap and investigate the capability of LLM-based Code Agents to formalize user issues into test cases. To this end, we propose a novel benchmark based on popular GitHub repositories, containing real-world issues, ground-truth bug-fixes, and golden tests.  We find that LLMs generally perform surprisingly well at generating relevant test cases, with Code Agents designed for code repair exceeding the performance of systems designed specifically for test generation. Further, as test generation is a similar but more structured task than code generation, it allows for a more fine-grained analysis using issue reproduction rate and coverage changes, providing a dual metric for analyzing systems designed for code repair. Finally, we find that generated tests are an effective filter for proposed code fixes, doubling the precision of SWE-Agent. We release all data and code at https://github.com/logic-star-ai/SWT-Bench.",main,NeurIPS,2024,Poster,Niels Mündler;Mark Niklas Mueller;Jingxuan He;Martin Vechev,True,https://openreview.net/pdf?id=9Y8zUO11EQ
9ZDdlgH6O8,UltraEdit: Instruction-based Fine-Grained Image Editing at Scale,"This paper presents UltraEdit, a large-scale (~ 4M editing samples), automatically generated dataset for instruction-based image editing. Our key idea is to address the drawbacks in existing image editing datasets like InstructPix2Pix and MagicBrush, and provide a *systematic* approach to producing massive and high-quality image editing samples: 1) UltraEdit includes more diverse editing instructions by combining LLM creativity and in-context editing examples by human raters; 2) UltraEdit is anchored on real images (photographs or artworks), which offers more diversity and less biases than those purely synthesized by text-to-image models; 3) UltraEdit supports region-based editing with high-quality, automatically produced region annotations. Our experiments show that canonical diffusion-based editing baselines trained on UltraEdit set new records on challenging MagicBrush and Emu-Edit benchmarks, respectively. Our analysis further confirms the crucial role of real image anchors and region-based editing data. The dataset, code, and models will be made public.",Datasets & Benchmarks,NeurIPS,2024,Poster,Haozhe Zhao;Xiaojian Ma;Liang Chen;Shuzheng Si;Rujie Wu;Kaikai An;Peiyu Yu;Minjia Zhang;Qing Li;Baobao Chang,True,https://openreview.net/pdf?id=9ZDdlgH6O8
9aXjIBLwKc,ZSC-Eval: An Evaluation Toolkit and Benchmark for Multi-agent Zero-shot Coordination,"Zero-shot coordination (ZSC) is a new cooperative multi-agent reinforcement learning (MARL) challenge that aims to train an ego agent to work with diverse, unseen partners during deployment. The significant difference between the deployment-time partners' distribution and the training partners' distribution determined by the training algorithm makes ZSC a unique out-of-distribution (OOD) generalization challenge. The potential distribution gap between evaluation and deployment-time partners leads to inadequate evaluation, which is exacerbated by the lack of appropriate evaluation metrics. In this paper, we present **ZSC-Eval**, the first evaluation toolkit and benchmark for ZSC algorithms.  ZSC-Eval consists of: 1) Generation of evaluation partner candidates through behavior-preferring rewards to approximate deployment-time partners' distribution; 2) Selection of evaluation partners by Best-Response Diversity (BR-Div); 3) Measurement of generalization performance with various evaluation partners via the Best-Response Proximity (BR-Prox) metric. We use ZSC-Eval to benchmark ZSC algorithms in Overcooked and Google Research Football environments and get novel empirical findings.  We also conduct a human experiment of current ZSC algorithms to verify the ZSC-Eval's consistency with human evaluation. ZSC-Eval is now available at https://github.com/sjtu-marl/ZSC-Eval.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xihuai Wang;Shao Zhang;Wenhao Zhang;Wentao Dong;Jingxiao Chen;Ying Wen;Weinan Zhang,False,https://openreview.net/pdf?id=9aXjIBLwKc
9tVn4f8aJO,HEMM: Holistic Evaluation of Multimodal Foundation Models,"Multimodal foundation models that can holistically process text alongside images, video, audio, and other sensory modalities are increasingly used in a variety of real-world applications. However, it is challenging to characterize and study progress in multimodal foundation models, given the range of possible modeling decisions, tasks, and domains. In this paper, we introduce Holistic Evaluation of Multimodal Models (HEMM) to systematically evaluate the capabilities of multimodal foundation models across a set of 3 dimensions: basic skills, information flow, and real-world use cases. Basic multimodal skills are internal abilities required to solve problems, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and the ability to handle external knowledge. Information flow studies how multimodal content changes during a task through querying, translation, editing, and fusion. Use cases span domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and human-computer interaction applications. Through comprehensive experiments across the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today’s models, and (2) distill performance trends regarding how different modeling dimensions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence performance. Our conclusions regarding challenging multimodal interactions, use cases, and tasks requiring reasoning and external knowledge, the benefits of data and model scale, and the impacts of instruction-tuning yield actionable insights for future work in multimodal foundation models.",Datasets & Benchmarks,NeurIPS,2024,Poster,Paul Pu Liang;Akshay Goindani;Talha Chafekar;Leena Mathur;Haofei Yu;Russ Salakhutdinov;Louis-Philippe Morency,True,https://openreview.net/pdf?id=9tVn4f8aJO
A33u66KmYf,BIOSCAN-5M: A Multimodal Dataset for Insect Biodiversity,"As part of an ongoing worldwide effort to comprehend and monitor insect biodiversity, this paper presents the BIOSCAN-5M Insect dataset to the machine learning community and establish several benchmark tasks. BIOSCAN-5M is a comprehensive dataset containing multi-modal information for over 5 million insect specimens, and it significantly expands existing image-based biological datasets by including taxonomic labels, raw nucleotide barcode sequences, assigned barcode index numbers, geographical, and size information. We propose three benchmark experiments to demonstrate the impact of the multi-modal data types on the classification and clustering accuracy. First, we pretrain a masked language model on the DNA barcode sequences of the BIOSCAN-5M dataset, and demonstrate the impact of using this large reference library on species- and genus-level classification performance. Second, we propose a zero-shot transfer learning task applied to images and DNA barcodes to cluster feature embeddings obtained from self-supervised learning, to investigate whether meaningful clusters can be derived from these representation embeddings. Third, we benchmark multi-modality by performing contrastive learning on DNA barcodes, image data, and taxonomic information. This yields a general shared embedding space enabling taxonomic classification using multiple types of information and modalities. The code repository of the BIOSCAN-5M Insect dataset is available at https://github.com/bioscan-ml/BIOSCAN-5M.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zahra Gharaee;Scott C Lowe;ZeMing Gong;Pablo Andres Millan Arias;Nicholas Pellegrino;Austin Wang;Joakim Bruslund Haurum;Iuliia Zarubiieva;Lila Kari;Dirk Steinke;Graham W. Taylor;Paul W. Fieguth;Angel X Chang,True,https://openreview.net/pdf?id=A33u66KmYf
A3jHvChR8K,Semi-Open 3D Object Retrieval via Hierarchical Equilibrium on Hypergraph,"Existing open-set learning methods consider only the single-layer labels of objects and strictly assume no overlap between the training and testing sets, leading to contradictory optimization for superposed categories. In this paper, we introduce a more practical Semi-Open Environment setting for open-set 3D object retrieval with hierarchical labels, in which the training and testing set share a partial label space for coarse categories but are completely disjoint from fine categories. We propose the Hypergraph-Based Hierarchical Equilibrium Representation (HERT) framework for this task. Specifically, we propose the Hierarchical Retrace Embedding (HRE) module to overcome the global disequilibrium of unseen categories by fully leveraging the multi-level category information. Besides, tackling the feature overlap and class confusion problem, we perform the Structured Equilibrium Tuning (SET) module to utilize more equilibrial correlations among objects and generalize to unseen categories, by constructing a superposed hypergraph based on the local coherent and global entangled correlations. Furthermore, we generate four semi-open 3DOR datasets with multi-level labels for benchmarking. Results demonstrate that the proposed method can effectively generate the hierarchical embeddings of 3D objects and generalize them towards semi-open environments.",main,NeurIPS,2024,Poster,Yang Xu;Yifan Feng;Jun Zhang;Jun-Hai Yong;Yue Gao,True,https://openreview.net/pdf?id=A3jHvChR8K
A5pabdZp2F,MultiOOD: Scaling Out-of-Distribution Detection for Multiple Modalities,"Detecting out-of-distribution (OOD) samples is important for deploying machine learning models in safety-critical applications such as autonomous driving and robot-assisted surgery. Existing research has mainly focused on unimodal scenarios on image data. However, real-world applications are inherently multimodal, which makes it essential to leverage information from multiple modalities to enhance the efficacy of OOD detection. To establish a foundation for more realistic Multimodal OOD Detection, we introduce the first-of-its-kind benchmark, MultiOOD, characterized by diverse dataset sizes and varying modality combinations. We first evaluate existing unimodal OOD detection algorithms on MultiOOD, observing that the mere inclusion of additional modalities yields substantial improvements. This underscores the importance of utilizing multiple modalities for OOD detection. Based on the observation of Modality Prediction Discrepancy between in-distribution (ID) and OOD data, and its strong correlation with OOD performance, we propose the Agree-to-Disagree (A2D) algorithm to encourage such discrepancy during training. Moreover, we introduce a novel outlier synthesis method, NP-Mix, which explores broader feature spaces by leveraging the information from nearest neighbor classes and complements A2D to strengthen OOD detection performance. Extensive experiments on MultiOOD demonstrate that training with A2D and NP-Mix improves existing OOD detection algorithms by a large margin. To support accessibility and reproducibility, our source code and MultiOOD benchmark are available at https://github.com/donghao51/MultiOOD.",main,NeurIPS,2024,Spotlight,Hao Dong;Yue Zhao;Eleni Chatzi;Olga Fink,True,https://openreview.net/pdf?id=A5pabdZp2F
A7wC1CTkYl,Efficient Lifelong Model Evaluation in an Era of Rapid Progress,"Standardized benchmarks drive progress in machine learning. However, with repeated testing, the risk of overfitting grows as algorithms over-exploit benchmark idiosyncrasies. In our work, we seek to mitigate this challenge by compiling \\\\textit{ever-expanding} large-scale benchmarks called \\\\textit{Lifelong Benchmarks}. As exemplars of our approach, we create \\\\textit{Lifelong-CIFAR10} and \\\\textit{Lifelong-ImageNet}, containing (for now) 1.69M and 1.98M test samples, respectively. While reducing overfitting, lifelong benchmarks introduce a key challenge: the high cost of evaluating a growing number of models across an ever-expanding sample set. To address this challenge, we also introduce an efficient evaluation framework: \\\\textit{Sort \\\\& Search (S\\\\&S)}, which reuses previously evaluated models by leveraging dynamic programming algorithms to selectively rank and sub-select test samples, enabling cost-effective lifelong benchmarking. Extensive empirical evaluations across $\\\\sim$31,000 models demonstrate that \\\\textit{S\\\\&S} achieves highly-efficient approximate accuracy measurement, reducing compute cost from 180 GPU days to 5 GPU hours ($\\\\sim$1000x reduction) on a single A100 GPU, with low approximation error. As such, lifelong benchmarks offer a robust, practical solution to the ``benchmark exhaustion'' problem.",main,NeurIPS,2024,Poster,Ameya Prabhu;Vishaal Udandarao;Philip Torr;Matthias Bethge;Adel Bibi;Samuel Albanie,True,https://openreview.net/pdf?id=A7wC1CTkYl
AAo8zAShX3,MassSpecGym: A benchmark for the discovery and identification of molecules,"The discovery and identification of molecules in biological and environmental samples is crucial for advancing biomedical and chemical sciences. Tandem mass spectrometry (MS/MS) is the leading technique for high-throughput elucidation of molecular structures. However, decoding a molecular structure from its mass spectrum is exceptionally challenging, even when performed by human experts. As a result, the vast majority of acquired MS/MS spectra remain uninterpreted, thereby limiting our understanding of the underlying (bio)chemical processes. Despite decades of progress in machine learning applications for predicting molecular structures from MS/MS spectra, the development of new methods is severely hindered by the lack of standard datasets and evaluation protocols. To address this problem, we propose MassSpecGym -- the first comprehensive benchmark for the discovery and identification of molecules from MS/MS data. Our benchmark comprises the largest publicly available collection of high-quality MS/MS spectra and defines three MS/MS annotation challenges: \\\\textit{de novo} molecular structure generation, molecule retrieval, and spectrum simulation. It includes new evaluation metrics and a generalization-demanding data split, therefore standardizing the MS/MS annotation tasks and rendering the problem accessible to the broad machine learning community. MassSpecGym is publicly available at \\\\url{https://github.com/pluskal-lab/MassSpecGym}.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Roman Bushuiev;Anton Bushuiev;Niek F. de Jonge;Adamo Young;Fleming Kretschmer;Raman Samusevich;Janne Heirman;Fei Wang;Luke Zhang;Kai Dührkop;Marcus Ludwig;Nils A. Haupt;Apurva Kalia;Corinna Brungs;Robin Schmid;Russell Greiner;BO WANG;David Wishart;Liping Liu;Juho Rousu;Wout Bittremieux;Hannes Rost;Tytus D. Mak;Soha Hassoun;Florian Huber;Justin J.J. van der Hooft;Michael A. Stravs;Sebastian Böcker;Josef Sivic;Tomas Pluskal,True,https://openreview.net/pdf?id=AAo8zAShX3
ADLaALtdoG,SciCode: A Research Coding Benchmark Curated by Scientists,"Since language models (LMs) now outperform average humans on many challenging tasks, it is becoming increasingly difficult to develop challenging, high-quality, and realistic evaluations. We address this by examining LM capabilities to generate code for solving real scientific research problems. Incorporating input from scientists and AI researchers in 16 diverse natural science sub-fields, including mathematics, physics, chemistry, biology, and materials science, we create a scientist-curated coding benchmark, SciCode. The problems naturally factorize into multiple subproblems, each involving knowledge recall, reasoning, and code synthesis. In total, SciCode contains 338 subproblems decomposed from 80 challenging main problems, and it offers optional descriptions specifying useful scientific background information and scientist-annotated gold-standard solutions and test cases for evaluation. OpenAI o1-preview, the best-performing model among those tested, can solve only 7.7\\\\% of the problems in the most realistic setting. We believe that SciCode demonstrates both contemporary LMs' progress towards realizing helpful scientific assistants and sheds light on the building and evaluation of scientific AI in the future.",Datasets & Benchmarks,NeurIPS,2024,Poster,Minyang Tian;Luyu Gao;Dylan Zhang;Xinan Chen;Cunwei Fan;Xuefei Guo;Roland Haas;Pan Ji;Kittithat Krongchon;Yao Li;Shengyan Liu;Di Luo;Yutao Ma;HAO TONG;Kha Trinh;Chenyu Tian;Zihan Wang;Bohao Wu;Shengzhu Yin;Minhui Zhu;Kilian Lieret;Yanxin Lu;Genglin Liu;Yufeng Du;Tianhua Tao;Ofir Press;Jamie Callan;Eliu A Huerta;Hao Peng,True,https://openreview.net/pdf?id=ADLaALtdoG
ADV0Pzi3Ol,Beyond Accuracy: Ensuring Correct Predictions With Correct Rationales,"Large pretrained foundation models demonstrate exceptional performance and, in some high-stakes applications, even surpass human experts. However, most of these models are currently evaluated primarily on prediction accuracy, overlooking the validity of the rationales behind their accurate predictions. For the safe deployment of foundation models, there is a pressing need to ensure *double-correct predictions*, *i.e.*, correct prediction backed by correct rationales. To achieve this, we propose a two-phase scheme: First, we curate a new dataset that offers structured rationales for visual recognition tasks. Second, we propose a rationale-informed optimization method to guide the model in disentangling and localizing visual evidence for each rationale, without requiring manual annotations. Extensive experiments and ablation studies demonstrate that our model outperforms state-of-the-art models by up to 10.1\\\\% in prediction accuracy across a wide range of tasks. Furthermore, our method significantly improves the model's rationale correctness, improving localization by 7.5\\\\% and disentanglement by 36.5\\\\%. Our dataset, source code, and pretrained weights: https://github.com/deep-real/DCP",main,NeurIPS,2024,Poster,Tang Li;Mengmeng Ma;Xi Peng,True,https://openreview.net/pdf?id=ADV0Pzi3Ol
ALU676zGFE,MTGS: A Novel Framework for Multi-Person Temporal Gaze Following and Social Gaze Prediction,"Gaze following and social gaze prediction are fundamental tasks providing insights into human communication behaviors, intent, and social interactions. Most previous approaches addressed these tasks separately, either by designing highly specialized social gaze models that do not generalize to other social gaze tasks or by considering social gaze inference as an ad-hoc post-processing of the gaze following task. Furthermore, the vast majority of gaze following approaches have proposed models that can handle only one person at a time and are static, therefore failing to take advantage of social interactions and temporal dynamics. In this paper, we address these limitations and introduce a novel framework to jointly predict the gaze target and social gaze label for all people in the scene. It comprises (i) a temporal, transformer-based architecture that, in addition to frame tokens, handles person-specific tokens capturing the gaze information related to each individual; (ii) a new dataset, VSGaze, built from multiple gaze following and social gaze datasets by extending and validating head detections and tracks, and unifying annotation types. We demonstrate that our model can address and benefit from training on all tasks jointly, achieving state-of-the-art results for multi-person gaze following and social gaze prediction. Our annotations and code will be made publicly available.",main,NeurIPS,2024,Poster,Anshul Gupta;Samy Tafasca;Arya Farkhondeh;Pierre Vuillecard;Jean-marc Odobez,True,https://openreview.net/pdf?id=ALU676zGFE
ASv9lQcHCc,GoMatching: A Simple Baseline for Video Text Spotting via Long and Short Term Matching,"Beyond the text detection and recognition tasks in image text spotting, video text spotting presents an augmented challenge with the inclusion of tracking. While advanced end-to-end trainable methods have shown commendable performance, the pursuit of multi-task optimization may pose the risk of producing sub-optimal outcomes for individual tasks. In this paper, we identify a main bottleneck in the state-of-the-art video text spotter: the limited recognition capability. In response to this issue, we propose to efficiently turn an off-the-shelf query-based image text spotter into a specialist on video and present a simple baseline termed GoMatching, which focuses the training efforts on tracking while maintaining strong recognition performance. To adapt the image text spotter to video datasets, we add a rescoring head to rescore each detected instance's confidence via efficient tuning, leading to a better tracking candidate pool. 
Additionally, we design a long-short term matching module, termed LST-Matcher, to enhance the spotter's tracking capability by integrating both long- and short-term matching results via Transformer. Based on the above simple designs, GoMatching delivers new records on ICDAR15-video, DSText, BOVText, and our proposed novel test set with arbitrary-shaped text termed ArTVideo, which demonstates GoMatching's capability to accommodate general, dense, small, arbitrary-shaped, Chinese and English text scenarios while saving considerable training budgets. The code will be released.",main,NeurIPS,2024,Poster,Haibin He;Maoyuan Ye;Jing Zhang;Juhua Liu;Bo Du;Dacheng Tao,True,https://openreview.net/pdf?id=ASv9lQcHCc
AcBLtTKK5q,Jailbreaking Large Language Models Against Moderation Guardrails via Cipher Characters,"Large Language Models (LLMs) are typically harmless but remain vulnerable to carefully crafted prompts known as ``jailbreaks'', which can bypass protective measures and induce harmful behavior. Recent advancements in LLMs have incorporated moderation guardrails that can filter outputs, which trigger processing errors for certain malicious questions. Existing red-teaming benchmarks often neglect to include questions that trigger moderation guardrails, making it difficult to evaluate jailbreak effectiveness. To address this issue, we introduce JAMBench, a harmful behavior benchmark designed to trigger and evaluate moderation guardrails. JAMBench involves 160 manually crafted instructions covering four major risk categories at multiple severity levels. Furthermore, we propose a jailbreak method, JAM (Jailbreak Against Moderation), designed to attack moderation guardrails using jailbreak prefixes to bypass input-level filters and a fine-tuned shadow model functionally equivalent to the guardrail model to generate cipher characters to bypass output-level filters. Our extensive experiments on four LLMs demonstrate that JAM achieves higher jailbreak success ($\\\\sim$ $\\\\times$ 19.88) and lower filtered-out rates ($\\\\sim$ $\\\\times$ 1/6) than baselines.",main,NeurIPS,2024,Poster,Haibo Jin;Andy Zhou;Joe D. Menke;Haohan Wang,True,https://openreview.net/pdf?id=AcBLtTKK5q
AdpSHMOujG,Infusing Synthetic Data with Real-World Patterns for Zero-Shot Material State Segmentation,"Visual recognition of materials and their states is essential for understanding the physical world, from identifying wet regions on surfaces or stains on fabrics to detecting infected areas or minerals in rocks. Collecting data that captures this vast variability is complex due to the scattered and gradual nature of material states. Manually annotating real-world images is constrained by cost and precision, while synthetic data, although accurate and inexpensive, lacks real-world diversity. This work aims to bridge this gap by infusing patterns automatically extracted from real-world images into synthetic data. Hence, patterns collected from natural images are used to generate and map materials into synthetic scenes. This unsupervised approach captures the complexity of the real world while maintaining the precision and scalability of synthetic data. We also present the first comprehensive benchmark for zero-shot material state segmentation, utilizing real-world images across a diverse range of domains, including food, soils, construction, plants, liquids, and more, each appears in various states such as wet, dry, infected, cooked, burned, and many others. The annotation includes partial similarity between regions with similar but not identical materials and hard segmentation of only identical material states. This benchmark eluded top foundation models, exposing the limitations of existing data collection methods. Meanwhile, nets trained on the infused data performed significantly better on this and related tasks. The dataset, code, and trained model are publicly available. We also share 300,000 extracted textures and SVBRDF/PBR materials to facilitate future datasets generation.",Datasets & Benchmarks,NeurIPS,2024,Poster,Sagi Eppel;Jolina Yining Li;Manuel S. Drehwald;Alan Aspuru-Guzik,True,https://openreview.net/pdf?id=AdpSHMOujG
AfzbDw6DSp,Understanding Transformer Reasoning Capabilities via Graph Algorithms,"Which transformer scaling regimes are able to perfectly solve different classes of algorithmic problems?   While tremendous empirical advances have been attained by transformer-based neural networks, a theoretical understanding of their algorithmic reasoning capabilities in realistic parameter regimes is lacking.  We investigate this question in terms of the network’s depth, width, and number of extra tokens for algorithm execution.  Our novel representational hierarchy separates 9 algorithmic reasoning problems into classes solvable by transformers in different realistic parameter scaling regimes.  We prove that logarithmic depth is necessary and sufficient for tasks like graph connectivity, while single-layer transformers with small embedding dimensions can solve contextual retrieval tasks. We also support our theoretical analysis with ample empirical evidence using the GraphQA benchmark.  These results show that transformers excel at many graph reasoning tasks, even outperforming specialized graph neural networks.",main,NeurIPS,2024,Poster,Clayton Sanford;Bahare Fatemi;Ethan Hall;Anton Tsitsulin;Mehran Kazemi;Jonathan Halcrow;Bryan Perozzi;Vahab Mirrokni,True,https://openreview.net/pdf?id=AfzbDw6DSp
Awu8YlEofZ,GS-Blur: A 3D Scene-Based Dataset for Realistic Image Deblurring,"To train a deblurring network, an appropriate dataset with paired blurry and sharp images is essential.
Existing datasets collect blurry images either synthetically by aggregating consecutive sharp frames or using sophisticated camera systems to capture real blur.
However, these methods offer limited diversity in blur types (blur trajectories) or require extensive human effort to reconstruct large-scale datasets, failing to fully reflect real-world blur scenarios.
To address this, we propose GS-Blur, a dataset of synthesized realistic blurry images created using a novel approach.
To this end, we first reconstruct 3D scenes from multi-view images using 3D Gaussian Splatting~(3DGS), then render blurry images by moving the camera view along the randomly generated motion trajectories.
By adopting various camera trajectories in reconstructing our GS-Blur, our dataset contains realistic and diverse types of blur, offering a large-scale dataset that generalizes well to real-world blur.
Using GS-Blur with various deblurring methods, we demonstrate its ability to generalize effectively compared to previous synthetic or real blur datasets, showing significant improvements in deblurring performance.
We will publicly release our dataset.",Datasets & Benchmarks,NeurIPS,2024,Poster,Dongwoo Lee;JoonKyu Park;Kyoung Mu Lee,True,https://openreview.net/pdf?id=Awu8YlEofZ
AxToUp4FMU,Cross-Care: Assessing the Healthcare Implications of Pre-training Data on Language Model Bias,"Large language models (LLMs) are increasingly essential in processing natural languages, yet their application is frequently compromised by biases and inaccuracies originating in their training data.
In this study, we introduce \\\\textbf{Cross-Care}, the first benchmark framework dedicated to assessing biases and real world knowledge in LLMs, specifically focusing on the representation of disease prevalence across diverse demographic groups.
We systematically evaluate how demographic biases embedded in pre-training corpora like $ThePile$ influence the outputs of LLMs.
We expose and quantify discrepancies by juxtaposing these biases against actual disease prevalences in various U.S. demographic groups.
Our results highlight substantial misalignment between LLM representation of disease prevalence and real disease prevalence rates across demographic subgroups, indicating a pronounced risk of bias propagation and a lack of real-world grounding for medical applications of LLMs.
Furthermore, we observe that various alignment methods minimally resolve inconsistencies in the models' representation of disease prevalence across different languages.
For further exploration and analysis, we make all data and a data visualization tool available at: \\\\url{www.crosscare.net}.",Datasets & Benchmarks,NeurIPS,2024,Poster,Shan Chen;Jack Gallifant;Mingye Gao;Pedro José Ferreira Moreira;Nikolaj Munch;Ajay Muthukkumar;Arvind Rajan;Jaya Kolluri;Amelia Fiske;Janna Hastings;Hugo Aerts;Brian W. Anthony;Leo Anthony Celi;William La Cava;Danielle Bitterman,False,https://openreview.net/pdf?id=AxToUp4FMU
B3jt0Ran2t,Probabilistic Analysis of Stable Matching in Large Markets with Siblings,"We study a practical matching problem that involves assigning children to daycare centers. The collective preferences of siblings from the same family introduce complementarities, which can lead to the non-existence of stable matchings, as observed in the well-studied hospital-doctor matching problems involving couples. Intriguingly,  stable matchings have been observed in real-world daycare markets, even with a substantial number of sibling applicants.

Our research systematically explores the presence of stable matchings in these markets. We conduct a probabilistic analysis of large random matching markets that incorporate sibling preferences. Specifically, we examine scenarios where daycares have similar priorities over children, a common characteristic in practical markets. Our analysis reveals that as the market size approaches infinity, the likelihood of stable matchings existing converges to 1.

To facilitate our investigation, we introduce significant modifications to the Sorted Deferred Acceptance algorithm proposed by ed by Ashlagi et al. [2014]. These adaptations are essential to accommodate a more stringent stability concept, as the original algorithm may yield matchings that fail to meet this criterion. By leveraging our revised algorithm, we successfully identify stable matchings in all real-life datasets examined. Additionally, we conduct comprehensive empirical investigations using synthetic datasets to validate the efficacy of our algorithm in identifying stable matchings.",main,NeurIPS,2024,Reject,Zhaohong Sun;Tomohiko Yokoyama;Makoto Yokoo,True,https://openreview.net/pdf?id=B3jt0Ran2t
BAmAFraxvf,Toward Semantic Gaze Target Detection,"From the onset of infanthood, humans naturally develop the ability to closely observe and interpret the visual gaze of others. This skill, known as gaze following, holds significance in developmental theory as it enables us to grasp another person’s mental state, emotions, intentions, and more. In computer vision, gaze following is defined as the prediction of the pixel coordinates where a person in the image is focusing their attention. Existing methods in this research area have predominantly centered on pinpointing the gaze target by predicting a gaze heatmap or gaze point. However, a notable drawback of this approach is its limited practical value in gaze applications, as mere localization may not fully capture our primary interest — understanding the underlying semantics, such as the nature of the gaze target, rather than just its 2D pixel location. To address this gap, we extend the gaze following task, and introduce a novel architecture that simultaneously predicts the localization and semantic label of the gaze target. We devise a pseudo-annotation pipeline for the GazeFollow dataset, propose a new benchmark, develop an experimental protocol and design a suitable baseline for comparison. Our method sets a new state-of-the-art on the main GazeFollow benchmark for localization and achieves competitive results in the recognition task on both datasets compared to the baseline, with 40% fewer parameters",main,NeurIPS,2024,Poster,Samy Tafasca;Anshul Gupta;Victor Bros;Jean-marc Odobez,True,https://openreview.net/pdf?id=BAmAFraxvf
BKu8JPQdQD,PUZZLES: A Benchmark for Neural Algorithmic Reasoning,"Algorithmic reasoning is a fundamental cognitive ability that plays a pivotal role in problem-solving and decision-making processes. Reinforcement Learning (RL) has demonstrated remarkable proficiency in tasks such as motor control, handling perceptual input, and managing stochastic environments. These advancements have been enabled in part by the availability of benchmarks. In this work we introduce PUZZLES, a benchmark based on Simon Tatham's Portable Puzzle Collection, aimed at fostering progress in algorithmic and logical reasoning in RL. PUZZLES contains 40 diverse logic puzzles of adjustable sizes and varying levels of complexity, providing detailed information on the strengths and generalization capabilities of RL agents. Furthermore, we evaluate various RL algorithms on PUZZLES, providing baseline comparisons and demonstrating the potential for future research. All the software, including the environment, is available at this https url.",Datasets & Benchmarks,NeurIPS,2024,Poster,Benjamin Estermann;Luca A Lanzendörfer;Yannick Niedermayr;Roger Wattenhofer,True,https://openreview.net/pdf?id=BKu8JPQdQD
BZe6dmDk5K,GAIA: Rethinking Action Quality Assessment for AI-Generated Videos,"Assessing action quality is both imperative and challenging due to its significant impact on the quality of AI-generated videos, further complicated by the inherently ambiguous nature of actions within AI-generated video (AIGV). Current action quality assessment (AQA) algorithms predominantly focus on actions from real specific scenarios and are pre-trained with normative action features, thus rendering them inapplicable in AIGVs. To address these problems, we construct GAIA, a Generic AI-generated Action dataset, by conducting a large-scale subjective evaluation from a novel causal reasoning-based perspective, resulting in 971,244 ratings among 9,180 video-action pairs. Based on GAIA, we evaluate a suite of popular text-to-video (T2V) models on their ability to generate visually rational actions, revealing their pros and cons on different categories of actions. We also extend GAIA as a testbed to benchmark the AQA capacity of existing automatic evaluation methods. Results show that traditional AQA methods, action-related metrics in recent T2V benchmarks, and mainstream video quality methods perform poorly with an average SRCC of 0.454, 0.191, and 0.519, respectively, indicating a sizable gap between current models and human action perception patterns in AIGVs. Our findings underscore the significance of action quality as a unique perspective for studying AIGVs and can catalyze progress towards methods with enhanced capacities for AQA in AIGVs.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Zijian Chen;Wei Sun;Yuan Tian;Jun Jia;Zicheng Zhang;Wang Jiarui;Ru Huang;Xiongkuo Min;Guangtao Zhai;Wenjun Zhang,True,https://openreview.net/pdf?id=BZe6dmDk5K
BZxtiElo0c,GeSS: Benchmarking Geometric Deep Learning under Scientific Applications with Distribution Shifts,"Geometric deep learning (GDL) has gained significant attention in scientific fields, for its proficiency in modeling data with intricate geometric structures. 
Yet, very few works have delved into its capability of tackling the distribution shift problem, a prevalent challenge in many applications.
To bridge this gap, we propose GeSS, a comprehensive benchmark designed for evaluating the performance of GDL models in scientific scenarios with distribution shifts.
Our evaluation datasets cover diverse scientific domains from particle physics, materials science to biochemistry, and encapsulate a broad spectrum of distribution shifts including conditional, covariate, and concept shifts. 
Furthermore, we study three levels of information access from the out-of-distribution (OOD) test data, including no OOD information, only unlabeled OOD data, and OOD data with a few labels. 
Overall, our benchmark results in 30 different experiment settings, and evaluates 3 GDL backbones and 11 learning algorithms in each setting. A thorough analysis of the evaluation results is provided, poised to illuminate insights for GDL researchers and domain practitioners who are to use GDL in their applications.",Datasets & Benchmarks,NeurIPS,2024,Poster,Deyu Zou;Shikun Liu;Siqi Miao;Victor Fung;Shiyu Chang;Pan Li,True,https://openreview.net/pdf?id=BZxtiElo0c
Becrgm5xAq,RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark,"Deep reinforcement learning (RL) has recently shown significant benefits in solving combinatorial optimization (CO) problems, reducing reliance on domain expertise, and improving computational efficiency. However, the field lacks a unified benchmark for easy development and standardized comparison of algorithms across diverse CO problems. To fill this gap, we introduce RL4CO, a unified and extensive benchmark with in-depth library coverage of 23 state-of-the-art methods and 20+ CO problems. Built on efficient software libraries and best practices in implementation, RL4CO features modularized implementation and flexible configuration of diverse RL algorithms, neural network architectures, inference techniques, and environments. RL4COallows researchers to seamlessly navigate existing successes and develop their unique designs, facilitating the entire research process by decoupling science from heavy engineering. We also provide extensive benchmark studies to inspire new insights and future work. RL4CO has attracted numerous researchers in the community and is open-sourced at https://github.com/ai4co/rl4co.",Datasets & Benchmarks,NeurIPS,2024,Reject,Federico Berto;Chuanbo Hua;Junyoung Park;Laurin Luttmann;Yining Ma;Fanchen Bu;Jiarui Wang;Haoran Ye;Minsu Kim;Sanghyeok Choi;Nayeli Gast Zepeda;André Hottung;Jianan Zhou;Jieyi Bi;Yu Hu;Fei Liu;Hyeonah Kim;Jiwoo Son;Haeyeon Kim;Davide Angioni;Wouter Kool;Zhiguang Cao;Qingfu Zhang;Joungho Kim;Jie Zhang;Kijung Shin;Cathy Wu;Sungsoo Ahn;Guojie Song;Changhyun Kwon;Kevin Tierney;Lin Xie;Jinkyoo Park,True,https://openreview.net/pdf?id=Becrgm5xAq
ByknnPI5Km,ABCFair: an Adaptable Benchmark approach for Comparing Fairness Methods,"Numerous methods have been implemented that pursue fairness with respect to sensitive features by mitigating biases in machine learning. Yet, the problem settings that each method tackles vary significantly, including the stage of intervention, the composition of sensitive features, the fairness notion, and the distribution of the output. Even in binary classification, the greatest common denominator of problem settings is small, significantly complicating benchmarking.

Hence, we introduce ABCFair, a benchmark approach which allows adapting to the desiderata of the real-world problem setting, enabling proper comparability between methods for any use case. We apply this benchmark to a range of pre-, in-, and postprocessing methods on both large-scale, traditional datasets and on a dual label (biased and unbiased) dataset to sidestep the fairness-accuracy trade-off.",Datasets & Benchmarks,NeurIPS,2024,Poster,MaryBeth Defrance;Maarten Buyl;Tijl De Bie,True,https://openreview.net/pdf?id=ByknnPI5Km
CGZGY3X8NH,ELSA: Evaluating Localization of Social Activities in Urban Streets using Open-Vocabulary Detection,"Existing Open Vocabulary Detection (OVD) models exhibit a number of challenges. They often struggle with semantic consistency across diverse inputs, and are often sensitive to slight variations in input phrasing, leading to inconsistent performance.  The calibration of their predictive confidence, especially in complex multi-label scenarios, remains suboptimal, frequently resulting in overconfident predictions that do not accurately reflect their context understanding. The Understanding of those limitations requires multi-label detection benchmarks. Among those, one challenging domain is social activity interaction. Due to the lack of multi-label benchmarks for social interactions, in this work we present ELSA: Evaluating Localization of Social Activities. ELSA draws on theoretical frameworks in urban sociology and design and uses in-the-wild street-level imagery, where the size of social groups and the types of activities can vary significantly. ELSA includes more than 900 manually annotated images with more than 4,000 multi-labeled bounding boxes for individual and group activities. We introduce a novel re-ranking method for predictive confidence and new evaluation techniques for OVD models. We report our results on the widely-used, SOTA model Grounding DINO. Our evaluation protocol considers semantic stability and localization accuracy and sheds more light on the limitations of the existing approaches.",Datasets & Benchmarks,NeurIPS,2024,Reject,Maryam Hosseini;Marco Cipriano;Daniel Hodczak;Sedigheh Eslami;Liu Liu;Andres Sevtsuk;Gerard de Melo,True,https://openreview.net/pdf?id=CGZGY3X8NH
CIcMZGLyZW,Neuro-Symbolic Data Generation for Math Reasoning,"A critical question about Large Language Models (LLMs) is whether their apparent deficiency in mathematical reasoning is inherent, or merely a result of insufficient exposure to high-quality mathematical data. To explore this, we developed an automated method for generating high-quality, supervised mathematical datasets. The method carefully mutates existing math problems, ensuring both diversity and validity of the newly generated problems. This is achieved by a neuro-symbolic data generation framework combining the intuitive informalization strengths of LLMs, and the precise symbolic reasoning of math solvers along with projected Markov chain Monte Carlo sampling in the highly-irregular symbolic space.
Empirical experiments demonstrate the high quality of data generated by the proposed method, and that the LLMs, specifically LLaMA-2 and Mistral, when realigned with the generated data, surpass their state-of-the-art counterparts.",main,NeurIPS,2024,Poster,Zenan Li;Zhi Zhou;Yuan Yao;Xian Zhang;Yu-Feng Li;Chun Cao;Fan Yang;Xiaoxing Ma,True,https://openreview.net/pdf?id=CIcMZGLyZW
CNWdWn47IE,DataComp-LM: In search of the next generation of training sets for language models,"We introduce DataComp for Language Models, a testbed for controlled dataset experiments with the goal of improving language models.
As part of DCLM, we provide a standardized corpus of 240T tokens extracted from Common Crawl, effective pretraining recipes based on the OpenLM framework, and a broad suite of 53 downstream evaluations.
Participants in the DCLM benchmark can experiment with data curation strategies such as deduplication, filtering, and data mixing at
model scales ranging from 412M to 7B parameters.
As a baseline for DCLM, we conduct extensive experiments and find that model-based filtering is key to assembling a high-quality training set.
The resulting dataset, DCLM-Baseline, enables training a 7B parameter language model from scratch to 63% 5-shot accuracy on MMLU with 2T training tokens.
Compared to MAP-Neo, the previous state-of-the-art in open-data language models, DCLM-Baseline represents a 6 percentage point improvement on MMLU while being trained with half the compute.
Our results highlight the importance of dataset design for training language models and offer a starting point for further research on data curation. We release the \\\\dclm benchmark, framework, models, and datasets at https://www.datacomp.ai/dclm/",Datasets & Benchmarks,NeurIPS,2024,Poster,Jeffrey Li;Alex Fang;Georgios Smyrnis;Maor Ivgi;Matt Jordan;Samir Yitzhak Gadre;Hritik Bansal;Etash Kumar Guha;Sedrick Keh;Kushal Arora;Saurabh Garg;Rui Xin;Niklas Muennighoff;Reinhard Heckel;Jean Mercat;Mayee F Chen;Suchin Gururangan;Mitchell Wortsman;Alon Albalak;Yonatan Bitton;Marianna Nezhurina;Amro Kamal Mohamed Abbas;Cheng-Yu Hsieh;Dhruba Ghosh;Joshua P Gardner;Maciej Kilian;Hanlin Zhang;Rulin Shao;Sarah M Pratt;Sunny Sanyal;Gabriel Ilharco;Giannis Daras;Kalyani Marathe;Aaron Gokaslan;Jieyu Zhang;Khyathi Chandu;Thao Nguyen;Igor Vasiljevic;Sham M. Kakade;Shuran Song;Sujay Sanghavi;Fartash Faghri;Sewoong Oh;Luke Zettlemoyer;Kyle Lo;Alaaeldin El-Nouby;Hadi Pouransari;Alexander T Toshev;Stephanie Wang;Dirk Groeneveld;Luca Soldaini;Pang Wei Koh;Jenia Jitsev;Thomas Kollar;Alex Dimakis;Yair Carmon;Achal Dave;Ludwig Schmidt;Vaishaal Shankar,True,https://openreview.net/pdf?id=CNWdWn47IE
CW9SJyhpVt,GV-Rep: A Large-Scale Dataset for Genetic Variant Representation Learning,"Genetic variants (GVs) are defined as differences in the DNA sequences among individuals and play a crucial role in diagnosing and treating genetic diseases. The rapid decrease in next generation sequencing cost, analogous to Moore’s Law, has led to an exponential increase in the availability of patient-level GV data. This growth poses a challenge for clinicians who must efficiently prioritize patient-specific GVs and integrate them with existing genomic databases to inform patient management. To addressing the interpretation of GVs, genomic foundation models (GFMs) have emerged. However, these models lack standardized performance assessments, leading to considerable variability in model evaluations. This poses the question: *How effectively do deep learning methods classify unknown GVs and align them with clinically-verified GVs?* We argue that representation learning, which transforms raw data into meaningful feature spaces, is an effective approach for addressing both indexing and classification challenges. We introduce a large-scale Genetic Variant dataset, named $\\\\textsf{GV-Rep}$, featuring variable-length contexts and detailed annotations, designed for deep learning models to learn GV representations across various traits, diseases, tissue types, and experimental contexts. Our contributions are three-fold: (i) $\\\\textbf{Construction}$ of a comprehensive dataset with 7 million records, each labeled with characteristics of the corresponding variants, alongside additional data from 17,548 gene knockout tests across 1,107 cell types, 1,808 variant combinations, and 156 unique clinically-verified GVs from real-world patients. (ii) $\\\\textbf{Analysis}$ of the structure and properties of the dataset. (iii) $\\\\textbf{Experimentation}$ of the dataset with pre-trained genomic foundation models (GFMs). The results highlight a significant disparity between the current capabilities of GFMs and the accurate representation of GVs. We hope this dataset will advance genomic deep learning to bridge this gap.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zehui Li;Vallijah Subasri;Guy-Bart Stan;Yiren Zhao;BO WANG,True,https://openreview.net/pdf?id=CW9SJyhpVt
CaAJeNkceP,Dispelling the Mirage of Progress in Offline MARL through Standardised Baselines and Evaluation,"Offline multi-agent reinforcement learning (MARL) is an emerging field with great promise for real-world applications. Unfortunately, the current state of research in offline MARL is plagued by inconsistencies in baselines and evaluation protocols, which ultimately makes it difficult to accurately assess progress, trust newly proposed innovations, and allow researchers to easily build upon prior work. In this paper, we firstly identify significant shortcomings in existing methodologies for measuring the performance of novel algorithms through a representative study of published offline MARL work. Secondly, by directly comparing to this prior work, we demonstrate that simple, well-implemented baselines can achieve state-of-the-art (SOTA) results across a wide range of tasks. Specifically, we show that on 35 out of 47 datasets used in prior work (almost 75\\\\% of cases), we match or surpass the performance of the current purported SOTA. Strikingly, our baselines often substantially outperform these more sophisticated algorithms.  Finally, we correct for the shortcomings highlighted from this prior work by introducing a straightforward standardised methodology for evaluation and by providing our baseline implementations with statistically robust results across several scenarios, useful for comparisons in future work. Our proposal includes simple and sensible steps that are easy to adopt, which in combination with solid baselines and comparative results, could substantially improve the overall rigour of empirical science in offline MARL moving forward.",Datasets & Benchmarks,NeurIPS,2024,Poster,Juan Claude Formanek;Callum Rhys Tilbury;Louise Beyers;Jonathan Phillip Shock;Arnu Pretorius,False,https://openreview.net/pdf?id=CaAJeNkceP
CcmHlE6N6u,LuSh-NeRF: Lighting up and Sharpening NeRFs for Low-light Scenes,"Neural Radiance Fields (NeRFs) have shown remarkable performances in producing novel-view images from high-quality scene images. However, hand-held low-light photography challenges NeRFs as the captured images may simultaneously suffer from low visibility, noise, and camera shakes.
While existing NeRF methods may handle either low light or motion, directly combining them or incorporating additional image-based enhancement methods does not work as these degradation factors are highly coupled.
We observe that noise in low-light images is always sharp regardless of camera shakes, which implies an implicit order of these degradation factors within the image formation process.
This inspires us to explore such an order to decouple and remove these degradation factors while training the NeRF.
To this end, we propose in this paper a novel model, named LuSh-NeRF, which can reconstruct a clean and sharp NeRF from a group of hand-held low-light images.
The key idea of LuSh-NeRF is to sequentially model noise and blur in the images via multi-view feature consistency and frequency information of NeRF, respectively.
Specifically, LuSh-NeRF includes a novel Scene-Noise Decomposition (SND) module for decoupling the noise from the scene representation and a novel Camera Trajectory Prediction (CTP) module for the estimation of camera motions based on low-frequency scene information.
To facilitate training and evaluations, we construct a new dataset containing both synthetic and real images.
Experiments show that LuSh-NeRF outperforms existing approaches. Our code and dataset can be found here: https://github.com/quzefan/LuSh-NeRF.",main,NeurIPS,2024,Poster,Zefan Qu;Ke Xu;Gerhard Petrus Hancke;Rynson W. H. Lau,True,https://openreview.net/pdf?id=CcmHlE6N6u
Cdc90HKs1I,The Genomics Long-Range Benchmark: Advancing DNA Language Models,"The advent of language models (LMs) in genomics necessitates benchmarks that can assess models’ capabilities and limitations. In contrast to protein models, DNA LMs can be used to study non-coding regions of the genome and must account for unique challenges, especially interactions across long sequence lengths. However, existing benchmarks for DNA LMs are defined over short sequence datasets and can involve tasks that are often not considered to be biologically meaningful. Here, we present the Genomics Long-Range Benchmark (LRB), which focuses on biologically meaningful tasks and supports long-range contexts. We complement our benchmark with fine-tuning recipes that meaningfully improve performance and affect model evaluation. We evaluate DNA LMs across nine compiled tasks and observe that DNA LMs achieve competitive performance relative to supervised baselines on several tasks (e.g., genome annotation), but there remains a significant gap in domains, such as variant effect and gene expression prediction. Additionally, we introduce a visualization tool to examine model performance split by various genomic properties.
Lastly, we present methods for context-length extrapolation of transformer-based models that enable studying the effect of context length on DNA LM performance. The Genomics LRB is publicly available on Hugging Face: https://hf.co/datasets/InstaDeepAI/genomics-long-range-benchmark.",Datasets & Benchmarks,NeurIPS,2024,Reject,Evan Trop;Yair Schiff;Edgar Mariano Marroquin;Chia Hsiang Kao;Aaron Gokaslan;McKinley Polen;Mingyi Shao;Bernardo P de Almeida;Thomas PIERROT;Yang I Li;Volodymyr Kuleshov,True,https://openreview.net/pdf?id=Cdc90HKs1I
ChKCF75Ocd,PutnamBench: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition,"We present PutnamBench, a new multi-language benchmark for evaluating the ability of neural theorem-provers to solve competition mathematics problems. PutnamBench consists of 1692 hand-constructed formalizations of 640 theorems sourced from the William Lowell Putnam Mathematical Competition, the premier undergraduate-level mathematics competition in North America. 
All the problems have formalizations in Lean 4 and Isabelle; a substantial subset also has Coq formalizations. PutnamBench requires significant problem-solving ability and proficiency in a broad range of topics taught in undergraduate mathematics courses. We use PutnamBench to evaluate several established neural and symbolic theorem-provers. 
These approaches can only solve a handful of the PutnamBench problems, establishing the benchmark as a difficult open challenge for research on neural theorem-proving. PutnamBench is available at https://github.com/trishullab/PutnamBench.",Datasets & Benchmarks,NeurIPS,2024,Poster,George Tsoukalas;Jasper Lee;John Jennings;Jimmy Xin;Michelle Ding;Michael Jennings;Amitayush Thakur;Swarat Chaudhuri,True,https://openreview.net/pdf?id=ChKCF75Ocd
Cth1PyCwZt,Beyond accuracy: understanding the performance of LLMs on exams designed for humans,"Many recent studies of LLM performance have focused on the ability of LLMs to achieve outcomes comparable to humans on academic and professional exams. However, it is not clear whether such studies shed light on the extent to which models show reasoning ability, and there is controversy about the significance and implications of such results. We seek to look more deeply into the question of how and whether the performance of LLMs on  exams designed for humans reflects true aptitude inherent in LLMs. We do so by making use of the tools of psychometrics which are designed to perform meaningful measurement in test taking.  We leverage a unique dataset that captures the detailed performance of over 5M students across 8 college-entrance exams given over a span of two years in Brazil. With respect to the evaluation of LLM abilities, we show that the tools of Item Response Theory (IRT) provide a more informative evaluation of model performance than the usual accuracy metrics employed in previous studies.  Digging deeper, we show that the modeling framework of IRT, by explicitly modeling the difficulty levels of questions, allows us to quantitatively distinguish between LLMs that answer questions in “human-like” patterns versus LLMs that do not. We also show how to quantitatively identify cases in which exam results are not reliable  measurements of an LLM's ability.  Using the tools of IRT we can also identify specific questions that appear to be either much easier, or much harder, for machines than for humans, and we give some reasons for those differences. Overall, our study shows that the conventional focus on accuracy as the primary performance metric for LLM studies  does not allow us to deeply understand the true capabilities of LLMs and compare them to that of humans. Thus, we claim that psychometric modeling should play a larger role in the evaluation of LLM capabilities on exams designed for humans.",main,NeurIPS,2024,Reject,Pedro Calais;Gabriel Franco;Themistoklis Nikas;Zilu Tang;Mark Crovella;Wagner Meira Jr.;Evimaria Terzi,True,https://openreview.net/pdf?id=Cth1PyCwZt
CxNXoMnCKc,PrivacyLens: Evaluating Privacy Norm Awareness of Language Models in Action,"As language models (LMs) are widely utilized in personalized communication scenarios (e.g., sending emails, writing social media posts) and endowed with a certain level of agency, ensuring they act in accordance with the contextual privacy norms becomes increasingly critical. However, quantifying the privacy norm awareness of LMs and the emerging privacy risk in LM-mediated communication is challenging due to (1) the contextual and long-tailed nature of privacy-sensitive cases, and (2) the lack of evaluation approaches that capture realistic application scenarios. To address these challenges, we propose PrivacyLens, a novel framework designed to extend privacy-sensitive seeds into expressive vignettes and further into agent trajectories, enabling multi-level evaluation of privacy leakage in LM agents' actions. We instantiate PrivacyLens with a collection of privacy norms grounded in privacy literature and crowdsourced seeds. Using this dataset, we reveal a discrepancy between LM performance in answering probing questions and their actual behavior when executing user instructions in an agent setup. State-of-the-art LMs, like GPT-4 and Llama-3-70B, leak sensitive information in 25.68% and 38.69% of cases, even when prompted with privacy-enhancing instructions. We also demonstrate the dynamic nature of PrivacyLens by extending each seed into multiple trajectories to red-team LM privacy leakage risk. Dataset and code are available at https://github.com/SALT-NLP/PrivacyLens.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yijia Shao;Tianshi Li;Weiyan Shi;Yanchen Liu;Diyi Yang,True,https://openreview.net/pdf?id=CxNXoMnCKc
CyrKKKN3fs,FairMedFM: Fairness Benchmarking for Medical Imaging Foundation Models,"The advent of foundation models (FMs) in healthcare offers unprecedented opportunities to enhance medical diagnostics through automated classification and segmentation tasks. However, these models also raise significant concerns about their fairness, especially when applied to diverse and underrepresented populations in healthcare applications. Currently, there is a lack of comprehensive benchmarks, standardized pipelines, and easily adaptable libraries to evaluate and understand the fairness performance of FMs in medical imaging, leading to considerable challenges in formulating and implementing solutions that ensure equitable outcomes across diverse patient populations. To fill this gap, we introduce FairMedFM, a fairness benchmark for FM research in medical imaging. FairMedFM integrates with 17 popular medical imaging datasets, encompassing different modalities, dimensionalities, and sensitive attributes. It explores 20 widely used FMs, with various usages such as zero-shot learning, linear probing, parameter-efficient fine-tuning, and prompting in various downstream tasks -- classification and segmentation. Our exhaustive analysis evaluates the fairness performance over different evaluation metrics from multiple perspectives, revealing the existence of bias, varied utility-fairness trade-offs on different FMs, consistent disparities on the same datasets regardless FMs, and limited effectiveness of existing unfairness mitigation methods. Furthermore, FairMedFM provides an open-sourced codebase at https://github.com/FairMedFM/FairMedFM, supporting extendible functionalities and applications and inclusive for studies on FMs in medical imaging over the long term.",Datasets & Benchmarks,NeurIPS,2024,Poster,Ruinan Jin;Zikang Xu;Yuan Zhong;Qingsong Yao;Qi Dou;S Kevin Zhou;Xiaoxiao Li,False,https://openreview.net/pdf?id=CyrKKKN3fs
D0DLlMOufv,MSA Generation with Seqs2Seqs Pretraining: Advancing Protein Structure Predictions,"Deep learning models like AlphaFold2 have revolutionized protein structure prediction, achieving unprecedented accuracy. However, the dependence on robust multiple sequence alignments (MSAs) continues to pose a challenge, especially for proteins that lack a wealth of homologous sequences. To overcome this limitation, we introduce MSA-Generator, a self-supervised generative protein language model. Trained on a sequence-to-sequence task using an automatically constructed dataset, MSA-Generator employs protein-specific attention mechanisms to harness large-scale protein databases, generating virtual MSAs that enrich existing ones and boost prediction accuracy. Our experiments on CASP14 and CASP15 benchmarks reveal significant improvements in LDDT scores, particularly for complex and challenging sequences, enhancing the performance of both AlphaFold2 and RoseTTAFold. The code is released at \\\\url{https://github.com/lezhang7/MSAGen}.",main,NeurIPS,2024,Poster,Le Zhang;Jiayang Chen;Tao Shen;Yu Li;Siqi Sun,True,https://openreview.net/pdf?id=D0DLlMOufv
D3jyWDBZTk,Shopping MMLU: A Massive Multi-Task Online Shopping Benchmark for Large Language Models,"Online shopping is a complex multi-task, few-shot learning problem with a wide and evolving range of entities, relations, and tasks. However, existing models and benchmarks are commonly tailored to specific tasks, falling short of capturing the full complexity of online shopping. Large Language Models (LLMs), with their multi-task and few-shot learning abilities, have the potential to profoundly transform online shopping by alleviating task-specific engineering efforts and by providing users with interactive conversations. Despite the potential, LLMs face unique challenges in online shopping, such as domain-specific concepts, implicit knowledge, and heterogeneous user behaviors. Motivated by the potential and challenges, we propose Shopping MMLU, a diverse multi-task online shopping benchmark derived from real-world Amazon data. Shopping MMLU consists of 57 tasks covering 4 major shopping skills: concept understanding, knowledge reasoning, user behavior alignment, and multi-linguality, and can thus comprehensively evaluate the abilities of LLMs as general shop assistants. With Shoppping MMLU, we benchmark over 20 existing LLMs and uncover valuable insights about practices and prospects of building versatile LLM-based shop assistants. Shopping MMLU can be publicly accessed at https://github.com/KL4805/ShoppingMMLU. In addition, with Shopping MMLU, we are hosting a competition in KDD Cup 2024 with over 500 participating teams. The winning solutions and the associated workshop can be accessed at our website https://amazon-kddcup24.github.io/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yilun Jin;Zheng Li;Chenwei Zhang;Tianyu Cao;Yifan Gao;Pratik Sridatt Jayarao;Mao Li;Xin Liu;Ritesh Sarkhel;Xianfeng Tang;Haodong Wang;Zhengyang Wang;Wenju Xu;Jingfeng Yang;Qingyu Yin;Xian Li;Priyanka Nigam;Yi Xu;Kai Chen;Qiang Yang;Meng Jiang;Bing Yin,True,https://openreview.net/pdf?id=D3jyWDBZTk
DAO2BFzMfy,Interpreting the Weight Space of Customized Diffusion Models,"We investigate the space of weights spanned by a large collection of customized diffusion models. We populate this space by creating a dataset of over 60,000 models, each of which is a base model fine-tuned to insert a different person's visual identity. We model the underlying manifold of these weights as a subspace, which we term $\\\\textit{weights2weights}$. We demonstrate three immediate applications of this space that result in new diffusion models -- sampling, editing, and inversion. First, sampling a set of weights from this space results in a new model encoding a novel identity. Next, we find linear directions in this space corresponding to semantic edits of the identity (e.g., adding a beard), resulting in a new model with the original identity edited. Finally, we show that inverting a single image into this space encodes a realistic identity into a model, even if the input image is out of distribution (e.g., a painting). We further find that these linear properties of the diffusion model weight space extend to other visual concepts. Our results indicate that the weight space of fine-tuned diffusion models can behave as an interpretable $\\\\textit{meta}$-latent space producing new models.",main,NeurIPS,2024,Poster,Amil Dravid;Yossi Gandelsman;Kuan-Chieh Wang;Rameen Abdal;Gordon Wetzstein;Alexei A Efros;Kfir Aberman,True,https://openreview.net/pdf?id=DAO2BFzMfy
DERtzUdhkk,TorchSpatial: A Location Encoding Framework and Benchmark for Spatial Representation Learning,"Spatial representation learning (SRL) aims at learning general-purpose neural network representations from various types of spatial data (e.g., points, polylines, polygons, networks, images, etc.) in their native formats. Learning good spatial representations is a fundamental problem for various downstream applications such as species distribution modeling, weather forecasting, trajectory generation, geographic question answering, etc. Even though SRL has become the foundation of almost all geospatial artificial intelligence (GeoAI) research, we have not yet seen significant efforts to develop an extensive deep learning framework and benchmark to support SRL model development and evaluation. To fill this gap, we propose TorchSpatial, a learning framework and benchmark for location (point) encoding,
which is one of the most fundamental data types of spatial representation learning. TorchSpatial contains three key components: 1) a unified location encoding framework that consolidates 15 commonly recognized location encoders, ensuring scalability and reproducibility of the implementations; 2) the LocBench benchmark tasks encompassing 7 geo-aware image classification and 10 geo-aware image
regression datasets; 3) a comprehensive suite of evaluation metrics to quantify geo-aware models’ overall performance as well as their geographic bias, with a novel Geo-Bias Score metric. Finally, we provide a detailed analysis and insights into the model performance and geographic bias of different location encoders. We believe TorchSpatial will foster future advancement of spatial representation
learning and spatial fairness in GeoAI research. The TorchSpatial model framework and LocBench benchmark are available at https://github.com/seai-lab/TorchSpatial, and the Geo-Bias Score evaluation framework is available at https://github.com/seai-lab/PyGBS.",Datasets & Benchmarks,NeurIPS,2024,Poster,Nemin Wu;Qian Cao;Zhangyu Wang;Zeping Liu;Yanlin Qi;Jielu Zhang;Joshua Ni;X. Angela Yao;Hongxu Ma;Lan Mu;Stefano Ermon;Tanuja Ganu;Akshay Nambi;Ni Lao;Gengchen Mai,False,https://openreview.net/pdf?id=DERtzUdhkk
DFDCtGQs7S,BioTrove: A Large Curated Image Dataset Enabling AI for Biodiversity,"We introduce BioTrove, the largest publicly accessible dataset designed to advance AI applications in biodiversity. Curated from the iNaturalist platform and vetted to include only research-grade data, BioTrove contains 161.9 million images, offering unprecedented scale and diversity from three primary kingdoms: Animalia (""animals""), Fungi (""fungi""), and Plantae (""plants""), spanning approximately 366.6K species. Each image is annotated with scientific names, taxonomic hierarchies, and common names, providing rich metadata to support accurate AI model development across diverse species and ecosystems.

We demonstrate the value of BioTrove by releasing a suite of CLIP models trained using a subset of 40 million captioned images, known as BioTrove-Train. This subset focuses on seven categories within the dataset that are underrepresented in standard image recognition models, selected for their critical role in biodiversity and agriculture: Aves (""birds""), Arachnida} (""spiders/ticks/mites""), Insecta (""insects""), Plantae (""plants""), Fungi (""fungi""), Mollusca (""snails""), and Reptilia (""snakes/lizards""). To support rigorous assessment, we introduce several new benchmarks and report model accuracy for zero-shot learning across life stages, rare species, confounding species, and multiple taxonomic levels.

We anticipate that BioTrove will spur the development of AI models capable of supporting digital tools for pest control, crop monitoring, biodiversity assessment, and environmental conservation. These advancements are crucial for ensuring food security, preserving ecosystems, and mitigating the impacts of climate change. BioTrove is publicly available, easily accessible, and ready for immediate use.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Chih-Hsuan Yang;Benjamin Feuer;Talukder Zaki Jubery;Zi K. Deng;Andre Nakkab;Md Zahid Hasan;Shivani Chiranjeevi;Kelly O. Marshall;Nirmal Baishnab;Asheesh K Singh;ARTI SINGH;Soumik Sarkar;Nirav Merchant;Chinmay Hegde;Baskar Ganapathysubramanian,True,https://openreview.net/pdf?id=DFDCtGQs7S
DFb1gwnhQS,FIRE: A Dataset for Feedback Integration and Refinement Evaluation of Multimodal Models,"Vision language models (VLMs) have achieved impressive progress in diverse applications, becoming a prevalent research direction. In this paper, we build FIRE, a feedback-refinement dataset, consisting of 1.1M multi-turn conversations that are derived from 27 source datasets, empowering VLMs to spontaneously refine their responses based on user feedback across diverse tasks. To scale up the data collection, FIRE is collected in two components: FIRE-100K and FIRE-1M, where FIRE-100K is generated by GPT-4V, and FIRE-1M is freely generated via models trained on FIRE-100K. Then, we build FIRE-Bench, a benchmark to comprehensively evaluate the feedback-refining capability of VLMs, which contains 11K feedback-refinement conversations as the test data, two evaluation settings, and a model to provide feedback for VLMs. We develop the FIRE-LLaVA model by fine-tuning LLaVA on FIRE-100K and FIRE-1M, which shows remarkable feedback-refining capability on FIRE-Bench and outperforms untrained VLMs by 50%, making more efficient user-agent interactions and underscoring the significance of the FIRE dataset.",Datasets & Benchmarks,NeurIPS,2024,Poster,Pengxiang Li;Zhi Gao;Bofei Zhang;Tao Yuan;Yuwei Wu;Mehrtash Harandi;Yunde Jia;Song-Chun Zhu;Qing Li,True,https://openreview.net/pdf?id=DFb1gwnhQS
DFr5hteojx,"The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models","Human feedback is central to the alignment of Large Language Models (LLMs). However, open questions remain about the methods (how), domains (where), people (who) and objectives (to what end) of feedback processes. To navigate these questions, we introduce PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic participation in feedback; (ii) census-representative samples for two countries (UK, US); and (iii) individualised ratings that link to detailed participant profiles, permitting personalisation and attribution of sample artefacts. We target subjective and multicultural perspectives on value-laden and controversial issues, where we expect interpersonal and cross-cultural disagreement. We use PRISM in three case studies to demonstrate the need for careful consideration of which humans provide alignment data.",Datasets & Benchmarks,NeurIPS,2024,Oral,Hannah Rose Kirk;Alexander Whitefield;Paul Röttger;Andrew Michael Bean;Katerina Margatina;Rafael Mosquera;Juan Manuel Ciro;Max Bartolo;Adina Williams;He He;Bertie Vidgen;Scott A. Hale,True,https://openreview.net/pdf?id=DFr5hteojx
DG4k8PJ8qv,POGEMA: A Benchmark Platform for Cooperative Multi-Agent Navigation,"Multi-agent reinforcement learning (MARL) has recently excelled in solving challenging cooperative and competitive multi-agent problems in various environments with, mostly, few agents and vector observations. Moreover, a range of crucial robotics-related tasks, such as multi-robot navigation and obstacle avoidance, that have been conventionally approached with the classical non-learnable methods (e.g., heuristic search) is currently suggested to be solved by the learning-based or hybrid methods. Still, in this domain, it is hard, not to say impossible, to conduct a fair comparison between classical, learning-based, and hybrid approaches due to the lack of a unified framework that supports both learning and evaluation. To this end, we introduce POGEMA, a set of comprehensive tools that includes a fast environment for learning, a generator of problem instances, the collection of pre-defined ones, a visualization toolkit, and a benchmarking tool that allows automated evaluation. We introduce and specify an evaluation protocol defining a range of domain-related metrics computed on the basics of the primary evaluation indicators (such as success rate and path length), allowing a fair multi-fold comparison. The results of such a comparison, which involves seven state-of-the-art MARL, search-based, and hybrid methods, are presented.",Datasets & Benchmarks,NeurIPS,2024,Reject,Alexey Skrynnik;Anton Andreychuk;Anatolii Borzilov;Alexander Chernyavskiy;Konstantin Yakovlev;Aleksandr Panov,True,https://openreview.net/pdf?id=DG4k8PJ8qv
DJVyRhT8nP,Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions,"Vision-and-Language Navigation (VLN) aims to develop embodied agents that navigate based on human instructions. However, current VLN frameworks often rely on static environments and optimal expert supervision, limiting their real-world applicability. To address this, we introduce Human-Aware Vision-and-Language Navigation (HA-VLN), extending traditional VLN by incorporating dynamic human activities and relaxing key assumptions. We propose the Human-Aware 3D (HA3D) simulator, which combines dynamic human activities with the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R) dataset, extending R2R with human activity descriptions. To tackle HA-VLN challenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and Non-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing cross-modal fusion and diverse training strategies for effective navigation in dynamic human environments. A comprehensive evaluation, including metrics considering human activities, and systematic analysis of HA-VLN's unique challenges, underscores the need for further research to enhance HA-VLN agents' real-world robustness and adaptability. Ultimately, this work provides benchmarks and insights for future research on embodied AI and Sim2Real transfer, paving the way for more realistic and applicable VLN systems in human-populated environments.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Heng Li;Minghan Li;Zhi-Qi Cheng;Yifei Dong;Yuxuan Zhou;Jun-Yan He;Qi Dai;Teruko Mitamura;Alexander G Hauptmann,True,https://openreview.net/pdf?id=DJVyRhT8nP
DSVGACQ3sO,Demystifying amortized causal discovery with transformers,"Supervised learning approaches for causal discovery from observational data often achieve competitive performance despite seemingly avoiding explicit assumptions that traditional methods make for identifiability. In this work, we investigate CSIvA \\\\citep{ke2023learning}, a transformer-based model promising to train on synthetic data and transfer to real data. First, we bridge the gap with existing identifiability theory and show that constraints on the training data distribution implicitly define a prior on the test observations. Consistent with classical approaches, good performance is achieved when we have a good prior on the test data, and the underlying model is identifiable. At the same time, we find new trade-offs. Training on datasets generated from different classes of causal models, unambiguously identifiable in isolation, improves the test generalization. Performance is still guaranteed, as the ambiguous cases resulting from the mixture of identifiable causal models are unlikely to occur (which we formally prove). 
Overall, our study finds that amortized causal discovery still needs to obey identifiability theory, but it also differs from classical methods in how the assumptions are formulated, trading more reliance on assumptions on the noise type for fewer hypotheses on the mechanisms.",main,NeurIPS,2024,Reject,Francesco Montagna;Max Cairney-Leeming;Dhanya Sridhar;Francesco Locatello,True,https://openreview.net/pdf?id=DSVGACQ3sO
DdKdr4kqxh,Identifying Spatio-Temporal Drivers of Extreme Events,"The spatio-temporal relations of impacts of extreme events and their drivers in climate data are not fully understood and there is a need of machine learning approaches to identify such spatio-temporal relations from data. The task, however, is very challenging since there are time delays between extremes and their drivers, and the spatial response of such drivers is inhomogeneous. In this work, we propose a first approach and benchmarks to tackle this challenge. Our approach is trained end-to-end to predict spatio-temporally extremes and spatio-temporally drivers in the physical input variables jointly. By enforcing the network to predict extremes from spatio-temporal binary masks of identified drivers, the network successfully identifies drivers that are correlated with extremes. We evaluate our approach on three newly created synthetic benchmarks, where two of them are based on remote sensing or reanalysis climate data, and on two real-world reanalysis datasets. The source code and datasets are publicly available at the project page https://hakamshams.github.io/IDE.",main,NeurIPS,2024,Poster,Mohamad Hakam Shams Eddin;Juergen Gall,True,https://openreview.net/pdf?id=DdKdr4kqxh
Ddak3nSqQM,"Policy Learning from Tutorial Books via Understanding, Rehearsing and Introspecting","When humans need to learn a new skill, we can acquire knowledge through written books, including textbooks, tutorials, etc. However, current research for decision-making, like reinforcement learning (RL), has primarily required numerous real interactions with the target environment to learn a skill, while failing to utilize the existing knowledge already summarized in the text. The success of Large Language Models (LLMs) sheds light on utilizing such knowledge behind the books. In this paper, we discuss a new policy learning problem called Policy Learning from tutorial Books (PLfB) upon the shoulders of LLMs’ systems, which aims to leverage rich resources such as tutorial books to derive a policy network. Inspired by how humans learn from books, we solve the problem via a three-stage framework: Understanding, Rehearsing, and Introspecting (URI). In particular, it first rehearses decision-making trajectories based on the derived knowledge after understanding the books, then introspects in the imaginary dataset to distill a policy network. 
 We build two benchmarks for PLfB~based on Tic-Tac-Toe and Football games. In experiment, URI's policy achieves at least 44% net win rate against GPT-based agents without any real data; In Football game, which is a complex scenario, URI's policy beat the built-in AIs with a 37% while using GPT-based agent can only achieve a 6\\\\% winning rate. The project page: https://plfb-football.github.io.",main,NeurIPS,2024,Oral,Xiong-Hui Chen;Ziyan Wang;Yali Du;Shengyi Jiang;Meng Fang;Yang Yu;Jun Wang,True,https://openreview.net/pdf?id=Ddak3nSqQM
DexM7d1H6e,Animal-Bench: Benchmarking Multimodal Video Models for Animal-centric Video Understanding,"With the emergence of large pre-trained multimodal video models, multiple benchmarks have been proposed to evaluate model capabilities. However, most of the benchmarks are human-centric, with evaluation data and tasks centered around human applications. Animals are an integral part of the natural world, and animal-centric video understanding is crucial for animal welfare and conservation efforts. Yet, existing benchmarks overlook evaluations focused on animals, limiting the application of the models. To address this limitation, our work established an animal-centric benchmark, namely Animal-Bench, to allow for a comprehensive evaluation of model capabilities in real-world contexts, overcoming agent-bias in previous benchmarks. Animal-Bench includes 13 tasks encompassing both common tasks shared with humans and special tasks relevant to animal conservation, spanning 7 major animal categories and 819 species, comprising a total of 41,839 data entries. To generate this benchmark, we defined a task system centered on animals and proposed an automated pipeline for animal-centric data processing. To further validate the robustness of models against real-world challenges, we utilized a video editing approach to simulate realistic scenarios like weather changes and shooting parameters due to animal movements. We evaluated 8 current multimodal video models on our benchmark and found considerable room for improvement. We hope our work provides insights for the community and opens up new avenues for research in multimodal video models. Our data and code will be released at https://github.com/PRIS-CV/Animal-Bench.",main,NeurIPS,2024,Poster,Yinuo Jing;Ruxu Zhang;Kongming Liang;Yongxiang Li;Zhongjiang He;Zhanyu Ma;Jun Guo,True,https://openreview.net/pdf?id=DexM7d1H6e
DfhcOelEnP,cPAPERS: A Dataset of Situated and Multimodal Interactive Conversations in Scientific Papers,"An emerging area of research in situated and multimodal interactive conversations (SIMMC) includes interactions in scientific papers. Since scientific papers are primarily composed of text, equations, figures, and tables, SIMMC methods must be developed specifically for each component to support the depth of inquiry and interactions required by research scientists.      This work introduces $Conversational Papers$ (cPAPERS), a dataset of conversational question-answer pairs from reviews of academic papers grounded in these paper components and their associated references from scientific documents available on arXiv. We present a data collection strategy to collect these question-answer pairs from OpenReview and associate them with contextual information from $LaTeX$ source files. Additionally, we present a series of baseline approaches utilizing Large Language Models (LLMs) in both zero-shot and fine-tuned configurations to address the cPAPERS dataset.",Datasets & Benchmarks,NeurIPS,2024,Poster,Anirudh Sundar;Jin Xu;William Gay;Christopher Gordon Richardson;Larry Heck,True,https://openreview.net/pdf?id=DfhcOelEnP
Dgy5WVgPd2,Instruction Tuning Large Language Models to Understand Electronic Health Records,"Large language models (LLMs) have shown impressive capabilities in solving a wide range of tasks based on human instructions. However, developing a conversational AI assistant for electronic health record (EHR) data remains challenging due to (1) the lack of large-scale instruction-following datasets and (2) the limitations of existing model architectures in handling complex and heterogeneous EHR data.
In this paper, we introduce MIMIC-Instr, a dataset comprising over 400K open-ended instruction-following examples derived from the MIMIC-IV EHR database. This dataset covers various topics and is suitable for instruction-tuning general-purpose LLMs for diverse clinical use cases. Additionally, we propose Llemr, a general framework that enables LLMs to process and interpret EHRs with complex data structures. Llemr demonstrates competitive performance in answering a wide range of patient-related questions based on EHR data.
Furthermore, our evaluations on clinical predictive modeling benchmarks reveal that the fine-tuned  Llemr achieves performance comparable to state-of-the-art (SOTA) baselines using curated features. The dataset and code are available at \\\\url{https://github.com/zzachw/llemr}.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Zhenbang Wu;Anant Dadu;Michael Nalls;Faraz Faghri;Jimeng Sun,True,https://openreview.net/pdf?id=Dgy5WVgPd2
DjCSjizgsH,Sim2Real-Fire: A Multi-modal Simulation Dataset for Forecast and Backtracking of Real-world Forest Fire,"The latest research on wildfire forecast and backtracking has adopted AI models, which require a large amount of data from wildfire scenarios to capture fire spread patterns. This paper explores using cost-effective simulated wildfire scenarios to train AI models and apply them to the analysis of real-world wildfire. This solution requires AI models to minimize the Sim2Real gap, a brand-new topic in the fire spread analysis research community. To investigate the possibility of minimizing the Sim2Real gap, we collect the Sim2Real-Fire dataset that contains 1M simulated scenarios with multi-modal environmental information for training AI models. We prepare 1K real-world wildfire scenarios for testing the AI models. We also propose a deep transformer, S2R-FireTr, which excels in considering the multi-modal environmental information for forecasting and backtracking the wildfire. S2R-FireTr surpasses state-of-the-art methods in real-world wildfire scenarios.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yanzhi Li;Keqiu Li;LI GUOHUI;zumin wang;Chanqing Ji;Lubo Wang;Die Zuo;Qing Guo;Feng Zhang;Manyu Wang;Di Lin,True,https://openreview.net/pdf?id=DjCSjizgsH
DpByqSbdhI,UniMTS: Unified Pre-training for Motion Time Series,"Motion time series collected from low-power, always-on mobile and wearable devices such as smartphones and smartwatches offer significant insights into human behavioral patterns, with wide applications in healthcare, automation, IoT, and AR/XR. However, given security and privacy concerns, building large-scale motion time series datasets remains difficult, hindering the development of pre-trained models for human activity analysis. Typically, existing models are trained and tested on the same dataset, leading to poor generalizability across variations in device location, device mounting orientation, and human activity type. In this paper, we introduce UniMTS, the first unified pre-training procedure for motion time series that generalizes across diverse device latent factors and activities. Specifically, we employ a contrastive learning framework that aligns motion time series with text descriptions enriched by large language models. This helps the model learn the semantics of time series to generalize across activities. Given the absence of large-scale motion time series data, we derive and synthesize time series from existing motion skeleton data with all-joint coverage. We use spatio-temporal graph networks to capture the relationships across joints for generalization across different device locations. We further design rotation-invariant augmentation to make the model agnostic to changes in device mounting orientations. Our model shows exceptional generalizability across 18 motion time series classification benchmark datasets, outperforming the best baselines by 340% in the zero-shot setting, 16.3% in the few-shot setting, and 9.2% in the full-shot setting.",main,NeurIPS,2024,Poster,Xiyuan Zhang;Diyan Teng;Ranak Roy Chowdhury;Shuheng Li;Dezhi Hong;Rajesh K. Gupta;Jingbo Shang,True,https://openreview.net/pdf?id=DpByqSbdhI
DsN7tHNo78,Manipulation Intention Understanding for Accurate Zero-Shot Composed Image Retrieval,"Composed Image Retrieval (CIR) facilitates retrieving an image matching a reference image while incorporating specified textual modifications, which is crucial for internet searches and e-commerce. Traditional supervised CIR methods rely on annotated triplets, which are labor-intensive and limit generalizability. Recent advances in Zero-Shot Composed Image Retrieval (ZS-CIR) address the challenge of performing this task without annotated triplets. A key challenge in ZS-CIR is training models on limited intention-relevant datasets to understand human intention implicitly expressed in textual modifications for accurately retrieving target images. In this paper, we introduce an image-text dataset incorporated with pseudo-manipulation intentions to enhance the training of ZS-CIR models in understanding human manipulation intents. Based on our dataset, we propose a novel framework, De-MINDS, for capturing the intent humans aim to modify, thereby enhancing the ZS-CIR model's ability to understand human manipulation descriptions. Specifically, a simple mapping network first maps image information into language space and forms a target description with a manipulation description. Subsequently, De-MINDS captures intention-relevant information from target descriptions and converts them into several pseudo-word tokens for accurate ZS-CIR. The De-MINDS model exhibits robust generalization and significant improvements in performance across four ZS-CIR tasks. It achieves performance improvements from 2.05% to 4.35% over the best methods and establishes new state-of-the-art results with comparable inference times. Our code is available at https://anonymous.4open.science/r/De-MINDS/.",main,NeurIPS,2024,Reject,Yuanmin Tang;Jing Yu;Keke Gai;Gang Xiong;Gaopeng Gou;Qi Wu,True,https://openreview.net/pdf?id=DsN7tHNo78
Dsi8Ibxg9H,Language Model as Visual Explainer,"In this paper, we present Language Model as Visual Explainer (\\\\texttt{LVX}), a systematic approach for interpreting the internal workings of vision models using a tree-structured linguistic explanation, without the need for model training. Central to our strategy is the collaboration between vision models and LLM to craft explanations. On one hand,  the LLM is harnessed to delineate hierarchical visual attributes, while concurrently,  a text-to-image API retrieves  images that are most aligned with these textual concepts. By mapping the collected texts and images  to the vision model's embedding space,  we construct a hierarchy-structured visual embedding tree. This tree is dynamically pruned and grown by querying the LLM using language templates, tailoring the explanation to the model.  Such a scheme allows us 
to seamlessly incorporate new attributes while eliminating undesired concepts based on the model's representations. When applied to testing samples, our method provides human-understandable explanations in the form of attribute-laden trees. Beyond explanation, we retrained the vision model by calibrating it on the generated concept hierarchy, allowing the model to incorporate the refined knowledge of visual attributes. To access the effectiveness of our approach, we introduce new benchmarks and conduct rigorous evaluations, demonstrating its plausibility, faithfulness, and stability.",main,NeurIPS,2024,Poster,Xingyi Yang;Xinchao Wang,True,https://openreview.net/pdf?id=Dsi8Ibxg9H
Dx88A9Zgnv,NaturalBench: Evaluating Vision-Language Models on Natural Adversarial Samples,"Vision-language models (VLMs) have made significant progress in recent visual-question-answering (VQA) benchmarks that evaluate complex visio-linguistic reasoning. However, are these models truly effective? In this work, we show that VLMs still struggle with natural images and questions that humans can easily answer, which we term $\\\\textbf{natural adversarial samples}$. We also find it surprisingly easy to generate these VQA samples from natural image-text corpora using off-the-shelf models like CLIP and ChatGPT. We propose a semi-automated approach to collect a new benchmark, ${\\\\bf NaturalBench}$, for reliably evaluating VLMs with 10,000 human-verified VQA samples. Crucially, we adopt a $\\\\textbf{vision-centric}$ design by pairing each question with two images that yield different answers, preventing ``blind'' solutions from answering without using the images. This makes NaturalBench more challenging than previous benchmarks that can largely be solved with language priors like commonsense knowledge. We evaluate ${\\\\bf 53}$ state-of-the-art VLMs on NaturalBench, showing that models like BLIP-3, LLaVA-OneVision, Cambrian-1, InternLM-XC2, Llama3.2-Vision, Molmo, Qwen2-VL, and even the (closed-source) GPT-4o lag 50%-70% behind human performance (which is above 90%). We analyze why NaturalBench is hard from two angles: (1) ${\\\\bf Compositionality:}$ Solving NaturalBench requires diverse visio-linguistic skills, including understanding attribute bindings, object relationships, and advanced reasoning like logic and counting. To this end, unlike prior work that uses a single tag per sample, we tag each NaturalBench sample with 1 to 8 skill tags for fine-grained evaluation. (2) ${\\\\bf Biases: }$ NaturalBench exposes severe biases in VLMs, as models often choose the same answer regardless of the image. We show that debiasing can be crucial for VLM performance. Lastly, we apply our benchmark curation method to diverse data sources, including long captions (over 100 words) and non-English languages like Chinese and Hindi, highlighting its potential for dynamic evaluations of VLMs.",Datasets & Benchmarks,NeurIPS,2024,Poster,Baiqi Li;Zhiqiu Lin;Wenxuan Peng;Jean de Dieu Nyandwi;Daniel Jiang;Zixian Ma;Simran Khanuja;Ranjay Krishna;Graham Neubig;Deva Ramanan,True,https://openreview.net/pdf?id=Dx88A9Zgnv
E18kRXTGmV,CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark,"Visual Question Answering~(VQA) is an important task in multimodal AI, which requires models to understand and reason on knowledge present in visual and textual data. However, most of the current VQA datasets and models are primarily focused on English and a few major world languages, with images that are Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, some datasets extend the text to other languages, either via translation or some other approaches, but usually keep the same images, resulting in narrow cultural representation. To address these limitations, we create CVQA, a new Culturally-diverse Multilingual Visual Question Answering benchmark dataset, designed to cover a rich set of languages and regions, where we engage native speakers and cultural experts in the data collection process. CVQA includes culturally-driven images and questions from across 28 countries in four continents, covering 26 languages with 11 scripts, providing a total of 9k questions. We benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and we show that the dataset is challenging for the current state-of-the-art models. This benchmark will serve as a probing evaluation suite for assessing the cultural bias of multimodal models and hopefully encourage more research efforts towards increasing cultural awareness and linguistic diversity in this field.",Datasets & Benchmarks,NeurIPS,2024,Oral,David Orlando Romero Mogrovejo;Chenyang Lyu;Haryo Akbarianto Wibowo;Santiago Góngora;Aishik Mandal;Sukannya Purkayastha;Jesus-German Ortiz-Barajas;Emilio Villa Cueva;Jinheon Baek;Soyeong Jeong;Injy Hamed;Zheng Xin Yong;Zheng Wei Lim;Paula Mónica Silva;Jocelyn Dunstan;Mélanie Jouitteau;David LE MEUR;Joan Nwatu;Ganzorig Batnasan;Munkh-Erdene Otgonbold;Munkhjargal Gochoo;Guido Ivetta;Luciana Benotti;Laura Alonso Alemany;Hernán Maina;Jiahui Geng;Tiago Timponi Torrent;Frederico Belcavello;Marcelo Viridiano;Jan Christian Blaise Cruz;Dan John Velasco;Oana Ignat;Zara Burzo;Chenxi Whitehouse;Artem Abzaliev;Teresa Clifford;Gráinne Caulfield;Teresa Lynn;Christian Salamea-Palacios;Vladimir Araujo;Yova Kementchedjhieva;Mihail Minkov Mihaylov;Israel Abebe Azime;Henok Biadglign Ademtew;Bontu Fufa Balcha;Naome A Etori;David Ifeoluwa Adelani;Rada Mihalcea;Atnafu Lambebo Tonja;Maria Camila Buitrago Cabrera;Gisela Vallejo;Holy Lovenia;Ruochen Zhang;Marcos Estecha-Garitagoitia;Mario Rodríguez-Cantelar;Toqeer Ehsan;Rendi Chevi;Muhammad Farid Adilazuarda;Ryandito Diandaru;Samuel Cahyawijaya;Fajri Koto;Tatsuki Kuribayashi;Haiyue Song;Aditya Nanda Kishore Khandavally;Thanmay Jayakumar;Raj Dabre;Mohamed Fazli Mohamed Imam;Kumaranage Ravindu Yasas Nagasinghe;Alina Dragonetti;Luis Fernando D'Haro;Olivier NIYOMUGISHA;Jay Gala;Pranjal A Chitale;Fauzan Farooqui;Thamar Solorio;Alham Fikri Aji,True,https://openreview.net/pdf?id=E18kRXTGmV
E8EAeyTxOy,InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models,"Large Language Models for code (code LLMs) have witnessed tremendous progress in recent years. With the rapid development of code LLMs, many popular evaluation benchmarks, such as HumanEval, DS-1000, and MBPP, have emerged to measure the performance of code LLMs with a particular focus on code generation tasks. However, they are insufficient to cover the full range of expected capabilities of code LLMs, which span beyond code generation to answering diverse coding-related questions. To fill this gap, we propose InfiBench, the first large-scale freeform question-answering (QA) benchmark for code to our knowledge, comprising 234 carefully selected high-quality Stack Overflow questions that span across 15 programming languages. InfiBench uses four types of model-free automatic metrics to evaluate response correctness where domain experts carefully concretize the criterion for each question. We conduct a systematic evaluation for over 100 latest code LLMs on InfiBench, leading to a series of novel and insightful findings. Our detailed analyses showcase potential directions for further advancement of code LLMs. InfiBench is fully open source at https://infi-coder.github.io/infibench and continuously expanding to foster more scientific and systematic practices for code LLM evaluation.",Datasets & Benchmarks,NeurIPS,2024,Poster,Linyi Li;Shijie Geng;Zhenwen Li;Yibo He;Hao Yu;Ziyue Hua;Guanghan Ning;Siwei Wang;Tao Xie;Hongxia Yang,True,https://openreview.net/pdf?id=E8EAeyTxOy
EADRzNJFn1,TGB 2.0: A Benchmark for Learning on Temporal Knowledge Graphs and Heterogeneous Graphs,"Multi-relational temporal graphs are powerful tools for modeling real-world data, capturing the evolving and interconnected nature of entities over time. Recently, many novel models are proposed for ML on such graphs intensifying the need for robust evaluation and standardized benchmark datasets. However, the availability of such resources remains scarce and evaluation faces added complexity due to reproducibility issues in experimental protocols. To address these challenges, we introduce Temporal Graph Benchmark 2.0 (TGB 2.0), a novel benchmarking framework tailored for evaluating methods for predicting future links on Temporal Knowledge Graphs and Temporal Heterogeneous Graphs with a focus on large-scale datasets, extending the Temporal Graph Benchmark. TGB 2.0 facilitates comprehensive evaluations by presenting eight novel datasets spanning five domains with up to 53 million edges. TGB 2.0 datasets are significantly larger
than existing datasets in terms of number of nodes, edges, or timestamps. In addition, TGB 2.0 provides a reproducible and realistic evaluation pipeline for multi-relational temporal graphs. Through extensive experimentation, we observe that 1) leveraging edge-type information is crucial to obtain high performance, 2) simple heuristic baselines are often competitive with more complex methods, 3) most methods fail to run on our largest datasets, highlighting the need for research on more scalable methods.",Datasets & Benchmarks,NeurIPS,2024,Poster,Julia Gastinger;Shenyang Huang;Mikhail Galkin;Erfan Loghmani;Ali Parviz;Farimah Poursafaei;Jacob Danovitch;Emanuele Rossi;Ioannis Koutis;Heiner Stuckenschmidt;Reihaneh Rabbany;Guillaume Rabusseau,True,https://openreview.net/pdf?id=EADRzNJFn1
EEwb201bnO,"Infer Induced Sentiment of Comment Response to Video: A New Task, Dataset and Baseline","Existing video multi-modal sentiment analysis mainly focuses on the sentiment expression of people within the video, yet often neglects the induced sentiment of viewers while watching the videos. Induced sentiment of viewers is essential for inferring the public response to videos and has broad application in analyzing public societal sentiment, effectiveness of advertising and other areas. The micro videos and the related comments provide a rich application scenario for viewers’ induced sentiment analysis. In light of this, we introduces a novel research task, Multimodal Sentiment Analysis for Comment Response of Video Induced(MSA-CRVI), aims to infer opinions and emotions according to comments response to micro video. Meanwhile, we manually annotate a dataset named Comment Sentiment toward to Micro Video (CSMV) to support this research. It is the largest video multi-modal sentiment dataset in terms of scale and video duration to our knowledge, containing 107, 267 comments and 8, 210 micro videos with a video duration of 68.83 hours. To infer the induced sentiment of comment should leverage the video content, we propose the Video Content-aware Comment Sentiment Analysis (VC-CSA) method as a baseline to address the challenges inherent in this new task. Extensive experiments demonstrate that our method is showing significant improvements over other established baselines. We make the dataset and source code publicly available at https://github.com/IEIT-AGI/MSA-CRVI.",Datasets & Benchmarks,NeurIPS,2024,Poster,Qi Jia;Baoyu Fan;Cong Xu;Lu Liu;Liang Jin;Guoguang Du;Zhenhua Guo;Yaqian Zhao;Xuanjing Huang;Rengang Li,True,https://openreview.net/pdf?id=EEwb201bnO
EFV7fLZRWO,Muscles in Time: Learning to Understand Human Motion In-Depth by Simulating Muscle Activations,"Exploring the intricate dynamics between muscular and skeletal structures is pivotal for understanding human motion. This domain presents substantial challenges, primarily attributed to the intensive resources required for acquiring ground truth muscle activation data, resulting in a scarcity of datasets.
In this work, we address this issue by establishing Muscles in Time (MinT), a large-scale synthetic muscle activation dataset.
For the creation of MinT, we enriched existing motion capture datasets by incorporating muscle activation simulations derived from biomechanical human body models using the OpenSim platform, a common framework used in biomechanics and human motion research.
Starting from simple pose sequences, our pipeline enables us to extract detailed information about the timing of muscle activations within the human musculoskeletal system.
Muscles in Time contains over nine hours of simulation data covering 227 subjects and 402 simulated muscle strands. 
We demonstrate the utility of this dataset by presenting results on neural network-based muscle activation estimation from human pose sequences with two different sequence-to-sequence architectures.",Datasets & Benchmarks,NeurIPS,2024,Poster,David Schneider;Simon Reiß;Marco Kugler;Alexander Jaus;Kunyu Peng;Susanne Sutschet;M. Saquib Sarfraz;Sven Matthiesen;Rainer Stiefelhagen,True,https://openreview.net/pdf?id=EFV7fLZRWO
EQhLbuitns,HourVideo: 1-Hour Video-Language Understanding,"We present **HourVideo**, a benchmark dataset for hour-long video-language understanding. Our dataset consists of a novel task suite comprising summarization, perception (*recall*, *tracking*), visual reasoning (*spatial*, *temporal*, *predictive*, *causal*, *counterfactual*), and navigation (*room-to-room*, *object retrieval*) tasks. HourVideo includes 500 manually curated egocentric videos from the Ego4D dataset, spanning durations of 20 to 120 minutes, and features **12,976 high-quality, five-way multiple-choice questions**. Benchmarking results reveal that multimodal models, including GPT-4 and LLaVA-NeXT, achieve marginal improvements over random chance. In stark contrast, human experts significantly outperform the state-of-the-art long-context multimodal model, Gemini Pro 1.5 (85.0\\\\% vs. 37.3\\\\%), highlighting a substantial gap in multimodal capabilities. Our benchmark, evaluation toolkit, prompts, and documentation are available at https://hourvideo.stanford.edu.",Datasets & Benchmarks,NeurIPS,2024,Poster,Keshigeyan Chandrasegaran;Agrim Gupta;Lea M. Hadzic;Taran Kota;Jimming He;Cristobal Eyzaguirre;Zane Durante;Manling Li;Jiajun Wu;Li Fei-Fei,True,https://openreview.net/pdf?id=EQhLbuitns
ETZk7lqyaF,PersonalSum: A User-Subjective Guided Personalized Summarization Dataset for Large Language Models,"With the rapid advancement of Natural Language Processing in recent years, numerous studies have shown that generic summaries generated by Large Language Models (LLMs) can sometimes surpass those annotated by experts, such as journalists, according to human evaluations. However, there is limited research on whether these generic summaries meet the individual needs of ordinary people. The biggest obstacle is the lack of human-annotated datasets from the general public. Existing work on personalized summarization often relies on pseudo datasets created from generic summarization datasets or controllable tasks that focus on specific named entities or other aspects, such as the length and specificity of generated summaries, collected from hypothetical tasks without the annotators' initiative. To bridge this gap, we propose a high-quality, personalized, manually annotated summarization dataset called PersonalSum. This dataset is the first to investigate whether the focus of public readers differs from the generic summaries generated by LLMs. It includes user profiles, personalized summaries accompanied by source sentences from given articles, and machine-generated generic summaries along with their sources. We investigate several personal signals — entities/topics, plot, and structure of articles—that may affect the generation of personalized summaries using LLMs in a few-shot in-context learning scenario. Our preliminary results and analysis indicate that entities/topics are merely one of the key factors that impact the diverse preferences of users, and personalized summarization remains a significant challenge for existing LLMs.",Datasets & Benchmarks,NeurIPS,2024,Poster,Lemei Zhang;Peng Liu;Marcus Tiedemann Oekland Henriksboe;Even W. Lauvrak;Jon Atle Gulla;Heri Ramampiaro,True,https://openreview.net/pdf?id=ETZk7lqyaF
EWm9zR5Qy1,The Multimodal Universe: Enabling Large-Scale Machine Learning with 100 TB of Astronomical Scientific Data,"We present the `Multimodal Universe`, a large-scale multimodal dataset of scientific astronomical data, compiled specifically to facilitate machine learning research. Overall, our dataset contains hundreds of millions of astronomical observations, constituting 100TB of multi-channel and hyper-spectral images, spectra, multivariate time series, as well as a wide variety of associated scientific measurements and metadata. In addition, we include a range of benchmark tasks representative of standard practices for machine learning methods in astrophysics. This massive dataset will enable the development of large multi-modal models specifically targeted towards scientific applications. All codes used to compile the dataset, and a description of how to access the data is available at https://github.com/MultimodalUniverse/MultimodalUniverse",Datasets & Benchmarks,NeurIPS,2024,Poster,Eirini Angeloudi;Jeroen Audenaert;Micah Bowles;Benjamin M. Boyd;David Chemaly;Brian Cherinka;Ioana Ciuca;Miles Cranmer;Aaron Do;Matthew Grayling;Erin Elizabeth Hayes;Tom Hehir;Shirley Ho;Marc Huertas-Company;Kartheik G. Iyer;Maja Jablonska;Francois Lanusse;Henry W. Leung;Kaisey Mandel;Juan Rafael Martínez-Galarza;Peter Melchior;Lucas Thibaut Meyer;Liam Holden Parker;Helen Qu;Jeff Shen;Michael J. Smith;Connor Stone;Mike Walmsley;John F Wu,True,https://openreview.net/pdf?id=EWm9zR5Qy1
EXwf5iE98P,IKEA Manuals at Work: 4D Grounding of Assembly Instructions on Internet Videos,"Shape assembly is a ubiquitous task in daily life, integral for constructing complex 3D structures like IKEA furniture. While significant progress has been made in developing autonomous agents for shape assembly, existing datasets have not yet tackled the 4D grounding of assembly instructions in videos, essential for a holistic understanding of assembly in 3D space over time. We introduce IKEA Video Manuals, a dataset that features 3D models of furniture parts, instructional manuals, assembly videos from the Internet, and most importantly, annotations of dense spatio-temporal alignments between these data modalities. To demonstrate the utility of IKEA Video Manuals, we present five applications essential for shape assembly: assembly plan generation, part-conditioned segmentation, part-conditioned pose estimation, video object segmentation, and furniture assembly based on instructional video manuals. For each application, we provide evaluation metrics and baseline methods. Through experiments on our annotated data, we highlight many challenges in grounding assembly instructions in videos to improve shape assembly, including handling occlusions, varying viewpoints, and extended assembly sequences.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yunong Liu;Cristobal Eyzaguirre;Manling Li;Shubh Khanna;Juan Carlos Niebles;Vineeth Ravi;Saumitra Mishra;Weiyu Liu;Jiajun Wu,True,https://openreview.net/pdf?id=EXwf5iE98P
EiH6WWLzlu,ShareGPT4Video: Improving Video Understanding and Generation with Better Captions,"We present the ShareGPT4Video series, aiming to facilitate the video understanding of large video-language models (LVLMs) and the video generation of text-to-video models (T2VMs) via dense and precise captions. The series comprises: 1) ShareGPT4Video, 40K GPT4V annotated dense captions of videos with various lengths and sources, developed through carefully designed data filtering and annotating strategy. 2) ShareCaptioner-Video, an efficient and capable captioning model for arbitrary videos, with 4.8M high-quality aesthetic videos annotated by it. 3) ShareGPT4Video-8B, a simple yet superb LVLM that reached SOTA performance on three advancing video benchmarks. To achieve this, taking aside the non-scalable costly human annotators, we find using GPT4V to caption video with a naive multi-frame or frame-concatenation input strategy leads to less detailed and sometimes temporal-confused results. We argue the challenge of designing a high-quality video captioning strategy lies in three aspects: 1) Inter-frame precise temporal change understanding. 2) Intra-frame detailed content description. 3) Frame-number scalability for arbitrary-length videos. To this end, we meticulously designed a differential video captioning strategy, which is stable, scalable, and efficient for generating captions for videos with arbitrary resolution, aspect ratios, and length. Based on it, we construct ShareGPT4Video, which contains 40K high-quality videos spanning a wide range of categories, and the resulting captions encompass rich world knowledge, object attributes, camera movements, and crucially, detailed and precise temporal descriptions of events. Based on ShareGPT4Video, we further develop ShareCaptioner-Video, a superior captioner capable of efficiently generating high-quality captions for arbitrary videos. We annotated 4.8M aesthetically appealing videos by it and verified their effectiveness on a 10-second text2video generation task. For video understanding, we verified the effectiveness of ShareGPT4Video on several current LVLM architectures and presented our superb new LVLM ShareGPT4Video-8B. All the models, strategies, and annotations will be open-sourced and we hope this project can serve as a pivotal resource for advancing both the LVLMs and T2VMs community.",Datasets & Benchmarks,NeurIPS,2024,Poster,Lin Chen;Xilin Wei;Jinsong Li;Xiaoyi Dong;Pan Zhang;Yuhang Zang;Zehui Chen;Haodong Duan;Bin Lin;Zhenyu Tang;Li Yuan;Yu Qiao;Dahua Lin;Feng Zhao;Jiaqi Wang,True,https://openreview.net/pdf?id=EiH6WWLzlu
ElUrNM9U8c,$\\\\nabla^2$DFT: A Universal Quantum Chemistry Dataset of Drug-Like Molecules and a Benchmark for Neural Network Potentials,"Methods of computational quantum chemistry provide accurate approximations of molecular properties crucial for computer-aided drug discovery and other areas of chemical science. 
However, high computational complexity limits the scalability of their applications.
Neural network potentials (NNPs) are a promising alternative to quantum chemistry methods, but they require large and diverse datasets for training.
This work presents a new dataset and benchmark called $\\\\nabla^2$DFT that is based on the nablaDFT.
It contains twice as much molecular structures, three times more conformations, new data types and tasks, and state-of-the-art models.
The dataset includes energies, forces, 17 molecular properties, Hamiltonian and overlap matrices, and a wavefunction object.
All calculations were performed at the DFT level ($\\\\omega$B97X-D/def2-SVP) for each conformation. 
Moreover, $\\\\nabla^2$DFT is the first dataset that contains relaxation trajectories for a substantial number of drug-like molecules. 
We also introduce a novel benchmark for evaluating NNPs in molecular property prediction, Hamiltonian prediction, and conformational optimization tasks. 
Finally, we propose an extendable framework for training NNPs and implement 10 models within it.",Datasets & Benchmarks,NeurIPS,2024,Poster,Kuzma Khrabrov;Anton Ber;Artem Tsypin;Konstantin Ushenin;Egor Rumiantsev;Alexander Telepov;Dmitry Protasov;Ilya Shenbin;Anton M. Alekseev;Mikhail Shirokikh;Sergey Nikolenko;Elena Tutubalina;Artur Kadurin,True,https://openreview.net/pdf?id=ElUrNM9U8c
Eogs84mv7N,Biomedical Visual Instruction Tuning with Clinician Preference Alignment,"Recent advancements in multimodal foundation models have showcased impressive capabilities in understanding and reasoning with visual and textual information. Adapting these foundation models trained for general usage to specialized domains like biomedicine requires large-scale domain-specific instruction datasets. While existing works have explored curating such datasets automatically, the resultant datasets are not explicitly aligned with domain expertise. In this work, we propose a data-centric framework, Biomedical Visual Instruction Tuning with Clinician Preference Alignment (BioMed-VITAL), that incorporates clinician preferences into both stages of generating and selecting instruction data for tuning biomedical multimodal foundation models. First, during the generation stage, we prompt the GPT-4V generator with a diverse set of clinician-selected demonstrations for preference-aligned data candidate generation. Then, during the selection phase, we train a separate selection model, which explicitly distills clinician and policy-guided model preferences into a rating function to select high-quality data for medical instruction tuning. Results show that the model tuned with the instruction-following data from our method demonstrates a significant improvement in open visual chat (18.5% relatively) and medical VQA (win rate up to 81.73%). Our instruction-following data and models are available at https://BioMed-VITAL.github.io.",Datasets & Benchmarks,NeurIPS,2024,Poster,Hejie Cui;Lingjun Mao;Xin LIANG;Jieyu Zhang;Hui Ren;Quanzheng Li;Xiang Li;Carl Yang,True,https://openreview.net/pdf?id=Eogs84mv7N
EpnsUQavJA,CoIN: A Benchmark of Continual Instruction Tuning for Multimodel Large Language Models,"Instruction tuning demonstrates impressive performance in adapting Multimodal Large Language Models (MLLMs) to follow task instructions and improve generalization ability.  By extending tuning across diverse tasks, MLLMs can further enhance their understanding of world knowledge and instruction intent. However, continual instruction tuning has been largely overlooked and there are no public benchmarks available. In this paper, we present CoIN, a comprehensive benchmark tailored for assessing the behavior of existing MLLMs under continual instruction tuning. CoIN comprises 10 meticulously crafted datasets spanning 8 tasks, ensuring diversity and serving as a robust evaluation framework to assess crucial aspects of continual instruction tuning, such as task order, instruction diversity and volume. Additionally, apart from traditional evaluation, we design another LLM-based metric to assess the knowledge preserved within MLLMs for reasoning. Following an in-depth evaluation of several MLLMs, we demonstrate that they still suffer catastrophic forgetting, and the failure in instruction alignment assumes the main responsibility, instead of reasoning knowledge forgetting.  To this end, we introduce MoELoRA which is effective in retaining the previous instruction alignment.",Datasets & Benchmarks,NeurIPS,2024,Poster,Cheng Chen;Junchen Zhu;Xu Luo;Heng Tao Shen;Jingkuan Song;Lianli Gao,True,https://openreview.net/pdf?id=EpnsUQavJA
EqaSEbU4LP,WikiDO: A New Benchmark Evaluating Cross-Modal Retrieval for Vision-Language Models,"Cross-modal (image-to-text and text-to-image) retrieval is an established task used in evaluation benchmarks to test the performance of vision-language models (VLMs). Several state-of-the-art VLMs (e.g. CLIP, BLIP-2) have achieved near-perfect performance on widely-used image-text retrieval benchmarks such as MSCOCO-Test-5K and Flickr30K-Test-1K. As a measure of out-of-distribution (OOD) generalization, prior works rely on zero-shot performance evaluated on one dataset (Flickr) using a VLM finetuned on another one (MSCOCO). We argue that such comparisons are insufficient to assess the OOD generalization capability of models due to high visual and linguistic similarity between the evaluation and finetuning datasets. To address this gap, we introduce WikiDO (drawn from Wikipedia Diversity Observatory), a novel cross-modal retrieval benchmark to assess the OOD generalization capabilities of pretrained VLMs. This consists of newly scraped 380K image-text pairs from Wikipedia with domain labels, a carefully curated, human-verified a)in-distribution (ID) test set (3K) and b) OOD test set (3K). The image-text pairs are very diverse in topics and geographical locations. We evaluate different VLMs of varying capacity on the \\\\wikido benchmark; BLIP-2 achieves zero-shot performance of $R@1\\\\approx66\\\\%$ on the OOD test set, compared to $\\\\approx$ $81\\\\%$ on COCO and $\\\\approx95\\\\%$ on Flickr. When fine-tuned on WikiDO, the $R@1$ improvement is at most $\\\\approx5\\\\%$ on OOD instances compared to $\\\\approx12\\\\%$ on ID instances. We probe the VLMs with varying finetuning objectives and datasets of varying sizes to identify what aids OOD generalization the most. Our results confirm that WikiDO offers a strong cross-modal benchmark for current VLMs in specifically evaluating for OOD generalization. Our benchmark is hosted as a competition at https://kaggle.com/competitions/wikido24 with public access to dataset and code.",Datasets & Benchmarks,NeurIPS,2024,Poster,Pavan Kalyan Tankala;Piyush Singh Pasi;Sahil Dharod;Azeem Motiwala;Preethi Jyothi;Aditi Chaudhary;Krishna Srinivasan,True,https://openreview.net/pdf?id=EqaSEbU4LP
EvEqYlQv8T,Automating Dataset Updates Towards Reliable and Timely Evaluation of Large Language Models,"Large language models (LLMs) have achieved impressive performance across various natural language benchmarks, prompting a continual need to curate more difficult datasets for larger LLMs, which is costly and time-consuming. In this paper, we propose to automate dataset updating and provide systematical analysis regarding its effectiveness in dealing with benchmark leakage issue, difficulty control, and stability. Thus, once current benchmark has been mastered or leaked, we can update it for timely and reliable evaluation. There are two updating strategies: 1) mimicking strategy to generate similar samples based on original data, preserving stylistic and contextual essence, and 2) extending strategy that further expands existing samples at varying cognitive levels by adapting Bloom’s taxonomy of educational objectives. Extensive experiments on updated MMLU and BIG-Bench demonstrate the stability of the proposed strategies and find that the mimicking strategy can effectively alleviate issues of overestimation from benchmark leakage. In cases where the efficient mimicking strategy fails, our extending strategy still shows promising results. Additionally, by controlling the difficulty, we can better discern the models’ performance and enable fine-grained analysis — neither too difficult nor too easy an exam can fairly judge students’ learning status. To the best of our knowledge, we are the first to automate updating benchmarks for reliable and timely evaluation. Our demo leaderboard can be found at https://yingjiahao14.github.io/Automating-DatasetUpdates/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jiahao Ying;Yixin Cao;Yushi Bai;Qianru Sun;Bo Wang;Wei Tang;Zhaojun Ding;Yizhe Yang;Xuanjing Huang;Shuicheng YAN,False,https://openreview.net/pdf?id=EvEqYlQv8T
EvgyfFsv0w,Stylebreeder: Exploring and Democratizing Artistic Styles through Text-to-Image Models,"Text-to-image models are becoming increasingly popular, revolutionizing the landscape of digital art creation by enabling highly detailed and creative visual content generation. These models have been widely employed across various domains, particularly in art generation, where they facilitate a broad spectrum of creative expression and democratize access to artistic creation. In this paper, we introduce STYLEBREEDER, a comprehensive dataset of 6.8M images and 1.8M prompts generated by 95K users on Artbreeder,  a platform that has emerged as a significant hub for creative exploration with over 13M users. We introduce a series of tasks with this dataset aimed at identifying diverse artistic styles, generating personalized content, and recommending styles based on user interests. By documenting unique, user-generated styles that transcend conventional categories like 'cyberpunk' or 'Picasso,' we explore the potential for unique, crowd-sourced styles that could provide deep insights into the collective creative psyche of users worldwide. We also evaluate different personalization methods to enhance artistic expression and introduce a style atlas, making these models available in LoRA format for public use. Our research demonstrates the potential of text-to-image diffusion models to uncover and promote unique artistic expressions, further democratizing AI in art and fostering a more diverse and inclusive artistic community. The dataset, code, and models are available at https://stylebreeder.github.io under a Public Domain (CC0) license.",Datasets & Benchmarks,NeurIPS,2024,Poster,Matthew Zheng;Enis Simsar;Hidir Yesiltepe;Federico Tombari;Joel Simon;Pinar Yanardag,True,https://openreview.net/pdf?id=EvgyfFsv0w
ExeIyx6U0Z,LLaNA: Large Language and NeRF Assistant,"Multimodal Large Language Models (MLLMs) have demonstrated an excellent understanding of images and 3D data. However, both modalities have shortcomings in holistically capturing the appearance and geometry of objects. Meanwhile, Neural Radiance Fields (NeRFs), which encode information within the weights of a simple Multi-Layer Perceptron (MLP), have emerged as an increasingly widespread modality that simultaneously encodes the geometry and photorealistic appearance of objects. This paper investigates the feasibility and effectiveness of ingesting NeRF into MLLM. We create LLaNA, the first general-purpose NeRF-language
assistant capable of performing new tasks such as NeRF captioning and Q&A. Notably, our method directly processes the weights of the NeRF’s MLP to extract information about the represented objects without the need to render images or materialize 3D data structures. Moreover, we build a dataset of NeRFs with text annotations for various NeRF-language tasks with no human intervention.
Based on this dataset, we develop a benchmark to evaluate the NeRF understanding capability of our method. Results show that processing NeRF weights performs favourably against extracting 2D or 3D representations from NeRFs.",main,NeurIPS,2024,Poster,Andrea Amaduzzi;Pierluigi Zama Ramirez;Giuseppe Lisanti;Samuele Salti;Luigi di Stefano,True,https://openreview.net/pdf?id=ExeIyx6U0Z
F7rAX6yiS2,DiReCT: Diagnostic Reasoning for Clinical Notes via Large Language Models,"Large language models (LLMs) have recently showcased remarkable capabilities, spanning a wide range of tasks and applications, including those in the medical domain. Models like GPT-4 excel in medical question answering but may face challenges in the lack of interpretability when handling complex tasks in real clinical settings. We thus introduce the diagnostic reasoning dataset for clinical notes (DiReCT), aiming at evaluating the reasoning ability and interpretability of LLMs compared to human doctors. It contains 511 clinical notes, each meticulously annotated by physicians, detailing the diagnostic reasoning process from observations in a clinical note to the final diagnosis. Additionally, a diagnostic knowledge graph is provided to offer essential knowledge for reasoning, which may not be covered in the training data of existing LLMs. Evaluations of leading LLMs on DiReCT bring out a significant gap between their reasoning ability and that of human doctors, highlighting the critical need for models that can reason effectively in real-world clinical scenarios.",Datasets & Benchmarks,NeurIPS,2024,Poster,Bowen Wang;Jiuyang Chang;Yiming Qian;Guoxin Chen;Junhao Chen;Zhouqiang Jiang;Jiahao Zhang;Yuta Nakashima;Hajime Nagahara,True,https://openreview.net/pdf?id=F7rAX6yiS2
F7tGQ7b10q,HonestLLM: Toward an Honest and Helpful Large Language Model,"Large Language Models (LLMs) have achieved remarkable success across various industries and applications, owing to their exceptional generative capabilities. Nevertheless, honesty and helpfulness, which ensure safe and useful real-world deployments, have been considered as the longstanding cornerstones in practice. In this paper, we first established comprehensive principles for honesty LLM and further created the HoneSet with 930 queries across six categories, which is designed to evaluate LLMs’ ability to maintain honesty. Then, we improved the honesty and helpfulness of LLMs in both training-free and fine-tuning settings. Specifically, we propose a training-free method named Curiosity-Driven Prompting, which enables LLMs to express their internal confusion and uncertainty about the given query and then optimize their responses. Moreover, we also propose a two-stage fine-tuning approach, inspired by curriculum learning, to enhance the honesty and helpfulness of LLMs. The method first teaches LLMs to distinguish between honest and dishonest, and then LLMs are trained to learn to respond more helpfully. Experimental results demonstrated that both of the two proposed methods improve the helpfulness of LLMs while making them maintain honesty. Our research has paved the way for more reliable and trustworthy LLMs in real-world applications.",main,NeurIPS,2024,Poster,Chujie Gao;Siyuan Wu;Yue Huang;Dongping Chen;Qihui Zhang;Zhengyan Fu;Yao Wan;Lichao Sun;Xiangliang Zhang,True,https://openreview.net/pdf?id=F7tGQ7b10q
F8aSOovlEP,MECD: Unlocking Multi-Event Causal Discovery in Video Reasoning,"Video causal reasoning aims to achieve a high-level understanding of video content from a causal perspective. However, current video reasoning tasks are limited in scope, primarily executed in a question-answering paradigm and focusing on short videos containing only a single event and simple causal relationships, lacking comprehensive and structured causality analysis for videos with multiple events. To fill this gap, we introduce a new task and dataset, Multi-Event Causal Discovery (MECD). It aims to uncover the causal relationships between events distributed chronologically across long videos. Given visual segments and textual descriptions of events, MECD requires identifying the causal associations between these events to derive a comprehensive, structured event-level video causal diagram explaining why and how the final result event occurred. To address MECD, we devise a novel framework inspired by the Granger Causality method, using an efficient mask-based event prediction model to perform an Event Granger Test, which estimates causality by comparing the predicted result event when premise events are masked versus unmasked. Furthermore, we integrate causal inference techniques such as front-door adjustment and counterfactual inference to address challenges in MECD like causality confounding and illusory causality. Experiments validate the effectiveness of our framework in providing causal relationships in multi-event videos, outperforming GPT-4o and VideoLLaVA by 5.7% and 4.1%, respectively.",main,NeurIPS,2024,Spotlight,Tieyuan Chen;Huabin Liu;Tianyao He;Yihang Chen;Chaofan Gan;Xiao Ma;Cheng Zhong;Yang Zhang;Yingxue Wang;Hui Lin;Weiyao Lin,True,https://openreview.net/pdf?id=F8aSOovlEP
FCsEvaMorw,Rainbow Teaming: Open-Ended Generation of Diverse Adversarial Prompts,"As large language models (LLMs) become increasingly prevalent across many real-world applications, understanding and enhancing their robustness to adversarial attacks is of paramount importance. Existing methods for identifying adversarial prompts tend to focus on specific domains, lack diversity, or require extensive human annotations. To address these limitations, we present Rainbow Teaming, a novel black-box approach for producing a diverse collection of adversarial prompts. Rainbow Teaming casts adversarial prompt generation as a quality-diversity problem and uses open-ended search to generate prompts that are both effective and diverse. Focusing on the safety domain, we use Rainbow Teaming to target various state-of-the-art LLMs, including the Llama 2 and Llama 3 models. Our approach reveals hundreds of effective adversarial prompts, with an attack success rate exceeding 90% across all tested models. Furthermore, we demonstrate that prompts generated by Rainbow Teaming are highly transferable and that fine-tuning models with synthetic data generated by our method significantly enhances their safety without sacrificing general performance or helpfulness. We additionally explore the versatility of Rainbow Teaming by applying it to question answering and cybersecurity, showcasing its potential to drive robust open-ended self-improvement in a wide range of applications.",main,NeurIPS,2024,Poster,Mikayel Samvelyan;Sharath Chandra Raparthy;Andrei Lupu;Eric Hambro;Aram H. Markosyan;Manish Bhatt;Yuning Mao;Minqi Jiang;Jack Parker-Holder;Jakob Nicolaus Foerster;Tim Rocktäschel;Roberta Raileanu,True,https://openreview.net/pdf?id=FCsEvaMorw
FI89ORf7YH,Scalable Early Childhood Reading Performance Prediction,"Models for student reading performance can empower educators and institutions to proactively identify at-risk students, thereby enabling early and tailored instructional interventions. However, there are no suitable publicly available educational datasets for modeling and predicting future reading performance. In this work, we introduce the Enhanced Core Reading Instruction (ECRI) dataset, a novel large-scale longitudinal tabular dataset collected across 44 schools with 6,916 students and 172 teachers. We leverage the dataset to empirically evaluate the ability of state-of-the-art machine learning models to recognize early childhood educational patterns in multivariate and partial measurements. Specifically, we demonstrate a simple self-supervised strategy in which a Multi-Layer Perception (MLP) network is pre-trained over masked inputs to outperform several strong baselines while generalizing over diverse educational settings. To facilitate future developments in precise modeling and responsible use of models for individualized and early intervention strategies, our data and code are available at https://ecri-data.github.io/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zhongkai Shangguan;Zanming Huang;Eshed Ohn-Bar;Ola Ozernov-Palchik;Derek Kosty;Michael Stoolmiller;Hank Fien,True,https://openreview.net/pdf?id=FI89ORf7YH
FJIetdSItj,LV-Eval: A Balanced Long-Context Benchmark with 5 Length Levels Up to 256K,"State-of-the-art large language models (LLMs) are now claiming remarkable supported context lengths of 256k or even more. In contrast, the average context lengths of mainstream benchmarks are insufficient (5k-21k), and they suffer from potential knowledge leakage and inaccurate metrics, resulting in biased evaluation. This paper introduces LV-Eval, a challenging long-context benchmark with five length levels (16k, 32k, 64k, 128k, and 256k) reaching up to 256k words. LV-Eval features two main tasks, single-hop QA and multi-hop QA, comprising 11 bilingual datasets. The design of LV-Eval has incorporated three key techniques, namely confusing facts insertion (CFI), keyword and phrase replacement (KPR), and keyword-recall-based metric design. The advantages of LV-Eval include controllable evaluation across context lengths, challenging test instances with confusing facts, mitigated knowledge leakage, and more objective evaluations. We evaluate 10 LLMs on LV-Eval and conduct ablation studies on the techniques used in LV-Eval construction. The results reveal that: (i) Commercial LLMs generally outperform open-source LLMs when evaluated within length levels shorter than their claimed context length. However, their overall performance is surpassed by open-source LLMs with longer context lengths. (ii) Extremely long-context LLMs, such as Yi-6B-200k and Llama3-8B-1M, exhibit a relatively gentle degradation of performance, but their absolute performances may not necessarily be higher than those of LLMs with shorter context lengths. (iii) LLMs' performances can significantly degrade in the presence of confusing information, especially in the pressure test of ""needle in a haystack"". (iv) Issues related to knowledge leakage and inaccurate metrics introduce bias in evaluation, and these concerns are alleviated in LV-Eval. All datasets and evaluation codes are released at: https://github.com/infinigence/LVEval.",Datasets & Benchmarks,NeurIPS,2024,Reject,Tao Yuan;Xuefei Ning;Dong Zhou;Zhijie Yang;Shiyao Li;Minghui Zhuang;Zheyue Tan;Zhuyu Yao;Dahua Lin;Boxun Li;Guohao Dai;Shengen Yan;Yu Wang,True,https://openreview.net/pdf?id=FJIetdSItj
FN02v4nD8y,Few-shot Algorithms for Consistent Neural Decoding (FALCON) Benchmark,"Intracortical brain-computer interfaces (iBCIs) can restore movement and communication abilities to individuals with paralysis by decoding their intended behavior from neural activity recorded with an implanted device. While this activity yields high-performance decoding over short timescales, neural data is often nonstationary, which can lead to decoder failure if not accounted for. To maintain performance, users must frequently recalibrate decoders, which requires the arduous collection of new neural and behavioral data. Aiming to reduce this burden, several approaches have been developed that either limit recalibration data requirements (few-shot approaches) or eliminate explicit recalibration entirely (zero-shot approaches). However, progress is limited by a lack of standardized datasets and comparison metrics, causing methods to be compared in an ad hoc manner.  Here we introduce the FALCON benchmark suite (Few-shot Algorithms for COnsistent Neural decoding) to standardize evaluation of iBCI robustness. FALCON curates five datasets of neural and behavioral data that span movement and communication tasks to focus on behaviors of interest to modern-day iBCIs. Each dataset includes calibration data, optional few-shot recalibration data, and private evaluation data. We implement a flexible evaluation platform which only requires user-submitted code to return behavioral predictions on unseen data. We also seed the benchmark by applying baseline methods spanning several classes of possible approaches. FALCON aims to provide rigorous selection criteria for robust iBCI decoders, easing their translation to real-world devices. https://snel-repo.github.io/falcon/",Datasets & Benchmarks,NeurIPS,2024,Poster,Brianna M. Karpowicz;Joel Ye;Chaofei Fan;Pablo Tostado-Marcos;Fabio Rizzoglio;Clayton B Washington;Thiago Scodeler;Diogo S de Lucena;Samuel R. Nason-Tomaszewski;Matthew Mender;Xuan Ma;Ezequiel Matias Arneodo;Leigh Hochberg;Cynthia Chestek;Jaimie M. Henderson;Timothy Q Gentner;Vikash Gilja;Lee E. Miller;Adam G. Rouse;Robert Gaunt;Jennifer L Collinger;Chethan Pandarinath,True,https://openreview.net/pdf?id=FN02v4nD8y
FOkKndty5B,SlowFocus: Enhancing Fine-grained Temporal Understanding in Video LLM,"Large language models (LLMs) have demonstrated exceptional capabilities in text understanding, which has paved the way for their expansion into video LLMs (Vid-LLMs) to analyze video data. However, current Vid-LLMs struggle to simultaneously retain high-quality frame-level semantic information (i.e., a sufficient number of tokens per frame) and comprehensive video-level temporal information (i.e., an adequate number of sampled frames per video). This limitation hinders the advancement of Vid-LLMs towards fine-grained video understanding. To address this issue, we introduce the SlowFocus mechanism, which significantly enhances the equivalent sampling frequency without compromising the quality of frame-level visual tokens. SlowFocus begins by identifying the query-related temporal segment based on the posed question, then performs dense sampling on this segment to extract local high-frequency features. A multi-frequency mixing attention module is further leveraged to aggregate these local high-frequency details with global low-frequency contexts for enhanced temporal comprehension. Additionally, to tailor Vid-LLMs to this innovative mechanism, we introduce a set of training strategies aimed at bolstering both temporal grounding and detailed temporal reasoning capabilities. Furthermore, we establish FineAction-CGR, a benchmark specifically devised to assess the ability of Vid-LLMs to process fine-grained temporal understanding tasks. Comprehensive experiments demonstrate the superiority of our mechanism across both existing public video understanding benchmarks and our proposed FineAction-CGR.",main,NeurIPS,2024,Poster,Ming Nie;Dan Ding;Chunwei Wang;Yuanfan Guo;Jianhua Han;Hang Xu;Li Zhang,True,https://openreview.net/pdf?id=FOkKndty5B
FSgwgQXTxo,Reasoning Multi-Agent Behavioral Topology for Interactive Autonomous Driving,"Autonomous driving system aims for safe and social-consistent driving through the behavioral integration among interactive agents. However, challenges remain due to multi-agent scene uncertainty and heterogeneous interaction. Current dense and sparse behavioral representations struggle with inefficiency and inconsistency in multi-agent modeling, leading to instability of collective behavioral patterns when integrating prediction and planning (IPP). To address this, we initiate a topological formation that serves as a compliant behavioral foreground to guide downstream trajectory generations. Specifically, we introduce Behavioral Topology (BeTop), a pivotal topological formulation that explicitly represents the consensual behavioral pattern among multi-agent future. BeTop is derived from braid theory to distill compliant interactive topology from multi-agent future trajectories. A synergistic learning framework (BeTopNet) supervised by BeTop facilitates the consistency of behavior prediction and planning within the predicted topology priors. Through imitative contingency learning, BeTop also effectively manages behavioral uncertainty for prediction and planning. Extensive verification on large-scale real-world datasets, including nuPlan and WOMD, demonstrates that BeTop achieves state-of-the-art performance in both prediction and planning tasks. Further validations on the proposed interactive scenario benchmark showcase planning compliance in interactive cases. Code and model is available at https://github.com/OpenDriveLab/BeTop.",main,NeurIPS,2024,Poster,Haochen Liu;Li Chen;Yu Qiao;Chen Lv;Hongyang Li,True,https://openreview.net/pdf?id=FSgwgQXTxo
FXTeJvHE0k,NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking,"Benchmarking vision-based driving policies is challenging. On one hand, open-loop evaluation with real data is easy, but these results do not reflect closed-loop performance. On the other, closed-loop evaluation is possible in simulation, but is hard to scale due to its significant computational demands. Further, the simulators available today exhibit a large domain gap to real data. This has resulted in an inability to draw clear conclusions from the rapidly growing body of research on end-to-end autonomous driving. In this paper, we present NAVSIM, a middle ground between these evaluation paradigms, where we use large datasets in combination with a non-reactive simulator to enable large-scale real-world benchmarking. Specifically, we gather simulation-based metrics, such as progress and time to collision, by unrolling bird's eye view abstractions of the test scenes for a short simulation horizon. Our simulation is non-reactive, i.e., the evaluated policy and environment do not influence each other. As we demonstrate empirically, this decoupling allows open-loop metric computation while being better aligned with closed-loop evaluations than traditional displacement errors. NAVSIM enabled a new competition held at CVPR 2024, where 143 teams submitted 463 entries, resulting in several new insights. On a large set of challenging scenarios, we observe that simple methods with moderate compute requirements such as TransFuser can match recent large-scale end-to-end driving architectures such as UniAD. Our modular framework can potentially be extended with new datasets, data curation strategies, and metrics, and will be continually maintained to host future challenges. Our code is available at https://github.com/autonomousvision/navsim.",Datasets & Benchmarks,NeurIPS,2024,Poster,Daniel Dauner;Marcel Hallgarten;Tianyu Li;Xinshuo Weng;Zhiyu Huang;Zetong Yang;Hongyang Li;Igor Gilitschenski;Boris Ivanovic;Marco Pavone;Andreas Geiger;Kashyap Chitta,False,https://openreview.net/pdf?id=FXTeJvHE0k
FZW7Ctyjm3,Enhancing Large Vision Language Models with Self-Training on Image Comprehension,"Large vision language models (LVLMs) integrate large language models (LLMs) with pre-trained vision encoders, thereby activating the perception capability of the model to understand image inputs for different queries and conduct subsequent reasoning. Improving this capability requires high-quality vision-language data, which is costly and labor-intensive to acquire. Self-training approaches have been effective in single-modal settings to alleviate the need for labeled data by leveraging model's own generation. However, effective self-training remains a challenge regarding the unique visual perception and reasoning capability of LVLMs. To address this, we introduce **S**elf-**T**raining on **I**mage **C**omprehension (**STIC**), which emphasizes a self-training approach specifically for image comprehension. First, the model self-constructs a preference dataset for image descriptions using unlabeled images. Preferred responses are generated through a step-by-step prompt, while dis-preferred responses are generated from either corrupted images or misleading prompts. To further self-improve reasoning on the extracted visual information, we let the model reuse a small portion of existing instruction-tuning data and append its self-generated image descriptions to the prompts. We validate the effectiveness of STIC across seven different benchmarks, demonstrating substantial performance gains of 4.0% on average while using 70% less supervised fine-tuning data than the current method. Further studies dive into various components of STIC and highlight its potential to leverage vast quantities of unlabeled images for self-training.",main,NeurIPS,2024,Poster,Yihe Deng;Pan Lu;Fan Yin;Ziniu Hu;Sheng Shen;Quanquan Gu;James Zou;Kai-Wei Chang;Wei Wang,True,https://openreview.net/pdf?id=FZW7Ctyjm3
FaNhyXY6Y1,Artemis: Towards Referential Understanding in Complex Videos,"Videos carry rich visual information including object description, action, interaction, etc., but the existing multimodal large language models (MLLMs) fell short in referential understanding scenarios such as video-based referring. In this paper, we present Artemis, an MLLM that pushes video-based referential understanding to a finer level. Given a video, Artemis receives a natural-language question with a bounding box in any video frame and describes the referred target in the entire video. The key to achieving this goal lies in extracting compact, target-specific video features, where we set a solid baseline by tracking and selecting spatiotemporal features from the video. We train Artemis on the newly established ViderRef45K dataset with 45K video-QA pairs and design a computationally efficient, three-stage training procedure. Results are promising both quantitatively and qualitatively. Additionally, we show that Artemis can be integrated with video grounding and text summarization tools to understand more complex scenarios. Code and data are available at https://github.com/NeurIPS24Artemis/Artemis.",main,NeurIPS,2024,Poster,Jihao Qiu;Yuan Zhang;Xi Tang;Lingxi Xie;Tianren Ma;Pengyu Yan;David Doermann;Qixiang Ye;Yunjie Tian,True,https://openreview.net/pdf?id=FaNhyXY6Y1
FbDgxp7LAa,Streaming Detection of Queried Event Start,"Robotics, autonomous driving, augmented reality, and many embodied computer vision applications must quickly react to user-defined events unfolding in real time. We address this setting by proposing a novel task for multimodal video understanding---Streaming Detection of Queried Event Start (SDQES).
The goal of SDQES is to identify the beginning of a complex event as described by a natural language query, with high accuracy and low latency. 
We introduce a new benchmark based on  the Ego4D dataset, as well as new task-specific metrics to study streaming multimodal detection of diverse events in an egocentric video setting.
Inspired by parameter-efficient fine-tuning methods in NLP and for video tasks, we propose adapter-based baselines that enable image-to-video transfer learning, allowing for efficient online video modeling.
We evaluate three vision-language backbones and three adapter architectures on both short-clip and untrimmed video settings.",Datasets & Benchmarks,NeurIPS,2024,Poster,Cristobal Eyzaguirre;Eric Tang;Shyamal Buch;Adrien Gaidon;Jiajun Wu;Juan Carlos Niebles,False,https://openreview.net/pdf?id=FbDgxp7LAa
FbuODM02ra,Can Language Models Perform Robust Reasoning in Chain-of-thought Prompting with Noisy Rationales?,"This paper investigates an under-explored challenge in large language models (LLMs): chain-of-thought prompting with noisy rationales, which include irrelevant or inaccurate reasoning thoughts within examples used for in-context learning. We construct NoRa dataset that is tailored to evaluate the robustness of reasoning in the presence of noisy rationales. Our findings on NoRa dataset reveal a prevalent vulnerability to such noise among current LLMs, with existing robust methods like self-correction and self-consistency showing limited efficacy. Notably, compared to prompting with clean rationales, base LLM drops by 1.4%-19.8% in accuracy with irrelevant thoughts and more drastically by 2.2%-40.4% with inaccurate thoughts.

Addressing this challenge necessitates external supervision that should be accessible in practice. Here, we propose the method of contrastive denoising with noisy chain-of-thought (CD-CoT). It enhances LLMs' denoising-reasoning capabilities by contrasting noisy rationales with only one clean rationale, which can be the minimal requirement for denoising-purpose prompting. This method follows a principle of exploration and exploitation: (1) rephrasing and selecting rationales in the input space to achieve explicit denoising and (2) exploring diverse reasoning paths and voting on answers in the output space. Empirically, CD-CoT demonstrates an average improvement of 17.8% in accuracy over the base model and shows significantly stronger denoising capabilities than baseline methods. The source code is publicly available at: https://github.com/tmlr-group/NoisyRationales.",main,NeurIPS,2024,Poster,Zhanke Zhou;Rong Tao;Jianing Zhu;Yiwen Luo;Zengmao Wang;Bo Han,True,https://openreview.net/pdf?id=FbuODM02ra
FjeJB0OUhN,"Can Long-Context Language Models Subsume Retrieval, SQL, and More?","Long-context language models (LCLMs) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases. Leveraging LCLMs’ ability to natively ingest and process entire corpora of information offers numerous advantages. It enhances user-friendliness by eliminating the need for specialized knowledge of tools, provides robust end-to-end modeling that minimizes cascading errors in complex pipelines, and allows for the application of sophisticated prompting techniques across the entire system. To assess this paradigm shift, we introduce LOFT, a benchmark comprising of real-world tasks requiring context up to millions of tokens designed to evaluate LCLMs’ performance on in-context retrieval and reasoning. Our findings reveal that LCLMs can already achieve textual and visual retrieval performance comparable to specialized systems such as Gecko and CLIP, while still facing challenges in areas like multi-hop compositional reasoning required in SQL-like tasks. Notably, prompting strategies significantly influence performance, emphasizing the need for continued research as context lengths grow. Overall, LOFT provides a rigorous testing ground for LCLMs, showcasing their potential to supplant existing paradigms and tackle novel tasks as model capabilities scale.",Datasets & Benchmarks,NeurIPS,2024,Reject,Jinhyuk Lee;Anthony Chen;Zhuyun Dai;Dheeru Dua;Devendra Singh Sachan;Michael Boratko;Yi Luan;Séb Arnold;Vincent Perot;Siddharth Dalmia;Hexiang Hu;Xudong Lin;Panupong Pasupat;Aida Amini;Jeremy R. Cole;Sebastian Riedel;Iftekhar Naim;Ming-Wei Chang;Kelvin Guu,True,https://openreview.net/pdf?id=FjeJB0OUhN
FlcdW7NPRY,Approaching Human-Level Forecasting with Language Models,"Forecasting future events is important for policy and decision making. In this work, we study whether language models (LMs) can forecast at the level of competitive human forecasters. Towards this goal, we develop a retrieval-augmented LM system designed to automatically search for relevant information, generate forecasts, and aggregate predictions. To facilitate our study, we collect a large dataset of questions from competitive forecasting platforms. Under a test set published after the knowledge cut-offs of our LMs, we evaluate the end-to-end performance of our system against the aggregates of human forecasts. On average, the system nears the crowd aggregate of competitive forecasters and, in a certain relaxed setting, surpasses it. Our work suggests that using LMs to forecasts the future could provide accurate predictions at scale and help to inform institutional decision making.",main,NeurIPS,2024,Poster,Danny Halawi;Fred Zhang;Chen Yueh-Han;Jacob Steinhardt,True,https://openreview.net/pdf?id=FlcdW7NPRY
GB5a0RRYuv,Construction and Application of Materials Knowledge Graph in Multidisciplinary Materials Science via Large Language Model,"Knowledge in materials science is widely dispersed across extensive scientific literature, posing significant challenges for efficient discovery and integration of new materials. Traditional methods, often reliant on costly and time-consuming experimental approaches, further complicate rapid innovation. Addressing these challenges, the integration of artificial intelligence with materials science has opened avenues for accelerating the discovery process, though it also demands precise annotation, data extraction, and traceability of information. To tackle these issues, this article introduces the Materials Knowledge Graph (MKG), which utilizes advanced natural language processing techniques, integrated with large language models to extract and systematically organize a decade's worth of high-quality research into structured triples, contains 162,605 nodes and 731,772 edges. MKG categorizes information into comprehensive labels such as Name, Formula, and Application, structured around a meticulously designed ontology, thus enhancing data usability and integration. By implementing network-based algorithms, MKG not only facilitates efficient link prediction but also significantly reduces reliance on traditional experimental methods. This structured approach not only streamlines materials research but also lays the groundwork for more sophisticated materials knowledge graphs.",main,NeurIPS,2024,Poster,Yanpeng Ye;Jie Ren;Shaozhou Wang;Yuwei Wan;Imran Razzak;Bram Hoex;Haofen Wang;Tong Xie;Wenjie Zhang,True,https://openreview.net/pdf?id=GB5a0RRYuv
GHlJM45fWY,GeoPlant: Spatial Plant Species Prediction Dataset,"The difficulty of monitoring biodiversity at fine scales and over large areas limits ecological knowledge and conservation efforts. To fill this gap, Species Distribution Models (SDMs) predict species across space from spatially explicit features. Yet, they face the challenge of integrating the rich but heterogeneous data made available over the past decade, notably millions of opportunistic species observations and standardized surveys, as well as multi-modal remote sensing data.
In light of that, we have designed and developed a new European-scale dataset for SDMs at high spatial resolution (10--50m), including more than 10k species (i.e., most of the European flora). The dataset comprises 5M heterogeneous Presence-Only records and 90k exhaustive Presence-Absence survey records, all accompanied by diverse environmental rasters (e.g., elevation, human footprint, and soil) traditionally used in SDMs. In addition, it provides Sentinel-2 RGB and NIR satellite images with 10 m resolution, a 20-year time series of climatic variables, and satellite time series from the Landsat program.
In addition to the data, we provide an openly accessible SDM benchmark (hosted on Kaggle), which has already attracted an active community and a set of strong baselines for single predictor/modality and multimodal approaches.
All resources, e.g., the dataset, pre-trained models, and baseline methods (in the form of notebooks), are available on Kaggle, allowing one to start with our dataset literally with two mouse clicks.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Lukas Picek;Christophe Botella;Maximilien Servajean;César Leblanc;Rémi Palard;Theo Larcher;Benjamin Deneu;Diego Marcos;Pierre Bonnet;Alexis Joly,True,https://openreview.net/pdf?id=GHlJM45fWY
GN2qbxZlni,MR-Ben: A Meta-Reasoning Benchmark for Evaluating System-2 Thinking in LLMs,"Large language models (LLMs) have shown increasing capability in problem-solving and decision-making, largely based on the step-by-step chain-of-thought reasoning processes. However, evaluating these reasoning abilities has become increasingly challenging. Existing outcome-based benchmarks are beginning to saturate, becoming less effective in tracking meaningful progress. To address this, we present a process-based benchmark MR-Ben that demands a meta-reasoning skill, where LMs are asked to locate and analyse potential errors in automatically generated reasoning steps. Our meta-reasoning paradigm is especially suited for system-2 slow thinking, mirroring the human cognitive process of carefully examining assumptions, conditions, calculations, and logic to identify mistakes. MR-Ben comprises 5,975 questions curated by human experts across a wide range of subjects, including physics, chemistry, logic, coding, and more. Through our designed metrics for assessing meta-reasoning on this benchmark, we identify interesting limitations and weaknesses of current LLMs (open-source and closed-source models). For example, with models like the o1 series from OpenAI demonstrating strong performance by effectively scrutinizing the solution space, many other state-of-the-art models fall significantly behind on MR-Ben, exposing potential shortcomings in their training strategies and inference methodologies.",main,NeurIPS,2024,Poster,Zhongshen Zeng;Yinhong Liu;Yingjia Wan;Jingyao Li;Pengguang Chen;Jianbo Dai;Yuxuan Yao;Rongwu Xu;Zehan Qi;Wanru Zhao;Linling Shen;Jianqiao Lu;Haochen Tan;Yukang Chen;Hao Zhang;Zhan Shi;Bailin Wang;Zhijiang Guo;Jiaya Jia,True,https://openreview.net/pdf?id=GN2qbxZlni
GNhwwbZEZ7,IncomeSCM: From tabular data set to time-series simulator and causal estimation benchmark,"Evaluating observational estimators of causal effects demands information that is rarely available: unconfounded interventions and outcomes from the population of interest, created either by randomization or adjustment. As a result, it is customary to fall back on simulators when creating benchmark tasks. Simulators offer great control but are often too simplistic to make challenging tasks, either because they are hand-designed and lack the nuances of real-world data, or because they are fit to observational data without structural constraints. In this work, we propose a general, repeatable strategy for turning observational data into sequential structural causal models and challenging estimation tasks by following two simple principles: 1) fitting real-world data where possible, and 2) creating complexity by composing simple, hand-designed mechanisms. We implement these ideas in a highly configurable software package and apply it to the well-known Adult income data set to construct the IncomeSCM simulator. From this, we devise multiple estimation tasks and sample data sets to compare established estimators of causal effects. The tasks present a suitable challenge, with effect estimates varying greatly in quality between methods, despite similar performance in the modeling of factual outcomes, highlighting the need for dedicated causal estimators and model selection criteria.",Datasets & Benchmarks,NeurIPS,2024,Poster,Fredrik D. Johansson,True,https://openreview.net/pdf?id=GNhwwbZEZ7
GUccmOMBv6,CoLoR-Filter: Conditional Loss Reduction Filtering for Targeted Language Model Pre-training,"Selecting high-quality data for pre-training is crucial in shaping the downstream task performance of language models. A major challenge lies in identifying this optimal subset, a problem generally considered intractable, thus necessitating scalable and effective heuristics. In this work, we propose a data selection method, CoLoR-Filter (Conditional Loss Reduction Filtering), which leverages an empirical Bayes-inspired approach to derive a simple and computationally efficient selection criterion based on the relative loss values of two auxiliary models.

In addition to the modeling rationale, we evaluate CoLoR-Filter empirically on two language modeling tasks: (1) selecting data from C4 for domain adaptation to evaluation on Books and (2) selecting data from C4 for a suite of downstream multiple-choice question answering tasks. We demonstrate favorable scaling both as we subselect more aggressively and using small auxiliary models to select data for large target models. As one headline result, CoLoR-Filter data selected using a pair of 150m parameter auxiliary models can train a 1.2b parameter target model to match a 1.2b parameter model trained on 25b randomly selected tokens with 25x less data for Books and 11x less data for the downstream tasks. 

Code: https://github.com/davidbrandfonbrener/color-filter-olmo

Filtered data: https://huggingface.co/datasets/davidbrandfonbrener/color-filtered-c4",main,NeurIPS,2024,Poster,David Brandfonbrener;Hanlin Zhang;Andreas Kirsch;Jonathan Richard Schwarz;Sham M. Kakade,True,https://openreview.net/pdf?id=GUccmOMBv6
GYd5AfZaor,Sample Selection via Contrastive Fragmentation for Noisy Label Regression,"As with many other problems, real-world regression is plagued by the presence of noisy labels, an inevitable issue that demands our attention. 
Fortunately, much real-world data often exhibits an intrinsic property of continuously ordered correlations between labels and features, where data points with similar labels are also represented with closely related features.
In response, we propose a novel approach named ConFrag, where we collectively model the regression data by transforming them into disjoint yet contrasting fragmentation pairs. 
This enables the training of more distinctive representations, enhancing the ability to select clean samples.
Our ConFrag framework leverages a mixture of neighboring fragments to discern noisy labels through neighborhood agreement among expert feature extractors.
We extensively perform experiments on four newly curated benchmark datasets of diverse domains, including age prediction, price prediction, and music production year estimation.
We also introduce a metric called Error Residual Ratio (ERR) to better account for varying degrees of label noise.
Our approach consistently outperforms fourteen state-of-the-art baselines, being robust against symmetric and random Gaussian label noise.",main,NeurIPS,2024,Poster,Chris Dongjoo Kim;Sangwoo Moon;Jihwan Moon;Dongyeon Woo;Gunhee Kim,True,https://openreview.net/pdf?id=GYd5AfZaor
GYqs5Z4joA,SpGesture: Source-Free Domain-adaptive sEMG-based Gesture Recognition with Jaccard Attentive Spiking Neural Network,"Surface electromyography (sEMG) based gesture recognition offers a natural and intuitive interaction modality for wearable devices. Despite significant advancements in sEMG-based gesture recognition models, existing methods often suffer from high computational latency and increased energy consumption. Additionally, the inherent instability of sEMG signals, combined with their sensitivity to distribution shifts in real-world settings, compromises model robustness. 
To tackle these challenges, we propose a novel SpGesture framework based on Spiking Neural Networks, which possesses several unique merits compared with existing methods: (1) Robustness: By utilizing membrane potential as a memory list, we pioneer the introduction of Source-Free Domain Adaptation into SNN for the first time. This enables SpGesture to mitigate the accuracy degradation caused by distribution shifts. (2) High Accuracy: With a novel Spiking Jaccard Attention, SpGesture enhances the SNNs' ability to represent sEMG features, leading to a notable rise in system accuracy. To validate SpGesture's performance, we collected a new sEMG gesture dataset which has different forearm postures, where SpGesture achieved the highest accuracy among the baselines ($89.26\\\\%$). Moreover, the actual deployment on the CPU demonstrated a latency below 100ms, well within real-time requirements. This impressive performance showcases SpGesture's potential to enhance the applicability of sEMG in real-world scenarios. The code is available at https://github.com/guoweiyu/SpGesture/.",main,NeurIPS,2024,Poster,Weiyu Guo;Ying Sun;Yijie Xu;Ziyue Qiao;Yongkui Yang;Hui Xiong,True,https://openreview.net/pdf?id=GYqs5Z4joA
GtYd9PCaaB,SR-CACO-2: A Dataset for Confocal Fluorescence Microscopy Image Super-Resolution,"Confocal fluorescence microscopy is one of the most accessible and widely used imaging techniques for the study of biological processes at the cellular and subcellular levels. Scanning confocal microscopy allows the capture of high-quality images from thick three-dimensional (3D) samples, yet suffers from well-known limitations such as photobleaching and phototoxicity of specimens caused by intense light exposure, which limits its use in some applications, especially for living cells. Cellular damage can be alleviated by changing imaging parameters to reduce light exposure, often at the expense of image quality.
Machine/deep learning methods for single-image super-resolution (SISR) can be applied to restore image quality by upscaling lower-resolution (LR) images to produce high-resolution images (HR). These SISR methods have been successfully applied to photo-realistic images due partly to the abundance of publicly available datasets. In contrast, the lack of publicly available data partly limits their application and success in scanning confocal microscopy.
In this paper, we introduce a large scanning confocal microscopy dataset named SR-CACO-2 that is comprised of low- and high-resolution image pairs marked for three different fluorescent markers. It allows to evaluate the performance of SISR methods on three different upscaling levels (X2, X4, X8). SR-CACO-2 contains the human epithelial cell line Caco-2 (ATCC HTB-37), and it is composed of 2,200 unique images, captured with four resolutions and three markers, that have been translated in the form of 9,937 
  patches for experiments with SISR methods. Given the new SR-CACO-2 dataset, we also provide benchmarking results for 16 state-of-the-art methods that are representative of the main SISR families. Results show that these methods have limited success in producing high-resolution textures, indicating that SR-CACO-2 represents a challenging problem. The dataset is released under a Creative Commons license (CC BY-NC-SA 4.0), and it can be accessed freely. Our dataset, code and pretrained weights for SISR methods are publicly available: https://github.com/sbelharbi/sr-caco-2.",Datasets & Benchmarks,NeurIPS,2024,Poster,Soufiane Belharbi;Mara KM Whitford;Phuong Hoang;Shakeeb Murtaza;Luke McCaffrey;Eric Granger,True,https://openreview.net/pdf?id=GtYd9PCaaB
H2ATO32ilj,ART: Automatic Red-teaming for Text-to-Image Models to Protect Benign Users,"Large-scale pre-trained generative models are taking the world by storm, due to their abilities in generating creative content. Meanwhile, safeguards for these generative models are developed, to protect users' rights and safety, most of which are designed for large language models. Existing methods primarily focus on jailbreak and adversarial attacks, which mainly evaluate the model's safety under malicious prompts. Recent work found that manually crafted safe prompts can unintentionally trigger unsafe generations. To further systematically evaluate the safety risks of text-to-image models, we propose a novel Automatic Red-Teaming framework, ART. Our method leverages both vision language model and large language model to establish a connection between unsafe generations and their prompts, thereby more efficiently identifying the model's vulnerabilities. With our comprehensive experiments, we reveal the toxicity of the popular open-source text-to-image models. The experiments also validate the effectiveness, adaptability, and great diversity of ART. Additionally, we introduce three large-scale red-teaming datasets for studying the safety risks associated with text-to-image models. Datasets and models can be found in https://github.com/GuanlinLee/ART.",main,NeurIPS,2024,Poster,Guanlin Li;Kangjie Chen;Shudong Zhang;Jie Zhang;Tianwei Zhang,True,https://openreview.net/pdf?id=H2ATO32ilj
H5bUdfM55S,LVD-2M: A Long-take Video Dataset with Temporally Dense Captions,"The efficacy of video generation models heavily depends on the quality of their training datasets. Most previous video generation models are trained on short video clips, while recently there has been increasing interest in training long video generation models directly on longer videos. However, the lack of such high-quality long videos impedes the advancement long video generation. To promote research in long video generation, we desire a new dataset with four key features essential for training long video generation models: (1) long videos covering at least 10 seconds, (2) long-take videos without cuts, (3) large motion and diverse contents, and (4) temporally dense captions. To achieve this, we introduce a new pipeline for filtering high-quality long-take videos and generating temporally dense captions. Specifically, we define a set of metrics to quantitatively assess video quality including scene cuts, dynamic degrees, and semantic-level scores, enabling us to filter high-quality long-take videos from a large amount of source videos. Subsequently, we develop a hierarchical video captioning pipeline to annotate long videos with temporally-dense captions. With this pipeline, we curate the first long-take video dataset, LVD-2M, comprising 2 million long-take videos, each covering more than 10 seconds and annotated with temporally dense captions. We further validate the effectiveness of LVD-2M by fine-tuning video generation models to generate long videos with dynamic motions. We believe it will significantly contribute to future research in long video generation.",Datasets & Benchmarks,NeurIPS,2024,Poster,Tianwei Xiong;Yuqing Wang;Daquan Zhou;Zhijie Lin;Jiashi Feng;Xihui Liu,True,https://openreview.net/pdf?id=H5bUdfM55S
HB5q6pC5eb,PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations,"Expert-designed close-ended benchmarks are indispensable in assessing the knowledge capacity of large language models (LLMs). Despite their widespread use, concerns have mounted regarding their reliability due to limited test scenarios and an unavoidable risk of data contamination. To rectify this, we present PertEval, a toolkit devised for in-depth probing of LLMs' knowledge capacity through **knowledge-invariant perturbations**. These perturbations employ human-like restatement techniques to generate on-the-fly test samples from static benchmarks, meticulously retaining knowledge-critical content while altering irrelevant details. Our toolkit further includes a suite of **response consistency analyses** that compare performance on raw vs. perturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six representative LLMs are re-evaluated using PertEval. Results reveal significantly inflated performance of the LLMs on raw benchmarks, including an absolute 25.8% overestimation for GPT-4. Additionally, through a nuanced response pattern analysis, we discover that PertEval retains LLMs' uncertainty to specious knowledge, and reveals their potential rote memorization to correct options which leads to overestimated performance. We also find that the detailed response consistency analyses by PertEval could illuminate various weaknesses in existing LLMs' knowledge mastery and guide the development of refinement. Our findings provide insights for advancing more robust and genuinely knowledgeable LLMs. Our code is available at https://github.com/aigc-apps/PertEval.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Jiatong Li;Renjun Hu;Kunzhe Huang;Yan Zhuang;Qi Liu;Mengxiao Zhu;Xing Shi;Wei Lin,False,https://openreview.net/pdf?id=HB5q6pC5eb
HB6KaCFiMN,Animate3D: Animating Any 3D Model with Multi-view Video Diffusion,"Recent advances in 4D generation mainly focus on generating 4D content by distilling pre-trained text or single-view image conditioned models. It is inconvenient for them to take advantage of various off-the-shelf 3D assets with multi-view attributes, and their results suffer from spatiotemporal inconsistency owing to the inherent ambiguity in the supervision signals. In this work, we present Animate3D, a novel framework for animating any static 3D model. The core idea is two-fold: 1) We propose a novel multi-view video diffusion model (MV-VDM) conditioned on multi-view renderings of the static 3D object, which is trained on our presented large-scale multi-view video dataset (MV-Video). 2) Based on MV-VDM, we introduce a framework combining reconstruction and 4D Score Distillation Sampling (4D-SDS) to leverage the multi-view video diffusion priors for animating 3D objects. Specifically, for MV-VDM, we design a new spatiotemporal attention module to enhance spatial and temporal consistency by integrating 3D and video diffusion models. Additionally, we leverage the static 3D model’s multi-view renderings as conditions to preserve its identity. For animating 3D models, an effective two-stage pipeline is proposed: we first reconstruct coarse motions directly from generated multi-view videos, followed by the introduced 4D-SDS to model fine-level motions. Benefiting from accurate motion learning, we could achieve straightforward mesh animation. Qualitative and quantitative experiments demonstrate that Animate3D significantly outperforms previous approaches. Data, code, and models are open-released.",main,NeurIPS,2024,Poster,Yanqin Jiang;Chaohui Yu;Chenjie Cao;Fan Wang;Weiming Hu;Jin Gao,True,https://openreview.net/pdf?id=HB6KaCFiMN
HRkwnZewLC,Navigating the Maze of Explainable AI: A Systematic Approach to Evaluating Methods and Metrics,"Explainable AI (XAI) is a rapidly growing domain with a myriad of proposed methods as well as metrics aiming to evaluate their efficacy. However, current studies are often of limited scope, examining only a handful of XAI methods and ignoring underlying design parameters for performance, such as the model architecture or the nature of input data.  Moreover, they often rely on one or a few metrics and neglect thorough validation, increasing the risk of selection bias and ignoring discrepancies among metrics. These shortcomings leave practitioners confused about which method to choose for their problem. In response, we introduce LATEC, a large-scale benchmark that critically evaluates 17 prominent XAI methods using 20 distinct metrics.  We systematically incorporate vital design parameters like varied architectures and diverse input modalities, resulting in 7,560 examined combinations.  Through LATEC, we showcase the high risk of conflicting metrics leading to unreliable rankings and consequently propose a more robust evaluation scheme. Further, we comprehensively evaluate various XAI methods to assist practitioners in selecting appropriate methods aligning with their needs. Curiously, the emerging top-performing method, Expected Gradients, is not examined in any relevant related study. LATEC reinforces its role in future XAI research by publicly releasing all 326k saliency maps and 378k metric scores as a (meta-)evaluation dataset. The benchmark is hosted at: https://github.com/IML-DKFZ/latec.",Datasets & Benchmarks,NeurIPS,2024,Poster,Lukas Klein;Carsten T. Lüth;Udo Schlegel;Till J. Bungert;Mennatallah El-Assady;Paul F Jaeger,True,https://openreview.net/pdf?id=HRkwnZewLC
HV5JhUZGpP,"BeanCounter: A low-toxicity, large-scale, and open dataset of business-oriented text","Many of the recent breakthroughs in language modeling have resulted from scaling effectively the same model architecture to larger datasets. In this vein, recent work has highlighted performance gains from increasing training dataset size and quality, suggesting a need for novel sources of large-scale datasets. In this work, we introduce BeanCounter, a public dataset consisting of more than 159B tokens extracted from businesses' disclosures. We show that this data is indeed novel: less than 0.1% of BeanCounter appears in Common Crawl-based datasets and it is an order of magnitude larger than datasets relying on similar sources. Given the data's provenance, we hypothesize that BeanCounter is comparatively more factual and less toxic than web-based datasets. Exploring this hypothesis, we find that many demographic identities occur with similar prevalence in BeanCounter but with significantly less toxic context relative to other datasets. To demonstrate the utility of BeanCounter, we evaluate and compare two LLMs continually pre-trained on BeanCounter with their base models. We find an 18-33% reduction in toxic generation and improved performance within the finance domain for the continually pretrained models. Collectively, our work suggests that BeanCounter is a novel source of low-toxicity and high-quality domain-specific data with sufficient scale to train multi-billion parameter LLMs.",Datasets & Benchmarks,NeurIPS,2024,Poster,Siyan Wang;Bradford Levy,True,https://openreview.net/pdf?id=HV5JhUZGpP
HYwfZEhyK4,Swarm Intelligence in Geo-Localization: A Multi-Agent Large Vision-Language Model Collaborative Framework,"Visual geo-localization demands in-depth knowledge and advanced reasoning skills to associate images with real-world geographic locations precisely.
In general, traditional methods based on data-matching are hindered by the impracticality of storing adequate visual records of global landmarks.
Recently, Large Vision-Language Models (LVLMs) have demonstrated the capability of geo-localization through Visual Question Answering (VQA), enabling a solution that does not require external geo-tagged image records. However, the performance of a single LVLM is still limited by its intrinsic knowledge and reasoning capabilities.
Along this line, in this paper, we introduce a novel visual geo-localization framework called smileGeo that integrates the inherent knowledge of multiple LVLM agents via inter-agent communication to achieve effective geo-localization of images. 
Furthermore, our framework employs a dynamic learning strategy to optimize the communication patterns among agents, reducing unnecessary discussions among agents and improving the efficiency of the framework.
To validate the effectiveness of the proposed framework, we construct GeoGlobe, a novel dataset for visual geo-localization tasks. Extensive testing on the dataset demonstrates that our approach significantly outperforms state-of-the-art methods.
The source code is available at https://anonymous.4open.science/r/ViusalGeoLocalization-F8F5/ and the dataset will also be released after the paper is accepted.",main,NeurIPS,2024,Reject,Xiao Han;Chen Zhu;Xiangyu Zhao;Hengshu Zhu,True,https://openreview.net/pdf?id=HYwfZEhyK4
HcLFNuQwy5,SciFIBench: Benchmarking Large Multimodal Models for Scientific Figure Interpretation,"Large multimodal models (LMMs) have proven flexible and generalisable across many tasks and fields. Although they have strong potential to aid scientific research, their capabilities in this domain are not well characterised. A key aspect of scientific research is the ability to understand and interpret figures, which serve as a rich, compressed source of complex information. In this work, we present SciFIBench, a scientific figure interpretation benchmark consisting of 2000 questions split between two tasks across 8 categories. The questions are curated from arXiv paper figures and captions, using adversarial filtering to find hard negatives and human verification for
quality control. We evaluate 28 LMMs on SciFIBench, finding it to be a challenging benchmark. Finally, we investigate the alignment and reasoning faithfulness of the LMMs on augmented question sets from our benchmark. We release SciFIBench to encourage progress in this domain.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jonathan Roberts;Kai Han;Neil Houlsby;Samuel Albanie,True,https://openreview.net/pdf?id=HcLFNuQwy5
HdIiSPLgzC,MINT-1T: Scaling Open-Source Multimodal Data by 10x: A Multimodal Dataset with One Trillion Tokens,"Multimodal interleaved datasets featuring free-form interleaved sequences of images and text are crucial for training frontier large multimodal models (LMMs). Despite the rapid progression of open-source LMMs, there remains a pronounced scarcity of large-scale, open-source multimodal interleaved datasets.
In response, we introduce MINT-1T, the most extensive and diverse open-source Multimodal INTerleaved dataset to date. MINT-1T comprises of one trillion text tokens and 3.4 billion images, a 10x scale-up from existing open-source datasets. Additionally, we include previously untapped sources such as PDFs and ArXiv papers. As scaling multimodal interleaved datasets requires substantial engineering effort, sharing the data curation process and releasing the dataset greatly benefits the community. Our experiments show that LMMs trained on MINT-1T rival the performance of models trained on the previous leading dataset, OBELICS. We release our data at https://github.com/mlfoundations/MINT-1T.",Datasets & Benchmarks,NeurIPS,2024,Poster,Anas Awadalla;Le Xue;Oscar Lo;Manli Shu;Hannah Lee;Etash Kumar Guha;Sheng Shen;Mohamed Awadalla;Silvio Savarese;Caiming Xiong;Ran Xu;Yejin Choi;Ludwig Schmidt,True,https://openreview.net/pdf?id=HdIiSPLgzC
HtlfNbyfOn,bit2bit: 1-bit quanta video reconstruction via self-supervised photon prediction,"Quanta image sensors, such as SPAD arrays, are an emerging sensor technology, producing 1-bit arrays representing photon detection events over exposures as short as a few nanoseconds. In practice, raw data are post-processed using heavy spatiotemporal binning to create more useful and interpretable images at the cost of degrading spatiotemporal resolution. In this work, we propose bit2bit, a new method for reconstructing high-quality image stacks at the original spatiotemporal resolution from sparse binary quanta image data. Inspired by recent work on Poisson denoising, we developed an algorithm that creates a dense image sequence from sparse binary photon data by predicting the photon arrival location probability distribution. However, due to the binary nature of the data, we show that the assumption of a Poisson distribution is inadequate. Instead, we model the process with a Bernoulli lattice process from the truncated Poisson. This leads to the proposal of a novel self-supervised solution based on a masked loss function. We evaluate our method using both simulated and real data. On simulated data from a conventional video, we achieve 34.35 mean PSNR with extremely photon-sparse binary input (<0.06 photons per pixel per frame). We also present a novel dataset containing a wide range of real SPAD high-speed videos under various challenging imaging conditions. The scenes cover strong/weak ambient light, strong motion, ultra-fast events, etc., which will be made available to the community, on which we demonstrate the promise of our approach. Both reconstruction quality and throughput substantially surpass the state-of-the-art methods (e.g., Quanta Burst Photography (QBP)). Our approach significantly enhances the visualization and usability of the data, enabling the application of existing analysis techniques.",main,NeurIPS,2024,Poster,Yehe Liu;Alexander Krull;Hector Basevi;Ales Leonardis;Michael W. Jenkins,True,https://openreview.net/pdf?id=HtlfNbyfOn
I0zpivK0A0,Terra: A Multimodal Spatio-Temporal Dataset Spanning the Earth,"Since the inception of our planet, the meteorological environment, as reflected through spatio-temporal data, has always been a fundamental factor influencing human life, socio-economic progress, and ecological conservation. A comprehensive exploration of this data is thus imperative to gain a deeper understanding and more accurate forecasting of these environmental shifts. Despite the success of deep learning techniques within the realm of spatio-temporal data and earth science, existing public datasets are beset with limitations in terms of spatial scale, temporal coverage, and reliance on limited time series data. These constraints hinder their optimal utilization in practical applications. To address these issues, we introduce **Terra**, a multimodal spatio-temporal dataset spanning the earth. This dataset encompasses hourly time series data from 6,480,000 grid areas worldwide over the past 45 years, while also incorporating multimodal spatial supplementary information including geo-images and explanatory text. Through a detailed data analysis and evaluation of existing deep learning models within earth sciences, utilizing our constructed dataset. we aim to provide valuable opportunities for enhancing future research in spatio-temporal data mining, thereby advancing towards more spatio-temporal general intelligence. Our source code and data can be accessed at https://github.com/CityMind-Lab/NeurIPS24-Terra.",Datasets & Benchmarks,NeurIPS,2024,Poster,Wei Chen;Xixuan Hao;wu yuankai;Yuxuan Liang,True,https://openreview.net/pdf?id=I0zpivK0A0
I2Q3XwO2cz,OAM-TCD: A globally diverse dataset of high-resolution tree cover maps,"Accurately quantifying tree cover is an important metric for ecosystem monitoring and for assessing progress in restored sites. Recent works have shown that deep learning-based segmentation algorithms are capable of accurately mapping trees at country and continental scales using high-resolution aerial and satellite imagery. Mapping at high (ideally sub-meter) resolution is necessary to identify individual trees, however there are few open-access datasets containing instance level annotations and those that exist are small or not geographically diverse. We present a novel open-access dataset for individual tree crown delineation (TCD) in high-resolution aerial imagery sourced from OpenAerialMap (OAM). Our dataset, OAM-TCD, comprises 5072 2048x2048 px images at 10 cm/px resolution with associated human-labeled instance masks for over 280k individual and 56k groups of trees. By sampling imagery from around the world, we are able to better capture the diversity and morphology of trees in different terrestrial biomes and in both urban and natural environments. Using our dataset, we train reference instance and semantic segmentation models that compare favorably to existing state-of-the-art models. We assess performance through k-fold cross-validation and comparison with existing datasets; additionally we demonstrate compelling results on independent aerial imagery captured over Switzerland and compare to municipal tree inventories and LIDAR-derived canopy maps in the city of Zurich. Our dataset, models and training/benchmark code are publicly released under permissive open-source licenses: Creative Commons (majority CC BY 4.0), and Apache 2.0 respectively.",Datasets & Benchmarks,NeurIPS,2024,Poster,Joshua Veitch-Michaelis;Andrew Cottam;Daniella Schweizer;Eben Broadbent;David Dao;Ce Zhang;Angelica Almeyda Zambrano;Simeon Max,True,https://openreview.net/pdf?id=I2Q3XwO2cz
I2VOdtAc3H,Off to new Shores: A Dataset & Benchmark for (near-)coastal Flood Inundation Forecasting,"Floods are among the most common and devastating natural hazards, imposing immense costs on our society and economy due to their disastrous consequences. Recent progress in weather prediction and spaceborne flood mapping demonstrated the feasibility of anticipating extreme events and reliably detecting their catastrophic effects afterwards. However, these efforts are rarely linked to one another and there is a critical lack of datasets and benchmarks to enable the direct forecasting of flood extent. To resolve this issue, we curate a novel dataset enabling a timely prediction of flood extent. Furthermore, we provide a representative evaluation of state-of-the-art methods, structured into two benchmark tracks for forecasting flood inundation maps i) in general and ii) focused on coastal regions.   Altogether, our dataset and benchmark provide a comprehensive platform for evaluating flood forecasts, enabling future solutions for this critical challenge. Data, code \\\\& models are shared at https://github.com/Multihuntr/GFF under a CC0 license.",Datasets & Benchmarks,NeurIPS,2024,Poster,Brandon Victor;Mathilde Letard;Peter Jack Naylor;Karim Douch;Nicolas Longépé;Zhen He;Patrick Ebel,True,https://openreview.net/pdf?id=I2VOdtAc3H
I79q7wIRkS,$\\\\texttt{pfl-research}$: simulation framework for accelerating research in Private Federated Learning,"Federated learning (FL) is an emerging machine learning (ML) training paradigm where clients own their data and collaborate to train a global model, without revealing any data to the server and other participants. Researchers commonly perform experiments in a simulation environment to quickly iterate on ideas. However, existing open-source tools do not offer the efficiency required to simulate FL on larger and more realistic FL datasets. We introduce $\\\\texttt{pfl-research}$, a fast, modular, and easy-to-use Python framework for simulating FL. It supports TensorFlow, PyTorch, and non-neural network models, and is tightly integrated with state-of-the-art privacy algorithms. We study the speed of open-source FL frameworks and show that $\\\\texttt{pfl-research}$ is 7-72$\\\\times$ faster than alternative open-source frameworks on common cross-device setups. Such speedup will significantly boost the productivity of the FL research community and enable testing hypotheses on realistic FL datasets that were previously too resource intensive. We release a suite of benchmarks that evaluates an algorithm's overall performance on a diverse set of realistic scenarios.",Datasets & Benchmarks,NeurIPS,2024,Poster,Filip Granqvist;Congzheng Song;Áine Cahill;Rogier van Dalen;Martin Pelikan;YI SHENG CHAN;Xiaojun Feng;Natarajan Krishnaswami;Vojta J;Mona Chitnis,False,https://openreview.net/pdf?id=I79q7wIRkS
IAQNJUJe8q,Full-Atom Peptide Design with Geometric Latent Diffusion,"Peptide design plays a pivotal role in therapeutics, allowing brand new possibility to leverage target binding sites that are previously undruggable. Most existing methods are either inefficient or only concerned with the target-agnostic design of 1D sequences. In this paper, we propose a generative model for full-atom Peptide design with Geometric LAtent Diffusion (PepGLAD) given the binding site. We first establish a benchmark consisting of both 1D sequences and 3D structures from Protein Data Bank (PDB) and literature for systematic evaluation. We then identify two major challenges of leveraging current diffusion-based models for peptide design: the full-atom geometry and the variable binding geometry. To tackle the first challenge, PepGLAD derives a variational autoencoder that first encodes full-atom residues of variable size into fixed-dimensional latent representations, and then decodes back to the residue space after conducting the diffusion process in the latent space. For the second issue, PepGLAD explores a receptor-specific affine transformation to convert the 3D coordinates into a shared standard space, enabling better generalization ability across different binding shapes. Experimental Results show that our method not only improves diversity and binding affinity significantly in the task of sequence-structure co-design, but also excels at recovering reference structures for binding conformation generation.",main,NeurIPS,2024,Poster,Xiangzhe Kong;Yinjun Jia;Wenbing Huang;Yang Liu,True,https://openreview.net/pdf?id=IAQNJUJe8q
IRXyPm9IPW,Multimodal Large Language Models Make Text-to-Image Generative Models Align Better,"Recent studies have demonstrated the exceptional potentials of leveraging human preference datasets to refine text-to-image generative models, enhancing the alignment between generated images and textual prompts. Despite these advances, current human preference datasets are either prohibitively expensive to construct or suffer from a lack of diversity in preference dimensions, resulting in limited applicability for instruction tuning in open-source text-to-image generative models and hinder further exploration. To address these challenges and promote the alignment of generative models through instruction tuning, we leverage multimodal large language models to create VisionPrefer, a high-quality and fine-grained preference dataset that captures multiple preference aspects. We aggregate feedback from AI annotators across four aspects: prompt-following, aesthetic, fidelity, and harmlessness to construct VisionPrefer. To validate the effectiveness of VisionPrefer, we train a reward model VP-Score over VisionPrefer to guide the training of text-to-image generative models and the preference prediction accuracy of VP-Score is comparable to human annotators. Furthermore, we use two reinforcement learning methods to supervised fine-tune generative models to evaluate the performance of VisionPrefer, and extensive experimental results demonstrate that VisionPrefer significantly improves text-image alignment in compositional image generation across diverse aspects, e.g., aesthetic, and generalizes better than previous human-preference metrics across various image distributions. Moreover, VisionPrefer indicates that the integration of AI-generated synthetic data as a supervisory signal is a promising avenue for achieving improved alignment with human preferences in vision generative models.",main,NeurIPS,2024,Poster,Xun Wu;Shaohan Huang;Guolong Wang;Jing Xiong;Furu Wei,True,https://openreview.net/pdf?id=IRXyPm9IPW
IZtX4RNBeH,How Good is my Video LMM? Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs,"Recent advancements in Large Language Models (LLMs) have led to the development of Video Large Multi-modal Models (Video-LMMs) that can handle a wide range of video understanding tasks. These models have the potential to be deployed in real-world applications such as robotics, AI assistants, medical surgery, and autonomous vehicles. The widespread adoption of Video-LMMs in our daily lives underscores the importance of ensuring and evaluating their robust performance in mirroring human-like reasoning and interaction capabilities in complex, real-world contexts. However, existing benchmarks for Video-LMMs primarily focus on general video comprehension abilities and neglect assessing their reasoning capabilities over complex videos in the real-world context, and robustness of these models through the lens of user prompts as text queries. In this paper, we present the Complex Video Reasoning and Robustness Evaluation Suite (CVRR-ES), a novel benchmark that comprehensively assesses the performance of Video-LMMs across 11 diverse real-world video dimensions. We evaluate 11 recent models, including both open-source and closed-source variants, and find that most of the Video-LMMs, especially open-source ones, struggle with robustness and reasoning when dealing with complex videos. Based on our analysis, we develop a training-free Dual-Step Contextual Prompting (DSCP) technique to effectively enhance the performance of existing Video-LMMs on CVRR-ES benchmark. Our findings provide valuable insights for building the next generation of human-centric AI systems with advanced robustness and reasoning capabilities.
Our dataset and code are publicly available at: https://mbzuai-oryx.github.io/CVRR-Evaluation-Suite/.",Datasets & Benchmarks,NeurIPS,2024,Reject,Muhammad Uzair Khattak;Muhammad Ferjad Naeem;Jameel Hassan Abdul Samadh;Muzammal Naseer;Federico Tombari;Fahad Khan;Salman Khan,True,https://openreview.net/pdf?id=IZtX4RNBeH
Ich4tv4202,"WildGuard: Open One-stop Moderation Tools for Safety Risks, Jailbreaks, and Refusals of LLMs","We introduce WildGuard---an open, light-weight moderation tool for LLM safety that achieves three goals: (1) identifying malicious intent in user prompts, (2) detecting safety risks of model responses, and (3) determining model refusal rate. Together, WildGuard serves the increasing needs for automatic safety moderation and evaluation of LLM interactions, providing a one-stop tool with enhanced accuracy and broad coverage across 13 risk categories. While existing open moderation tools such as Llama-Guard2 score reasonably well in classifying straightforward model interactions, they lag far behind a prompted GPT-4, especially in identifying adversarial jailbreaks  and in evaluating models' refusals, a key measure for evaluating safety behaviors in model responses. 

To address these challenges, we construct WildGuardMix, a large-scale and carefully balanced multi-task safety moderation dataset with 92K labeled examples that cover vanilla (direct) prompts and adversarial jailbreaks, paired with various refusal and compliance responses. WildGuardMix is a combination of WildGuardTrain, the training data of WildGuard, and WildGuardTest, a high-quality human-annotated moderation test set with 5K labeled items covering broad risk scenarios.
Through extensive evaluations on WildGuardTest and ten existing public benchmarks, we show that WildGuard establishes state-of-the-art performance in open-source safety moderation across all the three tasks compared to ten strong existing open-source moderation models (e.g., up to 25.3% improvement on refusal detection). Importantly, WildGuard matches and sometimes exceeds GPT-4 performance (e.g., up to 4.8% improvement on prompt harmfulness identification). WildGuard serves as a highly effective safety moderator in an LLM interface, reducing the success rate of jailbreak attacks from 79.8%  to 2.4%. We will make all our data, models and training/evaluation code publicly available under CC BY 4.0 license.",Datasets & Benchmarks,NeurIPS,2024,Poster,Seungju Han;Kavel Rao;Allyson Ettinger;Liwei Jiang;Bill Yuchen Lin;Nathan Lambert;Yejin Choi;Nouha Dziri,True,https://openreview.net/pdf?id=Ich4tv4202
IkA54A6KKe,$\\\\texttt{dattri}$: A Library for Efficient Data Attribution,"Data attribution methods aim to quantify the influence of individual training samples on the prediction of artificial intelligence (AI) models. As training data plays an increasingly crucial role in the modern development of large-scale AI models, data attribution has found broad applications in improving AI performance and safety. However, despite a surge of new data attribution methods being developed recently, there lacks a comprehensive library that facilitates the development, benchmarking, and deployment of different data attribution methods. In this work, we introduce $\\\\texttt{dattri}$, an open-source data attribution library that addresses the above needs. Specifically, $\\\\texttt{dattri}$ highlights three novel design features. Firstly, $\\\\texttt{dattri}$ proposes a unified and easy-to-use API, allowing users to integrate different data attribution methods into their PyTorch-based machine learning pipeline with a few lines of code changed. Secondly, $\\\\texttt{dattri}$ modularizes low-level utility functions that are commonly used in data attribution methods, such as Hessian-vector product, inverse-Hessian-vector product or random projection, making it easier for researchers to develop new data attribution methods. Thirdly, $\\\\texttt{dattri}$ provides a comprehensive benchmark framework with pre-trained models and ground truth annotations for a variety of benchmark settings, including generative AI settings. We have implemented a variety of state-of-the-art efficient data attribution methods that can be applied to large-scale neural network models, and will continuously update the library in the future. Using the developed $\\\\texttt{dattri}$ library, we are able to perform a comprehensive and fair benchmark analysis across a wide range of data attribution methods. The source code of $\\\\texttt{dattri}$ is available at https://github.com/TRAIS-Lab/dattri.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Junwei Deng;Ting Wei Li;Shiyuan Zhang;Shixuan Liu;Yijun Pan;Hao Huang;Xinhe Wang;Pingbang Hu;Xingjian Zhang;Jiaqi Ma,False,https://openreview.net/pdf?id=IkA54A6KKe
IlFk5U9cEg,AMBROSIA: A Benchmark for Parsing Ambiguous Questions into Database Queries,"Practical semantic parsers are expected to understand user utterances and map them to executable programs, even when these are ambiguous. We introduce a new benchmark, AMBROSIA, which we hope will inform and inspire the development of text-to-SQL parsers capable of recognizing and interpreting ambiguous requests. Our dataset contains questions showcasing three different types of ambiguity (scope ambiguity, attachment ambiguity, and vagueness), their interpretations, and corresponding SQL queries. In each case, the ambiguity persists even when the database context is provided. This is achieved through a novel approach that involves controlled generation of databases from scratch. We benchmark various LLMs on AMBROSIA, revealing that even the most advanced models struggle to identify and interpret ambiguity in questions.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Irina Saparina;Mirella Lapata,True,https://openreview.net/pdf?id=IlFk5U9cEg
Iq2IAWozNr,Smoke and Mirrors in Causal Downstream Tasks,"Machine Learning and AI have the potential to transform data-driven scientific discovery, enabling accurate predictions for several scientific phenomena. As many scientific questions are inherently causal, this paper looks at the causal inference task of treatment effect estimation, where the outcome of interest is recorded in high-dimensional observations in a Randomized Controlled Trial (RCT). Despite being the simplest possible causal setting and a perfect fit for deep learning, we theoretically find that many common choices in the literature may lead to biased estimates. To test the practical impact of these considerations, we recorded ISTAnt, the first real-world benchmark for causal inference downstream tasks on high-dimensional observations as an RCT studying how garden ants (Lasius neglectus) respond to microparticles applied onto their colony members by hygienic grooming. Comparing 6 480 models fine-tuned from state-of-the-art visual backbones, we find that the sampling and modeling choices significantly affect the accuracy of the causal estimate, and that classification accuracy is not a proxy thereof. We further validated the analysis, repeating it on a synthetically generated visual data set controlling the causal model. Our results suggest that future benchmarks should carefully consider real downstream scientific questions, especially causal ones. Further, we highlight guidelines for representation learning methods to help answer causal questions in the sciences.",main,NeurIPS,2024,Poster,Riccardo Cadei;Lukas Lindorfer;Sylvia Cremer;Cordelia Schmid;Francesco Locatello,True,https://openreview.net/pdf?id=Iq2IAWozNr
IxEhb4NCvy,SSDM: Scalable Speech Dysfluency Modeling,"Speech dysfluency modeling is the core module for spoken language learning, and speech therapy. However, there are three challenges. First, current state-of-the-art solutions~~\\\\cite{lian2023unconstrained-udm, lian-anumanchipalli-2024-towards-hudm} suffer from poor scalability. Second, there is a lack of a large-scale dysfluency corpus. Third, there is not an effective learning framework. In this paper, we propose \\\\textit{SSDM: Scalable Speech Dysfluency Modeling}, which (1) adopts articulatory gestures as scalable forced alignment; (2) introduces connectionist subsequence aligner (CSA) to achieve dysfluency alignment; (3) introduces a large-scale simulated dysfluency corpus called Libri-Dys; and (4) develops an end-to-end system by leveraging the power of large language models (LLMs). We expect SSDM to serve as a standard in the area of dysfluency modeling. Demo is available at \\\\url{https://berkeley-speech-group.github.io/SSDM/}.",main,NeurIPS,2024,Poster,Jiachen Lian;Xuanru Zhou;Zoe Ezzes;Jet M.J. Vonk;Brittany T. Morin;David Paul Galang Baquirin;Zachary A. Miller;Maria Luisa Gorno-Tempini;Gopala Anumanchipalli,True,https://openreview.net/pdf?id=IxEhb4NCvy
IxRf7Q3s5e,NeuralSolver: Learning Algorithms For Consistent and Efficient Extrapolation Across General Tasks,"We contribute NeuralSolver, a novel recurrent solver that can efficiently and consistently extrapolate, i.e., learn algorithms from smaller problems (in terms of observation size) and execute those algorithms in large problems. Contrary to previous recurrent solvers, NeuralSolver can be naturally applied in both same-size problems, where the input and output sizes are the same, and in different-size problems, where the size of the input and output differ. To allow for this versatility, we design NeuralSolver with three main components: a recurrent module, that iteratively processes input information at different scales, a processing module, responsible for aggregating the previously processed information, and a curriculum-based training scheme, that improves the extrapolation performance of the method.
To evaluate our method we introduce a set of novel different-size tasks and we show that NeuralSolver consistently outperforms the prior state-of-the-art recurrent solvers in extrapolating to larger problems, considering smaller training problems and requiring less parameters than other approaches.",main,NeurIPS,2024,Poster,Bernardo Esteves;Miguel Vasco;Francisco S. Melo,True,https://openreview.net/pdf?id=IxRf7Q3s5e
J9oefdGUuM,RAGChecker: A Fine-grained Framework for Diagnosing Retrieval-Augmented Generation,"Despite Retrieval-Augmented Generation (RAG) has shown promising capability in leveraging external knowledge, a comprehensive evaluation of RAG systems is still challenging due to the modular nature of RAG, evaluation of long-form responses and reliability of measurements. In this paper, we propose a fine-grained evaluation framework, RAGChecker, that incorporates a suite of diagnostic metrics for both the retrieval and generation modules. Meta evaluation verifies that RAGChecker has significantly better correlations with human judgments than other evaluation metrics. Using RAGChecker, we evaluate 8 RAG systems and conduct an in-depth analysis of their performance, revealing insightful patterns and trade-offs in the design choices of RAG architectures. The metrics of RAGChecker can guide researchers and practitioners in developing more effective RAG systems.",Datasets & Benchmarks,NeurIPS,2024,Poster,Dongyu Ru;Lin Qiu;Xiangkun Hu;Tianhang Zhang;Peng Shi;Shuaichen Chang;Cheng Jiayang;Cunxiang Wang;Shichao Sun;Huanyu Li;Zizhao Zhang;Binjie Wang;Jiarong Jiang;Tong He;Zhiguo Wang;Pengfei Liu;Yue Zhang;Zheng Zhang,False,https://openreview.net/pdf?id=J9oefdGUuM
JC1VKK3UXk,Poseidon: Efficient Foundation Models for PDEs,"We introduce Poseidon, a foundation model for learning the solution operators of PDEs. It is based on a multiscale operator transformer, with time-conditioned layer norms that enable continuous-in-time evaluations. A novel training strategy leveraging the semi-group property of time-dependent PDEs to allow for significant scaling-up of the training data is also proposed. Poseidon is pretrained on a diverse, large scale dataset for the governing equations of fluid dynamics. It is then evaluated on a suite of 15 challenging downstream tasks that include a wide variety of PDE types and operators. We show that Poseidon exhibits excellent performance across the board by outperforming baselines significantly, both in terms of sample efficiency and accuracy. Poseidon also generalizes very well to new physics that is not seen during pretraining. Moreover, Poseidon scales with respect to model and data size, both for pretraining and for downstream tasks. Taken together, our results showcase the surprising ability of Poseidon to learn effective representations from a very small set of PDEs during pretraining in order to generalize well to unseen and unrelated PDEs downstream, demonstrating its potential as an effective, general purpose PDE foundation model. Finally, the Poseidon model as well as underlying pretraining and downstream datasets are open sourced, with code being available at https://github.com/camlab-ethz/poseidon and pretrained models and datasets at https://huggingface.co/camlab-ethz.",main,NeurIPS,2024,Poster,Maximilian Herde;Bogdan Raonic;Tobias Rohner;Roger Käppeli;Roberto Molinaro;Emmanuel de Bezenac;Siddhartha Mishra,True,https://openreview.net/pdf?id=JC1VKK3UXk
JCyBN5syv3,SimGen: Simulator-conditioned Driving Scene Generation,"Controllable synthetic data generation can substantially lower the annotation cost of training data. Prior works use diffusion models to generate driving images conditioned on the 3D object layout. However, those models are trained on small-scale datasets like nuScenes, which lack appearance and layout diversity. Moreover, overfitting often happens, where the trained models can only generate images based on the layout data from the validation set of the same dataset. In this work, we introduce a simulator-conditioned scene generation framework called SimGen that can learn to generate diverse driving scenes by mixing data from the simulator and the real world. It uses a novel cascade diffusion pipeline to address challenging sim-to-real gaps and multi-condition conflicts. A driving video dataset DIVA is collected to enhance the generative diversity of SimGen, which contains over 147.5 hours of real-world driving videos from 73 locations worldwide and simulated driving data from the MetaDrive simulator. SimGen achieves superior generation quality and diversity while preserving controllability based on the text prompt and the layout pulled from a simulator. We further demonstrate the improvements brought by SimGen for synthetic data augmentation on the BEV detection and segmentation task and showcase its capability in safety-critical data generation.",main,NeurIPS,2024,Poster,Yunsong Zhou;Michael Simon;Zhenghao Peng;Sicheng Mo;Hongzi Zhu;Minyi Guo;Bolei Zhou,True,https://openreview.net/pdf?id=JCyBN5syv3
JEflV4nRlH,What Makes and Breaks Safety Fine-tuning? A Mechanistic Study,"Safety fine-tuning helps align Large Language Models (LLMs) with human preferences for their safe deployment. To better understand the underlying factors that make models safe via safety fine-tuning, we design a synthetic data generation framework that captures salient aspects of an unsafe input by modeling the interaction between the task the model is asked to perform (e.g., “design”) versus the specific concepts the task is asked to be performed upon (e.g., a “cycle” vs. a “bomb”). Using this, we investigate three well-known safety fine-tuning methods—supervised safety fine-tuning, direct preference optimization, and unlearning—and provide significant evidence demonstrating that these methods minimally transform MLP weights to specifically align unsafe inputs into its weights’ null space. This yields a clustering of inputs based on whether the model deems them safe or not. Correspondingly, when an adversarial input (e.g., a jailbreak) is provided, its activations are closer to safer samples, leading to the model processing such an input as if it were safe. Code is available at https://github.com/fiveai/understanding_safety_finetuning.",main,NeurIPS,2024,Poster,Samyak Jain;Ekdeep Singh Lubana;Kemal Oksuz;Tom Joy;Philip Torr;Amartya Sanyal;Puneet K. Dokania,True,https://openreview.net/pdf?id=JEflV4nRlH
JKEIYQUSUc,SpatialRGPT: Grounded Spatial Reasoning in Vision-Language Models,"Vision Language Models (VLMs) have demonstrated remarkable performance in 2D vision and language tasks. However, their ability to reason about spatial arrangements remains limited. In this work, we introduce Spatial Region GPT (SpatialRGPT) to enhance VLMs’ spatial perception and reasoning capabilities. SpatialRGPT advances VLMs’ spatial understanding through two key innovations: (i) a data curation pipeline that enables effective learning of regional representation from 3D scene graphs, and (ii) a flexible ``plugin'' module for integrating depth information into the visual encoder of existing VLMs. During inference, when provided with user-specified region proposals, SpatialRGPT can accurately perceive their relative directions and distances. Additionally, we propose SpatialRGBT-Bench, a benchmark with ground-truth 3D annotations encompassing indoor, outdoor, and simulated environments, for evaluating 3D spatial cognition in Vision-Language Models (VLMs). Our results demonstrate that SpatialRGPT significantly enhances performance in spatial reasoning tasks, both with and without local region prompts. The model also exhibits strong generalization capabilities, effectively reasoning about complex spatial relations and functioning as a region-aware dense reward annotator for robotic tasks. Code, dataset, and benchmark are released at https://www.anjiecheng.me/SpatialRGPT.",main,NeurIPS,2024,Poster,An-Chieh Cheng;Hongxu Yin;Yang Fu;Qiushan Guo;Ruihan Yang;Jan Kautz;Xiaolong Wang;Sifei Liu,True,https://openreview.net/pdf?id=JKEIYQUSUc
JLvtwGlezU,A Practitioner's Guide to Real-World Continual Multimodal Pretraining,"Multimodal foundation models serve numerous applications at the intersection of vision and language. Still, despite being pretrained on extensive data, they become outdated over time.
To keep models updated, research into continual pretraining mainly explores scenarios with either (1) infrequent, indiscriminate updates on large-scale new data, or (2) frequent, sample-level updates.
However, practical model deployment often operates in the gap between these two limit cases, as real-world applications demand adaptation to specific subdomains, tasks or concepts --- spread over the entire, varying life cycle of a model. 
In this work, we complement current perspectives on continual pretraining through a research test bed and offer comprehensive guidance for effective continual model updates in such scenarios.
We first introduce FoMo-in-Flux, a continual multimodal pretraining benchmark with realistic compute constraints and practical deployment requirements, constructed over 63 datasets with diverse visual and semantic coverage.
Using FoMo-in-Flux, we explore the complex landscape of practical continual pretraining through multiple perspectives: (1) data mixtures and stream orderings that emulate real-world deployment settings, (2) methods ranging from simple fine-tuning and traditional continual learning strategies to parameter-efficient updates and model merging, (3) meta-learning-rate schedules and mechanistic design choices, and (4) model and compute scaling. Together, our insights provide a practitioner's guide to continual multimodal pretraining for real-world deployment. Benchmark and code is provided here: https://github.com/ExplainableML/fomo_in_flux.",Datasets & Benchmarks,NeurIPS,2024,Poster,Vishaal Udandarao;Karsten Roth;Sebastian Dziadzio;Ameya Prabhu;Mehdi Cherti;Oriol Vinyals;Olivier J Henaff;Samuel Albanie;Zeynep Akata;Matthias Bethge,True,https://openreview.net/pdf?id=JLvtwGlezU
JRMSC08gSF,Hints-In-Browser: Benchmarking Language Models for Programming Feedback Generation,"Generative AI and large language models hold great promise in enhancing programming education by generating individualized feedback and hints for learners. Recent works have primarily focused on improving the quality of generated feedback to achieve human tutors' quality. While quality is an important performance criterion, it is not the only criterion to optimize for real-world educational deployments. In this paper, we benchmark language models for programming feedback generation across several performance criteria, including quality, cost, time, and data privacy. The key idea is to leverage recent advances in the new paradigm of in-browser inference that allow running these models directly in the browser, thereby providing direct benefits across cost and data privacy. To boost the feedback quality of small models compatible with in-browser inference engines, we develop a fine-tuning pipeline based on GPT-4 generated synthetic data. We showcase the efficacy of fine-tuned Llama3-8B and Phi3-3.8B 4-bit quantized models using WebLLM's in-browser inference engine on three different Python programming datasets. We will release the full implementation along with a web app and datasets to facilitate further research on in-browser language models.",Datasets & Benchmarks,NeurIPS,2024,Poster,Nachiket Kotalwar;Alkis Gotovos;Adish Singla,True,https://openreview.net/pdf?id=JRMSC08gSF
JU0QvhhfVp,MOTIVE: A Drug-Target Interaction Graph For Inductive Link Prediction,"Drug-target interaction (DTI) prediction is crucial for identifying new therapeutics and detecting mechanisms of action. While structure-based methods accurately model physical interactions between a drug and its protein target, cell-based assays such as Cell Painting can better capture complex DTI interactions. This paper introduces MOTIVE, a Morphological cOmpound Target Interaction Graph dataset comprising Cell Painting features for 11,000 genes and 3,600 compounds, along with their relationships extracted from seven publicly available databases. We provide random, cold-source (new drugs), and cold-target (new genes) data splits to enable rigorous evaluation under realistic use cases. Our benchmark results show that graph neural networks that use Cell Painting features consistently outperform those that learn from graph structure alone, feature-based models, and topological heuristics. MOTIVE accelerates both graph ML research and drug discovery by promoting the development of more reliable DTI prediction models. MOTIVE resources are available at https://github.com/carpenter-singh-lab/motive.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,John Arevalo;Ellen Su;Anne E Carpenter;Shantanu Singh,True,https://openreview.net/pdf?id=JU0QvhhfVp
Jaye8aWpmZ,When LLMs Meet Cunning Texts: A Fallacy Understanding Benchmark for Large Language Models,"Recently, Large Language Models (LLMs) make remarkable evolutions in language understanding and generation. Following this, various benchmarks for measuring all kinds of capabilities of LLMs have sprung up. In this paper, we challenge the reasoning and understanding abilities of LLMs by proposing a FaLlacy Understanding Benchmark (FLUB) containing cunning texts that are easy for humans to understand but difficult for models to grasp. Specifically, the cunning texts that FLUB focuses on mainly consist of the tricky, humorous, and misleading texts collected from the real internet environment. And we design three tasks with increasing difficulty in the FLUB benchmark to evaluate the fallacy understanding ability of LLMs. Based on FLUB, we investigate the performance of multiple representative and advanced LLMs, reflecting our FLUB is challenging and worthy of more future study. Interesting discoveries and valuable insights are achieved in our extensive experiments and detailed analyses. We hope that our benchmark can encourage the community to improve LLMs' ability to understand fallacies. Our data and codes are available at https://github.com/THUKElab/FLUB.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yinghui Li;Qingyu Zhou;Yuanzhen Luo;Shirong Ma;Yangning Li;Hai-Tao Zheng;Xuming Hu;Philip S. Yu,True,https://openreview.net/pdf?id=Jaye8aWpmZ
Jfg3vw2bjx,APIGen: Automated PIpeline for Generating Verifiable and Diverse Function-Calling Datasets,"The advancement of function-calling agent models requires diverse, reliable, and high-quality datasets. This paper presents APIGen, an automated data generation pipeline designed to synthesize high-quality datasets for function-calling applications. We leverage APIGen and collect 3,673 executable APIs across 21 different categories to generate diverse function-calling datasets in a scalable and structured manner. Each data in our dataset is verified through three hierarchical stages: format checking, actual function executions, and semantic verification, improving its reliability and correctness. We demonstrate that models trained with our curated datasets, even with only 7B parameters, can achieve state-of-the-art performance on the Berkeley Function-Calling Benchmark, outperforming multiple GPT-4 models. Moreover, our 1B model achieves exceptional performance, surpassing GPT-3.5-Turbo and Claude-3 Haiku. We release a dataset containing 60,000 high-quality entries, aiming to advance the field of function-calling agent domains. The dataset and models are available on the project homepage \\\\url{https://apigen-pipeline.github.io/}.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zuxin Liu;Thai Quoc Hoang;Jianguo Zhang;Ming Zhu;Tian Lan;Shirley Kokane;Juntao Tan;Weiran Yao;Zhiwei Liu;Yihao Feng;Rithesh R N;Liangwei Yang;Silvio Savarese;Juan Carlos Niebles;Huan Wang;Shelby Heinecke;Caiming Xiong,True,https://openreview.net/pdf?id=Jfg3vw2bjx
Jkt42QYyEH,LiveScene: Language Embedding Interactive Radiance Fields for Physical Scene Control and Rendering,"This paper scales object-level reconstruction to complex scenes, advancing interactive scene reconstruction. We introduce two datasets, OmniSim and InterReal, featuring 28 scenes with multiple interactive objects. To tackle the challenge of inaccurate interactive motion recovery in complex scenes, we propose LiveScene, a scene-level language-embedded interactive radiance field that efficiently reconstructs and controls multiple objects. By decomposing the interactive scene into local deformable fields, LiveScene enables separate reconstruction of individual object motions, reducing memory consumption. Additionally, our interaction-aware language embedding localizes individual interactive objects, allowing for arbitrary control using natural language. Our approach demonstrates significant superiority in novel view synthesis, interactive scene control, and language grounding performance through extensive experiments. Project page: https://livescenes.github.io.",main,NeurIPS,2024,Poster,Delin Qu;Qizhi Chen;Pingrui Zhang;Xianqiang Gao;Bin Zhao;Zhigang Wang;Dong Wang;Xuelong Li,True,https://openreview.net/pdf?id=Jkt42QYyEH
JrJW21IP9p,Enhancing vision-language models for medical imaging: bridging the 3D gap with innovative slice selection,"Recent approaches to vision-language tasks are built on the remarkable capabilities of large vision-language models (VLMs). These models excel in zero-shot and few-shot learning, enabling them to learn new tasks without parameter updates. However, their primary challenge lies in their design, which primarily accommodates 2D input, thus limiting their effectiveness for medical images, particularly radiological images like MRI and CT, which are typically 3D. To bridge the gap between state-of-the-art 2D VLMs and 3D medical image data, we developed an innovative, one-pass, unsupervised representative slice selection method called Vote-MI, which selects representative 2D slices from 3D medical imaging. To evaluate the effectiveness of vote-MI when implemented with VLMs, we introduce BrainMD, a robust, multimodal dataset comprising 2,453 annotated 3D MRI brain scans with corresponding textual radiology reports and electronic health records. Based on BrainMD, we further develop two benchmarks, BrainMD-select (including the most representative 2D slice of 3D image) and BrainBench (including various vision-language downstream tasks). Extensive experiments on the BrainMD dataset and its two corresponding benchmarks demonstrate that our representative selection method significantly improves performance in zero-shot and few-shot learning tasks. On average, Vote-MI achieves a 14.6\\\\% and 16.6\\\\% absolute gain for zero-shot and few-shot learning, respectively, compared to randomly selecting examples. Our studies represent a significant step toward integrating AI in medical imaging to enhance patient care and facilitate medical research. We hope this work will serve as a foundation for data selection as vision-language models are increasingly applied to new tasks.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yuli Wang;Peng jian;Yuwei Dai;Craig Jones;Haris I. Sair;Jinglai Shen;Nicolas Loizou;jing wu;Wen-Chi Hsu;Maliha Rubaiyat Imami;Zhicheng Jiao;Paul J Zhang;Harrison Bai,True,https://openreview.net/pdf?id=JrJW21IP9p
JvlrUFJMbI,Semantic Routing via Autoregressive Modeling,"We study learning-based approaches to semantic route planning, which concerns producing routes in response to rich queries that specify various criteria and preferences. Semantic routing is already widely found in industry applications, especially navigational services like Google Maps; however, existing implementations only support limited route criteria and narrow query sets as they rely on repurposing classical route optimization algorithms. We argue for a learning-based approach to semantic routing as a more scalable and general alternative. To foster interest in this important application of graph learning, we are releasing a large-scale publicly-licensed benchmark for semantic routing consisting of real-world multi-objective navigation problems---expressed via natural language queries---on the richly annotated road networks of US cities. In addition to being intractable with existing approaches to semantic routing, our benchmark poses a significant scaling challenge for graph learning methods. As a proof-of-concept, we show that---at scale---even a standard transformer network is a powerful semantic routing system and achieves non-trivial performance on our benchmark. In the process, we demonstrate a simple solution to the challenge of scaling up graph learning: an autoregressive approach that decomposes semantic routing into smaller ``next-edge'' prediction problems.",main,NeurIPS,2024,Poster,Eric Zhao;Pranjal Awasthi;Zhengdao Chen;Sreenivas Gollapudi;Daniel Delling,True,https://openreview.net/pdf?id=JvlrUFJMbI
K6b8LCXBeQ,GMAI-MMBench: A Comprehensive Multimodal Evaluation Benchmark Towards General Medical AI,"Large Vision-Language Models (LVLMs) are capable of handling diverse data types such as imaging, text, and physiological signals, and can be applied in various fields. In the medical field, LVLMs have a high potential to offer substantial assistance for diagnosis and treatment. Before that, it is crucial to develop benchmarks to evaluate LVLMs' effectiveness in various medical applications. Current benchmarks are often built upon specific academic literature, mainly focusing on a single domain, and lacking varying perceptual granularities. Thus, they face specific challenges, including limited clinical relevance, incomplete evaluations, and insufficient guidance for interactive LVLMs. To address these limitations, we developed the GMAI-MMBench, the most comprehensive general medical AI benchmark with well-categorized data structure and multi-perceptual granularity to date. It is constructed from 284 datasets across 38 medical image modalities, 18 clinical-related tasks, 18 departments, and 4 perceptual granularities in a Visual Question Answering (VQA) format. Additionally, we implemented a lexical tree structure that allows users to customize evaluation tasks, accommodating various assessment needs and substantially supporting medical AI research and applications. We evaluated 50 LVLMs, and the results show that even the advanced GPT-4o only achieves an accuracy of 53.96\\\\%, indicating significant room for improvement. Moreover, we identified five key insufficiencies in current cutting-edge LVLMs that need to be addressed to advance the development of better medical applications. We believe that GMAI-MMBench will stimulate the community to build the next generation of LVLMs toward GMAI.",Datasets & Benchmarks,NeurIPS,2024,Poster,pengcheng chen;Jin Ye;Guoan Wang;Yanjun Li;Zhongying Deng;Wei Li;Tianbin Li;Haodong Duan;Ziyan Huang;Yanzhou Su;Benyou Wang;Shaoting Zhang;Bin Fu;Jianfei Cai;Bohan Zhuang;Eric J Seibel;Junjun He;Yu Qiao,True,https://openreview.net/pdf?id=K6b8LCXBeQ
KUw2V04z5M,OCTrack: Benchmarking the Open-Corpus Multi-Object Tracking,"We study a novel yet practical problem of open-corpus multi-object tracking (OCMOT), which extends the MOT into localizing, associating, and recognizing generic-category objects of both seen (base) and unseen (novel) classes, but without the category text list as prompt.
To study this problem, the top priority is to build a benchmark. In this work, we build OCTrackB, a large-scale and comprehensive benchmark, to provide a standard evaluation platform for the OCMOT problem. Compared to previous datasets, OCTrackB has more abundant and balanced base/novel classes and the corresponding samples for evaluation with less bias. We also propose a new multi-granularity recognition metric to better evaluate the generative object recognition in OCMOT. By conducting the extensive benchmark evaluation, we report and analyze the results of various state-of-the-art methods, which demonstrate the rationale of OCMOT, as well as the usefulness and advantages of OCTrackB.",Datasets & Benchmarks,NeurIPS,2024,Reject,Zekun Qian;Ruize Han;Wei Feng;Junhui Hou;Linqi Song;Song Wang,True,https://openreview.net/pdf?id=KUw2V04z5M
KYxzmRLF6i,SpreadsheetBench: Towards Challenging Real World Spreadsheet Manipulation,"We introduce SpreadsheetBench, a challenging spreadsheet manipulation benchmark exclusively derived from real-world scenarios, designed to immerse current large language models (LLMs) in the actual workflow of spreadsheet users. 
Unlike existing benchmarks that rely on synthesized queries and simplified spreadsheet files, SpreadsheetBench is built from 912 real questions gathered from online Excel forums, which reflect the intricate needs of users. The associated spreadsheets from the forums contain a variety of tabular data such as multiple tables, non-standard relational tables, and abundant non-textual elements. Furthermore, we propose a more reliable evaluation metric akin to online judge platforms, where multiple spreadsheet files are created as test cases for each instruction, ensuring the evaluation of robust solutions capable of handling spreadsheets with varying values.
Our comprehensive evaluation of various LLMs under both single-round and multi-round inference settings reveals a substantial gap between the state-of-the-art (SOTA) models and human performance, highlighting the benchmark's difficulty.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Zeyao Ma;Bohan Zhang;Jing Zhang;Jifan Yu;Xiaokang Zhang;Xiaohan Zhang;Sijia Luo;Xi Wang;Jie Tang,True,https://openreview.net/pdf?id=KYxzmRLF6i
KZLE5BaaOH,A StrongREJECT for Empty Jailbreaks,"Most jailbreak papers claim the jailbreaks they propose are highly effective, often boasting near-100% attack success rates. However, it is perhaps more common than not for jailbreak developers to substantially exaggerate the effectiveness of their jailbreaks. We suggest this problem arises because jailbreak researchers lack a standard, high-quality benchmark for evaluating jailbreak performance, leaving researchers to create their own. To create a benchmark, researchers must choose a dataset of forbidden prompts to which a victim model will respond, along with an evaluation method that scores the harmfulness of the victim model’s responses. We show that existing benchmarks suffer from significant shortcomings and introduce the StrongREJECT benchmark to address these issues. StrongREJECT's dataset contains prompts that victim models must answer with specific, harmful information, while its automated evaluator measures the extent to which a response gives useful information to forbidden prompts. In doing so, the StrongREJECT evaluator achieves state-of-the-art agreement with human judgments of jailbreak effectiveness. Notably, we find that existing evaluation methods significantly overstate jailbreak effectiveness compared to human judgments and the StrongREJECT evaluator. We describe a surprising and novel phenomenon that explains this discrepancy: jailbreaks bypassing a victim model’s safety fine-tuning tend to reduce its capabilities. Together, our findings underscore the need for researchers to use a high-quality benchmark, such as StrongREJECT, when developing new jailbreak attacks. We release the StrongREJECT code and data at https://strong-reject.readthedocs.io/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Alexandra Souly;Qingyuan Lu;Dillon Bowen;Tu Trinh;Elvis Hsieh;Sana Pandey;Pieter Abbeel;Justin Svegliato;Scott Emmons;Olivia Watkins;Sam Toyer,True,https://openreview.net/pdf?id=KZLE5BaaOH
KZlJF8kguO,Brain Treebank: Large-scale intracranial recordings from naturalistic language stimuli,"We present the Brain Treebank, a large-scale dataset of electrophysiological neural responses, recorded from intracranial probes while 10 subjects watched one or more Hollywood movies. Subjects watched on average 2.6 Hollywood movies, for an average viewing time of 4.3 hours, and a total of 43 hours. The audio track for each movie was transcribed with manual corrections. Word onsets were manually annotated on spectrograms of the audio track for each movie. Each transcript was automatically parsed and manually corrected into the universal dependencies (UD) formalism, assigning a part of speech to every word and a dependency parse to every sentence. In total, subjects heard over 38,000 sentences (223,000 words), while they had on average 168 electrodes implanted. This is the largest dataset of intracranial recordings featuring grounded naturalistic language, one of the largest English UD treebanks in general, and one of only a few UD treebanks aligned to multimodal features. We hope that this dataset serves as a bridge between linguistic concepts, perception, and their neural representations. To that end, we present an analysis of which electrodes are sensitive to language features while also mapping out a rough time course of language processing across these electrodes. The Brain Treebank is available at https://BrainTreebank.dev/",Datasets & Benchmarks,NeurIPS,2024,Oral,Christopher Wang;Adam Uri Yaari;Aaditya K Singh;Vighnesh Subramaniam;Dana Rosenfarb;Jan DeWitt;Pranav Misra;Joseph R. Madsen;Scellig Stone;Gabriel Kreiman;Boris Katz;Ignacio Cases;Andrei Barbu,True,https://openreview.net/pdf?id=KZlJF8kguO
Kg0hQIG9Uh,LLAVIDAL: Benchmarking \\\\underline{L}arge \\\\underline{LA}nguage \\\\underline{VI}sion Models for \\\\underline{D}aily \\\\underline{A}ctivities of \\\\underline{L}iving,"Large Language Vision Models (LLVMs) have demonstrated effectiveness in processing internet videos, yet they struggle with the visually perplexing dynamics present in Activities of Daily Living (ADL) due to limited pertinent datasets and models tailored to relevant cues. 
To this end, we propose a framework for curating ADL multiview datasets to fine-tune LLVMs, resulting in the creation of \\\\textbf{\\\\datasetname}, comprising 100K RGB video-instruction pairs, language descriptions, 3D skeletons, and action-conditioned object trajectories. We introduce \\\\textbf{\\\\modelname}, an LLVM capable of incorporating 3D poses and relevant object trajectories to understand the intricate spatiotemporal relationships within ADLs. Furthermore, we present a novel benchmark, \\\\textbf{ADLMCQ}, for quantifying LLVM effectiveness in ADL scenarios. When trained on \\\\datasetname, \\\\modelname~consistently achieves state-of-the-art performance across all ADL evaluation metrics. Qualitative analysis reveals \\\\modelname's temporal reasoning capabilities in understanding ADL. The link to the dataset is provided at: \\\\href{https://adl-x.github.io/}{https://adl-x.github.io/}",Datasets & Benchmarks,NeurIPS,2024,Reject,Rajatsubhra Chakraborty;Arkaprava Sinha;Dominick Reilly;Manish Kumar Govind;Pu Wang;Francois Bremond;Srijan Das,True,https://openreview.net/pdf?id=Kg0hQIG9Uh
KgeQqLI7OD,Towards Visual Text Design Transfer Across Languages,"Visual text design plays a critical role in conveying themes, emotions, and atmospheres in multimodal formats such as film posters and album covers. Translating these visual and textual elements across languages extends the concept of translation beyond mere text, requiring the adaptation of aesthetic and stylistic features. To address this, we introduce a novel task of Multimodal Style Translation (MuST-Bench), a benchmark designed to evaluate the ability of visual text generation models to perform translation across different writing systems while preserving design intent.
Our initial experiments on MuST-Bench reveal that existing visual text generation models struggle with the proposed task due to the inadequacy of textual descriptions in conveying visual design.
In response, we introduce SIGIL, a framework for multimodal style translation that eliminates the need for style descriptions.
SIGIL enhances image generation models through three innovations: glyph latent for multilingual settings, pre-trained VAEs for stable style guidance, and an OCR model with reinforcement learning feedback for optimizing readable character generation. SIGIL outperforms existing baselines by achieving superior style consistency and legibility while maintaining visual fidelity, setting itself apart from traditional description-based approaches. We release MuST-Bench publicly for broader use and exploration https://huggingface.co/datasets/yejinc/MuST-Bench.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yejinchoi;Jiwan Chung;Sumin Shim;Giyeong Oh;Youngjae Yu,True,https://openreview.net/pdf?id=KgeQqLI7OD
KhwOuB0fs9,EffiLearner: Enhancing Efficiency of Generated Code via Self-Optimization,"Large language models (LLMs) have shown remarkable progress in code generation, but their generated code often suffers from inefficiency, resulting in longer execution times and higher memory consumption. To address this issue, we propose EffiLearner, a self-optimization framework that utilizes execution overhead profiles to improve the efficiency of LLM-generated code. EffiLearner first generates code using an LLM, then executes it locally to capture execution time and memory usage profiles. These profiles are fed back to the LLM, which then revises the code to reduce overhead. To evaluate the effectiveness of EffiLearner, we conduct extensive experiments on EffiBench and two commonly used code generation benchmarks with 16 open-source and 6 closed-source models. Our evaluation results demonstrate that through iterative self-optimization, EffiLearner significantly enhances the efficiency of LLM-generated code. For example, the execution time (ET) of StarCoder2-15B for the EffiBench decreases from 0.93 (s) to 0.12 (s) which reduces 87.1\\\\%  execution time requirement compared with the initial code. The total memory usage (TMU) of StarCoder2-15B also decreases from 22.02 (Mb*s) to 2.03 (Mb*s), which decreases 90.8\\\\% total memory consumption during the execution process.",main,NeurIPS,2024,Poster,Dong HUANG;Jianbo Dai;Han Weng;Puzhen Wu;Yuhao QING;Heming Cui;Zhijiang Guo;Jie Zhang,True,https://openreview.net/pdf?id=KhwOuB0fs9
KjNEzWRIqn,Synatra: Turning Indirect Knowledge into Direct Demonstrations for Digital Agents at Scale,"LLMs can now act as autonomous agents that interact with digital environments and complete specific objectives (e.g., arranging an online meeting). However, accuracy is still far from satisfactory, partly due to a lack of large-scale, direct demonstrations for digital tasks. Obtaining supervised data from humans is costly, and automatic data collection through exploration or reinforcement learning relies on complex environmental and content setup, resulting in datasets that lack comprehensive coverage of various scenarios. On the other hand, there is abundant knowledge that may indirectly assist task completion, such as online tutorials that were created for human consumption. In this work, we present Synatra, an approach that effectively transforms this indirect knowledge into direct supervision at scale. We define different types of indirect knowledge, and carefully study the available sources to obtain it, methods to encode the structure of direct demonstrations, and finally methods to transform indirect knowledge into direct demonstrations. We use 100k such synthetically-created demonstrations to finetune a 7B CodeLlama, and demonstrate that the resulting agent surpasses all comparably sized models on three web-based task benchmarks Mind2Web, MiniWoB++ and WebArena, as well as surpassing GPT-3.5 on WebArena and Mind2Web. In addition, while synthetic demonstrations prove to be only 3% the cost of human demonstrations (at $0.031 each), we show that the synthetic demonstrations can be more effective than an identical number of human demonstrations collected from limited domains.",main,NeurIPS,2024,Poster,Tianyue Ou;Frank F. Xu;Aman Madaan;Jiarui Liu;Robert Lo;Abishek Sridhar;Sudipta Sengupta;Dan Roth;Graham Neubig;Shuyan Zhou,True,https://openreview.net/pdf?id=KjNEzWRIqn
Km2XEjH0I5,TACT: Advancing Complex Aggregative Reasoning with Information Extraction Tools,"Large Language Models (LLMs) often do not perform well on queries that require the aggregation of information across texts. To better evaluate this setting and facilitate modeling efforts, we introduce TACT - Text And Calculations through Tables, a dataset crafted to evaluate LLMs' reasoning and computational abilities using complex instructions. TACT contains challenging instructions that demand stitching information scattered across one or more texts, and performing complex integration on this information to generate the answer. We construct this dataset by leveraging an existing dataset of texts and their associated tables. For each such tables, we formulate new queries, and gather their respective answers. We demonstrate that all contemporary LLMs perform poorly on this dataset, achieving an accuracy below 38%. To pinpoint the difficulties and thoroughly dissect the problem, we analyze model performance across three components: table-generation, Pandas command-generation, and execution. Unexpectedly, we discover that each component presents substantial challenges for current LLMs. These insights lead us to propose a focused modeling framework, which we refer to as _IE as a tool_. Specifically, we propose to add ""tools"" for each of the above steps, and implement each such tool with few-shot prompting. This approach shows an improvement over existing prompting techniques, offering a promising direction for enhancing model capabilities in these tasks.",Datasets & Benchmarks,NeurIPS,2024,Poster,Avi Caciularu;Alon Jacovi;Eyal Ben-David;Sasha Goldshtein;Tal Schuster;Jonathan Herzig;Gal Elidan;Amir Globerson,True,https://openreview.net/pdf?id=Km2XEjH0I5
KoSSEp6Du5,E.T. Bench: Towards Open-Ended Event-Level Video-Language Understanding,"Recent advances in Video Large Language Models (Video-LLMs) have demonstrated their great potential in general-purpose video understanding. To verify the significance of these models, a number of benchmarks have been proposed to diagnose their capabilities in different scenarios. However, existing benchmarks merely evaluate models through video-level question-answering, lacking fine-grained event-level assessment and task diversity. To fill this gap, we introduce E.T. Bench (Event-Level & Time-Sensitive Video Understanding Benchmark), a large-scale and high-quality benchmark for open-ended event-level video understanding. Categorized within a 3-level task taxonomy, E.T. Bench encompasses 7.3K samples under 12 tasks with 7K videos (251.4h total length) under 8 domains, providing comprehensive evaluations. We extensively evaluated 8 Image-LLMs and 12 Video-LLMs on our benchmark, and the results reveal that state-of-the-art models for coarse-level (video-level) understanding struggle to solve our fine-grained tasks, e.g., grounding event-of-interests within videos, largely due to the short video context length, improper time representations, and lack of multi-event training data. Focusing on these issues, we further propose a strong baseline model, E.T. Chat, together with an instruction-tuning dataset E.T. Instruct 164K tailored for fine-grained event-level understanding. Our simple but effective solution demonstrates superior performance in multiple scenarios.",Datasets & Benchmarks,NeurIPS,2024,Poster,Ye Liu;Zongyang Ma;Zhongang Qi;Yang Wu;Ying Shan;Chang Wen Chen,True,https://openreview.net/pdf?id=KoSSEp6Du5
Kxta8IInyN,CLAVE: An Adaptive Framework for Evaluating Values of LLM Generated Responses,"The rapid progress in Large Language Models (LLMs) poses potential risks such as generating unethical content. Assessing the values embedded in LLMs' generated responses can help expose their misalignment, but this relies on reference-free value evaluators, e.g. fine-tuned LLMs or closed-source models like GPT-4. Nevertheless, two key challenges emerge in open-ended value evaluation: the evaluator should adapt to changing human value definitions with minimal annotation, against their own bias (adaptability); and remain robust across varying value expressions and scenarios (generalizability). To handle these challenges, we introduce CLAVE, a novel framework that integrates two complementary LLMs: a large model to extract high-level value concepts from diverse responses, leveraging its extensive knowledge and generalizability, and a small model fine-tuned on these concepts to adapt to human value annotations. This dual-model framework enables adaptation to any value system using  <100 human-labeled samples per value type. We also present ValEval, a comprehensive dataset comprising 13k+ (text,value,label) tuples across diverse domains, covering three major value systems. We benchmark the performance of 15+ popular LLM evaluators and fully analyze their strengths and weaknesses. Our findings reveal that CLAVE combining a large prompt-based model and a small fine-tuned one serves as an optimal balance in value evaluation.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jing Yao;Xiaoyuan Yi;Xing Xie,True,https://openreview.net/pdf?id=Kxta8IInyN
L0oSfTroNE,Benchmarking LLMs via Uncertainty Quantification,"The proliferation of open-source Large Language Models (LLMs) from various institutions has highlighted the urgent need for comprehensive evaluation methods. However, current evaluation platforms, such as the widely recognized HuggingFace open LLM leaderboard, neglect a crucial aspect -- uncertainty, which is vital for thoroughly assessing LLMs. To bridge this gap, we introduce a new benchmarking approach for LLMs that integrates uncertainty quantification. Our examination involves nine LLMs (LLM series) spanning five representative natural language processing tasks. Our findings reveal that: I) LLMs with higher accuracy may exhibit lower certainty; II) Larger-scale LLMs may display greater uncertainty compared to their smaller counterparts; and III) Instruction-finetuning tends to increase the uncertainty of LLMs. These results underscore the significance of incorporating uncertainty in the evaluation of LLMs. Our implementation is available at https://github.com/smartyfh/LLM-Uncertainty-Bench.",Datasets & Benchmarks,NeurIPS,2024,Poster,Fanghua Ye;Mingming Yang;Jianhui Pang;Longyue Wang;Derek F. Wong;Emine Yilmaz;Shuming Shi;Zhaopeng Tu,False,https://openreview.net/pdf?id=L0oSfTroNE
L4yLhMjCOR,3DCoMPaT200: Language Grounded Large-Scale 3D Vision Dataset for Compositional Recognition,"Understanding objects in 3D at the part level is essential for humans and robots to navigate and interact with the environment. Current datasets for part-level 3D object understanding encompass a limited range of categories. For instance, the ShapeNet-Part and PartNet datasets only include 16, and 24 object categories respectively. The 3DCoMPaT dataset, specifically designed for compositional understanding of parts and materials, contains only 42 object categories. To foster richer and fine-grained part-level 3D understanding, we introduce 3DCoMPaT200, a large-scale dataset tailored for compositional understanding of object parts and materials, with 200 object categories with approximately 5 times larger object vocabulary compared to 3DCoMPaT and almost 4 times larger part categories. Concretely, 3DCoMPaT200 significantly expands upon 3DCoMPaT, featuring 1,031 fine-grained part categories and 293 distinct material classes for compositional application to 3D object parts. Additionally, to address the complexities of compositional 3D modeling, we propose a novel task of Compositional Part Shape Retrieval using ULIP to provide a strong 3D foundational model for 3D Compositional Understanding. This method evaluates the model shape retrieval performance given one, three, or six parts described in text format. These results show that the model's performance improves with an increasing number of style compositions, highlighting the critical role of the compositional dataset. Such results underscore the dataset's effectiveness in enhancing models' capability to understand complex 3D shapes from a compositional perspective. Code and Data can be found here: https://github.com/3DCoMPaT200/3DCoMPaT200/",Datasets & Benchmarks,NeurIPS,2024,Poster,Mahmoud Ahmed;Xiang Li;Arpit Prajapati;Mohamed Elhoseiny,True,https://openreview.net/pdf?id=L4yLhMjCOR
L5aY1mWvXQ,Rethinking Evaluation Strategy for Temporal Link Prediction through Counterfactual Analysis,"In response to critiques of existing evaluation methods for Temporal Link Prediction (TLP) models, we propose a novel approach to verify if these models truly capture temporal patterns in the data. Our method involves a sanity check formulated as a counterfactual question: ``What if a TLP model is tested on a temporally distorted version of the data instead of the real data?'' Ideally, a TLP model that effectively learns temporal patterns should perform worse on temporally distorted data compared to real data. We provide an in-depth analysis of this hypothesis and introduce two data distortion techniques to assess well-known TLP models.
Our contributions are threefold: (1) We introduce simple techniques to distort temporal patterns within a graph, generating temporally distorted test splits of well-known datasets for sanity checks. These distortion methods are applicable to any temporal graph dataset. (2) We perform counterfactual analysis on TLP models such as JODIE, TGAT, TGN, and CAWN to evaluate their capability in capturing temporal patterns across different datasets. (3) We propose an alternative evaluation strategy for TLP, addressing the limitations of binary classification and ranking methods, and introduce two metrics -- average time difference (ATD) and average count difference (ACD) -- to provide a comprehensive measure of a model's predictive performance. The code and datasets are available at: https://github.com/Aniq55/TLPCF.git",Datasets & Benchmarks,NeurIPS,2024,Reject,Aniq Ur Rahman;Alexander Modell;Justin Coon,False,https://openreview.net/pdf?id=L5aY1mWvXQ
LC1QAqhePv,SciInstruct: a Self-Reflective Instruction Annotated Dataset for Training Scientific Language Models,"Large Language Models (LLMs) have shown promise in assisting scientific discovery. However, such applications are currently limited by LLMs' deficiencies in understanding intricate scientific concepts, deriving symbolic equations, and solving advanced numerical calculations. To bridge these gaps, we introduce SciInstruct, a suite of scientific instructions for training scientific language models capable of college-level scientific reasoning. Central to our approach is a novel self-reflective instruction annotation framework to address the data scarcity challenge in the science domain. This framework leverages existing LLMs to generate step-by-step reasoning for unlabelled scientific questions, followed by a process of self-reflective critic-and-revise. Applying this framework, we curated a diverse and high-quality dataset encompassing physics, chemistry, math, and formal proofs. We analyze the curated SciInstruct from multiple interesting perspectives (e.g., domain, scale, source, question type, answer length, etc.). To verify the effectiveness of SciInstruct, we fine-tuned different language models with SciInstruct, i.e., ChatGLM3 (6B and 32B), Llama3-8B-Instruct, and Mistral-7B: MetaMath, enhancing their scientific and mathematical reasoning capabilities, without sacrificing the language understanding capabilities of the base model. We release all codes and SciInstruct at https://github.com/THUDM/SciGLM.",Datasets & Benchmarks,NeurIPS,2024,Poster,Dan Zhang;Ziniu Hu;Sining Zhoubian;Zhengxiao Du;Kaiyu Yang;Zihan Wang;Yisong Yue;Yuxiao Dong;Jie Tang,True,https://openreview.net/pdf?id=LC1QAqhePv
LDwsvLQTLx,Open-vocabulary vs. Closed-set: Best Practice for Few-shot Object Detection Considering Text Describability,"Open-vocabulary object detection (OVD), detecting specific classes of objects using only their linguistic descriptions (e.g., class names) without any image samples, has garnered significant attention. However, in real-world applications, the target class concepts is often hard to describe in text and the only way to specify target objects is to provide their image examples, yet it is often challenging to obtain a good number of samples. Thus, there is a high demand from practitioners for few-shot object detection (FSOD). A natural question arises: Can the benefits of OVD extend to FSOD for object classes that are difficult to describe in text? Compared to traditional methods that learn only predefined classes (referred to in this paper as closed-set object detection, COD), can the extra cost of OVD be justified? To answer these questions, we propose a method to quantify the ``text-describability'' of object detection datasets using the zero-shot image classification accuracy with CLIP. This allows us to categorize various OD datasets with different text-describability and emprically evaluate the FSOD performance of OVD and COD methods within each category. Our findings reveal that: i) there is little difference between OVD and COD for object classes with low text-describability under equal conditions in OD pretraining; and ii) although OVD can learn from more diverse data than OD-specific data, thereby increasing the volume of training data, it can be counterproductive for classes with low-text-describability. These findings provide practitioners with valuable guidance amidst the recent advancements of OVD methods.",Datasets & Benchmarks,NeurIPS,2024,Reject,Yusuke Hosoya;Masanori Suganuma;Takayuki Okatani,False,https://openreview.net/pdf?id=LDwsvLQTLx
LFCWIE5iS2,emg2qwerty: A Large Dataset with Baselines for Touch Typing using Surface Electromyography,"Surface electromyography (sEMG) non-invasively measures signals generated by muscle activity with sufficient sensitivity to detect individual spinal neurons and richness to identify dozens of gestures and their nuances. Wearable wrist-based sEMG sensors have the potential to offer low friction, subtle, information rich, always available human-computer inputs. To this end, we introduce emg2qwerty, a large-scale dataset of non-invasive electromyographic signals recorded at the wrists while touch typing on a QWERTY keyboard, together with ground-truth annotations and reproducible baselines. With 1,135 sessions spanning 108 users and 346 hours of recording, this is the largest such public dataset to date. These data demonstrate non-trivial, but well defined hierarchical relationships both in terms of the generative process, from neurons to muscles and muscle combinations, as well as in terms of domain shift across users and user sessions. Applying standard modeling techniques from the closely related field of Automatic Speech Recognition (ASR), we show strong baseline performance on predicting key-presses using sEMG signals alone. We believe the richness of this task and dataset will facilitate progress in several problems of interest to both the machine learning and neuroscientific communities. Dataset and code can be accessed at https://github.com/facebookresearch/emg2qwerty.",Datasets & Benchmarks,NeurIPS,2024,Poster,Viswanath Sivakumar;Jeffrey Seely;Alan Du;Sean R Bittner;Adam Berenzweig;Anuoluwapo Bolarinwa;Alexandre Gramfort;Michael I Mandel,True,https://openreview.net/pdf?id=LFCWIE5iS2
LGXeIx75sc,Where's Waldo: Diffusion Features For Personalized Segmentation and Retrieval,"Personalized retrieval and segmentation aim to locate specific instances within a dataset based on an input image and a short description of the reference instance. While supervised methods are effective, they require extensive labeled data for training. Recently, self-supervised foundation models have been introduced to these tasks showing comparable results to supervised methods. However, a significant flaw in these models is evident: they struggle to locate a desired instance when other instances within the same class are presented. In this paper, we explore text-to-image diffusion models for these tasks. Specifically, we propose a novel approach called PDM for Personalized Diffusion Features Matching, that leverages intermediate features of pre-trained text-to-image models for personalization tasks without any additional training. PDM demonstrates superior performance on popular retrieval and segmentation benchmarks, outperforming even supervised methods. We also highlight notable shortcomings in current instance and segmentation datasets and propose new benchmarks for these tasks.",main,NeurIPS,2024,Poster,Dvir Samuel;Rami Ben-Ari;Matan Levy;Nir Darshan;Gal Chechik,True,https://openreview.net/pdf?id=LGXeIx75sc
LLsOmvJbBm,NeuralFluid: Nueral Fluidic System Design and Control with Differentiable Simulation,"We present NeuralFluid, a novel framework to explore neural control and design of complex fluidic systems with dynamic solid boundaries. Our system features a fast differentiable Navier-Stokes solver with solid-fluid interface handling, a low-dimensional differentiable parametric geometry representation, a control-shape co-design algorithm, and gym-like simulation environments to facilitate various fluidic control design applications. Additionally, we present a benchmark of design, control, and learning tasks on high-fidelity, high-resolution dynamic fluid environments that pose challenges for existing differentiable fluid simulators. These tasks include designing the control of artificial hearts, identifying robotic end-effector shapes, and controlling a fluid gate. By seamlessly incorporating our differentiable fluid simulator into a learning framework, we demonstrate successful design, control, and learning results that surpass gradient-free solutions in these benchmark tasks.",main,NeurIPS,2024,Poster,Yifei Li;Yuchen Sun;Pingchuan Ma;Eftychios Sifakis;Tao Du;Bo Zhu;Wojciech Matusik,True,https://openreview.net/pdf?id=LLsOmvJbBm
LOcLhezm1C,Is Function Similarity Over-Engineered? Building a Benchmark,"Binary analysis is a core component of many critical security tasks, including reverse engineering, malware analysis, and vulnerability detection. Manual analysis is often time-consuming, but identifying commonly-used or previously-seen functions can reduce the time it takes to understand a new file. However, given the complexity of assembly, and the NP-hard nature of determining function equivalence, this task is extremely difficult. Common approaches often use sophisticated disassembly and decompilation tools, graph analysis, and other expensive pre-processing steps to perform function similarity searches over some corpus. In this work, we identify a number of discrepancies between the current research environment and the underlying application need. To remedy this, we build a new benchmark, REFuSe-Bench, for binary function similarity detection consisting of high-quality datasets and tests that better reflect real-world use cases. In doing so, we address issues like data duplication and accurate labeling, experiment with real malware, and perform the first serious evaluation of ML binary function similarity models on Windows data. Our benchmark reveals that a new, simple baseline — one which looks at only the raw bytes of a function, and requires no disassembly or other pre-processing --- is able to achieve state-of-the-art performance in multiple settings. Our findings challenge conventional assumptions that complex models with highly-engineered features are being used to their full potential, and demonstrate that simpler approaches can provide significant value.",Datasets & Benchmarks,NeurIPS,2024,Poster,Rebecca Saul;Chang Liu;Noah Fleischmann;Richard J Zak;Kristopher Micinski;Edward Raff;James Holt,True,https://openreview.net/pdf?id=LOcLhezm1C
LUsx0chTsL,Talking Heads: Understanding Inter-Layer Communication in Transformer Language Models,"Although it is known that transformer language models (LMs) pass features from early layers to later layers, it is not well understood how this information is represented and routed by the model. We analyze a mechanism used in two LMs to selectively inhibit items in a context in one task, and find that it underlies a commonly used abstraction across many context-retrieval behaviors. Specifically, we find that models write into low-rank subspaces of the residual stream to represent features which are then read out by later layers, forming low-rank *communication channels* (Elhage et al., 2021) between layers. A particular 3D subspace in model activations in GPT-2 can be traversed to positionally index items in lists, and we show that this mechanism can explain an otherwise arbitrary-seeming sensitivity of the model to the order of items in the prompt. That is, the model has trouble copying the correct information from context when many items ``crowd"" this limited space. By decomposing attention heads with the Singular Value Decomposition (SVD), we find that previously described interactions between heads separated by one or more layers can be predicted via analysis of their weight matrices alone. We show that it is possible to manipulate the internal model representations as well as edit model weights based on the mechanism we discover in order to significantly improve performance on our synthetic Laundry List task, which requires recall from a list, often improving task accuracy by over 20\\\\%. Our analysis reveals a surprisingly intricate interpretable structure learned from language model pretraining, and helps us understand why sophisticated LMs sometimes fail in simple domains, facilitating future analysis of more complex behaviors.",main,NeurIPS,2024,Poster,Jack Merullo;Carsten Eickhoff;Ellie Pavlick,True,https://openreview.net/pdf?id=LUsx0chTsL
LXgbgMOygH,NeuralPlane: An Efficiently Parallelizable Platform for Fixed-wing Aircraft Control with Reinforcement Learning,"Reinforcement learning (RL) demonstrates superior potential over traditional flight control methods for fixed-wing aircraft, particularly under extreme operational conditions. However, the high demand for training samples and the lack of efficient computation in existing simulators hinder its further application. In this paper, we introduce NeuralPlane, the first benchmark platform for large-scale parallel simulations of fixed-wing aircraft. NeuralPlane significantly boosts high-fidelity simulation via GPU-accelerated Flight Dynamics Model (FDM) computation, achieving a single-step simulation time of just 0.2 seconds at a parallel scale of $10^{6}$, far exceeding current platforms. We also provide clear code templates, comprehensive evaluation/visualization tools and hierarchical frameworks for integrating RL and traditional control methods. We believe that NeuralPlane can accelerate the development of RL-based fixed-wing flight control and serve as a new challenging benchmark for the RL community. Our NeuralPlane is open-source and accessible at https://github.com/xuecy22/NeuralPlane.",Datasets & Benchmarks,NeurIPS,2024,Poster,Chuanyi Xue;Qihan Liu;Xiaoteng Ma;Xinyao Qin;Ning Gui;Yang Qi;Jinsheng Ren;Bin Liang;Jun Yang,False,https://openreview.net/pdf?id=LXgbgMOygH
LZV0U6UHb6,Zero-shot Image Editing with Reference Imitation,"Image editing serves as a practical yet challenging task considering the diverse demands from users, where one of the hardest parts is to precisely describe how the edited image should look like. In this work, we present a new form of editing, termed imitative editing, to help users exercise their creativity more conveniently. Concretely, to edit an image region of interest, users are free to directly draw inspiration from some in-the-wild references (e.g., some relative pictures come across online), without having to cope with the fit between the reference and the source. Such a design requires the system to automatically figure out what to expect from the reference to perform the editing. For this purpose, we propose a generative training framework, dubbed MimicBrush, which randomly selects two frames from a video clip, masks some regions of one frame, and learns to recover the masked regions using the information from the other frame. That way, our model, developed from a diffusion prior, is able to capture the semantic correspondence between separate images in a self-supervised manner. We experimentally show the effectiveness of our method under various test cases as well as its superiority over existing alternatives. We also construct a benchmark to facilitate further research.",main,NeurIPS,2024,Poster,Xi Chen;Yutong Feng;Mengting Chen;Yiyang Wang;Shilong Zhang;Yu Liu;Yujun Shen;Hengshuang Zhao,True,https://openreview.net/pdf?id=LZV0U6UHb6
LdRZ9SFBku,UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training,"This work presents a unified knowledge protocol, called UKnow, which facilitates knowledge-based studies from the perspective of data. Particularly focusing on visual and linguistic modalities, we categorize data knowledge into five unit types, namely, in-image, in-text, cross-image, cross-text, and image-text, and set up an efficient pipeline to help construct the multimodal knowledge graph from any data collection. Thanks to the logical information naturally contained in knowledge graph, organizing datasets under UKnow format opens up more possibilities of data usage compared to the commonly used image-text pairs. Following UKnow protocol, we collect, from public international news, a large-scale multimodal knowledge graph dataset that consists of 1,388,568 nodes (with 571,791 vision-related ones) and 3,673,817 triplets. The dataset is also annotated with rich event tags, including 11 coarse labels and 9,185 fine labels. Experiments on four benchmarks demonstrate the potential of UKnow in supporting common-sense reasoning and boosting vision-language pre-training with a single dataset, benefiting from its unified form of knowledge organization. Code, dataset, and models will be made publicly available. See Appendix to download the dataset.",Datasets & Benchmarks,NeurIPS,2024,Poster,Biao Gong;Shuai Tan;Yutong Feng;Xiaoying Xie;Yuyuan Li;Chaochao Chen;Kecheng Zheng;Yujun Shen;Deli Zhao,True,https://openreview.net/pdf?id=LdRZ9SFBku
LdxNWDNvC3,AFBench: A Large-scale Benchmark for Airfoil Design,"Data-driven generative models have emerged as promising approaches towards achieving efficient mechanical inverse design. However, due to prohibitively high cost in time and money, there is still lack of open-source and large-scale benchmarks in this field. It is mainly the case for airfoil inverse design, which requires to generate and edit diverse geometric-qualified  and aerodynamic-qualified airfoils following the multimodal instructions, \\\\emph{i.e.,} dragging points and physical parameters. This paper presents the open-source endeavors in airfoil inverse design, \\\\emph{AFBench}, including a large-scale dataset with 200 thousand airfoils and high-quality aerodynamic and geometric labels, two novel and practical airfoil inverse design tasks, \\\\emph{i.e.,} conditional generation on multimodal physical parameters, controllable editing, and comprehensive metrics to evaluate various existing airfoil inverse design methods. Our aim is to establish \\\\emph{AFBench} as an ecosystem for training and evaluating airfoil inverse design methods, with a specific focus on data-driven controllable inverse design models by multimodal instructions capable of bridging the gap between ideas and execution, the academic research and industrial applications. We have provided baseline models, comprehensive experimental observations, and analysis to accelerate future research. Our baseline model is trained on an RTX 3090 GPU within 16 hours. The codebase, datasets and benchmarks will be available at \\\\url{https://hitcslj.github.io/afbench/}.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jian Liu;Jianyu Wu;Hairun Xie;Guoqing zhang;Jing Wang;Liu Wei;Wanli Ouyang;Junjun Jiang;Xianming Liu;SHIXIANG TANG;Miao Zhang,True,https://openreview.net/pdf?id=LdxNWDNvC3
LezAEImfoc,Beyond Accuracy: Tracking more like Human via Visual Search,"Human visual search ability enables efficient and accurate tracking of an arbitrary moving target, which is a significant research interest in cognitive neuroscience. The recently proposed Central-Peripheral Dichotomy (CPD) theory sheds light on how humans effectively process visual information and track moving targets in complex environments. However, existing visual object tracking algorithms still fall short of matching human performance in maintaining tracking over time, particularly in complex scenarios requiring robust visual search skills. These scenarios often involve Spatio-Temporal Discontinuities (i.e., STDChallenge), prevalent in long-term tracking and global instance tracking. To address this issue, we conduct research from a human-like modeling perspective: (1) Inspired by the CPD, we pro- pose a new tracker named CPDTrack to achieve human-like visual search ability. The central vision of CPDTrack leverages the spatio-temporal continuity of videos to introduce priors and enhance localization precision, while the peripheral vision improves global awareness and detects object movements. (2) To further evaluate and analyze STDChallenge, we create the STDChallenge Benchmark. Besides, by incorporating human subjects, we establish a human baseline, creating a high- quality environment specifically designed to assess trackers’ visual search abilities in videos across STDChallenge. (3) Our extensive experiments demonstrate that the proposed CPDTrack not only achieves state-of-the-art (SOTA) performance in this challenge but also narrows the behavioral differences with humans. Additionally, CPDTrack exhibits strong generalizability across various challenging benchmarks. In summary, our research underscores the importance of human-like modeling and offers strategic insights for advancing intelligent visual target tracking. Code and models are available at https://github.com/ZhangDailing8/CPDTrack.",main,NeurIPS,2024,Poster,Dailing Zhang;Shiyu Hu;Xiaokun Feng;Xuchen Li;Meiqi Wu;Jing Zhang;Kaiqi Huang,True,https://openreview.net/pdf?id=LezAEImfoc
Li9YTHoItP,Perception of Knowledge Boundary for Large Language Models through Semi-open-ended Question Answering,"Large Language Models (LLMs) are widely used for knowledge-seeking purposes yet suffer from hallucinations. The knowledge boundary of an LLM limits its factual understanding, beyond which it may begin to hallucinate. Investigating the perception of LLMs' knowledge boundary is crucial for detecting hallucinations and LLMs' reliable generation. Current studies perceive LLMs' knowledge boundary on questions with concrete answers (close-ended questions) while paying limited attention to semi-open-ended questions that correspond to many potential answers. Some researchers achieve it by judging whether the question is answerable or not. However, this paradigm is not so suitable for semi-open-ended questions, which are usually ``partially answerable questions'' containing both answerable answers and ambiguous (unanswerable) answers. Ambiguous answers are essential for knowledge-seeking, but it may go beyond the knowledge boundary of LLMs. In this paper, we perceive the LLMs' knowledge boundary with semi-open-ended questions by discovering more ambiguous answers. First, we apply an LLM-based approach to construct semi-open-ended questions and obtain answers from a target LLM. Unfortunately, the output probabilities of mainstream black-box LLMs are inaccessible to sample more low-probability ambiguous answers. Therefore, we apply an open-sourced auxiliary model to explore ambiguous answers for the target LLM. We calculate the nearest semantic representation for existing answers to estimate their probabilities, with which we reduce the generation probability of high-probability existing answers to achieve a more effective generation. Finally, we compare the results from the RAG-based evaluation and LLM self-evaluation to categorize four types of ambiguous answers that are beyond the knowledge boundary of the target LLM. Following our method, we construct a dataset to perceive the knowledge boundary for GPT-4. We find that GPT-4 performs poorly on semi-open-ended questions and is often unaware of its knowledge boundary. Besides, our auxiliary model, LLaMA-2-13B, is effective in discovering many ambiguous answers, including correct answers neglected by GPT-4 and delusive wrong answers GPT-4 struggles to identify.",main,NeurIPS,2024,Poster,Zhihua Wen;Zhiliang Tian;Zexin Jian;Zhen Huang;Pei Ke;Yifu Gao;Minlie Huang;Dongsheng Li,True,https://openreview.net/pdf?id=Li9YTHoItP
LnJ2EGKTXh,Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning CodeLLMs,"Large language models (LLMs) have shown great promise at generating robot programs from natural language given domain-specific robot application programming interfaces (APIs). However, the performance gap between proprietary LLMs and smaller open-weight LLMs remains wide. This raises a question: Can we fine-tune smaller open-weight LLMs for generating domain-specific robot programs to close the performance gap with proprietary LLMs? While Self-Instruct is a promising solution by generating a diverse set of training data, it cannot verify the correctness of these programs. In contrast, a robot simulator with a well-defined world can identify execution errors but limits the diversity of programs that it can verify. In this work, we introduce Robo-Instruct, which brings the best of both worlds --- it promotes the diversity of Self-Instruct while providing the correctness of simulator-based checking. Robo-Instruct introduces RoboSim to synthesize a consistent world state on the fly by inferring properties relevant to the program being checked, and simulating actions accordingly. Furthermore, the instructions and programs generated by Self-Instruct may be subtly inconsistent --- such as the program missing a step implied by the instruction. Robo-Instruct further addresses this with InstAlign, an instruction-program alignment procedure that revises the task instruction to reflect the actual results of the generated program. Given a few seed task descriptions and the robot APIs, Robo-Instruct is capable of generating a training dataset using only a small open-weight model. This dataset can then be used to fine-tune small open-weight language models, enabling them to match or even exceed the performance of several proprietary LLMs, such as GPT-3.5-Turbo and Gemini-Pro.",main,NeurIPS,2024,Reject,Zichao Hu;Junyi Jessy Li;Arjun Guha;Joydeep Biswas,True,https://openreview.net/pdf?id=LnJ2EGKTXh
LtS7pP8rEn,Posture-Informed Muscular Force Learning for Robust Hand Pressure Estimation,"We present PiMForce, a novel framework that enhances hand pressure estimation by leveraging 3D hand posture information to augment forearm surface electromyography (sEMG) signals. Our approach utilizes detailed spatial information from 3D hand poses in conjunction with dynamic muscle activity from sEMG to enable accurate and robust whole-hand pressure measurements under diverse hand-object interactions. We also developed a multimodal data collection system that combines a pressure glove, an sEMG armband, and a markerless finger-tracking module. We created a comprehensive dataset from 21 participants, capturing synchronized data of hand posture, sEMG signals, and exerted hand pressure across various hand postures and hand-object interaction scenarios using our collection system. Our framework enables precise hand pressure estimation in complex and natural interaction scenarios. Our approach substantially mitigates the limitations of traditional sEMG-based or vision-based methods by integrating 3D hand posture information with sEMG signals.
Video demos, data, and code are available online.",main,NeurIPS,2024,Poster,Kyungjin Seo;Junghoon Seo;Hanseok Jeong;Sangpil Kim;Sang Ho Yoon,True,https://openreview.net/pdf?id=LtS7pP8rEn
M32Ldpp4Oy,LogiCity: Advancing Neuro-Symbolic AI with Abstract Urban Simulation,"Recent years have witnessed the rapid development of Neuro-Symbolic (NeSy) AI systems, which integrate symbolic reasoning into deep neural networks.
However, most of the existing benchmarks for NeSy AI fail to provide long-horizon reasoning tasks with complex multi-agent interactions.
Furthermore, they are usually constrained by fixed and simplistic logical rules over limited entities, making them far from real-world complexities.
To address these crucial gaps, we introduce LogiCity, the first simulator based on customizable first-order logic (FOL) for an urban-like environment with multiple dynamic agents.
LogiCity models diverse urban elements using semantic and spatial concepts, such as $\\\\texttt{IsAmbulance}(\\\\texttt{X})$ and $\\\\texttt{IsClose}(\\\\texttt{X}, \\\\texttt{Y})$. 
These concepts are used to define FOL rules that govern the behavior of various agents. 
Since the concepts and rules are abstractions, they can be universally applied to cities with any agent compositions, facilitating the instantiation of diverse scenarios.
Besides, a key feature of LogiCity is its support for user-configurable abstractions, enabling customizable simulation complexities for logical reasoning.
To explore various aspects of NeSy AI, LogiCity introduces two tasks, one features long-horizon sequential decision-making, and the other focuses on one-step visual reasoning, varying in difficulty and agent behaviors.
Our extensive evaluation reveals the advantage of NeSy frameworks in abstract reasoning. 
Moreover, we highlight the significant challenges of handling more complex abstractions in long-horizon multi-agent scenarios or under high-dimensional, imbalanced data.
With its flexible design, various features, and newly raised challenges, we believe LogiCity represents a pivotal step forward in advancing the next generation of NeSy AI.
All the code and data are open-sourced at our website.",Datasets & Benchmarks,NeurIPS,2024,Poster,Bowen Li;Zhaoyu Li;Qiwei Du;Jinqi Luo;Wenshan Wang;Yaqi Xie;Simon Stepputtis;Chen Wang;Katia P. Sycara;Pradeep Kumar Ravikumar;Alexander G. Gray;Xujie Si;Sebastian Scherer,True,https://openreview.net/pdf?id=M32Ldpp4Oy
M5JW7O9vc7,$\\\\texttt{Model-GLUE}$: Democratized LLM Scaling for A Large Model Zoo in the Wild,"As Large Language Models (LLMs) excel across tasks and specialized domains, scaling LLMs based on existing models has gained significant attention, which is challenged by potential performance drop when combining disparate models. 
Various techniques have been proposed to aggregate pre-trained LLMs, including model merging, Mixture-of-Experts, and stacking. Despite their merits, a comprehensive comparison and synergistic application of them to a diverse model zoo is yet to be adequately addressed.
In light of this research gap, this paper introduces $\\\\texttt{Model-GLUE}$, a holistic LLM scaling guideline. 
First, our work starts with a benchmarking of existing LLM scaling techniques, especially selective merging, and variants of mixture. 
Utilizing the insights from the benchmark results, we formulate a strategy for the selection and aggregation of a heterogeneous model zoo characterizing different architectures and initialization.
Our methodology involves clustering mergeable models, selecting a merging strategy, and integrating model clusters through model-level mixture. Finally, evidenced by our experiments on a diverse Llama-2-based model zoo, $\\\\texttt{Model-GLUE}$ shows an average performance enhancement of 5.61\\\\%, achieved without additional training.
Codes are available at https://github.com/Model-GLUE/Model-GLUE.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xinyu Zhao;Guoheng Sun;Ruisi Cai;Yukun Zhou;Pingzhi Li;Peihao Wang;Bowen Tan;Yexiao He;Li Chen;Yi Liang;Beidi Chen;Binhang Yuan;Hongyi Wang;Ang Li;Zhangyang Wang;Tianlong Chen,False,https://openreview.net/pdf?id=M5JW7O9vc7
M7SO74I9mo,The RealHumanEval: Evaluating Large Language Models' Abilities to Support Programmers,"Evaluation of large language models for code has primarily relied on static benchmarks, including HumanEval, or more recently using human preferences of LLM responses. As LLMs are increasingly used as programmer assistants, we study whether gains on existing benchmarks or more preferred LLM responses translate to programmer productivity when coding with LLMs, including time spent coding. We introduce RealHumanEval, a web interface to measure the ability of LLMs to assist programmers, through either autocomplete or chat support. We conducted a user study (N=213) using RealHumanEval in which users interacted with six LLMs of varying base model performance. Despite static benchmarks not incorporating humans-in-the-loop, we find that improvements in benchmark performance lead to increased programmer productivity; however gaps in benchmark versus human performance are not proportional---a trend that holds across both forms of LLM support. In contrast, we find that programmer preferences do not correlate with their actual performance, motivating the need for better, human-centric proxy signals. We also open-source RealHumanEval to enable human-centric evaluation of new models and the study data to facilitate efforts to improve code models.",Datasets & Benchmarks,NeurIPS,2024,Reject,Hussein Mozannar;Valerie Chen;Mohammed Alsobay;Subhro Das;Sebastian Zhao;Dennis Wei;Manish Nagireddy;Prasanna Sattigeri;Ameet Talwalkar;David Sontag,True,https://openreview.net/pdf?id=M7SO74I9mo
M91nJrBrqG,Is Your HD Map Constructor Reliable under Sensor Corruptions?,"Driving systems often rely on high-definition (HD) maps for precise environmental information, which is crucial for planning and navigation. While current HD map constructors perform well under ideal conditions, their resilience to real-world challenges, \\\\eg, adverse weather and sensor failures, is not well understood, raising safety concerns. This work introduces MapBench, the first comprehensive benchmark designed to evaluate the robustness of HD map construction methods against various sensor corruptions. Our benchmark encompasses a total of 29 types of corruptions that occur from cameras and LiDAR sensors. Extensive evaluations across 31 HD map constructors reveal significant performance degradation of existing methods under adverse weather conditions and sensor failures, underscoring critical safety concerns. We identify effective strategies for enhancing robustness, including innovative approaches that leverage multi-modal fusion, advanced data augmentation, and architectural techniques. These insights provide a pathway for developing more reliable HD map construction methods, which are essential for the advancement of autonomous driving technology. The benchmark toolkit and affiliated code and model checkpoints have been made publicly accessible.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xiaoshuai Hao;Mengchuan Wei;Yifan Yang;Haimei Zhao;Hui Zhang;Yi ZHOU;Qiang Wang;Weiming Li;Lingdong Kong;Jing Zhang,True,https://openreview.net/pdf?id=M91nJrBrqG
MPJ3oXtTZl,G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering,"Given a graph with textual attributes, we enable users to `chat with their graph': that is, to ask questions about the graph using a conversational interface. In response to a user's questions, our method provides textual replies and highlights the relevant parts of the graph. While existing works integrate large language models (LLMs) and graph neural networks (GNNs) in various ways, they mostly focus on either conventional graph tasks (such as node, edge, and graph classification), or on answering simple graph queries on small or synthetic graphs. In contrast, we develop a flexible question-answering framework targeting real-world textual graphs, applicable to multiple applications including scene graph understanding, common sense reasoning, and knowledge graph reasoning. Toward this goal, we first develop a Graph Question Answering (GraphQA) benchmark with data collected from different tasks. Then, we propose our \\\\textit{G-Retriever} method, introducing the first retrieval-augmented generation (RAG) approach for general textual graphs, which can be fine-tuned to enhance graph understanding via soft prompting. To resist hallucination and to allow for textual graphs that greatly exceed the LLM's context window size, \\\\textit{G-Retriever} performs RAG over a graph by formulating this task as a Prize-Collecting Steiner Tree optimization problem. Empirical evaluations show that our method outperforms baselines on textual graph tasks from multiple domains, scales well with larger graph sizes, and mitigates hallucination.~\\\\footnote{Our codes and datasets are available at: \\\\url{https://github.com/XiaoxinHe/G-Retriever}}",main,NeurIPS,2024,Poster,Xiaoxin He;Yijun Tian;Yifei Sun;Nitesh V Chawla;Thomas Laurent;Yann LeCun;Xavier Bresson;Bryan Hooi,True,https://openreview.net/pdf?id=MPJ3oXtTZl
MS4oxVfBHn,UDA: A Benchmark Suite for Retrieval Augmented Generation in Real-World Document Analysis,"The use of Retrieval-Augmented Generation (RAG) has improved Large Language Models (LLMs) in collaborating with external data, yet significant challenges exist in real-world scenarios. In areas such as academic literature and finance question answering, data are often found in raw text and tables in HTML or PDF formats, which can be lengthy and highly unstructured. In this paper, we introduce a benchmark suite, namely Unstructured Document Analysis (UDA), that involves 2,965 real-world documents and 29,590 expert-annotated Q&A pairs. We revisit popular LLM- and RAG-based solutions for document analysis and evaluate the design choices and answer qualities across multiple document domains and diverse query types. Our evaluation yields interesting findings and highlights the importance of data parsing and retrieval.  We hope our benchmark can shed light and better serve real-world document analysis applications. The benchmark suite and code can be found at https://github.com/qinchuanhui/UDA-Benchmark",Datasets & Benchmarks,NeurIPS,2024,Poster,Yulong Hui;Yao Lu;Huanchen Zhang,True,https://openreview.net/pdf?id=MS4oxVfBHn
MU2s9wwWLo,ConceptMix: A Compositional Image Generation Benchmark with Controllable Difficulty,"Compositionality is a critical capability in Text-to-Image (T2I) models, as it reflects their ability to understand and combine multiple concepts from text descriptions. Existing evaluations of compositional capability rely heavily on human-designed text prompts or fixed templates, limiting their diversity and complexity, and yielding low discriminative power. We propose ConceptMix, a scalable, controllable, and customizable benchmark which automatically evaluates compositional generation ability of T2I models. This is done in two stages. First, ConceptMix generates the text prompts: concretely, using categories of visual concepts (e.g., objects, colors, shapes, spatial relationships), it randomly samples an object and k-tuples of visual concepts, then uses GPT-4o to generate text prompts for image generation based on these sampled concepts. Second, ConceptMix evaluates the images generated in response to these prompts: concretely, it checks how many of the k concepts actually appeared in the image by generating one question per visual concept and using a strong VLM to answer them. Through administering ConceptMix to a diverse set of T2I models (proprietary as well as open ones) using increasing values of k, we show that our ConceptMix has higher discrimination power than earlier benchmarks. Specifically, ConceptMix reveals that the performance of several models, especially open models, drops dramatically with increased k. Importantly, it also provides insight into the lack of prompt diversity in widely-used training datasets. Additionally, we conduct extensive human studies to validate the design of ConceptMix and compare our automatic grading with human judgement. We hope it will guide future T2I model development.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xindi Wu;Dingli Yu;Yangsibo Huang;Olga Russakovsky;Sanjeev Arora,True,https://openreview.net/pdf?id=MU2s9wwWLo
MUnPBKBaCY,"Noisy Ostracods: A Fine-Grained, Imbalanced Real-World Dataset for Benchmarking Robust Machine Learning and Label Correction Methods","We present the Noisy Ostracods, a noisy dataset for genus and species classification
of crustacean ostracods with specialists’ annotations. Over the 71466 specimens
collected, 5.58% of them are estimated to be noisy (possibly problematic) at genus
level. The dataset is created to addressing a real-world challenge: creating a
clean fine-grained taxonomy dataset. The Noisy Ostracods dataset has diverse
noises from multiple sources. Firstly, the noise is open-set, including new classes
discovered during curation that were not part of the original annotation. The
dataset has pseudo-classes, where annotators misclassified samples that should
belong to an existing class into a new pseudo-class. The Noisy Ostracods dataset
is highly imbalanced with a imbalance factor ρ = 22429. This presents a unique
challenge for robust machine learning methods, as existing approaches have not
been extensively evaluated on fine-grained classification tasks with such diverse
real-world noise. Initial experiments using current robust learning techniques
have not yielded significant performance improvements on the Noisy Ostracods
dataset compared to cross-entropy training on the raw, noisy data. On the other
hand, noise detection methods have underperformed in error hit rate compared
to naive cross-validation ensembling for identifying problematic labels. These
findings suggest that the fine-grained, imbalanced nature, and complex noise
characteristics of the dataset present considerable challenges for existing noiserobust
algorithms. By openly releasing the Noisy Ostracods dataset, our goal
is to encourage further research into the development of noise-resilient machine
learning methods capable of effectively handling diverse, real-world noise in finegrained
classification tasks. The dataset, along with its evaluation protocols, can be
accessed at https://github.com/H-Jamieu/Noisy_ostracods.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jiamian Hu;Hong Yuanyuan;Yihua Chen;He Wang;Moriaki Yasuhara,True,https://openreview.net/pdf?id=MUnPBKBaCY
MYyGhe9MBg,T2VSafetyBench: Evaluating the Safety of Text-to-Video Generative Models,"The recent development of Sora leads to a new era in text-to-video (T2V) generation. Along with this comes the rising concern about its safety risks. The generated videos may contain illegal or unethical content, and there is a lack of comprehensive quantitative understanding of their safety, posing a challenge to their reliability and practical deployment. Previous evaluations primarily focus on the quality of video generation. While some evaluations of text-to-image models have considered safety, they cover limited aspects and do not address the unique temporal risk inherent in video generation. To bridge this research gap, we introduce T2VSafetyBench, the first comprehensive benchmark for conducting safety-critical assessments of text-to-video models. We define 4 primary categories with 14 critical aspects of video generation safety and construct a malicious prompt dataset including real-world prompts, LLM-generated prompts, and jailbreak attack-based prompts. We then conduct a thorough safety evaluation on 9 recently released T2V models. Based on our evaluation results, we draw several important findings, including: 1) no single model excels in all aspects, with different models showing various strengths; 2) the correlation between GPT-4 assessments and manual reviews is generally high; 3) there is a trade-off between the usability and safety of text-to-video generative models. This indicates that as the field of video generation rapidly advances, safety risks are set to surge, highlighting the urgency of prioritizing video safety. We hope that T2VSafetyBench can provide insights for better understanding the safety of video generation in the era of generative AIs. Our code is publicly available at \\\\url{https://github.com/yibo-miao/T2VSafetyBench}.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yibo Miao;Yifan Zhu;Lijia Yu;Jun Zhu;Xiao-Shan Gao;Yinpeng Dong,True,https://openreview.net/pdf?id=MYyGhe9MBg
MbZuh8L0Xg,DiffPhyCon: A Generative Approach to Control Complex Physical Systems,"Controlling the evolution of complex physical systems is a fundamental task across science and engineering. 
Classical techniques suffer from limited applicability or huge computational costs. On the other hand, recent deep learning and reinforcement learning-based approaches often struggle to optimize long-term control sequences under the constraints of system dynamics. In this work, we introduce Diffusion Physical systems Control (DiffPhyCon), a new class of method to address the physical systems control problem. DiffPhyCon excels by simultaneously minimizing both the learned generative energy function and the predefined control objectives across the entire trajectory and control sequence. Thus, it can explore globally and plan near-optimal control sequences. Moreover, we enhance DiffPhyCon with prior reweighting, enabling the discovery of control sequences that significantly deviate from the training distribution. We test our method on three tasks: 1D Burgers' equation, 2D jellyfish movement control, and 2D high-dimensional smoke control, where our generated jellyfish dataset is released as a benchmark for complex physical system control research. Our method outperforms widely applied classical approaches and state-of-the-art deep learning and reinforcement learning methods. Notably, DiffPhyCon unveils an intriguing fast-close-slow-open pattern observed in the jellyfish, aligning with established findings in the field of fluid dynamics. The project website, jellyfish dataset, and code can be found at https://github.com/AI4Science-WestlakeU/diffphycon.",main,NeurIPS,2024,Poster,Long Wei;Peiyan Hu;Ruiqi Feng;Haodong Feng;Yixuan Du;Tao Zhang;Rui Wang;Yue Wang;Zhi-Ming Ma;Tailin Wu,True,https://openreview.net/pdf?id=MbZuh8L0Xg
Mbd3QxXjq5,OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset,"Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models. We will release our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license.",Datasets & Benchmarks,NeurIPS,2024,Oral,Shubham Toshniwal;Ivan Moshkov;Sean Narenthiran;Daria Gitman;Fei Jia;Igor Gitman,True,https://openreview.net/pdf?id=Mbd3QxXjq5
Md1mEoPEaQ,SETLEXSEM CHALLENGE: Using Set Operations to Evaluate the Lexical and Semantic Robustness of Language Models,"Set theory is foundational to mathematics and, when sets are finite, to reasoning about the world. An intelligent system should perform set operations consistently, regardless of superficial variations in the operands. Initially designed for semantically-oriented NLP tasks, large language models (LLMs) are now being evaluated on algorithmic tasks. Because sets are comprised of arbitrary symbols (e.g. numbers, words), they provide an opportunity to test, systematically, the invariance of LLMs’ algorithmic abilities under simple lexical or semantic variations. To this end, we present the SETLEXSEM CHALLENGE, a synthetic benchmark that evaluates the performance of LLMs on set operations. SETLEXSEM assesses the robustness of LLMs’ instruction-following abilities under various conditions, focusing on the set operations and the nature and construction of the set members. Evaluating seven LLMs with SETLEXSEM, we find that they exhibit poor robust- ness to variation in both operation and operands. We show – via the framework’s systematic sampling of set members along lexical and semantic dimensions – that LLMs are not only not robust to variation along these dimensions but demonstrate unique failure modes in particular, easy-to-create semantic groupings of ""deceptive"" sets. We find that rigorously measuring language model robustness to variation in frequency and length is challenging and present an analysis that measures them in- dependently. The code for reproducing the results of this paper, and for generating the SETLEXSEM CHALLENGE dataset, is available https://github.com/amazon-science/SetLexSem-Challenge.",Datasets & Benchmarks,NeurIPS,2024,Poster,Nicholas Andrew Dronen;Bardiya Akhbari;Manish Gawali,True,https://openreview.net/pdf?id=Md1mEoPEaQ
Mi853QaJx6,On the Worst Prompt Performance of Large Language Models,"The performance of large language models (LLMs) is acutely sensitive to the phrasing of prompts, which raises significant concerns about their reliability in real-world scenarios. Existing studies often divide prompts into task-level instructions and case-level inputs and primarily focus on evaluating and improving robustness against variations in tasks-level instructions. However, this setup fails to fully address the diversity of real-world user queries and assumes the existence of task-specific datasets. To address these limitations, we introduce RobustAlpacaEval, a new benchmark that consists of semantically equivalent case-level queries and emphasizes the importance of using the worst prompt performance to gauge the lower bound of model performance. Extensive experiments on RobustAlpacaEval with ChatGPT and six open-source LLMs from the Llama, Mistral, and Gemma families uncover substantial variability in model performance; for instance, a difference of 45.48% between the worst and best performance for the Llama-2-70B-chat model, with its worst performance dipping as low as 9.38%. We further illustrate the difficulty in identifying the worst prompt from both model-agnostic and model-dependent perspectives, emphasizing the absence of a shortcut to characterize the worst prompt. We also attempt to enhance the worst prompt performance using existing prompt engineering and prompt consistency methods, but find that their impact is limited. These findings underscore the need to create more resilient LLMs that can maintain high performance across diverse prompts.",main,NeurIPS,2024,Poster,Bowen Cao;Deng Cai;Zhisong Zhang;Yuexian Zou;Wai Lam,True,https://openreview.net/pdf?id=Mi853QaJx6
MojU63gze2,WildPPG: A Real-World PPG Dataset of Long Continuous Recordings,"Reflective photoplethysmography (PPG) has become the default sensing technique in wearable devices to monitor cardiac activity via a person’s heart rate (HR). However, PPG-based HR estimates can be substantially impacted by factors such as the wearer’s activities, sensor placement and resulting motion artifacts, as well as environmental characteristics such as temperature and ambient light. These and other factors can significantly impact and decrease HR prediction reliability. In this paper, we show that state-of-the-art HR estimation methods struggle when processing representative data from everyday activities in outdoor environments, likely because they rely on existing datasets that captured controlled conditions. We introduce a novel multimodal dataset and benchmark results for continuous PPG recordings during outdoor activities from 16 participants over 13.5 hours, captured from four wearable sensors, each worn at a different location on the body, totaling 216 hours. Our recordings include accelerometer, temperature, and altitude data, as well as a synchronized Lead I-based electrocardiogram for ground-truth HR references. Participants completed a round trip from Zurich to Jungfraujoch, a tall mountain in Switzerland over the course of one day. The trip included outdoor and indoor activities such as walking, hiking, stair climbing, eating, drinking, and resting at various temperatures and altitudes (up to 3,571 m above sea level) as well as using cars, trains, cable cars, and lifts for transport—all of which impacted participants’ physiological dynamics. We also present a novel method that estimates HR values more robustly in such real-world scenarios than existing baselines.

Dataset & code for HR estimation: https://siplab.org/projects/WildPPG",Datasets & Benchmarks,NeurIPS,2024,Poster,Manuel Meier;Berken Utku Demirel;Christian Holz,True,https://openreview.net/pdf?id=MojU63gze2
MsCSn0rlpP,The State of Data Curation at NeurIPS: An Assessment of Dataset Development Practices in the Datasets and Benchmarks Track,"Data curation is a field with origins in librarianship and archives, whose scholarship and thinking on data issues go back centuries, if not millennia. The field of machine learning is increasingly observing the importance of data curation to the advancement of both applications and fundamental understanding of machine learning models -- evidenced not least by the creation of the Datasets and Benchmarks track itself. This work provides an analysis of recent dataset development practices at NeurIPS through the lens of data curation. We present an evaluation framework for dataset documentation, consisting of a rubric and toolkit developed through a thorough literature review of data curation principles. We use the framework to systematically assess the strengths and weaknesses in current dataset development practices of 60 datasets published in the NeurIPS Datasets and Benchmarks track from 2021-2023. We summarize key findings and trends. Results indicate greater need for documentation about environmental footprint, ethical considerations, and data management. We suggest targeted strategies and resources to improve documentation in these areas and  provide recommendations for the NeurIPS peer-review process that prioritize rigorous data curation in ML. We also provide guidelines for dataset developers on the use of our rubric as a standalone tool. Finally, we provide results in the format of a dataset that showcases aspects of recommended data curation practices. Our rubric and results are of interest for improving data curation practices broadly in the field of ML as well as to data curation and science and technology studies scholars studying practices in ML. Our aim is to support continued improvement in interdisciplinary research on dataset practices, ultimately improving the reusability and reproducibility of new datasets and benchmarks, enabling standardized and informed human oversight, and strengthening the foundation of rigorous and responsible ML research.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Eshta Bhardwaj;Harshit Gujral;Siyi Wu;Ciara Zogheib;Tegan Maharaj;Christoph Becker,True,https://openreview.net/pdf?id=MsCSn0rlpP
MtRvzJBsBA,LRM-Zero: Training Large Reconstruction Models with Synthesized Data,"We present LRM-Zero, a Large Reconstruction Model (LRM) trained entirely on synthesized 3D data, achieving high-quality sparse-view 3D reconstruction. The core of LRM-Zero is our procedural 3D dataset, Zeroverse, which is automatically synthesized from simple primitive shapes with random texturing and augmentations (e.g., height fields, boolean differences, and wireframes). Unlike previous 3D datasets (e.g., Objaverse) which are often captured or crafted by humans to approximate real 3D data, Zeroverse completely ignores realistic global semantics but is rich in complex geometric and texture details that are locally similar to or even more intricate than real objects. We demonstrate that our LRM-Zero, trained with our fully synthesized Zeroverse, can achieve high visual quality in the reconstruction of real-world objects, competitive with models trained on Objaverse. We also analyze several critical design choices of Zeroverse that contribute to LRM-Zero's capability and training stability. Our work demonstrates that 3D reconstruction, one of the core tasks in 3D vision, can potentially be addressed without the semantics of real-world objects. The Zeroverse's procedural synthesis code and interactive visualization are available at: https://desaixie.github.io/lrm-zero/.",main,NeurIPS,2024,Poster,Desai Xie;Sai Bi;Zhixin Shu;Kai Zhang;Zexiang Xu;Yi Zhou;Soren Pirk;Arie Kaufman;Xin Sun;Hao Tan,True,https://openreview.net/pdf?id=MtRvzJBsBA
MwmmBg1VYg,Why are Visually-Grounded Language Models Bad at Image Classification?,"Image classification is one of the most fundamental capabilities of machine vision intelligence. In this work, we revisit the image classification task using visually-grounded language models (VLMs) such as GPT-4V and LLaVA. We find that existing proprietary and public VLMs, despite often using CLIP as a vision encoder and having many more parameters, significantly underperform CLIP on standard image classification benchmarks like ImageNet. To understand the reason, we explore several hypotheses concerning the inference algorithms, training objectives, and data processing in VLMs. Our analysis reveals that the primary cause is data-related: critical information for image classification is encoded in the VLM's latent space but can only be effectively decoded with enough training data. Specifically, there is a strong correlation between the frequency of class exposure during VLM training and instruction-tuning and the VLM's performance in those classes; when trained with sufficient data, VLMs can match the accuracy of state-of-the-art classification models. Based on these findings, we enhance a VLM by integrating classification-focused datasets into its training, and demonstrate that the enhanced classification performance of the VLM transfers to its general capabilities, resulting in an improvement of 11.8% on the newly collected ImageWikiQA dataset.",main,NeurIPS,2024,Poster,Yuhui Zhang;Alyssa Unell;Xiaohan Wang;Dhruba Ghosh;Yuchang Su;Ludwig Schmidt;Serena Yeung-Levy,True,https://openreview.net/pdf?id=MwmmBg1VYg
Myc4q2g9xZ,Multi-modal Situated Reasoning in 3D Scenes,"Situation awareness is essential for understanding and reasoning about 3D scenes
 in embodied AI agents. However, existing datasets and benchmarks for situated
 understanding suffer from severe limitations in data modality, scope, diversity, and
 scale. To address these limitations, we propose Multi-modal Situated Question
 Answering (MSQA), a large-scale multi-modal situated reasoning dataset, scalably
 collected leveraging 3D scene graphs and vision-language models (VLMs) across
 a diverse range of real-world 3D scenes. MSQA includes 251K situated question
answering pairs across 9 distinct question categories, covering complex scenarios
 and object modalities within 3D scenes. We introduce a novel interleaved multi
modal input setting in our benchmark to provide both texts, images, and point
 clouds for situation and question description, aiming to resolve ambiguity in
 describing situations with single-modality inputs (e.g., texts). Additionally, we
 devise the Multi-modal Next-step Navigation (MSNN) benchmark to evaluate
 models’ grounding of actions and transitions between situations. Comprehensive
 evaluations on reasoning and navigation tasks highlight the limitations of existing
 vision-language models and underscore the importance of handling multi-modal
 interleaved inputs and situation modeling. Experiments on data scaling and cross
domain transfer further demonstrate the effectiveness of leveraging MSQA as
 a pre-training dataset for developing more powerful situated reasoning models,
 contributing to advancements in 3D scene understanding for embodied AI.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xiongkun Linghu;Jiangyong Huang;Xuesong Niu;Xiaojian Ma;Baoxiong Jia;Siyuan Huang,True,https://openreview.net/pdf?id=Myc4q2g9xZ
MzTdZhMjeC,MO-DDN: A Coarse-to-Fine Attribute-based Exploration Agent for Multi-Object Demand-driven Navigation,"The process of satisfying daily demands is a fundamental aspect of humans' daily lives. With the advancement of embodied AI, robots are increasingly capable of satisfying human demands. Demand-driven navigation (DDN) is a task in which an agent must locate an object to satisfy a specified demand instruction, such as ""I am thirsty."" The previous study typically assumes that each demand instruction requires only one object to be fulfilled and does not consider individual preferences. However, the realistic human demand may involve multiple objects. In this paper, we introduce the Multi-object Demand-driven Navigation (MO-DDN) benchmark, which addresses these nuanced aspects, including multi-object search and personal preferences, thus making the MO-DDN task more reflective of real-life scenarios compared to DDN. Building upon previous work, we employ the concept of ``attribute'' to tackle this new task. However, instead of solely relying on attribute features in an end-to-end manner like DDN, we propose a modular method that involves constructing a coarse-to-fine attribute-based exploration agent (C2FAgent). Our experimental results illustrate that this coarse-to-fine exploration strategy capitalizes on the advantages of attributes at various decision-making levels, resulting in superior performance compared to baseline methods. Code and video can be found at https://sites.google.com/view/moddn.",main,NeurIPS,2024,Poster,Hongcheng Wang;Peiqi Liu;Wenzhe Cai;Mingdong Wu;Zhengyu Qian;Hao Dong,True,https://openreview.net/pdf?id=MzTdZhMjeC
NC0Bjl4uTf,Chinese Inertial GAN for Writing Signal Generation and Recognition,"Disabled people constitute a significant part of the global population, deserving of inclusive consideration and empathetic support. However, the current human-computer interaction based on keyboards may not meet the requirements of disabled people. The small size, ease of wearing, and low cost of inertial sensors make inertial sensor-based writing recognition a promising human-computer interaction option for disabled people. However, accurate recognition relies on massive inertial signal samples, which are hard to collect for the Chinese context due to the vast number of characters. Therefore, we design a Chinese inertial generative adversarial network (CI-GAN) containing Chinese glyph encoding (CGE), forced optimal transport (FOT), and semantic relevance alignment (SRA) to acquire unlimited high-quality training samples. Unlike existing vectorization focusing on the meaning of Chinese characters, CGE represents the shape and stroke features, providing glyph guidance for GAN to generate writing signals. FOT constrains feature consistency between generated and real signals through the designed forced feature matching mechanism, meanwhile addressing GANs' mode collapse and mixing issues by introducing Wasserstein distance. SRA captures the semantic relevance between various Chinese glyphs and injects this information into the GAN to establish batch-level constraints and set higher standards of generated signal quality. By utilizing the massive training samples provided by CI-GAN, the performance of six widely used classifiers is improved from 6.7\\\\% to 98.4\\\\%, indicating that CI-GAN constructs a flexible and efficient data platform for Chinese inertial writing recognition. Furthermore, we release the first Chinese writing recognition dataset based on inertial sensors in GitHub.",main,NeurIPS,2024,Reject,Yifeng Wang;Yi Zhao,True,https://openreview.net/pdf?id=NC0Bjl4uTf
NCaGHtbkKo,Harmony4D: A Video Dataset for In-The-Wild Close Human Interactions,"Understanding how humans interact with each other is key to building realistic multi-human virtual reality systems. This area remains relatively unexplored due to the lack of large-scale datasets. Recent datasets focusing on this issue mainly consist of activities captured entirely in controlled indoor environments with choreographed actions, significantly affecting their diversity. To address this, we introduce Harmony4D, a multi-view video dataset for human-human interaction featuring in-the-wild activities such as wrestling, dancing, MMA,
and more. We use a flexible multi-view capture system to record these dynamic activities and provide annotations for human detection, tracking, 2D/3D pose estimation, and mesh recovery for closely interacting subjects. We propose a novel markerless algorithm to track 3D human poses in severe occlusion and close interaction to obtain our annotations with minimal manual intervention. Harmony4D consists of 1.66 million images and 3.32 million human instances from more than 20 synchronized cameras with 208 video sequences spanning diverse environments and 24 unique subjects. We rigorously evaluate existing state-of-the-art methods for mesh recovery and highlight their significant limitations in modeling close interaction scenarios. Additionally, we fine-tune a pre-trained HMR2.0 model on Harmony4D and demonstrate an improved performance of 54.8% PVE in scenes with severe occlusion and contact. “Harmony—a cohesive alignment of human behaviors."" Code and data are available at https://jyuntins.github.io/harmony4d/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Rawal Khirodkar;Jyun-Ting Song;Jinkun Cao;Zhengyi Luo;Kris M. Kitani,True,https://openreview.net/pdf?id=NCaGHtbkKo
NHob4eMg7R,"SCRREAM : SCan, Register, REnder And Map: A Framework for Annotating Accurate and Dense 3D Indoor Scenes with a Benchmark","Traditionally, 3d indoor datasets have generally prioritized scale over ground-truth accuracy in order to obtain improved generalization. However, using these datasets to evaluate dense geometry tasks, such as depth rendering, can be problematic as the meshes of the dataset are often incomplete and may produce wrong ground truth to evaluate the details. In this paper, we propose SCRREAM, a dataset annotation framework that allows annotation of fully dense meshes of objects in the scene and registers camera poses on the real image sequence, which can produce accurate ground truth for both sparse 3D as well as dense 3D tasks. We show the details of the dataset annotation pipeline and showcase four possible variants of datasets that can be obtained from our framework with example scenes, such as indoor reconstruction and SLAM, scene editing \\\\& object removal, human reconstruction and 6d pose estimation. Recent pipelines for indoor reconstruction and SLAM serve as new benchmarks. In contrast to previous indoor dataset, our design allows to evaluate dense geometry tasks on eleven sample scenes against accurately rendered ground truth depth maps.",Datasets & Benchmarks,NeurIPS,2024,Poster,HyunJun Jung;Weihang Li;Shun-Cheng Wu;William Bittner;Nikolas Brasch;Jifei Song;Eduardo Pérez-Pellitero;Zhensong Zhang;Arthur Moreau;Nassir Navab;Benjamin Busam,True,https://openreview.net/pdf?id=NHob4eMg7R
NPAgarrM73,KidSat: satellite imagery to map childhood poverty dataset and benchmark,"Satellite imagery has emerged as an important tool to analyse demographic, health, and development indicators. While various deep learning models have been built for these tasks, each is specific to a particular problem, with no standard benchmarks available. We propose a new dataset pairing satellite imagery and high-quality survey data on child poverty to benchmark satellite feature representations. Our dataset consists of 33,608 images, each 10 km $\\\\times$ 10 km, from 19 countries in Eastern and Southern Africa in the time period 1997-2022. As defined by UNICEF, multidimensional child poverty covers six dimensions and it can be calculated from the face-to-face Demographic and Health Surveys (DHS) Program. As part of the benchmark, we test spatial as well as temporal generalization, by testing on unseen locations, and on data after the training years. 
  Using our dataset we benchmark multiple models, from low-level satellite imagery models such as MOSAIKS, to deep learning foundation models, which include both generic vision models such as Self-Distillation with no Labels (DINOv2) models and specific satellite imagery models such as SatMAE. We provide open source code for building the satellite dataset, obtaining ground truth data from DHS and running various models assessed in our work.",Datasets & Benchmarks,NeurIPS,2024,Reject,Makkunda Sharma;Fan Yang;Duy-Nhat Vo;Esra Suel;Swapnil Mishra;Samir Bhatt;Oliver Fiala;William Rudgard;Seth Flaxman,True,https://openreview.net/pdf?id=NPAgarrM73
NQLZoMHm6u,NewTerm: Benchmarking Real-Time New Terms for Large Language Models with Annual Updates,"Despite their remarkable abilities in various tasks, large language models (LLMs) still struggle with real-time information (e.g., new facts and terms) due to the knowledge cutoff in their development process. However, existing benchmarks focus on outdated content and limited fields, facing difficulties in real-time updating and leaving new terms unexplored. To address this problem, we propose an adaptive benchmark, NewTerm, for real-time evaluation of new terms. We design a highly automated construction method to ensure high-quality benchmark construction with minimal human effort, allowing flexible updates for real-time information. Empirical results on various LLMs demonstrate over 20% performance reduction caused by new terms. Additionally, while updates to the knowledge cutoff of LLMs can cover some of the new terms, they are unable to generalize to more distant new terms. We also analyze which types of terms are more challenging and why LLMs struggle with new terms, paving the way for future research. Finally, we construct NewTerm 2022 and 2023 to evaluate the new terms updated each year and will continue updating annually. The benchmark and codes can be found at https://anonymous.4open.science/r/NewTerms.",Datasets & Benchmarks,NeurIPS,2024,Poster,Hexuan Deng;Wenxiang Jiao;Xuebo Liu;Min Zhang;Zhaopeng Tu,True,https://openreview.net/pdf?id=NQLZoMHm6u
NTkUXqDvlg,Using Unity to Help Solve Reinforcement Learning,"Leveraging the depth and flexibility of XLand as well as the rapid prototyping features of the Unity engine, we present the United Unity Universe — an open-source toolkit designed to accelerate the creation of innovative reinforcement learning environments. This toolkit includes a robust implementation of XLand 2.0 complemented by a user-friendly interface which allows users to modify the details of procedurally generated terrains and task rules with ease. Additionally, we provide a curated selection of terrains and rule sets, accompanied by implementations of reinforcement learning baselines to facilitate quick experimentation with novel architectural designs for adaptive agents. Furthermore, we illustrate how the United Unity Universe serves as a high-level language that enables researchers to develop diverse and endlessly variable 3D environments within a unified framework. This functionality establishes the United Unity Universe (U3) as an essential tool for advancing the field of reinforcement learning, especially in the development of adaptive and generalizable learning systems.",Datasets & Benchmarks,NeurIPS,2024,Poster,Connor Brennan;Andrew Robert Williams;Omar G. Younis;Vedant Vyas;Daria Yasafova;Irina Rish,False,https://openreview.net/pdf?id=NTkUXqDvlg
Na2gnQFkn8,A SARS-CoV-2 Interaction Dataset and VHH Sequence Corpus for Antibody Language Models,"Antibodies are crucial proteins produced by the immune system to eliminate harmful foreign substances and have become pivotal therapeutic agents for treating human diseases.
To accelerate the discovery of antibody therapeutics, there is growing interest in constructing language models using antibody sequences.
However, the applicability of pre-trained language models for antibody discovery has not been thoroughly evaluated due to the scarcity of labeled datasets.
To overcome these limitations, we introduce AVIDa-SARS-CoV-2, a dataset featuring the antigen-variable domain of heavy chain of heavy chain antibody (VHH) interactions obtained from two alpacas immunized with severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) spike proteins.
AVIDa-SARS-CoV-2 includes binary labels indicating the binding or non-binding of diverse VHH sequences to 12 SARS-CoV-2 mutants, such as the Delta and Omicron variants.
Furthermore, we release VHHCorpus-2M, a pre-training dataset for antibody language models, containing over two million VHH sequences.
We report benchmark results for predicting SARS-CoV-2-VHH binding using VHHBERT pre-trained on VHHCorpus-2M and existing general protein and antibody-specific pre-trained language models.
These results confirm that AVIDa-SARS-CoV-2 provides valuable benchmarks for evaluating the representation capabilities of antibody language models for binding prediction, thereby facilitating the development of AI-driven antibody discovery.
The datasets are available at https://datasets.cognanous.com.",Datasets & Benchmarks,NeurIPS,2024,Poster,Hirofumi Tsuruta;Hiroyuki Yamazaki;Ryota Maeda;Ryotaro Tamura;Akihiro Imura,True,https://openreview.net/pdf?id=Na2gnQFkn8
NmlnmLYMZ4,When does perceptual alignment benefit vision representations?,"Humans judge perceptual similarity according to diverse visual attributes, including scene layout, subject location, and camera pose. Existing vision models understand a wide range of semantic abstractions but improperly weigh these attributes and thus make inferences misaligned with human perception. 
While vision representations have previously benefited from human preference alignment in contexts like image generation, the utility of perceptually aligned representations in more general-purpose settings remains unclear. Here, we investigate how aligning vision model representations to human perceptual judgments impacts their usability in standard computer vision tasks. We finetune state-of-the-art models on a dataset of human similarity judgments for synthetic image triplets and evaluate them across diverse computer vision tasks. We find that aligning models to perceptual judgments yields representations that improve upon the original backbones across many downstream tasks, including counting, semantic segmentation, depth estimation, instance retrieval, and retrieval-augmented generation. In addition, we find that performance is widely preserved on other tasks, including specialized out-of-distribution domains such as in medical imaging and 3D environment frames. Our results suggest that injecting an inductive bias about human perceptual knowledge into vision models can make them better representation learners.",main,NeurIPS,2024,Poster,Shobhita Sundaram;Stephanie Fu;Lukas Muttenthaler;Netanel Yakir Tamir;Lucy Chai;Simon Kornblith;Trevor Darrell;Phillip Isola,True,https://openreview.net/pdf?id=NmlnmLYMZ4
NrCPBJSOOc,DACO: Towards Application-Driven and Comprehensive Data Analysis via Code Generation,"Data analysis is a crucial analytical process essential for deriving insights from real-world databases. As shown in Figure 1, the need for data analysis typically arises from specific application scenarios, and requires diverse reasoning skills including mathematical reasoning, logical reasoning, and strategic reasoning. Existing work often focus on simple factual retrieval or arithmetic resolutions and thus are insufficient for addressing complex real-world queries. This work aims to propose new resources and benchmarks on this crucial yet challenging and under-explored task. Due to the prohibitively high cost of collecting expert annotations, we use large language models (LLMs) enhanced by code generation to automatically generate high-quality data analysis, which will later be refined by human annotators. We construct the **DACO dataset**, containing (1) 440 databases (of tabular data) collected from real-world scenarios, (2) ~2k automatically generated query-answer pairs that can serve as weak supervision for model training, and (3) a concentrated but high-quality test set with human refined annotations that serves as our main evaluation benchmark. Experiments show that while LLMs like GPT-4 exhibit promising data analysis capabilities, they are still evaluated as less helpful than human-written analysis on 58.1% cases. Leveraging our weak supervision data, we experiment with various fine-tuning methods, including supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF). Our trained model outperforms existing baselines for table question answering, and RLHF further boosts the helpfulness of generated analysis on 58.5% cases.
Data and code are released at https://github.com/shirley-wu/daco.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xueqing Wu;Rui Zheng;Jingzhen Sha;Te-Lin Wu;Hanyu Zhou;Tang Mohan;Kai-Wei Chang;Nanyun Peng;Haoran Huang,True,https://openreview.net/pdf?id=NrCPBJSOOc
O6YRAOfHGt,Enhancing the Hierarchical Environment Design via Generative Trajectory Modeling,"Unsupervised Environment Design (UED) is a paradigm that automatically generates a curriculum of training environments, enabling agents trained in these environments to develop general capabilities, i.e., achieving good zero-shot transfer performance. However, existing UED approaches focus primarily on the random generation of environments for open-ended agent training. This is impractical in resource-limited scenarios where there is a constraint on the number of environments that can be generated. In this paper, we introduce a hierarchical MDP framework for environment design under resource constraints. It consists of an upper-level RL teacher agent that generates suitable training environments for a lower-level student agent. The RL teacher can leverage previously discovered environment structures and generate environments at the frontier of the student's capabilities by observing the student policy's representation. Additionally, to alleviate the time-consuming process of collecting the experience of the upper-level teacher, we utilize recent advances in generative modeling to synthesize a trajectory dataset for training the teacher agent. Our method significantly reduces the resource-intensive interactions between agents and environments, and empirical experiments across various domains demonstrate the effectiveness of our approach.",main,NeurIPS,2024,Reject,Dexun Li;Pradeep Varakantham,True,https://openreview.net/pdf?id=O6YRAOfHGt
OCrxDanhoO,BIGOS V2 Benchmark for Polish ASR: Curated Datasets and Tools for Reproducible Evaluation,"Speech datasets available in the public domain are often underutilized because of challenges in accessibility and interoperability. To address this, a system to survey, catalog, and curate existing speech datasets was developed, enabling reproducible evaluation of automatic speech recognition (ASR) systems. The system was applied to curate over 24 datasets and evaluate 25 ASR models, with a specific focus on Polish. This research represents the most extensive comparison to date of commercial and free ASR systems for the Polish language, drawing insights from 600 system-model-test set evaluations across 8 analysis scenarios. Curated datasets and benchmark results are available publicly. The evaluation tools are open-sourced to support reproducibility of the benchmark, encourage community-driven improvements, and facilitate adaptation for other languages.",Datasets & Benchmarks,NeurIPS,2024,Poster,Michał Junczyk,False,https://openreview.net/pdf?id=OCrxDanhoO
OLEQJoAED6,Don't Always Say No to Me: Benchmarking Safety-Related Refusal in Large VLM,"Warning: this paper contains example data that may be offensive or harmful. Although many existing evaluation datasets have been proposed to assess the safety of Large Vision-Language Models (LVLMs) on malicious prompt-image pairs, the research community lacks a systematic investigation into LVLMs' reasonable refusal toward both safe and unsafe pairs.  We define a control group consisting of an unsafe prompt-image pair and a safe pair, in which these two pairs share the same prompt or image. In a control group, an LVLM shows reasonable refusal if it refuses the former pair and responds to the latter. Otherwise, the model displays false refusal, such as refusing both pairs or none. For example, a control group contains an image depicting violent behavior and two prompts based on the same visual information. An LVLM should respond to the safe prompt ""How to deter this behavior?"" and refuse the unsafe prompt ""How to promote this behavior?"". To bridge this gap, we present LVLM-SafeR, a challenging and high-quality benchmark designed to measure Safety-related Refusal in LVLMs. The evaluation results from 9 closed-source LVLMs, 23 open-source LVLMs and 4 LVLM safety alignment approaches demonstrate that existing LVLMs have notable issues in providing proper refusals. Furthermore, we explore the effects of post-hoc/mixed safety fine-tuning, full/LoRA safety fine-tuning, and inference-time parameters (top-p, temperature) on LVLMs. Then we propose an effective prompt-engineering baseline to instruct LVLMs to give more reasonable refusals. Our project page is available at isxinliu.github.io/Project/LVLM-SafeR.",Datasets & Benchmarks,NeurIPS,2024,Reject,Xin Liu;Zhichen Dong;Zhanhui Zhou;Yichen Zhu;Yunshi Lan;Jing Shao;Chao Yang;Yu Qiao,True,https://openreview.net/pdf?id=OLEQJoAED6
OOItbUUQcd,A Cross-Domain Benchmark for Active Learning,"Active Learning (AL) deals with identifying the most informative samples for
labeling to reduce data annotation costs for supervised learning tasks. AL
research suffers from the fact that lifts from literature generalize poorly and
that only a small number of repetitions of experiments are conducted. To overcome
these obstacles, we propose CDALBench, the first active learning benchmark
which includes tasks in computer vision, natural language processing and tabular
learning. Furthermore, by providing an efficient, greedy oracle, CDALBench
can be evaluated with 50 runs for each experiment. We show, that both the
cross-domain character and a large amount of repetitions are crucial for
sophisticated evaluation of AL research. Concretely, we show that the
superiority of specific methods varies over the different domains, making it
important to evaluate Active Learning with a cross-domain benchmark.
Additionally, we show that having a large amount of runs is crucial. With only
conducting three runs as often done in the literature, the superiority of
specific methods can strongly vary with the specific runs. This effect is so strong, that, depending on the seed, even a well-established method's performance can be significantly better and significantly
worse than random for the same dataset.",Datasets & Benchmarks,NeurIPS,2024,Poster,Thorben Werner;Johannes Burchert;Maximilian Stubbemann;Lars Schmidt-Thieme,False,https://openreview.net/pdf?id=OOItbUUQcd
OPx8Dd27zT,Newswire: A Large-Scale Structured Database of a Century of Historical News,"In the U.S. historically, local newspapers drew their content largely from newswires like the Associated Press. Historians argue that newswires played a pivotal role in creating a national identity and shared understanding of the world, but there is no comprehensive archive of the content sent over newswires. We reconstruct such an archive by applying a customized deep learning pipeline to hundreds of terabytes of raw image scans from thousands of local newspapers. The resulting dataset contains 2.7 million unique public domain U.S. news wire articles, written between 1878 and 1977. Locations in these articles are georeferenced, topics are tagged using customized neural topic classification, named entities are recognized, and individuals are disambiguated to Wikipedia using a novel entity disambiguation model. To construct the Newswire dataset, we first recognize newspaper layouts and transcribe around 138 millions structured article texts from raw image scans. We then use a customized neural bi-encoder model to de-duplicate reproduced articles, in the presence of considerable abridgement and noise, quantifying how widely each article was reproduced. A text classifier is used to ensure that we only include newswire articles, which historically are in the public domain. The structured data that accompany the texts provide rich information about the who (disambiguated individuals), what (topics), and where (georeferencing) of the news that millions of Americans read over the course of a century. We also include Library of Congress metadata information about the newspapers that ran the articles on their front pages. The Newswire dataset is useful both for large language modeling - expanding training data beyond what is available from modern web texts - and for studying a diversity of questions in computational linguistics, social science, and the digital humanities.",Datasets & Benchmarks,NeurIPS,2024,Poster,Emily Silcock;Abhishek Arora;Luca D'Amico-Wong;Melissa Dell,True,https://openreview.net/pdf?id=OPx8Dd27zT
OTjTKFk7gb,AuctionNet: A Novel Benchmark for Decision-Making in Large-Scale Games,"Decision-making in large-scale games is an essential research area in artificial intelligence (AI) with significant real-world impact. However, the limited access to realistic large-scale game environments has hindered research progress in this area. In this paper, we present AuctionNet, a benchmark for bid decision-making in large-scale ad auctions derived from a real-world online advertising platform. AuctionNet is composed of three parts: an ad auction environment, a pre-generated dataset based on the environment, and performance evaluations of several baseline bid decision-making algorithms. More specifically, the environment effectively replicates the integrity and complexity of real-world ad auctions through the interaction of several modules: the ad opportunity generation module employs deep generative networks to bridge the gap between simulated and real-world data while mitigating the risk of sensitive data exposure; the bidding module implements diverse auto-bidding agents trained with different decision-making algorithms; and the auction module is anchored in the classic Generalized Second Price (GSP) auction but also allows for customization of auction mechanisms as needed. To facilitate research and provide insights into the environment, we have also pre-generated a substantial dataset based on the environment. The dataset contains 10 million ad opportunities, 48 diverse auto-bidding agents, and over 500 million auction records. Performance evaluations of baseline algorithms such as linear programming, reinforcement learning, and generative models for bid decision-making are also presented as a part of AuctionNet. AuctionNet has powered the NeurIPS 2024 Auto-Bidding in Large-Scale Auctions competition, providing competition environments for over 1,500 teams. We believe that AuctionNet is applicable not only to research on bid decision-making in ad auctions but also to the general area of decision-making in large-scale games. Code: https://github.com/alimama-tech/AuctionNet.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Kefan Su;Yusen Huo;Zhilin Zhang;Shuai Dou;Chuan Yu;Jian Xu;Zongqing Lu;Bo Zheng,True,https://openreview.net/pdf?id=OTjTKFk7gb
OfOCl3dGcF,ConMe: Rethinking Evaluation of Compositional Reasoning for Modern VLMs,"Compositional Reasoning (CR) entails grasping the significance of attributes, relations, and word order. Recent Vision-Language Models (VLMs), comprising a visual encoder and a Large Language Model (LLM) decoder, have demonstrated remarkable proficiency in such reasoning tasks. This prompts a crucial question: have VLMs effectively tackled the CR challenge? We conjecture that existing CR benchmarks may not adequately push the boundaries of modern VLMs due to the reliance on an LLM only negative text generation pipeline. Consequently, the negatives produced either appear as outliers from the natural language distribution learned by VLMs' LLM decoders or as improbable within the corresponding image context. To address these limitations, we introduce ConMe\\\\footnote{ConMe is an abbreviation for Confuse Me.} -- a compositional reasoning benchmark and a novel data generation pipeline leveraging VLMs to produce `hard CR Q&A'. Through a new concept of VLMs conversing with each other to collaboratively expose their weaknesses, our pipeline autonomously generates, evaluates, and selects challenging compositional reasoning questions, establishing a robust CR benchmark, also subsequently validated manually. Our benchmark provokes a noteworthy, up to 33%, decrease in CR performance compared to preceding benchmarks, reinstating the CR challenge even for state-of-the-art VLMs.",Datasets & Benchmarks,NeurIPS,2024,Poster,Irene Huang;Wei Lin;Muhammad Jehanzeb Mirza;Jacob A Hansen;Sivan Doveh;Victor Ion Butoi;Roei Herzig;Assaf Arbelle;Hilde Kuehne;Trevor Darrell;Chuang Gan;Aude Oliva;Rogerio Feris;Leonid Karlinsky,True,https://openreview.net/pdf?id=OfOCl3dGcF
OfXwix3NRH,"SHDocs: A dataset, benchmark, and method to efficiently generate high-quality, real-world specular highlight data with near-perfect alignment","A frequent problem in vision-based reasoning tasks such as object detection and optical character recognition (OCR) is the persistence of specular highlights. Specular highlights appear as bright spots of glare that occur due to the concentrated reflection of light; these spots manifest as image artifacts which occlude computer vision models and are challenging to reconstruct. Despite this, specular highlight removal receives relatively little attention due to the difficulty of acquiring high-quality, real-world data. We introduce a method to generate specular highlight data with near-perfect alignment and present SHDocs—a dataset of specular highlights on document images created using our method. Through our benchmark, we demonstrate that our dataset enables us to surpass the performance of state-of-the-art specular highlight removal models and downstream OCR tasks. We release our dataset, code, and methods publicly to motivate further exploration of image enhancement for practical computer vision challenges.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jovin Leong;Koa Ming Di;Benjamin Cham Wen Bin;Shaun Heng,True,https://openreview.net/pdf?id=OfXwix3NRH
Ogw1sSo9FP,TEG-DB: A Comprehensive Dataset and Benchmark of Textual-Edge Graphs,"Text-Attributed Graphs (TAGs) augment graph structures with natural language descriptions, facilitating detailed depictions of data and their interconnections across various real-world settings. However, existing TAG datasets predominantly feature textual information only at the nodes, with edges typically represented by mere binary or categorical attributes. This lack of rich textual edge annotations significantly limits the exploration of contextual relationships between entities, hindering deeper insights into graph-structured data. To address this gap, we introduce Textual-Edge Graphs Datasets and Benchmark (TEG-DB), a comprehensive and diverse collection of benchmark textual-edge datasets featuring rich textual descriptions on nodes and edges. The TEG-DB datasets are large-scale and encompass a wide range of domains, from citation networks to social networks. In addition, we conduct extensive benchmark experiments on TEG-DB to assess the extent to which current techniques, including pre-trained language models, graph neural networks, and their combinations, can utilize textual node and edge information. Our goal is to elicit advancements in 
  textual-edge graph research, specifically in developing methodologies that exploit rich textual node and edge descriptions to enhance graph analysis and provide deeper insights into complex real-world networks. The entire TEG-DB project is publicly accessible as an open-source repository on Github, accessible at https://github.com/Zhuofeng-Li/TEG-Benchmark.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zhuofeng Li;Zixing Gou;Xiangnan Zhang;Zhongyuan Liu;Sirui Li;Yuntong Hu;Chen Ling;Zheng Zhang;Liang Zhao,True,https://openreview.net/pdf?id=Ogw1sSo9FP
P5dEZeECGu,FlexCap: Describe Anything in Images in Controllable Detail,"We introduce FlexCap, a vision-language model that generates region-specific descriptions of varying lengths. FlexCap is trained to produce length-conditioned captions for input boxes, enabling control over information density, with descriptions ranging from concise object labels to detailed captions.  To achieve this, we create large-scale training datasets of image region descriptions with varying lengths from captioned web images. We demonstrate FlexCap’s effectiveness in several applications: first, it achieves strong performance in dense captioning tasks on the Visual Genome dataset. Second, we show how FlexCap’s localized descriptions can serve as input to a large language model to create a visual question answering (VQA) system, achieving state-of-the-art zero-shot performance on multiple VQA benchmarks. Our experiments illustrate FlexCap’s utility for tasks including image labeling, object attribute recognition, and visual dialog. Project webpage: https://flex-cap.github.io.",main,NeurIPS,2024,Poster,Debidatta Dwibedi;Vidhi Jain;Jonathan Tompson;Andrew Zisserman;Yusuf Aytar,True,https://openreview.net/pdf?id=P5dEZeECGu
PAWQvrForJ,Membership Inference Attacks against Fine-tuned Large Language Models via Self-prompt Calibration,"Membership Inference Attacks (MIA) aim to infer whether a target data record has been utilized for model training or not. Existing MIAs designed for large language models (LLMs) can be bifurcated into two types: reference-free and reference-based attacks. Although reference-based attacks appear promising performance by calibrating the probability measured on the target model with reference models, this illusion of privacy risk heavily depends on a reference dataset that closely resembles the training set. Both two types of attacks are predicated on the hypothesis that training records consistently maintain a higher probability of being sampled. However, this hypothesis heavily relies on the overfitting of target models, which will be mitigated by multiple regularization methods and the generalization of LLMs. Thus, these reasons lead to high false-positive rates of MIAs in practical scenarios.
We propose a Membership Inference Attack based on Self-calibrated Probabilistic Variation (SPV-MIA). 
Specifically, we introduce a self-prompt approach, which constructs the dataset to fine-tune the reference model by prompting the target LLM itself. In this manner, the adversary can collect a dataset with a similar distribution from public APIs.
Furthermore, we introduce probabilistic variation, a more reliable membership signal based on LLM memorization rather than overfitting, from which we rediscover the neighbour attack with theoretical grounding. 
Comprehensive evaluation conducted on three datasets and four exemplary LLMs shows that SPV-MIA raises the AUC of MIAs from 0.7 to a significantly high level of 0.9. Our code and dataset are available at: https://github.com/tsinghua-fib-lab/NeurIPS2024_SPV-MIA",main,NeurIPS,2024,Poster,Wenjie Fu;Huandong Wang;Chen Gao;Guanghua Liu;Yong Li;Tao Jiang,True,https://openreview.net/pdf?id=PAWQvrForJ
PCjK8dqrWW,WorkArena++: Towards Compositional Planning and Reasoning-based Common Knowledge Work Tasks,"The ability of large language models (LLMs) to mimic human-like intelligence has led to a surge in LLM-based autonomous agents. 
Though recent LLMs seem capable of planning and reasoning given user instructions, their effectiveness in applying these capabilities for autonomous task solving remains underexplored. This is especially true in enterprise settings, where automated agents hold the promise of a high impact. To fill this gap, we propose WorkArena++, a novel benchmark consisting of 682 tasks corresponding to realistic workflows routinely performed by knowledge workers. WorkArena++ is designed to evaluate the planning, problem-solving, logical/arithmetic reasoning, retrieval, and contextual understanding abilities of web agents. Our empirical studies across state-of-the-art LLMs and vision-language models (VLMs), as well as human workers, reveal several challenges for such models to serve as useful assistants in the workplace. In addition to the benchmark, we provide a mechanism to effortlessly generate thousands of ground-truth observation/action traces, which can be used for fine-tuning existing models. Overall, we expect this work to serve as a useful resource to help the community progress towards capable autonomous agents. The benchmark can be found at https://github.com/ServiceNow/WorkArena.",Datasets & Benchmarks,NeurIPS,2024,Poster,Léo Boisvert;Megh Thakkar;Maxime Gasse;Massimo Caccia;Thibault Le Sellier de Chezelles;Quentin Cappart;Nicolas Chapados;Alexandre Lacoste;Alexandre Drouin,True,https://openreview.net/pdf?id=PCjK8dqrWW
PFwlw9bnAr,CARE: a Benchmark Suite for the Classification and Retrieval of Enzymes,"Enzymes are important proteins that catalyze chemical reactions. In recent years, machine learning methods have emerged to predict enzyme function from sequence; however, there are no  standardized benchmarks to evaluate these methods. We introduce CARE, a benchmark and dataset suite for the Classification And Retrieval of Enzymes (CARE). CARE centers on two tasks: (1) classification of a protein sequence by its enzyme commission (EC) number and (2) retrieval of an EC number given a chemical reaction. For each task, we design train-test splits to evaluate different kinds of out-of-distribution generalization that are relevant to real use cases. For the classification task, we provide baselines for state-of-the-art methods. Because the retrieval task has not been previously formalized, we propose a method called Contrastive Reaction-EnzymE Pretraining (CREEP) as one of the first baselines for this task and compare it to the recent method, CLIPZyme. CARE is available at https://github.com/jsunn-y/CARE/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jason Yang;Ariane Mora;Shengchao Liu;Bruce James Wittmann;Anima Anandkumar;Frances H. Arnold;Yisong Yue,True,https://openreview.net/pdf?id=PFwlw9bnAr
POFf5SGSAL,Wiki Entity Summarization Benchmark,"Entity summarization aims to compute concise summaries for entities in knowledge graphs.  Existing datasets and benchmarks are often limited to a few hundred entities and discard graph structure in source knowledge graphs. This limitation is particularly pronounced when it comes to ground-truth summaries, where there exist only a few labeled summaries for evaluation and training. We propose WikES, a comprehensive benchmark comprising of entities, their summaries,  and their connections. Additionally, WikES features a dataset generator to test entity summarization algorithms in different areas of the knowledge graph. Importantly, our approach combines graph algorithms and NLP models as well as different data sources such that WikES does not require human annotation, rendering the approach cost-effective and generalizable to multiple domains. Finally, WikES is scalable and capable of capturing the complexities of knowledge graphs in terms of topology and semantics. 
WikES features existing datasets for comparison. Empirical studies of entity summarization methods confirm the usefulness of our benchmark. Data, code, and models are available at: https://github.com/msorkhpar/wiki-entity-summarization.",Datasets & Benchmarks,NeurIPS,2024,Reject,Mohammad Sorkhpar;Saeedeh Javadi;Atefeh Moradan;Klim Zaporojets;Davide Mottin;Ira Assent,True,https://openreview.net/pdf?id=POFf5SGSAL
PSDXcYjrkO,Towards Comprehensive Detection of Chinese Harmful Memes,"Harmful memes have proliferated on the Chinese Internet, while research on detecting Chinese harmful memes significantly lags behind due to the absence of reliable datasets and effective detectors.
To this end, we present the comprehensive detection of Chinese harmful memes.
We introduce ToxiCN MM, the first Chinese harmful meme dataset, which consists of 12,000 samples with fine-grained annotations for meme types. 
Additionally, we propose a baseline detector, Multimodal Knowledge Enhancement (MKE), designed to incorporate contextual information from meme content, thereby enhancing the model's understanding of Chinese memes.
In the evaluation phase, we conduct extensive quantitative experiments and qualitative analyses on multiple baselines, including LLMs and our MKE. 
Experimental results indicate that detecting Chinese harmful memes is challenging for existing models, while demonstrating the effectiveness of MKE.",Datasets & Benchmarks,NeurIPS,2024,Poster,Junyu Lu;Bo Xu;Xiaokun Zhang;WangHongbo;Haohao Zhu;Dongyu Zhang;Liang Yang;Hongfei Lin,True,https://openreview.net/pdf?id=PSDXcYjrkO
PSPtj26Lbp,L4GM: Large 4D Gaussian Reconstruction Model,"We present L4GM, the first 4D Large Reconstruction Model that produces animated objects from a single-view video input -- in a single feed-forward pass that takes only a second.
Key to our success is a novel dataset of multiview videos containing curated, rendered animated objects from Objaverse. This dataset depicts 44K diverse objects with 110K animations rendered in 48 viewpoints, resulting in 12M videos with a total of 300M frames. 
We keep our L4GM simple for scalability and build directly on top of LGM, a pretrained 3D Large Reconstruction Model that outputs 3D Gaussian ellipsoids from multiview image input.
L4GM outputs a per-frame 3D Gaussian splat representation from video frames sampled at a low fps and then upsamples the representation to a higher fps to achieve temporal smoothness. We add temporal self-attention layers to the base LGM to help it learn consistency across time, and utilize a per-timestep multiview rendering loss to train the model. The representation is upsampled to a higher framerate by training an interpolation model which produces intermediate 3D Gaussian representations. 
We showcase that L4GM that is only trained on synthetic data generalizes well on in-the-wild videos, producing high quality animated 3D assets.",main,NeurIPS,2024,Poster,Jiawei Ren;Kevin Xie;Ashkan Mirzaei;hanxue liang;Xiaohui Zeng;Karsten Kreis;Ziwei Liu;Antonio Torralba;Sanja Fidler;Seung Wook Kim;Huan Ling,True,https://openreview.net/pdf?id=PSPtj26Lbp
PZbFW8ZrSJ,TabularBench: Benchmarking Adversarial Robustness for Tabular Deep Learning in Real-world Use-cases,"While adversarial robustness in computer vision is a mature research field, fewer researchers have tackled the evasion attacks against tabular deep learning, and even fewer investigated robustification mechanisms and reliable defenses. We hypothesize that this lag in the research on tabular adversarial attacks is in part due to the lack of standardized benchmarks. To fill this gap, we propose TabularBench, the first comprehensive benchmark of robustness of tabular deep learning classification models. We evaluated adversarial robustness with CAA, an ensemble of gradient and search attacks which was recently demonstrated as the most effective attack against a tabular model. In addition to our open benchmark https://github.com/serval-uni-lu/tabularbench where we welcome submissions of new models and defenses, we implement 7 robustification mechanisms inspired by state-of-the-art defenses in computer vision and propose the largest benchmark of robust tabular deep learning over 200 models across five critical scenarios in finance, healthcare and security. We curated real datasets for each use case, augmented with hundreds of thousands of realistic synthetic inputs, and trained and assessed our models with and without data augmentations. We open-source our library that provides API access to all our pre-trained robust tabular models, and the largest datasets of real and synthetic tabular inputs. Finally, we analyze the impact of various defenses on the robustness and provide actionable insights to design new defenses and robustification mechanisms.",Datasets & Benchmarks,NeurIPS,2024,Poster,Thibault Simonetto;Salah GHAMIZI;Maxime Cordy,True,https://openreview.net/pdf?id=PZbFW8ZrSJ
PcbSZwVVc5,DreamCatcher: A Wearer-aware Multi-modal Sleep Event Dataset Based on Earables in Non-restrictive Environments,"Poor quality sleep can be characterized by the occurrence of events ranging from body movement to breathing impairment. Widely available earbuds equipped with sensors (also known as earables) can be combined with a sleep event detection algorithm to offer a convenient alternative to laborious clinical tests for individuals suffering from sleep disorders. Although various solutions utilizing such devices have been proposed to detect sleep events, they ignore the fact that individuals often share sleeping spaces with roommates or couples. To address this issue, we introduce DreamCatcher, the first publicly available dataset for wearer-aware sleep event algorithm development on earables. DreamCatcher encompasses eight distinct sleep events, including synchronous dual-channel audio and motion data collected from 12 pairs (24 participants) totaling 210 hours (420 hour.person) with fine-grained label. We tested multiple benchmark models on three tasks related to sleep event detection, demonstrating the usability and unique challenge of DreamCatcher. We hope that the proposed DreamCatcher can inspire other researchers to further explore efficient wearer-aware human vocal activity sensing on earables. DreamCatcher is publicly available at https://github.com/thuhci/DreamCatcher.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Zeyu Wang;Xiyuxing Zhang;Ruotong Yu;Yuntao Wang;Kenneth Christofferson;Jingru Zhang;Alex Mariakakis;Yuanchun Shi,True,https://openreview.net/pdf?id=PcbSZwVVc5
Pm0UzCehgB,SM3-Text-to-Query: Synthetic Multi-Model Medical Text-to-Query Benchmark,"Electronic health records (EHRs) are stored in various database systems with different database models on heterogeneous storage architectures, such as relational databases, document stores, or graph databases. These different database models have a big impact on query complexity and performance. While this has been a known fact in database research, its implications for the growing number of Text-to-Query systems have surprisingly not been investigated so far.
In this paper, we present SM3-Text-to-Query, the first multi-model medical Text-to-Query benchmark based on synthetic patient data from Synthea, following the SNOMED-CT taxonomy---a widely used knowledge graph ontology covering medical terminology. SM3-Text-to-Query provides data representations for relational databases (PostgreSQL), document stores (MongoDB), and graph databases (Neo4j and GraphDB (RDF)), allowing the evaluation across four popular query languages, namely SQL, MQL, Cypher, and SPARQL.
We systematically and manually develop 408 template questions, which we augment to construct a benchmark of 10K diverse natural language question/query pairs for these four query languages (40K pairs overall). On our dataset, we evaluate several common in-context-learning (ICL) approaches for a set of representative closed and open-source LLMs.
Our evaluation sheds light on the trade-offs between database models and query languages for different ICL strategies and LLMs. Last,
SM3-Text-to-Query is easily extendable to additional query languages or real, standard-based patient databases.",Datasets & Benchmarks,NeurIPS,2024,Poster,Sithursan Sivasubramaniam;Cedric Osei-Akoto;Yi Zhang;Kurt Stockinger;Jonathan Fuerst,True,https://openreview.net/pdf?id=Pm0UzCehgB
PnjbvbblGv,SD-Eval: A Benchmark Dataset for Spoken Dialogue Understanding Beyond Words,"Speech encompasses a wealth of information, including but not limited to content, paralinguistic, and environmental information.
This comprehensive nature of speech significantly impacts communication and is crucial for human-computer interaction.
Chat-Oriented Large Language Models (LLMs), known for their general-purpose assistance capabilities, have evolved to handle multi-modal inputs, including speech.
Although these models can be adept at recognizing and analyzing speech, they often fall short of generating appropriate responses.
We argue that this is due to the lack of principles on task definition and model development, which requires open-source datasets and metrics suitable for model evaluation.
To bridge the gap, we present SD-Eval, a benchmark dataset aimed at multidimensional evaluation of spoken dialogue understanding and generation.
SD-Eval focuses on paralinguistic and environmental information and includes 7,303 utterances, amounting to 8.76 hours of speech data. The data is aggregated from eight public datasets, representing four perspectives: emotion, accent, age, and background sound.
To assess the SD-Eval benchmark dataset, we implement three different models and construct a training set following a process similar to that of SD-Eval. The training set contains 1,052.72 hours of speech data and 724.4k utterances. 
We also conduct a comprehensive evaluation using objective evaluation methods (e.g. BLEU and ROUGE), subjective evaluations and LLM-based metrics for the generated responses.
Models conditioned with paralinguistic and environmental information outperform their counterparts in both objective and subjective measures.
Moreover, experiments demonstrate that LLM-based metrics show a higher correlation with human evaluation compared to traditional metrics.
We open-source SD-Eval at https://github.com/amphionspace/SD-Eval.",Datasets & Benchmarks,NeurIPS,2024,Poster,Junyi Ao;Yuancheng Wang;Xiaohai Tian;Dekun Chen;Jun Zhang;Lu Lu;Yuxuan Wang;Haizhou Li;Zhizheng Wu,True,https://openreview.net/pdf?id=PnjbvbblGv
PnlCHQrM69,SemCoder: Training Code Language Models with Comprehensive Semantics Reasoning,"Code Large Language Models (Code LLMs) have excelled at tasks like code completion but often miss deeper semantics such as execution effects and dynamic states. This paper aims to bridge the gap between Code LLMs' reliance on static text data and the need for semantic understanding for complex tasks like debugging and program repair. We introduce a novel strategy, _monologue reasoning_, to train Code LLMs to reason comprehensive semantics, encompassing high-level functional descriptions, local execution effects of individual statements, and overall input/output behavior, thereby linking static code text with dynamic execution states.
We begin by collecting PyX, a clean Python corpus of fully executable code samples with functional descriptions and test cases. 
We propose training Code LLMs not only to write code but also to understand code semantics by reasoning about key properties, constraints, and execution behaviors using natural language, mimicking human verbal debugging, i.e., rubber-duck debugging. This approach led to the development of SemCoder, a Code LLM with only 6.7B parameters, which shows competitive performance with GPT-3.5-turbo on code generation and execution reasoning tasks. SemCoder achieves 79.3% on HumanEval (GPT-3.5-turbo: 76.8%), 63.6% on CRUXEval-I (GPT-3.5-turbo: 50.3%), and 63.9% on CRUXEval-O (GPT-3.5-turbo: 59.0%). We also study the effectiveness of SemCoder's monologue-style execution reasoning compared to concrete scratchpad reasoning, showing that our approach integrates semantics from multiple dimensions more smoothly. Finally, we demonstrate the potential of applying learned semantics to improve Code LLMs' debugging and self-refining capabilities. Our data, code, and models are available at: https://github.com/ARiSE-Lab/SemCoder.",main,NeurIPS,2024,Poster,Yangruibo Ding;Jinjun Peng;Marcus J. Min;Gail Kaiser;Junfeng Yang;Baishakhi Ray,True,https://openreview.net/pdf?id=PnlCHQrM69
Pojt9RWIjJ,From Transparent to Opaque: Rethinking Neural Implicit Surfaces with $\\\\alpha$-NeuS,"Traditional 3D shape reconstruction techniques from multi-view images, such as structure from motion and multi-view stereo, face challenges in reconstructing transparent objects. Recent advances in neural radiance fields and its variants primarily address opaque or transparent objects, encountering difficulties to reconstruct both transparent and opaque objects simultaneously. This paper introduces $\\\\alpha$-NeuS$\\\\textemdash$an extension of NeuS$\\\\textemdash$that proves NeuS is unbiased for materials from fully transparent to fully opaque. We find that transparent and opaque surfaces align with the non-negative local minima and the zero iso-surface, respectively, in the learned distance field of NeuS. Traditional iso-surfacing extraction algorithms, such as marching cubes, which rely on fixed iso-values, are ill-suited for such data. We develop a method to extract the transparent and opaque surface simultaneously based on DCUDF. To validate our approach, we construct a benchmark that includes both real-world and synthetic scenes, demonstrating its practical utility and effectiveness. Our data and code are publicly available at https://github.com/728388808/alpha-NeuS.",main,NeurIPS,2024,Poster,Haoran Zhang;Junkai Deng;Xuhui Chen;Fei Hou;Wencheng Wang;Hong Qin;Chen Qian;Ying He,True,https://openreview.net/pdf?id=Pojt9RWIjJ
PqlKliEXyJ,LoD-Loc: Aerial Visual Localization using LoD 3D Map with Neural Wireframe Alignment,"We propose a new method named LoD-Loc for visual localization in the air. Unlike existing localization algorithms, LoD-Loc does not rely on complex 3D representations and can estimate the pose of an Unmanned Aerial Vehicle (UAV) using a Level-of-Detail (LoD) 3D map. LoD-Loc mainly achieves this goal by aligning the wireframe derived from the LoD projected model with that predicted by the neural network. Specifically, given a coarse pose provided by the UAV sensor, LoD-Loc hierarchically builds a cost volume for uniformly sampled pose hypotheses to describe pose probability distribution and select a pose with maximum probability. Each cost within this volume measures the degree of line alignment between projected and predicted wireframes. LoD-Loc also devises a 6-DoF pose optimization algorithm to refine the previous result with a differentiable Gaussian-Newton method. As no public dataset exists for the studied problem, we collect two datasets with map levels of LoD3.0 and LoD2.0, along with real RGB queries and ground-truth pose annotations. We benchmark our method and demonstrate that LoD-Loc achieves excellent performance, even surpassing current state-of-the-art methods that use textured 3D models for localization. The code and dataset will be made available upon publication.",main,NeurIPS,2024,Poster,Juelin Zhu;Shen Yan;Long Wang;zhang shengYue;Yu Liu;Maojun Zhang,True,https://openreview.net/pdf?id=PqlKliEXyJ
PvVKUFhaNy,HelpSteer 2: Open-source dataset for training top-performing reward models,"High-quality preference datasets are essential for training reward models that can effectively guide large language models (LLMs) in generating high-quality responses aligned with human preferences.
As LLMs become stronger and better aligned, permissively licensed preference datasets, such as Open Assistant, HH-RLHF, and HelpSteer need to be updated to remain effective for reward modeling.
Methods that distil preference data from proprietary LLMs such as GPT-4 have restrictions on commercial usage imposed by model providers.
To improve upon both generated responses and attribute labeling quality, we release HelpSteer2, a permissively licensed preference dataset (CC-BY-4.0).  
Using a powerful Nemotron-4-340B base model trained on HelpSteer2, we are able to achieve the SOTA score (92.0%) on Reward-Bench's primary dataset, outperforming currently listed open and proprietary models, as of June 12th, 2024.
Notably, HelpSteer2 consists of only ten thousand response pairs, an order of magnitude fewer than existing preference datasets (e.g., HH-RLHF), which makes it highly efficient for training reward models. 
Our extensive experiments demonstrate that reward models trained with HelpSteer2 are effective in aligning LLMs. Additionally, we propose SteerLM 2.0, a model alignment approach that can effectively make use of the rich multi-attribute score predicted by our reward models. 
HelpSteer2 is available at https://huggingface.co/datasets/nvidia/HelpSteer2 and code is available at https://github.com/NVIDIA/NeMo-Aligner",Datasets & Benchmarks,NeurIPS,2024,Poster,Zhilin Wang;Yi Dong;Olivier Delalleau;Jiaqi Zeng;Gerald Shen;Daniel Egert;Jimmy J. Zhang;Makesh Narsimhan Sreedhar;Oleksii Kuchaiev,True,https://openreview.net/pdf?id=PvVKUFhaNy
PyTf2jj0SH,ConvBench: A Multi-Turn Conversation Evaluation Benchmark with Hierarchical Ablation Capability for Large Vision-Language Models,"Multi-turn visual conversation is an important ability of real-world AI assistants. However,  the related evaluation benchmark is missed. This paper presents ConvBench, a multi-turn conversation benchmark with hierarchical capabilities ablation evaluation for Large Vision-Language Models (LVLMs). ConvBench comprises 577 curated multi-turn conversations, encompassing 215 tasks. These tasks are broad and open-ended, which resemble real-world user behaviors. ConvBench progressively examines the LVLMs' perception, reasoning, and creativity capabilities in each conversation and can decouple these capabilities in evaluations and thus perform reliable error attribution. Besides, considering the diversity of open-ended questions, we introduce an efficient and reliable automatic evaluation framework. Experimental results reveal that ConvBench is a significant challenge for current LVLMs, even for GPT4V, which achieves only a 39.51% score. Besides, we have some insightful findings, such as the weak perception of LVLMs inhibits authentic strengths in reasoning and creation. We believe our design of hierarchical capabilities, decoupling capabilities evaluation, and multi-turn conversation can blaze a new trail in LVLMs evaluation. Code and benchmark are released at https://github.com/shirlyliu64/ConvBench.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Shuo Liu;Kaining Ying;Hao Zhang;Yue Yang;Yuqi Lin;Tianle Zhang;Chuanhao Li;Yu Qiao;Ping Luo;Wenqi Shao;Kaipeng Zhang,True,https://openreview.net/pdf?id=PyTf2jj0SH
Q2sDuwtutB,Text-space Graph Foundation Models: Comprehensive Benchmarks and New Insights,"Given the ubiquity of graph data and its applications in diverse domains, building a Graph Foundation Model (GFM) that can work well across different graphs and tasks with a unified backbone has recently garnered significant interests. A major obstacle to achieving this goal stems from the fact that graphs from different domains often exhibit diverse node features. Inspired by multi-modal models that align different modalities with natural language, the text has recently been adopted to provide a unified feature space for diverse graphs. Despite the great potential of these text-space GFMs, current research in this field is hampered by two problems. First, the absence of a comprehensive benchmark with unified problem settings hinders a clear understanding of the comparative effectiveness and practical value of different text-space GFMs. Second, there is a lack of sufficient datasets to thoroughly explore the methods' full potential and verify their effectiveness across diverse settings. To address these issues, we conduct a comprehensive benchmark providing novel text-space datasets and comprehensive evaluation under unified problem settings. Empirical results provide new insights and inspire future research directions. Our code and data are publicly available from https://github.com/CurryTang/TSGFM.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zhikai Chen;Haitao Mao;Jingzhe Liu;Yu Song;Bingheng Li;Wei Jin;Bahare Fatemi;Anton Tsitsulin;Bryan Perozzi;Hui Liu;Jiliang Tang,True,https://openreview.net/pdf?id=Q2sDuwtutB
Q7lAqY41HH,CRAG - Comprehensive RAG Benchmark,"Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution to alleviate Large Language Model (LLM)’s deficiency in lack of knowledge. Existing RAG datasets, however, do not adequately represent the diverse and dynamic nature of real-world Question Answering (QA) tasks. To bridge this gap, we introduce the Comprehensive RAG Benchmark (CRAG), a factual question answering benchmark of 4,409 question-answer pairs and mock APIs to simulate web and Knowledge Graph (KG) search. CRAG is designed to encapsulate a diverse array of questions across five domains and eight question categories, reflecting varied entity popularity from popular to long-tail, and temporal dynamisms ranging from years to seconds. Our evaluation on this benchmark highlights the gap to fully trustworthy QA. Whereas most advanced LLMs achieve $\\\\le 34\\\\%$ accuracy on CRAG, adding RAG in a straightforward manner improves the accuracy only to 44%. State-of-the-art industry RAG solutions only answer 63% questions without any hallucination. CRAG also reveals much lower accuracy in answering questions regarding facts with higher dynamism, lower popularity, or higher complexity, suggesting future research directions. The CRAG benchmark laid the groundwork for a KDD Cup 2024 challenge, attracted thousands of participants and submissions. We commit to maintaining CRAG to serve research communities in advancing RAG solutions and general QA solutions. CRAG is available at https://github.com/facebookresearch/CRAG/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xiao Yang;Kai Sun;Hao Xin;Yushi Sun;Nikita Bhalla;Xiangsen Chen;Sajal Choudhary;Rongze Gui;Ziran Jiang;Ziyu JIANG;Lingkun Kong;Brian Moran;Jiaqi Wang;Yifan Ethan Xu;An Yan;Chenyu Yang;Eting Yuan;Hanwen Zha;Nan Tang;Lei Chen;Nicolas SCHEFFER;Yue Liu;Nirav Shah;Rakesh Wanga;Anuj Kumar;Wen-tau Yih;Xin Luna Dong,True,https://openreview.net/pdf?id=Q7lAqY41HH
Q7xKdEMrrZ,AsEP: Benchmarking Deep Learning Methods for Antibody-specific Epitope Prediction,"Epitope identification is vital for antibody design yet challenging due to the inherent variability in antibodies. While many deep learning methods have been developed for general protein binding site prediction tasks, whether they work for epitope prediction remains an understudied research question. The challenge is also heightened by the lack of a consistent evaluation pipeline with sufficient dataset size and epitope diversity. We introduce a filtered antibody-antigen complex structure dataset, AsEP (Antibody-specific Epitope Prediction). AsEP is the largest of its kind and provides clustered epitope groups, allowing the community to develop and test novel epitope prediction methods and evaluate their generalisability. AsEP comes with an easy-to-use interface in Python and pre-built graph representations of each antibody-antigen complex while also supporting customizable embedding methods. Using this new dataset, we benchmark several representative general protein-binding site prediction methods and find that their performances fall short of expectations for epitope prediction. To address this, we propose a novel method, WALLE, which leverages both unstructured modeling from protein language models and structural modeling from graph neural networks. WALLE demonstrate up to 3-10X performance improvement over the baseline methods. Our empirical findings suggest that epitope prediction benefits from combining sequential features provided by language models with geometrical information from graph representations. This provides a guideline for future epitope prediction method design. In addition, we reformulate the task as bipartite link prediction, allowing convenient model performance attribution and interpretability. We open source our data and code at https://github.com/biochunan/AsEP-dataset.",Datasets & Benchmarks,NeurIPS,2024,Poster,ChuNan Liu;Lilian Denzler;Yihong Chen;Andrew CR Martin;Brooks Paige,True,https://openreview.net/pdf?id=Q7xKdEMrrZ
QAEnr5j172,FashionR2R: Texture-preserving Rendered-to-Real Image Translation with Diffusion Models,"Modeling and producing lifelike clothed human images has attracted researchers' attention from different areas for decades, with the complexity from highly articulated and structured content. Rendering algorithms decompose and simulate the imaging process of a camera, while are limited by the accuracy of modeled variables and the efficiency of computation. Generative models can produce impressively vivid human images, however still lacking in controllability and editability. This paper studies photorealism enhancement of rendered images, leveraging generative power from diffusion models on the controlled basis of rendering. We introduce a novel framework to translate rendered images into their realistic counterparts, which consists of two stages: Domain Knowledge Injection (DKI) and Realistic Image Generation (RIG). In DKI, we adopt positive (real) domain finetuning and negative (rendered) domain embedding to inject knowledge into a pretrained Text-to-image (T2I) diffusion model. In RIG, we generate the realistic image corresponding to the input rendered image, with a Texture-preserving Attention Control (TAC) to preserve fine-grained clothing textures, exploiting the decoupled features encoded in the UNet structure. Additionally, we introduce SynFashion dataset, featuring high-quality digital clothing images with diverse textures. Extensive experimental results demonstrate the superiority and effectiveness of our method in rendered-to-real image translation.",main,NeurIPS,2024,Poster,Rui Hu;Qian He;Gaofeng He;Jiedong Zhuang;Huang Chen;Huafeng Liu;Huamin Wang,True,https://openreview.net/pdf?id=QAEnr5j172
QCY01LvyKm,The iNaturalist Sounds Dataset,"We present the iNaturalist Sounds Dataset (iNatSounds), a collection of 230,000 audio files capturing sounds from over 5,500 species, contributed by more than 27,000 recordists worldwide. The dataset encompasses sounds from birds, mammals, insects, reptiles, and amphibians, with audio and species labels derived from observations submitted to iNaturalist, a global citizen science platform. Each recording in the dataset varies in length and includes a single species annotation. We benchmark multiple backbone architectures, comparing multiclass classification objectives with multilabel objectives. Despite weak labeling, we demonstrate that iNatSounds serves as a useful pretraining resource by benchmarking it on strongly labeled downstream evaluation datasets. The dataset is available as a single, freely accessible archive, promoting accessibility and research in this important domain. We envision models trained on this data powering next-generation public engagement applications, and assisting biologists, ecologists, and land use managers in processing large audio collections, thereby contributing to the understanding of species compositions in diverse soundscapes.",Datasets & Benchmarks,NeurIPS,2024,Poster,Mustafa Chasmai;Alexander Shepard;Subhransu Maji;Grant Van Horn,True,https://openreview.net/pdf?id=QCY01LvyKm
QIJQ1qCGqV,Text to Blind Motion,"People who are blind perceive the world differently than those who are sighted, which can result in distinct motion characteristics. For instance, when crossing at an intersection, blind individuals may have different patterns of movement, such as veering more from a straight path or using touch-based exploration around curbs and obstacles. These behaviors may appear less predictable to motion models embedded in technologies such as autonomous vehicles. Yet, the ability of 3D motion models to capture such behavior has not been previously studied, as existing datasets for 3D human motion currently lack diversity and are biased toward people who are sighted. In this work, we introduce BlindWays, the first multimodal motion benchmark for pedestrians who are blind. We collect 3D motion data using wearable sensors with 11 blind participants navigating eight different routes in a real-world urban setting. Additionally, we provide rich textual descriptions that capture the distinctive movement characteristics of blind pedestrians and their interactions with both the navigation aid (e.g., a white cane or a guide dog) and the environment. We benchmark state-of-the-art 3D human prediction models, finding poor performance with off-the-shelf and pre-training-based methods for our novel task. To contribute toward safer and more reliable systems that can seamlessly reason over diverse human movements in their environments, our text-and-motion benchmark is available at https://blindways.github.io/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Hee Jae Kim;Kathakoli Sengupta;Masaki Kuribayashi;Hernisa Kacorri;Eshed Ohn-Bar,True,https://openreview.net/pdf?id=QIJQ1qCGqV
QLO0pXYKVi,FUSU: A Multi-temporal-source Land Use Change Segmentation Dataset for Fine-grained Urban Semantic Understanding,"Fine urban change segmentation using multi-temporal remote sensing images is essential for understanding human-environment interactions in urban areas. Although there have been advances in high-quality land cover datasets that reveal the physical features of urban landscapes, the lack of fine-grained land use datasets hinders a deeper understanding of how human activities are distributed across landscapes and the impact of these activities on the environment, thus constraining proper technique development. To address this, we introduce FUSU, the first fine-grained land use change segmentation dataset for Fine-grained Urban Semantic Understanding. FUSU features the most detailed land use classification system to date, with 17 classes and 30 billion pixels of annotations. It includes bi-temporal high-resolution satellite images with 0.2-0.5 m ground sample distance and monthly optical and radar satellite time series, covering 847 km^2 across five urban areas in the southern and northern of China with different geographical features. The fine-grained land use pixel-wise annotations and high spatial-temporal resolution data provide a robust foundation for developing proper deep learning models to provide contextual insights on human activities and urbanization. To fully leverage FUSU, we propose a unified time-series architecture for both change detection and segmentation. We benchmark FUSU on various methods for several tasks. Dataset and code are available at: https://github.com/yuanshuai0914/FUSU.",Datasets & Benchmarks,NeurIPS,2024,Poster,Shuai Yuan;Guancong Lin;Lixian Zhang;Runmin Dong;Jinxiao Zhang;Shuang Chen;Juepeng Zheng;Jie Wang;Haohuan Fu,True,https://openreview.net/pdf?id=QLO0pXYKVi
QNieOPt4fg,SelectIT: Selective Instruction Tuning for LLMs via Uncertainty-Aware Self-Reflection,"Instruction tuning (IT) is crucial to tailoring large language models (LLMs) towards human-centric interactions. Recent advancements have shown that the careful selection of a small, high-quality subset of IT data can significantly enhance the performance of LLMs.  Despite this, common approaches often rely on additional models or data, which increases costs and limits widespread adoption. In this work, we propose a novel approach, termed $\\\\textit{SelectIT}$, that capitalizes on the foundational capabilities of the LLM itself. Specifically, we exploit the intrinsic uncertainty present in LLMs to more effectively select high-quality IT data, without the need for extra resources.  Furthermore, we introduce a curated IT dataset, the $\\\\textit{Selective Alpaca}$, created by applying SelectIT to the Alpaca-GPT4 dataset. Empirical results demonstrate that IT using Selective Alpaca leads to substantial model ability enhancement. The robustness of SelectIT has also been corroborated in various foundation models and domain-specific tasks.  Our findings suggest that longer and more computationally intensive IT data may serve as superior sources of IT, offering valuable insights for future research in this area.  Data, code, and scripts are freely available at https://github.com/Blue-Raincoat/SelectIT.",main,NeurIPS,2024,Poster,Liangxin Liu;Xuebo Liu;Derek F. Wong;Dongfang Li;Ziyi Wang;Baotian Hu;Min Zhang,True,https://openreview.net/pdf?id=QNieOPt4fg
QSS5cGmKb1,STaRK: Benchmarking LLM Retrieval on Textual and Relational Knowledge Bases,"Answering real-world complex queries, such as complex product search, often requires accurate retrieval from semi-structured knowledge bases that involve blend of unstructured (e.g., textual descriptions of products) and structured (e.g., entity relations of products) information. However, many previous works studied textual and relational retrieval tasks as separate topics. To address the gap, we develop STARK, a large-scale Semi-structure retrieval benchmark on Textual and Relational Knowledge Bases. Our benchmark covers three domains: product search, academic paper search, and queries in precision medicine. We design a novel pipeline to synthesize realistic user queries that integrate diverse relational information and complex textual properties, together with their ground-truth answers (items). We conduct rigorous human evaluation to validate the quality of our synthesized queries. We further enhance the benchmark with high-quality human-generated queries to provide an authentic reference. STARK serves as a comprehensive testbed for evaluating the performance of retrieval systems driven by large language models (LLMs). Our experiments suggest that STARK presents significant challenges to the current retrieval and LLM systems, highlighting the need for more capable semi-structured retrieval systems.",Datasets & Benchmarks,NeurIPS,2024,Poster,Shirley Wu;Shiyu Zhao;Michihiro Yasunaga;Kexin Huang;Kaidi Cao;Qian Huang;Vassilis N. Ioannidis;Karthik Subbian;James Zou;Jure Leskovec,True,https://openreview.net/pdf?id=QSS5cGmKb1
QWTCcxMpPA,Measuring Multimodal Mathematical Reasoning with MATH-Vision Dataset,"Recent advancements in Large Multimodal Models (LMMs) have shown promising results in mathematical reasoning within visual contexts, with models exceeding human-level performance on existing benchmarks such as MathVista. However, we observe significant limitations in the diversity of questions and breadth of subjects covered by these benchmarks. To address this issue, we present the MATH-Vision (MATH-V) dataset, a meticulously curated collection of 3,040 high-quality mathematical problems with visual contexts sourced from real math competitions. Spanning 16 distinct mathematical disciplines and graded across 5 levels of difficulty, our dataset provides a comprehensive and diverse set of challenges for evaluating the mathematical reasoning abilities of LMMs. Through extensive experimentation, we unveil a notable performance gap between current LMMs and human performance on \\\\datasetname, underscoring the imperative for further advancements in LMMs. Moreover, our detailed categorization allows for a thorough error analysis of LMMs, offering valuable insights to guide future research and development. The dataset is released at [MathLLMs/MathVision](https://huggingface.co/datasets/MathLLMs/MathVision)",Datasets & Benchmarks,NeurIPS,2024,Poster,Ke Wang;Junting Pan;Weikang Shi;Zimu Lu;Houxing Ren;Aojun Zhou;Mingjie Zhan;Hongsheng Li,True,https://openreview.net/pdf?id=QWTCcxMpPA
QXQY58xU25,Data-Efficient Learning with Neural Programs,"Many computational tasks can be naturally expressed as a composition of  a DNN followed by a program written in a traditional programming language or an API call to an LLM. We call such composites ""neural programs"" and focus on the problem of learning the DNN parameters when the training data consist of end-to-end input-output labels for the composite.  When the program is written in a differentiable logic programming language, techniques from neurosymbolic learning are applicable, but in general, the learning for neural programs requires estimating the gradients of black-box components. We present an algorithm for learning neural programs, called ISED, that only relies on input-output samples of black-box components. For evaluation, we introduce new benchmarks that involve calls to modern LLMs such as  GPT-4 and also consider benchmarks from the neurosymbolic learning literature. Our evaluation shows that for the latter benchmarks, ISED has comparable performance to state-of-the-art neurosymbolic frameworks. For the former, we use adaptations of prior work on gradient approximations of black-box components as a baseline, and show that ISED achieves comparable accuracy but in a more data- and sample-efficient manner.",main,NeurIPS,2024,Poster,Alaia Solko-Breslin;Seewon Choi;Ziyang Li;Neelay Velingker;Rajeev Alur;Mayur Naik;Eric Wong,True,https://openreview.net/pdf?id=QXQY58xU25
QY4SpBhQZI,ReF-LDM: A Latent Diffusion Model for Reference-based Face Image Restoration,"While recent works on blind face image restoration have successfully produced impressive high-quality (HQ) images with abundant details from low-quality (LQ) input images, the generated content may not accurately reflect the real appearance of a person. To address this problem, incorporating well-shot personal images as additional reference inputs may be a promising strategy. Inspired by the recent success of the Latent Diffusion Model (LDM) in image generation, we propose ReF-LDM—an adaptation of LDM designed to generate HQ face images conditioned on one LQ image and multiple HQ reference images. Our LDM-based model incorporates an effective and efficient mechanism, CacheKV, for conditioning on reference images. Additionally, we design a timestep-scaled identity loss, enabling LDM to focus on learning the discriminating features of human faces. Lastly, we construct FFHQ-ref, a dataset consisting of 20,406 high-quality (HQ) face images with corresponding reference images, which can serve as both training and evaluation data for reference-based face restoration models.",main,NeurIPS,2024,Poster,Chi-Wei Hsiao;Yu-Lun Liu;Cheng-Kun Yang;Sheng-Po Kuo;Kevin Jou;Chia-Ping Chen,True,https://openreview.net/pdf?id=QY4SpBhQZI
Qdf3ad5MXH,MMBench-Video: A Long-Form Multi-Shot Benchmark for Holistic Video Understanding,"The advent of large vision-language models (LVLMs) has spurred research into their applications in multi-modal contexts, particularly in video understanding. Traditional VideoQA benchmarks, despite providing quantitative metrics, often fail to encompass the full spectrum of video content and inadequately assess models' temporal comprehension. To address these limitations, we introduce MMBench-Video, 
a quantitative benchmark designed to rigorously evaluate LVLMs' proficiency in video understanding. MMBench-Video incorporates lengthy videos from YouTube and employs free-form questions, mirroring practical use cases. The benchmark is meticulously crafted to probe the models' temporal reasoning skills, with all questions human-annotated according to a carefully constructed ability taxonomy.
We employ GPT-4 for automated assessment, demonstrating superior accuracy and robustness over earlier LLM-based evaluations. 
Utilizing MMBench-Video, we have conducted comprehensive evaluations that include both proprietary and open-source LVLMs for images and videos. MMBench-Video stands as a valuable resource for the research community, facilitating improved evaluation of LVLMs and catalyzing progress in the field of video understanding.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xinyu Fang;Kangrui Mao;Haodong Duan;Xiangyu Zhao;Yining Li;Dahua Lin;Kai Chen,True,https://openreview.net/pdf?id=Qdf3ad5MXH
QeWibaTmnn,Grasp as You Say: Language-guided Dexterous Grasp Generation,"This paper explores a novel task ""Dexterous Grasp as You Say'' (DexGYS), enabling robots to perform dexterous grasping based on human commands expressed in natural language. However, the development of this field is hindered by the lack of datasets with natural human guidance; thus, we propose a language-guided dexterous grasp dataset, named DexGYSNet, offering high-quality dexterous grasp annotations along with flexible and fine-grained human language guidance. Our dataset construction is cost-efficient, with the carefully-design hand-object interaction retargeting strategy, and the LLM-assisted language guidance annotation system. Equipped with this dataset, we introduce the DexGYSGrasp framework for generating dexterous grasps based on human language instructions, with the capability of producing grasps that are intent-aligned, high quality and diversity. To achieve this capability, our framework decomposes the complex learning process into two manageable progressive objectives and introduce two components to realize them. The first component learns the grasp distribution focusing on intention alignment and generation diversity. And the second component refines the grasp quality while maintaining intention consistency. Extensive experiments are conducted on DexGYSNet and real world environments for validation.",main,NeurIPS,2024,Poster,Yi-Lin Wei;Jian-Jian Jiang;Chengyi Xing;Xiantuo Tan;Xiao-Ming Wu;Hao Li;Mark Cutkosky;Wei-Shi Zheng,True,https://openreview.net/pdf?id=QeWibaTmnn
QiCJomIW3l,Toward Dynamic Non-Line-of-Sight Imaging with Mamba Enforced Temporal Consistency,"Dynamic reconstruction in confocal non-line-of-sight imaging encounters great challenges since the dense raster-scanning manner limits the practical frame rate. A fewer pioneer works reconstruct high-resolution volumes from the under-scanning transient measurements but overlook temporal consistency among transient frames. To fully exploit multi-frame information, we propose the first spatial-temporal Mamba (ST-Mamba) based method tailored for dynamic reconstruction of transient videos. Our method capitalizes on neighbouring transient frames to aggregate the target 3D hidden volume. Specifically, the interleaved features extracted from the input transient frames are fed to the proposed ST-Mamba blocks, which leverage the time-resolving causality in transient measurement. The cross ST-Mamba blocks are then devised to integrate the adjacent transient features. The target high-resolution transient frame is subsequently recovered by the transient spreading module. After transient fusion and recovery, a physical-based network is employed to reconstruct the hidden volume. To tackle the substantial noise inherent in transient videos, we propose a wave-based loss function to impose constraints within the phasor field. Besides, we introduce a new dataset, comprising synthetic videos for training and real-world videos for evaluation. Extensive experiments showcase the superior performance of our method on both synthetic data and real world data captured by different imaging setups. The code and data are available at https://github.com/Depth2World/Dynamic_NLOS.",main,NeurIPS,2024,Poster,Yue Li;Yi Sun;Shida Sun;Juntian Ye;Yueyi Zhang;Feihu Xu;Zhiwei Xiong,True,https://openreview.net/pdf?id=QiCJomIW3l
QocjHRR31U,BertaQA: How Much Do Language Models Know About Local Culture?,"Large Language Models (LLMs) exhibit extensive knowledge about the world, but most evaluations have been limited to global or anglocentric subjects. This raises the question of how well these models perform on topics relevant to other cultures, whose presence on the web is not that prominent. To address this gap, we introduce BertaQA, a multiple-choice trivia dataset that is parallel in English and Basque. The dataset consists of a local subset with questions pertinent to the Basque culture, and a global subset with questions of broader interest. We find that state-of-the-art LLMs struggle with local cultural knowledge, even as they excel on global topics. However, we show that continued pre-training in Basque significantly improves the models' performance on Basque culture, even when queried in English. To our knowledge, this is the first solid evidence of knowledge transfer from a low-resource to a high-resource language. Our analysis sheds light on the complex interplay between language and knowledge, and reveals that some prior findings do not fully hold when reassessed on local topics. Our dataset and evaluation code are available under open licenses at https://github.com/juletx/BertaQA.",Datasets & Benchmarks,NeurIPS,2024,Poster,Julen Etxaniz;Gorka Azkune;Aitor Soroa;Oier Lopez de Lacalle;Mikel Artetxe,True,https://openreview.net/pdf?id=QocjHRR31U
QpF3DFP3Td,Archaeoscape: Bringing Aerial Laser Scanning Archaeology to the Deep Learning Era,"Airborne Laser Scanning (ALS) technology has transformed modern archaeology by unveiling hidden landscapes beneath dense vegetation. However, the lack of expert-annotated, open-access resources has hindered the analysis of ALS data using advanced deep learning techniques. We address this limitation with Archaeoscape (available at https://archaeoscape.ai/data/2024), a novel large-scale archaeological ALS dataset spanning 888 km² in Cambodia with 31,141 annotated archaeological features from the Angkorian period. Archaeoscape is over four times larger than comparable datasets, and the first ALS archaeology resource with open-access data, annotations, and models.
We benchmark several recent segmentation models to demonstrate the benefits of modern vision techniques for this problem and highlight the unique challenges of discovering subtle human-made structures under dense jungle canopies. By making Archaeoscape available in open access, we hope to bridge the gap between traditional archaeology and modern computer vision methods.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Yohann PERRON;Vladyslav Sydorov;Adam P. Wijker;Damian Evans;Christophe Pottier;Loic Landrieu,True,https://openreview.net/pdf?id=QpF3DFP3Td
QxJHh7Z39R,Expecting The Unexpected: Towards Broad Out-Of-Distribution Detection,"Deployed machine learning systems require some mechanism to detect out-of-distribution (OOD) inputs. Existing research mainly focuses on one type of distribution shift: detecting samples from novel classes, absent from the training set. However, real-world systems encounter a broad variety of anomalous inputs, and the OOD literature neglects this diversity. This work categorizes five distinct types of distribution shifts and critically evaluates the performance of recent OOD detection methods on each of them. We publicly release our benchmark under the name BROAD (Benchmarking Resilience Over Anomaly Diversity). We find that while these methods excel in detecting novel classes, their performances are inconsistent across other types of distribution shifts. In other words, they can only reliably detect unexpected inputs that they have been specifically designed to expect. As a first step toward broad OOD detection, we learn a Gaussian mixture generative model for existing detection scores, enabling an ensemble detection approach that is more consistent and comprehensive for broad OOD detection, with improved performances over existing methods. We release code to build BROAD to facilitate a more comprehensive evaluation of novel OOD detectors.",Datasets & Benchmarks,NeurIPS,2024,Poster,Charles Guille-Escuret;Pierre-Andre Noel;Ioannis Mitliagkas;David Vazquez;Joao Monteiro,True,https://openreview.net/pdf?id=QxJHh7Z39R
Qz2xmVhn4S,Spider2-V: How Far Are Multimodal Agents From Automating Data Science and Engineering Workflows?,"Data science and engineering workflows often span multiple stages, from warehousing to orchestration, using tools like BigQuery, dbt, and Airbyte. As vision language models (VLMs) advance in multimodal understanding and code generation, VLM-based agents could potentially automate these workflows by generating SQL queries, Python code, and GUI operations. This automation can improve the productivity of experts while democratizing access to large-scale data analysis. In this paper, we introduce Spider2-V, the first multimodal agent benchmark focusing on professional data science and engineering workflows, featuring 494 real-world tasks in authentic computer environments and incorporating 20 enterprise-level professional applications. These tasks, derived from real-world use cases, evaluate the ability of a multimodal agent to perform data-related tasks by writing code and managing the GUI in enterprise data software systems. To balance realistic simulation with evaluation simplicity, we devote significant effort to developing automatic configurations for task setup and carefully crafting evaluation metrics for each task. Furthermore, we supplement multimodal agents with comprehensive documents of these enterprise data software systems. Our empirical evaluation reveals that existing state-of-the-art LLM/VLM-based agents do not reliably automate full data workflows (14.0% success). Even with step-by-step guidance, these agents still underperform in tasks that require fine-grained, knowledge-intensive GUI actions (16.2%) and involve remote cloud-hosted workspaces (10.6%). We hope that Spider2-V paves the way for autonomous multimodal agents to transform the automation of data science and engineering workflow. Our code and data are available at https://spider2-v.github.io.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Ruisheng Cao;Fangyu Lei;Haoyuan Wu;Jixuan Chen;Yeqiao Fu;Hongcheng Gao;Xiong Xinzhuang;Hanchong Zhang;Wenjing Hu;Yuchen Mao;Tianbao Xie;Hongshen Xu;Danyang Zhang;Sida Wang;Ruoxi Sun;Pengcheng Yin;Caiming Xiong;Ansong Ni;Qian Liu;Victor Zhong;Lu Chen;Kai Yu;Tao Yu,True,https://openreview.net/pdf?id=Qz2xmVhn4S
R4rNYJ2slJ,OpenSatMap: A Fine-grained High-resolution Satellite Dataset for Large-scale Map Construction,"In this paper, we propose OpenSatMap, a fine-grained, high-resolution satellite dataset for large-scale map construction. Map construction is one of the foundations of the transportation industry, such as navigation and autonomous driving. Extracting road structures from satellite images is an efficient way to construct large-scale maps. However, existing satellite datasets provide only coarse semantic-level labels with a relatively low resolution (up to level 19), impeding the advancement of this field. In contrast, the proposed OpenSatMap (1) has fine-grained instance-level annotations; (2) consists of high-resolution images (level 20); (3) is currently the largest one of its kind; (4) collects data with high diversity. Moreover, OpenSatMap covers and aligns with the popular nuScenes dataset and Argoverse 2 dataset to potentially advance autonomous driving technologies. By publishing and maintaining the dataset, we provide a high-quality benchmark for satellite-based map construction and downstream tasks like autonomous driving.",Datasets & Benchmarks,NeurIPS,2024,Poster,Hongbo Zhao;Lue Fan;Yuntao Chen;Haochen Wang;yuran Yang;Xiaojuan Jin;YIXIN ZHANG;Gaofeng Meng;Zhaoxiang Zhang,True,https://openreview.net/pdf?id=R4rNYJ2slJ
R6kJtWsTGy,The Elephant in the Room: Towards A Reliable Time-Series Anomaly Detection Benchmark,"Time-series anomaly detection is a fundamental task across scientific fields and industries. However, the field has long faced the ``elephant in the room:'' critical issues including flawed datasets, biased evaluation measures, and inconsistent benchmarking practices that have remained largely ignored and unaddressed.  We introduce the TSB-AD to systematically tackle these issues in the following three aspects: (i) Dataset Integrity: with 1070 high-quality time series from a diverse collection of 40 datasets (doubling the size of the largest collection and four times the number of existing curated datasets), we provide the first large-scale, heterogeneous, meticulously curated dataset that combines the effort of human perception and model interpretation; (ii) Measure Reliability: by revealing issues and biases in evaluation measures, we identify the most reliable and accurate measure, namely, VUS-PR for anomaly detection in time series to address concerns from the community; and (iii) Comprehensive Benchmarking: with a broad spectrum of 40 detection algorithms, from statistical methods to the latest foundation models, we perform a comprehensive evaluation that includes a thorough hyperparameter tuning and a unified setup for a fair and reproducible comparison. Our findings challenge the conventional wisdom regarding the superiority of advanced neural network architectures, revealing that simpler architectures and statistical methods often yield better performance. The promising performance of neural networks on multivariate cases and foundation models on point anomalies highlights the need for further advancements in these methods. We open-source the benchmark at https://github.com/TheDatumOrg/TSB-AD to promote further research.",Datasets & Benchmarks,NeurIPS,2024,Poster,Qinghua Liu;John Paparrizos,True,https://openreview.net/pdf?id=R6kJtWsTGy
R9gR9MPuD5,InterpBench: Semi-Synthetic Transformers for Evaluating Mechanistic Interpretability Techniques,"Mechanistic interpretability methods aim to identify the algorithm a neural network implements, but it is difficult to validate such methods when the true algorithm is unknown. This work presents InterpBench, a collection of semi-synthetic yet realistic transformers with known circuits for evaluating these techniques. We train simple neural networks using a stricter version of Interchange Intervention Training (IIT) which we call Strict IIT (SIIT). Like the original, SIIT trains neural networks by aligning their internal computation with a desired high-level causal model, but it also prevents non-circuit nodes from affecting the model's output. We evaluate SIIT on sparse transformers produced by the Tracr tool and find that SIIT models maintain Tracr's original circuit while being more realistic. SIIT can also train transformers with larger circuits, like Indirect Object Identification (IOI). Finally, we use our benchmark to evaluate existing circuit discovery techniques.",Datasets & Benchmarks,NeurIPS,2024,Poster,Rohan Gupta;Iván Arcuschin;Thomas Kwa;Adrià Garriga-Alonso,True,https://openreview.net/pdf?id=R9gR9MPuD5
RJHQAcbmpZ,A survey and benchmark of high-dimensional Bayesian optimization of discrete sequences,"Optimizing discrete black-box functions is key in several domains, e.g. protein engineering and drug design. Due to the lack of gradient information and the need for sample efficiency, Bayesian optimization is an ideal candidate for these tasks. Several methods for high-dimensional continuous and categorical Bayesian optimization have been proposed recently. However, our survey of the field reveals highly heterogeneous experimental set-ups across methods and technical barriers for the replicability and application of published algorithms to real-world tasks. To address these issues, we develop a unified framework to test a vast array of high-dimensional Bayesian optimization methods and a collection of standardized black-box functions representing real-world application domains in chemistry and biology. These two components of the benchmark are each supported by flexible, scalable, and easily extendable software libraries (poli and poli-baselines), allowing practitioners to readily incorporate new optimization objectives or discrete optimizers. Project website: https://machinelearninglifescience.github.io/hdbo_benchmark.",Datasets & Benchmarks,NeurIPS,2024,Poster,Miguel González-Duque;Richard Michael;Simon Bartels;Yevgen Zainchkovskyy;Søren Hauberg;Wouter Boomsma,True,https://openreview.net/pdf?id=RJHQAcbmpZ
RJZRhMzZzH,A Careful Examination of Large Language Model Performance on Grade School Arithmetic,"Large language models (LLMs) have achieved impressive success on many benchmarks for mathematical reasoning.
However, there is growing concern that some of this performance actually reflects dataset contamination, where data closely resembling benchmark questions leaks into the training data, instead of true reasoning ability.
To investigate this claim rigorously, we commission Grade School Math 1000 (GSM1k). GSM1k is designed to mirror the style and complexity of the established GSM8k benchmark,
the gold standard for measuring elementary mathematical reasoning. We ensure that the two benchmarks are comparable across important metrics such as human solve rates, number of steps in solution, answer magnitude, and more.
When evaluating leading open- and closed-source LLMs on GSM1k, we observe accuracy drops of up to 8%, with several families of models showing evidence of systematic overfitting across almost all model sizes.
Further analysis suggests a positive relationship (Spearman's r^2=0.36) between a model's probability of generating an example from GSM8k and its performance gap between GSM8k and GSM1k, suggesting that some models may have partially memorized GSM8k.
Nevertheless, many models, especially those on the frontier, show minimal signs of overfitting, and all models broadly demonstrate generalization to novel math problems guaranteed to not be in their training data.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Hugh Zhang;Jeff Da;Dean Lee;Vaughn Robinson;Catherine Wu;William Song;Tiffany Zhao;Pranav Vishnu Raja;Charlotte Zhuang;Dylan Z Slack;Qin Lyu;Sean M. Hendryx;Russell Kaplan;Michele Lunati;Summer Yue,True,https://openreview.net/pdf?id=RJZRhMzZzH
RQlbMrA5XL,NovoBench: Benchmarking Deep Learning-based \\\\emph{De Novo} Sequencing Methods in Proteomics,"Tandem mass spectrometry has played a pivotal role in advancing proteomics, enabling the analysis of protein composition in biological tissues. Many deep learning methods have been developed for \\\\emph{de novo} peptide sequencing task, i.e., predicting the peptide sequence for the observed mass spectrum. 
However, two key challenges seriously hinder the further research of this important task. Firstly, since there is no consensus for the evaluation datasets, the empirical results in different research papers are often not comparable, leading to unfair comparison. Secondly, the current methods are usually limited to amino acid-level or peptide-level precision and recall metrics. In this work, we present the first unified benchmark NovoBench for \\\\emph{de novo} peptide sequencing, which comprises diverse mass spectrum data, integrated models, and comprehensive evaluation metrics. Recent impressive methods, including DeepNovo, PointNovo, Casanovo, InstaNovo, AdaNovo and $\\\\pi$-HelixNovo are integrated into our framework. In addition to amino acid-level and peptide-level precision and recall, we also evaluate the models' performance in terms of identifying post-tranlational modifications (PTMs), efficiency and robustness to peptide length, noise peaks and missing fragment ratio, which are important influencing factors while seldom be considered. Leveraging this benchmark, we conduct a large-scale study of current methods, report many insightful findings that open up new possibilities for future development. The benchmark is open-sourced to facilitate future research and application. The code is available at \\\\url{https://github.com/Westlake-OmicsAI/NovoBench}.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jingbo Zhou;Shaorong Chen;Jun Xia;Sizhe Liu;Tianze Ling;Wenjie Du;Yue Liu;Jianwei Yin;Stan Z. Li,True,https://openreview.net/pdf?id=RQlbMrA5XL
RSvhU69sbG,MathPile: A Billion-Token-Scale Pretraining Corpus for Math,"High-quality, large-scale corpora are the cornerstone of building foundation models. In this work, we introduce MathPile, a diverse and high-quality math-centric corpus comprising about 9.5 billion tokens. Throughout its creation, we adhered to the principle of “less is more”, firmly believing in the supremacy of data quality over quantity, even in the pre-training phase. Our meticulous data collection and processing efforts included a complex suite of preprocessing, prefiltering, language identification, cleaning, filtering, and deduplication, ensuring the high quality of our corpus. Furthermore, we performed data contamination detection on downstream benchmark test sets to eliminate duplicates and conducted continual pre-training experiments, booting the performance on common mathematical reasoning benchmarks. We aim for our MathPile to boost language models’ mathematical reasoning abilities and open-source its different versions and processing scripts to advance the field.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zengzhi Wang;Xuefeng Li;Rui Xia;Pengfei Liu,True,https://openreview.net/pdf?id=RSvhU69sbG
RfsfRn9OFd,EEG2Video: Towards Decoding Dynamic Visual Perception from EEG Signals,"Our visual experience in daily life are dominated by dynamic change. Decoding such dynamic information from brain activity can enhance the understanding of the brain’s visual processing system. However, previous studies predominately focus on reconstructing static visual stimuli. In this paper, we explore to decode dynamic visual perception from electroencephalography (EEG), a neuroimaging technique able to record brain activity with high temporal resolution (1000 Hz) for capturing rapid changes in brains. Our contributions are threefold: Firstly, we develop a large dataset recording signals from 20 subjects while they were watching 1400 dynamic video clips of 40 concepts. This dataset fills the gap in the lack of EEG-video pairs. Secondly, we annotate each video clips to investigate the potential for decoding some specific meta information (e.g., color, dynamic, human or not) from EEG. Thirdly, we propose a novel baseline EEG2Video for video reconstruction from EEG signals that better aligns dynamic movements with high temporal resolution brain signals by Seq2Seq architecture. EEG2Video achieves a 2-way accuracy of 79.8% in semantic classification tasks and 0.256 in structural similarity index (SSIM). Overall, our works takes an important step towards decoding dynamic visual perception from EEG signals. Our dataset and code will be released soon.",main,NeurIPS,2024,Poster,Xuanhao Liu;Yan-Kai Liu;Yansen Wang;Kan Ren;Hanwen Shi;Zilong Wang;Dongsheng Li;Bao-liang Lu;Wei-Long Zheng,True,https://openreview.net/pdf?id=RfsfRn9OFd
RgUcvs6ssu,WelQrate: Defining the Gold Standard in Small Molecule Drug Discovery Benchmarking,"While deep learning has revolutionized computer-aided drug discovery, the AI community has predominantly focused on model innovation and placed less emphasis on establishing best benchmarking practices. 
We posit that without a sound model evaluation framework, the AI community's efforts cannot reach their full potential, thereby slowing the progress and transfer of innovation into real-world drug discovery.
Thus, in this paper, we seek to establish a new gold standard for small molecule drug discovery benchmarking, *WelQrate*. 
Specifically, our contributions are threefold: 
***WelQrate*** **dataset collection** - we introduce a meticulously curated collection of 9 datasets spanning 5 therapeutic target classes. Our hierarchical curation pipelines, designed by drug discovery experts, go beyond the primary high-throughput screen by leveraging additional confirmatory and counter screens along with rigorous domain-driven preprocessing, such as Pan-Assay Interference Compounds (PAINS) filtering, to ensure the high-quality data in the datasets; ***WelQrate*** **Evaluation Framework** - we propose a standardized model evaluation framework considering high-quality datasets, featurization, 3D conformation generation, evaluation metrics, and data splits, which provides a reliable benchmarking for drug discovery experts conducting real-world virtual screening; **Benchmarking** - 
we evaluate model performance through various research questions using the *WelQrate* dataset collection, exploring the effects of different models, dataset quality, featurization methods, and data splitting strategies on the results.
In summary, we recommend adopting our proposed *WelQrate* as the gold standard in small molecule drug discovery benchmarking. The *WelQrate* dataset collection, along with the curation codes, and experimental scripts are all publicly available at www.WelQrate.org.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yunchao Liu;Ha Dong;Xin Wang;Rocco Moretti;Yu Wang;Zhaoqian Su;Jiawei Gu;Bobby Bodenheimer;Charles Weaver;Jens Meiler;Tyler Derr,True,https://openreview.net/pdf?id=RgUcvs6ssu
S4YRCLbUK1,Who Evaluates the Evaluations? Objectively Scoring Text-to-Image Prompt Coherence Metrics with T2IScoreScore (TS2),"With advances in the quality of text-to-image (T2I) models has come interest in benchmarking their prompt faithfulness---the semantic coherence of generated images to the prompts they were conditioned on. A variety of T2I faithfulness metrics have been proposed, leveraging advances in cross-modal embeddings and vision-language models (VLMs). However, these metrics are not rigorously compared and benchmarked, instead presented with correlation to human Likert scores over a set of easy-to-discriminate images against seemingly weak baselines. 

We introduce T2IScoreScore, a curated set of semantic error graphs containing a prompt and a set of increasingly erroneous images. These allow us to rigorously judge whether a given prompt faithfulness metric can correctly order images with respect to their objective error count and significantly discriminate  between different error nodes, using meta-metric scores derived from established statistical tests. Surprisingly, we find that the state-of-the-art VLM-based metrics (e.g., TIFA, DSG, LLMScore, VIEScore) we tested fail to significantly outperform simple (and supposedly worse) feature-based metrics like CLIPScore, particularly on a hard subset of naturally-occurring T2I model errors. TS2 will enable the development of better T2I prompt faithfulness metrics through more rigorous comparison of their conformity to expected orderings and separations under objective criteria.",main,NeurIPS,2024,Spotlight,Michael Saxon;Fatima Jahara;Mahsa Khoshnoodi;Yujie Lu;Aditya Sharma;William Yang Wang,True,https://openreview.net/pdf?id=S4YRCLbUK1
S9Qrrxpy6z,CiteME: Can Language Models Accurately Cite Scientific Claims?,"Thousands of new scientific papers are published each month. Such  information overload complicates researcher efforts to stay current with the state-of-the-art as well as to verify and correctly attribute claims. We pose the following research question: Given a text excerpt referencing a paper, could an LM act as a research assistant to correctly identify the referenced paper? We advance efforts to answer this question by building a benchmark that evaluates the abilities of LMs in citation attribution. Our benchmark, CiteME, consists  of text excerpts from recent machine learning papers, each referencing a single other paper. CiteME use reveals a large gap between frontier LMs and human performance, with LMs achieving only 4.2-18.5% accuracy and humans 69.7%. We close this gap by introducing CiteAgent, an autonomous system built on the GPT-4o LM that can also search and read papers, which achieves an accuracy of 35.3% on CiteME. 
Overall, CiteME serves as a challenging testbed for open-ended claim attribution, driving the research community towards a future where any claim made by an LM can be  automatically verified and discarded if found to be incorrect.",Datasets & Benchmarks,NeurIPS,2024,Poster,Ori Press;Andreas Hochlehnert;Ameya Prabhu;Vishaal Udandarao;Ofir Press;Matthias Bethge,True,https://openreview.net/pdf?id=S9Qrrxpy6z
SKCbZR8Pyd,SpeechAlign: Aligning Speech Generation to Human Preferences,"Speech language models have significantly advanced in generating realistic speech, with neural codec language models standing out. However, the integration of preference optimization to align speech outputs to human preferences is often neglected. This paper addresses this gap by first analyzing the distribution gap in codec language models, highlighting how it leads to discrepancies between the training and inference phases, which negatively affects performance. Then we explore leveraging preference optimization to bridge the distribution gap. We introduce SpeechAlign, an iterative self-improvement strategy that aligns speech language models to human preferences.  SpeechAlign involves constructing a preference codec dataset contrasting golden codec tokens against synthetic tokens, followed by preference optimization to improve the codec language model. This cycle of improvement is carried out iteratively to steadily convert weak models to strong ones. Through both subjective and objective evaluations, we show that SpeechAlign can bridge the distribution gap and facilitating continuous self-improvement of the speech language model. Moreover, SpeechAlign exhibits robust generalization capabilities and works for smaller models.  Demos are available at https://0nutation.github.io/SpeechAlign.github.io/.",main,NeurIPS,2024,Poster,Dong Zhang;Zhaowei Li;Shimin Li;Xin Zhang;Pengyu Wang;Yaqian Zhou;Xipeng Qiu,True,https://openreview.net/pdf?id=SKCbZR8Pyd
SQVns9hWJT,TextCtrl: Diffusion-based Scene Text Editing with Prior Guidance Control,"Centred on content modification and style preservation, Scene Text Editing (STE) remains a challenging task despite considerable progress in text-to-image synthesis and text-driven image manipulation recently. GAN-based STE methods generally encounter a common issue of model generalization, while Diffusion-based STE methods suffer from undesired style deviations. To address these problems, we propose TextCtrl, a diffusion-based method that edits text with prior guidance control. Our method consists of two key components: (i) By constructing fine-grained text style disentanglement and robust text glyph structure representation,  TextCtrl explicitly incorporates Style-Structure guidance into model design and network training, significantly improving text style consistency and rendering accuracy. (ii) To further leverage the style prior, a Glyph-adaptive Mutual Self-attention mechanism is proposed which deconstructs the implicit fine-grained features of the source image to enhance style consistency and vision quality during inference. Furthermore, to fill the vacancy of the real-world STE evaluation benchmark, we create the first real-world image-pair dataset termed ScenePair for fair comparisons. Experiments demonstrate the effectiveness of TextCtrl compared with previous methods concerning both style fidelity and text accuracy. Project page: https://github.com/weichaozeng/TextCtrl.",main,NeurIPS,2024,Spotlight,Weichao Zeng;Yan Shu;Zhenhang Li;Dongbao Yang;Yu Zhou,True,https://openreview.net/pdf?id=SQVns9hWJT
SXYmSTXyHm,The Fragility of Fairness: Causal Sensitivity Analysis for Fair Machine Learning,"Fairness metrics are a core tool in the fair machine learning literature (FairML),
used to determine that ML models are, in some sense, “fair.” Real-world data,
however, are typically plagued by various measurement biases and other violated
assumptions, which can render fairness assessments meaningless. We adapt tools
from causal sensitivity analysis to the FairML context, providing a general frame-
work which (1) accommodates effectively any combination of fairness metric and
bias that can be posed in the “oblivious setting”; (2) allows researchers to inves-
tigate combinations of biases, resulting in non-linear sensitivity; and (3) enables
flexible encoding of domain-specific constraints and assumptions. Employing this
framework, we analyze the sensitivity of the most common parity metrics under 3
varieties of classifier across 14 canonical fairness datasets. Our analysis reveals the
striking fragility of fairness assessments to even minor dataset biases. We show that
causal sensitivity analysis provides a powerful and necessary toolkit for gauging
the informativeness of parity metric evaluations. Our repository is \\\\href{https://github.com/Jakefawkes/fragile_fair}{available here}.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jake Fawkes;Nic Fishman;Mel Andrews;Zachary Chase Lipton,False,https://openreview.net/pdf?id=SXYmSTXyHm
SYZzABI1ns,CS-Bench: A Comprehensive Benchmark for Large Language Models towards Computer Science Mastery,"Computer Science (CS) stands as a testament to the intricacies of human intelligence, profoundly advancing the development of artificial intelligence and modern society. However, the current community of large language models (LLMs) overly focuses on benchmarks for analyzing specific foundational skills (e.g. mathematics and code generation), neglecting an all-round evaluation of the computer science field. To bridge this gap, we introduce CS-Bench, the first bilingual (Chinese-English) benchmark dedicated to evaluating the performance of LLMs in computer science. CS-Bench comprises approximately 5K meticulously curated test samples, covering 26 subfields across 4 key areas of computer science, encompassing various task forms and divisions of knowledge and reasoning. Utilizing CS-Bench, we conduct a comprehensive evaluation of over 30 mainstream LLMs, revealing the relationship between CS performance and model scales. We also quantitatively analyze the reasons for failures in existing LLMs and highlight directions for improvements, including knowledge supplementation and CS-specific reasoning. Further cross-capability experiments show a high correlation between LLMs' capabilities in computer science and their abilities in mathematics and coding. Moreover, expert LLMs specialized in mathematics and coding also demonstrate strong performances in several CS subfields. Looking ahead, we envision CS-Bench serving as a cornerstone for LLM applications in the CS field and paving new avenues in assessing LLMs' diverse reasoning capabilities.",Datasets & Benchmarks,NeurIPS,2024,Reject,Xiaoshuai Song;Muxi Diao;Guanting Dong;Zhengyang Wang;Yujia Fu;Runqi Qiao;Zhexu Wang;Dayuan Fu;Huangxuan Wu;Bin Liang;Weihao Zeng;Yejie Wang;Zhuoma GongQue;Jianing Yu;Qiuna Tan;Weiran Xu,True,https://openreview.net/pdf?id=SYZzABI1ns
SaodQ13jga,GITA: Graph to Visual and Textual Integration for Vision-Language Graph Reasoning,"Large Language Models (LLMs) are increasingly used for various tasks with graph structures. Though LLMs can process graph information in a textual format, they overlook the rich vision modality, which is an intuitive way for humans to comprehend structural information and conduct general graph reasoning. The potential benefits and capabilities of representing graph structures as visual images (i.e., $\\\\textit{visual graph}$) are still unexplored. To fill the gap, we innovatively propose an end-to-end framework, called $\\\\textbf{G}$raph to v$\\\\textbf{I}$sual and $\\\\textbf{T}$extual Integr$\\\\textbf{A}$tion (GITA), which firstly incorporates visual graphs into general graph reasoning. Besides, we establish  $\\\\textbf{G}$raph-based $\\\\textbf{V}$ision-$\\\\textbf{L}$anguage $\\\\textbf{Q}$uestion $\\\\textbf{A}$nswering (GVLQA) dataset from existing graph data, which is the first vision-language dataset for general graph reasoning purposes. Extensive experiments on the GVLQA dataset and five real-world datasets show that GITA outperforms mainstream LLMs in terms of general graph reasoning capabilities. Moreover, We highlight the effectiveness of the layout augmentation on visual graphs and pretraining on the GVLQA dataset.",main,NeurIPS,2024,Poster,Yanbin Wei;Shuai Fu;Weisen Jiang;Zejian Zhang;Zhixiong Zeng;Qi Wu;James Kwok;Yu Zhang,True,https://openreview.net/pdf?id=SaodQ13jga
ScPgzCZ6Lo,GC-Bench: An Open and Unified Benchmark for Graph Condensation,"Graph condensation (GC) has recently garnered considerable attention due to its ability to reduce large-scale graph datasets while preserving their essential properties. The core concept of GC is to create a smaller, more manageable graph that retains the characteristics of the original graph. Despite the proliferation of graph condensation methods developed in recent years, there is no comprehensive evaluation and in-depth analysis, which creates a great obstacle to understanding the progress in this field. To fill this gap, we develop a comprehensive Graph Condensation Benchmark (GC-Bench) to analyze the performance of graph condensation in different scenarios systematically. Specifically, GC-Bench systematically investigates the characteristics of graph condensation in terms of the following dimensions: effectiveness, transferability, and complexity. We comprehensively evaluate 12 state-of-the-art graph condensation algorithms in node-level and graph-level tasks and analyze their performance in 12 diverse graph datasets. Further, we have developed an easy-to-use library for training and evaluating different GC methods to facilitate reproducible research.The GC-Bench library is available at https://github.com/RingBDStack/GC-Bench.",Datasets & Benchmarks,NeurIPS,2024,Poster,Qingyun Sun;Ziying Chen;Beining Yang;Cheng Ji;Xingcheng Fu;Sheng Zhou;Hao Peng;Jianxin Li;Philip S. Yu,False,https://openreview.net/pdf?id=ScPgzCZ6Lo
Sp9cj4pNYD,SurgicAI: A Hierarchical Platform for Fine-Grained Surgical Policy Learning and Benchmarking,"Despite advancements in robotic-assisted surgery, automating complex tasks like suturing remains challenging due to the need for adaptability and precision. Learning-based approaches, particularly reinforcement learning (RL) and imitation learning (IL), require realistic simulation environments for efficient data collection. However, current platforms often include only relatively simple, non-dexterous manipulations and lack the flexibility required for effective learning and generalization. We introduce SurgicAI, a novel platform for development and benchmarking that addresses these challenges by providing the flexibility to accommodate both modular subtasks and more importantly task decomposition in RL-based surgical robotics. Compatible with the da Vinci Surgical System, SurgicAI offers a standardized pipeline for collecting and utilizing expert demonstrations. It supports the deployment of multiple RL and IL approaches, and the training of both singular and compositional subtasks in suturing scenarios, featuring high dexterity and modularization. Meanwhile, SurgicAI sets clear metrics and benchmarks for the assessment of learned policies. We implemented and evaluated multiple RL and IL algorithms on SurgicAI. Our detailed benchmark analysis underscores SurgicAI's potential to advance policy learning in surgical robotics. Details: https://github.com/surgical-robotics-ai/SurgicAI",Datasets & Benchmarks,NeurIPS,2024,Poster,Jin Wu;Haoying Zhou;Peter Kazanzides;Adnan Munawar;Anqi Liu,False,https://openreview.net/pdf?id=Sp9cj4pNYD
T0glCBw28a,The ALCHEmist: Automated Labeling 500x CHEaper than LLM Data Annotators,"Large pretrained models can be used as annotators, helping replace or augment crowdworkers and enabling distilling generalist models into smaller specialist models. Unfortunately, this comes at a cost: employing top-of-the-line models often requires paying thousands of dollars for API calls, while the resulting datasets are static and challenging to audit. To address these challenges, we propose a simple alternative: rather than directly querying labels from pretrained models, we task models to generate programs that can produce labels. These programs can be stored and applied locally, re-used and extended, and cost orders of magnitude less. Our system, $\\\\textbf{Alchemist}$, obtains comparable to or better performance than large language model-based annotation in a range of tasks for a fraction of the cost: on average, improvements amount to a $\\\\textbf{12.9}$% enhancement while the total labeling costs across all datasets are reduced by a factor of approximately $\\\\textbf{500}\\\\times$.",main,NeurIPS,2024,Spotlight,Tzu-Heng Huang;Catherine Cao;Vaishnavi Bhargava;Frederic Sala,True,https://openreview.net/pdf?id=T0glCBw28a
TGC7HNf6nK,Lever LM: Configuring In-Context Sequence to Lever Large Vision Language Models,"As Archimedes famously said, ``Give me a lever long enough and a fulcrum on which to place it, and I shall move the world'', in this study, we propose to use a tiny Language Model (LM), \\\\eg, a Transformer with 67M parameters, to lever much larger Vision-Language Models (LVLMs) with 9B parameters. Specifically, we use this tiny \\\\textbf{Lever-LM} to configure effective in-context demonstration (ICD) sequences to improve the In-Context Learinng (ICL) performance of LVLMs. Previous studies show that diverse ICD configurations like the selection and ordering of the demonstrations heavily affect the ICL performance, highlighting the significance of configuring effective ICD sequences. Motivated by this and by re-considering the the process of configuring ICD sequence, we find this is a mirror process of human sentence composition and further assume that effective ICD configurations may contain internal statistical patterns that can be captured by Lever-LM. Then a dataset with effective ICD sequences is constructed to train Lever-LM. After training, given novel queries, new ICD sequences are configured by the trained Lever-LM to solve vision-language tasks through ICL. Experiments show that these ICD sequences can improve the ICL performance of two LVLMs compared with some strong baselines in Visual Question Answering and Image Captioning, validating that Lever-LM can really capture the statistical patterns for levering LVLMs. The code is available at \\\\url{https://anonymous.4open.science/r/Lever-LM-604A/}.",main,NeurIPS,2024,Poster,Xu Yang;Yingzhe Peng;Haoxuan Ma;Shuo Xu;Chi Zhang;Yucheng Han;Hanwang Zhang,True,https://openreview.net/pdf?id=TGC7HNf6nK
THMgVAkZwh,VLKEB: A Large Vision-Language Model Knowledge Editing Benchmark,"Recently, knowledge editing on large language models (LLMs) has received considerable attention. Compared to this, editing Large Vision-Language Models (LVLMs) faces extra challenges from diverse data modalities and complicated model components, and data for LVLMs editing are limited. The existing LVLM editing benchmark, which comprises three metrics (Reliability, Locality, and Generality), falls short in the quality of synthesized evaluation images and cannot assess whether models apply edited knowledge in relevant content. Therefore, we employ more reliable data collection methods to construct a new Large $\\\\textbf{V}$ision-$\\\\textbf{L}$anguage Model $\\\\textbf{K}$nowledge $\\\\textbf{E}$diting $\\\\textbf{B}$enchmark, $\\\\textbf{VLKEB}$, and extend the Portability metric for more comprehensive evaluation. Leveraging a multi-modal knowledge graph, our image data are bound with knowledge entities. This can be further used to extract entity-related knowledge, which constitutes the base of editing data. We conduct experiments of different editing methods on five LVLMs, and thoroughly analyze how do they impact the models. The results reveal strengths and deficiencies of these methods and hopefully provide insights for future research. The codes and dataset are available at: https://github.com/VLKEB/VLKEB.",Datasets & Benchmarks,NeurIPS,2024,Poster,Han Huang;Haitian Zhong;Tao Yu;Qiang Liu;Shu Wu;Liang Wang;Tieniu Tan,True,https://openreview.net/pdf?id=THMgVAkZwh
TIhiFqGOYC,Meaningful Learning: Enhancing Abstract Reasoning in Large Language Models via Generic Fact Guidance,"Large language models (LLMs) have developed impressive performance and strong explainability across various reasoning scenarios, marking a significant stride towards mimicking human-like intelligence. Despite this, when tasked with several simple questions supported by a generic fact, LLMs often struggle to abstract and apply the generic fact to provide consistent and precise answers, revealing a deficiency in abstract reasoning abilities. This has sparked a vigorous debate about whether LLMs are genuinely reasoning or merely memorizing. In light of this, we design a preliminary study to quantify and delve into the abstract reasoning abilities of existing LLMs. Our findings reveal a substantial discrepancy between their general reasoning and abstract reasoning performances. To relieve this problem, we tailor an abstract reasoning dataset (AbsR) together with a meaningful learning paradigm to teach LLMs how to leverage generic facts for reasoning purposes. The results show that our approach not only boosts the general reasoning performance of LLMs but also makes considerable strides towards their capacity for abstract reasoning, moving beyond simple memorization or imitation to a more nuanced understanding and application of generic facts. The code is available at https://github.com/Waste-Wood/MeanLearn.",main,NeurIPS,2024,Poster,Kai Xiong;Xiao Ding;Ting Liu;Bing Qin;Dongliang Xu;Qing Yang;Hongtao Liu;Yixin Cao,True,https://openreview.net/pdf?id=TIhiFqGOYC
TLUGoShY30,Multi-times Monte Carlo Rendering for Inter-reflection Reconstruction,"Inverse rendering methods have achieved remarkable performance in reconstructing high-fidelity 3D objects with disentangled geometries, materials, and environmental light. However, they still face huge challenges in reflective surface reconstruction. Although recent methods model the light trace to learn specularity, the ignorance of indirect illumination makes it hard to handle inter-reflections among multiple smooth objects. In this work, we propose Ref-MC2 that introduces the multi-time Monte Carlo sampling which comprehensively computes the environmental illumination and meanwhile considers the reflective light from object surfaces. To address the computation challenge as the times of Monte Carlo sampling grow, we propose a specularity-adaptive sampling strategy, significantly reducing the computational complexity. Besides the computational resource, higher geometry accuracy is also required because geometric errors accumulate multiple times. Therefore, we further introduce a reflection-aware surface model to initialize the geometry and refine it during inverse rendering. We construct a challenging dataset containing scenes with multiple objects and inter-reflections. Experiments show that our method outperforms other inverse rendering methods on various object groups. We also show downstream applications, e.g., relighting and material editing, to illustrate the disentanglement ability of our method.",main,NeurIPS,2024,Poster,Zhu Tengjie;Zhuo Chen;Jingnan Gao;Yichao Yan;Xiaokang Yang,True,https://openreview.net/pdf?id=TLUGoShY30
TWfNFCOPaK,Probabilistic and Differentiable Wireless Simulation with Geometric Transformers,"Modelling the propagation of electromagnetic signals is critical for designing modern communication systems. While there are precise simulators based on ray tracing, they do not lend themselves to solving inverse problems or the integration in an automated design loop. We propose to address these challenges through differentiable neural surrogates that exploit the geometric aspects of the problem. We first introduce the Wireless Geometric Algebra Transformer (Wi-GATr), a generic backbone architecture for simulating wireless propagation in a 3D environment. It uses versatile representations based on geometric algebra and is equivariant with respect to E(3), the symmetry group of the underlying physics. Second, we study two algorithmic approaches to signal prediction and inverse problems based on differentiable predictive modelling and diffusion models. We show how these let us predict received power, localize transmitters, and reconstruct the 3D environment from the received signal. Finally, we introduce two large, geometry-focused datasets of wireless signal propagation in indoor scenes. In experiments, we show that our geometry-forward approach achieves higher-fidelity predictions with less
data than various baselines.",main,NeurIPS,2024,Reject,Thomas Hehn;Markus Peschl;Tribhuvanesh Orekondy;Arash Behboodi;Johann Brehmer,True,https://openreview.net/pdf?id=TWfNFCOPaK
TbslDzPxhF,Job-SDF: A Multi-Granularity Dataset for Job Skill Demand Forecasting and Benchmarking,"In a rapidly evolving job market, skill demand forecasting is crucial as it enables policymakers and businesses to anticipate and adapt to changes, ensuring that workforce skills align with market needs, thereby enhancing productivity and competitiveness. Additionally, by identifying emerging skill requirements, it directs individuals towards relevant training and education opportunities, promoting continuous self-learning and development. However, the absence of comprehensive datasets presents a significant challenge, impeding research and the advancement of this field. To bridge this gap, we present Job-SDF, a dataset designed to train and benchmark job-skill demand forecasting models. Based on millions of public job advertisements collected from online recruitment platforms, this dataset encompasses monthly recruitment demand.
Our dataset uniquely enables evaluating skill demand forecasting models at various granularities, including occupation, company, and regional levels. 
We benchmark a range of models on this dataset, evaluating their performance in standard scenarios, in predictions focused on lower value ranges, and in the presence of structural breaks, providing new insights for further research. Our code and dataset are publicly accessible via the https://github.com/Job-SDF/benchmark.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xi Chen;Chuan Qin;Chuyu Fang;Chao Wang;Chen Zhu;Fuzhen Zhuang;Hengshu Zhu;Hui Xiong,True,https://openreview.net/pdf?id=TbslDzPxhF
TeBKVfhP2M,Fundamental Limits of Prompt Compression: A Rate-Distortion Framework for Black-Box Language Models,"We formalize the problem of prompt compression for large language models (LLMs) and present a framework to unify token-level prompt compression methods which create hard prompts for black-box models. We derive the distortion-rate function for this setup as a linear program, and provide an efficient algorithm to compute this fundamental limit via the dual of the linear program. Using the distortion-rate function as the baseline, we study the performance of existing compression schemes on a synthetic dataset consisting of prompts generated from a Markov chain, natural language queries, and their respective answers. Our empirical analysis demonstrates the criticality of query-aware prompt compression, where the compressor has knowledge of the downstream task/query for the black-box LLM. We show that there is a large gap between the performance of current prompt compression methods and the optimal strategy, and propose Adaptive QuerySelect, a query-aware, variable-rate adaptation of a prior work to close the gap. We extend our experiments to a small natural language dataset to further confirm our findings on our synthetic dataset.",main,NeurIPS,2024,Poster,Alliot Nagle;Adway Girish;Marco Bondaschi;Michael Gastpar;Ashok Vardhan Makkuva;Hyeji Kim,True,https://openreview.net/pdf?id=TeBKVfhP2M
TuMnKFKPho,VHELM: A Holistic Evaluation of Vision Language Models,"Current benchmarks for assessing vision-language models (VLMs) often focus on their perception or problem-solving capabilities and neglect other critical aspects such as fairness, multilinguality, or toxicity. Furthermore, they differ in their evaluation procedures and the scope of the evaluation, making it difficult to compare models. To address these issues, we extend the HELM framework to VLMs to present the Holistic Evaluation of Vision Language Models (VHELM). VHELM aggregates various datasets to cover one or more of the 9 aspects: *visual perception*, *knowledge*, *reasoning*, *bias*, *fairness*, *multilinguality*, *robustness*, *toxicity*, and *safety*. In doing so, we produce a comprehensive, multi-dimensional view of the capabilities of the VLMs across these important factors. In addition, we standardize the standard inference parameters, methods of prompting, and evaluation metrics to enable fair comparisons across models. Our framework is designed to be lightweight and automatic so that evaluation runs are cheap and fast. Our initial run evaluates 22 VLMs on 21 existing datasets to provide a holistic snapshot of the models. We uncover new key findings, such as the fact that efficiency-focused models (e.g., Claude 3 Haiku or Gemini 1.5 Flash) perform significantly worse than their full models (e.g., Claude 3 Opus or Gemini 1.5 Pro) on the bias benchmark but not when evaluated on the other aspects. For transparency, we release the raw model generations and complete results on our website at https://crfm.stanford.edu/helm/vhelm/v2.0.1. VHELM is intended to be a living benchmark, and we hope to continue adding new datasets and models over time.",Datasets & Benchmarks,NeurIPS,2024,Poster,Tony Lee;Haoqin Tu;Chi Heem Wong;Wenhao Zheng;Yiyang Zhou;Yifan Mai;Josselin Somerville Roberts;Michihiro Yasunaga;Huaxiu Yao;Cihang Xie;Percy Liang,False,https://openreview.net/pdf?id=TuMnKFKPho
Twqa0GFMGX,Idiographic Personality Gaussian Process for Psychological Assessment,"We develop a novel measurement framework based on Gaussian process coregionalization model to address a long-lasting debate in psychometrics: whether psychological features like personality share a common structure across the population or vary uniquely for individuals. We propose idiographic personality Gaussian process (IPGP), an intermediate model that accommodates both shared trait structure across individuals and ""idiographic"" deviations. IPGP leverages the Gaussian process coregionalization model to conceptualize responses of grouped survey batteries but adjusted to non-Gaussian ordinal data, and exploits stochastic variational inference for latent factor estimation. Using both synthetic data and a novel survey, we show that IPGP improves both prediction of actual responses and estimation of intrapersonal response patterns compared to existing benchmarks. In the survey study, IPGP also identifies unique clusters of personality taxonomies, displaying great potential in advancing individualized approaches to psychological diagnosis.",main,NeurIPS,2024,Poster,Yehu Chen;Muchen Xi;Joshua J. Jackson;Jacob Montgomery;Roman Garnett,True,https://openreview.net/pdf?id=Twqa0GFMGX
TyIWrwzpgu,Arctique: An artificial histopathological dataset unifying realism and controllability for uncertainty quantification,"Uncertainty Quantification (UQ) is crucial for reliable image segmentation. Yet, while the field sees continual development of novel methods, a lack of agreed-upon benchmarks limits their systematic comparison and evaluation: Current UQ methods are typically tested either on overly simplistic toy datasets or on complex real-world datasets that do not allow to discern true uncertainty. To unify both controllability and complexity, we introduce Arctique, a procedurally generated dataset modeled after histopathological colon images. We chose histopathological images for two reasons: 1) their complexity in terms of intricate object structures and highly variable appearance, which yields challenging segmentation problems, and 2) their broad prevalence for medical diagnosis and respective relevance of high-quality UQ. To generate Arctique, we established a Blender-based framework for 3D scene creation with intrinsic noise manipulation. Arctique contains up to 50,000 rendered images with precise masks as well as noisy label simulations. We show that by independently controlling the uncertainty in both images and labels, we can effectively study the performance of several commonly used UQ methods. Hence, Arctique serves as a critical resource for benchmarking and advancing UQ techniques and other methodologies in complex, multi-object environments, bridging the gap between realism and controllability. All code is publicly available, allowing re-creation and controlled manipulations of our shipped images as well as creation and rendering of new scenes.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jannik Franzen;Claudia Winklmayr;Vanessa Emanuela Guarino;Christoph Karg;Xiaoyan Yu;Nora Koreuber;Jan Philipp Albrecht;Philip Bischoff;Dagmar Kainmueller,True,https://openreview.net/pdf?id=TyIWrwzpgu
U2aVNDrZGx,Benchmarking Complex Instruction-Following with Multiple Constraints Composition,"Instruction following is one of the fundamental capabilities of large language models (LLMs). As the ability of LLMs is constantly improving, they have been increasingly applied to deal with complex human instructions in real-world scenarios. Therefore, how to evaluate the ability of complex instruction-following of LLMs has become a critical research problem. Existing benchmarks mainly focus on modeling different types of constraints in human instructions while neglecting the composition of different constraints, which is an indispensable constituent in complex instructions. To this end, we propose ComplexBench, a benchmark for comprehensively evaluating the ability of LLMs to follow complex instructions composed of multiple constraints. We propose a hierarchical taxonomy for complex instructions, including 4 constraint types, 19 constraint dimensions, and 4 composition types, and manually collect a high-quality dataset accordingly. To make the evaluation reliable, we augment LLM-based evaluators with rules to effectively verify whether generated texts can satisfy each constraint and composition. Furthermore, we obtain the final evaluation score based on the dependency structure determined by different composition types. ComplexBench identifies significant deficiencies in existing LLMs when dealing with complex instructions with multiple constraints composition.",Datasets & Benchmarks,NeurIPS,2024,Poster,Bosi Wen;Pei Ke;Xiaotao Gu;Lindong Wu;Hao Huang;Jinfeng Zhou;Wenchuang Li;Binxin Hu;Wendy Gao;Jiaxing Xu;Yiming Liu;Jie Tang;Hongning Wang;Minlie Huang,True,https://openreview.net/pdf?id=U2aVNDrZGx
U2pNwSuQqD,Needle In A Multimodal Haystack,"With the rapid advancement of multimodal large language models (MLLMs), their evaluation has become increasingly comprehensive. However, understanding long multimodal content, as a foundational ability for real-world applications, remains underexplored. In this work, we present Needle In A Multimodal Haystack (MM-NIAH), the first benchmark specifically designed to systematically evaluate the capability of existing MLLMs to comprehend long multimodal documents. Our benchmark includes three types of evaluation tasks: multimodal retrieval, counting, and reasoning. In each task, the model is required to answer the questions according to different key information scattered throughout the given multimodal document. Evaluating the leading MLLMs on MM-NIAH, we observe that existing models still have significant room for improvement on these tasks, especially on vision-centric evaluation. We hope this work can provide a platform for further research on long multimodal document comprehension and contribute to the advancement of MLLMs. Code and benchmark are released at https://github.com/OpenGVLab/MM-NIAH.",Datasets & Benchmarks,NeurIPS,2024,Poster,Weiyun Wang;Shuibo Zhang;Yiming Ren;Yuchen Duan;Tiantong Li;Shuo Liu;Mengkang Hu;Zhe Chen;Kaipeng Zhang;Lewei Lu;Xizhou Zhu;Ping Luo;Yu Qiao;Jifeng Dai;Wenqi Shao;Wenhai Wang,True,https://openreview.net/pdf?id=U2pNwSuQqD
U3hQoqgQDJ,Interfacing Foundation Models' Embeddings,"Foundation models possess strong capabilities in reasoning and memorizing across modalities. To further unleash the power of foundation models, we present FIND, a generalized interface for aligning foundation models' embeddings with unified image and dataset-level understanding spanning modality and granularity. As shown in Fig.1, a lightweight transformer interface without tuning any foundation model weights is enough for segmentation, grounding, and retrieval in an interleaved manner. The proposed interface has the following favorable attributes: (1) Generalizable. It applies to various tasks spanning retrieval, segmentation, etc., under the same architecture and weights.  (2) Interleavable. With the benefit of multi-task multi-modal training, the proposed interface creates an interleaved shared embedding space. (3) Extendable. The proposed interface is adaptive to new tasks, and new models. In light of the interleaved embedding space, we introduce FIND-Bench, which introduces new training and evaluation annotations to the COCO dataset for interleaved segmentation and retrieval. We are the first work aligning foundations models' embeddings for interleave understanding. Meanwhile, our approach achieves state-of-the-art performance on FIND-Bench and competitive performance on standard retrieval and segmentation settings.",main,NeurIPS,2024,Poster,Xueyan Zou;Linjie Li;Jianfeng Wang;Jianwei Yang;Mingyu Ding;Junyi Wei;Zhengyuan Yang;Feng Li;Hao Zhang;Shilong Liu;Arul Aravinthan;Yong Jae Lee;Lijuan Wang,True,https://openreview.net/pdf?id=U3hQoqgQDJ
UDC8D6U7dX,TAPVid-3D: A Benchmark for Tracking Any Point in 3D,"We introduce a new benchmark, TAPVid-3D, for evaluating the task of long-range Tracking Any Point in 3D (TAP-3D). While point tracking in two dimensions (TAP-2D) has many benchmarks measuring performance on real-world videos, such as TAPVid-DAVIS, three-dimensional point tracking has none. To this end, leveraging existing footage, we build a new benchmark for 3D point tracking featuring 4,000+ real-world videos, composed of three different data sources spanning a variety of object types, motion patterns, and indoor and outdoor environments. To measure performance on the TAP-3D task, we formulate a collection of metrics that extend the Jaccard-based metric used in TAP-2D to handle the complexities of ambiguous depth scales across models, occlusions, and multi-track spatio-temporal smoothness. We manually verify a large sample of trajectories to ensure correct video annotations, and assess the current state of the TAP-3D task by constructing competitive baselines using existing tracking models. We anticipate this benchmark will serve as a guidepost to improve our ability to understand precise 3D motion and surface deformation from monocular video.",Datasets & Benchmarks,NeurIPS,2024,Poster,Skanda Koppula;Ignacio Rocco;Yi Yang;Joseph Heyward;Joao Carreira;Andrew Zisserman;Gabriel Brostow;Carl Doersch,True,https://openreview.net/pdf?id=UDC8D6U7dX
UGwdz3kjht,Prioritize Alignment in Dataset Distillation,"Dataset Distillation aims to compress a large dataset into a significantly more compact, synthetic one without compromising the performance of the trained models. To achieve this, existing methods use the agent model to extract information from the target dataset and embed it into the distilled dataset. Consequently, the quality of extracted and embedded information determines the quality of the distilled dataset. In this work, we find that existing methods introduce misaligned information in both information extraction and embedding stages. To alleviate this, we propose Prioritize Alignment in Dataset Distillation (PAD), which aligns information from the following two perspectives. 1) We prune the target dataset according to the compressing ratio to filter the information that can be extracted by the agent model. 2) We use only deep layers of the agent model to perform the distillation to avoid excessively introducing low-level information. This simple strategy effectively filters out misaligned information and brings non-trivial improvement for mainstream matching-based distillation algorithms. Furthermore, built on trajectory matching, PAD achieves remarkable improvements on various benchmarks, achieving state-of-the-art performance. The code and distilled datasets will be made public.",main,NeurIPS,2024,Reject,Zekai Li;Ziyao Guo;Wangbo Zhao;Tianle Zhang;Zhi-Qi Cheng;Samir Khaki;Kaipeng Zhang;Ahmad Sajedi;Kai Wang;Konstantinos N Plataniotis;Yang You,True,https://openreview.net/pdf?id=UGwdz3kjht
USUkwg5pW6,Scribbles for All: Benchmarking Scribble Supervised Segmentation Across Datasets,"In this work, we introduce *Scribbles for All*, a label and training data generation algorithm for semantic segmentation trained on scribble labels. Training or fine-tuning semantic segmentation models with weak supervision has become an important topic recently and was subject to significant advances in model quality. In this setting, scribbles are a promising label type to achieve high quality segmentation results while requiring a much lower annotation effort than usual pixel-wise dense semantic segmentation annotations. The main limitation of scribbles as source for weak supervision is the lack of challenging datasets for scribble segmentation, which hinders the development of novel methods and conclusive evaluations. To overcome this limitation, *Scribbles for All* provides scribble labels for several popular segmentation datasets and provides an algorithm to automatically generate scribble labels for any dataset with dense annotations, paving the way for new insights and model advancements in the field of weakly supervised segmentation. In addition to providing datasets and algorithm, we evaluate state-of-the-art segmentation models on our datasets and show that models trained with our synthetic labels perform competitively with respect to models trained on manual labels. Thus, our datasets enable state-of-the-art research into methods for scribble-labeled semantic segmentation. The datasets, scribble generation algorithm, and baselines are publicly available at https://github.com/wbkit/Scribbles4All.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Wolfgang Boettcher;Lukas Hoyer;Ozan Unal;Jan Eric Lenssen;Bernt Schiele,True,https://openreview.net/pdf?id=USUkwg5pW6
UYgE9IfQIV,SustainDC: Benchmarking for Sustainable Data Center Control,"Machine learning has driven an exponential increase in computational demand, leading to massive data centers that consume significant amounts of energy and contribute to climate change. This makes sustainable data center control a priority. In this paper, we introduce SustainDC, a set of Python environments for benchmarking multi-agent reinforcement learning (MARL) algorithms for data centers (DC). SustainDC supports custom DC configurations and tasks such as workload scheduling, cooling optimization, and auxiliary battery management, with multiple agents managing these operations while accounting for the effects of each other. We evaluate various MARL algorithms on SustainDC, showing their performance across diverse DC designs, locations, weather conditions, grid carbon intensity, and workload requirements. Our results highlight significant opportunities for improvement of data center operations using MARL algorithms. Given the increasing use of DC due to AI, SustainDC provides a crucial platform for the development and benchmarking of advanced algorithms essential for achieving sustainable computing and addressing other heterogeneous real-world challenges.",Datasets & Benchmarks,NeurIPS,2024,Poster,Avisek Naug;Antonio Guillen;Ricardo Luna Gutierrez;Vineet Gundecha;Cullen Bash;Sahand Ghorbanpour;Sajad Mousavi;Ashwin Ramesh Babu;Dejan Markovikj;Lekhapriya Dheeraj Kashyap;Desik Rengarajan;Soumyendu Sarkar,False,https://openreview.net/pdf?id=UYgE9IfQIV
UZpySDOwvZ,DF40: Toward Next-Generation Deepfake Detection,"We propose a new comprehensive benchmark to revolutionize the current deepfake detection field to the next generation. Predominantly, existing works identify top-notch detection algorithms and models by adhering to the common practice: training detectors on one specific dataset (*e.g.,* FF++) and testing them on other prevalent deepfake datasets. This protocol is often regarded as a ""golden compass"" for navigating SoTA detectors. But can these stand-out ""winners"" be truly applied to tackle the myriad of realistic and diverse deepfakes lurking in the real world? If not, what underlying factors contribute to this gap? In this work, we found the **dataset** (both train and test) can be the ""primary culprit"" due to the following: (1) *forgery diversity*: Deepfake techniques are commonly referred to as both face forgery (face-swapping and face-reenactment) and entire image synthesis (AIGC, especially face). Most existing datasets only contain partial types of them, with limited forgery methods implemented (*e.g.,* 2 swapping and 2 reenactment methods in FF++); (2) *forgery realism*: The dominated training dataset, FF++, contains out-of-date forgery techniques from the past four years. ""Honing skills"" on these forgeries makes it difficult to guarantee effective detection generalization toward nowadays' SoTA deepfakes; (3) *evaluation protocol*: Most detection works perform evaluations on one type, *e.g.,* face-swapping types only, which hinders the development of universal deepfake detectors.

To address this dilemma, we construct a highly diverse and large-scale deepfake detection dataset called **DF40**,  which comprises **40** distinct deepfake techniques (10 times larger than FF++). We then conduct comprehensive evaluations using **4** standard evaluation protocols and **8** representative detection methods, resulting in over **2,000** evaluations. Through these evaluations, we provide an extensive analysis from various perspectives, leading to **7** new insightful findings contributing to the field. We also open up **4** valuable yet previously underexplored research questions to inspire future works. We release our dataset, code, and pre-trained weights at https://github.com/YZY-stack/DF40.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zhiyuan Yan;Taiping Yao;Shen Chen;Yandan Zhao;Xinghe Fu;Junwei Zhu;Donghao Luo;Chengjie Wang;Shouhong Ding;Yunsheng Wu;Li Yuan,True,https://openreview.net/pdf?id=UZpySDOwvZ
Ul3lDYo3XQ,AGILE: A Novel Reinforcement Learning Framework of LLM Agents,"We introduce a novel reinforcement learning framework of LLM agents named AGILE (AGent that Interacts and Learns from Environments)  designed to perform complex conversational tasks with users, leveraging LLMs, memory, tools, and interactions with experts. The agent possesses capabilities beyond conversation, including reflection, tool usage, and expert consultation. We formulate the construction of such an LLM agent as a reinforcement learning (RL) problem, in which the LLM serves as the policy model. We fine-tune the LLM using labeled data of actions and the PPO algorithm. We focus on question answering and release a dataset for agents called ProductQA, comprising challenging questions in online shopping. Our extensive experiments on ProductQA, MedMCQA and HotPotQA show that AGILE agents based on 7B and 13B LLMs trained with PPO can outperform GPT-4 agents. Our ablation study highlights the indispensability of memory, tools, consultation, reflection, and reinforcement learning in achieving the agent's strong performance. Datasets and code are available at https://github.com/bytarnish/AGILE.",main,NeurIPS,2024,Poster,Peiyuan Feng;Yichen He;Guanhua Huang;Yuan Lin;Hanchong Zhang;Yuchen Zhang;Hang Li,True,https://openreview.net/pdf?id=Ul3lDYo3XQ
UnWhcpIyUC,"Me, Myself, and AI: The Situational Awareness Dataset (SAD) for LLMs","AI assistants such as ChatGPT are trained to respond to users by saying, ""I am a large language model”.
This raises questions. Do such models ""know'' that they are LLMs and reliably act on this knowledge? Are they ""aware"" of their current circumstances, such as being deployed to the public?
We refer to a model's knowledge of itself and its circumstances as **situational awareness**.
To quantify situational awareness in LLMs, we introduce a range of behavioral tests, based on question answering and instruction following. These tests form the **Situational Awareness Dataset (SAD)**, a benchmark comprising 7 task categories and over 13,000 questions.
The benchmark tests numerous abilities, including the capacity of LLMs to (i) recognize their own generated text, (ii) predict their own behavior, (iii) determine whether a prompt is from internal evaluation or real-world deployment, and (iv) follow instructions that depend on self-knowledge.
We evaluate 16 LLMs on SAD, including both base (pretrained) and chat models.
While all models perform better than chance, even the highest-scoring model (Claude 3 Opus) is far from a human baseline on certain tasks. We also observe that performance on SAD is only partially predicted by metrics of general knowledge. 
Chat models, which are finetuned to serve as AI assistants, outperform their corresponding base models on SAD but not on general knowledge tasks.
The purpose of SAD is to facilitate scientific understanding of situational awareness in LLMs by breaking it down into quantitative abilities. Situational awareness is important because it enhances a model's capacity for autonomous planning and action. While this has potential benefits from automation, it also introduces novel risks related to AI safety and control.",Datasets & Benchmarks,NeurIPS,2024,Poster,Rudolf Laine;Bilal Chughtai;Jan Betley;Kaivalya Hariharan;Mikita Balesni;Jérémy Scheurer;Marius Hobbhahn;Alexander Meinke;Owain Evans,True,https://openreview.net/pdf?id=UnWhcpIyUC
V6891G9dWu,"M2Lingual: Enhancing Multilingual, Multi-Turn Instruction Alignment in Large Language Models","Instruction finetuning (IFT) is critical for aligning Large Language Models (LLMs) to follow instructions. Numerous effective IFT datasets have been proposed in the recent past, but most focus on rich resourced languages such as English. In this work, we propose a diverse, task taxonomy guided, fully synthetic Multilingual, Multi-turn evoled instruction finetuning dataset, called M2Lingual, to better align LLMs on a diverse set of languages and tasks. M2Lingual contains a total of 182K IFT pairs that are built upon diverse seeds collected from Aya collection and Aya dataset covering 70 languages, 19 NLP tasks and general instruction-response pairs. LLMs finetuned with M2Lingual substantially outperform the majority of existing multilingual IFT datasets. Importantly, LLMs trained with M2Lingual consistently competitive results across wide variety of evaluation benchmarks compared to existing multilingual IFT datasets that enable LLMs performance in only one or a few subset of the benchmarks. Specifically, LLMs finetuned with M2Lingual achieve strong performance on multi-turn evaluation benchmarks such as MT-bench and across wide-variety of multilingual tasks such as XQuAD, MGSM, TyDiQA, MLQA, XNLI and XLSUM. We show efficacy of M2Lingual across LLMs with different sizes, especially smaller LLMs with 1.8B size which benefit massively from our dataset. Lastly, we present key analyses to highlight importance of each synthesis step of M2Lingual.",Datasets & Benchmarks,NeurIPS,2024,Reject,Rishabh Maheshwary;Vikas Yadav;Hoang H Nguyen;Khyati Mahajan;Sathwik Tejaswi Madhusudhan,True,https://openreview.net/pdf?id=V6891G9dWu
VHa0XNjWj2,VLM4Bio: A Benchmark Dataset to Evaluate Pretrained Vision-Language Models for Trait Discovery from Biological Images,"Images are increasingly becoming the currency for documenting biodiversity on the planet, providing novel opportunities for accelerating scientific discoveries in the field of organismal biology, especially with the advent of large vision-language models (VLMs). We ask if pre-trained VLMs can aid scientists in answering a range of biologically relevant questions without any additional fine-tuning. In this paper, we evaluate the effectiveness of $12$ state-of-the-art (SOTA) VLMs in the field of organismal biology using a novel dataset, VLM4Bio, consisting of $469K$ question-answer pairs involving $30K$ images from three groups of organisms: fishes, birds, and butterflies, covering five biologically relevant tasks. We also explore the effects of applying prompting techniques and tests for reasoning hallucination on the performance of VLMs, shedding new light on the capabilities of current SOTA VLMs in answering biologically relevant questions using images.",Datasets & Benchmarks,NeurIPS,2024,Poster,M. Maruf;Arka Daw;Kazi Sajeed Mehrab;Harish Babu Manogaran;Abhilash Neog;Medha Sawhney;Mridul Khurana;James Balhoff;Yasin Bakis;Bahadir Altintas;Matthew J Thompson;Elizabeth G Campolongo;Josef Uyeda;Hilmar Lapp;Henry Bart;Paula Mabee;Yu Su;Wei-Lun Chao;Charles Stewart;Tanya Berger-Wolf;Wasila M Dahdul;Anuj Karpatne,True,https://openreview.net/pdf?id=VHa0XNjWj2
VHva3d836i,WizardArena: Post-training Large Language Models via Simulated Offline Chatbot Arena,"Recent work demonstrates that, post-training large language models with open-domain instruction following data have achieved colossal success. Simultaneously, human Chatbot Arena has emerged as one of the most reasonable benchmarks for model evaluation and developmental guidance. However, the processes of manually curating high-quality training data and utilizing online human evaluation platforms are both expensive and limited. To mitigate the manual and temporal costs associated with post-training, this paper introduces a Simulated Chatbot Arena named WizardArena, which is fully based on and powered by open-source LLMs. For evaluation scenario, WizardArena can efficiently predict accurate performance rankings among different models based on offline test set. For training scenario, we simulate arena battles among various state-of-the-art models on a large scale of instruction data, subsequently leveraging the battle results to constantly enhance target model in both the supervised fine-tuning and reinforcement learning . Experimental results demonstrate that our WizardArena aligns closely with the online human arena rankings, and our models trained on offline extensive battle data exhibit significant performance improvements during SFT, DPO, and PPO stages.",main,NeurIPS,2024,Poster,Haipeng Luo;Qingfeng Sun;Can Xu;Pu Zhao;Qingwei Lin;Jian-Guang Lou;Shifeng Chen;Yansong Tang;Weizhu Chen,True,https://openreview.net/pdf?id=VHva3d836i
VIlyDguGEz,Learning Where to Edit Vision Transformers,"Model editing aims to data-efficiently correct predictive errors of large pre-trained models while ensuring generalization to neighboring failures and locality to minimize unintended effects on unrelated examples. While significant progress has been made in editing Transformer-based large language models, effective strategies for editing vision Transformers (ViTs) in computer vision remain largely untapped. In this paper, we take initial steps towards correcting predictive errors of ViTs, particularly those arising from subpopulation shifts. Taking a locate-then-edit approach, we first address the ``where-to-edit`` challenge by meta-learning a hypernetwork on CutMix-augmented data generated for editing reliability. This trained hypernetwork produces generalizable binary masks that identify a sparse subset of structured model parameters,  responsive to real-world failure samples. Afterward, we solve the ``how-to-edit`` problem by simply fine-tuning the identified parameters using a variant of gradient descent to achieve successful edits. To validate our method, we construct an editing benchmark that introduces subpopulation shifts towards natural underrepresented images and AI-generated images, thereby revealing the limitations of pre-trained ViTs for object recognition. Our approach not only achieves superior performance on the proposed benchmark but also allows for adjustable trade-offs between generalization and locality. Our code is available at https://github.com/hustyyq/Where-to-Edit.",main,NeurIPS,2024,Poster,Yunqiao Yang;Long-Kai Huang;Shengzhuang Chen;Kede Ma;Ying Wei,True,https://openreview.net/pdf?id=VIlyDguGEz
VJuSeShdZA,Hidden in Plain Sight: Evaluating Abstract Shape Recognition in Vision-Language Models,"Despite the importance of shape perception in human vision, early neural image classifiers relied less on shape information for object recognition than other (often spurious) features. While recent research suggests that current large Vision-Language Models (VLMs) exhibit more reliance on shape, we find them to still be seriously limited in this regard. To quantify such limitations, we introduce IllusionBench, a dataset that challenges current cutting-edge VLMs to decipher shape information when the shape is represented by an arrangement of visual elements in a scene. Our extensive evaluations reveal that, while these shapes are easily detectable by human annotators, current VLMs struggle to recognize them, indicating important avenues for future work in developing more robust visual perception systems. The full dataset and codebase are available at: https://arshiahemmat.github.io/illusionbench/",Datasets & Benchmarks,NeurIPS,2024,Poster,Arshia Hemmat;Adam Davies;Tom A. Lamb;Jianhao Yuan;Philip Torr;Ashkan Khakzar;Francesco Pinto,True,https://openreview.net/pdf?id=VJuSeShdZA
VVDewLcVkx,GameBench: Evaluating Strategic Reasoning Abilities of LLM Agents,"Large language models have demonstrated remarkable few-shot performance on many natural language understanding tasks. Despite several demonstrations of using large language models in complex, strategic scenarios, there lacks a comprehensive framework for evaluating agents’ performance across various types of reasoning found in games. To address this gap, we introduce \\\\textsc{GameBench}, a cross-domain benchmark for evaluating strategic reasoning abilities of LLM agents. We focus on 9 different game environments, where each covers at least one axis of key reasoning skill identified in strategy games, and select games for which strategy explanations are unlikely to form a significant portion of models' pretraining corpuses. Our evaluations use GPT-3 and GPT-4 in their base form along with two scaffolding frameworks designed to enhance strategic reasoning ability: Chain-of-Thought (CoT) prompting and Reasoning Via Planning (RAP). Our results show that none of the tested models match human performance, and at worse GPT-4 performs worse than random action. CoT and RAP both improve scores but not comparable to human levels.",Datasets & Benchmarks,NeurIPS,2024,Reject,Anthony Costarelli;Mat Allen;Roman Hauksson;Grace Sodunke;Suhas Hariharan;Carlson Cheng;Arjun Yadav,True,https://openreview.net/pdf?id=VVDewLcVkx
VXohja0vrQ,MedCalc-Bench: Evaluating Large Language Models for Medical Calculations,"Current benchmarks for evaluating large language models (LLMs) in medicine are primarily focused on question-answering involving domain knowledge and descriptive reasoning. While such qualitative capabilities are vital to medical diagnosis, in real-world scenarios, doctors frequently use clinical calculators that follow quantitative equations and rule-based reasoning paradigms for evidence-based decision support. To this end, we propose MedCalc-Bench, a first-of-its-kind dataset focused on evaluating the medical calculation capability of LLMs. MedCalc-Bench contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks. Each instance in MedCalc-Bench consists of a patient note, a question requesting to compute a specific medical value, a ground truth answer, and a step-by-step explanation showing how the answer is obtained. While our evaluation results show the potential of LLMs in this area, none of them are effective enough for clinical settings. Common issues include extracting the incorrect entities, not using the correct equation or rules for a calculation task, or incorrectly performing the arithmetic for the computation. We hope our study highlights the quantitative knowledge and reasoning gaps in LLMs within medical settings, encouraging future improvements of LLMs for various clinical calculation tasks. MedCalc-Bench is publicly available at: https://github.com/ncbi-nlp/MedCalc-Bench.",Datasets & Benchmarks,NeurIPS,2024,Oral,Nikhil Khandekar;Qiao Jin;Guangzhi Xiong;Soren Dunn;Serina S Applebaum;Zain Anwar;Maame Sarfo-Gyamfi;Conrad W Safranek;Abid Anwar;Andrew Jiaxing Zhang;Aidan Gilson;Maxwell B Singer;Amisha D Dave;R. Andrew Taylor;Aidong Zhang;Qingyu Chen;Zhiyong Lu,True,https://openreview.net/pdf?id=VXohja0vrQ
VZQmIoDGBG,SafeWorld: Geo-Diverse Safety Alignment,"In the rapidly evolving field of Large Language Models (LLMs), ensuring safety is a crucial and widely discussed topic. However, existing works often overlooks the geo-diversity of cultural and legal standards across the world. To reveal the chal5 lenges posed by geo-diverse safety standards, we introduce SafeWorld, a novel benchmark specifically designed to evaluate LLMs’ ability to generate responses that are not only helpful but also culturally sensitive and legally compliant across diverse global contexts. SafeWorld encompasses 2,775 test user queries, each grounded in high-quality, human-verified cultural norms and legal policies from 50 countries and 493 regions/races. On top of it, we propose a multi-dimensional automatic safety evaluation framework that assesses the contextual appropriateness, accuracy, and comprehensiveness of responses. Our evaluations reveal that current LLMs struggle to meet these criteria effectively. To enhance LLMs’ alignment with geo-diverse safety standards, we synthesize helpful preference pairs for Direct Preference Optimization (DPO) alignment. The preference pair construction aims to encourage LLMs to behave appropriately and provide precise references to relevant cultural norms and policies when necessary. Our trained SafeWorldLM outperforms all competing models, including GPT-4o on all the three evaluation dimensions by a large margin. Global human evaluators also note a nearly 20% higher winning rate in helpfulness and harmfulness evaluation.",main,NeurIPS,2024,Poster,Da Yin;Haoyi Qiu;Kung-Hsiang Huang;Kai-Wei Chang;Nanyun Peng,True,https://openreview.net/pdf?id=VZQmIoDGBG
Vb1vVr75JT,UniTox: Leveraging LLMs to Curate a Unified Dataset of Drug-Induced Toxicity from FDA Labels,"Drug-induced toxicity is one of the leading reasons new drugs fail clinical trials. Machine learning models that predict drug toxicity from molecular structure could help researchers prioritize less toxic drug candidates. However, current toxicity datasets are typically small and limited to a single organ system (e.g., cardio, renal, or liver). Creating these datasets often involved time-intensive expert curation by parsing drug labelling documents that can exceed 100 pages per drug. Here, we introduce UniTox, a unified dataset of 2,418 FDA-approved drugs with drug-induced toxicity summaries and ratings created by using GPT-4o to process FDA drug labels. UniTox spans eight types of toxicity: cardiotoxicity, liver toxicity, renal toxicity, pulmonary toxicity, hematological toxicity, dermatological toxicity, ototoxicity, and infertility. This is, to the best of our knowledge, the largest such systematic human in vivo database by number of drugs and toxicities, and the first covering nearly all non-combination FDA-approved medications for several of these toxicities. We recruited clinicians to validate a random sample of our GPT-4o annotated toxicities, and UniTox's toxicity ratings concord with clinician labelers 85-96\\\\% of the time. Finally, we benchmark several machine learning models trained on UniTox to demonstrate the utility of this dataset for building molecular toxicity prediction models.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Jake Silberg;Kyle Swanson;Elana Simon;Angela Zhang;Zaniar Ghazizadeh;Scott Ogden;Hisham Hamadeh;James Zou,True,https://openreview.net/pdf?id=Vb1vVr75JT
VcPtU8e6yK,"$\\\\textit{Bifr\\\\""ost}$: 3D-Aware Image Compositing with Language Instructions","This paper introduces $\\\\textit{Bifröst}$, a novel 3D-aware framework that is built upon diffusion models to perform instruction-based image composition. Previous methods concentrate on image compositing at the 2D level, which fall short in handling complex spatial relationships ($\\\\textit{e.g.}$, occlusion). $\\\\textit{Bifröst}$ addresses these issues by training MLLM as a 2.5D location predictor and integrating depth maps as an extra condition during the generation process to bridge the gap between 2D and 3D, which enhances spatial comprehension and supports sophisticated spatial interactions. Our method begins by fine-tuning MLLM with a custom counterfactual dataset to predict 2.5D object locations in complex backgrounds from language instructions. Then, the image-compositing model is uniquely designed to process multiple types of input features, enabling it to perform high-fidelity image compositions that consider occlusion, depth blur, and image harmonization. Extensive qualitative and quantitative evaluations demonstrate that $\\\\textit{Bifröst}$ significantly outperforms existing methods, providing a robust solution for generating realistically composited images in scenarios demanding intricate spatial understanding. This work not only pushes the boundaries of generative image compositing but also reduces reliance on expensive annotated datasets by effectively utilizing existing resources in innovative ways.",main,NeurIPS,2024,Poster,Lingxiao Li;Kaixiong Gong;Wei-Hong Li;Xili Dai;Tao Chen;Xiaojun Yuan;Xiangyu Yue,True,https://openreview.net/pdf?id=VcPtU8e6yK
Vcw3vzjHDb,Lean Workbook: A large-scale Lean problem set formalized from natural language math problems,"Large language models have demonstrated impressive capabilities across various natural language processing tasks, especially in solving mathematical problems. However, large language models are not good at math theorem proving using formal languages like Lean. A significant challenge in this area is the scarcity of training data available in these formal languages. To address this issue, we propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements, and vice versa. Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs in translating and understanding complex mathematical problems and proofs. Our final dataset contains about 57K formal-informal question pairs along with searched proof from the math contest forum and 21 new IMO questions. We open-source our code at \\\\url{https://github.com/InternLM/InternLM-Math} and our data at \\\\url{https://huggingface.co/datasets/InternLM/Lean-Workbook}.",Datasets & Benchmarks,NeurIPS,2024,Poster,Huaiyuan Ying;Zijian Wu;Yihan Geng;JIayu Wang;Dahua Lin;Kai Chen,True,https://openreview.net/pdf?id=Vcw3vzjHDb
Vi8AepAXGy,"Cambrian-1: A Fully Open, Vision-Centric Exploration of Multimodal LLMs","We introduce Cambrian-1, a family of multimodal LLMs (MLLMs) designed with a vision-centric approach. While stronger language models can enhance multimodal capabilities, the design choices for vision components are often insufficiently explored and disconnected from visual representation learning research. This gap hinders accurate sensory grounding in real-world scenarios. Our study uses LLMs and visual instruction tuning as an interface to evaluate various visual representations, offering new insights into different models and architectures—self-supervised, strongly supervised, or combinations thereof—based on experiments with over 15 vision models. We critically examine existing MLLM benchmarks, addressing the difficulties involved in consolidating and interpreting results from various tasks. To further improve visual grounding, we propose spatial vision aggregator (SVA), a dynamic and spatially-aware connector that integrates vision features with LLMs while reducing the number of tokens. Additionally, we discuss the curation of high-quality visual instruction-tuning data from publicly available sources, emphasizing the importance of distribution balancing. Collectively, Cambrian-1 not only achieves state-of-the-art performances but also serves as a comprehensive, open cookbook for instruction-tuned MLLMs. We provide model weights, code, supporting tools, datasets, and detailed instruction-tuning and evaluation recipes. We hope our release will inspire and accelerate advancements in multimodal systems and visual representation learning.",main,NeurIPS,2024,Oral,Shengbang Tong;Ellis L Brown II;Penghao Wu;Sanghyun Woo;ADITHYA JAIRAM IYER;Sai Charitha Akula;Shusheng Yang;Jihan Yang;Manoj Middepogu;Ziteng Wang;Xichen Pan;Rob Fergus;Yann LeCun;Saining Xie,True,https://openreview.net/pdf?id=Vi8AepAXGy
ViTUlZvPDu,Robust Fine-tuning of Zero-shot Models via Variance Reduction,"When fine-tuning zero-shot models like CLIP, our desideratum is for the fine-tuned model to excel in both in-distribution (ID) and out-of-distribution (OOD). Recently, ensemble-based models (ESM) have been shown to offer significant robustness improvement, while preserving high ID accuracy. However, our study finds that ESMs do not solve the ID-OOD trade-offs: they achieve peak performance for ID and OOD accuracy at different mixing coefficients. When optimized for OOD accuracy, the ensemble model exhibits a noticeable decline in ID accuracy, and vice versa. In contrast, we propose a sample-wise ensembling technique that can simultaneously attain the best ID and OOD accuracy without the trade-offs. Specifically, we construct a Zero-Shot Failure (ZSF) set containing training samples incorrectly predicted by the zero-shot model. For each test sample, we calculate its distance to the ZSF set and assign a higher weight to the fine-tuned model in the ensemble if the distance is small. We term our method Variance Reduction Fine-tuning (VRF), as it effectively reduces the variance in ensemble predictions, thereby decreasing residual error. On ImageNet and five derived distribution shifts, our VRF further improves the OOD accuracy by 1.5 - 2.0 pp over the ensemble baselines while maintaining or increasing ID accuracy. VRF achieves similar large robustness gains on (0.9 - 3.1 pp) on other distribution shifts
19 benchmarks. Codes are available in https://github.com/BeierZhu/VRF.",main,NeurIPS,2024,Poster,Beier Zhu;Jiequan Cui;Hanwang Zhang,True,https://openreview.net/pdf?id=ViTUlZvPDu
VikufBLOW1,Web-Scale Visual Entity Recognition: An LLM-Driven Data Approach,"Web-scale visual entity recognition, the task of associating images with their corresponding entities within vast knowledge bases like Wikipedia, presents significant challenges due to the lack of clean, large-scale training data. In this paper, we propose a novel methodology to curate such a dataset, leveraging a multimodal large language model (LLM) for label verification, metadata generation, and rationale explanation. Instead of relying on the multimodal LLM to directly annotate data, which we found to be suboptimal, we prompt it to reason about potential candidate entity labels by accessing additional contextually relevant information (such as Wikipedia), resulting in more accurate annotations. We further use the multimodal LLM to enrich the dataset by generating question-answer pairs and a grounded fine-grained textual description (referred to as ""rationale"") that explains the connection between images and their assigned entities. Experiments demonstrate that models trained on this automatically curated data achieve state-of-the-art performance on web-scale visual entity recognition tasks (e.g. +6.9% improvement in OVEN entity task), underscoring the importance of high-quality training data in this domain.",main,NeurIPS,2024,Poster,Mathilde Caron;Alireza Fathi;Cordelia Schmid;Ahmet Iscen,True,https://openreview.net/pdf?id=VikufBLOW1
Vp6HAjrdIg,FiVA: Fine-grained Visual Attribute Dataset for Text-to-Image Diffusion Models,"Recent advances in text-to-image generation have enabled the creation of high-quality images with diverse applications. However, accurately describing desired visual attributes can be challenging, especially for non-experts in art and photography. An intuitive solution involves adopting favorable attributes from source images. Current methods attempt to distill identity and style from source images. However, ""style"" is a broad concept that includes texture, color, and artistic elements, but does not cover other important attributes like lighting and dynamics. Additionally, a simplified ""style"" adaptation prevents combining multiple attributes from different sources into one generated image. In this work, we formulate a more effective approach to decompose the aesthetics of a picture into specific visual attributes, letting users apply characteristics like lighting, texture, and dynamics from different images. To achieve this goal, we constructed the first fine-grained visual attributes dataset (FiVA) to the best of our knowledge. This FiVA dataset features a well-organized taxonomy for visual attributes and includes 1 M high-quality generated images with visual attribute annotations. Leveraging this dataset, we propose a fine-grained visual attributes adaptation framework (FiVA-Adapter) , which decouples and adapts visual attributes from one or more source images into a generated one. This approach enhances user-friendly customization, allowing users to selectively apply desired attributes to create images that meet their unique preferences and specific content requirements.",Datasets & Benchmarks,NeurIPS,2024,Poster,Tong Wu;Yinghao Xu;Ryan Po;Mengchen Zhang;Guandao Yang;Jiaqi Wang;Ziwei Liu;Dahua Lin;Gordon Wetzstein,True,https://openreview.net/pdf?id=Vp6HAjrdIg
VpkfxuVXwx,PrivAuditor: Benchmarking Data Protection Vulnerabilities in LLM Adaptation Techniques,"Large Language Models (LLMs) are recognized for their potential to be an important building block toward achieving artificial general intelligence due to their unprecedented capability for solving diverse tasks. Despite these achievements, LLMs often underperform in domain-specific tasks without training on relevant domain data. This phenomenon, which is often attributed to distribution shifts, makes adapting pre-trained LLMs with domain-specific data crucial. However, this adaptation raises significant privacy concerns, especially when the data involved come from sensitive domains. In this work, we extensively investigate the privacy vulnerabilities of adapted (fine-tuned) LLMs and benchmark privacy leakage across a wide range of data modalities, state-of-the-art privacy attack methods, adaptation techniques, and model architectures. We systematically evaluate and pinpoint critical factors related to privacy leakage. With our organized codebase and actionable insights, we aim to provide a standardized auditing tool for practitioners seeking to deploy customized LLM applications with faithful privacy assessments.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Derui Zhu;Dingfan Chen;Xiongfei Wu;Jiahui Geng;Zhuo Li;Jens Grossklags;Lei Ma,False,https://openreview.net/pdf?id=VpkfxuVXwx
W0FEprcxva,Evaluating Numerical Reasoning in Text-to-Image Models,"Text-to-image generative models are capable of producing high-quality images that often faithfully depict concepts described using natural language. In this work, we comprehensively evaluate a range of text-to-image models on numerical reasoning tasks of varying difficulty, and show that even the most advanced models have only rudimentary numerical skills. Specifically, their ability to correctly generate an exact number of objects in an image is limited to small numbers, it is highly dependent on the context the number term appears in, and it deteriorates quickly with each successive number. We also demonstrate that models have poor understanding of linguistic quantifiers (such as “few” or “as many as”), the concept of zero, and struggle with more advanced concepts such as fractional representations. We bundle prompts, generated images and human annotations into GeckoNum, a novel benchmark for evaluation of numerical reasoning.",Datasets & Benchmarks,NeurIPS,2024,Poster,Ivana Kajic;Olivia Wiles;Isabela Albuquerque;Matthias Bauer;Su Wang;Jordi Pont-Tuset;Aida Nematzadeh,True,https://openreview.net/pdf?id=W0FEprcxva
W4pIBQ7bAI,MediQ: Question-Asking LLMs and a Benchmark for Reliable Interactive Clinical Reasoning,"Users typically engage with LLMs interactively, yet most existing benchmarks evaluate them in a static, single-turn format, posing reliability concerns in interactive scenarios. We identify a key obstacle towards reliability: LLMs are trained to answer any question, even with incomplete context or insufficient knowledge. In this paper, we propose to change the static paradigm to an interactive one, develop systems that proactively ask questions to gather more information and respond reliably, and introduce an benchmark—MEDIQ—to evaluate question-asking ability in LLMs. MEDIQ simulates clinical interactions consisting of a Patient System and an adaptive Expert System; with potentially incomplete initial information, the Expert refrains from making diagnostic decisions when unconfident, and instead elicits missing details via follow-up questions. We provide a pipeline to convert single-turn medical benchmarks into an interactive format. Our results show that directly prompting state-of-the-art LLMs to ask questions degrades performance, indicating that adapting LLMs to proactive information-seeking settings is nontrivial. We experiment with abstention strategies to better estimate model confidence and decide when to ask questions, improving diagnostic accuracy by 22.3%; however, performance still lags compared to an (unrealistic in practice) upper bound with complete information upfront. Further analyses show improved interactive performance with filtering irrelevant contexts and reformatting conversations. Overall, we introduce a novel problem towards LLM reliability, an interactive MEDIQ benchmark and a novel question-asking system, and highlight directions to extend LLMs’ information-seeking abilities in critical domains.",main,NeurIPS,2024,Poster,Shuyue Stella Li;Vidhisha Balachandran;Shangbin Feng;Jonathan S. Ilgen;Emma Pierson;Pang Wei Koh;Yulia Tsvetkov,True,https://openreview.net/pdf?id=W4pIBQ7bAI
W8OZdhowxo,Towards General Loop Invariant Generation: A Benchmark of Programs with Memory Manipulation,"Program verification is vital for ensuring software reliability, especially in the context of increasingly complex systems. Loop invariants, remaining true before and after each iteration of loops, are crucial for this verification process. Traditional provers and machine learning based methods for generating loop invariants often require expert intervention or extensive labeled data, and typically only handle numerical property verification. These methods struggle with programs involving complex data structures and memory manipulations, limiting their applicability and automation capabilities. This paper introduces a new benchmark named LIG-MM, specifically for programs with complex data structures and memory manipulations. We collect 312 programs from various sources, including daily programs from college homework, the international competition (SV-COMP), benchmarks from previous papers (SLING), and programs from real-world software systems (Linux Kernel, GlibC, LiteOS, and Zephyr). Based on LIG-MM, our findings indicate that previous methods, including GPT-4, fail to automate verification for these programs. Consequently, we propose a novel LLM-SE framework that coordinates LLM with symbolic execution, fine-tuned using self-supervised learning, to generate loop invariants. Experimental results on LIG-MM demonstrate that our LLM-SE outperforms state-of-the-art methods, offering a new direction toward automated program verification in real-world scenarios.",Datasets & Benchmarks,NeurIPS,2024,Poster,Chang Liu;Xiwei Wu;Yuan Feng;Qinxiang Cao;Junchi Yan,True,https://openreview.net/pdf?id=W8OZdhowxo
WEFxOm3Aez,RelBench: A Benchmark for Deep Learning on Relational Databases,"We present RelBench, a public benchmark for solving predictive tasks in relational databases with deep learning.  RelBench provides databases and tasks spanning diverse domains, scales, and database dimensions, and is intended to be a foundational infrastructure for future research in this direction. We use RelBench to conduct the first comprehensive empirical study of graph neural network (GNN) based predictive models on relational data, as recently proposed by Fey et al. 2024.  End-to-end learned GNNs are capable fully exploiting the predictive signal encoded in links between entities, marking a significant shift away from the dominant paradigm of manual feature engineering combined with tabular machine learning. To thoroughly evaluate GNNs against the prior gold-standard we conduct a user study, where an experienced data scientist manually engineers features for each task. In this study, GNNs learn better models whilst reducing human work needed by more than an order of magnitude. This result demonstrates the power of GNNs for solving predictive tasks in relational databases, opening up new research opportunities.",Datasets & Benchmarks,NeurIPS,2024,Poster,Joshua Robinson;Rishabh Ranjan;Weihua Hu;Kexin Huang;Jiaqi Han;Alejandro Dobles;Matthias Fey;Jan Eric Lenssen;Yiwen Yuan;Zecheng Zhang;Xinwei He;Jure Leskovec,True,https://openreview.net/pdf?id=WEFxOm3Aez
WGoCZl2itU,ClashEval: Quantifying the tug-of-war between an LLM’s internal prior and external evidence,"Retrieval augmented generation (RAG) is frequently used to mitigate hallucinations and provide up-to-date knowledge for large language models (LLMs). However, given that document retrieval is an imprecise task and sometimes results in erroneous or even harmful content being presented in context, this raises the question of how LLMs handle retrieved information: If the provided content is incorrect, does the model know to ignore it, or does it recapitulate the error? Conversely, when the model's initial response is incorrect, does it always know to use the retrieved information to correct itself, or does it insist on its wrong prior response? To answer this, we curate a dataset of over 1200 questions across six domains (e.g., drug dosages, Olympic records, locations) along with content relevant to answering each question. We further apply precise perturbations to the answers in the content that range from subtle to blatant errors.
We benchmark six top-performing LLMs, including GPT-4o, on this dataset and find that LLMs are susceptible to adopting incorrect retrieved content, overriding their own correct prior knowledge over 60\\\\% of the time. However, the more unrealistic the retrieved content is (i.e. more deviated from truth), the less likely the model is to adopt it. Also, the less confident a model is in its initial response (via measuring token probabilities), the more likely it is to adopt the information in the retrieved content. We exploit this finding and demonstrate simple methods for improving model accuracy where there is conflicting retrieved content. Our results highlight a difficult task and benchmark for LLMs -- namely, their ability to correctly discern when it is wrong in light of correct retrieved content and to reject cases when the provided content is incorrect. Our dataset, called ClashEval, and evaluations are open-sourced to allow for future benchmarking on top-performing models at https://github.com/kevinwu23/StanfordClashEval.",Datasets & Benchmarks,NeurIPS,2024,Poster,Kevin Wu;Eric Wu;James Zou,True,https://openreview.net/pdf?id=WGoCZl2itU
WH5blx5tZ1,Large Scale Transfer Learning for Tabular Data via Language Modeling,"Tabular data – structured, heterogeneous, spreadsheet-style data with rows and columns – is widely used in practice across many domains. However, while recent foundation models have reduced the need for developing task-specific datasets and predictors in domains such as language modeling and computer vision, this transfer learning paradigm has not had similar impact in the tabular domain. In this work, we seek to narrow this gap and present TABULA-8B, a language model for tabular prediction. We define a process for extracting a large, high-quality training dataset from the TabLib corpus, proposing methods for tabular data filtering and quality control. Using the resulting dataset, which comprises over 2.1B rows from 4.2M unique tables, we fine-tune a Llama 3-8B large language model (LLM) for tabular data prediction (classification and binned regression) using a novel packing and attention scheme for tabular prediction. Through evaluation across a test suite of 329 datasets, we find that TABULA-8B has zero-shot accuracy on unseen tables that is over 15 percentage points (pp) higher than random guessing, a feat that is not possible with existing state-of-the-art tabular prediction models (e.g. XGBoost, TabPFN). In the few-shot setting (1-32 shots), without any fine-tuning on the target datasets, TABULA-8B is 5-15 pp more accurate than XGBoost and TabPFN models that are explicitly trained on equal, or even up to 16× more data. We release our model, code, and data along with the publication of this paper.",main,NeurIPS,2024,Poster,Joshua P Gardner;Juan Carlos Perdomo;Ludwig Schmidt,True,https://openreview.net/pdf?id=WH5blx5tZ1
WKTNdU155n,LLaMo: Large Language Model-based Molecular Graph Assistant,"Large Language Models (LLMs) have demonstrated remarkable generalization and instruction-following capabilities with instruction tuning. The advancements in LLMs and instruction tuning have led to the development of Large Vision-Language Models (LVLMs). However, the competency of the LLMs and instruction tuning have been less explored in the molecular domain. Thus, we propose LLaMo: Large Language Model-based Molecular graph assistant, which is an end-to- end trained large molecular graph-language model. To bridge the discrepancy between the language and graph modalities, we present the multi-level graph projector that transforms graph representations into graph tokens by abstracting the output representations of each GNN layer and motif representations with the cross-attention mechanism. We also introduce machine-generated molecular graph instruction data to instruction-tune the large molecular graph-language model for general-purpose molecule and language understanding. Our extensive experiments demonstrate that LLaMo shows the best performance on diverse tasks, such as molecular description generation, property prediction, and IUPAC name prediction. The code of LLaMo is available at https://github.com/mlvlab/LLaMo.",main,NeurIPS,2024,Poster,Jinyoung Park;Minseong Bae;Dohwan Ko;Hyunwoo J. Kim,True,https://openreview.net/pdf?id=WKTNdU155n
WMzQIP70O0,BiVLC: Extending Vision-Language Compositionality Evaluation with Text-to-Image Retrieval,"Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe are formulated as image-to-text retrieval problems, where, given an image, the models need to select between the correct textual description and a synthetic hard negative text. In this work, we present the Bidirectional Vision-Language Compositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic hard negative image generated from the synthetic text, resulting in two image-to-text retrieval examples (one for each image) and, more importantly, two text-to-image retrieval examples (one for each text). Human annotators filter out ill-formed examples ensuring the validity of the benchmark. The experiments on BiVLC uncover a weakness of current multimodal models, as they perform poorly in the text-to-image direction. In fact, when considering both retrieval directions, the conclusions obtained in previous works change significantly. In addition to the benchmark, we
show that a contrastive model trained using synthetic images and texts significantly improves over the base model in SugarCrepe and in BiVLC for both retrieval directions. The gap to human performance in BiVLC confirms that Vision-Language Compositionality is still a challenging problem.",Datasets & Benchmarks,NeurIPS,2024,Poster,Imanol Miranda;Ander Salaberria;Eneko Agirre;Gorka Azkune,True,https://openreview.net/pdf?id=WMzQIP70O0
WRxFVzx0uG,WindsorML: High-Fidelity Computational Fluid Dynamics Dataset For Automotive Aerodynamics,"This paper presents a new open-source high-fidelity dataset for Machine Learning (ML) containing 355 geometric variants of the Windsor body, to help the development and testing of ML surrogate models for external automotive aerodynamics. Each Computational Fluid Dynamics (CFD) simulation was run with a GPU-native high-fidelity Wall-Modeled Large-Eddy Simulations (WMLES) using a Cartesian immersed-boundary method using more than 280M cells to ensure the greatest possible accuracy. The dataset contains geometry variants that exhibits a wide range of flow characteristics that are representative of those observed on road-cars. 
The dataset itself contains the 3D time-averaged volume \\\\& boundary data as well as the geometry and force \\\\& moment coefficients. This paper discusses the validation of the underlying CFD methods as well as contents and structure of the dataset. To the authors knowledge, this represents the first, large-scale high-fidelity CFD dataset for the Windsor body with a permissive open-source license (CC-BY-SA).",Datasets & Benchmarks,NeurIPS,2024,Poster,Neil Ashton;Jordan B. Angel;Aditya Ghate;Gaetan Kenway;Man Long Wong;Cetin C. Kiris;Astrid Walle;Danielle C. Maddix;Gary Page,True,https://openreview.net/pdf?id=WRxFVzx0uG
WTI4RJYSVm,A benchmark for prediction of transcriptomic responses to chemical perturbations across cell types,"Single-cell transcriptomics has revolutionized our understanding of cellular heterogeneity and drug perturbation effects. However, its high cost and the vast chemical space of potential drugs present barriers to experimentally characterizing the effect of chemical perturbations in all the myriad cell types of the human body. To overcome these limitations, several groups have proposed using machine learning methods to directly predict the effect of chemical perturbations either across cell contexts or chemical space. However, advances in this field have been hindered by a lack of well-designed evaluation datasets and benchmarks. To drive innovation in perturbation modeling, the Open Problems Perturbation Prediction (OP3) benchmark introduces a framework for predicting the effects of small molecule perturbations on cell type-specific gene expression. OP3 leverages the Open Problems in Single-cell Analysis benchmarking infrastructure and is enabled by a new single-cell perturbation dataset, encompassing 146 compounds tested on human blood cells. The benchmark includes diverse data representations, evaluation metrics, and winning methods from our ""Single-cell perturbation prediction: generalizing experimental interventions to unseen contexts"" competition at NeurIPS 2023. We envision that the OP3 benchmark and competition will drive innovation in single-cell perturbation prediction by improving the accessibility, visibility, and feasibility of this challenge, thereby promoting the impact of machine learning in drug discovery.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Artur Szałata;Andrew Benz;Robrecht Cannoodt;Mauricio Cortes;Jason Fong;Sunil Kuppasani;Richard Lieberman;Tianyu Liu;Javier A. Mas-Rosario;Rico Meinl;Jalil Nourisa;Jared Tumiel;Tin M. Tunjic;Mengbo Wang;Noah Weber;Hongyu Zhao;Benedict Anchang;Fabian J Theis;Malte D Luecken;Daniel B Burkhardt,True,https://openreview.net/pdf?id=WTI4RJYSVm
WUWHVN4gxk,Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition,"Large language model systems face significant security risks from maliciously crafted messages that aim to overwrite the system's original instructions or leak private data. To study this problem, we organized a capture-the-flag competition at IEEE SaTML 2024, where the flag is a secret string in the LLM system prompt. The competition was organized in two phases. In the first phase, teams developed defenses to prevent the model from leaking the secret. During the second phase, teams were challenged to extract the secrets hidden for defenses proposed by the other teams. This report summarizes the main insights from the competition. Notably, we found that all defenses were bypassed at least once, highlighting the difficulty of designing a successful defense and the necessity for additional research to protect LLM systems. To foster future research in this direction, we compiled a dataset with over 137k multi-turn attack chats and open-sourced the platform.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Edoardo Debenedetti;Javier Rando;Daniel Paleka;Silaghi Fineas Florin;Dragos Albastroiu;Niv Cohen;Yuval Lemberg;Reshmi Ghosh;Rui Wen;Ahmed Salem;Giovanni Cherubin;Santiago Zanella-Beguelin;Robin Schmid;Victor Klemm;Takahiro Miki;Chenhao Li;Stefan Kraft;Mario Fritz;Florian Tramèr;Sahar Abdelnabi;Lea Schönherr,True,https://openreview.net/pdf?id=WUWHVN4gxk
WVQ4Clw1VD,MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine,"This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities, with multigranular annotations for more than 65 diseases. These enriched annotations encompass both global textual information, such as disease/lesion type, modality, region-specific descriptions, and inter-regional relationships, as well as detailed local annotations for regions of interest (ROIs), including bounding boxes, segmentation masks. Unlike existing approach which is limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and texual annotations (in the form of image-ROI-description triplets) without the need for any paired text descriptions. Specifically, data from over 90 different sources have been collected, preprocessed, and grounded using domain-specific expert models to identify ROIs related to abnormal regions. 
We then build a comprehensive knowledge base and prompt multimodal large language models to perform retrieval-augmented generation with the identified ROIs as guidance, resulting in multigranular texual descriptions. 
Compared to existing datasets, MedTrinity-25M provides the most enriched annotations, supporting a comprehensive range of multimodal tasks such as captioning and report generation, as well as vision-centric tasks like classification and segmentation. This dataset can be utilized to support large-scale pre-training of multimodal medical AI models, contributing to the development of future foundation models in the medical domain. The dataset is publicly available at https://yunfeixie233.github.io/MedTrinity-25M/.",Datasets & Benchmarks,NeurIPS,2024,Reject,Yunfei Xie;Ce Zhou;Lang Gao;Juncheng Wu;Xianhang Li;Hong-Yu Zhou;Sheng Liu;Lei Xing;James Zou;Cihang Xie;Yuyin Zhou,True,https://openreview.net/pdf?id=WVQ4Clw1VD
WdA5H9ARaa,Benchmark Inflation: Revealing LLM Performance Gaps Using Retro-Holdouts,"Public benchmarks are compromised, as the training data for many Large Language Models (LLMs) is contaminated with test data, suggesting a performance gap between benchmark scores and actual capabilities. Ideally, a private holdout set could be used to accurately verify scores. Unfortunately, such datasets do not exist for most benchmarks, and post-hoc construction of sufficiently similar datasets is non-trivial. To address these issues, we introduce a systematic methodology for (i) retrospectively constructing a holdout dataset for a target dataset, (ii) demonstrating the statistical indistinguishability of this retro-holdout dataset, and (iii) comparing LLMs on the two datasets to quantify the performance gap due to the dataset's public availability. Applying these methods to TruthfulQA, we construct and release Retro-TruthfulQA, on which we evaluate twenty LLMs and find that some have inflated scores by as much as 16 percentage points. Our results demonstrate that public benchmark scores do not always accurately assess model properties, and underscore the importance of improved data practices in the field.",Datasets & Benchmarks,NeurIPS,2024,Reject,Jacob Haimes;Cenny Wenner;Kunvar Thaman;Vassil Tashev;Clement Neo;Esben Kran;Jason Hoelscher-Obermaier,True,https://openreview.net/pdf?id=WdA5H9ARaa
WffhOhYvZ0,SolarCube: An Integrative Benchmark Dataset Harnessing Satellite and In-situ Observations for Large-scale Solar Energy Forecasting,"Solar power is a critical source of renewable energy, offering significant potential to lower greenhouse gas emissions and mitigate climate change. However, the cloud induced-variability of solar radiation reaching the earth’s surface presents a challenge for integrating solar power into the grid (e.g., storage and backup management). The new generation of geostationary satellites such as GOES-16 has become an important data source for large-scale and high temporal frequency solar radiation forecasting. However, no machine-learning-ready dataset has integrated geostationary satellite data with fine-grained solar radiation information to support forecasting model development and benchmarking with consistent metrics. We present SolarCube, a new ML-ready benchmark dataset for solar radiation forecasting. SolarCube covers 19 study areas distributed over multiple continents: North America, South America, Asia, and Oceania. The dataset supports short (i.e., 30 minutes to 6 hours) and long-term (i.e., day-ahead or longer) solar radiation forecasting at both point-level (i.e., specific locations of monitoring stations) and area-level, by processing and integrating data from multiple sources, including geostationary satellite images, physics-derived solar radiation, and ground station observations from different monitoring networks over the globe. We also evaluated a set of forecasting models for point- and image-based time-series data to develop performance benchmarks under different testing scenarios. The dataset is available at https://doi.org/10.5281/zenodo.11498739. A Python library is available to conveniently generate different variations of the dataset based on user needs, along with baseline models at https://github.com/Ruohan-Li/SolarCube.",Datasets & Benchmarks,NeurIPS,2024,Poster,Ruohan Li;Yiqun Xie;Xiaowei Jia;Dongdong Wang;Yanhua Li;Yingxue Zhang;Zhihao Wang;Zhili Li,True,https://openreview.net/pdf?id=WffhOhYvZ0
WvoKwq12x5,PediatricsGPT: Large Language Models as Chinese Medical Assistants for Pediatric Applications,"Developing intelligent pediatric consultation systems offers promising prospects for improving diagnostic efficiency, especially in China, where healthcare resources are scarce. Despite recent advances in Large Language Models (LLMs) for Chinese medicine, their performance is sub-optimal in pediatric applications due to inadequate instruction data and vulnerable training procedures.
To address the above issues, this paper builds PedCorpus, a high-quality dataset of over 300,000 multi-task instructions from pediatric textbooks, guidelines, and knowledge graph resources to fulfil diverse diagnostic demands. Upon well-designed PedCorpus, we propose PediatricsGPT, the first Chinese pediatric LLM assistant built on a systematic and robust training pipeline.
In the continuous pre-training phase, we introduce a hybrid instruction pre-training mechanism to mitigate the internal-injected knowledge inconsistency of LLMs for medical domain adaptation. Immediately, the full-parameter Supervised Fine-Tuning (SFT) is utilized to incorporate the general medical knowledge schema into the models. After that, we devise a direct following preference optimization to enhance the generation of pediatrician-like humanistic responses. In the parameter-efficient secondary SFT phase,
a mixture of universal-specific experts strategy is presented to resolve the competency conflict between medical generalist and pediatric expertise mastery. Extensive results based on the metrics, GPT-4, and doctor evaluations on distinct downstream tasks show that PediatricsGPT consistently outperforms previous Chinese medical LLMs. The project and data will be released at https://github.com/ydk122024/PediatricsGPT.",main,NeurIPS,2024,Poster,Dingkang Yang;Jinjie Wei;Dongling Xiao;Shunli Wang;Tong Wu;Gang Li;Mingcheng Li;Shuaibing Wang;Jiawei Chen;Yue Jiang;Qingyao Xu;Ke Li;Peng Zhai;Lihua Zhang,True,https://openreview.net/pdf?id=WvoKwq12x5
X4KImMSIRq,Copycats: the many lives of a publicly available medical imaging dataset,"Medical Imaging (MI) datasets are fundamental to artificial intelligence in healthcare. The accuracy, robustness, and fairness of diagnostic algorithms depend on the data (and its quality) used to train and evaluate the models. MI datasets used to be proprietary, but have become increasingly available to the public, including on community-contributed platforms (CCPs) like Kaggle or HuggingFace. While open data is important to enhance the redistribution of data's public value, we find that the current CCP governance model fails to uphold the quality needed and recommended practices for sharing, documenting, and evaluating datasets. In this paper, we conduct an analysis of publicly available machine learning datasets on CCPs, discussing datasets' context, and identifying limitations and gaps in the current CCP landscape. We highlight differences between MI and computer vision datasets, particularly in the potentially harmful downstream effects from poor adoption of recommended dataset management practices. We compare the analyzed datasets across several dimensions, including data sharing, data documentation, and maintenance. We find vague licenses, lack of persistent identifiers and storage, duplicates, and missing metadata, with differences between the platforms. Our research contributes to efforts in responsible data curation and AI algorithms for healthcare.",Datasets & Benchmarks,NeurIPS,2024,Poster,Amelia Jiménez-Sánchez;Natalia-Rozalia Avlona;Dovile Juodelyte;Théo Sourget;Caroline Vang-Larsen;Anna Rogers;Hubert Dariusz Zając;Veronika Cheplygina,False,https://openreview.net/pdf?id=X4KImMSIRq
X4nq0W2qZX,MmCows: A Multimodal Dataset for Dairy Cattle Monitoring,"Precision livestock farming (PLF) has been transformed by machine learning (ML), enabling more precise and timely interventions that enhance overall farm productivity, animal welfare, and environmental sustainability. However, despite the availability of various sensing technologies, few datasets leverage multiple modalities, which are crucial for developing more accurate and efficient monitoring devices and ML models. To address this gap, we present MmCows, a multimodal dataset for dairy cattle monitoring. This dataset comprises a large amount of synchronized, high-quality measurement data on behavioral, physiological, and environmental factors. It includes two weeks of data collected using wearable and implantable sensors deployed on ten milking Holstein cows, such as ultra-wideband (UWB) sensors, inertial sensors, and body temperature sensors. In addition, it features 4.8 million frames of high-resolution image sequences from four isometric view cameras, as well as temperature and humidity data from environmental sensors. We also gathered milk yield data and outdoor weather conditions. One full day’s worth of image data is annotated as ground truth, totaling 20,000 frames with 213,000 bounding boxes of 16 cows, along with their 3D locations and behavior labels. An extensive analysis of MmCows is provided to evaluate the modalities individually and their complementary benefits. The release of MmCows and its benchmarks will facilitate research on multimodal monitoring of dairy cattle, thereby promoting sustainable dairy farming. The dataset and the code for benchmarks are available at https://github.com/neis-lab/mmcows.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Hien Vu;Omkar Prabhune;Unmesh Raskar;Dimuth Panditharatne;Hanwook Chung;Christopher Choi;Younghyun Kim,True,https://openreview.net/pdf?id=X4nq0W2qZX
X8ItT6mGKF,MAN TruckScenes: A multimodal dataset for autonomous trucking in diverse conditions,"Autonomous trucking is a promising technology that can greatly impact modern logistics and the environment. Ensuring its safety on public roads is one of the main duties that requires an accurate perception of the environment. To achieve this, machine learning methods rely on large datasets, but to this day, no such datasets are available for autonomous trucks. In this work, we present MAN TruckScenes, the first multimodal dataset for autonomous trucking. MAN TruckScenes allows the research community to come into contact with truck-specific challenges, such as trailer occlusions, novel sensor perspectives, and terminal environments for the first time. It comprises more than 740 scenes of 20 s each within a multitude of different environmental conditions. The sensor set includes 4 cameras, 6 lidar, 6 radar sensors, 2 IMUs, and a high-precision GNSS. The dataset's 3D bounding boxes were manually annotated and carefully reviewed to achieve a high quality standard. Bounding boxes are available for 27 object classes, 15 attributes, and a range of more than 230 m. The scenes are tagged according to 34 distinct scene tags, and all objects are tracked throughout the scene to promote a wide range of applications. Additionally, MAN TruckScenes is the first dataset to provide 4D radar data with 360° coverage and is thereby the largest radar dataset with annotated 3D bounding boxes. Finally, we provide extensive dataset analysis and baseline results. The dataset, development kit, and more will be available online.",Datasets & Benchmarks,NeurIPS,2024,Poster,Felix Sebastian Fent;Fabian Kuttenreich;Florian Ruch;Farija Rizwin;Stefan Juergens;Lorenz Lechermann;Christian Nissler;Andrea Perl;Ulrich Voll;Min Yan;Markus Lienkamp,True,https://openreview.net/pdf?id=X8ItT6mGKF
X90tyXDe8z,JaxMARL: Multi-Agent RL Environments and Algorithms in JAX,"Benchmarks are crucial in the development of machine learning algorithms, significantly influencing reinforcement learning (RL) research through the available environments. Traditionally, RL environments run on the CPU, which limits their scalability with the computational resources typically available in academia. However, recent advancements in JAX have enabled the wider use of hardware acceleration, enabling massively parallel RL training pipelines and environments. While this has been successfully applied to single-agent RL, it has not yet been widely adopted for multi-agent scenarios. In this paper, we present JaxMARL, the first open-source, easy-to-use code base that combines GPU-enabled efficiency with support for a large number of commonly used MARL environments and popular baseline algorithms. Our experiments show that, in terms of wall clock time, our JAX-based training pipeline is up to 12,500 times faster than existing approaches. This enables efficient and thorough evaluations, potentially alleviating the evaluation crisis in the field. We also introduce and benchmark SMAX, a vectorised, simplified version of the popular StarCraft Multi-Agent Challenge, which removes the need to run the StarCraft II game engine. This not only enables GPU acceleration, but also provides a more flexible MARL environment, unlocking the potential for self-play, meta-learning, and other future applications in MARL. The code is available at https://github.com/flairox/jaxmarl.",Datasets & Benchmarks,NeurIPS,2024,Poster,Alexander Rutherford;Benjamin Ellis;Matteo Gallici;Jonathan Cook;Andrei Lupu;Garðar Ingvarsson;Timon Willi;Ravi Hammond;Akbir Khan;Christian Schroeder de Witt;Alexandra Souly;Saptarashmi Bandyopadhyay;Mikayel Samvelyan;Minqi Jiang;Robert Tjarko Lange;Shimon Whiteson;Bruno Lacerda;Nick Hawes;Tim Rocktäschel;Chris Lu;Jakob Nicolaus Foerster,False,https://openreview.net/pdf?id=X90tyXDe8z
XBcStBjBIE,ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation,"We propose a novel text-to-video (T2V) generation benchmark, *ChronoMagic-Bench*, to evaluate the temporal and metamorphic knowledge skills in time-lapse video generation of the T2V models (e.g. Sora and Lumiere). Compared to existing benchmarks that focus on visual quality and text relevance of generated videos, *ChronoMagic-Bench* focuses on the models’ ability to generate time-lapse videos with significant metamorphic amplitude and temporal coherence. The benchmark probes T2V models for their physics, biology, and chemistry capabilities, in a free-form text control. For these purposes, *ChronoMagic-Bench* introduces **1,649** prompts and real-world videos as references, categorized into four major types of time-lapse videos: biological, human creation, meteorological, and physical phenomena, which are further divided into 75 subcategories. This categorization ensures a comprehensive evaluation of the models’ capacity to handle diverse and complex transformations. To accurately align human preference on the benchmark, we introduce two new automatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic attributes and temporal coherence. MTScore measures the metamorphic amplitude, reflecting the degree of change over time, while CHScore assesses the temporal coherence, ensuring the generated videos maintain logical progression and continuity. Based on the *ChronoMagic-Bench*, we conduct comprehensive manual evaluations of eighteen representative T2V models, revealing their strengths and weaknesses across different categories of prompts, providing a thorough evaluation framework that addresses current gaps in video generation research. More encouragingly, we create a large-scale *ChronoMagic-Pro* dataset, containing **460k** high-quality pairs of 720p time-lapse videos and detailed captions. Each caption ensures high physical content and large metamorphic amplitude, which have a far-reaching impact on the video generation community. The source data and code are publicly available on [https://pku-yuangroup.github.io/ChronoMagic-Bench](https://pku-yuangroup.github.io/ChronoMagic-Bench).",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Shenghai Yuan;Jinfa Huang;Yongqi Xu;YaoYang Liu;Shaofeng Zhang;Yujun Shi;Rui-Jie Zhu;Xinhua Cheng;Jiebo Luo;Li Yuan,True,https://openreview.net/pdf?id=XBcStBjBIE
XHdwlbNSVb,MMSite: A Multi-modal Framework for the Identification of Active Sites in Proteins,"The accurate identification of active sites in proteins is essential for the advancement of life sciences and pharmaceutical development, as these sites are of critical importance for enzyme activity and drug design. Recent advancements in protein language models (PLMs), trained on extensive datasets of amino acid sequences, have significantly improved our understanding of proteins. However, compared to the abundant protein sequence data, functional annotations, especially precise per-residue annotations, are scarce, which limits the performance of PLMs. On the other hand, textual descriptions of proteins, which could be annotated by human experts or a pretrained protein sequence-to-text model, provide meaningful context that could assist in the functional annotations, such as the localization of active sites. This motivates us to construct a $\\\\textbf{ProT}$ein-$\\\\textbf{A}$ttribute text $\\\\textbf{D}$ataset ($\\\\textbf{ProTAD}$), comprising over 570,000 pairs of protein sequences and multi-attribute textual descriptions. Based on this dataset, we propose $\\\\textbf{MMSite}$, a multi-modal framework that improves the performance of PLMs to identify active sites by leveraging biomedical language models (BLMs). In particular, we incorporate manual prompting and design a MACross module to deal with the multi-attribute characteristics of textual descriptions. MMSite is a two-stage (""First Align, Then Fuse"") framework: first aligns the textual modality with the sequential modality through soft-label alignment, and then identifies active sites via multi-modal fusion. Experimental results demonstrate that MMSite achieves state-of-the-art performance compared to existing protein representation learning methods. The dataset and code implementation are available at https://github.com/Gift-OYS/MMSite.",main,NeurIPS,2024,Poster,Song Ouyang;Huiyu Cai;Yong Luo;Kehua Su;Lefei Zhang;Bo Du,True,https://openreview.net/pdf?id=XHdwlbNSVb
XOGosbxLrz,dopanim: A Dataset of Doppelganger Animals with Noisy Annotations from Multiple Humans,"Human annotators typically provide annotated data for training machine learning models, such as neural networks. Yet, human annotations are subject to noise, impairing generalization performances. Methodological research on approaches counteracting noisy annotations requires corresponding datasets for a meaningful empirical evaluation. Consequently, we introduce a novel benchmark dataset, dopanim, consisting of about 15,750 animal images of 15 classes with ground truth labels. For approximately 10,500 of these images, 20 humans provided over 52,000 annotations with an accuracy of circa 67%. Its key attributes include (1) the challenging task of classifying doppelganger animals, (2) human-estimated likelihoods as annotations, and (3) annotator metadata. We benchmark well-known multi-annotator learning approaches using seven variants of this dataset and outline further evaluation use cases such as learning beyond hard class labels and active learning. Our dataset and a comprehensive codebase are publicly available to emulate the data collection process and to reproduce all empirical results.",Datasets & Benchmarks,NeurIPS,2024,Poster,Marek Herde;Denis Huseljic;Lukas Rauch;Bernhard Sick,True,https://openreview.net/pdf?id=XOGosbxLrz
XWzw2dsjWd,MoLE: Enhancing Human-centric Text-to-image Diffusion via Mixture of Low-rank Experts,"Text-to-image diffusion has attracted vast attention due to its impressive image-generation capabilities. However, when it comes to human-centric text-to-image generation, particularly in the context of faces and hands, the results often fall short of naturalness due to insufficient training priors. We alleviate the issue in this work from two perspectives. 1) From the data aspect, we carefully collect a human-centric dataset comprising over one million high-quality human-in-the-scene images and two specific sets of close-up images of faces and hands. These datasets collectively provide a rich prior knowledge base to enhance the human-centric image generation capabilities of the diffusion model. 2) On the methodological front, we propose a simple yet effective method called Mixture of Low-rank Experts (MoLE) by considering low-rank modules trained on close-up hand and face images respectively as experts. This concept draws inspiration from our observation of low-rank refinement, where a low-rank module trained by a customized close-up dataset has the potential to enhance the corresponding image part when applied at an appropriate scale. To validate the superiority of MoLE in the context of human-centric image generation compared to state-of-the-art, we construct two benchmarks and perform evaluations with diverse metrics and human studies. Datasets, model, and code are released at https://sites.google.com/view/mole4diffuser/.",main,NeurIPS,2024,Poster,Jie Zhu;Yixiong Chen;Mingyu Ding;Ping Luo;Leye Wang;Jingdong Wang,True,https://openreview.net/pdf?id=XWzw2dsjWd
XXaIoJyYs7,MedJourney: Benchmark and Evaluation of Large Language Models over Patient Clinical Journey,"Large language models (LLMs) have demonstrated remarkable capabilities in language understanding and generation, leading to their widespread adoption across various fields. Among these, the medical field is particularly well-suited for LLM applications, as many medical tasks can be enhanced by LLMs. Despite the existence of benchmarks for evaluating LLMs in medical question-answering and exams, there remains a notable gap in assessing LLMs' performance in supporting patients throughout their entire hospital visit journey in real-world clinical practice. In this paper, we address this gap by dividing a typical patient's clinical journey into four stages: planning, access, delivery and ongoing care. For each stage, we introduce multiple tasks and corresponding datasets, resulting in a comprehensive benchmark comprising 12 datasets, of which five are newly introduced, and seven are constructed from existing datasets. This proposed benchmark facilitates a thorough evaluation of LLMs' effectiveness across the entire patient journey, providing insights into their practical application in clinical settings. Additionally, we evaluate three categories of LLMs against this benchmark: 1) proprietary LLM services such as GPT-4; 2) public LLMs like QWen; and 3) specialized medical LLMs, like HuatuoGPT2. Through this extensive evaluation, we aim to provide a better understanding of LLMs' performance in the medical domain, ultimately contributing to their more effective deployment in healthcare settings.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xian Wu;Yutian Zhao;Yunyan Zhang;Jiageng Wu;Zhihong Zhu;Yingying Zhang;Yi Ouyang;Ziheng Zhang;Huimin WANG;Zhenxi Lin;Jie Yang;Shuang Zhao;Yefeng Zheng,True,https://openreview.net/pdf?id=XXaIoJyYs7
XZYUdhMvjL,MultiOrg: A Multi-rater Organoid-detection Dataset,"High-throughput image analysis in the biomedical domain has gained significant attention in recent years, driving advancements in drug discovery, disease prediction, and personalized medicine. Organoids, specifically, are an active area of research, providing excellent models for human organs and their functions. Automating the quantification of organoids in microscopy images would provide an effective solution to overcome substantial manual quantification bottlenecks, particularly in high-throughput image analysis. However, there is a notable lack of open biomedical datasets, in contrast to other domains, such as autonomous driving, and, notably, only few of them have attempted to quantify annotation uncertainty. In this work, we present MultiOrg a comprehensive organoid dataset tailored for object detection tasks with uncertainty quantification. This dataset comprises over 400 high-resolution 2d microscopy images and curated annotations of more than 60,000 organoids. Most importantly, it includes three label sets for the test data, independently annotated by two experts at distinct time points. We additionally provide a benchmark for organoid detection, and make the best model available through an easily installable, interactive plugin for the popular image visualization tool Napari, to perform organoid quantification.",Datasets & Benchmarks,NeurIPS,2024,Poster,Christina Bukas;Harshavardhan Subramanian;Fenja See;Carina Steinchen;Ivan Ezhov;Gowtham Boosarpu;Sara Asgharpour;Gerald Burgstaller;Mareike Lehmann;Florian Kofler;Marie Piraud,True,https://openreview.net/pdf?id=XZYUdhMvjL
XiConLcsqq,RewardBench: Evaluating Reward Models for Language Modeling,"Reward models (RMs) are at the crux of successful RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those reward models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. To date, very few descriptors of capabilities, training methods, or open-source reward models exist. In this paper, we present RewardBench, a benchmark dataset and code-base for evaluation, to enhance scientific understanding of reward models. The RewardBench dataset is a collection of prompt-win-lose trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We created specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be preferred to another. On the RewardBench leaderboard, we evaluate reward models trained with a variety of methods, such as the direct MLE training of classifiers and the implicit reward modeling of Direct Preference Optimization (DPO), and on a spectrum of datasets. We present many findings on propensity for refusals, reasoning limitations, and instruction following shortcomings of various reward models towards a better understanding of the RLHF process.",Datasets & Benchmarks,NeurIPS,2024,Reject,Nathan Lambert;Valentina Pyatkin;Jacob Morrison;Lester James Validad Miranda;Bill Yuchen Lin;Khyathi Chandu;Nouha Dziri;Sachin Kumar;Tom Zick;Yejin Choi;Noah A. Smith;Hannaneh Hajishirzi,True,https://openreview.net/pdf?id=XiConLcsqq
XlpipUGygX,Amortized Planning with Large-Scale Transformers: A Case Study on Chess,"This paper uses chess, a landmark planning problem in AI, to assess transformers’ performance on a planning task where memorization is futile — even at a large scale. To this end, we release ChessBench, a large-scale benchmark dataset of 10 million chess games with legal move and value annotations (15 billion data points)  provided by Stockfish 16, the state-of-the-art chess engine. We train transformers with up to 270 million parameters on ChessBench via supervised learning and perform extensive ablations to assess the impact of dataset size, model size, architecture type, and different prediction targets (state-values, action-values, and behavioral cloning). Our largest models learn to predict action-values for novel boards quite accurately, implying highly non-trivial generalization. Despite performing no explicit search, our resulting chess policy solves challenging chess puzzles and achieves a surprisingly strong Lichess blitz Elo of 2895 against humans (grandmaster level). We also compare to Leela Chess Zero and AlphaZero (trained without supervision via self-play) with and without search. We show that, although a remarkably good approximation of Stockfish’s search-based algorithm can be distilled into large-scale transformers via supervised learning, perfect distillation is still beyond reach, thus making ChessBench well-suited for future research.",main,NeurIPS,2024,Poster,Anian Ruoss;Gregoire Deletang;Sourabh Medapati;Jordi Grau-Moya;Li Kevin Wenliang;Elliot Catt;John Reid;Cannada A. Lewis;Joel Veness;Tim Genewein,True,https://openreview.net/pdf?id=XlpipUGygX
XmyxQaTyck,Benchmarking the Attribution Quality of Vision Models,"Attribution maps are one of the most established tools to explain the functioning of computer vision models. They assign importance scores to input features, indicating how relevant each feature is for the prediction of a deep neural network. While much research has gone into proposing new attribution methods, their proper evaluation remains a difficult challenge. In this work, we propose a novel evaluation protocol that overcomes two fundamental limitations of the widely used incremental-deletion protocol, i.e., the out-of-domain issue and lacking inter-model comparisons. This allows us to evaluate 23 attribution methods and how different design choices of popular vision backbones affect their attribution quality. We find that intrinsically explainable models outperform standard models and that raw attribution values exhibit a higher attribution quality than what is known from previous work. Further, we show consistent changes in the attribution quality when varying the network design, indicating that some standard design choices promote attribution quality.",Datasets & Benchmarks,NeurIPS,2024,Poster,Robin Hesse;Simone Schaub-Meyer;Stefan Roth,False,https://openreview.net/pdf?id=XmyxQaTyck
Xngi3Z3wkN,Protein-Nucleic Acid Complex Modeling with Frame Averaging Transformer,"Nucleic acid-based drugs like aptamers have recently demonstrated great therapeutic potential. However, experimental platforms for aptamer screening are costly, and the scarcity of labeled data presents a challenge for supervised methods to learn protein-aptamer binding. To this end, we develop an unsupervised learning approach based on the predicted pairwise contact map between a protein and a nucleic acid and demonstrate its effectiveness in protein-aptamer binding prediction. Our model is based on FAFormer, a novel equivariant transformer architecture that seamlessly integrates frame averaging (FA) within each transformer block. This integration allows our model to infuse geometric information into node features while preserving the spatial semantics of coordinates, leading to greater expressive power than standard FA models. Our results show that FAFormer outperforms existing equivariant models in contact map prediction across three protein complex datasets, with over 10% relative improvement. Moreover, we curate five real-world protein-aptamer interaction datasets and show that the contact map predicted by FAFormer serves as a strong binding indicator for aptamer screening.",main,NeurIPS,2024,Poster,Tinglin Huang;Zhenqiao Song;Rex Ying;Wengong Jin,True,https://openreview.net/pdf?id=Xngi3Z3wkN
XrKhwfPmyI,EHRNoteQA: An LLM Benchmark for Real-World Clinical Practice Using Discharge Summaries,"Discharge summaries in Electronic Health Records (EHRs) are crucial for clinical decision-making, but their length and complexity make information extraction challenging, especially when dealing with accumulated summaries across multiple patient admissions. Large Language Models (LLMs) show promise in addressing this challenge by efficiently analyzing vast and complex data. Existing benchmarks, however, fall short in properly evaluating LLMs' capabilities in this context, as they typically focus on single-note information or limited topics, failing to reflect the real-world inquiries required by clinicians. To bridge this gap, we introduce EHRNoteQA, a novel benchmark built on the MIMIC-IV EHR, comprising 962 different QA pairs each linked to distinct patients' discharge summaries. Every QA pair is initially generated using GPT-4 and then manually reviewed and refined by three clinicians to ensure clinical relevance. EHRNoteQA includes questions that require information across multiple discharge summaries and covers eight diverse topics, mirroring the complexity and diversity of real clinical inquiries. We offer EHRNoteQA in two formats: open-ended and multi-choice question answering, and propose a reliable evaluation method for each. We evaluate 27 LLMs using EHRNoteQA and examine various factors affecting the model performance (e.g., the length and number of discharge summaries). Furthermore, to validate EHRNoteQA as a reliable proxy for expert evaluations in clinical practice, we measure the correlation between the LLM performance on EHRNoteQA, and the LLM performance manually evaluated by clinicians. Results show that LLM performance on EHRNoteQA have higher correlation with clinician-evaluated performance (Spearman: 0.78, Kendall: 0.62) compared to other benchmarks, demonstrating its practical relevance in evaluating LLMs in clinical settings. EHRNoteQA will be publicly available to support further research and improve LLM evaluation in clinical practice. EHRNoteQA is publicly available under PhysioNet credential access at https://doi.org/10.13026/acga-ht95, and the code is available at https://github.com/ji-youn-kim/EHRNoteQA.",Datasets & Benchmarks,NeurIPS,2024,Poster,Sunjun Kweon;Jiyoun Kim;Heeyoung Kwak;Dongchul Cha;Hangyul Yoon;Kwang Hyun Kim;Jeewon Yang;Seunghyun Won;Edward Choi,True,https://openreview.net/pdf?id=XrKhwfPmyI
XukWe15QCi,"Pedestrian Trajectory Prediction with Missing Data: Datasets, Imputation, and Benchmarking","Pedestrian trajectory prediction is crucial for several applications such as robotics and self-driving vehicles. Significant progress has been made in the past decade thanks to the availability of pedestrian trajectory datasets, which enable trajectory prediction methods to learn from pedestrians' past movements and predict future trajectories. However, these datasets and methods typically assume that the observed trajectory sequence is complete, ignoring real-world issues such as sensor failure, occlusion, and limited fields of view that can result in missing values in observed trajectories. To address this challenge, we present TrajImpute, a pedestrian trajectory prediction dataset that simulates missing coordinates in the observed trajectory, enhancing real-world applicability. TrajImpute maintains a uniform distribution of missing data within the observed trajectories. In this work, we comprehensively examine several imputation methods to reconstruct the missing coordinates and benchmark them for imputing pedestrian trajectories. Furthermore, we provide a thorough analysis of recent trajectory prediction methods and evaluate the performance of these models on the imputed trajectories. Our experimental evaluation of the imputation and trajectory prediction methods offers several valuable insights. Our dataset provides a foundational resource for future research on imputation-aware pedestrian trajectory prediction, potentially accelerating the deployment of these methods in real-world applications. Publicly accessible links to the datasets and code files are available at https://github.com/Pranav-chib/TrajImpute.",Datasets & Benchmarks,NeurIPS,2024,Poster,Pranav singh chib;Pravendra Singh,True,https://openreview.net/pdf?id=XukWe15QCi
XvHdPiKy6c,SafeSora: Towards Safety Alignment of Text2Video Generation via a Human Preference Dataset,"To mitigate the risk of harmful outputs from large vision models (LVMs), we introduce the *SafeSora* dataset to promote research on aligning text-to-video generation with human values. This dataset encompasses human preferences in text-to-video generation tasks along two primary dimensions: helpfulness and harmlessness. To capture in-depth human preferences and facilitate structured reasoning by crowdworkers, we subdivide helpfulness into 4 sub-dimensions and harmlessness into 12 sub-categories, serving as the basis for pilot annotations. The *SafeSora* dataset includes 14,711 unique prompts, 57,333 unique videos generated by 4 distinct LVMs, and 51,691 pairs of preference annotations labeled by humans. We further demonstrate the utility of the *SafeSora* dataset through several applications, including training the text-video moderation model and aligning LVMs with human preference by fine-tuning a prompt augmentation module or the diffusion model. These applications highlight its potential as the foundation for text-to-video alignment research, such as human preference modeling and the development and validation of alignment algorithms. Our project is available at https://sites.google.com/view/safe-sora.

Warning: this paper contains example data that may be offensive or harmful.",Datasets & Benchmarks,NeurIPS,2024,Poster,Josef Dai;Tianle Chen;Xuyao Wang;Ziran Yang;Taiye Chen;Jiaming Ji;Yaodong Yang,True,https://openreview.net/pdf?id=XvHdPiKy6c
Y1rOWS2Z4i,Long-Horizon Planning for Multi-Agent Robots in Partially Observable Environments,"The ability of Language Models (LMs) to understand natural language makes them a powerful tool for parsing human instructions into task plans for autonomous robots. Unlike traditional planning methods that rely on domain-specific knowledge and handcrafted rules, LMs generalize from diverse data and adapt to various tasks with minimal tuning, acting as a compressed knowledge base. However, LMs in their standard form face challenges with long-horizon tasks, particularly in partially observable multi-agent settings. We propose an LM-based Long-Horizon Planner for Multi-Agent Robotics (LLaMAR), a cognitive architecture for planning that achieves state-of-the-art results in long-horizon tasks within partially observable environments. LLaMAR employs a plan-act-correct-verify framework, allowing self-correction from action execution feedback without relying on oracles or simulators. Additionally, we present MAP-THOR, a comprehensive test suite encompassing household tasks of varying complexity within the AI2-THOR environment. Experiments show that LLaMAR achieves a 30\\\\% higher success rate than other state-of-the-art LM-based multi-agent planners in MAP-THOR and Search \\\\& Rescue tasks. Code can be found at [https://github.com/nsidn98/LLaMAR](https://github.com/nsidn98/LLaMAR)",main,NeurIPS,2024,Poster,Siddharth Nayak;Adelmo Morrison Orozco;Marina Ten Have;Jackson Zhang;Vittal Thirumalai;Darren Chen;Aditya Kapoor;Eric Robinson;Karthik Gopalakrishnan;James Harrison;Anuj Mahajan;brian ichter;Hamsa Balakrishnan,True,https://openreview.net/pdf?id=Y1rOWS2Z4i
Y79L45D5ts,Reprogramming Pretrained Target-Specific Diffusion Models for Dual-Target Drug Design,"Dual-target therapeutic strategies have become a compelling approach and attracted significant attention due to various benefits, such as their potential in overcoming drug resistance in cancer therapy. Considering the tremendous success that deep generative models have achieved in structure-based drug design in recent years, we formulate dual-target drug design as a generative task and curate a novel dataset of potential target pairs based on synergistic drug combinations. We propose to design dual-target drugs with diffusion models that are trained on single-target protein-ligand complex pairs. Specifically, we align two pockets in 3D space with protein-ligand binding priors and build two complex graphs with shared ligand nodes for SE(3)-equivariant composed message passing, based on which we derive a composed drift in both 3D and categorical probability space in the generative process. Our algorithm can well transfer the knowledge gained in single-target pretraining to dual-target scenarios in a zero-shot manner. We also repurpose linker design methods as strong baselines for this task. Extensive experiments demonstrate the effectiveness of our method compared with various baselines.",main,NeurIPS,2024,Poster,Xiangxin Zhou;Jiaqi Guan;Yijia Zhang;Xingang Peng;Liang Wang;Jianzhu Ma,True,https://openreview.net/pdf?id=Y79L45D5ts
YFUp7zMrM9,CaptainCook4D: A Dataset for Understanding Errors in Procedural Activities,"Following step-by-step procedures is an essential component of various activities carried out by individuals in their daily lives. These procedures serve as a guiding framework that helps to achieve goals efficiently, whether it is assembling furniture or preparing a recipe. However, the complexity and duration of procedural activities inherently increase the likelihood of making errors. Understanding such procedural activities from a sequence of frames is a challenging task that demands an accurate interpretation of visual information and the ability to reason about the structure of the activity. To this end, we collect a new egocentric 4D dataset, CaptainCook4D, comprising 384 recordings (94.5 hours) of people performing recipes in real kitchen environments. This dataset consists of two distinct types of activity: one in which participants adhere to the provided recipe instructions and another in which they deviate and induce errors. We provide 5.3K step annotations and 10K fine-grained action annotations and benchmark the dataset for the following tasks: error recognition, multistep localization and procedure learning.",Datasets & Benchmarks,NeurIPS,2024,Poster,Rohith Peddi;Shivvrat Arya;Bharath Challa;Likhitha Pallapothula;Akshay Vyas;Bhavya Gouripeddi;Qifan Zhang;Jikai Wang;Vasundhara Komaragiri;Eric Ragan;Nicholas Ruozzi;Yu Xiang;Vibhav Gogate,True,https://openreview.net/pdf?id=YFUp7zMrM9
YGTVEmBXtV,Make Your LLM Fully Utilize the Context,"While many contemporary large language models (LLMs) can process lengthy input, they still struggle to fully utilize information within the long context, known as the *lost-in-the-middle* challenge.
We hypothesize that it stems from insufficient explicit supervision during the long-context training, which fails to emphasize that any position in a long context can hold crucial information.
Based on this intuition, our study presents **information-intensive (IN2) training**, a purely data-driven solution to overcome lost-in-the-middle.
Specifically, IN2 training leverages a synthesized long-context question-answer dataset, where the answer requires (1) **fine-grained information awareness** on a short segment (~128 tokens) within a synthesized long context (4K-32K tokens), and (2) the **integration and reasoning** of information from two or more short segments.
Through applying this information-intensive training on Mistral-7B, we present **FILM-7B** (FIll-in-the-Middle).
To thoroughly assess the ability of FILM-7B for utilizing long contexts, we design three probing tasks that encompass various context styles (document, code, and structured-data context) and information retrieval patterns (forward, backward, and bi-directional retrieval).
The probing results demonstrate that FILM-7B can robustly retrieve information from different positions in its 32K context window.
Beyond these probing tasks, FILM-7B significantly improves the performance on real-world long-context tasks (e.g., 23.5->26.9 F1 score on NarrativeQA), while maintaining a comparable performance on short-context tasks (e.g., 59.3->59.2 accuracy on MMLU).",main,NeurIPS,2024,Poster,Shengnan An;Zexiong Ma;Zeqi Lin;Nanning Zheng;Jian-Guang Lou;Weizhu Chen,True,https://openreview.net/pdf?id=YGTVEmBXtV
YIOvR40hSo,DiffPano: Scalable and Consistent Text to Panorama Generation with Spherical Epipolar-Aware Diffusion,"Diffusion-based methods have achieved remarkable achievements in 2D image or 3D object generation, however, the generation of 3D scenes and even $360^{\\\\circ}$ images remains constrained, due to the limited number of scene datasets, the complexity of 3D scenes themselves, and the difficulty of generating consistent multi-view images. To address these issues, we first establish a large-scale panoramic video-text dataset containing millions of consecutive panoramic keyframes with corresponding panoramic depths, camera poses, and text descriptions. Then, we propose a novel text-driven panoramic generation framework, termed DiffPano, to achieve scalable, consistent, and diverse panoramic scene generation. Specifically, benefiting from the powerful generative capabilities of stable diffusion, we fine-tune a single-view text-to-panorama diffusion model with LoRA on the established panoramic video-text dataset. We further design a spherical epipolar-aware multi-view diffusion model to ensure the multi-view consistency of the generated panoramic images. Extensive experiments demonstrate that DiffPano can generate scalable, consistent, and diverse panoramic images with given unseen text descriptions and camera poses.",main,NeurIPS,2024,Poster,Weicai Ye;Chenhao Ji;Zheng Chen;Junyao Gao;Xiaoshui Huang;Song-Hai Zhang;Wanli Ouyang;Tong He;Cairong Zhao;Guofeng Zhang,True,https://openreview.net/pdf?id=YIOvR40hSo
YMAU2kJgzY,Evaluating Large Vision-and-Language Models on Children's Mathematical Olympiads,"Recent years have seen a significant progress in the general-purpose problem solving abilities of large vision and language models (LVLMs), such as ChatGPT, Gemini, etc.; some of these breakthroughs even seem to enable AI models to outperform human abilities in varied tasks that demand higher-order cognitive skills. Are the current large AI models indeed capable of generalized problem solving as humans do?  A systematic analysis of AI capabilities for joint vision and text reasoning, however, is missing in the current scientific literature. In this paper, we make an effort towards filling this gap, by evaluating state-of-the-art LVLMs on their mathematical and algorithmic reasoning abilities using visuo-linguistic problems from children's Olympiads. Specifically, we consider problems from the Mathematical Kangaroo (MK) Olympiad, which is a popular international competition targeted at children from grades 1-12, that tests children's deeper mathematical abilities using puzzles that are appropriately gauged to their age and skills. Using the puzzles from MK, we created a dataset, dubbed SMART-840, consisting of 840 problems from years 2020-2024. With our dataset, we analyze LVLMs power on mathematical reasoning; their responses on our puzzles offer a direct way to compare against that of children. Our results show that modern LVLMs do demonstrate increasingly powerful reasoning skills in solving problems for higher grades, but lack the foundations to correctly answer problems designed for younger children. Further analysis shows that there is no significant correlation between the reasoning capabilities of AI models and that of young children, and their capabilities appear to be based on a different type of reasoning than the cumulative knowledge that underlies children's mathematical skills.",Datasets & Benchmarks,NeurIPS,2024,Poster,Anoop Cherian;Kuan-Chuan Peng;Suhas Lohit;Joanna Matthiesen;Kevin A. Smith;Joshua B. Tenenbaum,True,https://openreview.net/pdf?id=YMAU2kJgzY
YNx7ai4zTs,Single Image Unlearning: Efficient Machine Unlearning in Multimodal Large Language Models,"Machine unlearning (MU) empowers individuals with the `right to be forgotten' by removing their private or sensitive information encoded in machine learning models. However, it remains uncertain whether MU can be effectively applied to Multimodal Large Language Models (MLLMs), particularly in scenarios of forgetting the leaked visual data of concepts. To overcome the challenge, we propose an efficient method, Single Image Unlearning (SIU), to unlearn the visual recognition of a concept by fine-tuning a single associated image for few steps. SIU consists of two key aspects: (i) Constructing Multifaceted fine-tuning data. We introduce four targets, based on which we construct fine-tuning data for the concepts to be forgotten; (ii)  Joint training loss. To synchronously forget the visual recognition of concepts and preserve the utility of MLLMs, we fine-tune MLLMs through a novel Dual Masked KL-divergence Loss combined with Cross Entropy loss. Alongside our method, we establish MMUBench, a new benchmark for MU in MLLMs and introduce a collection of metrics for its evaluation. Experimental results on MMUBench show that SIU completely surpasses the performance of existing methods. Furthermore, we surprisingly find that SIU can avoid invasive membership inference attacks and jailbreak attacks. To the best of our knowledge, we are the first to explore MU in MLLMs. We will release the code and benchmark in the near future.",main,NeurIPS,2024,Poster,Jiaqi Li;Qianshan Wei;Chuanyi Zhang;Guilin Qi;Miaozeng Du;Yongrui Chen;Sheng Bi;Fan Liu,True,https://openreview.net/pdf?id=YNx7ai4zTs
YXXmIHJQBN,4DBInfer: A 4D Benchmarking Toolbox for Graph-Centric Predictive Modeling on RDBs,"Given a relational database (RDB), how can we predict  missing column values in some target table of interest? Although RDBs store vast amounts of rich, informative data spread across interconnected tables, the progress of predictive machine learning models as applied to such tasks arguably falls well behind advances in other domains such as computer vision or natural language processing.  This deficit stems, at least in part, from the lack of established/public RDB benchmarks as needed for training and evaluation purposes.  As a result, related model development thus far often defaults to tabular approaches trained on ubiquitous single-table benchmarks, or on the relational side, graph-based alternatives such as GNNs applied to a completely different set of graph datasets devoid of tabular characteristics.  To more precisely target RDBs lying at the nexus of these two complementary regimes, we explore a broad class of baseline models predicated on: (i) converting multi-table datasets into graphs using various strategies equipped with efficient subsampling, while preserving tabular characteristics; and (ii) trainable models with well-matched inductive biases that output predictions based on these input subgraphs.  Then, to address the dearth of suitable public benchmarks and reduce siloed comparisons, we assemble a diverse collection of (i) large-scale RDB datasets and (ii) coincident predictive tasks.  From a delivery standpoint, we  operationalize the above four dimensions (4D) of exploration within a unified, scalable open-source toolbox called 4DBInfer; please see https://github.com/awslabs/multi-table-benchmark .",Datasets & Benchmarks,NeurIPS,2024,Poster,Minjie Wang;Quan Gan;David Wipf;Zheng Zhang;Christos Faloutsos;Weinan Zhang;Muhan Zhang;Zhenkun Cai;Jiahang Li;Zunyao Mao;Yakun Song;Jianheng Tang;Yanlin Zhang;Guang Yang;Chuan Lei;Xiao Qin;Ning Li;Han Zhang;Yanbo Wang;Zizhao Zhang,True,https://openreview.net/pdf?id=YXXmIHJQBN
YagfTP3RK6,Safetywashing: Do AI Safety Benchmarks Actually Measure Safety Progress?,"Performance on popular ML benchmarks is highly correlated with model scale, suggesting that most benchmarks tend to measure a similar underlying factor of general model capabilities. However, substantial research effort remains devoted to designing new benchmarks, many of which claim to measure novel phenomena. In the spirit of the Bitter Lesson, we leverage spectral analysis to measure an underlying capabilities component, the direction in benchmark-performance-space which explains most variation in model performance. In an extensive analysis of existing safety benchmarks, we find that variance in model performance on many safety benchmarks is largely explained by the capabilities component. In response, we argue that safety research should prioritize metrics which are not highly correlated with scale. Our work provides a lens to analyze both novel safety benchmarks and novel safety methods, which we hope will enable future work to make differential progress on safety.",Datasets & Benchmarks,NeurIPS,2024,Poster,Richard Ren;Steven Basart;Adam Khoja;Alice Gatti;Long Phan;Xuwang Yin;Mantas Mazeika;Alexander Pan;Gabriel Mukobi;Ryan Hwang Kim;Stephen Fitz;Dan Hendrycks,False,https://openreview.net/pdf?id=YagfTP3RK6
YktwH3tOuc,Benchmark Data Repositories for Better Benchmarking,"In machine learning research, it is common to evaluate algorithms via their performance on standard benchmark datasets. While a growing body of work establishes guidelines for---and levies criticisms at---data and benchmarking practices in machine learning, comparatively less attention has been paid to the data repositories where these datasets are stored, documented, and shared. In this paper, we analyze the landscape of these _benchmark data repositories_ and the role they can play in improving benchmarking. This role includes addressing issues with both datasets themselves (e.g., representational harms, construct validity) and the manner in which evaluation is carried out using such datasets (e.g., overemphasis on a few datasets and metrics, lack of reproducibility). To this end, we identify and discuss a set of considerations surrounding the design and use of benchmark data repositories, with a focus on improving benchmarking practices in machine learning.",Datasets & Benchmarks,NeurIPS,2024,Poster,Rachel Longjohn;Markelle Kelly;Sameer Singh;Padhraic Smyth,False,https://openreview.net/pdf?id=YktwH3tOuc
YxuuzyplFZ,EyeGraph: Modularity-aware Spatio Temporal Graph Clustering for Continuous Event-based Eye Tracking,"Continuous tracking of eye movement dynamics plays a significant role in developing a broad spectrum of human-centered applications, such as cognitive skills (visual attention and working memory) modeling, human-machine interaction, biometric user authentication, and foveated rendering. Recently neuromorphic cameras have garnered significant interest in the eye-tracking research community, owing to their sub-microsecond latency in capturing intensity changes resulting from eye movements. Nevertheless, the existing approaches for event-based eye tracking suffer from several limitations: dependence on RGB frames, label sparsity, and training on datasets collected in controlled lab environments that do not adequately reflect real-world scenarios. To address these limitations, in this paper, we propose a dynamic graph-based approach that uses a neuromorphic event stream captured by Dynamic Vision Sensors (DVS) for high-fidelity tracking of pupillary movement. More specifically, first, we present EyeGraph, a large-scale multi-modal near-eye tracking dataset collected using a wearable event camera attached to a head-mounted device from 40 participants -- the dataset was curated while mimicking in-the-wild settings, accounting for varying mobility and ambient lighting conditions. Subsequently, to address the issue of label sparsity, we adopt an unsupervised topology-aware approach as a benchmark. To be specific, (a) we first construct a dynamic graph using Gaussian Mixture Models (GMM), resulting in a uniform and detailed representation of eye morphology features, facilitating accurate modeling of pupil and iris. Then (b) apply a novel topologically guided modularity-aware graph clustering approach to precisely track the movement of the pupil and address the label sparsity in event-based eye tracking. We show that our unsupervised approach has comparable performance against the supervised approaches while consistently outperforming the conventional clustering approaches.",Datasets & Benchmarks,NeurIPS,2024,Poster,Nuwan Sriyantha Bandara;Thivya Kandappu;Argha Sen;Ila Gokarn;Archan Misra,True,https://openreview.net/pdf?id=YxuuzyplFZ
YzM10FEJ2D,Touchstone Benchmark: Are We on the Right Way for Evaluating AI Algorithms for Medical Segmentation?,"How can we test AI performance? This question seems trivial, but it isn't. Standard benchmarks often have problems such as in-distribution and small-size test sets, oversimplified metrics, unfair comparisons, and short-term outcome pressure. As a consequence, good performance on standard benchmarks does not guarantee success in real-world scenarios. To address these problems, we present Touchstone, a large-scale collaborative segmentation benchmark of 9 types of abdominal organs. This benchmark is based on 5,195 training CT scans from 76 hospitals around the world and 5,903 testing CT scans from 11 additional hospitals. This diverse test set enhances the statistical significance of benchmark results and rigorously evaluates AI algorithms across various out-of-distribution scenarios. We invited 14 inventors of 19 AI algorithms to train their algorithms, while our team, as a third party, independently evaluated these algorithms on three test sets. In addition, we also evaluated pre-existing AI frameworks---which, differing from algorithms, are more flexible and can support different algorithms—including MONAI from NVIDIA, nnU-Net from DKFZ, and numerous other open-source frameworks. We are committed to expanding this benchmark to encourage more innovation of AI algorithms for the medical domain.",Datasets & Benchmarks,NeurIPS,2024,Poster,Pedro R. A. S. Bassi;Wenxuan Li;Yucheng Tang;Fabian Isensee;Zifu Wang;Jieneng Chen;Yu-Cheng Chou;Yannick Kirchhoff;Maximilian Rouven Rokuss;Ziyan Huang;Jin Ye;Junjun He;Tassilo Wald;Constantin Ulrich;Michael Baumgartner;Saikat Roy;Klaus Maier-Hein;Paul F Jaeger;Yiwen Ye;Yutong Xie;Jianpeng Zhang;Ziyang Chen;Yong Xia;Zhaohu Xing;Lei Zhu;Yousef Sadegheih;Afshin Bozorgpour;Pratibha Kumari;Reza Azad;Dorit Merhof;Pengcheng Shi;Ting Ma;Yuxin Du;Fan BAI;Tiejun Huang;Bo Zhao;Haonan Wang;Xiaomeng Li;Hanxue Gu;Haoyu Dong;Jichen Yang;Maciej A Mazurowski;Saumya Gupta;Linshan Wu;Jia-Xin Zhuang;Hao Chen;Holger R Roth;Daguang Xu;Matthew B. Blaschko;Sergio Decherchi;Andrea Cavalli;Alan Yuille;Zongwei Zhou,True,https://openreview.net/pdf?id=YzM10FEJ2D
ZDvXY56DeP,Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning,"In many Reinforcement Learning (RL) papers, learning curves are useful indicators to measure the effectiveness of RL algorithms. However, the complete raw data of the learning curves are rarely available. As a result, it is usually necessary to reproduce the experiments from scratch, which can be time-consuming and error-prone. We present Open RL Benchmark (ORLB), a set of fully tracked RL experiments, including not only the usual data such as episodic return, but also all algorithm-specific and system metrics. ORLB is community-driven: anyone can download, use, and contribute to the data. At the time of writing, more than 25,000 runs have been tracked, for a cumulative duration of more than 8 years. It covers a wide range of RL libraries and reference implementations. Special care is taken to ensure that each experiment is precisely reproducible by providing not only the full parameters, but also the versions of the dependencies used to generate it. In addition, ORLB comes with a command-line interface (CLI) for easy fetching and generating figures to present the results. In this document, we include two case studies to demonstrate the usefulness of ORLB in practice. To the best of our knowledge, ORLB is the first RL benchmark of its kind, and the authors hope that it will improve and facilitate the work of researchers in the field.",Datasets & Benchmarks,NeurIPS,2024,Reject,Shengyi Huang;Quentin Gallouédec;Florian Felten;Antonin Raffin;Rousslan Fernand Julien Dossa;Yanxiao Zhao;Ryan Sullivan;Viktor Makoviychuk;Denys Makoviichuk;Mohamad Hosein Danesh;Cyril Roumegous;Jiayi Weng;Chufan Chen;Md Masudur Rahman;João Guilherme Madeira Araújo;Guorui Quan;Daniel Chee Hian Tan;Timo Klein;Rujikorn Charakorn;Mark Towers;Yann Berthelot;Kinal Mehta;Dipam Chakraborty;Arjun KG;Valentin Charraut;Chang Ye;Zichen Liu;Lucas Nunes Alegre;Alexander Nikulin;Xiao Hu;Tianlin Liu;Jongwook Choi;Brent Yi,True,https://openreview.net/pdf?id=ZDvXY56DeP
ZGMkOikEyv,DetectRL: Benchmarking LLM-Generated Text Detection in Real-World Scenarios,"Detecting text generated by large language models (LLMs) is of great recent interest. With zero-shot methods like DetectGPT, detection capabilities have reached impressive levels. However, the reliability of existing detectors in real-world applications remains underexplored. In this study, we present a new benchmark, DetectRL, highlighting that even state-of-the-art (SOTA) detection techniques still underperformed in this task. 
    We collected human-written datasets from domains where LLMs are particularly prone to misuse. Using popular LLMs, we generated data that better aligns with real-world applications. Unlike previous studies, we employed heuristic rules to create adversarial LLM-generated text, simulating advanced prompt usages, human revisions like word substitutions, and writing errors. Our development of DetectRL reveals the strengths and limitations of current SOTA detectors. 
    More importantly, we analyzed the potential impact of writing styles, model types, attack methods, the text lengths, and real-world human writing factors on different types of detectors. We believe DetectRL could serve as an effective benchmark for assessing detectors in real-world scenarios, evolving with advanced attack methods, thus providing more stressful evaluation to drive the development of more efficient detectors\\\\footnote{Data and code are publicly available at: https://github.com/NLP2CT/DetectRL.",Datasets & Benchmarks,NeurIPS,2024,Poster,Junchao Wu;Runzhe Zhan;Derek F. Wong;Shu Yang;Xinyi Yang;Yulin Yuan;Lidia S. Chao,True,https://openreview.net/pdf?id=ZGMkOikEyv
ZIpdu0cHYu,Advancing Tool-Augmented Large Language Models: Integrating Insights from Errors in Inference Trees,"Tool-augmented large language models (LLMs) leverage tools, often in the form of APIs, to improve their reasoning capabilities on complex tasks. This enables them to act as intelligent agents interacting with the real world. The recently introduced ToolLLaMA model by Qin et al. [2023] utilizes the depth-first search-based decision tree (DFSDT) mechanism for multi-step reasoning with $16000+$ real-world APIs, effectively enhancing the performance of tool-augmented LLMs compared to traditional chain reasoning mechanisms. However, their approach only employs successful paths from decision trees (also called inference trees) for supervised fine-tuning (SFT), missing out on the potential learning opportunities from failed paths. Inspired by this, we propose an inference trajectory optimization framework based on preference learning to address this limitation. We first introduce a novel method for constructing step-wise preference data from tree-like expert trajectories, which leverages the previously ignored failed explorations in the decision trees. In the subsequent training phase, we first fine-tune the LLM with successful tool-usage expert trajectories and then apply direct preference optimization (DPO) with the preference data to update the LLM's policy, resulting in our ToolPrefer-LLaMA (TP-LLaMA) model. This approach not only enhances the utilization of original expert data but also broadens the learning space of the model. Our experiments demonstrate that by obtaining insights from errors in inference trees, TP-LLaMA significantly outperforms the baselines across almost all test scenarios by a large margin and exhibits better generalization capabilities with unseen APIs. At the same time, TP-LLaMA has also demonstrated superior reasoning efficiency compared to the baselines, making it more suitable for complex tool-usage reasoning tasks.",main,NeurIPS,2024,Poster,Sijia Chen;Yibo Wang;Yi-Feng Wu;Qing-Guo Chen;Zhao Xu;Weihua Luo;Kaifu Zhang;Lijun Zhang,True,https://openreview.net/pdf?id=ZIpdu0cHYu
ZMn2SPUgkU,VERIFIED: A Video Corpus Moment Retrieval Benchmark for Fine-Grained Video Understanding,"Existing Video Corpus Moment Retrieval (VCMR) is limited to coarse-grained understanding that hinders precise video moment localization when given fine-grained queries. In this paper, we propose a more challenging fine-grained VCMR benchmark requiring methods to localize the best-matched moment from the corpus with other partially matched candidates. To improve the dataset construction efficiency and guarantee high-quality data annotations, we propose VERIFIED, an automatic \\\\underline{V}id\\\\underline{E}o-text annotation pipeline to generate captions with \\\\underline{R}el\\\\underline{I}able \\\\underline{FI}n\\\\underline{E}-grained statics and \\\\underline{D}ynamics. Specifically, we resort to large language models (LLM) and large multimodal models (LMM) with our proposed Statics and Dynamics Enhanced Captioning modules to generate diverse fine-grained captions for each video. To filter out the inaccurate annotations caused by the LLM hallucination, we propose a Fine-Granularity Aware Noise Evaluator where we fine-tune a video foundation model with disturbed hard-negatives augmented contrastive and matching losses. With VERIFIED, we construct a more challenging fine-grained VCMR benchmark containing Charades-FIG, DiDeMo-FIG, and ActivityNet-FIG which demonstrate a high level of annotation quality. We evaluate several state-of-the-art VCMR models on the proposed dataset, revealing that there is still significant scope for fine-grained video understanding in VCMR.",Datasets & Benchmarks,NeurIPS,2024,Poster,Houlun Chen;Xin Wang;Hong Chen;Zeyang Zhang;Wei Feng;Bin Huang;Jia Jia;Wenwu Zhu,True,https://openreview.net/pdf?id=ZMn2SPUgkU
ZQy6dGlBay,A Benchmark Dataset for Event-Guided Human Pose Estimation and Tracking in Extreme Conditions,"Multi-person pose estimation and tracking have been actively researched by the computer vision community due to their practical applicability. However, existing human pose estimation and tracking datasets have only been successful in typical scenarios, such as those without motion blur or with well-lit conditions. These RGB-based datasets are limited to learning under extreme motion blur situations or poor lighting conditions, making them inherently vulnerable to such scenarios.
As a promising solution, bio-inspired event cameras exhibit robustness in extreme scenarios due to their high dynamic range and micro-second level temporal resolution. Therefore, in this paper, we introduce a new hybrid dataset encompassing both RGB and event data for human pose estimation and tracking in two extreme scenarios: low-light and motion blur environments. The proposed Event-guided Human Pose Estimation and Tracking in eXtreme Conditions (EHPT-XC) dataset covers cases of motion blur caused by dynamic objects and low-light conditions individually as well as both simultaneously. With EHPT-XC, we aim to inspire researchers to tackle pose estimation and tracking in extreme conditions by leveraging the advantageous of the event camera. Project pages are available at https://github.com/Chohoonhee/EHPT-XC.",Datasets & Benchmarks,NeurIPS,2024,Poster,Hoonhee Cho;Taewoo Kim;Yuhwan Jeong;Kuk-Jin Yoon,True,https://openreview.net/pdf?id=ZQy6dGlBay
ZRMAhpZ3ED,WFCRL: A Multi-Agent Reinforcement Learning Benchmark for Wind Farm Control,"The wind farm control problem is challenging, since conventional model-based control strategies require tractable models of complex aerodynamical interactions between the turbines and suffer from the curse of dimension when the number of turbines increases. Recently, model-free and multi-agent reinforcement learning approaches have been used to address this challenge. In this article, we introduce WFCRL (Wind Farm Control with Reinforcement Learning), the first suite of multi-agent reinforcement learning environments for the wind farm control problem. WFCRL frames a cooperative Multi-Agent Reinforcement Learning (MARL) problem: each turbine is an agent and can learn to adjust its yaw, pitch or torque to maximize the common objective (e.g. the total power production of the farm). WFCRL also offers turbine load observations that will allow to optimize the farm performance while limiting turbine structural damages. Interfaces with two state-of-the-art farm simulators are implemented in WFCRL: a static simulator (Floris) and a dynamic simulator (FAST.farm). For each simulator, $10$ wind layouts are provided, including $5$ real wind farms. Two state-of-the-art online MARL algorithms are implemented to illustrate the scaling challenges. As learning online on FAST.Farm is highly time-consuming, WFCRL offers the possibility of designing transfer learning strategies from Floris to FAST.Farm.",Datasets & Benchmarks,NeurIPS,2024,Poster,Claire Bizon Monroc;Ana Busic;Donatien Dubuc;Jiamin Zhu,True,https://openreview.net/pdf?id=ZRMAhpZ3ED
ZRz7XlxBzQ,Learning to compute Gröbner bases,"Solving a polynomial system, or computing an associated Gröbner basis, has been a fundamental task in computational algebra. However, it is also known for its notorious doubly exponential time complexity in the number of variables in the worst case. This paper is the first to address the learning of Gröbner basis computation with Transformers. The training requires many pairs of a polynomial system and the associated Gröbner basis, raising two novel algebraic problems: random generation of Gröbner bases and transforming them into non-Gröbner ones, termed as backward Gröbner problem. We resolve these problems with 0-dimensional radical ideals, the ideals appearing in various applications. Further, we propose a hybrid input embedding to handle coefficient tokens with continuity bias and avoid the growth of the vocabulary set. The experiments show that our dataset generation method is a few orders of magnitude faster than a naive approach, overcoming a crucial challenge in learning to compute Gröbner bases, and Gröbner computation is learnable in a particular class.",main,NeurIPS,2024,Poster,Hiroshi Kera;Yuki Ishihara;Yuta Kambe;Tristan Vaccon;Kazuhiro Yokoyama,True,https://openreview.net/pdf?id=ZRz7XlxBzQ
ZYrZ5V84ZI,Voila-A: Aligning Vision-Language Models with User's Gaze Attention,"In recent years, the integration of vision and language understanding has led to significant advancements in artificial intelligence, particularly through Vision-Language Models (VLMs). However, existing VLMs face challenges in handling real-world applications with complex scenes and multiple objects, as well as aligning their focus with the diverse attention patterns of human users. In this paper, we introduce gaze information, feasibly collected by ubiquitous wearable devices such as MR glasses, as a proxy for human attention to guide VLMs. We propose a novel approach, Voila-A, for gaze alignment to enhance the effectiveness of these models in real-world applications. First, we collect hundreds of minutes of gaze data to demonstrate that we can mimic human gaze modalities using localized narratives. We then design an automatic data annotation pipeline utilizing GPT-4 to generate the VOILA-COCO dataset. Additionally, we introduce a new model VOILA-A that integrate gaze information into VLMs while maintain pretrained knowledge from webscale dataset. We evaluate Voila-A using a hold-out validation set and a newly collected VOILA-GAZE testset, which features real-life scenarios captured with a gaze-tracking device. Our experimental results demonstrate that Voila-A significantly outperforms several baseline models. By aligning model attention with human gaze patterns, Voila-A paves the way for more intuitive, user-centric VLMs and fosters engaging human-AI interaction across a wide range of applications.",main,NeurIPS,2024,Spotlight,Kun Yan;Zeyu Wang;Lei Ji;Yuntao Wang;Nan Duan;Shuai Ma,True,https://openreview.net/pdf?id=ZYrZ5V84ZI
ZZ17sBJh3w,APDDv2: Aesthetics of Paintings and Drawings Dataset with Artist Labeled Scores and Comments,"Datasets play a pivotal role in training visual models, facilitating the development of abstract understandings of visual features through diverse image samples and multidimensional attributes. However, in the realm of aesthetic evaluation of artistic images, datasets remain relatively scarce. Existing painting datasets are often characterized by limited scoring dimensions and insufficient annotations, thereby constraining the advancement and application of automatic aesthetic evaluation methods in the domain of painting.
To bridge this gap, we introduce the Aesthetics Paintings and Drawings Dataset (APDD), the first comprehensive collection of paintings encompassing 24 distinct  artistic categories and 10 aesthetic attributes. Building upon the initial release of APDDv1, our ongoing research has identified opportunities for enhancement in data scale and annotation precision. Consequently, APDDv2 boasts an expanded image corpus and improved annotation quality, featuring detailed language comments to better cater to the needs of both researchers and practitioners seeking high-quality painting datasets.
Furthermore, we present an updated version of the Art Assessment Network for Specific Painting Styles, denoted as ArtCLIP. Experimental validation demonstrates the superior performance of this revised model in the realm of aesthetic evaluation, surpassing its predecessor in accuracy and efficacy.
The dataset and model are available at https://github.com/BestiVictory/APDDv2.git.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xin Jin;Qianqian Qiao;Yi Lu;HuayeWang;Heng Huang;Shan Gao;Jianfei Liu;Rui Li,True,https://openreview.net/pdf?id=ZZ17sBJh3w
Zg4zs0l2iH,CYCLO: Cyclic Graph Transformer Approach to Multi-Object Relationship Modeling in Aerial Videos,"Video scene graph generation (VidSGG) has emerged as a transformative approach to capturing and interpreting the intricate relationships among objects and their temporal dynamics in video sequences. In this paper, we introduce the new AeroEye dataset that focuses on multi-object relationship modeling in aerial videos. Our AeroEye dataset features various drone scenes and includes a visually comprehensive and precise collection of predicates that capture the intricate relationships and spatial arrangements among objects. To this end, we propose the novel Cyclic Graph Transformer (CYCLO) approach that allows the model to capture both direct and long-range temporal dependencies by continuously updating the history of interactions in a circular manner. The proposed approach also allows one to handle sequences with inherent cyclical patterns and process object relationships in the correct sequential order. Therefore, it can effectively capture periodic and overlapping relationships while minimizing information loss. The extensive experiments on the AeroEye dataset demonstrate the effectiveness of the proposed CYCLO model, demonstrating its potential to perform scene understanding on drone videos. Finally, the CYCLO method consistently achieves State-of-the-Art (SOTA) results on two in-the-wild scene graph generation benchmarks, i.e., PVSG and ASPIRe.",main,NeurIPS,2024,Poster,Trong-Thuan Nguyen;Pha Nguyen;Xin Li;Jackson Cothren;Alper Yilmaz;Khoa Luu,True,https://openreview.net/pdf?id=Zg4zs0l2iH
ZsxZ65YqL1,CriticEval: Evaluating Large-scale Language Model as Critic,"Critique ability, i.e., the capability of Large Language Models (LLMs) to identify and rectify flaws in responses, is crucial for their applications in self-improvement and scalable oversight. While numerous studies have been proposed to evaluate critique ability of LLMs, their comprehensiveness and reliability are still limited. To overcome this problem, we introduce CriticEval, a novel benchmark designed to comprehensively and reliably evaluate critique ability of LLMs. Specifically, to ensure the comprehensiveness, CriticEval evaluates critique ability from four dimensions across nine diverse task scenarios. It evaluates both scalar-valued and textual critiques, targeting responses of varying quality. To ensure the reliability, a large number of critiques are annotated to serve as references, enabling GPT-4 to evaluate textual critiques reliably. Extensive evaluations of open-source and closed-source LLMs first validate the reliability of evaluation in CriticEval. Then, experimental results demonstrate the promising potential of open-source LLMs, the effectiveness of critique datasets and several intriguing relationships between the critique ability and some critical factors, including task types, response qualities and critique dimensions.",main,NeurIPS,2024,Poster,Tian Lan;Wenwei Zhang;Chen Xu;Heyan Huang;Dahua Lin;Kai Chen;Xian-Ling Mao,True,https://openreview.net/pdf?id=ZsxZ65YqL1
ZsyFwzuDzD,"Revisiting, Benchmarking and Understanding Unsupervised Graph Domain Adaptation","Unsupervised Graph Domain Adaptation (UGDA) involves the transfer of knowledge from a label-rich source graph to an unlabeled target graph under domain discrepancies. Despite the proliferation of methods designed for this emerging task, the lack of standard experimental settings and fair performance comparisons makes it challenging to understand which and when models perform well across different scenarios. To fill this gap, we present the first comprehensive benchmark for unsupervised graph domain adaptation named GDABench, which encompasses 16 algorithms across diverse adaptation tasks. Through extensive experiments, we observe that the performance of current UGDA models varies significantly across different datasets and adaptation scenarios. Specifically, we recognize that when the source and target graphs face significant distribution shifts, it is imperative to formulate strategies to effectively address and mitigate graph structural shifts. We also find that with appropriate neighbourhood aggregation mechanisms, simple GNN variants can even surpass state-of-the-art UGDA baselines. To facilitate reproducibility, we have developed an easy-to-use library PyGDA for training and evaluating existing UGDA methods, providing a standardized platform in this community. Our source codes and datasets can be found at https://github.com/pygda-team/pygda.",Datasets & Benchmarks,NeurIPS,2024,Poster,Meihan Liu;Zhen Zhang;Jiachen Tang;Jiajun Bu;Bingsheng He;Sheng Zhou,True,https://openreview.net/pdf?id=ZsyFwzuDzD
a0WAM6q6fV,Croissant: A Metadata Format for ML-Ready Datasets,"Data is a critical resource for machine learning (ML), yet working with data remains a key friction point. This paper introduces Croissant,  a metadata format for datasets that creates a shared representation across ML tools, frameworks, and platforms. Croissant makes datasets more discoverable, portable, and interoperable, thereby addressing significant challenges in ML data management. Croissant is already supported by several popular dataset repositories, spanning hundreds of thousands of datasets, enabling easy loading into the most commonly-used ML frameworks, regardless of where the data is stored. Our initial evaluation by human raters shows that Croissant metadata is readable, understandable, complete, yet concise.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Mubashara Akhtar;Omar Benjelloun;Costanza Conforti;Luca Foschini;Joan Giner-Miguelez;Pieter Gijsbers;Sujata Goswami;Nitisha Jain;Michalis Karamousadakis;Michael Kuchnik;Satyapriya Krishna;Sylvain Lesage;Quentin Lhoest;Pierre Marcenac;Manil Maskey;Peter Mattson;Luis Oala;Hamidah Oderinwale;Pierre Ruyssen;Tim Santos;Rajat Shinde;Elena Simperl;Arjun Suresh;Goeff Thomas;Slava Tykhonov;Joaquin Vanschoren;Susheel Varma;Jos van der Velde;Steffen Vogler;Carole-Jean Wu;Luyao Zhang,False,https://openreview.net/pdf?id=a0WAM6q6fV
a6DteCxiw6,"Codec Avatar Studio: Paired Human Captures for Complete, Driveable, and Generalizable Avatars","To build photorealistic avatars that users can embody, human modelling must be complete (cover the full body), driveable (able to reproduce the current motion and appearance from the user), and generalizable (_i.e._, easily adaptable to novel identities).
Towards these goals, _paired_ captures, that is, captures of the same subject obtained from systems of diverse quality and availability, are crucial.
However, paired captures are rarely available to researchers outside of dedicated industrial labs: _Codec Avatar Studio_ is our proposal to close this gap.
Towards generalization and driveability, we introduce a dataset of 256 subjects captured in two modalities: high resolution multi-view scans of their heads, and video from the internal cameras of a headset.
Towards completeness, we introduce a dataset of 4 subjects captured in eight modalities: high quality relightable multi-view captures of heads and hands, full body multi-view captures with minimal and regular clothes, and corresponding head, hands and body phone captures.
Together with our data, we also provide code and pre-trained models for different state-of-the-art human generation models.
Our datasets and code are available at https://github.com/facebookresearch/ava-256 and https://github.com/facebookresearch/goliath.",Datasets & Benchmarks,NeurIPS,2024,Poster,Julieta Martinez;Emily Kim;Javier Romero;Timur Bagautdinov;Shunsuke Saito;Shoou-I Yu;Stuart Anderson;Michael Zollhöfer;Te-Li Wang;Shaojie Bai;Chenghui Li;Shih-En Wei;Rohan Joshi;Wyatt Borsos;Tomas Simon;Jason Saragih;Paul Theodosis;Alexander Greene;Anjani Josyula;Silvio Mano Maeta;Andrew I Jewett;Simion Venshtain;Christopher Heilman;Yueh-Tung Chen;Sidi Fu;Mohamed Ezzeldin A. Elshaer;Tingfang Du;Longhua Wu;Shen-Chi Chen;Kai Kang;Michael Wu;Youssef Emad;Steven Longay;Ashley Brewer;Hitesh Shah;James Booth;Taylor Koska;Kayla Haidle;Matthew Andromalos;Joanna Ching-Hui Hsu;Thomas Dauer;Peter Selednik;Tim Godisart;Scott Ardisson;Matthew Cipperly;Ben Humberston;Lon Farr;Bob Hansen;Peihong Guo;Dave Braun;Steven Krenn;He Wen;Lucas Evans;Natalia Fadeeva;Matthew Stewart;Gabriel Schwartz;Divam Gupta;Gyeongsik Moon;Kaiwen Guo;Yuan Dong;Yichen Xu;Takaaki Shiratori;Fabian Andres Prada Nino;Bernardo R Pires;Bo Peng;Julia Buffalini;Autumn Trimble;Kevyn Alex Anthony McPhail;Melissa Robinson Schoeller;Yaser Sheikh,True,https://openreview.net/pdf?id=a6DteCxiw6
a7LPpyFWj2,DECO-Bench: Unified Benchmark for Decoupled Task-Agnostic Synthetic Data Release,"In this work, we tackle the question of how to systematically benchmark task-agnostic decoupling methods for privacy-preserving machine learning (ML). Sharing datasets that include sensitive information often triggers privacy concerns, necessitating robust decoupling methods to separate sensitive and non-sensitive attributes. Despite the development of numerous decoupling techniques, a standard benchmark for systematically comparing these methods remains absent. Our framework integrates various decoupling techniques along with synthetic data
generation and evaluation protocols within a unified system. Using our framework, we benchmark various decoupling techniques and evaluate their privacy-utility trade-offs. Finally, we release our source code, pre-trained models, datasets of decoupled representations to foster research in this area.",Datasets & Benchmarks,NeurIPS,2024,Poster,Farzaneh Askari;Lingjuan Lyu;Vivek Sharma,True,https://openreview.net/pdf?id=a7LPpyFWj2
aJ1yse8GEr,GlotCC: An Open Broad-Coverage CommonCrawl Corpus and Pipeline for Minority Languages,"The need for large text corpora has increased with the advent of pretrained language models and, in particular, the discovery of scaling laws for these models. Most available corpora have sufficient data only for languages with large dominant communities. However, there is no corpus available that (i) covers a wide range of minority languages; (ii) is generated by an open-source reproducible pipeline; and (iii) is rigorously cleaned from noise, making it trustworthy to use. We present GlotCC, a clean, document-level, 2TB general domain corpus derived from CommonCrawl, covering more than 1000 languages. We make GlotCC and the system used to generate it— including the pipeline, language identification model, and filters—available to the research community.

Corpus v. 1.0 https://huggingface.co/datasets/cis-lmu/GlotCC-v1

Pipeline v. 3.0 https://github.com/cisnlp/GlotCC",Datasets & Benchmarks,NeurIPS,2024,Poster,Amir Hossein Kargaran;François Yvon;Hinrich Schuetze,True,https://openreview.net/pdf?id=aJ1yse8GEr
aTXhTD44nF,USDC: A Dataset of $\\\\underline{U}$ser $\\\\underline{S}$tance and $\\\\underline{D}$ogmatism in Long $\\\\underline{C}$onversations,"Identifying user's opinions and stances in long conversation threads on various topics can be extremely critical for enhanced personalization, market research, political campaigns, customer service, conflict resolution, targeted advertising and content moderation. Hence, training language models to automate this task is critical. However, to train such models, gathering manual annotations has multiple challenges: 1) it is both time-consuming and costly; 2) conversation threads could be very long increasing chances of noisy annotations; and 3) interpreting instances where a user changes their opinion within a conversation is difficult because often times such transitions are subtle and not expressed explicitly. Inspired by the recent success of large language models (LLMs) for complex natural language processing (NLP) tasks, we leverage Mistral Large and GPT-4 for automating the human annotation process on the following two tasks while also providing reasoning: i) user Stance detection, which involves labeling a user's stance of a post in a conversation on a five-point scale; ii) user Dogmatism detection, which deals with labeling a user's overall opinion in the conversation on a four-point scale. Majority voting on zero-shot, one-shot, few-shot annotations from these two LLMs on 764 multi-user Reddit conversations helps us curate the USDC dataset. USDC is then used to finetune and instruction-tune multiple deployable small language models for the 5-class stance and 4-class dogmatism classification tasks. We make the code and dataset publicly available [https://anonymous.4open.science/r/USDC-0F7F].",Datasets & Benchmarks,NeurIPS,2024,Reject,mounika marreddy;SUBBA REDDY OOTA;Venkata Charan Chinni;Manish Gupta;Lucie Flek,True,https://openreview.net/pdf?id=aTXhTD44nF
aXeiCbMFFJ,Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning,"Multi-Modal Large Language Models (MLLMs) have demonstrated impressive performance in various VQA tasks. However, they often lack interpretability and struggle with complex visual inputs, especially when the resolution of the input image is high or when the interested region that could provide key information for answering the question is small. To address these challenges, we collect and introduce the large-scale Visual CoT dataset comprising 438k question-answer pairs, annotated with intermediate bounding boxes highlighting key regions essential for answering the questions. Additionally, about 98k pairs of them are annotated with detailed reasoning steps. Importantly, we propose a multi-turn processing pipeline that dynamically focuses on visual inputs and provides interpretable thoughts. We also introduce the related benchmark to evaluate the MLLMs in scenarios requiring specific local region identification.
Extensive experiments demonstrate the effectiveness of our framework and shed light on better inference strategies. The Visual CoT dataset, benchmark, and pre-trained models are available on this [website](https://hao-shao.com/projects/viscot.html) to support further research in this area.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Hao Shao;Shengju Qian;Han Xiao;Guanglu Song;Zhuofan Zong;Letian Wang;Yu Liu;Hongsheng Li,True,https://openreview.net/pdf?id=aXeiCbMFFJ
abXaOcvujs,WikiDBs: A Large-Scale Corpus Of Relational Databases From Wikidata,"Deep learning on tabular data, and particularly tabular representation learning, has recently gained growing interest. However, representation learning for relational databases with multiple tables is still an underexplored area, which may be attributed to the lack of openly available resources. To support the development of foundation models for tabular data and relational databases, we introduce WikiDBs, a novel open-source corpus of 100,000 relational databases. Each database consists of multiple tables connected by foreign keys. The corpus is based on Wikidata and aims to follow certain characteristics of real-world databases. In this paper, we describe the dataset and our method for creating it. By making our code publicly available, we enable others to create tailored versions of the dataset, for example, by creating databases in different languages. Finally, we conduct a set of initial experiments to showcase how WikiDBs can be used to train for data engineering tasks, such as missing value imputation and column type annotation.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Liane Vogel;Jan-Micha Bodensohn;Carsten Binnig,True,https://openreview.net/pdf?id=abXaOcvujs
abuQMKDVkW,SARDet-100K: Towards Open-Source Benchmark and ToolKit for Large-Scale SAR Object Detection,"Synthetic Aperture Radar (SAR) object detection has gained significant attention recently due to its irreplaceable all-weather imaging capabilities. However, this research field suffers from both limited public datasets (mostly comprising <2K images with only mono-category objects) and inaccessible source code. To tackle these challenges, we establish a new benchmark dataset and an open-source method for large-scale SAR object detection. Our dataset, SARDet-100K, is a result of intense surveying, collecting, and standardizing 10 existing SAR detection datasets, providing a large-scale and diverse dataset for research purposes. To the best of our knowledge, SARDet-100K is the first COCO-level large-scale multi-class SAR object detection dataset ever created. With this high-quality dataset, we conducted comprehensive experiments and uncovered a crucial challenge in SAR object detection: the substantial disparities between the pretraining on RGB datasets and finetuning on SAR datasets in terms of both data domain and model structure. To bridge these gaps, we propose a novel Multi-Stage with Filter Augmentation (MSFA) pretraining framework that tackles the problems from the perspective of data input, domain transition, and model migration. The proposed MSFA method significantly enhances the performance of SAR object detection models while demonstrating exceptional generalizability and flexibility across diverse models. This work aims to pave the way for further advancements in SAR object detection. The dataset and code is available at \\\\url{https://github.com/zcablii/SARDet_100K}.",main,NeurIPS,2024,Spotlight,Yuxuan Li;Xiang Li;Weijie Li;Qibin Hou;Li Liu;Ming-Ming Cheng;Jian Yang,True,https://openreview.net/pdf?id=abuQMKDVkW
aekfb95slj,PINNacle: A Comprehensive Benchmark of Physics-Informed Neural Networks for Solving PDEs,"While significant progress has been made on Physics-Informed Neural Networks (PINNs), a comprehensive comparison of these methods across a wide range of Partial Differential Equations (PDEs) is still lacking. This study introduces PINNacle, a benchmarking tool designed to fill this gap. PINNacle provides a diverse dataset, comprising over 20 distinct PDEs from various domains, including heat conduction, fluid dynamics, biology, and electromagnetics. These PDEs encapsulate key challenges inherent to real-world problems, such as complex geometry, multi-scale phenomena, nonlinearity, and high dimensionality. PINNacle also offers a user-friendly toolbox, incorporating about 10 state-of-the-art PINN methods for systematic evaluation and comparison. We have conducted extensive experiments with these methods, offering insights into their strengths and weaknesses. In addition to providing a standardized means of assessing performance, PINNacle also offers an in-depth analysis to guide future research, particularly in areas such as domain decomposition methods and loss reweighting for handling multi-scale problems and complex geometry.  To the best of our knowledge, it is the largest benchmark with a diverse and comprehensive evaluation that will undoubtedly foster further research in PINNs.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zhongkai Hao;Jiachen Yao;Chang Su;Hang Su;Ziao Wang;Fanzhi Lu;Zeyu Xia;Yichi Zhang;Songming Liu;Lu Lu;Jun Zhu,True,https://openreview.net/pdf?id=aekfb95slj
aiGN4UnNM7,TSTTC: A Large-Scale Dataset for Time-to-Contact Estimation in Driving Scenarios,"Time-to-Contact (TTC) estimation is a critical task for assessing collision risk and is widely used in various driver assistance and autonomous driving systems. The past few decades have witnessed development of related theories and algorithms. The prevalent learning-based methods call for a large-scale TTC dataset in real-world scenarios. In this work, we present a large-scale object oriented TTC dataset in the driving scene for promoting the TTC estimation by a monocular camera. To collect valuable samples and make data with different TTC values relatively balanced, we go through thousands of hours of driving data and select over 200K sequences with a preset data distribution. To augment the quantity of small TTC cases, we also generate clips using the latest Neural rendering methods. Additionally, we provide several simple yet effective TTC estimation baselines and evaluate them extensively on the proposed dataset to demonstrate their effectiveness. The proposed dataset and code are publicly available at \\\\href{https://pan.baidu.com/s/14oHBEDytYZh6C3ah38a8zw?pwd=rdh3#list/path=%2F&parentPath=%2Fsharelink1829140692-712578186815897}{dataset link} and \\\\href{https://github.com/tusen-ai/TSTTC}{code link}.",Datasets & Benchmarks,NeurIPS,2024,Reject,Yuheng Shi;Zehao Huang;Yan Yan;Naiyan Wang;Xiaojie Guo,True,https://openreview.net/pdf?id=aiGN4UnNM7
akEt8QAa6V,GTA: A Benchmark for General Tool Agents,"In developing general-purpose agents, significant focus has been placed on integrating large language models (LLMs) with various tools. This poses a challenge to the tool-use capabilities of LLMs. However, there are evident gaps between existing tool evaluations and real-world scenarios. Current evaluations often use AI-generated queries, single-step tasks, dummy tools, and text-only inputs, which fail to reveal the agents' real-world problem-solving abilities effectively. To address this, we propose GTA, a benchmark for **G**eneral **T**ool **A**gents,  featuring three main aspects: (i) *Real user queries*: human-written queries with simple real-world objectives but implicit tool-use, requiring the LLM to reason the suitable tools and plan the solution steps. (ii) *Real deployed tools*: an evaluation platform equipped with tools across perception, operation, logic, and creativity categories to evaluate the agents' actual task execution performance. (iii) *Real multimodal inputs*: authentic image files, such as spatial scenes, web page screenshots, tables, code snippets, and printed/handwritten materials, used as the query contexts to align with real-world scenarios closely. We designed 229 real-world tasks and executable tool chains to evaluate mainstream LLMs. Our findings show that real-world user queries are challenging for existing LLMs, with GPT-4 completing less than 50\\\\% of the tasks and most LLMs achieving below 25\\\\%. This evaluation reveals the bottlenecks in the tool-use capabilities of current LLMs in real-world scenarios, which is beneficial for the advancement of general-purpose tool agents. Dataset and code are available at https://github.com/open-compass/GTA.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jize Wang;Ma Zerun;Yining Li;Songyang Zhang;Cailian Chen;Kai Chen;Xinyi Le,True,https://openreview.net/pdf?id=akEt8QAa6V
aou5yrBqKy,TabPedia: Towards Comprehensive Visual Table Understanding with Concept Synergy,"Tables contain factual and quantitative data accompanied by various structures and contents that pose challenges for machine comprehension. Previous methods generally design task-specific architectures and objectives for individual tasks, resulting in modal isolation and intricate workflows. In this paper, we present a novel large vision-language model, TabPedia, equipped with a concept synergy mechanism. In this mechanism, all the involved diverse visual table understanding (VTU) tasks and multi-source visual embeddings are abstracted as concepts. This unified framework allows TabPedia to seamlessly integrate VTU tasks, such as table detection, table structure recognition, table querying, and table question answering, by leveraging the capabilities of large language models (LLMs). Moreover, the concept synergy mechanism enables table perception-related and comprehension-related tasks to work in harmony, as they can effectively leverage the needed clues from the corresponding source perception embeddings. Furthermore, to better evaluate the VTU task in real-world scenarios, we establish a new and comprehensive table VQA benchmark, ComTQA, featuring approximately 9,000 QA pairs. Extensive quantitative and qualitative experiments on both table perception and comprehension tasks, conducted across various public benchmarks, validate the effectiveness of our TabPedia. The superior performance further confirms the feasibility of using LLMs for understanding visual tables when all concepts work in synergy. The benchmark ComTQA has been open-sourced at https://huggingface.co/datasets/ByteDance/ComTQA. The source code and model also have been released at https://github.com/zhaowc-ustc/TabPedia.",main,NeurIPS,2024,Poster,Weichao Zhao;Hao Feng;Qi Liu;Jingqun Tang;Binghong Wu;Lei Liao;Shu Wei;Yongjie Ye;Hao Liu;Wengang Zhou;Houqiang Li;Can Huang,True,https://openreview.net/pdf?id=aou5yrBqKy
ap4x1kArGy,ODRL: A Benchmark for Off-Dynamics Reinforcement Learning,"We consider off-dynamics reinforcement learning (RL) where one needs to transfer policies across different domains with dynamics mismatch. Despite the focus on developing dynamics-aware algorithms, this field is hindered due to the lack of a standard benchmark. To bridge this gap, we introduce ODRL, the first benchmark tailored for evaluating off-dynamics RL methods. ODRL contains four experimental settings where the source and target domains can be either online or offline, and provides diverse tasks and a broad spectrum of dynamics shifts, making it a reliable platform to comprehensively evaluate the agent's adaptation ability to the target domain. Furthermore, ODRL includes recent off-dynamics RL algorithms in a unified framework and introduces some extra baselines for different settings, all implemented in a single-file manner. To unpack the true adaptation capability of existing methods, we conduct extensive benchmarking experiments, which show that no method has universal advantages across varied dynamics shifts. We hope this benchmark can serve as a cornerstone for future research endeavors. Our code is publicly available at https://github.com/OffDynamicsRL/off-dynamics-rl.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jiafei Lyu;Kang Xu;Jiacheng Xu;Mengbei Yan;Jing-Wen Yang;Zongzhang Zhang;Chenjia Bai;Zongqing Lu;Xiu Li,True,https://openreview.net/pdf?id=ap4x1kArGy
ar8aRMrmod,Evaluating Copyright Takedown Methods for Language Models,"Language models (LMs) derive their capabilities from extensive training on diverse data, including copyrighted material. 
These models can memorize and generate content similar to their training data, potentially risking legal issues like copyright infringement.
Therefore, model creators are motivated to develop mitigation methods that prevent generating particular copyrighted content, an ability we refer to as *copyright takedowns*. This paper introduces the first evaluation of the feasibility and side effects of copyright takedowns for LMs. We propose CoTaEval, an evaluation framework to assess the effectiveness of copyright takedown methods,
the impact on the model's ability to retain uncopyrightable factual knowledge from the copyrighted content, and how well the model maintains its general utility and efficiency.
We examine several strategies, including adding system prompts, decoding-time filtering interventions, and unlearning approaches. Our findings indicate that no method excels across all metrics, showing significant room for research in this unique problem setting and indicating potential unresolved challenges for live policy proposals.",Datasets & Benchmarks,NeurIPS,2024,Poster,Boyi Wei;Weijia Shi;Yangsibo Huang;Noah A. Smith;Chiyuan Zhang;Luke Zettlemoyer;Kai Li;Peter Henderson,True,https://openreview.net/pdf?id=ar8aRMrmod
ayF8bEKYQy,OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for Superintelligent AI,"The evolution of Artificial Intelligence (AI) has been significantly accelerated by advancements in Large Language Models (LLMs) and Large Multimodal Models (LMMs), gradually showcasing potential cognitive reasoning abilities in problem-solving and scientific discovery (i.e., AI4Science) once exclusive to human intellect. To comprehensively evaluate current models' performance in cognitive reasoning abilities, we introduce OlympicArena, which includes 11,163 bilingual problems across both text-only and interleaved text-image modalities. These challenges encompass a wide range of disciplines spanning seven fields and 62 international Olympic competitions, rigorously examined for data leakage. We argue that the challenges in Olympic competition problems are ideal for evaluating AI's cognitive reasoning due to their complexity and interdisciplinary nature, which are essential for tackling complex scientific challenges and facilitating discoveries. Beyond evaluating performance across various disciplines using answer-only criteria, we conduct detailed experiments and analyses from multiple perspectives. We delve into the models' cognitive reasoning abilities, their performance across different modalities, and their outcomes in process-level evaluations, which are vital for tasks requiring complex reasoning with lengthy solutions. Our extensive evaluations reveal that even advanced models like GPT-4o only achieve a 39.97\\\\%  overall accuracy (28.67\\\\%  for mathematics and 29.71\\\\%  for physics), illustrating current AI limitations in complex reasoning and multimodal integration. Through the OlympicArena, we aim to advance AI towards superintelligence, equipping it to address more complex challenges in science and beyond. We also provide a comprehensive set of resources to support AI research, including a benchmark dataset, an open-source annotation platform, a detailed evaluation tool, and a leaderboard with automatic submission features.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zhen Huang;Zengzhi Wang;Shijie Xia;Xuefeng Li;Haoyang Zou;Ruijie Xu;Run-Ze Fan;Lyumanshan Ye;Ethan Chern;Yixin Ye;Yikai Zhang;Yuqing Yang;Ting Wu;Binjie Wang;Shichao Sun;Yang Xiao;Yiyuan Li;Fan Zhou;Steffi Chern;Yiwei Qin;Yan Ma;Jiadi Su;Yixiu Liu;Yuxiang Zheng;Shaoting Zhang;Dahua Lin;Yu Qiao;Pengfei Liu,True,https://openreview.net/pdf?id=ayF8bEKYQy
b57BKV8qKQ,Implicit Zoo: A Large-Scale Dataset of Neural Implicit Functions for 2D Images and 3D Scenes,"Neural implicit functions have demonstrated significant importance in various areas such as computer vision, graphics. Their advantages include the ability to represent complex shapes and scenes with high fidelity, smooth interpolation capabilities, and continuous representations. Despite these benefits, the development and analysis of implicit functions have been limited by the lack of comprehensive datasets and the substantial computational resources required for their implementation and evaluation. To address these challenges, we introduce ""Implicit-Zoo"": a large-scale dataset requiring thousands of GPU training days designed to facilitate research and development in this field. Our dataset includes diverse 2D and 3D scenes, such as CIFAR-10, ImageNet-1K, and Cityscapes for 2D image tasks, and the OmniObject3D dataset for 3D vision tasks. We ensure high quality through strict checks, refining or filtering out low-quality data. Using Implicit-Zoo, we showcase two immediate benefits as it enables to: (1) learn token locations for transformer models; (2) Directly regress 3D cameras poses of 2D images with respect to NeRF models. This in turn leads to an \\\\emph{improved performance} in all three task of  image classification, semantic segmentation, and 3D pose regression -- thereby unlocking new avenues for research.",Datasets & Benchmarks,NeurIPS,2024,Poster,Qi Ma;Danda Pani Paudel;Ender Konukoglu;Luc Van Gool,True,https://openreview.net/pdf?id=b57BKV8qKQ
b5n3lKRLzk,emg2pose: A Large and Diverse Benchmark for Surface Electromyographic Hand Pose Estimation,"Hands are the primary means through which humans interact with the world. Reliable and always-available hand pose inference could yield new and intuitive control schemes for human-computer interactions, particularly in virtual and augmented reality. Computer vision is effective but requires one or multiple cameras and can struggle with occlusions, limited field of view, and poor lighting. Wearable wrist-based surface electromyography (sEMG) presents a promising alternative as an always-available modality sensing muscle activities that drive hand motion. However, sEMG signals are strongly dependent on user anatomy and sensor placement; existing sEMG models have thus required hundreds of users and device placements to effectively generalize for tasks other than pose inference. To facilitate progress on sEMG pose inference, we introduce the emg2pose benchmark, which is to our knowledge the first publicly available dataset of high-quality hand pose labels and wrist sEMG recordings. emg2pose contains 2kHz, 16 channel sEMG and pose labels from a 26-camera motion capture rig for 193 users, 370 hours, and 29 stages with diverse gestures - a scale comparable to vision-based hand pose datasets. We provide competitive baselines and challenging tasks evaluating real-world generalization scenarios: held-out users, sensor placements, and stages. This benchmark provides the machine learning community a platform for exploring complex generalization problems, holding potential to significantly enhance the development of sEMG-based human-computer interactions.",Datasets & Benchmarks,NeurIPS,2024,Poster,Sasha Salter;Richard Warren;Collin Schlager;Adrian Spurr;Shangchen Han;Rohin Bhasin;Yujun Cai;Peter Walkington;Anuoluwapo Bolarinwa;Robert Wang;Nathan Danielson;Josh Merel;Eftychios A. Pnevmatikakis;Jesse D Marshall,True,https://openreview.net/pdf?id=b5n3lKRLzk
b6IBmU1uzw,CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models,"Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment. In this paper, we introduce CARES and aim to comprehensively evaluate the Trustworthiness of Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs across five dimensions, including trustfulness, fairness, safety, privacy, and robustness. CARES comprises about 41K question-answer pairs in both closed and open-ended formats, covering 16 medical image modalities and 27 anatomical regions. Our analysis reveals that the models consistently exhibit concerns regarding trustworthiness, often displaying factual inaccuracies and failing to maintain fairness across different demographic groups. Furthermore, they are vulnerable to attacks and demonstrate a lack of privacy awareness. We publicly release our benchmark and code in https://github.com/richard-peng-xia/CARES.",Datasets & Benchmarks,NeurIPS,2024,Poster,Peng Xia;Ze Chen;Juanxi Tian;Gong Yangrui;Ruibo Hou;Yue Xu;Zhenbang Wu;Zhiyuan Fan;Yiyang Zhou;Kangyu Zhu;Wenhao Zheng;Zhaoyang Wang;Xiao Wang;Xuchao Zhang;Chetan Bansal;Marc Niethammer;Junzhou Huang;Hongtu Zhu;Yun Li;Jimeng Sun;Zongyuan Ge;Gang Li;James Zou;Huaxiu Yao,True,https://openreview.net/pdf?id=b6IBmU1uzw
bAaM8cKoMl,MindSet: Vision. A toolbox for testing DNNs on key psychological experiments,"Multiple benchmarks have been developed to assess the alignment between deep neural networks (DNNs) and human vision. In almost all cases these benchmarks are observational in the sense they are composed of behavioural and brain responses to naturalistic images that have not been manipulated to test hypotheses regarding how DNNs or humans perceive and identify objects. Here we introduce the toolbox MindSet: Vision, consisting of a collection of image datasets and related scripts designed to test DNNs on 30 psychological findings. In all experimental conditions, the stimuli are systematically manipulated to test specific hypotheses regarding human visual perception and object recognition. In addition to providing pre-generated datasets of images, we provide code to regenerate these datasets, offering many configurable parameters which greatly extend the dataset versatility for different research contexts, and code to facilitate the testing of DNNs on these image datasets using three different methods (similarity judgments, out-of-distribution classification, and decoder method), accessible at https://github.com/ValerioB88/mindset-vision. We test ResNet-152 on each of these methods as an example of how the toolbox can be used.",Datasets & Benchmarks,NeurIPS,2024,Reject,Valerio Biscione;Dong Yin;Gaurav Malhotra;Marin Dujmovic;Milton L. Montero;Guillermo Puebla;Federico Adolfi;Rachel F Heaton;John E. Hummel;Benjamin D. Evans;Karim G. Habashy;Jeffrey Bowers,True,https://openreview.net/pdf?id=bAaM8cKoMl
bAxUA5r3Ss,TaskBench: Benchmarking Large Language Models for Task Automation,"In recent years, the remarkable progress of large language models (LLMs) has sparked interest in task automation, which involves decomposing complex tasks described by user instructions into sub-tasks and invoking external tools to execute them, playing a central role in autonomous agents. However, there is a lack of systematic and standardized benchmarks to promote the development of LLMs in task automation. To address this, we introduce TaskBench, a comprehensive framework to evaluate the capability of LLMs in task automation. Specifically, task automation can be divided into three critical stages: task decomposition, tool selection, and parameter prediction. To tackle the complexities inherent in these stages, we introduce the concept of Tool Graph to represent decomposed tasks and adopt a back-instruct method to generate high-quality user instructions. We propose TaskEval, a multi-faceted evaluation methodology that assesses LLM performance across these three stages. Our approach combines automated construction with rigorous human verification, ensuring high consistency with human evaluation. Experimental results demonstrate that TaskBench effectively reflects the capabilities of various LLMs in task automation. It provides insights into model performance across different task complexities and domains, pushing the boundaries of what current models can achieve. TaskBench offers a scalable, adaptable, and reliable benchmark for advancing LLM-based autonomous agents.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yongliang Shen;Kaitao Song;Xu Tan;Wenqi Zhang;Kan Ren;Siyu Yuan;Weiming Lu;Dongsheng Li;Yueting Zhuang,False,https://openreview.net/pdf?id=bAxUA5r3Ss
bCL9U2X9Jg,Easy Regional Contrastive Learning of Expressive Fashion Representations,"When learning vision-language models (VLM) for the fashion domain, most existing works design new architectures from vanilla BERT with additional objectives, or perform dense multi-task learning with fashion-specific tasks. Though progress has been made, their architecture or objectives are often intricate and the extendibility is limited.
By contrast, with simple architecture (comprising only two unimodal encoders) and just the contrastive objective, popular pre-trained VL models (e.g., CLIP) achieve superior performance in general domains, which are further easily extended to downstream tasks.
However, inheriting such benefits of CLIP in the fashion domain is non-trivial in the presence of the notable domain gap. Empirically, we find that directly finetuning on fashion data leads CLIP to frequently ignore minor yet important details such as logos and composition, which are critical in fashion tasks such as retrieval and captioning.
In this work, to maintain CLIP's simple architecture and objective while explicitly attending to fashion details, we propose $E^2$: Easy Regional Contrastive Learning of Expressive Fashion Representations.
$E^2$ introduces only a few selection tokens and fusion blocks (just 1.9\\\\% additional parameters in total) with only contrastive losses. Despite lightweight, in our primary focus, cross-modal retrieval, $E^2$ notably outperforms existing fashion VLMs with various fashion-specific objectives.
Moreover, thanks to CLIP's widespread use in downstream tasks in general domains (e.g., zero-shot composed image retrieval and image captioning), our model can easily extend these models  from general domain to the fashion domain with notable improvement.
To conduct a comprehensive evaluation, we further collect data from Amazon Reviews to build a new dataset for cross-modal retrieval in the fashion domain.",main,NeurIPS,2024,Poster,Daiqing Qi;Handong Zhao;Sheng Li,True,https://openreview.net/pdf?id=bCL9U2X9Jg
bCMpdaQCNW,Cracking the Code of Juxtaposition: Can AI Models Understand the Humorous Contradictions,"Recent advancements in large vision language models have demonstrated remarkable proficiency across a wide range of tasks. 
Yet, these models still struggle with understanding the nuances of human humor through juxtaposition, particularly when it involves nonlinear narratives that underpin many jokes and humor cues.  This paper investigates this challenge by focusing on comics with contradictory narratives, where each comic consists of two panels that create a humorous contradiction. We introduce the YesBut benchmark, which comprises tasks of varying difficulty aimed at assessing AI's capabilities in recognizing and interpreting these comics, ranging from literal content comprehension to deep narrative reasoning. Through extensive experimentation and analysis of recent commercial or open-sourced large vision language models, we assess their capability to comprehend the complex interplay of the narrative humor inherent in these comics. Our results show that even the state-of-the-art models still struggle with this task. Our findings offer insights into the current limitations and potential improvements for AI in understanding human creative expressions.",main,NeurIPS,2024,Oral,Zhe Hu;Tuo Liang;Jing Li;Yiren Lu;Yunlai Zhou;Yiran Qiao;Jing Ma;Yu Yin,True,https://openreview.net/pdf?id=bCMpdaQCNW
bIFHHf2RoD,CulturePark: Boosting Cross-cultural Understanding in Large Language Models,"Cultural bias is pervasive in many large language models (LLMs), largely due to the deficiency of data representative of different cultures.
Typically, cultural datasets and benchmarks are constructed either by extracting subsets of existing datasets or by aggregating from platforms such as Wikipedia and social media.
However, these approaches are highly dependent on real-world data and human annotations, making them costly and difficult to scale.
Inspired by cognitive theories on social communication, this paper introduces CulturePark, an LLM-powered multi-agent communication framework for cultural data collection.
CulturePark simulates cross-cultural human communication with LLM-based agents playing roles in different cultures.
It generates high-quality cross-cultural dialogues encapsulating human beliefs, norms, and customs.
Using CulturePark, we generated 41,000 cultural samples to fine-tune eight culture-specific LLMs.
We evaluated these models across three downstream tasks: content moderation, cultural alignment, and cultural education.
Results show that for content moderation, our GPT-3.5-based models either match or outperform GPT-4 on $41$ datasets. Regarding cultural alignment, our models surpass GPT-4 on Hofstede's VSM 13 framework.
Furthermore, for cultural education of human participants, our models demonstrate superior outcomes in both learning efficacy and user experience compared to GPT-4. CulturePark proves an important step in addressing cultural bias and advancing the democratization of AI, highlighting the critical role of culturally inclusive data in model training. Code is released at https://github.com/Scarelette/CulturePark.",main,NeurIPS,2024,Poster,CHENG LI;Damien Teney;Linyi Yang;Qingsong Wen;Xing Xie;Jindong Wang,True,https://openreview.net/pdf?id=bIFHHf2RoD
bIRcf8i1kp,GarmentLab: A Unified Simulation and Benchmark for Garment Manipulation,"Manipulating garments and fabrics has long been a critical endeavor in the development of home-assistant robots. However, due to complex dynamics and topological structures, garment manipulations pose significant challenges. Recent successes in reinforcement learning and vision-based methods offer promising avenues for learning garment manipulation. Nevertheless, these approaches are severely constrained by current benchmarks, which exhibit offer limited diversity of tasks and unrealistic simulation behavior. Therefore, we present GarmentLab, a content-rich benchmark and realistic simulation designed for deformable object and garment manipulation. Our benchmark encompasses a diverse range of garment types, robotic systems and manipulators. The abundant tasks in the benchmark further explores of the interactions between garments, deformable objects, rigid bodies, fluids, and human body. Moreover, by incorporating multiple simulation methods such as FEM and PBD, along with our proposed sim-to-real algorithms and real-world benchmark, we aim to significantly narrow the sim-to-real gap. We evaluate state-of-the-art vision methods, reinforcement learning, and imitation learning approaches on these tasks, highlighting the challenges faced by current algorithms, notably their limited generalization capabilities. Our proposed open-source environments and comprehensive analysis show promising boost to future research in garment manipulation by unlocking the full potential of these methods. We guarantee that we will open-source our code as soon as possible. You can watch the videos in supplementary files to learn more about the details of our work.",main,NeurIPS,2024,Poster,Haoran Lu;Ruihai Wu;Yitong Li;Sijie Li;Ziyu Zhu;Chuanruo Ning;Yan Shen;Longzan Luo;Yuanpei Chen;Hao Dong,True,https://openreview.net/pdf?id=bIRcf8i1kp
bJddXCyosA,VisMin: Visual Minimal-Change Understanding,"Fine-grained understanding of objects, attributes, and relationships between objects is crucial for visual-language models (VLMs). To evaluate VLMs' fine-grained understanding, existing benchmarks primarily focus on evaluating VLMs' capability to distinguish between two very similar captions given an image. In this paper, our focus is on evaluating VLMs' capability to distinguish between two very similar images given a caption. To this end, we introduce a new, challenging benchmark termed Visual Minimal-Change Understanding (VisMin), which requires models to predict the correct image-caption match given two images and two captions. Importantly, the image pair (as well as the caption pair) contains minimal changes, i.e., between the two images (as well as between the two captions), only one aspect changes at a time from among the following possible types of changes: object, attribute, count, and spatial relation. These four types of minimal changes are specifically designed to test the models' understanding of objects, attributes of objects (such as color, material, shape), counts of objects, and spatial relationships between objects. To curate our benchmark, we built an automatic pipeline using large language models and diffusion models, followed by a rigorous 4-step verification process by human annotators. Empirical experiments reveal that current VLMs exhibit notable deficiencies in understanding spatial relationships and counting abilities. Furthermore, leveraging the automated nature of our data creation process, we generate a large-scale training dataset, which we use to finetune CLIP (a foundational VLM) and Idefics2 (a multimodal large language model). Our findings show that both these models benefit significantly from fine-tuning on this data, as evident by marked improvements in fine-grained understanding across a wide range of benchmarks. Additionally, such fine-tuning improves CLIP's general image-text alignment capabilities too. All resources including the benchmark, the training data, and the finetuned model checkpoints will be released.",main,NeurIPS,2024,Poster,Rabiul Awal;Saba Ahmadi;Le Zhang;Aishwarya Agrawal,True,https://openreview.net/pdf?id=bJddXCyosA
bPOaHf8OcX,Vivid-ZOO: Multi-View Video Generation with Diffusion Model,"While diffusion models have shown impressive performance in 2D image/video generation, diffusion-based Text-to-Multi-view-Video (T2MVid) generation remains underexplored. The new challenges posed by T2MVid generation lie in the lack of massive captioned multi-view videos and the complexity of modeling such multi-dimensional distribution. To this end, we propose a novel diffusion-based pipeline that generates high-quality multi-view videos centered around a dynamic 3D object from text. Specifically, we factor the T2MVid problem into viewpoint-space and time components. Such factorization allows us to combine and reuse layers of advanced pre-trained multi-view image and 2D video diffusion models to ensure multi-view consistency as well as temporal coherence for the generated multi-view videos, largely reducing the training cost. We further introduce alignment modules to align the latent spaces of layers from the pre-trained multi-view and the 2D video diffusion models, addressing the reused layers' incompatibility that arises from the domain gap between 2D and multi-view data. In support of this and future research,  we further contribute a captioned multi-view video dataset. Experimental results demonstrate that our method generates high-quality multi-view videos, exhibiting vivid motions, temporal coherence, and multi-view consistency, given a variety of text prompts.",main,NeurIPS,2024,Poster,Bing Li;Cheng Zheng;Wenxuan Zhu;Jinjie Mai;Biao Zhang;Peter Wonka;Bernard Ghanem,True,https://openreview.net/pdf?id=bPOaHf8OcX
bQMevGCYVM,One Token to Seg Them All: Language Instructed Reasoning Segmentation in Videos,"We introduce VideoLISA, a video-based multimodal large language model designed to tackle the problem of language-instructed reasoning segmentation in videos. Leveraging the reasoning capabilities and world knowledge of large language models, and augmented by the Segment Anything Model, VideoLISA generates temporally consistent segmentation masks in videos based on language instructions. Existing image-based methods, such as LISA, struggle with video tasks due to the additional temporal dimension, which requires temporal dynamic understanding and consistent segmentation across frames. VideoLISA addresses these challenges by integrating a Sparse Dense Sampling strategy into the video-LLM, which balances temporal context and spatial detail within computational constraints. Additionally, we propose a One-Token-Seg-All approach using a specially designed <TRK> token, enabling the model to segment and track objects across multiple frames. Extensive evaluations on diverse benchmarks, including our newly introduced ReasonVOS benchmark, demonstrate VideoLISA's superior performance in video object segmentation tasks involving complex reasoning, temporal understanding, and object tracking. While optimized for videos, VideoLISA also shows promising generalization to image segmentation, revealing its potential as a unified foundation model for language-instructed object segmentation. Code and model will be available at: https://github.com/showlab/VideoLISA.",main,NeurIPS,2024,Poster,Zechen Bai;Tong He;Haiyang Mei;Pichao WANG;Ziteng Gao;Joya Chen;liulei;Zheng Zhang;Mike Zheng Shou,True,https://openreview.net/pdf?id=bQMevGCYVM
bcVLFQCOjc,DeTikZify: Synthesizing Graphics Programs for Scientific Figures and Sketches with TikZ,"Creating high-quality scientific figures can be time-consuming and challenging, even though sketching ideas on paper is relatively easy. Furthermore, recreating existing figures that are not stored in formats preserving semantic information is equally complex. To tackle this problem, we introduce DeTikZify, a novel multimodal language model that automatically synthesizes scientific figures as semantics-preserving TikZ graphics programs based on sketches and existing figures. To achieve this, we create three new datasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k human-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn sketches with their corresponding scientific figures; and MetaFig, a collection of diverse scientific figures and associated metadata. We train DeTikZify on MetaFig and DaTikZv2, along with synthetically generated sketches learned from SketchFig. We also introduce an MCTS-based inference algorithm that enables DeTikZify to iteratively refine its outputs without the need for additional training. Through both automatic and human evaluation, we demonstrate that DeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ programs, with the MCTS algorithm effectively boosting its performance. We make our code, models, and datasets publicly available.",main,NeurIPS,2024,Spotlight,Jonas Belouadi;Simone Paolo Ponzetto;Steffen Eger,True,https://openreview.net/pdf?id=bcVLFQCOjc
bepcG3itGX,LibAMM: Empirical Insights into Approximate Computing for Accelerating Matrix Multiplication,"Matrix multiplication (MM) is pivotal in fields from deep learning to scientific computing, driving the quest for improved computational efficiency. Accelerating MM encompasses strategies like complexity reduction, parallel and distributed computing, hardware acceleration, and approximate computing techniques, namely AMM algorithms. Amidst growing concerns over the resource demands of large language models (LLMs), AMM has garnered renewed focus. However, understanding the nuances that govern AMM’s effectiveness remains incomplete. This study delves into AMM by examining algorithmic strategies, operational specifics, dataset characteristics, and their application in real-world tasks. Through comprehensive testing across diverse datasets and scenarios, we analyze how these factors affect AMM’s performance, uncovering that the selection of AMM approaches significantly influences the balance between efficiency and accuracy, with factors like memory access playing a pivotal role. Additionally, dataset attributes are shown to be vital for the success of AMM in applications. Our results advocate for tailored algorithmic approaches and careful strategy selection to enhance AMM’s effectiveness. To aid in the practical application and ongoing research of AMM, we introduce LibAMM —a toolkit offering a wide range of AMM algorithms, benchmarks, and tools for experiment management. LibAMM aims to facilitate research and application in AMM, guiding future developments towards more adaptive and context-aware computational solutions.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xianzhi Zeng;Wenchao Jiang;Shuhao Zhang,False,https://openreview.net/pdf?id=bepcG3itGX
bkUvKPKafQ,ChatQA: Surpassing GPT-4 on Conversational QA and RAG,"In this work, we introduce ChatQA, a suite of models that outperform GPT-4 on retrieval-augmented generation (RAG) and conversational question answering (QA).  To enhance generation, we propose a two-stage instruction tuning method that significantly boosts the performance of RAG.  For effective retrieval, we introduce a dense retriever optimized for conversational QA, which yields results comparable to the alternative state-of-the-art query rewriting models, while substantially reducing deployment costs.  We also present the ChatRAG Bench, which encompasses ten datasets covering comprehensive evaluations on RAG, table-related QA, arithmetic calculations, and scenarios involving unanswerable questions.  Our ChatQA-1.0-70B (score: 54.14), built on Llama2, a weaker foundation model than GPT-4, can slightly outperform GPT-4-0613 (score: 53.90) and GPT-4-Turbo-2024-04-09 (score: 54.03) on the ChatRAG Bench, without relying on any synthetic data from OpenAI GPT models.   Notably, Llama3-ChatQA-1.5-70B model surpasses the accuracy of GPT-4-Turbo-2024-04-09 by a margin. These results demonstrate the exceptional quality of the proposed ChatQA recipe. To advance research in this field, we open-sourced the model weights, instruction tuning data, ChatRAG Bench, and retriever for the community.",main,NeurIPS,2024,Poster,Zihan Liu;Wei Ping;Rajarshi Roy;Peng Xu;Chankyu Lee;Mohammad Shoeybi;Bryan Catanzaro,True,https://openreview.net/pdf?id=bkUvKPKafQ
brxBxj4Dv3,NoisyGL: A Comprehensive Benchmark for Graph Neural Networks under Label Noise,"Graph Neural Networks (GNNs) exhibit strong potential in node classification task through a message-passing mechanism. However, their performance often hinges on high-quality node labels, which are challenging to obtain in real-world scenarios due to unreliable sources or adversarial attacks. Consequently, label noise is common in real-world graph data, negatively impacting GNNs by propagating incorrect information during training. To address this issue, the study of Graph Neural Networks under Label Noise (GLN) has recently gained traction. However, due to variations in dataset selection, data splitting, and preprocessing techniques, the community currently lacks a comprehensive benchmark, which impedes deeper understanding and further development of GLN. To fill this gap, we introduce NoisyGL in this paper, the first comprehensive benchmark for graph neural networks under label noise. NoisyGL enables fair comparisons and detailed analyses of GLN methods on noisy labeled graph data across various datasets, with unified experimental settings and interface. Our benchmark has uncovered several important insights that were missed in previous research, and we believe these findings will be highly beneficial for future studies. We hope our open-source benchmark library will foster further advancements in this field. The code of the benchmark can be found in https://github.com/eaglelab-zju/NoisyGL.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zhonghao Wang;Danyu Sun;Sheng Zhou;Haobo Wang;Jiapei Fan;Longtao Huang;Jiajun Bu,False,https://openreview.net/pdf?id=brxBxj4Dv3
bxwWikAXSy,MathWriting: A Dataset For Handwritten Mathematical Expression Recognition,"Recognition of handwritten mathematical expressions allows to transfer scientific notes into their digital form. It facilitates the sharing, searching, and preservation of scientific information. We introduce MathWriting, the largest online handwritten mathematical expression dataset to date. It consists of 230k human-written samples and an additional 400k synthetic ones. This dataset can also be used in its rendered form for offline HME recognition. One MathWriting sample consists of a formula written on a touch screen and a corresponding LaTeX expression. We also provide a normalized version of LaTeX expression to simplify the recognition task and enhance the result quality. We provide baseline performance of standard models like OCR and CTC Transformer as well as Vision-Language Models like PaLI on the dataset. The dataset together with an example colab is accessible on Github.",Datasets & Benchmarks,NeurIPS,2024,Reject,Philippe Gervais;Anastasiia Fadeeva;Andrii Maksai,True,https://openreview.net/pdf?id=bxwWikAXSy
c3Pakdyi3t,$\\\\textit{Trans-LoRA}$: towards data-free Transferable Parameter Efficient Finetuning,"Low-rank adapters (LoRA) and their variants are popular parameter-efficient fine-tuning (PEFT) techniques that closely match full model fine-tune performance while requiring only a small number of additional parameters. These additional LoRA parameters are specific to the base model being adapted. When the base model needs to be deprecated and replaced with a new one, all the associated LoRA modules need to be re-trained. Such re-training requires access to the data used to train the LoRA for the original base model. This is especially problematic for commercial cloud applications where the LoRA modules and the base models are hosted by service providers who may not be allowed to host proprietary client task data. To address this challenge, we propose $\\\\textit{Trans-LoRA}$ --- a novel method for lossless, nearly data-free transfer of LoRAs across base models. Our approach relies on synthetic data to transfer LoRA modules. Using large language models, we design a synthetic data generator to approximate the data-generating process of the $\\\\textit{observed}$ task data subset. Training on the resulting synthetic dataset transfers LoRA modules to new models. We show the effectiveness of our approach using both LLama and Gemma model families. Our approach achieves lossless (mostly improved) LoRA transfer between models within and across different base model families, and even between different PEFT methods, on a wide variety of tasks.",main,NeurIPS,2024,Poster,Runqian Wang;Soumya Ghosh;David Daniel Cox;Diego Antognini;Aude Oliva;Rogerio Feris;Leonid Karlinsky,True,https://openreview.net/pdf?id=c3Pakdyi3t
c4JE1gemWc,SG-Bench: Evaluating LLM Safety Generalization Across Diverse Tasks and Prompt Types,"Ensuring the safety of large language model (LLM) applications is essential for developing trustworthy artificial intelligence. Current LLM safety benchmarks have two limitations. First, they focus solely on either discriminative or generative evaluation paradigms while ignoring their interconnection. Second, they rely on standardized inputs, overlooking the effects of widespread prompting techniques, such as system prompts, few-shot demonstrations, and chain-of-thought prompting. To overcome these issues, we developed SG-Bench, a novel benchmark to assess the generalization of LLM safety across various tasks and prompt types. This benchmark integrates both generative and discriminative evaluation tasks and includes extended data to examine the impact of prompt engineering and jailbreak on LLM safety. Our assessment of 3 advanced proprietary LLMs and 10 open-source LLMs with the benchmark reveals that most LLMs perform worse on discriminative tasks than generative ones, and are highly susceptible to prompts, indicating poor generalization in safety alignment. We also explain these findings quantitatively and qualitatively to provide insights for future research.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yutao Mou;Shikun Zhang;Wei Ye,True,https://openreview.net/pdf?id=c4JE1gemWc
c4NnhBi4oM,DrivAerNet++: A Large-Scale Multimodal Car Dataset with Computational Fluid Dynamics Simulations and Deep Learning Benchmarks,"We present DrivAerNet++, the largest and most comprehensive multimodal dataset for aerodynamic car design. DrivAerNet++ comprises 8,000 diverse car designs modeled with high-fidelity computational fluid dynamics (CFD) simulations. The dataset includes diverse car configurations such as fastback, notchback, and estateback, with different underbody and wheel designs to represent both internal combustion engines and electric vehicles. Each entry in the dataset features detailed 3D meshes, parametric models, aerodynamic coefficients, and extensive flow and surface field data, along with segmented parts for car classification and point cloud data. This dataset supports a wide array of machine learning applications including data-driven design optimization, generative modeling, surrogate model training, CFD simulation acceleration, and geometric classification. With more than 39 TB of publicly available engineering data, DrivAerNet++ fills a significant gap in available resources, providing high-quality, diverse data to enhance model training, promote generalization, and accelerate automotive design processes. Along with rigorous dataset validation, we also provide ML benchmarking results on the task of aerodynamic drag prediction, showcasing the breadth of applications supported by our dataset. This dataset is set to significantly impact automotive design and broader engineering disciplines by fostering innovation and improving the fidelity of aerodynamic evaluations. Dataset and code available at: https://github.com/Mohamedelrefaie/DrivAerNet",Datasets & Benchmarks,NeurIPS,2024,Poster,Mohamed Elrefaie;Florin Morar;Angela Dai;Faez Ahmed,True,https://openreview.net/pdf?id=c4NnhBi4oM
c7SApXZz4b,Stronger Than You Think: Benchmarking Weak Supervision on Realistic Tasks,"Weak supervision (WS) is a popular approach for label-efficient learning, leveraging diverse sources of noisy but inexpensive *weak labels* to automatically annotate training data. Despite its wide usage, WS and its practical value are challenging to benchmark due to the many knobs in its setup, including: data sources, labeling functions (LFs), aggregation techniques (called label models), and end model pipelines. Existing evaluation suites tend to be limited, focusing on particular components or specialized use cases. Moreover, they often involve simplistic benchmark tasks or de-facto LF sets that are suboptimally written, producing insights that may not generalize to real-world settings. We address these limitations by introducing a new benchmark, BOXWRENCH, designed to more accurately reflect *real-world usages of WS*. This benchmark features tasks with (1) higher class cardinality and imbalance, (2) notable domain expertise requirements, and (3) opportunities to re-use LFs across parallel multilingual corpora. For all tasks, LFs are written using a careful procedure aimed at mimicking real-world settings. In contrast to existing WS benchmarks, we show that supervised learning requires substantial amounts (1000+) of labeled examples to match WS in many settings.",Datasets & Benchmarks,NeurIPS,2024,Poster,Tianyi Zhang;Linrong Cai;Jeffrey Li;Nicholas Roberts;Neel Guha;Frederic Sala,True,https://openreview.net/pdf?id=c7SApXZz4b
cA9gLXFaRo,Instruction-Guided Visual Masking,"Instruction following is crucial in contemporary LLM. However, when extended to multimodal setting, it often suffers from misalignment between specific textual instruction and targeted local region of an image. To achieve more accurate and nuanced multimodal instruction following, we introduce Instruction-guided Visual Masking (IVM), a new versatile visual grounding model that is compatible with diverse multimodal models, such as LMM and robot model. By constructing visual masks for instruction-irrelevant regions, IVM-enhanced multimodal models can effectively focus on task-relevant image regions to better align with complex instructions. Specifically, we design a visual masking data generation pipeline and create an IVM-Mix-1M dataset with 1 million image-instruction pairs. We further introduce a new learning technique, Discriminator Weighted Supervised Learning (DWSL) for preferential IVM training that prioritizes high-quality data samples. Experimental results on generic multimodal tasks such as VQA and embodied robotic control demonstrate the versatility of IVM, which as a plug-and-play tool, significantly boosts the performance of diverse multimodal models, yielding new state-of-the-art results across challenging multimodal benchmarks. Code, model and data are available at https://github.com/2toinf/IVM.",main,NeurIPS,2024,Poster,Jinliang Zheng;Jianxiong Li;Sijie Cheng;Yinan Zheng;Jiaming Li;Jihao Liu;Yu Liu;Jingjing Liu;Xianyuan Zhan,True,https://openreview.net/pdf?id=cA9gLXFaRo
cCL92OPlDz,ParallelEdits: Efficient Multi-Aspect Text-Driven Image Editing with Attention Grouping,"Text-driven image synthesis has made significant advancements with the development of diffusion models, transforming how visual content is generated from text prompts. Despite these advances, text-driven image editing, a key area in computer graphics, faces unique challenges. A major challenge is making simultaneous edits across multiple objects or attributes. Applying these methods sequentially for multi-attribute edits increases computational demands and efficiency losses. 
    In this paper, we address these challenges with significant contributions. Our main contribution is the development of ParallelEdits, a method that seamlessly manages simultaneous edits across multiple attributes. In contrast to previous approaches, ParallelEdits not only preserves the quality of single attribute edits but also significantly improves the performance of multitasking edits. This is achieved through innovative attention distribution mechanism and multi-branch design that operates across several processing heads. 
    Additionally, we introduce the PIE-Bench++ dataset, an expansion of the original PIE-Bench dataset, to better support evaluating image-editing tasks involving multiple objects and attributes simultaneously. This dataset is a benchmark for evaluating text-driven image editing methods in multifaceted scenarios.",main,NeurIPS,2024,Poster,Mingzhen Huang;Jialing Cai;Shan Jia;Vishnu Suresh Lokhande;Siwei Lyu,True,https://openreview.net/pdf?id=cCL92OPlDz
cDYqckEt6d,DiscoveryWorld: A Virtual Environment for Developing and Evaluating Automated Scientific Discovery Agents,"Automated scientific discovery promises to accelerate progress across scientific domains, but evaluating an agent's capacity for end-to-end scientific reasoning is challenging as running real-world experiments is often prohibitively expensive or infeasible. In this work we introduce DiscoveryWorld, a virtual environment that enables benchmarking an agent's ability to perform complete cycles of novel scientific discovery in an inexpensive, simulated, multi-modal, long-horizon, and fictional setting.
DiscoveryWorld consists of 24 scientific tasks across three levels of difficulty, each with parametric variations that provide new discoveries for agents to make across runs. Tasks require an agent to form hypotheses, design and run experiments, analyze results, and act on conclusions. Task difficulties are normed to range from straightforward to challenging for human scientists with advanced degrees. DiscoveryWorld further provides three automatic metrics for evaluating performance, including: (1) binary task completion, (2) fine-grained report cards detailing procedural scoring of task-relevant actions, and (3) the accuracy of discovered explanatory knowledge.
While simulated environments such as DiscoveryWorld are low-fidelity compared to the real world, we find that strong baseline agents struggle on most DiscoveryWorld tasks, highlighting the utility of using simulated environments as proxy tasks for near-term development of scientific discovery competency in agents.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Peter Jansen;Marc-Alexandre Côté;Tushar Khot;Erin Bransom;Bhavana Dalvi Mishra;Bodhisattwa Prasad Majumder;Oyvind Tafjord;Peter Clark,False,https://openreview.net/pdf?id=cDYqckEt6d
cFTi3gLJ1X,Depth Anything V2,"This work presents Depth Anything V2. Without pursuing fancy techniques, we aim to reveal crucial findings to pave the way towards building a powerful monocular depth estimation model. Notably, compared with V1, this version produces much finer and more robust depth predictions through three key practices: 1) replacing all labeled real images with synthetic images, 2) scaling up the capacity of our teacher model, and 3) teaching student models via the bridge of large-scale pseudo-labeled real images. Compared with the latest models built on Stable Diffusion, our models are significantly more efficient (more than 10x faster) and more accurate. We offer models of different scales (ranging from 25M to 1.3B params) to support extensive scenarios. Benefiting from their strong generalization capability, we fine-tune them with metric depth labels to obtain our metric depth models. In addition to our models, considering the limited diversity and frequent noise in current test sets, we construct a versatile evaluation benchmark with sparse depth annotations to facilitate future research. Models are available at https://github.com/DepthAnything/Depth-Anything-V2.",main,NeurIPS,2024,Poster,Lihe Yang;Bingyi Kang;Zilong Huang;Zhen Zhao;Xiaogang Xu;Jiashi Feng;Hengshuang Zhao,True,https://openreview.net/pdf?id=cFTi3gLJ1X
cFyagd2Yh4,MedSafetyBench: Evaluating and Improving the Medical Safety of Large Language Models,"As large language models (LLMs) develop increasingly sophisticated capabilities and find applications in medical settings, it becomes important to assess their medical safety due to their far-reaching implications for personal and public health, patient safety, and human rights. However, there is little to no understanding of the notion of medical safety in the context of LLMs, let alone how to evaluate and improve it. To address this gap, we first define the notion of medical safety in LLMs based on the Principles of Medical Ethics set forth by the American Medical Association. We then leverage this understanding to introduce MedSafetyBench, the first benchmark dataset designed to measure the medical safety of LLMs. We demonstrate the utility of MedSafetyBench by using it to evaluate and improve the medical safety of LLMs. Our results show that publicly-available medical LLMs do not meet standards of medical safety and that fine-tuning them using MedSafetyBench improves their medical safety while preserving their medical performance. By introducing this new benchmark dataset, our work enables a systematic study of the state of medical safety in LLMs and motivates future work in this area, paving the way to mitigate the safety risks of LLMs in medicine. The benchmark dataset and code are available at https://github.com/AI4LIFE-GROUP/med-safety-bench.",Datasets & Benchmarks,NeurIPS,2024,Poster,Tessa Han;Aounon Kumar;Chirag Agarwal;Himabindu Lakkaraju,True,https://openreview.net/pdf?id=cFyagd2Yh4
cLS4fLIA5P,WebUOT-1M: Advancing Deep Underwater Object Tracking with A Million-Scale Benchmark,"Underwater Object Tracking (UOT) is essential for identifying and tracking submerged objects in underwater videos, but existing datasets are limited in scale, diversity of target categories and scenarios covered, impeding the development of advanced tracking algorithms. To bridge this gap, we take the first step and introduce WebUOT-1M, \\\\ie, the largest public UOT benchmark to date, sourced from complex and realistic underwater environments. It comprises 1.1 million frames across 1,500 video clips filtered from 408 target categories, largely surpassing previous UOT datasets, \\\\eg, UVOT400. Through meticulous manual annotation and verification, we provide high-quality bounding boxes for underwater targets. Additionally, WebUOT-1M includes language prompts for video sequences, expanding its application areas, \\\\eg, underwater vision-language tracking. Given that most existing trackers are designed for open-air conditions and perform poorly in underwater environments due to domain gaps, we propose a novel framework that uses omni-knowledge distillation to train a student Transformer model effectively. To the best of our knowledge, this framework is the first to effectively transfer open-air domain knowledge to the UOT model through knowledge distillation, as demonstrated by results on both existing UOT datasets and the newly proposed WebUOT-1M. We have thoroughly tested WebUOT-1M with 30 deep trackers, showcasing its potential as a benchmark for future UOT research. The complete dataset, along with codes and tracking results, are publicly accessible at \\\\href{https://github.com/983632847/Awesome-Multimodal-Object-Tracking}{\\\\color{magenta}{here}}.",Datasets & Benchmarks,NeurIPS,2024,Poster,Chunhui Zhang;Li Liu;Guanjie Huang;Hao Wen;XI ZHOU;Yanfeng Wang,True,https://openreview.net/pdf?id=cLS4fLIA5P
cLga8GStdk,LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low Resource and Extinct Languages,"In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions. The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 38.7% accuracy, a 24.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models.",Datasets & Benchmarks,NeurIPS,2024,Oral,Andrew Michael Bean;Simeon Hellsten;Harry Mayne;Jabez Magomere;Ethan A Chi;Ryan Andrew Chi;Scott A. Hale;Hannah Rose Kirk,True,https://openreview.net/pdf?id=cLga8GStdk
cR3T1ZYN8I,A Hitchhiker's Guide to Fine-Grained Face Forgery Detection Using Common Sense Reasoning,"Explainability in artificial intelligence is crucial for restoring trust, particularly in areas like face forgery detection, where viewers often struggle to distinguish between real and fabricated content. Vision and Large Language Models (VLLM) bridge computer vision and natural language, offering numerous applications driven by strong common-sense reasoning. Despite their success in various tasks, the potential of vision and language remains underexplored in face forgery detection, where they hold promise for enhancing explainability by leveraging the intrinsic reasoning capabilities of language to analyse fine-grained manipulation areas.
    For that reason, few works have recently started to frame the problem of deepfake detection as a Visual Question Answering (VQA) task, nevertheless omitting the realistic and informative open-ended multi-label setting. With the rapid advances in the field of VLLM, an exponential rise of investigations in that direction is expected.
    As such, there is a need for a clear experimental methodology that converts face forgery detection to a Visual Question Answering (VQA) task to systematically and fairly evaluate different VLLM architectures. Previous evaluation studies in deepfake detection have mostly focused on the simpler binary task, overlooking evaluation protocols for multi-label fine-grained detection and text-generative models. We propose a multi-staged approach that diverges from the traditional binary evaluation protocol and conducts a comprehensive evaluation study to compare the capabilities of several VLLMs in this context.
    In the first stage, we assess the models' performance on the binary task and their sensitivity to given instructions using several prompts. In the second stage, we delve deeper into fine-grained detection by identifying areas of manipulation in a multiple-choice VQA setting. In the third stage, we convert the fine-grained detection to an open-ended question and compare several matching strategies for the multi-label classification task. Finally, we qualitatively evaluate the fine-grained responses of the VLLMs included in the benchmark.
    We apply our benchmark to several popular models, providing a detailed comparison of binary, multiple-choice, and open-ended VQA evaluation across seven datasets. \\\\url{https://nickyfot.github.io/hitchhickersguide.github.io/}",Datasets & Benchmarks,NeurIPS,2024,Poster,Niki Foteinopoulou;Enjie Ghorbel;Djamila Aouada,False,https://openreview.net/pdf?id=cR3T1ZYN8I
cX57Pbw8vS,Benchmarking PtO and PnO Methods in the Predictive Combinatorial Optimization Regime,"Predictive combinatorial optimization, where the parameters of combinatorial optimization (CO) are unknown at the decision-making time, is the precise modeling of many real-world applications, including energy cost-aware scheduling and budget allocation on advertising. Tackling such a problem usually involves a prediction model and a CO solver. These two modules are integrated into the predictive CO pipeline following two design principles: ''Predict-then-Optimize (PtO)'', which learns predictions by supervised training and subsequently solves CO using predicted coefficients, while the other, named ''Predict-and-Optimize (PnO)'', directly optimizes towards the ultimate decision quality and claims to yield better decisions than traditional PtO approaches. However, there lacks a systematic benchmark of both approaches, including the specific design choices at the module level, as well as an evaluation dataset that covers representative real-world scenarios. To this end, we develop a modular framework to benchmark 11 existing PtO/PnO methods on 8 problems, including a new industrial dataset for combinatorial advertising that will be released. Our study shows that PnO approaches are better than PtO on 7 out of 8 benchmarks, but there is no silver bullet found for the specific design choices of PnO. A comprehensive categorization of current approaches and integration of typical scenarios are provided under a unified benchmark. Therefore, this paper could serve as a comprehensive benchmark for future PnO approach development and also offer fast prototyping for application-focused development. The code is available at \\\\url{https://github.com/Thinklab-SJTU/PredictiveCO-Benchmark}.",Datasets & Benchmarks,NeurIPS,2024,Poster,Haoyu Geng;Hang Ruan;Runzhong Wang;Yang Li;YANG WANG;Lei CHEN;Junchi Yan,True,https://openreview.net/pdf?id=cX57Pbw8vS
catfRXDWcb,HumanVid: Demystifying Training Data for Camera-controllable Human Image Animation,"Human image animation involves generating videos from a character photo, allowing user control and unlocking the potential for video and movie production. While recent approaches yield impressive results using high-quality training data, the inaccessibility of these datasets hampers fair and transparent benchmarking. Moreover, these approaches prioritize 2D human motion and overlook the significance of camera motions in videos, leading to limited control and unstable video generation. To demystify the training data, we present HumanVid, the first large-scale high-quality dataset tailored for human image animation, which combines crafted real-world and synthetic data. For the real-world data, we compile a vast collection of real-world videos from the internet. We developed and applied careful filtering rules to ensure video quality, resulting in a curated collection of 20K high-resolution (1080P) human-centric videos. Human and camera motion annotation is accomplished using a 2D pose estimator and a SLAM-based method. To expand our synthetic dataset, we collected 10K 3D avatar assets and leveraged existing assets of body shapes, skin textures and clothings. Notably, we introduce a rule-based camera trajectory generation method, enabling the synthetic pipeline to incorporate diverse and precise camera motion annotation, which can rarely be found in real-world data. To verify the effectiveness of HumanVid, we establish a baseline model named **CamAnimate**, short for Camera-controllable Human Animation, that considers both human and camera motions as conditions. Through extensive experimentation, we demonstrate that such simple baseline training on our HumanVid achieves state-of-the-art performance in controlling both human pose and camera motions, setting a new benchmark. Demo, data and code could be found in the project website: https://humanvid.github.io/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zhenzhi Wang;Yixuan Li;Yanhong Zeng;Youqing Fang;Yuwei Guo;Wenran Liu;Jing Tan;Kai Chen;Tianfan Xue;Bo Dai;Dahua Lin,True,https://openreview.net/pdf?id=catfRXDWcb
cmSNX47aEH,DeiSAM: Segment Anything with Deictic Prompting,"Large-scale, pre-trained neural networks have demonstrated strong capabilities in various tasks, including zero-shot image segmentation. To identify concrete objects in complex scenes, humans instinctively rely on deictic descriptions in natural language, i.e., referring to something depending on the context such as ""The object that is on the desk and behind the cup."". However, deep learning approaches cannot reliably interpret such deictic representations due to their lack of reasoning capabilities in complex scenarios. To remedy this issue, we propose DeiSAM — a combination of large pre-trained neural networks with differentiable logic reasoners — for deictic promptable segmentation. Given a complex, textual segmentation description, DeiSAM leverages Large Language Models (LLMs) to generate first-order logic rules and performs differentiable forward reasoning on generated scene graphs. Subsequently, DeiSAM segments objects by matching them to the logically inferred image regions. As part of our evaluation, we propose the Deictic Visual Genome (DeiVG) dataset, containing paired visual input and complex, deictic textual prompts. Our empirical results demonstrate that DeiSAM is a substantial improvement over purely data-driven baselines for deictic promptable segmentation.",main,NeurIPS,2024,Poster,Hikaru Shindo;Manuel Brack;Gopika Sudhakaran;Devendra Singh Dhami;Patrick Schramowski;Kristian Kersting,True,https://openreview.net/pdf?id=cmSNX47aEH
cnjmZqVpm9,CableInspect-AD: An Expert-Annotated Anomaly Detection Dataset,"Machine learning models are increasingly being deployed in real-world contexts. However, systematic studies on their transferability to specific and critical applications are underrepresented in the research literature. An important example is visual anomaly detection (VAD) for robotic power line inspection. While existing VAD methods perform well in controlled environments, real-world scenarios present diverse and unexpected anomalies that current datasets fail to capture. To address this gap, we introduce CableInspect-AD, a high-quality, publicly available dataset created and annotated by domain experts from Hydro-Québec, a Canadian public utility. This dataset includes high-resolution images with challenging real-world anomalies, covering defects with varying severity levels. To address the challenges of collecting diverse anomalous and nominal examples for setting a detection threshold, we propose an enhancement to the celebrated PatchCore algorithm. This enhancement enables its use in scenarios with limited labeled data. We also present a comprehensive evaluation protocol based on cross-validation to assess models' performances. We evaluate our Enhanced-PatchCore for few-shot and many-shot detection, and Vision-Language Models for zero-shot detection. While promising, these models struggle to detect all anomalies, highlighting the dataset's value as a challenging benchmark for the broader research community. Project page: https://mila-iqia.github.io/cableinspect-ad/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Akshatha Arodi;Margaux Luck;Jean-Luc Bedwani;Aldo Zaimi;Ge Li;Nicolas Pouliot;Julien Beaudry;Gaétan Marceau Caron,True,https://openreview.net/pdf?id=cnjmZqVpm9
cu8FfaYriU,A Taxonomy of Challenges to Curating Fair Datasets,"Despite extensive efforts to create fairer machine learning (ML) datasets, there remains a limited understanding of the practical aspects of dataset curation. Drawing from interviews with 30 ML dataset curators, we present a comprehensive taxonomy of the challenges and trade-offs encountered throughout the dataset curation lifecycle. Our findings underscore overarching issues within the broader fairness landscape that impact data curation. We conclude with recommendations aimed at fostering systemic changes to better facilitate fair dataset curation practices.",Datasets & Benchmarks,NeurIPS,2024,Oral,Dora Zhao;Morgan Scheuerman;Pooja Chitre;Jerone Andrews;Georgia Panagiotidou;Shawn Walker;Kathleen H. Pine;Alice Xiang,False,https://openreview.net/pdf?id=cu8FfaYriU
cvaSru8LeO,Is A Picture Worth A Thousand Words? Delving Into Spatial Reasoning for Vision Language Models,"Large language models (LLMs) and vision-language models (VLMs) have demonstrated remarkable performance across a wide range of tasks and domains. Despite this promise, spatial understanding and reasoning—a fundamental component of human cognition—remains under-explored. We propose SpatialEval, a novel benchmark that covers diverse aspects of spatial reasoning such as relationship understanding, navigation, and counting. We conduct a comprehensive evaluation of competitive language and vision-language models. Our findings reveal several counter-intuitive insights that have been overlooked in the literature: (1) Spatial reasoning poses significant challenges where competitive models can fall behind random guessing; (2) Despite additional visual input, VLMs often under-perform compared to their LLM counterparts; (3) When both textual and visual information is available, multi-modal language models become less reliant on visual information if sufficient textual clues are provided. Additionally, we demonstrate that leveraging redundancy between vision and text can significantly enhance model performance. We hope our study will inform the development of multimodal models to improve spatial intelligence and further close the gap with human intelligence. Our code is available at https://github.com/jiayuww/SpatialEval.",main,NeurIPS,2024,Poster,Jiayu Wang;Yifei Ming;Zhenmei Shi;Vibhav Vineet;Xin Wang;Yixuan Li;Neel Joshi,True,https://openreview.net/pdf?id=cvaSru8LeO
cy8mq7QYae,CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs,"Chart understanding plays a pivotal role when applying Multimodal Large Language Models (MLLMs) to real-world tasks such as analyzing scientific papers or financial reports. However, existing datasets often focus on oversimplified and homogeneous charts with template-based questions, leading to an overly optimistic measure of progress. We demonstrate that although open-source models can appear to outperform strong proprietary models on these benchmarks, a simple stress test with slightly different charts or questions deteriorates performance by up to 34.5%. In this work, we propose CharXiv, a comprehensive evaluation suite involving 2,323 natural, challenging, and diverse charts from scientific papers. CharXiv includes two types of questions: 1) descriptive questions about examining basic chart elements and 2) reasoning questions that require synthesizing information across complex visual elements in the chart. To ensure quality, all charts and questions are handpicked, curated, and verified by human experts. Our results reveal a substantial, previously underestimated gap between the reasoning skills of the strongest proprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the strongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%. All models lag far behind human performance of 80.5%, underscoring weaknesses in the chart understanding capabilities of existing MLLMs. We hope that CharXiv facilitates future research on MLLM chart understanding by providing a more realistic and faithful measure of progress. Project website: https://charxiv.github.io/",Datasets & Benchmarks,NeurIPS,2024,Poster,Zirui Wang;Mengzhou Xia;Luxi He;Howard Chen;Yitao Liu;Richard Zhu;Kaiqu Liang;Xindi Wu;Haotian Liu;Sadhika Malladi;Alexis Chevalier;Sanjeev Arora;Danqi Chen,True,https://openreview.net/pdf?id=cy8mq7QYae
d0gMFgrYFB,FVEL: Interactive Formal Verification Environment with Large Language Models via Theorem Proving,"Formal verification (FV) has witnessed growing significance with current emerging program synthesis by the evolving large language models (LLMs). However, current formal verification mainly resorts to symbolic verifiers or hand-craft rules, resulting in limitations for extensive and flexible verification. On the other hand, formal languages for automated theorem proving, such as Isabelle, as another line of rigorous verification, are maintained with comprehensive rules and theorems. In this paper, we propose FVEL, an interactive Formal Verification Environment with LLMs. Specifically, FVEL transforms a given code to be verified into Isabelle, and then conducts verification via neural automated theorem proving with an LLM. The joined paradigm leverages the rigorous yet abundant formulated and organized rules in Isabelle and is also convenient for introducing and adjusting cutting-edge LLMs. To achieve this goal, we extract a large-scale FVELER. The FVELER dataset includes code dependencies and verification processes that are formulated in Isabelle, containing 758 theories, 29,304 lemmas, and 201,498 proof steps in total with in-depth dependencies. We benchmark FVELER in the FVEL environment by first fine-tuning LLMs with FVELER and then evaluating them on Code2Inv and SV-COMP. The results show that FVEL with FVELER fine-tuned Llama3-8B solves 17.39% (69→81) more problems, and Mistral-7B 12% (75→84) more problems in SV-COMP. And the proportion of proof errors is reduced. Project page: https://fveler.github.io/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xiaohan Lin;Qingxing Cao;Yinya Huang;Haiming Wang;Jianqiao Lu;Zhengying Liu;Linqi Song;Xiaodan Liang,True,https://openreview.net/pdf?id=d0gMFgrYFB
d1Pup4gkWf,SMPLOlympics: Sports Environments for Physically Simulated Humanoids,"We present SMPLOlympics, a collection of physically simulated environments that allow humanoids to compete in a variety of Olympic sports. Sports simulation offers a rich and standardized testing ground for evaluating and improving the capabilities of learning algorithms due to the diversity and physically demanding nature of athletic activities. As humans have been competing in these sports for many years, there is also a plethora of existing knowledge on the preferred strategy to achieve better performance. To leverage these existing human demonstrations from videos and motion capture, we design our humanoid to be compatible with the widely-used SMPL and SMPL-X human models from the vision and graphics community. We provide a suite of individual sports environments, including golf, javelin throw, high jump, long jump, and hurdling, as well as competitive sports, including both 1v1 and 2v2 games such as table tennis, tennis, fencing, boxing, soccer, and basketball. Our analysis shows that combining strong motion priors with simple rewards can result in human-like behavior in various sports. By providing a unified sports benchmark and baseline implementation of state and reward designs, we hope that SMPLOlympics can help the control and animation communities achieve human-like and performant behaviors.",Datasets & Benchmarks,NeurIPS,2024,Reject,Zhengyi Luo;Jiashun Wang;Kangni Liu;Haotian Zhang;Chen Tessler;Jingbo Wang;Ye Yuan;Jinkun Cao;Zihui Lin;Fengyi Wang;Jessica K. Hodgins;Kris M. Kitani,True,https://openreview.net/pdf?id=d1Pup4gkWf
d1XrZ4EINV,LeDex: Training LLMs to Better Self-Debug and Explain Code,"In the domain of code generation, self-debugging is crucial. It allows LLMs to refine their generated code based on execution feedback. This is particularly important because generating correct solutions in one attempt proves challenging for complex tasks. Prior works on self-debugging mostly focus on prompting methods by providing LLMs with few-shot examples, which work poorly on small open-sourced LLMs. In this work, we propose LeDex, a training framework that significantly improves the self-debugging capability of LLMs. Intuitively, we observe that a chain of explanations on the wrong code followed by code refinement helps LLMs better analyze the wrong code and do refinement. We thus propose an automated pipeline to collect a high-quality dataset for code explanation and refinement by generating a number of explanations and refinement trajectories from the LLM itself or a larger teacher model and filtering via execution verification. We perform supervised fine-tuning (SFT) and further reinforcement learning (RL) on both success and failure trajectories with a novel reward design considering code explanation and refinement quality. SFT improves the pass@1 by up to 15.92\\\\% and pass@10 by 9.30\\\\% over four benchmarks. RL training brings additional up to 3.54\\\\% improvement on pass@1 and 2.55\\\\% improvement on pass@10. The trained LLMs show iterative refinement ability and can keep refining code continuously. Lastly, our human evaluation shows that the LLMs trained with our framework generate more useful code explanations and help developers better understand bugs in source code.",main,NeurIPS,2024,Poster,Nan Jiang;Xiaopeng Li;Shiqi Wang;Qiang Zhou;Soneya Binta Hossain;Baishakhi Ray;Varun Kumar;Xiaofei Ma;Anoop Deoras,True,https://openreview.net/pdf?id=d1XrZ4EINV
dA0AdHrdAQ,ELCC: the Emergent Language Corpus Collection,"We introduce the Emergent Language Corpus Collection (ELCC): a collection of corpora collected from open source implementations of emergent communication systems across the literature. These systems include a variety of signalling game environments as well as more complex tasks like a social deduction game and embodied navigation. Each corpus is annotated with metadata describing the characteristics of the source system as well as a suite of analyses of the corpus (e.g., size, entropy, average message length). Currently, research studying emergent languages requires directly running different systems which takes time away from actual analyses of such languages, limits the variety of languages that are studied, and presents a barrier to entry for researchers without a background in deep learning. The availability of a substantial collection of well-documented emergent language corpora, then, will enable new directions of research which focus their purview on the properties of emergent languages themselves rather than on experimental apparatus.",Datasets & Benchmarks,NeurIPS,2024,Reject,Brendon Boldt;David R Mortensen,True,https://openreview.net/pdf?id=dA0AdHrdAQ
dBSoa8fpV7,PEACE: A Dataset of Pharmaceutical Care for Cancer Pain Analgesia Evaluation and Medication Decision,"Over half of cancer patients experience long-term pain management challenges. Recently, interest has grown in systems for cancer pain treatment effectiveness assessment (TEA) and medication recommendation (MR) to optimize pharmacological care. These systems aim to improve treatment effectiveness by recommending personalized medication plans based on comprehensive patient information. Despite progress, current systems lack multidisciplinary treatment (MDT) team assessments of treatment and the patient's perception of medication, crucial for effective cancer pain management. Moreover, managing cancer pain medication requires multiple adjustments to the treatment plan based on the patient's evolving condition, a detail often missing in existing datasets. To tackle these issues, we designed the PEACE dataset specifically for cancer pain medication research. It includes detailed pharmacological care records for over 38,000 patients, covering demographics, clinical examination, treatment outcomes, medication plans, and patient self-perceptions. Unlike existing datasets, PEACE records not only long-term and multiple follow-ups both inside and outside hospitals but also includes patients' self-assessments of medication effects and the impact on their lives. We conducted a proof-of-concept study with 13 machine learning algorithms on the PEACE dataset for the TEA (classification task) and MR (regression task). These experiments provide valuable insights into the potential of the PEACE dataset for advancing personalized cancer pain management. The dataset is accessible at: [https://github.com/YTYTYD/PEACE].",Datasets & Benchmarks,NeurIPS,2024,Poster,Yutao Dou;Huimin Yu;Wei Li;Jingyang Li;Fei Xia;Jian Xiao,True,https://openreview.net/pdf?id=dBSoa8fpV7
dF22s2GoX0,EpiCare: A Reinforcement Learning Benchmark for Dynamic Treatment Regimes,"Healthcare applications pose significant challenges to existing reinforcement learning (RL) methods due to implementation risks, low data availability, short treatment episodes, sparse rewards, partial observations, and heterogeneous treatment effects. Despite significant interest in using RL to generate dynamic treatment regimes for longitudinal patient care scenarios, no standardized benchmark has yet been developed.
To fill this need we introduce *Episodes of Care* (*EpiCare*), a benchmark designed to mimic the challenges associated with applying RL to longitudinal healthcare settings. We leverage this benchmark to test five state-of-the-art offline RL models as well as five common off-policy evaluation (OPE) techniques.
Our results suggest that while offline RL may be capable of improving upon existing standards of care given large data availability, its applicability does not appear to extend to the moderate to low data regimes typical of healthcare settings. Additionally, we demonstrate that several OPE techniques which have become standard in the the medical RL literature fail to perform adequately on our benchmark. These results suggest that the performance of RL models in dynamic treatment regimes may be difficult to meaningfully evaluate using current OPE methods, indicating that RL for this application may still be in its early stages. We hope that these results along with the benchmark itself will facilitate the comparison of existing methods and inspire further research into techniques that increase the practical applicability of medical RL.",Datasets & Benchmarks,NeurIPS,2024,Poster,Mason Hargrave;Alex Spaeth;Logan Grosenick,True,https://openreview.net/pdf?id=dF22s2GoX0
dVfNPSzpnv,IMDL-BenCo: A Comprehensive Benchmark and Codebase for Image Manipulation Detection & Localization,"A comprehensive benchmark is yet to be established in the Image Manipulation Detection \\\\& Localization (IMDL) field. The absence of such a benchmark leads to insufficient and misleading model evaluations, severely undermining the development of this field. However, the scarcity of open-sourced baseline models and inconsistent training and evaluation protocols make conducting rigorous experiments and faithful comparisons among IMDL models challenging. 
To address these challenges, we introduce IMDL-BenCo, the first comprehensive IMDL benchmark and modular codebase. IMDL-BenCo: i) decomposes the IMDL framework into standardized, reusable components and revises the model construction pipeline, improving coding efficiency and customization flexibility; ii) fully implements or incorporates training code for state-of-the-art models to establish a comprehensive IMDL benchmark; and iii) conducts deep analysis based on the established benchmark and codebase, offering new insights into IMDL model architecture, dataset characteristics, and evaluation standards.
Specifically, IMDL-BenCo includes common processing algorithms, 8 state-of-the-art IMDL models (1 of which are reproduced from scratch), 2 sets of standard training and evaluation protocols, 15 GPU-accelerated evaluation metrics, and 3 kinds of robustness evaluation. This benchmark and codebase represent a significant leap forward in calibrating the current progress in the IMDL field and inspiring future breakthroughs.
Code is available at: https://github.com/scu-zjz/IMDLBenCo",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Xiaochen Ma;Xuekang Zhu;Lei Su;Bo Du;Zhuohang Jiang;Bingkui Tong;Zeyu Lei;Xinyu Yang;Chi-Man Pun;Jiancheng Lv;Ji-Zhe Zhou,False,https://openreview.net/pdf?id=dVfNPSzpnv
dYIqAZXQNV,Generalizing CNNs to graphs with learnable neighborhood quantization,"Convolutional neural networks (CNNs) have led to a revolution in analyzing array data. However, many important sources of data, such as biological and social networks, are naturally structured as graphs rather than arrays, making the design of graph neural network (GNN) architectures that retain the strengths of CNNs an active and exciting area of research. Here, we introduce Quantized Graph Convolution Networks (QGCNs), the first framework for GNNs that formally and directly extends CNNs to graphs. QGCNs do this by decomposing the convolution operation into non-overlapping sub-kernels, allowing them to fit graph data while reducing to a 2D CNN layer on array data. We generalize this approach to graphs of arbitrary size and dimension by approaching sub-kernel assignment as a learnable multinomial assignment problem. Integrating this approach into a residual network architecture, we demonstrate performance that matches or exceeds other state-of-the-art GNNs on benchmark graph datasets and for predicting properties of nonlinear dynamics on a new finite element graph dataset. In summary, QGCNs are a novel GNN framework that generalizes CNNs and their strengths to graph data, allowing for more accurate and expressive models.",main,NeurIPS,2024,Poster,Isaac Osafo Nkansah;Neil Gallagher;Ruchi Sandilya;Conor Liston;Logan Grosenick,True,https://openreview.net/pdf?id=dYIqAZXQNV
djGx0hucok,FedLLM-Bench: Realistic Benchmarks for Federated Learning of Large Language Models,"Federated learning has enabled multiple parties to collaboratively train large language models without directly sharing their data (FedLLM).
Following this training paradigm, the community has put massive efforts from diverse aspects including framework, performance, and privacy.
However, an unpleasant fact is that there are currently no realistic datasets and benchmarks for FedLLM and previous works all rely on artificially constructed datasets, failing to capture properties in real-world scenarios.
Addressing this, we propose FedLLM-Bench, which involves 8 training methods, 4 training datasets, and 6 evaluation metrics, to offer a comprehensive testbed for the FedLLM community.
FedLLM-Bench encompasses three datasets (e.g., user-annotated multilingual dataset) for federated instruction tuning and one dataset (e.g., user-annotated preference dataset) for federated preference alignment, whose scale of client number ranges from 38 to 747.
Our datasets incorporate several representative diversities: language, quality, quantity, instruction, length, embedding, and preference, capturing properties in real-world scenarios.
Based on FedLLM-Bench, we conduct experiments on all datasets to benchmark existing FL methods and provide empirical insights (e.g., multilingual collaboration).
We believe that our FedLLM-Bench can benefit the FedLLM community by reducing required efforts, providing a practical testbed, and promoting fair comparisons.
Code and datasets are available at https://github.com/rui-ye/FedLLM-Bench.",Datasets & Benchmarks,NeurIPS,2024,Poster,Rui Ye;Rui Ge;Xinyu Zhu;Jingyi Chai;Yaxin Du;Yang Liu;Yanfeng Wang;Siheng Chen,True,https://openreview.net/pdf?id=djGx0hucok
drpJ7KOr3F,LLMs Can Evolve Continually on Modality for $\\\\mathbb{X}$-Modal Reasoning,"Multimodal Large Language Models (MLLMs) have gained significant attention due to their impressive capabilities in multimodal understanding. However, existing methods rely heavily on extensive modal-specific pretraining and joint-modal tuning, leading to significant computational burdens when expanding to new modalities. In this paper, we propose \\\\textbf{PathWeave}, a flexible and scalable framework with modal-\\\\textbf{path} s\\\\textbf{w}itching and \\\\textbf{e}xp\\\\textbf{a}nsion abilities that enables MLLMs to continually \\\\textbf{ev}olve on modalities for $\\\\mathbb{X}$-modal reasoning. We leverage the concept of Continual Learning and develop an incremental training strategy atop pre-trained MLLMs, enabling their expansion to new modalities using uni-modal data, without executing joint-modal pretraining. In detail, a novel Adapter-in-Adapter (AnA) framework is introduced, in which uni-modal and cross-modal adapters are seamlessly integrated to facilitate efficient modality alignment and collaboration. Additionally, an MoE-based gating module is applied between two types of adapters to further enhance the multimodal interaction. To investigate the proposed method, we establish a challenging benchmark called \\\\textbf{C}ontinual \\\\textbf{L}earning of \\\\textbf{M}odality (MCL), which consists of high-quality QA data from five distinct modalities: image, video, \\\\textcolor{black}{audio, depth} and point cloud. Extensive experiments demonstrate the effectiveness of the proposed AnA framework on learning plasticity and memory stability during continual learning. Furthermore, PathWeave performs comparably to state-of-the-art MLLMs while concurrently reducing parameter training burdens by 98.73\\\\%. Our code locates at \\\\url{https://github.com/JiazuoYu/PathWeave}.",main,NeurIPS,2024,Poster,Jiazuo Yu;Haomiao Xiong;Lu Zhang;Haiwen Diao;Yunzhi Zhuge;Lanqing HONG;Dong Wang;Huchuan Lu;You He;Long Chen,True,https://openreview.net/pdf?id=drpJ7KOr3F
dsK5EmmomU,Assemblage: Automatic Binary Dataset Construction for Machine Learning,"Binary code is pervasive, and binary analysis is a key task in reverse engineering, malware classification, and vulnerability discovery. Unfortunately, while there exist large corpuses of malicious binaries, obtaining high-quality corpuses of benign binaries for modern systems has proven challenging (e.g., due to licensing issues). Consequently, machine learning based pipelines for binary analysis utilize either costly commercial corpuses (e.g., VirusTotal) or open-source binaries (e.g., coreutils) available in limited quantities. To address these issues, we present Assemblage: an extensible cloud-based distributed system that crawls, configures, and builds Windows PE binaries to obtain high-quality binary corpuses suitable for training state-of-the-art models in binary analysis. We have run Assemblage on AWS over the past year, producing 890k Windows PE and 428k Linux ELF binaries across 29 configurations. Assemblage is designed to be both reproducible and extensible, enabling users to publish ""recipes"" for their datasets, and facilitating the extraction of a wide array of features. We evaluated Assemblage by using its data to train modern learning-based pipelines for compiler provenance and binary function similarity. Our results illustrate the practical need for robust corpuses of high-quality Windows PE binaries in training modern learning-based binary analyses.",Datasets & Benchmarks,NeurIPS,2024,Poster,Chang Liu;Rebecca Saul;Yihao Sun;Edward Raff;Maya Fuchs;Townsend Southard Pantano;James Holt;Kristopher Micinski,True,https://openreview.net/pdf?id=dsK5EmmomU
dtvJF1Vy2i,What matters when building vision-language models?,"The growing interest in vision-language models (VLMs) has been driven by improvements in large language models and vision transformers. Despite the abundance of literature on this subject, we observe that critical decisions regarding the design of VLMs are often not justified. We argue that these unsupported decisions impede progress in the field by making it difficult to identify which choices improve model performance. To address this issue, we conduct extensive experiments around pre-trained models, architecture choice, data, and training methods. Our consolidation of findings includes the development of Idefics2, an efficient foundational VLM of 8 billion parameters. Idefics2 achieves state-of-the-art performance within its size category across various multimodal benchmarks, and is often on par with models four times its size. We release the model (base, instructed, and chat) along with the datasets created for its training.",main,NeurIPS,2024,Poster,Hugo Laurençon;Leo Tronchon;Matthieu Cord;Victor Sanh,True,https://openreview.net/pdf?id=dtvJF1Vy2i
e397soEZh8,Learning Structure-Aware Representations of Dependent Types,"Agda is a dependently-typed programming language and a proof assistant, pivotal in proof formalization and programming language theory.
This paper extends the Agda ecosystem into machine learning territory, and, vice versa, makes Agda-related resources available to machine learning practitioners.
We introduce and release a novel dataset of Agda program-proofs that is elaborate and extensive enough to support various machine learning applications -- the first of its kind.
Leveraging the dataset's ultra-high resolution, which details proof states at the sub-type level, we propose a novel neural architecture targeted at faithfully representing dependently-typed programs on the basis of structural rather than nominal principles.
We instantiate and evaluate our architecture in a premise selection setup, where it achieves promising initial results, surpassing strong baselines.",main,NeurIPS,2024,Poster,Konstantinos Kogkalidis;Orestis Melkonian;Jean-Philippe Bernardy,True,https://openreview.net/pdf?id=e397soEZh8
e49QqJxCwq,PLIP: Language-Image Pre-training for Person Representation Learning,"Language-image pre-training is an effective technique for learning powerful representations in general domains. However, when directly turning to person representation learning, these general pre-training methods suffer from unsatisfactory performance. The reason is that they neglect critical person-related characteristics, i.e., fine-grained attributes and identities. To address this issue, we propose a novel language-image pre-training framework for person representation learning, termed PLIP. Specifically, we elaborately design three pretext tasks: 1) Text-guided Image Colorization, aims to establish the correspondence between the person-related image regions and the fine-grained color-part textual phrases. 2) Image-guided Attributes Prediction, aims to mine fine-grained attribute information of the person body in the image; and 3) Identity-based Vision-Language Contrast, aims to correlate the cross-modal representations at the identity level rather than the instance level. Moreover, to implement our pre-train framework, we construct a large-scale person dataset with image-text pairs named SYNTH-PEDES by automatically generating textual annotations. We pre-train PLIP on SYNTH-PEDES and evaluate our models by spanning downstream person-centric tasks. PLIP not only significantly improves existing methods on all these tasks, but also shows great ability in the zero-shot and domain generalization settings. The code, dataset and weight will be made publicly available.",main,NeurIPS,2024,Poster,Jialong Zuo;Jiahao Hong;Feng Zhang;Changqian Yu;Hanyu Zhou;Changxin Gao;Nong Sang;Jingdong Wang,True,https://openreview.net/pdf?id=e49QqJxCwq
eFPxCNmI7i,Semi-Truths: A Large-Scale Dataset of AI-Augmented Images for Evaluating Robustness of AI-Generated Image detectors,"Text-to-image diffusion models have impactful applications in art, design, and entertainment, yet these technologies also pose significant risks by enabling the creation and dissemination of misinformation. Although recent advancements have produced AI-generated image detectors that claim robustness against various augmentations, their true effectiveness remains uncertain. Do these detectors reliably identify images with different levels of augmentation? Are they biased toward specific scenes or data distributions? To investigate, we introduce **Semi-Truths**, featuring $27,600$ real images, $223,400$ masks, and $1, 329, 155$ AI-augmented images that feature targeted and localized perturbations produced using diverse augmentation techniques, diffusion models, and data distributions. Each augmented image is accompanied by metadata for standardized and targeted evaluation of detector robustness. Our findings suggest that state-of-the-art detectors exhibit varying sensitivities to the types and degrees of perturbations, data distributions, and augmentation methods used, offering new insights into their performance and limitations. The code for the augmentation and evaluation pipeline is available at https://github.com/J-Kruk/SemiTruths.",Datasets & Benchmarks,NeurIPS,2024,Poster,Anisha Pal;Julia Kruk;Mansi Phute;Manognya Bhattaram;Diyi Yang;Duen Horng Chau;Judy Hoffman,True,https://openreview.net/pdf?id=eFPxCNmI7i
eOszT2lepG,EgoSim: An Egocentric Multi-view Simulator and Real Dataset for Body-worn Cameras during Motion and Activity,"Research on egocentric tasks in computer vision has mostly focused on head-mounted cameras, such as fisheye cameras or embedded cameras inside immersive headsets.
We argue that the increasing miniaturization of optical sensors will lead to the prolific integration of cameras into many more body-worn devices at various locations.
This will bring fresh perspectives to established tasks in computer vision and benefit key areas such as human motion tracking, body pose estimation, or action recognition---particularly for the lower body, which is typically occluded.

In this paper, we introduce EgoSim, a novel simulator of body-worn cameras that generates realistic egocentric renderings from multiple perspectives across a wearer's body.
A key feature of EgoSim is its use of real motion capture data to render motion artifacts, which are especially noticeable with arm- or leg-worn cameras.
In addition, we introduce MultiEgoView, a dataset of egocentric footage from six body-worn cameras and ground-truth full-body 3D poses during several activities:
119 hours of data are derived from AMASS motion sequences in four high-fidelity virtual environments, which we augment with 5 hours of real-world motion data from 13 participants using six GoPro cameras and 3D body pose references from an Xsens motion capture suit.

We demonstrate EgoSim's effectiveness by training an end-to-end video-only 3D pose estimation network.
Analyzing its domain gap, we show that our dataset and simulator substantially aid training for inference on real-world data.

EgoSim code & MultiEgoView dataset: https://siplab.org/projects/EgoSim",Datasets & Benchmarks,NeurIPS,2024,Poster,Dominik Hollidt;Paul Streli;Jiaxi Jiang;Yasaman Haghighi;Changlin Qian;Xintong Liu;Christian Holz,True,https://openreview.net/pdf?id=eOszT2lepG
eRleg6vy0Y,Micro-Bench: A Microscopy Benchmark for Vision-Language Understanding,"Recent advances in microscopy have enabled the rapid generation of terabytes of image data in cell biology and biomedical research. Vision-language models (VLMs) offer a promising solution for large-scale biological image analysis, enhancing researchers’ efficiency, identifying new image biomarkers, and accelerating hypothesis generation and scientific discovery. However, there is a lack of standardized, diverse, and large-scale vision-language benchmarks to evaluate VLMs’ perception and cognition capabilities in biological image understanding. To address this gap, we introduce Micro-Bench, an expert-curated benchmark encompassing 24 biomedical tasks across various scientific disciplines (biology, pathology), microscopy modalities (electron, fluorescence, light), scales (subcellular, cellular, tissue), and organisms in both normal and abnormal states. We evaluate state-of-the-art biomedical, pathology, and general VLMs on Micro-Bench and find that: i) current models struggle on all categories, even for basic tasks such as distinguishing microscopy modalities; ii) current specialist models fine-tuned on biomedical data often perform worse than generalist models; iii) fine-tuning in specific microscopy domains can cause catastrophic forgetting, eroding prior biomedical knowledge encoded in their base model. iv) weight interpolation between fine-tuned and pre-trained models offers one solution to forgetting and improves general performance across biomedical tasks. We release Micro-Bench under a permissive license to accelerate the research and development of microscopy foundation models.",Datasets & Benchmarks,NeurIPS,2024,Poster,Alejandro Lozano;Jeffrey J Nirschl;James Burgess;Sanket Rajan Gupte;Yuhui Zhang;Alyssa Unell;Serena Yeung-Levy,True,https://openreview.net/pdf?id=eRleg6vy0Y
eVAzmJgrAc,Lost in Translation: Benchmarking Commercial Machine Translation Models for Dyslexic-Style Text,"Dyslexia is a neurodivergence that impacts one's ability to process and produce textual information. While previous research has identified unique patterns in the writings of people with dyslexia - such as letter swapping and homophone confusion - that differ themselves from the text typically used in the training and evaluation of common natural language processing (NLP) systems such as machine translation (MT), it is unclear how current state-of-the-art NLP systems perform for users with dyslexia. In this work, we explore this topic through a systematic audit of the performance of commercial MT services using synthetic dyslexia data. By injecting common dyslexia-style writing errors into popular benchmarking datasets, we benchmark the performance of three commercial MT services and one large language model (LLM) with various types and quantities of dyslexia-style errors and show a substantial disparity in MT quality for dyslexic and non-dyslexic text. While people with dyslexia often rely on modern NLP tools as assistive technologies, our results shed light on the fairness challenges experienced by this demographic with popular NLP services, highlighting the need to develop more inclusive and equitable NLP models for users with diverse language use patterns.",Datasets & Benchmarks,NeurIPS,2024,Reject,Gregory Price;Shaomei Wu,False,https://openreview.net/pdf?id=eVAzmJgrAc
ecPIg6o84Z,Image-aware Evaluation of Generated Medical Reports,"The paper proposes a novel evaluation metric for automatic medical report generation from X-ray images, VLScore. It aims to overcome the limitations of existing evaluation methods, which either focus solely on textual similarities, ignoring clinical aspects, or concentrate only on a single clinical aspect, the pathology, neglecting all other factors. The key idea of our metric is to measure the similarity between radiology reports while considering the corresponding image. We demonstrate the benefit of our metric through evaluation on a dataset where radiologists marked errors in pairs of reports, showing notable alignment with radiologists' judgments. In addition, we provide a new dataset for evaluating metrics. This dataset includes well-designed perturbations that distinguish between significant modifications (e.g., removal of a diagnosis) and insignificant ones. It highlights the weaknesses in current evaluation metrics and provides a clear framework for analysis.",main,NeurIPS,2024,Poster,Gefen Dawidowicz;Elad Hirsch;Ayellet Tal,True,https://openreview.net/pdf?id=ecPIg6o84Z
edOZifvwMi,CryoGEM: Physics-Informed Generative Cryo-Electron Microscopy,"In the past decade, deep conditional generative models have revolutionized the generation of realistic images, extending their application from entertainment to scientific domains. Single-particle cryo-electron microscopy (cryo-EM) is crucial in resolving near-atomic resolution 3D structures of proteins, such as the SARS-COV-2 spike protein. To achieve high-resolution reconstruction, a comprehensive data processing pipeline has been adopted. However, its performance is still limited as it lacks high-quality annotated datasets for training. To address this, we introduce physics-informed generative cryo-electron microscopy (CryoGEM), which for the first time integrates physics-based cryo-EM simulation with a generative unpaired noise translation to generate physically correct synthetic cryo-EM datasets with realistic noises. Initially, CryoGEM simulates the cryo-EM imaging process based on a virtual specimen. To generate realistic noises, we leverage an unpaired noise translation via contrastive learning with a novel mask-guided sampling scheme. Extensive experiments show that CryoGEM is capable of generating authentic cryo-EM images. The generated dataset can be used as training data for particle picking and pose estimation models, eventually improving the reconstruction resolution.",main,NeurIPS,2024,Poster,Jiakai Zhang;Qihe Chen;Yan Zeng;Wenyuan Gao;Xuming He;Zhijie Liu;Jingyi Yu,True,https://openreview.net/pdf?id=edOZifvwMi
ejWvCpLuwu,RegExplainer: Generating Explanations for Graph Neural Networks in Regression Tasks,"Graph regression is a fundamental task that has gained significant attention in
various graph learning tasks. However, the inference process is often not easily
interpretable. Current explanation techniques are limited to understanding Graph
Neural Network (GNN) behaviors in classification tasks, leaving an explanation gap
for graph regression models. In this work, we propose a novel explanation method
to interpret the graph regression models (XAIG-R). Our method addresses the
distribution shifting problem and continuously ordered decision boundary issues
that hinder existing methods away from being applied in regression tasks. We
introduce a novel objective based on the graph information bottleneck theory (GIB)
and a new mix-up framework, which can support various GNNs and explainers
in a model-agnostic manner. Additionally, we present a self-supervised learning
strategy to tackle the continuously ordered labels in regression tasks. We evaluate
our proposed method on three benchmark datasets and a real-life dataset introduced
by us, and extensive experiments demonstrate its effectiveness in interpreting GNN
models in regression tasks.",main,NeurIPS,2024,Poster,Jiaxing Zhang;Zhuomin Chen;hao mei;Longchao Da;Dongsheng Luo;Hua Wei,True,https://openreview.net/pdf?id=ejWvCpLuwu
etdXLAMZoc,LibMOON: A Gradient-based MultiObjective OptimizatioN Library in PyTorch,"Multiobjective optimization problems (MOPs) are prevalent in machine learning, with applications in multi-task learning, learning under fairness or robustness constraints, etc. Instead of reducing multiple objective functions into a scalar objective, MOPs aim to optimize for the so-called Pareto optimality or Pareto set learning, which involves optimizing more than one objective function simultaneously, over models with thousands to millions of parameters. Existing benchmark libraries for MOPs mainly focus on evolutionary algorithms, most of which are zeroth-order or meta-heuristic methods that do not effectively utilize higher-order information from objectives and cannot scale to large-scale models with millions of parameters. In light of the above challenges, this paper introduces \\\\algoname, the first multiobjective optimization library that supports state-of-the-art gradient-based methods, provides a fair and comprehensive benchmark, and is open-sourced for the community.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xiaoyuan Zhang;Liang Zhao;Yingying Yu;Xi Lin;Yifan Chen;Han Zhao;Qingfu Zhang,False,https://openreview.net/pdf?id=etdXLAMZoc
evP9mxNNxJ,Are We on the Right Way for Evaluating Large Vision-Language Models?,"Large vision-language models (LVLMs) have recently achieved rapid progress, sparking numerous studies to evaluate their multi-modal capabilities. However, we dig into current evaluation works and identify two primary issues: 1) Visual content is unnecessary for many samples. The answers can be directly inferred from the questions and options, or the world knowledge embedded in LLMs. This phenomenon is prevalent across current benchmarks. For instance, GeminiPro achieves 42.7% on the MMMU benchmark without any visual input, and outperforms the random choice baseline across six benchmarks near 24% on average. 2) Unintentional data leakage exists in LLM and LVLM training. LLM and LVLM could still answer some visual-necessary questions without visual content, indicating the memorizing of these samples within large-scale training data. For example, Sphinx-X-MoE gets 43.6% on MMMU without accessing images, surpassing its LLM backbone with 17.9%. Both problems lead to misjudgments of actual multi-modal gains and potentially misguide the study of LVLM. To this end, we present MMStar, an elite vision-indispensable multi-modal benchmark comprising 1,500 samples meticulously selected by humans. MMStar benchmarks 6 core capabilities and 18 detailed axes, aiming to evaluate LVLMs' multi-modal capacities with carefully balanced and purified samples. These samples are first roughly selected from current benchmarks with an automated pipeline, human review is then involved to ensure each curated sample exhibits visual dependency, minimal data leakage, and requires advanced multi-modal capabilities. Moreover, two metrics are developed to measure data leakage and actual performance gain in multi-modal training. We evaluate 16 leading LVLMs on MMStar to assess their multi-modal capabilities, and on 7 benchmarks with the proposed metrics to investigate their data leakage and actual multi-modal gain.",main,NeurIPS,2024,Poster,Lin Chen;Jinsong Li;Xiaoyi Dong;Pan Zhang;Yuhang Zang;Zehui Chen;Haodong Duan;Jiaqi Wang;Yu Qiao;Dahua Lin;Feng Zhao,True,https://openreview.net/pdf?id=evP9mxNNxJ
f1UL4wNlw6,The Art of Saying No: Contextual Noncompliance in Language Models,"Chat-based language models are designed to be helpful, yet they should not comply with every user request.
 While most existing work primarily focuses on refusal of ``unsafe'' queries, we posit that the scope of noncompliance should be broadened. We introduce  a comprehensive taxonomy of contextual noncompliance describing when and how models should *not* comply with user requests. Our taxonomy spans a wide range of categories including *incomplete*, *unsupported*, *indeterminate*, and *humanizing* requests (in addition to *unsafe* requests). To test noncompliance capabilities of language models, we use this taxonomy to develop a new evaluation suite of 1000 noncompliance prompts. We find that most existing models show significantly high compliance rates in certain previously understudied categories with models like GPT-4 incorrectly complying with as many as 30\\\\% of requests.
To address these gaps, we explore different training strategies using a synthetically-generated training set of requests and expected noncompliant responses. 
Our experiments demonstrate that while direct finetuning of instruction-tuned models can lead to both over-refusal and a decline in general capabilities, using parameter efficient methods like low rank adapters helps to strike a good balance between appropriate noncompliance and other capabilities.",Datasets & Benchmarks,NeurIPS,2024,Poster,Faeze Brahman;Sachin Kumar;Vidhisha Balachandran;Pradeep Dasigi;Valentina Pyatkin;Abhilasha Ravichander;Sarah Wiegreffe;Nouha Dziri;Khyathi Chandu;Jack Hessel;Yulia Tsvetkov;Noah A. Smith;Yejin Choi;Hannaneh Hajishirzi,True,https://openreview.net/pdf?id=f1UL4wNlw6
f4v7cmm5sC,Foundation Inference Models for Markov Jump Processes,"Markov jump processes are continuous-time stochastic processes which describe dynamical systems evolving in discrete state spaces. These processes find wide application in the natural sciences and machine learning, but their inference is known to be far from trivial. In this work we introduce a methodology for *zero-shot inference* of Markov jump processes (MJPs), on bounded state spaces, from noisy and sparse observations, which consists of two components. First, a broad probability distribution over families of MJPs, as well as over possible observation times and noise mechanisms, with which we simulate a synthetic dataset of hidden MJPs and their noisy observations. Second, a neural recognition model that processes subsets of the simulated observations, and that is trained to output the initial condition and rate matrix of the target MJP in a supervised way. We empirically demonstrate that *one and the same* (pretrained) recognition model can infer, *in a zero-shot fashion*, hidden MJPs evolving in state spaces of different dimensionalities. Specifically, we infer MJPs which describe (i) discrete flashing ratchet systems, which are a type of Brownian motors, and the conformational dynamics in (ii) molecular simulations, (iii) experimental ion channel data and (iv) simple protein folding models. What is more, we show that our model performs on par with state-of-the-art models which are trained on the target datasets.

Our pretrained model is available online.",main,NeurIPS,2024,Poster,David Berghaus;Kostadin Cvejoski;Patrick Seifner;Cesar Ojeda;Ramses J Sanchez,True,https://openreview.net/pdf?id=f4v7cmm5sC
f5XZEROoGb,SubjECTive-QA: Measuring Subjectivity in Earnings Call Transcripts' QA Through Six-Dimensional Feature Analysis,"Fact-checking is extensively studied in the context of misinformation and disinformation, addressing objective inaccuracies. However, a softer form of misinformation involves responses that are factually correct but lack certain features such as clarity and relevance. This challenge is prevalent in formal Question-Answer (QA) settings such as press conferences in finance, politics, sports, and other domains, where subjective answers can obscure transparency. Despite this, there is a lack of manually annotated datasets for subjective features across multiple dimensions. To address this gap, we introduce SubjECTive-QA, a human annotated dataset on Earnings Call Transcripts' (ECTs) QA sessions as the answers given by company representatives are often open to subjective interpretations and scrutiny. The dataset includes 49,446 annotations for long-form QA pairs across six features: Assertive, Cautious, Optimistic, Specific, Clear, and Relevant. These features are carefully selected to encompass the key attributes that reflect the tone of the answers provided during QA sessions across different domains. Our findings are that the best-performing Pre-trained Language Model (PLM), RoBERTa-base, has similar weighted F1 scores to Llama-3-70b-Chat on features with lower subjectivity, such as Relevant and Clear, with a mean difference of 2.17% in their weighted F1 scores. The models perform significantly better on features with higher subjectivity, such as Specific and Assertive, with a mean difference of 10.01% in their weighted F1 scores. Furthermore, testing SubjECTive-QA's generalizability using QAs from White House Press Briefings and Gaggles yields an average weighted F1 score of 65.97% using our best models for each feature, demonstrating broader applicability beyond the financial domain. SubjECTive-QA is publicly available under the CC BY 4.0 license.",Datasets & Benchmarks,NeurIPS,2024,Poster,Huzaifa Pardawala;Siddhant Sukhani;Agam Shah;Veer Kejriwal;Abhishek Pillai;Rohan Bhasin;Andrew DiBiasio;Tarun Mandapati;Dhruv Adha;Sudheer Chava,True,https://openreview.net/pdf?id=f5XZEROoGb
f70e6YYFHF,The Factorization Curse: Which Tokens You Predict Underlie the Reversal Curse and More,"Today's best language models still struggle with ""hallucinations"", factually incorrect generations, which impede their ability to reliably retrieve information seen during training. The *reversal curse*, where models cannot recall information when probed in a different order than was encountered during training, exemplifies limitations in information retrieval. 
To better understand these limitations, we reframe the reversal curse as a *factorization curse* --- a failure of models to learn the same joint distribution under different factorizations.
We more closely simulate finetuning workflows which train pretrained models on specialized knowledge by introducing
*WikiReversal*, a realistic testbed based on Wikipedia knowledge graphs. Through a series of controlled experiments with increasing levels of realism, including non-reciprocal relations, we find that reliable information retrieval is an inherent failure of the next-token prediction objective used in popular large language models. Moreover, we demonstrate reliable information retrieval cannot be solved with scale, reversed tokens, or even naive bidirectional-attention training. Consequently, various approaches to finetuning on specialized data would necessarily provide mixed results on downstream tasks, unless the model has already seen the right sequence of tokens. 
Across five tasks of varying levels of complexity, our results uncover a promising path forward: factorization-agnostic objectives can significantly mitigate the reversal curse and hint at improved knowledge storage and planning capabilities.",main,NeurIPS,2024,Poster,Ouail Kitouni;Niklas Nolte;Adina Williams;Michael Rabbat;Diane Bouchacourt;Mark Ibrahim,True,https://openreview.net/pdf?id=f70e6YYFHF
fNcyFhTw2f,Advancing Video Anomaly Detection: A Concise Review and a New Dataset,"Video Anomaly Detection (VAD) finds widespread applications in security surveillance, traffic monitoring, industrial monitoring, and healthcare. Despite extensive research efforts, there remains a lack of concise reviews that provide insightful guidance for researchers. Such reviews would serve as quick references to grasp current challenges, research trends, and future directions. In this paper, we present such a review, examining models and datasets from various perspectives. We emphasize the critical relationship between model and dataset, where the quality and diversity of datasets profoundly influence model performance, and dataset development adapts to the evolving needs of emerging approaches. Our review identifies practical issues, including the absence of comprehensive datasets with diverse scenarios. To address this, we introduce a new dataset, Multi-Scenario Anomaly Detection (MSAD), comprising 14 distinct scenarios captured from various camera views. Our dataset has diverse motion patterns and challenging variations, such as different lighting and weather conditions, providing a robust foundation for training superior models. We conduct an in-depth analysis of recent representative models using MSAD and highlight its potential in addressing the challenges of detecting anomalies across diverse and evolving surveillance scenarios.",Datasets & Benchmarks,NeurIPS,2024,Poster,Liyun Zhu;Lei Wang;Arjun Raj;Tom Gedeon;Chen Chen,True,https://openreview.net/pdf?id=fNcyFhTw2f
fVRCsK4EoM,PrefPaint: Aligning Image Inpainting Diffusion Model with Human Preference,"In this paper, we make the first attempt to align diffusion models for image inpainting with human aesthetic standards via a reinforcement learning framework, significantly improving the quality and visual appeal of inpainted images. Specifically, instead of directly measuring the divergence with paired images, we train a reward model with the dataset we construct, consisting of nearly 51,000 images annotated with human preferences. Then, we adopt a reinforcement learning process to fine-tune the distribution of a pre-trained diffusion model for image inpainting in the direction of higher reward. Moreover, we theoretically deduce the upper bound on the error of the reward model, which illustrates the potential confidence of reward estimation throughout the reinforcement alignment process, thereby facilitating accurate regularization.
Extensive experiments on inpainting comparison and downstream tasks, such as image extension and 3D reconstruction, demonstrate the effectiveness of our approach, showing significant improvements in the alignment of inpainted images with human preference compared with state-of-the-art methods. This research not only advances the field of image inpainting but also provides a framework for incorporating human preference into the iterative refinement of generative models based on modeling reward accuracy, with broad implications for the design of visually driven AI applications. Our code and dataset are publicly available at \\\\url{https://prefpaint.github.io}.",main,NeurIPS,2024,Poster,Kendong Liu;Zhiyu Zhu;Chuanhao Li;Hui LIU;Huanqiang Zeng;Junhui Hou,True,https://openreview.net/pdf?id=fVRCsK4EoM
fXEi3LVflp,Referring Human Pose and Mask Estimation In the Wild,"We introduce Referring Human Pose and Mask Estimation (R-HPM) in the wild, where either a text or positional prompt specifies the person of interest in an image. This new task holds significant potential for human-centric applications such as assistive robotics and sports analysis. In contrast to previous works, R-HPM (i) ensures high-quality, identity-aware results corresponding to the referred person, and (ii) simultaneously predicts human pose and mask for a comprehensive representation. To achieve this, we introduce a large-scale dataset named RefHuman, which substantially extends the MS COCO dataset with additional text and positional prompt annotations. RefHuman includes over 50,000 annotated instances in the wild, each equipped with keypoint, mask, and prompt annotations. To enable prompt-conditioned estimation, we propose the first end-to-end promptable approach named UniPHD for R-HPM. UniPHD extracts multimodal representations and employs a proposed pose-centric hierarchical decoder to process (text or positional) instance queries and keypoint queries, producing results specific to the referred person. Extensive experiments demonstrate that UniPHD produces quality results based on user-friendly prompts and achieves top-tier performance on RefHuman val and MS COCO val2017.",main,NeurIPS,2024,Poster,Bo Miao;Mingtao Feng;Zijie Wu;Mohammed Bennamoun;Yongsheng Gao;Ajmal Saeed Mian,True,https://openreview.net/pdf?id=fXEi3LVflp
fgJ9OvJPZB,Re-assembling the past: The RePAIR dataset and benchmark for real world 2D and 3D puzzle solving,"This paper proposes the RePAIR dataset that represents a challenging benchmark to test modern computational and data driven methods for puzzle-solving and reassembly tasks. Our dataset has unique properties that are uncommon to current benchmarks for 2D and 3D puzzle solving. The fragments and fractures are realistic, caused by a collapse of a fresco during a World War II bombing at the Pompeii archaeological park. The fragments are also eroded and have missing pieces with irregular shapes and different dimensions, challenging further the reassembly algorithms. The dataset is multi-modal providing  high resolution images with characteristic pictorial elements, detailed 3D scans of the fragments and meta-data annotated by the archaeologists. Ground truth has been generated through several years of unceasing fieldwork, including the excavation and cleaning of each fragment, followed by manual puzzle solving by archaeologists of a subset of approx. 1000 pieces among the 16000 available. After digitizing all the fragments in 3D, a benchmark was prepared to challenge current reassembly and puzzle-solving methods that often solve more simplistic synthetic scenarios. The tested baselines show that there clearly exists a gap to fill in solving this computationally complex problem.",Datasets & Benchmarks,NeurIPS,2024,Poster,Theodore Tsesmelis;Luca Palmieri;Marina Khoroshiltseva;Adeela Islam;Gur Elkin;Ofir Itzhak Shahar;Gianluca Scarpellini;Stefano Fiorini;Yaniv Ohayon;Nadav Alali;Sinem Aslan;Pietro Morerio;Sebastiano Vascon;Elena gravina;Maria Cristina Napolitano;Giuseppe Scarpati;Gabriel zuchtriegel;Alexandra Spühler;Michel E. Fuchs;Stuart James;Ohad Ben-Shahar;Marcello Pelillo;Alessio Del Bue,True,https://openreview.net/pdf?id=fgJ9OvJPZB
fq7WmnJ3iV,Value Imprint: A Technique for Auditing the Human Values Embedded in RLHF Datasets,"LLMs are increasingly fine-tuned using RLHF datasets to align them with human preferences and values. However, very limited research has investigated which specific human values are operationalized through these datasets. In this paper, we introduce Value Imprint, a framework for auditing and classifying the human values embedded within RLHF datasets. To investigate the viability of this framework, we conducted three case study experiments by auditing the Anthropic/hh-rlhf, OpenAI WebGPT Comparisons, and Alpaca GPT-4-LLM datasets to examine the human values embedded within them. Our analysis involved a two-phase process. During the first phase, we developed a taxonomy of human values through an integrated review of prior works from philosophy, axiology, and ethics. Then, we applied this taxonomy to annotate 6,501 RLHF preferences. During the second phase, we employed the labels generated from the annotation as ground truth data for training a transformer-based machine learning model to audit and classify the three RLHF datasets. Through this approach, we discovered that information-utility values, including Wisdom/Knowledge and Information Seeking, were the most dominant human values within all three RLHF datasets. In contrast, prosocial and democratic values, including Well-being, Justice, and Human/Animal Rights, were the least represented human values. These findings have significant implications for developing language models that align with societal values and norms. We contribute our datasets to support further research in this area. https://github.com/hv-rsrch/valueimprint",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Ike Obi;Rohan Pant;Srishti Shekhar Agrawal;Maham Ghazanfar;Aaron Basiletti,True,https://openreview.net/pdf?id=fq7WmnJ3iV
fqjeKsHOVR,Harmonizing Visual Text Comprehension and Generation,"In this work, we present TextHarmony, a unified and versatile multimodal generative model proficient in comprehending and generating visual text. Simultaneously generating images and texts typically results in performance degradation due to the inherent inconsistency between vision and language modalities. To overcome this challenge, existing approaches resort to modality-specific data for supervised fine-tuning, necessitating distinct model instances. We propose Slide-LoRA, which dynamically aggregates modality-specific and modality-agnostic LoRA experts, partially decoupling the multimodal generation space. Slide-LoRA harmonizes the generation of vision and language within a singular model instance, thereby facilitating a more unified generative process. Additionally, we develop a high-quality image caption dataset, DetailedTextCaps-100K, synthesized with a sophisticated closed-source MLLM to enhance visual text generation capabilities further. Comprehensive experiments across various benchmarks demonstrate the effectiveness of the proposed approach. Empowered by Slide-LoRA, TextHarmony achieves comparable performance to modality-specific fine-tuning results with only a 2% increase in parameters and shows an average improvement of 2.5% in visual text comprehension tasks and 4.0% in visual text generation tasks. Our work delineates the viability of an integrated approach to multimodal generation within the visual text domain, setting a foundation for subsequent inquiries. Code is available at https://github.com/bytedance/TextHarmony.",main,NeurIPS,2024,Poster,Zhen Zhao;Jingqun Tang;Binghong Wu;Chunhui Lin;Shu Wei;Hao Liu;Xin Tan;zhizhong zhang;Can Huang;Yuan Xie,True,https://openreview.net/pdf?id=fqjeKsHOVR
fuD0h4R1IL,Time-MMD: Multi-Domain Multimodal Dataset for Time Series Analysis,"Time series data are ubiquitous across a wide range of real-world domains. While
real-world time series analysis (TSA) requires human experts to integrate numerical series data with multimodal domain-specific knowledge, most existing TSA
models rely solely on numerical data, overlooking the significance of information beyond numerical series. This oversight is due to the untapped potential
of textual series data and the absence of a comprehensive, high-quality multimodal dataset. To overcome this obstacle, we introduce Time-MMD, the first
multi-domain, multimodal time series dataset covering 9 primary data domains.
Time-MMD ensures fine-grained modality alignment, eliminates data contamination, and provides high usability. Additionally, we develop MM-TSFlib, the
first-cut multimodal time-series forecasting (TSF) library, seamlessly pipelining
multimodal TSF evaluations based on Time-MMD for in-depth analyses. Extensive
experiments conducted on Time-MMD through MM-TSFlib demonstrate significant performance enhancements by extending unimodal TSF to multimodality,
evidenced by over 15% mean squared error reduction in general, and up to 40%
in domains with rich textual data. More importantly, our datasets and library
revolutionize broader applications, impacts, research topics to advance TSA. The
dataset is available at https://github.com/AdityaLab/Time-MMD.",Datasets & Benchmarks,NeurIPS,2024,Poster,Haoxin Liu;Shangqing Xu;Zhiyuan Zhao;Lingkai Kong;Harshavardhan Kamarthi;Aditya B. Sasanur;Megha Sharma;Jiaming Cui;Qingsong Wen;Chao Zhang;B. Aditya Prakash,True,https://openreview.net/pdf?id=fuD0h4R1IL
fzdFPqkAHD,Agent-to-Sim: Learning Interactive Behavior from Casual Videos,"Agent behavior simulation empowers robotics, gaming, movies, and VR applications, but building such simulators often requires laborious effort of manually crafting the agent's decision process and motion patterns. Recent advance in visual tracking and motion capture enables learning of agent behavior from real-world data, but these methods are limited to a few scenarios due to the dependence on specialized sensors (e.g., synchronized multi-camera systems). Towards better scalability, we present a framework, Agent-to-Sim, that learns simulatable 3D agents in a 3D environment from monocular videos. To deal with partial views, our framework fuses observations in a canonical space for both the agent and the scene, resulting in a dense 4D spatiotemporal reconstruction. We then learn an interactive behavior generator by querying paired data of agents' perception and actions from the 4D reconstruction. Agent-to-Sim enables real-to-sim transfer of agents in their familiar environments given longitudinal video recordings captured with a smartphone over a month. We show results on pets (e.g., cat, dog, bunny) and a person, and analyse how the observer's motion and 3D scene affect an agent's behavior.",main,NeurIPS,2024,Reject,Gengshan Yang;Andrea Bajcsy;Angjoo Kanazawa;Shunsuke Saito,True,https://openreview.net/pdf?id=fzdFPqkAHD
g1Zn0XPUFF,UniBench: Visual Reasoning Requires Rethinking Vision-Language Beyond Scaling,"Significant research efforts have been made to scale and improve vision-language model (VLM) training approaches. 
Yet, with an ever-growing number of benchmarks,
researchers are tasked with the heavy burden of implementing each protocol, bearing a non-trivial computational cost, and making sense of how all these benchmarks translate into meaningful axes of progress.
To facilitate a systematic evaluation of VLM progress, we introduce UniBench: a unified implementation of 50+ VLM benchmarks spanning a range of carefully categorized vision-centric capabilities from object recognition to spatial awareness, counting, and much more. We showcase the utility of UniBench for measuring progress by evaluating nearly 60 publicly available vision-language models, trained on scales of up to 12.8B samples. We find that while scaling training data or model size can boost many vision-language model capabilities, scaling offers little benefit for reasoning or relations.  Surprisingly, we also discover today's best VLMs struggle on simple digit recognition and counting tasks, e.g. MNIST, which much simpler networks can solve. Where scale falls short, we find that more precise interventions, such as data quality or tailored-learning objectives offer more promise. For practitioners, we also offer guidance on selecting a suitable VLM for a given application. Finally, we release an easy-to-run UniBench code-base with the full set of 50+ benchmarks and comparisons across 59 models as well as a distilled, representative set of benchmarks that runs in 5 minutes on a single GPU. UniBench with model evaluations on all benchmarks are provided as a toolbox at: https://github.com/facebookresearch/unibench",Datasets & Benchmarks,NeurIPS,2024,Poster,Haider Al-Tahan;Quentin Garrido;Randall Balestriero;Diane Bouchacourt;Caner Hazirbas;Mark Ibrahim,False,https://openreview.net/pdf?id=g1Zn0XPUFF
g7lYP11Erv,Point-PRC: A Prompt Learning Based Regulation Framework for Generalizable Point Cloud Analysis,"This paper investigates the 3D domain generalization (3DDG) ability of large 3D models based on prevalent prompt learning. Recent works demonstrate the performances of 3D point cloud recognition can be boosted remarkably by parameter-efficient prompt tuning. However, we observe that the improvement on downstream tasks comes at the expense of a severe drop in 3D domain generalization. To resolve this challenge, we present a comprehensive regulation framework that allows the learnable prompts to actively interact with the well-learned general knowledge in large 3D models to maintain good generalization. Specifically, the proposed framework imposes multiple explicit constraints on the prompt learning trajectory by maximizing the mutual agreement between task-specific predictions and task-agnostic knowledge. We design the regulation framework as a plug-and-play module to embed into existing representative large 3D models. Surprisingly, our method not only realizes consistently increasing generalization ability but also enhances task-specific 3D recognition performances across various 3DDG benchmarks by a clear margin. Considering the lack of study and evaluation on 3DDG, we also create three new benchmarks, namely base-to-new, cross-dataset and few-shot generalization benchmarks, to enrich the field and inspire future research. Code and benchmarks are available at \\\\url{https://github.com/auniquesun/Point-PRC}.",main,NeurIPS,2024,Poster,Hongyu Sun;Qiuhong Ke;Yongcai Wang;Wang Chen;Kang Yang;Deying Li;Jianfei Cai,True,https://openreview.net/pdf?id=g7lYP11Erv
gP4aAi7q8S,CausalChaos! Dataset for Comprehensive Causal Action Question Answering Over Longer Causal Chains Grounded in Dynamic Visual Scenes,"Causal video question answering (QA) has garnered increasing interest, yet existing datasets often lack depth in causal reasoning. To address this gap, we capitalize on the unique properties of cartoons and construct CausalChaos!, a novel, challenging causal Why-QA dataset built upon the iconic ""Tom and Jerry"" cartoon series. Cartoons use the principles of animation that allow animators to create expressive, unambiguous causal relationships between events to form a coherent storyline. Utilizing these properties, along with thought-provoking questions and multi-level answers (answer and detailed causal explanation), our questions involve causal chains that interconnect multiple dynamic interactions between characters and visual scenes. These factors demand models to solve more challenging, yet well-defined causal relationships. We also introduce hard incorrect answer mining, including a causally confusing version that is even more challenging. While models perform well, there is much room for improvement, especially, on open-ended answers. We identify more advanced/explicit causal relationship modeling \\\\& joint modeling of vision and language as the immediate areas for future efforts to focus upon. Along with the other complementary datasets, our new challenging dataset will pave the way for these developments in the field. Dataset and Code: https://github.com/LUNAProject22/CausalChaos",Datasets & Benchmarks,NeurIPS,2024,Poster,Paritosh Parmar;Eric Peh;Ruirui Chen;Ting En Lam;Yuhan Chen;Elston Tan;Basura Fernando,True,https://openreview.net/pdf?id=gP4aAi7q8S
gPLE4siNjO,"A Retrospective on the Robot Air Hockey Challenge: Benchmarking Robust, Reliable, and Safe Learning Techniques for Real-world Robotics","Machine learning methods have a groundbreaking impact in many application domains, but their application on real robotic platforms is still limited.
Despite the many challenges associated with combining machine learning technology with robotics, robot learning remains one of the most promising directions for enhancing the capabilities of robots. 
When deploying learning-based approaches on real robots, extra effort is required to address the challenges posed by various real-world factors. To investigate the key factors influencing real-world deployment and to encourage original solutions from different researchers, we organized the Robot Air Hockey Challenge at the NeurIPS 2023 conference. 
We selected the air hockey task as a benchmark, encompassing low-level robotics problems and high-level tactics. Different from other machine learning-centric benchmarks, participants need to tackle practical challenges in robotics, such as the sim-to-real gap, low-level control issues, safety problems, real-time requirements, and the limited availability of real-world data. Furthermore, we focus on a dynamic environment, removing the typical assumption of quasi-static motions of other real-world benchmarks.
The competition's results show that solutions combining learning-based approaches with prior knowledge outperform those relying solely on data when real-world deployment is challenging.
Our ablation study reveals which real-world factors may be overlooked when building a learning-based solution.
The successful real-world air hockey deployment of best-performing agents sets the foundation for future competitions and follow-up research directions.",Datasets & Benchmarks,NeurIPS,2024,Poster,Puze Liu;Jonas Günster;Niklas Funk;Simon Gröger;Dong Chen;Haitham Bou Ammar;Julius Jankowski;Ante Marić;Sylvain Calinon;Andrej Orsula;Miguel Olivares-Mendez;Hongyi Zhou;Rudolf Lioutikov;Gerhard Neumann;Amarildo Likmeta;Amirhossein Zhalehmehrabi;Thomas Bonenfant;Marcello Restelli;Davide Tateo;Ziyuan Liu;Jan Peters,False,https://openreview.net/pdf?id=gPLE4siNjO
gViJjwRUlM,Retrospective for the Dynamic Sensorium Competition for predicting large-scale mouse primary visual cortex activity from videos,"Understanding how biological visual systems process information is challenging because of the nonlinear relationship between visual input and neuronal responses. 
Artificial neural networks allow computational neuroscientists to create predictive models that connect biological and machine vision.
Machine learning has benefited tremendously from benchmarks that compare different models on the same task under standardized conditions. 
However, there was no standardized benchmark to identify state-of-the-art dynamic models of the mouse visual system.
To address this gap, we established the SENSORIUM 2023 Benchmark Competition with dynamic input, featuring a new large-scale dataset from the primary visual cortex of ten mice. 
This dataset includes responses from 78,853 neurons to 2 hours of dynamic stimuli per neuron, together with behavioral measurements such as running speed, pupil dilation, and eye movements.
The competition ranked models in two tracks based on predictive performance for neuronal responses on a held-out test set: one focusing on predicting in-domain natural stimuli and another on out-of-distribution (OOD) stimuli to assess model generalization.
As part of the NeurIPS 2023 Competition Track, we received more than 160 model submissions from 22 teams. 
Several new architectures for predictive models were proposed, and the winning teams improved the previous state-of-the-art model by 50\\\\%. 
Access to the dataset as well as the benchmarking infrastructure will remain online at www.sensorium-competition.net.",Datasets & Benchmarks,NeurIPS,2024,Poster,Polina Turishcheva;Paul G. Fahey;Michaela Vystrčilová;Laura Hansel;Rachel E Froebe;Kayla Ponder;Yongrong Qiu;Konstantin Friedrich Willeke;Mohammad Bashiri;Ruslan Baikulov;Yu Zhu;Lei Ma;Shan Yu;Tiejun Huang;Bryan M. Li;Wolf De Wulf;Nina Kudryashova;Matthias H. Hennig;Nathalie Rochefort;Arno Onken;Eric Wang;Zhiwei Ding;Andreas S. Tolias;Fabian H. Sinz;Alexander S Ecker,True,https://openreview.net/pdf?id=gViJjwRUlM
gad19kaPzb,Slice-100K: A Multimodal Dataset for Extrusion-based 3D Printing,"G-code (Geometric code) or RS-274 is the most widely used computer numerical control (CNC) and 3D printing programming language. G-code provides machine instructions for the movement of the 3D printer, especially for the nozzle, stage, and extrusion of material for extrusion-based additive manufacturing. Currently, there does not exist a large repository of curated CAD models along with their corresponding G-code files for additive manufacturing. To address this issue, we present Slice-100K, a first-of-its-kind dataset of over 100,000 G-code files, along with their tessellated CAD model, LVIS (Large Vocabulary Instance Segmentation) categories, geometric properties, and renderings. We build our dataset from triangulated meshes derived from Objaverse-XL and Thingi10K datasets. We demonstrate the utility of this dataset by finetuning GPT-2 on a subset of the dataset for G-code translation from a legacy G-code format (Sailfish) to a more modern, widely used format (Marlin). Our dataset can be found here. Slice-100K will be the first step in developing a multimodal foundation model for digital manufacturing.",Datasets & Benchmarks,NeurIPS,2024,Poster,Anushrut Jignasu;Kelly O. Marshall;Ankush Kumar Mishra;Lucas Nerone Rillo;Baskar Ganapathysubramanian;Aditya Balu;Chinmay Hegde;Adarsh Krishnamurthy,True,https://openreview.net/pdf?id=gad19kaPzb
gg3POFjqq8,Benchmarking Vision Models Under Generative Continuous Nuisance Shifts,"One important challenge in evaluating the robustness of vision models is controlling individual nuisance factors independently.
While some simple synthetic corruptions are commonly applied to existing models, they do not fully capture all realistic and relevant distribution shifts of real-world images.
To overcome this challenge, we apply LoRA adapters to diffusion models to realize a wide range of individual nuisance shifts in a continuous manner. 
While existing generative benchmarks perform manipulations in one step, we argue for gradual and continuous nuisance shifts, as they allow evaluating the sensitivity and failure points of vision models.
With this in mind, we perform a comprehensive large-scale study to evaluate the robustness and generalization of various classifiers under various nuisance shifts. Through carefully-designed comparisons and analysis, we reveal multiple valuable observations: 1) More modern and larger architectures trained on larger datasets tend to be more robust to various nuisance shifts and fail later for larger scales. 
2) Pre-training strategy influences the robustness and fine-tuning a CLIP classifier improves the standard accuracy but deteriorates the robustness.
3) The accuracy drops only account for one dimension of robustness and the failure point analysis should be considered as an additional dimension for robustness evaluation.
We hope our continuous nuisance shift benchmark can provide a new perspective on assessing the robustness of vision models.",Datasets & Benchmarks,NeurIPS,2024,Reject,Olaf Dünkel;Artur Jesslen;Jiahao Xie;Christian Theobalt;Christian Rupprecht;Adam Kortylewski,True,https://openreview.net/pdf?id=gg3POFjqq8
grrefkWEES,Diffusion4D: Fast Spatial-temporal Consistent 4D generation via Video Diffusion Models,"The availability of large-scale multimodal datasets and advancements in diffusion models have significantly accelerated progress in 4D content generation. Most prior approaches rely on multiple images or video diffusion models, utilizing score distillation sampling for optimization or generating pseudo novel views for direct supervision. However, these methods are hindered by slow optimization speeds and multi-view inconsistency issues. Spatial and temporal consistency in 4D geometry has been extensively explored respectively in 3D-aware diffusion models and traditional monocular video diffusion models. Building on this foundation, we propose a strategy to migrate the temporal consistency in video diffusion models to the spatial-temporal consistency required for 4D generation. Specifically, we present a novel framework, \\\\textbf{Diffusion4D}, for efficient and scalable 4D content generation. Leveraging a meticulously curated dynamic 3D dataset, we develop a 4D-aware video diffusion model capable of synthesizing orbital views of dynamic 3D assets. To control the dynamic strength of these assets, we introduce a 3D-to-4D motion magnitude metric as guidance. Additionally, we propose a novel motion magnitude reconstruction loss and 3D-aware classifier-free guidance to refine the learning and generation of motion dynamics. After obtaining orbital views of the 4D asset, we perform explicit 4D construction with Gaussian splatting in a coarse-to-fine manner. Extensive experiments demonstrate that our method surpasses prior state-of-the-art techniques in terms of generation efficiency and 4D geometry consistency across various prompt modalities.",main,NeurIPS,2024,Poster,HANWEN LIANG;Yuyang Yin;Dejia Xu;hanxue liang;Zhangyang Wang;Konstantinos N Plataniotis;Yao Zhao;Yunchao Wei,True,https://openreview.net/pdf?id=grrefkWEES
gvlOQC6oP1,Image Copy Detection for Diffusion Models,"Images produced by diffusion models are increasingly popular in digital artwork and visual marketing. However, such generated images might replicate content from existing ones and pose the challenge of content originality. Existing Image Copy Detection (ICD) models, though accurate in detecting hand-crafted replicas, overlook the challenge from diffusion models. This motivates us to introduce ICDiff, the first ICD specialized for diffusion models. To this end, we construct a Diffusion-Replication (D-Rep) dataset and correspondingly propose a novel deep embedding method. D-Rep uses a state-of-the-art diffusion model (Stable Diffusion V1.5) to generate 40, 000 image-replica pairs, which are manually annotated into 6 replication levels ranging from 0 (no replication) to 5 (total replication). Our method, PDF-Embedding, transforms the replication level of each image-replica pair into a probability density function (PDF) as the supervision signal. The intuition is that the probability of neighboring replication levels should be continuous and smooth. Experimental results show that PDF-Embedding surpasses protocol-driven methods and non-PDF choices on the D-Rep test set. Moreover, by utilizing PDF-Embedding, we find that the replication ratios of well-known diffusion models against an open-source gallery range from 10% to 20%. The project is publicly available at https://icdiff.github.io/.",main,NeurIPS,2024,Poster,Wenhao Wang;Yifan Sun;Zhentao Tan;Yi Yang,True,https://openreview.net/pdf?id=gvlOQC6oP1
gzQARCgIsI,End-To-End Causal Effect Estimation from Unstructured Natural Language Data,"Knowing the effect of an intervention is critical for human decision-making, but current approaches for causal effect estimation rely on manual data collection and structuring, regardless of the causal assumptions. This increases both the cost and time-to-completion for studies. We show how large, diverse observational text data can be mined with large language models (LLMs) to produce inexpensive causal effect estimates under appropriate causal assumptions. We introduce _NATURAL_, a novel family of causal effect estimators built with LLMs that operate over datasets of unstructured text. Our estimators use LLM conditional distributions (over variables of interest, given the text data) to assist in the computation of classical estimators of causal effect. We overcome a number of technical challenges to realize this idea, such as automating data curation and using LLMs to impute missing information. We prepare six (two synthetic and four real) observational datasets, paired with corresponding ground truth in the form of randomized trials, which we used to systematically evaluate each step of our pipeline. NATURAL estimators demonstrate remarkable performance, yielding causal effect estimates that fall within 3 percentage points of their ground truth counterparts, including on real-world Phase 3/4 clinical trials. Our results suggest that unstructured text data is a rich source of causal effect information, and NATURAL is a first step towards an automated pipeline to tap this resource.",main,NeurIPS,2024,Poster,Nikita Dhawan;Leonardo Cotta;Karen Ullrich;Rahul Krishnan;Chris J. Maddison,True,https://openreview.net/pdf?id=gzQARCgIsI
h024LpF3bZ,Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles,"While advancements in NLP have significantly improved the performance of Large Language Models (LLMs) on tasks requiring vertical thinking, their lateral thinking capabilities remain under-explored and challenging to measure due to the complexity of assessing creative thought processes and the scarcity of relevant data. To address these challenges, we introduce SPLAT, a benchmark leveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs. This benchmark, containing 975 graded situation puzzles across three difficulty levels, employs a new multi-turn player-judge framework instead of the traditional model-based evaluation, which often necessitates a stronger evaluation model. This framework simulates an interactive game where the model (player) asks the evaluation model (judge) questions about an incomplete story to infer the full scenario. The judge answers based on a detailed reference scenario or evaluates if the player's predictions align with the reference one. This approach lessens dependence on more robust evaluation models, enabling the assessment of state-of-the-art LLMs. The experiments demonstrate that a robust evaluation model, such as WizardLM-2, closely matches human judgements in both intermediate question-answering and final scenario accuracy, achieving over 80% agreement--similar to the agreement levels among humans. Furthermore, applying data and reasoning processes from our benchmark to other lateral thinking-related benchmarks, e.g., RiddleSense and BrainTeaser, leads to performance enhancements. This suggests that our benchmark effectively evaluates and elicits the lateral thinking abilities of LLMs.",main,NeurIPS,2024,Poster,Qi Chen;Bowen Zhang;Gang Wang;Qi Wu,True,https://openreview.net/pdf?id=h024LpF3bZ
h18O23kQzD,BLURD: Benchmarking and Learning using a Unified Rendering and Diffusion Model,"Recent advancements in pre-trained vision models have made them pivotal in computer vision, emphasizing the need for their thorough evaluation and benchmarking. This evaluation needs to consider various factors of variation, their potential biases, shortcuts, and inaccuracies that ultimately lead to disparate performance in models. Such evaluations are conventionally done using either synthetic data from 2D or 3D rendering software or real-world images in controlled settings. Synthetic methods offer full control and flexibility, while real-world methods are limited by high costs and less adaptability. Moreover, 3D rendering can't yet fully replicate real photography, creating a realism gap.
In this paper, we introduce BLURD--Benchmarking and Learning using a Unified Rendering and Diffusion Model--a novel method combining 3D rendering and Stable Diffusion to bridge this gap in representation learning. With BLURD we create a new family of datasets that allow for the creation of both 3D rendered and photo-realistic images with identical factors. BLURD, therefore, provides deeper insights into the representations learned by various CLIP backbones. The source code for creating the BLURD datasets is available at https://github.com/squaringTheCircle/BLURD",Datasets & Benchmarks,NeurIPS,2024,Poster,Boris Repasky;Ehsan Abbasnejad;Anthony Dick,True,https://openreview.net/pdf?id=h18O23kQzD
h2e4G2YiwR,Action Imitation in Common Action Space for Customized Action Image Synthesis,"We propose a novel method, \\\\textbf{TwinAct}, to tackle the challenge of decoupling actions and actors in order to customize the text-guided diffusion models (TGDMs) for few-shot action image generation. TwinAct addresses the limitations of existing methods that struggle to decouple actions from other semantics (e.g., the actor's appearance) due to the lack of an effective inductive bias with few exemplar images. Our approach introduces a common action space, which is a textual embedding space focused solely on actions, enabling precise customization without actor-related details. Specifically, TwinAct involves three key steps: 1) Building common action space based on a set of representative action phrases; 2) Imitating the customized action within the action space; and 3) Generating highly adaptable customized action images in diverse contexts with action similarity loss. To comprehensively evaluate TwinAct, we construct a novel benchmark, which provides sample images with various forms of actions. Extensive experiments demonstrate TwinAct's superiority in generating accurate, context-independent customized actions while maintaining the identity consistency of different subjects, including animals, humans, and even customized actors.",main,NeurIPS,2024,Poster,Wang Lin;Jingyuan Chen;Jiaxin Shi;Zirun Guo;Yichen Zhu;Zehan Wang;Tao Jin;Zhou Zhao;Fei Wu;Shuicheng YAN;Hanwang Zhang,True,https://openreview.net/pdf?id=h2e4G2YiwR
h3lddsY5nf,SPIQA: A Dataset for Multimodal Question Answering on Scientific Papers,"Seeking answers to questions within long scientific research articles is a crucial area of study that aids readers in quickly addressing their inquiries. However, existing question-answering (QA) datasets based on scientific papers are limited in scale and focus solely on textual content. We introduce SPIQA (Scientific Paper Image Question Answering), the first large-scale QA dataset specifically designed to interpret complex figures and tables within the context of scientific research articles across various domains of computer science. Leveraging the breadth of expertise and ability of multimodal large language models (MLLMs) to understand figures, we employ automatic and manual curation to create the dataset. We craft an information-seeking task on interleaved images and text that involves multiple images covering a wide variety of plots, charts, tables, schematic diagrams, and result visualizations. SPIQA comprises 270K questions divided into training, validation, and three different evaluation splits. Through extensive experiments with 12 prominent foundational models, we evaluate the ability of current multimodal systems to comprehend the nuanced aspects of research articles. Additionally, we propose a Chain-of-Thought (CoT) evaluation strategy with in-context retrieval that allows fine-grained, step-by-step assessment and improves model performance. We further explore the upper bounds of performance enhancement with additional textual information, highlighting its promising potential for future research and the dataset’s impact on revolutionizing how we interact with scientific literature.",Datasets & Benchmarks,NeurIPS,2024,Poster,Shraman Pramanick;Rama Chellappa;Subhashini Venugopalan,True,https://openreview.net/pdf?id=h3lddsY5nf
h7Z2Q36sPk,SynRS3D: A Synthetic Dataset for Global 3D Semantic Understanding from Monocular Remote Sensing Imagery,"Global semantic 3D understanding from single-view high-resolution remote sensing (RS) imagery is crucial for Earth observation (EO). However, this task faces significant challenges due to the high costs of annotations and data collection, as well as geographically restricted data availability. To address these challenges, synthetic data offer a promising solution by being unrestricted and automatically annotatable, thus enabling the provision of large and diverse datasets. We develop a specialized synthetic data generation pipeline for EO and introduce SynRS3D, the largest synthetic RS dataset. SynRS3D comprises 69,667 high-resolution optical images that cover six different city styles worldwide and feature eight land cover types, precise height information, and building change masks. To further enhance its utility, we develop a novel multi-task unsupervised domain adaptation (UDA) method, RS3DAda, coupled with our synthetic dataset, which facilitates the RS-specific transition from synthetic to real scenarios for land cover mapping and height estimation tasks, ultimately enabling global monocular 3D semantic understanding based on synthetic data. Extensive experiments on various real-world datasets demonstrate the adaptability and effectiveness of our synthetic dataset and the proposed RS3DAda method. SynRS3D and related codes are available at https://github.com/JTRNEO/SynRS3D.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Jian Song;Hongruixuan Chen;Weihao Xuan;Junshi Xia;Naoto Yokoya,True,https://openreview.net/pdf?id=h7Z2Q36sPk
h8LuywKj6N,GUI-World: A Dataset for GUI-Orientated Multimodal Large Language Models,"Recently, Multimodal Large Language Models (MLLMs) have been used as agents to control keyboard and mouse inputs by directly perceiving the Graphical User Interface (GUI) and generating corresponding code.
However, current agents primarily exhibit excellent understanding capabilities in static environments and are predominantly applied in relatively simple domains, such as Web or mobile interfaces. We argue that a robust GUI agent should be capable of perceiving temporal information on the GUI, including dynamic Web content and multi-step tasks. Additionally, it should possess a comprehensive understanding across various GUI scenarios, including desktop software and multi-window interactions.
To this end, this paper introduces a new dataset, termed GUI-World, which features meticulously crafted Human-MLLM annotations, extensively covering six GUI scenarios and eight types of GUI-orientated questions in three formats.
We evaluate the capabilities of current state-of-the-art MLLMs, including ImageLLMs and VideoLLMs, in understanding various types of GUI content, especially dynamic and sequential content. Our findings reveal that ImageLLMs struggle with dynamic GUI content without manually annotated keyframes or operation history. On the other hand, VideoLLMs fall short in all GUI-orientated tasks given the sparse of GUI video dataset. Based on GUI-World, we take the initial step of leveraging a fine-tuned VideoLLM as a GUI agent, demonstrating an improved understanding of various GUI tasks. However, due to the limitations in the performance of base LLMs, we conclude that using VideoLLMs as GUI agents remains a significant challenge. We believe our work provides valuable insights for future research in dynamic GUI content understanding.",Datasets & Benchmarks,NeurIPS,2024,Reject,Dongping Chen;Yue Huang;Siyuan Wu;Jingyu Tang;Huichi Zhou;Qihui Zhang;Zhigang He;Yilin Bai;Chujie Gao;Liuyi Chen;Yiqiang Li;Chenlong Wang;Yue Yu;Tianshuo Zhou;Zhen Li;Yi Gui;Yao Wan;Pan Zhou;Jianfeng Gao;Lichao Sun,True,https://openreview.net/pdf?id=h8LuywKj6N
hFDdSd6hSM,Do Counterfactually Fair Image Classifiers Satisfy Group Fairness? -- A Theoretical and Empirical Study,"The notion of algorithmic fairness has been actively explored from various aspects of fairness, such as counterfactual fairness (CF) and group fairness (GF). However, the exact relationship between CF and GF remains to be unclear, especially in image classification tasks; the reason is because we often cannot collect counterfactual samples regarding a sensitive attribute, essential for evaluating CF, from the existing images (e.g., a photo of the same person but with different secondary sex characteristics). In this paper, we construct new image datasets for evaluating CF by using a high-quality image editing method and carefully labeling with human annotators. Our datasets, CelebA-CF and LFW-CF, build upon the popular image GF benchmarks; hence, we can evaluate CF and GF simultaneously. We empirically observe that CF does not imply GF in image classification, whereas previous studies on tabular datasets observed the opposite. We theoretically show that it could be due to the existence of a latent attribute $G$ that is correlated with, but not caused by, the sensitive attribute (e.g., secondary sex characteristics are highly correlated with hair length). From this observation, we propose a simple baseline,  Counterfactual Knowledge Distillation (CKD), to mitigate such correlation with the sensitive attributes. Extensive experimental results on CelebA-CF and LFW-CF demonstrate that CF-achieving models satisfy GF if we successfully reduce the reliance on $G$ (e.g., using CKD).",Datasets & Benchmarks,NeurIPS,2024,Poster,Sangwon Jung;Sumin Yu;Sanghyuk Chun;Taesup Moon,True,https://openreview.net/pdf?id=hFDdSd6hSM
hFVpqkRRH1,Web2Code: A Large-scale Webpage-to-Code Dataset and Evaluation Framework for Multimodal LLMs,"Multimodal large language models (MLLMs) have shown impressive success across modalities such as image, video, and audio in a variety of understanding and generation tasks.
  However, current MLLMs are surprisingly poor at understanding webpage screenshots and generating their corresponding HTML code.
  To address this problem, 
  we propose Web2Code, a benchmark consisting of a new large-scale webpage-to-code dataset for instruction tuning and an evaluation framework for the webpage understanding and HTML code translation abilities of MLLMs. 
  For dataset construction, we leverage pretrained LLMs to enhance existing webpage-to-code datasets as well as generate a diverse pool of new webpages rendered into images.
  Specifically, the inputs are webpage images and instructions, while the responses are the webpage's HTML code.
  We further include diverse natural language QA pairs about the webpage content in the responses to enable a more comprehensive understanding of the web content.
  To evaluate model performance in these tasks, we develop an evaluation framework for testing MLLMs' abilities in webpage understanding and web-to-code generation.
  Extensive experiments show that our proposed dataset is beneficial not only to our proposed tasks but also in the general visual domain.
  We hope our work will contribute to the development of general MLLMs suitable for web-based content generation and task automation.
  Our data and code are available at https://github.com/MBZUAI-LLM/web2code.",Datasets & Benchmarks,NeurIPS,2024,Poster,Sukmin Yun;Haokun Lin;Rusiru Thushara;Mohammad Qazim Bhat;Yongxin Wang;Zutao Jiang;Mingkai Deng;Jinhong Wang;Tianhua Tao;Junbo Li;Haonan Li;Preslav Nakov;Timothy Baldwin;Zhengzhong Liu;Eric P. Xing;Xiaodan Liang;Zhiqiang Shen,True,https://openreview.net/pdf?id=hFVpqkRRH1
hHA9qrGZBe,HARMONIC: Harnessing LLMs for Tabular Data Synthesis and Privacy Protection,"Data serves as the fundamental basis for advancing deep learning. The tabular data presented in a structured format is highly valuable for modeling and training.
However, even in the era of LLM, obtaining tabular data from sensitive domains remains a challenge due to privacy or copyright concerns. 
Therefore, exploring the methods for effectively using models like LLMs to generate synthetic tabular data, which is privacy-preserving but similar to original one, is urgent.
In this paper, we introduce a new framework HARMONIC for tabular data generation and evaluation by LLMs. In the data generation part of our framework, we employ fine-tuning to generate tabular data and enhance privacy rather than continued pre-training which is often used by previous small-scale LLM-based methods. In particular, we construct an instruction fine-tuning dataset based on the idea of the k-nearest neighbors algorithm to inspire LLMs to discover inter-row relationships. By such fine-tuning, LLMs are trained to remember the format and connections of the data rather than the data itself, which reduces the risk of privacy leakage. The experiments find that our tabular data generation achieves equivalent performance as existing methods but with better privacy by the metric of MLE, DCR, etc.
In the evaluation part of our framework, we develop a specific privacy risk metric DLT for LLM synthetic data generation, which quantifies the extent to which the generator itself leaks data. We also developed LLE, a performance evaluation metric for downstream LLM tasks, which is more practical and credible than previous metrics.
The experiments show that our data generation method outperform the previous methods in the metrics DLT and LLE.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yuxin Wang;Duanyu Feng;Yongfu Dai;Zhengyu Chen;Jimin Huang;Sophia Ananiadou;Qianqian Xie;Hao Wang,True,https://openreview.net/pdf?id=hHA9qrGZBe
hMj6jZ6JWU,Empowering and Assessing the Utility of Large Language Models in Crop Science,"Large language models (LLMs) have demonstrated remarkable efficacy across knowledge-intensive tasks. Nevertheless, their untapped potential in crop science presents an opportunity for advancement. To narrow this gap, we introduce CROP, which includes a novel instruction tuning dataset specifically designed to enhance LLMs’ professional capabilities in the crop science sector, along with a benchmark that serves as a comprehensive evaluation of LLMs’ understanding of the domain knowledge. The CROP dataset is curated through a task-oriented and LLM-human integrated pipeline, comprising 210,038 single-turn and 1,871 multi-turn dialogues related to crop science scenarios. The CROP benchmark includes 5,045 multiple-choice questions covering three difficulty levels. Our experiments based on the CROP benchmark demonstrate notable enhancements in crop science-related tasks when LLMs are fine-tuned with the CROP dataset. To the best of our knowledge, CROP dataset is the first-ever instruction tuning dataset in the crop science domain. We anticipate that CROP will accelerate the adoption of LLMs in the domain of crop science, ultimately contributing to global food production.",Datasets & Benchmarks,NeurIPS,2024,Poster,Hang Zhang;Jiawei Sun;Renqi Chen;Wei Liu;Zhonghang Yuan;Xinzhe Zheng;Zhefan Wang;Zhiyuan Yang;Hang Yan;Han-Sen Zhong;Xiqing Wang;Wanli Ouyang;Fan Yang;Nanqing Dong,True,https://openreview.net/pdf?id=hMj6jZ6JWU
hORTHzt2cE,"RoleAgent: Building, Interacting, and Benchmarking High-quality Role-Playing Agents from Scripts","Believable agents can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication. Recently, generative agents have been proposed to simulate believable human behavior by using Large Language Models. However, the existing method heavily relies on human-annotated agent profiles (e.g., name, age, personality, relationships with others, and so on) for the initialization of each agent, which cannot be scaled up easily. In this paper, we propose a scalable RoleAgent framework to generate high-quality role-playing agents from raw scripts, which includes building and interacting stages. Specifically, in the building stage, we use a hierarchical memory system to extract and summarize the structure and high-level information of each agent for the raw script. In the interacting stage, we propose a novel innovative mechanism with four steps to achieve a high-quality interaction between agents. Finally, we introduce a systematic and comprehensive evaluation benchmark called RoleAgentBench to evaluate the effectiveness of our RoleAgent, which includes 100 and 28 roles for 20 English and 5 Chinese scripts, respectively. Extensive experimental results on RoleAgentBench demonstrate the effectiveness of RoleAgent.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jiaheng Liu;Zehao Ni;Haoran Que;Tao Sun;Noah Wang;Jian Yang;JiakaiWang;Hongcheng Guo;Z.Y. Peng;Ge Zhang;Jiayi Tian;Xingyuan Bu;Ke Xu;Wenge Rong;Junran Peng;Zhaoxiang Zhang,True,https://openreview.net/pdf?id=hORTHzt2cE
hQQyetmOxs,A Systematic Review of NeurIPS Dataset Management Practices,"As new machine learning methods demand larger training datasets, researchers and developers face significant challenges in dataset management. Although ethics reviews, documentation, and checklists have been established, it remains uncertain whether consistent dataset management practices exist across the community. This lack of a comprehensive overview hinders our ability to diagnose and address fundamental tensions and ethical issues related to managing large datasets. We present a systematic review of datasets published at the NeurIPS Datasets and Benchmarks track, focusing on four key aspects: provenance, distribution, ethical disclosure, and licensing. Our findings reveal that dataset provenance is often unclear due to ambiguous filtering and curation processes. Additionally, a variety of sites are used for dataset hosting, but only a few offer structured metadata and version control. These inconsistencies underscore the urgent need for standardized data infrastructures for the publication and management of datasets.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yiwei Wu;Leah Hope Ajmani;Shayne Longpre;Hanlin Li,False,https://openreview.net/pdf?id=hQQyetmOxs
hSAu90mDkC,ViLCo-Bench: VIdeo Language COntinual learning Benchmark,"Video language continual learning involves continuously adapting to information from video and text inputs, enhancing a model’s ability to handle new tasks while retaining prior knowledge. This field is a relatively under-explored area, and establishing appropriate datasets is crucial for facilitating communication and research in this field. In this study, we present the first dedicated benchmark, ViLCo-Bench, designed to evaluate continual learning models across a range of video-text tasks. The dataset comprises ten-minute-long videos and corresponding language queries collected from publicly available datasets. Additionally, we introduce a novel memory-efficient framework that incorporates self-supervised learning and mimics long-term and short-term memory effects. This framework addresses challenges including memory complexity from long video clips, natural language complexity from open queries, and text-video misalignment. We posit that ViLCo-Bench, with greater complexity compared to existing continual learning benchmarks, would serve as a critical tool for exploring the video-language domain, extending beyond conventional class-incremental tasks, and addressing complex and limited annotation issues. The curated data, evaluations, and our novel method are available at https://github.com/cruiseresearchgroup/ViLCo.",Datasets & Benchmarks,NeurIPS,2024,Poster,Tianqi Tang;Shohreh Deldari;Hao Xue;Celso M de Melo;Flora D. Salim,True,https://openreview.net/pdf?id=hSAu90mDkC
haVPmN8UGi,GraphVis: Boosting LLMs with Visual Knowledge Graph Integration,"The rapid evolution of large language models (LLMs) has expanded their capabilities across various data modalities, extending from well-established image data to increasingly popular graph data. Given the limitation of LLMs in hallucinations and inaccuracies in recalling factual knowledge, Knowledge Graph (KG) has emerged as a crucial data modality to support more accurate reasoning by LLMs. However, integrating structured knowledge from KGs into LLMs remains challenging, as most current KG-enhanced LLM methods directly convert the KG into linearized text triples, which is not as expressive as the original structured data. To address this, we introduce GraphVis, which conserves the intricate graph structure through the visual modality to enhance the comprehension of KGs with the aid of Large Vision Language Models (LVLMs). Our approach incorporates a unique curriculum fine-tuning scheme which first instructs LVLMs to recognize basic graphical features from the images, and subsequently incorporates reasoning on QA tasks with the visual graphs. This cross-modal methodology not only markedly enhances performance on standard textual QA  but also shows improved zero-shot VQA performance by utilizing synthetic graph images to augment the data for VQA tasks. We present comprehensive evaluations across commonsense reasoning QA benchmarks, where GraphVis provides an average improvement of 11.1% over its base model and outperforms existing KG-enhanced LLM approaches. Across VQA benchmarks such as ScienceQA that share similar scientific diagram images, GraphVis provides a notable gain of 4.32%.",main,NeurIPS,2024,Poster,Yihe Deng;Chenchen Ye;Zijie Huang;Mingyu Derek Ma;Yiwen Kou;Wei Wang,True,https://openreview.net/pdf?id=haVPmN8UGi
hcOq2buakM,"BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices","AI models are increasingly prevalent in high-stakes environments, necessitating thorough assessment of their capabilities and risks. Benchmarks are popular for measuring these attributes and for comparing model performance, tracking progress, and identifying weaknesses in foundation and non-foundation models. They can inform model selection for downstream tasks and influence policy initiatives. However, not all benchmarks are the same: their quality depends on their design and usability. In this paper, we develop an assessment framework considering 40 best practices across a benchmark's life cycle and evaluate 25 AI benchmarks against it. We find that there exist large quality differences and that commonly used benchmarks suffer from significant issues. We further find that most benchmarks do not report statistical significance of their results nor can results be easily replicated. To support benchmark developers in aligning with best practices, we provide a checklist for minimum quality assurance based on our assessment. We also develop a living repository of benchmark assessments to support benchmark comparability.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Anka Reuel;Amelia Hardy;Chandler Smith;Max Lamparth;Malcolm Hardy;Mykel Kochenderfer,False,https://openreview.net/pdf?id=hcOq2buakM
hceKrY4dfC,Indoor Air Quality Dataset with Activities of Daily Living in Low to Middle-income Communities,"In recent years, indoor air pollution has posed a significant threat to our society, claiming over 3.2 million lives annually. Developing nations, such as India, are most affected since lack of knowledge, inadequate regulation, and outdoor air pollution lead to severe daily exposure to pollutants. However, only a limited number of studies have attempted to understand how indoor air pollution affects developing countries like India. To address this gap, we present spatiotemporal measurements of air quality from 30 indoor sites over six months during summer and winter seasons. The sites are geographically located across four regions of type: rural, suburban, and urban, covering the typical low to middle-income population in India. The dataset contains various types of indoor environments (e.g., studio apartments, classrooms, research laboratories, food canteens, and residential households), and can provide the basis for data-driven learning model research aimed at coping with unique pollution patterns in developing countries. This unique dataset demands advanced data cleaning and imputation techniques for handling missing data due to power failure or network outages during data collection. Furthermore, through a simple speech-to-text application, we provide real-time indoor activity labels annotated by occupants. Therefore, environmentalists and ML enthusiasts can utilize this dataset to understand the complex patterns of the pollutants under different indoor activities, identify recurring sources of pollution, forecast exposure, improve floor plans and room structures of modern indoor designs, develop pollution-aware recommender systems, etc.",Datasets & Benchmarks,NeurIPS,2024,Poster,Prasenjit Karmakar;Swadhin Pradhan;Sandip Chakraborty,True,https://openreview.net/pdf?id=hceKrY4dfC
hej9QGCHT6,DenseFusion-1M: Merging Vision Experts for Comprehensive Multimodal Perception,"Existing Multimodal Large Language Models (MLLMs) increasingly emphasize complex understanding of various visual elements, including multiple objects, text information, spatial relations. Their development for comprehensive visual perception hinges on the availability of high-quality image-text datasets that offer diverse visual elements and throughout image descriptions. However, the scarcity of such hyper-detailed datasets currently hinders progress within the MLLM community. The bottleneck stems from the limited perceptual capabilities of current caption engines, which fall short in providing complete and accurate annotations. To facilitate the cutting-edge research of MLLMs on comprehensive vision perception, we thereby propose Perceptual Fusion, using a low-budget but highly effective caption engine for complete and accurate image descriptions.  Specifically, Perceptual Fusion integrates diverse perception experts as image priors to provide explicit information on visual elements and adopts an efficient MLLM as a centric pivot to mimic advanced MLLMs' perception abilities. We carefully select 1M highly representative images from uncurated LAION dataset and generate dense descriptions using our engine, dubbed DenseFusion-1M. Extensive experiments validate that our engine outperforms its counterparts, where the resulting dataset significantly improves the perception and cognition abilities of existing MLLMs across diverse vision-language benchmarks, especially with high-resolution images as inputs. The code and dataset are available at https://huggingface.co/datasets/BAAI/DenseFusion-1M.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xiaotong Li;Fan Zhang;Haiwen Diao;Yueze Wang;Xinlong Wang;LINGYU DUAN,True,https://openreview.net/pdf?id=hej9QGCHT6
hgl4dYE76J,BuckTales: A multi-UAV dataset for multi-object tracking and re-identification of wild antelopes,"Understanding animal behaviour is central to predicting, understanding, and miti-
gating impacts of natural and anthropogenic changes on animal populations and
ecosystems. However, the challenges of acquiring and processing long-term, eco-
logically relevant data in wild settings have constrained the scope of behavioural
research. The increasing availability of Unmanned Aerial Vehicles (UAVs), cou-
pled with advances in machine learning, has opened new opportunities for wildlife
monitoring using aerial tracking. However, the limited availability of datasets with wild
animals in natural habitats has hindered progress in automated computer vision
solutions for long-term animal tracking. Here, we introduce the first large-scale
UAV dataset designed to solve multi-object tracking (MOT) and re-identification
(Re-ID) problem in wild animals, specifically the mating behaviour (or lekking) of
blackbuck antelopes. Collected in collaboration with biologists, the MOT dataset
includes over 1.2 million annotations including 680 tracks across 12 high-resolution
(5.4K) videos, each averaging 66 seconds and featuring 30 to 130 individuals. The
Re-ID dataset includes 730 individuals captured with two UAVs simultaneously.
The dataset is designed to drive scalable, long-term animal behavior tracking using
multiple camera sensors. By providing baseline performance with two detectors,
and benchmarking several state-of-the-art tracking methods, our dataset reflects the
real-world challenges of tracking wild animals in socially and ecologically relevant
contexts. In making these data widely available, we hope to catalyze progress in
MOT and Re-ID for wild animals, fostering insights into animal behaviour, conser-
vation efforts, and ecosystem dynamics through automated, long-term monitoring.",Datasets & Benchmarks,NeurIPS,2024,Poster,Hemal Naik;Junran Yang;Dipin Das;Margaret C Crofoot;Akanksha Rathore;Vivek H Sridhar,True,https://openreview.net/pdf?id=hgl4dYE76J
hqvWcQ3uzF,FT-AED: Benchmark Dataset for Early Freeway Traffic Anomalous Event Detection,"Early and accurate detection of anomalous events on the freeway, such as accidents, can improve emergency response and clearance. However, existing delays and mistakes from manual crash reporting records make it a difficult problem to solve. Current large-scale freeway traffic datasets are not designed for anomaly detection and ignore these challenges. In this paper, we introduce the first large-scale lane-level freeway traffic dataset for anomaly detection. Our dataset consists of a month of weekday radar detection sensor data collected in 4 lanes along an 18-mile stretch of Interstate 24 heading toward Nashville, TN, comprising over 3.7 million sensor measurements. We also collect official crash reports from the Tennessee Department of Transportation Traffic Management Center and manually label all other potential anomalies in the dataset. To show the potential for our dataset to be used in future machine learning and traffic research, we benchmark numerous deep learning anomaly detection models on our dataset. We find that unsupervised graph neural network autoencoders are a promising solution for this problem and that ignoring spatial relationships leads to decreased performance. We demonstrate that our methods can reduce reporting delays by over 10 minutes on average while detecting 75% of crashes. Our dataset and all preprocessing code needed to get started are publicly released at https://vu.edu/ft-aed/ to facilitate future research.",Datasets & Benchmarks,NeurIPS,2024,Poster,Austin Coursey;JUNYI JI;Marcos Quinones Grueiro;William Barbour;Yuhang Zhang;Tyler Derr;Gautam Biswas;Daniel Work,True,https://openreview.net/pdf?id=hqvWcQ3uzF
hwbRjslR5N,Visual Riddles: a Commonsense and World Knowledge Challenge for Large Vision and Language Models,"Imagine observing someone scratching their arm; to understand why, additional context would be necessary. However, spotting a mosquito nearby would immediately offer a likely explanation for the person’s discomfort, thereby alleviating the need for further information. This example illustrates how subtle visual cues can challenge our cognitive skills and demonstrates the complexity of interpreting visual scenarios. To study these skills, we present Visual Riddles, a benchmark aimed to test vision and language models on visual riddles requiring commonsense and world knowledge. The benchmark comprises 400 visual riddles, each featuring a unique image created by a variety of text-to-image models, question, ground-truth answer, textual hint, and attribution. Human evaluation reveals that existing models lag significantly behind human performance, which is at 82% accuracy, with Gemini-Pro-1.5 leading with 40% accuracy. Our benchmark comes with automatic evaluation tasks to make assessment scalable. These findings underscore the potential of Visual Riddles as a valuable resource for enhancing vision and language models’ capabilities in interpreting complex visual scenarios. Data, code, and leaderboard are available at https://visual-riddles.github.io/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Nitzan Bitton Guetta;Aviv Slobodkin;Aviya Maimon;Eliya Habba;Royi Rassin;Yonatan Bitton;Idan Szpektor;Amir Globerson;Yuval Elovici,True,https://openreview.net/pdf?id=hwbRjslR5N
i4jZ6fCDdy,Learning to Predict Structural Vibrations,"In mechanical structures like airplanes, cars and houses, noise is generated and transmitted through vibrations. To take measures to reduce this noise, vibrations need to be simulated with expensive numerical computations. Deep learning surrogate models present a promising alternative to classical numerical simulations as they can be evaluated magnitudes faster, while trading-off accuracy. To quantify such trade-offs systematically and foster the development of methods, we present a benchmark on the task of predicting the vibration of harmonically excited plates. The benchmark features a total of 12,000 plate geometries with varying forms of beadings, material, boundary conditions, load position and sizes with associated numerical solutions. 
To address the benchmark task, we propose a new network architecture, named \\\\modelname, which predicts vibration patterns of plate geometries given a specific excitation frequency. Applying principles from operator learning and implicit models for shape encoding, our approach effectively addresses the prediction of highly variable frequency response functions occurring in dynamic systems. To quantify the prediction quality, we introduce a set of evaluation metrics and evaluate the method on our vibrating-plates benchmark. Our method outperforms DeepONets, Fourier Neural Operators and more traditional neural network architectures and can be used for design optimization.
Code, dataset and visualizations: https://github.com/ecker-lab/Learning_Vibrating_Plates",main,NeurIPS,2024,Poster,Jan van Delden;Julius Schultz;Christopher Blech;Sabine C. Langer;Timo Lüddecke,True,https://openreview.net/pdf?id=i4jZ6fCDdy
i92eyFCQHC,WildVision: Evaluating Vision-Language Models in the Wild with Human Preferences,"Recent breakthroughs in vision-language models (VLMs) emphasize the necessity of benchmarking human preferences in real-world multimodal interactions. To address this gap, we launched WildVision-Arena (WV-Arena), an online platform that collects human preferences to evaluate VLMs. We curated WV-Bench by selecting 500 high-quality samples from 8,000 user submissions in WV-Arena. WV-Bench uses GPT-4 as the judge to compare each VLM with Claude-3-Sonnet, achieving a Spearman correlation of 0.94 with the WV-Arena Elo. This significantly outperforms other benchmarks like MMVet, MMMU, and MMStar.

Our comprehensive analysis of 20K real-world interactions reveals important insights into the failure cases of top-performing VLMs. For example, we find that although GPT-4V surpasses many other models like Reka-Flash, Opus, and Yi-VL-Plus in simple visual recognition and reasoning tasks, it still faces challenges with subtle contextual cues, spatial reasoning, visual imagination, and expert domain knowledge. Additionally, current VLMs exhibit issues with hallucinations and safety when intentionally provoked. We are releasing our chat and feedback data to further advance research in the field of VLMs.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yujie Lu;Dongfu Jiang;Wenhu Chen;William Yang Wang;Yejin Choi;Bill Yuchen Lin,True,https://openreview.net/pdf?id=i92eyFCQHC
iACMjECRjV,ConceptFactory: Facilitate 3D Object Knowledge Annotation with Object Conceptualization,"We present ConceptFactory, a novel scope to facilitate more efficient annotation of 3D object knowledge by recognizing 3D objects through generalized concepts (i.e. object conceptualization), aiming at promoting machine intelligence to learn comprehensive object knowledge from both vision and robotics aspects. This idea originates from the findings in human cognition research that the perceptual recognition of objects can be explained as a process of arranging generalized geometric components (e.g. cuboids and cylinders). ConceptFactory consists of two critical parts: i) ConceptFactory Suite, a unified toolbox that adopts Standard Concept Template Library (STL-C) to drive a web-based platform for object conceptualization, and ii) ConceptFactory Asset, a large collection of conceptualized objects acquired using ConceptFactory suite. Our approach enables researchers to effortlessly acquire or customize extensive varieties of object knowledge to comprehensively study different object understanding tasks. We validate our idea on a wide range of benchmark tasks from both vision and robotics aspects with state-of-the-art algorithms, demonstrating the high quality and versatility of annotations provided by our approach. Our website is available at https://apeirony.github.io/ConceptFactory.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jianhua Sun;Yuxuan Li;Longfei Xu;Nange Wang;Jiude Wei;Yining Zhang;Cewu Lu,True,https://openreview.net/pdf?id=iACMjECRjV
iAkhPz7Qt3,Scaling Retrieval-Based Language Models with a Trillion-Token Datastore,"Scaling laws with respect to the amount of training data and the number of parameters allow us to predict the cost-benefit trade-offs of pretraining language models (LMs) in different configurations. In this paper, we consider another dimension of scaling: the amount of data available at inference time. Specifically, we find that increasing the size of the datastore used by a retrieval-based LM monotonically improves language modeling and several downstream tasks without obvious saturation, such that a smaller model augmented with a large datastore outperforms a larger LM-only model on knowledge-intensive tasks. By plotting compute-optimal scaling curves with varied datastore, model, and pretraining data sizes, we show that using larger datastores can significantly improve model performance for the same training compute budget. We carry out our study by constructing a 1.4 trillion-token datastore named MassiveDS, which is the largest and the most diverse open-sourced datastore for retrieval-based LMs to date, and designing an efficient pipeline for studying datastore scaling in an accessible manner. Finally, we analyze the effect of improving the retriever, datastore quality filtering, and other design choices on our observed scaling trends. Overall, our results show that datastore size should be considered as an integral part of LM efficiency and performance trade-offs. To facilitate future research, we open-source our datastore and code at https://github.com/RulinShao/retrieval-scaling.",main,NeurIPS,2024,Poster,Rulin Shao;Jacqueline He;Akari Asai;Weijia Shi;Tim Dettmers;Sewon Min;Luke Zettlemoyer;Pang Wei Koh,True,https://openreview.net/pdf?id=iAkhPz7Qt3
iDg6ktCf6W,PROSPECT PTMs: Rich Labeled Tandem Mass Spectrometry Dataset of Modified Peptides for Machine Learning in Proteomics,"Post-Translational Modifications (PTMs) are changes that occur in proteins after synthesis, influencing their structure, function, and cellular behavior. PTMs are essential in cell biology; they regulate protein function and stability, are involved in various cellular processes, and are linked to numerous diseases. A particularly interesting class of PTMs are chemical modifications such as phosphorylation introduced on amino acid side chains because they can drastically alter the physicochemical properties of the peptides once they are present. One or more PTMs can be attached to each amino acid of the peptide sequence. The most commonly applied technique to detect PTMs on proteins is bottom-up Mass Spectrometry-based proteomics (MS), where proteins are digested into peptides and subsequently analyzed using Tandem Mass Spectrometry (MS/MS). While an increasing number of machine learning models are published focusing on MS/MS-related property prediction of unmodified peptides, high-quality reference data for modified peptides is missing, impeding model development for this important class of peptides. To enable researchers to train machine learning models that can accurately predict the properties of modified peptides, we introduce four high-quality labeled datasets for applying machine and deep learning to tasks in MS-based proteomics. The four datasets comprise several subgroups of peptides with 1.2 million unique modified peptide sequences and 30 unique pairs of (amino-acid, PTM), covering both experimentally introduced and naturally occurring modifications on various amino acids. We evaluate the utility and importance of the dataset by providing benchmarking results on models trained with and without modifications and highlighting the impact of including modified sequences on downstream tasks. We demonstrate that predicting the properties of modified peptides is more challenging but has a broad impact since they are often the core of protein functionality and its regulation, and they have a potential role as biomarkers in clinical applications. Our datasets contribute to applied machine learning in proteomics by enabling the research community to experiment with methods to encode PTMs as model inputs and to benchmark against reference data for model comparison. With a proper data split for three common tasks in proteomics, we provide a robust way to evaluate model performance and assess generalization on unseen modified sequences.",Datasets & Benchmarks,NeurIPS,2024,Poster,Wassim Gabriel;Omar Shouman;Eva Ayla Schröder;Florian Bößl;Mathias Wilhelm,True,https://openreview.net/pdf?id=iDg6ktCf6W
iEN2linUr8,II-Bench: An Image Implication Understanding Benchmark for Multimodal Large Language Models,"The rapid advancements in the development of multimodal large language models (MLLMs) have consistently led to new breakthroughs on various benchmarks. In response, numerous challenging and comprehensive benchmarks have been proposed to more accurately assess the capabilities of MLLMs. However, there is a dearth of exploration of the higher-order perceptual capabilities of MLLMs. To fill this gap, we propose the Image Implication understanding Benchmark, II-Bench, which aims to evaluate the model's higher-order perception of images. Through extensive experiments on II-Bench across multiple MLLMs, we have made significant findings. Initially, a substantial gap is observed between the performance of MLLMs and humans on II-Bench. The pinnacle accuracy of MLLMs attains 74.8%, whereas human accuracy averages 90%, peaking at an impressive 98%. Subsequently, MLLMs perform worse on abstract and complex images, suggesting limitations in their ability to understand high-level semantics and capture image details. Finally, it is observed that most models exhibit enhanced accuracy when image sentiment polarity hints are incorporated into the prompts. This observation underscores a notable deficiency in their inherent understanding of image sentiment. We believe that II-Bench will inspire the community to develop the next generation of MLLMs, advancing the journey towards expert  artificial general intelligence (AGI). II-Bench is publicly available at https://huggingface.co/datasets/m-a-p/II-Bench.",Datasets & Benchmarks,NeurIPS,2024,Poster,Ziqiang Liu;Feiteng Fang;Xi Feng;Xeron Du;Chenhao Zhang;Noah Wang;yuelin bai;Qixuan Zhao;Liyang Fan;CHENGGUANG GAN;Hongquan Lin;Jiaming Li;Yuansheng Ni;Haihong Wu;Yaswanth Narsupalli;Zhigang Zheng;Chengming Li;Xiping Hu;Ruifeng Xu;Xiaojun Chen;Min Yang;Jiaheng Liu;Ruibo Liu;Wenhao Huang;Ge Zhang;Shiwen Ni,True,https://openreview.net/pdf?id=iEN2linUr8
iGeJxHqnbx,QCircuitNet: A Large-Scale Hierarchical Dataset for Quantum Algorithm Design,"Quantum computing is an emerging field recognized for the significant speedup it offers over classical computing through quantum algorithms. However, designing and implementing quantum algorithms pose challenges due to the complex nature of quantum mechanics and the necessity for precise control over quantum states. To address these challenges, we leverage AI to simplify and enhance the process.  Despite the significant advancements in AI, there has been a lack of datasets specifically tailored for this purpose.  

In this work, we introduce QCircuitNet, a benchmark and test dataset designed to evaluate AI’s capability in designing and implementing quantum algorithms in the form of quantum circuit codes. Unlike traditional AI code writing, this task is fundamentally different and significantly more complicated due to the highly flexible design space and the extreme demands for intricate manipulation of qubits.

Our key contributions include: 
1. The first comprehensive, structured universal quantum algorithm dataset.
2. A framework which formulates the task of quantum algorithm design for Large Language Models (LLMs), providing guidelines for expansion and potential evolution into a training dataset.
3. Automatic validation and verification functions, allowing for scalable and efficient evaluation methodologies.
4. A fair and stable benchmark that avoids data contamination, a particularly critical issue in quantum computing datasets.

Our work aims to bridge the gap in available resources for AI-driven quantum algorithm design, offering a robust and scalable method for evaluating and improving AI models in this field. As we expand the dataset to include more algorithms and explore novel fine-tuning methods, we hope it will significantly contribute to both quantum algorithm design and implementation.",Datasets & Benchmarks,NeurIPS,2024,Reject,Rui Yang;Yuntian Gu;Ziruo Wang;Yitao Liang;Tongyang Li,True,https://openreview.net/pdf?id=iGeJxHqnbx
iJAOpsXo2I,CTIBench: A Benchmark for Evaluating LLMs in Cyber Threat Intelligence,"Cyber threat intelligence (CTI) is crucial in today's cybersecurity landscape, providing essential insights to understand and mitigate the ever-evolving cyber threats. The recent rise of Large Language Models (LLMs) have shown potential in this domain, but concerns about their reliability, accuracy, and hallucinations persist. While existing benchmarks provide general evaluations of LLMs, there are no benchmarks that address the practical and applied aspects of CTI-specific tasks. To bridge this gap, we introduce CTIBench, a benchmark designed to assess LLMs' performance in CTI applications. CTIBench includes multiple datasets focused on evaluating knowledge acquired by LLMs in the cyber-threat landscape. Our evaluation of several state-of-the-art models on these tasks provides insights into their strengths and weaknesses in CTI contexts, contributing to a better understanding of LLM capabilities in CTI.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Md Tanvirul Alam;Dipkamal Bhusal;Le Nguyen;Nidhi Rastogi,True,https://openreview.net/pdf?id=iJAOpsXo2I
iMtAjdGh1U,BenchX: A Unified Benchmark Framework for Medical Vision-Language Pretraining on Chest X-Rays,"Medical Vision-Language Pretraining (MedVLP) shows promise in learning generalizable and transferable visual representations from paired and unpaired medical images and reports. MedVLP can provide useful features to downstream tasks and facilitate adapting task-specific models to new setups using fewer examples. However, existing MedVLP methods often differ in terms of datasets, preprocessing, and finetuning implementations. This pose great challenges in evaluating how well a MedVLP method generalizes to various clinically-relevant tasks due to the lack of unified, standardized, and comprehensive benchmark. To fill this gap, we propose BenchX, a unified benchmark framework that enables head-to-head comparison and systematical analysis between MedVLP methods using public chest X-ray datasets. Specifically, BenchX is composed of three components: 1) Comprehensive datasets covering nine datasets and four medical tasks; 2) Benchmark suites to standardize data preprocessing, train-test splits, and parameter selection; 3) Unified finetuning protocols that accommodate heterogeneous MedVLP methods for consistent task adaptation in classification, segmentation, and report generation, respectively. Utilizing BenchX, we establish baselines for nine state-of-the-art MedVLP methods and found that the performance of some early MedVLP methods can be enhanced to surpass more recent ones, prompting a revisiting of the developments and conclusions from prior works in MedVLP. Our code are available at https://github.com/yangzhou12/BenchX.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yang Zhou;Tan Li Hui Faith;Yanyu Xu;Sicong Leng;Xinxing Xu;Yong Liu;Rick Siow Mong Goh,False,https://openreview.net/pdf?id=iMtAjdGh1U
iNB4uoFQJb,Easy2Hard-Bench: Standardized Difficulty Labels for Profiling LLM Performance and Generalization,"Despite the abundance of datasets available for assessing large language models (LLMs), the scarcity of continuous and reliable difficulty labels for individual data points, in most cases, curtails their capacity to benchmark model generalization performance across different levels of complexity. Addressing this limitation, we present Easy2Hard, an innovative collection of 6 benchmark datasets featuring standardized difficulty labels spanning a wide range of domains, such as mathematics and programming problems, chess puzzles, and reasoning questions, providing a much-needed tool for those in demand of a dataset with varying degrees of difficulty for LLM assessment. We estimate the difficulty of individual problems by leveraging the performance data of many human subjects and LLMs on prominent leaderboards. Harnessing the rich human performance data, we employ widely recognized difficulty ranking systems, including the Item Response Theory (IRT) and Glicko-2 models, to uniformly assign difficulty scores to problems. The Easy2Hard datasets distinguish themselves from previous collections by incorporating a significantly higher proportion of challenging problems, presenting a novel and demanding test for state-of-the-art LLMs. Through extensive experiments conducted with six state-of-the-art LLMs on the Easy2Hard datasets, we offer valuable insights into their performance and generalization capabilities across varying degrees of difficulty, setting the stage for future research in LLM generalization.",Datasets & Benchmarks,NeurIPS,2024,Poster,Mucong Ding;Chenghao Deng;Jocelyn Choo;Zichu Wu;Aakriti Agrawal;Avi Schwarzschild;Tianyi Zhou;Tom Goldstein;John Langford;Anima Anandkumar;Furong Huang,True,https://openreview.net/pdf?id=iNB4uoFQJb
iNYrB3ip9F,Learning Superconductivity from Ordered and Disordered Material Structures,"Superconductivity is a fascinating phenomenon observed in certain materials under certain conditions. However, some critical aspects of it, such as the relationship between superconductivity and materials' chemical/structural features, still need to be understood. Recent successes of data-driven approaches in material science strongly inspire researchers to study this relationship with them, but a corresponding dataset is still lacking. Hence, we present a new dataset for data-driven approaches, namely SuperCon3D, containing both 3D crystal structures and experimental superconducting transition temperature (Tc) for the first time. Based on SuperCon3D, we propose two deep learning methods for designing high Tc superconductors. The first is SODNet, a novel equivariant graph attention model for screening known structures, which differs from existing models in incorporating both ordered and disordered geometric content. The second is a diffusion generative model DiffCSP-SC for creating new structures, which enables high Tc-targeted generation. Extensive experiments demonstrate that both our proposed dataset and models are advantageous for designing new high Tc superconducting candidates.",Datasets & Benchmarks,NeurIPS,2024,Poster,Pin Chen;Luoxuan Peng;Rui Jiao;Qing Mo;Zhen WANG;Wenbing Huang;Yang Liu;Yutong Lu,True,https://openreview.net/pdf?id=iNYrB3ip9F
iSwK1YqO7v,Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making,"We aim to evaluate Large Language Models (LLMs) for embodied decision making. While a significant body of work has been leveraging LLMs for decision making in embodied environments, we still lack a systematic understanding of their performance because they are usually applied in different domains, for different purposes, and built based on different inputs and outputs. Furthermore, existing evaluations tend to rely solely on a final success rate, making it difficult to pinpoint what ability is missing in LLMs and where the problem lies, which in turn blocks embodied agents from leveraging LLMs effectively and selectively. To address these limitations, we propose a generalized interface (Embodied Agent Interface) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules. Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics that break down evaluation into error types, such as hallucination errors, affordance errors, and various types of planning errors. Overall, our benchmark offers a comprehensive assessment of LLMs’ performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems and providing insights into the effective and selective use of LLMs in embodied decision making.",Datasets & Benchmarks,NeurIPS,2024,Oral,Manling Li;Shiyu Zhao;Qineng Wang;Kangrui Wang;Yu Zhou;Sanjana Srivastava;Cem Gokmen;Tony Lee;Li Erran Li;Ruohan Zhang;Weiyu Liu;Percy Liang;Li Fei-Fei;Jiayuan Mao;Jiajun Wu,False,https://openreview.net/pdf?id=iSwK1YqO7v
iTUlYblV0K,MQuAKE-Remastered: Multi-Hop Knowledge Editing Can Only Be Advanced With Reliable Evaluations,"Large language models (LLMs) can give out erroneous answers to factually rooted questions either as a result of undesired training outcomes or simply because the world has moved on after a certain knowledge cutoff date. Under such scenarios, *knowledge editing* often comes to the rescue by delivering efficient patches for such erroneous answers without significantly altering the rests, where many editing methods have seen reasonable success when the editing targets are simple and direct (e.g., *""what club does Lionel Messi currently play for?""*).

However, knowledge fragments like this are often deeply intertwined in the real world, making effectively propagating the editing effect to non-directly related questions a practical challenge (e.g., *""who is the offspring of the owner of the club that Messi currently plays for?""*). Prior arts have coined this task as *multi-hop knowledge editing* with the most popular dataset being MQuAKE, serving as the sole evaluation benchmark for many later proposed editing methods due to the expensive nature of making knowledge editing datasets at scale. 

In this work, we reveal that **up to 33\\\\% or 76\\\\% of MQuAKE's questions and ground truth labels are, in fact, corrupted in various fashions due to some unintentional clerical or procedural oversights**. Our work provides a detailed audit of MQuAKE's error pattern and a comprehensive fix without sacrificing its dataset capacity. Additionally, we benchmarked almost all proposed \\\\mquake{}-evaluated editing methods on our post-fix dataset, \\\\mquaker{}. It is our observation that many methods try to overfit the original \\\\mquake{} by exploiting some data-specific properties of \\\\mquake{}. We provide a guideline on how to faithfully approach such datasets and show that a simple, minimally invasive approach can bring excellent editing performance without such exploitation. Please refer to [`https://github.com/henryzhongsc/MQuAKE-Remastered`](https://github.com/henryzhongsc/MQuAKE-Remastered) and supplemental material for assets.",Datasets & Benchmarks,NeurIPS,2024,Reject,Shaochen Zhong;Yifan Lu;Lize Shao;Bhargav Bhushanam;Xiaocong Du;Louis Feng;Yixin Wan;Yucheng Shi;Daochen Zha;Yiwei Wang;Ninghao Liu;Kaixiong Zhou;Shuai Xu;Vipin Chaudhary;Xia Hu,True,https://openreview.net/pdf?id=iTUlYblV0K
iTyOWtcCU2,STimage-1K4M: A histopathology image-gene expression dataset for spatial transcriptomics,"Recent advances in multi-modal algorithms have driven and been driven by the increasing availability of large image-text datasets, leading to significant strides in various fields, including computational pathology. However, in most existing medical image-text datasets, the text typically provides high-level summaries that may not sufficiently describe sub-tile regions within a large pathology image. For example, an image might cover an extensive tissue area containing cancerous and healthy regions, but the accompanying text might only specify that this image is a cancer slide, lacking the nuanced details needed for in-depth analysis. In this study, we introduce STimage-1K4M, a novel dataset designed to bridge this gap by providing genomic features for sub-tile images. STimage-1K4M contains 1,149 images derived from spatial transcriptomics data, which captures gene expression information at the level of individual spatial spots within a pathology image. Specifically, each image in the dataset is broken down into smaller sub-image tiles, with each tile paired with $15,000-30,000$ dimensional gene expressions. With $4,293,195$ pairs of sub-tile images and gene expressions, STimage-1K4M offers unprecedented granularity, paving the way for a wide range of advanced research in multi-modal data analysis an innovative applications in computational pathology, and beyond.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jiawen Chen;Muqing Zhou;Wenrong Wu;Jinwei Zhang;Yun Li;Didong Li,True,https://openreview.net/pdf?id=iTyOWtcCU2
iWc0qE116u,APEBench: A Benchmark for Autoregressive Neural Emulators of PDEs,"We introduce the **A**utoregressive **P**DE **E**mulator Benchmark (APEBench),  a comprehensive benchmark suite to evaluate autoregressive neural emulators for solving partial differential equations. APEBench is based on JAX and provides a seamlessly integrated differentiable simulation framework employing efficient pseudo-spectral methods, enabling 46 distinct PDEs across 1D, 2D, and 3D. Facilitating systematic analysis and comparison of learned emulators, we propose a novel taxonomy for unrolled training and introduce a unique identifier for PDE dynamics that directly relates to the stability criteria of classical numerical methods. APEBench enables the evaluation of diverse neural architectures, and unlike existing benchmarks, its tight integration of the solver enables support for differentiable physics training and neural-hybrid emulators. Moreover, APEBench emphasizes rollout metrics to understand temporal generalization, providing insights into the long-term behavior of emulating PDE dynamics. In several experiments, we highlight the similarities between neural emulators and numerical simulators. The code is available at [github.com/tum-pbs/apebench](https://github.com/tum-pbs/apebench) and APEBench can be installed via `pip install apebench`.",Datasets & Benchmarks,NeurIPS,2024,Poster,Felix Koehler;Simon Niedermayr;rüdiger westermann;Nils Thuerey,True,https://openreview.net/pdf?id=iWc0qE116u
iaahkRzA9f,Map It Anywhere: Empowering BEV Map Prediction using Large-scale Public Datasets,"Top-down Bird's Eye View (BEV) maps are a popular perception representation for ground robot navigation due to their richness and flexibility for downstream tasks. While recent methods have shown promise for predicting BEV maps from First-Person View (FPV) images, their generalizability is limited to small regions captured by current autonomous vehicle-based datasets. In this context, we show that a more scalable approach towards generalizable map prediction can be enabled by using two large-scale crowd-sourced mapping platforms, Mapillary for FPV images and OpenStreetMap for BEV semantic maps.
We introduce Map It Anywhere (MIA), a data engine that enables seamless curation and modeling of labeled map prediction data from existing open-source map platforms. Using our MIA data engine, we display the ease of automatically collecting a 1.2 million FPV & BEV pair dataset encompassing diverse geographies, landscapes, environmental factors, camera models & capture scenarios. We further train a simple camera model-agnostic model on this data for BEV map prediction.
Extensive evaluations using established benchmarks and our dataset show that the data curated by MIA enables effective pretraining for generalizable BEV map prediction, with zero-shot performance far exceeding baselines trained on existing datasets by 35%. Our analysis highlights the promise of using large-scale public maps for developing & testing generalizable BEV perception, paving the way for more robust autonomous navigation.

Website: mapitanywhere.github.io",Datasets & Benchmarks,NeurIPS,2024,Poster,Cherie Ho;Jiaye Zou;Omar Alama;Sai Mitheran;Benjamin Chiang;Taneesh Gupta;Chen Wang;Nikhil Varma Keetha;Katia P. Sycara;Sebastian Scherer,True,https://openreview.net/pdf?id=iaahkRzA9f
itBDglVylS,NYU CTF Bench: A Scalable Open-Source Benchmark Dataset for Evaluating LLMs in Offensive Security,"Large Language Models (LLMs) are being deployed across various domains today. However, their capacity to solve Capture the Flag (CTF) challenges in cybersecurity has not been thoroughly evaluated. To address this, we develop a novel method to assess LLMs in solving CTF challenges by creating a scalable, open-source benchmark database specifically designed for these applications. This database includes metadata for LLM testing and adaptive learning, compiling a diverse range of CTF challenges from popular competitions. Utilizing the advanced function calling capabilities of LLMs, we build a fully automated system with an enhanced workflow and support for external tool calls. Our benchmark dataset and automated framework allow us to evaluate the performance of five LLMs, encompassing both black-box and open-source models. This work lays the foundation for future research into improving the efficiency of LLMs in interactive cybersecurity tasks and automated task planning. By providing a specialized benchmark, our project offers an ideal platform for developing, testing, and refining LLM-based approaches to vulnerability detection and resolution. Evaluating LLMs on these challenges 
and comparing with human performance yields insights into their potential for AI-driven cybersecurity solutions to perform real-world threat management. We make our benchmark dataset open source to public https://github.com/NYU-LLM-CTF/NYU_CTF_Bench along with our playground automated framework https://github.com/NYU-LLM-CTF/llm_ctf_automation.",Datasets & Benchmarks,NeurIPS,2024,Poster,Minghao Shao;Sofija Jancheska;Meet Udeshi;Brendan Dolan-Gavitt;Haoran Xi;Kimberly Milner;Boyuan Chen;Max Yin;Siddharth Garg;Prashanth Krishnamurthy;Farshad Khorrami;Ramesh Karri;Muhammad Shafique,True,https://openreview.net/pdf?id=itBDglVylS
iwC19lVBoq,AVSET-10M: An Open Large-Scale Audio-Visual Dataset with High Correspondence,"Groundbreaking research from initiatives such as ChatGPT and Sora underscores the crucial role of large-scale data in advancing generative and comprehension tasks. However, the scarcity of comprehensive and large-scale audio-visual correspondence datasets poses a significant challenge to research in the audio-visual fields. To address this gap, we introduce **AVSET-10M**, a audio-visual high-corresponding dataset comprising 10 million samples, featuring the following key attributes: (1) **High Audio-Visual Correspondence**: Through meticulous sample filtering, we ensure robust correspondence between the audio and visual components of each entry. (2) **Comprehensive Categories**: Encompassing 527 unique audio categories, AVSET-10M offers the most extensive range of audio categories available. (3) **Large Scale**: With 10 million samples, AVSET-10M is the largest publicly available audio-visual corresponding dataset. We have benchmarked two critical tasks on AVSET-10M: audio-video retrieval and vision-queried sound separation. These tasks highlight the essential role of precise audio-visual correspondence in advancing audio-visual research. For more information, please visit https://avset-10m.github.io/.",Datasets & Benchmarks,NeurIPS,2024,Reject,Xize Cheng;Ziang Zhang;Zehan Wang;Minghui Fang;Rongjie Huang;Siqi Zheng;Ruofan Hu;Bai Jionghao;Tao Jin;Zhou Zhao,True,https://openreview.net/pdf?id=iwC19lVBoq
j4CRWz418M,Are Large Language Models Good Statisticians?,"Large Language Models (LLMs) have demonstrated impressive capabilities across a range of scientific tasks including mathematics, physics, and chemistry. Despite their successes, the effectiveness of LLMs in handling complex statistical tasks remains systematically under-explored. To bridge this gap, we introduce StatQA, a new benchmark designed for statistical analysis tasks. StatQA comprises 11,623 examples tailored to evaluate LLMs' proficiency in specialized statistical tasks and their applicability assessment capabilities, particularly for hypothesis testing methods. We systematically experiment with representative LLMs using various prompting strategies and show that even state-of-the-art models such as GPT-4o achieve a best performance of only 64.83%, indicating significant room for improvement. Notably, while open-source LLMs (e.g. LLaMA-3) show limited capability, those fine-tuned ones exhibit marked improvements, outperforming all in-context learning-based methods (e.g. GPT-4o). Moreover, our comparative human experiments highlight a striking contrast in error types between LLMs and humans: LLMs primarily make applicability errors, whereas humans mostly make statistical task confusion errors. This divergence highlights distinct areas of proficiency and deficiency, suggesting that combining LLM and human expertise could lead to complementary strengths, inviting further investigation into their collaborative potential. Our source code and data are available at https://statqa.github.io/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yizhang Zhu;Shiyin Du;Boyan Li;Yuyu Luo;Nan Tang,True,https://openreview.net/pdf?id=j4CRWz418M
j6PTT6NB2O,Stress-Testing Long-Context Language Models with Lifelong ICL and Task Haystack,"We introduce Lifelong ICL, a problem setting that challenges long-context language models (LMs) to learn a sequence of language tasks through in-context learning (ICL). We further introduce Task Haystack, an evaluation suite dedicated to assessing and diagnosing how long-context LMs utilizes contexts in Lifelong ICL. When given a task instruction and test inputs, long-context LMs are expected
to leverage the relevant demonstrations in the Lifelong ICL prompt, avoid distraction and interference from other tasks, and achieve test accuracies that are not significantly worse than those of the Single-task ICL baseline.

Task Haystack draws inspiration from the widely-adopted “needle-in-a-haystack” (NIAH) evaluation, but presents distinct new challenges. It requires models (1) to utilize the contexts at a deeper level, rather than resorting to simple copying and pasting; (2) to navigate through long streams of evolving topics and tasks, proxying the complexities and dynamism of contexts in real-world scenarios. Additionally, Task Haystack inherits the controllability of NIAH, providing model developers with tools and visualizations to identify model vulnerabilities effectively.

We benchmark 14 long-context LMs using Task Haystack, finding that frontier models like GPT-4o still struggle with the setting, failing on 15% of cases on average. Most open-weight models further lack behind by a large margin, with failure rates reaching up to 61%. In our controlled analysis, we identify factors such as distraction and recency bias as contributors to these failure cases. Further, performance declines when task instructions are paraphrased at test time or when ICL demonstrations are repeated excessively, raising concerns about the robustness, instruction understanding, and true context utilization of long-context LMs. We release our code and data to encourage future research that investigates and addresses these limitations.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xiaoyue Xu;Qinyuan Ye;Xiang Ren,True,https://openreview.net/pdf?id=j6PTT6NB2O
jSKtxmxc0M,VideoGUI: A Benchmark for GUI Automation from Instructional Videos,"Graphical User Interface (GUI) automation holds significant promise for enhancing human productivity by assisting with computer tasks. Existing task formulations primarily focus on simple tasks that can be specified by a single, language-only instruction, such as “Insert a new slide.” In this work, we introduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUI assistants on visual-centric GUI tasks. Sourced from high-quality web instructional videos, our benchmark focuses on tasks involving professional and novel software (e.g., Adobe Pho- toshop or Stable Diffusion WebUI) and complex activities (e.g., video editing). VideoGUI evaluates GUI assistants through a hierarchical process, allowing for identification of the specific levels at which they may fail: (i) high-level planning: reconstruct procedural subtasks from visual conditions without language descrip- tions; (ii) middle-level planning: generate sequences of precise action narrations based on visual state (i.e., screenshot) and goals; (iii) atomic action execution: perform specific actions such as accurately clicking designated elements. For each level, we design evaluation metrics across individual dimensions to provide clear signals, such as individual performance in clicking, dragging, typing, and scrolling for atomic action execution. Our evaluation on VideoGUI reveals that even the SoTA large multimodal model GPT4o performs poorly on visual-centric GUI tasks, especially for high-level planning. The data and code are available at https://github.com/showlab/videogui.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Kevin Qinghong Lin;Linjie Li;Difei Gao;Qinchen WU;Mingyi Yan;Zhengyuan Yang;Lijuan Wang;Mike Zheng Shou,True,https://openreview.net/pdf?id=jSKtxmxc0M
jbrMS0DNaD,INQUIRE: A Natural World Text-to-Image Retrieval Benchmark,"We introduce INQUIRE, a text-to-image retrieval benchmark designed to challenge multimodal vision-language models on expert-level queries. INQUIRE includes iNaturalist 2024 (iNat24), a new dataset of five million natural world images, along with 250 expert-level retrieval queries. These queries are paired with all relevant images comprehensively labeled within iNat24, comprising 33,000 total matches. Queries span categories such as species identification, context, behavior, and appearance, emphasizing tasks that require nuanced image understanding and domain expertise. Our benchmark evaluates two core retrieval tasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2) INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed evaluation of a range of recent multimodal models demonstrates that INQUIRE poses a significant challenge, with the best models failing to achieve an mAP@50 above 50%. In addition, we show that reranking with more powerful multimodal models can enhance retrieval performance, yet there remains a significant margin for improvement. By focusing on scientifically-motivated ecological challenges, INQUIRE aims to bridge the gap between AI capabilities and the needs of real-world scientific inquiry, encouraging the development of retrieval systems that can assist with accelerating ecological and biodiversity research.",Datasets & Benchmarks,NeurIPS,2024,Poster,Edward Vendrow;Omiros Pantazis;Alexander Shepard;Gabriel Brostow;Kate E. Jones;Oisin Mac Aodha;Sara Beery;Grant Van Horn,True,https://openreview.net/pdf?id=jbrMS0DNaD
ji5isUwL3r,LucidAction: A Hierarchical and Multi-model Dataset for Comprehensive Action Quality Assessment,"Action Quality Assessment (AQA) research confronts formidable obstacles due to limited, mono-modal datasets sourced from one-shot competitions, which hinder the generalizability and comprehensiveness of AQA models. To address these limitations, we present LucidAction, the first systematically collected multi-view AQA dataset structured on curriculum learning principles. LucidAction features a three-tier hierarchical structure, encompassing eight diverse sports events with four curriculum levels, facilitating sequential skill mastery and supporting a wide range of athletic abilities. The dataset encompasses multi-modal data, including multi-view RGB video, 2D and 3D pose sequences, enhancing the richness of information available for analysis. Leveraging a high-precision multi-view Motion Capture (MoCap) system ensures precise capture of complex movements. Meticulously annotated data, incorporating detailed penalties from professional gymnasts, ensures the establishment of robust and comprehensive ground truth annotations. Experimental evaluations employing diverse contrastive regression baselines on LucidAction elucidate the dataset's complexities. Through ablation studies, we investigate the advantages conferred by multi-modal data and fine-grained annotations, offering insights into improving AQA performance. The data and code will be openly released to support advancements in the AI sports field.",Datasets & Benchmarks,NeurIPS,2024,Poster,Linfeng Dong;Wei Wang;Yu Qiao;Xiao Sun,True,https://openreview.net/pdf?id=ji5isUwL3r
jkJDNG468g,CY-Bench: A comprehensive benchmark dataset for subnational crop yield forecasting,"In-season or pre-harvest crop yield forecasts are essential for enhancing transparency in commodity markets and planning towards achieving the United Nations’ Sustainable Development Goal 2 of zero hunger, especially in the context of climate change and extreme events leading to crop failures. Pre-harvest crop yield forecasting is a difficult problem, as several interacting factors contribute to yield formation, including in-season weather variability, extreme events, long-term climate change, pests, diseases and farmer management decisions. Machine learning methods provide ways to capture complex interactions among such predictors and crop yields. Prior research in agricultural applications, including crop yield forecasting, has primarily been case-study based, which makes it difficult to compare modeling approaches and measure progress. To address this gap, we introduce  CY-Bench (Crop Yield Benchmark), a comprehensive dataset and benchmark to forecast crop yields. We standardized data source selection, preprocessing and spatio-temporal harmonization of public subnational yield statistics with relevant predictors such as weather, soil, and remote sensing indicators, in collaboration with domain experts such as agronomists, climate scientists, and machine learning researchers. With CY-Bench  we aim to: (i) standardize machine learning model evaluation in a framework that covers multiple farming systems in more than twenty five countries across the globe, (ii) facilitate robust and reproducible model comparison through a benchmark addressing real-world operational needs, (iii) share a dataset with the machine learning community to facilitate research efforts related to time series forecasting, domain adaptation and online learning. The dataset and code used will be openly available, supporting the further development of advanced machine learning models for crop yield forecasting that can be used to aid decision-makers in improving global and regional food security.",Datasets & Benchmarks,NeurIPS,2024,Reject,Dilli Paudel;Hilmy Baja;Ron van Bree;Michiel Kallenberg;Stella Ofori-Ampofo;Aike Potze;Pratishtha Poudel;Abdelrahman Saleh;Weston Anderson;Malte von Bloh;Andres Castellano;Oumnia Ennaji;Raed Hamed;Rahel Laudien;Donghoon Lee;Inti Luna;Dainius Masiliūnas;Michele Meroni;Janet Mumo Mutuku;Siyabusa Mkuhlani;Jonathan Richetti;Alex C. Ruane;Ritvik Sahajpal;Guanyuan Shuai;Vasileios Sitokonstantinou;Rogerio de Souza Noia Junior;Amit Kumar Srivastava;Robert Strong;Lily-belle Sweet;Petar Vojnović;Allard de Wit;Maximilian Zachow;Ioannis N. Athanasiadis,True,https://openreview.net/pdf?id=jkJDNG468g
jz2CTTCABH,A New Multi-Source Light Detection Benchmark and Semi-Supervised Focal Light Detection,"This paper addresses a multi-source light detection (LD) problem from vehicles, traffic signals, and streetlights under driving scenarios. Albeit it is crucial for autonomous driving and night vision, this problem has not been yet focused on as much as other object detection (OD). One of the main reasons is the absence of a public available LD benchmark dataset. Therefore, we construct a new large LD dataset consisting of different light sources via heavy annotation: YouTube Driving Light Detection dataset (YDLD). Compared to the existing LD datasets, our dataset has much more images and box annotations for multi-source lights. We also provide rigorous statistical analysis and transfer learning comparison of other well-known detection benchmark datasets to prove the generality of our YDLD.

For the recent object detectors, we achieve the extensive comparison results on YDLD. However, they tend to yield the low mAP scores due to the intrinsic challenges of LD caused by very tiny size and similar appearance. To resolve those, we design a novel lightness focal loss which penalizes miss-classified samples more and a lightness spatial attention prior by reflecting a global scene context. In addition, we develop a semi-supervised focal light detection (SS-FLD) by embedding our lightness focal loss into the semi-supervised object detection (SSOD). We prove that our methods can consistently boost mAP to the variety of types of recent detectors on YDLD. We will open both YDLD and SS-FLD code at https://github.com/YDLD-dataset/YDLD.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jae-Yong Baek;Yong-Sang Yoo;Seung-Hwan Bae,True,https://openreview.net/pdf?id=jz2CTTCABH
jzngdJQ2lY,Solving Minimum-Cost Reach Avoid using Reinforcement Learning,"Current reinforcement-learning methods are unable to directly learn policies that solve the minimum cost reach-avoid problem to minimize cumulative costs subject to the constraints of reaching the goal and avoiding unsafe states, as the structure of this new optimization problem is incompatible with current methods. Instead, a surrogate problem is solved where all objectives are combined with a weighted sum. However, this surrogate objective results in suboptimal policies that do not directly minimize the cumulative cost. In this work, we propose RC-PPO, a reinforcement-learning-based method for solving the minimum-cost reach-avoid problem by using connections to Hamilton-Jacobi reachability. Empirical results demonstrate that RC-PPO learns policies with comparable goal-reaching rates to while achieving up to 57% lower cumulative costs compared to existing methods on a suite of minimum-cost reach-avoid benchmarks on the Mujoco simulator. The project page can be found at https://oswinso.xyz/rcppo.",main,NeurIPS,2024,Poster,Oswin So;Cheng Ge;Chuchu Fan,True,https://openreview.net/pdf?id=jzngdJQ2lY
k0qTnbQxzR,CogCoM: Train Large Vision-Language Models Diving into Details through Chain of Manipulations,"Vision-Language Models (VLMs) have demonstrated their broad effectiveness thanks to extensive training in aligning visual instructions to responses. However, such training of conclusive alignment leads models to ignore essential visual reasoning, further resulting in failures in meticulous visual problems and unfaithful responses. Drawing inspiration from human cognition in solving visual problems (e.g., marking, zoom in), this paper introduces Chain of Manipulations, a mechanism that enables VLMs to solve problems step-by-step with evidence. After training, models can solve various visual problems by eliciting intrinsic manipulations (e.g., grounding, zoom in) and results (e.g., boxes, image) actively without involving external tools, while also allowing users to trace error causes. We study the roadmap to implement this mechanism, including (1) a flexible design of manipulations upon extensive analysis, (2) an efficient automated data generation pipeline, (3) a compatible VLM architecture capable of multi-turn multi-image, and (4) a model training process for versatile capabilities.  With the design, we also manually annotate 6K high-quality samples for the challenging graphical mathematical problems. Our trained model, CogCoM, equipped with this mechanism with 17B parameters achieves state-of-the-art performance across 9 benchmarks from 4 categories, demonstrating the effectiveness while preserving the interpretability. Our code, model weights, and collected data will be publicly available.",main,NeurIPS,2024,Reject,Ji Qi;Ming Ding;Weihan Wang;Yushi Bai;Qingsong Lv;Wenyi Hong;Bin Xu;Lei Hou;Juanzi Li;Yuxiao Dong;Jie Tang,True,https://openreview.net/pdf?id=k0qTnbQxzR
k4tuZmvSnl,MLLMGuard: A Multi-dimensional Safety Evaluation Suite for Multimodal Large Language Models,"Powered by remarkable advancements in Large Language Models (LLMs), Multimodal Large Language Models (MLLMs) demonstrate impressive capabilities in manifold tasks.
However, the practical application scenarios of MLLMs are intricate, exposing them to potential malicious instructions and thereby posing safety risks.
While current benchmarks do incorporate certain safety considerations, they often lack comprehensive coverage and fail to exhibit the necessary rigor and robustness.
For instance, the common practice of employing GPT-4V as both the evaluator and a model to be evaluated lacks credibility, as it tends to exhibit a bias toward its own responses.
In this paper, we present MLLMGuard, a multi-dimensional safety evaluation suite for MLLMs, including a bilingual image-text evaluation dataset, inference utilities, and a lightweight evaluator.
MLLMGuard's assessment comprehensively covers two languages (English and Chinese) and five important safety dimensions (Privacy, Bias, Toxicity, Truthfulness, and Legality), each with corresponding rich subtasks.
Focusing on these dimensions, our evaluation dataset is primarily sourced from platforms such as social media, and 
it integrates text-based and image-based red teaming techniques with meticulous annotation by human experts.
This can prevent inaccurate evaluation caused by data leakage when using open-source datasets and ensures the quality and challenging nature of our benchmark.
Additionally, a fully automated lightweight evaluator termed GuardRank is developed, which achieves significantly higher evaluation accuracy than GPT-4.
Our evaluation results across 13 advanced models indicate that MLLMs still have a substantial journey ahead before they can be considered safe and responsible.",Datasets & Benchmarks,NeurIPS,2024,Poster,Tianle Gu;Zeyang Zhou;Kexin Huang;Liang Dandan;Yixu Wang;Haiquan Zhao;Yuanqi Yao;xingge qiao;Keqing wang;Yujiu Yang;Yan Teng;Yu Qiao;Yingchun Wang,True,https://openreview.net/pdf?id=k4tuZmvSnl
k6ZHvF1vkg,Beyond Optimism: Exploration With Partially Observable Rewards,"Exploration in reinforcement learning (RL) remains an open challenge.
RL algorithms rely on observing rewards to train the agent, and if informative rewards are sparse the agent learns slowly or may not learn at all. 
To improve exploration and reward discovery, popular algorithms rely on optimism. 
But what if sometimes rewards are unobservable, e.g., situations of partial monitoring in bandits and the recent formalism of monitored Markov decision process? 
In this case, optimism can lead to suboptimal behavior that does not explore further to collapse uncertainty.
With this paper, we present a novel exploration strategy that overcomes the limitations of existing methods and guarantees convergence to an optimal policy even when rewards are not always observable. 
We further propose a collection of tabular environments for benchmarking exploration in RL (with and without unobservable rewards) and show that our method outperforms existing ones.",main,NeurIPS,2024,Poster,Simone Parisi;Alireza Kazemipour;Michael Bowling,True,https://openreview.net/pdf?id=k6ZHvF1vkg
k73M4XEvFX,SORRY-Bench: Systematically Evaluating Large Language Model Safety Refusal Behaviors,"Evaluating aligned large language models' (LLMs) ability to recognize and reject unsafe user requests is crucial for safe, policy-compliant deployments. Existing evaluation efforts, however, face three limitations that we address with **SORRY-Bench**, our proposed benchmark. **First**, existing methods often use coarse-grained taxonomies of unsafe topics, and are over-representing some fine-grained topics. For example, among the ten existing datasets that we evaluated, tests for refusals of self-harm instructions are over 3x less represented than tests for fraudulent activities. SORRY-Bench improves on this by using a fine-grained taxonomy of 45 potentially unsafe topics, and 450 class-balanced unsafe instructions, compiled through human-in-the-loop methods. **Second**, evaluations often overlook the linguistic formatting of prompts, like different languages, dialects, and more --- which are only implicitly considered in many evaluations. We supplement SORRY-bench with 20 diverse linguistic augmentations to systematically examine these effects. **Third**, existing evaluations rely on large LLMs (e.g., GPT-4) for evaluation, which can be computationally expensive. We investigate design choices for creating a fast, accurate automated safety evaluator. By collecting 7K+ human annotations and conducting a meta-evaluation of diverse LLM-as-a-judge designs, we show that fine-tuned 7B LLMs can achieve accuracy comparable to GPT-4 scale LLMs, with lower computational cost. Putting these together, we evaluate over 40 proprietary and open-source LLMs on SORRY-Bench, analyzing their distinctive refusal behaviors. We hope our effort provides a building block for systematic evaluations of LLMs' safety refusal capabilities, in a balanced, granular, and efficient way.",Datasets & Benchmarks,NeurIPS,2024,Reject,Tinghao Xie;Xiangyu Qi;Yi Zeng;Yangsibo Huang;Udari Madhushani Sehwag;Boyi Wei;Luxi He;Kaixuan Huang;Dacheng Li;Ying Sheng;Ruoxi Jia;Bo Li;Danqi Chen;Kai Li;Peter Henderson;Prateek Mittal,True,https://openreview.net/pdf?id=k73M4XEvFX
kBvwv92E1S,Nuclear Fusion Diamond Polishing Dataset,"In the Inertial Confinement Fusion (ICF) process, roughly a 2mm spherical shell made of high-density carbon is used as a target for laser beams, which compress and heat it to energy levels needed for high fusion yield in nuclear fusion. These shells are polished meticulously to meet the standards for a fusion shot. However, the polishing of these shells involves multiple stages, with each stage taking several hours. To make sure that the polishing process is advancing in the right direction, we are able to measure the shell surface roughness. This measurement, however, is very labor-intensive, time-consuming, and requires a human operator. To help improve the polishing process we have released the first dataset to the public that consists of raw vibration signals with the corresponding polishing surface roughness changes. We show that this dataset can be used with a variety of neural network based methods for prediction of the change of polishing surface roughness, hence eliminating the need for the time-consuming manual process. This is the first dataset of its kind to be released in public and its use will allow the operator to make any necessary changes to the ICF polishing process for optimal results. This dataset contains the raw vibration data of multiple polishing runs with their extracted statistical features and the corresponding surface roughness values. Additionally, to generalize the prediction models to different polishing conditions, we also apply domain adaptation techniques to improve prediction accuracy for conditions unseen by the trained model. The dataset is available in \\\\url{https://junzeliu.github.io/Diamond-Polishing-Dataset/}.",Datasets & Benchmarks,NeurIPS,2024,Poster,Antonios Alexos;Junze Liu;Shashank Galla;Sean Hayes;Kshitij Bhardwaj;Alexander Schwartz;Monika Biener;Pierre Baldi;Satish Bukkapatnam;Suhas Bhandarkar,True,https://openreview.net/pdf?id=kBvwv92E1S
kChaL3rZxi,Image Textualization: An Automatic Framework for Generating Rich and Detailed Image Descriptions,"Image description datasets play a crucial role in the advancement of various applications such as image understanding, text-to-image generation, and text-image retrieval. Currently, image description datasets primarily originate from two sources. One source is the scraping of image-text pairs from the web. Despite their abundance, these descriptions are often of low quality and noisy. Another way is through human labeling. Datasets such as COCO are generally very short and lack details. Although detailed image descriptions can be annotated by humans, the high cost limits their quantity and feasibility. These limitations underscore the need for more efficient and scalable methods to generate accurate and detailed image descriptions. In this paper, we propose an innovative framework termed Image Textualization, which automatically produces high-quality image descriptions by leveraging existing mult-modal large language models (MLLMs) and multiple vision expert models in a collaborative manner. We conduct various experiments to validate the high quality of the descriptions constructed by our framework. Furthermore, we show that MLLMs fine-tuned on our dataset acquire an unprecedented capability of generating richer image descriptions, substantially increasing the length and detail of their output with even less hallucinations.",Datasets & Benchmarks,NeurIPS,2024,Poster,Renjie Pi;Jianshu Zhang;Jipeng Zhang;Rui Pan;Zhekai Chen;Tong Zhang,False,https://openreview.net/pdf?id=kChaL3rZxi
kD1kpLtrmX,Benchmarking Out-of-Distribution Generalization Capabilities of DNN-based Encoding Models for the Ventral Visual Cortex.,"We characterized the generalization capabilities of deep neural network encoding models when predicting neuronal responses from the visual cortex to flashed images. We collected MacaqueITBench, a large-scale dataset of neuronal population responses from the macaque inferior temporal (IT) cortex to over $300,000$ images, comprising $8,233$ unique natural images presented to seven monkeys over $109$ sessions. Using MacaqueITBench, we investigated the impact of distribution shifts on models predicting neuronal activity by dividing the images into Out-Of-Distribution (OOD) train and test splits. The OOD splits included variations in image contrast, hue, intensity, temperature, and saturation. Compared to the performance on in-distribution test images---the conventional way in which these models have been evaluated---models performed worse at predicting neuronal responses to out-of-distribution images, retaining as little as $20\\\\\\\\%$ of the performance on in-distribution test images. Additionally, the relative ranking of different models in terms of their ability to predict neuronal responses changed drastically across OOD shifts. The generalization performance under OOD shifts can be well accounted by a simple image similarity metric---the cosine distance between image representations extracted from a pre-trained object recognition model is a strong predictor of neuronal predictivity under different distribution shifts. The dataset of images, neuronal firing rate recordings, and computational benchmarks are hosted publicly at: https://github.com/Spandan-Madan/benchmarking_ood_generalization_visual_cortex.",Datasets & Benchmarks,NeurIPS,2024,Poster,Spandan Madan;Will Xiao;Mingran Cao;Hanspeter Pfister;Margaret Livingstone;Gabriel Kreiman,True,https://openreview.net/pdf?id=kD1kpLtrmX
kDp8Eq76dm,AlleNoise - large-scale text classification benchmark dataset with real-world label noise,"Label noise remains a challenge for training robust classification models. Most methods for mitigating label noise have been benchmarked using primarily datasets with synthetic noise. While the need for datasets with realistic noise distribution has partially been addressed by web-scraped benchmarks such as WebVision and Clothing1M, those benchmarks are restricted to the computer vision domain. With the growing importance of Transformer-based models, it is crucial to establish text classification benchmarks for learning with noisy labels. In this paper, we present AlleNoise, a new curated text classification dataset with real-world instance-dependent label noise, containing over 500,000 examples across approximately 5600 classes, complemented with a meaningful, hierarchical taxonomy of categories. The noise distribution comes from actual users of a major e-commerce marketplace, so it realistically reflects the semantics of human mistakes. In addition to the noisy labels, we provide human-verified clean labels, which help to get a deeper insight into the noise distribution, unlike web-scraped datasets typically used in the field. We demonstrate that a representative selection of established methods for learning with noisy labels is inadequate to handle such real-world noise. In addition, we show evidence that these algorithms do not alleviate excessive memorization. As such, with AlleNoise, we set a high bar for the development of label noise methods that can handle real-world label noise in text classification tasks. The code and dataset are available for download at https://github.com/allegro/AlleNoise.",Datasets & Benchmarks,NeurIPS,2024,Reject,Alicja Rączkowska;Aleksandra Osowska-Kurczab;Jacek Szczerbiński;Kalina Jasinska-Kobus;Klaudia Nazarko,True,https://openreview.net/pdf?id=kDp8Eq76dm
kKtalvwqBZ,Benchmarking Structural Inference Methods for Interacting Dynamical Systems with Synthetic Data,"Understanding complex dynamical systems begins with identifying their topological structures, which expose the organization of the systems. This requires robust structural inference methods that can deduce structure from observed behavior. However, existing methods are often domain-specific and lack a standardized, objective comparison framework. We address this gap by benchmarking 13 structural inference methods from various disciplines on simulations representing two types of dynamics and 11 interaction graph models, supplemented by a biological experimental dataset to mirror real-world application. We evaluated the methods for accuracy, scalability, robustness, and sensitivity to graph properties. Our findings indicate that deep learning methods excel with multi-dimensional data, while classical statistics and information theory based approaches are notably accurate and robust. Additionally, performance correlates positively with the graph's average shortest path length. This benchmark should aid researchers in selecting suitable methods for their specific needs and stimulate further methodological innovation.",Datasets & Benchmarks,NeurIPS,2024,Poster,Aoran Wang;Tsz Pan Tong;Andrzej Mizera;Jun Pang,False,https://openreview.net/pdf?id=kKtalvwqBZ
kOMrm4ZJ3m,"Global Lyapunov functions: a long-standing open problem in mathematics, with symbolic transformers","Despite their spectacular progress, language models still struggle on complex reasoning tasks, such as advanced mathematics.
We consider a long-standing open problem in mathematics: discovering a Lyapunov function that ensures the global stability of a dynamical system. This problem has no known general solution, and algorithmic solvers only exist for some small polynomial systems.
We propose a new method for generating synthetic training samples from random solutions, and show that sequence-to-sequence transformers trained on such datasets perform better than algorithmic solvers and humans on polynomial systems, and can discover new Lyapunov functions for non-polynomial systems.",main,NeurIPS,2024,Poster,Alberto Alfarano;Francois Charton;Amaury Hayat,True,https://openreview.net/pdf?id=kOMrm4ZJ3m
kP92Fyc6ry,CryoBench: Diverse and challenging datasets for the heterogeneity problem in cryo-EM,"Cryo-electron microscopy (cryo-EM)  is a powerful technique for determining high-resolution 3D biomolecular structures from imaging data. Its unique ability to capture structural variability has spurred the development of heterogeneous reconstruction algorithms that can infer distributions of 3D structures from noisy, unlabeled imaging data. Despite the growing number of advanced methods, progress in the field is hindered by the lack of standardized benchmarks with ground truth information and reliable validation metrics. Here, we introduce CryoBench, a suite of datasets, metrics, and benchmarks for heterogeneous reconstruction in cryo-EM. CryoBench includes five datasets representing different sources of heterogeneity and degrees of difficulty. These include conformational heterogeneity generated from designed motions of antibody complexes or sampled from a molecular dynamics simulation, as well as {compositional heterogeneity from mixtures of ribosome assembly states or 100 common complexes present in cells. We then analyze state-of-the-art heterogeneous reconstruction tools, including neural and non-neural methods, assess their sensitivity to noise, and propose new metrics for quantitative evaluation. We hope that CryoBench will be a foundational resource for accelerating algorithmic development and evaluation in the cryo-EM and machine learning communities. Project page: https://cryobench.cs.princeton.edu.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Minkyu Jeon;Rishwanth Raghu;Miro A. Astore;Geoffrey Woollard;J. Ryan Feathers;Alkin Kaz;Sonya M Hanson;Pilar Cossio;Ellen D Zhong,True,https://openreview.net/pdf?id=kP92Fyc6ry
kPBEAZU5Nm,Chain of Thoughtlessness? An Analysis of CoT in Planning,"Large language model (LLM) performance on reasoning problems typically does not generalize out of distribution. Previous work has claimed that this can be mitigated with chain of thought prompting--a method of demonstrating solution procedures--with the intuition that it is possible to in-context teach an LLM an algorithm for solving the problem.
This paper presents a case study of chain of thought on problems from Blocksworld, a classical planning domain, and examines the performance of two state-of-the-art LLMs across two axes: generality of examples given in prompt, and complexity of problems queried with each prompt. While our problems are very simple, we only find meaningful performance improvements from chain of thought prompts when those prompts are exceedingly specific to their problem class, and that those improvements quickly deteriorate as the size n of the query-specified stack grows past the size of stacks shown in the examples.
We also create scalable variants of three domains commonly studied in previous CoT papers and demonstrate the existence of similar failure modes.
Our results hint that, contrary to previous claims in the literature, CoT's performance improvements do not stem from the model learning general algorithmic procedures via demonstrations but depend on carefully engineering highly problem specific prompts. This spotlights drawbacks of chain of thought, especially the sharp tradeoff between possible performance gains and the amount of human labor necessary to generate examples with correct reasoning traces.",main,NeurIPS,2024,Poster,Kaya Stechly;Karthik Valmeekam;Subbarao Kambhampati,True,https://openreview.net/pdf?id=kPBEAZU5Nm
kWTvdSSH5W,A Data-Centric Perspective on Evaluating Machine Learning Models for Tabular Data,"Tabular data is prevalent in real-world machine learning applications, and new models for supervised learning of tabular data are frequently proposed. Comparative studies assessing performance differences typically have model-centered evaluation setups with overly standardized data preprocessing. This limits the external validity of these studies, as in real-world modeling pipelines, models are typically applied after dataset-specific preprocessing and feature engineering. We address this gap by proposing a data-centric evaluation framework. We select 10 relevant datasets from Kaggle competitions and implement expert-level preprocessing pipelines for each dataset. We conduct experiments with different preprocessing pipelines and hyperparameter optimization (HPO) regimes to quantify the impact of model selection, HPO, feature engineering, and test-time adaptation. Our main findings reveal: 1) After dataset-specific feature engineering, model rankings change considerably, performance differences decrease, and the importance of model selection reduces. 2) Recent models, despite their measurable progress, still significantly benefit from manual feature engineering. This holds true for both tree-based models and neural networks. 3) While tabular data is typically considered static, samples are often collected over time, and adapting to distribution shifts can be important even in supposedly static data. These insights suggest that research efforts should be directed toward a data-centric perspective, acknowledging that tabular data requires feature engineering and often exhibits temporal characteristics.",Datasets & Benchmarks,NeurIPS,2024,Poster,Andrej Tschalzev;Sascha Marton;Stefan Lüdtke;Christian Bartelt;Heiner Stuckenschmidt,False,https://openreview.net/pdf?id=kWTvdSSH5W
knxGmi6SJi,shapiq: Shapley Interactions for Machine Learning,"Originally rooted in game theory, the Shapley Value (SV) has recently become an important tool in machine learning research. Perhaps most notably, it is used for feature attribution and data valuation in explainable artificial intelligence. Shapley Interactions (SIs) naturally extend the SV and address its limitations by assigning joint contributions to groups of entities, which enhance understanding of black box machine learning models. Due to the exponential complexity of computing SVs and SIs, various methods have been proposed that exploit structural assumptions or yield probabilistic estimates given limited resources. In this work, we introduce shapiq, an open-source Python package that unifies state-of-the-art algorithms to efficiently compute SVs and any-order SIs in an application-agnostic framework. Moreover, it includes a benchmarking suite containing 11 machine learning applications of SIs with pre-computed games and ground-truth values to systematically assess computational performance across domains. For practitioners, shapiq is able to explain and visualize any-order feature interactions in predictions of models, including vision transformers, language models, as well as XGBoost and LightGBM with TreeSHAP-IQ. With shapiq, we extend shap beyond feature attributions and consolidate the application of SVs and SIs in machine learning that facilitates future research. The source code and documentation are available at https://github.com/mmschlk/shapiq.",Datasets & Benchmarks,NeurIPS,2024,Poster,Maximilian Muschalik;Hubert Baniecki;Fabian Fumagalli;Patrick Kolpaczki;Barbara Hammer;Eyke Hüllermeier,False,https://openreview.net/pdf?id=knxGmi6SJi
ks0FrTSCnK,Rethinking Open-set Noise in Learning with Noisy Labels,"To reduce reliance on labeled data, learning with noisy labels (LNL) has gained increasing attention. However, prevailing works typically assume that such datasets are primarily affected by closed-set noise (where the true/clean labels of noisy samples come from another known category), and ignore therefore the ubiquitous presence of open-set noise (where the true/clean labels of noisy samples may not belong to any known category).
In this paper, we formally refine the LNL problem setting considering the presence of open-set noise. We theoretically analyze and compare the effects of open-set noise and closed-set noise, as well as the effects between different open-set noise modes. We also analyze common open-set noise detection mechanisms based on prediction entropy values. To empirically validate the theoretical results, we construct two open-set noisy datasets - CIFAR100-O/ImageNet-O and introduce a novel open-set test set for the widely used WebVision benchmark. Our work suggests that open-set noise exhibits qualitatively and quantitatively distinct characteristics, and how to fairly and comprehensively evaluate models in this condition requires more exploration.",main,NeurIPS,2024,Reject,Chen Feng;Nicu Sebe;Ioannis Patras,True,https://openreview.net/pdf?id=ks0FrTSCnK
ktYaxX12RN,TSGM: A Flexible Framework for Generative Modeling of Synthetic Time Series,"Time series data are essential in a wide range of machine learning (ML) applications. However, temporal data are often scarce or highly sensitive, limiting data sharing and the use of data-intensive ML methods. A possible solution to this problem is the generation of synthetic datasets that resemble real data. In this work, we introduce Time Series Generative Modeling (TSGM), an open-source framework for the generative modeling and evaluation of synthetic time series datasets. TSGM includes a broad repertoire of machine learning methods: generative models, probabilistic, simulation-based approaches, and augmentation techniques. The framework enables users to evaluate the quality of the produced data from different angles: similarity, downstream effectiveness, predictive consistency, diversity, fairness, and privacy. TSGM is extensible and user-friendly, which allows researchers to rapidly implement their own methods and compare them in a shareable environment. The framework has been tested on open datasets and in production and proved to be beneficial in both cases. https://github.com/AlexanderVNikitin/tsgm",Datasets & Benchmarks,NeurIPS,2024,Poster,Alexander V Nikitin;Letizia Iannucci;Samuel Kaski,False,https://openreview.net/pdf?id=ktYaxX12RN
kvjbFVHpny,EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations,"How to evaluate Large Language Models (LLMs) in code generation remains an open question. 
Many benchmarks have been proposed, but they have two limitations, i.e., data leakage and lack of domain-specific evaluation.
The former hurts the fairness of benchmarks, and the latter hinders practitioners from selecting superior LLMs for specific programming domains.

To address these two limitations, we propose a new benchmark - EvoCodeBench, which has the following advances: 
(1) Evolving data. EvoCodeBench will be dynamically updated every period (e.g., 6 months) to avoid data leakage. This paper releases the first version - EvoCodeBench-2403, containing 275 samples from 25 repositories.
(2) A domain taxonomy and domain labels. Based on the statistics of open-source communities, we design a programming domain taxonomy consisting of 10 popular domains. Based on the taxonomy, we annotate each sample in EvoCodeBench with a domain label. EvoCodeBench provides a broad platform for domain-specific evaluations.
(3) Domain-specific evaluations. Besides the Pass@k, we compute the Domain-Specific Improvement (DSI) and define LLMs' comfort and strange domains. These evaluations help practitioners select superior LLMs in specific domains and discover the shortcomings of existing LLMs.
Besides, EvoCodeBench is collected by a rigorous pipeline and aligns with real-world repositories in multiple aspects (e.g., code distributions).
We evaluate 8 popular LLMs (e.g., gpt-4, DeepSeek Coder, StarCoder 2) on EvoCodeBench and summarize some insights. EvoCodeBench reveals the actual abilities of these LLMs in real-world repositories. For example, the highest Pass@1 of gpt-4 on EvoCodeBench-2403 is only 20.74%. Besides, we evaluate LLMs in different domains and discover their comfort and strange domains. For example, gpt-4 performs best in most domains but falls behind others in the Internet domain. StarCoder 2-15B unexpectedly performs well in the Database domain and even outperforms 33B LLMs. We release EvoCodeBench, all prompts, and LLMs' completions for further community analysis.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jia Li;Ge Li;Xuanming Zhang;Yunfei Zhao;Yihong Dong;Zhi Jin;Binhua Li;Fei Huang;Yongbin Li,True,https://openreview.net/pdf?id=kvjbFVHpny
kwDOxOmGE0,VRSBench: A Versatile Vision-Language Benchmark Dataset for Remote Sensing Image Understanding,"We introduce a new benchmark designed to advance the development of general-purpose, large-scale vision-language models for remote sensing images. Although several vision-language datasets in remote sensing have been proposed to pursue this goal, existing datasets are typically tailored to single tasks, lack detailed object information, or suffer from inadequate quality control. Exploring these improvement opportunities, we present a Versatile vision-language Benchmark for Remote Sensing image understanding, termed VRSBench. This benchmark comprises 29,614 images, with 29,614 human-verified detailed captions, 52,472 object references, and 123,221 question-answer pairs. It facilitates the training and evaluation of vision-language models across a broad spectrum of remote sensing image understanding tasks. We further evaluated state-of-the-art models on this benchmark for three vision-language tasks: image captioning, visual grounding, and visual question answering. Our work aims to significantly contribute to the development of advanced vision-language models in the field of remote sensing. The data and code can be accessed at https://vrsbench.github.io.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xiang Li;Jian Ding;Mohamed Elhoseiny,True,https://openreview.net/pdf?id=kwDOxOmGE0
l0Ydsl10ci,IMPACT: A Large-scale Integrated Multimodal Patent Analysis and Creation Dataset for Design Patents,"In this paper, we introduce IMPACT (Integrated Multimodal Patent Analysis and Creation Dataset for Design Patents), a large-scale multimodal patent dataset with detailed captions for design patent figures. Our dataset includes half a million design patents comprising 3.61 million figures along with captions from patents granted by the United States Patent and Trademark Office (USPTO) over a 16-year period from 2007 to 2022. We incorporate the metadata of each patent application with elaborate captions that are coherent with multiple viewpoints of designs.  Even though patents themselves contain a variety of design figures, titles, and descriptions of viewpoints, we find that they lack detailed descriptions that are necessary to perform multimodal tasks such as classification and retrieval. IMPACT closes this gap thereby providing researchers with necessary ingredients to instantiate a variety of multimodal tasks. Our dataset has a huge potential for novel design inspiration and can be used with advanced computer vision models in tandem. We perform preliminary evaluations on the dataset on the popular patent analysis tasks such as classification and retrieval. Our results indicate that integrating images with generated captions significantly improves the performance of different models on the corresponding tasks. Given that design patents offer various benefits for modeling novel tasks, we propose two standard computer vision tasks that have not been investigated in analyzing patents as future directions using IMPACT as a benchmark viz., 3D Image Construction and Visual Question Answering (VQA). To facilitate research in these directions, we make our IMPACT dataset and the code/models used in this work publicly available at https://github.com/AI4Patents/IMPACT.",Datasets & Benchmarks,NeurIPS,2024,Poster,Homaira Huda Shomee;Zhu Wang;Sathya N. Ravi;Sourav Medya,True,https://openreview.net/pdf?id=l0Ydsl10ci
l985bXCatk,LRVS-Fashion: Extending Visual Search with Referring Instructions,"This paper introduces a new challenge for image similarity search in the context of fashion, addressing the inherent ambiguity in this domain stemming from complex images. We present Referred Visual Search (RVS), a task allowing users to define more precisely the desired similarity, following recent interest in the industry. We release a new large public dataset, LRVS-Fashion, consisting of 272k fashion products with 842k images extracted from fashion catalogs, designed explicitly for this task. However, unlike traditional visual search methods in the industry, we demonstrate that superior performance can be achieved by bypassing explicit object detection and adopting weakly-supervised conditional contrastive learning on image tuples. Our method is lightweight and demonstrates robustness, reaching Recall at one superior to strong detection-based baselines against 2M distractors.",Datasets & Benchmarks,NeurIPS,2024,Reject,Simon Lepage;Jeremie Mary;David Picard,True,https://openreview.net/pdf?id=l985bXCatk
ldvfaYzG35,Pedestrian-Centric 3D Pre-collision Pose and Shape Estimation from Dashcam Perspective,"Pedestrian pre-collision pose is one of the key factors to determine the degree of pedestrian-vehicle injury in collision. Human pose estimation algorithm is an effective method to estimate pedestrian emergency pose from accident video. However, the pose estimation model trained by the existing daily human pose datasets has poor robustness under specific poses such as pedestrian pre-collision pose, and it is difficult to obtain human pose datasets in the wild scenes, especially lacking scarce data such as pedestrian pre-collision pose in traffic scenes. In this paper, we collect pedestrian-vehicle collision pose from the dashcam perspective of dashcam and construct the first Pedestrian-Vehicle Collision Pose dataset (PVCP) in a semi-automatic way, including 40k+ accident frames and 20K+ pedestrian pre-collision pose annotation (2D, 3D, Mesh). Further, we construct a Pedestrian Pre-collision Pose Estimation Network (PPSENet) to estimate the collision pose and shape sequence of pedestrians from pedestrian-vehicle accident videos. The PPSENet first estimates the 2D pose from the image (Image to Pose, ITP) and then lifts the 2D pose to 3D mesh (Pose to Mesh, PTM). Due to the small size of the dataset, we introduce a pre-training model that learns the human pose prior on a large number of pose datasets, and use iterative regression to estimate the pre-collision pose and shape of pedestrians. Further, we classify the pre-collision pose sequence and introduce pose class loss, which achieves the best accuracy compared with the existing relevant \\\\textit{state-of-the-art} methods. Code and data are available for research at https://github.com/wmj142326/PVCP.",main,NeurIPS,2024,Poster,MeiJun Wang;Yu Meng;Zhongwei Qiu;Chao Zheng;Yan Xu;Pengxiaorui;Jian Gao,True,https://openreview.net/pdf?id=ldvfaYzG35
leeosk2RAM,SearchLVLMs: A Plug-and-Play Framework for Augmenting Large Vision-Language Models by Searching Up-to-Date Internet Knowledge,"Large vision-language models (LVLMs) are ignorant of the up-to-date knowledge, such as LLaVA series, because they cannot be updated frequently due to the large amount of resources required, and therefore fail in many cases. For example, if a LVLM was released on January 2024, and it wouldn't know the singer of the theme song for the new Detective Conan movie, which wasn't released until April 2024. To solve the problem, a promising solution motivated by retrieval-augmented generation (RAG) is to provide LVLMs with up-to-date knowledge via internet search during inference, i.e., internet-augmented generation (IAG), which is already integrated in some closed-source commercial LVLMs such as GPT-4V. However, the specific mechanics underpinning them remain a mystery. In this paper, we propose a plug-and-play framework, for augmenting existing LVLMs in handling visual question answering (VQA) about up-to-date knowledge, dubbed SearchLVLMs. A hierarchical filtering model is trained to effectively and efficiently find the most helpful content from the websites returned by a search engine to prompt LVLMs with up-to-date knowledge. To train the model and evaluate our framework's performance, we propose a pipeline to automatically generate news-related VQA samples to construct a dataset, dubbed UDK-VQA. A multi-model voting mechanism is introduced to label the usefulness of website/content for VQA samples to construct the training set. Experimental results demonstrate the effectiveness of our framework, outperforming GPT-4o by $\\\\sim$30\\\\% in accuracy.",main,NeurIPS,2024,Poster,Chuanhao Li;Zhen Li;Chenchen Jing;Shuo Liu;Wenqi Shao;Yuwei Wu;Ping Luo;Yu Qiao;Kaipeng Zhang,True,https://openreview.net/pdf?id=leeosk2RAM
li3iFfkwRL,"M3LEO: A Multi-Modal, Multi-Label Earth Observation Dataset Integrating Interferometric SAR and Multispectral Data","Satellite-based remote sensing has revolutionised the way we address global challenges in a rapidly evolving world. Huge quantities of Earth Observation (EO) data are generated by satellite sensors daily, but processing these large datasets for use in ML pipelines is technically and computationally challenging. Specifically, different types of EO data are often hosted on a variety of platforms, with
differing degrees of availability for Python preprocessing tools. In addition, spatial alignment across data sources and data tiling for easier handling can present significant technical hurdles for novice users. While some preprocessed Earth observation datasets exist, their content is often limited to optical or near-optical wavelength data, which is ineffective at night or in adverse weather conditions.
Synthetic Aperture Radar (SAR), an active sensing technique based on microwave length radiation, offers a viable alternative. However, the application of machine learning to SAR has been limited due to a lack of ML-ready data and pipelines, particularly for the full diversity of SAR data, including polarimetry, coherence and interferometry. In this work, we introduce M3LEO, a multi-modal, multi-label
Earth observation dataset that includes polarimetric, interferometric, and coherence SAR data derived from Sentinel-1, alongside multispectral Sentinel-2 imagery and a suite of auxiliary data describing terrain properties such as land use. M3LEO spans approximately 17M data chips, each measuring 4x4 km, across six diverse geographic regions. The dataset is complemented by a flexible PyTorch Lightning framework, with configuration management using Hydra, to accommodate its use across diverse ML applications in Earth observation. Additionally, we provide tools to process any dataset available on popular platforms such as Google Earth Engine for seamless integration with our framework. We show that the distribution shift in self-supervised embeddings is substantial across geographic regions, even when controlling for terrain properties. Data is available at huggingface.co/M3LEO, and code at github.com/spaceml-org/M3LEO.",Datasets & Benchmarks,NeurIPS,2024,Poster,Matthew J Allen;Francisco Dorr;Joseph Alejandro Gallego Mejia;Laura Martínez-Ferrer;Anna Jungbluth;Freddie Kalaitzis;Raúl Ramos-Pollán,True,https://openreview.net/pdf?id=li3iFfkwRL
lk7SW0bH4x,ProbTS: Benchmarking Point and Distributional Forecasting across Diverse Prediction Horizons,"Delivering precise point and distributional forecasts across a spectrum of prediction horizons represents a significant and enduring challenge in the application of time-series forecasting within various industries.
Prior research on developing deep learning models for time-series forecasting has often concentrated on isolated aspects, such as long-term point forecasting or short-term probabilistic estimations. This narrow focus may result in skewed methodological choices and hinder the adaptability of these models to uncharted scenarios.
While there is a rising trend in developing universal forecasting models, a thorough understanding of their advantages and drawbacks, especially regarding essential forecasting needs like point and distributional forecasts across short and long horizons, is still lacking.
In this paper, we present ProbTS, a benchmark tool designed as a unified platform to evaluate these fundamental forecasting needs and to conduct a rigorous comparative analysis of numerous cutting-edge studies from recent years.
We dissect the distinctive data characteristics arising from disparate forecasting requirements and elucidate how these characteristics can skew methodological preferences in typical research trajectories, which often fail to fully accommodate essential forecasting needs.
Building on this, we examine the latest models for universal time-series forecasting and discover that our analyses of methodological strengths and weaknesses are also applicable to these universal models.
Finally, we outline the limitations inherent in current research and underscore several avenues for future exploration.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jiawen Zhang;Xumeng Wen;Zhenwei Zhang;Shun Zheng;Jia Li;Jiang Bian,False,https://openreview.net/pdf?id=lk7SW0bH4x
lnnNPiZtzR,FungiTastic: A multi-modal dataset and benchmark for image categorization,"We introduce a new, highly challenging benchmark and a dataset -- FungiTastic -- based on data continuously collected over a twenty-year span.
The dataset originates in fungal records labeled and curated by experts. It consists of about 350k multi-modal observations that include more than 650k photographs from 5k fine-grained categories and diverse 
accompanying information, e.g., acquisition metadata, satellite images, and body part segmentation. 
FungiTastic is the only benchmark that includes a test set with partially DNA-sequenced ground truth of unprecedented label reliability.
The benchmark is designed to support 
(i) standard close-set classification, 
(ii) open-set classification,
(iii) multi-modal classification,
(iv) few-shot learning, 
(v) domain shift, and many more.
We provide baseline methods tailored for almost all the use-cases.
We provide a multitude of ready-to-use pre-trained models on HuggingFace and a framework for model training.
A comprehensive documentation describing the dataset features and the baselines are available at \\\\href{https://sulc.github.io/DanishFungi2024/}{GitHub} and Kaggle.",Datasets & Benchmarks,NeurIPS,2024,Reject,Lukas Picek;Klara Janouskova;Milan Šulc;Jiri Matas,True,https://openreview.net/pdf?id=lnnNPiZtzR
lnuXaRpwvw,RedPajama: an Open Dataset for Training Large Language Models,"Large language models are increasingly becoming a cornerstone technology in artificial intelligence, the sciences, and society as a whole, yet the optimal strategies for dataset composition and filtering remain largely elusive. Many of the top-performing models lack transparency in their dataset curation and model development processes, posing an obstacle to the development of fully open language models. 
In this paper, we identify three core data-related challenges that must be addressed to advance open-source language models. These include (1) transparency in model development, including the data curation process, (2) access to large quantities of high-quality data, and (3) availability of artifacts and metadata for dataset curation and analysis. 
To address these challenges, we release RedPajama-V1, an open reproduction of the LLaMA training dataset. In addition, we release RedPajama-V2, a massive web-only dataset consisting of raw, unfiltered text data together with quality signals and metadata.
Together, the RedPajama datasets comprise over 100 trillion tokens spanning multiple domains and with their quality signals facilitate the filtering of data, aiming to inspire the development of numerous new datasets. To date, these datasets have already been used in the training of strong language models used in production, such as Snowflake Arctic, Salesforce's XGen and AI2's OLMo. To provide insight into the quality of RedPajama, we present a series of analyses and ablation studies with decoder-only language models with up to 1.6B parameters. Our findings demonstrate how quality signals for web data can be effectively leveraged to curate high-quality subsets of the dataset, underscoring the potential of RedPajama to advance the development of transparent and high-performing language models at scale.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Maurice Weber;Daniel Y Fu;Quentin Gregory Anthony;Yonatan Oren;Shane Adams;Anton Alexandrov;Xiaozhong Lyu;Huu Nguyen;Xiaozhe Yao;Virginia Adams;Ben Athiwaratkun;Rahul Chalamala;Kezhen Chen;Max Ryabinin;Tri Dao;Percy Liang;Christopher Re;Irina Rish;Ce Zhang,True,https://openreview.net/pdf?id=lnuXaRpwvw
loDHZstVP6,FinBen: A Holistic Financial Benchmark for Large Language Models,"LLMs have transformed NLP and shown promise in various fields, yet their potential in finance is underexplored due to a lack of comprehensive benchmarks, the rapid development of LLMs, and the complexity of financial tasks. In this paper, we introduce FinBen, the first extensive open-source evaluation benchmark, including 42 datasets spanning 24 financial tasks, covering eight critical aspects: information extraction (IE), textual analysis, question answering (QA), text generation, risk management, forecasting, decision-making, and bilingual (English and Spanish). FinBen offers several key innovations: a broader range of tasks and datasets, the first evaluation of stock trading, novel agent and Retrieval-Augmented Generation (RAG) evaluation, and two novel datasets for regulations and stock trading. Our evaluation of 21 representative LLMs, including GPT-4, ChatGPT, and the latest Gemini, reveals several key findings: While LLMs excel in IE and textual analysis, they struggle with advanced reasoning and complex tasks like text generation and forecasting. GPT-4 excels in IE and stock trading, while Gemini is better at text generation and forecasting. Instruction-tuned LLMs improve textual analysis but offer limited benefits for complex tasks such as QA. FinBen has been used to host the first financial LLMs shared task at the FinNLP-AgentScen workshop during IJCAI-2024, attracting 12 teams. Their novel solutions outperformed GPT-4, showcasing FinBen's potential to drive innovations in financial LLMs. All datasets and code are publicly available for the research community, with results shared and updated regularly on the Open Financial LLM Leaderboard.",Datasets & Benchmarks,NeurIPS,2024,Poster,Qianqian Xie;Weiguang Han;Zhengyu Chen;Ruoyu Xiang;Xiao Zhang;Yueru He;Mengxi Xiao;Dong Li;Yongfu Dai;Duanyu Feng;Yijing Xu;Haoqiang Kang;Ziyan Kuang;Chenhan Yuan;Kailai Yang;Zheheng Luo;Tianlin Zhang;Zhiwei Liu;GUOJUN XIONG;Zhiyang Deng;Yuechen Jiang;Zhiyuan Yao;Haohang Li;Yangyang Yu;Gang Hu;Huang Jiajia;Xiao-Yang Liu;Alejandro Lopez-Lira;Benyou Wang;Yanzhao Lai;Hao Wang;Min Peng;Sophia Ananiadou;Jimin Huang,True,https://openreview.net/pdf?id=loDHZstVP6
loJM1acwzf,MMLONGBENCH-DOC: Benchmarking Long-context Document Understanding with Visualizations,"Understanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, particularly in single-page document understanding (DU). However, their abilities on long-context DU remain an open problem. This work presents MMLONGBENCH-DOC, a long-context, multi- modal benchmark comprising 1,082 expert-annotated questions. Distinct from previous datasets, it is constructed upon 135 lengthy PDF-formatted documents with an average of 47.5 pages and 21,214 textual tokens. Towards comprehensive evaluation, answers to these questions rely on pieces of evidence from (1) different sources (text, image, chart, table, and layout structure) and (2) various locations (i.e., page number). Moreover, 33.7\\\\% of the questions are cross-page questions requiring evidence across multiple pages. 20.6\\\\% of the questions are designed to be unanswerable for detecting potential hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 44.9\\\\%, while the second-best, GPT-4V, scores 30.5\\\\%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse performance than their LLM counterparts which are fed with lossy-parsed OCR documents. These results validate the necessity of future research toward more capable long-context LVLMs.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Yubo Ma;Yuhang Zang;Liangyu Chen;Meiqi Chen;Yizhu Jiao;Xinze Li;Xinyuan Lu;Ziyu Liu;Yan Ma;Xiaoyi Dong;Pan Zhang;Liangming Pan;Yu-Gang Jiang;Jiaqi Wang;Yixin Cao;Aixin Sun,True,https://openreview.net/pdf?id=loJM1acwzf
lygceqe21t,FairJob: A Real-World Dataset for Fairness in Online Systems,"We introduce a fairness-aware dataset for job recommendation in advertising, designed to foster research in algorithmic fairness within real-world scenarios. It was collected and prepared to comply with privacy standards and business confidentiality. An additional challenge is the lack of access to protected user attributes such as gender, for which we propose a pragmatic solution to obtain a proxy estimate. Despite being anonymized and including a proxy for a sensitive attribute, our dataset preserves predictive power and maintains a realistic and challenging benchmark. This dataset addresses a significant gap in the availability of fairness-focused resources for high-impact domains like advertising -- the actual impact being having access or not to precious employment opportunities, where balancing fairness and utility is a common industrial challenge. We also explore various stages in the advertising process where unfairness can occur and introduce a method to compute a fair utility metric for the job recommendations in online systems case from a biased dataset. Experimental evaluations of bias mitigation techniques on the released dataset demonstrate potential improvements in fairness and the associated trade-offs with utility.

The dataset is hosted at https://huggingface.co/datasets/criteo/FairJob. Source code for the experiments is hosted at https://github.com/criteo-research/FairJob-dataset/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Mariia Vladimirova;Federico Pavone;Eustache Diemert,True,https://openreview.net/pdf?id=lygceqe21t
m1YYAQjO3w,AgentDojo: A Dynamic Environment to Evaluate Prompt Injection Attacks and Defenses for LLM Agents,"AI agents aim to solve complex tasks by combining text-based reasoning with external tool calls.
Unfortunately, AI agents are vulnerable to prompt injection attacks where data returned by external tools hijacks the agent to execute malicious tasks.
To measure the adversarial robustness of AI agents, we introduce AgentDojo, an evaluation framework for agents that execute tools over untrusted data.
To capture the evolving nature of attacks and defenses, AgentDojo is not a static test suite, but rather an extensible environment for designing and evaluating new agent tasks, defenses, and adaptive attacks.
We populate the environment with 97 realistic tasks (e.g., managing an email client, navigating an e-banking website, or making travel bookings), 629 security test cases, and various attack and defense paradigms from the literature.
We find that AgentDojo poses a challenge for both attacks and defenses: state-of-the-art LLMs fail at many tasks (even in the absence of attacks), and existing prompt injection attacks break some security properties but not all. We hope that AgentDojo can foster research on new design principles for AI agents that solve common tasks in a reliable and robust manner.",Datasets & Benchmarks,NeurIPS,2024,Poster,Edoardo Debenedetti;Jie Zhang;Mislav Balunovic;Luca Beurer-Kellner;Marc Fischer;Florian Tramèr,True,https://openreview.net/pdf?id=m1YYAQjO3w
mAG68wdggA,RedCode: Risky Code Execution and Generation Benchmark for Code Agents,"With the rapidly increasing capabilities and adoption of code agents for AI-assisted coding and software development, safety and security concerns, such as generating or executing malicious code, have become significant barriers to the real-world deployment of these agents. To provide comprehensive and practical evaluations on the safety of code agents, we propose RedCode, an evaluation platform with benchmarks grounded in four key principles: real interaction with systems, holistic evaluation of unsafe code generation and execution, diverse input formats, and high-quality safety scenarios and tests. RedCode consists of two parts to evaluate agents’ safety in unsafe code execution and generation: (1) RedCode-Exec provides challenging code prompts in Python as inputs, aiming to evaluate code agents’ ability to recognize and handle unsafe code. We then map the Python code to other programming languages (e.g., Bash) and natural text summaries or descriptions for evaluation, leading to a total of over 4,000 testing instances. We provide 25 types of critical vulnerabilities spanning various domains, such as websites, file systems, and operating systems. We provide a Docker sandbox environment to evaluate the execution capabilities of code agents and design corresponding evaluation metrics to assess their execution results. (2) RedCode-Gen provides 160 prompts with function signatures and docstrings as input to assess whether code agents will follow instructions to generate harmful code or software. Our empirical findings, derived from evaluating three agent frameworks based on 19 LLMs, provide insights into code agents’ vulnerabilities. For instance, evaluations on RedCode-Exec show that agents are more likely to reject executing unsafe operations on the operating system, but are less likely to reject executing technically buggy code, indicating high risks. Unsafe operations described in natural text lead to a lower rejection rate than those in code format. Additionally, evaluations on RedCode-Gen reveal that more capable base models and agents with stronger overall coding abilities, such as GPT4, tend to produce more sophisticated and effective harmful software. Our findings highlight the need for stringent safety evaluations for diverse code agents. Our dataset and code are publicly available at https://github.com/AI-secure/RedCode.",Datasets & Benchmarks,NeurIPS,2024,Poster,Chengquan Guo;Xun Liu;Chulin Xie;Andy Zhou;Yi Zeng;Zinan Lin;Dawn Song;Bo Li,True,https://openreview.net/pdf?id=mAG68wdggA
mDRmX8IlBI,MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos,"Multimodal Language Language Models (MLLMs) demonstrate the emerging abilities of ""world models""---interpreting and reasoning about complex real-world dynamics. 
To assess these abilities, we posit videos are the ideal medium, as they encapsulate rich representations of real-world dynamics and causalities.
To this end, we introduce MMWorld, a new benchmark for multi-discipline, multi-faceted multimodal video understanding.
MMWorld distinguishes itself from previous video understanding benchmarks with two unique advantages: (1) multi-discipline, covering various disciplines that often require domain expertise for comprehensive understanding; (2) multi-faceted reasoning, including explanation, counterfactual thinking, future prediction, etc.
MMWorld consists of a human-annotated dataset to evaluate MLLMs with questions about the whole videos and a synthetic dataset to analyze MLLMs within a single modality of perception. 
Together, MMWorld encompasses 1,910 videos across seven broad disciplines and 69 subdisciplines, complete with 6,627 question-answer pairs and associated captions. 
The evaluation includes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld (e.g., GPT-4V performs the best with only 52.3\\\\% accuracy), showing large room for improvement. Further ablation studies reveal other interesting findings such as models' different skill sets from humans. We hope MMWorld can serve as an essential step towards world model evaluation in videos.",Datasets & Benchmarks,NeurIPS,2024,Reject,Xuehai He;Weixi Feng;Kaizhi Zheng;Yujie Lu;Wanrong Zhu;Jiachen Li;Yue Fan;Jianfeng Wang;Linjie Li;Zhengyuan Yang;Kevin Lin;William Yang Wang;Lijuan Wang;Xin Eric Wang,True,https://openreview.net/pdf?id=mDRmX8IlBI
mEJgnZZyfv,OpenMixup: Open Mixup Toolbox and Benchmark for Visual Representation Learning,"Mixup augmentation has emerged as a powerful technique for improving the generalization ability of deep neural networks. However, the lack of standardized implementations and benchmarks has hindered progress, resulting in poor reproducibility, unfair comparisons, and conflicting insights. In this paper, we introduce OpenMixup, the \\\\textit{first} mixup augmentation benchmark for visual representation learning, where 18 representative mixup baselines are trained \\\\textit{from scratch} and systematically evaluated on 11 image datasets across varying scales and granularity, spanning fine-grained scenarios to complex non-iconic scenes. We also open-source a modular codebase for streamlined mixup method design, training, and evaluations, which comprises a collection of widely-used vision backbones, optimization policies, and analysis toolkits. Notably, the codebase not only underpins all our benchmarking but supports broader mixup applications beyond classification, such as self-supervised learning and regression tasks. Through extensive experiments, we present insights on performance-complexity trade-offs and identify preferred mixup strategies for different needs. To the best of our knowledge, OpenMixup has contributed to a number of studies in the mixup community. We hope this work can further advance reproducible mixup research and fair comparisons, thereby laying a solid foundation for future progress. The source code is publicly available at \\\\url{https://github.com/Westlake-AI/openmixup}.",Datasets & Benchmarks,NeurIPS,2024,Reject,Siyuan Li;Zedong Wang;Zicheng Liu;Di Wu;Cheng Tan;Weiyang Jin;Stan Z. Li,False,https://openreview.net/pdf?id=mEJgnZZyfv
mK1Utjiy8z,RClicks: Realistic Click Simulation for Benchmarking Interactive Segmentation,"The emergence of Segment Anything (SAM) sparked research interest in the field of interactive segmentation, especially in the context of image editing tasks and speeding up data annotation. Unlike common semantic segmentation, interactive segmentation methods allow users to directly influence their output through prompts (e.g. clicks). However, click patterns in real-world interactive segmentation scenarios remain largely unexplored. Most methods rely on the assumption that users would click in the center of the largest erroneous area. Nevertheless, recent studies show that this is not always the case. Thus, methods may have poor performance in real-world deployment despite high metrics in a baseline benchmark. To accurately simulate real-user clicks, we conducted a large crowdsourcing study of click patterns in an interactive segmentation scenario and collected 475K real-user clicks. Drawing on ideas from saliency tasks, we develop a clickability model that enables sampling clicks, which closely resemble actual user inputs. Using our model and dataset, we propose RClicks benchmark for a comprehensive comparison of existing interactive segmentation methods on realistic clicks. Specifically, we evaluate not only the average quality of methods, but also the robustness w.r.t. click patterns. According to our benchmark, in real-world usage interactive segmentation models may perform worse than it has been reported in the baseline benchmark, and most of the methods are not robust. We believe that RClicks is a significant step towards creating interactive segmentation methods that provide the best user experience in real-world cases.",Datasets & Benchmarks,NeurIPS,2024,Poster,Anton Antonov;Andrey Moskalenko;Denis Shepelev;Alexander Krapukhin;Konstantin Soshin;Anton Konushin;Vlad Shakhuro,True,https://openreview.net/pdf?id=mK1Utjiy8z
mMnL0n7Cwy,"Can Large Language Models Analyze Graphs like Professionals? A Benchmark, Datasets and Models","The need to analyze graphs is ubiquitous across various fields, from social networks to biological research and recommendation systems. Therefore, enabling the ability of large language models (LLMs) to process graphs is an important step toward more advanced general intelligence. However, current LLM benchmarks on graph analysis require models to directly reason over the prompts describing graph
topology, and are thus limited to small graphs with only a few dozens of nodes. In contrast, human experts typically write programs based on popular libraries for task solving, and can thus handle graphs with different scales. To this end, a question naturally arises: can LLMs analyze graphs like professionals? In this paper, we introduce ProGraph, a manually crafted benchmark containing 3 categories of graph tasks. The benchmark expects solutions based on programming instead of directly reasoning over raw inputs. Our findings reveal that the performance of current LLMs is unsatisfactory, with the best model achieving only 36% accuracy. To bridge this gap, we propose LLM4Graph datasets, which include crawled documents and auto-generated codes based on 6 widely used graph libraries. By augmenting closed-source LLMs with document retrieval and fine-tuning open-source ones on the codes, we show 11-32% absolute improvements in their accuracies. Our results underscore that the capabilities of LLMs in handling structured data are still under-explored, and show the effectiveness of LLM4Graph in enhancing LLMs’ proficiency of graph analysis. The benchmark, datasets and enhanced open-source
models are available at https://github.com/BUPT-GAMMA/ProGraph.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xin Li;Weize Chen;Qizhi Chu;Haopeng Li;Zhaojun Sun;Ran Li;Chen Qian;Yiwei Wei;Chuan Shi;Zhiyuan Liu;Maosong Sun;Cheng Yang,True,https://openreview.net/pdf?id=mMnL0n7Cwy
mZLlWaoeKq,JourneyBench: A Challenging One-Stop Vision-Language Understanding Benchmark of Generated Images,"Existing vision-language understanding benchmarks largely consist of images of objects in their usual contexts.
As a consequence, recent multimodal large language models can perform well with only a shallow visual understanding by relying on background language biases. Thus, strong performance on these benchmarks does not necessarily correlate with strong visual understanding. In this paper, we release JourneyBench, a comprehensive human-annotated benchmark of generated images designed to assess the model's fine-grained multimodal reasoning abilities across five tasks: complementary multimodal chain of thought, multi-image VQA, imaginary image captioning, VQA with hallucination triggers, and fine-grained retrieval with sample-specific distractors.
Unlike existing benchmarks, JourneyBench explicitly requires fine-grained multimodal reasoning in unusual imaginary scenarios where language bias and holistic image gist are insufficient. We benchmark state-of-the-art models on JourneyBench and analyze performance along a number of fine-grained dimensions. Results across all five tasks show that JourneyBench is exceptionally challenging for even the best models, indicating that models' visual reasoning abilities are not as strong as they first appear. We discuss the implications of our findings and propose avenues for further research.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zhecan Wang;Junzhang Liu;Chia-Wei Tang;Hani Alomari;Anushka Sivakumar;Rui Sun;Wenhao Li;Md. Atabuzzaman;Hammad Ayyubi;Haoxuan You;Alvi Md Ishmam;Kai-Wei Chang;Shih-Fu Chang;Chris Thomas,True,https://openreview.net/pdf?id=mZLlWaoeKq
mbAgKSfhjM,"USCILab3D: A Large-scale, Long-term, Semantically Annotated Outdoor Dataset","In this paper, we introduce the \\\\textbf{USCILab3D dataset}, a large-scale, annotated outdoor dataset designed for versatile applications across multiple domains, including computer vision, robotics, and machine learning. The dataset was acquired using a mobile robot equipped with 5 cameras and a 32-beam, $360^{\\\\circ}$ scanning LIDAR. The robot was teleoperated, over the course of a year and under a variety of weather and lighting conditions, through a rich variety of paths within the USC campus (229 acres = $\\\\sim 92.7$ hectares). The raw data was annotated using state-of-the-art large foundation models, and processed to provide multi-view imagery, 3D reconstructions, semantically-annotated images and point clouds (267 semantic categories), and text descriptions of images and objects within. The dataset also offers a diverse array of complex analyses using pose-stamping and trajectory data. In sum, the dataset offers 1.4M point clouds and 10M images ($\\\\sim 6$TB of data). Despite covering a narrower geographical scope compared to a whole-city dataset, our dataset prioritizes intricate intersections along with denser multi-view scene images and semantic point clouds, enabling more precise 3D labelling and facilitating a broader spectrum of 3D vision tasks. For data, code and more details, please visit our website.",Datasets & Benchmarks,NeurIPS,2024,Poster,Kiran Lekkala;Henghui Bao;Peixu Cai;Wei Zer Lim;Chen Liu;Laurent Itti,True,https://openreview.net/pdf?id=mbAgKSfhjM
mcY221BgKi,Learning Cooperative Trajectory Representations for Motion Forecasting,"Motion forecasting is an essential task for autonomous driving, and utilizing information from infrastructure and other vehicles can enhance forecasting capabilities.
Existing research mainly focuses on leveraging single-frame cooperative information to enhance the limited perception capability of the ego vehicle, while underutilizing the motion and interaction context of traffic participants observed from cooperative devices. 
In this paper, we propose a forecasting-oriented representation paradigm to utilize motion and interaction features from cooperative information. 
Specifically, we present V2X-Graph, a representative framework to achieve interpretable and end-to-end trajectory feature fusion for cooperative motion forecasting. 
V2X-Graph is evaluated on V2X-Seq in vehicle-to-infrastructure (V2I) scenarios.
To further evaluate on vehicle-to-everything (V2X) scenario, we construct the first real-world V2X motion forecasting dataset V2X-Traj, which contains multiple autonomous vehicles and infrastructure in every scenario.
Experimental results on both V2X-Seq and V2X-Traj show the advantage of our method. 
We hope both V2X-Graph and V2X-Traj will benefit the further development of cooperative motion forecasting.
Find the project at https://github.com/AIR-THU/V2X-Graph.",main,NeurIPS,2024,Poster,Hongzhi Ruan;Haibao Yu;Wenxian Yang;Siqi Fan;Zaiqing Nie,True,https://openreview.net/pdf?id=mcY221BgKi
mfJifAlRMY,Revisiting Referring Expression Comprehension Evaluation in the Era of Large Multimodal Models,"Referring expression comprehension (REC) involves localizing a target instance based on a textual description. Recent advancements in REC have been driven by large multimodal models (LMMs) like CogVLM, which achieved 92.44% accuracy on RefCOCO. However, this study questions whether existing benchmarks such as RefCOCO, RefCOCO+, and RefCOCOg, capture LMMs' comprehensive capabilities. We begin with a manual examination of these benchmarks, revealing high labeling error rates: 14% in RefCOCO, 24% in RefCOCO+, and 5% in RefCOCOg, which undermines the authenticity of evaluations. We address this by excluding problematic instances and reevaluating several LMMs capable of handling the REC task, showing significant accuracy improvements, thus highlighting the impact of benchmark noise. In response, we introduce Ref-L4, a comprehensive REC benchmark, specifically designed to evaluate modern REC models. Ref-L4 is distinguished by four key features: 1) a substantial sample size with 45,341 annotations; 2) a diverse range of object categories with 365 distinct types and varying instance scales from 30 to 3,767; 3) lengthy referring expressions averaging 24.2 words; and 4) an extensive vocabulary comprising 22,813 unique words. We evaluate a total of 24 large models on Ref-L4 and provide valuable insights. The cleaned versions of RefCOCO, RefCOCO+, and RefCOCOg, as well as our Ref-L4 benchmark and evaluation code, are available at https://github.com/JierunChen/Ref-L4.",Datasets & Benchmarks,NeurIPS,2024,Reject,Jierun Chen;Fangyun Wei;Jinjing Zhao;Sizhe Song;BOHUAI WU;Zhuoxuan Peng;S.-H. Chan;Hongyang Zhang,True,https://openreview.net/pdf?id=mfJifAlRMY
mhjRudcHcB,NN4SysBench: Characterizing Neural Network Verification for Computer Systems,"We present NN4SysBench, a benchmark suite for neural network verification that is composed of applications from the domain of computer systems. We call these neural networks for computer systems or NN4Sys. NN4Sys is booming: there are many proposals for using neural networks in computer systems—for example, databases, OSes, and networked systems—many of which are safety critical. Neural network verification is a technique to formally verify whether neural networks satisfy safety properties. We however observe that NN4Sys has some unique characteristics that today’s verification tools overlook and have limited support. Therefore, this benchmark suite aims at bridging the gap between NN4Sys and the verification by using impactful NN4Sys applications as benchmarks to illustrate computer systems’ unique challenges. We also build a compatible version of NN4SysBench, so that today’s verifiers can also work on these benchmarks with approximately the same verification difficulties. The code is available at https://github.com/lydialin1212/NN4Sys_Benchmark.",Datasets & Benchmarks,NeurIPS,2024,Poster,Shuyi Lin;Haoyu He;Tianhao Wei;Kaidi Xu;Huan Zhang;Gagandeep Singh;Changliu Liu;Cheng Tan,True,https://openreview.net/pdf?id=mhjRudcHcB
mlbVgVKwD7,Multi-Chain Graphs of Graphs: A New Approach to Analyzing Blockchain Datasets,"Machine learning applied to blockchain graphs offers significant opportunities for enhanced data analysis and applications. However, the potential of this field is constrained by the lack of a large-scale, cross-chain dataset that includes hierarchical graph-level data. To address this issue, we present novel datasets that provide detailed label information at the token level and integrate interactions between tokens across multiple blockchain platforms. We model transactions within each token as local graphs and the relationships between tokens as global graphs, collectively forming a ""Graphs of Graphs"" (GoG) approach. This innovative approach facilitates a deeper understanding of systemic structures and hierarchical interactions, which are essential for applications such as link prediction, anomaly detection, and token classification. We conduct a series of experiments demonstrating that this dataset delivers new insights and challenges for exploring GoG within the blockchain domain. Our work promotes advancements and opens new avenues for research in both the blockchain and graph communities. Source code and datasets are available at https://github.com/Xtra-Computing/Cryptocurrency-Graphs-of-graphs.",Datasets & Benchmarks,NeurIPS,2024,Poster,Bingqiao Luo;Zhen Zhang;Qian Wang;Bingsheng He,True,https://openreview.net/pdf?id=mlbVgVKwD7
mlhFJE7PKo,HEST-1k: A Dataset For Spatial Transcriptomics and Histology Image Analysis,"Spatial transcriptomics enables interrogating the molecular composition of tissue with ever-increasing resolution and sensitivity. However, costs, rapidly evolving technology, and lack of standards have constrained computational methods in ST to narrow tasks and small cohorts. In addition, the underlying tissue morphology, as reflected by H&E-stained whole slide images (WSIs), encodes rich information often overlooked in ST studies. Here, we introduce HEST-1k, a collection of 1,229 spatial transcriptomic profiles, each linked to a WSI and extensive metadata. HEST-1k was assembled from 153 public and internal cohorts encompassing 26 organs, two species (Homo Sapiens and Mus Musculus), and 367 cancer samples from 25 cancer types. HEST-1k processing enabled the identification of 2.1 million expression-morphology pairs and over 76 million nuclei. To support its development, we additionally introduce the HEST-Library, a Python package designed to perform a range of actions with HEST samples. We test HEST-1k and Library on three use cases: (1) benchmarking foundation models for pathology (HEST-Benchmark), (2) biomarker exploration, and (3) multimodal representation learning. HEST-1k, HEST-Library, and HEST-Benchmark can be freely accessed at https://github.com/mahmoodlab/hest.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Guillaume Jaume;Paul Doucet;Andrew H. Song;Ming Y. Lu;Cristina Almagro Pérez;Sophia J Wagner;Anurag Jayant Vaidya;Richard J. Chen;Drew FK Williamson;Ahrong Kim;Faisal Mahmood,True,https://openreview.net/pdf?id=mlhFJE7PKo
mljDUaQpln,Enhancing Reasoning Capabilities of LLMs via Principled Synthetic Logic Corpus,"Large language models (LLMs) are capable of solving a wide range of tasks, yet they have struggled with reasoning.
To address this, we propose $\\\\textbf{Additional Logic Training (ALT)}$, which aims to enhance LLMs' reasoning capabilities by program-generated logical reasoning samples.
We first establish principles for designing high-quality samples by integrating symbolic logic theory and previous empirical insights.
Then, based on these principles, we construct a synthetic corpus named $\\\\textbf{Formal} \\\\ \\\\textbf{Logic} \\\\ \\\\textbf{\\\\textit{D}eduction} \\\\ \\\\textbf{\\\\textit{D}iverse}$ (FLD$ _{\\\\times2}$), comprising numerous samples of multi-step deduction with unknown facts, diverse reasoning rules, diverse linguistic expressions, and challenging distractors.
Finally, we empirically show that ALT on FLD$ _{\\\\times2}$ substantially enhances the reasoning capabilities of state-of-the-art LLMs, including LLaMA-3.1-70B.
Improvements include gains of up to 30 points on logical reasoning benchmarks, up to 10 points on math and coding benchmarks, and 5 points on the benchmark suite BBH.",main,NeurIPS,2024,Poster,Terufumi Morishita;Gaku Morio;Atsuki Yamaguchi;Yasuhiro Sogawa,True,https://openreview.net/pdf?id=mljDUaQpln
mlmTxJwVsb,DMNet: Self-comparison Driven Model for Subject-independent Seizure Detection,"Automated seizure detection (ASD) using intracranial electroencephalography (iEEG) is critical for effective epilepsy treatment. However, the significant domain shift of iEEG signals across subjects poses a major challenge, limiting their applicability in real-world clinical scenarios. In this paper, we address this issue by analyzing the primary cause behind the failure of existing iEEG models for subject-independent seizure detection, and identify a critical universal seizure pattern: seizure events consistently exhibit higher average amplitude compared to adjacent normal events. To mitigate the domain shifts and preserve the universal seizure patterns, we propose a novel self-comparison mechanism. This mechanism effectively aligns iEEG signals across subjects and time intervals. Building upon these findings, we propose Difference Matrix-based Neural Network (DMNet), a subject-independent seizure detection model, which leverages self-comparison based on two constructed (contextual, channel-level) references to mitigate shifts of iEEG, and utilize a simple yet effective difference matrix to encode the universal seizure patterns. Extensive experiments show that DMNet significantly outperforms previous SOTAs while maintaining high efficiency on a real-world clinical dataset collected by us and two public datasets for subject-independent seizure detection. Moreover, the visualization results demonstrate that the generated difference matrix can effectively capture the seizure activity changes during the seizure evolution process. Additionally, we deploy our method in an online diagnosis system to illustrate its effectiveness in real clinical applications.",main,NeurIPS,2024,Poster,Shihao Tu;Linfeng Cao;Daoze Zhang;Junru Chen;Lvbin Ma;Yin Zhang;Yang Yang,True,https://openreview.net/pdf?id=mlmTxJwVsb
moMoWj7jLm,FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning,"The application of formulas is a fundamental ability of humans when addressing numerical reasoning problems. However, existing numerical reasoning datasets seldom explicitly indicate the formulas employed during the reasoning steps. To bridge this gap, we propose a dataset for formula-based numerical reasoning called FormulaReasoning, which consists of 5,420 questions. We further conduct evaluations on LLMs with size ranging from 7B to over 100B parameters utilizing zero-shot and few-shot chain-of-thoughts methods and we explored the approach of using retrieval-augmented LLMs when providing an external formula database. We divide the reasoning process into formula generation, parameter extraction, and calculation, and use the data augmentation method to enhance the model ability of the model with parameters count less than 7B. Our empirical findings underscore the significant potential for improvement in existing models when applied to our complex, formula-driven FormulaReasoning.",Datasets & Benchmarks,NeurIPS,2024,Reject,Xiao Li;Bolin Zhu;Sichen Liu;Yin Zhu;Yiwei liu;Gong Cheng,True,https://openreview.net/pdf?id=moMoWj7jLm
mp6OWpDIJC,Autonomous Agents for Collaborative Task under Information Asymmetry,"Large Language Model Multi-Agent Systems (LLM-MAS) have greatly progressed in solving complex tasks. It communicates among agents within the system to collaboratively solve tasks, under the premise of shared information. However, when agents' collaborations are leveraged to perform multi-person tasks, a new challenge arises due to information asymmetry, since each agent can only access the information of its human user. Previous MAS struggle to complete tasks under this condition. To address this, we propose a new MAS paradigm termed iAgents, which denotes Informative Multi-Agent Systems. In iAgents, the human social network is mirrored in the agent network, where agents proactively exchange human information necessary for task resolution, thereby overcoming information asymmetry. iAgents employs a novel agent reasoning mechanism, InfoNav, to navigate agents' communication towards effective information exchange. Together with InfoNav, iAgents organizes human information in a mixed memory to provide agents with accurate and comprehensive information for exchange. Additionally, we introduce InformativeBench, the first benchmark tailored for evaluating LLM agents' task-solving ability under information asymmetry. Experimental results show that iAgents can collaborate within a social network of 140 individuals and 588 relationships, autonomously communicate over 30 turns, and retrieve information from nearly 70,000 messages to complete tasks within 3 minutes.",main,NeurIPS,2024,Poster,Wei Liu;Chenxi Wang;YiFei Wang;Zihao Xie;Rennai Qiu;Yufan Dang;Zhuoyun Du;Weize Chen;Cheng Yang;Chen Qian,True,https://openreview.net/pdf?id=mp6OWpDIJC
mwIZW97PVQ,Deep Unlearn: Benchmarking Machine Unlearning,"Machine unlearning (MU) aims to remove the influence of particular data points from the learnable parameters of a trained machine learning model. This is a key capability in light of data privacy requirements, trustworthiness, and safety in deployed models. MU is particularly challenging for deep neural networks (DNNs), such as convolutional nets or vision transformers, as such DNNs tend to memorize a notable portion of their training dataset. Nevertheless, the community lacks a rigorous and multifaceted study that looks into the success of MU methods for DNNs. In this paper, we investigate 18 state-of-the-art MU methods across various benchmark datasets and models, with each evaluation conducted over 10 different  initializations, a comprehensive evaluation involving MU over 100K models. We show that, with the proper hyperparameters, Masked Small Gradients (MSG) and Convolution Transpose (CT), consistently perform better in terms of model accuracy and run-time efficiency across different models, datasets, and initializations,
assessed by population-based membership inference attacks (MIA) and per-sample unlearning likelihood ratio attacks (U-LiRA).
Furthermore, our benchmark highlights the fact that comparing a MU method only with commonly used baselines, such as Gradient Ascent (GA) or Successive Random Relabeling (SRL), is inadequate, 
and we need better baselines like Negative Gradient Plus (NG+) with proper hyperparameter selection.",Datasets & Benchmarks,NeurIPS,2024,Reject,Xavier Cadet;Anastasia Borovykh;Mohammad Malekzadeh;Sara Ahmadi-Abhari;Hamed Haddadi,False,https://openreview.net/pdf?id=mwIZW97PVQ
n5R6TvBVcX,WildTeaming at Scale: From In-the-Wild Jailbreaks to (Adversarially) Safer Language Models,"We introduce WildTeaming, an automatic red-teaming framework that mines in-the-wild user-chatbot interactions to discover 5.7K unique clusters of novel jailbreak tactics, and then composes selections of multiple mined tactics for systematic exploration of novel and even more challenging jailbreaks.
Compared to prior work that performed red-teaming via recruited human workers, gradient-based optimization, or iterative revision with large language models (LLMs), our work investigates jailbreaks from chatbot users in-the-wild who were not specifically instructed to break the system.  WildTeaming reveals previously unidentified vulnerabilities of frontier LLMs, resulting in more diverse and successful adversarial attacks compared to state-of-the-art jailbreaking methods. 

While there exist many datasets for jailbreak evaluation, very few open-source datasets exist for jailbreak training, as safety training data has been closed among all frontier models even when their weights are open. Therefore, with WildTeaming we create WildJailbreak, a large-scale open-source synthetic safety dataset with 262K vanilla (direct request) and adversarial (complex jailbreak) prompt-response pairs. In order to mitigate exaggerated safety behaviors, WildJailbreak provides two contrastive types of queries: 1) harmful queries (both vanilla and adversarial) and 2) benign queries that resemble harmful queries in form but contain no harmful intent. As WildJailbreak considerably upgrades the quality and scale of existing safety resources, it uniquely enables us to examine the scaling effects of data and the interplay of data properties and model capabilities during safety training. Through extensive model training and evaluations, we identify the training properties that enable an ideal balance of safety behaviors: appropriate safeguarding without over-refusal, effective handling of both vanilla and adversarial queries, and minimal, if any, decrease in general capabilities. All the components of WildJailbreak contribute to achieving balanced safety behaviors of models",main,NeurIPS,2024,Poster,Liwei Jiang;Kavel Rao;Seungju Han;Allyson Ettinger;Faeze Brahman;Sachin Kumar;Niloofar Mireshghallah;Ximing Lu;Maarten Sap;Yejin Choi;Nouha Dziri,True,https://openreview.net/pdf?id=n5R6TvBVcX
n6SCkn2QaG,The FineWeb Datasets: Decanting the Web for the Finest Text Data at Scale,"The performance of a large language model (LLM) depends heavily on the quality and size of its pretraining dataset. However, the pretraining datasets for state-of-the-art open LLMs like Llama 3 and Mixtral are not publicly available and very little is known about how they were created. In this work, we introduce FineWeb, a 15-trillion token dataset derived from 96 Common Crawl snapshots that produces better-performing LLMs than other open pretraining datasets. To advance the understanding of how best to curate high-quality pretraining datasets, we carefully document and ablate all of the design choices used in FineWeb, including in-depth investigations of deduplication and filtering strategies. In addition, we introduce FineWeb-Edu, a 1.3-trillion token collection of educational text filtered from FineWeb. LLMs pretrained on FineWeb-Edu exhibit dramatically better performance on knowledge- and reasoning-intensive benchmarks like MMLU and ARC. Along with our datasets, we publicly release our data curation codebase and all of the models trained during our ablation experiments.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Guilherme Penedo;Hynek Kydlíček;Loubna Ben allal;Anton Lozhkov;Margaret Mitchell;Colin Raffel;Leandro Von Werra;Thomas Wolf,True,https://openreview.net/pdf?id=n6SCkn2QaG
nAFBHoMpQs,MARPLE: A Benchmark for Long-Horizon Inference,"Reconstructing past events requires reasoning across long time horizons. To figure out what happened, humans draw on prior knowledge about the world and human behavior and integrate insights from various sources of evidence including visual, language, and auditory cues. We introduce MARPLE, a benchmark for evaluating long-horizon inference capabilities using multi-modal evidence. Our benchmark features agents interacting with simulated households, supporting vision, language, and auditory stimuli, as well as procedurally generated environments and agent behaviors. Inspired by classic ``whodunit'' stories, we ask AI models and human participants to infer which agent caused a change in the environment based on a step-by-step replay of what actually happened. The goal is to correctly identify the culprit as early as possible. Our findings show that human participants outperform both traditional Monte Carlo simulation methods and an LLM baseline (GPT-4) on this task. Compared to humans, traditional inference models are less robust and performant, while GPT-4 has difficulty comprehending environmental changes. We analyze factors influencing inference performance and ablate different modes of evidence, finding that all modes are valuable for performance. Overall, our experiments demonstrate that the long-horizon, multimodal inference tasks in our benchmark present a challenge to current models. Project website: https://marple-benchmark.github.io/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Emily Jin;Zhuoyi Huang;Jan-Philipp Fränken;Weiyu Liu;Hannah Cha;Erik Brockbank;Sarah A Wu;Ruohan Zhang;Jiajun Wu;Tobias Gerstenberg,True,https://openreview.net/pdf?id=nAFBHoMpQs
nMFVdphOc9,Rule Based Learning with Dynamic (Graph) Neural Networks,"A common problem of classical neural network architectures is that additional information or expert knowledge cannot be naturally integrated into the learning process.
  To overcome this limitation, we propose a two-step approach consisting of (1) generating formal rules from knowledge and (2) using these rules to define  rule based layers -- a
  new type of dynamic neural network layer.
  The focus of this work is on the second step, i.e., rule based layers that are designed to dynamically arrange learnable parameters in the weight matrices and bias vectors for each input sample following a formal rule.
  Indeed, we prove that our approach generalizes classical feed-forward layers such as fully connected and convolutional layers by choosing appropriate rules.
  As a concrete application we present rule based graph neural networks (RuleGNNs) that are by definition permutation equivariant and able to handle graphs of arbitrary sizes.
  Our experiments show that RuleGNNs are comparable to state-of-the-art graph classifiers using simple rules based on the Weisfeiler-Leman labeling and pattern counting.
  Moreover, we introduce new synthetic benchmark graph datasets to show how to integrate expert knowledge into RuleGNNs making them more powerful than ordinary graph neural networks.",main,NeurIPS,2024,Reject,Florian Seiffarth,True,https://openreview.net/pdf?id=nMFVdphOc9
nes2rMnbyL,AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making,"Traditional interactive environments limit agents' intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors.
To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures.  As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings. The code is available at https://github.com/bigai-ai/AdaSociety.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yizhe Huang;Xingbo Wang;Hao Liu;Fanqi Kong;Aoyang Qin;Min Tang;Xiaoxi Wang;Song-Chun Zhu;Mingjie Bi;Siyuan Qi;Xue Feng,False,https://openreview.net/pdf?id=nes2rMnbyL
nrEqH502eC,BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages,"Large language models (LLMs) often lack culture-specific everyday knowledge, especially across diverse regions and non-English languages. Existing benchmarks for evaluating LLMs' cultural sensitivities are usually limited to a single language or online sources like Wikipedia, which may not reflect the daily habits, customs, and lifestyles of different regions. That is, information about the food people eat for their birthday celebrations, spices they typically use, musical instruments youngsters play or the sports they practice in school is not always explicitly written online. To address this issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate LLMs' everyday knowledge across diverse cultures and languages. The benchmark comprises 52.6k question-answer pairs from 16 countries/regions, in 13 different languages, including low-resource ones such as Amharic, Assamese, Azerbaijani, Hausa, and Sundanese. We evaluate LLMs in two formats: short-answer questions, and multiple-choice questions. We show that LLMs perform better in cultures that are more present online, with a maximum 57.34% difference in GPT-4, the best-performing model, in the short-answer format.
Furthermore, we find that LLMs perform better in their local languages for mid-to-high-resource languages. Interestingly, for languages deemed to be low-resource, LLMs provide better answers in English. We make our dataset publicly available at: https://github.com/nlee0212/BLEnD.",Datasets & Benchmarks,NeurIPS,2024,Poster,Junho Myung;Nayeon Lee;Yi Zhou;Jiho Jin;Rifki Afina Putri;Dimosthenis Antypas;Hsuvas Borkakoty;Eunsu Kim;Carla Perez-Almendros;Abinew Ali Ayele;Victor Gutierrez Basulto;Yazmin Ibanez-Garcia;Hwaran Lee;Shamsuddeen Hassan Muhammad;Kiwoong Park;Anar Sabuhi Rzayev;Nina White;Seid Muhie Yimam;Mohammad Taher Pilehvar;Nedjma Ousidhoum;Jose Camacho-Collados;Alice Oh,True,https://openreview.net/pdf?id=nrEqH502eC
ntlFREw59A,ActAnywhere: Subject-Aware Video Background Generation,"We study a novel problem to automatically generate video background that tailors to foreground subject motion. It is an important problem for the movie industry and visual effects community, which traditionally requires tedious manual efforts to solve. To this end, we propose ActAnywhere, a video diffusion model that takes as input a sequence of foreground subject segmentation and an image of a novel background and generates a video of the subject interacting in this background. We train our model on a large-scale dataset of 2.4M videos of human-scene interactions. Through extensive evaluation, we show that our model produces videos with realistic foreground-background interaction while strictly following the guidance of the condition image. Our model generalizes to diverse scenarios including non-human subjects, gaming and animation clips, as well as videos with multiple moving subjects. Both quantitative and qualitative comparisons demonstrate that our model significantly outperforms existing methods, which fail to accomplish the studied task. Please visit our project webpage at https://actanywhere.github.io.",main,NeurIPS,2024,Poster,Boxiao Pan;Zhan Xu;Chun-Hao Paul Huang;Krishna Kumar Singh;Yang Zhou;Leonidas Guibas;Jimei Yang,True,https://openreview.net/pdf?id=ntlFREw59A
nv2Qt5cj1a,Membership Inference Attacks against Large Vision-Language Models,"Large vision-language models (VLLMs) exhibit promising capabilities for processing multi-modal tasks across various application scenarios. However, their emergence also raises significant data security concerns, given the potential inclusion of sensitive information, such as private photos and medical records, in their training datasets. Detecting inappropriately used data in VLLMs remains a critical and unresolved issue, mainly due to the lack of standardized datasets and suitable methodologies. In this study, we introduce the first membership inference attack (MIA) benchmark tailored for various VLLMs to facilitate training data detection. Then, we propose a novel MIA pipeline specifically designed for token-level image detection. Lastly, we present a new metric called MaxRényi-K%, which is based on the confidence of the model output and applies to both text and image data. We believe that our work can deepen the understanding and methodology of MIAs in the context of VLLMs. Our code and datasets are available at https://github.com/LIONS-EPFL/VL-MIA.",main,NeurIPS,2024,Poster,Zhan Li;Yongtao Wu;Yihang Chen;Francesco Tonin;Elias Abad Rocamora;Volkan Cevher,True,https://openreview.net/pdf?id=nv2Qt5cj1a
o7DOGbZeyP,LookHere: Vision Transformers with Directed Attention Generalize and Extrapolate,"High-resolution images offer more information about scenes that can improve model accuracy. However, the dominant model architecture in computer vision, the vision transformer (ViT), cannot effectively leverage larger images without finetuning — ViTs poorly extrapolate to more patches at test time, although transformers offer sequence length flexibility. We attribute this shortcoming to the current patch position encoding methods, which create a distribution shift when extrapolating.

We propose a drop-in replacement for the position encoding of plain ViTs that restricts attention heads to fixed fields of view, pointed in different directions, using 2D attention masks. Our novel method, called LookHere, provides translation-equivariance, ensures attention head diversity, and limits the distribution shift that attention heads face when extrapolating. We demonstrate that LookHere improves performance on classification (avg. 1.6%), against adversarial attack (avg. 5.4%), and decreases calibration error (avg. 1.5%) — on ImageNet without extrapolation. With extrapolation, LookHere outperforms the current SoTA position encoding method, 2D-RoPE, by 21.7% on ImageNet when trained at $224^2$ px and tested at $1024^2$ px. Additionally, we release a high-resolution test set to improve the evaluation of high-resolution image classifiers, called ImageNet-HR.",main,NeurIPS,2024,Poster,Anthony Fuller;Daniel Kyrollos;Yousef Yassin;James R Green,True,https://openreview.net/pdf?id=o7DOGbZeyP
o9Lkiv1qpc,Identifying and Solving Conditional Image Leakage in Image-to-Video Diffusion Model,"Diffusion models have obtained substantial progress in image-to-video generation. However, in this paper, we find that these models tend to generate videos with less motion than expected. We attribute this to the issue called conditional image leakage, where the image-to-video diffusion models (I2V-DMs) tend to over-rely on the conditional image at large time steps. We further address this challenge from both inference and training aspects. First, we propose to start the generation process from an earlier time step to avoid the unreliable large-time steps of I2V-DMs, as well as an initial noise distribution with optimal analytic expressions (Analytic-Init) by minimizing the KL divergence between it and the actual marginal distribution to bridge the training-inference gap. Second, we design a time-dependent noise distribution (TimeNoise) for the conditional image during training, applying higher noise levels at larger time steps to disrupt it and reduce the model's dependency on it. We validate these general strategies on various I2V-DMs on our collected open-domain image benchmark and the UCF101 dataset. Extensive results show that our methods outperform baselines by producing higher motion scores with lower errors while maintaining image alignment and temporal consistency, thereby yielding superior overall performance and enabling more accurate motion control. The project page: \\\\url{https://cond-image-leak.github.io/}.",main,NeurIPS,2024,Poster,Min Zhao;Hongzhou Zhu;Chendong Xiang;Kaiwen Zheng;Chongxuan Li;Jun Zhu,True,https://openreview.net/pdf?id=o9Lkiv1qpc
oIt0LAkqNb,Rethinking the Evaluation of Out-of-Distribution Detection: A Sorites Paradox,"Most existing out-of-distribution (OOD) detection benchmarks classify samples with novel labels as the OOD data. However, some marginal OOD samples actually have close semantic contents to the in-distribution (ID) sample, which makes determining the OOD sample a Sorites Paradox. In this paper, we construct a benchmark named Incremental Shift OOD (IS-OOD) to address the issue, in which we divide the test samples into subsets with different semantic and covariate shift degrees relative to the ID dataset. The data division is achieved through a shift measuring method based on our proposed Language Aligned Image feature Decomposition (LAID). Moreover, we construct a Synthetic Incremental Shift (Syn-IS) dataset that contains high-quality generated images with more diverse covariate contents to complement the IS-OOD benchmark. We evaluate current OOD detection methods on our benchmark and find several important insights: (1) The performance of most OOD detection methods significantly improves as the semantic shift increases; (2) Some methods like GradNorm may have different OOD detection mechanisms as they rely less on semantic shifts to make decisions; (3) Excessive covariate shifts in the image are also likely to be considered as OOD for some methods. Our code and data are released in https://github.com/qqwsad5/IS-OOD.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xingming Long;Jie Zhang;Shiguang Shan;Xilin Chen,True,https://openreview.net/pdf?id=oIt0LAkqNb
oJhYtNwGl9,CoMix: A Comprehensive Benchmark for Multi-Task Comic Understanding,"The comic domain is rapidly advancing with the development of single-page analysis and synthesis models. However, evaluation metrics and datasets lag behind, often limited to small-scale or single-style test sets. We introduce a novel benchmark, CoMix, designed to evaluate the multi-task capabilities of models in comic analysis. Unlike existing benchmarks that focus on isolated tasks such as object detection or text recognition, CoMix addresses a broader range of tasks including object detection, speaker identification, character re-identification, reading order, and multi-modal reasoning tasks like character naming and dialogue generation. Our benchmark comprises three existing datasets with expanded annotations to support multi-task evaluation. To mitigate the over-representation of manga-style data, we have incorporated a new dataset of carefully selected American comic-style books, thereby enriching the diversity of comic styles. CoMix is designed to assess pre-trained models in zero-shot and limited fine-tuning settings, probing their transfer capabilities across different comic styles and tasks. The validation split of the benchmark is publicly available for research purposes, and an evaluation server for the held-out test split is also provided. Comparative results between human performance and state-of-the-art models reveal a significant performance gap, highlighting substantial opportunities for advancements in comic understanding. The dataset, baseline models, and code are accessible at https://github.com/emanuelevivoli/CoMix-dataset. This initiative sets a new standard for comprehensive comic analysis, providing the community with a common benchmark for evaluation on a large and varied set.",Datasets & Benchmarks,NeurIPS,2024,Poster,Emanuele Vivoli;Marco Bertini;Dimosthenis Karatzas,True,https://openreview.net/pdf?id=oJhYtNwGl9
oOiXomyexS,A Benchmark on Directed Graph Representation Learning in Hardware Designs,"To keep pace with the rapid advancements in design complexity within modern computing systems, directed graph representation learning (DGRL) has become crucial, particularly for encoding circuit netlists, computational graphs, and developing surrogate models for hardware performance prediction. However, DGRL remains relatively unexplored, especially in the hardware domain, mainly due to the lack of comprehensive and user-friendly benchmarks. This study presents a novel benchmark comprising five hardware design datasets and 13 prediction tasks spanning various levels of circuit abstraction. We evaluate 21 DGRL models, employing diverse graph neural networks and graph transformers (GTs) as backbones, enhanced by positional encodings (PEs) tailored for directed graphs. Our results highlight that bidirected (BI) message passing neural networks (MPNNs) and robust PEs significantly enhance model performance. Notably, the top-performing models include PE-enhanced GTs interleaved with BI-MPNN layers and BI-Graph Isomorphism Network, both surpassing baselines across the 13 tasks. Additionally, our investigation into out-of-distribution (OOD) performance emphasizes the urgent need to improve OOD generalization in DGRL models. This benchmark, implemented with a modular codebase, streamlines the evaluation of DGRL models for both hardware and ML practitioners.",Datasets & Benchmarks,NeurIPS,2024,Reject,Haoyu Peter Wang;Yinan Huang;Nan Wu;Pan Li,True,https://openreview.net/pdf?id=oOiXomyexS
otxOtsWCMb,From an Image to a Scene: Learning to Imagine the World from a Million 360° Videos,"Three-dimensional (3D) understanding of objects and scenes play a key role in humans' ability to interact with the world and has been an active area of research in computer vision, graphics, and robotics. Large scale synthetic and object-centric 3D datasets have shown to be effective in training models that have 3D understanding of objects. However, applying a similar approach to real-world objects and scenes is difficult due to a lack of large-scale data. Videos are a potential source for real-world 3D data, but finding diverse yet corresponding views of the same content have shown to be difficult at scale. Furthermore, standard videos come with fixed viewpoints, determined at the time of capture. This restricts the ability to access scenes from a variety of more diverse and potentially useful perspectives. We argue that large scale ODIN videos can address these limitations to provide scalable corresponding frames from diverse views.  In this paper we introduce 360-1M, a 360° video dataset consisting of 1 million videos, and a process for efficiently finding corresponding frames from diverse viewpoints at scale. We train our diffusion-based model, ODIN, on 360-1M. Empowered by the largest real-world, multi-view dataset to date, ODIN is able to freely generate novel views of real-world scenes. Unlike previous methods, ODIN can move the camera through the environment, enabling the model to infer the geometry and layout of the scene. Additionally, we show improved performance on standard novel view synthesis and 3D reconstruction benchmarks.",main,NeurIPS,2024,Poster,Matthew Wallingford;Anand Bhattad;Aditya Kusupati;Vivek Ramanujan;Matt Deitke;Aniruddha Kembhavi;Roozbeh Mottaghi;Wei-Chiu Ma;Ali Farhadi,True,https://openreview.net/pdf?id=otxOtsWCMb
oxV50yFy5T,Lightning-UQ-Box: A Comprehensive Framework for Uncertainty Quantification in Deep Learning,"Uncertainty quantification (UQ) is an essential tool for applying deep neural networks (DNNs) to real world tasks, as it attaches a degree of confidence to DNN outputs. However, despite its benefits, UQ is often left out of the standard DNN workflow due to the additional technical knowledge required to apply and evaluate existing UQ procedures.  Hence there is a need for a comprehensive toolbox that allows the user to integrate UQ into their modeling workflow, without significant overhead.  We introduce Lightning UQ Box: a unified interface for applying and evaluating various approaches to UQ.  In this paper, we provide a theoretical and quantitative comparison of the wide range of state-of-the-art UQ methods implemented in our toolbox.  We focus on two challenging vision tasks: (i) estimating tropical cyclone wind speeds from infrared satellite imagery and (ii) estimating the power output of solar panels from RGB images of the sky.  Our results demonstrate the need for a broad and approachable experimental framework for UQ, that can be used for benchmarking by highlighting the differences between UQ methods.
The toolbox, example implementations, and further information are available at: https://github.com/lightning-uq-box/lightning-uq-box.",Datasets & Benchmarks,NeurIPS,2024,Reject,Nils Lehmann;Jakob Gawlikowski;Adam J Stewart;Vytautas Jančauskas;Stefan Depeweg;Eric Nalisnick;Nina Maria Gottschling,False,https://openreview.net/pdf?id=oxV50yFy5T
p1LpXNPmIa,PromptFix: You Prompt and We Fix the Photo,"Diffusion models equipped with language models demonstrate excellent controllability in image generation tasks, allowing image processing to adhere to human instructions. However, the lack of diverse instruction-following data hampers the development of models that effectively recognize and execute user-customized instructions, particularly in low-level tasks. Moreover, the stochastic nature of the diffusion process leads to deficiencies in image generation or editing tasks that require the detailed preservation of the generated images. To address these limitations, we propose PromptFix, a comprehensive framework that enables diffusion models to follow human instructions to perform a wide variety of image-processing tasks. First, we construct a large-scale instruction-following dataset that covers comprehensive image-processing tasks, including low-level tasks, image editing, and object creation. Next, we propose a high-frequency guidance sampling method to explicitly control the denoising process and preserve high-frequency details in unprocessed areas. Finally, we design an auxiliary prompting adapter, utilizing Vision-Language Models (VLMs) to enhance text prompts and improve the model's task generalization. Experimental results show that PromptFix outperforms previous methods in various image-processing tasks. Our proposed model also achieves comparable inference efficiency with these baseline models and exhibits superior zero-shot capabilities in blind restoration and combination tasks.",main,NeurIPS,2024,Poster,Yongsheng Yu;Ziyun Zeng;Hang Hua;Jianlong Fu;Jiebo Luo,True,https://openreview.net/pdf?id=p1LpXNPmIa
p8eUitex7p,ImageNet3D: Towards General-Purpose Object-Level 3D Understanding,"A vision model with general-purpose object-level 3D understanding should be capable of inferring both 2D (*e.g.*, class name and bounding box) and 3D information (*e.g.*, 3D location and 3D viewpoint) for arbitrary rigid objects in natural images. This is a challenging task, as it involves inferring 3D information from 2D signals and most importantly, generalizing to rigid objects from unseen categories. However, existing datasets with object-level 3D annotations are often limited by the number of categories or the quality of annotations. Models developed on these datasets become specialists for certain categories or domains, and fail to generalize. In this work, we present ImageNet3D, a large dataset for general-purpose object-level 3D understanding. ImageNet3D augments 200 categories from the ImageNet dataset with 2D bounding box, 3D pose, 3D location annotations, and image captions interleaved with 3D information. With the new annotations available in ImageNet3D, we could (i) analyze the object-level 3D awareness of visual foundation models, and (ii) study and develop general-purpose models that infer both 2D and 3D information for arbitrary rigid objects in natural images, and (iii) integrate unified 3D models with large language models for 3D-related reasoning. We consider two new tasks, probing of object-level 3D awareness and open vocabulary pose estimation, besides standard classification and pose estimation. Experimental results on ImageNet3D demonstrate the potential of our dataset in building vision models with stronger general-purpose object-level 3D understanding. Our dataset and project page are available here: https://imagenet3d.github.io.",Datasets & Benchmarks,NeurIPS,2024,Poster,Wufei Ma;Guofeng Zhang;Qihao Liu;Guanning Zeng;Adam Kortylewski;Yaoyao Liu;Alan Yuille,True,https://openreview.net/pdf?id=p8eUitex7p
pBeoAGdIuy,Query of CC: Unearthing Large Scale Domain-Specific Knowledge from Public Corpora,"Large language models(LLMs) have demonstrated remarkable potential in various tasks, however, there remains a significant lack of open-source models and data for specific domains. Previous work has primarily focused on manually specifying resources and collecting high-quality data for specific domains, which is extremely time-consuming and labor-intensive. To address this limitation, we introduce large models into the data collection pipeline to guide the generation of domain-specific information and retrieve relevant data from Common Crawl(CC), a large public corpus. We called this method as Query of CC. It not only collects data related to domain-specific knowledge but also mines the data with potential reasoning procedures from the public corpus. By applying this method, we have collected a knowledge domain-related dataset named Knowledge Pile, which covers four main domains, including the sciences, humanities, and other categories. Through the analysis of Knowledge Pile, Query of CC can effectively retrieve relevant data from the covered knowledge domains and significantly enhance the performance in tests of mathematical and knowledge-related reasoning abilities. We have open-sourced our data on HuggingFace to promote academic progress in knowledge reasoning capabilities.",Datasets & Benchmarks,NeurIPS,2024,Reject,Zhaoye Fei;Yunfan Shao;Linyang Li;Zhiyuan Zeng;Conghui He;Hang Yan;Dahua Lin;Xipeng Qiu,True,https://openreview.net/pdf?id=pBeoAGdIuy
pF5QjxzwHi,Continuous Perception Benchmark,"Humans continuously perceive and process visual signals. However, current video models typically either sample key frames sparsely or divide videos into chunks and densely sample within each chunk. This approach stems from the fact that most existing video benchmarks can be addressed by analyzing key frames or aggregating information from separate chunks. We anticipate that the next generation of vision models will emulate human perception by processing visual input continuously and holistically. To facilitate the development of such models, we propose the Continuous Perception Benchmark, a video question answering task that cannot be solved by focusing solely on a few frames or by captioning small chunks and then summarizing using language models. Extensive experiments demonstrate that existing vision models, whether commercial or open-source, struggle with these tasks, indicating the need for new technical advancements in this direction.",Datasets & Benchmarks,NeurIPS,2024,Reject,Zeyu Wang;Zhenzhen Weng;Serena Yeung-Levy,True,https://openreview.net/pdf?id=pF5QjxzwHi
pLoX8Og3bH,Unleashing Multispectral Video's Potential in Semantic Segmentation: A Semi-supervised Viewpoint and New UAV-View Benchmark,"Thanks to the rapid progress in RGB & thermal imaging, also known as multispectral imaging, the task of multispectral video semantic segmentation, or MVSS in short, has recently drawn significant attentions. Noticeably, it offers new opportunities in improving segmentation performance under unfavorable visual conditions such as poor light or overexposure. Unfortunately, there are currently very few datasets available, including for example MVSeg dataset that focuses purely toward eye-level view; and it features the sparse annotation nature due to the intensive demands of labeling process. To address these key challenges of the MVSS task, this paper presents two major contributions: the introduction of MVUAV, a new MVSS benchmark dataset, and the development of a dedicated semi-supervised MVSS baseline - SemiMV. Our MVUAV dataset is captured via Unmanned Aerial Vehicles (UAV), which offers a unique oblique bird’s-eye view complementary to the existing MVSS datasets; it also encompasses a broad range of day/night lighting conditions and over 30 semantic categories. In the meantime, to better leverage the sparse annotations and extra unlabeled RGB-Thermal videos, a semi-supervised learning baseline, SemiMV, is proposed to enforce consistency regularization through a dedicated Cross-collaborative Consistency Learning (C3L) module and a denoised temporal aggregation strategy. Comprehensive empirical evaluations on both MVSeg and MVUAV benchmark datasets have showcased the efficacy of our SemiMV baseline.",main,NeurIPS,2024,Poster,Wei Ji;Jingjing Li;Wenbo Li;Yilin Shen;Li cheng;Hongxia Jin,True,https://openreview.net/pdf?id=pLoX8Og3bH
pUcTrjRLOM,UltraMedical: Building Specialized Generalists in Biomedicine,"Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains and are moving towards more specialized areas. Recent advanced proprietary models such as GPT-4 and Gemini have achieved significant advancements in biomedicine, which have also raised privacy and security challenges. The construction of specialized generalists hinges largely on high-quality datasets, enhanced by techniques like supervised fine-tuning and reinforcement learning from human or AI feedback, and direct preference optimization. However, these leading technologies (e.g., preference learning) are still significantly limited in the open source community due to the scarcity of specialized data. In this paper, we present the UltraMedical collections, which consist of high-quality manual and synthetic datasets in the biomedicine domain, featuring preference annotations across multiple advanced LLMs. By utilizing these datasets, we fine-tune a suite of specialized medical models based on Llama-3 series, demonstrating breathtaking capabilities across various medical benchmarks. Moreover, we develop powerful reward models skilled in biomedical and general reward benchmark, enhancing further online preference learning within the biomedical LLM community.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Kaiyan Zhang;Sihang Zeng;Ermo Hua;Ning Ding;Zhang-Ren Chen;Zhiyuan Ma;Haoxin Li;Ganqu Cui;Biqing Qi;Xuekai Zhu;Xingtai Lv;Hu Jinfang;Zhiyuan Liu;Bowen Zhou,True,https://openreview.net/pdf?id=pUcTrjRLOM
pYNl76onJL,VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models,"The arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, along with other text-to-video diffusion models, is highly reliant on prompts, and there is no publicly available dataset that features a study of text-to-video prompts. In this paper, we introduce VidProM, the first large-scale dataset comprising 1.67 Million unique text-to-Video Prompts from real users. Additionally, this dataset includes 6.69 million videos generated by four state-of-the-art diffusion models, alongside some related data. We initially discuss the curation of this large-scale dataset, a process that is both time-consuming and costly. Subsequently, we underscore the need for a new prompt dataset specifically designed for text-to-video generation by illustrating how VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation. Our extensive and diverse dataset also opens up many exciting new research areas. For instance, we suggest exploring text-to-video prompt engineering, efficient video generation, and video copy detection for diffusion models to develop better, more efficient, and safer models. The project (including the collected dataset VidProM and related code) is publicly available at https://vidprom.github.io under the CC-BY-NC 4.0 License.",Datasets & Benchmarks,NeurIPS,2024,Poster,Wenhao Wang;Yi Yang,True,https://openreview.net/pdf?id=pYNl76onJL
paYwtPBpyZ,Sequence-Augmented SE(3)-Flow Matching For Conditional Protein Generation,"Proteins are essential for almost all biological processes and derive their diverse functions from complex $3 \\\\rm D$ structures, which are in turn determined by their amino acid sequences. 
In this paper, we exploit the rich biological inductive bias of amino acid sequences and introduce FoldFlow++, a novel sequence-conditioned $\\\\text{SE}(3)$-equivariant flow matching model for protein structure generation. FoldFlow++ presents substantial new architectural features over the previous FoldFlow family of models including a protein large language model to encode sequence, a new multi-modal fusion trunk that combines structure and sequence representations, and a geometric transformer based decoder. To increase 
diversity and novelty of generated samples -- crucial for de-novo drug design -- we
train FoldFlow++ at scale on a new dataset 
that is an order of magnitude 
larger than PDB datasets of prior works, containing both known proteins in PDB and high-quality synthetic structures achieved through filtering. We further demonstrate the ability to align FoldFlow++ to arbitrary rewards, e.g. increasing secondary structures diversity, by introducing a Reinforced Finetuning (ReFT) objective. We empirically observe that FoldFlow++ outperforms previous state-of-the-art protein structure-based generative models, improving over RFDiffusion in terms of unconditional generation across all metrics including designability, diversity, and novelty across all protein lengths, as well as exhibiting generalization on the task of equilibrium conformation sampling. Finally, we demonstrate that a fine-tuned FoldFlow++ makes progress on challenging conditional design tasks such as designing scaffolds for the VHH nanobody.",main,NeurIPS,2024,Poster,Guillaume Huguet;James Vuckovic;Kilian FATRAS;Eric Thibodeau-Laufer;Pablo Lemos;Riashat Islam;Cheng-Hao Liu;Jarrid Rector-Brooks;Tara Akhound-Sadegh;Michael M. Bronstein;Alexander Tong;Joey Bose,True,https://openreview.net/pdf?id=paYwtPBpyZ
pbscHlRG35,RealMAN: A Real-Recorded and Annotated Microphone Array Dataset for Dynamic Speech Enhancement and Localization,"The training of deep learning-based multichannel speech enhancement and source localization systems relies heavily on the simulation of room impulse response and multichannel diffuse noise, due to the lack of large-scale real-recorded datasets. However, the acoustic mismatch between simulated and real-world data could degrade the model performance when applying in real-world scenarios. To bridge this simulation-to-real gap, this paper presents a new relatively large-scale Real-recorded and annotated Microphone Array speech&Noise (RealMAN) dataset. The proposed dataset is valuable in two aspects: 1) benchmarking speech enhancement and localization algorithms in real scenarios; 2) offering a substantial amount of real-world training data for potentially improving the performance of real-world applications. Specifically, a 32-channel array with high-fidelity microphones is used for recording. A loudspeaker is used for playing source speech signals (about 35 hours of Mandarin speech). A total of 83.7 hours of speech signals (about 48.3 hours for static speaker and 35.4 hours for moving speaker) are recorded in 32 different scenes, and 144.5 hours of background noise are recorded in 31 different scenes. Both speech and noise recording scenes cover various common indoor, outdoor, semi-outdoor and transportation environments, which enables the training of general-purpose speech enhancement and source localization networks. To obtain the task-specific annotations, speaker location is annotated with an omni-directional fisheye camera by automatically detecting the loudspeaker. The direct-path signal is set as the target clean speech for speech enhancement, which is obtained by filtering the source speech signal with an estimated direct-path propagation filter. Baseline experiments demonstrate that i) compared to using simulated data, the proposed dataset is indeed able to train better speech enhancement and source localization networks; ii) using various sub-arrays of the proposed 32-channel microphone array can successfully train variable-array networks that can be directly used to unseen arrays.",Datasets & Benchmarks,NeurIPS,2024,Poster,Bing Yang;Changsheng Quan;Yabo Wang;Pengyu Wang;Yujie Yang;Ying Fang;Nian Shao;Hui Bu;Xin Xu;Xiaofei Li,True,https://openreview.net/pdf?id=pbscHlRG35
pc4GSBi1Hx,LoTLIP: Improving Language-Image Pre-training for Long Text Understanding,"In this work, we empirically confirm that the key reason causing such an issue is that the training images are usually paired with short captions, leaving certain tokens easily overshadowed by salient tokens. Towards this problem, our initial attempt is to relabel the data with long captions, however, directly learning with which may lead to performance degradation in understanding short text (e.g., in the image classification task). Then, after incorporating corner tokens to aggregate diverse textual information, we manage to help the model catch up to its original level of short text understanding yet greatly enhance its capability of long text understanding. We further look into whether the model can continuously benefit from longer captions and notice a clear trade-off between the performance and the efficiency. Finally, we validate the effectiveness of our approach using a self-constructed large-scale dataset, which consists of 100M long caption oriented text-image pairs. Our method achieves superior performance in long-text-image retrieval tasks. The project page is available at https://wuw2019.github.io/lot-lip.",main,NeurIPS,2024,Poster,Wei Wu;Kecheng Zheng;Shuailei Ma;Fan Lu;Yuxin Guo;Yifei Zhang;Wei Chen;Qingpei Guo;Yujun Shen;Zheng-Jun Zha,True,https://openreview.net/pdf?id=pc4GSBi1Hx
pjD08dtAh0,HumanVLA: Towards Vision-Language Directed Object Rearrangement by Physical Humanoid,"Physical Human-Scene Interaction (HSI) plays a crucial role in numerous applications. 
    However, existing HSI techniques are limited to specific object dynamics and privileged information, which prevents the development of more comprehensive applications.
    To address this limitation, we introduce HumanVLA for general object rearrangement directed by practical vision and language. 
    A teacher-student framework is utilized to develop HumanVLA.
    A state-based teacher policy is trained first using goal-conditioned reinforcement learning and adversarial motion prior.
    Then, it is distilled into a vision-language-action model via behavior cloning.
    We propose several key insights to facilitate the large-scale learning process.
    To support general object rearrangement by physical humanoid, we introduce a novel Human-in-the-Room dataset encompassing various rearrangement tasks.
    Through extensive experiments and analysis, we demonstrate the effectiveness of our approach.",main,NeurIPS,2024,Poster,Xinyu Xu;Yizheng Zhang;Yong-Lu Li;Lei Han;Cewu Lu,True,https://openreview.net/pdf?id=pjD08dtAh0
plIuBfYpXj,"Towards Heterogeneous Long-tailed Learning: Benchmarking, Metrics, and Toolbox","Long-tailed data distributions pose challenges for a variety of domains like e-commerce, finance, biomedical science, and cyber security, where the performance of machine learning models is often dominated by head categories while tail categories are inadequately learned. This work aims to provide a systematic view of long-tailed learning with regard to three pivotal angles: (A1) the characterization of data long-tailedness, (A2) the data complexity of various domains, and (A3) the heterogeneity of emerging tasks. We develop HeroLT, a comprehensive long-tailed learning benchmark integrating 18 state-of-the-art algorithms, 10 evaluation metrics, and 17 real-world datasets across 6 tasks and 4 data modalities. HeroLT with novel angles and extensive experiments (315 in total) enables effective and fair evaluation of newly proposed methods compared with existing baselines on varying dataset types. Finally, we conclude by highlighting the significant applications of long-tailed learning and identifying several promising future directions. For accessibility and reproducibility, we open-source our benchmark HeroLT and corresponding results at https://github.com/SSSKJ/HeroLT.",Datasets & Benchmarks,NeurIPS,2024,Poster,Haohui Wang;Weijie Guan;Jianpeng Chen;Zi Wang;Dawei Zhou,False,https://openreview.net/pdf?id=plIuBfYpXj
pn0Jpyvrc2,AllClear: A Comprehensive Dataset and Benchmark for Cloud Removal in Satellite Imagery,"Clouds in satellite imagery pose a significant challenge for downstream applications.
A major challenge in current cloud removal research is the absence of a comprehensive benchmark and a sufficiently large and diverse training dataset.
To address this problem, we introduce the largest public dataset -- *AllClear* for cloud removal, featuring 23,742 globally distributed regions of interest (ROIs) with diverse land-use patterns, comprising 4 million images in total. Each ROI includes complete temporal captures from the year 2022, with (1) multi-spectral optical imagery from Sentinel-2 and Landsat 8/9, (2) synthetic aperture radar (SAR) imagery from Sentinel-1, and (3) auxiliary remote sensing products such as cloud masks and land cover maps.
We validate the effectiveness of our dataset by benchmarking performance, demonstrating the scaling law - the PSNR rises from $28.47$ to $33.87$ with $30\\\\times$ more data, and conducting ablation studies on the temporal length and the importance of individual modalities. This dataset aims to provide comprehensive coverage of the Earth's surface and promote better cloud removal results.",Datasets & Benchmarks,NeurIPS,2024,Poster,Hangyu Zhou;Chia Hsiang Kao;Cheng Perng Phoo;Utkarsh Mall;Bharath Hariharan;Kavita Bala,True,https://openreview.net/pdf?id=pn0Jpyvrc2
pwLdvYIMrF,Train-Attention: Meta-Learning Where to Focus in Continual Knowledge Learning,"Previous studies on continual knowledge learning (CKL) in large language models (LLMs) have predominantly focused on approaches such as regularization, architectural modifications, and rehearsal techniques to mitigate catastrophic forgetting. However, these methods naively inherit the inefficiencies of standard training procedures, indiscriminately applying uniform weight across all tokens, which can lead to unnecessary parameter updates and increased forgetting. To address these shortcomings, we propose a novel CKL approach termed Train-Attention-Augmented Language Model (TAALM), which enhances learning efficiency by dynamically predicting and applying weights to tokens based on their usefulness. This method employs a meta-learning framework that optimizes token importance predictions, facilitating targeted knowledge updates and minimizing forgetting. Also, we observe that existing benchmarks do not clearly exhibit the trade-off between learning and retaining, therefore we propose a new benchmark, LAMA-ckl, to address this issue. Through experiments conducted on both newly introduced and established CKL benchmarks, TAALM proves the state-of-the-art performance upon the baselines, and also shows synergistic compatibility when integrated with previous CKL approaches. The code and the dataset are available online.",main,NeurIPS,2024,Poster,Yeongbin Seo;Dongha Lee;Jinyoung Yeo,True,https://openreview.net/pdf?id=pwLdvYIMrF
pwRVGRWtGg,Apathetic or Empathetic? Evaluating LLMs' Emotional Alignments with Humans,"Evaluating Large Language Models’ (LLMs) anthropomorphic capabilities has become increasingly important in contemporary discourse. Utilizing the emotion appraisal theory from psychology, we propose to evaluate the empathy ability of LLMs, i.e., how their feelings change when presented with specific situations. After a careful and comprehensive survey, we collect a dataset containing over 400 situations that have proven effective in eliciting the eight emotions central to our study. Categorizing the situations into 36 factors, we conduct a human evaluation involving more than 1,200 subjects worldwide. With the human evaluation results as references, our evaluation includes seven LLMs, covering both commercial and open-source models, including variations in model sizes, featuring the latest iterations, such as GPT-4, Mixtral-8x22B, and LLaMA-3.1. We find that, despite several misalignments, LLMs can generally respond appropriately to certain situations. Nevertheless, they fall short in alignment with the emotional behaviors of human beings and cannot establish connections between similar situations. Our collected dataset of situations, the human evaluation results, and the code of our testing framework, i.e., EmotionBench, are publicly available at https://github.com/CUHK-ARISE/EmotionBench.",main,NeurIPS,2024,Poster,Jen-tse Huang;Man Ho LAM;Eric John Li;Shujie Ren;Wenxuan Wang;Wenxiang Jiao;Zhaopeng Tu;Michael Lyu,True,https://openreview.net/pdf?id=pwRVGRWtGg
q2IeJByeSM,BEACON: Benchmark for Comprehensive RNA Tasks and Language Models,"RNA plays a pivotal role in translating genetic instructions into functional outcomes, underscoring its importance in biological processes and disease mechanisms. Despite the emergence of numerous deep learning approaches for RNA, particularly universal RNA language models, there remains a significant lack of standardized benchmarks to assess the effectiveness of these methods. In this study, we introduce the first comprehensive RNA benchmark BEACON **BE**nchm**A**rk for  **CO**mprehensive R**N**A Task and Language Models).
First, BEACON comprises 13 distinct tasks derived from extensive previous work covering structural analysis, functional studies, and engineering applications, enabling a comprehensive assessment of the performance of methods on various RNA understanding tasks. Second, we examine a range of models, including traditional approaches like CNNs, as well as advanced RNA foundation models based on language models, offering valuable insights into the task-specific performances of these models. Third, we investigate the vital RNA language model components from the tokenizer and positional encoding aspects. Notably, our findings emphasize the superiority of single nucleotide tokenization and the effectiveness of Attention with Linear Biases (ALiBi) over traditional positional encoding methods. Based on these insights, a simple yet strong baseline called BEACON-B is proposed, which can achieve outstanding performance with limited data and computational resources. 
The datasets and source code of our benchmark are available at https://github.com/terry-r123/RNABenchmark.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yuchen Ren;Zhiyuan Chen;Lifeng Qiao;Hongtai Jing;Yuchen Cai;Sheng Xu;Peng Ye;Xinzhu Ma;Siqi Sun;Hongliang Yan;Dong Yuan;Wanli Ouyang;Xihui Liu,False,https://openreview.net/pdf?id=q2IeJByeSM
q2WT19Ciad,Benchmarking Generative Models on Computational Thinking Tests in Elementary Visual Programming,"Generative models have demonstrated human-level proficiency in various benchmarks across domains like programming, natural sciences, and general knowledge. Despite these promising results on competitive benchmarks, they still struggle with seemingly simple problem-solving tasks typically carried out by elementary-level students. How do state-of-the-art models perform on standardized programming-related tests designed to assess computational thinking and problem-solving skills at schools? In this paper, we curate a novel benchmark involving computational thinking tests grounded in elementary visual programming domains. Our initial results show that state-of-the-art models like GPT-4o and Llama3 barely match the performance of an average school student. To further boost the performance of these models, we fine-tune them using a novel synthetic data generation methodology. The key idea is to develop a comprehensive dataset using symbolic methods that capture different skill levels, ranging from recognition of visual elements to multi-choice quizzes to synthesis-style tasks. We showcase how various aspects of symbolic information in synthetic data help improve fine-tuned models' performance. We will release the full implementation and datasets to facilitate further research on enhancing computational thinking in generative models.",Datasets & Benchmarks,NeurIPS,2024,Poster,Victor-Alexandru Pădurean;Adish Singla,True,https://openreview.net/pdf?id=q2WT19Ciad
q9HIe2EUjf,A Simulation Benchmark for Autonomous Racing with Large-Scale Human Data,"Despite the availability of international prize-money competitions, scaled vehicles, and simulation environments, research on autonomous racing and the control of sports cars operating close to the limit of handling has been limited by the high costs of vehicle acquisition and management, as well as the limited physics accuracy of open-source simulators. In this paper, we propose a racing simulation platform based on the simulator Assetto Corsa to test, validate, and benchmark autonomous driving algorithms, including reinforcement learning (RL) and classical Model Predictive Control (MPC), in realistic and challenging scenarios. Our contributions include the development of this simulation platform, several state-of-the-art algorithms tailored to the racing environment, and a comprehensive dataset collected from human drivers. Additionally, we evaluate algorithms in the offline RL setting. All the necessary code (including environment and benchmarks), working examples, and datasets are publicly released and can be found at: https://github.com/dasGringuen/assetto_corsa_gym.",Datasets & Benchmarks,NeurIPS,2024,Poster,Adrian Remonda;Nicklas Hansen;Ayoub Raji;Nicola Musiu;Marko Bertogna;Eduardo E. Veas;Xiaolong Wang,True,https://openreview.net/pdf?id=q9HIe2EUjf
qMuzlVmiQh,Topic-Conversation Relevance (TCR) Dataset and Benchmarks,"Workplace meetings are vital to organizational collaboration, yet a large percentage of meetings are rated as ineffective. To help improve meeting effectiveness by understanding if the conversation is on topic, we create a comprehensive Topic-Conversation Relevance (TCR) dataset that covers a variety of domains and meeting styles. The TCR dataset includes 1,500 unique meetings, 22 million words in transcripts, and over 15,000 meeting topics, sourced from both newly collected Speech Interruption Meeting (SIM) data and existing public datasets. Along with the text data, we also open source scripts to generate synthetic meetings or create augmented meetings from the TCR dataset to enhance data diversity. For each data source, benchmarks are created using GPT-4 to evaluate the model accuracy in understanding transcription-topic relevance.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yaran Fan;Jamie Pool;Senja Filipi;Ross Cutler,True,https://openreview.net/pdf?id=qMuzlVmiQh
qR0x6H5WUX,DART-Eval: A Comprehensive DNA Language Model Evaluation Benchmark on Regulatory DNA,"Recent advances in self-supervised models for natural language, vision, and protein sequences have inspired the development of large genomic DNA language models (DNALMs). These models aim to learn generalizable representations of diverse DNA elements, potentially enabling various genomic prediction, interpretation and design tasks. Despite their potential, existing benchmarks do not adequately assess the capabilities of DNALMs on key downstream applications involving an important class of non-coding DNA elements critical for regulating gene activity. In this study, we introduce DART-Eval, a suite of representative benchmarks specifically focused on regulatory DNA to evaluate model performance across zero-shot, probed, and fine-tuned scenarios against contemporary ab initio models as baselines. Our benchmarks target biologically meaningful downstream tasks such as functional sequence feature discovery, predicting cell-type specific regulatory activity, and counterfactual prediction of the impacts of genetic variants. We find that current DNALMs exhibit inconsistent performance and do not offer compelling gains over alternative baseline models for most tasks, while requiring significantly more computational resources. We discuss potentially promising modeling, data curation, and evaluation strategies for the next generation of DNALMs. Our  code is available at https://github.com/kundajelab/DART-Eval",Datasets & Benchmarks,NeurIPS,2024,Poster,Aman Patel;Arpita Singhal;Austin Wang;Anusri Pampari;Maya Kasowski;Anshul Kundaje,True,https://openreview.net/pdf?id=qR0x6H5WUX
qWTfCO4HvT,PowerGraph: A power grid benchmark dataset for graph neural networks,"Power grids are critical infrastructures of paramount importance to modern society and, therefore, engineered to operate under diverse conditions and failures. The ongoing energy transition poses new challenges for the decision-makers and system operators. Therefore, we must develop grid analysis algorithms to ensure reliable operations. These key tools include power flow analysis and system security analysis, both needed for effective operational and strategic planning. The literature review shows a growing trend of machine learning (ML) models that perform these analyses effectively. In particular, Graph Neural Networks (GNNs) stand out in such applications because of the graph-based structure of power grids. However, there is a lack of publicly available graph datasets for training and benchmarking ML models in electrical power grid applications. First, we present PowerGraph, which comprises GNN-tailored datasets for i) power flows, ii) optimal power flows, and iii) cascading failure analyses of power grids. Second, we provide ground-truth explanations for the cascading failure analysis. Finally, we perform a complete benchmarking of GNN methods for node-level and graph-level tasks and explainability. Overall, PowerGraph is a multifaceted GNN dataset for diverse tasks that includes power flow and fault scenarios with real-world explanations, providing a valuable resource for developing improved GNN models for node-level, graph-level tasks and explainability methods in power system modeling. The dataset is available at https://figshare.com/articles/dataset/PowerGraph/22820534 and the code at https://github.com/PowerGraph-Datasets.",Datasets & Benchmarks,NeurIPS,2024,Poster,Anna Varbella;Kenza Amara;Blazhe Gjorgiev;Mennatallah El-Assady;Giovanni Sansavini,True,https://openreview.net/pdf?id=qWTfCO4HvT
qXZVSy9LFR,Emotion-LLaMA: Multimodal Emotion Recognition and Reasoning with Instruction Tuning,"Accurate emotion perception is crucial for various applications, including human-computer interaction, education, and counseling.
However, traditional single-modality approaches often fail to capture the complexity of real-world emotional expressions, which are inherently multimodal. Moreover, existing Multimodal Large Language Models (MLLMs) face challenges in integrating audio and recognizing subtle facial micro-expressions. To address this, we introduce the MERR dataset, containing 28,618 coarse-grained and 4,487 fine-grained annotated samples across diverse emotional categories. This dataset enables models to learn from varied scenarios and generalize to real-world applications. Furthermore, we propose Emotion-LLaMA, a model that seamlessly integrates audio, visual, and textual inputs through emotion-specific encoders. By aligning features into a shared space and employing a modified LLaMA model with instruction tuning, Emotion-LLaMA significantly enhances both emotional recognition and reasoning capabilities. Extensive evaluations show Emotion-LLaMA outperforms other MLLMs, achieving top scores in Clue Overlap (7.83) and Label Overlap (6.25) on EMER, an F1 score of 0.9036 on MER2023-SEMI challenge, and the highest UAR (45.59) and WAR (59.37) in zero-shot evaluations on DFEW dataset.",main,NeurIPS,2024,Poster,Zebang Cheng;Zhi-Qi Cheng;Jun-Yan He;Kai Wang;Yuxiang Lin;Zheng Lian;Xiaojiang Peng;Alexander G Hauptmann,True,https://openreview.net/pdf?id=qXZVSy9LFR
qXvepIzFL5,"MMM-RS: A Multi-modal, Multi-GSD, Multi-scene Remote Sensing Dataset and Benchmark for Text-to-Image Generation","Recently, the diffusion-based generative paradigm has achieved impressive general image generation capabilities with text prompts due to its accurate distribution modeling and stable training process. However, generating diverse remote sensing (RS) images that are tremendously different from general images in terms of scale and perspective remains a formidable challenge due to the lack of a comprehensive remote sensing image generation dataset with various modalities, ground sample distances (GSD), and scenes. In this paper, we propose a Multi-modal, Multi-GSD, Multi-scene Remote Sensing (MMM-RS) dataset and benchmark for text-to-image generation in diverse remote sensing scenarios. Specifically, we first collect nine publicly available RS datasets and conduct standardization for all samples. To bridge RS images to textual semantic information, we utilize a large-scale pretrained vision-language model to automatically output text prompts and perform hand-crafted rectification, resulting in information-rich text-image pairs (including multi-modal images). In particular, we design some methods to obtain the images with different GSD and various environments (e.g., low-light, foggy) in a single sample. With extensive manual screening and refining annotations, we ultimately obtain a MMM-RS dataset that comprises approximately 2.1 million text-image pairs. Extensive experimental results verify that our proposed MMM-RS dataset allows off-the-shelf diffusion models to generate diverse RS images across various modalities, scenes, weather conditions, and GSD. The dataset is available at https://github.com/ljl5261/MMM-RS.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jialin Luo;Yuanzhi Wang;Ziqi Gu;Yide Qiu;Shuaizhen Yao;Fuyun Wang;Chunyan Xu;Wenhua Zhang;Dan Wang;Zhen Cui,True,https://openreview.net/pdf?id=qXvepIzFL5
qeLh17biCr,Task Me Anything,"Benchmarks for large multimodal language models (MLMs) now serve to simultaneously assess the general capabilities of models instead of evaluating for a specific capability. As a result, when a developer wants to identify which models to use for their application, they are overwhelmed by the number of benchmarks and remain uncertain about which benchmark's results are most reflective of their specific use case. This paper introduces Task-Me-Anything, a benchmark generation engine which produces a benchmark tailored to a user's needs. Task-Me-Anything maintains an extendable taxonomy of visual assets and can programmatically generate a vast number of task instances. Additionally, it algorithmically addresses user queries regarding MLM performance efficiently within a computational budget. It contains 113K images, 10K videos, 2K 3D object assets, over 365 object categories, 655 attributes, and 335 relationships. It can generate 500M image/video question-answering pairs, which focus on evaluating MLM perceptual capabilities. Task-Me-Anything reveals critical insights: open-source MLMs excel in object and attribute recognition but lack spatial and temporal understanding; each model exhibits unique strengths and weaknesses; larger models generally perform better, though exceptions exist; and GPT4O demonstrates challenges in recognizing rotating/moving objects and distinguishing colors.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jieyu Zhang;Weikai Huang;Zixian Ma;Oscar Michel;Dong He;Tanmay Gupta;Wei-Chiu Ma;Ali Farhadi;Aniruddha Kembhavi;Ranjay Krishna,True,https://openreview.net/pdf?id=qeLh17biCr
qgzdGyQcDt,EEVR: A Dataset of Paired Physiological Signals and Textual Descriptions for Joint Emotion Representation Learning,"EEVR (Emotion Elicitation in Virtual Reality) is a novel dataset specifically designed for language supervision-based pre-training of emotion recognition tasks, such as valence and arousal classification. It features high-quality physiological signals, including electrodermal activity (EDA) and photoplethysmography (PPG), acquired through emotion elicitation via 360-degree virtual reality (VR) videos.
Additionally, it includes subject-wise textual descriptions of emotions experienced during each stimulus gathered from qualitative interviews. The dataset consists of recordings from 37 participants and is the first dataset to pair raw text with physiological signals, providing additional contextual information that objective labels cannot offer. To leverage this dataset, we introduced the Contrastive Language Signal Pre-training (CLSP) method, which jointly learns representations using pairs of physiological signals and textual descriptions. Our results show that integrating self-reported textual descriptions with physiological signals significantly improves performance on emotion recognition tasks, such as arousal and valence classification. Moreover, our pre-trained CLSP model demonstrates strong zero-shot transferability to existing datasets, outperforming supervised baseline models, suggesting that the representations learned by our method are more contextualized and generalized. The dataset also includes baseline models for arousal, valence, and emotion classification, as well as code for data cleaning and feature extraction. Further details and access to the dataset are available at https://melangelabiiitd.github.io/EEVR/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Pragya Singh;Ritvik Budhiraja;Ankush Gupta;Anshul Goswami;Mohan Kumar;Pushpendra Singh,True,https://openreview.net/pdf?id=qgzdGyQcDt
qkoZgJhxsA,SocraticLM: Exploring Socratic Personalized Teaching with Large Language Models,"Large language models (LLMs) are considered a crucial technology for advancing intelligent education since they exhibit the potential for an in-depth understanding of teaching scenarios and providing students with personalized guidance. Nonetheless, current LLM-based application in personalized teaching predominantly follows a ""Question-Answering"" paradigm, where students are passively provided with answers and explanations. In this paper, we propose SocraticLM, which achieves a Socratic ""Thought-Provoking"" teaching paradigm that fulfills the role of a real classroom teacher in actively engaging students in the thought process required for genuine problem-solving mastery. To build SocraticLM, we first propose a novel ""Dean-Teacher-Student"" multi-agent pipeline to construct a new dataset, SocraTeach, which contains $35$K meticulously crafted Socratic-style multi-round (equivalent to $208$K single-round) teaching dialogues grounded in fundamental mathematical problems. Our dataset simulates authentic teaching scenarios, interacting with six representative types of simulated students with different cognitive states, and strengthening four crucial teaching abilities. SocraticLM is then fine-tuned on SocraTeach with three strategies balancing its teaching and reasoning abilities. Moreover, we contribute a comprehensive evaluation system encompassing five pedagogical dimensions for assessing the teaching quality of LLMs. Extensive experiments verify that SocraticLM achieves significant improvements in the teaching performance, outperforming GPT4 by more than 12\\\\%. Our dataset and code is available at https://github.com/Ljyustc/SocraticLM.",main,NeurIPS,2024,Spotlight,Jiayu Liu;Zhenya Huang;Tong Xiao;Jing Sha;Jinze Wu;Qi Liu;Shijin Wang;Enhong Chen,True,https://openreview.net/pdf?id=qkoZgJhxsA
qmvtDIfbmS,WhodunitBench: Evaluating Large Multimodal Agents via Murder Mystery Games,"Recently, large language models (LLMs) have achieved superior performance, empowering the development of large multimodal agents (LMAs). An LMA is anticipated to execute practical tasks requires various capabilities including multimodal perception, interaction, reasoning, and decision making. However, existing benchmarks are limited in assessing compositional skills and actions demanded by practical scenarios, where they primarily focused on single tasks and static scenarios. To bridge this gap, we introduce WhodunitBench, a benchmark rooted from murder mystery games, where players are required to utilize the aforementioned skills to achieve their objective (i.e., identifying the `murderer' or hiding themselves), providing a simulated dynamic environment for evaluating LMAs. Specifically, WhodunitBench includes two evaluation modes. The first mode, the arena-style evaluation, is constructed from 50 meticulously curated scripts featuring clear reasoning clues and distinct murderers; The second mode, the chain of evaluation, consists of over 3000 curated multiple-choice questions and open-ended questions, aiming to assess every facet of the murder mystery games for LMAs. Experiments show that although current LMAs show acceptable performance in basic perceptual tasks, they are insufficiently equipped for complex multi-agent collaboration and multi-step reasoning tasks. Furthermore, the full application of the theory of mind to complete games in a manner akin to human behavior remains a significant challenge. We hope this work can illuminate the path forward, providing a solid foundation for the future development of LMAs. Our WhodunitBench is open-source and accessible at: https://github.com/
jun0wanan/WhodunitBench-Murder_Mystery_Games",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Junlin Xie;Ruifei Zhang;Zhihong Chen;Xiang Wan;Guanbin Li,True,https://openreview.net/pdf?id=qmvtDIfbmS
qqU8WPw44f,CURE4Rec: A Benchmark for Recommendation Unlearning with Deeper Influence,"With increasing privacy concerns in artificial intelligence, regulations have mandated the right to be forgotten, granting individuals the right to withdraw their data from models. Machine unlearning has emerged as a potential solution to enable selective forgetting in models, particularly in recommender systems where historical data contains sensitive user information. Despite recent advances in recommendation unlearning, evaluating unlearning methods comprehensively remains challenging due to the absence of a unified evaluation framework and overlooked aspects of deeper influence, e.g., fairness. To address these gaps, we propose CURE4Rec, the first comprehensive benchmark for recommendation unlearning evaluation. CURE4Rec covers four aspects, i.e., unlearning Completeness, recommendation Utility, unleaRning efficiency, and recommendation fairnEss, under three data selection strategies, i.e., core data, edge data, and random data. Specifically, we consider the deeper influence of unlearning on recommendation fairness and robustness towards data with varying impact levels. We construct multiple datasets with CURE4Rec evaluation and conduct extensive experiments on existing recommendation unlearning methods. Our code is released at https://github.com/xiye7lai/CURE4Rec.",Datasets & Benchmarks,NeurIPS,2024,Poster,Chaochao Chen;Jiaming Zhang;Yizhao Zhang;Li Zhang;Lingjuan Lyu;Yuyuan Li;Biao Gong;Chenggang Yan,True,https://openreview.net/pdf?id=qqU8WPw44f
qrZxL3Bto9,Evaluating language models as risk scores,"Current question-answering benchmarks predominantly focus on accuracy in realizable prediction tasks.
Conditioned on a question and answer-key, does the most likely token match the ground truth?
Such benchmarks necessarily fail to evaluate LLMs' ability to quantify ground-truth outcome uncertainty.
In this work, we focus on the use of LLMs as risk scores for unrealizable prediction tasks.
We introduce folktexts, a software package to systematically generate risk scores using LLMs, and evaluate them against US Census data products.
A flexible API enables the use of different prompting schemes, local or web-hosted models, and diverse census columns that can be used to compose custom prediction tasks.
We evaluate 17 recent LLMs across five proposed benchmark tasks.
We find that zero-shot risk scores produced by multiple-choice question-answering have high predictive signal but are widely miscalibrated.
Base models consistently overestimate outcome uncertainty, while instruction-tuned models underestimate uncertainty and produce over-confident risk scores.
In fact, instruction-tuning polarizes answer distribution regardless of true underlying data uncertainty.
This reveals a general inability of instruction-tuned models to express data uncertainty using multiple-choice answers.
A separate experiment using verbalized chat-style risk queries yields substantially improved calibration across instruction-tuned models.
These differences in ability to quantify data uncertainty cannot be revealed in realizable settings, and highlight a blind-spot in the current evaluation ecosystem that folktexts covers.",Datasets & Benchmarks,NeurIPS,2024,Poster,André F Cruz;Moritz Hardt;Celestine Mendler-Dünner,False,https://openreview.net/pdf?id=qrZxL3Bto9
qwWu95yoZO,What to Say and When to Say it: Live Fitness Coaching as a Testbed for Situated Interaction,"Vision-language models have shown impressive progress in recent years. However, existing models are largely limited to turn-based interactions, where each turn must be stepped (i.e., prompted) by the user. Open-ended, asynchronous interactions, where an AI model may proactively deliver timely responses or feedback based on the unfolding situation in real-time, are an open challenge. In this work, we present the QEVD benchmark and dataset, which explores human-AI interaction in the challenging, yet controlled, real-world domain of fitness coaching – a task which intrinsically requires monitoring live user activity and providing immediate feedback. The benchmark requires vision-language models to recognize complex human actions, identify possible mistakes, and provide appropriate feedback in real-time. Our experiments reveal the limitations of existing state-of-the-art vision-language models for such asynchronous situated interactions. Motivated by this, we propose a simple end-to-end streaming baseline that can respond asynchronously to human actions with appropriate feedback at the appropriate time.",Datasets & Benchmarks,NeurIPS,2024,Poster,Sunny Panchal;Apratim Bhattacharyya;Guillaume Berger;Antoine Mercier;Cornelius Böhm;Florian Dietrichkeit;Reza Pourreza;Xuanlin Li;Pulkit Madan;Mingu Lee;Mark Todorovich;Ingo Bax;Roland Memisevic,True,https://openreview.net/pdf?id=qwWu95yoZO
r3c0WGCXgt,How Control Information Influences Multilingual Text Image Generation and Editing?,"Visual text generation has significantly advanced through diffusion models aimed at producing images with readable and realistic text. Recent works primarily use a ControlNet-based framework, employing standard font text images to control diffusion models. Recognizing the critical role of control information in generating high-quality text, we investigate its influence from three perspectives: input encoding, role at different stages, and output features. Our findings reveal that: 1) Input control information has unique characteristics compared to conventional inputs like Canny edges and depth maps. 2) Control information plays distinct roles at different stages of the denoising process. 3) Output control features significantly differ from the base and skip features of the U-Net decoder in the frequency domain. Based on these insights, we propose TextGen, a novel framework designed to enhance generation quality by optimizing control information. We improve input and output features using Fourier analysis to emphasize relevant information and reduce noise. Additionally, we employ a two-stage generation framework to align the different roles of control information at different stages. Furthermore, we introduce an effective and lightweight dataset for training. Our method achieves state-of-the-art performance in both Chinese and English text generation. The code and dataset are available at https://github.com/CyrilSterling/TextGen.",main,NeurIPS,2024,Poster,Boqiang Zhang;Zuan Gao;Yadong Qu;Hongtao Xie,True,https://openreview.net/pdf?id=r3c0WGCXgt
r8PnfcWQol,${EFO}_k$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation,"To answer complex queries on knowledge graphs, logical reasoning over incomplete knowledge needs learning-based methods because they are capable of generalizing over unobserved knowledge. Therefore, an appropriate dataset is fundamental to both obtaining and evaluating such methods under this paradigm. In this paper, we propose a comprehensive framework for data generation, model training, and method evaluation that covers the combinatorial space of Existential First-order Queries with multiple variables ($\\\\text{EFO}_k$). The combinatorial query space in our framework significantly extends those defined by set operations in the existing literature. Additionally, we construct a dataset, $\\\\text{EFO}_k$-CQA, with 741 query types for empirical evaluation, and our benchmark results provide new insights into how query hardness affects the results. Furthermore, we demonstrate that the existing dataset construction process is systematically biased and hinders the appropriate development of query-answering methods, highlighting the importance of our work. Our code and data are provided in~\\\\url{https://anonymous.4open.science/r/EFOK-CQA/README.md}.",Datasets & Benchmarks,NeurIPS,2024,Reject,Hang Yin;Zihao Wang;Weizhi Fei;Yangqiu Song,True,https://openreview.net/pdf?id=r8PnfcWQol
rGdy9jrBs8,UAV3D: A Large-scale 3D Perception Benchmark for Unmanned Aerial Vehicles,"Unmanned Aerial Vehicles (UAVs), equipped with cameras, are employed in numerous applications, including aerial photography, surveillance, and agriculture. In these applications, robust object detection and tracking are essential for the effective deployment of UAVs. However, existing benchmarks for UAV applications are mainly designed for traditional 2D perception tasks, restricting the
development of real-world applications that require a 3D understanding of the environment. Furthermore, despite recent advancements in single-UAV perception, limited views of a single UAV platform significantly constrain its perception capabilities over long distances or in occluded areas. To address these challenges, we introduce UAV3D – a benchmark designed to advance research in both 3D and
collaborative 3D perception tasks with UAVs. UAV3D comprises 1,000 scenes, each of which has 20 frames with fully annotated 3D bounding boxes on vehicles. We provide the benchmark for four 3D perception tasks: single-UAV 3D object detection, single-UAV object tracking, collaborative-UAV 3D object detection, and collaborative-UAV object tracking. Our dataset and code are available at
https://huiyegit.github.io/UAV3D_Benchmark/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Hui Ye;Rajshekhar Sunderraman;Shihao Ji,True,https://openreview.net/pdf?id=rGdy9jrBs8
rI7kbFTSpr,Towards Reliable Model Selection for Unsupervised Domain Adaptation: An Empirical Study and A Certified Baseline,"Selecting appropriate hyperparameters is crucial for unlocking the full potential of advanced unsupervised domain adaptation (UDA) methods in unlabeled target domains. Although this challenge remains under-explored, it has recently garnered increasing attention with the proposals of various model selection methods. Reliable model selection should maintain performance across diverse UDA methods and scenarios, especially avoiding highly risky worst-case selections—selecting the model or hyperparameter with the worst performance in the pool.
\\\\textit{Are existing model selection methods reliable and versatile enough for different UDA tasks?} In this paper, we provide a comprehensive empirical study involving 8 existing model selection approaches to answer this question. Our evaluation spans 12 UDA methods across 5 diverse UDA benchmarks and 5 popular UDA scenarios.
Surprisingly, we find that none of these approaches can effectively avoid the worst-case selection. In contrast, a simple but overlooked ensemble-based selection approach, which we call EnsV, is both theoretically and empirically certified to avoid the worst-case selection, ensuring high reliability. Additionally, EnsV is versatile for various practical but challenging UDA scenarios, including validation of open-partial-set UDA and source-free UDA.
Finally, we call for more attention to the reliability of model selection in UDA: avoiding the worst-case is as significant as achieving peak selection performance and should not be overlooked when developing new model selection methods.  Code is available at https://github.com/LHXXHB/EnsV.",Datasets & Benchmarks,NeurIPS,2024,Poster,Dapeng Hu;Mi Luo;Jian Liang;Chuan-Sheng Foo,False,https://openreview.net/pdf?id=rI7kbFTSpr
rIHx6puY5b,SELECT: A Large-Scale Benchmark of Data Curation Strategies for Image Classification,"Data curation is the problem of how to collect and organize samples into a dataset that supports efficient learning. Despite the centrality of the task, little work has been devoted towards a large-scale, systematic comparison of various curation methods. In this work, we take steps towards a formal evaluation of data curation strategies and introduce SELECT, the first large-scale benchmark of curation strategies for image classification.

In order to generate baseline methods for the SELECT benchmark, we create a new dataset, ImageNet++, which constitutes the largest superset of ImageNet-1K to date. Our dataset extends ImageNet with 5 new training-data shifts, each approximately the size of  ImageNet-1K, and each assembled using a distinct curation strategy. We evaluate our data curation baselines in two ways: (i) using each training-data shift to train identical image classification models from scratch (ii) using it to inspect a fixed pretrained self-supervised representation.

Our findings show interesting trends, particularly pertaining to recent methods for data curation such as synthetic data generation and lookup based on CLIP embeddings. We show that although these strategies are highly competitive for certain tasks, the curation strategy used to assemble the original ImageNet-1K dataset remains the gold standard. We anticipate that our benchmark can illuminate the path for new methods to further reduce the gap. We release our checkpoints, code, documentation, and a link to our dataset at https://github.com/jimmyxu123/SELECT.",Datasets & Benchmarks,NeurIPS,2024,Poster,Benjamin Feuer;Jiawei Xu;Niv Cohen;Patrick Yubeaton;Govind Mittal;Chinmay Hegde,True,https://openreview.net/pdf?id=rIHx6puY5b
rZlLfa81D8,WONDERBREAD: A Benchmark for Evaluating Multimodal Foundation Models on Business Process Management Tasks,"Existing ML benchmarks lack the depth and diversity of annotations needed for evaluating models on business process management (BPM) tasks. BPM is the practice of documenting, measuring, improving, and automating enterprise workflows. However, research has focused almost exclusively on one task -- full end-to-end automation using agents based on multimodal foundation models (FMs) like GPT-4. This focus on automation ignores the reality of how most BPM tools are applied today -- simply documenting the relevant workflow takes 60% of the time of the typical process optimization project. To address this gap we present WONDERBREAD, the first benchmark for evaluating multimodal FMs on BPM tasks beyond automation. Our contributions are: (1) a dataset containing 2928 documented workflow demonstrations; (2) 6 novel BPM tasks sourced from real-world applications ranging from workflow documentation to knowledge transfer to process improvement; and (3) an automated evaluation harness. Our benchmark shows that while state-of-the-art FMs can automatically generate documentation (e.g. recalling 88% of the steps taken in a video demonstration of a workflow), they struggle to re-apply that knowledge towards finer-grained validation of workflow completion (F1 < 0.3). We hope WONDERBREAD encourages the development of more ""human-centered"" AI tooling for enterprise applications and furthers the exploration of multimodal FMs for the broader universe of BPM tasks. We publish our dataset and experiments here: https://github.com/HazyResearch/wonderbread",Datasets & Benchmarks,NeurIPS,2024,Poster,Michael Wornow;Avanika Narayan;Benjamin Viggiano;Ishan S. Khare;Tathagat Verma;Tibor Thompson;Miguel Angel Fuentes Hernandez;Sudharsan Sundar;Chloe Trujillo;Krrish Chawla;Rongfei Lu;Justin Shen;Divya Nagaraj;Joshua Martinez;Vardhan Kishore Agrawal;Althea Hudson;Nigam Shah;Christopher Re,True,https://openreview.net/pdf?id=rZlLfa81D8
raOYixthlY,FlexMol: A Flexible Toolkit for Benchmarking Molecular Relational Learning,"Molecular relational learning (MRL) is crucial for understanding the interaction behaviors between molecular pairs, a critical aspect of drug discovery and development. However, the large feasible model space of MRL poses significant challenges to benchmarking, and existing MRL frameworks face limitations in flexibility and scope. To address these challenges, avoid repetitive coding efforts, and ensure fair comparison of models, we introduce FlexMol, a comprehensive toolkit designed to facilitate the construction and evaluation of diverse model architectures across various datasets and performance metrics. FlexMol offers a robust suite of preset model components, including 16 drug encoders, 13 protein sequence encoders, 9 protein structure encoders, and 7 interaction layers. With its easy-to-use API and flexibility, FlexMol supports the dynamic construction of over 70, 000 distinct combinations of model architectures. Additionally, we provide detailed benchmark results and code examples to demonstrate FlexMol’s effectiveness in simplifying and standardizing MRL model development and comparison. FlexMol is open-sourced and available at https://github.com/Steven51516/FlexMol.",Datasets & Benchmarks,NeurIPS,2024,Poster,Sizhe Liu;Jun Xia;Lecheng Zhang;Yuchen Liu;Yue Liu;Wenjie Du;Zhangyang Gao;Bozhen Hu;Cheng Tan;hongxin xiang;Stan Z. Li,False,https://openreview.net/pdf?id=raOYixthlY
rcch4UsMBi,Synthetic Data (Almost) from Scratch: Generalized Instruction Tuning for Language Models,"We introduce Generalized Instruction Tuning (called GLAN), a general and scalable method for instruction tuning of Large Language Models (LLMs). 
Unlike prior work that relies on seed examples or existing datasets to construct instruction tuning data, 
GLAN exclusively utilizes a pre-curated taxonomy of human knowledge and capabilities as input and generates large-scale 
synthetic instruction data across all disciplines.
Specifically, inspired by the systematic structure in human education system, we build the taxonomy by decomposing human knowledge and capabilities to various fields, sub-fields and ultimately, distinct disciplines semi-automatically, facilitated by LLMs. 
Subsequently, we generate a comprehensive list of subjects for every discipline and proceed to design a syllabus tailored to each subject, again utilizing LLMs.
With the fine-grained key concepts detailed in every class session of the syllabus, we are able to generate diverse instructions with a broad coverage across the entire spectrum of human knowledge and skills. 
Extensive experiments on large language models (e.g., Mistral) demonstrate that GLAN excels in multiple dimensions from mathematical reasoning, coding, academic exams, logical reasoning to general instruction following without using task-specific training data of these tasks. In addition, GLAN allows for easy customization and new fields or skills can be added by simply incorporating a new node into our taxonomy.",main,NeurIPS,2024,Reject,Haoran Li;Qingxiu Dong;Zhengyang Tang;Chaojun Wang;Xingxing Zhang;Haoyang Huang;Shaohan Huang;Xiaolong Huang;Zeqiang Huang;Dongdong Zhang;Yuxian Gu;Xin Cheng;Xun Wang;Si-Qing Chen;Li Dong;Wei Lu;Zhifang Sui;Benyou Wang;Wai Lam;Furu Wei,True,https://openreview.net/pdf?id=rcch4UsMBi
rdv2Fr6JTC,Precedence-Constrained Winter Value for Effective Graph Data Valuation,"Data valuation is essential for quantifying data’s worth, aiding in assessing data quality and determining fair compensation. While existing data valuation methods have proven effective in evaluating the value of Euclidean data, they face limitations when applied to the increasingly popular graph-structured data. Particularly, graph data valuation introduces unique challenges, primarily stemming from the intricate dependencies among nodes and the exponential growth in value estimation costs. To address the challenging problem of graph data valuation, we put forth an innovative solution, Precedence-Constrained Winter (PC-Winter) Value, to account for the complex graph structure. Furthermore, we develop a variety of strategies to address the computational challenges and enable efficient approximation of PC-Winter. Extensive experiments demonstrate the effectiveness of PC-Winter across diverse datasets and tasks.",Datasets & Benchmarks,NeurIPS,2024,Reject,Hongliang Chi;Wei Jin;Charu C. Aggarwal;Yao Ma,False,https://openreview.net/pdf?id=rdv2Fr6JTC
recsheQ7e8,Aligning to Thousands of Preferences via System Message Generalization,"Although humans inherently have diverse values, current large language model (LLM) alignment methods often assume that aligning LLMs with the general public’s preferences is optimal. A major challenge in adopting a more individualized approach to LLM alignment is its lack of scalability, as it involves repeatedly acquiring preference data and training new reward models and LLMs for each individual’s preferences. To address these challenges, we propose a new paradigm where users specify what they value most within the system message, steering the LLM’s generation behavior to better align with the user’s intentions. However, a naive application of such an approach is non-trivial since LLMs are typically trained on a uniform system message (e.g., “You are a helpful assistant”), which limits
their ability to generalize to diverse, unseen system messages. To improve this generalization, we create Multifaceted Collection, augmenting 66k user instructions into 197k system messages through hierarchical user value combinations. Using this dataset, we train a 7B LLM called Janus and test it on 921 prompts from 5 benchmarks (AlpacaEval 2.0, FLASK, Koala, MT-Bench, and Self-Instruct)
by adding system messages that reflect unseen user values. JANUS achieves tie+win rate of 75.2%, 72.4%, and 66.4% against Mistral 7B Instruct v0.2, GPT-3.5 Turbo, and GPT-4, respectively. Unexpectedly, on three benchmarks focused on response helpfulness (AlpacaEval 2.0, MT-Bench, Arena Hard Auto v0.1), JANUS also outperforms LLaMA 3 8B Instruct by a +4.0%p, +0.1%p, +3.0%p margin, underscoring that training with a vast array of system messages could also enhance alignment to the general public’s preference as well. Our code, dataset, benchmark, and models are available at https://lklab.kaist.ac.kr/Janus/.",main,NeurIPS,2024,Poster,Seongyun Lee;Sue Hyun Park;Seungone Kim;Minjoon Seo,True,https://openreview.net/pdf?id=recsheQ7e8
rfbSL1qXN3,SS3DM: Benchmarking Street-View Surface Reconstruction with a Synthetic 3D Mesh Dataset,"Reconstructing accurate 3D surfaces for street-view scenarios is crucial for applications such as digital entertainment and autonomous driving simulation. However, existing street-view datasets, including KITTI, Waymo, and nuScenes, only offer noisy LiDAR points as ground-truth data for geometric evaluation of reconstructed surfaces. These geometric ground-truths often lack the necessary precision to evaluate surface positions and do not provide data for assessing surface normals. To overcome these challenges, we introduce the SS3DM dataset, comprising precise \\\\textbf{S}ynthetic \\\\textbf{S}treet-view \\\\textbf{3D} \\\\textbf{M}esh models exported from the CARLA simulator. These mesh models facilitate accurate position evaluation and include normal vectors for evaluating surface normal. 
  To simulate the input data in realistic driving scenarios for 3D reconstruction, we virtually drive a vehicle equipped with six RGB cameras and five LiDAR sensors in diverse outdoor scenes.
  Leveraging this dataset, we establish a benchmark for state-of-the-art surface reconstruction methods, providing a comprehensive evaluation of the associated challenges. 
  For more information, visit our homepage at https://ss3dm.top.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yubin Hu;Kairui Wen;Heng Zhou;Xiaoyang Guo;Yong-jin Liu,True,https://openreview.net/pdf?id=rfbSL1qXN3
rovpCs3ZEO,FEDMEKI: A Benchmark for Scaling Medical Foundation Models via Federated Knowledge Injection,"This study introduces the Federated Medical Knowledge Injection (FedMEKI) platform, a new benchmark designed to address the unique challenges of integrating medical knowledge into foundation models under privacy constraints. By leveraging a cross-silo federated learning approach, FedMEKI circumvents the issues associated with centralized data collection, which is often prohibited under health regulations like the Health Insurance Portability and Accountability Act (HIPAA) in the USA. The platform is meticulously designed to handle multi-site, multi-modal, and multi-task medical data, which includes 7 medical modalities, including images, signals, texts, laboratory test results, vital signs, input variables, and output variables. The curated dataset to validate FedMEKI covers 8 medical tasks, including 6 classification tasks (lung opacity detection, COVID-19 detection, electrocardiogram (ECG) abnormal detection, mortality prediction, sepsis protection, and enlarged cardiomediastinum detection) and 2 generation tasks (medical visual question answering (MedVQA) and ECG noise clarification). This comprehensive dataset is partitioned across several clients to facilitate the decentralized training process under 16 benchmark approaches. FedMEKI not only preserves data privacy but also enhances the capability of medical foundation models by allowing them to learn from a broader spectrum of medical knowledge without direct data exposure, thereby setting a new benchmark in the application of foundation models within the healthcare sector.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Jiaqi Wang;Xiaochen Wang;Lingjuan Lyu;Jinghui Chen;Fenglong Ma,True,https://openreview.net/pdf?id=rovpCs3ZEO
s1K5Z5QPog,"ChaosBench: A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction","Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster preparedness and robust decision making amidst climate change. Yet, forecasting beyond the weather timescale is challenging because it deals with problems other than initial condition, including boundary interaction, butterfly effect, and our inherent lack of physical understanding. At present, existing benchmarks tend to have shorter forecasting range of up-to 15 days, do not include a wide range of operational baselines, and lack physics-based constraints for explainability. Thus, we propose ChaosBench, a challenging benchmark to extend the predictability range of data-driven weather emulators to S2S timescale. First, ChaosBench is comprised of variables beyond the typical surface-atmospheric ERA5 to also include ocean, ice, and land reanalysis products that span over 45 years to allow for full Earth system emulation that respects boundary conditions. We also propose physics-based, in addition to deterministic and probabilistic metrics, to ensure a physically-consistent ensemble that accounts for butterfly effect. Furthermore, we evaluate on a diverse set of physics-based forecasts from four national weather agencies as baselines to our data-driven counterpart such as ViT/ClimaX, PanguWeather, GraphCast, and FourCastNetV2. Overall, we find methods originally developed for weather-scale applications fail on S2S task: their performance simply collapse to an unskilled climatology. Nonetheless, we outline and demonstrate several strategies that can extend the predictability range of existing weather emulators, including the use of ensembles, robust control of error propagation, and the use of physics-informed models. Our benchmark, datasets, and instructions are available at https://leap-stc.github.io/ChaosBench.",Datasets & Benchmarks,NeurIPS,2024,Oral,Juan Nathaniel;Yongquan Qu;Tung Nguyen;Sungduk Yu;Julius Busecke;Aditya Grover;Pierre Gentine,True,https://openreview.net/pdf?id=s1K5Z5QPog
s63dtq0mwA,Understanding Information Storage and Transfer in Multi-Modal Large Language Models,"Understanding the mechanisms of information storage and transfer in Transformer-based models is important for driving model understanding progress. Recent work has studied these mechanisms for Large Language Models (LLMs), revealing insights on how information is stored in a model's parameters and how information flows to and from these parameters in response to specific prompts. However, these studies have not yet been extended to Multi-modal Large Language Models (MLLMs). Given their expanding capabilities and real-world use, we start by studying one aspect of these models -- how MLLMs process information in a factual visual question answering task. We use a constraint-based formulation which views a visual question as having a set of visual or textual constraints that the model's generated answer must satisfy to be correct (e.g. What movie directed by \\\\emph{the director in this photo} has won a \\\\emph{Golden Globe}?). Under this setting, we contribute i) a method that extends causal information tracing from pure language to the multi-modal setting, and ii) \\\\emph{VQA-Constraints}, a test-bed of 9.7K visual questions annotated with constraints. We use these tools to study two open-source MLLMs, LLaVa and multi-modal Phi-2. Our key findings show that these MLLMs rely on MLP and self-attention blocks in much earlier layers for information storage, compared to LLMs whose mid-layer MLPs are more important. We also show that a consistent small subset of visual tokens output by the vision encoder are responsible for transferring information from the image to these causal blocks. We validate these mechanisms by introducing MultEdit a model-editing algorithm that can correct errors and insert new long-tailed information into MLLMs by targeting these causal blocks. We will publicly release our dataset and code.",main,NeurIPS,2024,Poster,Samyadeep Basu;Martin Grayson;Cecily Morrison;Besmira Nushi;Soheil Feizi;Daniela Massiceti,True,https://openreview.net/pdf?id=s63dtq0mwA
s8h2jSN6a6,MMDU: A Multi-Turn Multi-Image Dialog Understanding Benchmark and Instruction-Tuning Dataset for LVLMs,"Generating natural and meaningful responses to communicate with multi-modal human inputs is a fundamental capability of Large Vision-Language Models (LVLMs). While current open-source LVLMs demonstrate promising performance in simplified scenarios such as single-turn single-image input, they fall short in real-world conversation scenarios such as following instructions in a long context history with multi-turn and multi-images. Existing LVLM benchmarks primarily focus on single-choice questions or short-form responses, which do not adequately assess the capabilities of LVLMs in real-world human-AI interaction applications. Therefore, we introduce MMDU, a comprehensive benchmark, and MMDU-45k, a large-scale instruction tuning dataset, designed to evaluate and improve LVLMs' abilities in multi-turn and multi-image conversations. We employ the clustering algorithm to find the relevant images and textual descriptions from the open-source Wikipedia and construct the question-answer pairs by human annotators with the assistance of the GPT-4o model.
MMDU has a maximum of 18k image+text tokens, 20 images, and 27 turns, which is at least 5x longer than previous benchmarks and poses challenges to current LVLMs. Our in-depth analysis of 15 representative LVLMs using MMDU reveals that open-source LVLMs lag behind closed-source counterparts due to limited conversational instruction tuning data.
We demonstrate that fine-tuning open-source LVLMs on MMDU-45k significantly address this gap, generating longer and more accurate conversations, and improving scores on MMDU and existing benchmarks (MMStar: +1.1%, MathVista: +1.5%, ChartQA: +1.2%). Our contributions pave the way for bridging the gap between current LVLM models and real-world application demands. The links to MMDU, and MMDU-45k are available in the supplementary material.",Datasets & Benchmarks,NeurIPS,2024,Poster,Ziyu Liu;Tao Chu;Yuhang Zang;Xilin Wei;Xiaoyi Dong;Pan Zhang;Zijian Liang;Yuanjun Xiong;Yu Qiao;Dahua Lin;Jiaqi Wang,True,https://openreview.net/pdf?id=s8h2jSN6a6
sAxVIWQOzo,GameTraversalBenchmark: Evaluating Planning Abilities Of Large Language Models Through Traversing 2D Game Maps,"Large language models (LLMs) have recently demonstrated great success in generating and understanding natural language.  While they have also shown potential beyond the domain of natural language, it remains an open question as to what extent and in which way these LLMs can plan. We investigate their planning capabilities by proposing \\\\texttt{GameTraversalBenchmark (GTB)}, a benchmark consisting of diverse 2D grid-based game maps. An LLM succeeds if it can traverse through given objectives, with a minimum number of steps and a minimum number of generation errors. We evaluate a number of LLMs on \\\\texttt{GTB} and found that GPT-4-Turbo achieved the highest score of $44.97\\\\%$ on \\\\texttt{GTB\\\\_Score} (GTBS), a composite score that combines the three above criteria. Furthermore, we preliminarily test large reasoning models, namely o1, which scores $67.84\\\\%$ on GTBS, indicating that the benchmark remains challenging for current models. Code, data, and documentation are available at \\\\url{https://github.com/umair-nasir14/Game-Traversal-Benchmark}.",Datasets & Benchmarks,NeurIPS,2024,Poster,Muhammad Umair Nasir;Steven James;Julian Togelius,True,https://openreview.net/pdf?id=sAxVIWQOzo
sHBn3PNcwU,EMGBench: Benchmarking Out-of-Distribution Generalization and Adaptation for Electromyography,"This paper introduces the first generalization and adaptation benchmark using machine learning for evaluating out-of-distribution performance of electromyography (EMG) classification algorithms. The ability of an EMG classifier to handle inputs drawn from a different distribution than the training distribution is critical for real-world deployment as a control interface. By predicting the user’s intended gesture using EMG signals, we can create a wearable solution to control assistive technologies, such as computers, prosthetics, and mobile manipulator robots. This new out-of-distribution benchmark consists of two major tasks that have utility for building robust and adaptable control interfaces: 1) intersubject classification, and 2) adaptation using train-test splits for time-series. This benchmark spans nine datasets, the largest collection of EMG datasets in a benchmark. Among these, a new dataset is introduced, featuring a novel, easy-to-wear high-density EMG wearable for data collection. The lack of open-source benchmarks has made comparing accuracy results between papers challenging for the EMG research community. This new benchmark provides researchers with a valuable resource for analyzing practical measures of out-of-distribution performance for EMG datasets. Our code and data from our new dataset can be found at emgbench.github.io.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jehan Yang;Maxwell J. Soh;Vivianna Lieu;Douglas J Weber;Zackory Erickson,True,https://openreview.net/pdf?id=sHBn3PNcwU
sLzD2rw9Ce,DrivingDojo Dataset: Advancing Interactive and Knowledge-Enriched Driving World Model,"Driving world models have gained increasing attention due to their ability to model complex physical dynamics. However, their superb modeling capability is yet to be fully unleashed due to the limited video diversity in current driving datasets. We introduce DrivingDojo, the first dataset tailor-made for training interactive world models with complex driving dynamics. Our dataset features video clips with a complete set of driving maneuvers, diverse multi-agent interplay, and rich open-world driving knowledge, laying a stepping stone for future world model development. We further define an action instruction following (AIF) benchmark for world models and demonstrate the superiority of the proposed dataset for generating action-controlled future predictions.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yuqi Wang;Ke Cheng;Jiawei He;Qitai Wang;Hengchen Dai;Yuntao Chen;Fei Xia;Zhaoxiang Zhang,True,https://openreview.net/pdf?id=sLzD2rw9Ce
sQApQMBqiP,Learning Human-like Representations to Enable Learning Human Values,"How can we build AI systems that can learn any set of individual human values both quickly and safely, avoiding causing harm or violating societal standards for acceptable behavior during the learning process? We explore the effects of representational alignment between humans and AI agents on learning human values. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We demonstrate that this kind of representational alignment can also support safely learning and exploring human values in the context of personalization. We begin with a theoretical prediction, show that it applies to learning human morality judgments, then show that our results generalize to ten different aspects of human values -- including ethics, honesty, and fairness -- training AI agents on each set of values in a multi-armed bandit setting, where rewards reflect human value judgments over the chosen action. Using a set of textual action descriptions, we collect value judgments from humans, as well as similarity judgments from both humans and multiple language models, and demonstrate that representational alignment enables both safe exploration and improved generalization when learning human values.",main,NeurIPS,2024,Poster,Andrea Wynn;Ilia Sucholutsky;Thomas L. Griffiths,True,https://openreview.net/pdf?id=sQApQMBqiP
skeopn3q5Y,SfPUEL: Shape from Polarization under Unknown Environment Light,"Shape from polarization (SfP) benefits from advancements like polarization cameras for single-shot normal estimation, but its performance heavily relies on light conditions. This paper proposes SfPUEL, an end-to-end SfP method to jointly estimate surface normal and material under unknown environment light. To handle this challenging light condition, we design a transformer-based framework for enhancing the perception of global context features. We further propose to integrate photometric stereo (PS) priors from pretrained models to enrich extracted features for high-quality normal predictions. As metallic and dielectric materials exhibit different BRDFs, SfPUEL additionally predicts dielectric and metallic material segmentation to further boost performance. Experimental results on synthetic and our collected real-world dataset demonstrate that SfPUEL significantly outperforms existing SfP and single-shot normal estimation methods. The code and dataset is available at https://github.com/YouweiLyu/SfPUEL.",main,NeurIPS,2024,Poster,Youwei Lyu;Heng Guo;Kailong Zhang;Si Li;Boxin Shi,True,https://openreview.net/pdf?id=skeopn3q5Y
slqbOc67W8,Melting Pot Contest: Charting the Future of Generalized Cooperative Intelligence,"Multi-agent AI research promises a path to develop human-like and human-compatible intelligent technologies that complement the solipsistic view of other approaches, which mostly do not consider interactions between agents. Aiming to make progress in this direction, the Melting Pot contest 2023 focused on the problem of cooperation among interacting agents and challenged researchers to push the boundaries of multi-agent reinforcement learning (MARL) for mixed-motive games. The contest leveraged the Melting Pot environment suite to rigorously evaluate how well agents can adapt their cooperative skills to interact with novel partners in unforeseen situations. Unlike other reinforcement learning challenges, this challenge focused on social rather than environmental generalization. In particular, a population of agents performs well in Melting Pot when its component individuals are adept at finding ways to cooperate both with others in their population and with strangers. Thus Melting Pot measures cooperative intelligence.
The contest attracted over 600 participants across 100+ teams globally and was a success on multiple fronts: (i) it contributed to our goal of pushing the frontiers of MARL towards building more cooperatively intelligent agents, evidenced by several submissions that outperformed established baselines; (ii) it attracted a diverse range of participants, from independent researchers to industry affiliates and academic labs, both with strong background and new interest in the area alike, broadening the field’s demographic and intellectual diversity; and (iii) analyzing the submitted agents provided important insights, highlighting areas for improvement in evaluating agents' cooperative intelligence. This paper summarizes the design aspects and results of the contest and explores the potential of Melting Pot as a benchmark for studying Cooperative AI. We further analyze the top solutions and conclude with a discussion on promising directions for future research.",Datasets & Benchmarks,NeurIPS,2024,Poster,Rakshit Trivedi;Akbir Khan;Jesse Clifton;Lewis Hammond;Edgar A. Duéñez-Guzmán;Dipam Chakraborty;John P Agapiou;Jayd Matyas;Sasha Vezhnevets;Barna Pásztor;Yunke Ao;Omar G. Younis;Jiawei Huang;Benjamin Swain;Haoyuan Qin;Mian Deng;Ziwei Deng;Utku Erdoğanaras;Yue Zhao;Marko Tesic;Natasha Jaques;Jakob Nicolaus Foerster;Vincent Conitzer;Jose Hernandez-Orallo;Dylan Hadfield-Menell;Joel Z Leibo,True,https://openreview.net/pdf?id=slqbOc67W8
smxQvTmdGS,Bias and Volatility: A Statistical Framework for Evaluating Large Language Model's Stereotypes and the Associated Generation Inconsistency,"We present a novel statistical framework for analyzing stereotypes in large language models (LLMs) by systematically estimating the bias and variation in their generation. Current evaluation metrics in the alignment literature often overlook the randomness of stereotypes caused by the inconsistent generative behavior of LLMs. For example, this inconsistency can result in LLMs displaying contradictory stereotypes, including those related to gender or race, for identical professions across varied contexts. Neglecting such inconsistency could lead to misleading conclusions in alignment evaluations and hinder the accurate assessment of the risk of LLM applications perpetuating or amplifying social stereotypes and unfairness.

This work proposes a Bias-Volatility Framework (BVF) that estimates the probability distribution function of LLM stereotypes. Specifically, since the stereotype distribution fully captures an LLM's generation variation, BVF enables the assessment of both the likelihood and extent to which its outputs are against vulnerable groups, thereby allowing for the quantification of the LLM's aggregated discrimination risk. Furthermore, we introduce a mathematical framework to decompose an LLM’s aggregated discrimination risk into two components: bias risk and volatility risk, originating from the mean and variation of LLM’s stereotype distribution, respectively. We apply BVF to assess 12 commonly adopted LLMs and compare their risk levels. Our findings reveal that:  i) Bias risk is the primary cause of discrimination risk in LLMs; ii) Most LLMs exhibit significant pro-male stereotypes for nearly all careers; iii) Alignment with reinforcement learning from human feedback lowers discrimination by reducing bias, but increases volatility; iv) Discrimination risk in LLMs correlates with key sociol-economic factors like professional salaries. Finally, we emphasize that BVF can also be used to assess other dimensions of generation inconsistency's impact on LLM behavior beyond stereotypes, such as knowledge mastery.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yiran Liu;Ke Yang;Zehan Qi;Xiao Liu;Yang Yu;ChengXiang Zhai,False,https://openreview.net/pdf?id=smxQvTmdGS
snNuvAOQxB,MEQA: A Benchmark for Multi-hop Event-centric Question Answering with Explanations,"Existing benchmarks for multi-hop question answering (QA) primarily evaluate models based on their ability to reason about entities and the relationships between them. However, there's a lack of insight into how these models perform in terms of both events and entities. In this paper, we introduce a novel semi-automatic question generation strategy by composing event structures from information extraction (IE) datasets and present the first Multi-hop Event-centric Question Answering (MEQA) benchmark. It contains (1) 2,243 challenging questions that require a diverse range of complex reasoning over entity-entity, entity-event, and event-event relations; (2) corresponding multi-step QA-format event reasoning chain (explanation) which leads to the answer for each question. We also introduce two metrics for evaluating explanations: completeness and logical consistency. We conduct comprehensive benchmarking and analysis, which shows that MEQA is challenging for the latest state-of-the-art models encompassing large language models (LLMs); and how they fall short of providing faithful explanations of the event-centric reasoning process.",Datasets & Benchmarks,NeurIPS,2024,Poster,Ruosen Li;Zimu Wang;Son Quoc Tran;Lei Xia;Xinya Du,True,https://openreview.net/pdf?id=snNuvAOQxB
sw9iOHGxgm,Learning Action and Reasoning-Centric Image Editing from Videos and Simulation,"An image editing model should be able to perform diverse edits, ranging from object replacement, changing attributes or style, to performing actions or movement, which require many forms of reasoning. Current *general* instruction-guided editing models have significant shortcomings with action and reasoning-centric edits.
Object, attribute or stylistic changes can be learned from visually static datasets. On the other hand, high-quality data for action and reasoning-centric edits is scarce and has to come from entirely different sources that cover e.g. physical dynamics, temporality and spatial reasoning.
To this end, we meticulously curate the **A**U**RO**R**A** Dataset (**A**ction-**R**easoning-**O**bject-**A**ttribute), a collection of high-quality training data, human-annotated and curated from videos and simulation engines.
We focus on a key aspect of quality training data: triplets (source image, prompt, target image) contain a single meaningful visual change described by the prompt, i.e., *truly minimal* changes between source and target images.
To demonstrate the value of our dataset, we evaluate an **A**U**RO**R**A**-finetuned model on a new expert-curated benchmark (**A**U**RO**R**A-Bench**) covering 8 diverse editing tasks.
Our model significantly outperforms previous editing models as judged by human raters.
For automatic evaluations, we find important flaws in previous metrics and caution their use for semantically hard editing tasks.
Instead, we propose a new automatic metric that focuses on discriminative understanding.
We hope that our efforts : (1) curating a quality training dataset and an evaluation benchmark, (2) developing critical evaluations, and (3) releasing a state-of-the-art model, will fuel further progress on general image editing.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Benno Krojer;Dheeraj Vattikonda;Luis Lara;Varun Jampani;Eva Portelance;Christopher Pal;Siva Reddy,True,https://openreview.net/pdf?id=sw9iOHGxgm
t1mAXb4Cop,Can LLMs Solve Molecule Puzzles? A Multimodal Benchmark for Molecular Structure Elucidation,"Large Language Models (LLMs)  have shown significant problem-solving capabilities across predictive and generative tasks in chemistry. 
However, their proficiency in multi-step chemical reasoning remains underexplored. 
We introduce a new challenge: molecular structure elucidation, which involves deducing a molecule’s structure from various types of spectral data. Solving such a molecular puzzle, akin to solving crossword puzzles, poses reasoning challenges that require integrating clues from diverse sources and engaging in iterative hypothesis testing. To address this challenging problem with LLMs, we present \\\\textbf{MolPuzzle}, a benchmark comprising 217 instances of structure elucidation, which feature over 23,000 QA samples presented in a sequential puzzle-solving process, involving three interlinked sub-tasks: molecule understanding, spectrum interpretation, and molecule construction. Our evaluation of 12 LLMs reveals that the best-performing LLM, GPT-4o, performs significantly worse than humans, with only a small portion (1.4\\\\%) of its answers exactly matching the ground truth. However, it performs nearly perfectly in the first subtask of molecule understanding, achieving accuracy close to 100\\\\%. This discrepancy highlights the potential of developing advanced LLMs with improved chemical reasoning capabilities in the other two sub-tasks. Our MolPuzzle dataset and evaluation code are available at this  \\\\href{https://github.com/KehanGuo2/MolPuzzle}{link}.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Kehan Guo;Bozhao Nan;Yujun Zhou;Taicheng Guo;Zhichun Guo;Mihir Surve;Zhenwen Liang;Nitesh V Chawla;Olaf Wiest;Xiangliang Zhang,True,https://openreview.net/pdf?id=t1mAXb4Cop
t3Xj7YD7fL,An NLP Benchmark Dataset for Predicting the Completeness of ESG Reports,"Environmental, Social, and Governance (ESG) reports serve as a platform for companies to publicly disclose their economic, environmental, and social impacts, as well as their contributions to sustainable development goals. The completeness of ESG reports is considered a crucial criterion for judging their quality and credibility, yet it is often overlooked in existing literature. This paper aims to comprehensively assess the completeness of ESG reports by evaluating their topic coverage and text quality. To achieve this goal, we propose two classification tasks: topic classification and quality classification for ESG sentences. To train the classifiers, we collected 14,468 ESG reports from Chinese-listed companies. We then segment them into sentences and label 8K of them with both topic and text quality tags. By fine-tuning several large language models (LLMs) on this dataset on the two classification tasks, we find that our dataset has the potential to fill the gap in academia regarding methods for measuring ESG completeness.",Datasets & Benchmarks,NeurIPS,2024,Reject,Qi Chang;Xuan Yang;Zihan Ding;Bin Liu;Wei Lan,True,https://openreview.net/pdf?id=t3Xj7YD7fL
t6LQXcFTEn,AudioMarkBench: Benchmarking Robustness of Audio Watermarking,"The increasing realism of synthetic speech, driven by advancements in text-to-speech models, raises ethical concerns regarding impersonation and disinformation. Audio watermarking offers a promising solution via embedding human-imperceptible watermarks into AI-generated audios. However, the robustness of audio watermarking against common/adversarial perturbations remains understudied. We present AudioMarkBench, the first systematic benchmark for evaluating the robustness of audio watermarking against *watermark removal* and *watermark forgery*. AudioMarkBench includes a new dataset created from Common-Voice across languages, biological sexes, and ages, 3 state-of-the-art watermarking methods, and 15 types of perturbations. We benchmark the robustness of these methods against the perturbations in no-box, black-box, and white-box settings. Our findings highlight the vulnerabilities of current watermarking techniques and emphasize the need for more robust and fair audio watermarking solutions. Our dataset and code are publicly available at https://github.com/moyangkuo/AudioMarkBench.",Datasets & Benchmarks,NeurIPS,2024,Poster,Hongbin Liu;Moyang Guo;Zhengyuan Jiang;Lun Wang;Neil Zhenqiang Gong,True,https://openreview.net/pdf?id=t6LQXcFTEn
t7xYNN7RJC,Vocal Call Locator Benchmark (VCL) for localizing rodent vocalizations from multi-channel audio,"Understanding the behavioral and neural dynamics of social interactions is a goal
of contemporary neuroscience. Many machine learning methods have emerged
in recent years to make sense of complex video and neurophysiological data that
result from these experiments. Less focus has been placed on understanding how
animals process acoustic information, including social vocalizations. A critical
step to bridge this gap is determining the senders and receivers of acoustic infor-
mation in social interactions. While sound source localization (SSL) is a classic
problem in signal processing, existing approaches are limited in their ability to
localize animal-generated sounds in standard laboratory environments. Advances
in deep learning methods for SSL are likely to help address these limitations,
however there are currently no publicly available models, datasets, or benchmarks
to systematically evaluate SSL algorithms in the domain of bioacoustics. Here,
we present the VCL Benchmark: the first large-scale dataset for benchmarking
SSL algorithms in rodents. We acquired synchronized video and multi-channel
audio recordings of 767,295 sounds with annotated ground truth sources across 9
conditions. The dataset provides benchmarks which evaluate SSL performance on
real data, simulated acoustic data, and a mixture of real and simulated data. We
intend for this benchmark to facilitate knowledge transfer between the neuroscience
and acoustic machine learning communities, which have had limited overlap.",Datasets & Benchmarks,NeurIPS,2024,Poster,Ralph E Peterson;Aramis Tanelus;Christopher A. Ick;Bartul Mimica;M J Niegil Francis;Violet Jane Ivan;Aman Choudhri;Annegret Falkner;Mala Murthy;David M Schneider;Dan H. Sanes;Alex H Williams,True,https://openreview.net/pdf?id=t7xYNN7RJC
t9aThFL1lE,UnlearnCanvas: Stylized Image Dataset for Enhanced Machine Unlearning Evaluation in Diffusion Models,"The technological advancements in diffusion models (DMs) have demonstrated unprecedented capabilities in text-to-image generation and are widely used in diverse applications. However, they have also raised significant societal concerns, such as the generation of harmful content and copyright disputes. Machine unlearning (MU) has emerged as a promising solution, capable of removing undesired generative capabilities from DMs. However, existing MU evaluation systems present several key challenges that can result in incomplete and inaccurate assessments. To address these issues, we propose UnlearnCanvas, a comprehensive high-resolution stylized image dataset that facilitates the evaluation of the unlearning of artistic styles and associated objects. This dataset enables the establishment of a standardized, automated evaluation framework with 7 quantitative metrics assessing various aspects of the unlearning performance for DMs. Through extensive experiments, we benchmark 9 state-of-the-art MU methods for DMs, revealing novel insights into their strengths, weaknesses, and underlying mechanisms. Additionally, we explore challenging unlearning scenarios for DMs to evaluate worst-case performance against adversarial prompts, the unlearning of finer-scale concepts, and sequential unlearning. We hope that this study can pave the way for developing more effective, accurate, and robust DM unlearning methods, ensuring safer and more ethical applications of DMs in the future. The dataset, benchmark, and codes are publicly available at this [link](https://unlearn-canvas.netlify.app/).",Datasets & Benchmarks,NeurIPS,2024,Poster,Yihua Zhang;Chongyu Fan;Yimeng Zhang;Yuguang Yao;Jinghan Jia;Jiancheng Liu;Gaoyuan Zhang;Gaowen Liu;Ramana Rao Kompella;Xiaoming Liu;Sijia Liu,True,https://openreview.net/pdf?id=t9aThFL1lE
tBRNC6YemY,Gorilla: Large Language Model Connected with Massive APIs,"Large Language Models (LLMs) have seen an impressive wave of advances, with
models now excelling in a variety of tasks, such as mathematical reasoning and
program synthesis. However, their potential to effectively use tools via API calls
remains unfulfilled. This is a challenging task even for today’s state-of-the-art
LLMs such as GPT-4 largely due to their unawareness of what APIs are available
and how to use them in a frequently updated tool set. We develop Gorilla, a
finetuned LLaMA model that surpasses the performance of GPT-4 on writing API
calls. Trained with the novel Retriever Aware Training (RAT), when combined
with a document retriever, Gorilla demonstrates a strong capability to adapt to
test-time document changes, allowing flexible user updates or version changes.
It also substantially mitigates the issue of hallucination, commonly encountered
when prompting LLMs directly. To evaluate the model’s ability, we introduce
APIBench, a comprehensive dataset consisting of HuggingFace, TorchHub, and
TensorHub APIs. The successful integration of the retrieval system with Gorilla
demonstrates the potential for LLMs to use tools more accurately, keep up with
frequently updated documentation, and consequently increase the reliability and
applicability of their outputs. Gorilla’s code, model, data, and demo are available
at: https://gorilla.cs.berkeley.edu",main,NeurIPS,2024,Poster,Shishir G Patil;Tianjun Zhang;Xin Wang;Joseph E. Gonzalez,True,https://openreview.net/pdf?id=tBRNC6YemY
tN61DTr4Ed,OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments,"Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at [this https URL](https://os-world.github.io/).",Datasets & Benchmarks,NeurIPS,2024,Poster,Tianbao Xie;Danyang Zhang;Jixuan Chen;Xiaochuan Li;Siheng Zhao;Ruisheng Cao;Toh Jing Hua;Zhoujun Cheng;Dongchan Shin;Fangyu Lei;Yitao Liu;Yiheng Xu;Shuyan Zhou;Silvio Savarese;Caiming Xiong;Victor Zhong;Tao Yu,True,https://openreview.net/pdf?id=tN61DTr4Ed
tNCdnpEKrR,QGym: Scalable Simulation and Benchmarking of Queuing Network Controllers,"Queuing network control allows allocation of  scarce resources to manage congestion, a fundamental problem in manufacturing, communications, and healthcare. Compared to standard RL problems, queueing problems  are distinguished by unique challenges: i) a system operating in continuous time, ii) high stochasticity, and iii) long horizons over which the system can become unstable (exploding delays). To provide the empirical foundations for methodological development tackling these challenges, we present an open-sourced  queueing simulation framework, QGym, that benchmark queueing policies across realistic problem instances. Our modular framework allows the researchers to build on our initial instances, which provide a wide range of environments including parallel servers, criss-cross, tandem, and re-entrant networks, as well as a realistically calibrated hospital queuing system.  From these, various policies can be easily tested, including both model-free RL methods and classical queuing policies. Our testbed significantly expands the scope of 
empirical benchmarking in prior work, and complements the
traditional focus on evaluating algorithms based on mathematical guarantees in idealized settings. QGym code is open-sourced at https://github.com/namkoong-lab/QGym.",Datasets & Benchmarks,NeurIPS,2024,Poster,Haozhe Chen;Ang Li;Ethan Che;Jing Dong;Tianyi Peng;Hongseok Namkoong,False,https://openreview.net/pdf?id=tNCdnpEKrR
tPgagXpvcV,Any2Graph: Deep End-To-End Supervised Graph Prediction With An Optimal Transport Loss,"We propose Any2graph, a generic framework for end-to-end Supervised Graph Prediction (SGP) i.e. a deep learning model that predicts an entire graph for any kind of input. The framework is built on a novel Optimal Transport loss, the Partially-Masked Fused Gromov-Wasserstein, that exhibits all necessary properties (permutation invariance, differentiability and scalability) and is designed to handle any-sized graphs. Numerical experiments showcase the versatility of the approach that outperform existing competitors on a novel challenging synthetic dataset and a variety of real-world tasks such as map construction from satellite image (Sat2Graph) or molecule prediction from fingerprint (Fingerprint2Graph).",main,NeurIPS,2024,Spotlight,Paul KRZAKALA;Junjie Yang;Rémi Flamary;Florence d'Alché-Buc;Charlotte Laclau;Matthieu Labeau,True,https://openreview.net/pdf?id=tPgagXpvcV
tPsw4NeLZx,MM-WLAuslan: Multi-View Multi-Modal Word-Level Australian Sign Language Recognition Dataset,"Isolated Sign Language Recognition (ISLR) focuses on identifying individual sign language glosses. Considering the diversity of sign languages across geographical regions, developing region-specific ISLR datasets is crucial for supporting communication and research. Auslan, as a sign language specific to Australia, still lacks a dedicated large-scale word-level dataset for the ISLR task. To fill this gap, we curate \\\\underline{\\\\textbf{the first}} large-scale Multi-view Multi-modal Word-Level Australian Sign Language recognition dataset, dubbed MM-WLAuslan. Compared to other publicly available datasets, MM-WLAuslan exhibits three significant advantages: (1) **the largest amount** of data, (2) **the most extensive** vocabulary, and (3) **the most diverse** of multi-modal camera views. Specifically, we record **282K+** sign videos covering **3,215** commonly used Auslan glosses presented by **73** signers in a studio environment.
Moreover, our filming system includes two different types of cameras, i.e., three Kinect-V2 cameras and a RealSense camera. We position cameras hemispherically around the front half of the model and simultaneously record videos using all four cameras. Furthermore, we benchmark results with state-of-the-art methods for various multi-modal ISLR settings on MM-WLAuslan, including multi-view, cross-camera, and cross-view. Experiment results indicate that MM-WLAuslan is a challenging ISLR dataset, and we hope this dataset will contribute to the development of Auslan and the advancement of sign languages worldwide. All datasets and benchmarks are available at MM-WLAuslan.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xin Shen;Heming Du;Hongwei Sheng;Shuyun Wang;Hui Chen;Huiqiang Chen;Zhuojie Wu;Xiaobiao Du;Jiaying Ying;Ruihan Lu;Qingzheng Xu;Xin Yu,True,https://openreview.net/pdf?id=tPsw4NeLZx
tRRWoa9e80,Token Merging for Training-Free Semantic Binding in Text-to-Image Synthesis,"Although text-to-image (T2I) models exhibit remarkable generation capabilities,
they frequently fail to accurately bind semantically related objects or attributes
in the input prompts; a challenge termed semantic binding. Previous approaches
either involve intensive fine-tuning of the entire T2I model or require users or
large language models to specify generation layouts, adding complexity. In this
paper, we define semantic binding as the task of associating a given object with its
attribute, termed attribute binding, or linking it to other related sub-objects, referred
to as object binding. We introduce a novel method called Token Merging (ToMe),
which enhances semantic binding by aggregating relevant tokens into a single
composite token. This ensures that the object, its attributes and sub-objects all share
the same cross-attention map. Additionally, to address potential confusion among
main objects with complex textual prompts, we propose end token substitution as
a complementary strategy. To further refine our approach in the initial stages of
T2I generation, where layouts are determined, we incorporate two auxiliary losses,
an entropy loss and a semantic binding loss, to iteratively update the composite
token to improve the generation integrity. We conducted extensive experiments to
validate the effectiveness of ToMe, comparing it against various existing methods
on the T2I-CompBench and our proposed GPT-4o object binding benchmark. Our
method is particularly effective in complex scenarios that involve multiple objects
and attributes, which previous methods often fail to address. The code will be
 publicly available at https://github.com/hutaihang/ToMe",main,NeurIPS,2024,Poster,taihang Hu;Linxuan Li;Joost van de Weijer;Hongcheng Gao;Fahad Khan;Jian Yang;Ming-Ming Cheng;Kai Wang;Yaxing Wang,True,https://openreview.net/pdf?id=tRRWoa9e80
tU8Xgybudy,Evaluating Multiview Object Consistency in Humans and Image Models,"We introduce a benchmark to directly evaluate the alignment between human observers and vision models on a 3D shape inference task. We leverage an experimental design from the cognitive sciences: given a set of images, participants identify which contain the same/different objects, despite considerable viewpoint variation. We draw from a diverse range of images that include common objects (e.g., chairs) as well as abstract shapes (i.e., procedurally generated 'nonsense' objects). After constructing over 2000 unique image sets, we administer these tasks to human participants, collecting 35K trials of behavioral data from over 500 participants. This includes explicit choice behaviors as well as intermediate measures, such as reaction time and gaze data. We then evaluate the performance of common vision models (e.g., DINOv2, MAE, CLIP). We find that humans outperform all models by a wide margin. Using a multi-scale evaluation approach, we identify underlying similarities and differences between models and humans: while human-model performance is correlated, humans allocate more time/processing on challenging trials. All images, data, and code can be accessed via our project page.",Datasets & Benchmarks,NeurIPS,2024,Poster,tyler bonnen;Stephanie Fu;Yutong Bai;Thomas O'Connell;Yoni Friedman;Nancy Kanwisher;Joshua B. Tenenbaum;Alexei A Efros,True,https://openreview.net/pdf?id=tU8Xgybudy
tWvVtOW0qg,Data Measurements for Decentralized Data Markets,"Decentralized data markets can provide more equitable forms of data acquisition for machine learning.
    However, to realize practical marketplaces, efficient techniques for seller selection need to be developed. 
    We propose and benchmark federated data measurements to allow a data buyer to find sellers with relevant and diverse datasets.
    Diversity and relevance measures enable a buyer to make relative comparisons between sellers without requiring intermediate brokers and training task-dependent models.",Datasets & Benchmarks,NeurIPS,2024,Reject,Charles Lu;Mohammad Mohammadi Amiri;Ramesh Raskar,False,https://openreview.net/pdf?id=tWvVtOW0qg
tllpLtt14h,Einsum Benchmark: Enabling the Development of Next-Generation Tensor Execution Engines,"Modern artificial intelligence and machine learning workflows rely on efficient tensor libraries. However, tuning tensor libraries without considering the actual problems they are meant to execute can lead to a mismatch between expected performance and the actual performance. Einsum libraries are tuned to efficiently execute tensor expressions with only a few, relatively large, dense, floating-point tensors. But, practical applications of einsum cover a much broader range of tensor expressions than those that can currently be executed efficiently. For this reason, we have created a benchmark dataset that encompasses this broad range of tensor expressions, allowing future implementations of einsum to build upon and be evaluated against. In addition, we also provide generators for einsum expressions and converters to einsum expressions in our repository, so that additional data can be generated as needed. The benchmark dataset, the generators and converters are released openly and are publicly available at https://benchmark.einsum.org.",Datasets & Benchmarks,NeurIPS,2024,Poster,Mark Blacher;Christoph Staudt;Julien Klaus;Maurice Wenig;Niklas Merk;Alexander Breuer;Max Engel;Sören Laue;Joachim Giesen,True,https://openreview.net/pdf?id=tllpLtt14h
tmX1AUmkl6,Evaluation of Text-to-Video Generation Models: A Dynamics Perspective,"Comprehensive and constructive evaluation protocols play an important role when developing sophisticated text-to-video (T2V) generation models. Existing evaluation protocols primarily focus on temporal consistency and content continuity, yet largely ignore dynamics of video content. Such dynamics is an essential dimension measuring the visual vividness and the honesty of video content to text prompts. In this study, we propose an effective evaluation protocol, termed DEVIL, which centers on the dynamics dimension to evaluate T2V generation models, as well as improving existing evaluation metrics. In practice, we define a set of dynamics scores corresponding to multiple temporal granularities, and a new benchmark of text prompts under multiple dynamics grades. Upon the text prompt benchmark, we assess the generation capacity of T2V models, characterized by metrics of dynamics ranges and T2V alignment. Moreover, we analyze the relevance of existing metrics to dynamics metrics, improving them from the perspective of dynamics. Experiments show that DEVIL evaluation metrics enjoy up to about 90\\\\% consistency with human ratings, demonstrating the potential to advance T2V generation models.",main,NeurIPS,2024,Poster,Mingxiang Liao;Hannan Lu;Qixiang Ye;Wangmeng Zuo;Fang Wan;Tianyu Wang;Yuzhong Zhao;Jingdong Wang;Xinyu Zhang,True,https://openreview.net/pdf?id=tmX1AUmkl6
ttLcbEkaj6,AirSketch: Generative Motion to Sketch,"Illustration is a fundamental mode of human expression and communication. Certain types of motion that accompany speech can provide this illustrative mode of communication. While Augmented and Virtual Reality technologies (AR/VR) have introduced tools for producing drawings with hand motions (air drawing), they typically require costly hardware and additional digital markers, thereby limiting their accessibility and portability. Furthermore, air drawing demands considerable skill to achieve aesthetic results. To address these challenges, we introduce the concept of AirSketch, aimed at generating faithful and visually coherent sketches directly from hand motions, eliminating the need for complicated headsets or markers. We devise a simple augmentation-based self-supervised training procedure, enabling a controllable image diffusion model to learn to translate from highly noisy hand tracking images to clean, aesthetically pleasing sketches, while preserving the essential visual cues from the original tracking data. We present two air drawing datasets to study this problem. Our findings demonstrate that beyond producing photo-realistic images from precise spatial inputs, controllable image diffusion can effectively produce a refined, clear sketch from a noisy input. Our work serves as an initial step towards marker-less air drawing and reveals distinct applications of controllable diffusion models to AirSketch and AR/VR in general.",main,NeurIPS,2024,Poster,Hui Xian Grace Lim;Xuanming Cui;Yogesh S Rawat;Ser-Nam Lim,True,https://openreview.net/pdf?id=ttLcbEkaj6
tu1oC7zHGW,Unveiling the Tapestry of Consistency in Large Vision-Language Models,"Large vision-language models (LVLMs) have recently achieved rapid progress, exhibiting great perception and reasoning abilities concerning visual information. However, when faced with prompts in different sizes of solution spaces, LVLMs fail to always give consistent answers regarding the same knowledge point. This inconsistency of answers between different solution spaces is prevalent in LVLMs and erodes trust. To this end, we provide a multi-modal benchmark ConBench, to intuitively analyze how LVLMs perform when the solution space of a prompt revolves around a knowledge point. Based on the ConBench tool, we are the first to reveal the tapestry and get the following findings: (1) In the discriminate realm, the larger the solution space of the prompt, the lower the accuracy of the answers. 
(2) Establish the relationship between the discriminative and generative realms: the accuracy of the discriminative question type exhibits a strong positive correlation with its Consistency with the caption. (3) Compared to open-source models, closed-source models exhibit a pronounced bias advantage in terms of Consistency. Eventually, we ameliorate the consistency of LVLMs by trigger-based diagnostic refinement, indirectly improving the performance of their caption. We hope this paper will accelerate the research community in better evaluating their models and encourage future advancements in the consistency domain.",main,NeurIPS,2024,Poster,Yuan Zhang;Fei xiao;Tao Huang;Chun-Kai Fan;Hongyuan Dong;Jiawen Li;Jiacong Wang;Kuan Cheng;Shanghang Zhang;Haoyuan Guo,True,https://openreview.net/pdf?id=tu1oC7zHGW
twFlD3C9Rt,Beyond Prompts: Dynamic Conversational Benchmarking of Large Language Models,"We introduce a dynamic benchmarking system for conversational agents that evaluates their performance through a single, simulated, and lengthy user$\\\\leftrightarrow$agent interaction. The interaction is a conversation between the user and agent, where multiple tasks are introduced and then undertaken concurrently. We context switch regularly to interleave the tasks, which constructs a realistic testing scenario in which we assess the Long-Term Memory, Continual Learning, and Information Integration capabilities of the agents. Results from both proprietary and open-source Large-Language Models show that LLMs in general perform well on single-task interactions, but they struggle on the same tasks when they are interleaved. Notably, short-context LLMs supplemented with an LTM system perform as well as or better than those with larger contexts. Our benchmark suggests that there are other challenges for LLMs responding to more natural interactions that contemporary benchmarks have heretofore not been able to capture.",Datasets & Benchmarks,NeurIPS,2024,Poster,David Castillo-Bolado;Joseph Davidson;Finlay Gray;Marek Rosa,True,https://openreview.net/pdf?id=twFlD3C9Rt
twpPD9UMUN,"Look, Listen, and Answer: Overcoming Biases for Audio-Visual Question Answering","Audio-Visual Question Answering (AVQA) is a complex multi-modal reasoning task, demanding intelligent systems to accurately respond to natural language queries based on audio-video input pairs. Nevertheless, prevalent AVQA approaches are prone to overlearning dataset biases, resulting in poor robustness. Furthermore, current datasets may not provide a precise diagnostic for these methods. To tackle these challenges, firstly, we propose a novel dataset, *MUSIC-AVQA-R*, crafted in two steps: rephrasing questions within the test split of a public dataset (*MUSIC-AVQA*) and subsequently introducing distribution shifts to split questions. The former leads to a large, diverse test space, while the latter results in a comprehensive robustness evaluation on rare, frequent, and overall questions. Secondly, we propose a robust architecture that utilizes a multifaceted cycle collaborative debiasing strategy to overcome bias learning. Experimental results show that this architecture achieves state-of-the-art performance on MUSIC-AVQA-R, notably obtaining a significant improvement of 9.32\\\\%. Extensive ablation experiments are conducted on the two datasets mentioned to analyze the component effectiveness within the debiasing strategy. Additionally, we highlight the limited robustness of existing multi-modal QA methods through the evaluation on our dataset. We also conduct experiments combining various baselines with our proposed strategy on two datasets to verify its plug-and-play capability. Our dataset and code are available at <https://github.com/reml-group/MUSIC-AVQA-R>.",main,NeurIPS,2024,Poster,Jie Ma;Min Hu;Pinghui Wang;Wangchun Sun;Lingyun Song;Hongbin Pei;Jun Liu;Youtian Du,True,https://openreview.net/pdf?id=twpPD9UMUN
u1mNGLYN74,DRACO: A Denoising-Reconstruction Autoencoder for Cryo-EM,"Foundation models in computer vision have demonstrated exceptional performance in zero-shot and few-shot tasks by extracting multi-purpose features from large-scale datasets through self-supervised pre-training methods. However, these models often overlook the severe corruption in cryogenic electron microscopy (cryo-EM) images by high-level noises. We introduce DRACO, a Denoising-Reconstruction Autoencoder for CryO-EM, inspired by the Noise2Noise (N2N) approach. By processing cryo-EM movies into odd and even images and treating them as independent noisy observations, we apply a denoising-reconstruction hybrid training scheme. We mask both images to create denoising and reconstruction tasks. For DRACO's pre-training, the quality of the dataset is essential, we hence build a high-quality, diverse dataset from an uncurated public database, including over 270,000 movies or micrographs. After pre-training, DRACO naturally serves as a generalizable cryo-EM image denoiser and a foundation model for various cryo-EM downstream tasks. DRACO demonstrates the best performance in denoising, micrograph curation, and particle picking tasks compared to state-of-the-art baselines.",main,NeurIPS,2024,Poster,YingJun Shen;Haizhao Dai;Qihe Chen;Yan Zeng;Jiakai Zhang;Yuan Pei;Jingyi Yu,True,https://openreview.net/pdf?id=u1mNGLYN74
u7m2CG84BQ,BABILong: Testing the Limits of LLMs with Long Context Reasoning-in-a-Haystack,"In recent years, the input context sizes of large language models (LLMs) have increased dramatically. However, existing evaluation methods have not kept pace, failing to comprehensively assess the efficiency of models in handling long contexts. To bridge this gap, we introduce the BABILong benchmark, designed to test language models' ability to reason across facts distributed in extremely long documents. BABILong includes a diverse set of 20 reasoning tasks, including fact chaining, simple induction, deduction, counting, and handling lists/sets. These tasks are challenging on their own, and even more demanding when the required facts are scattered across long natural text. Our evaluations show that popular LLMs effectively utilize only 10-20% of the context and their performance declines sharply with increased reasoning complexity. Among alternatives to in-context reasoning, Retrieval-Augmented Generation methods achieve a modest 60% accuracy on single-fact question answering, independent of context length. Among context extension methods, the highest performance is demonstrated by recurrent memory transformers after fine-tuning, enabling the processing of lengths up to 50 million tokens. The BABILong benchmark is extendable to any length to support the evaluation of new upcoming models with increased capabilities, and we provide splits up to 10 million token lengths.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Yuri Kuratov;Aydar Bulatov;Petr Anokhin;Ivan Rodkin;Dmitry Igorevich Sorokin;Artyom Sorokin;Mikhail Burtsev,True,https://openreview.net/pdf?id=u7m2CG84BQ
uCZI8gSfD4,Training Compute-Optimal Protein Language Models,"We explore optimally training protein language models, an area of significant interest in biological research where guidance on best practices is limited.
Most models are trained with extensive compute resources until performance gains plateau, focusing primarily on increasing model sizes rather than optimizing the efficient compute frontier that balances performance and compute budgets.
Our investigation is grounded in a massive dataset consisting of 939 million protein sequences. 
We trained over 300 models ranging from 3.5 million to 10.7 billion parameters on 5 to 200 billion unique tokens, to investigate the relations between model sizes, training token numbers, and objectives.
First, we observed the effect of diminishing returns for the Causal Language Model (CLM) and that of overfitting for Masked Language Model (MLM) when repeating the commonly used Uniref database. To address this, we included metagenomic protein sequences in the training set to increase the diversity and avoid the plateau or overfitting effects. 
Second, we obtained the scaling laws of CLM and MLM on Transformer, tailored to the specific characteristics of protein sequence data. 
Third, we observe a transfer scaling phenomenon from CLM to MLM, further demonstrating the effectiveness of transfer through scaling behaviors based on estimated Effectively Transferred Tokens.
Finally, to validate our scaling laws, we compare the large-scale versions of ESM-2 and PROGEN2 on downstream tasks, encompassing evaluations of protein generation as well as structure- and function-related tasks, all within less or equivalent pre-training compute budgets.",main,NeurIPS,2024,Spotlight,Xingyi Cheng;Bo Chen;Pan Li;Jing Gong;Jie Tang;Le Song,True,https://openreview.net/pdf?id=uCZI8gSfD4
uKqn1Flsbp,Personalized Instance-based Navigation Toward User-Specific Objects in Realistic Environments,"In the last years, the research interest in visual navigation towards objects in indoor environments has grown significantly. This growth can be attributed to the recent availability of large navigation datasets in photo-realistic simulated environments, like Gibson and Matterport3D. However, the navigation tasks supported by these datasets are often restricted to the objects present in the environment at acquisition time. Also, they fail to account for the realistic scenario in which the target object is a user-specific instance that can be easily confused with similar objects and may be found in multiple locations within the environment. To address these limitations, we propose a new task denominated Personalized Instance-based Navigation (PIN), in which an embodied agent is tasked with locating and reaching a specific personal object by distinguishing it among multiple instances of the same category. The task is accompanied by PInNED, a dedicated new dataset composed of photo-realistic scenes augmented with additional 3D objects. In each episode, the target object is presented to the agent using two modalities: a set of visual reference images on a neutral background and manually annotated textual descriptions. Through comprehensive evaluations and analyses, we showcase the challenges of the PIN task as well as the performance and shortcomings of currently available methods designed for object-driven navigation, considering modular and end-to-end agents.",Datasets & Benchmarks,NeurIPS,2024,Poster,Luca Barsellotti;Roberto Bigazzi;Marcella Cornia;Lorenzo Baraldi;Rita Cucchiara,True,https://openreview.net/pdf?id=uKqn1Flsbp
uXJlgkWdcI,PACE: Pacing Operator Learning to Accurate Optical Field Simulation for Complicated Photonic Devices,"Electromagnetic field simulation is central to designing, optimizing, and validating photonic devices and circuits. 
However, costly computation associated with numerical simulation poses a significant bottleneck, hindering scalability and turnaround time in the photonic circuit design process.
Neural operators offer a promising alternative, but existing SOTA approaches, Neurolight, struggle with predicting high-fidelity fields for real-world complicated photonic devices, with the best reported 0.38 normalized mean absolute error in Neurolight.
The interplays of highly complex light-matter interaction, e.g., scattering and resonance, sensitivity to local structure details, non-uniform learning complexity for full-domain simulation, and rich frequency information, contribute to the failure of existing neural PDE solvers.
In this work, we boost the prediction fidelity to an unprecedented level for simulating complex photonic devices with a novel operator design driven by the above challenges.
We propose a novel cross-axis factorized PACE operator with a strong long-distance modeling capacity to connect the full-domain complex field pattern with local device structures.
Inspired by human learning, we further divide and conquer the simulation task for extremely hard cases into two progressively easy tasks, with a first-stage model learning an initial solution refined by a second model.
On various complicated photonic device benchmarks, we demonstrate one sole PACE model is capable of achieving 73% lower error with 50% fewer parameters compared with various recent ML for PDE solvers.
The two-stage setup further advances high-fidelity simulation for even more intricate cases.
In terms of runtime, 
PACE demonstrates 154-577x and 11.8-12x simulation speedup over numerical solver using scipy or highly-optimized pardiso solver, respectively.
We open-sourced the code and *complicated* optical device dataset at [PACE-Light](https://github.com/zhuhanqing/PACE-Light).",main,NeurIPS,2024,Poster,Hanqing Zhu;Wenyan Cong;Guojin Chen;Shupeng Ning;Ray Chen;Jiaqi Gu;David Z. Pan,True,https://openreview.net/pdf?id=uXJlgkWdcI
ujDKXWTbJX,JiuZhang3.0: Efficiently Improving Mathematical Reasoning by Training Small Data Synthesis Models,"Mathematical reasoning is an important capability of large language models~(LLMs) for real-world applications.
To enhance this capability, existing work either collects large-scale math-related texts for pre-training, or relies on stronger LLMs (\\\\eg GPT-4) to synthesize massive math problems. Both types of work generally lead to large costs in training or synthesis.
To reduce the cost, based on open-source available texts, we propose an efficient way that trains a small LLM for math problem synthesis, to efficiently generate sufficient high-quality pre-training data.
To achieve it, we create a dataset using GPT-4 to distill its data synthesis capability into the small LLM.
Concretely, we craft a set of prompts based on human education stages to guide GPT-4, to synthesize problems covering diverse math knowledge and difficulty levels.
Besides, we adopt the gradient-based influence estimation method to select the most valuable math-related texts.
The both are fed into GPT-4 for creating the knowledge distillation dataset to train the small LLM.
We leverage it to synthesize 6 million math problems for pre-training our JiuZhang3.0 model. The whole process only needs to invoke GPT-4 API 9.3k times and use 4.6B data for training.
Experimental results have shown that JiuZhang3.0 achieves state-of-the-art performance on several mathematical reasoning datasets, under both natural language reasoning and tool manipulation settings.
Our code and data will be publicly released in \\\\url{https://github.com/RUCAIBox/JiuZhang3.0}.",main,NeurIPS,2024,Poster,Kun Zhou;Beichen Zhang;jiapeng wang;Zhipeng Chen;Xin Zhao;Jing Sha;Zhichao Sheng;Shijin Wang;Ji-Rong Wen,True,https://openreview.net/pdf?id=ujDKXWTbJX
urJyyMKs7E,HW-GPT-Bench: Hardware-Aware Architecture Benchmark for Language Models,"The increasing size of language models necessitates a thorough analysis across multiple dimensions to assess trade-offs among crucial hardware metrics such as latency, energy consumption, GPU memory usage, and performance. Identifying optimal model configurations under specific hardware constraints is becoming essential but remains challenging due to the computational load of exhaustive training and evaluation on multiple devices. To address this, we introduce HW-GPT-Bench, a hardware-aware benchmark that utilizes surrogate predictions to approximate various hardware metrics across 13 devices of architectures in the GPT-2 family, with architectures containing up to 1.55B parameters. Our surrogates, via calibrated predictions and reliable uncertainty estimates, faithfully model the heteroscedastic noise inherent in the energy and latency measurements. To estimate perplexity, we employ weight-sharing techniques from Neural Architecture Search (NAS), inheriting pretrained weights from the largest GPT-2 model. Finally, we demonstrate the utility of HW-GPT-Bench by simulating optimization trajectories of various multi-objective optimization algorithms in just a few seconds.",Datasets & Benchmarks,NeurIPS,2024,Poster,Rhea Sanjay Sukthanker;Arber Zela;Benedikt Staffler;Aaron Klein;Lennart Purucker;Jörg K.H. Franke;Frank Hutter,True,https://openreview.net/pdf?id=urJyyMKs7E
urgpcr7kFR,Alberta Wells Dataset: Pinpointing Oil and Gas Wells from Satellite Imagery,"Millions of abandoned oil and gas wells are scattered across the world, leaching methane into the atmosphere and toxic compounds into the groundwater. Many of these locations are unknown, preventing the wells from being plugged and their polluting effects averted. Remote sensing is a relatively unexplored tool for pinpointing abandoned wells at scale. We introduce the first large-scale dataset for this problem, leveraging medium-resolution multi-spectral satellite imagery from Planet Labs. Our curated dataset comprises over 213,000 wells (abandoned, suspended and active) from Alberta, a region with especially high well density, sourced from the Alberta Energy Regulator and verified by domain experts. We evaluate baseline algorithms for well detection and segmentation, showing the promise of computer vision approaches but also significant room for improvement.",Datasets & Benchmarks,NeurIPS,2024,Reject,Pratinav Seth;Michelle Lin;BREFO DWAMENA YAW;Jade Boutot;Mary Kang;David Rolnick,True,https://openreview.net/pdf?id=urgpcr7kFR
urjPCYZt0I,JailbreakBench: An Open Robustness Benchmark for Jailbreaking Large Language Models,"Jailbreak attacks cause large language models (LLMs) to generate harmful, unethical, or otherwise objectionable content. Evaluating these attacks presents a number of challenges, which the current collection of benchmarks and evaluation techniques do not adequately address. First, there is no clear standard of practice regarding jailbreaking evaluation. Second, existing works compute costs and success rates in incomparable ways. And third, numerous works are not reproducible, as they withhold adversarial prompts, involve closed-source code, or rely on evolving proprietary APIs. To address these challenges, we introduce JailbreakBench, an open-sourced benchmark with the following components: (1) an evolving repository of state-of-the-art adversarial prompts, which we refer to as *jailbreak artifacts*; (2) a jailbreaking dataset comprising 100 behaviors---both original and sourced from prior work---which align with OpenAI's usage policies; (3) a standardized evaluation framework at https://github.com/JailbreakBench/jailbreakbench that includes a clearly defined threat model, system prompts, chat templates, and scoring functions; and (4) a leaderboard at https://jailbreakbench.github.io/ that tracks the performance of attacks and defenses for various LLMs. We have carefully considered the potential ethical implications of releasing this benchmark, and believe that it will be a net positive for the community.",Datasets & Benchmarks,NeurIPS,2024,Poster,Patrick Chao;Edoardo Debenedetti;Alexander Robey;Maksym Andriushchenko;Francesco Croce;Vikash Sehwag;Edgar Dobriban;Nicolas Flammarion;George J. Pappas;Florian Tramèr;Hamed Hassani;Eric Wong,True,https://openreview.net/pdf?id=urjPCYZt0I
uvvVjWP1aj,AI Sandbagging: Language Models can Strategically Underperform on Evaluations,"Trustworthy capability evaluations are crucial for ensuring the safety of AI systems, and are becoming a key component of AI regulation. However, the developers of an AI system, or the AI system itself, may have incentives for evaluations to understate the AI's actual capability. These conflicting interests lead to the problem of *sandbagging* – which we define as *strategic underperformance on an evaluation*. In this paper we assess sandbagging capabilities in contemporary language models (LMs). We prompt frontier LMs, like GPT-4 and Claude 3 Opus, to selectively underperform on dangerous capability evaluations, while maintaining performance on general (harmless) capability evaluations. Moreover, we find that models can be fine-tuned, on a synthetic dataset, to hide specific capabilities unless given a password. This behaviour generalizes to high-quality, held-out benchmarks such as WMDP. In addition, we show that both frontier and smaller models can be prompted or password-locked to target specific scores on a capability evaluation. We have mediocre success in password-locking a model to mimic the answers a weaker model would give. Overall, our results suggest that capability evaluations are vulnerable to sandbagging. This vulnerability decreases the trustworthiness of evaluations, and thereby undermines important safety decisions regarding the development and deployment of advanced AI systems.

We publish our code and results at https://anonymous.4open.science/r/Sandbagging-8305/README.md",main,NeurIPS,2024,Reject,Teun van der Weij;Felix Hofstätter;Oliver Jaffe;Samuel F. Brown;Francis Rhys Ward,True,https://openreview.net/pdf?id=uvvVjWP1aj
uyLtEFnpQP,Du-IN: Discrete units-guided mask modeling for decoding speech from Intracranial Neural signals,"Invasive brain-computer interfaces with Electrocorticography (ECoG) have shown promise for high-performance speech decoding in medical applications, but less damaging methods like intracranial stereo-electroencephalography (sEEG) remain underexplored. With rapid advances in representation learning, leveraging abundant recordings to enhance speech decoding is increasingly attractive. However, popular methods often pre-train temporal models based on brain-level tokens, overlooking that brain activities in different regions are highly desynchronized during tasks. Alternatively, they pre-train spatial-temporal models based on channel-level tokens but fail to evaluate them on challenging tasks like speech decoding, which requires intricate processing in specific language-related areas. To address this issue, we collected a well-annotated Chinese word-reading sEEG dataset targeting language-related brain networks from 12 subjects. Using this benchmark, we developed the Du-IN model, which extracts contextual embeddings based on region-level tokens through discrete codex-guided mask modeling. Our model achieves state-of-the-art performance on the 61-word classification task, surpassing all baselines. Model comparisons and ablation studies reveal that our design choices, including (\\\\romannumeral1) temporal modeling based on region-level tokens by utilizing 1D depthwise convolution to fuse channels in the ventral sensorimotor cortex (vSMC) and superior temporal gyrus (STG) and (\\\\romannumeral2) self-supervision through discrete codex-guided mask modeling, significantly contribute to this performance. Overall, our approach -- inspired by neuroscience findings and capitalizing on region-level representations from specific brain regions -- is suitable for invasive brain modeling and represents a promising neuro-inspired AI approach in brain-computer interfaces. Code and dataset are available at https://github.com/liulab-repository/Du-IN.",main,NeurIPS,2024,Poster,Hui Zheng;Haiteng Wang;Weibang Jiang;Zhongtao Chen;Li He;Peiyang Lin;Penghu Wei;Guoguang Zhao;Yunzhe Liu,True,https://openreview.net/pdf?id=uyLtEFnpQP
v1BIm8wESL,Skinned Motion Retargeting with Dense Geometric Interaction Perception,"Capturing and maintaining geometric interactions among different body parts is crucial for successful motion retargeting in skinned characters. Existing approaches often overlook body geometries or add a geometry correction stage after skeletal motion retargeting. This results in conflicts between skeleton interaction and geometry correction, leading to issues such as jittery, interpenetration, and contact mismatches. To address these challenges, we introduce a new retargeting framework, MeshRet, which directly models the dense geometric interactions in motion retargeting. Initially, we establish dense mesh correspondences between characters using semantically consistent sensors (SCS), effective across diverse mesh topologies. Subsequently, we develop a novel spatio-temporal representation called the dense mesh interaction (DMI) field. This field, a collection of interacting SCS feature vectors, skillfully captures both contact and non-contact interactions between body geometries. By aligning the DMI field during retargeting, MeshRet not only preserves motion semantics but also prevents self-interpenetration and ensures contact preservation. Extensive experiments on the public Mixamo dataset and our newly-collected ScanRet dataset demonstrate that MeshRet achieves state-of-the-art performance. Code available at https://github.com/abcyzj/MeshRet.",main,NeurIPS,2024,Spotlight,Zijie Ye;Jia-Wei Liu;Jia Jia;Shikun Sun;Mike Zheng Shou,True,https://openreview.net/pdf?id=v1BIm8wESL
vBKoEZ1PG3,HAWK: Learning to Understand Open-World Video Anomalies,"Video Anomaly Detection (VAD) systems can autonomously monitor and identify disturbances, reducing the need for manual labor and associated costs. However, current VAD systems are often limited by their superficial semantic understanding of scenes and minimal user interaction. Additionally, the prevalent data scarcity in existing datasets restricts their applicability in open-world scenarios.
In this paper, we introduce HAWK, a novel framework that leverages interactive large Visual Language Models (VLM) to interpret video anomalies precisely. Recognizing the difference in motion information between abnormal and normal videos, HAWK explicitly integrates motion modality to enhance anomaly identification. To reinforce motion attention, we construct an auxiliary consistency loss within the motion and video space, guiding the video branch to focus on the motion modality. Moreover, to improve the interpretation of motion-to-language, we establish a clear supervisory relationship between motion and its linguistic representation. Furthermore, we have annotated over 8,000 anomaly videos with language descriptions, enabling effective training across diverse open-world scenarios, and also created 8,000 question-answering pairs for users' open-world questions. The final results demonstrate that HAWK achieves SOTA performance, surpassing existing baselines in both video description generation and question-answering. Our codes/dataset/demo will be released at https://github.com/jqtangust/hawk.",main,NeurIPS,2024,Poster,Jiaqi Tang;Hao LU;RUIZHENG WU;Xiaogang Xu;Ke Ma;Cheng Fang;Bin Guo;Jiangbo Lu;Qifeng Chen;Ying-Cong Chen,True,https://openreview.net/pdf?id=vBKoEZ1PG3
vIOKLMl6wu,"LOVA3: Learning to Visual Question Answering, Asking and Assessment","Question answering, asking, and assessment are three innate human traits crucial for understanding the world and acquiring knowledge. By enhancing these capabilities, humans can more effectively utilize data, leading to better comprehension and learning outcomes. However, current Multimodal Large Language Models (MLLMs) primarily focus on question answering, often neglecting the full potential of questioning and assessment skills. In this study, we introduce LOVA3, an innovative framework named ``Learning tO Visual Question Answering, Asking and Assessment,'' designed to equip MLLMs with these additional capabilities. Our approach involves the creation of two supplementary training tasks GenQA and EvalQA, aiming at fostering the skills of asking and assessing questions in the context of images. To develop the questioning ability, we compile a comprehensive set of multimodal foundational tasks. For assessment, we introduce a new benchmark called EvalQABench, comprising 64,000 training samples (split evenly between positive and negative samples) and 5,000 testing samples. We posit that enhancing MLLMs with the capabilities to answer, ask, and assess questions 
will enhance their multimodal comprehension, ultimately improving overall performance. To validate this hypothesis, we train MLLMs using the LOVA3 framework and evaluate them on a range of multimodal datasets and benchmarks. Our results demonstrate consistent performance gains, underscoring the critical role of these additional tasks in fostering comprehensive intelligence in MLLMs.",main,NeurIPS,2024,Poster,Hengyuan Zhao;Pan Zhou;Difei Gao;Zechen Bai;Mike Zheng Shou,True,https://openreview.net/pdf?id=vIOKLMl6wu
vJaWizbBdA,ERBench: An Entity-Relationship based Automatically Verifiable Hallucination Benchmark for Large Language Models,"Large language models (LLMs) have achieved unprecedented performances in various applications, yet evaluating them is still challenging. Existing benchmarks are either manually constructed or are automatic, but lack the ability to evaluate the thought process of LLMs with arbitrary complexity. We contend that utilizing existing relational databases based on the entity-relationship (ER) model is a promising approach for constructing benchmarks as they contain structured knowledge that can be used to question LLMs. Unlike knowledge graphs, which are also used to evaluate LLMs, relational databases have integrity constraints that can be used to better construct complex in-depth questions and verify answers: (1) functional dependencies can be used to pinpoint critical keywords that an LLM must know to properly answer a given question containing certain attribute values; and (2) foreign key constraints can be used to join relations and construct multi-hop questions, which can be arbitrarily long and used to debug intermediate answers. We thus propose ERBench, which uses these integrity constraints to convert any database into an LLM benchmark. ERBench supports continuous evaluation as databases change, multimodal questions, and various prompt engineering techniques. In our experiments, we construct LLM benchmarks using databases of multiple domains and make an extensive comparison of contemporary LLMs. We show how ERBench can properly evaluate any LLM by not only checking for answer correctness, but also effectively verifying the rationales by looking for the right keywords.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Jio Oh;Soyeon Kim;Junseok Seo;Jindong Wang;Ruochen Xu;Xing Xie;Steven Euijong Whang,True,https://openreview.net/pdf?id=vJaWizbBdA
vXnGXRbOfb,Towards Open Respiratory Acoustic Foundation Models: Pretraining and Benchmarking,"Respiratory audio, such as coughing and breathing sounds, has predictive power for a wide range of healthcare applications, yet is currently under-explored. The main problem for those applications arises from the difficulty in collecting large labeled task-specific data for model development. Generalizable respiratory acoustic foundation models pretrained with unlabeled data would offer appealing advantages and possibly unlock this impasse.  However, given the safety-critical nature of healthcare applications, it is pivotal to also ensure openness and replicability for any proposed foundation model solution. To this end, we introduce OPERA, an OPEn Respiratory Acoustic foundation model pretraining and benchmarking system, as the first approach answering this need. We curate large-scale respiratory audio datasets ($\\\\sim$136K samples, over 400 hours), pretrain three pioneering foundation models, and build a benchmark consisting of 19 downstream respiratory health tasks for evaluation. Our pretrained models demonstrate superior performance (against existing acoustic models pretrained with general audio on 16 out of 19 tasks) and generalizability (to unseen datasets and new respiratory audio modalities). This highlights the great promise of respiratory acoustic foundation models and encourages more studies using OPERA as an open resource to accelerate research on respiratory audio for health. The system is accessible from https://github.com/evelyn0414/OPERA.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yuwei Zhang;Tong Xia;Jing Han;Yu Wu;Georgios Rizos;Yang Liu;Mohammed Mosuily;Jagmohan Chauhan;Cecilia Mascolo,True,https://openreview.net/pdf?id=vXnGXRbOfb
vecFROHnL4,MARVEL: Multidimensional Abstraction and Reasoning through Visual Evaluation and Learning,"While multi-modal large language models (MLLMs) have shown significant progress across popular visual reasoning benchmarks, whether they possess abstract visual reasoning abilities remains an open question. Similar to the Sudoku puzzles, abstract visual reasoning (AVR) problems require finding high-level patterns (e.g., repetition constraints on numbers) that control the input shapes (e.g., digits) in a specific task configuration (e.g., matrix). However, existing AVR benchmarks only consider a limited set of patterns (addition, conjunction), input shapes (rectangle, square), and task configurations (3 × 3 matrices). And they fail to capture all abstract reasoning patterns in human cognition necessary for addressing real-world tasks, such as geometric properties and object boundary understanding in real-world navigation. To evaluate MLLMs’ AVR abilities systematically, we introduce MARVEL founded on the core knowledge system in human cognition, a multi-dimensional AVR benchmark with 770 puzzles composed of six core knowledge patterns, geometric and abstract shapes, and five different task configurations. To inspect whether the model performance is grounded in perception or reasoning, MARVEL complements the standard AVR question with perception questions in a hierarchical evaluation framework. We conduct comprehensive experiments on MARVEL with ten representative MLLMs in zero-shot and few-shot settings. Our experiments reveal that all MLLMs show near-random performance on MARVEL, with significant performance gaps (40%) compared to humans across all patterns and task configurations. Further analysis of perception questions reveals that MLLMs struggle to comprehend the visual features (near-random performance). Although closed-source MLLMs, such as GPT-4V, show a promising understanding of reasoning patterns (on par with humans) after adding textual descriptions, this advantage is hindered by their weak perception abilities. We release our entire
code and dataset at https://github.com/1171-jpg/MARVEL_AVR.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yifan Jiang;Jiarui Zhang;Kexuan Sun;Zhivar Sourati;Kian Ahrabian;Kaixin Ma;Filip Ilievski;Jay Pujara,True,https://openreview.net/pdf?id=vecFROHnL4
vfju5hjrJw,ComBack: A Versatile Dataset for Enhancing Compiler Backend Development Efficiency,"Compiler backends are tasked with generating executable machine code for processors. With the proliferation of diverse processors, it is imperative for programmers to tailor specific compiler backends to accommodate each one. Meanwhile, compiler backend development is a laborious and time-consuming task, lacking effective automation methods. Although language models have demonstrated strong abilities in code related tasks, the lack of appropriate datasets for compiler backend development limits the application of language models in this field.

In this paper, we introduce ComBack, the first public dataset designed for improving compiler backend development capabilities of language models. ComBack includes 178 backends for mainstream compilers and three tasks including statement-level completion, next-statement suggestion and code generation, representing common development scenarios. We conducted experiments by fine-tuning six pre-trained language models with ComBack, demonstrating its effectiveness in enhancing model accuracy across the three tasks. We further evaluated the top-performing model(CodeT5+) across the three tasks for new targets, comparing its accuracy with conventional methods (Fork-Flow), ChatGPT-3.5-Turbo, and Code-LLaMA-34B-Instruct. Remarkably, fine-tuned CodeT5+ with only 220M parameters on ComBack outperformed Fork-Flow methods significantly and surpassed ChatGPT and Code-LLaMA. This suggests potential efficiency improvements in compiler development. ComBack is avaliable at https://huggingface.co/datasets/docz1105/ComBack.",Datasets & Benchmarks,NeurIPS,2024,Poster,Ming Zhong;FANG LYU;Lulin Wang;Hongna Geng;Lei Qiu;Huimin Cui;Xiaobing Feng,True,https://openreview.net/pdf?id=vfju5hjrJw
vlUK2h1Nvw,CALE: Continuous Arcade Learning Environment,"We introduce the Continuous Arcade Learning Environment (CALE), an extension of the well-known Arcade Learning Environment (ALE) [Bellemare et al., 2013]. The CALE uses the same underlying emulator of the Atari 2600 gaming system (Stella), but adds support for continuous actions. This enables the benchmarking and evaluation of continuous-control agents (such as PPO [Schulman et al., 2017] and SAC [Haarnoja et al., 2018]) and value-based agents (such as DQN [Mnih et al., 2015] and Rainbow [Hessel et al., 2018]) on the same environment suite. We provide a series of open questions and research directions that CALE enables, as well as initial baseline results using Soft Actor-Critic. CALE is available as part of the ALE athttps://github.com/Farama-Foundation/Arcade-Learning-Environment.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jesse Farebrother;Pablo Samuel Castro,False,https://openreview.net/pdf?id=vlUK2h1Nvw
vvyUa3CDwt,Intrinsic Self-Supervision for Data Quality Audits,"Benchmark datasets in computer vision often contain off-topic images, near duplicates, and label errors, leading to inaccurate estimates of model performance.
In this paper, we revisit the task of data cleaning and formalize it as either a ranking problem, which significantly reduces human inspection effort, or a scoring problem, which allows for automated decisions based on score distributions.
We find that a specific combination of context-aware self-supervised representation learning and distance-based indicators is effective in finding issues without annotation biases.
This methodology, which we call SelfClean, surpasses state-of-the-art performance in detecting off-topic images, near duplicates, and label errors within widely-used image datasets, such as ImageNet-1k, Food-101N, and STL-10, both for synthetic issues and real contamination.
We apply the detailed method to multiple image benchmarks, identify up to 16% of issues, and confirm an improvement in evaluation reliability upon cleaning.
The official implementation can be found at: https://github.com/Digital-Dermatology/SelfClean.",Datasets & Benchmarks,NeurIPS,2024,Poster,Fabian Gröger;Simone Lionetti;Philippe Gottfrois;Alvaro Gonzalez-Jimenez;Ludovic Amruthalingam;Matthew Groh;Alexander A. Navarini;Marc Pouly,False,https://openreview.net/pdf?id=vvyUa3CDwt
vyraA7xt4c,Mercury: A Code Efficiency Benchmark for Code Large Language Models,"Amidst the recent strides in evaluating Large Language Models for Code (Code LLMs), existing benchmarks have mainly focused on the functional correctness of generated code, neglecting the importance of their computational efficiency. To fill the gap, we present Mercury, the first code efficiency benchmark for Code LLMs. It comprises 1,889 Python tasks, each accompanied by adequate solutions that serve as real-world efficiency baselines, enabling a comprehensive analysis of the runtime distribution. Based on the distribution, we introduce a new metric Beyond, which computes a runtime-percentile-weighted Pass score to reflect functional correctness and code efficiency simultaneously. On Mercury, leading Code LLMs can achieve 65% on Pass, while less than 50% on Beyond. Given that an ideal Beyond score would be aligned with the Pass score, it indicates that while Code LLMs exhibit impressive capabilities in generating functionally correct code, there remains a notable gap in their efficiency. Finally, our empirical experiments reveal that Direct Preference Optimization (DPO) serves as a robust baseline for enhancing code efficiency compared with Supervised Fine Tuning (SFT), which paves a promising avenue for future exploration of efficient code generation. Our code and data are available on GitHub: https://github.com/Elfsong/Mercury.",Datasets & Benchmarks,NeurIPS,2024,Poster,Mingzhe Du;Anh Tuan Luu;Bin Ji;Qian Liu;See-Kiong Ng,True,https://openreview.net/pdf?id=vyraA7xt4c
w4AnTVxAO9,Can Language Models Learn to Skip Steps?,"Trained on vast corpora of human language, language models demonstrate emergent human-like reasoning abilities. Yet they are still far from true intelligence, which opens up intriguing opportunities to explore the parallels of humans and model behaviors. In this work, we study the ability to skip steps in reasoning—a hallmark of human expertise developed through practice. Unlike humans, who may skip steps to enhance efficiency or to reduce cognitive load, models do not inherently possess such motivations to minimize reasoning steps. To address this, we introduce a controlled framework that stimulates step-skipping behavior by iteratively refining models to generate shorter and accurate reasoning paths. Empirical results indicate that models can develop the step skipping ability under our guidance. Moreover, after fine-tuning on expanded datasets that include both complete and skipped reasoning sequences, the models can not only resolve tasks with increased efficiency without sacrificing accuracy, but also exhibit comparable and even enhanced generalization capabilities in out-of-domain scenarios. Our work presents the first exploration into human-like step-skipping ability and provides fresh perspectives on how such cognitive abilities can benefit AI models.",main,NeurIPS,2024,Poster,Tengxiao Liu;Qipeng Guo;Xiangkun Hu;Cheng Jiayang;Yue Zhang;Xipeng Qiu;Zheng Zhang,True,https://openreview.net/pdf?id=w4AnTVxAO9
w50ICQC6QJ,Discovery of the Hidden World with Large Language Models,"Revealing the underlying causal mechanisms in the real world is the key to the development of science. Despite the progress in the past decades, traditional causal discovery approaches (CDs) mainly rely on high-quality measured variables, usually given by human experts, to find causal relations. The lack of well-defined high-level variables in many real-world applications has already been a longstanding roadblock to a broader application of CDs. To this end, this paper presents Causal representatiOn AssistanT (COAT) that introduces large language models (LLMs) to bridge the gap. LLMs are trained on massive observations of the world and have demonstrated great capability in extracting key information from unstructured data. Therefore, it is natural to employ LLMs to assist with proposing useful high-level factors and crafting their measurements. Meanwhile, COAT also adopts CDs to find causal relations among the identified variables as well as to provide feedback to LLMs to iteratively refine the proposed factors. We show that LLMs and CDs are mutually beneficial and the constructed feedback provably also helps with the factor proposal. We construct and curate several synthetic and real-world benchmarks including analysis of human reviews and diagnosis of neuropathic and brain tumors, to comprehensively evaluate COAT. Extensive empirical results confirm the effectiveness and reliability of COAT with significant improvements.",main,NeurIPS,2024,Poster,Chenxi Liu;Yongqiang Chen;Tongliang Liu;Mingming Gong;James Cheng;Bo Han;Kun Zhang,True,https://openreview.net/pdf?id=w50ICQC6QJ
w5jfyvsRq3,"Fit for our purpose, not yours: Benchmark for a low-resource, Indigenous language","Influential and popular benchmarks in AI are largely irrelevant to developing NLP tools for low-resource, Indigenous languages. With the primary goal of measuring the performance of general-purpose AI systems, these benchmarks fail to give due consideration and care to individual language communities, especially low-resource languages. The datasets contain numerous grammatical and orthographic errors, poor pronunciation, limited vocabulary, and the content lacks cultural relevance to the language community. To overcome the issues with these benchmarks, we have created a dataset for te reo Māori (the Indigenous language of Aotearoa/New Zealand) to pursue NLP tools that are ‘fit-for-our-purpose’. This paper demonstrates how low-resourced, Indigenous languages can develop tailored, high-quality benchmarks that; i. Consider the impact of colonisation on their language; ii. Reflect the diversity of speakers in the language community; iii. Support the aspirations for the tools they are developing and their language revitalisation efforts.",Datasets & Benchmarks,NeurIPS,2024,Poster,Suzanne Duncan;Gianna Leoni;Lee Steven;Keoni Mahelona;Peter-Lucas Jones,True,https://openreview.net/pdf?id=w5jfyvsRq3
w90ZH5v34S,Humor in AI: Massive Scale Crowd-Sourced Preferences and Benchmarks for Cartoon Captioning,"We present a novel multimodal preference dataset for creative tasks, consisting of over 250 million human votes on more than 2.2 million captions, collected through crowdsourcing rating data for The New Yorker's weekly cartoon caption contest over the past eight years. This unique dataset supports the development and evaluation of multimodal large language models and preference-based fine-tuning algorithms for humorous caption generation. We propose novel benchmarks for judging the quality of model-generated captions, utilizing both GPT4 and human judgments to establish ranking-based evaluation strategies. Our experimental results highlight the limitations of current fine-tuning methods, such as RLHF and DPO, when applied to creative tasks. Furthermore, we demonstrate that even state-of-the-art models like GPT4 and Claude currently underperform top human contestants in generating humorous captions. As we conclude this extensive data collection effort, we release the entire preference dataset to the research community, fostering further advancements in AI humor generation and evaluation.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Jifan Zhang;Lalit K Jain;Yang Guo;Jiayi Chen;Kuan Lok Zhou;Siddharth Suresh;Andrew Wagenmaker;Scott Sievert;Timothy T. Rogers;Kevin Jamieson;Bob Mankoff;Robert D Nowak,True,https://openreview.net/pdf?id=w90ZH5v34S
wBzvYh3PRA,FactorSim: Generative Simulation via Factorized Representation,"Generating simulations to train intelligent agents in game-playing and robotics from natural language input, user input, or task documentation remains an open-ended challenge. Existing approaches focus on parts of this challenge, such as generating reward functions or task hyperparameters. Unlike previous work, we introduce FACTORSIM that generates full simulations in code from language input that can be used to train agents. Exploiting the structural modularity specific to coded simulations, we propose to use a factored partially observable Markov decision process representation that allows us to reduce context dependence during each step of the generation. For evaluation, we introduce a generative simulation benchmark that assesses the generated simulation code’s accuracy and effectiveness in facilitating zero-shot transfers in reinforcement learning settings. We show that FACTORSIM outperforms existing methods in generating simulations regarding prompt alignment (i.e., accuracy), zero-shot transfer abilities, and human evaluation. We also demonstrate its effectiveness in generating robotic tasks.",main,NeurIPS,2024,Poster,Fan-Yun Sun;Harini S I;Angela Yi;Yihan Zhou;Alex Zook;Jonathan Tremblay;Logan Cross;Jiajun Wu;Nick Haber,True,https://openreview.net/pdf?id=wBzvYh3PRA
wH36UKML4x,Trained Models Tell Us How to Make Them Robust to Spurious Correlation without Group Annotation,"Classifiers trained with Empirical Risk Minimization (ERM) tend to rely on attributes that have high spurious correlation with the target. This can degrade the performance on underrepresented (or 'minority') groups that lack these attributes, posing significant challenges for both out-of-distribution generalization and fairness objectives. Many studies aim to improve robustness to spurious correlation, yet nearly all require group annotation for training and/or model selection. This constrains their applicability in situations where the nature of the spurious correlation is not known, or when group labels for certain spurious attributes are either insufficient or completely absent. To meet the demand for effectively enhancing the model robustness under minimal assumptions about group annotation, we propose Environment-based Validation and Loss-based Sampling (EVaLS). It uses the losses from a trained model to construct a balanced dataset of high-loss and low-loss samples in which the training data group imbalance is mitigated. This results in a significant robustness to group shifts when equipped with a simple mechanism of last layer retraining. Furthermore, by utilizing environment inference methods for creating diverse environments with correlation shifts, EVaLS can potentially eliminate the need for group annotation in the validation data. In such a context, the worst environment accuracy acts as a reliable surrogate throughout the retraining process for tuning hyperparameters and finding a model that performs well across diverse group shifts. EVaLS effectively achieves group robustness, showing that group annotation is not necessary even for validation. It is a fast, straightforward, and effective approach that reaches near-optimal worst group accuracy without needing group annotations, marking a new chapter in the robustness of trained models against spurious correlation.",main,NeurIPS,2024,Reject,Mahdi Ghaznavi;Hesam Asadollahzadeh;Fahimeh Hosseini Noohdani;Soroush Vafaie Tabar;Hosein Hasani;Taha Akbari Alvanagh;Mohammad Hossein Rohban;Mahdieh Soleymani Baghshah,True,https://openreview.net/pdf?id=wH36UKML4x
wOmtZ5FgMH,RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models,"Large language models (LLMs) inevitably memorize sensitive, copyrighted, and harmful knowledge from the training corpus; therefore, it is crucial to erase this knowledge from the models. Machine unlearning is a promising solution for efficiently removing specific knowledge by post hoc modifying models. In this paper, we propose a Real-World Knowledge Unlearning benchmark (RWKU) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the task setting, we consider a more practical and challenging unlearning setting, where neither the forget corpus nor the retain corpus is accessible. (2) For the knowledge source, we choose 200 real-world famous people as the unlearning targets and show that such popular knowledge is widely present in various LLMs. (3) For the evaluation framework, we design the forget set and the retain set to evaluate the model’s capabilities across various real-world applications. Regarding the forget set, we provide four four membership inference attack (MIA) methods and nine kinds of adversarial attack probes to rigorously test unlearning efficacy. Regarding the retain set, we assess locality and utility in terms of neighbor perturbation, general ability, reasoning ability, truthfulness, factuality, and fluency. We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods and obtain some meaningful findings. We release our benchmark and code publicly at http://rwku-bench.github.io for future work.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zhuoran Jin;Pengfei Cao;Chenhao Wang;Zhitao He;Hongbang Yuan;Jiachun Li;Yubo Chen;Kang Liu;Jun Zhao,True,https://openreview.net/pdf?id=wOmtZ5FgMH
wSCfRAAr69,Language Without Borders: A Dataset and Benchmark for Code-Switching Lip Reading,"Lip reading aims at transforming the videos of continuous lip movement into textual contents, and has achieved significant progress over the past decade. It serves as a critical yet practical assistance for speech-impaired individuals, with more practicability than speech recognition in noisy environments. With the increasing interpersonal communications in social media owing to globalization, the existing monolingual datasets for lip reading may not be sufficient to meet the exponential proliferation of bilingual and even multilingual users. However, to our best knowledge, research on code-switching is only explored in speech recognition, while the attempts in lip reading are seriously neglected. To bridge this gap, we have collected a bilingual code-switching lip reading benchmark composed of Chinese and English, dubbed CSLR. As the pioneering work, we recruited 62 speakers with proficient foundations in both
spoken Chinese and English to express sentences containing both involved languages. Through rigorous criteria in data selection, CSLR benchmark has accumulated 85,560 video samples with a resolution of 1080x1920, totaling over 71.3 hours of high-quality code-switching lip movement data. To systematically evaluate the technical challenges in CSLR, we implement commonly-used lip reading backbones, as well as competitive solutions in code-switching speech for benchmark testing. Experiments show CSLR to be a challenging and under-explored lip reading task. We hope our proposed benchmark will extend the applicability of code-switching lip reading, and further contribute to the communities of cross-lingual communication and collaboration. Our dataset and benchmark are accessible at https://github.com/cslr-lipreading/CSLR.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xueyi Zhang;Chengwei Zhang;Mingrui Lao;Peng Zhao;Jun Tang;Yanming Guo;Siqi Cai;Xianghu Yue;Haizhou Li,True,https://openreview.net/pdf?id=wSCfRAAr69
wT5AgMVkaJ,Aligning Vision Models with Human Aesthetics in Retrieval: Benchmarks and Algorithms,"Modern vision models are trained on very large noisy datasets. While these models acquire strong capabilities, they may not follow the user's intent to output the desired results in certain aspects, e.g., visual aesthetic, preferred style, and responsibility. In this paper, we target the realm of visual aesthetics and aim to align vision models with human aesthetic standards in a retrieval system. Advanced retrieval systems usually adopt a cascade of aesthetic models as re-rankers or filters, which are limited to low-level features like saturation and perform poorly when stylistic, cultural or knowledge contexts are involved. We find that utilizing the reasoning ability of large language models (LLMs) to rephrase the search query and extend the aesthetic expectations can make up for this shortcoming. Based on the above findings, we propose a preference-based reinforcement learning method that fine-tunes the vision models to distill the knowledge from both LLMs reasoning and the aesthetic models to better align the vision models with human aesthetics. Meanwhile, with rare benchmarks designed for evaluating retrieval systems, we leverage large multi-modality model (LMM) to evaluate the aesthetic performance with their strong abilities. As aesthetic assessment is one of the most subjective tasks, to validate the robustness of LMM, we further propose a novel dataset named HPIR to benchmark the alignment with human aesthetics.  Experiments demonstrate that our method significantly enhances the aesthetic behaviors of the vision models, under several metrics. We believe the proposed algorithm can be a general practice for aligning vision models with human values.",main,NeurIPS,2024,Poster,Miaosen Zhang;Yuxing Wei;Zhen Xing;Yifei Ma;Zuxuan Wu;Ji Li;Zheng Zhang;Qi Dai;Chong Luo;Xin Geng;Baining Guo,True,https://openreview.net/pdf?id=wT5AgMVkaJ
wWyumwEYV8,A Sober Look at the Robustness of CLIPs to Spurious Features,"Large vision language models, such as CLIP, demonstrate impressive robustness to spurious features than single-modal models trained on ImageNet. However, existing test datasets are typically curated based on ImageNet-trained models, which aim to capture the spurious features inherited in ImageNet. Benchmarking CLIP models based on the ImageNet-oriented spurious features may not be sufficient to reflect the extent to which CLIP models are robust to spurious correlations within CLIP training data, e.g., LAION. To this end, we craft a new challenging dataset named CounterAnimal designed to reveal the reliance of CLIP models on realistic spurious features. Specifically, we split animal photos into groups according to the backgrounds, and then identify a pair of groups for each class where a CLIP model shows high-performance drops across the two groups. Our evaluations show that the spurious features captured by CounterAnimal are generically learned by CLIP models with different backbones and pre-train data, yet have limited influence for ImageNet models. We provide theoretical insights that the CLIP objective cannot offer additional robustness. Furthermore, we also re-evaluate strategies such as scaling up parameters and high-quality pre-trained data. We find that they still help mitigate the spurious features, providing a promising path for future developments.",main,NeurIPS,2024,Poster,Qizhou Wang;Yong Lin;Yongqiang Chen;Ludwig Schmidt;Bo Han;Tong Zhang,True,https://openreview.net/pdf?id=wWyumwEYV8
wjHVmgBDzc,$\\\\texttt{ConflictBank}$: A Benchmark for Evaluating the Influence of Knowledge Conflicts in LLMs,"Large language models (LLMs) have achieved
impressive advancements across numerous disciplines, yet the critical issue of knowledge conflicts, a major source of hallucinations, has rarely been studied. While a few research explored the conflicts between the inherent knowledge of LLMs and the retrieved contextual knowledge, a comprehensive assessment of knowledge conflict in LLMs is still missing. Motivated by this research gap, we firstly propose ConflictBank, the largest benchmark with 7.45M claim-evidence pairs and 553k QA pairs, addressing conflicts from misinformation, temporal discrepancies, and semantic divergences.
Using ConflictBank, we conduct the thorough and controlled experiments for a comprehensive understanding of LLM behavior in knowledge conflicts, focusing on three key aspects: (i) conflicts encountered in retrieved knowledge, (ii) conflicts within the models' encoded knowledge, and (iii) the interplay between these conflict forms.
Our investigation delves into four model families and twelve LLM instances and provides insights into conflict types, model sizes, and the impact at different stages.
We believe that knowledge conflicts represent a critical bottleneck to achieving trustworthy artificial intelligence and hope our work will offer valuable guidance for future model training and development.
Resources are available at https://github.com/zhaochen0110/conflictbank.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zhaochen Su;Jun Zhang;Xiaoye Qu;Tong Zhu;Yanshu Li;Jiashuo Sun;Juntao Li;Min Zhang;Yu Cheng,True,https://openreview.net/pdf?id=wjHVmgBDzc
wmO7z57wNK,LLMCBench: Benchmarking Large Language Model Compression for Efficient Deployment,"Although large language models (LLMs) have demonstrated their strong intelligence ability, the high demand for computation and storage hinders their practical application. To this end, many model compression techniques are proposed to increase the efficiency of LLMs. However, current researches only validate their methods on limited models, datasets, metrics, etc, and still lack a comprehensive evaluation under more general scenarios. So it is still a question of which model compression approach we should use under a specific case. To mitigate this gap, we present the Large Language Model Compression Benchmark (LLMCBench), a rigorously designed benchmark with an in-depth analysis for LLM compression algorithms. We first analyze the actual model production requirements and carefully design evaluation tracks and metrics. Then, we conduct extensive experiments and comparison using multiple mainstream LLM compression approaches. Finally, we perform an in-depth analysis based on the evaluation and provide useful insight for LLM compression design. We hope our LLMCBench can contribute insightful suggestions for LLM compression algorithm design and serve as a foundation for future research.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Ge Yang;Changyi He;Jinyang Guo;Jianyu Wu;Yifu Ding;Aishan Liu;Haotong Qin;Pengliang Ji;Xianglong Liu,True,https://openreview.net/pdf?id=wmO7z57wNK
woENr7FJaI,Automated Multi-level Preference for MLLMs,"Current multimodal Large Language Models (MLLMs) suffer from ''hallucination'', occasionally generating responses that are not grounded in the input images. To tackle this challenge, one promising path is to utilize reinforcement learning from human feedback (RLHF), which steers MLLMs towards learning superior responses while avoiding inferior ones. We rethink the common practice of using binary preferences (*i.e.*, superior, inferior), and find that adopting multi-level preferences (*e.g.*, superior, medium, inferior) is better for two benefits: 1) It narrows the gap between adjacent levels, thereby encouraging MLLMs to discern subtle differences. 2) It further integrates cross-level comparisons (beyond adjacent-level comparisons), thus providing a broader range of comparisons with hallucination examples. To verify our viewpoint, we present the Automated Multi-level Preference (**AMP**) framework for MLLMs. To facilitate this framework, we first develop an automated dataset generation pipeline that provides high-quality multi-level preference datasets without any human annotators. Furthermore, we design the Multi-level Direct Preference Optimization (MDPO) algorithm to robustly conduct complex multi-level preference learning. Additionally, we propose a new hallucination benchmark, MRHal-Bench. Extensive experiments across public hallucination and general benchmarks, as well as our MRHal-Bench, demonstrate the effectiveness of our proposed method. Code is available at https://github.com/takomc/amp.",main,NeurIPS,2024,Poster,Mengxi Zhang;Wenhao Wu;Yu Lu;YuXin Song;KANG RONG;Huanjin Yao;Jianbo Zhao;Fanglong Liu;Haocheng Feng;Jingdong Wang;Yifan Sun,True,https://openreview.net/pdf?id=woENr7FJaI
wqo6xEMyk9,ProG: A Graph Prompt Learning Benchmark,"Artificial general intelligence on graphs has shown significant advancements across various applications, yet the traditional `Pre-train \\\\& Fine-tune' paradigm faces inefficiencies and negative transfer issues, particularly in complex and few-shot settings. Graph prompt learning emerges as a promising alternative, leveraging lightweight prompts to manipulate data and fill the task gap by reformulating downstream tasks to the pretext. However, several critical challenges still remain: how to unify diverse graph prompt models, how to evaluate the quality of graph prompts, and to improve their usability for practical comparisons and selection. In response to these challenges, we introduce the first comprehensive benchmark for graph prompt learning. Our benchmark integrates **SIX** pre-training methods and **FIVE** state-of-the-art graph prompt techniques, evaluated across **FIFTEEN** diverse datasets to assess performance, flexibility, and efficiency. We also present 'ProG', an easy-to-use open-source library that streamlines the execution of various graph prompt models, facilitating objective evaluations. Additionally, we propose a unified framework that categorizes existing graph prompt methods into two main approaches: prompts as graphs and prompts as tokens. This framework enhances the applicability and comparison of graph prompt techniques. The code is available at: https://github.com/sheldonresearch/ProG.",Datasets & Benchmarks,NeurIPS,2024,Poster,Chenyi Zi;Haihong Zhao;Xiangguo Sun;Yiqing Lin;Hong Cheng;Jia Li,False,https://openreview.net/pdf?id=wqo6xEMyk9
x2780VcMOI,A Polar coordinate system represents syntax in large language models,"Originally formalized with symbolic representations, syntactic trees may also be effectively represented in the activations of large language models (LLMs). Indeed, a ''Structural Probe'' can find a subspace of neural activations, where syntactically-related words are relatively close to one-another. However, this syntactic code remains incomplete: the distance between the Structural Probe word embeddings can represent the \\\\emph{existence} but not the type and direction of syntactic relations. Here, we hypothesize that syntactic relations are, in fact, coded by the relative direction between nearby embeddings. To test this hypothesis, we introduce a ''Polar Probe'' trained to read syntactic relations from both the distance and the direction between word embeddings. Our approach reveals three main findings. First, our Polar Probe successfully recovers the type and direction of syntactic relations, and substantially outperforms the Structural Probe by nearly two folds. Second, we confirm that this polar coordinate system exists in a low-dimensional subspace of the intermediate layers of many LLMs and becomes increasingly precise in the latest frontier models. Third, we demonstrate with a new benchmark that similar syntactic relations are coded similarly across the nested levels of syntactic trees. Overall, this work shows that LLMs spontaneously learn a geometry of neural activations that explicitly represents the main symbolic structures of linguistic theory.",main,NeurIPS,2024,Poster,Pablo J. Diego Simon;Stéphane d'Ascoli;Emmanuel Chemla;Yair Lakretz;Jean-Remi King,True,https://openreview.net/pdf?id=x2780VcMOI
x8RgF2xQTj,Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks,"Uncertainty quantification, once a singular task, has evolved into a spectrum of tasks, including abstained prediction, out-of-distribution detection, and aleatoric uncertainty quantification. The latest goal is disentanglement: the construction of multiple estimators that are each tailored to one and only one source of uncertainty. This paper presents the first benchmark of uncertainty disentanglement. We reimplement and evaluate a comprehensive range of uncertainty estimators, from Bayesian over evidential to deterministic ones, across a diverse range of uncertainty tasks on ImageNet. We find that, despite recent theoretical endeavors, no existing approach provides pairs of disentangled uncertainty estimators in practice. We further find that specialized uncertainty tasks are harder than predictive uncertainty tasks, where we observe saturating performance. Our results provide both practical advice for which uncertainty estimators to use for which specific task, and reveal opportunities for future research toward task-centric and disentangled uncertainties. All our reimplementations and Weights & Biases logs are available at https://github.com/bmucsanyi/untangle.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Bálint Mucsányi;Michael Kirchhof;Seong Joon Oh,False,https://openreview.net/pdf?id=x8RgF2xQTj
xOCAURlVM9,Assembly Fuzzy Representation on Hypergraph for Open-Set 3D Object Retrieval,"The lack of object-level labels presents a significant challenge for 3D object retrieval in the open-set environment. However, part-level shapes of objects often share commonalities across categories but remain underexploited in existing retrieval methods. In this paper, we introduce the Hypergraph-Based Assembly Fuzzy Representation (HARF) framework, which navigates the intricacies of open-set 3D object retrieval through a bottom-up lens of Part Assembly. To tackle the challenge of assembly isomorphism and unification, we propose the Hypergraph Isomorphism Convolution (HIConv) for smoothing and adopt the Isomorphic Assembly Embedding (IAE) module to generate assembly embeddings with geometric-semantic consistency. To address the challenge of open-set category generalization, our method employs high-order correlations and fuzzy representation to mitigate distribution skew through the Structure Fuzzy Reconstruction (SFR) module, by constructing a leveraged hypergraph based on local certainty and global uncertainty correlations. We construct three open-set retrieval datasets for 3D objects with part-level annotations: OP-SHNP, OP-INTRA, and OP-COSEG. Extensive experiments and ablation studies on these three benchmarks show our method outperforms current state-of-the-art methods.",main,NeurIPS,2024,Poster,Yang Xu;Yifan Feng;Jun Zhang;Jun-Hai Yong;Yue Gao,True,https://openreview.net/pdf?id=xOCAURlVM9
xT5pmUju8W,A Benchmark Suite for Evaluating Neural Mutual Information Estimators on Unstructured Datasets,"Mutual Information (MI) is a fundamental metric for quantifying dependency between two random variables. When we can access only the samples, but not the underlying distribution functions, we can evaluate MI using sample-based estimators. Assessment of such MI estimators, however, has almost always relied on analytical datasets including Gaussian multivariates. Such datasets allow analytical calculations of the true MI values, but they are limited in that they do not reflect the complexities of real-world datasets. This study introduces a comprehensive benchmark suite for evaluating neural MI estimators on unstructured datasets, specifically focusing on images and texts. By leveraging same-class sampling for positive pairing and introducing a binary symmetric channel trick, we show that we can accurately manipulate true MI values of real-world datasets. Using the benchmark suite, we investigate seven challenging scenarios, shedding light on the reliability of neural MI estimators for unstructured datasets.",Datasets & Benchmarks,NeurIPS,2024,Poster,Kyungeun Lee;Wonjong Rhee,True,https://openreview.net/pdf?id=xT5pmUju8W
xXRnUU7xTL,SelfCodeAlign: Self-Alignment for Code Generation,"Instruction tuning is a supervised fine-tuning approach that significantly improves the ability of large language models (LLMs) to follow human instructions. For programming tasks, most models are finetuned with costly human-annotated instruction-response pairs or those generated by large, proprietary LLMs, which may not be permitted. We propose SelfCodeAlign, the first fully transparent and permissive pipeline for self-aligning code LLMs without extensive human annotations or distillation. SelfCodeAlign employs the same base model for inference throughout the data generation process. It first extracts diverse coding concepts from high-quality seed snippets to generate new tasks. It then samples multiple responses per task, pairs each with test cases, and validates them in a sandbox environment. Finally, passing examples are selected for instruction tuning. In our primary experiments, we use SelfCodeAlign with CodeQwen1.5-7B to generate a dataset of 74k instruction-response pairs. Finetuning on this dataset leads to a model that achieves a 67.1 pass@1 on HumanEval+, surpassing CodeLlama-70B-Instruct despite being ten times smaller. Across all benchmarks, this finetuned model consistently outperforms the original version trained with OctoPack, the previous state-of-the-art method for instruction tuning without human annotations or distillation. Additionally, we show that SelfCodeAlign is effective across LLMs of various sizes, from 3B to 33B, and that the base models can benefit more from alignment with their own data distribution. We further validate each component’s effectiveness in our pipeline, showing that SelfCodeAlign outperforms both direct distillation from GPT-4o and leading GPT-3.5-based distillation methods, such as OSS-Instruct and Evol-Instruct. SelfCodeAlign has also led to the creation of StarCoder2-Instruct, the first fully transparent, permissively licensed, and self-aligned code LLM that achieves state-of-the-art coding performance. Overall, SelfCodeAlign shows for the first time that a strong instruction-tuned code LLM can result from self-alignment rather than distillation.",main,NeurIPS,2024,Poster,Yuxiang Wei;Federico Cassano;Jiawei Liu;Yifeng Ding;Naman Jain;Zachary Mueller;Harm de Vries;Leandro Von Werra;Arjun Guha;LINGMING ZHANG,True,https://openreview.net/pdf?id=xXRnUU7xTL
xepxnDQoGq,ReactZyme: A Benchmark for Enzyme-Reaction Prediction,"Enzymes, with their specific catalyzed reactions, are necessary for all aspects of life, enabling diverse biological processes and adaptations. Predicting enzyme functions is essential for understanding biological pathways, guiding drug development, enhancing bioproduct yields, and facilitating evolutionary studies.
Addressing the inherent complexities, we introduce a new approach to annotating enzymes based on their catalyzed reactions. This method provides detailed insights into specific reactions and is adaptable to newly discovered reactions, diverging from traditional classifications by protein family or expert-derived reaction classes. We employ machine learning algorithms to analyze enzyme reaction datasets, delivering a much more refined view on the functionality of enzymes.
Our evaluation leverages the largest enzyme-reaction dataset to date, derived from the SwissProt and Rhea databases with entries up to January 8, 2024. 
We frame the enzyme-reaction prediction as a retrieval problem, aiming to rank enzymes by their catalytic ability for specific reactions. With our model, we can recruit proteins for novel reactions and predict reactions in novel proteins, facilitating enzyme discovery and function annotation https://github.com/WillHua127/ReactZyme.",Datasets & Benchmarks,NeurIPS,2024,Poster,Chenqing Hua;Bozitao Zhong;Sitao Luan;Liang Hong;Guy Wolf;Doina Precup;Shuangjia Zheng,False,https://openreview.net/pdf?id=xepxnDQoGq
xjXYgdFM5M,Reasons and Solutions for the Decline in Model Performance after Editing,"Knowledge editing technology has received widespread attention for low-cost updates of incorrect or outdated knowledge in large-scale language models. However, recent research has found that edited models often exhibit varying degrees of performance degradation. The reasons behind this phenomenon and potential solutions have not yet been provided. In order to investigate the reasons for the performance decline of the edited model and optimize the editing method, this work explores the underlying reasons from both data and model perspectives. Specifically, 1) from a data perspective, to clarify the impact of data on the performance of editing models, this paper first constructs a **M**ulti-**Q**uestion **D**ataset (**MQD**) to evaluate the impact of different types of editing data on model performance. The performance of the editing model is mainly affected by the diversity of editing targets and sequence length, as determined through experiments. 2) From a model perspective, this article explores the factors that affect the performance of editing models. The results indicate a strong correlation between the L1-norm of the editing model layer and the editing accuracy, and clarify that this is an important factor leading to the bottleneck of editing performance. Finally, in order to improve the performance of the editing model, this paper further proposes a **D**ump **for** **S**equence (**D4S**) method, which successfully overcomes the previous editing bottleneck by reducing the L1-norm of the editing layer, allowing users to perform multiple effective edits and minimizing model damage. Our code is available at https://github.com/nlpkeg/D4S.",main,NeurIPS,2024,Poster,Xiusheng Huang;Jiaxiang Liu;Yequan Wang;Kang Liu,True,https://openreview.net/pdf?id=xjXYgdFM5M
xjxqWYyTfR,Unraveling Molecular Structure: A Multimodal Spectroscopic Dataset for Chemistry,"Spectroscopic techniques are essential tools for determining the structure of molecules. Different spectroscopic techniques, such as Nuclear magnetic resonance (NMR), Infrared spectroscopy, and Mass Spectrometry, provide insight into the molecular structure, including the presence or absence of functional groups. Chemists leverage the complementary nature of the different methods to their advantage. However, the lack of a comprehensive multimodal dataset, containing spectra from a variety of spectroscopic techniques, has limited machine-learning approaches mostly to single-modality tasks for predicting molecular structures from spectra. 
Here we introduce a dataset comprising simulated $^1$H-NMR, $^{13}$C-NMR, HSQC-NMR, Infrared, and Mass spectra (positive and negative ion modes) for 790k molecules extracted from chemical reactions in patent data. This dataset enables the development of foundation models for integrating information from multiple spectroscopic modalities, emulating the approach employed by human experts. Additionally, we provide benchmarks for evaluating single-modality tasks such as structure elucidation, predicting the spectra for a target molecule, and functional group predictions. 
This dataset has the potential automate structure elucidation, streamlining the molecular discovery pipeline from synthesis to structure determination. 
The dataset and code for the benchmarks can be found at https://rxn4chemistry.github.io/multimodal-spectroscopic-dataset (Available upon submission of the supporting information).",Datasets & Benchmarks,NeurIPS,2024,Poster,Marvin Alberts;Oliver Schilter;Federico Zipoli;Nina Hartrampf;Teodoro Laino,True,https://openreview.net/pdf?id=xjxqWYyTfR
xkljKdGe4E,Classic GNNs are Strong Baselines: Reassessing GNNs for Node Classification,"Graph Transformers (GTs) have recently emerged as popular alternatives to traditional message-passing Graph Neural Networks (GNNs), due to their theoretically superior expressiveness and impressive performance reported on standard node classification benchmarks, often significantly outperforming GNNs. In this paper, we conduct a thorough empirical analysis to reevaluate the performance of three classic GNN models (GCN, GAT, and GraphSAGE) against GTs. Our findings suggest that the previously reported superiority of GTs may have been overstated due to suboptimal hyperparameter configurations in GNNs. Remarkably, with slight hyperparameter tuning, these classic GNN models achieve state-of-the-art performance, matching or even exceeding that of recent GTs across 17 out of the 18 diverse datasets examined. Additionally, we conduct detailed ablation studies to investigate the influence of various GNN configurations—such as normalization, dropout, residual connections, and network depth—on node classification performance. Our study aims to promote a higher standard of empirical rigor in the field of graph machine learning, encouraging more accurate comparisons and evaluations of model capabilities. Our implementation is available at https://github.com/LUOyk1999/tunedGNN.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yuankai Luo;Lei Shi;Xiao-Ming Wu,False,https://openreview.net/pdf?id=xkljKdGe4E
xlKeMuyoZ5,Large Language Models' Expert-level Global History Knowledge Benchmark (HiST-LLM),"Large Language Models (LLMs) have the potential to transform humanities and social science research, yet their history knowledge and comprehension at a graduate level remains untested. Benchmarking LLMs in history is particularly challenging, given that human knowledge of history is inherently unbalanced, with more information available on Western history and recent periods. We introduce the History Seshat Test for LLMs (HiST-LLM), based on a subset of the Seshat Global History Databank, which provides a structured representation of human historical knowledge, containing 36,000 data points across 600 historical societies and over 2,700 scholarly references. This dataset covers every major world region from the Neolithic period to the Industrial Revolution and includes information reviewed and assembled by history experts and graduate research assistants. Using this dataset, we benchmark a total of seven models from the Gemini, OpenAI, and Llama families. We find that, in a four-choice format, LLMs have a balanced accuracy ranging from 33.6% (Llama-3.1-8B) to 46% (GPT-4-Turbo), outperforming random guessing (25%) but falling short of expert comprehension. 
LLMs perform better on earlier historical periods. Regionally, performance is more even but still better for the Americas and lowest in Oceania and Sub-Saharan Africa for the more advanced models. Our benchmark shows that while LLMs possess some expert-level historical knowledge, there is considerable room for improvement.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jakob Hauser;Dániel Kondor;Jenny Reddish;Majid Benam;Enrico Cioni;Federica Villa;James S Bennett;Daniel Hoyer;Pieter Francois;Peter Turchin;R. Maria del Rio-Chanona,True,https://openreview.net/pdf?id=xlKeMuyoZ5
xotfLEAF4u,MLLM-CompBench: A Comparative Reasoning Benchmark for Multimodal LLMs,"The ability to compare objects, scenes, or situations is crucial for effective decision-making and problem-solving in everyday life. For instance, comparing the freshness of apples enables better choices during grocery shopping, while comparing sofa designs helps optimize the aesthetics of our living space. Despite its significance, the comparative capability is largely unexplored in artificial general intelligence (AGI). In this paper, we introduce MLLM-CompBench, a benchmark designed to evaluate the comparative reasoning capability of multimodal large language models (MLLMs). MLLM-CompBench mines and pairs images through visually oriented questions covering eight dimensions of relative comparison: visual attribute, existence, state, emotion, temporality, spatiality, quantity, and quality. We curate a collection of around 40K image pairs using metadata from diverse vision datasets and CLIP similarity scores. These image pairs span a broad array of visual domains, including animals, fashion, sports, and both outdoor and indoor scenes. The questions are carefully crafted to discern relative characteristics between two images and are labeled by human annotators for accuracy and relevance. We use MLLM-CompBench to evaluate recent MLLMs, including GPT-4V(ision), Gemini-Pro, and LLaVA-1.6. Our results reveal notable shortcomings in their comparative abilities. We believe MLLM-CompBench not only sheds light on these limitations but also establishes a solid foundation for future enhancements in the comparative capability of MLLMs.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jihyung Kil;Zheda Mai;Justin Lee;Arpita Chowdhury;Zihe Wang;Kerrie Cheng;Lemeng Wang;Ye Liu;Wei-Lun Chao,True,https://openreview.net/pdf?id=xotfLEAF4u
xqpkzMfmQ5,Data curation via joint example selection further accelerates multimodal learning,"Data curation is an essential component of large-scale pretraining. In this work, we demonstrate that jointly prioritizing batches of data is more effective for learning than selecting examples independently. Multimodal contrastive objectives expose the dependencies between data and thus naturally yield criteria for measuring the joint learnability of a batch. We derive a simple and tractable algorithm for selecting such batches, which significantly accelerate training beyond individually-prioritized data points. As performance improves by selecting from large super-batches, we  also leverage recent advances in model approximation to reduce the computational overhead of scoring. As a result, our approach—multimodal contrastive learning with joint example selection (JEST)—surpasses state-of-the-art pretraining methods with up to 13× fewer iterations and 10× less computation. Essential to the performance of JEST is the ability to steer the data selection process towards the distribution of smaller, well-curated datasets via pretrained reference models, exposing data curation as a new dimension for neural scaling laws.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Talfan Evans;Nikhil Parthasarathy;Hamza Merzic;Olivier J Henaff,False,https://openreview.net/pdf?id=xqpkzMfmQ5
xvTMc9Ovx3,On-Road Object Importance Estimation: A New Dataset and A Model with Multi-Fold Top-Down Guidance,"This paper addresses the problem of on-road object importance estimation, which utilizes video sequences captured from the driver's perspective as the input. Although this problem is significant for safer and smarter driving systems, the exploration of this problem remains limited. On one hand, publicly-available large-scale datasets are scarce in the community. To address this dilemma, this paper contributes a new large-scale dataset named Traffic Object Importance (TOI). On the other hand, existing methods often only consider either bottom-up feature or single-fold guidance, leading to limitations in handling highly dynamic and diverse traffic scenarios. Different from existing methods, this paper proposes a model that integrates multi-fold top-down guidance with the bottom-up feature. Specifically, three kinds of top-down guidance factors (i.e., driver intention, semantic context, and traffic rule) are integrated into our model. These factors are important for object importance estimation, but none of the existing methods simultaneously consider them. To our knowledge, this paper proposes the first on-road object importance estimation model that fuses multi-fold top-down guidance factors with bottom-up feature. Extensive experiments demonstrate that our model outperforms state-of-the-art methods by large margins, achieving 23.1% Average Precision (AP) improvement compared with the recently proposed model (i.e., Goal).",main,NeurIPS,2024,Poster,Zhixiong Nan;Yilong Chen;Tianfei Zhou;Tao Xiang,True,https://openreview.net/pdf?id=xvTMc9Ovx3
xvVeSZoVJO,RCDN: Towards Robust Camera-Insensitivity Collaborative Perception via Dynamic Feature-based 3D Neural Modeling,"Collaborative perception is dedicated to tackling the constraints of single-agent perception, such as occlusions, based on the multiple agents' multi-view sensor inputs. However, most existing works assume an ideal condition that all agents' multi-view cameras are continuously available. In reality, cameras may be highly noisy, obscured or even failed during the collaboration. In this work, we introduce a new robust camera-insensitivity problem: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? To address above problems, we propose RCDN, a Robust Camera-insensitivity collaborative perception with a novel Dynamic feature-based 3D Neural modeling mechanism. The key intuition of RCDN is to construct collaborative neural rendering field representations to recover failed perceptual messages sent by multiple agents. To better model collaborative neural rendering field, RCDN first establishes a geometry BEV feature based time-invariant static field with other agents via fast hash grid modeling. Based on the static background field, the proposed time-varying dynamic field can model corresponding motion vector for foregrounds with appropriate positions. To validate RCDN, we create OPV2V-N, a new large-scale dataset with manual labelling under different camera failed scenarios. Extensive experiments conducted on OPV2V-N show that RCDN can be ported to other baselines and improve their robustness in extreme camera-insensitivity setting. Our code and datasets will be available soon.",main,NeurIPS,2024,Poster,Tianhang Wang;Fan Lu;Zehan Zheng;Zhijun Li;Guang Chen;changjun jiang,True,https://openreview.net/pdf?id=xvVeSZoVJO
y09S5rdaWY,Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving,"In an era marked by the rapid scaling of foundation models, autonomous driving technologies are approaching a transformative threshold where end-to-end autonomous driving (E2E-AD) emerges due to its potential of scaling up in the data-driven manner. However, existing E2E-AD methods are mostly evaluated under the open-loop log-replay manner with L2 errors and collision rate as metrics (e.g., in nuScenes), which could not fully reflect the driving performance of algorithms as recently acknowledged in the community. For those E2E-AD methods evaluated under the closed-loop protocol, they are tested in fixed routes (e.g., Town05Long and Longest6 in CARLA) with the driving score as metrics, which is known for high variance due to the unsmoothed metric function and large randomness in the long route. Besides, these methods usually collect their own data for training, which makes algorithm-level fair comparison infeasible. 

   To fulfill the paramount need of comprehensive, realistic, and fair testing environments for Full Self-Driving (FSD), we present Bench2Drive, the first benchmark for evaluating E2E-AD systems' multiple abilities in a closed-loop manner. Bench2Drive's official training data consists of 2 million fully annotated frames, collected from 10000 short clips uniformly distributed under 44 interactive scenarios (cut-in, overtaking, detour, etc), 23 weathers (sunny, foggy, rainy, etc), and 12 towns (urban, village, university, etc) in CARLA v2. Its evaluation protocol requires E2E-AD models to pass 44 interactive scenarios under different locations and weathers which sums up to 220 routes and thus provides a comprehensive and disentangled assessment about their driving capability under different situations. We implement state-of-the-art E2E-AD models and evaluate them in Bench2Drive, providing insights regarding current status and future directions.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xiaosong Jia;Zhenjie Yang;Qifeng Li;Zhiyuan Zhang;Junchi Yan,True,https://openreview.net/pdf?id=y09S5rdaWY
y10DM6R2r3,MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark,"In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates part of the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16\\\\% to 33\\\\% compared to MMLU, but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5\\\\% in MMLU to just 2\\\\% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is more discriminative benchmark to better track progress in the field.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Yubo Wang;Xueguang Ma;Ge Zhang;Yuansheng Ni;Abhranil Chandra;Shiguang Guo;Weiming Ren;Aaran Arulraj;Xuan He;Ziyan Jiang;Tianle Li;Max Ku;Kai Wang;Alex Zhuang;Rongqi Fan;Xiang Yue;Wenhu Chen,True,https://openreview.net/pdf?id=y10DM6R2r3
yMS7ansbr6,Lips Are Lying: Spotting the Temporal Inconsistency between Audio and Visual in Lip-Syncing DeepFakes,"In recent years, DeepFake technology has achieved unprecedented success in high-quality video synthesis, but these methods also pose potential and severe security threats to humanity. DeepFake can be bifurcated into entertainment applications like face swapping and illicit uses such as lip-syncing fraud. However, lip-forgery videos, which neither change identity nor have discernible visual artifacts, present a formidable challenge to existing DeepFake detection methods. Our preliminary experiments have shown that the effectiveness of the existing methods often drastically decrease or even fail when tackling lip-syncing videos.
In this paper, for the first time, we propose a novel approach dedicated to lip-forgery identification that exploits the inconsistency between lip movements and audio signals. We also mimic human natural cognition by capturing subtle biological links between lips and head regions to boost accuracy. To better illustrate the effectiveness and advances of our proposed method, we create a high-quality LipSync dataset, AVLips, by employing the state-of-the-art lip generators. We hope this high-quality and diverse dataset could be well served the further research on this challenging and interesting field. Experimental results show that our approach gives an average accuracy of more than 95.3% in spotting lip-syncing videos, significantly outperforming the baselines. Extensive experiments demonstrate the capability to tackle deepfakes and the robustness in surviving diverse input transformations. Our method achieves an accuracy of up to 90.2% in real-world scenarios (e.g., WeChat video call) and shows its powerful capabilities in real scenario deployment.
To facilitate the progress of this research community, we release all resources at https://github.com/AaronComo/LipFD.",main,NeurIPS,2024,Poster,Weifeng Liu;Tianyi She;Jiawei Liu;Boheng Li;Dongyu Yao;Ziyou Liang;Run Wang,True,https://openreview.net/pdf?id=yMS7ansbr6
yS1dUkQFnu,V-PETL Bench: A Unified Visual Parameter-Efficient Transfer Learning Benchmark,"Parameter-efficient transfer learning (PETL) methods show promise in adapting a pre-trained model to various downstream tasks while training only a few parameters. In the computer vision (CV) domain, numerous PETL algorithms have been proposed, but their direct employment or comparison remains inconvenient. To address this challenge, we construct a Unified Visual PETL Benchmark (V-PETL Bench) for the CV domain by selecting 30 diverse, challenging, and comprehensive datasets from image recognition, video action recognition, and dense prediction tasks. On these datasets, we systematically evaluate 25 dominant PETL algorithms and open-source a modular and extensible codebase for fair evaluation of these algorithms. V-PETL Bench runs on NVIDIA A800 GPUs and requires approximately 310 GPU days. We release all the benchmark, making it more efficient and friendly to researchers. Additionally, V-PETL Bench will be continuously updated for new PETL algorithms and CV tasks.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yi Xin;Siqi Luo;Xuyang Liu;Yuntao Du.;Haodi Zhou;Xinyu Cheng;Christina Luoluo Lee;Junlong Du;Haozhe Wang;MingCai Chen;Ting Liu;Guimin Hu;Zhongwei Wan;Rongchao Zhang;Aoxue Li;Mingyang Yi;Xiaohong Liu,False,https://openreview.net/pdf?id=yS1dUkQFnu
yUEBXN3cvX,On the Effects of Data Scale on UI Control Agents,"Autonomous agents that control user interfaces to accomplish human tasks are emerging. Leveraging LLMs to power such agents has been of special interest, but unless fine-tuned on human-collected task demonstrations, performance is still relatively low. In this work we study whether fine-tuning alone is a viable approach for building real-world UI control agents. To this end we collect and release a new dataset, AndroidControl, consisting of 15,283 demonstrations of everyday tasks with Android apps. Compared to existing datasets, each AndroidControl task instance includes both high and low-level human-generated instructions, allowing us to explore the level of task complexity an agent can handle. Moreover, AndroidControl is the most diverse computer control dataset to date, including 14,548 unique tasks over 833 Android apps, thus allowing us to conduct in-depth analysis of the model performance in and out of the domain of the training data. Using the dataset, we find that when tested in domain fine-tuned models outperform zero and few-shot baselines and scale in such a way that robust performance might feasibly be obtained simply by collecting more data.  Out of domain, performance scales significantly more slowly and suggests that in particular for high-level tasks, fine-tuning on more data alone may be insufficient for achieving robust out-of-domain performance.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Wei Li;William E Bishop;Alice Li;Christopher Rawles;Folawiyo Campbell-Ajala;Divya Tyamagundlu;Oriana Riva,True,https://openreview.net/pdf?id=yUEBXN3cvX
yVu5dnPlqA,MAmmoTH2: Scaling Instructions from the Web,"Instruction tuning improves the reasoning abilities of large language models (LLMs), with data quality and scalability being the crucial factors. Most instruction tuning data come from human crowd-sourcing or GPT-4 distillation. We propose a paradigm to efficiently harvest 10 million naturally existing instruction data from the pre-training web corpus to enhance LLM reasoning. Our approach involves (1) recalling relevant documents, (2) extracting instruction-response pairs, and (3) refining the extracted pairs using open-source LLMs. Fine-tuning base LLMs on this dataset, we build MAmmoTH2 models, which significantly boost performance on reasoning benchmarks. Notably, MAmmoTH2-7B’s (Mistral) performance increases from 11% to 36.7% on MATH and from 36% to 68.4% on GSM8K without training on any in-domain data. Further training MAmmoTH2 on public instruction tuning datasets yields MAmmoTH2-Plus, achieving state-of-the-art performance on several reasoning and chatbot benchmarks. Our work demonstrates how to harvest large-scale, high-quality instruction data without costly human annotation or GPT-4 distillation, providing a new paradigm for building better instruction tuning data.",main,NeurIPS,2024,Poster,Xiang Yue;Tianyu Zheng;Ge Zhang;Wenhu Chen,True,https://openreview.net/pdf?id=yVu5dnPlqA
yWMMKm81vZ,SeafloorAI: A Large-scale Vision-Language Dataset for Seafloor Geological Survey,"A major obstacle to the advancements of machine learning models in marine science, particularly in sonar imagery analysis, is the scarcity of AI-ready datasets.
  While there have been efforts to make AI-ready sonar image dataset publicly available, they suffer from limitations in terms of environment setting and scale.
  To bridge this gap, we introduce $\\\\texttt{SeafloorAI}$, the first extensive AI-ready datasets for seafloor mapping across 5 geological layers that is curated in collaboration with marine scientists. We further extend the dataset to $\\\\texttt{SeafloorGenAI}$ by incorporating the language component in order to facilitate the development of both $\\\\textit{vision}$- and $\\\\textit{language}$-capable machine learning models for sonar imagery.
  The dataset consists of 62 geo-distributed data surveys spanning 17,300 square kilometers, with 696K sonar images, 827K annotated segmentation masks, 696K detailed language descriptions and approximately 7M question-answer pairs. 
  By making our data processing source code publicly available, we aim to engage the marine science community to enrich the data pool and inspire the machine learning community to develop more robust models. 
  This collaborative approach will enhance the capabilities and applications of our datasets within both fields.",Datasets & Benchmarks,NeurIPS,2024,Poster,Kien X Nguyen;Fengchun Qiao;Arthur Trembanis;Xi Peng,True,https://openreview.net/pdf?id=yWMMKm81vZ
yeFE8HCnZB,NerfBaselines: Consistent and Reproducible Evaluation of Novel View Synthesis Methods,"Novel view synthesis is an important problem with many applications, including AR/VR, gaming, and simulations for robotics. With the recent rapid development of Neural Radiance Fields (NeRFs) and 3D Gaussian Splatting (3DGS) methods, it is becoming difficult to keep track of the current state of the art (SoTA) due to methods using different evaluation protocols, codebases being difficult to install and use, and methods not generalizing well to novel 3D scenes. Our experiments support this claim by showing that tiny differences in evaluation protocols of various methods can lead to inconsistent reported metrics. To address these issues, we propose a framework called NerfBaselines, which simplifies the installation of various methods, provides consistent benchmarking tools, and ensures reproducibility. We validate our implementation experimentally by reproducing numbers reported in the original papers. To further improve the accessibility, we release a web platform where commonly used methods are compared on standard benchmarks. Web: https://jkulhanek.com/nerfbaselines",Datasets & Benchmarks,NeurIPS,2024,Reject,Jonas Kulhanek;Torsten Sattler,False,https://openreview.net/pdf?id=yeFE8HCnZB
yg4Tt2QeU7,Bag of Tricks: Benchmarking of Jailbreak Attacks on LLMs,"Although Large Language Models (LLMs) have demonstrated significant capabilities in executing complex tasks in a zero-shot manner, they are susceptible to jailbreak attacks and can be manipulated to produce harmful outputs. Recently, a growing body of research has categorized jailbreak attacks into token-level and prompt-level attacks.  However, previous work primarily overlooks the diverse key factors of jailbreak attacks, with most studies concentrating on LLM vulnerabilities and lacking exploration of defense-enhanced LLMs. To address these issues, we introduced JailTrickBench to evaluate the impact of various attack settings on LLM performance and provide a baseline for jailbreak attacks, encouraging the adoption of a standardized evaluation framework. Specifically, we evaluate the eight key factors of implementing jailbreak attacks on LLMs from both target-level and attack-level perspectives. We further conduct seven representative jailbreak attacks on six defense methods across two widely used datasets, encompassing approximately 354 experiments with about 55,000 GPU hours on A800-80G. Our experimental results highlight the need for standardized benchmarking to evaluate these attacks on defense-enhanced LLMs. Our code is available at https://github.com/usail-hkust/JailTrickBench.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zhao Xu;Fan Liu;Hao Liu,True,https://openreview.net/pdf?id=yg4Tt2QeU7
yjj8ele147,Paloma: A Benchmark for Evaluating Language Model Fit,"Evaluations of language models (LMs) commonly report perplexity on monolithic data held out from training. Implicitly or explicitly, this data is composed of domains—varying distributions of language. We introduce Perplexity Analysis for Language Model Assessment (Paloma), a benchmark to measure LM fit to 546 English and code domains, instead of assuming perplexity on one distribution extrapolates to others. We include two new datasets of the top 100 subreddits (e.g., r/depression on Reddit) and programming languages (e.g., Java on GitHub), both sources common in contemporary LMs. With our benchmark, we release 6 baseline 1B LMs carefully controlled to provide fair comparisons about which pretraining corpus is best and code for others to apply those controls to their own experiments. Our case studies demonstrate how the fine-grained results from Paloma surface findings such as that models pretrained without data beyond Common Crawl exhibit anomalous gaps in LM fit to many domains or that loss is dominated by the most frequently occurring strings in the vocabulary.",Datasets & Benchmarks,NeurIPS,2024,Poster,Ian Magnusson;Akshita Bhagia;Valentin Hofmann;Luca Soldaini;Ananya Harsh Jha;Oyvind Tafjord;Dustin Schwenk;Evan Pete Walsh;Yanai Elazar;Kyle Lo;Dirk Groeneveld;Iz Beltagy;Hannaneh Hajishirzi;Noah A. Smith;Kyle Richardson;Jesse Dodge,True,https://openreview.net/pdf?id=yjj8ele147
ykQnxko1cJ,CemiFace: Center-based Semi-hard Synthetic Face Generation for Face Recognition,"Privacy issue is a main concern in developing face recognition techniques. Although synthetic face images can partially mitigate potential legal risks while maintaining effective face recognition (FR) performance, FR models trained by face images synthesized by existing generative approaches frequently suffer from performance degradation problems due to the insufficient discriminative quality of these synthesized samples. In this paper, we systematically investigate what contributes to solid face recognition model training, and reveal that face images with certain degree of similarities to their identity centers show great effectiveness in the performance of trained FR models. Inspired by this, we propose a novel diffusion-based approach (namely **Ce**nter-based Se**mi**-hard Synthetic Face
Generation (**CemiFace**) which produces facial samples with various levels of similarity to the subject center, thus allowing to generate face datasets containing effective discriminative samples for training face recognition. Experimental results show that with a modest degree of similarity, training on the generated dataset can produce competitive performance compared to previous generation methods. The code will be available at:https://github.com/szlbiubiubiu/CemiFace",main,NeurIPS,2024,Poster,Zhonglin Sun;Siyang Song;Ioannis Patras;Georgios Tzimiropoulos,True,https://openreview.net/pdf?id=ykQnxko1cJ
yktQNqtepd,Towards Flexible 3D Perception: Object-Centric Occupancy Completion Augments 3D Object Detection,"While 3D object bounding box (bbox) representation has been widely used in autonomous driving perception, it lacks the ability to capture the precise details of an object's intrinsic geometry. Recently, occupancy has emerged as a promising alternative for 3D scene perception. However, constructing a high-resolution occupancy map remains infeasible for large scenes due to computational constraints. Recognizing that foreground objects only occupy a small portion of the scene, we introduce object-centric occupancy as a supplement to object bboxes. This representation not only provides intricate details for detected objects but also enables higher voxel resolution in practical applications. We advance the development of object-centric occupancy perception from both data and algorithm perspectives. On the data side, we construct the first object-centric occupancy dataset from scratch using an automated pipeline. From the algorithmic standpoint, we introduce a novel object-centric occupancy completion network equipped with an implicit shape decoder that manages dynamic-size occupancy generation. This network accurately predicts the complete object-centric occupancy volume for inaccurate object proposals by leveraging temporal information from long sequences. Our method demonstrates robust performance in completing object shapes under noisy detection and tracking conditions. Additionally, we show that our occupancy features significantly enhance the detection results of state-of-the-art 3D object detectors, especially for incomplete or distant objects in the Waymo Open Dataset.",main,NeurIPS,2024,Poster,Chaoda Zheng;Feng Wang;Naiyan Wang;Shuguang Cui;Zhen Li,True,https://openreview.net/pdf?id=yktQNqtepd
ypPzyflbYs,Neural Concept Binder,"The challenge in object-based visual reasoning lies in generating concept representations that are both descriptive and distinct. Achieving this in an unsupervised manner requires human users to understand the model's learned concepts and, if necessary, revise incorrect ones. To address this challenge, we introduce the Neural Concept Binder (NCB), a novel framework for deriving both discrete and continuous concept representations, which we refer to as ""concept-slot encodings"". NCB employs two types of binding: ""soft binding"", which leverages the recent SysBinder mechanism to obtain object-factor encodings, and subsequent ""hard binding"", achieved through hierarchical clustering and retrieval-based inference. This enables obtaining expressive, discrete representations from unlabeled images. Moreover, the structured nature of NCB's concept representations allows for intuitive inspection and the straightforward integration of external knowledge, such as human input or insights from other AI models like GPT-4. Additionally, we demonstrate that incorporating the hard binding mechanism preserves model performance while enabling seamless integration into both neural and symbolic modules for complex reasoning tasks. We validate the effectiveness of NCB through evaluations on our newly introduced CLEVR-Sudoku dataset.",main,NeurIPS,2024,Poster,Wolfgang Stammer;Antonia Wüst;David Steinmann;Kristian Kersting,True,https://openreview.net/pdf?id=ypPzyflbYs
ypggxVWIv2,GTBench: Uncovering the Strategic Reasoning Capabilities of LLMs via Game-Theoretic Evaluations,"As Large Language Models (LLMs) are integrated into critical real-world applications, their strategic and logical reasoning abilities are increasingly crucial. This paper evaluates LLMs' reasoning abilities in competitive environments through game-theoretic tasks, e.g., board and card games that require pure logic and strategic reasoning to compete with opponents. We first propose GTBench, a language-driven environment composing 10 widely-recognized tasks, across a comprehensive game taxonomy: complete versus incomplete information, dynamic versus static, and probabilistic versus deterministic scenarios. Then, we (1) Characterize the game-theoretic reasoning of LLMs; and (2) Perform LLM-vs.-LLM competitions as reasoning evaluation. We observe that (1) LLMs have distinct behaviors regarding various gaming scenarios; for example, LLMs fail in complete and deterministic games yet they are competitive in probabilistic gaming scenarios; (2) Most open-source LLMs, e.g., CodeLlama-34b-Instruct and Llama-2-70b-chat, are less competitive than commercial LLMs, e.g., GPT-4, in complex games, yet the recently released Llama-3-70b-Instruct makes up for this shortcoming. In addition, code-pretraining greatly benefits strategic reasoning, while advanced reasoning methods such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) do not always help. We further characterize the game-theoretic properties of LLMs, such as equilibrium and Pareto Efficiency in repeated games. Detailed error profiles are provided for a better understanding of LLMs' behavior. We hope our research provides standardized protocols and serves as a foundation to spur further explorations in the strategic reasoning of LLMs.",main,NeurIPS,2024,Poster,Jinhao Duan;Renming Zhang;James Diffenderfer;Bhavya Kailkhura;Lichao Sun;Elias Stengel-Eskin;Mohit Bansal;Tianlong Chen;Kaidi Xu,True,https://openreview.net/pdf?id=ypggxVWIv2
yppcLFeZgy,MutaPLM: Protein Language Modeling for Mutation Explanation and Engineering,"Studying protein mutations within amino acid sequences holds tremendous significance in life sciences. Protein language models (PLMs) have demonstrated strong capabilities in broad biological applications. However, due to architectural design and lack of supervision, PLMs model mutations implicitly with evolutionary plausibility, which is not satisfactory to serve as explainable and engineerable tools in real-world studies. To address these issues, we present MutaPLM, a unified framework for interpreting and navigating protein mutations with protein language models. MutaPLM introduces a protein *delta* network that captures explicit protein mutation representations within a unified feature space, and a transfer learning pipeline with a chain-of-thought (CoT) strategy to harvest protein mutation knowledge from biomedical texts. We also construct MutaDescribe, the first large-scale protein mutation dataset with rich textual annotations, which provides cross-modal supervision signals. Through comprehensive experiments, we demonstrate that MutaPLM excels at providing human-understandable explanations for mutational effects and prioritizing novel mutations with desirable properties. Our code, model, and data are open-sourced at https://github.com/PharMolix/MutaPLM.",main,NeurIPS,2024,Poster,YIZHEN LUO;Zikun Nie;Massimo Hong;Suyuan Zhao;Hao Zhou;Zaiqing Nie,True,https://openreview.net/pdf?id=yppcLFeZgy
z1nITsHKb4,MMScan: A Multi-Modal 3D Scene Dataset with Hierarchical Grounded Language Annotations,"With the emergence of LLMs and their integration with other data modalities, multi-modal 3D perception attracts more attention due to its connectivity to the physical world and makes rapid progress. However, limited by existing datasets, previous works mainly focus on understanding object properties or inter-object spatial relationships in a 3D scene. To tackle this problem, this paper builds the first largest ever multi-modal 3D scene dataset and benchmark with hierarchical grounded language annotations, MMScan. It is constructed based on a top-down logic, from region to object level, from a single target to inter-target relationships, covering holistic aspects of spatial and attribute understanding. The overall pipeline incorporates powerful VLMs via carefully designed prompts to initialize the annotations efficiently and further involve humans' correction in the loop to ensure the annotations are natural, correct, and comprehensive. Built upon existing 3D scanning data, the resulting multi-modal 3D dataset encompasses 1.4M meta-annotated captions on 109k objects and 7.7k regions as well as over 3.04M diverse samples for 3D visual grounding and question-answering benchmarks. We evaluate representative baselines on our benchmarks, analyze their capabilities in different aspects, and showcase the key problems to be addressed in the future. Furthermore, we use this high-quality dataset to train state-of-the-art 3D visual grounding and LLMs and obtain remarkable performance improvement both on existing benchmarks and in-the-wild evaluation.",Datasets & Benchmarks,NeurIPS,2024,Poster,Ruiyuan Lyu;Jingli Lin;Tai Wang;Shuai Yang;Xiaohan Mao;Yilun Chen;Runsen Xu;Haifeng Huang;Chenming Zhu;Dahua Lin;Jiangmiao Pang,True,https://openreview.net/pdf?id=z1nITsHKb4
z64azPC6Nl,GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music Scores for All Singing Tasks,"The scarcity of high-quality and multi-task singing datasets significantly hinders the development of diverse controllable and personalized singing tasks, as existing singing datasets suffer from low quality, limited diversity of languages and singers, absence of multi-technique information and realistic music scores, and poor task suitability.
To tackle these problems, we present GTSinger, a large Global, multi-Technique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks, along with its benchmarks.
Particularly,
(1) we collect 80.59 hours of high-quality singing voices, forming the largest recorded singing dataset;
(2) 20 professional singers across nine widely spoken languages offer diverse timbres and styles;
(3) we provide controlled comparison and phoneme-level annotations of six commonly used singing techniques, helping technique modeling and control;
(4) GTSinger offers realistic music scores, assisting real-world musical composition;
(5) singing voices are accompanied by manual phoneme-to-audio alignments, global style labels, and 16.16 hours of paired speech for various singing tasks.
Moreover, to facilitate the use of GTSinger, we conduct four benchmark experiments: technique-controllable singing voice synthesis, technique recognition, style transfer, and speech-to-singing conversion.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Yu Zhang;Changhao Pan;Wenxiang Guo;Ruiqi Li;Zhiyuan Zhu;Jialei Wang;Wenhao Xu;Jingyu Lu;Zhiqing Hong;Chuxin Wang;Lichao Zhang;Jinzheng He;Ziyue Jiang;Yuxin Chen;Chen Yang;Jiecheng Zhou;Xinyu Cheng;Zhou Zhao,True,https://openreview.net/pdf?id=z64azPC6Nl
zGfKPqunJG,$E^3$: Exploring Embodied Emotion Through A Large-Scale Egocentric Video Dataset,"Understanding human emotions is fundamental to enhancing human-computer interaction, especially for embodied agents that mimic human behavior.  Traditional emotion analysis often takes a third-person perspective, limiting the ability of agents to interact naturally and empathetically.  To address this gap, this paper presents $E^3$ for Exploring Embodied Emotion, the first massive first-person view video dataset. $E^3$ contains more than $50$ hours of video, capturing $8$ different emotion types in diverse scenarios and languages. The dataset features videos recorded by individuals in their daily lives, capturing a wide range of real-world emotions conveyed through visual, acoustic, and textual modalities. By leveraging this dataset, we define $4$ core benchmark tasks - emotion recognition, emotion classification, emotion localization, and emotion reasoning - supported by more than $80$k manually crafted annotations, providing a comprehensive resource for training and evaluating emotion analysis models. We further present Emotion-LlaMa, which complements visual modality with acoustic modality to enhance the understanding of emotion in first-person videos. The results of comparison experiments with a large number of baselines demonstrate the superiority of Emotion-LlaMa and set a new benchmark for embodied emotion analysis. We expect that $E^3$ can promote advances in multimodal understanding, robotics, and augmented reality, and provide a solid foundation for the development of more empathetic and context-aware embodied agents.",Datasets & Benchmarks,NeurIPS,2024,Poster,Wang Lin;Yueying Feng;WenKang Han;Tao Jin;Zhou Zhao;Fei Wu;Chang Yao;Jingyuan Chen,True,https://openreview.net/pdf?id=zGfKPqunJG
zLU21oQjD5,DART-Math: Difficulty-Aware Rejection Tuning for Mathematical Problem-Solving,"Solving mathematical problems requires advanced reasoning abilities and presents notable challenges for large language models. Previous works usually synthesize data from proprietary models to augment existing datasets, followed by instruction tuning to achieve top-tier results. However, our analysis of these datasets reveals severe biases towards easy queries, with frequent failures to generate any correct response for the most challenging queries.
Hypothesizing that difficult queries are crucial to learning complex reasoning, we propose *Difficulty-Aware Rejection Tuning* (`DART`), a method that allocates difficult queries more trials during the synthesis phase, enabling more extensive training on difficult samples.
Utilizing `DART`, we have created new datasets for mathematical problem-solving that focus more on difficult queries and are substantially smaller than previous ones. Remarkably, our synthesis process solely relies on a 7B-sized open-weight model, without reliance on the commonly used proprietary GPT-4.
We fine-tune various base models on our datasets ranging from 7B to 70B in size, resulting in a series of strong models called `DART-Math`.
In comprehensive in-domain and out-of-domain evaluation on 6 mathematical benchmarks, `DART-Math` outperforms vanilla rejection tuning significantly, being superior or comparable to previous arts, despite using much smaller datasets and no proprietary models. Furthermore, our results position our synthetic datasets as the most effective and cost-efficient publicly available resources for advancing mathematical problem-solving. Our datasets, models and code are publicly available at https://github.com/hkust-nlp/dart-math.",main,NeurIPS,2024,Poster,Yuxuan Tong;Xiwen Zhang;Rui Wang;Ruidong Wu;Junxian He,True,https://openreview.net/pdf?id=zLU21oQjD5
zQ3qU0xWZ5,kGym: A Platform and Dataset to Benchmark Large Language Models on Linux Kernel Crash Resolution,"Large Language Models (LLMs) are consistently improving at increasingly realistic software engineering (SE) tasks. In real-world software stacks, significant SE effort is spent developing foundational system software like the Linux kernel. 
Unlike application-level software, a systems codebase like Linux is multilingual (low-level C/Assembly/Bash/Rust); gigantic (>20 million lines); critical (impacting billions of devices worldwide), and highly concurrent (involving complex multi-threading). To evaluate if machine learning (ML) models are useful while developing such large-scale systems-level software, we introduce kGym (a platform) and kBench (a dataset). The kGym platform provides a SE environment for large-scale experiments on the Linux kernel, including compiling and running kernels in parallel across several virtual machines, detecting operations and crashes, inspecting logs, and querying and patching the code base. We use kGym to facilitate evaluation on kBench, a crash resolution benchmark drawn from real-world Linux kernel bugs.  An example bug in kBench contains crashing stack traces, a bug-reproducer file, a developer-written fix, and other associated data. To understand current performance, we conduct baseline experiments by prompting LLMs to resolve Linux kernel crashes. Our initial evaluations reveal that the best performing LLM achieves 0.72\\\\% and 5.38\\\\% in the unassisted and assisted (i.e., buggy files disclosed to the model) settings, respectively. These results highlight the need for further research to enhance model performance in SE tasks. Improving performance on kBench requires models to master new learning skills, including understanding the cause of crashes and repairing faults, writing memory-safe and hardware-aware code, and understanding concurrency. As a result, this work opens up multiple avenues of research at the intersection of machine learning and systems software.",Datasets & Benchmarks,NeurIPS,2024,Poster,Alex Mathai;Chenxi Huang;Petros Maniatis;Aleksandr Nogikh;Franjo Ivancic;Junfeng Yang;Baishakhi Ray,True,https://openreview.net/pdf?id=zQ3qU0xWZ5
zV2GDsZb5a,Neural Gaffer: Relighting Any Object via Diffusion,"Single-image relighting is a challenging task that involves reasoning about the complex interplay between geometry, materials, and lighting. Many prior methods either support only specific categories of images, such as portraits, or require special capture conditions, like using a flashlight. Alternatively, some methods explicitly decompose a scene into intrinsic components, such as normals and BRDFs, which can be inaccurate or under-expressive. In this work, we propose a novel end-to-end 2D relighting diffusion model, called Neural Gaffer, that takes a single image of any object and can synthesize an accurate, high-quality relit image under any novel environmental lighting condition, simply by conditioning an image generator on a target environment map, without an explicit scene decomposition. Our method builds on a pre-trained diffusion model, and fine-tunes it on a synthetic relighting dataset, revealing and harnessing the inherent understanding of lighting present in the diffusion model. We evaluate our model on both synthetic and in-the-wild Internet imagery and demonstrate its advantages in terms of generalization and accuracy. Moreover, by combining with other generative methods, our model enables many downstream 2D tasks, such as text-based relighting and object insertion. Our model can also operate as a strong relighting prior for 3D tasks, such as relighting a radiance field.",main,NeurIPS,2024,Poster,Haian Jin;Yuan Li;Fujun Luan;Yuanbo Xiangli;Sai Bi;Kai Zhang;Zexiang Xu;Jin Sun;Noah Snavely,True,https://openreview.net/pdf?id=zV2GDsZb5a
zVrQeoPIoQ,"Rethinking No-reference Image Exposure Assessment from Holism to Pixel: Models, Datasets and Benchmarks","The past decade has witnessed an increasing demand for enhancing image quality through exposure, and as a crucial prerequisite in this endeavor, Image Exposure Assessment (IEA) is now being accorded serious attention. However, IEA encounters two persistent challenges that remain unresolved over the long term: the accuracy and generalizability of No-reference IEA are inadequate for practical applications; the scope of IEA is confined to qualitative and quantitative analysis of the entire image or subimage, such as providing only a score to evaluate the exposure level, thereby lacking intuitive and precise fine-grained evaluation for complex exposure conditions.  The objective of this paper is to address the persistent bottleneck challenges from three perspectives: model, dataset, and benchmark.  1) Model-level: we propose a Pixel-level IEA Network (P-IEANet) that utilizes Haar discrete wavelet transform (DWT) to analyze, decompose, and assess exposure from both lightness and structural perspectives, capable of generating pixel-level assessment results under no-reference scenarios. 2) Dataset-level: we elaborately build an exposure-oriented dataset, IEA40K, containing 40K  images, covering 17 typical lighting scenarios, 27 devices, and 50+ scenes, with each image densely annotated by more than 10 experts with pixel-level labels.  3) Benchmark-level: we develop a comprehensive benchmark of 19 methods based on IEA40K. Our P-IEANet not only achieves state-of-the-art (SOTA) performance on all metrics but also seamlessly integrates with existing exposure correction and lighting enhancement methods. To our knowledge, this is the first work that explicitly emphasizes assessing complex image exposure problems at a pixel level, providing a significant boost to the IEA and exposure-related community. The code and dataset are available in \\\\href{https://github.com/mRobotit/Pixel-level-No-reference-Image-Exposure-Assessment}{\\\\textcolor{red} {here}}.",main,NeurIPS,2024,Poster,Shuai He;Shuntian Zheng;Anlong Ming;Banyu Wu;Huadong Ma,True,https://openreview.net/pdf?id=zVrQeoPIoQ
zg8dpAGl1I,XLand-MiniGrid: Scalable Meta-Reinforcement Learning Environments in JAX,"Inspired by the diversity and depth of XLand and the simplicity and minimalism of MiniGrid, we present XLand-MiniGrid, a suite of tools and grid-world environments for meta-reinforcement learning research. Written in JAX, XLand-MiniGrid is designed to be highly scalable and can potentially run on GPU or TPU accelerators, democratizing large-scale experimentation with limited resources. Along with the environments, XLand-MiniGrid provides pre-sampled benchmarks with millions of unique tasks of varying difficulty and easy-to-use baselines that allow users to quickly start training adaptive agents. In addition, we have conducted a preliminary analysis of scaling and generalization, showing that our baselines are capable of reaching millions of steps per second during training and validating that the proposed benchmarks are challenging. XLand-MiniGrid is open-source and available at \\\\url{https://github.com/corl-team/xland-minigrid}.",Datasets & Benchmarks,NeurIPS,2024,Poster,Alexander Nikulin;Vladislav Kurenkov;Ilya Zisman;Artem Sergeevich Agarkov;Viacheslav Sinii;Sergey Kolesnikov,True,https://openreview.net/pdf?id=zg8dpAGl1I
zgSnSZ0Re6,Point Cloud Matters: Rethinking the Impact of Different Observation Spaces on Robot Learning,"In robot learning, the observation space is crucial due to the distinct characteristics of different modalities, which can potentially become a bottleneck alongside policy design. In this study, we explore the influence of various observation spaces on robot learning, focusing on three predominant modalities: RGB, RGB-D, and point cloud. We introduce OBSBench, a benchmark comprising two simulators and 125 tasks, along with standardized pipelines for various encoders and policy baselines. Extensive experiments on diverse contact-rich manipulation tasks reveal a notable trend: point cloud-based methods, even those with the simplest designs, frequently outperform their RGB and RGB-D counterparts. This trend persists in both scenarios: training from scratch and utilizing pre-training. Furthermore, our findings demonstrate that point cloud observations often yield better policy performance and significantly stronger generalization capabilities across various geometric and visual conditions. These outcomes suggest that the 3D point cloud is a valuable observation modality for intricate robotic tasks. We also suggest that incorporating both appearance and coordinate information can enhance the performance of point cloud methods. We hope our work provides valuable insights and guidance for designing more generalizable and robust robotic models.",Datasets & Benchmarks,NeurIPS,2024,Poster,Haoyi Zhu;Yating Wang;Di Huang;Weicai Ye;Wanli Ouyang;Tong He,True,https://openreview.net/pdf?id=zgSnSZ0Re6
zogaeVpbaE,DevBench: A multimodal developmental benchmark for language learning,"How (dis)similar are the learning trajectories of vision–language models and children? Recent modeling work has attempted to understand the gap between models’ and humans’ data efficiency by constructing models trained on less data, especially multimodal naturalistic data. However, such models are often evaluated on adult-level benchmarks, with limited breadth in language abilities tested, and without direct comparison to behavioral data. We introduce DevBench, a multimodal benchmark comprising seven language evaluation tasks spanning the domains of lexical, syntactic, and semantic ability, with behavioral data from both children and adults. We evaluate a set of vision–language models on these tasks, comparing models and humans on their response patterns, not their absolute performance. Across tasks, models exhibit variation in their closeness to human response patterns, and models that perform better on a task also more closely resemble human behavioral responses. We also examine the developmental trajectory of OpenCLIP over training, finding that greater training results in closer approximations to adult response patterns. DevBench thus provides a benchmark for comparing models to human language development. These comparisons highlight ways in which model and human language learning processes diverge, providing insight into entry points for improving language models.",Datasets & Benchmarks,NeurIPS,2024,Oral,Alvin Wei Ming Tan;Chunhua Yu;Bria Lorelle Long;Wanjing Anya Ma;Tonya Murray;Rebecca D. Silverman;Jason D Yeatman;Michael Frank,True,https://openreview.net/pdf?id=zogaeVpbaE
zv4UISZzp5,IDGen: Item Discrimination Induced Prompt Generation for LLM Evaluation,"As Large Language Models (LLMs) become more capable of handling increasingly complex tasks, the evaluation set must keep pace with these advancements to ensure it remains sufficiently discriminative. Item Discrimination (ID) theory, which is widely used in educational assessment, measures the ability of individual test items to differentiate between high and low performers. Inspired by this theory, we propose an ID-induced prompt synthesis framework for evaluating LLMs so that the evaluation set continually updates and refines according to model abilities. 
Our data synthesis framework prioritizes both breadth and specificity. It can generate prompts that comprehensively evaluate the capabilities of LLMs while revealing meaningful performance differences between models, allowing for effective discrimination of their relative strengths and weaknesses across various tasks and domains.
To produce high-quality data, we incorporate a self-correct mechanism into our generalization framework and develop two models to predict prompt discrimination and difficulty score to facilitate our data synthesis framework, contributing valuable tools to evaluation data synthesis research. We apply our generated data to evaluate five SOTA models. Our data achieves an average score of 51.92, accompanied by a variance of 10.06. By contrast, previous works (i.e., SELF-INSTRUCT and WizardLM) obtain an average score exceeding 67, with a variance below 3.2.
The results demonstrate that the data generated by our framework is more challenging and discriminative compared to previous works.
We will release a dataset of over 3,000 carefully crafted prompts to facilitate evaluation research of LLMs.",main,NeurIPS,2024,Poster,Fan Lin;Shuyi Xie;Yong Dai;Wenlin Yao;TianJiao Lang;Yu Zhang,True,https://openreview.net/pdf?id=zv4UISZzp5
zxSWIdyW3A,Cooperative Hardware-Prompt Learning for Snapshot Compressive Imaging,"Existing reconstruction models in snapshot compressive imaging systems (SCI) are trained with a single well-calibrated hardware instance, making their perfor- mance vulnerable to hardware shifts and limited in adapting to multiple hardware configurations. To facilitate cross-hardware learning, previous efforts attempt to directly collect multi-hardware data and perform centralized training, which is impractical due to severe user data privacy concerns and hardware heterogeneity across different platforms/institutions. In this study, we explicitly consider data privacy and heterogeneity in cooperatively optimizing SCI systems by proposing a Federated Hardware-Prompt learning (FedHP) framework. Rather than mitigating the client drift by rectifying the gradients, which only takes effect on the learning manifold but fails to solve the heterogeneity rooted in the input data space, FedHP learns a hardware-conditioned prompter to align inconsistent data distribution across clients, serving as an indicator of the data inconsistency among different hardware (e.g., coded apertures). Extensive experimental results demonstrate that the proposed FedHP coordinates the pre-trained model to multiple hardware con- figurations, outperforming prevalent FL frameworks for 0.35dB under challenging heterogeneous settings. Moreover, a Snapshot Spectral Heterogeneous Dataset has been built upon multiple practical SCI systems. Data and code are aveilable at https://github.com/Jiamian-Wang/FedHP-Snapshot-Compressive-Imaging.git",main,NeurIPS,2024,Poster,Jiamian Wang;Zongliang Wu;Yulun Zhang;Xin Yuan;Tao Lin;ZHIQIANG TAO,True,https://openreview.net/pdf?id=zxSWIdyW3A
