id,title,abstract,track,conference,year,status,author,score,url_link
3HCT3xfNm9r,Pile of Law: Learning Responsible Data Filtering from the Law and a 256GB Open-Source Legal Dataset,"One concern with the rise of large language models lies with their potential for significant harm, particularly from pretraining on biased, obscene, copyrighted, and private information. Emerging ethical approaches have attempted to filter pretraining material, but such approaches have been ad hoc and failed to take context into account. We offer an approach to filtering grounded in law, which has directly addressed the tradeoffs in filtering material. First, we gather and make available the Pile of Law, a ~256GB (and growing) dataset of open-source English-language legal and administrative data, covering court opinions, contracts, administrative rules, and legislative records. Pretraining on the Pile of Law may help with legal tasks that have the promise to improve access to justice. Second, we distill the legal norms that governments have developed to constrain the inclusion of toxic or private content into actionable lessons for researchers and discuss how our dataset reflects these norms. Third, we show how the Pile of Law offers researchers the opportunity to learn such filtering rules directly from the data, providing an exciting new research direction in model-based processing.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Peter Henderson;Mark Simon Krass;Lucia Zheng;Neel Guha;Christopher D Manning;Dan Jurafsky;Daniel E. Ho,True,https://openreview.net/pdf?id=3HCT3xfNm9r
M3Y74vmsMcY,LAION-5B: An open large-scale dataset for training next generation image-text models,"Groundbreaking language-vision architectures like CLIP and DALL-E proved the utility of training on large amounts of noisy image-text data, without relying on expensive accurate labels used in standard vision unimodal supervised learning. The resulting models showed capabilities of strong text-guided image generation and transfer to downstream tasks, while performing remarkably at zero-shot classification with noteworthy out-of-distribution robustness. Since then, large-scale language-vision models like ALIGN, BASIC, GLIDE, Flamingo and Imagen made further improvements. Studying the training and capabilities of such models requires datasets containing billions of image-text pairs. Until now, no datasets of this size have been made openly available for the broader research community. To address this problem and democratize research on large-scale multi-modal models, we present LAION-5B - a dataset consisting of 5.85 billion CLIP-filtered image-text pairs, of which 2.32B contain English language. We show successful replication and fine-tuning of foundational models like CLIP, GLIDE and Stable Diffusion using the dataset, and discuss further experiments enabled with an openly available dataset of this scale. Additionally we provide several nearest neighbor indices, an improved web-interface for dataset exploration and subset generation, and detection scores for watermark, NSFW, and toxic content detection.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Christoph Schuhmann;Romain Beaumont;Richard Vencu;Cade W Gordon;Ross Wightman;Mehdi Cherti;Theo Coombes;Aarush Katta;Clayton Mullis;Mitchell Wortsman;Patrick Schramowski;Srivatsa R Kundurthy;Katherine Crowson;Ludwig Schmidt;Robert Kaczmarczyk;Jenia Jitsev,True,https://openreview.net/pdf?id=M3Y74vmsMcY
Vk4-HUnkEak,AMOS: A Large-Scale Abdominal Multi-Organ Benchmark for Versatile Medical Image Segmentation,"Despite the considerable progress in automatic abdominal multi-organ segmentation from CT/MRI scans in recent years, a comprehensive evaluation of the models' capabilities is hampered by the lack of a large-scale benchmark from diverse clinical scenarios. Constraint by the high cost of collecting and labeling 3D medical data, most of the deep learning models to date are driven by datasets with a limited number of organs of interest or samples, which still limits the power of modern deep models and makes it difficult to provide a fully comprehensive and fair estimate of various methods. To mitigate the limitations, we present AMOS, a large-scale, diverse, clinical dataset for abdominal organ segmentation. AMOS provides 500 CT and 100 MRI scans collected from multi-center, multi-vendor, multi-modality, multi-phase, multi-disease patients, each with voxel-level annotations of 15 abdominal organs, providing challenging examples and test-bed for studying robust segmentation algorithms under diverse targets and scenarios. We further benchmark several state-of-the-art medical segmentation models to evaluate the status of the existing methods on this new challenging dataset. We have made our datasets, benchmark servers, and baselines publicly available, and hope to inspire future research. Information can be found at https://amos22.grand-challenge.org.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Yuanfeng Ji;Haotian Bai;Chongjian GE;Jie Yang;Ye Zhu;Ruimao Zhang;Zhen Li;Lingyan Zhanng;Wanling Ma;Xiang Wan;Ping Luo,True,https://openreview.net/pdf?id=Vk4-HUnkEak
z1d8fUiS8Cr,Multi-LexSum: Real-world Summaries of Civil Rights Lawsuits at Multiple Granularities,"With the advent of large language models, methods for abstractive summarization have made great strides, creating potential for use in applications to aid knowledge workers processing unwieldy document collections. One such setting is the Civil Rights Litigation Clearinghouse (CRLC, https://clearinghouse.net), which posts information about large-scale civil rights lawsuits, serving lawyers, scholars, and the general public. Today, summarization in the CRLC requires extensive training of lawyers and law students who spend hours per case understanding multiple relevant documents in order to produce high-quality summaries of key events and outcomes. Motivated by this ongoing real-world summarization effort, we introduce Multi-LexSum, a collection of 9,280 expert-authored summaries drawn from ongoing CRLC writing. Multi-LexSum presents a challenging multi-document summarization task given the length of the source documents, often exceeding two hundred pages per case. Furthermore, Multi-LexSum is distinct from other datasets in its multiple target summaries, each at a different granularity (ranging from one-sentence ""extreme"" summaries to multi-paragraph narrations of over five hundred words). We present extensive analysis demonstrating that despite the high-quality summaries in the training data (adhering to strict content and style guidelines), state-of-the-art summarization models perform poorly on this task. We release Multi-LexSum for further summarization research and to facilitate the development of applications to assist in the CRLC's mission at https://multilexsum.github.io.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Zejiang Shen;Kyle Lo;Lauren Yu;Nathan Dahlberg;Margo Schlanger;Doug Downey,True,https://openreview.net/pdf?id=z1d8fUiS8Cr
rc8o_j8I8PX,MineDojo: Building Open-Ended Embodied Agents with Internet-Scale Knowledge,"Autonomous agents have made great strides in specialist domains like Atari games and Go. However, they typically learn tabula rasa in isolated environments with limited and manually conceived objectives, thus failing to generalize across a wide spectrum of tasks and capabilities. Inspired by how humans continually learn and adapt in the open world, we advocate a trinity of ingredients for building generalist agents: 1) an environment that supports a multitude of tasks and goals, 2) a large-scale database of multimodal knowledge, and 3) a flexible and scalable agent architecture. We introduce MineDojo, a new framework built on the popular Minecraft game that features a simulation suite with thousands of diverse open-ended tasks and an internet-scale knowledge base with Minecraft videos, tutorials, wiki pages, and forum discussions. Using MineDojo's data, we propose a novel agent learning algorithm that leverages large pre-trained video-language models as a learned reward function. Our agent is able to solve a variety of open-ended tasks specified in free-form language without any manually designed dense shaping reward. We open-source the simulation suite, knowledge bases, algorithm implementation, and pretrained models (https://minedojo.org) to promote research towards the goal of generally capable embodied agents.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Linxi Fan;Guanzhi Wang;Yunfan Jiang;Ajay Mandlekar;Yuncong Yang;Haoyi Zhu;Andrew Tang;De-An Huang;Yuke Zhu;Anima Anandkumar,True,https://openreview.net/pdf?id=rc8o_j8I8PX
jbdp9m7nr0R,How Would The Viewer Feel? Estimating Wellbeing From Video Scenarios,"In recent years, deep neural networks have demonstrated increasingly strong abilities to recognize objects and activities in videos. However, as video understanding becomes widely used in real-world applications, a key consideration is developing human-centric systems that understand not only the content of the video but also how it would affect the wellbeing and emotional state of viewers. To facilitate research in this setting, we introduce two large-scale datasets with over 60,000 videos manually annotated for emotional response and subjective wellbeing. The Video Cognitive Empathy (VCE) dataset contains annotations for distributions of fine-grained emotional responses, allowing models to gain a detailed understanding of affective states. The Video to Valence (V2V) dataset contains annotations of relative pleasantness between videos, which enables predicting a continuous spectrum of wellbeing. In experiments, we show how video models that are primarily trained to recognize actions and find contours of objects can be repurposed to understand human preferences and the emotional content of videos. Although there is room for improvement, predicting wellbeing and emotional response is on the horizon for state-of-the-art models. We hope our datasets can help foster further advances at the intersection of commonsense video understanding and human preference learning.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Mantas Mazeika;Eric Tang;Andy Zou;Steven Basart;Jun Shern Chan;Dawn Song;David Forsyth;Jacob Steinhardt;Dan Hendrycks,True,https://openreview.net/pdf?id=jbdp9m7nr0R
aJtVdI251Vv,WinoGAViL: Gamified Association Benchmark to Challenge Vision-and-Language Models,"While vision-and-language models perform well on tasks such as visual question answering, they struggle when it comes to basic human commonsense reasoning skills. In this work, we introduce WinoGAViL: an online game of vision-and-language associations (e.g., between werewolves and a full moon), used as a dynamic evaluation benchmark. Inspired by the popular card game Codenames, a spymaster gives a textual cue related to several visual candidates, and another player tries to identify them. Human players are rewarded for creating associations that are challenging for a rival AI model but still solvable by other human players. We use the game to collect 3.5K instances, finding that they are intuitive for humans (>90% Jaccard index) but challenging for state-of-the-art AI models, where the best model (ViLT) achieves a score of 52%, succeeding mostly where the cue is visually salient. Our analysis as well as the feedback we collect from players indicate that the collected associations require diverse reasoning skills, including general knowledge, common sense, abstraction, and more. We release the dataset, the code and the interactive game, allowing future data collection that can be used to develop models with better association abilities.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Yonatan Bitton;Nitzan Bitton Guetta;Ron Yosef;Yuval Elovici;Mohit Bansal;Gabriel Stanovsky;Roy Schwartz,True,https://openreview.net/pdf?id=aJtVdI251Vv
bKO6BPtYQA7,Change Event Dataset for Discovery from Spatio-temporal Remote Sensing Imagery,"Satellite imagery is increasingly available, high resolution, and temporally detailed.  Changes in spatio-temporal datasets such as satellite images are particularly interesting as they reveal the many events and forces that shape our world.  However, finding such interesting and meaningful change events from the vast data is challenging.  In this paper, we present new datasets for such change events that include semantically meaningful events like road construction.  Instead of manually annotating the very large corpus of satellite images, we introduce a novel unsupervised approach that takes a large spatio-temporal dataset from satellite images and finds interesting change events.  To evaluate the meaningfulness on these datasets we create 2 benchmarks namely CaiRoad and CalFire which capture the events of road construction and forest fires.  These new benchmarks can be used to evaluate semantic retrieval/classification performance.  We explore these benchmarks qualitatively and quantitatively by using several methods and show that these new datasets are indeed challenging for many existing methods. 
",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Utkarsh Mall;Bharath Hariharan;Kavita Bala,True,https://openreview.net/pdf?id=bKO6BPtYQA7
OxFoLTKDcNm,Communicating Natural Programs to Humans and Machines,"The Abstraction and Reasoning Corpus (ARC) is a set of procedural tasks that tests an agent's ability to flexibly solve novel problems. While most ARC tasks are easy for humans, they are challenging for state-of-the-art AI. What makes building intelligent systems that can generalize to novel situations such as ARC difficult? We posit that the answer might be found by studying the difference of $\\\\textit{language}$: While humans readily generate and interpret instructions in a general language, computer systems are shackled to a narrow domain-specific language that they can precisely execute. We present LARC, the $\\\\textit{Language-complete ARC}$: a collection of natural language descriptions by a group of human participants  who instruct each other on how to solve ARC tasks using language alone, which contains successful instructions for 88\\\\% of the ARC tasks. We analyze the collected instructions as `natural programs', finding that while they resemble computer programs, they are distinct in two ways: First, they contain a wide range of primitives; Second, they frequently leverage communicative strategies beyond directly executable codes. We demonstrate that these two distinctions prevent current program synthesis techniques from leveraging LARC to its full potential, and give concrete suggestions on how to build the next-generation program synthesizers.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Sam Acquaviva;Yewen Pu;Marta Kryven;Theodoros Sechopoulos;Catherine Wong;Gabrielle Ecanow;Maxwell Nye;Michael Henry Tessler;Joshua B. Tenenbaum,True,https://openreview.net/pdf?id=OxFoLTKDcNm
zBBmV-i84Go,Addressing Resource Scarcity across Sign Languages with Multilingual Pretraining and Unified-Vocabulary Datasets,"There are over 300 sign languages in the world, many of which have very limited or no labelled sign-to-text datasets. To address low-resource data scenarios, self-supervised pretraining and multilingual finetuning have been shown to be effective in natural language and speech processing. In this work, we apply these ideas to sign language recognition.

We make three contributions.
- First, we release SignCorpus, a large pretraining dataset on sign languages comprising about 4.6K hours of signing data across 10 sign languages. SignCorpus is curated from sign language videos on the internet, filtered for data quality, and converted into sequences of pose keypoints thereby removing all personal identifiable information (PII).
- Second, we release Sign2Vec, a graph-based model with 5.2M parameters that is pretrained on SignCorpus. We envisage Sign2Vec as a multilingual large-scale pretrained model which can be fine-tuned for various sign recognition tasks across languages.
- Third, we create MultiSign-ISLR -- a multilingual and label-aligned dataset of sequences of pose keypoints from 11 labelled datasets across 7 sign languages, and MultiSign-FS -- a new finger-spelling training and test set across 7 languages. On these datasets, we fine-tune Sign2Vec to create multilingual isolated sign recognition models. With experiments on multiple benchmarks, we show that pretraining and multilingual transfer are effective giving significant gains over state-of-the-art results.

All datasets, models, and code has been made open-source via the OpenHands toolkit.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Gokul NC;Manideep Ladi;Sumit Negi;Prem Selvaraj;Pratyush Kumar;Mitesh M Khapra,True,https://openreview.net/pdf?id=zBBmV-i84Go
UoEw6KigkUn,The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset,"As language models grow ever larger, the need for large-scale high-quality text datasets has never been more pressing, especially in multilingual settings. The BigScience workshop, a 1-year international and multidisciplinary initiative, was formed with the goal of researching and training large language models as a values-driven undertaking, putting issues of ethics, harm, and governance in the foreground. This paper documents the data creation and curation efforts undertaken by BigScience to assemble the Responsible Open-science Open-collaboration Text Sources (ROOTS) corpus, a 1.6TB dataset spanning 59 languages that was used to train the 176-billion-parameter BigScience Large Open-science Open-access Multilingual (BLOOM) language model. We further release a large initial subset of the corpus and analyses thereof, and hope to empower large-scale monolingual and multilingual modeling projects with both the data and the processing tools, as well as stimulate research around this large multilingual corpus.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Hugo Laurençon;Lucile Saulnier;Thomas Wang;Christopher Akiki;Albert Villanova del Moral;Teven Le Scao;Leandro Von Werra;Chenghao Mou;Eduardo González Ponferrada;Huu Nguyen;Jörg Frohberg;Mario Šaško;Quentin Lhoest;Angelina McMillan-Major;Gérard Dupont;Stella Biderman;Anna Rogers;Loubna Ben allal;Francesco De Toni;Giada Pistilli;Olivier Nguyen;Somaieh Nikpoor;Maraim Masoud;Pierre Colombo;Javier de la Rosa;Paulo Villegas;Tristan Thrush;Shayne Longpre;Sebastian Nagel;Leon Weber;Manuel Romero Muñoz;Jian Zhu;Daniel Van Strien;Zaid Alyafeai;Khalid Almubarak;Vu Minh Chien;Itziar Gonzalez-Dios;Aitor Soroa;Kyle Lo;Manan Dey;Pedro Ortiz Suarez;Aaron Gokaslan;Shamik Bose;David Ifeoluwa Adelani;Long Phan;Hieu Tran;Ian Yu;Suhas Pai;Jenny Chim;Violette Lepercq;Suzana Ilic;Margaret Mitchell;Sasha Luccioni;Yacine Jernite,True,https://openreview.net/pdf?id=UoEw6KigkUn
_vSn5XxGRnG,SCAMPS: Synthetics for Camera Measurement of Physiological Signals,"The use of cameras and computational algorithms for noninvasive, low-cost and scalable measurement of physiological (e.g., cardiac and pulmonary) vital signs is very attractive. However, diverse data representing a range of environments, body motions, illumination conditions and physiological states is laborious, time consuming and expensive to obtain. Synthetic data have proven a valuable tool in several areas of machine learning, yet are not widely available for camera measurement of physiological states. Synthetic data offer ""perfect"" labels (e.g., without noise and with precise synchronization), labels that may not be possible to obtain otherwise (e.g., precise pixel level segmentation maps) and provide a high degree of control over variation and diversity in the dataset.  We present SCAMPS, a dataset of synthetics containing 2,800 videos (1.68M frames) with aligned cardiac and respiratory signals and facial action intensities. The RGB frames are provided alongside segmentation maps and precise descriptive statistics about the underlying waveforms, including inter-beat interval, heart rate variability, and pulse arrival time. Finally, we present baseline results training on these synthetic data and testing on real-world datasets to illustrate generalizability.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Daniel McDuff;Miah Wander;Xin Liu;Brian L. Hill;Javier Hernandez;Jonathan Lester;Tadas Baltrusaitis,True,https://openreview.net/pdf?id=_vSn5XxGRnG
c7f9uoPnzgE,Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark,"Vision-Language Pre-training (VLP) models have shown remarkable performance on various downstream tasks. Their success heavily relies on the scale of pre-trained cross-modal datasets. However, the lack of large-scale datasets and benchmarks in Chinese hinders the development of Chinese VLP models and broader multilingual applications. In this work, we release a large-scale Chinese cross-modal dataset named Wukong, which contains 100 million Chinese image-text pairs collected from the web. Wukong aims to benchmark different multi-modal pre-training methods to facilitate the VLP research and community development. Furthermore, we release a group of models pre-trained with various image encoders (ViT-B/ViT-L/SwinT) and also apply advanced pre-training techniques into VLP such as locked-image text tuning, token-wise similarity in contrastive learning, and reduced-token interaction. Extensive experiments and a benchmarking of different downstream tasks including a new largest human-verified image-text test dataset are also provided. Experiments show that Wukong can serve as a promising Chinese pre-training dataset and benchmark for different cross-modal learning methods. For the zero-shot image classification task on 10 datasets, $Wukong_\\\\text{ViT-L}$ achieves an average accuracy of 73.03%. For the image-text retrieval task, it achieves a mean recall of 71.6% on AIC-ICC which is 12.9% higher than WenLan 2.0. Also, our Wukong models are benchmarked on downstream tasks with other variants on multiple datasets, e.g., Flickr8K-CN, Flickr-30K-CN, COCO-CN, et al. More information can be referred to https://wukong-dataset.github.io/wukong-dataset/.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Jiaxi Gu;Xiaojun Meng;Guansong Lu;Lu Hou;Minzhe Niu;Xiaodan Liang;Lewei Yao;Runhui Huang;Wei Zhang;Xin Jiang;Chunjing Xu;Hang Xu,True,https://openreview.net/pdf?id=c7f9uoPnzgE
mJWt6pOcHNy,Breaking Bad: A Dataset for Geometric Fracture and Reassembly,"We introduce Breaking Bad, a large-scale dataset of fractured objects. Our dataset consists of over one million fractured objects simulated from ten thousand base models. The fracture simulation is powered by a recent physically based algorithm that efficiently generates a variety of fracture modes of an object. Existing shape assembly datasets decompose objects according to semantically meaningful parts, effectively modeling the construction process. In contrast, Breaking Bad models the destruction process of how a geometric object naturally breaks into fragments. Our dataset serves as a benchmark that enables the study of fractured object reassembly and presents new challenges for geometric shape understanding. We analyze our dataset with several geometry measurements and benchmark three state-of-the-art shape assembly deep learning methods under various settings. Extensive experimental results demonstrate the difficulty of our dataset, calling on future research in model designs specifically for the geometric shape assembly task. We host our dataset at https://breaking-bad-dataset.github.io/.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Silvia Sellán;Yun-Chun Chen;Ziyi Wu;Animesh Garg;Alec Jacobson,True,https://openreview.net/pdf?id=mJWt6pOcHNy
TzNuIdrHoU,Avalon: A Benchmark for RL Generalization Using Procedurally Generated Worlds,"Despite impressive successes, deep reinforcement learning (RL) systems still fall short of human performance on generalization to new tasks and environments that differ from their training. As a benchmark tailored for studying RL generalization, we introduce Avalon, a set of tasks in which embodied agents in highly diverse procedural 3D worlds must survive by navigating terrain, hunting or gathering food, and avoiding hazards. Avalon is unique among existing RL benchmarks in that the reward function, world dynamics, and action space are the same for every task, with tasks differentiated solely by altering the environment; its 20 tasks, ranging in complexity from eat and throw to hunt and navigate, each create worlds in which the agent must perform specific skills in order to survive. This setup enables investigations of generalization within tasks, between tasks, and to compositional tasks that require combining skills learned from previous tasks. Avalon includes a highly efficient simulator, a library of baselines, and a benchmark with scoring metrics evaluated against hundreds of hours of human performance, all of which are open-source and publicly available. We find that standard RL baselines make progress on most tasks but are still far from human performance, suggesting Avalon is challenging enough to advance the quest for generalizable RL.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Joshua Albrecht;Abraham J Fetterman;Bryden Fogelman;Ellie Kitanidis;Bartosz Wróblewski;Nicole Seo;Michael Rosenthal;Maksis Knutins;Zachary Polizzi;James B Simon;Kanjun Qiu,True,https://openreview.net/pdf?id=TzNuIdrHoU
_HLcjaVlqJ,JAHS-Bench-201: A Foundation For Research On Joint Architecture And Hyperparameter Search,"The past few years have seen the development of many benchmarks for Neural Architecture Search (NAS), fueling rapid progress in NAS research. However, recent work, which shows that good hyperparameter settings can be more important than using the best architecture, calls for a shift in focus towards Joint Architecture and Hyperparameter Search (JAHS). Therefore, we present JAHS-Bench-201, the first collection of surrogate benchmarks for JAHS, built to also facilitate research on multi-objective, cost-aware and (multi) multi-fidelity optimization algorithms. To the best of our knowledge, JAHS-Bench-201 is based on the most extensive dataset of neural network performance data in the public domain. It is composed of approximately 161 million data points and 20 performance metrics for three deep learning tasks, while featuring a 14-dimensional search and fidelity space that extends the popular NAS-Bench-201 space. With JAHS-Bench-201, we hope to democratize research on JAHS and lower the barrier to entry of an extremely compute intensive field, e.g., by reducing the compute time to run a JAHS algorithm from 5 days to only a few seconds.",Datasets & Benchmarks,NeurIPS,2022,Highlighted,Archit Bansal;Danny Stoll;Maciej Janowski;Arber Zela;Frank Hutter,True,https://openreview.net/pdf?id=_HLcjaVlqJ
qnfYsave0U4,The Dollar Street Dataset: Images Representing the Geographic and Socioeconomic Diversity of the World,"It is crucial that image datasets for computer vision are representative and contain accurate demographic information to ensure their robustness and fairness, especially for smaller subpopulations. To address this issue, we present Dollar Street - a supervised dataset that contains 38,479 images of everyday household items from homes around the world. This dataset was manually curated and fully labeled, including tags for objects (e.g. “toilet,” “toothbrush,” “stove”) and demographic data such as region, country and home monthly income. This dataset includes images from homes with no internet access and incomes as low as \\\\$26.99 per month, visually capturing valuable socioeconomic diversity of traditionally under-represented populations. All images and data are licensed under CC-BY, permitting their use in academic and commercial work. Moreover, we show that this dataset can improve the performance of classification tasks for images of household items from lower income homes, addressing a critical need for datasets that combat bias.",Datasets & Benchmarks,NeurIPS,2022,Poster,William A Gaviria Rojas;Sudnya Diamos;Keertan Ranjan Kini;David Kanter;Vijay Janapa Reddi;Cody Coleman,True,https://openreview.net/pdf?id=qnfYsave0U4
HCnb1TByvx7,Multilingual Abusive Comment Detection at Scale for Indic Languages,"Social media platforms were conceived to act as online `town squares' where people could get together, share information and communicate with each other peacefully. However, harmful content borne out of bad actors are constantly plaguing these platforms slowly converting them into `mosh pits' where the bad actors take the liberty to extensively abuse various marginalised groups. Accurate and timely detection of abusive content on social media platforms is therefore very important for facilitating safe interactions between users.  However, due to the small scale and sparse linguistic coverage of Indic abusive speech datasets, development of such algorithms for Indic social media users (one-sixth of global population) is severely impeded.
To facilitate and encourage research in this important direction, we contribute for the first time MACD - a large-scale (150K), human-annotated, multilingual (5 languages), balanced (49\\\\% abusive content) and diverse (70K users) abuse detection dataset of user comments, sourced from a popular social media platform - ShareChat. We also release AbuseXLMR, an abusive content detection model pretrained on large number of social media comments in 15+ Indic languages which outperforms XLM-R and MuRIL on multiple Indic datasets. Along with the annotations, we also release the mapping between comment, post and user id's to facilitate modelling the relationship between them. We share competitive monolingual, cross-lingual and few-shot baselines so that MACD can be used as a dataset benchmark for future research. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Vikram Gupta;Sumegh Roychowdhury;Mithun Das;Somnath Banerjee;Punyajoy Saha;Binny Mathew;hastagiri prakash vanchinathan;Animesh Mukherjee,True,https://openreview.net/pdf?id=HCnb1TByvx7
vrnqr3PG4yB,TempEL: Linking Dynamically Evolving and Newly Emerging Entities,"In our continuously evolving world, entities change over time and new, previously non-existing or unknown, entities appear. We study how this evolutionary scenario impacts the performance on a well established entity linking (EL) task. For that study, we introduce TempEL, an entity linking dataset that consists of time-stratified English Wikipedia snapshots from 2013 to 2022, from which we collect both anchor mentions of entities, and these target entities’ descriptions. By capturing such temporal aspects, our newly introduced TempEL resource contrasts with currently existing entity linking datasets, which are composed of fixed mentions linked to a single static version of a target Knowledge Base (e.g., Wikipedia 2010 for CoNLL-AIDA). Indeed, for each of our collected temporal snapshots, TempEL contains links to entities that are continual, i.e., occur in all of the years, as well as completely new entities that appear for the first time at some point. Thus, we enable to quantify the performance of current state-of-the-art EL models for: (i) entities that are subject to changes over time in their Knowledge Base descriptions as well as their mentions’ contexts, and (ii) newly created entities that were previously non-existing (e.g., at the time the EL model was trained). Our experimental results show that in terms of temporal performance degradation, (i) continual entities suffer a decrease of up to 3.1% EL accuracy, while (ii) for new entities this accuracy drop is up to 17.9%. This highlights the challenge of the introduced TempEL dataset and opens new research prospects in the area of time-evolving entity disambiguation. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Klim Zaporojets;Lucie-Aimée Kaffee;Johannes Deleu;Thomas Demeester;Chris Develder;Isabelle Augenstein,True,https://openreview.net/pdf?id=vrnqr3PG4yB
70_Wx-dON3q,Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification,"We introduce Meta-Album, an image classification meta-dataset designed to facilitate few-shot learning, transfer learning, meta-learning, among other tasks. It includes 40 open datasets, each having at least 20 classes with 40 examples per class, with verified licences. They stem from diverse domains, such as ecology (fauna and flora), manufacturing (textures, vehicles), human actions, and optical character recognition, featuring various image scales (microscopic, human scales, remote sensing). All datasets are preprocessed, annotated, and formatted uniformly, and come in 3 versions (Micro $\\\\subset$ Mini $\\\\subset$ Extended) to match users’ computational resources. We showcase the utility of the first 30 datasets on few-shot learning problems. The other 10 will be released shortly after. Meta-Album is already more diverse and larger (in number of datasets) than similar efforts, and we are committed to keep enlarging it via a series of competitions. As competitions terminate, their test data are released, thus creating a rolling benchmark, available through OpenML.org. Our website https://meta-album.github.io/ contains the source code of challenge winning methods, baseline methods, data loaders, and instructions for contributing either new datasets or algorithms to our expandable meta-dataset.",Datasets & Benchmarks,NeurIPS,2022,Poster,Ihsan Ullah;Dustin Carrión-Ojeda;Sergio Escalera;Isabelle M Guyon;Mike Huisman;Felix Mohr;Jan N. van Rijn;Haozhe Sun;Joaquin Vanschoren;Phan Anh Vu,True,https://openreview.net/pdf?id=70_Wx-dON3q
gT6j4_tskUt,OpenOOD: Benchmarking Generalized Out-of-Distribution Detection,"Out-of-distribution (OOD) detection is vital to safety-critical machine learning applications and has thus been extensively studied, with a plethora of methods developed in the literature. However, the field currently lacks a unified, strictly formulated, and comprehensive benchmark, which often results in unfair comparisons and inconclusive results. From the problem setting perspective, OOD detection is closely related to neighboring fields including anomaly detection (AD), open set recognition (OSR), and model uncertainty, since methods developed for one domain are often applicable to each other. To help the community to improve the evaluation and advance, we build a unified, well-structured codebase called OpenOOD, which implements over 30 methods developed in relevant fields and provides a comprehensive benchmark under the recently proposed generalized OOD detection framework. With a comprehensive comparison of these methods, we are gratified that the field has progressed significantly over the past few years, where both preprocessing methods and the orthogonal post-hoc methods show strong potential. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Jingkang Yang;Pengyun Wang;Dejian Zou;Zitang Zhou;Kunyuan Ding;WENXUAN PENG;Haoqi Wang;Guangyao Chen;Bo Li;Yiyou Sun;Xuefeng Du;Kaiyang Zhou;Wayne Zhang;Dan Hendrycks;Yixuan Li;Ziwei Liu,False,https://openreview.net/pdf?id=gT6j4_tskUt
4nAe0PS7D-l,PROSPECT: Labeled Tandem Mass Spectrometry Dataset for Machine Learning in Proteomics,"Proteomics is the interdisciplinary field focusing on the large-scale study of proteins. Proteins essentially organize and execute all functions within organisms. Today, the bottom-up analysis approach is the most commonly used workflow, where proteins are digested into peptides and subsequently analyzed using Tandem Mass Spectrometry (MS/MS). MS-based proteomics has transformed various fields in life sciences, such as drug discovery and biomarker identification. Today, proteomics is entering a phase where it is helpful for clinical decision-making. Computational methods are vital in turning large amounts of acquired raw MS data into information and, ultimately, knowledge. Deep learning has proved its success in multiple domains as a robust framework for supervised and unsupervised machine learning problems. In proteomics, scientists are increasingly leveraging the potential of deep learning to predict the properties of peptides based on their sequence to improve their confident identification. However, a reference dataset is missing, covering several proteomics tasks, enabling performance comparison, and evaluating reproducibility and generalization. Here, we present a large labeled proteomics dataset spanning several tasks in the domain to address this challenge. We focus on two common applications: peptide retention time and MS/MS spectrum prediction. We review existing methods and task formulations from a machine learning perspective and recommend suitable evaluation metrics and visualizations. With an accessible dataset, we aim to lower the entry barrier and enable faster development in machine learning for proteomics.",Datasets & Benchmarks,NeurIPS,2022,Poster,Omar Shouman;Wassim Gabriel;Victor-George Giurcoiu;Vitor Sternlicht;Mathias Wilhelm,True,https://openreview.net/pdf?id=4nAe0PS7D-l
UXPXs-OYbks,Robustness Disparities in Face Detection,"Facial analysis systems have been deployed by large companies and critiqued by scholars and activists for the past decade. Many existing algorithmic audits examine the performance of these systems on later stage elements of facial analysis systems like facial recognition and age, emotion, or perceived gender prediction; however, a core component to these systems has been vastly understudied from a fairness perspective: face detection, sometimes called face localization. Since face detection is a pre-requisite step in facial analysis systems, the bias we observe in face detection will flow downstream to the other components like facial recognition and emotion prediction. Additionally, no prior work has focused on the robustness of these systems under various perturbations and corruptions, which leaves open the question of how various people are impacted by these phenomena. We present the first of its kind detailed benchmark of face detection systems, specifically examining the robustness to noise of commercial and academic models. We use both standard and recently released academic facial datasets to quantitatively analyze trends in face detection robustness. Across all the datasets and systems, we generally find that photos of individuals who are masculine presenting, older, of darker skin type, or have dim lighting are more susceptible to errors than their counterparts in other identities.",Datasets & Benchmarks,NeurIPS,2022,Poster,Samuel Dooley;George Z Wei;Tom Goldstein;John P Dickerson,False,https://openreview.net/pdf?id=UXPXs-OYbks
BubxnHpuMbG,EnvPool: A Highly Parallel Reinforcement Learning Environment Execution Engine,"There has been significant progress in developing reinforcement learning (RL) training systems. Past works such as IMPALA, Apex, Seed RL, Sample Factory, and others, aim to improve the system's overall throughput. In this paper, we aim to address a common bottleneck in the RL training system, i.e., parallel environment execution, which is often the slowest part of the whole system but receives little attention. With a curated design for paralleling RL environments, we have improved the RL environment simulation speed across different hardware setups, ranging from a laptop and a modest workstation, to a high-end machine such as NVIDIA DGX-A100. On a high-end machine, EnvPool achieves one million frames per second for the environment execution on Atari environments and three million frames per second on MuJoCo environments. When running EnvPool on a laptop, the speed is 2.8x that of the Python subprocess. Moreover, great compatibility with existing RL training libraries has been demonstrated in the open-sourced community, including CleanRL, rl_games, DeepMind Acme, etc. Finally, EnvPool allows researchers to iterate their ideas at a much faster pace and has great potential to become the de facto RL environment execution engine. Example runs show that it only takes five minutes to train agents to play Atari Pong and MuJoCo Ant on a laptop.  EnvPool is open-sourced at https://github.com/sail-sg/envpool.",Datasets & Benchmarks,NeurIPS,2022,Poster,Jiayi Weng;Min Lin;Shengyi Huang;Bo Liu;Denys Makoviichuk;Viktor Makoviychuk;Zichen Liu;Yufan Song;Ting Luo;Yukun Jiang;Zhongwen Xu;Shuicheng YAN,False,https://openreview.net/pdf?id=BubxnHpuMbG
W_bsDmzwaZ7,K-Radar: 4D Radar Object Detection for Autonomous Driving in Various Weather Conditions,"Unlike RGB cameras that use visible light bands (384∼769 THz) and Lidars that use infrared bands (361∼331 THz), Radars use relatively longer wavelength radio bands (77∼81 GHz), resulting in robust measurements in adverse weathers. Unfortunately, existing Radar datasets only contain a relatively small number of samples compared to the existing camera and Lidar datasets. This may hinder the development of sophisticated data-driven deep learning techniques for Radar-based perception. Moreover, most of the existing Radar datasets only provide 3D Radar tensor (3DRT) data that contain power measurements along the Doppler, range, and azimuth dimensions. As there is no elevation information, it is challenging to estimate the 3D bounding box of an object from 3DRT. In this work, we introduce KAIST-Radar (K-Radar), a novel large-scale object detection dataset and benchmark that contains 35K frames of 4D Radar tensor (4DRT) data with power measurements along the Doppler, range, azimuth, and elevation dimensions, together with carefully annotated 3D bounding box labels of objects on the roads. K-Radar includes challenging driving conditions such as adverse weathers (fog, rain, and snow) on various road structures (urban, suburban roads, alleyways, and highways). In addition to the 4DRT, we provide auxiliary measurements from carefully calibrated high-resolution Lidars, surround stereo cameras, and RTK-GPS. We also provide 4DRT-based object detection baseline neural networks (baseline NNs) and show that the height information is crucial for 3D object detection. And by comparing the baseline NN with a similarly-structured Lidar-based neural network, we demonstrate that 4D Radar is a more robust sensor for adverse weather conditions. All codes are available at https://github.com/kaist-avelab/k-radar.",Datasets & Benchmarks,NeurIPS,2022,Poster,Dong-Hee Paek;Seung-Hyun Kong;Kevin Tirta Wijaya,True,https://openreview.net/pdf?id=W_bsDmzwaZ7
31_U7n18gM7,BackdoorBench: A Comprehensive Benchmark of Backdoor Learning,"Backdoor learning is an emerging and vital topic for studying deep neural networks' vulnerability (DNNs). Many pioneering backdoor attack and defense methods are being proposed, successively or concurrently, in the status of a rapid arms race. However, we find that the evaluations of new methods are often unthorough to verify their claims and accurate performance, mainly due to the rapid development, diverse settings, and the difficulties of implementation and reproducibility.  Without thorough evaluations and comparisons, it is not easy to track the current progress and design the future development roadmap of the literature. To alleviate this dilemma, we build a comprehensive benchmark of backdoor learning called BackdoorBench. It consists of an extensible modular-based codebase (currently including implementations of 8 state-of-the-art (SOTA) attacks and 9 SOTA defense algorithms) and a standardized protocol of complete backdoor learning. We also provide comprehensive evaluations of every pair of 8 attacks against 9 defenses, with 5 poisoning ratios, based on 5 models and 4 datasets, thus 8,000 pairs of evaluations in total. We present abundant analysis from different perspectives about these 8,000 evaluations, studying the effects of different factors in backdoor learning.  All codes and evaluations of BackdoorBench are publicly available at https://backdoorbench.github.io.",Datasets & Benchmarks,NeurIPS,2022,Poster,Baoyuan Wu;Hongrui Chen;Mingda Zhang;Zihao Zhu;Shaokui Wei;Danni Yuan;Chao Shen,False,https://openreview.net/pdf?id=31_U7n18gM7
9K-8l0WgSK3,CEDe: A collection of expert-curated datasets with atom-level entity annotations for Optical Chemical Structure Recognition,"Optical Chemical Structure Recognition (OCSR) deals with the translation from chemical images to molecular structures, this being the main way chemical compounds are depicted in scientific documents. Traditionally, rule-based methods have followed a framework based on the detection of chemical entities, such as atoms and bonds, followed by a compound structure reconstruction step. Recently, neural architectures analog to image captioning have been explored to solve this task, yet they still show to be data inefficient, using millions of examples just to show performances comparable with traditional methods. Looking to motivate and benchmark new approaches based on atomic-level entities detection and graph reconstruction, we present CEDe, a unique collection of chemical entity bounding boxes manually curated by experts for scientific literature datasets. These annotations combine to more than 700,000 chemical entity bounding boxes with the necessary information for structure reconstruction. Also, a large synthetic dataset containing one million molecular images and annotations is released in order to explore transfer-learning techniques that could help these architectures perform better under low-data regimes. Benchmarks show that detection-reconstruction based models can achieve performances on par with or better than image captioning-like models, even with 100x fewer training examples.",Datasets & Benchmarks,NeurIPS,2022,Poster,Rodrigo Hormazabal;Changyoung Park;Soonyoung Lee;Sehui Han;Yeonsik Jo;Jaewan Lee;Ahra Jo;Seung Hwan Kim;Jaegul Choo;Moontae Lee;Honglak Lee,True,https://openreview.net/pdf?id=9K-8l0WgSK3
c0l2YolqD2T,How Well Do Unsupervised Learning Algorithms Model Human Real-time and Life-long Learning?,"Humans learn from visual inputs at multiple timescales, both rapidly and flexibly acquiring visual knowledge over short periods, and robustly accumulating online learning progress over longer periods. Modeling these powerful learning capabilities is an important problem for computational visual cognitive science, and models that could replicate them would be of substantial utility in real-world computer vision settings. In this work, we establish benchmarks for both real-time and life-long continual visual learning. Our real-time learning benchmark measures a model's ability to match the rapid visual behavior changes of real humans over the course of minutes and hours, given a stream of visual inputs. Our life-long learning benchmark evaluates the performance of models in a purely online learning curriculum obtained directly from child visual experience over the course of years of development. We evaluate a spectrum of recent deep self-supervised visual learning algorithms on both benchmarks, finding that none of them perfectly match human performance, though some algorithms perform substantially better than others. Interestingly, algorithms embodying recent trends in self-supervised learning -- including BYOL, SwAV and MAE -- are substantially worse on our benchmarks than an earlier generation of self-supervised algorithms such as SimCLR and MoCo-v2. We present analysis indicating that the failure of these newer algorithms is primarily due to their inability to handle the kind of sparse low-diversity datastreams that naturally arise in the real world, and that actively leveraging memory through negative sampling -- a mechanism eschewed by these newer algorithms -- appears useful for facilitating learning in such low-diversity environments. We also illustrate a complementarity between the short and long timescales in the two benchmarks, showing how requiring a single learning algorithm to be locally context-sensitive enough to match real-time learning changes while stable enough to avoid catastrophic forgetting over the long term induces a trade-off that human-like algorithms may have to straddle. Taken together, our benchmarks establish a quantitative way to directly compare learning between neural networks models and human learners, show how choices in the mechanism by which such algorithms handle sample comparison and memory strongly impact their ability to match human learning abilities, and expose an open problem space for identifying more flexible and robust visual self-supervision algorithms. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Chengxu Zhuang;Violet Xiang;Yoon Bai;Xiaoxuan Jia;Nicholas Turk-Browne;Kenneth Norman;James J. DiCarlo;Daniel LK Yamins,True,https://openreview.net/pdf?id=c0l2YolqD2T
yCZRdI0Y7G,Sample Efficiency Matters: A Benchmark for Practical Molecular Optimization,"Molecular optimization is a fundamental goal in the chemical sciences and is of central interest to drug and material design. In recent years, significant progress has been made in solving challenging problems across various aspects of computational molecular optimizations, emphasizing high validity, diversity, and, most recently, synthesizability. Despite this progress, many papers report results on trivial or self-designed tasks, bringing additional challenges to directly assessing the performance of new methods. Moreover, the sample efficiency of the optimization---the number of molecules evaluated by the oracle---is rarely discussed, despite being an essential consideration for realistic discovery applications.

To fill this gap, we have created an open-source benchmark for practical molecular optimization, PMO, to facilitate the transparent and reproducible evaluation of algorithmic advances in molecular optimization. This paper thoroughly investigates the performance of 25 molecular design algorithms on 23 single-objective (scalar) optimization tasks with a particular focus on sample efficiency. Our results show that most ``state-of-the-art'' methods fail to outperform their predecessors under a limited oracle budget allowing 10K queries and that no existing algorithm can efficiently solve certain molecular optimization problems in this setting. We analyze the influence of the optimization algorithm choices, molecular assembly strategies, and oracle landscapes on the optimization performance to inform future algorithm development and benchmarking. PMO provides a standardized experimental setup to comprehensively evaluate and compare new molecule optimization methods with existing ones. All code can be found at https://github.com/wenhao-gao/mol_opt.",Datasets & Benchmarks,NeurIPS,2022,Poster,Wenhao Gao;Tianfan Fu;Jimeng Sun;Connor W. Coley,True,https://openreview.net/pdf?id=yCZRdI0Y7G
PfuW84q25y9,Tenrec: A Large-scale Multipurpose Benchmark Dataset for Recommender Systems,"Existing benchmark datasets for recommender systems (RS)  either are created  at a small scale or involve very limited forms of user feedback. RS models evaluated on such datasets often lack practical values for large-scale real-world applications. In this paper, we describe Tenrec, a novel and publicly available data collection for RS that records various user feedback from four different recommendation scenarios. To be specific, Tenrec has the following five characteristics: (1) it is large-scale, containing around 5 million users and 140 million interactions; (2) it has not only positive user feedback, but also true  negative feedback (vs. one-class recommendation); (3) it contains overlapped users and items across four different scenarios; (4) it contains various types of  user positive feedback, in forms of clicking, liking, sharing, and following, etc; (5) it contains additional features beyond the user IDs and item IDs. We verify Tenrec on ten diverse  recommendation  tasks by running several classical baseline models per task. Tenrec has the potential to become a  useful benchmark dataset for a majority of popular recommendation tasks.  Our source codes and datasets will be included  in supplementary materials.",Datasets & Benchmarks,NeurIPS,2022,Poster,Guanghu Yuan;Fajie Yuan;Yudong Li;Beibei Kong;Shujie Li;Lei Chen;Min Yang;Chenyun Yu;Bo Hu;Zang Li;Yu Xu;Xiaohu Qie,True,https://openreview.net/pdf?id=PfuW84q25y9
ttxAvIQA4i_,EgoTaskQA: Understanding Human Tasks in Egocentric Videos,"Understanding human tasks through video observations is an essential capability of intelligent agents. The challenges of such capability lie in the difficulty of generating a detailed understanding of situated actions, their effects on object states (\\\\ie, state changes), and their causal dependencies. These challenges are further aggravated by the natural parallelism from multi-tasking and partial observations in multi-agent collaboration. Most prior works leverage action localization or future prediction as an \\\\textit{indirect} metric for evaluating such task understanding from videos. To make a \\\\textit{direct} evaluation, we introduce the EgoTaskQA benchmark that provides a single home for the crucial dimensions of task understanding through question answering on real-world egocentric videos. We meticulously design questions that target the understanding of (1) action dependencies and effects, (2) intents and goals, and (3) agents' beliefs about others. These questions are divided into four types, including descriptive (what status?), predictive (what will?), explanatory (what caused?), and counterfactual (what if?) to provide diagnostic analyses on \\\\textit{spatial, temporal, and causal} understandings of goal-oriented tasks. We evaluate state-of-the-art video reasoning models on our benchmark and show their significant gaps between humans in understanding complex goal-oriented egocentric videos. We hope this effort would drive the vision community to move onward with goal-oriented video understanding and reasoning.",Datasets & Benchmarks,NeurIPS,2022,Poster,Baoxiong Jia;Ting Lei;Song-Chun Zhu;Siyuan Huang,True,https://openreview.net/pdf?id=ttxAvIQA4i_
ObD_o92z4p,LIPS - Learning Industrial Physical Simulation benchmark suite,"Physical simulations are at the core of many critical industrial systems. However, today's physical simulators  have some limitations such as computation time, dealing with missing or uncertain data, or even  non-convergence for some feasible cases. Recently, the use of data-driven approaches to learn complex physical simulations has been considered as a promising approach to address those issues. However, this comes often at the cost of some accuracy which may hinder the industrial use. To drive this new research topic towards a better real-world applicability, we propose a new benchmark suite ""Learning Industrial Physical Simulations""(LIPS) to meet the need of developing efficient, industrial application-oriented, augmented simulators. To define how to assess such benchmark performance, we propose a set of four generic categories of criteria. The proposed benchmark suite is a modular and configurable framework that can deal with different physical problems. To demonstrate this ability, we propose in this paper to investigate two distinct use-cases with different physical simulations, namely: the power grid and the pneumatic. For each use case, several benchmarks are described and assessed with existing models. None of the models perform well under all expected criteria, inviting the community to develop  new industry-applicable solutions and possibly showcase their performance publicly upon online LIPS instance on Codabench.",Datasets & Benchmarks,NeurIPS,2022,Poster,Milad Leyli-abadi;Antoine Marot;Jérôme Picault;David Danan;Mouadh Yagoubi;Benjamin Donnot;Seif-Eddine Attoui;Pavel Dimitrov;Asma Farjallah;Clement Etienam,True,https://openreview.net/pdf?id=ObD_o92z4p
6Hl7XoPNAVX,Ambiguous Images With Human Judgments for Robust Visual Event Classification,"Contemporary vision benchmarks predominantly consider tasks on which humans can achieve near-perfect performance. However, humans are frequently presented with visual data that they cannot classify with 100% certainty, and models trained on standard vision benchmarks achieve low performance when evaluated on this data. To address this issue, we introduce a procedure for creating datasets of ambiguous images and use it to produce SQUID-E (""Squidy""), a collection of noisy images extracted from videos. All images are annotated with ground truth values and a test set is annotated with human uncertainty judgments. We use this dataset to characterize human uncertainty in vision tasks and evaluate existing visual event classification models. Experimental results suggest that existing vision models are not sufficiently equipped to provide meaningful outputs for ambiguous images and that datasets of this nature can be used to assess and improve such models through model training and direct evaluation of model calibration. These findings motivate large-scale ambiguous dataset creation and further research focusing on noisy visual data.",Datasets & Benchmarks,NeurIPS,2022,Poster,Kate Sanders;Reno Kriz;Anqi Liu;Benjamin Van Durme,True,https://openreview.net/pdf?id=6Hl7XoPNAVX
Proso5bUa,Flare7K: A Phenomenological Nighttime Flare Removal Dataset,"Artificial lights commonly leave strong lens flare artifacts on images captured at night. Nighttime flare not only affects the visual quality but also degrades the performance of vision algorithms. Existing flare removal methods mainly focus on removing daytime flares and fail in nighttime. Nighttime flare removal is challenging because of the unique luminance and spectrum of artificial lights and the diverse patterns and image degradation of the flares captured at night. The scarcity of nighttime flare removal datasets limits the research on this crucial task. In this paper, we introduce, Flare7K, the first nighttime flare removal dataset, which is generated based on the observation and statistics of real-world nighttime lens flares. It offers 5,000 scattering and 2,000 reflective flare images, consisting of 25 types of scattering flares and 10 types of reflective flares. The 7,000 flare patterns can be randomly added to flare-free images, forming the flare-corrupted and flare-free image pairs. With the paired data, we can train deep models to restore flare-corrupted images taken in the real world effectively. Apart from abundant flare patterns, we also provide rich annotations, including the labeling of light source, glare with shimmer, reflective flare, and streak, which are commonly absent from existing datasets. Hence, our dataset can facilitate new work in nighttime flare removal and more fine-grained analysis of flare patterns. Extensive experiments show that our dataset adds diversity to existing flare datasets and pushes the frontier of nighttime flare removal.",Datasets & Benchmarks,NeurIPS,2022,Poster,Yuekun Dai;Chongyi Li;Shangchen Zhou;Ruicheng Feng;Chen Change Loy,True,https://openreview.net/pdf?id=Proso5bUa
qiDmAaG6mP,"M4Singer: A Multi-Style, Multi-Singer and Musical Score Provided Mandarin Singing Corpus","The lack of publicly available high-quality and accurately labeled datasets has long been a major bottleneck for singing voice synthesis (SVS). To tackle this problem, we present M4Singer, a free-to-use Multi-style, Multi-singer Mandarin singing collection with elaborately annotated Musical scores as well as its benchmarks. Specifically, 1) we construct and release a large high-quality Chinese singing voice corpus, which is recorded by 20 professional singers, covering 700 Chinese pop songs as well as all the four SATB types (i.e.,  soprano, alto, tenor, and bass); 2) we take extensive efforts to manually compose the musical scores for each recorded song, which are necessary to the study of the prosody modeling for SVS. 3) To facilitate the use and demonstrate the quality of M4Singer, we conduct four different benchmark experiments: score-based SVS, controllable singing voice (CSV), singing voice conversion (SVC) and automatic music transcription (AMT).",Datasets & Benchmarks,NeurIPS,2022,Poster,Lichao Zhang;Ruiqi Li;Shoutong Wang;Liqun Deng;Jinglin Liu;Yi Ren;Jinzheng He;Rongjie Huang;Jieming Zhu;Xiao Chen;Zhou Zhao,True,https://openreview.net/pdf?id=qiDmAaG6mP
Bs8iFQ7AM6,DC-BENCH: Dataset Condensation Benchmark,"Dataset Condensation is a newly emerging technique aiming at learning a tiny dataset that captures the rich information encoded in the original dataset. As the size of datasets contemporary machine learning models rely on becomes increasingly large, condensation methods become a prominent direction for accelerating network training and reducing data storage. Despite numerous methods have been proposed in this rapidly growing field, evaluating and comparing different condensation methods is non-trivial and still remains an open issue. 
The quality of condensed dataset are often shadowed by many critical contributing factors to the end performance, such as data augmentation and model architectures. The lack of a systematic way to evaluate and compare condensation methods not only hinders our understanding of existing techniques, but also discourages practical usage of the synthesized datasets. This work provides the first large-scale standardized benchmark on Dataset Condensation. It consists of a suite of evaluations to comprehensively reflect the generability and effectiveness of condensation methods through the lens of their generated dataset. Leveraging this benchmark, we conduct a large-scale study of current condensation methods, and report many insightful findings that open up new possibilities for future development. The benchmark library, including evaluators, baseline methods, and generated datasets, is open-sourced to facilitate future research and application.",Datasets & Benchmarks,NeurIPS,2022,Poster,Justin Cui;Ruochen Wang;Si Si;Cho-Jui Hsieh,True,https://openreview.net/pdf?id=Bs8iFQ7AM6
BkMGK9dv2Z9,pyKT: A Python Library to Benchmark Deep Learning based Knowledge Tracing Models,"Knowledge tracing (KT) is the task of using students' historical learning interaction data to model their knowledge mastery over time so as to make predictions on their future interaction performance. Recently, remarkable progress has been made of using various deep learning techniques to solve the KT problem. However, the success behind deep learning based knowledge tracing (DLKT) approaches is still left somewhat unknown and proper measurement and analysis of these DLKT approaches remain a challenge. First, data preprocessing procedures in existing works are often private and custom, which limits experimental standardization. Furthermore, existing DLKT studies often differ in terms of the evaluation protocol and are far away real-world educational contexts. To address these problems, we introduce a comprehensive python based benchmark platform, \\\\textsc{pyKT}, to guarantee valid comparisons across DLKT methods via thorough evaluations. The \\\\textsc{pyKT} library consists of a standardized set of integrated data preprocessing procedures on 7 popular datasets across different domains, and 10 frequently compared DLKT model implementations for transparent experiments. Results from our fine-grained and rigorous empirical KT studies yield a set of observations and suggestions for effective DLKT, e.g., wrong evaluation setting may cause label leakage that generally leads to performance inflation; and the improvement of many DLKT approaches is minimal compared to the very first DLKT model proposed by Piech et al. \\\\cite{piech2015deep}. We have open sourced \\\\textsc{pyKT} and our experimental results at \\\\url{https://pykt.org/}. We welcome contributions from other research groups and practitioners.",Datasets & Benchmarks,NeurIPS,2022,Poster,Zitao Liu;Qiongqiong Liu;Jiahao Chen;Shuyan Huang;Jiliang Tang;Weiqi Luo,False,https://openreview.net/pdf?id=BkMGK9dv2Z9
gud0qopqJc4,SkinCon: A skin disease dataset densely annotated by domain experts for fine-grained debugging and analysis,"For the deployment of artificial intelligence (AI) in high risk settings, such as healthcare, methods that provide interpretability/explainability or allow fine-grained error analysis are critical. Many recent methods for interpretability/explainability and fine-grained error analysis use concepts, which are meta-labels which are semantically meaningful to humans.  However, there are only a few datasets that include concept-level meta-labels and most of these meta-labels are relevant for natural images that do not require domain expertise. Previous densely annotated datasets in medicine focused on meta-labels that are relevant to a single disease such as osteoarthritis or melanoma. In dermatology, skin disease is described using an established clinical lexicon that allow clinicians to describe physical exam findings to one another. To provide the first medical dataset densely annotated by domain experts to provide annotations useful across multiple disease processes, we developed SkinCon: a skin disease dataset densely annotated by dermatologists. SkinCon includes 3230 images from the Fitzpatrick 17k skin disease dataset densely annotated with 48 clinical concepts, 22 of which have at least 50 images representing the concept. The concepts used were chosen by two dermatologists considering the clinical descriptor terms used to describe skin lesions. Examples include ""plaque"", ""scale"", and ""erosion"". These same concepts were also used to label 656 skin disease images from the Diverse Dermatology Images dataset, providing an additional external dataset with diverse skin tone representations. We review the potential applications for the SkinCon dataset, such as probing models, concept-based explanations, concept bottlenecks, error analysis, and slice discovery. Furthermore, we use SkinCon to demonstrate two of these use cases: debugging mistakes of an existing dermatology AI model with concepts and developing interpretable models with post-hoc concept bottleneck models.",Datasets & Benchmarks,NeurIPS,2022,Poster,Roxana Daneshjou;Mert Yuksekgonul;Zhuo Ran Cai;Roberto A. Novoa;James Zou,True,https://openreview.net/pdf?id=gud0qopqJc4
ChWo6qLgILf,SoundSpaces 2.0: A Simulation Platform for Visual-Acoustic Learning,"We introduce SoundSpaces 2.0, a platform for on-the-fly geometry-based audio rendering for 3D environments. Given a 3D mesh of a real-world environment, SoundSpaces can generate highly realistic acoustics for arbitrary sounds captured from arbitrary microphone locations. Together with existing 3D visual assets, it supports an array of audio-visual research tasks, such as audio-visual navigation, mapping, source localization and separation, and acoustic matching. Compared to existing resources, SoundSpaces 2.0 has the advantages of allowing continuous spatial sampling, generalization to novel environments, and configurable microphone and material properties. To our knowledge, this is the first geometry-based acoustic simulation that offers high fidelity and realism while also being fast enough to use for embodied learning. We showcase the simulator's properties and  benchmark its performance against real-world audio measurements. In addition, we demonstrate two downstream tasks---embodied navigation and far-field automatic speech recognition---and highlight sim2real performance for the latter. SoundSpaces 2.0 is publicly available to facilitate wider research for perceptual systems that can both see and hear.",Datasets & Benchmarks,NeurIPS,2022,Poster,Changan Chen;Carl Schissler;Sanchit Garg;Philip Kobernik;Alexander Clegg;Paul Calamia;Dhruv Batra;Philip W Robinson;Kristen Grauman,False,https://openreview.net/pdf?id=ChWo6qLgILf
mZke5vYdF99,OLIVES Dataset: Ophthalmic Labels for Investigating Visual Eye Semantics,"Clinical diagnosis of the eye is performed over multifarious data modalities including scalar clinical labels, vectorized biomarkers, two-dimensional fundus images, and three-dimensional Optical Coherence Tomography (OCT) scans. Clinical practitioners use all available data modalities for diagnosing and treating eye diseases like Diabetic Retinopathy (DR) or Diabetic Macular Edema (DME). Enabling usage of machine learning algorithms within the ophthalmic medical domain requires research into the relationships and interactions between all relevant data over a treatment period. Existing datasets are limited in that they neither provide data nor consider the explicit relationship modeling between the data modalities. In this paper, we introduce the Ophthalmic Labels for Investigating Visual Eye Semantics (OLIVES) dataset that addresses the above limitation. This is the first OCT and near-IR fundus dataset that includes clinical labels, biomarker labels, disease labels, and time-series patient treatment information from associated clinical trials. The dataset consists of 1268 near-IR fundus images each with at least 49 OCT scans, and 16 biomarkers, along with 4 clinical labels and a disease diagnosis of DR or DME. In total, there are 96 eyes' data averaged over a period of at least two years with each eye treated for an average of 66 weeks and 7 injections. We benchmark the utility of OLIVES dataset for ophthalmic data as well as provide benchmarks and concrete research directions for core and emerging machine learning paradigms within medical image analysis.",Datasets & Benchmarks,NeurIPS,2022,Poster,Mohit Prabhushankar;Kiran Premdat Kokilepersaud;Yash-yee Logan;Stephanie Trejo Corona;Ghassan AlRegib;Charles Wykoff,True,https://openreview.net/pdf?id=mZke5vYdF99
Kyswf8Kj83,TwiBot-22: Towards Graph-Based Twitter Bot Detection,"Twitter bot detection has become an increasingly important task to combat misinformation, facilitate social media moderation, and preserve the integrity of the online discourse. State-of-the-art bot detection methods generally leverage the graph structure of the Twitter network, and they exhibit promising performance when confronting novel Twitter bots that traditional methods fail to detect. However, very few of the existing Twitter bot detection datasets are graph-based, and even these few graph-based datasets suffer from limited dataset scale, incomplete graph structure, as well as low annotation quality. In fact, the lack of a large-scale graph-based Twitter bot detection benchmark that addresses these issues has seriously hindered the development and evaluation of novel graph-based bot detection approaches. In this paper, we propose TwiBot-22, a comprehensive graph-based Twitter bot detection benchmark that presents the largest dataset to date, provides diversified entities and relations on the Twitter network, and has considerably better annotation quality than existing datasets. In addition, we re-implement 35 representative Twitter bot detection baselines and evaluate them on 9 datasets, including TwiBot-22, to promote a fair comparison of model performance and a holistic understanding of research progress. To facilitate further research, we consolidate all implemented codes and datasets into the TwiBot-22 evaluation framework, where researchers could consistently evaluate new models and datasets. The TwiBot-22 Twitter bot detection benchmark and evaluation framework are publicly available at \\\\url{https://twibot22.github.io/}.",Datasets & Benchmarks,NeurIPS,2022,Poster,Shangbin Feng;Zhaoxuan Tan;Herun Wan;Ningnan Wang;Zilong Chen;Binchi Zhang;Qinghua Zheng;Wenqian Zhang;Zhenyu Lei;Shujie Yang;Xinshun Feng;Qingyue Zhang;Hongrui Wang;Yuhan Liu;Yuyang Bai;Heng Wang;Zijian Cai;Yanbo Wang;Lijing Zheng;Zihan Ma;Jundong Li;Minnan Luo,True,https://openreview.net/pdf?id=Kyswf8Kj83
7w-a8PYPlP,OpenFWI: Large-scale Multi-structural Benchmark Datasets for Full Waveform Inversion,"Full waveform inversion (FWI) is widely used in geophysics to reconstruct high-resolution velocity maps from seismic data. The recent success of data-driven FWI methods results in a rapidly increasing demand for open datasets to serve the geophysics community. We present OpenFWI, a collection of large-scale multi-structural benchmark datasets, to facilitate diversified, rigorous, and reproducible research on FWI. In particular, OpenFWI consists of $12$ datasets ($2.1$TB in total) synthesized from multiple sources. It encompasses diverse domains in geophysics (interface, fault, CO$_2$ reservoir, etc.), covers different geological subsurface structures (flat, curve, etc.), and contain various amounts of data samples (2K - 67K). It also includes a dataset for 3D FWI. Moreover, we use OpenFWI to perform benchmarking over four deep learning methods, covering both supervised and unsupervised learning regimes. Along with the benchmarks, we implement additional experiments, including physics-driven methods, complexity analysis, generalization study, uncertainty quantification, and so on, to sharpen our understanding of datasets and methods. The studies either provide valuable insights into the datasets and the performance, or uncover their current limitations. We hope OpenFWI supports prospective research on FWI and inspires future open-source efforts on AI for science. All datasets and related information can be accessed through our website at https://openfwi-lanl.github.io/",Datasets & Benchmarks,NeurIPS,2022,Poster,Chengyuan Deng;Shihang Feng;Hanchen Wang;Xitong Zhang;Peng Jin;Yinan Feng;Qili Zeng;Yinpeng Chen;Youzuo Lin,True,https://openreview.net/pdf?id=7w-a8PYPlP
NAYoSV3tk9,VLMbench: A Compositional Benchmark for Vision-and-Language Manipulation,"Benefiting from language flexibility and compositionality, humans naturally intend to use language to command an embodied agent for complex tasks such as navigation and object manipulation. In this work, we aim to fill the blank of the last mile of embodied agents---object manipulation by following human guidance, e.g., “move the red mug next to the box while keeping it upright.” To this end, we introduce an Automatic Manipulation Solver (AMSolver) system and build a Vision-and-Language Manipulation benchmark (VLMbench) based on it, containing various language instructions on categorized robotic manipulation tasks. Specifically, modular rule-based task templates are created to automatically generate robot demonstrations with language instructions, consisting of diverse object shapes and appearances, action types, and motion constraints. We also develop a keypoint-based model 6D-CLIPort to deal with multi-view observations and language input and output a sequence of 6 degrees of freedom (DoF) actions. We hope the new simulator and benchmark will facilitate future research on language-guided robotic manipulation.",Datasets & Benchmarks,NeurIPS,2022,Poster,Kaizhi Zheng;Xiaotong Chen;Odest Jenkins;Xin Eric Wang,True,https://openreview.net/pdf?id=NAYoSV3tk9
tXEe-Ew_ikh,Unravelling the Performance of Physics-informed Graph Neural Networks for Dynamical Systems,"Recently, graph neural networks have been gaining a lot of attention to simulate dynamical systems due to their inductive nature leading to zero-shot generalizability. Similarly, physics-informed inductive biases in deep-learning frameworks have been shown to give superior performance in learning the dynamics of physical systems. There is a growing volume of literature that attempts to combine these two approaches. Here, we evaluate the performance of thirteen different graph neural networks, namely, Hamiltonian and Lagrangian graph neural networks, graph neural ODE, and their variants with explicit constraints and different architectures. We briefly explain the theoretical formulation highlighting the similarities and differences in the inductive biases and graph architecture of these systems. Then, we evaluate them on spring, pendulum, and gravitational and 3D deformable solid systems to compare the performance in terms of rollout error, conserved quantities such as energy and momentum, and generalizability to unseen system sizes. Our study demonstrates that GNNs with additional inductive biases, such as explicit constraints and decoupling of kinetic and potential energies, exhibit significantly enhanced performance. Further, all the physics-informed GNNs exhibit zero-shot generalizability to system sizes an order of magnitude larger than the training system, thus providing a promising route to simulate large-scale realistic systems.",Datasets & Benchmarks,NeurIPS,2022,Poster,Abishek Thangamuthu;Gunjan Kumar;Suresh Bishnoi;Ravinder Bhattoo;N M Anoop Krishnan;Sayan Ranu,False,https://openreview.net/pdf?id=tXEe-Ew_ikh
X2dHozbd1at,3DOS: Towards 3D Open Set Learning - Benchmarking and Understanding Semantic Novelty Detection on Point Clouds,"In recent years there has been significant progress in the field of 3D learning on classification, detection and segmentation problems. The vast majority of the existing studies focus on canonical closed-set conditions, neglecting the intrinsic open nature of the real-world. This limits the abilities of robots and autonomous systems involved in safety-critical applications that require managing novel and unknown signals. In this context exploiting 3D data can be a valuable asset since it provides rich information about the geometry of perceived objects and scenes. 
With this paper we provide the first broad study on 3D Open Set learning. We introduce 3DOS: a novel testbed for semantic novelty detection that considers several settings with increasing difficulties in terms of semantic (category) shift, and covers both in-domain (synthetic-to-synthetic, real-to-real) and cross-domain (synthetic-to-real) scenarios. Moreover, we investigate the related 2D Open Set literature to understand if and how its recent improvements are effective on 3D data. Our extensive benchmark positions several algorithms in the same coherent picture, revealing their strengths and limitations. The results of our analysis may serve as a reliable foothold for future tailored 3D Open Set methods.",Datasets & Benchmarks,NeurIPS,2022,Poster,Antonio Alliegro;Francesco Cappio Borlino;Tatiana Tommasi,True,https://openreview.net/pdf?id=X2dHozbd1at
47qVX2pa-2,A new dataset for multilingual keyphrase generation,"  Keyphrases  are an important tool for efficiently dealing with the ever-increasing amount of information present on the internet. While there are many recent papers on English keyphrase generation, keyphrase generation for other languages remains vastly understudied, mostly due to the absence of datasets. To address this, we present a novel dataset called Papyrus, composed of 16427 pairs of abstracts and keyphrases. We release four versions of this dataset, corresponding to different subtasks. Papyrus-e considers only English keyphrases, Papyrus-f considers French keyphrases, Papyrus-m considers keyphrase generation in any language (mostly French and English), and Papyrus-a considers keyphrase generation in several languages. We train a state-of-the-art model on all four tasks and show that they lead to better results for non-English languages, with an average improvement of 14.2\\\\% on keyphrase extraction and 2.0\\\\% on generation. We also show an improvement of 0.4\\\\% on extraction and 0.7\\\\% on generation over English state-of-the-art results by concatenating Papyrus-e with the Kp20K training set.",Datasets & Benchmarks,NeurIPS,2022,Poster,Frédéric Piedboeuf;Philippe Langlais,True,https://openreview.net/pdf?id=47qVX2pa-2
u46CbCaLufp,Characteristics of Harmful Text: Towards Rigorous Benchmarking of Language Models,"Large language models produce human-like text that drive a growing number of applications.  However, recent literature and, increasingly, real world observations, have demonstrated that these models can generate language that is toxic, biased, untruthful or otherwise harmful. 
 Though work to evaluate language model harms is under way, translating foresight about which harms may arise into rigorous benchmarks is not straightforward.  To facilitate this translation, we outline six ways of characterizing harmful text which merit explicit consideration when designing new benchmarks.  We then use these characteristics as a lens to identify trends and gaps in existing benchmarks. Finally, we apply them in a case study of the Perspective API, a toxicity classifier that is widely used in harm benchmarks.  Our characteristics provide one piece of the bridge that translates between foresight and effective evaluation.",Datasets & Benchmarks,NeurIPS,2022,Poster,Maribeth Rauh;John F J Mellor;Jonathan Uesato;Po-Sen Huang;Johannes Welbl;Laura Weidinger;Sumanth Dathathri;Amelia Glaese;Geoffrey Irving;Iason Gabriel;William Isaac;Lisa Anne Hendricks,False,https://openreview.net/pdf?id=u46CbCaLufp
FhqzyGoTSH,CLiMB: A Continual Learning Benchmark for Vision-and-Language Tasks,"Current state-of-the-art vision-and-language models are evaluated on tasks either individually or in a multi-task setting, overlooking the challenges of continually learning (CL) tasks as they arrive. Existing CL benchmarks have facilitated research on task adaptation and mitigating ""catastrophic forgetting"", but are limited to vision-only and language-only tasks. We present CLiMB, a benchmark to study the challenge of learning multimodal tasks in a CL setting, and to systematically evaluate how upstream continual learning can rapidly generalize to new multimodal and unimodal tasks. CLiMB includes implementations of several CL algorithms and a modified Vision-Language Transformer (ViLT) model that can be deployed on both multimodal and unimodal tasks. We find that common CL methods can help mitigate forgetting during multimodal task learning, but do not enable cross-task knowledge transfer. We envision that CLiMB will facilitate research on a new class of CL algorithms for this challenging multimodal setting.",Datasets & Benchmarks,NeurIPS,2022,Poster,Tejas Srinivasan;Ting-Yun Chang;Leticia Leonor Pinto Alva;Georgios Chochlakis;Mohammad Rostami;Jesse Thomason,False,https://openreview.net/pdf?id=FhqzyGoTSH
ml1NjI-ujzf,Active-Passive SimStereo - Benchmarking the Cross-Generalization Capabilities of Deep Learning-based Stereo Methods,"In stereo vision, self-similar or bland regions can make it difficult to match patches between two images. Active stereo-based methods mitigate this problem by projecting a pseudo-random pattern on the scene so that each patch of an image pair can be identified without ambiguity. However, the projected pattern significantly alters the appearance of the image. If this pattern acts as a form of adversarial noise, it could negatively impact the performance of deep learning-based methods, which are now the de-facto standard for dense stereo vision. In this paper, we propose the Active-Passive SimStereo dataset and a corresponding benchmark to evaluate the performance gap between passive and active stereo images for stereo matching algorithms. Using the proposed benchmark and an additional ablation study, we show that the feature extraction and matching modules of a selection of twenty selected deep learning-based stereo matching methods generalize to active stereo without a problem. However, the disparity refinement modules of three of the twenty architectures (ACVNet, CascadeStereo, and StereoNet) are negatively affected by the active stereo patterns due to their reliance on the appearance of the input images.

",Datasets & Benchmarks,NeurIPS,2022,Poster,Laurent Valentin Jospin;Allen Antony;Lian Xu;Hamid Laga;Farid Boussaid;Mohammed Bennamoun,True,https://openreview.net/pdf?id=ml1NjI-ujzf
A79jAS4MeW9,Robustness Analysis of Video-Language Models Against Visual and Language Perturbations,"Joint visual and language modeling on large-scale datasets has recently shown good progress in multi-modal tasks when compared to single modal learning. However, robustness of these  approaches against real-world perturbations has not been studied. In this work, we perform the first extensive robustness study of video-language models against various real-world perturbations. We focus on text-to-video retrieval and propose two large-scale benchmark datasets, MSRVTT-P and YouCook2-P, which utilize 90 different visual and 35 different text perturbations. The study reveals some interesting initial findings from the studied models: 1) models are more robust when text is perturbed versus when video is perturbed, 2) models that are pre-trained are more robust than those trained from scratch, 3) models attend more to scene and objects rather than motion and action. We hope this study will serve as a benchmark and guide future research in robust video-language learning. The benchmark introduced in this study along with the code and datasets is available at https://bit.ly/3CNOly4.",Datasets & Benchmarks,NeurIPS,2022,Poster,Madeline Chantry Schiappa;Shruti Vyas;Hamid Palangi;Yogesh S Rawat;Vibhav Vineet,True,https://openreview.net/pdf?id=A79jAS4MeW9
1kIZiRelqFt,FLAIR: Federated Learning Annotated Image Repository,"Cross-device federated learning is an emerging machine learning (ML) paradigm where a large population of devices collectively train an ML model while the data remains on the devices.
This research field has a unique set of practical challenges, and to systematically make advances, new datasets curated to be compatible with this paradigm are needed.
Existing federated learning benchmarks in the image domain do not accurately capture the scale and heterogeneity of many real-world use cases. 
We introduce FLAIR, a challenging large-scale annotated image dataset for multi-label classification suitable for federated learning.
FLAIR has 429,078 images from  51,414  Flickr users and captures many of the intricacies typically encountered in federated learning, such as heterogeneous user data and a long-tailed label distribution.
We implement multiple baselines in different learning setups for different tasks on this dataset. 
We believe FLAIR can serve as a challenging benchmark for advancing the state-of-the art in federated learning.
Dataset access and the code for the benchmark are available at https://github.com/apple/ml-flair.
",Datasets & Benchmarks,NeurIPS,2022,Poster,Congzheng Song;Filip Granqvist;Kunal Talwar,True,https://openreview.net/pdf?id=1kIZiRelqFt
rbrouCKPiej,AnoShift: A Distribution Shift Benchmark for Unsupervised Anomaly Detection,"Analyzing the distribution shift of data is a growing research direction in nowadays Machine Learning (ML), leading to emerging new benchmarks that focus on providing a suitable scenario for studying the generalization properties of ML models. The existing benchmarks are focused on supervised learning, and to the best of our knowledge, there is none for unsupervised learning. Therefore, we introduce an unsupervised anomaly detection benchmark with data that shifts over time, built over Kyoto-2006+, a traffic dataset for network intrusion detection. This type of data meets the premise of shifting the input distribution: it covers a large time span (10 years), with naturally occurring changes over time (e.g. users modifying their behavior patterns, and software updates). We first highlight the non-stationary nature of the data, using a basic per-feature analysis, t-SNE, and an Optimal Transport approach for measuring the overall distribution distances between years. Next, we propose AnoShift, a protocol splitting the data in IID, NEAR, and FAR testing splits. We validate the performance degradation over time with diverse models, ranging from classical approaches to deep learning. Finally, we show that by acknowledging the distribution shift problem and properly addressing it, the performance can be improved compared to the classical training which assumes independent and identically distributed data (on average, by up to 3% for our approach). Dataset and code are available at https://github.com/bit-ml/AnoShift/.",Datasets & Benchmarks,NeurIPS,2022,Poster,Marius Dragoi;Elena Burceanu;Emanuela Haller;Andrei Manolache;Florin Brad,False,https://openreview.net/pdf?id=rbrouCKPiej
sWOdnSkB0qu,MoCapAct: A Multi-Task Dataset for Simulated Humanoid Control,"Simulated humanoids are an appealing research domain due to their physical capabilities. Nonetheless, they are also challenging to control, as a policy must drive an unstable, discontinuous, and high-dimensional physical system. One widely studied approach is to utilize motion capture (MoCap) data to teach the humanoid agent low-level skills (e.g., standing, walking, and running) that can then be re-used to synthesize high-level behaviors. However, even with MoCap data, controlling simulated humanoids remains very hard, as MoCap data offers only kinematic information. Finding physical control inputs to realize the demonstrated motions requires computationally intensive methods like reinforcement learning. Thus, despite the publicly available MoCap data, its utility has been limited to institutions with large-scale compute. In this work, we dramatically lower the barrier for productive research on this topic by training and releasing high-quality agents that can track over three hours of MoCap data for a simulated humanoid in the dm_control physics-based environment. We release MoCapAct (Motion Capture with Actions), a dataset of these expert agents and their rollouts, which contain proprioceptive observations and actions. We demonstrate the utility of MoCapAct by using it to train a single hierarchical policy capable of tracking the entire MoCap dataset within dm_control and show the learned low-level component can be re-used to efficiently learn downstream high-level tasks. Finally, we use MoCapAct to train an autoregressive GPT model and show that it can control a simulated humanoid to perform natural motion completion given a motion prompt.

Videos of the results and links to the code and dataset are available at https://microsoft.github.io/MoCapAct.",Datasets & Benchmarks,NeurIPS,2022,Poster,Nolan Wagener;Andrey Kolobov;Felipe Vieira Frujeri;Ricky Loynd;Ching-An Cheng;Matthew Hausknecht,True,https://openreview.net/pdf?id=sWOdnSkB0qu
yWhuIjIjH8k,NAS-Bench-Suite-Zero: Accelerating Research on Zero Cost Proxies,"Zero-cost proxies (ZC proxies) are a recent architecture performance prediction technique aiming to significantly speed up algorithms for neural architecture search (NAS). Recent work has shown that these techniques show great promise, but certain aspects, such as evaluating and exploiting their complementary strengths, are under-studied. In this work, we create NAS-Bench-Suite: we evaluate 13 ZC proxies across 28 tasks, creating by far the largest dataset (and unified codebase) for ZC proxies, enabling orders-of-magnitude faster experiments on ZC proxies, while avoiding confounding factors stemming from different implementations. To demonstrate the usefulness of NAS-Bench-Suite, we run a large-scale analysis of ZC proxies, including a bias analysis, and the first information-theoretic analysis which concludes that ZC proxies capture substantial complementary information. Motivated by these findings, we present a procedure to improve the performance of ZC proxies by reducing biases such as cell size, and we also show that incorporating all 13 ZC proxies into the surrogate models used by NAS algorithms can improve their predictive performance by up to 42%. Our code and datasets are available at https://github.com/automl/naslib/tree/zerocost.",Datasets & Benchmarks,NeurIPS,2022,Poster,Arjun Krishnakumar;Colin White;Arber Zela;Renbo Tu;Mahmoud Safari;Frank Hutter,True,https://openreview.net/pdf?id=yWhuIjIjH8k
F9ENmZABB0,Wild-Time: A Benchmark of in-the-Wild Distribution Shift over Time,"Distribution shifts occur when the test distribution differs from the training distribution, and can considerably degrade performance of machine learning models deployed in the real world. While recent works have studied robustness to distribution shifts, distribution shifts arising from the passage of time have the additional structure of timestamp metadata. Real-world examples of such shifts are underexplored, and it is unclear whether existing models can leverage trends in past distribution shifts to reliably extrapolate into the future. To address this gap, we curate Wild-Time, a benchmark of 5 datasets that reflect temporal distribution shifts arising in a variety of real-world applications, including drug discovery, patient prognosis, and news classification. On these datasets, we systematically benchmark 13 approaches with various inductive biases. We evaluate methods in domain-generalization, continual learning, self-supervised learning, and ensemble learning, which leverage timestamps to extract the common structure of the distribution shifts. We extend several domain-generalization methods to the temporal distribution shift setting by treating windows of time as different domains. Finally, we propose two evaluation strategies to evaluate model performance under temporal distribution shifts---evaluation with a fixed time split (Eval-Fix) and evaluation with a data stream (Eval-Stream). Eval-Fix, our primary evaluation strategy, aims to provide a simple evaluation protocol for the broader machine learning community, while Eval-Stream serves as a complementary benchmark for continual learning approaches. Our experiments demonstrate that existing methods are limited in tackling temporal distribution shift: across all settings, we observe an average performance drop of 20% from in-distribution to out-of-distribution data.",Datasets & Benchmarks,NeurIPS,2022,Poster,Huaxiu Yao;Caroline Choi;Bochuan Cao;Yoonho Lee;Pang Wei Koh;Chelsea Finn,True,https://openreview.net/pdf?id=F9ENmZABB0
5wNiiIDynDF,CGLB: Benchmark Tasks for Continual Graph Learning,"Continual learning on graph data, which aims to accommodate new tasks over newly emerged graph data while maintaining the model performance over existing tasks, is attracting increasing attention from the community. Unlike continual learning on Euclidean data ($\\\\textit{e.g.}$, images, texts, etc.) that has established benchmarks and unified experimental settings, benchmark tasks are rare for Continual Graph Learning (CGL). Moreover, due to the variety of graph data and its complex topological structures, existing works adopt different protocols to configure datasets and experimental settings. This creates a great obstacle to compare different techniques and thus hinders the development of CGL. To this end, we systematically study the task configurations in different application scenarios and develop a comprehensive Continual Graph Learning Benchmark (CGLB) curated from different public datasets. Specifically, CGLB contains both node-level and graph-level continual graph learning tasks under task-incremental (currently widely adopted) and class-incremental (more practical, challenging, yet underexplored) settings, as well as a toolkit for training, evaluating, and visualizing different CGL methods. Within CGLB, we also systematically explain the difference among these task configurations by comparing them to classical continual learning settings. Finally, we comprehensively compare state-of-the-art baselines on CGLB to investigate their effectiveness. Given CGLB and the developed toolkit, the barrier to exploring CGL has been greatly lowered and researchers can focus more on the model development without worrying about tedious work on pre-processing of datasets or encountering unseen pitfalls. The benchmark and the toolkit are available through https://github.com/QueuQ/CGLB.",Datasets & Benchmarks,NeurIPS,2022,Poster,Xikun ZHANG;Dongjin Song;Dacheng Tao,False,https://openreview.net/pdf?id=5wNiiIDynDF
3lk54yE2tYJ,Geoclidean: Few-Shot Generalization in Euclidean Geometry,"Euclidean geometry is among the earliest forms of mathematical thinking. While the geometric primitives underlying its constructions, such as perfect lines and circles, do not often occur in the natural world, humans rarely struggle to perceive and reason with them. Will computer vision models trained on natural images show the same sensitivity to Euclidean geometry? Here we explore these questions by studying few-shot generalization in the universe of Euclidean geometry constructions. We introduce Geoclidean, a domain-specific language for Euclidean geometry, and use it to generate two datasets of geometric concept learning tasks for benchmarking generalization judgements of humans and machines. We find that humans are indeed sensitive to Euclidean geometry and generalize strongly from a few visual examples of a geometric concept. In contrast, low-level and high-level visual features from standard computer vision models pretrained on natural images do not support correct generalization. Thus Geoclidean represents a novel few-shot generalization benchmark for geometric concept learning, where the performance of humans and of AI models diverge. The Geoclidean framework and dataset are publicly available for download.
",Datasets & Benchmarks,NeurIPS,2022,Poster,Joy Hsu;Jiajun Wu;Noah Goodman,True,https://openreview.net/pdf?id=3lk54yE2tYJ
kQUOIyPg-ux,SurDis: A Surface Discontinuity Dataset for Wearable Technology to Assist Blind Navigation in Urban Environments,"According to World Health Organization, there is an estimated 2.2 billion people with a near or distance vision impairment worldwide. Difficulty in self-navigation is one of the greatest challenges to independence for the blind and low vision (BLV) people. Through consultations with several BLV service providers, we realized that negotiating surface discontinuities is one of the very prominent challenges when navigating an outdoor environment within the urban. Surface discontinuities are commonly formed by rises and drop-offs along a pathway. They could be a threat to balancing during a walk and perceiving such a threat is highly challenging to the BLVs. In this paper, we introduce SurDis, a novel dataset of depth maps and stereo images that exemplifies the issue of surface discontinuity in the urban areas of Klang Valley, Malaysia. We seek to address the limitation of existing datasets of such nature in these areas. Current mobility tools for the BLVs predominantly focus on furniture, indoor built environments, traffic signs, vehicles, humans and various types of objects' detection above the surface of a pathway. We emphasize a specific purpose for SurDis – to support the development of assistive wearable technology for the BLVs to negotiate surface discontinuity. We consulted BLV volunteers on the specifications of surface condition that could become hazardous for navigation using 3D printed replicas of actual scaled-down scenes, and identified locations that are frequented by the BLVs as our target data collection fields. With feedback from these volunteers, we developed a lightweight, small and unobtrusive prototype equipped with a tiny stereo camera and an embedded system on a single board computer to capture the samples from 10 different locations. We describe instrument development, data collection, preprocessing, annotation, and experiments conducted. The dataset contains: (1) more than 17000 depth maps generated from 200 sets of stereo image sequences, (2) annotations of surface discontinuity in the depth maps, and (3) bitmap stereo image pairs corresponding to the depth maps in (1).",Datasets & Benchmarks,NeurIPS,2022,Poster,Kuan Yew Leong;Siew Mooi Lim,True,https://openreview.net/pdf?id=kQUOIyPg-ux
YmacJv0i_UR,GriddlyJS: A Web IDE for Reinforcement Learning,"Progress in reinforcement learning (RL) research is often driven by the design of new, challenging environments---a costly undertaking requiring skills orthogonal to that of a typical machine learning researcher. The complexity of environment development has only increased with the rise of procedural-content generation (PCG) as the prevailing paradigm for producing varied environments capable of testing the robustness and generalization of RL agents. Moreover, existing environments often require complex build processes, making reproducing results difficult. To address these issues, we introduce GriddlyJS, a web-based Integrated Development Environment (IDE) based on the Griddly engine. GriddlyJS allows researchers to easily design and debug arbitrary, complex PCG grid-world environments, as well as visualize, evaluate, and record the performance of trained agent models. By connecting the RL workflow to the advanced functionality enabled by modern web standards, GriddlyJS allows publishing interactive agent-environment demos that reproduce experimental results directly to the web. To demonstrate the versatility of GriddlyJS, we use it to quickly develop a complex compositional puzzle-solving environment alongside arbitrary human-designed environment configurations and their solutions for use in a automatic curriculum learning and offline RL context. The GriddlyJS IDE is open source and freely available at https://griddly.ai.",Datasets & Benchmarks,NeurIPS,2022,Poster,Christopher Bamford;Minqi Jiang;Mikayel Samvelyan;Tim Rocktäschel,False,https://openreview.net/pdf?id=YmacJv0i_UR
w7VPQWgnn3s,Pythae: Unifying Generative Autoencoders in Python - A Benchmarking Use Case,"In recent years, deep generative models have attracted increasing interest due to their capacity to model complex distributions. Among those models, variational autoencoders have gained popularity as they have proven both to be computationally efficient and yield impressive results in multiple fields. Following this breakthrough, extensive research has been done in order to improve the original publication, resulting in a variety of different VAE models in response to different tasks. In this paper we present \\\\textbf{Pythae}, a versatile \\\\textit{open-source} Python library providing both a \\\\textit{unified implementation} and a dedicated framework allowing \\\\textit{straightforward}, \\\\emph{reproducible} and \\\\textit{reliable} use of generative autoencoder models. We then propose to use this library to perform a case study benchmark where we present and compare 19 generative autoencoder models representative of some of the main improvements on downstream tasks such as image reconstruction, generation, classification, clustering and interpolation. The open-source library can be found at \\\\url{https://github.com/clementchadebec/benchmark_VAE}.",Datasets & Benchmarks,NeurIPS,2022,Poster,Clément Chadebec;Louis J. Vincent;Stephanie Allassonniere,False,https://openreview.net/pdf?id=w7VPQWgnn3s
LbOdQrnOb2q,Forecasting Future World Events With Neural Networks,"Forecasting future world events is a challenging but valuable task. Forecasts of climate, geopolitical conflict, pandemics and economic indicators help shape policy and decision making. In these domains, the judgment of expert humans contributes to the best forecasts. Given advances in language modeling, can these forecasts be automated? To this end, we introduce Autocast, a dataset containing thousands of forecasting questions and an accompanying news corpus. Questions are taken from forecasting tournaments, ensuring high quality, real-world importance, and diversity. The news corpus is organized by date, allowing us to precisely simulate the conditions under which humans made past forecasts (avoiding leakage from the future). Motivated by the difficulty of forecasting numbers across orders of magnitude (e.g. global cases of COVID-19 in 2022), we also curate IntervalQA, a dataset of numerical questions and metrics for calibration. We test language models on our forecasting task and find that performance is far below a human expert baseline. However, performance improves with increased model size and incorporation of relevant information from the news corpus. In sum, Autocast poses a novel challenge for large language models and improved performance could bring large practical benefits.",Datasets & Benchmarks,NeurIPS,2022,Poster,Andy Zou;Tristan Xiao;Ryan Jia;Joe Kwon;Mantas Mazeika;Richard Li;Dawn Song;Jacob Steinhardt;Owain Evans;Dan Hendrycks,True,https://openreview.net/pdf?id=LbOdQrnOb2q
b0VDQiNLPy9,ETAB: A Benchmark Suite for Visual Representation Learning in Echocardiography,"Echocardiography is one of the most commonly used diagnostic imaging modalities in cardiology. Application of deep learning models to echocardiograms can enable automated identification of cardiac structures, estimation of cardiac function, and prediction of clinical outcomes. However, a major hindrance to realizing the full potential of deep learning is the lack of large-scale, fully curated and annotated data sets required for supervised training. High-quality pre-trained representations that can transfer useful visual features of echocardiograms to downstream tasks can help adapt deep learning models to new setups using fewer examples. In this paper, we design a suite of benchmarks that can be used to pre-train and evaluate echocardiographic representations with respect to various clinically-relevant tasks using publicly accessible data sets. In addition, we develop a unified evaluation protocol---which we call the echocardiographic task adaptation benchmark (ETAB)---that measures how well a visual representation of echocardiograms generalizes to common downstream tasks of interest. We use our benchmarking framework to evaluate state-of-the-art vision modeling pipelines. We envision that our standardized, publicly accessible benchmarks would encourage future research and expedite progress in applying deep learning to high-impact problems in cardiovascular medicine.",Datasets & Benchmarks,NeurIPS,2022,Poster,Ahmed Alaa;Anthony Philippakis;David Sontag,False,https://openreview.net/pdf?id=b0VDQiNLPy9
wPEXGTzZJt,ViSioNS: Visual Search in Natural Scenes Benchmark,"Visual search is an essential part of almost any everyday human interaction with the visual environment. Nowadays, several algorithms are able to predict gaze positions during simple observation, but few models attempt to simulate human behavior during visual search in natural scenes. Furthermore, these models vary widely in their design and exhibit differences in the datasets and metrics with which they were evaluated. Thus, there is a need for a reference point, on which each model can be tested and from where potential improvements can be derived. In this study, we select publicly available state-of-the-art visual search models and datasets in natural scenes, and provide a common framework for their evaluation. To this end, we apply a unified format and criteria, bridging the gaps between them, and we estimate the models’ efficiency and similarity with humans using a specific set of metrics. This integration has allowed us to enhance the Ideal Bayesian Searcher by combining it with a neural network-based visual search model, which enables it to generalize to other datasets. The present work sheds light on the limitations of current models and how integrating different approaches with a unified criteria can lead to better algorithms. Moreover, it moves forward on bringing forth a solution for the urgent need for benchmarking data and metrics to support the development of more general human visual search computational models. All of the code used here, including metrics, plots, and visual search models, alongside the preprocessed datasets, are available at $\\\\url{https://github.com/FerminT/VisualSearchBenchmark}$.",Datasets & Benchmarks,NeurIPS,2022,Poster,Fermín Travi;Gonzalo Ruarte;Gaston Bujia;Juan E Kamienkowski,False,https://openreview.net/pdf?id=wPEXGTzZJt
heBKnuV42O,DDXPlus: A New Dataset For Automatic Medical Diagnosis,"There has been a rapidly growing interest in Automatic Symptom Detection (ASD) and Automatic Diagnosis (AD) systems in the machine learning research literature, aiming to assist doctors in telemedicine services. These systems are designed to interact with patients, collect evidence about their symptoms and relevant antecedents, and possibly make predictions about the underlying diseases. Doctors would review the interactions, including the evidence and the predictions, collect if necessary additional information from patients, before deciding on next steps. Despite recent progress in this area, an important piece of doctors' interactions with patients is missing in the design of these systems, namely the differential diagnosis. Its absence is largely due to the lack of datasets that include such information for models to train on. In this work, we present a large-scale synthetic dataset of roughly 1.3 million patients that includes a differential diagnosis, along with the ground truth pathology, symptoms and antecedents for each patient. Unlike existing datasets which only contain binary symptoms and antecedents, this dataset also contains categorical and multi-choice symptoms and antecedents useful for efficient data collection. Moreover, some symptoms are organized in a hierarchy, making it possible to design systems able to interact with patients in a logical way. As a proof-of-concept, we extend two existing AD and ASD systems to incorporate the differential diagnosis, and provide empirical evidence that using differentials as training signals is essential for the efficiency of such systems or for helping doctors better understand the reasoning of those systems.",Datasets & Benchmarks,NeurIPS,2022,Poster,Arsene Fansi Tchango;Rishab Goel;Zhi Wen;Julien Martel;Joumana Ghosn,True,https://openreview.net/pdf?id=heBKnuV42O
-VyJim9UBxQ,Understanding Aesthetics with Language: A Photo Critique Dataset for Aesthetic Assessment,"Computational inference of aesthetics is an ill-defined task due to its subjective nature. Many datasets have been proposed to tackle the problem by providing pairs of images and aesthetic scores based on human ratings. However, humans are better at expressing their opinion, taste, and emotions by means of language rather than summarizing them in a single number. In fact, photo critiques provide much richer information as they reveal how and why users rate the aesthetics of visual stimuli. In this regard, we propose the Reddit Photo Critique Dataset (RPCD), which contains tuples of image and photo critiques. RPCD consists of 74K images and 220K comments and is collected from a Reddit community used by hobbyists and professional photographers to improve their photography skills by leveraging constructive community feedback. The proposed dataset differs from previous aesthetics datasets mainly in three aspects, namely (i) the large scale of the dataset and the extension of the comments criticizing different aspects of the image, (ii) it contains mostly UltraHD images, and (iii) it can easily be extended to new data as it is collected through an automatic pipeline. To the best of our knowledge, in this work, we propose the first attempt to estimate the aesthetic quality of visual stimuli from the critiques. To this end, we exploit the polarity of the sentiment of criticism as an indicator of aesthetic judgment. We demonstrate how sentiment polarity correlates positively with the aesthetic judgment available for two aesthetic assessment benchmarks. Finally, we experiment with several models by using the sentiment scores as a target for ranking images. Dataset and baselines are available https://github.com/mediatechnologycenter/aestheval.",Datasets & Benchmarks,NeurIPS,2022,Poster,Daniel Vera Nieto;Luigi Celona;Clara Fernandez Labrador,True,https://openreview.net/pdf?id=-VyJim9UBxQ
SyoUVEyzJbE,MATE: Benchmarking Multi-Agent Reinforcement Learning in Distributed Target Coverage Control,"We introduce the Multi-Agent Tracking Environment (MATE), a novel multi-agent environment simulates the target coverage control problems in the real world. MATE hosts an asymmetric cooperative-competitive game consisting of two groups of learning agents--""cameras"" and ""targets""--with opposing interests. Specifically, ""cameras"", a group of directional sensors, are mandated to actively control the directional perception area to maximize the coverage rate of targets. On the other side, ""targets"" are mobile agents that aim to transport cargo between multiple randomly assigned warehouses while minimizing the exposure to the camera sensor networks. To showcase the practicality of MATE, we benchmark the multi-agent reinforcement learning (MARL) algorithms from different aspects, including cooperation, communication, scalability, robustness, and asymmetric self-play. We start by reporting results for cooperative tasks using MARL algorithms (MAPPO, IPPO, QMIX, MADDPG) and the results after augmenting with multi-agent communication protocols (TarMAC, I2C). We then evaluate the effectiveness of the popular self-play techniques (PSRO, fictitious self-play) in an asymmetric zero-sum competitive game. This process of co-evolution between cameras and targets helps to realize a less exploitable camera network. We also observe the emergence of different roles of the target agents while incorporating I2C into target-target communication. MATE is written purely in Python and integrated with OpenAI Gym API to enhance user-friendliness. Our project is released at https://github.com/UnrealTracking/mate.",Datasets & Benchmarks,NeurIPS,2022,Poster,Xuehai Pan;Mickel Liu;fangwei zhong;Yaodong Yang;Song-Chun Zhu;Yizhou Wang,True,https://openreview.net/pdf?id=SyoUVEyzJbE
Oa2-cdfBxun,"mRI: Multi-modal 3D Human Pose Estimation Dataset using mmWave, RGB-D, and Inertial Sensors","The ability to estimate 3D human body pose and movement, also known as human pose estimation (HPE), enables many applications for home-based health monitoring, such as remote rehabilitation training. Several possible solutions have emerged using sensors ranging from RGB cameras, depth sensors, millimeter-Wave (mmWave) radars, and wearable inertial sensors. Despite previous efforts on datasets and benchmarks for HPE, few dataset exploits multiple modalities and focuses on home-based health monitoring. To bridge the gap, we present mRI, a multi-modal 3D human pose estimation dataset with mmWave, RGB-D, and Inertial Sensors. Our dataset consists of over 160k synchronized frames from 20 subjects performing rehabilitation exercises and supports the benchmarks of HPE and action detection. We perform extensive experiments using our dataset and delineate the strength of each modality. We hope that the release of mRI can catalyze the research in pose estimation, multi-modal learning, and action understanding, and more importantly facilitate the applications of home-based health monitoring. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Sizhe An;Yin Li;Umit Ogras,True,https://openreview.net/pdf?id=Oa2-cdfBxun
B2W8Vy0rarw,EHRSQL: A Practical Text-to-SQL Benchmark for Electronic Health Records,"We present a new text-to-SQL dataset for electronic health records (EHRs). The utterances were collected from 222 hospital staff, including physicians, nurses, insurance review and health records teams, and more. To construct the QA dataset on structured EHR data, we conducted a poll at a university hospital and templatized the responses to create seed questions. Then, we manually linked them to two open-source EHR databases—MIMIC-III and eICU—and included them with various time expressions and held-out unanswerable questions in the dataset, which were all collected from the poll. Our dataset poses a unique set of challenges: the model needs to 1) generate SQL queries that reflect a wide range of needs in the hospital, including simple retrieval and complex operations such as calculating survival rate, 2) understand various time expressions to answer time-sensitive questions in healthcare, and 3) distinguish whether a given question is answerable or unanswerable based on the prediction confidence. We believe our dataset, EHRSQL, could serve as a practical benchmark to develop and assess QA models on structured EHR data and take one step further towards bridging the gap between text-to-SQL research and its real-life deployment in healthcare. EHRSQL is available at https://github.com/glee4810/EHRSQL.",Datasets & Benchmarks,NeurIPS,2022,Poster,Gyubok Lee;Hyeonji Hwang;Seongsu Bae;Yeonsu Kwon;Woncheol Shin;Seongjun Yang;Minjoon Seo;Jong-Yeup Kim;Edward Choi,True,https://openreview.net/pdf?id=B2W8Vy0rarw
EONuSdDjJrp,MSDS: A Large-Scale Chinese Signature and Token Digit String Dataset for Handwriting Verification,"Although online handwriting verification has made great progress recently, the verification performances are still far behind the real usage owing to the small scale of the datasets as well as the limited biometric mediums. Therefore, this paper proposes a new handwriting verification benchmark dataset named Multimodal Signature and Digit String (MSDS), which consists of two subsets: MSDS-ChS (Chinese Signatures) and MSDS-TDS (Token Digit Strings), contributed by 402 users, with 20 genuine samples and 20 skilled forgeries per user per subset. MSDS-ChS consists of handwritten Chinese signatures, which, to the best of our knowledge, is the largest publicly available Chinese signature dataset for handwriting verification, at least eight times larger than existing online datasets. Meanwhile, MSDS-TDS consists of handwritten Token Digit Strings, i.e, the actual phone numbers of users, which have not been explored yet. Extensive experiments with different baselines are respectively conducted for MSDS-ChS and MSDS-TDS. Surprisingly, verification performances of state-of-the-art methods on MSDS-TDS are generally better than those on MSDS-ChS, which indicates that the handwritten Token Digit String could be a more effective biometric than handwritten Chinese signature. This is a promising discovery that could inspire us to explore new biometric traits. The MSDS dataset is available at https://github.com/HCIILAB/MSDS.",Datasets & Benchmarks,NeurIPS,2022,Poster,Peirong Zhang;Jiajia Jiang;Yuliang Liu;Lianwen Jin,True,https://openreview.net/pdf?id=EONuSdDjJrp
MU2495w47rz,OpenXAI: Towards a Transparent Evaluation of Model Explanations,"While several types of post hoc explanation methods have been proposed in recent literature, there is very little work on systematically benchmarking these methods. Here, we introduce OpenXAI, a comprehensive and extensible open-source framework for evaluating and benchmarking post hoc explanation methods. OpenXAI comprises of the following key components: (i) a flexible synthetic data generator and a collection of diverse real-world datasets, pre-trained models, and state-of-the-art feature attribution methods, (ii) open-source implementations of twenty-two quantitative metrics for evaluating faithfulness, stability (robustness), and fairness of explanation methods, and (iii) the first ever public XAI leaderboards to readily compare several explanation methods across a wide variety of metrics, models, and datasets. OpenXAI is easily extensible, as users can readily evaluate custom explanation methods and incorporate them into our leaderboards. 
Overall, OpenXAI provides an automated end-to-end pipeline that not only simplifies and standardizes the evaluation of post hoc explanation methods, but also promotes transparency and reproducibility in benchmarking these methods. While the first release of OpenXAI supports only tabular datasets, the explanation methods and metrics that we consider are general enough to be applicable to other data modalities. OpenXAI datasets and data loaders, implementations of state-of-the-art explanation methods and evaluation metrics, as well as leaderboards are publicly available at https://open-xai.github.io/. OpenXAI will be regularly updated to incorporate text and image datasets, other new metrics and explanation methods, and welcomes inputs from the community.",Datasets & Benchmarks,NeurIPS,2022,Poster,Chirag Agarwal;Satyapriya Krishna;Eshika Saxena;Martin Pawelczyk;Nari Johnson;Isha Puri;Marinka Zitnik;Himabindu Lakkaraju,False,https://openreview.net/pdf?id=MU2495w47rz
8lQDn9zTQlW,BigBio: A Framework for Data-Centric Biomedical Natural Language Processing,"Training and evaluating language models increasingly requires the construction of meta-datasets -- diverse collections of curated data with clear provenance. Natural language prompting has recently lead to improved zero-shot generalization by transforming existing, supervised datasets into a variety of novel instruction tuning tasks, highlighting the benefits of meta-dataset curation. While successful in general-domain text, translating these data-centric approaches to biomedical language modeling remains challenging, as labeled biomedical datasets are significantly underrepresented in popular data hubs. To address this challenge, we introduce BigBio a community library of 126+ biomedical NLP datasets, currently covering 13 task categories and 10+ languages. BigBio facilitates reproducible meta-dataset curation via programmatic access to datasets and their metadata, and is compatible with current platforms for prompt engineering and end-to-end few/zero shot language model evaluation. We discuss our process for task schema harmonization, data auditing, contribution guidelines, and outline two illustrative use cases: zero-shot evaluation of biomedical prompts and large-scale, multi-task learning. BigBio is an ongoing community effort and is available at https://github.com/bigscience-workshop/biomedical",Datasets & Benchmarks,NeurIPS,2022,Poster,Jason Alan Fries;Leon Weber;Natasha Seelam;Gabriel Altay;Debajyoti Datta;Samuele Garda;Myungsun Kang;Ruisi Su;Wojciech Kusa;Samuel Cahyawijaya;Fabio Barth;Simon Ott;Matthias Samwald;Stephen Bach;Stella Biderman;Mario Sänger;Bo Wang;Alison Callahan;Daniel León Periñán;Théo Gigant;Patrick Haller;Jenny Chim;Jose David Posada;John Michael Giorgi;Karthik Rangasai Sivaraman;Marc Pàmies;Marianna Nezhurina;Robert Martin;Michael Cullan;Moritz Freidank;Nathan Dahlberg;Shubhanshu Mishra;Shamik Bose;Nicholas Michio Broad;Yanis Labrak;Shlok S Deshmukh;Sid Kiblawi;Ayush Singh;Minh Chien Vu;Trishala Neeraj;Jonas Golde;Albert Villanova del Moral;Benjamin Beilharz,True,https://openreview.net/pdf?id=8lQDn9zTQlW
93cqcWFpTex,A Dataset for Efforts Towards Achieving the Sustainable Development Goal of Safe Working Environments,"Among United Nations' 17 Sustainable Development Goals (SDGs), we highlight SDG 8 on Decent Work and Economic Growth.  Specifically, we consider how to achieve subgoal 8.8, ""protect labour rights and promote safe working environments for all workers [...]"", in light of poor health, safety and environment (HSE) conditions being a widespread problem at workplaces. In EU alone, it is estimated that more than 4000 deaths occur each year due to poor working conditions. To handle the problem and achieve SDG 8, governmental agencies conduct labour inspections and it is therefore essential that these are carried out efficiently. Current research suggests that machine learning (ML) can be used to improve labour inspections, for instance by selecting organisations for inspections more effectively. However, the research in this area is very limited, in part due to a lack of publicly available data. Consequently, we introduce a new dataset called the Labour Inspection Checklists Dataset (LICD), which we have made publicly available. LICD consists of 63634 instances where each instance is an inspection conducted by the Norwegian Labour Inspection Authority. LICD has 577 features and labels. The dataset provides several ML research opportunities; we discuss two demonstration experiments. One experiment deals with the problem of selecting a relevant checklist for inspecting a given target organisation. The other experiment concerns the problem of predicting HSE violations, given a specific checklist and a target organisation. Our experimental results, while promising, suggest that achieving good ML classification performance is difficult for both problems. This motivates future research to improve ML performance, inspire other data analysis efforts, and ultimately achieve SDG 8.",Datasets & Benchmarks,NeurIPS,2022,Poster,Eirik Lund Flogard;Ole Jakob Mengshoel,True,https://openreview.net/pdf?id=93cqcWFpTex
8hHg-zs_p-h,GOOD: A Graph Out-of-Distribution Benchmark,"Out-of-distribution (OOD) learning deals with scenarios in which training and test data follow different distributions. Although general OOD problems have been intensively studied in machine learning, graph OOD is only an emerging area of research. Currently, there lacks a systematic benchmark tailored to graph OOD method evaluation. In this work, we aim at developing an OOD benchmark, known as GOOD, for graphs specifically. We explicitly make distinctions between covariate and concept shifts and design data splits that accurately reflect different shifts. We consider both graph and node prediction tasks as there are key differences in designing shifts. Overall, GOOD contains 11 datasets with 17 domain selections. When combined with covariate, concept, and no shifts, we obtain 51 different splits. We provide performance results on 10 commonly used baseline methods with 10 random runs. This results in 510 dataset-model combinations in total. Our results show significant performance gaps between in-distribution and OOD settings. Our results also shed light on different performance trends between covariate and concept shifts by different methods. Our GOOD benchmark is a growing project and expects to expand in both quantity and variety of resources as the area develops. The GOOD benchmark can be accessed via https://github.com/divelab/GOOD/.",Datasets & Benchmarks,NeurIPS,2022,Poster,Shurui Gui;Xiner Li;Limei Wang;Shuiwang Ji,True,https://openreview.net/pdf?id=8hHg-zs_p-h
mV4EKzUVI96,APT-36K: A Large-scale Benchmark for Animal Pose Estimation and Tracking,"Animal pose estimation and tracking (APT) is a fundamental task for detecting and tracking animal keypoints from a sequence of video frames. Previous animal-related datasets focus either on animal tracking or single-frame animal pose estimation, and never on both aspects. The lack of APT datasets hinders the development and evaluation of video-based animal pose estimation and tracking methods, limiting the applications in real world, e.g., understanding animal behavior in wildlife conservation. To fill this gap, we make the first step and propose APT-36K, i.e., the first large-scale benchmark for animal pose estimation and tracking. Specifically, APT-36K consists of 2,400 video clips collected and filtered from 30 animal species with 15 frames for each video, resulting in 36,000 frames in total. After manual annotation and careful double-check, high-quality keypoint and tracking annotations are provided for all the animal instances. Based on APT-36K, we benchmark several representative models on the following three tracks: (1) supervised animal pose estimation on a single frame under intra- and inter-domain transfer learning settings, (2) inter-species domain generalization test for unseen animals, and (3) animal pose estimation with animal tracking. Based on the experimental results, we gain some empirical insights and show that APT-36K provides a useful animal pose estimation and tracking benchmark, offering new challenges and opportunities for future research. The code and dataset will be made publicly available at https://github.com/pandorgan/APT-36K.",Datasets & Benchmarks,NeurIPS,2022,Poster,Yuxiang Yang;Junjie Yang;Yufei Xu;Jing Zhang;Long Lan;Dacheng Tao,True,https://openreview.net/pdf?id=mV4EKzUVI96
YXvGXEmtZ5N,BOND: Benchmarking Unsupervised Outlier Node Detection on Static Attributed Graphs,"Detecting which nodes in graphs are outliers is a relatively new machine learning task with numerous applications. Despite the proliferation of algorithms developed in recent years for this task, there has been no standard comprehensive setting for performance evaluation. Consequently, it has been difficult to understand which methods work well and when under a broad range of settings. To bridge this gap, we present—to the best of our knowledge—the first comprehensive benchmark for unsupervised outlier node detection on static attributed graphs called BOND, with the following highlights. (1) We benchmark the outlier detection performance of 14 methods ranging from classical matrix factorization to the latest graph neural networks. (2) Using nine real datasets, our benchmark assesses how the different detection methods respond to two major types of synthetic outliers and separately to “organic” (real non-synthetic) outliers. (3) Using an existing random graph generation technique, we produce a family of synthetically generated datasets of different graph sizes that enable us to compare the running time and memory usage of the different outlier detection algorithms. Based on our experimental results, we discuss the pros and cons of existing graph outlier detection algorithms, and we highlight opportunities for future research. Importantly, our code is freely available and meant to be easily extendable: https://github.com/pygod-team/pygod/tree/main/benchmark",Datasets & Benchmarks,NeurIPS,2022,Poster,Kay Liu;Yingtong Dou;Yue Zhao;Xueying Ding;Xiyang Hu;Ruitong Zhang;Kaize Ding;Canyu Chen;Hao Peng;Kai Shu;Lichao Sun;Jundong Li;George H. Chen;Zhihao Jia;Philip S. Yu,True,https://openreview.net/pdf?id=YXvGXEmtZ5N
x_kBZYiUrxR,PulseImpute: A Novel Benchmark Task for Pulsative Physiological Signal Imputation,"The promise of Mobile Health (mHealth) is the ability to use wearable sensors to monitor participant physiology at high frequencies during daily life to enable temporally-precise health interventions. However, a major challenge is frequent missing data. Despite a rich imputation literature, existing techniques are ineffective for the pulsative signals which comprise many mHealth applications, and a lack of available datasets has stymied progress. We address this gap with PulseImpute, the first large-scale pulsative signal imputation challenge which includes realistic mHealth missingness models, an extensive set of baselines, and clinically-relevant downstream tasks. Our baseline models include a novel transformer-based architecture designed to exploit the structure of pulsative signals. We hope that PulseImpute will enable the ML community to tackle this important and challenging task.",Datasets & Benchmarks,NeurIPS,2022,Poster,Maxwell Xu;Alexander Moreno;Supriya Nagesh;Varol Burak Aydemir;David W Wetter;Santosh Kumar;James Matthew Rehg,True,https://openreview.net/pdf?id=x_kBZYiUrxR
FPgCB_Z_0O,DART: Articulated Hand Model with Diverse Accessories and Rich Textures,"Hand, the bearer of human productivity and intelligence, is receiving much attention due to the recent fever of digital twins. Among different hand morphable models, MANO has been widely used in vision and graphics community. However, MANO disregards textures and accessories, which largely limits its power to synthesize photorealistic hand data. In this paper, we extend MANO with Diverse Accessories and Rich Textures, namely DART. DART is composed of 50 daily 3D accessories which varies in appearance and shape, and 325 hand-crafted 2D texture maps covers different kinds of blemishes or make-ups. Unity GUI is also provided to generate synthetic hand data with user-defined settings, e.g., pose, camera, background, lighting, textures, and accessories. Finally, we release DARTset, which contains large-scale (800K), high-fidelity synthetic hand images, paired with perfect-aligned 3D labels. Experiments demonstrate its superiority in diversity. As a complement to existing hand datasets, DARTset boosts the generalization in both hand pose estimation and mesh recovery tasks. Raw ingredients (textures, accessories), Unity GUI, source code and DARTset are publicly available at dart2022.github.io.
",Datasets & Benchmarks,NeurIPS,2022,Poster,Daiheng Gao;Yuliang Xiu;Kailin Li;Lixin Yang;Feng Wang;Peng Zhang;Bang Zhang;Cewu Lu;Ping Tan,True,https://openreview.net/pdf?id=FPgCB_Z_0O
5mi-CkvEqj,TweetNERD - End to End Entity Linking Benchmark for Tweets,"Named Entity Recognition and Disambiguation (NERD) systems are foundational for information retrieval, question answering, event detection, and other natural language processing (NLP) applications. We introduce TweetNERD, a dataset of 340K+ Tweets across 2010-2021, for benchmarking NERD systems on Tweets. This is the largest and most temporally diverse open sourced dataset benchmark for NERD on Tweets and can be used to facilitate research in this area. We describe evaluation setup with TweetNERD for three NERD tasks: Named Entity Recognition (NER), Entity Linking with True Spans (EL), and End to End Entity Linking (End2End); and provide performance of existing publicly available methods on specific TweetNERD splits. TweetNERD is available at: https://doi.org/10.5281/zenodo.6617192 under Creative Commons Attribution 4.0 International (CC BY 4.0) license. Check out more details at https://github.com/twitter-research/TweetNERD.",Datasets & Benchmarks,NeurIPS,2022,Poster,Shubhanshu Mishra;Aman Saini;Raheleh Makki;Sneha Mehta;Aria Haghighi;Ali Mollahosseini,True,https://openreview.net/pdf?id=5mi-CkvEqj
eJhc_CPXQIT,MOMA-LRG: Language-Refined Graphs for Multi-Object Multi-Actor Activity Parsing,"Video-language models (VLMs), large models pre-trained on numerous but noisy video-text pairs from the internet, have revolutionized activity recognition through their remarkable generalization and open-vocabulary capabilities. While complex human activities are often hierarchical and compositional, most existing tasks for evaluating VLMs focus only on high-level video understanding, making it difficult to accurately assess and interpret the ability of VLMs to understand complex and fine-grained human activities. Inspired by the recently proposed MOMA framework, we define activity graphs as a single universal representation of human activities that encompasses video understanding at the activity, sub-activity, and atomic action level.  We redefine activity parsing as the overarching task of activity graph generation, requiring understanding human activities across all three levels. To facilitate the evaluation of models on activity parsing, we introduce MOMA-LRG (Multi-Object Multi-Actor Language-Refined Graphs), a large dataset of complex human activities with activity graph annotations that can be readily transformed into natural language sentences. Lastly, we present a model-agnostic and lightweight approach to adapting and evaluating VLMs by incorporating structured knowledge from activity graphs into VLMs, addressing the individual limitations of language and graphical models. We demonstrate strong performance on few-shot activity parsing, and our framework is intended to foster future research in the joint modeling of videos, graphs, and language.",Datasets & Benchmarks,NeurIPS,2022,Poster,Zelun Luo;Zane Durante;Linden Li;Wanze Xie;Ruochen Liu;Emily Jin;Zhuoyi Huang;Lun Yu Li;Jiajun Wu;Juan Carlos Niebles;Ehsan Adeli;Li Fei-Fei,True,https://openreview.net/pdf?id=eJhc_CPXQIT
DcfsR89KUa,Nocturne: a scalable driving benchmark for bringing multi-agent learning one step closer to the real world,"We introduce \\\\textit{Nocturne}, a new 2D driving simulator for investigating multi-agent coordination under partial observability. The focus of Nocturne is to enable research into inference and theory of mind in real-world multi-agent settings without the computational overhead of computer vision and feature extraction from images. Agents in this simulator only observe an obstructed view of the scene, mimicking human visual sensing constraints. Unlike existing benchmarks that are bottlenecked by rendering human-like observations directly using a camera input, Nocturne uses efficient intersection methods to compute a vectorized set of visible features in a C++ back-end, allowing the simulator to run at $2000+$ steps-per-second. Using open-source trajectory and map data, we construct a simulator to load and replay arbitrary trajectories and scenes from real-world driving data. Using this environment, we benchmark reinforcement-learning and imitation-learning agents and demonstrate that the agents are quite far from human-level coordination ability and deviate significantly from the expert trajectories.",Datasets & Benchmarks,NeurIPS,2022,Poster,Eugene Vinitsky;Nathan Lichtlé;Xiaomeng Yang;Brandon Amos;Jakob Nicolaus Foerster,False,https://openreview.net/pdf?id=DcfsR89KUa
PfyWdxM-S4N,xView3-SAR: Detecting Dark Fishing Activity Using Synthetic Aperture Radar Imagery,"Unsustainable fishing practices worldwide pose a major threat to marine resources and ecosystems. Identifying vessels that do not show up in conventional monitoring systems---known as ``dark vessels''---is key to managing and securing the health of marine environments. With the rise of satellite-based synthetic aperture radar (SAR) imaging and modern machine learning (ML), it is now possible to automate detection of dark vessels day or night, under all-weather conditions. SAR images, however, require a domain-specific treatment and are not widely accessible to the ML community. Maritime objects (vessels and offshore infrastructure) are relatively small and sparse, challenging traditional computer vision approaches. We present the largest labeled dataset for training ML models to detect and characterize vessels and ocean structures in SAR imagery. xView3-SAR consists of nearly 1,000 analysis-ready SAR images from the Sentinel-1 mission that are, on average, 29,400-by-24,400 pixels each. The images are annotated using a combination of automated and manual analysis. Co-located bathymetry and wind state rasters accompany every SAR image. We also provide an overview of the xView3 Computer Vision Challenge, an international competition using xView3-SAR for ship detection and characterization at large scale. We release the data  (\\\\href{https://iuu.xview.us/}{https://iuu.xview.us/}) and code (\\\\href{https://github.com/DIUx-xView}{https://github.com/DIUx-xView}) to support ongoing development and evaluation of ML approaches for this important application.",Datasets & Benchmarks,NeurIPS,2022,Poster,Fernando Paolo;Tsu-ting Tim Lin;Ritwik Gupta;Bryce Goodman;Nirav Patel;Daniel Kuster;David Kroodsma;Jared Dunnmon,True,https://openreview.net/pdf?id=PfyWdxM-S4N
k3462dQtQhg,A Unified Evaluation of Textual Backdoor Learning: Frameworks and Benchmarks,"Textual backdoor attacks are a kind of practical threat to NLP systems. By injecting a backdoor in the training phase, the adversary could control model predictions via predefined triggers. As various attack and defense models have been proposed, it is of great significance to perform rigorous evaluations. However, we highlight two issues in previous backdoor learning evaluations: (1) The differences between real-world scenarios (e.g. releasing poisoned datasets or models) are neglected, and we argue that each scenario has its own constraints and concerns, thus requires specific evaluation protocols; (2) The evaluation metrics only consider whether the attacks could flip the models' predictions on poisoned samples and retain performances on benign samples, but ignore that poisoned samples should also be stealthy and semantic-preserving. To address these issues, we categorize existing works into three practical scenarios in which attackers release datasets, pre-trained models, and fine-tuned models respectively, then discuss their unique evaluation methodologies. On metrics, to completely evaluate poisoned samples, we use grammar error increase and perplexity difference for stealthiness, along with text similarity for validity. After formalizing the frameworks, we develop an open-source toolkit OpenBackdoor to foster the implementations and evaluations of textual backdoor learning. With this toolkit, we perform extensive experiments to benchmark attack and defense models under the suggested paradigm. To facilitate the underexplored defenses against poisoned datasets, we further propose CUBE, a simple yet strong clustering-based defense baseline. We hope that our frameworks and benchmarks could serve as the cornerstones for future model development and evaluations.",Datasets & Benchmarks,NeurIPS,2022,Poster,Ganqu Cui;Lifan Yuan;Bingxiang He;Yangyi Chen;Zhiyuan Liu;Maosong Sun,False,https://openreview.net/pdf?id=k3462dQtQhg
GKOa7yNH8Uh,GLOBEM Dataset: Multi-Year Datasets for Longitudinal Human Behavior Modeling Generalization,"Recent research has demonstrated the capability of behavior signals captured by smartphones and wearables for longitudinal behavior modeling. However, there is a lack of a comprehensive public dataset that serves as an open testbed for fair comparison among algorithms. Moreover, prior studies mainly evaluate algorithms using data from a single population within a short period, without measuring the cross-dataset generalizability of these algorithms. We present the first multi-year passive sensing datasets, containing over 700 user-years and 497 unique users’ data collected from mobile and wearable sensors, together with a wide range of well-being metrics. Our datasets can support multiple cross-dataset evaluations of behavior modeling algorithms’ generalizability across different users and years. As a starting point, we provide the benchmark results of 18 algorithms on the task of depression detection. Our results indicate that both prior depression detection algorithms and domain generalization techniques show potential but need further research to achieve adequate cross-dataset generalizability. We envision our multi-year datasets can support the ML community in developing generalizable longitudinal behavior modeling algorithms.",Datasets & Benchmarks,NeurIPS,2022,Poster,Xuhai Xu;Han Zhang;Yasaman S Sefidgar;Yiyi Ren;Xin Liu;Woosuk Seo;Jennifer Brown;Kevin Scott Kuehn;Mike A Merrill;Paula S Nurius;Shwetak Patel;Tim Althoff;Margaret E Morris;Eve A. Riskin;Jennifer Mankoff;Anind Dey,True,https://openreview.net/pdf?id=GKOa7yNH8Uh
bBff294gqLp,NAS-Bench-Graph: Benchmarking Graph Neural Architecture Search,"Graph neural architecture search (GraphNAS) has recently aroused considerable attention in both academia and industry. However, two key challenges seriously hinder the further research of GraphNAS. First, since there is no consensus for the experimental setting, the empirical results in different research papers are often not comparable and even not reproducible, leading to unfair comparisons. Secondly, GraphNAS often needs extensive computations, which makes it highly inefficient and inaccessible to researchers without access to large-scale computation. To solve these challenges, we propose NAS-Bench-Graph, a tailored benchmark that supports unified, reproducible, and efficient evaluations for GraphNAS. Specifically, we construct a unified, expressive yet compact search space, covering 26,206 unique graph neural network (GNN) architectures and propose a principled evaluation protocol. To avoid unnecessary repetitive training, we have trained and evaluated all of these architectures on nine representative graph datasets, recording detailed metrics including train, validation, and test performance in each epoch, the latency, the number of parameters, etc. Based on our proposed benchmark, the performance of GNN architectures can be directly obtained by a look-up table without any further computation, which enables fair, fully reproducible, and efficient comparisons.  To demonstrate its usage, we make in-depth analyses of our proposed NAS-Bench-Graph, revealing several interesting findings for GraphNAS. We also showcase how the benchmark can be easily compatible with GraphNAS open libraries such as AutoGL and NNI. To the best of our knowledge, our work is the first benchmark for graph neural architecture search.   ",Datasets & Benchmarks,NeurIPS,2022,Poster,Yijian Qin;Ziwei Zhang;Xin Wang;Zeyang Zhang;Wenwu Zhu,False,https://openreview.net/pdf?id=bBff294gqLp
iAxH-ikIP0I,TaiSu: A 166M Large-scale High-Quality Dataset for Chinese Vision-Language Pre-training,"Vision-Language Pre-training (VLP) has been shown to be an efficient method to improve the performance of models on different vision-and-language downstream tasks. Substantial studies have shown that neural networks may be able to learn some general rules about language and visual concepts from a large-scale weakly labeled image-text dataset. However, most of the public cross-modal datasets that contain more than 100M image-text pairs are in English; there is a lack of available large-scale and high-quality Chinese VLP datasets. In this work, we propose a new framework for automatic dataset acquisition and cleaning with which we construct a new large-scale and high-quality cross-modal dataset named as TaiSu, containing 166 million images and 219 million Chinese captions. Compared with the recently released Wukong dataset, our dataset is achieved with much stricter restrictions on the semantic correlation of image-text pairs. We also propose to combine texts collected from the web with texts generated by a pre-trained image-captioning model. To the best of our knowledge, TaiSu is currently the largest publicly accessible Chinese cross-modal dataset. Furthermore, we test our dataset on several vision-language downstream tasks. TaiSu outperforms BriVL by a large margin on the zero-shot image-text retrieval task and zero-shot image classification task. TaiSu also shows better performance than Wukong on the image-retrieval task without using image augmentation for training. Results demonstrate that TaiSu can serve as a promising VLP dataset, both for understanding and generative tasks. More information can be referred to https://github.com/ksOAn6g5/TaiSu.",Datasets & Benchmarks,NeurIPS,2022,Poster,Yulong Liu;Guibo Zhu;Bin Zhu;Qi Song;Guojing Ge;Haoran Chen;GuanHui Qiao;Ru Peng;Lingxiang Wu;Jinqiao Wang,True,https://openreview.net/pdf?id=iAxH-ikIP0I
CZAd_6uiUx0,"This is the way: designing and compiling LEPISZCZE, a comprehensive NLP benchmark for Polish","The availability of compute and data to train larger and larger language models increases the demand for robust methods of benchmarking the true progress of LM training. Recent years witnessed significant progress in standardized benchmarking for English. Benchmarks such as GLUE, SuperGLUE, or KILT have become a de facto standard tools to compare large language models. Following the trend to replicate GLUE for other languages, the KLEJ benchmark\\\\ (klej is the word for glue in Polish) has been released for Polish. In this paper, we evaluate the progress in benchmarking for low-resourced languages. We note that only a handful of languages have such comprehensive benchmarks. We also note the gap in the number of tasks being evaluated by benchmarks for resource-rich English/Chinese and the rest of the world.

In this paper, we introduce LEPISZCZE (lepiszcze is the Polish word for glew, the Middle English predecessor of glue), a new, comprehensive benchmark for Polish NLP with a large variety of tasks and high-quality operationalization of the benchmark.
We design LEPISZCZE with flexibility in mind. Including new models, datasets, and tasks is as simple as possible while still offering data versioning and model tracking. In the first run of the benchmark, we test 13 experiments (task and dataset pairs) based on the five most recent LMs for Polish. We use five datasets from the Polish benchmark and add eight novel datasets. As the paper's main contribution, apart from LEPISZCZE, we provide insights and experiences learned while creating the benchmark for Polish as the blueprint to design similar benchmarks for other low-resourced languages.",Datasets & Benchmarks,NeurIPS,2022,Poster,Lukasz Augustyniak;Kamil Tagowski;Albert Sawczyn;Denis Janiak;Roman Bartusiak;Adrian Dominik Szymczak;Arkadiusz Janz;Piotr Szymański;Marcin Wątroba;Mikołaj Morzy;Tomasz Jan Kajdanowicz;Maciej Piasecki,True,https://openreview.net/pdf?id=CZAd_6uiUx0
Z0s5T89qfjc,ENS-10: A Dataset For Post-Processing Ensemble Weather Forecasts,"Post-processing ensemble prediction systems can improve the reliability of weather forecasting, especially for extreme event prediction. In recent years, different machine learning models have been developed to improve the quality of weather post-processing. However, these models require a comprehensive dataset of weather simulations to produce high-accuracy results, which comes at a high computational cost to generate. This paper introduces the ENS-10 dataset, consisting of ten ensemble members spanning 20 years (1998--2017). The ensemble members are generated by perturbing numerical weather simulations to capture the chaotic behavior of the Earth. To represent the three-dimensional state of the atmosphere, ENS-10 provides the most relevant atmospheric variables at 11 distinct pressure levels and the surface at \\\\ang{0.5} resolution for forecast lead times T=0, 24, and 48 hours (two data points per week). We propose the ENS-10 prediction correction task for improving the forecast quality at a 48-hour lead time through ensemble post-processing. We provide a set of baselines and compare their skill at correcting the predictions of three important atmospheric variables. Moreover, we measure the baselines' skill at improving predictions of extreme weather events using our dataset. The ENS-10 dataset is available under the Creative Commons Attribution 4.0 International (CC BY 4.0) license.",Datasets & Benchmarks,NeurIPS,2022,Poster,Saleh Ashkboos;Langwen Huang;Nikoli Dryden;Tal Ben-Nun;Peter Dominik Dueben;Lukas Gianinazzi;Luca Nicola Kummer;Torsten Hoefler,True,https://openreview.net/pdf?id=Z0s5T89qfjc
dh_MkX0QfrK,PDEBench: An Extensive Benchmark for Scientific Machine Learning,"Machine learning-based modeling of physical systems has experienced increased interest in recent years. Despite some impressive progress, there is still a lack of benchmarks for Scientific ML that are easy to use but still challenging and repre- sentative of a wide range of problems. We introduce PDEBENCH, a benchmark suite of time-dependent simulation tasks based on Partial Differential Equations (PDEs). PDEBENCH comprises both code and data to benchmark the performance of novel machine learning models against both classical numerical simulations and machine learning baselines. Our proposed set of benchmark problems con- tribute the following unique features: (1) A much wider range of PDEs compared to existing benchmarks, ranging from relatively common examples to more real- istic and difficult problems; (2) much larger ready-to-use datasets compared to prior work, comprising multiple simulation runs across a larger number of ini- tial and boundary conditions and PDE parameters; (3) more extensible source codes with user-friendly APIs for data generation and baseline results with popular machine learning models (FNO, U-Net, PINN, Gradient-Based Inverse Method). PDEBENCH allows researchers to extend the benchmark freely for their own pur- poses using a standardized API and to compare the performance of new models to existing baseline methods. We also propose new evaluation metrics with the aim to provide a more holistic understanding of learning methods in the context of Scientific ML. With those metrics we identify tasks which are challenging for recent ML methods and propose these tasks as future challenges for the community. The code is available at https://github.com/pdebench/PDEBench.",Datasets & Benchmarks,NeurIPS,2022,Poster,Makoto Takamoto;Timothy Praditia;Raphael Leiteritz;Dan MacKinlay;Francesco Alesiani;Dirk Pflüger;Mathias Niepert,True,https://openreview.net/pdf?id=dh_MkX0QfrK
MzaPEKHv-0J,PeRFception: Perception using Radiance Fields,"The recent progress in implicit 3D representation, i.e., Neural Radiance Fields (NeRFs), has made accurate and photorealistic 3D reconstruction possible in a differentiable manner. This new representation can effectively convey the information of hundreds of high-resolution images in one compact format and allows photorealistic synthesis of novel views. In this work, using the variant of NeRF called Plenoxels, we create the first large-scale radiance fields datasets  for perception tasks, called the PeRFception, which consists of two parts that incorporate both object-centric and scene-centric scans for classification and segmentation. It shows a significant memory compression rate (96.4\\\\%) from the original dataset, while containing both 2D and 3D information in a unified form. We construct the  classification and segmentation models that directly take this radiance fields format as input and also propose a novel augmentation technique to avoid overfitting on backgrounds of images. The code and data are publicly available in ""https://postech-cvlab.github.io/PeRFception/"".",Datasets & Benchmarks,NeurIPS,2022,Poster,Yoonwoo Jeong;Seungjoo Shin;Junha Lee;Chris Choy;Anima Anandkumar;Minsu Cho;Jaesik Park,True,https://openreview.net/pdf?id=MzaPEKHv-0J
r2DdJQ9AJvI,TGEA 2.0: A Large-Scale Diagnostically Annotated Dataset with Benchmark Tasks for Text Generation of Pretrained Language Models,"In order to diagnostically analyze and improve the capability of pretrained language models (PLMs) in text generation, we propose TGEA 2.0, to date the largest dataset built on machine-authored texts by PLMs with fine-grained semantic annotations on a wide variety of pathological generation errors. We collect 170K nominal, phrasal and sentential prompts from 6M natural sentences in 3 domains. These prompts are fed into 4 generative PLMs with their best decoding strategy to generate paragraphs. 195,629 sentences are extracted from these generated paragraphs for manual annotation, where 36K erroneous sentences are detected, 42K erroneous spans are located and categorized into an error type defined in a two-level error taxonomy. We define a \\\\textbf{Mi}nimal \\\\textbf{S}et of \\\\textbf{E}rror-related \\\\textbf{W}ords (MiSEW) for each erroneous span, which not only provides error-associated words but also rationalizes the reasoning behind the error. Quality control with a pre-annotation and feedback loop is performed before and during the entire annotation process. With the diagnostically annotated dataset, we propose 5 diagnosis benchmark tasks (i.e., erroneous text detection, MiSEW extraction, erroneous span location and correction together with error type classification) and 2 pathology mitigation benchmark tasks (pairwise comparison and word prediction). Experiment results on these benchmark tasks demonstrate that TGEA 2.0 is a challenging dataset that could facilitate further research on automatic diagnosis and pathology mitigation over machine texts. The dataset will be publicly available at https://github.com/tjunlp-lab/TGEA/.",Datasets & Benchmarks,NeurIPS,2022,Poster,Huibin Ge;Xiaohu Zhao;Chuang Liu;Yulong Zeng;Qun Liu;Deyi Xiong,True,https://openreview.net/pdf?id=r2DdJQ9AJvI
UrAYT2QwOX8,"Turning the Tables: Biased, Imbalanced, Dynamic Tabular Datasets for ML Evaluation","Evaluating new techniques on realistic datasets plays a crucial role in the development of ML research and its broader adoption by practitioners. In recent years, there has been a significant increase of publicly available unstructured data resources for computer vision and NLP tasks. However, tabular data — which is prevalent in many high-stakes domains — has been lagging behind. To bridge this gap, we present Bank Account Fraud (BAF), the first publicly available 1 privacy-preserving, large-scale, realistic suite of tabular datasets. The suite was generated by applying state-of-the-art tabular data generation techniques on an anonymized,real-world bank account opening fraud detection dataset. This setting carries a set of challenges that are commonplace in real-world applications, including temporal dynamics and significant class imbalance. Additionally, to allow practitioners to stress test both performance and fairness of ML methods, each dataset variant of BAF contains specific types of data bias. With this resource, we aim to provide the research community with a more realistic, complete, and robust test bed to evaluate novel and existing methods.",Datasets & Benchmarks,NeurIPS,2022,Poster,Sérgio Jesus;José Pombal;Duarte Alves;André Cruz;Pedro Saleiro;Rita P. Ribeiro;João Gama;Pedro Bizarro,True,https://openreview.net/pdf?id=UrAYT2QwOX8
IIbJ9m5G73t,BLOX: Macro Neural Architecture Search Benchmark and Algorithms,"Neural architecture search (NAS) has been successfully used to design numerous high-performance neural networks. However, NAS is typically compute-intensive, so most existing approaches restrict the search to decide the operations and topological structure of a single block only, then the same block is stacked repeatedly to form an end-to-end model. Although such an approach reduces the size of search space, recent studies show that a macro search space, which allows blocks in a model to be different, can lead to better performance. To provide a systematic study of the performance of NAS algorithms on a macro search space, we release Blox – a benchmark that consists of 91k unique models trained on the CIFAR-100 dataset. The dataset also includes runtime measurements of all the models on a diverse set of hardware platforms. We perform extensive experiments to compare existing algorithms that are well studied on cell-based search spaces, with the emerging blockwise approaches that aim to make NAS scalable to much larger macro search spaces. The Blox benchmark and code are available at https://github.com/SamsungLabs/blox.",Datasets & Benchmarks,NeurIPS,2022,Poster,Thomas Chun Pong Chau;Łukasz Dudziak;Hongkai Wen;Nicholas Donald Lane;Mohamed S Abdelfattah,True,https://openreview.net/pdf?id=IIbJ9m5G73t
2N8JzuiWZ25,OpenSRH: optimizing brain tumor surgery using intraoperative stimulated Raman histology,"Accurate intraoperative diagnosis is essential for providing safe and effective care during brain tumor surgery. Our standard-of-care diagnostic methods are time, resource, and labor intensive, which restricts access to optimal surgical treatments. To address these limitations, we propose an alternative workflow that combines stimulated Raman histology (SRH), a rapid optical imaging method, with deep learning-based automated interpretation of SRH images for intraoperative brain tumor diagnosis and real-time surgical decision support. Here, we present OpenSRH, the first public dataset of clinical SRH images from 300+ brain tumors patients and 1300+ unique whole slide optical images. OpenSRH contains data from the most common brain tumors diagnoses, full pathologic annotations, whole slide tumor segmentations, raw and processed optical imaging data for end-to-end model development and validation. We provide a framework for patch-based whole slide SRH classification and inference using weak (i.e. patient-level) diagnostic labels. Finally, we benchmark two computer vision tasks: multi-class histologic brain tumor classification and patch-based contrastive representation learning. We hope OpenSRH will facilitate the clinical translation of rapid optical imaging and real-time ML-based surgical decision support in order to improve the access, safety, and efficacy of cancer surgery in the era of precision medicine.",Datasets & Benchmarks,NeurIPS,2022,Poster,Cheng Jiang;Asadur Zaman Chowdury;Xinhai Hou;Akhil Kondepudi;Christian Freudiger;Kyle Stephen Conway;Sandra Camelo-Piragua;Daniel A Orringer;Honglak Lee;Todd Hollon,True,https://openreview.net/pdf?id=2N8JzuiWZ25
nQZHEunntbJ,AutoWS-Bench-101: Benchmarking Automated Weak Supervision with 100 Labels,"Weak supervision (WS) is a powerful method to build labeled datasets for training supervised models in the face of little-to-no labeled data. It replaces hand-labeling data with aggregating multiple noisy-but-cheap label estimates expressed by labeling functions (LFs). While it has been used successfully in many domains, weak supervision's application scope is limited by the difficulty of constructing labeling functions for domains with complex or high-dimensional features. To address this, a handful of methods have proposed automating the LF design process using a small set of ground truth labels. In this work, we introduce AutoWS-Bench-101: a framework for evaluating automated WS (AutoWS) techniques in challenging WS settings---a set of diverse application domains on which it has been previously difficult or impossible to apply traditional WS techniques. While AutoWS is a promising direction toward expanding the application-scope of WS, the emergence of powerful methods such as zero-shot foundation models reveal the need to understand how AutoWS techniques compare or cooperate with modern zero-shot or few-shot learners. This informs the central question of AutoWS-Bench-101: given an initial set of 100 labels for each task, we ask whether a practitioner should use an AutoWS method to generate additional labels or use some simpler baseline, such as zero-shot predictions from a foundation model or supervised learning. We observe that it is necessary for AutoWS methods to incorporate signal from foundation models if they are to outperform simple few-shot baselines, and AutoWS-Bench-101 promotes future research in this direction. We conclude with a thorough ablation study of AutoWS methods. ",Datasets & Benchmarks,NeurIPS,2022,Poster,Nicholas Roberts;Xintong Li;Tzu-Heng Huang;Dyah Adila;Spencer Schoenberg;Cheng-Yu Liu;Lauren Pick;Haotian Ma;Aws Albarghouthi;Frederic Sala,False,https://openreview.net/pdf?id=nQZHEunntbJ
ddPXQt-gM--,Benchmarking Heterogeneous Treatment Effect Models through the Lens of Interpretability,"Estimating personalized effects of treatments is a complex, yet pervasive problem. To tackle it, recent developments in the machine learning (ML) literature on heterogeneous treatment effect estimation gave rise to many sophisticated, but opaque, tools: due to their flexibility, modularity and ability to learn constrained representations, neural networks in particular have become central to this literature. Unfortunately, the assets of such black boxes come at a cost: models typically involve countless nontrivial operations, making it difficult to understand what they have learned. Yet, understanding these models can be crucial -- in a medical context, for example, discovered knowledge on treatment effect heterogeneity could inform treatment prescription in clinical practice. In this work, we therefore use post-hoc feature importance methods to identify features that influence the model's predictions. This allows us to evaluate treatment effect estimators along a new and important dimension that has been overlooked in previous work: We construct a benchmarking environment to empirically investigate the ability of personalized treatment effect models to identify predictive covariates -- covariates that determine differential responses to treatment. Our benchmarking environment then enables us to provide new insight into the strengths and weaknesses of different types of treatment effects models as we modulate different challenges specific to treatment effect estimation -- e.g. the ratio of prognostic to predictive information, the possible nonlinearity of potential outcomes and the presence and type of confounding.  ",Datasets & Benchmarks,NeurIPS,2022,Poster,Jonathan Crabbé;Alicia Curth;Ioana Bica;Mihaela van der Schaar,True,https://openreview.net/pdf?id=ddPXQt-gM--
GgM5DiAb6A2,FLamby: Datasets and Benchmarks for Cross-Silo Federated Learning in Realistic Healthcare Settings,"Federated Learning (FL) is a novel approach enabling several clients holding sensitive data to collaboratively train machine learning models, without centralizing data. The cross-silo FL setting corresponds to the case of few ($2$--$50$) reliable clients, each holding medium to large datasets, and is typically found in applications such as healthcare, finance, or industry. While previous works have proposed representative datasets for cross-device FL, few realistic healthcare cross-silo FL datasets exist, thereby slowing algorithmic research in this critical application. In this work, we propose a novel cross-silo dataset suite focused on healthcare, FLamby (Federated Learning AMple Benchmark of Your cross-silo strategies), to bridge the gap between theory and practice of cross-silo FL.
FLamby encompasses 7 healthcare datasets with natural splits, covering multiple tasks, modalities, and data volumes, each accompanied with baseline training code. As an illustration, we additionally benchmark standard FL algorithms on all datasets.
Our flexible and modular suite allows researchers to easily download datasets, reproduce results and re-use the different components for their research. FLamby is available at~\\\\url{www.github.com/owkin/flamby}.",Datasets & Benchmarks,NeurIPS,2022,Poster,Jean Ogier du Terrail;Samy-Safwan Ayed;Edwige Cyffers;Felix Grimberg;Chaoyang He;Regis Loeb;Paul Mangold;Tanguy Marchand;Othmane Marfoq;Erum Mushtaq;Boris Muzellec;Constantin Philippenko;Santiago Silva;Maria Teleńczuk;Shadi Albarqouni;Salman Avestimehr;Aurélien Bellet;Aymeric Dieuleveut;Martin Jaggi;Sai Praneeth Karimireddy;Marco Lorenzi;Giovanni Neglia;Marc Tommasi;Mathieu Andreux,True,https://openreview.net/pdf?id=GgM5DiAb6A2
ChWf1E43l4,DABS 2.0: Improved Datasets and Algorithms for Universal Self-Supervision,"Universal self-supervised (SSL) algorithms hold enormous promise for making machine learning accessible to high-impact domains such as protein biology, manufacturing, and genomics. We present DABS 2.0: a set of improved datasets and algorithms for advancing research on universal SSL. We extend the recently-introduced DABS benchmark with the addition of five real-world science and engineering domains: protein biology, bacterial genomics, multispectral satellite imagery, semiconductor wafers, and particle physics, bringing the total number of domains in the benchmark to twelve. We also propose a new universal SSL algorithm, Capri, and a generalized version of masked autoencoding, and apply both on all twelve domains---the most wide-ranging exploration of SSL yet. We find that multiple algorithms show gains across domains, outperforming previous baselines. In addition, we demonstrate the usefulness of DABS for scientific study of SSL by investigating the optimal corruption rate for each algorithm, showing that the best setting varies based on the domain. Code will be released at http://github.com/alextamkin/dabs}{http://github.com/alextamkin/dabs",Datasets & Benchmarks,NeurIPS,2022,Poster,Alex Tamkin;Gaurab Banerjee;Mohamed Owda;Vincent Liu;Shashank Rammoorthy;Noah Goodman,True,https://openreview.net/pdf?id=ChWf1E43l4
VtEEpi-dGlt,Kantorovich Strikes Back! Wasserstein GANs are not Optimal Transport?,"Wasserstein Generative Adversarial Networks (WGANs) are the popular generative models built on the theory of Optimal Transport (OT) and the Kantorovich duality. Despite the success of WGANs, it is still unclear how well the underlying OT dual solvers approximate the OT cost (Wasserstein-1 distance, W1) and the OT gradient needed to update the generator. In this paper, we address these questions. We construct 1-Lipschitz functions and use them to build ray monotone transport plans. This strategy yields pairs of continuous benchmark distributions with the analytically known OT plan, OT cost and OT gradient in high-dimensional spaces such as spaces of images. We thoroughly evaluate popular WGAN dual form solvers (gradient penalty, spectral normalization, entropic regularization, etc.) using these benchmark pairs. Even though these solvers perform well in WGANs, none of them faithfully compute W1 in high dimensions. Nevertheless, many provide a meaningful approximation of the OT gradient. These observations suggest that these solvers should not be treated as good estimators of W1 but to some extent they indeed can be used in variational problems requiring the minimization of W1.",Datasets & Benchmarks,NeurIPS,2022,Poster,Alexander Korotin;Alexander Kolesov;Evgeny Burnaev,True,https://openreview.net/pdf?id=VtEEpi-dGlt
UiRSQykVNiC,Myriad: a real-world testbed to bridge trajectory optimization and deep learning,"We present Myriad, a testbed written in JAX which enables machine learning researchers to benchmark imitation learning and reinforcement learning algorithms against trajectory optimization-based methods in real-world environments. Myriad contains 17 optimal control problems presented in continuous time which span medicine, ecology, epidemiology, and engineering. As such, Myriad strives to serve as a stepping stone towards application of modern machine learning techniques for impactful real-world tasks. The repository also provides machine learning practitioners access to trajectory optimization techniques, not only for standalone use, but also for integration within a typical automatic differentiation workflow. Indeed, the combination of classical control theory and deep learning in a fully GPU-compatible package unlocks potential for new algorithms to arise. We present one such novel approach for use in dynamics learning and control tasks. Trained in a fully end-to-end fashion, our model leverages an implicit planning module over neural ordinary differential equations, enabling simultaneous learning and planning with unknown environment dynamics. All environments, optimizers and tools are available in the software package at \\\\url{https://github.com/nikihowe/myriad}.",Datasets & Benchmarks,NeurIPS,2022,Poster,Nikolaus H. R. Howe;Simon Dufort-Labbé;Nitarshan Rajkumar;Pierre-Luc Bacon,True,https://openreview.net/pdf?id=UiRSQykVNiC
CZeIOfCjMf,HAPI: A Large-scale Longitudinal Dataset of Commercial ML API Predictions,"Commercial ML APIs offered by providers such as Google, Amazon and Microsoft have dramatically simplified ML adoptions in many applications. Numerous companies and academics pay to use ML APIs for tasks such as object detection, OCR and sentiment analysis. Different ML APIs tackling the same task can have very heterogeneous performances. Moreover, the ML models underlying the APIs also evolve over time. As ML APIs rapidly become a valuable marketplace and an integral part of analytics, it is critical to systematically study and compare different APIs with each other and to characterize how individual APIs change over time. However, this practically important topic is currently underexplored due to the lack of data. In this paper, we present HAPI (History of APIs), a longitudinal dataset of 1,761,417 instances of commercial ML API applications (involving APIs from Amazon, Google, IBM, Microsoft and other providers) across diverse tasks including image tagging, speech recognition, and text mining from 2020 to 2022. Each instance consists of a query input for an API (e.g., an image or text) along with the API’s output prediction/annotation and confidence scores. HAPI is the first large-scale dataset of ML API usages and is a unique resource for studying ML  as-a-service (MLaaS). As examples of the types of analyses that HAPI enables, we show that ML APIs’ performance changes substantially over time—several APIs’ accuracies dropped on specific benchmark datasets. Even when the API’s aggregate performance stays steady, its error modes can shift across different subtypes of data between 2020 and 2022. Such changes can substantially impact the entire analytics pipelines that use some ML API as a component. We further use HAPI to study commercial APIs’ performance disparities across demographic subgroups over time. HAPI can stimulate more research in the growing field of MLaaS.",Datasets & Benchmarks,NeurIPS,2022,Poster,Lingjiao Chen;Zhihua Jin;Sabri Eyuboglu;Christopher Re;Matei Zaharia;James Y. Zou,True,https://openreview.net/pdf?id=CZeIOfCjMf
D1MOK2t2t2,BEDD: The MineRL BASALT Evaluation and Demonstrations Dataset for Training and Benchmarking Agents that Solve Fuzzy Tasks,"The MineRL BASALT competition has served to catalyze advances in learning from human feedback through four hard-to-specify tasks in Minecraft, such as create and photograph a waterfall. Given the completion of two years of BASALT competitions, we offer to the community a formalized benchmark through the BASALT Evaluation and Demonstrations Dataset (BEDD), which serves as a resource for algorithm development and performance assessment. BEDD consists of a collection of 26 million image-action pairs from nearly 14,000 videos of human players completing the BASALT tasks in Minecraft. It also includes over 3,000 dense pairwise human evaluations of human and algorithmic agents. These comparisons serve as a fixed, preliminary leaderboard for evaluating newly-developed algorithms.  To enable this comparison, we present a streamlined codebase for benchmarking new algorithms against the leaderboard. In addition to presenting these datasets, we conduct a detailed analysis of the data from both datasets to guide algorithm development and evaluation. The released code and data are available at https://github.com/minerllabs/basalt-benchmark.",Datasets & Benchmarks,NeurIPS,2023,Oral,Stephanie Milani;Anssi Kanervisto;Karolis Ramanauskas;Sander V Schulhoff;Brandon Houghton;Rohin Shah,True,https://openreview.net/pdf?id=D1MOK2t2t2
OL2JQoO0kq,Quilt-1M: One Million Image-Text Pairs for Histopathology,"Recent accelerations in multi-modal applications have been made possible with the plethora of image and text data available online. However, the scarcity of analogous data in the medical field, specifically in histopathology, has slowed comparable progress. 
To enable similar representation learning for histopathology, we turn to YouTube, an untapped resource of videos, offering $1,087$ hours of valuable educational histopathology videos from expert clinicians.
From YouTube, we curate QUILT: a large-scale vision-language dataset consisting of $802, 144$ image and text pairs.
QUILT was automatically curated using a mixture of models, including large language models, handcrafted algorithms, human knowledge databases, and automatic speech recognition.
In comparison, the most comprehensive datasets curated for histopathology amass only around $200$K samples.
We combine QUILT with datasets from other sources, including Twitter, research papers, and the internet in general, to create an even larger dataset: QUILT-1M, with $1$M paired image-text samples, marking it as the largest vision-language histopathology dataset to date. 
We demonstrate the value of QUILT-1M by fine-tuning a pre-trained CLIP model. 
Our model outperforms state-of-the-art models on both zero-shot and linear probing tasks for classifying new histopathology images across $13$ diverse patch-level datasets of $8$ different sub-pathologies and cross-modal retrieval tasks.",Datasets & Benchmarks,NeurIPS,2023,Oral,Wisdom Oluchi Ikezogwo;Mehmet Saygin Seyfioglu;Fatemeh Ghezloo;Dylan Stefan Chan Geva;Fatwir Sheikh Mohammed;Pavan Kumar Anand;Ranjay Krishna;Linda Shapiro,True,https://openreview.net/pdf?id=OL2JQoO0kq
Qf8uzIT1OK,Ethical Considerations for Responsible Data Curation,"Human-centric computer vision (HCCV) data curation practices often neglect privacy and bias concerns, leading to dataset retractions and unfair models. HCCV datasets constructed through nonconsensual web scraping lack crucial metadata for comprehensive fairness and robustness evaluations. Current remedies are post hoc, lack persuasive justification for adoption, or fail to provide proper contextualization for appropriate application. Our research focuses on proactive, domain-specific recommendations, covering purpose, privacy and consent, and diversity, for curating HCCV evaluation datasets, addressing privacy and bias concerns. We adopt an ante hoc reflective perspective, drawing from current practices, guidelines, dataset withdrawals, and audits, to inform our considerations and recommendations.",Datasets & Benchmarks,NeurIPS,2023,Oral,Jerone Andrews;Dora Zhao;William Thong;Apostolos Modas;Orestis Papakyriakopoulos;Alice Xiang,False,https://openreview.net/pdf?id=Qf8uzIT1OK
VH1vxapUTs,Mesogeos: A multi-purpose dataset for data-driven wildfire modeling in the Mediterranean,"We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire modeling in the Mediterranean. Mesogeos integrates variables representing wildfire drivers (meteorology, vegetation, human activity) and historical records of wildfire ignitions and burned areas for 17 years (2006-2022). It is designed as a cloud-friendly spatio-temporal dataset, namely a datacube, harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The datacube structure offers opportunities to assess machine learning (ML) usage in various wildfire modeling tasks. We extract two ML-ready datasets that establish distinct tracks to demonstrate this potential: (1) short-term wildfire danger forecasting and (2) final burned area estimation given the point of ignition. We define appropriate metrics and baselines to evaluate the performance of models in each track. By publishing the datacube, along with the code to create the ML datasets and models, we encourage the community to foster the implementation of additional tracks for mitigating the increasing threat of wildfires in the Mediterranean.",Datasets & Benchmarks,NeurIPS,2023,Oral,Spyros Kondylatos;Ioannis Prapas;Gustau Camps-Valls;Ioannis Papoutsis,True,https://openreview.net/pdf?id=VH1vxapUTs
VSJotgbPHF,OpenAssistant Conversations - Democratizing Large Language Model Alignment,"Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT.
Alignment techniques such as supervised fine-tuning (\\\\textit{SFT}) and  reinforcement learning from human feedback (\\\\textit{RLHF}) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains.
However, state-of-the-art alignment techniques like \\\\textit{RLHF} rely on high-quality human feedback data, which is expensive to create and often remains proprietary.
In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages in 35 different languages, annotated with 461,292 quality ratings, resulting in over 10,000 complete and fully annotated conversation trees.
The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers.
Models trained on OpenAssistant Conversations show consistent improvements on standard benchmarks over respective base models.
We release our code\\\\footnote{\\\\git} and data\\\\footnote{\\\\data} under a fully permissive licence.",Datasets & Benchmarks,NeurIPS,2023,Oral,Andreas Köpf;Yannic Kilcher;Dimitri von Rütte;Sotiris Anagnostidis;Zhi Rui Tam;Keith Stevens;Abdullah Barhoum;Duc Minh Nguyen;Oliver Stanley;Richárd Nagyfi;Shahul ES;Sameer Suri;David Alexandrovich Glushkov;Arnav Varma Dantuluri;Andrew Maguire;Christoph Schuhmann;Huu Nguyen;Alexander Julian Mattick,True,https://openreview.net/pdf?id=VSJotgbPHF
dVaWCDMBof,DataComp: In search of the next generation of multimodal datasets,"Multimodal datasets are a critical component in recent breakthroughs such as CLIP, Stable Diffusion and GPT-4, yet their design does not receive the same research attention as model architectures or training algorithms. To address this shortcoming in the machine learning ecosystem, we introduce DataComp, a testbed for dataset experiments centered around a new candidate pool of 12.8 billion image-text pairs from Common Crawl. Participants in our benchmark design new filtering techniques or curate new data sources and then evaluate their new dataset by running our standardized CLIP training code and testing the resulting model on 38 downstream test sets. Our benchmark consists of multiple compute scales spanning four orders of magnitude, which enables the study of scaling trends and makes the benchmark accessible to researchers with varying resources. Our baseline experiments show that the DataComp workflow leads to better training sets. Our best baseline, DataComp-1B, enables training a CLIP ViT-L/14 from scratch to 79.2% zero-shot accuracy on ImageNet, outperforming OpenAI's CLIP ViT-L/14 by 3.7 percentage points while using the same training procedure and compute. We release \\\\datanet and all accompanying code at www.datacomp.ai.",Datasets & Benchmarks,NeurIPS,2023,Oral,Samir Yitzhak Gadre;Gabriel Ilharco;Alex Fang;Jonathan Hayase;Georgios Smyrnis;Thao Nguyen;Ryan Marten;Mitchell Wortsman;Dhruba Ghosh;Jieyu Zhang;Eyal Orgad;Rahim Entezari;Giannis Daras;Sarah M Pratt;Vivek Ramanujan;Yonatan Bitton;Kalyani Marathe;Stephen Mussmann;Richard Vencu;Mehdi Cherti;Ranjay Krishna;Pang Wei Koh;Olga Saukh;Alexander Ratner;Shuran Song;Hannaneh Hajishirzi;Ali Farhadi;Romain Beaumont;Sewoong Oh;Alex Dimakis;Jenia Jitsev;Yair Carmon;Vaishaal Shankar;Ludwig Schmidt,True,https://openreview.net/pdf?id=dVaWCDMBof
g7OX2sOJtn,LeanDojo: Theorem Proving with Retrieval-Augmented Language Models,"Large language models (LLMs) have shown promise in proving formal theorems using proof assistants such as Lean. However, existing methods are difficult to reproduce or build on, due to private code, data, and large compute requirements. This has created substantial barriers to research on machine learning methods for theorem proving. This paper removes these barriers by introducing LeanDojo: an open-source Lean playground consisting of toolkits, data, models, and benchmarks. LeanDojo extracts data from Lean and enables interaction with the proof environment programmatically. It contains fine-grained annotations of premises in proofs, providing valuable data for premise selection—a key bottleneck in theorem proving. Using this data, we develop ReProver (Retrieval-Augmented Prover): an LLM-based prover augmented with retrieval for selecting premises from a vast math library. It is inexpensive and needs only one GPU week of training. Our retriever leverages LeanDojo's program analysis capability to identify accessible premises and hard negative examples, which makes retrieval much more effective. Furthermore, we construct a new benchmark consisting of 98,734 theorems and proofs extracted from Lean's math library. It features challenging data split requiring the prover to generalize to theorems relying on novel premises that are never used in training. We use this benchmark for training and evaluation, and experimental results demonstrate the effectiveness of ReProver over non-retrieval baselines and GPT-4. We thus provide the first set of open-source LLM-based theorem provers without any proprietary datasets and release it under a permissive MIT license to facilitate further research.",Datasets & Benchmarks,NeurIPS,2023,Oral,Kaiyu Yang;Aidan M Swope;Alex Gu;Rahul Chalamala;Peiyang Song;Shixing Yu;Saad Godil;Ryan Prenger;Anima Anandkumar,True,https://openreview.net/pdf?id=g7OX2sOJtn
j2wasUypqN,MetaBox: A Benchmark Platform for Meta-Black-Box Optimization with Reinforcement Learning,"Recently, Meta-Black-Box Optimization with Reinforcement Learning (MetaBBO-RL) has showcased the power of leveraging RL at the meta-level to mitigate manual fine-tuning of low-level black-box optimizers. However, this field is hindered by the lack of a unified benchmark. To fill this gap, we introduce MetaBox, the first benchmark platform expressly tailored for developing and evaluating MetaBBO-RL methods. MetaBox offers a flexible algorithmic template that allows users to effortlessly implement their unique designs within the platform. Moreover, it provides a broad spectrum of over 300 problem instances, collected from synthetic to realistic scenarios, and an extensive library of 19 baseline methods, including both traditional black-box optimizers and recent MetaBBO-RL methods. Besides, MetaBox introduces three standardized performance metrics, enabling a more thorough assessment of the methods. In a bid to illustrate the utility of MetaBox for facilitating rigorous evaluation and in-depth analysis, we carry out a wide-ranging benchmarking study on existing MetaBBO-RL methods. Our MetaBox is open-source and accessible at: https://github.com/GMC-DRL/MetaBox.",Datasets & Benchmarks,NeurIPS,2023,Oral,Zeyuan Ma;Hongshu Guo;Jiacheng Chen;Zhenrui Li;Guojun Peng;Yue-Jiao Gong;Yining Ma;Zhiguang Cao,True,https://openreview.net/pdf?id=j2wasUypqN
kaHpo8OZw2,DecodingTrust: A Comprehensive Assessment of Trustworthiness in GPT Models,"Generative Pre-trained Transformer (GPT) models have exhibited exciting progress in capabilities, capturing the interest of practitioners and the public alike. Yet, while the literature on the trustworthiness of GPT models remains limited, practitioners have proposed employing capable GPT models for sensitive applications to healthcare and finance – where mistakes can be costly. To this end, this work proposes a comprehensive trustworthiness evaluation for large language models with a focus on GPT-4 and GPT-3.5, considering diverse perspectives – including toxicity, stereotype bias, adversarial robustness, out-of-distribution robustness, robustness on adversarial demonstrations, privacy, machine ethics, and fairness. Based on our evaluations, we discover previously unpublished vulnerabilities to trustworthiness threats. For instance, we find that GPT models can be easily misled to generate toxic and biased outputs and leak private information in both training data and conversation history. We also find that although GPT-4 is usually more trustworthy than GPT-3.5 on standard benchmarks, GPT-4 is more vulnerable given jailbreaking system or user prompts, potentially due to the reason that GPT-4 follows the (misleading) instructions more precisely. Our work illustrates a comprehensive trustworthiness evaluation of GPT models and sheds light on the trustworthiness gaps. Our benchmark is publicly available at https://decodingtrust.github.io/.",Datasets & Benchmarks,NeurIPS,2023,Oral,Boxin Wang;Weixin Chen;Hengzhi Pei;Chulin Xie;Mintong Kang;Chenhui Zhang;Chejian Xu;Zidi Xiong;Ritik Dutta;Rylan Schaeffer;Sang T. Truong;Simran Arora;Mantas Mazeika;Dan Hendrycks;Zinan Lin;Yu Cheng;Sanmi Koyejo;Dawn Song;Bo Li,True,https://openreview.net/pdf?id=kaHpo8OZw2
7NR2ZVzZxx,LogicBench: A Benchmark for Evaluation of Logical Reasoning,"Recently developed large language models (LLMs) have been shown to perform remarkably well on a wide range of language understanding tasks. But, can they really ""Reason"" over the natural language? This question has been receiving significant research attention and a number of reasoning skills such as commonsense, numerical, and qualitative have been studied. However, the crucial skill pertaining to 'logical reasoning' has remained underexplored. Existing work investigating this reasoning ability has focused only on a couple of axioms (such as modus ponens and modus tollens) of propositional and first-order logic. To study logical reasoning,  we introduce LogicBench, a systematically created natural language question-answering dataset encompassing 25 reasoning patterns spanning over propositional, first-order, and non-monotonic logics. Key steps of our dataset construction consist of (1) controlled generation of sentences and their negations containing different ontologies, (2) (context, question, answer) triplets creation using heuristically designed templates, and (3) semantic variations of triplets adding more diversity. We first evaluate easily accessible and widely used LLMs such as GPT-3, ChatGPT, and FLAN-T5 and show that they do not fare well on LogicBench, achieving just above random accuracy on average (~52%). Then, we show that LLMs trained using our data exhibit a better understanding of logical reasoning leading to performance improvements on several existing logical reasoning datasets such as LogicNLI, FOLIO, LogiQA, and ReClor.",Datasets & Benchmarks,NeurIPS,2023,Reject,Mihir Parmar;Neeraj Varshney;Nisarg Patel;Santosh Mashetty;Man Luo;Arindam Mitra;Chitta Baral,True,https://openreview.net/pdf?id=7NR2ZVzZxx
7fRThuXp3h,Off-the-Grid MARL: Datasets with Baselines for Offline Multi-Agent Reinforcement Learning,"Being able to harness the power of large datasets for developing cooperative multi-agent controllers promises to unlock enormous value for real-world applications. 
Many important industrial systems are multi-agent in nature and are difficult to model using bespoke simulators. 
However, in industry, distributed processes can often be recorded during operation, and large quantities of demonstrative data stored.
Offline multi-agent reinforcement learning (MARL) provides a promising paradigm for building effective decentralised controllers from such datasets. 
However, offline MARL is still in its infancy and therefore lacks standardised benchmark datasets and baselines typically found in more mature subfields of reinforcement learning (RL). 
These deficiencies make it difficult for the community to sensibly measure progress. 
In this work, we aim to fill this gap by releasing \\\\emph{off-the-grid MARL (OG-MARL)}: a growing repository of high-quality datasets with baselines for cooperative offline MARL research.
Our datasets provide settings that are characteristic of real-world systems, including complex environment dynamics, heterogeneous agents, non-stationarity, many agents, partial observability, suboptimality, sparse rewards and demonstrated coordination.
For each setting, we provide a range of different dataset types (e.g. \\\\texttt{Good}, \\\\texttt{Medium}, \\\\texttt{Poor}, and \\\\texttt{Replay}) and profile the composition of experiences for each dataset.
We hope that OG-MARL will serve the community as a reliable source of datasets and help drive progress, while also providing an accessible entry point for researchers new to the field.",Datasets & Benchmarks,NeurIPS,2023,Reject,Juan Claude Formanek;Asad Jeewa;Jonathan Phillip Shock;Arnu Pretorius,True,https://openreview.net/pdf?id=7fRThuXp3h
OdylEgIR1D,PufferLib: Making Reinforcement Learning Libraries and Environments Play Nice,"Reinforcement learning (RL) frameworks often falter in complex environments due to inherent simplifying assumptions. This gap necessitates labor-intensive and error-prone intermediate conversion layers, limiting the applicability of RL as a whole. To address this challenge, we introduce PufferLib, a novel middleware solution. PufferLib transforms complex environments into a broadly compatible, vectorized format, eliminating the need for bespoke conversion layers and enabling more rigorous testing. Users interact with PufferLib through concise bindings, significantly reducing the technical overhead. We release PufferLib's complete source code under the MIT license, a pip module, a containerized setup, comprehensive documentation, and example integrations. We also maintain a community Discord channel to facilitate support and discussion.",Datasets & Benchmarks,NeurIPS,2023,Reject,Joseph Suarez,False,https://openreview.net/pdf?id=OdylEgIR1D
uZedGmxGUg,EasyTPP: Towards Open Benchmarking the Temporal Point Processes,"Continuous-time event sequences play a vital role in real-world domains such as healthcare, finance, online shopping, social networks, and so on. To model such data, temporal point processes (TPPs) have emerged as the most natural and competitive models, making a significant impact in both academic and application communities. Despite the emergence of many powerful models in recent years, there hasn't been a central benchmark for these models and future research endeavors. This lack of standardization impedes researchers and practitioners from comparing methods and reproducing results, potentially slowing down progress in this field. In this paper, we present EasyTPP, the first central repository of research assets (e.g., data, models, evaluation programs, documentations) in the area of event sequence modeling. Our EasyTPP makes several unique contributions to this area: a unified interface of using existing datasets and adding new datasets; a wide range of evaluation programs that are easy to use and extend as well as facilitate reproducible research; 
implementations of popular neural TPPs, together with a rich library of modules by composing which one could quickly build complex models. Our benchmark is open-sourced: all the data and implementation can be found at this \\\\href{https://github.com/ant-research/EasyTemporalPointProcess}{\\\\textcolor{blue}{Github repository}}.} We will actively maintain this benchmark and welcome contributions from other researchers and practitioners. Our benchmark will help promote reproducible research in this field, thus accelerating research progress as well as making more significant real-world impacts.",Datasets & Benchmarks,NeurIPS,2023,Reject,Siqiao Xue;Xiaoming Shi;Zhixuan Chu;Yan Wang;Hongyan Hao;Caigao JIANG;Chen Pan;James Y. Zhang;Qingsong Wen;JUN ZHOU;Hongyuan Mei,False,https://openreview.net/pdf?id=uZedGmxGUg
qkhpbRNSSE,ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics,"We introduce ProofNet, a benchmark for autoformalization and formal proving of undergraduate-level mathematics. The ProofNet benchmarks consists of 371 examples, each consisting of a formal theorem statement in Lean 3, a natural language theorem statement, and a natural language proof. The problems are primarily drawn from popular undergraduate pure mathematics textbooks and cover topics such as real and complex analysis, linear algebra, abstract algebra, and topology. We intend for ProofNet to be a challenging benchmark that will drive progress in autoformalization and automatic theorem proving.  We report baseline results on statement autoformalization via in-context learning. Moreover we demonstrate improvements over our baselines by applying prompt retrieval and distilled backtranslation.",Datasets & Benchmarks,NeurIPS,2023,Reject,Zhangir Azerbayev;Bartosz Piotrowski;Hailey Schoelkopf;Edward W. Ayers;Dragomir Radev;Jeremy Avigad,True,https://openreview.net/pdf?id=qkhpbRNSSE
hHKBiMPfCj,Rectifying Open-Set Object Detection: Proper Evaluation and a Taxonomy,"Open-set object detection (OSOD), a task involving the detection of unknown objects while accurately detecting known objects, has recently gained attention. However, we identify a fundamental issue with the problem formulation employed in current OSOD studies. Inherent to object detection is knowing ''what to detect,'' which contradicts the idea of identifying ''unknown'' objects. 
This sets OSOD apart from open-set recognition (OSR). This contradiction complicates a proper evaluation of methods' performance, a fact that previous studies have overlooked. Next, we propose a novel formulation wherein detectors are required to detect both known and unknown classes within specified super-classes of object classes. This new formulation is free from the aforementioned issues and has practical applications. Finally, we design benchmark tests utilizing existing datasets and report the experimental evaluation of existing OSOD methods. As a byproduct, we introduce a taxonomy of OSOD, resolving confusion prevalent in the literature. We anticipate that our study will encourage the research community to reconsider OSOD and facilitate progress in the right direction.",Datasets & Benchmarks,NeurIPS,2023,Reject,Yusuke Hosoya;Masanori Suganuma;Takayuki Okatani,False,https://openreview.net/pdf?id=hHKBiMPfCj
calKOSmBxj,HoK3v3: an Environment for Generalization in Heterogeneous Multi-agent Reinforcement Learning,"We introduce HoK3v3, a 3v3 game environment for multi-agent reinforcement learning (MARL) research, based on Honor of Kings, the world's most popular Multiplayer Online Battle Arena (MOBA) game at present. Due to the presence of diverse heroes and lineups (a.k.a., hero combinations), this environment poses a unique challenge for generalization in heterogeneous MARL. A detailed description of the tasks contained in HoK3v3, including observations, structured actions, and multi-head reward specifications, has been provided. We validate the environment by applying conventional MARL baseline algorithms.  We examine the challenges of generalization through experiments involving the 3v3 MOBA full game task and its decomposed sub tasks, executed by lineups picked from the hero pool. The results indicate the limitations of existing RL methods in addressing scenarios that require heterogeneous generalization. All of the code, tutorial, encrypted game engine, can be accessed at: https://github.com/tencent-ailab/hok_env.",Datasets & Benchmarks,NeurIPS,2023,Reject,Lin Liu;Jianzhun Shao;Xinkai Chen;Yun Qu;Boyuan Wang;Zhenbin Ye;Yuexuan Tu;Hongyang Qin;Yang Jun Feng;Lin Lai;Yuanqin Wang;Meng Meng;Wenjun Wang;Xiyang Ji;QIANG FU;Lanxiao Huang;Minwen Deng;Yang Wei;Houqiang Li;Wengang Zhou;Ning Xie;Xiangyang Ji;Lvfang Tao;Lin Yuan;Juchao Zhuo;YANG GUANG;Deheng Ye,False,https://openreview.net/pdf?id=calKOSmBxj
dUFf0pgkC7,HHD-Ethiopic: A Historical Handwritten Dataset for Ethiopic OCR with Baseline Models and Human-level Performance,"This paper introduces HHD-Ethiopic, a new OCR dataset for historical handwritten Ethiopic script, characterized by a unique syllabic writing system, low resource availability, and complex orthographic diacritics. The dataset consists of roughly 80,000 annotated text-line images from 1700 pages of $18^{th}$ to $20^{th}$ century documents, including a training set with text-line images from the $19^{th}$ to $20^{th}$ century and two test sets. One is distributed similarly to the training set with nearly 6,000 text-line images, and the other contains only images from the $18^{th}$ century manuscripts, with around 16,000 images. The former test set allows us to check baseline performance in the classical IID setting (Independently and Identically Distributed), while the latter addresses a more realistic setting in which the test set is drawn from a different distribution than the training set (Out-Of-Distribution or OOD). Multiple annotators labeled all text-line images for the HHD-Ethiopic dataset, and an expert supervisor double-checked them. We assessed human-level recognition performance and compared it with state-of-the-art OCR models using the Character Error Rate (CER) metric. Our results show that the model performed comparably to human-level recognition on the $18^{th}$ century test set and outperformed humans on the IID test set. However, the unique challenges posed by the Ethiopic script, such as detecting complex diacritics, still present difficulties for the models. Our baseline evaluation and HHD-Ethiopic dataset will stimulate further research on tailored OCR techniques for the Ethiopic script. The HHD-Ethiopic dataset and the code are  publicly available at https://github.com/bdu-birhanu/HHD-Ethiopic",Datasets & Benchmarks,NeurIPS,2023,Reject,Birhanu Hailu Belay;Isabelle Guyon;Tadele Mengiste;Bezawork Tilahun;Marcus Liwicki;Tesfa Tegegne;Romain Egele;Tsiyon Worku,True,https://openreview.net/pdf?id=dUFf0pgkC7
8eVgdwKs2N,G4SATBench: Benchmarking and Advancing SAT Solving with Graph Neural Networks,"Graph neural networks (GNNs) have recently emerged as a promising approach for solving the Boolean Satisfiability Problem (SAT), offering potential alternatives to traditional backtracking or local search SAT solvers. However, despite the growing volume of literature in this field, there remains a notable absence of a unified dataset and a fair benchmark to evaluate and compare existing approaches. To address this crucial gap, we present G4SATBench, the first benchmark study that establishes a comprehensive evaluation framework for GNN-based SAT solvers. In G4SATBench, we meticulously curate a large and diverse set of SAT datasets comprising 7 problems with 3 difficulty levels and benchmark a broad range of GNN models across various prediction tasks, training objectives, and inference algorithms. To explore the learning abilities and comprehend the strengths and limitations of GNN-based SAT solvers, we also compare their solving processes with the heuristics in search-based SAT solvers. Our empirical results provide valuable insights into the performance of GNN-based SAT solvers and further suggest that existing GNN models can effectively learn a solving strategy akin to greedy local search but struggle to learn backtracking search in the latent space.",Datasets & Benchmarks,NeurIPS,2023,Reject,Zhaoyu Li;Jinpei Guo;Xujie Si,True,https://openreview.net/pdf?id=8eVgdwKs2N
xbUz5DsW5T,Visual Abductive Reasoning Meets Driving Hazard Prediction: Problem Formulation and Dataset,"This paper addresses the problem of predicting hazards that drivers may encounter while driving a car. We formulate it as a task of anticipating impending accidents using a single input image captured by car dashcams. Unlike existing approaches to driving hazard prediction that rely on computational simulations or anomaly detection from videos, this study focuses on high-level inference from static images. The problem needs predicting and reasoning about future events based on uncertain observations, which falls under visual abductive reasoning. To enable research in this understudied area, a new dataset named the DHPR (Driving Hazard Prediction and Reasoning) dataset is created. The dataset consists of 15K dashcam images of street scenes, and each image is associated with a tuple containing car speed, a hypothesized hazard description, and visual entities present in the scene. These are annotated by human annotators, who identify risky scenes and provide descriptions of potential accidents that could occur a few seconds later. We present several baseline methods and evaluate their performance on our dataset, identifying remaining issues and discussing future directions. This study contributes to the field by introducing a novel problem formulation and dataset, enabling researchers to explore the potential of multi-modal AI for driving hazard prediction.",Datasets & Benchmarks,NeurIPS,2023,Reject,Korawat Charoenpitaks;Van-Quang Nguyen;Masanori Suganuma;Masahiro Takahashi;Ryoma Niihara;Takayuki Okatani,True,https://openreview.net/pdf?id=xbUz5DsW5T
LJhfKeqZdu,RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark,"We introduce RL4CO, an extensive reinforcement learning (RL) for combinatorial optimization (CO) benchmark. RL4CO employs state-of-the-art software libraries as well as best practices in implementation, such as modularity and configuration management, to be efficient and easily modifiable by researchers for adaptations of neural network architecture, environments, and RL algorithms. Contrary to the existing focus on specific tasks like the traveling salesman problem (TSP) for performance assessment, we underline the importance of scalability and generalization capabilities for diverse CO tasks. We also systematically benchmark zero-shot generalization, sample efficiency, and adaptability to changes in data distributions of various models. Our experiments show that some recent state-of-the-art methods fall behind their predecessors when evaluated using these metrics, suggesting the necessity for a more balanced view of the performance of neural CO (NCO) solvers. We hope RL4CO will encourage the exploration of novel solutions to complex real-world tasks, allowing the NCO community to compare with existing methods through a standardized interface that decouples the science from software engineering. We make our library publicly available at https://github.com/kaist-silab/rl4co.",Datasets & Benchmarks,NeurIPS,2023,Reject,Federico Berto;Chuanbo Hua;Junyoung Park;Minsu Kim;Hyeonah Kim;Jiwoo Son;Haeyeon Kim;Joungho Kim;Jinkyoo Park,False,https://openreview.net/pdf?id=LJhfKeqZdu
argZAtDMMF,FLOP: Tasks for Fitness Landscapes Of Protein wildtypes,"Protein engineering has the potential to create optimized protein variants with improved properties and function. 
An initial step in the protein optimization process typically consists of a search among natural (wildtype) sequences to find the naturally occurring proteins with the most desirable properties. 
Promising candidates from this initial discovery phase then form the basis of the second step: a more local optimization procedure, exploring the space of variants separated from this candidate by a number of mutations. 
While considerable progress has been made on evaluating machine learning methods on single protein datasets, benchmarks of data-driven approaches for global fitness landscape exploration are still lacking. 
In this paper, we have carefully curated a representative benchmark dataset, which reflects industrially relevant scenarios for the initial wildtype discovery phase of protein engineering.
We focus on exploration within a protein family, and investigate the downstream predictive power of various protein representation paradigms, i.e., protein language model-based representations, structure-based representations, and evolution-based representations.
Our benchmark highlights the importance of coherent split strategies, and how we can be misled into overly optimistic estimates of the state of the field. The codebase and data can be accessed via https://github.com/petergroth/FLOP.",Datasets & Benchmarks,NeurIPS,2023,Reject,Peter Mørch Groth;Richard Michael;Jesper Salomon;Pengfei Tian;Wouter Boomsma,True,https://openreview.net/pdf?id=argZAtDMMF
fnQ2QPl5n7,GUARD: A Safe Reinforcement Learning Benchmark,"Due to the trial-and-error nature, it is typically challenging to apply RL algorithms to safety-critical real-world applications, such as autonomous driving, human-robot interaction, robot manipulation, etc, where such errors are not tolerable. Recently, safe RL (i.e. constrained RL) has emerged rapidly in the literature, in which the agents explore the environment while satisfying constraints. Due to the diversity of algorithms and tasks, it remains difficult to compare existing safe RL algorithms. To fill that gap, we introduce GUARD, a Generalized Unified SAfe Reinforcement Learning Development Benchmark. GUARD has several advantages compared to existing benchmarks. First, GUARD is a generalized benchmark with a wide variety of RL agents, tasks, and safety constraint specifications. Second, GUARD comprehensively covers state-of-the-art safe RL algorithms with self-contained implementations. Third, GUARD is highly customizable in tasks and algorithms. We present a comparison of state-of-the-art safe RL algorithms in various task settings using GUARD and establish baselines that future work can build on.",Datasets & Benchmarks,NeurIPS,2023,Reject,Weiye Zhao;Rui Chen;Yifan Sun;Ruixuan Liu;Tianhao Wei;Changliu Liu,True,https://openreview.net/pdf?id=fnQ2QPl5n7
q1NaqDadKM,LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models,"Large Vision-Language Models (LVLM) have recently played a dominant role in multimodal vision-language learning. Despite the great success, it lacks a holistic evaluation of their efficacy. This paper presents a comprehensive evaluation of publicly available large multimodal models by building an LVLM evaluation Hub (LVLM-eHub). Our LVLM-eHub consists of $8$ representative LVLMs such as InstructBLIP and MiniGPT-4, which are thoroughly evaluated by a quantitative capability evaluation and an online arena platform. The former evaluates $6$ categories of multimodal capabilities of LVLMs such as visual question answering and embodied artificial intelligence on $40$ standard text-related visual benchmarks, while the latter provides the user-level evaluation of LVLMs in an open-world question-answering scenario. The study reveals several innovative findings. First, Instruction-tuned LVLM with massive in-domain data such as InstructBLIP heavily overfits many existing tasks, generalizing poorly in the open-world scenario. Second, Instruction-tuned LVLM with moderate instruction-following data may result in object hallucination issues (i.e., generate objects that are inconsistent with target images in the descriptions). It either makes the current evaluation metric such as CIDER for image captioning ineffective or generates wrong answers. Third, employing a multi-turn reasoning evaluation framework can mitigate the issue of object hallucination, shedding light on developing an effective metric for LVLM evaluation. The findings provide a foundational framework for the conception and assessment of innovative strategies aimed at enhancing zero-shot multimodal techniques. The evaluation pipeline will be available at [vlarena page](https://github.com/OpenGVLab/Multi-Modality-Arena/tree/main/LVLM_evaluation).",Datasets & Benchmarks,NeurIPS,2023,Reject,Peng Xu;Wenqi Shao;Kaipeng Zhang;Peng Gao;Shuo Liu;Fanqing Meng;Siyuan Huang;Meng Lei;Ping Luo;Yu Qiao,False,https://openreview.net/pdf?id=q1NaqDadKM
9FLkxTGY3B,${\\\\rm EFO}_k$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation,"To answer complex queries on knowledge graphs, logical reasoning over incomplete knowledge is required due to the open-world assumption. Learning-based methods are essential because they are capable of generalizing over unobserved knowledge. Therefore, an appropriate dataset is fundamental to both obtaining and evaluating such methods under this paradigm. In this paper, we propose ${\\\\rm EFO}_k$-CQA, a comprehensive framework for data generation, model training, and method evaluation that covers the combinatorial space of Existential First-order Queries with multiple variables (${\\\\rm EFO}_k$). The combinatorial query space in our framework significantly extends those defined by set operations in the existing literature. Additionally, we construct a dataset with 741 query types for empirical evaluation, and our benchmark results provide new insights into how query hardness affects the results. Furthermore, we demonstrate that the existing dataset construction process is systematically biased that hinders the appropriate development of query-answering methods, highlighting the importance of our work. Our code and data are provided in~\\\\url{https://anonymous.4open.science/r/EFOK-CQA/README.md}.",Datasets & Benchmarks,NeurIPS,2023,Reject,Hang Yin;Zihao Wang;Fei Weizhi;Yangqiu Song,True,https://openreview.net/pdf?id=9FLkxTGY3B
XfQbPqRPXi,Towards Better Evaluation of GNN Expressiveness with BREC Dataset,"Research on the theoretical expressiveness of Graph Neural Networks~(GNNs) has developed rapidly, and many methods have been proposed to enhance the expressiveness. However, most methods do not have a uniform expressiveness measure except for a few that strictly follow the $k$-dimensional Weisfeiler-Lehman ($k$-WL) test hierarchy. Their theoretical analyses are often limited to distinguishing certain families of non-isomorphic graphs, leading to difficulties in quantitatively comparing their expressiveness. In contrast to theoretical analysis, another way to measure expressiveness is by evaluating model performance on certain datasets containing 1-WL-indistinguishable graphs. Previous datasets specifically designed for this purpose, however, face problems with difficulty (any model surpassing 1-WL has nearly 100\\\\% accuracy), granularity (models tend to be either 100\\\\% correct or near random guess), and scale (only a few essentially different graphs in each dataset). To address these limitations, we propose a new expressiveness dataset, **BREC**, which includes 400 pairs of non-isomorphic graphs carefully selected from four primary categories (Basic, Regular, Extension, and CFI). These graphs have higher difficulty (up to 4-WL-indistinguishable), finer granularity (able to compare models between 1-WL and 3-WL), and a larger scale (400 pairs). Further, we synthetically test 23 models with higher-than-1-WL expressiveness on our BREC dataset. Our experiment gives the first thorough comparison of the expressiveness of those state-of-the-art beyond-1-WL GNN models. We expect this dataset to serve as a benchmark for testing the expressiveness of future GNNs. Our dataset and evaluation code are released at: https://github.com/GraphPKU/BREC.",Datasets & Benchmarks,NeurIPS,2023,Reject,Yanbo Wang;Muhan Zhang,True,https://openreview.net/pdf?id=XfQbPqRPXi
oIUXpBnyjv,LightZero: A Unified Benchmark for Monte Carlo Tree Search in General Sequential Decision Scenarios,"Building agents based on tree-search planning capabilities with learned models has achieved remarkable success in classic decision-making problems, such as Go and Atari.
However, it has been deemed challenging or even infeasible to extend Monte Carlo Tree Search (MCTS) based algorithms to diverse real-world applications, especially when these environments involve complex action spaces and significant simulation costs, or inherent stochasticity.
In this work, we introduce LightZero, the first unified benchmark for deploying MCTS/MuZero in general sequential decision scenarios. 
Specificially, we summarize the most critical challenges in designing a general MCTS-style decision-making solver, then decompose the tightly-coupled algorithm and system design of tree-search RL methods into distinct sub-modules.
By incorporating more appropriate exploration and optimization strategies, we can significantly enhance these sub-modules and construct powerful LightZero agents to tackle tasks across a wide range of domains, such as board games, Atari, MuJoCo, MiniGrid and GoBigger.
Detailed benchmark results reveal the significant potential of such methods in building scalable and efficient decision intelligence.
The code is available as part of OpenDILab at https://github.com/opendilab/LightZero.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Yazhe Niu;Yuan Pu;Zhenjie Yang;Xueyan Li;Tong Zhou;Jiyuan Ren;Shuai Hu;Hongsheng Li;Yu Liu,True,https://openreview.net/pdf?id=oIUXpBnyjv
GSuP99u2kR,LLaVA-Med: Training a Large Language-and-Vision Assistant for Biomedicine in One Day,"Conversational generative AI has demonstrated remarkable promise for empowering biomedical practitioners, but current investigations focus on unimodal text. Multimodal conversational AI has seen rapid progress by leveraging billions of image-text pairs from the public web, but such general-domain vision-language models still lack sophistication in understanding and conversing about biomedical images. In this paper, we propose a cost-efficient approach for training a vision-language conversational assistant that can answer open-ended research questions of biomedical images. The key idea is to leverage a large-scale, broad-coverage biomedical figure-caption dataset extracted from PubMed Central, use GPT-4 to self-instruct open-ended instruction-following data from the captions, and then fine-tune a large general-domain vision-language model using a novel curriculum learning method. Specifically, the model first learns to align biomedical vocabulary using the figure-caption pairs as is, then learns to master open-ended conversational semantics using GPT-4 generated instruction-following data, broadly mimicking how a layperson gradually acquires biomedical knowledge. This enables us to train a Large Language and Vision Assistant for BioMedicine (LLaVA-Med) in less than 15 hours (with eight A100s). LLaVA-Med exhibits excellent multimodal conversational capability and can follow open-ended instruction to assist with inquiries about a biomedical image. On three standard biomedical visual question answering datasets, LLaVA-Med outperforms previous supervised state-of-the-art on certain metrics. To facilitate biomedical multimodal research, we will release our instruction-following data and the LLaVA-Med model.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Chunyuan Li;Cliff Wong;Sheng Zhang;Naoto Usuyama;Haotian Liu;Jianwei Yang;Tristan Naumann;Hoifung Poon;Jianfeng Gao,True,https://openreview.net/pdf?id=GSuP99u2kR
0Wmglu8zak,BubbleML: A Multiphase Multiphysics Dataset and Benchmarks for Machine Learning,"In the field of phase change phenomena, the lack of accessible and diverse datasets suitable for machine learning (ML) training poses a significant challenge. Existing experimental datasets are often restricted, with limited availability and sparse ground truth, impeding our understanding of this complex multiphysics phenomena. To bridge this gap, we present the BubbleML dataset which leverages physics-driven simulations to provide accurate ground truth information for various boiling scenarios, encompassing nucleate pool boiling, flow boiling, and sub-cooled boiling. This extensive dataset covers a wide range of parameters, including varying gravity conditions, flow rates, sub-cooling levels, and wall superheat, comprising 79 simulations.  BubbleML is validated against experimental observations and trends, establishing it as an invaluable resource for ML research. Furthermore, we showcase its potential to facilitate the exploration of diverse downstream tasks by introducing two benchmarks: (a) optical flow analysis to capture bubble dynamics, and (b) neural PDE solvers for learning temperature and flow dynamics. The BubbleML dataset and its benchmarks aim to catalyze progress in ML-driven research on multiphysics phase change phenomena, providing robust baselines for the development and comparison of state-of-the-art techniques and models.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Sheikh Md Shakeel Hassan;Arthur Feeney;Akash Dhruv;Jihoon Kim;Youngjoon Suh;Jaiyoung Ryu;Yoonjin Won;Aparna Chandramowlishwaran,True,https://openreview.net/pdf?id=0Wmglu8zak
GF5l0F19Bt,An NLP Benchmark Dataset for Assessing Corporate Climate Policy Engagement,"As societal awareness of climate change grows, corporate climate policy engagements are attracting attention.
We propose a dataset to estimate corporate climate policy engagement from various PDF-formatted documents.
Our dataset comes from LobbyMap (a platform operated by global think tank InfluenceMap) that provides engagement categories and stances on the documents.
To convert the LobbyMap data into the structured dataset, we developed a pipeline using text extraction and OCR.
Our contributions are: (i) Building an NLP dataset including 10K documents on corporate climate policy engagement. (ii) Analyzing the properties and challenges of the dataset. (iii) Providing experiments for the dataset using pre-trained language models.
The results show that while Longformer outperforms baselines and other pre-trained models, there is still room for significant improvement.
We hope our work begins to bridge research on NLP and climate change.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Gaku Morio;Christopher D Manning,True,https://openreview.net/pdf?id=GF5l0F19Bt
q9hc7R8N7P,Exploring Why Object Recognition Performance Degrades Across Income Levels and Geographies with Factor Annotations,"Despite impressive advances in object-recognition, deep learning systems’ performance degrades significantly across geographies and lower income levels---raising pressing concerns of inequity. Addressing such performance gaps remains a challenge, as little is understood about why performance degrades across incomes or geographies.
We take a step in this direction by annotating images from Dollar Street, a popular benchmark of geographically and economically diverse images, labeling each image with factors such as color, shape, and background. These annotations unlock a new granular view into how objects differ across incomes/regions. We then use these object differences to pinpoint model vulnerabilities across incomes and regions.
We study a range of modern vision models, finding that performance disparities are most associated with differences in _texture, occlusion_, and images with _darker lighting_.
We illustrate how insights from our factor labels can surface mitigations to improve models' performance disparities.
As an example, we show that mitigating a model's vulnerability to texture 
can improve performance on the lower income level.
**We release all the factor annotations along with an interactive dashboard
to facilitate research into more equitable vision systems.**",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Laura Gustafson;Megan Richards;Melissa Hall;Caner Hazirbas;Diane Bouchacourt;Mark Ibrahim,True,https://openreview.net/pdf?id=q9hc7R8N7P
W5If9P1xqO,ClimSim: A large multi-scale dataset for hybrid physics-ML climate emulation,"Modern climate projections lack adequate spatial and temporal resolution due to computational constraints. A consequence is inaccurate and imprecise predictions of critical processes such as storms. Hybrid methods that combine physics with machine learning (ML) have introduced a new generation of higher fidelity climate simulators that can sidestep Moore's Law by outsourcing compute-hungry, short, high-resolution simulations to ML emulators. However, this hybrid ML-physics simulation approach requires domain-specific treatment and has been inaccessible to ML experts because of lack of training data and relevant, easy-to-use workflows. We present ClimSim, the largest-ever dataset designed for hybrid ML-physics research. It comprises multi-scale climate simulations, developed by a consortium of climate scientists and ML researchers. It consists of 5.7 billion pairs of multivariate input and output vectors that isolate the influence of locally-nested, high-resolution, high-fidelity physics on a host climate simulator's macro-scale physical state.

The dataset is global in coverage, spans multiple years at high sampling frequency, and is designed such that resulting emulators are compatible with downstream coupling into operational climate simulators. We implement a range of deterministic and stochastic regression baselines to highlight the ML challenges and their scoring. The data (https://huggingface.co/datasets/LEAP/ClimSim_high-res) and code (https://leap-stc.github.io/ClimSim) are released openly to support the development of hybrid ML-physics and high-fidelity climate simulations for the benefit of science and society.",Datasets & Benchmarks,NeurIPS,2023,Oral,Sungduk Yu;Walter Hannah;Liran Peng;Jerry Lin;Mohamed Aziz Bhouri;Ritwik Gupta;Björn Lütjens;Justus Christopher Will;Gunnar Behrens;Julius Busecke;Nora Loose;Charles I Stern;Tom Beucler;Bryce Harrop;Benjamin R Hillman;Andrea Jenney;Savannah Ferretti;Nana Liu;Anima Anandkumar;Noah D Brenowitz;Veronika Eyring;Nicholas Geneva;Pierre Gentine;Stephan Mandt;Jaideep Pathak;Akshay Subramaniam;Carl Vondrick;Rose Yu;Laure Zanna;Tian Zheng;Ryan Abernathey;Fiaz Ahmed;David C Bader;Pierre Baldi;Elizabeth Barnes;Christopher Bretherton;Peter Caldwell;Wayne Chuang;Yilun Han;YU HUANG;Fernando Iglesias-Suarez;Sanket Jantre;Karthik Kashinath;Marat Khairoutdinov;Thorsten Kurth;Nicholas Lutsko;Po-Lun Ma;Griffin Mooers;J. David Neelin;David Randall;Sara Shamekh;Mark A Taylor;Nathan Urban;Janni Yuval;Guang Zhang;Michael Pritchard,True,https://openreview.net/pdf?id=W5If9P1xqO
3BxYAaovKr,Ego4D Goal-Step: Toward Hierarchical Understanding of Procedural Activities,"Human activities are goal-oriented and hierarchical, comprising primary goals at the top level, sequences of steps and substeps in the middle, and atomic actions at the lowest level. Recognizing human activities thus requires relating atomic actions and steps to their functional objectives (what the actions contribute to) and modeling their sequential and hierarchical dependencies towards achieving the goals. Current activity recognition research has primarily focused on only the lowest levels of this hierarchy, i.e., atomic or low-level actions, often in trimmed videos with annotations spanning only a few seconds. In this work, we introduce Ego4D Goal-Step, a new set of annotations on the recently released Ego4D with a novel hierarchical taxonomy of goal-oriented activity labels. It provides dense annotations for 48K procedural step segments (430 hours) and high-level goal annotations for 2,807 hours of Ego4D videos. Compared to existing procedural video datasets, it is substantially larger in size, contains hierarchical action labels (goals - steps - substeps), and provides goal-oriented auxiliary information including natural language summary description, step completion status, and step-to-goal relevance information. We take a data-driven approach to build our taxonomy, resulting in dense step annotations that do not suffer from poor label-data alignment issues resulting from a taxonomy defined a priori. Through comprehensive evaluations and analyses, we demonstrate how Ego4D Goal-Step supports exploring various questions in procedural activity understanding, including goal inference, step prediction, hierarchical relation learning, and long-term temporal modeling.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Yale Song;Gene Byrne;Tushar Nagarajan;Huiyu Wang;Miguel Martin;Lorenzo Torresani,True,https://openreview.net/pdf?id=3BxYAaovKr
dI4wzAE6uV,Can LLM Already Serve as A Database Interface? A BIg Bench for Large-Scale Database Grounded Text-to-SQLs,"Text-to-SQL parsing, which aims at converting natural language instructions into executable SQLs, has gained increasing attention in recent years.  In particular, GPT-4 and Claude-2 have shown impressive results in this task. However, most of the prevalent benchmarks, i.e., Spider, and WikiSQL, focus on database schema with few rows of database contents leaving the gap between academic study and real-world applications. To mitigate this gap, we present BIRD, a BIg benchmark for laRge-scale Database grounded in text-to-SQL tasks, containing 12,751 pairs of text-to-SQL data and 95 databases with a total size of 33.4 GB, spanning 37 professional domains. Our emphasis on database values highlights the new challenges of dirty database contents, external knowledge between NL questions and database contents,  and SQL efficiency, particularly in the context of massive databases. To solve these problems, text-to-SQL models must feature database value comprehension in addition to semantic parsing. The experimental results demonstrate the significance of database values in generating accurate text-to-SQLs for big databases. Furthermore, even the most popular and effective text-to-SQL models, i.e. GPT-4, only achieve 54.89% in execution accuracy, which is still far from the human result of 92.96%, proving that challenges still stand. We also provide an efficiency analysis to offer insights into generating text-to-efficient-SQLs that are beneficial to industries. 
We believe that BIRD will contribute to advancing real-world applications of text-to-SQL research.
The leaderboard and source code are available: https://bird-bench.github.io/.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Jinyang Li;Binyuan Hui;GE QU;Jiaxi Yang;Binhua Li;Bowen Li;Bailin Wang;Bowen Qin;Ruiying Geng;Nan Huo;Xuanhe Zhou;Chenhao Ma;Guoliang Li;Kevin Chang;Fei Huang;Reynold Cheng;Yongbin Li,True,https://openreview.net/pdf?id=dI4wzAE6uV
9gLnjw8DfA,Digital Typhoon: Long-term Satellite Image Dataset for the Spatio-Temporal Modeling of Tropical Cyclones,"This paper presents the official release of the Digital Typhoon dataset, the longest typhoon satellite image dataset for 40+ years aimed at benchmarking machine learning models for long-term spatio-temporal data. To build the dataset, we developed a workflow to create an infrared typhoon-centered image for cropping using Lambert azimuthal equal-area projection referring to the best track data. We also address data quality issues such as inter-satellite calibration to create a homogeneous dataset. To take advantage of the dataset, we organized machine learning tasks by the types and targets of inference, with other tasks for meteorological analysis, societal impact, and climate change. The benchmarking results on the analysis, forecasting, and reanalysis for the intensity suggest that the dataset is challenging for recent deep learning models, due to many choices that affect the performance of various models. This dataset reduces the barrier for machine learning researchers to meet large-scale real-world events called tropical cyclones and develop machine learning models that may contribute to advancing scientific knowledge on tropical cyclones as well as solving societal and sustainability issues such as disaster reduction and climate change. The dataset is publicly available at http://agora.ex.nii.ac.jp/digital-typhoon/dataset/ and https://github.com/kitamoto-lab/digital-typhoon/.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Asanobu Kitamoto;Jared Hwang;Bastien Vuillod;Lucas Gautier;Yingtao Tian;Tarin Clanuwat,True,https://openreview.net/pdf?id=9gLnjw8DfA
5FnttJZQFn,The Waymo Open Sim Agents Challenge,"Simulation with realistic, interactive agents represents a key task for autonomous vehicle software development. In this work, we introduce the Waymo Open Sim Agents Challenge (WOSAC). WOSAC is the first public challenge to tackle this task and propose corresponding metrics. The goal of the challenge is to stimulate the design of realistic simulators that can be used to evaluate and train a behavior model for autonomous driving. We outline our evaluation methodology, present results for a number of different baseline simulation agent methods, and analyze several submissions to the 2023 competition which ran from March 16, 2023 to May 23, 2023. The WOSAC evaluation server remains open for submissions and we discuss open problems for the task.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Nico Montali;John Lambert;Paul Mougin;Alex Kuefler;Nicholas Rhinehart;Michelle Li;Cole Gulino;Tristan Emrich;Zoey Zeyu Yang;Shimon Whiteson;Brandyn White;Dragomir Anguelov,False,https://openreview.net/pdf?id=5FnttJZQFn
QXTjde8evS,DiffInfinite: Large Mask-Image Synthesis via Parallel Random Patch Diffusion in Histopathology,"We present DiffInfinite, a hierarchical diffusion model that generates arbitrarily large histological images while preserving long-range correlation structural information. Our approach first generates synthetic segmentation masks, subsequently used as conditions for the high-fidelity generative diffusion process. The proposed sampling method can be scaled up to any desired image size while only requiring small patches for fast training. Moreover, it can be parallelized more efficiently than previous large-content generation methods while avoiding tiling artifacts. The training leverages classifier-free guidance to augment a small, sparsely annotated dataset with unlabelled data. Our method alleviates unique challenges in histopathological imaging practice: large-scale information, costly manual annotation, and protective data handling. The biological plausibility of DiffInfinite data is evaluated in a survey by ten experienced pathologists as well as a downstream classification and segmentation task. Samples from the model score strongly on anti-copying metrics which is relevant for the protection of patient data.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Marco Aversa;Gabriel Nobis;Miriam Hägele;Kai Standvoss;Mihaela Chirica;Roderick Murray-Smith;Ahmed Alaa;Lukas Ruff;Daniela Ivanova;Wojciech Samek;Frederick Klauschen;Bruno Sanguinetti;Luis Oala,False,https://openreview.net/pdf?id=QXTjde8evS
tk27oD2cBw,"The Harvard USPTO Patent Dataset: A Large-Scale, Well-Structured, and Multi-Purpose Corpus of Patent Applications","Innovation is a major driver of economic and social development, and information about many kinds of innovation is embedded in semi-structured data from patents and patent applications. Though the impact and novelty of innovations expressed in patent data are difficult to measure through traditional means, machine learning offers a promising set of techniques for evaluating novelty, summarizing contributions, and embedding semantics. In this paper, we introduce the Harvard USPTO Patent Dataset (HUPD), a large-scale, well-structured, and multi-purpose corpus of English-language patent applications filed to the United States Patent and Trademark Office (USPTO) between 2004 and 2018. With more than 4.5 million patent documents, HUPD is two to three times larger than comparable corpora. Unlike other NLP patent datasets, HUPD contains the inventor-submitted versions of patent applications, not the final versions of granted patents, allowing us to study patentability at the time of filing using NLP methods for the first time. It is also novel in its inclusion of rich structured data alongside the text of patent filings: By providing each application’s metadata along with all of its text fields, HUPD enables researchers to perform new sets of NLP tasks that leverage variation in structured covariates. As a case study on the types of research HUPD makes possible, we introduce a new task to the NLP community -- patent acceptance prediction. We additionally show the structured metadata provided in HUPD allows us to conduct explicit studies of concept shifts for this task. We find that performance on patent acceptance prediction decays when models trained in one context are evaluated on different innovation categories and over time. Finally, we demonstrate how HUPD can be used for three additional tasks: Multi-class classification of patent subject areas, language modeling, and abstractive summarization. Put together, our publicly-available dataset aims to advance research extending language and classification models to diverse and dynamic real-world data distributions.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Mirac Suzgun;Luke Melas-Kyriazi;Suproteem K Sarkar;Scott Kominers;Stuart Shieber,True,https://openreview.net/pdf?id=tk27oD2cBw
6iRH9SITva,Into the Single Cell Multiverse: an End-to-End Dataset for Procedural Knowledge Extraction in Biomedical Texts,"Many of the most commonly explored natural language processing (NLP) information extraction tasks can be thought of as evaluations of declarative knowledge, or fact-based information extraction. Procedural knowledge extraction, i.e., breaking down a described process into a series of steps, has received much less attention, perhaps in part due to the lack of structured datasets that capture the knowledge extraction process from end-to-end. To address this unmet need, we present FlaMBé (Flow annotations for Multiverse Biological entities), a collection of expert-curated datasets across a series of complementary tasks that capture procedural knowledge in biomedical texts. This dataset is inspired by the observation that one ubiquitous source of procedural knowledge that is described as unstructured text is within academic papers describing their methodology. The workflows annotated in FlaMBé are from texts in the burgeoning field of single cell research, a research area that has become notorious for the number of software tools and complexity of workflows used. Additionally, FlaMBé provides, to our knowledge, the largest manually curated named entity recognition (NER) and disambiguation (NED) datasets for tissue/cell type, a fundamental biological entity that is critical for knowledge extraction in the biomedical research domain. Beyond providing a valuable dataset to enable further development of NLP models for procedural knowledge extraction, automating the process of workflow mining also has important implications for advancing reproducibility in biomedical research.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Ruth Dannenfelser;Jeffrey Zhong;Ran Zhang;Vicky Yao,True,https://openreview.net/pdf?id=6iRH9SITva
8bqjirgxQM,Understanding Social Reasoning in Language Models with Language Models,"As Large Language Models (LLMs) become increasingly integrated into our everyday lives, understanding their ability to comprehend human mental states becomes critical for ensuring effective interactions. However, despite the recent attempts to assess the Theory-of-Mind (ToM) reasoning capabilities of LLMs, the degree to which these models can align with human ToM remains a nuanced topic of exploration. This is primarily due to two distinct challenges: (1) the presence of inconsistent results from previous evaluations, and (2) concerns surrounding the validity of existing evaluation methodologies. To address these challenges, we present a novel framework for procedurally generating evaluations with LLMs by populating causal templates. Using our framework, we create a new social reasoning benchmark (BigToM) for LLMs which consists of 25 controls and 5,000 model-written evaluations. We find that human participants rate the quality of our benchmark higher than previous crowd-sourced evaluations and comparable to expert-written evaluations. Using BigToM, we evaluate the social reasoning capabilities of a variety of LLMs and compare model performances with human performance. Our results suggest that GPT4 has ToM capabilities that mirror human inference patterns, though less reliable, while other LLMs struggle.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Kanishk Gandhi;Jan-Philipp Fränken;Tobias Gerstenberg;Noah Goodman,True,https://openreview.net/pdf?id=8bqjirgxQM
qVXYU3F017,Stable Bias: Evaluating Societal Representations in Diffusion Models,"As machine learning-enabled Text-to-Image (TTI) systems are becoming increasingly prevalent and seeing growing adoption as commercial services, characterizing the social biases they exhibit is a necessary first step to lowering their risk of discriminatory outcomes. This evaluation, however, is made more difficult by the synthetic nature of these systems’ outputs: common definitions of diversity are grounded in social categories of people living in the world, whereas the artificial depictions of fictive humans created by these systems have no inherent gender or ethnicity. To address this need, we propose a new method for exploring the social biases in TTI systems. Our approach relies on characterizing the variation in generated images triggered by enumerating gender and ethnicity markers in the prompts, and comparing it to the variation engendered by spanning different professions. This allows us to (1) identify specific bias trends, (2) provide targeted scores to directly compare models in terms of diversity and representation, and (3) jointly model interdependent social variables to support a multidimensional analysis. We leverage this method to analyze images generated by 3 popular TTI systems (Dall·E 2 , Stable Diffusion v 1.4 and 2) and find that while all of their outputs show correlations with US labor demographics, they also consistently under-represent marginalized identities to different extents. We also release the datasets and low-code interactive bias exploration platforms developed for
this work, as well as the necessary tools to similarly evaluate additional TTI systems.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,,True,https://openreview.net/pdf?id=qVXYU3F017
pX5xlL1T4C,Renku: a platform for sustainable data science,"Data and code working together is fundamental to machine learning (ML), but the context around datasets and interactions between datasets and code are in general captured only rudimentarily. Context such as how the dataset was prepared and created, what source data were used, what code was used in processing, how the dataset evolved, and where it has been used and reused can provide much insight, but this information is often poorly documented. That is unfortunate since it makes datasets into black-boxes with potentially hidden characteristics that have downstream consequences. We argue that making dataset preparation more accessible and dataset usage easier to record and document would have significant benefits for the ML community: it would allow for greater diversity in datasets by inviting modification to published sources, simplify use of alternative datasets and, in doing so, make results more transparent and robust, while allowing for all contributions to be adequately credited. We present a platform, Renku, designed to support and encourage such sustainable development and use of data, datasets, and code, and we demonstrate its benefits through a few illustrative projects which span the spectrum from dataset creation to dataset consumption and showcasing.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,,False,https://openreview.net/pdf?id=pX5xlL1T4C
w4zZNC4ZaV,How Far Can Camels Go? Exploring the State of Instruction Tuning on Open Resources,"In this work we explore recent advances in instruction-tuning language models on a range of open instruction-following datasets. Despite recent claims that open models can be on par with state-of-the-art proprietary models, these claims are often accompanied by limited evaluation, making it difficult to compare models across the board and determine the utility of various resources. We provide a large set of instruction-tuned models from 6.7B to 65B parameters in size, trained on 12 instruction datasets ranging from manually curated (e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and systematically evaluate them on their factual knowledge, reasoning, multilinguality, coding, safety, and open-ended instruction following abilities through a collection of automatic, model-based, and human-based metrics. We further introduce Tülu, our best performing instruction-tuned model suite finetuned on a combination of high-quality open resources.

Our experiments show that different instruction-tuning datasets can uncover or enhance specific skills, while no single dataset (or combination) provides the best performance across all evaluations. Interestingly, we find that model and human preference-based evaluations fail to reflect differences in model capabilities exposed by benchmark-based evaluations, suggesting the need for the type of systemic evaluation performed in this work. Our evaluations show that the best model in any given evaluation reaches on average 87% of ChatGPT performance, and 73% of GPT-4 performance, suggesting that further investment in building better base models and instruction-tuning data is required to close the gap. We release our instruction-tuned models, including a fully finetuned 65B Tülu, along with our code, data, and evaluation framework to facilitate future research.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Yizhong Wang;Hamish Ivison;Pradeep Dasigi;Jack Hessel;Tushar Khot;Khyathi Chandu;David Wadden;Kelsey MacMillan;Noah A. Smith;Iz Beltagy;Hannaneh Hajishirzi,False,https://openreview.net/pdf?id=w4zZNC4ZaV
xKYtTmtyI2,Validated Image Caption Rating Dataset,"We present a new high-quality validated image caption rating (VICR) dataset. How well a caption fits an image can be difficult to assess due to the subjective nature of caption quality. How do we evaluate whether a caption is good? We generated a new dataset to help answer this question by using our new image caption rating system, which consists of a novel robust rating scale and gamified approach to gathering human ratings. We show that our approach is consistent and teachable. 113 participants were involved in generating the dataset, which is composed of 68,217 ratings among 15,646 image-caption pairs. Our new dataset has greater inter-rater agreement than the state of the art, and custom machine learning rating predictors that were trained on our dataset outperform previous metrics. We improve over Flickr8k-Expert in Kendall's $W$ by 12\\\\% and in Fleiss' $\\\\kappa$ by 19\\\\%, and thus provide a new benchmark dataset for image caption rating.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Lothar Narins;Andrew T Scott;Aakash Gautam;Anagha Kulkarni;Mar Castanon;Benjamin Kao;Shasta Ihorn;Yue-Ting Siu;James M Mason;Alexander Mario Blum;Ilmi Yoon,True,https://openreview.net/pdf?id=xKYtTmtyI2
EBYZSRRzSE,Video Timeline Modeling For News Story Understanding,"In this paper, we present a novel problem, namely video timeline modeling. Our objective is to create a video-associated timeline from a set of videos related to a specific topic, thereby facilitating the content and structure understanding of the story being told. This problem has significant potential in various real-world applications, for instance, news story summarization. To bootstrap research in this area, we curate a realistic benchmark dataset, YouTube-News-Timeline, consisting of over $12$k timelines and $300$k YouTube news videos. Additionally, we propose a set of quantitative metrics to comprehensively evaluate and compare methodologies. With such a testbed, we further develop and benchmark several deep learning approaches to tackling this problem. We anticipate that this exploratory work will pave the way for further research in video timeline modeling. The assets are available via https://github.com/google-research/google-research/tree/master/video_timeline_modeling.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Meng Liu;Mingda Zhang;Jialu Liu;Hanjun Dai;Ming-Hsuan Yang;Shuiwang Ji;Zheyun Feng;Boqing Gong,True,https://openreview.net/pdf?id=EBYZSRRzSE
JVlWseddak,EgoSchema: A Diagnostic Benchmark for Very Long-form Video Language Understanding,"We introduce EgoSchema, a very long-form video question-answering dataset, and benchmark to evaluate long video understanding capabilities of modern vision and language systems. Derived from Ego4D, EgoSchema consists of over 5000 human curated multiple choice question answer pairs, spanning over 250 hours of real video data, covering a very broad range of natural human activity and behavior. For each question, EgoSchema requires the correct answer to be selected between five given options based on a three-minute-long video clip. While some prior works have proposed video datasets with long clip lengths, we posit that merely the length of the video clip does not truly capture the temporal difficulty of the video task that is being considered. To remedy this, we introduce temporal certificate sets, a general notion for capturing the intrinsic temporal understanding length associated with a broad range of video understanding tasks & datasets. Based on this metric, we find EgoSchema to have intrinsic temporal lengths over 5.7x longer than the second closest dataset and 10x to 100x longer than any other video understanding dataset. Further, our evaluation of several current state-of-the-art video and language models shows them to be severely lacking in long-term video understanding capabilities. Even models with several billions of parameters achieve QA accuracy less than 33% (random is 20%) on the EgoSchema multi-choice question answering task, while humans achieve about 76% accuracy. We posit that EgoSchema, with its long intrinsic temporal structures and diverse complexity, would serve as a valuable evaluation probe for developing effective long-term video understanding systems in the future. Data and Zero-shot model evaluation code will all be open-sourced under the Ego4D license at http://egoschema.github.io.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,,True,https://openreview.net/pdf?id=JVlWseddak
kiYqbO3wqw,Mind2Web: Towards a Generalist Agent for the Web,"We introduce Mind2Web, the first dataset for developing and evaluating generalist agents for the web that can follow language instructions to complete complex tasks on any website. Existing datasets for web agents either use simulated websites or only cover a limited set of websites and tasks, thus not suitable for generalist web agents. With over 2,000 open-ended tasks collected from 137 websites spanning 31 domains and crowdsourced action sequences for the tasks, Mind2Web provides three necessary ingredients for building generalist web agents: 1) diverse domains, websites, and tasks, 2) use of real-world websites instead of simulated and simplified ones, and 3) a broad spectrum of user interaction patterns. Based on Mind2Web, we conduct an initial exploration of using large language models (LLMs) for building generalist web agents. While the raw HTML of real-world websites are often too large to be fed to LLMs, we show that first filtering it with a small LM significantly improves the effectiveness and efficiency of LLMs. Our solution demonstrates a decent level of performance, even on websites or entire domains the model has never seen before, but there is still a substantial room to improve towards truly generalizable agents. We open-source our dataset, model implementation, and trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further research on building a generalist agent for the web.",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Xiang Deng;Yu Gu;Boyuan Zheng;Shijie Chen;Samuel Stevens;Boshi Wang;Huan Sun;Yu Su,True,https://openreview.net/pdf?id=kiYqbO3wqw
qY9LR74O3Z,Holistic Evaluation of Text-to-Image Models,"The stunning qualitative improvement of text-to-image models has led to their widespread attention and adoption. However, we lack a comprehensive quantitative understanding of their capabilities and risks. To fill this gap, we introduce a new benchmark, Holistic Evaluation of Text-to-Image Models (HEIM). Whereas previous evaluations focus mostly on image-text alignment and image quality, we identify 12 aspects, including text-image alignment, image quality, aesthetics, originality, reasoning, knowledge, bias, toxicity, fairness, robustness, multilinguality, and efficiency. We curate 62 scenarios encompassing these aspects and evaluate 26 state-of-the-art text-to-image models on this benchmark. Our results reveal that no single model excels in all aspects, with different models demonstrating different strengths. We release the generated images and human evaluation results for full transparency at https://crfm.stanford.edu/heim/latest and the code at https://github.com/stanford-crfm/helm, which is integrated with the HELM codebase",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Tony Lee;Michihiro Yasunaga;Chenlin Meng;Yifan Mai;Joon Sung Park;Agrim Gupta;Yunzhi Zhang;Deepak Narayanan;Hannah Benita Teufel;Marco Bellagente;Minguk Kang;Taesung Park;Jure Leskovec;Jun-Yan Zhu;Li Fei-Fei;Jiajun Wu;Stefano Ermon;Percy Liang,True,https://openreview.net/pdf?id=qY9LR74O3Z
CsXC6IcdwI,EHRSHOT: An EHR Benchmark for Few-Shot Evaluation of Foundation Models,"While the general machine learning (ML) community has benefited from public datasets, tasks, and models, the progress of ML in healthcare has been hampered by a lack of such shared assets. The success of foundation models creates new challenges for healthcare ML by requiring access to shared pretrained models to validate performance benefits. We help address these challenges through three contributions. First, we publish a new dataset, EHRSHOT, which contains de-identified structured data from the electronic health records (EHRs) of 6,739 patients from Stanford Medicine. Unlike MIMIC-III/IV and other popular EHR datasets, EHRSHOT is longitudinal and not restricted to ICU/ED patients. Second, we publish the weights of CLMBR-T-base, a 141M parameter clinical foundation model pretrained on the structured EHR data of 2.57M patients. We are one of the first to fully release such a model for coded EHR data; in contrast, most prior models released for clinical data  (e.g. GatorTron, ClinicalBERT) only work with unstructured text and cannot process the rich, structured data within an EHR. We provide an end-to-end pipeline for the community to validate and build upon its performance. Third, we define 15 few-shot clinical prediction tasks, enabling evaluation of foundation models on benefits such as sample efficiency and task adaptation. Our model, dataset, and code are available here: https://ehrshot.stanford.edu/",Datasets & Benchmarks,NeurIPS,2023,Spotlight,Michael Wornow;Rahul Thapa;Ethan Steinberg;Jason Alan Fries;Nigam Shah,True,https://openreview.net/pdf?id=CsXC6IcdwI
JqWtIIaS8n,LithoBench: Benchmarking AI Computational Lithography for Semiconductor Manufacturing,"Computational lithography provides algorithmic and mathematical support for resolution enhancement in optical lithography, which is the critical step in semiconductor manufacturing. 
The time-consuming lithography simulation and mask optimization processes limit the practical application of inverse lithography technology (ILT), a promising solution to the challenges of advanced-node lithography. 
Although various machine learning methods for ILT have shown promise for reducing the computational burden, this field is in lack of a dataset that can train the models thoroughly and evaluate the performance comprehensively. 
To boost the development of AI-driven computational lithography, we present the LithoBench dataset, a collection of circuit layout tiles for deep-learning-based lithography simulation and mask optimization. 
LithoBench consists of more than 120k tiles that are cropped from real circuit designs or synthesized according to the layout topologies of famous ILT testcases. 
The ground truths are generated by a famous lithography model in academia and an advanced ILT method. 
Based on the data, we provide a framework to design and evaluate deep neural networks (DNNs) with the data. 
The framework is used to benchmark state-of-the-art models on lithography simulation and mask optimization. 
We hope LithoBench can promote the research and development of computational lithography. 
LithoBench is available at https://anonymous.4open.science/r/lithobench-APPL.",Datasets & Benchmarks,NeurIPS,2023,Poster,Su Zheng;Haoyu Yang;Binwu Zhu;Bei Yu;Martin D.F. Wong,True,https://openreview.net/pdf?id=JqWtIIaS8n
uIppiU2JKP,Synthcity: a benchmark framework for diverse use cases of tabular synthetic data,"Accessible high-quality data is the bread and butter of machine learning research, and the demand for data has exploded as larger and more advanced ML models are built across different domains. Yet, real data often contain sensitive information, are subject to various biases, and are costly to acquire, which compromise their quality and accessibility. Synthetic data have thus emerged as a complement to, sometimes even a replacement for, real data for ML training. However, the landscape of synthetic data research has been fragmented due to the diverse range of data modalities, such as tabular, time series, and images, and the wide array of use cases, including privacy preservation, fairness considerations, and data augmentation. This fragmentation poses practical challenges when comparing and selecting synthetic data generators in for different problem settings. To this end, we develop Synthcity, an open-source Python library that allows researchers and practitioners to perform one-click benchmarking of synthetic data generators across data modalities and use cases. Beyond benchmarking, Synthcity serves as a centralized toolkit for accessing cutting-edge data generators. In addition, Synthcity’s flexible plug-in style API makes it easy to incorporate additional data generators into the framework. Using examples of tabular data generation and data augmentation, we illustrate the general applicability of Synthcity, and the insight one can obtain.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zhaozhi Qian;Rob Davis;Mihaela van der Schaar,False,https://openreview.net/pdf?id=uIppiU2JKP
9gkrbrFzZj,OpenSTL: A Comprehensive Benchmark of Spatio-Temporal Predictive Learning,"Spatio-temporal predictive learning is a learning paradigm that enables models to learn spatial and temporal patterns by predicting future frames from given past frames in an unsupervised manner. Despite remarkable progress in recent years, a lack of systematic understanding persists due to the diverse settings, complex implementation, and difficult reproducibility. Without standardization, comparisons can be unfair and insights inconclusive. To address this dilemma, we propose OpenSTL, a comprehensive benchmark for spatio-temporal predictive learning that categorizes prevalent approaches into recurrent-based and recurrent-free models. OpenSTL provides a modular and extensible framework implementing various state-of-the-art methods. We conduct standard evaluations on datasets across various domains, including synthetic moving object trajectory, human motion, driving scenes, traffic flow, and weather forecasting. Based on our observations, we provide a detailed analysis of how model architecture and dataset properties affect spatio-temporal predictive learning performance. Surprisingly, we find that recurrent-free models achieve a good balance between efficiency and performance than recurrent models. Thus, we further extend the common MetaFormers to boost recurrent-free spatial-temporal predictive learning. We open-source the code and models at https://github.com/chengtan9907/OpenSTL.",Datasets & Benchmarks,NeurIPS,2023,Poster,Cheng Tan;Siyuan Li;Zhangyang Gao;Wenfei Guan;Zedong Wang;Zicheng Liu;Lirong Wu;Stan Z. Li,False,https://openreview.net/pdf?id=9gkrbrFzZj
cF6rQz8V3V,Bitstream-Corrupted Video Recovery: A Novel Benchmark Dataset and Method,"The past decade has witnessed great strides in video recovery by specialist technologies, like video inpainting, completion, and error concealment. However, they typically simulate the missing content by manual-designed error masks, thus failing to fill in the realistic video loss in video communication (e.g., telepresence, live streaming, and internet video) and multimedia forensics. To address this, we introduce the bitstream-corrupted video (BSCV) benchmark, the first benchmark dataset with more than 28,000 video clips, which can be used for bitstream-corrupted video recovery in the real world. The BSCV is a collection of 1) a proposed three-parameter corruption model for video bitstream, 2) a large-scale dataset containing rich error patterns, multiple corruption levels, and flexible dataset branches, and 3) a new video recovery framework that serves as a benchmark. We evaluate state-of-the-art video inpainting methods on the BSCV dataset, demonstrating existing approaches' limitations and our framework's advantages in solving the bitstream-corrupted video recovery problem. The benchmark and dataset are released at https://github.com/LIUTIGHE/BSCV-Dataset.",Datasets & Benchmarks,NeurIPS,2023,Poster,Tianyi Liu;Kejun Wu;YI WANG;Wenyang Liu;Kim-Hui Yap;Lap-Pui Chau,True,https://openreview.net/pdf?id=cF6rQz8V3V
WtajAo0JWU,Motion-X: A Large-scale 3D Expressive Whole-body Human Motion Dataset,"In this paper, we present Motion-X, a large-scale 3D expressive whole-body motion dataset. Existing motion datasets predominantly contain body-only poses, lacking facial expressions, hand gestures, and fine-grained pose descriptions. Moreover, they are primarily collected from limited laboratory scenes with textual descriptions manually labeled, which greatly limits their scalability. To overcome these limitations, we develop a whole-body motion and text annotation pipeline, which can automatically annotate motion from either single- or multi-view videos and provide comprehensive semantic labels for each video and fine-grained whole-body pose descriptions for each frame. This pipeline is of high precision, cost-effective, and scalable for further research. Based on it, we construct Motion-X, which comprises 15.6M precise 3D whole-body pose annotations (i.e., SMPL-X) covering 81.1K motion sequences from massive scenes. Besides, Motion-X provides 15.6M frame-level whole-body pose descriptions and 81.1K sequence-level semantic labels. Comprehensive experiments demonstrate the accuracy of the annotation pipeline and the significant benefit of Motion-X in enhancing expressive, diverse, and natural motion generation, as well as 3D whole-body human mesh recovery.",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=WtajAo0JWU
T3FKjN4p8d,Intelligent Knee Sleeves: A Real-time Multimodal Dataset for 3D Lower Body Motion Estimation Using Smart Textile,"The kinematics of human movements and locomotion are closely linked to the activation and contractions of muscles. To investigate this, we present a multimodal dataset with benchmarks collected using a novel pair of Intelligent Knee Sleeves (Texavie MarsWear Knee Sleeves) for human pose estimation. Our system utilizes synchronized datasets that comprise time-series data from the Knee Sleeves and the corresponding ground truth labels from visualized motion capture camera system. We employ these to generate 3D human models solely based on the wearable data of individuals performing different activities. We demonstrate the effectiveness of this camera-free system and machine learning algorithms in the assessment of various movements and exercises, including extension to unseen exercises and individuals. The results show an average error of 7.21 degrees across all eight lower body joints when compared to the ground truth, indicating the effectiveness and reliability of the Knee Sleeve system for the prediction of different lower body joints beyond knees. The results enable human pose estimation in a seamless manner without being limited by visual occlusion or the field of view of cameras. Our results show the potential of multimodal wearable sensing in a variety of applications from home fitness to sports, healthcare, and physical rehabilitation focusing on pose and movement estimation.",Datasets & Benchmarks,NeurIPS,2023,Poster,Wenwen Zhang;Arvin Tashakori;Zenan Jiang;Amir Servati;Harishkumar Narayana;Saeid Soltanian;Rou Yi Yeap;Menghan Ma;Lauren Toy;Peyman Servati,True,https://openreview.net/pdf?id=T3FKjN4p8d
msWIK6SKBK,SUPA: A Lightweight Diagnostic Simulator for Machine Learning in Particle Physics,"Deep learning methods have gained popularity in high energy physics for fast modeling of particle showers in detectors. Detailed simulation frameworks such as the gold standard \\\\textsc{Geant4} are computationally intensive, and current deep generative architectures work on discretized, lower resolution versions of the detailed simulation. The development of models that work at higher spatial resolutions is currently hindered by the complexity of the full simulation data, and by the lack of simpler, more interpretable benchmarks. Our contribution is \\\\textsc{SUPA}, the SUrrogate PArticle propagation simulator, an algorithm and software package for generating data by simulating simplified particle propagation, scattering and shower development in matter. The generation is extremely fast and easy to use compared to \\\\textsc{Geant4}, but still exhibits the key characteristics and challenges of the detailed simulation. The proposed simulator generates thousands of particle showers per second on a desktop machine, a speed up of up to 6 orders of magnitudes over \\\\textsc{Geant4}, and stores detailed geometric information about the shower propagation. \\\\textsc{\\\\textsc{SUPA}} provides much greater flexibility for setting initial conditions and defining multiple benchmarks for the development of models. Moreover, interpreting particle showers as point clouds creates a connection to geometric machine learning and provides challenging and fundamentally new datasets for the field.",Datasets & Benchmarks,NeurIPS,2023,Poster,Atul Kumar Sinha;Daniele Paliotta;Bálint Máté;John Andrew Raine;Tobias Golling;François Fleuret,True,https://openreview.net/pdf?id=msWIK6SKBK
RwNIqaNOgd,RL-ViGen: A Reinforcement Learning Benchmark for Visual Generalization,"Visual Reinforcement Learning (Visual RL), coupled with high-dimensional observations, has consistently confronted the long-standing challenge of out-of-distribution generalization. Despite the focus on algorithms aimed at resolving visual generalization problems, we argue that the devil is in the existing benchmarks as they are restricted to isolated tasks and generalization categories, undermining a comprehensive evaluation of agents' visual generalization capabilities. To bridge this gap, we introduce RL-ViGen: a novel **R**einforcement **L**earning Benchmark for **Vi**sual **Gen**eralization, which contains diverse tasks and a wide spectrum of generalization types, thereby facilitating the derivation of more reliable conclusions. Furthermore, RL-ViGen incorporates the latest generalization visual RL algorithms into a unified framework, under which the experiment results indicate that no single existing algorithm has prevailed universally across tasks. Our aspiration is that Rl-ViGen will serve as a catalyst in this area, and lay a foundation for the future creation of universal visual generalization RL agents suitable for real-world scenarios.  Access to our code and implemented algorithms is provided at https://gemcollector.github.io/RL-ViGen/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zhecheng Yuan;Sizhe Yang;Pu Hua;Can Chang;Kaizhe Hu;Huazhe Xu,True,https://openreview.net/pdf?id=RwNIqaNOgd
1plAfmP5ms,MVDoppler: Unleashing the Power of Multi-View Doppler for MicroMotion-based Gait Classification,"Modern perception systems rely heavily on high-resolution cameras, LiDARs, and advanced deep neural networks, enabling exceptional performance across various applications. However, these optical systems predominantly depend on geometric features and shapes of objects, which can be challenging to capture in long-range perception applications. To overcome this limitation, alternative approaches such as Doppler-based perception using high-resolution radars have been proposed. 
Doppler-based systems are capable of measuring micro-motions of targets remotely and with very high precision. When compared to geometric features, the resolution of micro-motion features exhibits significantly greater resilience to the influence of distance. However, the true potential of Doppler-based perception has yet to be fully realized due to several factors. These include the unintuitive nature of Doppler signals, the limited availability of public Doppler datasets, and the current datasets' inability to capture the specific co-factors that are unique to Doppler-based perception, such as the effect of the radar's observation angle and the target's motion trajectory.
This paper introduces a new large multi-view Doppler dataset together with baseline perception models for micro-motion-based gait analysis and classification. The dataset captures the impact of the subject's walking trajectory and radar's observation angle on the classification performance. Additionally, baseline multi-view data fusion techniques are provided to mitigate these effects. This work demonstrates that sub-second micro-motion snapshots can be sufficient for reliable detection of hand movement patterns and even changes in a pedestrian's walking behavior when distracted by their phone. Overall, this research not only showcases the potential of Doppler-based perception, but also offers valuable solutions to tackle its fundamental challenges.",Datasets & Benchmarks,NeurIPS,2023,Poster,Soheil Hor;Shubo Yang;Jae-Ho Choi;Amin Arbabian,True,https://openreview.net/pdf?id=1plAfmP5ms
LE4AN1FGjJ,Degraded Polygons Raise Fundamental Questions of Neural Network Perception,"It is well-known that modern computer vision systems often exhibit behaviors misaligned with those of humans: from adversarial attacks to image corruptions, deep
learning vision models suffer in a variety of settings that humans capably handle. In
light of these phenomena, here we introduce another, orthogonal perspective studying the human-machine vision gap. We revisit the task of recovering images under
degradation, first introduced over 30 years ago in the Recognition-by-Components
theory of human vision. Specifically, we study the performance and behavior of
neural networks on the seemingly simple task of classifying regular polygons at
varying orders of degradation along their perimeters. To this end, we implement the
Automated Shape Recoverability Test
for rapidly generating large-scale datasets
of perimeter-degraded regular polygons, modernizing the historically manual creation of image recoverability experiments. We then investigate the capacity of
neural networks to recognize and recover such degraded shapes when initialized
with different priors. Ultimately, we find that neural networks’ behavior on this
simple task conflicts with human behavior, raising a fundamental question of the
robustness and learning capabilities of modern computer vision models",Datasets & Benchmarks,NeurIPS,2023,Poster,Leonard Tang;Dan Ley,True,https://openreview.net/pdf?id=LE4AN1FGjJ
XZf2bnMBag,Realistic Synthetic Financial Transactions for Anti-Money Laundering Models,"With the widespread digitization of finance and the increasing popularity of cryptocurrencies, the sophistication of fraud schemes devised by cybercriminals is growing. Money laundering -- the movement of illicit funds to conceal their origins -- can cross bank and national boundaries, producing complex transaction patterns. The UN estimates 2-5\\\\% of global GDP or \\\\$0.8 - \\\\$2.0 trillion dollars are laundered globally each year.  Unfortunately, real data to train machine learning models to detect laundering is generally not available, and previous synthetic data generators have had significant shortcomings. A realistic, standardized, publicly-available benchmark is needed for comparing models and for the advancement of the area.

To this end, this paper contributes a synthetic financial transaction dataset generator and a set of synthetically generated AML (Anti-Money Laundering) datasets.  We have calibrated this agent-based generator to match real transactions as closely as possible and made the datasets public. We describe the generator in detail and demonstrate how the datasets generated can help compare different machine learning models in terms of their AML abilities. In a key way, using synthetic data in these comparisons can be even better than using real data: the ground truth labels are complete, whilst many laundering transactions in real data are never detected.",Datasets & Benchmarks,NeurIPS,2023,Poster,Erik Altman;Jovan Blanuša;Luc Von Niederhäusern;Beni Egressy;Andreea Anghel;Kubilay Atasu,True,https://openreview.net/pdf?id=XZf2bnMBag
xJ7YWXQOrg,Mathematical Capabilities of ChatGPT,"We investigate the mathematical capabilities of two versions of ChatGPT (released 9-January-2023 and 30-January-2023) and of GPT-4 by testing them on publicly available datasets, as well as hand-crafted ones, using a novel evaluation scheme. In contrast to formal mathematics, where large databases of formal proofs are available (e.g., mathlib, the Lean Mathematical Library), current datasets of natural-language mathematics used to benchmark language models either cover only elementary mathematics or are very small. We address this by publicly releasing two new datasets: GHOSTS and miniGHOSTS. These are the first natural-language datasets curated by working researchers in mathematics that (1) aim to cover graduate-level mathematics, (2) provide a holistic overview of the mathematical capabilities of language models, and (3) distinguish multiple dimensions of mathematical reasoning. These datasets test, by using 1636 human expert evaluations, whether ChatGPT and GPT-4 can be helpful assistants to professional mathematicians by emulating use cases that arise in the daily professional activities of mathematicians. We benchmark the models on a range of fine-grained performance metrics. For advanced mathematics, this is the most detailed evaluation effort to date. We find that ChatGPT and GPT-4 can be used most successfully as mathematical assistants for querying facts, acting as mathematical search engines and knowledge base interfaces. GPT-4 can additionally be used for undergraduate-level mathematics but fails on graduate-level difficulty. Contrary to many positive reports in the media about GPT-4 and ChatGPT's exam-solving abilities (a potential case of selection bias), their overall mathematical performance is well below the level of a graduate student. Hence, if you aim to use ChatGPT to pass a graduate-level math exam, you would be better off copying from your average peer!",Datasets & Benchmarks,NeurIPS,2023,Poster,Simon Frieder;Luca Pinchetti;Alexis Chevalier;Ryan-Rhys Griffiths;Tommaso Salvatori;Thomas Lukasiewicz;Philipp Christian Petersen;Julius Berner,True,https://openreview.net/pdf?id=xJ7YWXQOrg
7AjdHnjIHX,COCO-Counterfactuals: Automatically Constructed Counterfactual Examples for Image-Text Pairs,"Counterfactual examples have proven to be valuable in the field of natural language processing (NLP) for both evaluating and improving the robustness of language models to spurious correlations in datasets. Despite their demonstrated utility for NLP, multimodal counterfactual examples have been relatively unexplored due to the difficulty of creating paired image-text data with minimal counterfactual changes. To address this challenge, we introduce a scalable framework for automatic generation of counterfactual examples using text-to-image diffusion models. We use our framework to create COCO-Counterfactuals, a multimodal counterfactual dataset of paired image and text captions based on the MS-COCO dataset. We validate the quality of COCO-Counterfactuals through human evaluations and show that existing multimodal models are challenged by our counterfactual image-text pairs. Additionally, we demonstrate the usefulness of COCO-Counterfactuals for improving out-of-domain generalization of multimodal vision-language models via training data augmentation. We make our code and the COCO-Counterfactuals dataset publicly available.",Datasets & Benchmarks,NeurIPS,2023,Poster,Tiep Le;Vasudev Lal;Phillip Howard,True,https://openreview.net/pdf?id=7AjdHnjIHX
xzEtNSuDJk,LIBERO: Benchmarking Knowledge Transfer for Lifelong Robot Learning,"Lifelong learning offers a promising paradigm of building a generalist agent that learns and adapts over its lifespan. 
Unlike traditional lifelong learning problems in image and text domains, which primarily involve the transfer of declarative knowledge of entities and concepts, lifelong learning in decision-making (LLDM) also necessitates the transfer of procedural knowledge, such as actions and behaviors. To advance research in LLDM, we introduce LIBERO, a novel benchmark of lifelong learning for robot manipulation. Specifically, LIBERO highlights five key research topics in LLDM: 1) how to efficiently transfer declarative knowledge, procedural knowledge, or the mixture of both; 2) how to design effective policy architectures and 3) effective algorithms for LLDM; 4) the robustness of a lifelong learner with respect to task ordering; and 5) the effect of model pretraining for LLDM. We develop an extendible procedural generation pipeline that can in principle generate infinitely many tasks. For benchmarking purpose, we create four task suites (130 tasks in total) that we use to investigate the above-mentioned research topics. To support sample-efficient learning, we provide high-quality human-teleoperated demonstration data for all tasks. Our extensive experiments present several insightful or even unexpected discoveries: sequential finetuning outperforms existing lifelong learning methods in forward transfer, no single visual encoder architecture excels at all types of knowledge transfer, and naive supervised pretraining can hinder agents' performance in the subsequent LLDM.",Datasets & Benchmarks,NeurIPS,2023,Poster,Bo Liu;Yifeng Zhu;Chongkai Gao;Yihao Feng;qiang liu;Yuke Zhu;Peter Stone,True,https://openreview.net/pdf?id=xzEtNSuDJk
l7Ggnzaws5,TradeMaster: A Holistic Quantitative Trading Platform Empowered by Reinforcement Learning,"The financial markets, which involve over \\\\$90 trillion market capitals, attract the attention of innumerable profit-seeking investors globally. Recent explosion of reinforcement learning in financial trading (RLFT) research has shown stellar performance on many quantitative trading tasks. However, it is still challenging to deploy reinforcement learning (RL) methods into real-world financial markets due to the highly composite nature of this domain, which entails design choices and interactions between components that collect financial data, conduct feature engineering, build market environments, make investment decisions, evaluate model behaviors and offers user interfaces. Despite the availability of abundant financial data and advanced RL techniques, a remarkable gap still exists between the potential and realized utilization of RL in financial trading. In particular, orchestrating an RLFT project lifecycle poses challenges in engineering (i.e. hard to build), benchmarking (i.e. hard to compare) and usability (i.e. hard to optimize, maintain and use). To overcome these challenges, we introduce TradeMaster, a holistic open-source RLFT platform that serves as a i) software toolkit, ii) empirical benchmark, and iii) user interface. Our ultimate goal is to provide infrastructures for transparent and reproducible RLFT research and facilitate their real-world deployment with industry impact. TradeMaster will be updated continuously and welcomes contributions from both RL and finance communities.",Datasets & Benchmarks,NeurIPS,2023,Poster,Shuo Sun;Molei Qin;Wentao Zhang;Haochong Xia;Chuqiao Zong;Jie Ying;Yonggang Xie;Lingxuan Zhao;Xinrun Wang;Bo An,False,https://openreview.net/pdf?id=l7Ggnzaws5
Hm1Ih3uLII,DVSOD: RGB-D Video Salient Object Detection,"Salient object detection (SOD) aims to identify standout elements in a scene, with recent advancements primarily focused on integrating depth data (RGB-D) or temporal data from videos to enhance SOD in complex scenes. However, the unison of two types of crucial information remains largely underexplored due to data constraints. To bridge this gap, we in this work introduce the DViSal dataset, fueling further research in the emerging field of RGB-D video salient object detection (DVSOD). Our dataset features 237 diverse RGB-D videos alongside comprehensive annotations, including object and instance-level markings, as well as bounding boxes and scribbles. These resources enable a broad scope for potential research directions. We also conduct benchmarking experiments using various SOD models, affirming the efficacy of multimodal video input for salient object detection. Lastly, we highlight some intriguing findings and promising future research avenues. To foster growth in this field, our dataset and benchmark results are publicly accessible at: https://dvsod.github.io/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jingjing Li;Wei Ji;Size Wang;Wenbo Li;Li Cheng,True,https://openreview.net/pdf?id=Hm1Ih3uLII
9lOVNw7guQ,Low-shot Object Learning with Mutual Exclusivity Bias,"This paper introduces Low-shot Object Learning with Mutual Exclusivity Bias (LSME), the first computational framing of mutual exclusivity bias, a phenomenon commonly observed in infants during word learning. We provide a novel dataset, comprehensive baselines, and a SOTA method to enable the ML community to tackle this challenging learning task. The goal of LSME is to analyze an RGB image of a scene containing multiple objects and correctly associate a previously-unknown object instance with a provided category label. This association is then used to perform low-shot learning to test category generalization. We provide a data generation pipeline for the LSME problem and conduct a thorough analysis of the factors that contribute to its difficulty. Additionally, we evaluate the performance of multiple baselines, including state-of-the-art foundation models. Finally, we present a baseline approach that outperforms state-of-the-art models in terms of low-shot accuracy. Code and data are available at https://github.com/rehg-lab/LSME.",Datasets & Benchmarks,NeurIPS,2023,Poster,Ngoc Anh Thai;Ahmad Humayun;Stefan Stojanov;Zixuan Huang;Bikram Boote;James Matthew Rehg,True,https://openreview.net/pdf?id=9lOVNw7guQ
1ODvxEwsGk,Diverse Community Data for Benchmarking Data Privacy Algorithms,"The Collaborative Research Cycle (CRC) is a National Institute of Standards and Technology (NIST) benchmarking program intended to strengthen understanding of tabular data deidentification technologies. Deidentification algorithms are vulnerable to the same bias and privacy issues that impact other data analytics and machine learning applications, and it can even amplify those issues by contaminating downstream applications. This paper summarizes four CRC contributions: theoretical work on the relationship between diverse populations and challenges for equitable deidentification; public benchmark data focused on diverse populations and challenging features; a comprehensive open source suite of evaluation metrology for deidentified datasets; and an archive of more than 450 deidentified data samples from a broad range of techniques. The initial set of evaluation results demonstrate the value of the CRC tools for investigations in this field.",Datasets & Benchmarks,NeurIPS,2023,Poster,Aniruddha Sen;Christine Task;Dhruv Kapur;Gary Stanley Howarth;Karan Bhagat,True,https://openreview.net/pdf?id=1ODvxEwsGk
n8hpztIuet,SMPLer-X: Scaling Up Expressive Human Pose and Shape Estimation,"Expressive human pose and shape estimation (EHPS) unifies body, hands, and face motion capture with numerous applications. Despite encouraging progress, current state-of-the-art methods still depend largely on a confined set of training datasets. In this work, we investigate scaling up EHPS towards the first generalist foundation model (dubbed SMPLer-X), with up to ViT-Huge as the backbone and training with up to 4.5M instances from diverse data sources. With big data and the large model, SMPLer-X exhibits strong performance across diverse test benchmarks and excellent transferability to even unseen environments. 1) For the data scaling, we perform a systematic investigation on 32 EHPS datasets, including a wide range of scenarios that a model trained on any single dataset cannot handle. More importantly, capitalizing on insights obtained from the extensive benchmarking process, we optimize our training scheme and select datasets that lead to a significant leap in EHPS capabilities. 2) For the model scaling, we take advantage of vision transformers to study the scaling law of model sizes in EHPS. Moreover, our finetuning strategy turn SMPLer-X into specialist models, allowing them to achieve further performance boosts. Notably, our foundation model SMPLer-X consistently delivers state-of-the-art results on seven benchmarks such as AGORA (107.2 mm NMVE), UBody (57.4 mm PVE), EgoBody (63.6 mm PVE), and EHF (62.3 mm PVE without finetuning).",Datasets & Benchmarks,NeurIPS,2023,Poster,Zhongang Cai;Wanqi Yin;Ailing Zeng;CHEN WEI;Qingping SUN;Yanjun Wang;Hui En Pang;Haiyi Mei;Mingyuan Zhang;Lei Zhang;Chen Change Loy;Lei Yang;Ziwei Liu,False,https://openreview.net/pdf?id=n8hpztIuet
tOd8rSjcWz,"Multimodal C4: An Open, Billion-scale Corpus of Images Interleaved with Text","In-context vision and language models like Flamingo support arbitrarily interleaved sequences of images and text as input.
This format not only enables few-shot learning via interleaving independent supervised (image, text) examples, but also, more complex prompts involving interaction between images, e.g., ``What do image A and image B have in common?''
To support this interface, pretraining occurs over web corpora that similarly contain interleaved images+text.
To date, however, large-scale data of this form have not been publicly available.


We release Multimodal C4, an augmentation of the popular text-only C4 corpus with images interleaved.
We use a linear assignment algorithm to place images into longer bodies of text using CLIP features, a process that we show outperforms alternatives.
Multimodal C4 spans everyday topics like cooking, travel, technology, etc. A manual inspection of a random sample of documents shows that a vast majority (88\\\\%) of images are topically relevant, and that linear assignment frequently selects individual sentences specifically well-aligned with each image (80\\\\%). 
After filtering NSFW images, ads, etc., the resulting corpus consists of 101.2M documents with 571M images interleaved in 43B English tokens.",Datasets & Benchmarks,NeurIPS,2023,Poster,Wanrong Zhu;Jack Hessel;Anas Awadalla;Samir Yitzhak Gadre;Jesse Dodge;Alex Fang;Youngjae Yu;Ludwig Schmidt;William Yang Wang;Yejin Choi,True,https://openreview.net/pdf?id=tOd8rSjcWz
Sq3CLKJeiz,Objaverse-XL: A Universe of 10M+ 3D Objects,"Natural language processing and 2D vision models have attained remarkable proficiency on many tasks primarily by escalating the scale of training data. However, 3D vision tasks have not seen the same progress, in part due to the challenges of acquiring high-quality 3D data. In this work, we present Objaverse-XL, a dataset of over 10 million 3D objects. Our compilation comprises deduplicated 3D objects from a diverse set of sources, including manually designed objects, photogrammetry scans of landmarks and everyday items, and professional scans of historic and antique artifacts. Representing the largest scale and diversity in the realm of 3D datasets, Objaverse-XL enables significant new possibilities for 3D vision. Our experiments demonstrate the vast improvements enabled with the scale provided by Objaverse-XL. We show that by training Zero123 on novel view synthesis, utilizing over 100 million multi-view rendered images, we achieve strong zero-shot generalization abilities. We hope that releasing Objaverse-XL will enable further innovations in the field of 3D vision at scale.",Datasets & Benchmarks,NeurIPS,2023,Poster,Matt Deitke;Ruoshi Liu;Matthew Wallingford;Huong Ngo;Oscar Michel;Aditya Kusupati;Alan Fan;Christian Laforte;Vikram Voleti;Samir Yitzhak Gadre;Eli VanderBilt;Aniruddha Kembhavi;Carl Vondrick;Georgia Gkioxari;Kiana Ehsani;Ludwig Schmidt;Ali Farhadi,True,https://openreview.net/pdf?id=Sq3CLKJeiz
zbEYTg2F1U,ASL Citizen: A Community-Sourced Dataset for Advancing Isolated Sign Language Recognition,"Sign languages are used as a primary language by approximately 70 million D/deaf people world-wide. However, most communication technologies operate in spoken and written languages, creating inequities in access. To help tackle this problem, we release ASL Citizen, the first crowdsourced Isolated Sign Language Recognition (ISLR) dataset, collected with consent and containing 83,399 videos for 2,731 distinct signs filmed by 52 signers in a variety of environments. We propose that this dataset be used for sign language dictionary retrieval for American Sign Language (ASL), where a user demonstrates a sign to their webcam to retrieve matching signs from a dictionary. We show that training supervised machine learning classifiers with our dataset advances the state-of-the-art on metrics relevant for dictionary retrieval, achieving 63\\\\% accuracy and a recall-at-10 of 91\\\\%, evaluated entirely on videos of users who are not present in the training or validation sets.",Datasets & Benchmarks,NeurIPS,2023,Poster,Aashaka Desai;Lauren Berger;Fyodor O Minakov;Vanessa Milan;Chinmay Singh;Kriston L Pumphrey;Richard Ladner;Hal Daumé III;Alex Xijie Lu;Naomi Caselli;Danielle Bragg,True,https://openreview.net/pdf?id=zbEYTg2F1U
xhbIud48JN,SituatedGen: Incorporating Geographical and Temporal Contexts into Generative Commonsense Reasoning,"Recently, commonsense reasoning in text generation has attracted much attention. Generative commonsense reasoning is the task that requires machines, given a group of keywords, to compose a single coherent sentence with commonsense plausibility. While existing datasets targeting generative commonsense reasoning focus on everyday scenarios, it is unclear how well machines reason under specific geographical and temporal contexts. We formalize this challenging task as SituatedGen, where machines with commonsense should generate a pair of contrastive sentences given a group of keywords including geographical or temporal entities. We introduce a corresponding English dataset consisting of 8,268 contrastive sentence pairs, which are built upon several existing commonsense reasoning benchmarks with minimal manual labor. Experiments show that state-of-the-art generative language models struggle to generate sentences with commonsense plausibility and still lag far behind human performance. Our dataset is publicly available at https://github.com/yunx-z/situated_gen.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yunxiang Zhang;Xiaojun Wan,True,https://openreview.net/pdf?id=xhbIud48JN
NoE8g3LRAM,Benchmarking Encoder-Decoder Architectures for Biplanar X-ray to 3D Bone Shape Reconstruction,"Various deep learning models have been proposed for 3D bone shape reconstruction from two orthogonal (biplanar) X-ray images.
However, it is unclear how these models compare against each other since they are evaluated on different anatomy, cohort and (often privately held) datasets.
Moreover, the impact of the commonly optimized image-based segmentation metrics such as dice score on the estimation of clinical parameters relevant in 2D-3D bone shape reconstruction is not well known.
To move closer toward clinical translation, we propose a benchmarking framework that evaluates tasks relevant to real-world clinical scenarios, including reconstruction of fractured bones, bones with implants, robustness to population shift, and error in estimating clinical parameters.
Our open-source platform provides reference implementations of 8 models (many of whose implementations were not publicly available), APIs to easily collect and preprocess 6 public datasets, and the implementation of automatic clinical parameter and landmark extraction methods. 
We present an extensive evaluation of 8 2D-3D models on equal footing using 6 public datasets comprising images for four different anatomies.
Our results show that attention-based methods that capture global spatial relationships tend to perform better across all anatomies and datasets; performance on clinically relevant subgroups may be overestimated without disaggregated reporting; ribs are substantially more difficult to reconstruct compared to femur, hip and spine; and the dice score improvement does not always bring corresponding improvement in the automatic estimation of clinically relevant parameters.",Datasets & Benchmarks,NeurIPS,2023,Poster,Mahesh Shakya;Bishesh Khanal,False,https://openreview.net/pdf?id=NoE8g3LRAM
HfKOIPCvsv,RealTime QA: What's the Answer Right Now?,"We introduce RealTime QA, a dynamic question answering (QA) platform that announces questions and evaluates systems on a regular basis (weekly in this version). RealTime QA inquires about the current world, and QA systems need to answer questions about novel events or information. It therefore challenges static, conventional assumptions in open-domain QA datasets and pursues instantaneous applications. We build strong baseline models upon large pretrained language models, including GPT-3 and T5. Our benchmark is an ongoing effort, and this paper presents real-time evaluation results over the past year. Our experimental results show that GPT-3 can often properly update its generation results, based on newly-retrieved documents, highlighting the importance of up-to-date information retrieval. Nonetheless, we find that GPT-3 tends to return outdated answers when retrieved documents do not provide sufficient information to find an answer. This suggests an important avenue for future research: can an open-domain QA system identify such unanswerable cases and communicate with the user or even the retrieval module to modify the retrieval results? We hope that RealTime QA will spur progress in instantaneous applications of question answering and beyond.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jungo Kasai;Keisuke Sakaguchi;yoichi takahashi;Ronan Le Bras;Akari Asai;Xinyan Velocity Yu;Dragomir Radev;Noah A. Smith;Yejin Choi;Kentaro Inui,False,https://openreview.net/pdf?id=HfKOIPCvsv
vv3cocNsEK,HT-Step: Aligning Instructional Articles with How-To Videos,"We introduce HT-Step, a large-scale dataset containing temporal annotations of instructional article steps in cooking videos. It includes 122k segment-level annotations over 20k narrated videos (approximately 2.3k hours) of the HowTo100M dataset.
Each annotation provides a temporal interval, and a categorical step label from a taxonomy of 4,958 unique steps automatically mined from wikiHow articles which include rich descriptions of each step.
Our dataset significantly surpasses existing labeled step datasets in terms of scale, number of tasks, and richness of natural language step descriptions. Based on these annotations, we introduce a strongly supervised benchmark for aligning instructional articles with how-to videos and present a comprehensive evaluation of baseline methods for this task.
By publicly releasing these annotations and defining rigorous evaluation protocols and metrics,
we hope to significantly accelerate research in the field of procedural activity understanding.",Datasets & Benchmarks,NeurIPS,2023,Poster,Triantafyllos Afouras;Effrosyni Mavroudi;Tushar Nagarajan;Huiyu Wang;Lorenzo Torresani,True,https://openreview.net/pdf?id=vv3cocNsEK
wiw5mnja8W,AllSim: Simulating and Benchmarking Resource Allocation Policies in Multi-User Systems,"Numerous real-world systems, ranging from healthcare to energy grids, involve users competing for finite and potentially scarce resources. Designing policies for resource allocation in such real-world systems is challenging for many reasons, including the changing nature of user types and their (possibly urgent) need for resources. Researchers have developed numerous machine learning solutions for determining resource allocation policies in these challenging settings. However, a key limitation has been the absence of good methods and test-beds for benchmarking these policies; almost all resource allocation policies are benchmarked in environments which are either completely synthetic or do not allow _any_ deviation from historical data. In this paper we introduce AllSim, which is a benchmarking environment for realistically simulating the impact and utility of policies for resource allocation in systems in which users compete for such scarce resources. Building such a benchmarking environment is challenging because it needs to successfully take into account _the entire collective_ of potential users and the impact a resource allocation policy has on all the other users in the system. AllSim's benchmarking environment is modular (each component being parameterized individually), learnable (informed by historical data), and customizable (adaptable to changing conditions). These, when interacting with an allocation policy, produce a dataset of simulated outcomes for evaluation and comparison of such policies. We believe AllSim is an essential step towards a more systematic evaluation of policies for scarce resource allocation compared to current approaches for benchmarking such methods.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jeroen Berrevoets;Daniel Jarrett;Alex James Chan;Mihaela van der Schaar,True,https://openreview.net/pdf?id=wiw5mnja8W
PF0lxayYST,On the Need for a Language Describing Distribution Shifts: Illustrations on Tabular Datasets,"Different distribution shifts require different algorithmic and operational
  interventions. Methodological research must be grounded by the specific
  shifts they address.  Although nascent benchmarks provide a promising
  empirical foundation, they \\\\emph{implicitly} focus on covariate
  shifts, and the validity of empirical findings depends on the type of shift, 
  e.g., previous observations on algorithmic performance can fail to be valid when
  the $Y|X$ distribution changes.  We conduct a thorough investigation of
  natural shifts in 5 tabular datasets over 86,000 model configurations, and
  find that $Y|X$-shifts are most prevalent.  To encourage researchers to
  develop a refined language for distribution shifts, we build
 ``WhyShift``, an empirical testbed of curated real-world shifts where
  we characterize the type of shift we benchmark performance over.  Since
  $Y|X$-shifts are prevalent in tabular settings, we \\\\emph{identify covariate
  regions} that suffer the biggest $Y|X$-shifts and discuss implications for
  algorithmic and data-based interventions.  Our testbed highlights the
  importance of future research that builds an understanding of why
  distributions differ.",Datasets & Benchmarks,NeurIPS,2023,Poster,Jiashuo Liu;Tianyu Wang;Peng Cui;Hongseok Namkoong,True,https://openreview.net/pdf?id=PF0lxayYST
CiRHWaRbp0,Benchmarking Robustness to Adversarial Image Obfuscations,"Automated content filtering and moderation is an important tool that allows online platforms to build striving user communities that facilitate cooperation and prevent abuse. Unfortunately, resourceful actors try to bypass automated filters in a bid to post content that violate platform policies and codes of conduct. To reach this goal, these malicious actors may obfuscate policy violating images (e.g., overlay harmful images by carefully selected benign images or visual patterns) to prevent machine learning models from reaching the correct decision. In this paper, we invite researchers to tackle this specific issue and present a new image benchmark. This benchmark, based on ImageNet, simulates the type of obfuscations created by malicious actors. It goes beyond Image-Net-C and ImageNet-C-bar by proposing general, drastic, adversarial modifications that preserve the original content intent. It aims to tackle a more common adversarial threat than the one considered by lp-norm bounded adversaries. We evaluate 33 pretrained models on the benchmark and train models with different augmentations, architectures and training methods on subsets of the obfuscations to measure generalization. Our hope is that this benchmark will encourage researchers to test their models and methods and try to find new approaches that are more robust to these obfuscations.",Datasets & Benchmarks,NeurIPS,2023,Poster,Florian Stimberg;Ayan Chakrabarti;Chun-Ta Lu;Hussein Hazimeh;Otilia Stretcu;Wei Qiao;Yintao Liu;Merve Kaya;Cyrus Rashtchian;Ariel Fuxman;Mehmet Nejat Tek;Sven Gowal,True,https://openreview.net/pdf?id=CiRHWaRbp0
n4OwK8cpx2,REASONER: An Explainable Recommendation Dataset with Comprehensive Labeling Ground Truths,"Explainable recommendation has attracted much attention from the industry and academic communities. It has shown great potential to improve the recommendation persuasiveness, informativeness and user satisfaction. In the past few years, while a lot of promising explainable recommender models have been proposed, the datasets used to evaluate them still suffer from several limitations, for example, the explanation ground truths are not labeled by the real users, the explanations are mostly single-modal and around only one aspect. To bridge these gaps, in this paper, we build a new explainable recommendation dataset, which, to our knowledge, is the first contribution that provides a large amount of real user labeled multi-modal and multi-aspect explaination ground truths. In specific, we firstly develop a video recommendation platform, where a series of questions around the recommendation explainability are carefully designed. Then, we recruit about 3000 high-quality labelers with different backgrounds to use the system, and collect their behaviors and feedback to our questions. In this paper, we detail the construction process of our dataset and also provide extensive analysis on its characteristics. In addition, we develop a library, where ten well-known explainable recommender models are implemented in a unified framework. Based on this library, we build several benchmarks for different explainable recommendation tasks. At last, we present many new opportunities brought by our dataset, which are expected to promote the field of explainable recommendation. Our dataset, library and the related documents have been released at https://reasoner2023.github.io/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Xu Chen;Jingsen Zhang;Lei Wang;Quanyu Dai;Zhenhua Dong;Ruiming Tang;Rui Zhang;Li Chen;Xin Zhao;Ji-Rong Wen,True,https://openreview.net/pdf?id=n4OwK8cpx2
9CKx9SsSSc,ADGym: Design Choices for Deep Anomaly Detection,"Deep learning (DL) techniques have recently found success in anomaly detection (AD) across various fields such as finance, medical services, and cloud computing. However, most of the current research tends to view deep AD algorithms as a whole, without dissecting the contributions of individual design choices like loss functions and network architectures. This view tends to diminish the value of preliminary steps like data preprocessing, as more attention is given to newly designed loss functions, network architectures, and learning paradigms. In this paper, we aim to bridge this gap by asking two key questions: (i) Which design choices in deep AD methods are crucial for detecting anomalies? (ii) How can we automatically select the optimal design choices for a given AD dataset, instead of relying on generic, pre-existing solutions? To address these questions, we introduce ADGym, a platform specifically crafted for comprehensive evaluation and automatic selection of AD design elements in deep methods. Our extensive experiments reveal that relying solely on existing leading methods is not sufficient. In contrast, models developed using ADGym significantly surpass current state-of-the-art techniques.",Datasets & Benchmarks,NeurIPS,2023,Poster,Minqi Jiang;Chaochuan Hou;Ao Zheng;Songqiao Han;Hailiang Huang;Qingsong Wen;Xiyang Hu;Yue Zhao,False,https://openreview.net/pdf?id=9CKx9SsSSc
C0zw2ERKiQ,Revisiting the Evaluation of Image Synthesis with GANs,"A good metric, which promises a reliable comparison between solutions, is essential for any well-defined task. Unlike most vision tasks that have per-sample ground-truth, image synthesis tasks target generating unseen data and hence are usually evaluated through a distributional distance between one set of real samples and another set of generated samples. This study presents an empirical investigation into the evaluation of synthesis performance, with generative adversarial networks (GANs) as a representative of generative models. In particular, we make in-depth analyses of various factors, including how to represent a data point in the representation space, how to calculate a fair distance using selected samples, and how many instances to use from each set. Extensive experiments conducted on multiple datasets and settings reveal several important findings. Firstly, a group of models that include both CNN-based and ViT-based architectures serve as reliable and robust feature extractors for measurement evaluation. Secondly, Centered Kernel Alignment (CKA) provides a better comparison across various extractors and hierarchical layers in one model. Finally, CKA is more sample-efficient and enjoys better agreement with human judgment in characterizing the similarity between two internal data correlations. These findings contribute to the development of a new measurement system, which enables a consistent and reliable re-evaluation of current state-of-the-art generative models.",Datasets & Benchmarks,NeurIPS,2023,Poster,Mengping Yang;Ceyuan Yang;Yichi Zhang;Qingyan Bai;Yujun Shen;Bo Dai,False,https://openreview.net/pdf?id=C0zw2ERKiQ
yZQDF9f6bR,Diplomat: A Dialogue Dataset for Situated PragMATic Reasoning,"The ability to discern and comprehend pragmatic meanings is a cornerstone of social and emotional intelligence, referred to as pragmatic reasoning. Despite the strides made in the development of Large Language Models (LLMs), such as ChatGPT, these models grapple with capturing the nuanced and ambiguous facets of language, falling short of the aspiration to build human-like conversational agents. In this work, we introduce a novel benchmark, the **DiPlomat**, which delves into the fundamental components of conversational pragmatic reasoning, encompassing situational context reasoning, open-world knowledge acquisition, and unified figurative language understanding. We start by collecting a new human-annotated dialogue dataset, composed of 4,177 multi-turn dialogues and a vocabulary of 48,900 words. Along with the dataset, two tasks are proposed to evaluate machines' pragmatic reasoning capabilities, namely, Pragmatic Reasoning and Identification(PIR) and Conversational Question Answering (CQA). Furthermore, we probe into a zero-shot natural language inference task, where the significance of context in pragmatic reasoning is underscored. Experimental findings illustrate the existing limitations of current prevailing LLMs in the realm of pragmatic reasoning, shedding light on the pressing need for further research to facilitate the emergence of emotional intelligence within human-like conversational agents.",Datasets & Benchmarks,NeurIPS,2023,Poster,Hengli Li;Song-Chun Zhu;Zilong Zheng,True,https://openreview.net/pdf?id=yZQDF9f6bR
RgdGkPRQ03,WildfireSpreadTS: A dataset of multi-modal time series for wildfire spread prediction,"We present a multi-temporal, multi-modal remote-sensing dataset for predicting how active wildfires will spread at a resolution of 24 hours. The dataset consists of 13607 images across 607 fire events in the United States from January 2018 to October 2021. For each fire event, the dataset contains a full time series of daily observations, containing detected active fires and variables related to fuel, topography and weather conditions. The dataset is challenging due to: a) its inputs being multi-temporal, b) the high number of 23 multi-modal input channels, c) highly imbalanced labels and d) noisy labels, due to smoke, clouds, and inaccuracies in the active fire detection. The underlying complexity of the physical processes adds to these challenges. Compared to existing public datasets in this area, WildfireSpreadTS allows for multi-temporal modeling of spreading wildfires, due to its time series structure. Furthermore, we provide additional input modalities and a high spatial resolution of 375m for the active fire maps. We publish this dataset to encourage further research on this important task with multi-temporal, noise-resistant or generative methods, uncertainty estimation or advanced optimization techniques that deal with the high-dimensional input space.",Datasets & Benchmarks,NeurIPS,2023,Poster,Sebastian Gerard;Yu Zhao;Josephine Sullivan,True,https://openreview.net/pdf?id=RgdGkPRQ03
u2cXRGm95Y,UUKG: Unified Urban Knowledge Graph Dataset for Urban Spatiotemporal Prediction,"Accurate Urban SpatioTemporal Prediction (USTP) is of great importance to the development and operation of the smart city. As an emerging building block, multi-sourced urban data are usually integrated as urban knowledge graphs (UrbanKGs) to provide critical knowledge for urban spatiotemporal prediction models. However, existing UrbanKGs are often tailored for specific downstream prediction tasks and are not publicly available, which limits the potential advancement. This paper presents UUKG, the unified urban knowledge graph dataset for knowledge-enhanced urban spatiotemporal predictions. Specifically, we first construct UrbanKGs consisting of millions of triplets for two metropolises by connecting heterogeneous urban entities such as administrative boroughs, POIs, and road segments. 
Moreover, we conduct qualitative and quantitative analysis on constructed UrbanKGs and uncover diverse high-order structural patterns, such as hierarchies and cycles, that can be leveraged to benefit downstream USTP tasks. To validate and facilitate the use of UrbanKGs, we implement and evaluate 15 KG embedding methods on the KG completion task and integrate the learned KG embeddings into 9 spatiotemporal models for five different USTP tasks. The extensive experimental results not only provide benchmarks of knowledge-enhanced USTP models under different task settings but also highlight the potential of state-of-the-art high-order structure-aware UrbanKG embedding methods. We hope the proposed UUKG fosters research on urban knowledge graphs and broad smart city applications. The dataset and source code are available at https://github.com/usail-hkust/UUKG/.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yansong Ning;Hao Liu;Hao Henry Wang;Zhenyu Zeng;Hui Xiong,True,https://openreview.net/pdf?id=u2cXRGm95Y
aKnWIrDPiR,Multimodal Clinical Benchmark for Emergency Care (MC-BEC): A Comprehensive Benchmark for Evaluating Foundation Models in Emergency Medicine,"We propose the Multimodal Clinical Benchmark for Emergency Care (MC-BEC), a comprehensive benchmark for evaluating foundation models in Emergency Medicine using a dataset of 100K+ continuously monitored Emergency Department visits from 2020-2022. MC-BEC focuses on clinically relevant prediction tasks at timescales from minutes to days, including predicting patient decompensation, disposition, and emergency department (ED) revisit, and includes a standardized evaluation framework with train-test splits and evaluation metrics. The multimodal dataset includes a wide range of detailed clinical data, including triage information, prior diagnoses and medications, continuously measured vital signs, electrocardiogram and photoplethysmograph waveforms, orders placed and medications administered throughout the visit, free-text reports of imaging studies, and information on ED diagnosis, disposition, and subsequent revisits. We provide performance baselines for each prediction task to enable the evaluation of multimodal, multitask models. We believe that MC-BEC will encourage researchers to develop more effective, generalizable, and accessible foundation models for multimodal clinical data.",Datasets & Benchmarks,NeurIPS,2023,Poster,Emma Chen;Aman Kansal;Julie Chen;Boyang Tom Jin;Julia Rachel Reisler;David A Kim;Pranav Rajpurkar,True,https://openreview.net/pdf?id=aKnWIrDPiR
yEf8NSqTPu,PopSign ASL v1.0: An Isolated American Sign Language Dataset Collected via Smartphones,"PopSign is a smartphone-based bubble-shooter game that helps hearing parents
of deaf infants learn sign language. To help parents practice their ability to sign,
PopSign is integrating sign language recognition as part of its gameplay. For
training the recognizer, we introduce the PopSign ASL v1.0 dataset that collects
examples of 250 isolated American Sign Language (ASL) signs using Pixel 4A
smartphone selfie cameras in a variety of environments. It is the largest publicly
available, isolated sign dataset by number of examples and is the first dataset to
focus on one-handed, smartphone signs. We collected over 210,000 examples
at 1944x2592 resolution made by 47 consenting Deaf adult signers for whom
American Sign Language is their primary language. We manually reviewed 217,866
of these examples, of which 175,022 (approximately 700 per sign) were the sign
intended for the educational game. 39,304 examples were recognizable as a sign
but were not the desired variant or were a different sign. We provide a training set
of 31 signers, a validation set of eight signers, and a test set of eight signers. A
baseline LSTM model for the 250-sign vocabulary achieves 82.1% accuracy (81.9%
class-weighted F1 score) on the validation set and 84.2% (83.9% class-weighted
F1 score) on the test set. Gameplay suggests that accuracy will be sufficient for
creating educational games involving sign language recognition.",Datasets & Benchmarks,NeurIPS,2023,Poster,Thad Starner;Sean Forbes;Matthew So;David Martin;Rohit Sridhar;Gururaj Deshpande;Sam Sepah;Sahir Shahryar;Khushi Bhardwaj;Tyler Kwok;Daksh Sehgal;Saad Hassan;Bill Neubauer;Sofia Anandi Vempala;Alec Tan;Jocelyn Heath;Unnathi Utpal Kumar;Priyanka Vijayaraghavan Mosur;Tavenner M. Hall;Rajandeep Singh;Christopher Zhang Cui;Glenn Cameron;Sohier Dane;Garrett Tanzer,True,https://openreview.net/pdf?id=yEf8NSqTPu
QEDjXv9OyY,"YouTube-ASL: A Large-Scale, Open-Domain American Sign Language-English Parallel Corpus","Machine learning for sign languages is bottlenecked by data. In this paper, we present YouTube-ASL, a large-scale, open-domain corpus of American Sign Language (ASL) videos and accompanying English captions drawn from YouTube. With ~1000 hours of videos and >2500 unique signers, YouTube-ASL is ~3x as large and has ~10x as many unique signers as the largest prior ASL dataset. We train baseline models for ASL to English translation on YouTube-ASL and evaluate them on How2Sign, where we achieve a new fine-tuned state of the art of 12.397 BLEU and, for the first time, nontrivial zero-shot results.",Datasets & Benchmarks,NeurIPS,2023,Poster,David Uthus;Garrett Tanzer;Manfred Georg,True,https://openreview.net/pdf?id=QEDjXv9OyY
ZJWQfgXQb6,The ToMCAT Dataset,"We present a rich, multimodal dataset consisting of data from 40 teams of three humans conducting simulated urban search-and-rescue (SAR) missions in a Minecraft-based testbed, collected for the Theory of Mind-based Cognitive Architecture for Teams (ToMCAT) project. Modalities include two kinds of brain scan data---functional near-infrared spectroscopy (fNIRS) and electroencephalography (EEG), as well as skin conductance, heart rate, eye tracking, face images, spoken dialog audio data with automatic speech recognition (ASR) transcriptions, game screenshots, gameplay data, game performance data, demographic data, and self-report questionnaires. Each team undergoes up to six consecutive phases: three behavioral tasks, one mission training session, and two collaborative SAR missions. As time-synchronized multimodal data collected under a variety of circumstances, this dataset will support studying a large variety of research questions on topics including teamwork, coordination, plan recognition, affective computing, physiological linkage, entrainment, and dialog understanding.  We provide an initial public release of the de-identified data, along with analyses illustrating the utility of this dataset to both computer scientists and social scientists.",Datasets & Benchmarks,NeurIPS,2023,Poster,Adarsh Pyarelal;Eric Duong;Caleb Jones Shibu;Paulo Soares;Savannah Boyd;Payal Khosla;Valeria Pfeifer;Diheng Zhang;Eric S Andrews;Rick Champlin;Vincent Paul Raymond;Meghavarshini Krishnaswamy;Clayton Morrison;Emily Butler;Kobus Barnard,True,https://openreview.net/pdf?id=ZJWQfgXQb6
DILUIcDmU9,HA-ViD: A Human Assembly Video Dataset for Comprehensive Assembly Knowledge Understanding,"Understanding comprehensive assembly knowledge from videos is critical for futuristic ultra-intelligent industry. To enable technological breakthrough, we present HA-ViD – the first human assembly video dataset that features representative industrial assembly scenarios, natural procedural knowledge acquisition process, and consistent human-robot shared annotations. Specifically, HA-ViD captures diverse collaboration patterns of real-world assembly, natural human behaviors and learning progression during assembly, and granulate action annotations to subject, action verb, manipulated object, target object, and tool. We provide 3222 multi-view and multi-modality videos), 1.5M frames, 96K temporal labels and 2M spatial labels. We benchmark four foundational video understanding tasks: action recognition, action segmentation, object detection and multi-object tracking. Importantly, we analyze their performance and the further reasoning steps for comprehending knowledge in assembly progress, process efficiency, task collaboration, skill parameters and human intention. Details of HA-ViD is available at: https://iai-hrc.github.io/ha-vid.",Datasets & Benchmarks,NeurIPS,2023,Poster,Hao Zheng;Regina Lee;Yuqian Lu,True,https://openreview.net/pdf?id=DILUIcDmU9
XYxNklOMMX,Benchmarking Distribution Shift in Tabular Data with TableShift,"Robustness to distribution shift has become a growing concern for text and image models as they transition from research subjects to deployment in the real world. However, high-quality benchmarks for distribution shift in tabular machine learning tasks are still lacking despite the widespread real-world use of tabular data and differences in the models used for tabular data in comparison to text and images. As a consequence, the robustness of tabular models to distribution shift is poorly understood. To address this issue, we introduce TableShift, a distribution shift benchmark for tabular data. TableShift contains 15 binary classification tasks in total, each with an associated shift, and includes a diverse set of data sources, prediction targets, and distribution shifts. The benchmark covers domains including finance, education, public policy, healthcare, and civic participation, and is accessible using only a few lines of Python code via the TableShift API. We conduct a large-scale study comparing several state-of-the-art tabular data models alongside robust learning and domain generalization methods on the benchmark tasks. Our study demonstrates (1) a linear trend between in-distribution (ID) and out-of-distribution (OOD) accuracy; (2) domain robustness methods can reduce shift gaps but at the cost of reduced ID accuracy; (3) a strong relationship between shift gap (difference between ID and OOD performance) and shifts in the label distribution. The benchmark data, Python package, model implementations, and more information about TableShift are available at https://github.com/mlfoundations/tableshift and https://tableshift.org .",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=XYxNklOMMX
EFl8zjjXeX,OV-PARTS: Towards Open-Vocabulary Part Segmentation,"Segmenting and recognizing diverse object parts is a crucial ability in applications spanning various computer vision and robotic tasks. While significant progress has been made in object-level Open-Vocabulary Semantic Segmentation (OVSS), i.e., segmenting objects with arbitrary text, the corresponding part-level research poses additional challenges. Firstly, part segmentation inherently involves intricate boundaries, while limited annotated data compounds the challenge. Secondly, part segmentation introduces an open granularity challenge due to the diverse and often ambiguous definitions of parts in the open world. Furthermore, the large-scale vision and language models, which play a key role in the open vocabulary setting, struggle to recognize parts as effectively as objects. To comprehensively investigate and tackle these challenges, we propose an Open-Vocabulary Part Segmentation (OV-PARTS) benchmark. OV-PARTS includes refined versions of two publicly available datasets: Pascal-Part-116 and ADE20K-Part-234. And it covers three specific tasks: Generalized Zero-Shot Part Segmentation, Cross-Dataset Part Segmentation, and Few-Shot Part Segmentation, providing insights into analogical reasoning, open granularity and few-shot adapting abilities of models. Moreover, we analyze and adapt two prevailing paradigms of existing object-level OVSS methods for OV-PARTS. Extensive experimental analysis is conducted to inspire future research in leveraging foundational models for OV-PARTS. The code and dataset are available at https://github.com/kellyiss/OV_PARTS.",Datasets & Benchmarks,NeurIPS,2023,Poster,Meng Wei;Xiaoyu Yue;Wenwei Zhang;Shu Kong;Xihui Liu;Jiangmiao Pang,True,https://openreview.net/pdf?id=EFl8zjjXeX
vfzXDRTcF4,JourneyDB: A Benchmark for Generative Image Understanding,"While recent advancements in vision-language models have had a transformative impact on multi-modal comprehension, the extent to which these models possess the ability to comprehend generated images remains uncertain. Synthetic images, in comparison to real data, encompass a higher level of diversity in terms of both content and style, thereby presenting significant challenges for the models to fully grasp. In light of this challenge, we introduce a comprehensive dataset, referred to as JourneyDB, that caters to the domain of generative images within the context of multi-modal visual understanding. Our meticulously curated dataset comprises 4 million distinct and high-quality generated images, each paired with the corresponding text prompts that were employed in their creation. Furthermore, we additionally introduce an external subset with results of another 22 text-to-image generative models, which makes JourneyDB a comprehensive benchmark for evaluating the comprehension of generated images. On our dataset, we have devised four benchmarks to assess the performance of generated image comprehension in relation to both content and style interpretation. These benchmarks encompass prompt inversion, style retrieval, image captioning, and visual question answering. Lastly, we evaluate the performance of state-of-the-art multi-modal models when applied to the JourneyDB dataset, providing a comprehensive analysis of their strengths and limitations in comprehending generated content. We anticipate that the proposed dataset and benchmarks will facilitate further research in the field of generative content understanding. The dataset is publicly available at https://journeydb.github.io.",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=vfzXDRTcF4
ZDnnzsado4,A Step Towards Worldwide Biodiversity Assessment: The BIOSCAN-1M Insect Dataset,"In an effort to catalog insect biodiversity, we propose a new large dataset of hand-labelled insect images, the BIOSCAN-1M Insect Dataset. Each record is taxonomically classified by an expert, and also has associated genetic information including raw nucleotide barcode sequences and assigned barcode index numbers, which are genetic-based proxies for species classification. This paper presents a curated million-image dataset, primarily to train computer-vision models capable of providing image-based taxonomic assessment, however, the dataset also presents compelling characteristics, the study of which would be of interest to the broader machine learning community. Driven by the biological nature inherent to the dataset, a characteristic long-tailed class-imbalance distribution is exhibited. Furthermore, taxonomic labelling is a hierarchical classification scheme, presenting a highly fine-grained classification problem at lower levels. Beyond spurring interest in biodiversity research within the machine learning community, progress on creating an image-based taxonomic classifier will also further the ultimate goal of all BIOSCAN research: to lay the foundation for a comprehensive survey of global biodiversity. This paper introduces the dataset and explores the classification task through the implementation and analysis of a baseline classifier. The code repository of the BIOSCAN-1M-Insect dataset is available at https://github.com/zahrag/BIOSCAN-1M",Datasets & Benchmarks,NeurIPS,2023,Poster,Zahra Gharaee;ZeMing Gong;Nicholas Pellegrino;Iuliia Zarubiieva;Joakim Bruslund Haurum;Scott C Lowe;Jaclyn McKeown;Chris C.Y. Ho;Joschka McLeod;Yi-Yun Catherine Wei;Jireh Agda;Sujeevan Ratnasingham;Dirk Steinke;Angel X Chang;Graham W. Taylor;Paul W. Fieguth,True,https://openreview.net/pdf?id=ZDnnzsado4
dOeBYjxSoq,SG×P : A Sorghum Genotype × Phenotype Prediction Dataset and Benchmark,"Large scale field-phenotyping approaches have the potential to solve important questions about the relationship of plant genotype to plant phenotype.  Computational approaches to measuring the phenotype (the observable plant features) are required to address the problem at a large scale, but machine learning approaches to extract phenotypes from sensor data have been hampered by limited access to (a) sufficiently large, organized multi-sensor datasets, (b) field trials that have a large scale and significant number of genotypes, (c) full genetic sequencing of those phenotypes, and (d) datasets sufficiently organized so that algorithm centered researchers can directly address the real biological problems.  To address this, we present SGxP, a novel benchmark dataset from a large-scale field trial consisting of the complete genotype of over 300 sorghum varieties, and time sequences of imagery from several field plots growing each variety, taken with RGB and laser 3D scanner imaging.  To lower the barrier to entry and facilitate further developments, we provide a set of well organized, multi-sensor imagery and corresponding genomic data.  We implement baseline deep learning based phenotyping approaches to create baseline results for individual sensors and multi-sensor fusion for detecting genetic mutations with known impacts.  We also provide and support an open-ended challenge by identifying thousands of genetic mutations whose phenotypic impacts are currently unknown.  A web interface for machine learning researchers and practitioners to share approaches, visualizations and hypotheses supports engagement with plant biologists to further the understanding of the sorghum genotype x phenotype relationship. The full dataset, leaderboard (including baseline results) and discussion forums can be found at http://sorghumsnpbenchmark.com.",Datasets & Benchmarks,NeurIPS,2023,Poster,Zeyu Zhang;Robert Pless;Nadia Shakoor;Austin Carnahan;Abby Stylianou,True,https://openreview.net/pdf?id=dOeBYjxSoq
6jOlRwnqbb,Humans in Kitchens: A Dataset for Multi-Person Human Motion Forecasting with Scene Context,"Forecasting human motion of multiple persons is very challenging. It requires to model the interactions between humans and the interactions with objects and the environment. For example, a person might want to make a coffee, but if the coffee machine is already occupied the person will have
to wait. These complex relations between scene geometry and persons arise
constantly in our daily lives, and models that wish to accurately forecast
human behavior will have to take them into consideration. To facilitate research in this direction, we propose Humans in Kitchens, a
large-scale multi-person human motion dataset with annotated 3D human poses, scene geometry and activities per person and frame.
Our dataset consists of over 7.3h recorded data of up to 16 persons at the same time in four kitchen scenes, with more than 4M annotated human poses, represented by a parametric 3D body model. In addition, dynamic scene geometry and objects like chair or cupboard are annotated per frame. As first benchmarks, we propose two protocols for short-term and long-term human motion forecasting.",Datasets & Benchmarks,NeurIPS,2023,Poster,Julian Alexander Tanke;Oh-Hun Kwon;Felix Benjamin Mueller;Andreas Doering;Juergen Gall,True,https://openreview.net/pdf?id=6jOlRwnqbb
4dsMX3RnF0,"Knowledge-based in silico models and dataset for the comparative evaluation of mammography AI for a range of breast characteristics, lesion conspicuities and doses","To generate evidence regarding the safety and efficacy of artificial intelligence (AI) enabled medical devices, AI models need to be evaluated on a diverse population of patient cases, some of which may not be readily available. We propose an evaluation approach for testing medical imaging AI models that relies on in silico imaging pipelines in which stochastic digital models of human anatomy (in object space) with and without pathology are imaged using a digital replica imaging acquisition system to generate realistic synthetic image datasets. Here, we release M-SYNTH, a dataset of cohorts with four breast fibroglandular density distributions imaged at different exposure levels using Monte Carlo x-ray simulations with the publicly available Virtual Imaging Clinical Trial for Regulatory Evaluation (VICTRE) toolkit. We utilize the synthetic dataset to analyze AI model performance and find that model performance decreases with increasing breast density and increases with higher mass density, as expected. As exposure levels decrease, AI model performance drops with the highest performance achieved at exposure levels lower than the nominal recommended dose for the breast type.",Datasets & Benchmarks,NeurIPS,2023,Poster,Elena Sizikova;Niloufar Saharkhiz;Diksha Sharma;Miguel Lago;Berkman Sahiner;Jana Gut Delfino;Aldo Badano,True,https://openreview.net/pdf?id=4dsMX3RnF0
5ADv5OfQgU,trajdata: A Unified Interface to Multiple Human Trajectory Datasets,"The field of trajectory forecasting has grown significantly in recent years, partially owing to the release of numerous large-scale, real-world human trajectory datasets for autonomous vehicles (AVs) and pedestrian motion tracking. While such datasets have been a boon for the community, they each use custom and unique data formats and APIs, making it cumbersome for researchers to train and evaluate methods across multiple datasets. To remedy this, we present trajdata: a unified interface to multiple human trajectory datasets. At its core, trajdata provides a simple, uniform, and efficient representation and API for trajectory and map data. As a demonstration of its capabilities, in this work we conduct a comprehensive empirical evaluation of existing trajectory datasets, providing users with a rich understanding of the data underpinning much of current pedestrian and AV motion forecasting research, and proposing suggestions for future datasets from these insights. trajdata is permissively licensed (Apache 2.0) and can be accessed online at https://github.com/NVlabs/trajdata.",Datasets & Benchmarks,NeurIPS,2023,Poster,Boris Ivanovic;Guanyu Song;Igor Gilitschenski;Marco Pavone,False,https://openreview.net/pdf?id=5ADv5OfQgU
n2wW7goGky,AirDelhi: Fine-Grained Spatio-Temporal Particulate Matter Dataset From Delhi For ML based Modeling,"Air pollution poses serious health concerns  in developing countries, such as India, necessitating large-scale measurement for correlation analysis, policy recommendations, and informed decision-making. However, fine-grained data collection is costly.  Specifically, static sensors for pollution measurement cost several thousand dollars per unit, leading to inadequate deployment and coverage. To complement the existing sparse static sensor network, we propose a mobile sensor network utilizing lower-cost PM2.5 sensors mounted on public buses in the Delhi-NCR region of India. Through this exercise, we introduce a novel dataset AirDelhi comprising PM2.5 and PM10 measurements. This dataset is made publicly available, at https://www.cse.iitd.ac.in/pollutiondata, serving as a valuable resource for machine learning (ML) researchers and environmentalists. We present three key contributions with the release of this dataset. Firstly, through in-depth statistical analysis, we demonstrate that the released dataset significantly differs from existing pollution datasets, highlighting its uniqueness and potential for new insights. Secondly, the dataset quality been validated against existing expensive sensors. Thirdly, we conduct a benchmarking exercise (https://github.com/sachin-iitd/DelhiPMDatasetBenchmark), evaluating state-of-the-art methods for interpolation, feature imputation, and forecasting on this dataset, which is the largest publicly available PM dataset to date. The results of the benchmarking exercise underscore the substantial disparities in accuracy between the proposed dataset and other publicly available datasets. This finding highlights the complexity and richness of our dataset, emphasizing its value for advancing research in the field of air pollution.",Datasets & Benchmarks,NeurIPS,2023,Poster,,True,https://openreview.net/pdf?id=n2wW7goGky
SDJ3kYpJFX,DISCO-10M: A Large-Scale Music Dataset,"Music datasets play a crucial role in advancing research in machine learning for music. However, existing music datasets suffer from limited size, accessibility, and lack of audio resources. To address these shortcomings, we present DISCO-10M, a novel and extensive music dataset that surpasses the largest previously available music dataset by an order of magnitude. To ensure high-quality data, we implement a multi-stage filtering process. This process incorporates similarities based on textual descriptions and audio embeddings. Moreover, we provide precomputed CLAP embeddings alongside DISCO-10M, facilitating direct application on various downstream tasks. These embeddings enable efficient exploration of machine learning applications on the provided data. With DISCO-10M, we aim to democratize and facilitate new research to help advance the development of novel machine learning models for music: https://huggingface.co/DISCOX",Datasets & Benchmarks,NeurIPS,2023,Poster,Luca A Lanzendörfer;Florian Grötschla;Emil Funke;Roger Wattenhofer,True,https://openreview.net/pdf?id=SDJ3kYpJFX
gO0kS0eE0F,OpenProteinSet: Training data for structural biology at scale,"Multiple sequence alignments (MSAs) of proteins encode rich biological information and have been workhorses in bioinformatic methods for tasks like protein design and protein structure prediction for decades. Recent breakthroughs like AlphaFold2 that use transformers to attend directly over large quantities of raw MSAs have reaffirmed their importance. Generation of MSAs is highly computationally intensive, however, and no datasets comparable to those used to train AlphaFold2 have been made available to the research community, hindering progress in machine learning for proteins. To remedy this problem, we introduce OpenProteinSet, an open-source corpus of more than 16 million MSAs, associated structural homologs from the Protein Data Bank, and AlphaFold2 protein structure predictions. We have previously demonstrated the utility of OpenProteinSet by successfully retraining AlphaFold2 on it. We expect OpenProteinSet to be broadly useful as training and validation data for 1) diverse tasks focused on protein structure, function, and design and 2) large-scale multimodal machine learning research.",Datasets & Benchmarks,NeurIPS,2023,Poster,Gustaf Ahdritz;Nazim Bouatta;Sachin Kadyan;Lukas Jarosch;Dan Berenberg;Ian Fisk;Andrew Martin Watkins;Stephen Ra;Richard Bonneau;Mohammed AlQuraishi,True,https://openreview.net/pdf?id=gO0kS0eE0F
HvcLKgtbco,Towards a Comprehensive Benchmark for High-Level Synthesis Targeted to FPGAs,"High-level synthesis (HLS) aims to raise the abstraction layer in hardware design, enabling the design of domain-specific accelerators (DSAs) like field-programmable gate arrays (FPGAs) using C/C++ instead of hardware description languages (HDLs). Compiler directives in the form of pragmas play a crucial role in modifying the microarchitecture within the HLS framework. However, the space of possible microarchitectures grows exponentially with the number of pragmas. Moreover, the evaluation of each candidate design using the HLS tool consumes significant time, ranging from minutes to hours, leading to a time-consuming optimization process. To accelerate this process, machine learning models have been used to predict design quality in milliseconds. However, existing open-source datasets for training such models are limited in terms of design complexity and available optimizations. In this paper, we present HLSyn, the first benchmark that addresses these limitations. It contains more complex programs with a wider range of optimization pragmas, making it a comprehensive dataset for training and evaluating design quality prediction models. The HLSyn benchmark consists of 42 unique programs/kernels, resulting in over 42,000 labeled designs. We conduct an extensive comparison of state-of-the-art baselines to assess their effectiveness in predicting design quality. As an ongoing project, we anticipate expanding the HLSyn benchmark in terms of both quantity and variety of programs to further support the development of this field.",Datasets & Benchmarks,NeurIPS,2023,Poster,Yunsheng Bai;Atefeh Sohrabizadeh;Zongyue Qin;Ziniu Hu;Yizhou Sun;Jason Cong,True,https://openreview.net/pdf?id=HvcLKgtbco
01I55gys19,Few-Class Arena: A Benchmark for Efficient Vision Model Selection and Dataset Difficulty,"A wide variety of benchmark datasets with many classes (80-1000) have been created to assist Computer Vision architectural evolution. An increasing number of vision models are evaluated with these many-class datasets. However, real-world applications often involve substantially fewer classes of interest (2-10). This gap between many and few classes makes it difficult to predict performance of the few-class applications using the available many-class datasets. To date, little has been offered to evaluate models in this Few-Class Regime. We propose Few-Class Arena (FCA), as a unified benchmark with focus on testing efficient image classification models for few classes. We conduct a systematic evaluation of the ResNet family trained on ImageNet subsets from 2 to 1000 classes, and test a wide spectrum of Convolutional Neural Networks and Transformer architectures over ten datasets by using our newly proposed FCA tool. Furthermore, to aid an up-front assessment of dataset difficulty and a more efficient selection of models, we incorporate a difficulty measure as a function of class similarity. FCA offers a new tool for efficient machine learning in the Few-Class Regime, with goals ranging from new efficient similarity proposal, lightweight model architecture design to new scaling law discovery. FCA is user-friendly and can be easily extended to new models and datasets, facilitating future research work. Our benchmark is available at https://github.com/fewclassarena/fca.",Datasets & Benchmarks,NeurIPS,2024,Reject,Bryan Bo Cao;Lawrence O'Gorman;Michael Coss;Shubham Jain,True,https://openreview.net/pdf?id=01I55gys19
0SMhqvgHST,EEVEE and GATE: Finding the right benchmarks and how to run them seamlessly,"Model evaluation is a cornerstone of machine learning, guiding model design and progress measurement. Designing generalizable evaluation processes remains a challenge, however, partly due to the vast number of possible domain, task and modality combinations and lack of knowledge of how informative they are. In this paper, we propose EEVEE (Efficient Evaluation process Evolution Engine) - pronounced as \\\\textipa{/'i:vi:/} EE-vee - a method that frames evaluation process design as a learning problem. By analyzing a large number of evaluation metrics from diverse benchmarks and models, EEVEE identifies a smaller subset of tasks with high predictive power over the full set of evaluation metrics, reducing evaluation time. To find the optimal subset maximizing signal while minimizing GPU hours, EEVEE evaluates pre-trained models of various architectures, pretraining schemes, and modalities on diverse downstream tasks and datasets including image classification, segmentation, relational reasoning, zero-shot image-to-text tasks, medical classification and segmentation, video classification, and regression. Our results identify three subsets of benchmarks, with 8, 15 and 21 tasks, providing high quality signal for model generalization. Key benchmarks selected include iWildCam, CLEVR-Math, ACDC, WinoGround, CIFAR100, Fungi, and ADE20K. We structure the subsets into three tiers for 12, 24, and 36 GPU-hour budgets and package them into a unified, efficient, and user-friendly Python framework that we built with the researcher in mind -- which we refer to as the GATE engine.
Our experiments reveal ConvNextV2, SigLIP and CLIP as top-performing model encoders, with EfficientNetV2 and ResNext50 excelling in medical tasks and challenging image classification, in particular in Happy Whale Individual classification, ConvNet based models seem to outperform transformer models by a factor of 2.5x, which is surprising. The top performing encoder being ConvNextV2 followed by CLIP seems to agree with other recent large scale evaluations. We also demonstrate the framework's versatility in fine-tuning models from text and audio modalities, paving the way for future cross-modal evaluations.",Datasets & Benchmarks,NeurIPS,2024,Reject,Antreas Antoniou;Eleni Triantafillou;Hugo Larochelle;Sebastien Montella;Fady Rezk;Kiyoon Kim;Linus Ericsson;Pavlos Vougiouklis;Justin Engelmann;Elliot J. Crowley;Srihari Humbarwadi;Yi Liu;Guang Yang;Jeff Z. Pan;Amos Storkey,False,https://openreview.net/pdf?id=0SMhqvgHST
2HzZIDo48o,Meta-Referential Games to Learn Compositional Learning Behaviours,"Human beings use compositionality to generalise from past experiences to novel experiences, by assuming that past experiences can be separated into fundamental atomic components that can be recombined in novel ways. % to support our ability to engage with novel experiences.
We frame this as the ability to learn to generalise compositionally, and refer to behaviours making use of this ability as compositional learning behaviours (CLBs).
A central problem to learning CLBs is the resolution of a binding problem (BP).
While it is another feat of intelligence that human beings perform with ease, it is not the case for state-of-the-art artificial agents.
Thus, in order to build artificial agents able to collaborate with human beings, we propose to develop a novel benchmark to investigate agents' abilities to exhibit CLBs by solving a domain-agnostic version of the BP. 
We take inspiration from the language emergence and grounding framework of referential games and propose a meta-learning extension of referential games, entitled Meta-Referential Games, and use this framework to build our benchmark, the Symbolic Behaviour Benchmark (S2B). 
We provide baseline results and error analysis showing that the S2B is a compelling challenge that we hope will spur the research community towards developing more capable artificial agents.",Datasets & Benchmarks,NeurIPS,2024,Reject,Kevin Yandoka Denamganai;Sondess Missaoui;James Alfred Walker,True,https://openreview.net/pdf?id=2HzZIDo48o
5WFzk0H27p,The Tournesol dataset: Which videos should be more largely recommended?,"This paper introduces the Tournesol public dataset, which was collected as part of the online deployed platform https://tournesol.app. Our dataset contains a list of 200,000 comparative judgments made by Tournesol’s 20,000 users on which YouTube videos should be more largely recommended. It also provides 600,000 comparisons along secondary criteria like content reliability, topic importance and layman-friendliness. The dataset also exports information about users’ pretrust statuses and vouches. It is published at https://api.tournesol.app/exports/all under ODC-By license. The data is currently used by Tournesol to make community-driven video content recommendations to over 10,000 users.",Datasets & Benchmarks,NeurIPS,2024,Reject,Lê-Nguyên Hoang;Romain Beylerian;Julien Fageot;Louis Faucon;Aidan Jungo;Adrien Matissart;Nathaël Noguès,True,https://openreview.net/pdf?id=5WFzk0H27p
67N3FWoDtU,Chronicling Germany: An Annotated Historical Newspaper Dataset,"The correct detection of article layout in historical newspaper pages remains challenging
but is important for Natural Language Processing ( NLP) and machine
learning applications in the field of digital history. Digital newspaper portals
typically provide Optical Character Recognition ( OCR) text, albeit of varying quality.
Unfortunately, layout information is often missing, limiting this rich source’s
scope. Our dataset is designed to address this issue for historic German-language
newspapers. The Chronicling Germany dataset contains 581 annotated historical
newspaper pages from the time period between 1852 and 1924. Historic domain
experts have spent more than 1,500 hours annotating the dataset. The paper presents
a processing pipeline and establishes baseline results on in- and out-of-domain test
data using this pipeline. Both our dataset and the corresponding baseline code are
freely available online. This work creates a starting point for future research in
the field of digital history and historic German language newspaper processing.
Furthermore, it provides the opportunity to study a low-resource task in computer
vision.",Datasets & Benchmarks,NeurIPS,2024,Reject,Christian Schultze;Niklas Kerkfeld;Kara Kuebart;Princilia Weber;Moritz Wolter;Felix Selgert,True,https://openreview.net/pdf?id=67N3FWoDtU
IZtX4RNBeH,How Good is my Video LMM? Complex Video Reasoning and Robustness Evaluation Suite for Video-LMMs,"Recent advancements in Large Language Models (LLMs) have led to the development of Video Large Multi-modal Models (Video-LMMs) that can handle a wide range of video understanding tasks. These models have the potential to be deployed in real-world applications such as robotics, AI assistants, medical surgery, and autonomous vehicles. The widespread adoption of Video-LMMs in our daily lives underscores the importance of ensuring and evaluating their robust performance in mirroring human-like reasoning and interaction capabilities in complex, real-world contexts. However, existing benchmarks for Video-LMMs primarily focus on general video comprehension abilities and neglect assessing their reasoning capabilities over complex videos in the real-world context, and robustness of these models through the lens of user prompts as text queries. In this paper, we present the Complex Video Reasoning and Robustness Evaluation Suite (CVRR-ES), a novel benchmark that comprehensively assesses the performance of Video-LMMs across 11 diverse real-world video dimensions. We evaluate 11 recent models, including both open-source and closed-source variants, and find that most of the Video-LMMs, especially open-source ones, struggle with robustness and reasoning when dealing with complex videos. Based on our analysis, we develop a training-free Dual-Step Contextual Prompting (DSCP) technique to effectively enhance the performance of existing Video-LMMs on CVRR-ES benchmark. Our findings provide valuable insights for building the next generation of human-centric AI systems with advanced robustness and reasoning capabilities.
Our dataset and code are publicly available at: https://mbzuai-oryx.github.io/CVRR-Evaluation-Suite/.",Datasets & Benchmarks,NeurIPS,2024,Reject,Muhammad Uzair Khattak;Muhammad Ferjad Naeem;Jameel Hassan Abdul Samadh;Muzammal Naseer;Federico Tombari;Fahad Khan;Salman Khan,True,https://openreview.net/pdf?id=IZtX4RNBeH
L5aY1mWvXQ,Rethinking Evaluation Strategy for Temporal Link Prediction through Counterfactual Analysis,"In response to critiques of existing evaluation methods for Temporal Link Prediction (TLP) models, we propose a novel approach to verify if these models truly capture temporal patterns in the data. Our method involves a sanity check formulated as a counterfactual question: ``What if a TLP model is tested on a temporally distorted version of the data instead of the real data?'' Ideally, a TLP model that effectively learns temporal patterns should perform worse on temporally distorted data compared to real data. We provide an in-depth analysis of this hypothesis and introduce two data distortion techniques to assess well-known TLP models.
Our contributions are threefold: (1) We introduce simple techniques to distort temporal patterns within a graph, generating temporally distorted test splits of well-known datasets for sanity checks. These distortion methods are applicable to any temporal graph dataset. (2) We perform counterfactual analysis on TLP models such as JODIE, TGAT, TGN, and CAWN to evaluate their capability in capturing temporal patterns across different datasets. (3) We propose an alternative evaluation strategy for TLP, addressing the limitations of binary classification and ranking methods, and introduce two metrics -- average time difference (ATD) and average count difference (ACD) -- to provide a comprehensive measure of a model's predictive performance. The code and datasets are available at: https://github.com/Aniq55/TLPCF.git",Datasets & Benchmarks,NeurIPS,2024,Reject,Aniq Ur Rahman;Alexander Modell;Justin Coon,False,https://openreview.net/pdf?id=L5aY1mWvXQ
WVQ4Clw1VD,MedTrinity-25M: A Large-scale Multimodal Dataset with Multigranular Annotations for Medicine,"This paper introduces MedTrinity-25M, a comprehensive, large-scale multimodal dataset for medicine, covering over 25 million images across 10 modalities, with multigranular annotations for more than 65 diseases. These enriched annotations encompass both global textual information, such as disease/lesion type, modality, region-specific descriptions, and inter-regional relationships, as well as detailed local annotations for regions of interest (ROIs), including bounding boxes, segmentation masks. Unlike existing approach which is limited by the availability of image-text pairs, we have developed the first automated pipeline that scales up multimodal data by generating multigranular visual and texual annotations (in the form of image-ROI-description triplets) without the need for any paired text descriptions. Specifically, data from over 90 different sources have been collected, preprocessed, and grounded using domain-specific expert models to identify ROIs related to abnormal regions. 
We then build a comprehensive knowledge base and prompt multimodal large language models to perform retrieval-augmented generation with the identified ROIs as guidance, resulting in multigranular texual descriptions. 
Compared to existing datasets, MedTrinity-25M provides the most enriched annotations, supporting a comprehensive range of multimodal tasks such as captioning and report generation, as well as vision-centric tasks like classification and segmentation. This dataset can be utilized to support large-scale pre-training of multimodal medical AI models, contributing to the development of future foundation models in the medical domain. The dataset is publicly available at https://yunfeixie233.github.io/MedTrinity-25M/.",Datasets & Benchmarks,NeurIPS,2024,Reject,Yunfei Xie;Ce Zhou;Lang Gao;Juncheng Wu;Xianhang Li;Hong-Yu Zhou;Sheng Liu;Lei Xing;James Zou;Cihang Xie;Yuyin Zhou,True,https://openreview.net/pdf?id=WVQ4Clw1VD
ZDvXY56DeP,Open RL Benchmark: Comprehensive Tracked Experiments for Reinforcement Learning,"In many Reinforcement Learning (RL) papers, learning curves are useful indicators to measure the effectiveness of RL algorithms. However, the complete raw data of the learning curves are rarely available. As a result, it is usually necessary to reproduce the experiments from scratch, which can be time-consuming and error-prone. We present Open RL Benchmark (ORLB), a set of fully tracked RL experiments, including not only the usual data such as episodic return, but also all algorithm-specific and system metrics. ORLB is community-driven: anyone can download, use, and contribute to the data. At the time of writing, more than 25,000 runs have been tracked, for a cumulative duration of more than 8 years. It covers a wide range of RL libraries and reference implementations. Special care is taken to ensure that each experiment is precisely reproducible by providing not only the full parameters, but also the versions of the dependencies used to generate it. In addition, ORLB comes with a command-line interface (CLI) for easy fetching and generating figures to present the results. In this document, we include two case studies to demonstrate the usefulness of ORLB in practice. To the best of our knowledge, ORLB is the first RL benchmark of its kind, and the authors hope that it will improve and facilitate the work of researchers in the field.",Datasets & Benchmarks,NeurIPS,2024,Reject,Shengyi Huang;Quentin Gallouédec;Florian Felten;Antonin Raffin;Rousslan Fernand Julien Dossa;Yanxiao Zhao;Ryan Sullivan;Viktor Makoviychuk;Denys Makoviichuk;Mohamad Hosein Danesh;Cyril Roumegous;Jiayi Weng;Chufan Chen;Md Masudur Rahman;João Guilherme Madeira Araújo;Guorui Quan;Daniel Chee Hian Tan;Timo Klein;Rujikorn Charakorn;Mark Towers;Yann Berthelot;Kinal Mehta;Dipam Chakraborty;Arjun KG;Valentin Charraut;Chang Ye;Zichen Liu;Lucas Nunes Alegre;Alexander Nikulin;Xiao Hu;Tianlin Liu;Jongwook Choi;Brent Yi,True,https://openreview.net/pdf?id=ZDvXY56DeP
aTXhTD44nF,USDC: A Dataset of $\\\\underline{U}$ser $\\\\underline{S}$tance and $\\\\underline{D}$ogmatism in Long $\\\\underline{C}$onversations,"Identifying user's opinions and stances in long conversation threads on various topics can be extremely critical for enhanced personalization, market research, political campaigns, customer service, conflict resolution, targeted advertising and content moderation. Hence, training language models to automate this task is critical. However, to train such models, gathering manual annotations has multiple challenges: 1) it is both time-consuming and costly; 2) conversation threads could be very long increasing chances of noisy annotations; and 3) interpreting instances where a user changes their opinion within a conversation is difficult because often times such transitions are subtle and not expressed explicitly. Inspired by the recent success of large language models (LLMs) for complex natural language processing (NLP) tasks, we leverage Mistral Large and GPT-4 for automating the human annotation process on the following two tasks while also providing reasoning: i) user Stance detection, which involves labeling a user's stance of a post in a conversation on a five-point scale; ii) user Dogmatism detection, which deals with labeling a user's overall opinion in the conversation on a four-point scale. Majority voting on zero-shot, one-shot, few-shot annotations from these two LLMs on 764 multi-user Reddit conversations helps us curate the USDC dataset. USDC is then used to finetune and instruction-tune multiple deployable small language models for the 5-class stance and 4-class dogmatism classification tasks. We make the code and dataset publicly available [https://anonymous.4open.science/r/USDC-0F7F].",Datasets & Benchmarks,NeurIPS,2024,Reject,mounika marreddy;SUBBA REDDY OOTA;Venkata Charan Chinni;Manish Gupta;Lucie Flek,True,https://openreview.net/pdf?id=aTXhTD44nF
aiGN4UnNM7,TSTTC: A Large-Scale Dataset for Time-to-Contact Estimation in Driving Scenarios,"Time-to-Contact (TTC) estimation is a critical task for assessing collision risk and is widely used in various driver assistance and autonomous driving systems. The past few decades have witnessed development of related theories and algorithms. The prevalent learning-based methods call for a large-scale TTC dataset in real-world scenarios. In this work, we present a large-scale object oriented TTC dataset in the driving scene for promoting the TTC estimation by a monocular camera. To collect valuable samples and make data with different TTC values relatively balanced, we go through thousands of hours of driving data and select over 200K sequences with a preset data distribution. To augment the quantity of small TTC cases, we also generate clips using the latest Neural rendering methods. Additionally, we provide several simple yet effective TTC estimation baselines and evaluate them extensively on the proposed dataset to demonstrate their effectiveness. The proposed dataset and code are publicly available at \\\\href{https://pan.baidu.com/s/14oHBEDytYZh6C3ah38a8zw?pwd=rdh3#list/path=%2F&parentPath=%2Fsharelink1829140692-712578186815897}{dataset link} and \\\\href{https://github.com/tusen-ai/TSTTC}{code link}.",Datasets & Benchmarks,NeurIPS,2024,Reject,Yuheng Shi;Zehao Huang;Yan Yan;Naiyan Wang;Xiaojie Guo,True,https://openreview.net/pdf?id=aiGN4UnNM7
bAaM8cKoMl,MindSet: Vision. A toolbox for testing DNNs on key psychological experiments,"Multiple benchmarks have been developed to assess the alignment between deep neural networks (DNNs) and human vision. In almost all cases these benchmarks are observational in the sense they are composed of behavioural and brain responses to naturalistic images that have not been manipulated to test hypotheses regarding how DNNs or humans perceive and identify objects. Here we introduce the toolbox MindSet: Vision, consisting of a collection of image datasets and related scripts designed to test DNNs on 30 psychological findings. In all experimental conditions, the stimuli are systematically manipulated to test specific hypotheses regarding human visual perception and object recognition. In addition to providing pre-generated datasets of images, we provide code to regenerate these datasets, offering many configurable parameters which greatly extend the dataset versatility for different research contexts, and code to facilitate the testing of DNNs on these image datasets using three different methods (similarity judgments, out-of-distribution classification, and decoder method), accessible at https://github.com/ValerioB88/mindset-vision. We test ResNet-152 on each of these methods as an example of how the toolbox can be used.",Datasets & Benchmarks,NeurIPS,2024,Reject,Valerio Biscione;Dong Yin;Gaurav Malhotra;Marin Dujmovic;Milton L. Montero;Guillermo Puebla;Federico Adolfi;Rachel F Heaton;John E. Hummel;Benjamin D. Evans;Karim G. Habashy;Jeffrey Bowers,True,https://openreview.net/pdf?id=bAaM8cKoMl
d1Pup4gkWf,SMPLOlympics: Sports Environments for Physically Simulated Humanoids,"We present SMPLOlympics, a collection of physically simulated environments that allow humanoids to compete in a variety of Olympic sports. Sports simulation offers a rich and standardized testing ground for evaluating and improving the capabilities of learning algorithms due to the diversity and physically demanding nature of athletic activities. As humans have been competing in these sports for many years, there is also a plethora of existing knowledge on the preferred strategy to achieve better performance. To leverage these existing human demonstrations from videos and motion capture, we design our humanoid to be compatible with the widely-used SMPL and SMPL-X human models from the vision and graphics community. We provide a suite of individual sports environments, including golf, javelin throw, high jump, long jump, and hurdling, as well as competitive sports, including both 1v1 and 2v2 games such as table tennis, tennis, fencing, boxing, soccer, and basketball. Our analysis shows that combining strong motion priors with simple rewards can result in human-like behavior in various sports. By providing a unified sports benchmark and baseline implementation of state and reward designs, we hope that SMPLOlympics can help the control and animation communities achieve human-like and performant behaviors.",Datasets & Benchmarks,NeurIPS,2024,Reject,Zhengyi Luo;Jiashun Wang;Kangni Liu;Haotian Zhang;Chen Tessler;Jingbo Wang;Ye Yuan;Jinkun Cao;Zihui Lin;Fengyi Wang;Jessica K. Hodgins;Kris M. Kitani,True,https://openreview.net/pdf?id=d1Pup4gkWf
gg3POFjqq8,Benchmarking Vision Models Under Generative Continuous Nuisance Shifts,"One important challenge in evaluating the robustness of vision models is controlling individual nuisance factors independently.
While some simple synthetic corruptions are commonly applied to existing models, they do not fully capture all realistic and relevant distribution shifts of real-world images.
To overcome this challenge, we apply LoRA adapters to diffusion models to realize a wide range of individual nuisance shifts in a continuous manner. 
While existing generative benchmarks perform manipulations in one step, we argue for gradual and continuous nuisance shifts, as they allow evaluating the sensitivity and failure points of vision models.
With this in mind, we perform a comprehensive large-scale study to evaluate the robustness and generalization of various classifiers under various nuisance shifts. Through carefully-designed comparisons and analysis, we reveal multiple valuable observations: 1) More modern and larger architectures trained on larger datasets tend to be more robust to various nuisance shifts and fail later for larger scales. 
2) Pre-training strategy influences the robustness and fine-tuning a CLIP classifier improves the standard accuracy but deteriorates the robustness.
3) The accuracy drops only account for one dimension of robustness and the failure point analysis should be considered as an additional dimension for robustness evaluation.
We hope our continuous nuisance shift benchmark can provide a new perspective on assessing the robustness of vision models.",Datasets & Benchmarks,NeurIPS,2024,Reject,Olaf Dünkel;Artur Jesslen;Jiahao Xie;Christian Theobalt;Christian Rupprecht;Adam Kortylewski,True,https://openreview.net/pdf?id=gg3POFjqq8
h8LuywKj6N,GUI-World: A Dataset for GUI-Orientated Multimodal Large Language Models,"Recently, Multimodal Large Language Models (MLLMs) have been used as agents to control keyboard and mouse inputs by directly perceiving the Graphical User Interface (GUI) and generating corresponding code.
However, current agents primarily exhibit excellent understanding capabilities in static environments and are predominantly applied in relatively simple domains, such as Web or mobile interfaces. We argue that a robust GUI agent should be capable of perceiving temporal information on the GUI, including dynamic Web content and multi-step tasks. Additionally, it should possess a comprehensive understanding across various GUI scenarios, including desktop software and multi-window interactions.
To this end, this paper introduces a new dataset, termed GUI-World, which features meticulously crafted Human-MLLM annotations, extensively covering six GUI scenarios and eight types of GUI-orientated questions in three formats.
We evaluate the capabilities of current state-of-the-art MLLMs, including ImageLLMs and VideoLLMs, in understanding various types of GUI content, especially dynamic and sequential content. Our findings reveal that ImageLLMs struggle with dynamic GUI content without manually annotated keyframes or operation history. On the other hand, VideoLLMs fall short in all GUI-orientated tasks given the sparse of GUI video dataset. Based on GUI-World, we take the initial step of leveraging a fine-tuned VideoLLM as a GUI agent, demonstrating an improved understanding of various GUI tasks. However, due to the limitations in the performance of base LLMs, we conclude that using VideoLLMs as GUI agents remains a significant challenge. We believe our work provides valuable insights for future research in dynamic GUI content understanding.",Datasets & Benchmarks,NeurIPS,2024,Reject,Dongping Chen;Yue Huang;Siyuan Wu;Jingyu Tang;Huichi Zhou;Qihui Zhang;Zhigang He;Yilin Bai;Chujie Gao;Liuyi Chen;Yiqiang Li;Chenlong Wang;Yue Yu;Tianshuo Zhou;Zhen Li;Yi Gui;Yao Wan;Pan Zhou;Jianfeng Gao;Lichao Sun,True,https://openreview.net/pdf?id=h8LuywKj6N
iTUlYblV0K,MQuAKE-Remastered: Multi-Hop Knowledge Editing Can Only Be Advanced With Reliable Evaluations,"Large language models (LLMs) can give out erroneous answers to factually rooted questions either as a result of undesired training outcomes or simply because the world has moved on after a certain knowledge cutoff date. Under such scenarios, *knowledge editing* often comes to the rescue by delivering efficient patches for such erroneous answers without significantly altering the rests, where many editing methods have seen reasonable success when the editing targets are simple and direct (e.g., *""what club does Lionel Messi currently play for?""*).

However, knowledge fragments like this are often deeply intertwined in the real world, making effectively propagating the editing effect to non-directly related questions a practical challenge (e.g., *""who is the offspring of the owner of the club that Messi currently plays for?""*). Prior arts have coined this task as *multi-hop knowledge editing* with the most popular dataset being MQuAKE, serving as the sole evaluation benchmark for many later proposed editing methods due to the expensive nature of making knowledge editing datasets at scale. 

In this work, we reveal that **up to 33\\\\% or 76\\\\% of MQuAKE's questions and ground truth labels are, in fact, corrupted in various fashions due to some unintentional clerical or procedural oversights**. Our work provides a detailed audit of MQuAKE's error pattern and a comprehensive fix without sacrificing its dataset capacity. Additionally, we benchmarked almost all proposed \\\\mquake{}-evaluated editing methods on our post-fix dataset, \\\\mquaker{}. It is our observation that many methods try to overfit the original \\\\mquake{} by exploiting some data-specific properties of \\\\mquake{}. We provide a guideline on how to faithfully approach such datasets and show that a simple, minimally invasive approach can bring excellent editing performance without such exploitation. Please refer to [`https://github.com/henryzhongsc/MQuAKE-Remastered`](https://github.com/henryzhongsc/MQuAKE-Remastered) and supplemental material for assets.",Datasets & Benchmarks,NeurIPS,2024,Reject,Shaochen Zhong;Yifan Lu;Lize Shao;Bhargav Bhushanam;Xiaocong Du;Louis Feng;Yixin Wan;Yucheng Shi;Daochen Zha;Yiwei Wang;Ninghao Liu;Kaixiong Zhou;Shuai Xu;Vipin Chaudhary;Xia Hu,True,https://openreview.net/pdf?id=iTUlYblV0K
iwC19lVBoq,AVSET-10M: An Open Large-Scale Audio-Visual Dataset with High Correspondence,"Groundbreaking research from initiatives such as ChatGPT and Sora underscores the crucial role of large-scale data in advancing generative and comprehension tasks. However, the scarcity of comprehensive and large-scale audio-visual correspondence datasets poses a significant challenge to research in the audio-visual fields. To address this gap, we introduce **AVSET-10M**, a audio-visual high-corresponding dataset comprising 10 million samples, featuring the following key attributes: (1) **High Audio-Visual Correspondence**: Through meticulous sample filtering, we ensure robust correspondence between the audio and visual components of each entry. (2) **Comprehensive Categories**: Encompassing 527 unique audio categories, AVSET-10M offers the most extensive range of audio categories available. (3) **Large Scale**: With 10 million samples, AVSET-10M is the largest publicly available audio-visual corresponding dataset. We have benchmarked two critical tasks on AVSET-10M: audio-video retrieval and vision-queried sound separation. These tasks highlight the essential role of precise audio-visual correspondence in advancing audio-visual research. For more information, please visit https://avset-10m.github.io/.",Datasets & Benchmarks,NeurIPS,2024,Reject,Xize Cheng;Ziang Zhang;Zehan Wang;Minghui Fang;Rongjie Huang;Siqi Zheng;Ruofan Hu;Bai Jionghao;Tao Jin;Zhou Zhao,True,https://openreview.net/pdf?id=iwC19lVBoq
l985bXCatk,LRVS-Fashion: Extending Visual Search with Referring Instructions,"This paper introduces a new challenge for image similarity search in the context of fashion, addressing the inherent ambiguity in this domain stemming from complex images. We present Referred Visual Search (RVS), a task allowing users to define more precisely the desired similarity, following recent interest in the industry. We release a new large public dataset, LRVS-Fashion, consisting of 272k fashion products with 842k images extracted from fashion catalogs, designed explicitly for this task. However, unlike traditional visual search methods in the industry, we demonstrate that superior performance can be achieved by bypassing explicit object detection and adopting weakly-supervised conditional contrastive learning on image tuples. Our method is lightweight and demonstrates robustness, reaching Recall at one superior to strong detection-based baselines against 2M distractors.",Datasets & Benchmarks,NeurIPS,2024,Reject,Simon Lepage;Jeremie Mary;David Picard,True,https://openreview.net/pdf?id=l985bXCatk
lnnNPiZtzR,FungiTastic: A multi-modal dataset and benchmark for image categorization,"We introduce a new, highly challenging benchmark and a dataset -- FungiTastic -- based on data continuously collected over a twenty-year span.
The dataset originates in fungal records labeled and curated by experts. It consists of about 350k multi-modal observations that include more than 650k photographs from 5k fine-grained categories and diverse 
accompanying information, e.g., acquisition metadata, satellite images, and body part segmentation. 
FungiTastic is the only benchmark that includes a test set with partially DNA-sequenced ground truth of unprecedented label reliability.
The benchmark is designed to support 
(i) standard close-set classification, 
(ii) open-set classification,
(iii) multi-modal classification,
(iv) few-shot learning, 
(v) domain shift, and many more.
We provide baseline methods tailored for almost all the use-cases.
We provide a multitude of ready-to-use pre-trained models on HuggingFace and a framework for model training.
A comprehensive documentation describing the dataset features and the baselines are available at \\\\href{https://sulc.github.io/DanishFungi2024/}{GitHub} and Kaggle.",Datasets & Benchmarks,NeurIPS,2024,Reject,Lukas Picek;Klara Janouskova;Milan Šulc;Jiri Matas,True,https://openreview.net/pdf?id=lnnNPiZtzR
mDRmX8IlBI,MMWorld: Towards Multi-discipline Multi-faceted World Model Evaluation in Videos,"Multimodal Language Language Models (MLLMs) demonstrate the emerging abilities of ""world models""---interpreting and reasoning about complex real-world dynamics. 
To assess these abilities, we posit videos are the ideal medium, as they encapsulate rich representations of real-world dynamics and causalities.
To this end, we introduce MMWorld, a new benchmark for multi-discipline, multi-faceted multimodal video understanding.
MMWorld distinguishes itself from previous video understanding benchmarks with two unique advantages: (1) multi-discipline, covering various disciplines that often require domain expertise for comprehensive understanding; (2) multi-faceted reasoning, including explanation, counterfactual thinking, future prediction, etc.
MMWorld consists of a human-annotated dataset to evaluate MLLMs with questions about the whole videos and a synthetic dataset to analyze MLLMs within a single modality of perception. 
Together, MMWorld encompasses 1,910 videos across seven broad disciplines and 69 subdisciplines, complete with 6,627 question-answer pairs and associated captions. 
The evaluation includes 2 proprietary and 10 open-source MLLMs, which struggle on MMWorld (e.g., GPT-4V performs the best with only 52.3\\\\% accuracy), showing large room for improvement. Further ablation studies reveal other interesting findings such as models' different skill sets from humans. We hope MMWorld can serve as an essential step towards world model evaluation in videos.",Datasets & Benchmarks,NeurIPS,2024,Reject,Xuehai He;Weixi Feng;Kaizhi Zheng;Yujie Lu;Wanrong Zhu;Jiachen Li;Yue Fan;Jianfeng Wang;Linjie Li;Zhengyuan Yang;Kevin Lin;William Yang Wang;Lijuan Wang;Xin Eric Wang,True,https://openreview.net/pdf?id=mDRmX8IlBI
mEJgnZZyfv,OpenMixup: Open Mixup Toolbox and Benchmark for Visual Representation Learning,"Mixup augmentation has emerged as a powerful technique for improving the generalization ability of deep neural networks. However, the lack of standardized implementations and benchmarks has hindered progress, resulting in poor reproducibility, unfair comparisons, and conflicting insights. In this paper, we introduce OpenMixup, the \\\\textit{first} mixup augmentation benchmark for visual representation learning, where 18 representative mixup baselines are trained \\\\textit{from scratch} and systematically evaluated on 11 image datasets across varying scales and granularity, spanning fine-grained scenarios to complex non-iconic scenes. We also open-source a modular codebase for streamlined mixup method design, training, and evaluations, which comprises a collection of widely-used vision backbones, optimization policies, and analysis toolkits. Notably, the codebase not only underpins all our benchmarking but supports broader mixup applications beyond classification, such as self-supervised learning and regression tasks. Through extensive experiments, we present insights on performance-complexity trade-offs and identify preferred mixup strategies for different needs. To the best of our knowledge, OpenMixup has contributed to a number of studies in the mixup community. We hope this work can further advance reproducible mixup research and fair comparisons, thereby laying a solid foundation for future progress. The source code is publicly available at \\\\url{https://github.com/Westlake-AI/openmixup}.",Datasets & Benchmarks,NeurIPS,2024,Reject,Siyuan Li;Zedong Wang;Zicheng Liu;Di Wu;Cheng Tan;Weiyang Jin;Stan Z. Li,False,https://openreview.net/pdf?id=mEJgnZZyfv
moMoWj7jLm,FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning,"The application of formulas is a fundamental ability of humans when addressing numerical reasoning problems. However, existing numerical reasoning datasets seldom explicitly indicate the formulas employed during the reasoning steps. To bridge this gap, we propose a dataset for formula-based numerical reasoning called FormulaReasoning, which consists of 5,420 questions. We further conduct evaluations on LLMs with size ranging from 7B to over 100B parameters utilizing zero-shot and few-shot chain-of-thoughts methods and we explored the approach of using retrieval-augmented LLMs when providing an external formula database. We divide the reasoning process into formula generation, parameter extraction, and calculation, and use the data augmentation method to enhance the model ability of the model with parameters count less than 7B. Our empirical findings underscore the significant potential for improvement in existing models when applied to our complex, formula-driven FormulaReasoning.",Datasets & Benchmarks,NeurIPS,2024,Reject,Xiao Li;Bolin Zhu;Sichen Liu;Yin Zhu;Yiwei liu;Gong Cheng,True,https://openreview.net/pdf?id=moMoWj7jLm
r8PnfcWQol,${EFO}_k$-CQA: Towards Knowledge Graph Complex Query Answering beyond Set Operation,"To answer complex queries on knowledge graphs, logical reasoning over incomplete knowledge needs learning-based methods because they are capable of generalizing over unobserved knowledge. Therefore, an appropriate dataset is fundamental to both obtaining and evaluating such methods under this paradigm. In this paper, we propose a comprehensive framework for data generation, model training, and method evaluation that covers the combinatorial space of Existential First-order Queries with multiple variables ($\\\\text{EFO}_k$). The combinatorial query space in our framework significantly extends those defined by set operations in the existing literature. Additionally, we construct a dataset, $\\\\text{EFO}_k$-CQA, with 741 query types for empirical evaluation, and our benchmark results provide new insights into how query hardness affects the results. Furthermore, we demonstrate that the existing dataset construction process is systematically biased and hinders the appropriate development of query-answering methods, highlighting the importance of our work. Our code and data are provided in~\\\\url{https://anonymous.4open.science/r/EFOK-CQA/README.md}.",Datasets & Benchmarks,NeurIPS,2024,Reject,Hang Yin;Zihao Wang;Weizhi Fei;Yangqiu Song,True,https://openreview.net/pdf?id=r8PnfcWQol
rdv2Fr6JTC,Precedence-Constrained Winter Value for Effective Graph Data Valuation,"Data valuation is essential for quantifying data’s worth, aiding in assessing data quality and determining fair compensation. While existing data valuation methods have proven effective in evaluating the value of Euclidean data, they face limitations when applied to the increasingly popular graph-structured data. Particularly, graph data valuation introduces unique challenges, primarily stemming from the intricate dependencies among nodes and the exponential growth in value estimation costs. To address the challenging problem of graph data valuation, we put forth an innovative solution, Precedence-Constrained Winter (PC-Winter) Value, to account for the complex graph structure. Furthermore, we develop a variety of strategies to address the computational challenges and enable efficient approximation of PC-Winter. Extensive experiments demonstrate the effectiveness of PC-Winter across diverse datasets and tasks.",Datasets & Benchmarks,NeurIPS,2024,Reject,Hongliang Chi;Wei Jin;Charu C. Aggarwal;Yao Ma,False,https://openreview.net/pdf?id=rdv2Fr6JTC
tWvVtOW0qg,Data Measurements for Decentralized Data Markets,"Decentralized data markets can provide more equitable forms of data acquisition for machine learning.
    However, to realize practical marketplaces, efficient techniques for seller selection need to be developed. 
    We propose and benchmark federated data measurements to allow a data buyer to find sellers with relevant and diverse datasets.
    Diversity and relevance measures enable a buyer to make relative comparisons between sellers without requiring intermediate brokers and training task-dependent models.",Datasets & Benchmarks,NeurIPS,2024,Reject,Charles Lu;Mohammad Mohammadi Amiri;Ramesh Raskar,False,https://openreview.net/pdf?id=tWvVtOW0qg
2myGfVgfva,MiraData: A Large-Scale Video Dataset with Long Durations and Structured Captions,"Sora's high-motion intensity and long consistent videos have significantly impacted the field of video generation, attracting unprecedented attention. However, existing publicly available datasets are inadequate for generating Sora-like videos, as they mainly contain short videos with low motion intensity and brief captions. To address these issues, we propose MiraData, a high-quality video dataset that surpasses previous ones in video duration, caption detail, motion strength, and visual quality. We curate MiraData from diverse, manually selected sources and meticulously process the data to obtain semantically consistent clips. GPT-4V is employed to annotate structured captions, providing detailed descriptions from four different perspectives along with a summarized dense caption. To better assess temporal consistency and motion intensity in video generation, we introduce MiraBench, which enhances existing benchmarks by adding 3D consistency and tracking-based motion strength metrics. MiraBench includes 150 evaluation prompts and 17 metrics covering temporal consistency, motion strength, 3D consistency, visual quality, text-video alignment, and distribution similarity. To demonstrate the utility and effectiveness of MiraData, we conduct experiments using our DiT-based video generation model, MiraDiT. The experimental results on MiraBench demonstrate the superiority of MiraData, especially in motion strength.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xuan Ju;Yiming Gao;Zhaoyang Zhang;Ziyang Yuan;Xintao Wang;Ailing Zeng;Yu Xiong;Qiang Xu;Ying Shan,True,https://openreview.net/pdf?id=2myGfVgfva
4S8agvKjle,AgentBoard: An Analytical Evaluation Board of Multi-turn LLM Agents,"Evaluating large language models (LLMs) as general-purpose agents is essential for understanding their capabilities and facilitating their integration into practical applications. However, the evaluation process presents substantial challenges. A primary obstacle is the benchmarking of agent performance across diverse scenarios within a unified framework, especially in maintaining partially-observable environments and ensuring multi-round interactions. Moreover, current evaluation frameworks mostly focus on the final success rate, revealing few insights during the process and failing to provide a deep understanding of the model abilities. To address these challenges, we introduce AgentBoard, a pioneering comprehensive benchmark and accompanied open-source evaluation framework tailored to analytical evaluation of LLM agents. AgentBoard offers a fine-grained progress rate metric that captures incremental advancements as well as a comprehensive evaluation toolkit that features easy assessment of agents for multi-faceted analysis through interactive visualization. This not only sheds light on the capabilities and limitations of LLM agents but also propels the interpretability of their performance to the forefront. Ultimately, AgentBoard serves as a significant step towards demystifying agent behaviors and accelerating the development of stronger LLM agents.",Datasets & Benchmarks,NeurIPS,2024,Oral,Chang Ma;Junlei Zhang;Zhihao Zhu;Cheng Yang;Yujiu Yang;Yaohui Jin;Zhenzhong Lan;Lingpeng Kong;Junxian He,True,https://openreview.net/pdf?id=4S8agvKjle
5c1hh8AeHv,MultiTrust: A Comprehensive Benchmark Towards Trustworthy Multimodal Large Language Models,"Despite the superior capabilities of Multimodal Large Language Models (MLLMs) across diverse tasks, they still face significant trustworthiness challenges. Yet, current literature on the assessment of trustworthy MLLMs remains limited, lacking a holistic evaluation to offer thorough insights into future improvements. In this work, we establish **MultiTrust**, the first comprehensive and unified benchmark on the trustworthiness of MLLMs across five primary aspects: *truthfulness*, *safety*, *robustness*, *fairness*, and *privacy*. Our benchmark employs a rigorous evaluation strategy that addresses both multimodal risks and cross-modal impacts, encompassing 32 diverse tasks with self-curated datasets. Extensive experiments with 21 modern MLLMs reveal some previously unexplored trustworthiness issues and risks, highlighting the complexities introduced by the multimodality and underscoring the necessity for advanced methodologies to enhance their reliability. For instance, typical proprietary models still struggle with the perception of visually confusing images and are vulnerable to multimodal jailbreaking and adversarial attacks; MLLMs are more inclined to disclose privacy in text and reveal ideological and cultural biases even when paired with irrelevant images in inference, indicating that the multimodality amplifies the internal risks from base LLMs. Additionally, we release a scalable toolbox for standardized trustworthiness research, aiming to facilitate future advancements in this important field. Code and resources are publicly available at: [https://multi-trust.github.io/](https://multi-trust.github.io/).",Datasets & Benchmarks,NeurIPS,2024,Poster,Yichi Zhang;Yao Huang;Yitong Sun;Chang Liu;Zhe Zhao;Zhengwei Fang;Yifan Wang;Huanran Chen;Xiao Yang;Xingxing Wei;Hang Su;Yinpeng Dong;Jun Zhu,True,https://openreview.net/pdf?id=5c1hh8AeHv
6cCFK69vJI,Building Timeseries Dataset: Empowering Large-Scale Building Analytics,"Buildings play a crucial role in human well-being, influencing occupant comfort, health, and safety.
Additionally, they contribute significantly to global energy consumption, accounting for one-third of total energy usage, and carbon emissions.
Optimizing building performance presents a vital opportunity to combat climate change and promote human flourishing.
However, research in building analytics has been hampered by the lack of accessible, available, and comprehensive real-world datasets on multiple building operations.
In this paper, we introduce the Building TimeSeries (BTS) dataset.
Our dataset covers three buildings over a three-year period, comprising more than ten thousand timeseries data points with hundreds of unique ontologies.
Moreover, the metadata is standardized using the Brick schema.
To demonstrate the utility of this dataset, we performed benchmarks on two tasks: timeseries ontology classification and zero-shot forecasting.
These tasks represent an essential initial step in addressing challenges related to interoperability in building analytics.
Access to the dataset and the code used for benchmarking are available here: https://github.com/cruiseresearchgroup/DIEF\\\\_BTS",Datasets & Benchmarks,NeurIPS,2024,Poster,Arian Prabowo;Xiachong LIN;Imran Razzak;Hao Xue;Emily W. Yap;Matt Amos;Flora D. Salim,True,https://openreview.net/pdf?id=6cCFK69vJI
7TCK0aBL1C,IaC-Eval: A Code Generation Benchmark for Cloud Infrastructure-as-Code Programs,"Infrastructure-as-Code (IaC), an important component of cloud computing, allows the definition of cloud infrastructure in high-level programs. However, developing IaC programs is challenging, complicated by factors that include the burgeoning complexity of the cloud ecosystem (e.g., diversity of cloud services and workloads), and the relative scarcity of IaC-specific code examples and public repositories. 
While large language models (LLMs) have shown promise in general code generation and could potentially aid in IaC development, no benchmarks currently exist for evaluating their ability to generate IaC code. We present IaC-Eval, a first step in this research direction. IaC-Eval's dataset includes 458 human-curated scenarios covering a wide range of popular AWS services, at varying difficulty levels. Each scenario mainly comprises a natural language IaC problem description and an infrastructure intent specification. The former is fed as user input to the LLM, while the latter is a general notion used to verify if the generated IaC program conforms to the user's intent; by making explicit the problem's requirements that can encompass various cloud services, resources and internal infrastructure details.  Our in-depth evaluation shows that contemporary LLMs perform poorly on IaC-Eval, with the top-performing model, GPT-4, obtaining a pass@1 accuracy of 19.36%. In contrast, it scores 86.6% on EvalPlus, a popular Python code generation benchmark, highlighting a need for advancements in this domain. We open-source the IaC-Eval dataset and evaluation framework at https://github.com/autoiac-project/iac-eval to enable future research on LLM-based IaC code generation.",Datasets & Benchmarks,NeurIPS,2024,Poster,Patrick Tser Jern Kon;Jiachen Liu;Yiming Qiu;Weijun Fan;Ting He;Lei Lin;Haoran Zhang;Owen M. Park;George Sajan Elengikal;Yuxin Kang;Ang Chen;Mosharaf Chowdhury;Myungjin Lee;Xinyu Wang,True,https://openreview.net/pdf?id=7TCK0aBL1C
RgUcvs6ssu,WelQrate: Defining the Gold Standard in Small Molecule Drug Discovery Benchmarking,"While deep learning has revolutionized computer-aided drug discovery, the AI community has predominantly focused on model innovation and placed less emphasis on establishing best benchmarking practices. 
We posit that without a sound model evaluation framework, the AI community's efforts cannot reach their full potential, thereby slowing the progress and transfer of innovation into real-world drug discovery.
Thus, in this paper, we seek to establish a new gold standard for small molecule drug discovery benchmarking, *WelQrate*. 
Specifically, our contributions are threefold: 
***WelQrate*** **dataset collection** - we introduce a meticulously curated collection of 9 datasets spanning 5 therapeutic target classes. Our hierarchical curation pipelines, designed by drug discovery experts, go beyond the primary high-throughput screen by leveraging additional confirmatory and counter screens along with rigorous domain-driven preprocessing, such as Pan-Assay Interference Compounds (PAINS) filtering, to ensure the high-quality data in the datasets; ***WelQrate*** **Evaluation Framework** - we propose a standardized model evaluation framework considering high-quality datasets, featurization, 3D conformation generation, evaluation metrics, and data splits, which provides a reliable benchmarking for drug discovery experts conducting real-world virtual screening; **Benchmarking** - 
we evaluate model performance through various research questions using the *WelQrate* dataset collection, exploring the effects of different models, dataset quality, featurization methods, and data splitting strategies on the results.
In summary, we recommend adopting our proposed *WelQrate* as the gold standard in small molecule drug discovery benchmarking. The *WelQrate* dataset collection, along with the curation codes, and experimental scripts are all publicly available at www.WelQrate.org.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yunchao Liu;Ha Dong;Xin Wang;Rocco Moretti;Yu Wang;Zhaoqian Su;Jiawei Gu;Bobby Bodenheimer;Charles Weaver;Jens Meiler;Tyler Derr,True,https://openreview.net/pdf?id=RgUcvs6ssu
9tVn4f8aJO,HEMM: Holistic Evaluation of Multimodal Foundation Models,"Multimodal foundation models that can holistically process text alongside images, video, audio, and other sensory modalities are increasingly used in a variety of real-world applications. However, it is challenging to characterize and study progress in multimodal foundation models, given the range of possible modeling decisions, tasks, and domains. In this paper, we introduce Holistic Evaluation of Multimodal Models (HEMM) to systematically evaluate the capabilities of multimodal foundation models across a set of 3 dimensions: basic skills, information flow, and real-world use cases. Basic multimodal skills are internal abilities required to solve problems, such as learning interactions across modalities, fine-grained alignment, multi-step reasoning, and the ability to handle external knowledge. Information flow studies how multimodal content changes during a task through querying, translation, editing, and fusion. Use cases span domain-specific challenges introduced in real-world multimedia, affective computing, natural sciences, healthcare, and human-computer interaction applications. Through comprehensive experiments across the 30 tasks in HEMM, we (1) identify key dataset dimensions (e.g., basic skills, information flows, and use cases) that pose challenges to today’s models, and (2) distill performance trends regarding how different modeling dimensions (e.g., scale, pre-training data, multimodal alignment, pre-training, and instruction tuning objectives) influence performance. Our conclusions regarding challenging multimodal interactions, use cases, and tasks requiring reasoning and external knowledge, the benefits of data and model scale, and the impacts of instruction-tuning yield actionable insights for future work in multimodal foundation models.",Datasets & Benchmarks,NeurIPS,2024,Poster,Paul Pu Liang;Akshay Goindani;Talha Chafekar;Leena Mathur;Haofei Yu;Russ Salakhutdinov;Louis-Philippe Morency,True,https://openreview.net/pdf?id=9tVn4f8aJO
AdpSHMOujG,Infusing Synthetic Data with Real-World Patterns for Zero-Shot Material State Segmentation,"Visual recognition of materials and their states is essential for understanding the physical world, from identifying wet regions on surfaces or stains on fabrics to detecting infected areas or minerals in rocks. Collecting data that captures this vast variability is complex due to the scattered and gradual nature of material states. Manually annotating real-world images is constrained by cost and precision, while synthetic data, although accurate and inexpensive, lacks real-world diversity. This work aims to bridge this gap by infusing patterns automatically extracted from real-world images into synthetic data. Hence, patterns collected from natural images are used to generate and map materials into synthetic scenes. This unsupervised approach captures the complexity of the real world while maintaining the precision and scalability of synthetic data. We also present the first comprehensive benchmark for zero-shot material state segmentation, utilizing real-world images across a diverse range of domains, including food, soils, construction, plants, liquids, and more, each appears in various states such as wet, dry, infected, cooked, burned, and many others. The annotation includes partial similarity between regions with similar but not identical materials and hard segmentation of only identical material states. This benchmark eluded top foundation models, exposing the limitations of existing data collection methods. Meanwhile, nets trained on the infused data performed significantly better on this and related tasks. The dataset, code, and trained model are publicly available. We also share 300,000 extracted textures and SVBRDF/PBR materials to facilitate future datasets generation.",Datasets & Benchmarks,NeurIPS,2024,Poster,Sagi Eppel;Jolina Yining Li;Manuel S. Drehwald;Alan Aspuru-Guzik,True,https://openreview.net/pdf?id=AdpSHMOujG
BZe6dmDk5K,GAIA: Rethinking Action Quality Assessment for AI-Generated Videos,"Assessing action quality is both imperative and challenging due to its significant impact on the quality of AI-generated videos, further complicated by the inherently ambiguous nature of actions within AI-generated video (AIGV). Current action quality assessment (AQA) algorithms predominantly focus on actions from real specific scenarios and are pre-trained with normative action features, thus rendering them inapplicable in AIGVs. To address these problems, we construct GAIA, a Generic AI-generated Action dataset, by conducting a large-scale subjective evaluation from a novel causal reasoning-based perspective, resulting in 971,244 ratings among 9,180 video-action pairs. Based on GAIA, we evaluate a suite of popular text-to-video (T2V) models on their ability to generate visually rational actions, revealing their pros and cons on different categories of actions. We also extend GAIA as a testbed to benchmark the AQA capacity of existing automatic evaluation methods. Results show that traditional AQA methods, action-related metrics in recent T2V benchmarks, and mainstream video quality methods perform poorly with an average SRCC of 0.454, 0.191, and 0.519, respectively, indicating a sizable gap between current models and human action perception patterns in AIGVs. Our findings underscore the significance of action quality as a unique perspective for studying AIGVs and can catalyze progress towards methods with enhanced capacities for AQA in AIGVs.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Zijian Chen;Wei Sun;Yuan Tian;Jun Jia;Zicheng Zhang;Wang Jiarui;Ru Huang;Xiongkuo Min;Guangtao Zhai;Wenjun Zhang,True,https://openreview.net/pdf?id=BZe6dmDk5K
DFr5hteojx,"The PRISM Alignment Dataset: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models","Human feedback is central to the alignment of Large Language Models (LLMs). However, open questions remain about the methods (how), domains (where), people (who) and objectives (to what end) of feedback processes. To navigate these questions, we introduce PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. With PRISM, we contribute (i) wider geographic and demographic participation in feedback; (ii) census-representative samples for two countries (UK, US); and (iii) individualised ratings that link to detailed participant profiles, permitting personalisation and attribution of sample artefacts. We target subjective and multicultural perspectives on value-laden and controversial issues, where we expect interpersonal and cross-cultural disagreement. We use PRISM in three case studies to demonstrate the need for careful consideration of which humans provide alignment data.",Datasets & Benchmarks,NeurIPS,2024,Oral,Hannah Rose Kirk;Alexander Whitefield;Paul Röttger;Andrew Michael Bean;Katerina Margatina;Rafael Mosquera;Juan Manuel Ciro;Max Bartolo;Adina Williams;He He;Bertie Vidgen;Scott A. Hale,True,https://openreview.net/pdf?id=DFr5hteojx
DJVyRhT8nP,Human-Aware Vision-and-Language Navigation: Bridging Simulation to Reality with Dynamic Human Interactions,"Vision-and-Language Navigation (VLN) aims to develop embodied agents that navigate based on human instructions. However, current VLN frameworks often rely on static environments and optimal expert supervision, limiting their real-world applicability. To address this, we introduce Human-Aware Vision-and-Language Navigation (HA-VLN), extending traditional VLN by incorporating dynamic human activities and relaxing key assumptions. We propose the Human-Aware 3D (HA3D) simulator, which combines dynamic human activities with the Matterport3D dataset, and the Human-Aware Room-to-Room (HA-R2R) dataset, extending R2R with human activity descriptions. To tackle HA-VLN challenges, we present the Expert-Supervised Cross-Modal (VLN-CM) and Non-Expert-Supervised Decision Transformer (VLN-DT) agents, utilizing cross-modal fusion and diverse training strategies for effective navigation in dynamic human environments. A comprehensive evaluation, including metrics considering human activities, and systematic analysis of HA-VLN's unique challenges, underscores the need for further research to enhance HA-VLN agents' real-world robustness and adaptability. Ultimately, this work provides benchmarks and insights for future research on embodied AI and Sim2Real transfer, paving the way for more realistic and applicable VLN systems in human-populated environments.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Heng Li;Minghan Li;Zhi-Qi Cheng;Yifei Dong;Yuxuan Zhou;Jun-Yan He;Qi Dai;Teruko Mitamura;Alexander G Hauptmann,True,https://openreview.net/pdf?id=DJVyRhT8nP
DjCSjizgsH,Sim2Real-Fire: A Multi-modal Simulation Dataset for Forecast and Backtracking of Real-world Forest Fire,"The latest research on wildfire forecast and backtracking has adopted AI models, which require a large amount of data from wildfire scenarios to capture fire spread patterns. This paper explores using cost-effective simulated wildfire scenarios to train AI models and apply them to the analysis of real-world wildfire. This solution requires AI models to minimize the Sim2Real gap, a brand-new topic in the fire spread analysis research community. To investigate the possibility of minimizing the Sim2Real gap, we collect the Sim2Real-Fire dataset that contains 1M simulated scenarios with multi-modal environmental information for training AI models. We prepare 1K real-world wildfire scenarios for testing the AI models. We also propose a deep transformer, S2R-FireTr, which excels in considering the multi-modal environmental information for forecasting and backtracking the wildfire. S2R-FireTr surpasses state-of-the-art methods in real-world wildfire scenarios.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yanzhi Li;Keqiu Li;LI GUOHUI;zumin wang;Chanqing Ji;Lubo Wang;Die Zuo;Qing Guo;Feng Zhang;Manyu Wang;Di Lin,True,https://openreview.net/pdf?id=DjCSjizgsH
E18kRXTGmV,CVQA: Culturally-diverse Multilingual Visual Question Answering Benchmark,"Visual Question Answering~(VQA) is an important task in multimodal AI, which requires models to understand and reason on knowledge present in visual and textual data. However, most of the current VQA datasets and models are primarily focused on English and a few major world languages, with images that are Western-centric. While recent efforts have tried to increase the number of languages covered on VQA datasets, they still lack diversity in low-resource languages. More importantly, some datasets extend the text to other languages, either via translation or some other approaches, but usually keep the same images, resulting in narrow cultural representation. To address these limitations, we create CVQA, a new Culturally-diverse Multilingual Visual Question Answering benchmark dataset, designed to cover a rich set of languages and regions, where we engage native speakers and cultural experts in the data collection process. CVQA includes culturally-driven images and questions from across 28 countries in four continents, covering 26 languages with 11 scripts, providing a total of 9k questions. We benchmark several Multimodal Large Language Models (MLLMs) on CVQA, and we show that the dataset is challenging for the current state-of-the-art models. This benchmark will serve as a probing evaluation suite for assessing the cultural bias of multimodal models and hopefully encourage more research efforts towards increasing cultural awareness and linguistic diversity in this field.",Datasets & Benchmarks,NeurIPS,2024,Oral,David Orlando Romero Mogrovejo;Chenyang Lyu;Haryo Akbarianto Wibowo;Santiago Góngora;Aishik Mandal;Sukannya Purkayastha;Jesus-German Ortiz-Barajas;Emilio Villa Cueva;Jinheon Baek;Soyeong Jeong;Injy Hamed;Zheng Xin Yong;Zheng Wei Lim;Paula Mónica Silva;Jocelyn Dunstan;Mélanie Jouitteau;David LE MEUR;Joan Nwatu;Ganzorig Batnasan;Munkh-Erdene Otgonbold;Munkhjargal Gochoo;Guido Ivetta;Luciana Benotti;Laura Alonso Alemany;Hernán Maina;Jiahui Geng;Tiago Timponi Torrent;Frederico Belcavello;Marcelo Viridiano;Jan Christian Blaise Cruz;Dan John Velasco;Oana Ignat;Zara Burzo;Chenxi Whitehouse;Artem Abzaliev;Teresa Clifford;Gráinne Caulfield;Teresa Lynn;Christian Salamea-Palacios;Vladimir Araujo;Yova Kementchedjhieva;Mihail Minkov Mihaylov;Israel Abebe Azime;Henok Biadglign Ademtew;Bontu Fufa Balcha;Naome A Etori;David Ifeoluwa Adelani;Rada Mihalcea;Atnafu Lambebo Tonja;Maria Camila Buitrago Cabrera;Gisela Vallejo;Holy Lovenia;Ruochen Zhang;Marcos Estecha-Garitagoitia;Mario Rodríguez-Cantelar;Toqeer Ehsan;Rendi Chevi;Muhammad Farid Adilazuarda;Ryandito Diandaru;Samuel Cahyawijaya;Fajri Koto;Tatsuki Kuribayashi;Haiyue Song;Aditya Nanda Kishore Khandavally;Thanmay Jayakumar;Raj Dabre;Mohamed Fazli Mohamed Imam;Kumaranage Ravindu Yasas Nagasinghe;Alina Dragonetti;Luis Fernando D'Haro;Olivier NIYOMUGISHA;Jay Gala;Pranjal A Chitale;Fauzan Farooqui;Thamar Solorio;Alham Fikri Aji,True,https://openreview.net/pdf?id=E18kRXTGmV
EADRzNJFn1,TGB 2.0: A Benchmark for Learning on Temporal Knowledge Graphs and Heterogeneous Graphs,"Multi-relational temporal graphs are powerful tools for modeling real-world data, capturing the evolving and interconnected nature of entities over time. Recently, many novel models are proposed for ML on such graphs intensifying the need for robust evaluation and standardized benchmark datasets. However, the availability of such resources remains scarce and evaluation faces added complexity due to reproducibility issues in experimental protocols. To address these challenges, we introduce Temporal Graph Benchmark 2.0 (TGB 2.0), a novel benchmarking framework tailored for evaluating methods for predicting future links on Temporal Knowledge Graphs and Temporal Heterogeneous Graphs with a focus on large-scale datasets, extending the Temporal Graph Benchmark. TGB 2.0 facilitates comprehensive evaluations by presenting eight novel datasets spanning five domains with up to 53 million edges. TGB 2.0 datasets are significantly larger
than existing datasets in terms of number of nodes, edges, or timestamps. In addition, TGB 2.0 provides a reproducible and realistic evaluation pipeline for multi-relational temporal graphs. Through extensive experimentation, we observe that 1) leveraging edge-type information is crucial to obtain high performance, 2) simple heuristic baselines are often competitive with more complex methods, 3) most methods fail to run on our largest datasets, highlighting the need for research on more scalable methods.",Datasets & Benchmarks,NeurIPS,2024,Poster,Julia Gastinger;Shenyang Huang;Mikhail Galkin;Erfan Loghmani;Ali Parviz;Farimah Poursafaei;Jacob Danovitch;Emanuele Rossi;Ioannis Koutis;Heiner Stuckenschmidt;Reihaneh Rabbany;Guillaume Rabusseau,True,https://openreview.net/pdf?id=EADRzNJFn1
FXTeJvHE0k,NAVSIM: Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking,"Benchmarking vision-based driving policies is challenging. On one hand, open-loop evaluation with real data is easy, but these results do not reflect closed-loop performance. On the other, closed-loop evaluation is possible in simulation, but is hard to scale due to its significant computational demands. Further, the simulators available today exhibit a large domain gap to real data. This has resulted in an inability to draw clear conclusions from the rapidly growing body of research on end-to-end autonomous driving. In this paper, we present NAVSIM, a middle ground between these evaluation paradigms, where we use large datasets in combination with a non-reactive simulator to enable large-scale real-world benchmarking. Specifically, we gather simulation-based metrics, such as progress and time to collision, by unrolling bird's eye view abstractions of the test scenes for a short simulation horizon. Our simulation is non-reactive, i.e., the evaluated policy and environment do not influence each other. As we demonstrate empirically, this decoupling allows open-loop metric computation while being better aligned with closed-loop evaluations than traditional displacement errors. NAVSIM enabled a new competition held at CVPR 2024, where 143 teams submitted 463 entries, resulting in several new insights. On a large set of challenging scenarios, we observe that simple methods with moderate compute requirements such as TransFuser can match recent large-scale end-to-end driving architectures such as UniAD. Our modular framework can potentially be extended with new datasets, data curation strategies, and metrics, and will be continually maintained to host future challenges. Our code is available at https://github.com/autonomousvision/navsim.",Datasets & Benchmarks,NeurIPS,2024,Poster,Daniel Dauner;Marcel Hallgarten;Tianyu Li;Xinshuo Weng;Zhiyu Huang;Zetong Yang;Hongyang Li;Igor Gilitschenski;Boris Ivanovic;Marco Pavone;Andreas Geiger;Kashyap Chitta,False,https://openreview.net/pdf?id=FXTeJvHE0k
GHlJM45fWY,GeoPlant: Spatial Plant Species Prediction Dataset,"The difficulty of monitoring biodiversity at fine scales and over large areas limits ecological knowledge and conservation efforts. To fill this gap, Species Distribution Models (SDMs) predict species across space from spatially explicit features. Yet, they face the challenge of integrating the rich but heterogeneous data made available over the past decade, notably millions of opportunistic species observations and standardized surveys, as well as multi-modal remote sensing data.
In light of that, we have designed and developed a new European-scale dataset for SDMs at high spatial resolution (10--50m), including more than 10k species (i.e., most of the European flora). The dataset comprises 5M heterogeneous Presence-Only records and 90k exhaustive Presence-Absence survey records, all accompanied by diverse environmental rasters (e.g., elevation, human footprint, and soil) traditionally used in SDMs. In addition, it provides Sentinel-2 RGB and NIR satellite images with 10 m resolution, a 20-year time series of climatic variables, and satellite time series from the Landsat program.
In addition to the data, we provide an openly accessible SDM benchmark (hosted on Kaggle), which has already attracted an active community and a set of strong baselines for single predictor/modality and multimodal approaches.
All resources, e.g., the dataset, pre-trained models, and baseline methods (in the form of notebooks), are available on Kaggle, allowing one to start with our dataset literally with two mouse clicks.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Lukas Picek;Christophe Botella;Maximilien Servajean;César Leblanc;Rémi Palard;Theo Larcher;Benjamin Deneu;Diego Marcos;Pierre Bonnet;Alexis Joly,True,https://openreview.net/pdf?id=GHlJM45fWY
HB5q6pC5eb,PertEval: Unveiling Real Knowledge Capacity of LLMs with Knowledge-Invariant Perturbations,"Expert-designed close-ended benchmarks are indispensable in assessing the knowledge capacity of large language models (LLMs). Despite their widespread use, concerns have mounted regarding their reliability due to limited test scenarios and an unavoidable risk of data contamination. To rectify this, we present PertEval, a toolkit devised for in-depth probing of LLMs' knowledge capacity through **knowledge-invariant perturbations**. These perturbations employ human-like restatement techniques to generate on-the-fly test samples from static benchmarks, meticulously retaining knowledge-critical content while altering irrelevant details. Our toolkit further includes a suite of **response consistency analyses** that compare performance on raw vs. perturbed test sets to precisely assess LLMs' genuine knowledge capacity. Six representative LLMs are re-evaluated using PertEval. Results reveal significantly inflated performance of the LLMs on raw benchmarks, including an absolute 25.8% overestimation for GPT-4. Additionally, through a nuanced response pattern analysis, we discover that PertEval retains LLMs' uncertainty to specious knowledge, and reveals their potential rote memorization to correct options which leads to overestimated performance. We also find that the detailed response consistency analyses by PertEval could illuminate various weaknesses in existing LLMs' knowledge mastery and guide the development of refinement. Our findings provide insights for advancing more robust and genuinely knowledgeable LLMs. Our code is available at https://github.com/aigc-apps/PertEval.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Jiatong Li;Renjun Hu;Kunzhe Huang;Yan Zhuang;Qi Liu;Mengxiao Zhu;Xing Shi;Wei Lin,False,https://openreview.net/pdf?id=HB5q6pC5eb
JU0QvhhfVp,MOTIVE: A Drug-Target Interaction Graph For Inductive Link Prediction,"Drug-target interaction (DTI) prediction is crucial for identifying new therapeutics and detecting mechanisms of action. While structure-based methods accurately model physical interactions between a drug and its protein target, cell-based assays such as Cell Painting can better capture complex DTI interactions. This paper introduces MOTIVE, a Morphological cOmpound Target Interaction Graph dataset comprising Cell Painting features for 11,000 genes and 3,600 compounds, along with their relationships extracted from seven publicly available databases. We provide random, cold-source (new drugs), and cold-target (new genes) data splits to enable rigorous evaluation under realistic use cases. Our benchmark results show that graph neural networks that use Cell Painting features consistently outperform those that learn from graph structure alone, feature-based models, and topological heuristics. MOTIVE accelerates both graph ML research and drug discovery by promoting the development of more reliable DTI prediction models. MOTIVE resources are available at https://github.com/carpenter-singh-lab/motive.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,John Arevalo;Ellen Su;Anne E Carpenter;Shantanu Singh,True,https://openreview.net/pdf?id=JU0QvhhfVp
KZlJF8kguO,Brain Treebank: Large-scale intracranial recordings from naturalistic language stimuli,"We present the Brain Treebank, a large-scale dataset of electrophysiological neural responses, recorded from intracranial probes while 10 subjects watched one or more Hollywood movies. Subjects watched on average 2.6 Hollywood movies, for an average viewing time of 4.3 hours, and a total of 43 hours. The audio track for each movie was transcribed with manual corrections. Word onsets were manually annotated on spectrograms of the audio track for each movie. Each transcript was automatically parsed and manually corrected into the universal dependencies (UD) formalism, assigning a part of speech to every word and a dependency parse to every sentence. In total, subjects heard over 38,000 sentences (223,000 words), while they had on average 168 electrodes implanted. This is the largest dataset of intracranial recordings featuring grounded naturalistic language, one of the largest English UD treebanks in general, and one of only a few UD treebanks aligned to multimodal features. We hope that this dataset serves as a bridge between linguistic concepts, perception, and their neural representations. To that end, we present an analysis of which electrodes are sensitive to language features while also mapping out a rough time course of language processing across these electrodes. The Brain Treebank is available at https://BrainTreebank.dev/",Datasets & Benchmarks,NeurIPS,2024,Oral,Christopher Wang;Adam Uri Yaari;Aaditya K Singh;Vighnesh Subramaniam;Dana Rosenfarb;Jan DeWitt;Pranav Misra;Joseph R. Madsen;Scellig Stone;Gabriel Kreiman;Boris Katz;Ignacio Cases;Andrei Barbu,True,https://openreview.net/pdf?id=KZlJF8kguO
LdRZ9SFBku,UKnow: A Unified Knowledge Protocol with Multimodal Knowledge Graph Datasets for Reasoning and Vision-Language Pre-Training,"This work presents a unified knowledge protocol, called UKnow, which facilitates knowledge-based studies from the perspective of data. Particularly focusing on visual and linguistic modalities, we categorize data knowledge into five unit types, namely, in-image, in-text, cross-image, cross-text, and image-text, and set up an efficient pipeline to help construct the multimodal knowledge graph from any data collection. Thanks to the logical information naturally contained in knowledge graph, organizing datasets under UKnow format opens up more possibilities of data usage compared to the commonly used image-text pairs. Following UKnow protocol, we collect, from public international news, a large-scale multimodal knowledge graph dataset that consists of 1,388,568 nodes (with 571,791 vision-related ones) and 3,673,817 triplets. The dataset is also annotated with rich event tags, including 11 coarse labels and 9,185 fine labels. Experiments on four benchmarks demonstrate the potential of UKnow in supporting common-sense reasoning and boosting vision-language pre-training with a single dataset, benefiting from its unified form of knowledge organization. Code, dataset, and models will be made publicly available. See Appendix to download the dataset.",Datasets & Benchmarks,NeurIPS,2024,Poster,Biao Gong;Shuai Tan;Yutong Feng;Xiaoying Xie;Yuyuan Li;Chaochao Chen;Kecheng Zheng;Yujun Shen;Deli Zhao,True,https://openreview.net/pdf?id=LdRZ9SFBku
LdxNWDNvC3,AFBench: A Large-scale Benchmark for Airfoil Design,"Data-driven generative models have emerged as promising approaches towards achieving efficient mechanical inverse design. However, due to prohibitively high cost in time and money, there is still lack of open-source and large-scale benchmarks in this field. It is mainly the case for airfoil inverse design, which requires to generate and edit diverse geometric-qualified  and aerodynamic-qualified airfoils following the multimodal instructions, \\\\emph{i.e.,} dragging points and physical parameters. This paper presents the open-source endeavors in airfoil inverse design, \\\\emph{AFBench}, including a large-scale dataset with 200 thousand airfoils and high-quality aerodynamic and geometric labels, two novel and practical airfoil inverse design tasks, \\\\emph{i.e.,} conditional generation on multimodal physical parameters, controllable editing, and comprehensive metrics to evaluate various existing airfoil inverse design methods. Our aim is to establish \\\\emph{AFBench} as an ecosystem for training and evaluating airfoil inverse design methods, with a specific focus on data-driven controllable inverse design models by multimodal instructions capable of bridging the gap between ideas and execution, the academic research and industrial applications. We have provided baseline models, comprehensive experimental observations, and analysis to accelerate future research. Our baseline model is trained on an RTX 3090 GPU within 16 hours. The codebase, datasets and benchmarks will be available at \\\\url{https://hitcslj.github.io/afbench/}.",Datasets & Benchmarks,NeurIPS,2024,Poster,Jian Liu;Jianyu Wu;Hairun Xie;Guoqing zhang;Jing Wang;Liu Wei;Wanli Ouyang;Junjun Jiang;Xianming Liu;SHIXIANG TANG;Miao Zhang,True,https://openreview.net/pdf?id=LdxNWDNvC3
M32Ldpp4Oy,LogiCity: Advancing Neuro-Symbolic AI with Abstract Urban Simulation,"Recent years have witnessed the rapid development of Neuro-Symbolic (NeSy) AI systems, which integrate symbolic reasoning into deep neural networks.
However, most of the existing benchmarks for NeSy AI fail to provide long-horizon reasoning tasks with complex multi-agent interactions.
Furthermore, they are usually constrained by fixed and simplistic logical rules over limited entities, making them far from real-world complexities.
To address these crucial gaps, we introduce LogiCity, the first simulator based on customizable first-order logic (FOL) for an urban-like environment with multiple dynamic agents.
LogiCity models diverse urban elements using semantic and spatial concepts, such as $\\\\texttt{IsAmbulance}(\\\\texttt{X})$ and $\\\\texttt{IsClose}(\\\\texttt{X}, \\\\texttt{Y})$. 
These concepts are used to define FOL rules that govern the behavior of various agents. 
Since the concepts and rules are abstractions, they can be universally applied to cities with any agent compositions, facilitating the instantiation of diverse scenarios.
Besides, a key feature of LogiCity is its support for user-configurable abstractions, enabling customizable simulation complexities for logical reasoning.
To explore various aspects of NeSy AI, LogiCity introduces two tasks, one features long-horizon sequential decision-making, and the other focuses on one-step visual reasoning, varying in difficulty and agent behaviors.
Our extensive evaluation reveals the advantage of NeSy frameworks in abstract reasoning. 
Moreover, we highlight the significant challenges of handling more complex abstractions in long-horizon multi-agent scenarios or under high-dimensional, imbalanced data.
With its flexible design, various features, and newly raised challenges, we believe LogiCity represents a pivotal step forward in advancing the next generation of NeSy AI.
All the code and data are open-sourced at our website.",Datasets & Benchmarks,NeurIPS,2024,Poster,Bowen Li;Zhaoyu Li;Qiwei Du;Jinqi Luo;Wenshan Wang;Yaqi Xie;Simon Stepputtis;Chen Wang;Katia P. Sycara;Pradeep Kumar Ravikumar;Alexander G. Gray;Xujie Si;Sebastian Scherer,True,https://openreview.net/pdf?id=M32Ldpp4Oy
MU2s9wwWLo,ConceptMix: A Compositional Image Generation Benchmark with Controllable Difficulty,"Compositionality is a critical capability in Text-to-Image (T2I) models, as it reflects their ability to understand and combine multiple concepts from text descriptions. Existing evaluations of compositional capability rely heavily on human-designed text prompts or fixed templates, limiting their diversity and complexity, and yielding low discriminative power. We propose ConceptMix, a scalable, controllable, and customizable benchmark which automatically evaluates compositional generation ability of T2I models. This is done in two stages. First, ConceptMix generates the text prompts: concretely, using categories of visual concepts (e.g., objects, colors, shapes, spatial relationships), it randomly samples an object and k-tuples of visual concepts, then uses GPT-4o to generate text prompts for image generation based on these sampled concepts. Second, ConceptMix evaluates the images generated in response to these prompts: concretely, it checks how many of the k concepts actually appeared in the image by generating one question per visual concept and using a strong VLM to answer them. Through administering ConceptMix to a diverse set of T2I models (proprietary as well as open ones) using increasing values of k, we show that our ConceptMix has higher discrimination power than earlier benchmarks. Specifically, ConceptMix reveals that the performance of several models, especially open models, drops dramatically with increased k. Importantly, it also provides insight into the lack of prompt diversity in widely-used training datasets. Additionally, we conduct extensive human studies to validate the design of ConceptMix and compare our automatic grading with human judgement. We hope it will guide future T2I model development.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xindi Wu;Dingli Yu;Yangsibo Huang;Olga Russakovsky;Sanjeev Arora,True,https://openreview.net/pdf?id=MU2s9wwWLo
Mbd3QxXjq5,OpenMathInstruct-1: A 1.8 Million Math Instruction Tuning Dataset,"Recent work has shown the immense potential of synthetically generated datasets for training large language models (LLMs), especially for acquiring targeted skills. Current large-scale math instruction tuning datasets such as MetaMathQA (Yu et al., 2024) and MAmmoTH (Yue et al., 2024) are constructed using outputs from closed-source LLMs with commercially restrictive licenses. A key reason limiting the use of open-source LLMs in these data generation pipelines has been the wide gap between the mathematical skills of the best closed-source LLMs, such as GPT-4, and the best open-source LLMs. Building on the recent progress in open-source LLMs, our proposed prompting novelty, and some brute-force scaling, we construct OpenMathInstruct-1, a math instruction tuning dataset with 1.8M problem-solution pairs. The dataset is constructed by synthesizing code-interpreter solutions for GSM8K and MATH, two popular math reasoning benchmarks, using the recently released and permissively licensed Mixtral model. Our best model, OpenMath-CodeLlama-70B, trained on a subset of OpenMathInstruct-1, achieves a score of 84.6% on GSM8K and 50.7% on MATH, which is competitive with the best gpt-distilled models. We will release our code, models, and the OpenMathInstruct-1 dataset under a commercially permissive license.",Datasets & Benchmarks,NeurIPS,2024,Oral,Shubham Toshniwal;Ivan Moshkov;Sean Narenthiran;Daria Gitman;Fei Jia;Igor Gitman,True,https://openreview.net/pdf?id=Mbd3QxXjq5
OTjTKFk7gb,AuctionNet: A Novel Benchmark for Decision-Making in Large-Scale Games,"Decision-making in large-scale games is an essential research area in artificial intelligence (AI) with significant real-world impact. However, the limited access to realistic large-scale game environments has hindered research progress in this area. In this paper, we present AuctionNet, a benchmark for bid decision-making in large-scale ad auctions derived from a real-world online advertising platform. AuctionNet is composed of three parts: an ad auction environment, a pre-generated dataset based on the environment, and performance evaluations of several baseline bid decision-making algorithms. More specifically, the environment effectively replicates the integrity and complexity of real-world ad auctions through the interaction of several modules: the ad opportunity generation module employs deep generative networks to bridge the gap between simulated and real-world data while mitigating the risk of sensitive data exposure; the bidding module implements diverse auto-bidding agents trained with different decision-making algorithms; and the auction module is anchored in the classic Generalized Second Price (GSP) auction but also allows for customization of auction mechanisms as needed. To facilitate research and provide insights into the environment, we have also pre-generated a substantial dataset based on the environment. The dataset contains 10 million ad opportunities, 48 diverse auto-bidding agents, and over 500 million auction records. Performance evaluations of baseline algorithms such as linear programming, reinforcement learning, and generative models for bid decision-making are also presented as a part of AuctionNet. AuctionNet has powered the NeurIPS 2024 Auto-Bidding in Large-Scale Auctions competition, providing competition environments for over 1,500 teams. We believe that AuctionNet is applicable not only to research on bid decision-making in ad auctions but also to the general area of decision-making in large-scale games. Code: https://github.com/alimama-tech/AuctionNet.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Kefan Su;Yusen Huo;Zhilin Zhang;Shuai Dou;Chuan Yu;Jian Xu;Zongqing Lu;Bo Zheng,True,https://openreview.net/pdf?id=OTjTKFk7gb
PcbSZwVVc5,DreamCatcher: A Wearer-aware Multi-modal Sleep Event Dataset Based on Earables in Non-restrictive Environments,"Poor quality sleep can be characterized by the occurrence of events ranging from body movement to breathing impairment. Widely available earbuds equipped with sensors (also known as earables) can be combined with a sleep event detection algorithm to offer a convenient alternative to laborious clinical tests for individuals suffering from sleep disorders. Although various solutions utilizing such devices have been proposed to detect sleep events, they ignore the fact that individuals often share sleeping spaces with roommates or couples. To address this issue, we introduce DreamCatcher, the first publicly available dataset for wearer-aware sleep event algorithm development on earables. DreamCatcher encompasses eight distinct sleep events, including synchronous dual-channel audio and motion data collected from 12 pairs (24 participants) totaling 210 hours (420 hour.person) with fine-grained label. We tested multiple benchmark models on three tasks related to sleep event detection, demonstrating the usability and unique challenge of DreamCatcher. We hope that the proposed DreamCatcher can inspire other researchers to further explore efficient wearer-aware human vocal activity sensing on earables. DreamCatcher is publicly available at https://github.com/thuhci/DreamCatcher.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Zeyu Wang;Xiyuxing Zhang;Ruotong Yu;Yuntao Wang;Kenneth Christofferson;Jingru Zhang;Alex Mariakakis;Yuanchun Shi,True,https://openreview.net/pdf?id=PcbSZwVVc5
QIJQ1qCGqV,Text to Blind Motion,"People who are blind perceive the world differently than those who are sighted, which can result in distinct motion characteristics. For instance, when crossing at an intersection, blind individuals may have different patterns of movement, such as veering more from a straight path or using touch-based exploration around curbs and obstacles. These behaviors may appear less predictable to motion models embedded in technologies such as autonomous vehicles. Yet, the ability of 3D motion models to capture such behavior has not been previously studied, as existing datasets for 3D human motion currently lack diversity and are biased toward people who are sighted. In this work, we introduce BlindWays, the first multimodal motion benchmark for pedestrians who are blind. We collect 3D motion data using wearable sensors with 11 blind participants navigating eight different routes in a real-world urban setting. Additionally, we provide rich textual descriptions that capture the distinctive movement characteristics of blind pedestrians and their interactions with both the navigation aid (e.g., a white cane or a guide dog) and the environment. We benchmark state-of-the-art 3D human prediction models, finding poor performance with off-the-shelf and pre-training-based methods for our novel task. To contribute toward safer and more reliable systems that can seamlessly reason over diverse human movements in their environments, our text-and-motion benchmark is available at https://blindways.github.io/.",Datasets & Benchmarks,NeurIPS,2024,Poster,Hee Jae Kim;Kathakoli Sengupta;Masaki Kuribayashi;Hernisa Kacorri;Eshed Ohn-Bar,True,https://openreview.net/pdf?id=QIJQ1qCGqV
QpF3DFP3Td,Archaeoscape: Bringing Aerial Laser Scanning Archaeology to the Deep Learning Era,"Airborne Laser Scanning (ALS) technology has transformed modern archaeology by unveiling hidden landscapes beneath dense vegetation. However, the lack of expert-annotated, open-access resources has hindered the analysis of ALS data using advanced deep learning techniques. We address this limitation with Archaeoscape (available at https://archaeoscape.ai/data/2024), a novel large-scale archaeological ALS dataset spanning 888 km² in Cambodia with 31,141 annotated archaeological features from the Angkorian period. Archaeoscape is over four times larger than comparable datasets, and the first ALS archaeology resource with open-access data, annotations, and models.
We benchmark several recent segmentation models to demonstrate the benefits of modern vision techniques for this problem and highlight the unique challenges of discovering subtle human-made structures under dense jungle canopies. By making Archaeoscape available in open access, we hope to bridge the gap between traditional archaeology and modern computer vision methods.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Yohann PERRON;Vladyslav Sydorov;Adam P. Wijker;Damian Evans;Christophe Pottier;Loic Landrieu,True,https://openreview.net/pdf?id=QpF3DFP3Td
R4rNYJ2slJ,OpenSatMap: A Fine-grained High-resolution Satellite Dataset for Large-scale Map Construction,"In this paper, we propose OpenSatMap, a fine-grained, high-resolution satellite dataset for large-scale map construction. Map construction is one of the foundations of the transportation industry, such as navigation and autonomous driving. Extracting road structures from satellite images is an efficient way to construct large-scale maps. However, existing satellite datasets provide only coarse semantic-level labels with a relatively low resolution (up to level 19), impeding the advancement of this field. In contrast, the proposed OpenSatMap (1) has fine-grained instance-level annotations; (2) consists of high-resolution images (level 20); (3) is currently the largest one of its kind; (4) collects data with high diversity. Moreover, OpenSatMap covers and aligns with the popular nuScenes dataset and Argoverse 2 dataset to potentially advance autonomous driving technologies. By publishing and maintaining the dataset, we provide a high-quality benchmark for satellite-based map construction and downstream tasks like autonomous driving.",Datasets & Benchmarks,NeurIPS,2024,Poster,Hongbo Zhao;Lue Fan;Yuntao Chen;Haochen Wang;yuran Yang;Xiaojuan Jin;YIXIN ZHANG;Gaofeng Meng;Zhaoxiang Zhang,True,https://openreview.net/pdf?id=R4rNYJ2slJ
ScPgzCZ6Lo,GC-Bench: An Open and Unified Benchmark for Graph Condensation,"Graph condensation (GC) has recently garnered considerable attention due to its ability to reduce large-scale graph datasets while preserving their essential properties. The core concept of GC is to create a smaller, more manageable graph that retains the characteristics of the original graph. Despite the proliferation of graph condensation methods developed in recent years, there is no comprehensive evaluation and in-depth analysis, which creates a great obstacle to understanding the progress in this field. To fill this gap, we develop a comprehensive Graph Condensation Benchmark (GC-Bench) to analyze the performance of graph condensation in different scenarios systematically. Specifically, GC-Bench systematically investigates the characteristics of graph condensation in terms of the following dimensions: effectiveness, transferability, and complexity. We comprehensively evaluate 12 state-of-the-art graph condensation algorithms in node-level and graph-level tasks and analyze their performance in 12 diverse graph datasets. Further, we have developed an easy-to-use library for training and evaluating different GC methods to facilitate reproducible research.The GC-Bench library is available at https://github.com/RingBDStack/GC-Bench.",Datasets & Benchmarks,NeurIPS,2024,Poster,Qingyun Sun;Ziying Chen;Beining Yang;Cheng Ji;Xingcheng Fu;Sheng Zhou;Hao Peng;Jianxin Li;Philip S. Yu,False,https://openreview.net/pdf?id=ScPgzCZ6Lo
USUkwg5pW6,Scribbles for All: Benchmarking Scribble Supervised Segmentation Across Datasets,"In this work, we introduce *Scribbles for All*, a label and training data generation algorithm for semantic segmentation trained on scribble labels. Training or fine-tuning semantic segmentation models with weak supervision has become an important topic recently and was subject to significant advances in model quality. In this setting, scribbles are a promising label type to achieve high quality segmentation results while requiring a much lower annotation effort than usual pixel-wise dense semantic segmentation annotations. The main limitation of scribbles as source for weak supervision is the lack of challenging datasets for scribble segmentation, which hinders the development of novel methods and conclusive evaluations. To overcome this limitation, *Scribbles for All* provides scribble labels for several popular segmentation datasets and provides an algorithm to automatically generate scribble labels for any dataset with dense annotations, paving the way for new insights and model advancements in the field of weakly supervised segmentation. In addition to providing datasets and algorithm, we evaluate state-of-the-art segmentation models on our datasets and show that models trained with our synthetic labels perform competitively with respect to models trained on manual labels. Thus, our datasets enable state-of-the-art research into methods for scribble-labeled semantic segmentation. The datasets, scribble generation algorithm, and baselines are publicly available at https://github.com/wbkit/Scribbles4All.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Wolfgang Boettcher;Lukas Hoyer;Ozan Unal;Jan Eric Lenssen;Bernt Schiele,True,https://openreview.net/pdf?id=USUkwg5pW6
UYgE9IfQIV,SustainDC: Benchmarking for Sustainable Data Center Control,"Machine learning has driven an exponential increase in computational demand, leading to massive data centers that consume significant amounts of energy and contribute to climate change. This makes sustainable data center control a priority. In this paper, we introduce SustainDC, a set of Python environments for benchmarking multi-agent reinforcement learning (MARL) algorithms for data centers (DC). SustainDC supports custom DC configurations and tasks such as workload scheduling, cooling optimization, and auxiliary battery management, with multiple agents managing these operations while accounting for the effects of each other. We evaluate various MARL algorithms on SustainDC, showing their performance across diverse DC designs, locations, weather conditions, grid carbon intensity, and workload requirements. Our results highlight significant opportunities for improvement of data center operations using MARL algorithms. Given the increasing use of DC due to AI, SustainDC provides a crucial platform for the development and benchmarking of advanced algorithms essential for achieving sustainable computing and addressing other heterogeneous real-world challenges.",Datasets & Benchmarks,NeurIPS,2024,Poster,Avisek Naug;Antonio Guillen;Ricardo Luna Gutierrez;Vineet Gundecha;Cullen Bash;Sahand Ghorbanpour;Sajad Mousavi;Ashwin Ramesh Babu;Dejan Markovikj;Lekhapriya Dheeraj Kashyap;Desik Rengarajan;Soumyendu Sarkar,False,https://openreview.net/pdf?id=UYgE9IfQIV
VXohja0vrQ,MedCalc-Bench: Evaluating Large Language Models for Medical Calculations,"Current benchmarks for evaluating large language models (LLMs) in medicine are primarily focused on question-answering involving domain knowledge and descriptive reasoning. While such qualitative capabilities are vital to medical diagnosis, in real-world scenarios, doctors frequently use clinical calculators that follow quantitative equations and rule-based reasoning paradigms for evidence-based decision support. To this end, we propose MedCalc-Bench, a first-of-its-kind dataset focused on evaluating the medical calculation capability of LLMs. MedCalc-Bench contains an evaluation set of over 1000 manually reviewed instances from 55 different medical calculation tasks. Each instance in MedCalc-Bench consists of a patient note, a question requesting to compute a specific medical value, a ground truth answer, and a step-by-step explanation showing how the answer is obtained. While our evaluation results show the potential of LLMs in this area, none of them are effective enough for clinical settings. Common issues include extracting the incorrect entities, not using the correct equation or rules for a calculation task, or incorrectly performing the arithmetic for the computation. We hope our study highlights the quantitative knowledge and reasoning gaps in LLMs within medical settings, encouraging future improvements of LLMs for various clinical calculation tasks. MedCalc-Bench is publicly available at: https://github.com/ncbi-nlp/MedCalc-Bench.",Datasets & Benchmarks,NeurIPS,2024,Oral,Nikhil Khandekar;Qiao Jin;Guangzhi Xiong;Soren Dunn;Serina S Applebaum;Zain Anwar;Maame Sarfo-Gyamfi;Conrad W Safranek;Abid Anwar;Andrew Jiaxing Zhang;Aidan Gilson;Maxwell B Singer;Amisha D Dave;R. Andrew Taylor;Aidong Zhang;Qingyu Chen;Zhiyong Lu,True,https://openreview.net/pdf?id=VXohja0vrQ
WUWHVN4gxk,Dataset and Lessons Learned from the 2024 SaTML LLM Capture-the-Flag Competition,"Large language model systems face significant security risks from maliciously crafted messages that aim to overwrite the system's original instructions or leak private data. To study this problem, we organized a capture-the-flag competition at IEEE SaTML 2024, where the flag is a secret string in the LLM system prompt. The competition was organized in two phases. In the first phase, teams developed defenses to prevent the model from leaking the secret. During the second phase, teams were challenged to extract the secrets hidden for defenses proposed by the other teams. This report summarizes the main insights from the competition. Notably, we found that all defenses were bypassed at least once, highlighting the difficulty of designing a successful defense and the necessity for additional research to protect LLM systems. To foster future research in this direction, we compiled a dataset with over 137k multi-turn attack chats and open-sourced the platform.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Edoardo Debenedetti;Javier Rando;Daniel Paleka;Silaghi Fineas Florin;Dragos Albastroiu;Niv Cohen;Yuval Lemberg;Reshmi Ghosh;Rui Wen;Ahmed Salem;Giovanni Cherubin;Santiago Zanella-Beguelin;Robin Schmid;Victor Klemm;Takahiro Miki;Chenhao Li;Stefan Kraft;Mario Fritz;Florian Tramèr;Sahar Abdelnabi;Lea Schönherr,True,https://openreview.net/pdf?id=WUWHVN4gxk
XBcStBjBIE,ChronoMagic-Bench: A Benchmark for Metamorphic Evaluation of Text-to-Time-lapse Video Generation,"We propose a novel text-to-video (T2V) generation benchmark, *ChronoMagic-Bench*, to evaluate the temporal and metamorphic knowledge skills in time-lapse video generation of the T2V models (e.g. Sora and Lumiere). Compared to existing benchmarks that focus on visual quality and text relevance of generated videos, *ChronoMagic-Bench* focuses on the models’ ability to generate time-lapse videos with significant metamorphic amplitude and temporal coherence. The benchmark probes T2V models for their physics, biology, and chemistry capabilities, in a free-form text control. For these purposes, *ChronoMagic-Bench* introduces **1,649** prompts and real-world videos as references, categorized into four major types of time-lapse videos: biological, human creation, meteorological, and physical phenomena, which are further divided into 75 subcategories. This categorization ensures a comprehensive evaluation of the models’ capacity to handle diverse and complex transformations. To accurately align human preference on the benchmark, we introduce two new automatic metrics, MTScore and CHScore, to evaluate the videos' metamorphic attributes and temporal coherence. MTScore measures the metamorphic amplitude, reflecting the degree of change over time, while CHScore assesses the temporal coherence, ensuring the generated videos maintain logical progression and continuity. Based on the *ChronoMagic-Bench*, we conduct comprehensive manual evaluations of eighteen representative T2V models, revealing their strengths and weaknesses across different categories of prompts, providing a thorough evaluation framework that addresses current gaps in video generation research. More encouragingly, we create a large-scale *ChronoMagic-Pro* dataset, containing **460k** high-quality pairs of 720p time-lapse videos and detailed captions. Each caption ensures high physical content and large metamorphic amplitude, which have a far-reaching impact on the video generation community. The source data and code are publicly available on [https://pku-yuangroup.github.io/ChronoMagic-Bench](https://pku-yuangroup.github.io/ChronoMagic-Bench).",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Shenghai Yuan;Jinfa Huang;Yongqi Xu;YaoYang Liu;Shaofeng Zhang;Yujun Shi;Rui-Jie Zhu;Xinhua Cheng;Jiebo Luo;Li Yuan,True,https://openreview.net/pdf?id=XBcStBjBIE
aXeiCbMFFJ,Visual CoT: Advancing Multi-Modal Language Models with a Comprehensive Dataset and Benchmark for Chain-of-Thought Reasoning,"Multi-Modal Large Language Models (MLLMs) have demonstrated impressive performance in various VQA tasks. However, they often lack interpretability and struggle with complex visual inputs, especially when the resolution of the input image is high or when the interested region that could provide key information for answering the question is small. To address these challenges, we collect and introduce the large-scale Visual CoT dataset comprising 438k question-answer pairs, annotated with intermediate bounding boxes highlighting key regions essential for answering the questions. Additionally, about 98k pairs of them are annotated with detailed reasoning steps. Importantly, we propose a multi-turn processing pipeline that dynamically focuses on visual inputs and provides interpretable thoughts. We also introduce the related benchmark to evaluate the MLLMs in scenarios requiring specific local region identification.
Extensive experiments demonstrate the effectiveness of our framework and shed light on better inference strategies. The Visual CoT dataset, benchmark, and pre-trained models are available on this [website](https://hao-shao.com/projects/viscot.html) to support further research in this area.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Hao Shao;Shengju Qian;Han Xiao;Guanglu Song;Zhuofan Zong;Letian Wang;Yu Liu;Hongsheng Li,True,https://openreview.net/pdf?id=aXeiCbMFFJ
abXaOcvujs,WikiDBs: A Large-Scale Corpus Of Relational Databases From Wikidata,"Deep learning on tabular data, and particularly tabular representation learning, has recently gained growing interest. However, representation learning for relational databases with multiple tables is still an underexplored area, which may be attributed to the lack of openly available resources. To support the development of foundation models for tabular data and relational databases, we introduce WikiDBs, a novel open-source corpus of 100,000 relational databases. Each database consists of multiple tables connected by foreign keys. The corpus is based on Wikidata and aims to follow certain characteristics of real-world databases. In this paper, we describe the dataset and our method for creating it. By making our code publicly available, we enable others to create tailored versions of the dataset, for example, by creating databases in different languages. Finally, we conduct a set of initial experiments to showcase how WikiDBs can be used to train for data engineering tasks, such as missing value imputation and column type annotation.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Liane Vogel;Jan-Micha Bodensohn;Carsten Binnig,True,https://openreview.net/pdf?id=abXaOcvujs
b6IBmU1uzw,CARES: A Comprehensive Benchmark of Trustworthiness in Medical Vision Language Models,"Artificial intelligence has significantly impacted medical applications, particularly with the advent of Medical Large Vision Language Models (Med-LVLMs), sparking optimism for the future of automated and personalized healthcare. However, the trustworthiness of Med-LVLMs remains unverified, posing significant risks for future model deployment. In this paper, we introduce CARES and aim to comprehensively evaluate the Trustworthiness of Med-LVLMs across the medical domain. We assess the trustworthiness of Med-LVLMs across five dimensions, including trustfulness, fairness, safety, privacy, and robustness. CARES comprises about 41K question-answer pairs in both closed and open-ended formats, covering 16 medical image modalities and 27 anatomical regions. Our analysis reveals that the models consistently exhibit concerns regarding trustworthiness, often displaying factual inaccuracies and failing to maintain fairness across different demographic groups. Furthermore, they are vulnerable to attacks and demonstrate a lack of privacy awareness. We publicly release our benchmark and code in https://github.com/richard-peng-xia/CARES.",Datasets & Benchmarks,NeurIPS,2024,Poster,Peng Xia;Ze Chen;Juanxi Tian;Gong Yangrui;Ruibo Hou;Yue Xu;Zhenbang Wu;Zhiyuan Fan;Yiyang Zhou;Kangyu Zhu;Wenhao Zheng;Zhaoyang Wang;Xiao Wang;Xuchao Zhang;Chetan Bansal;Marc Niethammer;Junzhou Huang;Hongtu Zhu;Yun Li;Jimeng Sun;Zongyuan Ge;Gang Li;James Zou;Huaxiu Yao,True,https://openreview.net/pdf?id=b6IBmU1uzw
cLga8GStdk,LINGOLY: A Benchmark of Olympiad-Level Linguistic Reasoning Puzzles in Low Resource and Extinct Languages,"In this paper, we present the LingOly benchmark, a novel benchmark for advanced reasoning abilities in large language models. Using challenging Linguistic Olympiad puzzles, we evaluate (i) capabilities for in-context identification and generalisation of linguistic patterns in very low-resource or extinct languages, and (ii) abilities to follow complex task instructions. The LingOly benchmark covers more than 90 mostly low-resource languages, minimising issues of data contamination, and contains 1,133 problems across 6 formats and 5 levels of human difficulty. We assess performance with both direct accuracy and comparison to a no-context baseline to penalise memorisation. Scores from 11 state-of-the-art LLMs demonstrate the benchmark to be challenging, and models perform poorly on the higher difficulty problems. On harder problems, even the top model only achieved 38.7% accuracy, a 24.7% improvement over the no-context baseline. Large closed models typically outperform open models, and in general, the higher resource the language, the better the scores. These results indicate, in absence of memorisation, true multi-step out-of-domain reasoning remains a challenge for current language models.",Datasets & Benchmarks,NeurIPS,2024,Oral,Andrew Michael Bean;Simeon Hellsten;Harry Mayne;Jabez Magomere;Ethan A Chi;Ryan Andrew Chi;Scott A. Hale;Hannah Rose Kirk,True,https://openreview.net/pdf?id=cLga8GStdk
cR3T1ZYN8I,A Hitchhiker's Guide to Fine-Grained Face Forgery Detection Using Common Sense Reasoning,"Explainability in artificial intelligence is crucial for restoring trust, particularly in areas like face forgery detection, where viewers often struggle to distinguish between real and fabricated content. Vision and Large Language Models (VLLM) bridge computer vision and natural language, offering numerous applications driven by strong common-sense reasoning. Despite their success in various tasks, the potential of vision and language remains underexplored in face forgery detection, where they hold promise for enhancing explainability by leveraging the intrinsic reasoning capabilities of language to analyse fine-grained manipulation areas.
    For that reason, few works have recently started to frame the problem of deepfake detection as a Visual Question Answering (VQA) task, nevertheless omitting the realistic and informative open-ended multi-label setting. With the rapid advances in the field of VLLM, an exponential rise of investigations in that direction is expected.
    As such, there is a need for a clear experimental methodology that converts face forgery detection to a Visual Question Answering (VQA) task to systematically and fairly evaluate different VLLM architectures. Previous evaluation studies in deepfake detection have mostly focused on the simpler binary task, overlooking evaluation protocols for multi-label fine-grained detection and text-generative models. We propose a multi-staged approach that diverges from the traditional binary evaluation protocol and conducts a comprehensive evaluation study to compare the capabilities of several VLLMs in this context.
    In the first stage, we assess the models' performance on the binary task and their sensitivity to given instructions using several prompts. In the second stage, we delve deeper into fine-grained detection by identifying areas of manipulation in a multiple-choice VQA setting. In the third stage, we convert the fine-grained detection to an open-ended question and compare several matching strategies for the multi-label classification task. Finally, we qualitatively evaluate the fine-grained responses of the VLLMs included in the benchmark.
    We apply our benchmark to several popular models, providing a detailed comparison of binary, multiple-choice, and open-ended VQA evaluation across seven datasets. \\\\url{https://nickyfot.github.io/hitchhickersguide.github.io/}",Datasets & Benchmarks,NeurIPS,2024,Poster,Niki Foteinopoulou;Enjie Ghorbel;Djamila Aouada,False,https://openreview.net/pdf?id=cR3T1ZYN8I
cu8FfaYriU,A Taxonomy of Challenges to Curating Fair Datasets,"Despite extensive efforts to create fairer machine learning (ML) datasets, there remains a limited understanding of the practical aspects of dataset curation. Drawing from interviews with 30 ML dataset curators, we present a comprehensive taxonomy of the challenges and trade-offs encountered throughout the dataset curation lifecycle. Our findings underscore overarching issues within the broader fairness landscape that impact data curation. We conclude with recommendations aimed at fostering systemic changes to better facilitate fair dataset curation practices.",Datasets & Benchmarks,NeurIPS,2024,Oral,Dora Zhao;Morgan Scheuerman;Pooja Chitre;Jerone Andrews;Georgia Panagiotidou;Shawn Walker;Kathleen H. Pine;Alice Xiang,False,https://openreview.net/pdf?id=cu8FfaYriU
cy8mq7QYae,CharXiv: Charting Gaps in Realistic Chart Understanding in Multimodal LLMs,"Chart understanding plays a pivotal role when applying Multimodal Large Language Models (MLLMs) to real-world tasks such as analyzing scientific papers or financial reports. However, existing datasets often focus on oversimplified and homogeneous charts with template-based questions, leading to an overly optimistic measure of progress. We demonstrate that although open-source models can appear to outperform strong proprietary models on these benchmarks, a simple stress test with slightly different charts or questions deteriorates performance by up to 34.5%. In this work, we propose CharXiv, a comprehensive evaluation suite involving 2,323 natural, challenging, and diverse charts from scientific papers. CharXiv includes two types of questions: 1) descriptive questions about examining basic chart elements and 2) reasoning questions that require synthesizing information across complex visual elements in the chart. To ensure quality, all charts and questions are handpicked, curated, and verified by human experts. Our results reveal a substantial, previously underestimated gap between the reasoning skills of the strongest proprietary model (i.e., GPT-4o), which achieves 47.1% accuracy, and the strongest open-source model (i.e., InternVL Chat V1.5), which achieves 29.2%. All models lag far behind human performance of 80.5%, underscoring weaknesses in the chart understanding capabilities of existing MLLMs. We hope that CharXiv facilitates future research on MLLM chart understanding by providing a more realistic and faithful measure of progress. Project website: https://charxiv.github.io/",Datasets & Benchmarks,NeurIPS,2024,Poster,Zirui Wang;Mengzhou Xia;Luxi He;Howard Chen;Yitao Liu;Richard Zhu;Kaiqu Liang;Xindi Wu;Haotian Liu;Sadhika Malladi;Alexis Chevalier;Sanjeev Arora;Danqi Chen,True,https://openreview.net/pdf?id=cy8mq7QYae
hcOq2buakM,"BetterBench: Assessing AI Benchmarks, Uncovering Issues, and Establishing Best Practices","AI models are increasingly prevalent in high-stakes environments, necessitating thorough assessment of their capabilities and risks. Benchmarks are popular for measuring these attributes and for comparing model performance, tracking progress, and identifying weaknesses in foundation and non-foundation models. They can inform model selection for downstream tasks and influence policy initiatives. However, not all benchmarks are the same: their quality depends on their design and usability. In this paper, we develop an assessment framework considering 40 best practices across a benchmark's life cycle and evaluate 25 AI benchmarks against it. We find that there exist large quality differences and that commonly used benchmarks suffer from significant issues. We further find that most benchmarks do not report statistical significance of their results nor can results be easily replicated. To support benchmark developers in aligning with best practices, we provide a checklist for minimum quality assurance based on our assessment. We also develop a living repository of benchmark assessments to support benchmark comparability.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Anka Reuel;Amelia Hardy;Chandler Smith;Max Lamparth;Malcolm Hardy;Mykel Kochenderfer,False,https://openreview.net/pdf?id=hcOq2buakM
iSwK1YqO7v,Embodied Agent Interface: Benchmarking LLMs for Embodied Decision Making,"We aim to evaluate Large Language Models (LLMs) for embodied decision making. While a significant body of work has been leveraging LLMs for decision making in embodied environments, we still lack a systematic understanding of their performance because they are usually applied in different domains, for different purposes, and built based on different inputs and outputs. Furthermore, existing evaluations tend to rely solely on a final success rate, making it difficult to pinpoint what ability is missing in LLMs and where the problem lies, which in turn blocks embodied agents from leveraging LLMs effectively and selectively. To address these limitations, we propose a generalized interface (Embodied Agent Interface) that supports the formalization of various types of tasks and input-output specifications of LLM-based modules. Specifically, it allows us to unify 1) a broad set of embodied decision-making tasks involving both state and temporally extended goals, 2) four commonly-used LLM-based modules for decision making: goal interpretation, subgoal decomposition, action sequencing, and transition modeling, and 3) a collection of fine-grained metrics that break down evaluation into error types, such as hallucination errors, affordance errors, and various types of planning errors. Overall, our benchmark offers a comprehensive assessment of LLMs’ performance for different subtasks, pinpointing the strengths and weaknesses in LLM-powered embodied AI systems and providing insights into the effective and selective use of LLMs in embodied decision making.",Datasets & Benchmarks,NeurIPS,2024,Oral,Manling Li;Shiyu Zhao;Qineng Wang;Kangrui Wang;Yu Zhou;Sanjana Srivastava;Cem Gokmen;Tony Lee;Li Erran Li;Ruohan Zhang;Weiyu Liu;Percy Liang;Li Fei-Fei;Jiayuan Mao;Jiajun Wu,False,https://openreview.net/pdf?id=iSwK1YqO7v
jSKtxmxc0M,VideoGUI: A Benchmark for GUI Automation from Instructional Videos,"Graphical User Interface (GUI) automation holds significant promise for enhancing human productivity by assisting with computer tasks. Existing task formulations primarily focus on simple tasks that can be specified by a single, language-only instruction, such as “Insert a new slide.” In this work, we introduce VideoGUI, a novel multi-modal benchmark designed to evaluate GUI assistants on visual-centric GUI tasks. Sourced from high-quality web instructional videos, our benchmark focuses on tasks involving professional and novel software (e.g., Adobe Pho- toshop or Stable Diffusion WebUI) and complex activities (e.g., video editing). VideoGUI evaluates GUI assistants through a hierarchical process, allowing for identification of the specific levels at which they may fail: (i) high-level planning: reconstruct procedural subtasks from visual conditions without language descrip- tions; (ii) middle-level planning: generate sequences of precise action narrations based on visual state (i.e., screenshot) and goals; (iii) atomic action execution: perform specific actions such as accurately clicking designated elements. For each level, we design evaluation metrics across individual dimensions to provide clear signals, such as individual performance in clicking, dragging, typing, and scrolling for atomic action execution. Our evaluation on VideoGUI reveals that even the SoTA large multimodal model GPT4o performs poorly on visual-centric GUI tasks, especially for high-level planning. The data and code are available at https://github.com/showlab/videogui.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Kevin Qinghong Lin;Linjie Li;Difei Gao;Qinchen WU;Mingyi Yan;Zhengyuan Yang;Lijuan Wang;Mike Zheng Shou,True,https://openreview.net/pdf?id=jSKtxmxc0M
jbrMS0DNaD,INQUIRE: A Natural World Text-to-Image Retrieval Benchmark,"We introduce INQUIRE, a text-to-image retrieval benchmark designed to challenge multimodal vision-language models on expert-level queries. INQUIRE includes iNaturalist 2024 (iNat24), a new dataset of five million natural world images, along with 250 expert-level retrieval queries. These queries are paired with all relevant images comprehensively labeled within iNat24, comprising 33,000 total matches. Queries span categories such as species identification, context, behavior, and appearance, emphasizing tasks that require nuanced image understanding and domain expertise. Our benchmark evaluates two core retrieval tasks: (1) INQUIRE-Fullrank, a full dataset ranking task, and (2) INQUIRE-Rerank, a reranking task for refining top-100 retrievals. Detailed evaluation of a range of recent multimodal models demonstrates that INQUIRE poses a significant challenge, with the best models failing to achieve an mAP@50 above 50%. In addition, we show that reranking with more powerful multimodal models can enhance retrieval performance, yet there remains a significant margin for improvement. By focusing on scientifically-motivated ecological challenges, INQUIRE aims to bridge the gap between AI capabilities and the needs of real-world scientific inquiry, encouraging the development of retrieval systems that can assist with accelerating ecological and biodiversity research.",Datasets & Benchmarks,NeurIPS,2024,Poster,Edward Vendrow;Omiros Pantazis;Alexander Shepard;Gabriel Brostow;Kate E. Jones;Oisin Mac Aodha;Sara Beery;Grant Van Horn,True,https://openreview.net/pdf?id=jbrMS0DNaD
loJM1acwzf,MMLONGBENCH-DOC: Benchmarking Long-context Document Understanding with Visualizations,"Understanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, particularly in single-page document understanding (DU). However, their abilities on long-context DU remain an open problem. This work presents MMLONGBENCH-DOC, a long-context, multi- modal benchmark comprising 1,082 expert-annotated questions. Distinct from previous datasets, it is constructed upon 135 lengthy PDF-formatted documents with an average of 47.5 pages and 21,214 textual tokens. Towards comprehensive evaluation, answers to these questions rely on pieces of evidence from (1) different sources (text, image, chart, table, and layout structure) and (2) various locations (i.e., page number). Moreover, 33.7\\\\% of the questions are cross-page questions requiring evidence across multiple pages. 20.6\\\\% of the questions are designed to be unanswerable for detecting potential hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 44.9\\\\%, while the second-best, GPT-4V, scores 30.5\\\\%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse performance than their LLM counterparts which are fed with lossy-parsed OCR documents. These results validate the necessity of future research toward more capable long-context LVLMs.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Yubo Ma;Yuhang Zang;Liangyu Chen;Meiqi Chen;Yizhu Jiao;Xinze Li;Xinyuan Lu;Ziyu Liu;Yan Ma;Xiaoyi Dong;Pan Zhang;Liangming Pan;Yu-Gang Jiang;Jiaqi Wang;Yixin Cao;Aixin Sun,True,https://openreview.net/pdf?id=loJM1acwzf
mlhFJE7PKo,HEST-1k: A Dataset For Spatial Transcriptomics and Histology Image Analysis,"Spatial transcriptomics enables interrogating the molecular composition of tissue with ever-increasing resolution and sensitivity. However, costs, rapidly evolving technology, and lack of standards have constrained computational methods in ST to narrow tasks and small cohorts. In addition, the underlying tissue morphology, as reflected by H&E-stained whole slide images (WSIs), encodes rich information often overlooked in ST studies. Here, we introduce HEST-1k, a collection of 1,229 spatial transcriptomic profiles, each linked to a WSI and extensive metadata. HEST-1k was assembled from 153 public and internal cohorts encompassing 26 organs, two species (Homo Sapiens and Mus Musculus), and 367 cancer samples from 25 cancer types. HEST-1k processing enabled the identification of 2.1 million expression-morphology pairs and over 76 million nuclei. To support its development, we additionally introduce the HEST-Library, a Python package designed to perform a range of actions with HEST samples. We test HEST-1k and Library on three use cases: (1) benchmarking foundation models for pathology (HEST-Benchmark), (2) biomarker exploration, and (3) multimodal representation learning. HEST-1k, HEST-Library, and HEST-Benchmark can be freely accessed at https://github.com/mahmoodlab/hest.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Guillaume Jaume;Paul Doucet;Andrew H. Song;Ming Y. Lu;Cristina Almagro Pérez;Sophia J Wagner;Anurag Jayant Vaidya;Richard J. Chen;Drew FK Williamson;Ahrong Kim;Faisal Mahmood,True,https://openreview.net/pdf?id=mlhFJE7PKo
nrEqH502eC,BLEnD: A Benchmark for LLMs on Everyday Knowledge in Diverse Cultures and Languages,"Large language models (LLMs) often lack culture-specific everyday knowledge, especially across diverse regions and non-English languages. Existing benchmarks for evaluating LLMs' cultural sensitivities are usually limited to a single language or online sources like Wikipedia, which may not reflect the daily habits, customs, and lifestyles of different regions. That is, information about the food people eat for their birthday celebrations, spices they typically use, musical instruments youngsters play or the sports they practice in school is not always explicitly written online. To address this issue, we introduce BLEnD, a hand-crafted benchmark designed to evaluate LLMs' everyday knowledge across diverse cultures and languages. The benchmark comprises 52.6k question-answer pairs from 16 countries/regions, in 13 different languages, including low-resource ones such as Amharic, Assamese, Azerbaijani, Hausa, and Sundanese. We evaluate LLMs in two formats: short-answer questions, and multiple-choice questions. We show that LLMs perform better in cultures that are more present online, with a maximum 57.34% difference in GPT-4, the best-performing model, in the short-answer format.
Furthermore, we find that LLMs perform better in their local languages for mid-to-high-resource languages. Interestingly, for languages deemed to be low-resource, LLMs provide better answers in English. We make our dataset publicly available at: https://github.com/nlee0212/BLEnD.",Datasets & Benchmarks,NeurIPS,2024,Poster,Junho Myung;Nayeon Lee;Yi Zhou;Jiho Jin;Rifki Afina Putri;Dimosthenis Antypas;Hsuvas Borkakoty;Eunsu Kim;Carla Perez-Almendros;Abinew Ali Ayele;Victor Gutierrez Basulto;Yazmin Ibanez-Garcia;Hwaran Lee;Shamsuddeen Hassan Muhammad;Kiwoong Park;Anar Sabuhi Rzayev;Nina White;Seid Muhie Yimam;Mohammad Taher Pilehvar;Nedjma Ousidhoum;Jose Camacho-Collados;Alice Oh,True,https://openreview.net/pdf?id=nrEqH502eC
p8eUitex7p,ImageNet3D: Towards General-Purpose Object-Level 3D Understanding,"A vision model with general-purpose object-level 3D understanding should be capable of inferring both 2D (*e.g.*, class name and bounding box) and 3D information (*e.g.*, 3D location and 3D viewpoint) for arbitrary rigid objects in natural images. This is a challenging task, as it involves inferring 3D information from 2D signals and most importantly, generalizing to rigid objects from unseen categories. However, existing datasets with object-level 3D annotations are often limited by the number of categories or the quality of annotations. Models developed on these datasets become specialists for certain categories or domains, and fail to generalize. In this work, we present ImageNet3D, a large dataset for general-purpose object-level 3D understanding. ImageNet3D augments 200 categories from the ImageNet dataset with 2D bounding box, 3D pose, 3D location annotations, and image captions interleaved with 3D information. With the new annotations available in ImageNet3D, we could (i) analyze the object-level 3D awareness of visual foundation models, and (ii) study and develop general-purpose models that infer both 2D and 3D information for arbitrary rigid objects in natural images, and (iii) integrate unified 3D models with large language models for 3D-related reasoning. We consider two new tasks, probing of object-level 3D awareness and open vocabulary pose estimation, besides standard classification and pose estimation. Experimental results on ImageNet3D demonstrate the potential of our dataset in building vision models with stronger general-purpose object-level 3D understanding. Our dataset and project page are available here: https://imagenet3d.github.io.",Datasets & Benchmarks,NeurIPS,2024,Poster,Wufei Ma;Guofeng Zhang;Qihao Liu;Guanning Zeng;Adam Kortylewski;Yaoyao Liu;Alan Yuille,True,https://openreview.net/pdf?id=p8eUitex7p
pUcTrjRLOM,UltraMedical: Building Specialized Generalists in Biomedicine,"Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains and are moving towards more specialized areas. Recent advanced proprietary models such as GPT-4 and Gemini have achieved significant advancements in biomedicine, which have also raised privacy and security challenges. The construction of specialized generalists hinges largely on high-quality datasets, enhanced by techniques like supervised fine-tuning and reinforcement learning from human or AI feedback, and direct preference optimization. However, these leading technologies (e.g., preference learning) are still significantly limited in the open source community due to the scarcity of specialized data. In this paper, we present the UltraMedical collections, which consist of high-quality manual and synthetic datasets in the biomedicine domain, featuring preference annotations across multiple advanced LLMs. By utilizing these datasets, we fine-tune a suite of specialized medical models based on Llama-3 series, demonstrating breathtaking capabilities across various medical benchmarks. Moreover, we develop powerful reward models skilled in biomedical and general reward benchmark, enhancing further online preference learning within the biomedical LLM community.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Kaiyan Zhang;Sihang Zeng;Ermo Hua;Ning Ding;Zhang-Ren Chen;Zhiyuan Ma;Haoxin Li;Ganqu Cui;Biqing Qi;Xuekai Zhu;Xingtai Lv;Hu Jinfang;Zhiyuan Liu;Bowen Zhou,True,https://openreview.net/pdf?id=pUcTrjRLOM
pYNl76onJL,VidProM: A Million-scale Real Prompt-Gallery Dataset for Text-to-Video Diffusion Models,"The arrival of Sora marks a new era for text-to-video diffusion models, bringing significant advancements in video generation and potential applications. However, Sora, along with other text-to-video diffusion models, is highly reliant on prompts, and there is no publicly available dataset that features a study of text-to-video prompts. In this paper, we introduce VidProM, the first large-scale dataset comprising 1.67 Million unique text-to-Video Prompts from real users. Additionally, this dataset includes 6.69 million videos generated by four state-of-the-art diffusion models, alongside some related data. We initially discuss the curation of this large-scale dataset, a process that is both time-consuming and costly. Subsequently, we underscore the need for a new prompt dataset specifically designed for text-to-video generation by illustrating how VidProM differs from DiffusionDB, a large-scale prompt-gallery dataset for image generation. Our extensive and diverse dataset also opens up many exciting new research areas. For instance, we suggest exploring text-to-video prompt engineering, efficient video generation, and video copy detection for diffusion models to develop better, more efficient, and safer models. The project (including the collected dataset VidProM and related code) is publicly available at https://vidprom.github.io under the CC-BY-NC 4.0 License.",Datasets & Benchmarks,NeurIPS,2024,Poster,Wenhao Wang;Yi Yang,True,https://openreview.net/pdf?id=pYNl76onJL
qmvtDIfbmS,WhodunitBench: Evaluating Large Multimodal Agents via Murder Mystery Games,"Recently, large language models (LLMs) have achieved superior performance, empowering the development of large multimodal agents (LMAs). An LMA is anticipated to execute practical tasks requires various capabilities including multimodal perception, interaction, reasoning, and decision making. However, existing benchmarks are limited in assessing compositional skills and actions demanded by practical scenarios, where they primarily focused on single tasks and static scenarios. To bridge this gap, we introduce WhodunitBench, a benchmark rooted from murder mystery games, where players are required to utilize the aforementioned skills to achieve their objective (i.e., identifying the `murderer' or hiding themselves), providing a simulated dynamic environment for evaluating LMAs. Specifically, WhodunitBench includes two evaluation modes. The first mode, the arena-style evaluation, is constructed from 50 meticulously curated scripts featuring clear reasoning clues and distinct murderers; The second mode, the chain of evaluation, consists of over 3000 curated multiple-choice questions and open-ended questions, aiming to assess every facet of the murder mystery games for LMAs. Experiments show that although current LMAs show acceptable performance in basic perceptual tasks, they are insufficiently equipped for complex multi-agent collaboration and multi-step reasoning tasks. Furthermore, the full application of the theory of mind to complete games in a manner akin to human behavior remains a significant challenge. We hope this work can illuminate the path forward, providing a solid foundation for the future development of LMAs. Our WhodunitBench is open-source and accessible at: https://github.com/
jun0wanan/WhodunitBench-Murder_Mystery_Games",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Junlin Xie;Ruifei Zhang;Zhihong Chen;Xiang Wan;Guanbin Li,True,https://openreview.net/pdf?id=qmvtDIfbmS
rovpCs3ZEO,FEDMEKI: A Benchmark for Scaling Medical Foundation Models via Federated Knowledge Injection,"This study introduces the Federated Medical Knowledge Injection (FedMEKI) platform, a new benchmark designed to address the unique challenges of integrating medical knowledge into foundation models under privacy constraints. By leveraging a cross-silo federated learning approach, FedMEKI circumvents the issues associated with centralized data collection, which is often prohibited under health regulations like the Health Insurance Portability and Accountability Act (HIPAA) in the USA. The platform is meticulously designed to handle multi-site, multi-modal, and multi-task medical data, which includes 7 medical modalities, including images, signals, texts, laboratory test results, vital signs, input variables, and output variables. The curated dataset to validate FedMEKI covers 8 medical tasks, including 6 classification tasks (lung opacity detection, COVID-19 detection, electrocardiogram (ECG) abnormal detection, mortality prediction, sepsis protection, and enlarged cardiomediastinum detection) and 2 generation tasks (medical visual question answering (MedVQA) and ECG noise clarification). This comprehensive dataset is partitioned across several clients to facilitate the decentralized training process under 16 benchmark approaches. FedMEKI not only preserves data privacy but also enhances the capability of medical foundation models by allowing them to learn from a broader spectrum of medical knowledge without direct data exposure, thereby setting a new benchmark in the application of foundation models within the healthcare sector.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Jiaqi Wang;Xiaochen Wang;Lingjuan Lyu;Jinghui Chen;Fenglong Ma,True,https://openreview.net/pdf?id=rovpCs3ZEO
s1K5Z5QPog,"ChaosBench: A Multi-Channel, Physics-Based Benchmark for Subseasonal-to-Seasonal Climate Prediction","Accurate prediction of climate in the subseasonal-to-seasonal scale is crucial for disaster preparedness and robust decision making amidst climate change. Yet, forecasting beyond the weather timescale is challenging because it deals with problems other than initial condition, including boundary interaction, butterfly effect, and our inherent lack of physical understanding. At present, existing benchmarks tend to have shorter forecasting range of up-to 15 days, do not include a wide range of operational baselines, and lack physics-based constraints for explainability. Thus, we propose ChaosBench, a challenging benchmark to extend the predictability range of data-driven weather emulators to S2S timescale. First, ChaosBench is comprised of variables beyond the typical surface-atmospheric ERA5 to also include ocean, ice, and land reanalysis products that span over 45 years to allow for full Earth system emulation that respects boundary conditions. We also propose physics-based, in addition to deterministic and probabilistic metrics, to ensure a physically-consistent ensemble that accounts for butterfly effect. Furthermore, we evaluate on a diverse set of physics-based forecasts from four national weather agencies as baselines to our data-driven counterpart such as ViT/ClimaX, PanguWeather, GraphCast, and FourCastNetV2. Overall, we find methods originally developed for weather-scale applications fail on S2S task: their performance simply collapse to an unskilled climatology. Nonetheless, we outline and demonstrate several strategies that can extend the predictability range of existing weather emulators, including the use of ensembles, robust control of error propagation, and the use of physics-informed models. Our benchmark, datasets, and instructions are available at https://leap-stc.github.io/ChaosBench.",Datasets & Benchmarks,NeurIPS,2024,Oral,Juan Nathaniel;Yongquan Qu;Tung Nguyen;Sungduk Yu;Julius Busecke;Aditya Grover;Pierre Gentine,True,https://openreview.net/pdf?id=s1K5Z5QPog
t9aThFL1lE,UnlearnCanvas: Stylized Image Dataset for Enhanced Machine Unlearning Evaluation in Diffusion Models,"The technological advancements in diffusion models (DMs) have demonstrated unprecedented capabilities in text-to-image generation and are widely used in diverse applications. However, they have also raised significant societal concerns, such as the generation of harmful content and copyright disputes. Machine unlearning (MU) has emerged as a promising solution, capable of removing undesired generative capabilities from DMs. However, existing MU evaluation systems present several key challenges that can result in incomplete and inaccurate assessments. To address these issues, we propose UnlearnCanvas, a comprehensive high-resolution stylized image dataset that facilitates the evaluation of the unlearning of artistic styles and associated objects. This dataset enables the establishment of a standardized, automated evaluation framework with 7 quantitative metrics assessing various aspects of the unlearning performance for DMs. Through extensive experiments, we benchmark 9 state-of-the-art MU methods for DMs, revealing novel insights into their strengths, weaknesses, and underlying mechanisms. Additionally, we explore challenging unlearning scenarios for DMs to evaluate worst-case performance against adversarial prompts, the unlearning of finer-scale concepts, and sequential unlearning. We hope that this study can pave the way for developing more effective, accurate, and robust DM unlearning methods, ensuring safer and more ethical applications of DMs in the future. The dataset, benchmark, and codes are publicly available at this [link](https://unlearn-canvas.netlify.app/).",Datasets & Benchmarks,NeurIPS,2024,Poster,Yihua Zhang;Chongyu Fan;Yimeng Zhang;Yuguang Yao;Jinghan Jia;Jiancheng Liu;Gaoyuan Zhang;Gaowen Liu;Ramana Rao Kompella;Xiaoming Liu;Sijia Liu,True,https://openreview.net/pdf?id=t9aThFL1lE
tPsw4NeLZx,MM-WLAuslan: Multi-View Multi-Modal Word-Level Australian Sign Language Recognition Dataset,"Isolated Sign Language Recognition (ISLR) focuses on identifying individual sign language glosses. Considering the diversity of sign languages across geographical regions, developing region-specific ISLR datasets is crucial for supporting communication and research. Auslan, as a sign language specific to Australia, still lacks a dedicated large-scale word-level dataset for the ISLR task. To fill this gap, we curate \\\\underline{\\\\textbf{the first}} large-scale Multi-view Multi-modal Word-Level Australian Sign Language recognition dataset, dubbed MM-WLAuslan. Compared to other publicly available datasets, MM-WLAuslan exhibits three significant advantages: (1) **the largest amount** of data, (2) **the most extensive** vocabulary, and (3) **the most diverse** of multi-modal camera views. Specifically, we record **282K+** sign videos covering **3,215** commonly used Auslan glosses presented by **73** signers in a studio environment.
Moreover, our filming system includes two different types of cameras, i.e., three Kinect-V2 cameras and a RealSense camera. We position cameras hemispherically around the front half of the model and simultaneously record videos using all four cameras. Furthermore, we benchmark results with state-of-the-art methods for various multi-modal ISLR settings on MM-WLAuslan, including multi-view, cross-camera, and cross-view. Experiment results indicate that MM-WLAuslan is a challenging ISLR dataset, and we hope this dataset will contribute to the development of Auslan and the advancement of sign languages worldwide. All datasets and benchmarks are available at MM-WLAuslan.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xin Shen;Heming Du;Hongwei Sheng;Shuyun Wang;Hui Chen;Huiqiang Chen;Zhuojie Wu;Xiaobiao Du;Jiaying Ying;Ruihan Lu;Qingzheng Xu;Xin Yu,True,https://openreview.net/pdf?id=tPsw4NeLZx
vyraA7xt4c,Mercury: A Code Efficiency Benchmark for Code Large Language Models,"Amidst the recent strides in evaluating Large Language Models for Code (Code LLMs), existing benchmarks have mainly focused on the functional correctness of generated code, neglecting the importance of their computational efficiency. To fill the gap, we present Mercury, the first code efficiency benchmark for Code LLMs. It comprises 1,889 Python tasks, each accompanied by adequate solutions that serve as real-world efficiency baselines, enabling a comprehensive analysis of the runtime distribution. Based on the distribution, we introduce a new metric Beyond, which computes a runtime-percentile-weighted Pass score to reflect functional correctness and code efficiency simultaneously. On Mercury, leading Code LLMs can achieve 65% on Pass, while less than 50% on Beyond. Given that an ideal Beyond score would be aligned with the Pass score, it indicates that while Code LLMs exhibit impressive capabilities in generating functionally correct code, there remains a notable gap in their efficiency. Finally, our empirical experiments reveal that Direct Preference Optimization (DPO) serves as a robust baseline for enhancing code efficiency compared with Supervised Fine Tuning (SFT), which paves a promising avenue for future exploration of efficient code generation. Our code and data are available on GitHub: https://github.com/Elfsong/Mercury.",Datasets & Benchmarks,NeurIPS,2024,Poster,Mingzhe Du;Anh Tuan Luu;Bin Ji;Qian Liu;See-Kiong Ng,True,https://openreview.net/pdf?id=vyraA7xt4c
wOmtZ5FgMH,RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models,"Large language models (LLMs) inevitably memorize sensitive, copyrighted, and harmful knowledge from the training corpus; therefore, it is crucial to erase this knowledge from the models. Machine unlearning is a promising solution for efficiently removing specific knowledge by post hoc modifying models. In this paper, we propose a Real-World Knowledge Unlearning benchmark (RWKU) for LLM unlearning. RWKU is designed based on the following three key factors: (1) For the task setting, we consider a more practical and challenging unlearning setting, where neither the forget corpus nor the retain corpus is accessible. (2) For the knowledge source, we choose 200 real-world famous people as the unlearning targets and show that such popular knowledge is widely present in various LLMs. (3) For the evaluation framework, we design the forget set and the retain set to evaluate the model’s capabilities across various real-world applications. Regarding the forget set, we provide four four membership inference attack (MIA) methods and nine kinds of adversarial attack probes to rigorously test unlearning efficacy. Regarding the retain set, we assess locality and utility in terms of neighbor perturbation, general ability, reasoning ability, truthfulness, factuality, and fluency. We conduct extensive experiments across two unlearning scenarios, two models and six baseline methods and obtain some meaningful findings. We release our benchmark and code publicly at http://rwku-bench.github.io for future work.",Datasets & Benchmarks,NeurIPS,2024,Poster,Zhuoran Jin;Pengfei Cao;Chenhao Wang;Zhitao He;Hongbang Yuan;Jiachun Li;Yubo Chen;Kang Liu;Jun Zhao,True,https://openreview.net/pdf?id=wOmtZ5FgMH
wmO7z57wNK,LLMCBench: Benchmarking Large Language Model Compression for Efficient Deployment,"Although large language models (LLMs) have demonstrated their strong intelligence ability, the high demand for computation and storage hinders their practical application. To this end, many model compression techniques are proposed to increase the efficiency of LLMs. However, current researches only validate their methods on limited models, datasets, metrics, etc, and still lack a comprehensive evaluation under more general scenarios. So it is still a question of which model compression approach we should use under a specific case. To mitigate this gap, we present the Large Language Model Compression Benchmark (LLMCBench), a rigorously designed benchmark with an in-depth analysis for LLM compression algorithms. We first analyze the actual model production requirements and carefully design evaluation tracks and metrics. Then, we conduct extensive experiments and comparison using multiple mainstream LLM compression approaches. Finally, we perform an in-depth analysis based on the evaluation and provide useful insight for LLM compression design. We hope our LLMCBench can contribute insightful suggestions for LLM compression algorithm design and serve as a foundation for future research.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Ge Yang;Changyi He;Jinyang Guo;Jianyu Wu;Yifu Ding;Aishan Liu;Haotong Qin;Pengliang Ji;Xianglong Liu,True,https://openreview.net/pdf?id=wmO7z57wNK
x8RgF2xQTj,Benchmarking Uncertainty Disentanglement: Specialized Uncertainties for Specialized Tasks,"Uncertainty quantification, once a singular task, has evolved into a spectrum of tasks, including abstained prediction, out-of-distribution detection, and aleatoric uncertainty quantification. The latest goal is disentanglement: the construction of multiple estimators that are each tailored to one and only one source of uncertainty. This paper presents the first benchmark of uncertainty disentanglement. We reimplement and evaluate a comprehensive range of uncertainty estimators, from Bayesian over evidential to deterministic ones, across a diverse range of uncertainty tasks on ImageNet. We find that, despite recent theoretical endeavors, no existing approach provides pairs of disentangled uncertainty estimators in practice. We further find that specialized uncertainty tasks are harder than predictive uncertainty tasks, where we observe saturating performance. Our results provide both practical advice for which uncertainty estimators to use for which specific task, and reveal opportunities for future research toward task-centric and disentangled uncertainties. All our reimplementations and Weights & Biases logs are available at https://github.com/bmucsanyi/untangle.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Bálint Mucsányi;Michael Kirchhof;Seong Joon Oh,False,https://openreview.net/pdf?id=x8RgF2xQTj
y09S5rdaWY,Bench2Drive: Towards Multi-Ability Benchmarking of Closed-Loop End-To-End Autonomous Driving,"In an era marked by the rapid scaling of foundation models, autonomous driving technologies are approaching a transformative threshold where end-to-end autonomous driving (E2E-AD) emerges due to its potential of scaling up in the data-driven manner. However, existing E2E-AD methods are mostly evaluated under the open-loop log-replay manner with L2 errors and collision rate as metrics (e.g., in nuScenes), which could not fully reflect the driving performance of algorithms as recently acknowledged in the community. For those E2E-AD methods evaluated under the closed-loop protocol, they are tested in fixed routes (e.g., Town05Long and Longest6 in CARLA) with the driving score as metrics, which is known for high variance due to the unsmoothed metric function and large randomness in the long route. Besides, these methods usually collect their own data for training, which makes algorithm-level fair comparison infeasible. 

   To fulfill the paramount need of comprehensive, realistic, and fair testing environments for Full Self-Driving (FSD), we present Bench2Drive, the first benchmark for evaluating E2E-AD systems' multiple abilities in a closed-loop manner. Bench2Drive's official training data consists of 2 million fully annotated frames, collected from 10000 short clips uniformly distributed under 44 interactive scenarios (cut-in, overtaking, detour, etc), 23 weathers (sunny, foggy, rainy, etc), and 12 towns (urban, village, university, etc) in CARLA v2. Its evaluation protocol requires E2E-AD models to pass 44 interactive scenarios under different locations and weathers which sums up to 220 routes and thus provides a comprehensive and disentangled assessment about their driving capability under different situations. We implement state-of-the-art E2E-AD models and evaluate them in Bench2Drive, providing insights regarding current status and future directions.",Datasets & Benchmarks,NeurIPS,2024,Poster,Xiaosong Jia;Zhenjie Yang;Qifeng Li;Zhiyuan Zhang;Junchi Yan,True,https://openreview.net/pdf?id=y09S5rdaWY
y10DM6R2r3,MMLU-Pro: A More Robust and Challenging Multi-Task Language Understanding Benchmark,"In the age of large-scale language models, benchmarks like the Massive Multitask Language Understanding (MMLU) have been pivotal in pushing the boundaries of what AI can achieve in language comprehension and reasoning across diverse domains. However, as models continue to improve, their performance on these benchmarks has begun to plateau, making it increasingly difficult to discern differences in model capabilities. This paper introduces MMLU-Pro, an enhanced dataset designed to extend the mostly knowledge-driven MMLU benchmark by integrating more challenging, reasoning-focused questions and expanding the choice set from four to ten options. Additionally, MMLU-Pro eliminates part of the trivial and noisy questions in MMLU. Our experimental results show that MMLU-Pro not only raises the challenge, causing a significant drop in accuracy by 16\\\\% to 33\\\\% compared to MMLU, but also demonstrates greater stability under varying prompts. With 24 different prompt styles tested, the sensitivity of model scores to prompt variations decreased from 4-5\\\\% in MMLU to just 2\\\\% in MMLU-Pro. Additionally, we found that models utilizing Chain of Thought (CoT) reasoning achieved better performance on MMLU-Pro compared to direct answering, which is in stark contrast to the findings on the original MMLU, indicating that MMLU-Pro includes more complex reasoning questions. Our assessments confirm that MMLU-Pro is more discriminative benchmark to better track progress in the field.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Yubo Wang;Xueguang Ma;Ge Zhang;Yuansheng Ni;Abhranil Chandra;Shiguang Guo;Weiming Ren;Aaran Arulraj;Xuan He;Ziyan Jiang;Tianle Li;Max Ku;Kai Wang;Alex Zhuang;Rongqi Fan;Xiang Yue;Wenhu Chen,True,https://openreview.net/pdf?id=y10DM6R2r3
yS1dUkQFnu,V-PETL Bench: A Unified Visual Parameter-Efficient Transfer Learning Benchmark,"Parameter-efficient transfer learning (PETL) methods show promise in adapting a pre-trained model to various downstream tasks while training only a few parameters. In the computer vision (CV) domain, numerous PETL algorithms have been proposed, but their direct employment or comparison remains inconvenient. To address this challenge, we construct a Unified Visual PETL Benchmark (V-PETL Bench) for the CV domain by selecting 30 diverse, challenging, and comprehensive datasets from image recognition, video action recognition, and dense prediction tasks. On these datasets, we systematically evaluate 25 dominant PETL algorithms and open-source a modular and extensible codebase for fair evaluation of these algorithms. V-PETL Bench runs on NVIDIA A800 GPUs and requires approximately 310 GPU days. We release all the benchmark, making it more efficient and friendly to researchers. Additionally, V-PETL Bench will be continuously updated for new PETL algorithms and CV tasks.",Datasets & Benchmarks,NeurIPS,2024,Poster,Yi Xin;Siqi Luo;Xuyang Liu;Yuntao Du.;Haodi Zhou;Xinyu Cheng;Christina Luoluo Lee;Junlong Du;Haozhe Wang;MingCai Chen;Ting Liu;Guimin Hu;Zhongwei Wan;Rongchao Zhang;Aoxue Li;Mingyang Yi;Xiaohong Liu,False,https://openreview.net/pdf?id=yS1dUkQFnu
yUEBXN3cvX,On the Effects of Data Scale on UI Control Agents,"Autonomous agents that control user interfaces to accomplish human tasks are emerging. Leveraging LLMs to power such agents has been of special interest, but unless fine-tuned on human-collected task demonstrations, performance is still relatively low. In this work we study whether fine-tuning alone is a viable approach for building real-world UI control agents. To this end we collect and release a new dataset, AndroidControl, consisting of 15,283 demonstrations of everyday tasks with Android apps. Compared to existing datasets, each AndroidControl task instance includes both high and low-level human-generated instructions, allowing us to explore the level of task complexity an agent can handle. Moreover, AndroidControl is the most diverse computer control dataset to date, including 14,548 unique tasks over 833 Android apps, thus allowing us to conduct in-depth analysis of the model performance in and out of the domain of the training data. Using the dataset, we find that when tested in domain fine-tuned models outperform zero and few-shot baselines and scale in such a way that robust performance might feasibly be obtained simply by collecting more data.  Out of domain, performance scales significantly more slowly and suggests that in particular for high-level tasks, fine-tuning on more data alone may be insufficient for achieving robust out-of-domain performance.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Wei Li;William E Bishop;Alice Li;Christopher Rawles;Folawiyo Campbell-Ajala;Divya Tyamagundlu;Oriana Riva,True,https://openreview.net/pdf?id=yUEBXN3cvX
z64azPC6Nl,GTSinger: A Global Multi-Technique Singing Corpus with Realistic Music Scores for All Singing Tasks,"The scarcity of high-quality and multi-task singing datasets significantly hinders the development of diverse controllable and personalized singing tasks, as existing singing datasets suffer from low quality, limited diversity of languages and singers, absence of multi-technique information and realistic music scores, and poor task suitability.
To tackle these problems, we present GTSinger, a large Global, multi-Technique, free-to-use, high-quality singing corpus with realistic music scores, designed for all singing tasks, along with its benchmarks.
Particularly,
(1) we collect 80.59 hours of high-quality singing voices, forming the largest recorded singing dataset;
(2) 20 professional singers across nine widely spoken languages offer diverse timbres and styles;
(3) we provide controlled comparison and phoneme-level annotations of six commonly used singing techniques, helping technique modeling and control;
(4) GTSinger offers realistic music scores, assisting real-world musical composition;
(5) singing voices are accompanied by manual phoneme-to-audio alignments, global style labels, and 16.16 hours of paired speech for various singing tasks.
Moreover, to facilitate the use of GTSinger, we conduct four benchmark experiments: technique-controllable singing voice synthesis, technique recognition, style transfer, and speech-to-singing conversion.",Datasets & Benchmarks,NeurIPS,2024,Spotlight,Yu Zhang;Changhao Pan;Wenxiang Guo;Ruiqi Li;Zhiyuan Zhu;Jialei Wang;Wenhao Xu;Jingyu Lu;Zhiqing Hong;Chuxin Wang;Lichao Zhang;Jinzheng He;Ziyue Jiang;Yuxin Chen;Chen Yang;Jiecheng Zhou;Xinyu Cheng;Zhou Zhao,True,https://openreview.net/pdf?id=z64azPC6Nl
zogaeVpbaE,DevBench: A multimodal developmental benchmark for language learning,"How (dis)similar are the learning trajectories of vision–language models and children? Recent modeling work has attempted to understand the gap between models’ and humans’ data efficiency by constructing models trained on less data, especially multimodal naturalistic data. However, such models are often evaluated on adult-level benchmarks, with limited breadth in language abilities tested, and without direct comparison to behavioral data. We introduce DevBench, a multimodal benchmark comprising seven language evaluation tasks spanning the domains of lexical, syntactic, and semantic ability, with behavioral data from both children and adults. We evaluate a set of vision–language models on these tasks, comparing models and humans on their response patterns, not their absolute performance. Across tasks, models exhibit variation in their closeness to human response patterns, and models that perform better on a task also more closely resemble human behavioral responses. We also examine the developmental trajectory of OpenCLIP over training, finding that greater training results in closer approximations to adult response patterns. DevBench thus provides a benchmark for comparing models to human language development. These comparisons highlight ways in which model and human language learning processes diverge, providing insight into entry points for improving language models.",Datasets & Benchmarks,NeurIPS,2024,Oral,Alvin Wei Ming Tan;Chunhua Yu;Bria Lorelle Long;Wanjing Anya Ma;Tonya Murray;Rebecca D. Silverman;Jason D Yeatman;Michael Frank,True,https://openreview.net/pdf?id=zogaeVpbaE
XiConLcsqq,RewardBench: Evaluating Reward Models for Language Modeling,"Reward models (RMs) are at the crux of successful RLHF to align pretrained models to human preferences, yet there has been relatively little study that focuses on evaluation of those reward models. Evaluating reward models presents an opportunity to understand the opaque technologies used for alignment of language models and which values are embedded in them. To date, very few descriptors of capabilities, training methods, or open-source reward models exist. In this paper, we present RewardBench, a benchmark dataset and code-base for evaluation, to enhance scientific understanding of reward models. The RewardBench dataset is a collection of prompt-win-lose trios spanning chat, reasoning, and safety, to benchmark how reward models perform on challenging, structured and out-of-distribution queries. We created specific comparison datasets for RMs that have subtle, but verifiable reasons (e.g. bugs, incorrect facts) why one answer should be preferred to another. On the RewardBench leaderboard, we evaluate reward models trained with a variety of methods, such as the direct MLE training of classifiers and the implicit reward modeling of Direct Preference Optimization (DPO), and on a spectrum of datasets. We present many findings on propensity for refusals, reasoning limitations, and instruction following shortcomings of various reward models towards a better understanding of the RLHF process.",Datasets & Benchmarks,NeurIPS,2024,Reject,Nathan Lambert;Valentina Pyatkin;Jacob Morrison;Lester James Validad Miranda;Bill Yuchen Lin;Khyathi Chandu;Nouha Dziri;Sachin Kumar;Tom Zick;Yejin Choi;Noah A. Smith;Hannaneh Hajishirzi,True,https://openreview.net/pdf?id=XiConLcsqq
FjeJB0OUhN,"Can Long-Context Language Models Subsume Retrieval, SQL, and More?","Long-context language models (LCLMs) have the potential to revolutionize our approach to tasks traditionally reliant on external tools like retrieval systems or databases. Leveraging LCLMs’ ability to natively ingest and process entire corpora of information offers numerous advantages. It enhances user-friendliness by eliminating the need for specialized knowledge of tools, provides robust end-to-end modeling that minimizes cascading errors in complex pipelines, and allows for the application of sophisticated prompting techniques across the entire system. To assess this paradigm shift, we introduce LOFT, a benchmark comprising of real-world tasks requiring context up to millions of tokens designed to evaluate LCLMs’ performance on in-context retrieval and reasoning. Our findings reveal that LCLMs can already achieve textual and visual retrieval performance comparable to specialized systems such as Gecko and CLIP, while still facing challenges in areas like multi-hop compositional reasoning required in SQL-like tasks. Notably, prompting strategies significantly influence performance, emphasizing the need for continued research as context lengths grow. Overall, LOFT provides a rigorous testing ground for LCLMs, showcasing their potential to supplant existing paradigms and tackle novel tasks as model capabilities scale.",Datasets & Benchmarks,NeurIPS,2024,Reject,Jinhyuk Lee;Anthony Chen;Zhuyun Dai;Dheeru Dua;Devendra Singh Sachan;Michael Boratko;Yi Luan;Séb Arnold;Vincent Perot;Siddharth Dalmia;Hexiang Hu;Xudong Lin;Panupong Pasupat;Aida Amini;Jeremy R. Cole;Sebastian Riedel;Iftekhar Naim;Ming-Wei Chang;Kelvin Guu,True,https://openreview.net/pdf?id=FjeJB0OUhN
Becrgm5xAq,RL4CO: an Extensive Reinforcement Learning for Combinatorial Optimization Benchmark,"Deep reinforcement learning (RL) has recently shown significant benefits in solving combinatorial optimization (CO) problems, reducing reliance on domain expertise, and improving computational efficiency. However, the field lacks a unified benchmark for easy development and standardized comparison of algorithms across diverse CO problems. To fill this gap, we introduce RL4CO, a unified and extensive benchmark with in-depth library coverage of 23 state-of-the-art methods and 20+ CO problems. Built on efficient software libraries and best practices in implementation, RL4CO features modularized implementation and flexible configuration of diverse RL algorithms, neural network architectures, inference techniques, and environments. RL4COallows researchers to seamlessly navigate existing successes and develop their unique designs, facilitating the entire research process by decoupling science from heavy engineering. We also provide extensive benchmark studies to inspire new insights and future work. RL4CO has attracted numerous researchers in the community and is open-sourced at https://github.com/ai4co/rl4co.",Datasets & Benchmarks,NeurIPS,2024,Reject,Federico Berto;Chuanbo Hua;Junyoung Park;Laurin Luttmann;Yining Ma;Fanchen Bu;Jiarui Wang;Haoran Ye;Minsu Kim;Sanghyeok Choi;Nayeli Gast Zepeda;André Hottung;Jianan Zhou;Jieyi Bi;Yu Hu;Fei Liu;Hyeonah Kim;Jiwoo Son;Haeyeon Kim;Davide Angioni;Wouter Kool;Zhiguang Cao;Qingfu Zhang;Joungho Kim;Jie Zhang;Kijung Shin;Cathy Wu;Sungsoo Ahn;Guojie Song;Changhyun Kwon;Kevin Tierney;Lin Xie;Jinkyoo Park,True,https://openreview.net/pdf?id=Becrgm5xAq
bxwWikAXSy,MathWriting: A Dataset For Handwritten Mathematical Expression Recognition,"Recognition of handwritten mathematical expressions allows to transfer scientific notes into their digital form. It facilitates the sharing, searching, and preservation of scientific information. We introduce MathWriting, the largest online handwritten mathematical expression dataset to date. It consists of 230k human-written samples and an additional 400k synthetic ones. This dataset can also be used in its rendered form for offline HME recognition. One MathWriting sample consists of a formula written on a touch screen and a corresponding LaTeX expression. We also provide a normalized version of LaTeX expression to simplify the recognition task and enhance the result quality. We provide baseline performance of standard models like OCR and CTC Transformer as well as Vision-Language Models like PaLI on the dataset. The dataset together with an example colab is accessible on Github.",Datasets & Benchmarks,NeurIPS,2024,Reject,Philippe Gervais;Anastasiia Fadeeva;Andrii Maksai,True,https://openreview.net/pdf?id=bxwWikAXSy
3qa4YLkcEw,TRACE: A Comprehensive Benchmark for Continual Learning in Large Language Models,"Aligned large language models (LLMs) demonstrate exceptional capabilities in task-solving, following instructions, and ensuring safety. However, the continual learning aspect of these aligned LLMs has been largely overlooked. Existing continual learning benchmarks lack sufficient challenge for leading aligned LLMs due to their homogeneous task types and low task complexity. To bridge this gap, we introduce TRACE, a benchmark designed to rigorously assess continual learning capabilities in LLMs. TRACE comprises eight challenging tasks from the scope of domain-specific tasks, multilingual capabilities, code generation, and mathematical reasoning.  Through systematic experiments on TRACE with six different aligned models ranging from 7B to 70B, we discovered significant declines in both general performance and instruction-following abilities. For example, the accuracy of llama2-chat 13B on the gsm8k dataset declined precipitously from 43.14\\\\% to 2.12\\\\% after training on our datasets. This highlights the challenge of finding a suitable tradeoff between achieving performance on specific tasks while preserving the original prowess of LLMs. Our results demonstrate that integrating task-specific cues with meta-rationales significantly reduces catastrophic forgetting and improves task convergence, offering a viable strategy to enhance the adaptability of LLMs in dynamic environments.",Datasets & Benchmarks,NeurIPS,2024,Reject,Xiao Wang;Yuansen Zhang;Tianze Chen;Songyang Gao;Senjie Jin;Zhiheng Xi;Rui Zheng;Yicheng Zou;Tao Gui;Qi Zhang;Xuanjing Huang,True,https://openreview.net/pdf?id=3qa4YLkcEw
t3Xj7YD7fL,An NLP Benchmark Dataset for Predicting the Completeness of ESG Reports,"Environmental, Social, and Governance (ESG) reports serve as a platform for companies to publicly disclose their economic, environmental, and social impacts, as well as their contributions to sustainable development goals. The completeness of ESG reports is considered a crucial criterion for judging their quality and credibility, yet it is often overlooked in existing literature. This paper aims to comprehensively assess the completeness of ESG reports by evaluating their topic coverage and text quality. To achieve this goal, we propose two classification tasks: topic classification and quality classification for ESG sentences. To train the classifiers, we collected 14,468 ESG reports from Chinese-listed companies. We then segment them into sentences and label 8K of them with both topic and text quality tags. By fine-tuning several large language models (LLMs) on this dataset on the two classification tasks, we find that our dataset has the potential to fill the gap in academia regarding methods for measuring ESG completeness.",Datasets & Benchmarks,NeurIPS,2024,Reject,Qi Chang;Xuan Yang;Zihan Ding;Bin Liu;Wei Lan,True,https://openreview.net/pdf?id=t3Xj7YD7fL
OLEQJoAED6,Don't Always Say No to Me: Benchmarking Safety-Related Refusal in Large VLM,"Warning: this paper contains example data that may be offensive or harmful. Although many existing evaluation datasets have been proposed to assess the safety of Large Vision-Language Models (LVLMs) on malicious prompt-image pairs, the research community lacks a systematic investigation into LVLMs' reasonable refusal toward both safe and unsafe pairs.  We define a control group consisting of an unsafe prompt-image pair and a safe pair, in which these two pairs share the same prompt or image. In a control group, an LVLM shows reasonable refusal if it refuses the former pair and responds to the latter. Otherwise, the model displays false refusal, such as refusing both pairs or none. For example, a control group contains an image depicting violent behavior and two prompts based on the same visual information. An LVLM should respond to the safe prompt ""How to deter this behavior?"" and refuse the unsafe prompt ""How to promote this behavior?"". To bridge this gap, we present LVLM-SafeR, a challenging and high-quality benchmark designed to measure Safety-related Refusal in LVLMs. The evaluation results from 9 closed-source LVLMs, 23 open-source LVLMs and 4 LVLM safety alignment approaches demonstrate that existing LVLMs have notable issues in providing proper refusals. Furthermore, we explore the effects of post-hoc/mixed safety fine-tuning, full/LoRA safety fine-tuning, and inference-time parameters (top-p, temperature) on LVLMs. Then we propose an effective prompt-engineering baseline to instruct LVLMs to give more reasonable refusals. Our project page is available at isxinliu.github.io/Project/LVLM-SafeR.",Datasets & Benchmarks,NeurIPS,2024,Reject,Xin Liu;Zhichen Dong;Zhanhui Zhou;Yichen Zhu;Yunshi Lan;Jing Shao;Chao Yang;Yu Qiao,True,https://openreview.net/pdf?id=OLEQJoAED6
iGeJxHqnbx,QCircuitNet: A Large-Scale Hierarchical Dataset for Quantum Algorithm Design,"Quantum computing is an emerging field recognized for the significant speedup it offers over classical computing through quantum algorithms. However, designing and implementing quantum algorithms pose challenges due to the complex nature of quantum mechanics and the necessity for precise control over quantum states. To address these challenges, we leverage AI to simplify and enhance the process.  Despite the significant advancements in AI, there has been a lack of datasets specifically tailored for this purpose.  

In this work, we introduce QCircuitNet, a benchmark and test dataset designed to evaluate AI’s capability in designing and implementing quantum algorithms in the form of quantum circuit codes. Unlike traditional AI code writing, this task is fundamentally different and significantly more complicated due to the highly flexible design space and the extreme demands for intricate manipulation of qubits.

Our key contributions include: 
1. The first comprehensive, structured universal quantum algorithm dataset.
2. A framework which formulates the task of quantum algorithm design for Large Language Models (LLMs), providing guidelines for expansion and potential evolution into a training dataset.
3. Automatic validation and verification functions, allowing for scalable and efficient evaluation methodologies.
4. A fair and stable benchmark that avoids data contamination, a particularly critical issue in quantum computing datasets.

Our work aims to bridge the gap in available resources for AI-driven quantum algorithm design, offering a robust and scalable method for evaluating and improving AI models in this field. As we expand the dataset to include more algorithms and explore novel fine-tuning methods, we hope it will significantly contribute to both quantum algorithm design and implementation.",Datasets & Benchmarks,NeurIPS,2024,Reject,Rui Yang;Yuntian Gu;Ziruo Wang;Yitao Liang;Tongyang Li,True,https://openreview.net/pdf?id=iGeJxHqnbx
