{"id": "CVPR-2024-1865", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Benjamin Attal, Eliot Laidlaw, Aaron Gokaslan, Changil Kim, Christian Richardt, James Tompkin, and Matthew O'Toole. T\u00f6RF: Time-of-flight radiance fields for dynamic scene view synthesis. In NeurIPS, 2021.\\n\\n[2] Paul J Besl and Neil D McKay. Method for registration of 3-d shapes. In Sensor fusion IV: control paradigms and data structures, pages 586\u2013606. Spie, 1992.\\n\\n[3] Clara Callenberg, Zheng Shi, Felix Heide, and Matthias B Hullin. Low-cost spad sensing for non-line-of-sight tracking, material classification and depth imaging. ACM Transactions on Graphics (TOG), 40(4):1\u201312, 2021.\\n\\n[4] Edoardo Charbon. Introduction to time-of-flight imaging. In SENSORS, 2014 IEEE, pages 610\u2013613. IEEE, 2014.\\n\\n[5] Yuki Fujimura, Takahiro Kushida, Takuya Funatomi, and Yasuhiro Mukaigawa. Nlos-neus: Non-line-of-sight neural implicit surface. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 10532\u201310541, 2023.\\n\\n[6] Connor Henley, Tomohiro Maeda, Tristan Swedish, and Ramesh Raskar. Imaging behind occluders using two-bounce light. In ECCV, 2020.\\n\\n[7] Connor Henley, Joseph Hollmann, and Ramesh Raskar. Bounce-flash lidar. IEEE Transactions on Computational Imaging, 8:411\u2013424, 2022.\\n\\n[8] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.\\n\\n[9] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. LRM: Large reconstruction model for single image to 3D. arXiv:2311.04400, 2023.\\n\\n[10] Shengyu Huang, Zan Gojcic, Zian Wang, Francis Williams, Yoni Kasten, Sanja Fidler, Konrad Schindler, and Or Litany. Neural lidar fields for novel view synthesis. Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023.\\n\\n[11] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf on a diet: Semantically consistent few-shot view synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 5885\u20135894, 2021.\\n\\n[12] Wenzel Jakob. Mitsuba renderer, 2010. http://www.mitsuba-renderer.org.\\n\\n[13] Suren Jayasuriya, Adithya Pediredla, Sriram Sivaramakrishnan, Alyosha Molnar, and Ashok Veeraraghavan. Depth fields: Extending light field techniques to time-of-flight imaging. In 2015 International Conference on 3D Vision, pages 1\u20139. IEEE, 2015.\\n\\n[14] Asaf Karnieli, Ohad Fried, and Yacov Hel-Or. Deepshadow: Neural shape from shadow. In European Conference on Computer Vision, pages 415\u2013430. Springer, 2022.\\n\\n[15] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\n[16] Ahmed Kirmani, Tyler Hutchison, James Davis, and Ramesh Raskar. Looking around the corner using transient imaging. In 2009 IEEE 12th International Conference on Computer Vision, pages 159\u2013166. IEEE, 2009.\\n\\n[17] Jos\u00e9 Luis Landabaso, Montse Pard\u00e0s, and Josep Ramon Casas. Shape from inconsistent silhouette. Comput. Vis. Image Underst., 112:210\u2013224, 2008.\\n\\n[18] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300\u2013309, 2023.\\n\\n[19] Jingwang Ling, Zhibo Wang, and Feng Xu. Shadowneus: Neural sdf reconstruction by shadow ray supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 175\u2013185, 2023.\\n\\n[20] Ruoshi Liu, Sachit Menon, Chengzhi Mao, Dennis Park, Simon Stent, and Carl von Ondr. Shadows shed light on 3D objects. arXiv:2206.08990, 2022.\\n\\n[21] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl von Ondr. Zero-1-to-3: Zero-shot one image to 3D object. arXiv:2303.11328, 2023.\\n\\n[22] William E. Lorensen and Harvey E. Cline. Marching cubes: A high resolution 3D surface construction algorithm. Computer Graphics (Proceedings of SIGGRAPH), 21(4):163\u2013169, 1987.\\n\\n[23] Anagh Malik, Parsa Mirdehghan, Sotiris Nousias, Kiriakos N. Kutulakos, and David B. Lindell. Transient neural radiance fields for lidar view synthesis and 3D reconstruction. arXiv:2307.09555, 2023.\\n\\n[24] Worthy N. Martin and J. K. Aggarwal. V olumetric descriptions of objects from multiple views. IEEE Transactions on Pattern Analysis and Machine Intelligence, PAMI-5(2):150\u2013158, 1983.\\n\\n[25] Andreas Meuleman, Hakyeong Kim, James Tompkin, and Min H Kim. Floatingfusion: Depth from tof and image-stabilized stereo cameras. In European Conference on Computer Vision, pages 602\u2013618. Springer, 2022.\\n\\n[26] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\\n\\n[27] Fangzhou Mu, Sicheng Mo, Jiayong Peng, Xiaochun Liu, Ji Hyun Nam, Siddeshwar Raghavan, Andreas Velten, and Yin Li. Physics to the rescue: Deep non-line-of-sight reconstruction for high-speed imaging. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2022.\\n\\n[28] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019.\\n\\n[29] Adithya Pediredla, Ashok Veeraraghavan, and Ioannis Gkioulekas. Ellipsoidal path connections for time-gated rendering. ACM Transactions on Graphics (TOG), 38(4):1\u201312, 2019.\"}"}
{"id": "CVPR-2024-1865", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adithya K Pediredla, Aswin C Sankaranarayanan, Mauro Buttafava, Alberto Tosi, and Ashok Veeraraghavan. Signal processing based pile-up compensation for gated single-photon avalanche diodes. arXiv preprint arXiv:1806.07437, 2018.\\n\\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv, 2022.\\n\\nKyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lapun, Li Fei-Fei, Deqing Sun, and Jiajun Wu. ZeroNVS: Zero-shot 360-degree view synthesis from a single real image. arXiv:2310.17994, 2023.\\n\\nSilvio Savarese, Holly Rushmeier, Fausto Bernardini, and Pietro Perona. Shadow carving. In Proceedings Eighth IEEE International Conference on Computer Vision. ICCV 2001, pages 190\u2013197. IEEE, 2001.\\n\\nSiyuan Shen, Zi Wang, Ping Liu, Zhengqing Pan, Ruiqian Li, Tian Gao, Shiying Li, and Jingyi Yu. Non-line-of-sight imaging via neural transient fields. TPAMI, 43(7):2257\u20132268, 2021.\\n\\nSiddharth Somasundaram, Akshat Dave, Connor Henley, Ashok Veeraraghavan, and Ramesh Raskar. Role of transients in two-bounce non-line-of-sight imaging. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9192\u20139201, 2023.\\n\\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.\\n\\nTang Tao, Longfei Gao, Guangrun Wang, Peng Chen, Dayang Hao, Xiaodan Liang, Mathieu Salzmann, and Kaicheng Yu. LiDAR-NeRF: Novel lidar view synthesis via neural radiance fields. arXiv preprint arXiv:2304.10406, 2023.\\n\\nKushagra Tiwary, Tzofi Klinghoffer, and Ramesh Raskar. Towards learning neural representations from shadows. In European Conference on Computer Vision, pages 300\u2013316. Springer, 2022.\\n\\nAndreas Velten, Thomas Willwacher, Otkrist Gupta, Ashok Veeraraghavan, Moungi G Bawendi, and Ramesh Raskar. Recovering three-dimensional shape around a corner using ultrafast time-of-flight imaging. Nature communications, 3(1):745, 2012.\\n\\nHaochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pre-trained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12619\u201312629, 2023.\\n\\nZhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. arXiv preprint arXiv:2305.16213, 2023.\\n\\nDejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey Shi, and Zhangyang Wang. SinNeRF: Training neural radiance fields on complex scenes from a single image. In ECCV, 2022.\\n\\nWenqi Yang, Guanying Chen, Chaofeng Chen, Zhenfang Chen, and Kwan-Yee K. Wong. S$^3$-NeRF: Neural reflectance field from shading and shadow under a single viewpoint. In NeurIPS, 2022.\\n\\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4578\u20134587, 2021.\\n\\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023.\\n\\nJunzhe Zhu and Peiye Zhuang. HiFA: High-fidelity text-to-3D with advanced diffusion guidance. arXiv:2305.18766, 2023.\"}"}
{"id": "CVPR-2024-1865", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"PlatoNeRF: 3D Reconstruction in Plato\u2019s Cave via Single-View Two-Bounce Lidar\\n\\nTzofi Klinghoffer\\nXiaoyu Xiang\\nSiddharth Somasundaram\\nYuchen Fan\\nChristian Richardt\\nRamesh Raskar\\nRakesh Ranjan\\n\\n1 Massachusetts Institute of Technology\\n2 Meta\\n3 Codec Avatars Lab, Meta\\n\\nProject page: https://platonerf.github.io\\n\\nFigure 1. PlatoNeRF. We propose PlatoNeRF: a method to recover scene geometry from a single view using two-bounce signals captured by a single-photon lidar.\\n\\n(a) A laser illuminates a scene point, which diffusely reflects light in all directions. The reflected light illuminates the rest of the scene and casts shadows. Light that returns to the lidar sensor provides information about the visible scene, and cast shadows provide information about occluded portions of the scene.\\n\\n(b) The lidar sensor captures 3D time-of-flight images.\\n\\n(c) By aggregating several such images (by scanning the position of the laser), we are able to reconstruct the entire 3D scene geometry with volumetric rendering.\\n\\nAbstract\\n\\n3D reconstruction from a single-view is challenging because of the ambiguity from monocular cues and lack of information about occluded regions. Neural radiance fields (NeRF), while popular for view synthesis and 3D reconstruction, are typically reliant on multi-view images. Existing methods for single-view 3D reconstruction with NeRF rely on either data priors to hallucinate views of occluded regions, which may not be physically accurate, or shadows observed by RGB cameras, which are difficult to detect in ambient light and low albedo backgrounds. We propose using time-of-flight data captured by a single-photon avalanche diode to overcome these limitations. Our method models two-bounce optical paths with NeRF, using lidar transient data for supervision. By leveraging the advantages of both NeRF and two-bounce light measured by lidar, we demonstrate that we can reconstruct visible and occluded geometry without data priors or reliance on controlled ambient lighting or scene albedo. In addition, we demonstrate improved generalization under practical constraints on sensor spatial- and temporal-resolution. We believe our method is a promising direction as single-photon lidars become ubiquitous on consumer devices, such as phones, tablets, and headsets.\\n\\nPlatoNeRF is named after the allegory of Plato\u2019s Cave, in which reality is discerned from shadows cast on a cave wall.\\n\\n* Equal contribution.\\n\\n1. Introduction\\n\\nRecovering 3D scene geometry from a single-view is critical for many applications, ranging from autonomous vehicles (AV) to extended reality (XR). Consider playing a game of catch with a virtual ball in XR: if the ball drops and bounces behind your couch, it should bounce out in a physically realistic manner, dependent on the occluded geometry. Maintaining a complete and up-to-date scan of the scene is tedious for XR users and infeasible in many other applications, such as robotics and AV. Thus, methods are needed that recover geometry from single or few views; we address the former.\\n\\nWhile neural radiance fields (NeRF) [26] are a popular representation for scene geometry, single-view 3D reconstruction with NeRF is challenging and remains an open problem. Existing methods in single-view 3D reconstruction with NeRF either rely on data priors [9, 21, 42, 47] or use visual cues, such as shadows, to infer occluded geometry from a single view [19, 20, 38, 44]. Approaches such as diffusion, generative adversarial networks, and transformers rely on data priors to exploit correlations between observations and a large corpus of training data. As a result, these methods are known to hallucinate content which, while statistically likely, may not be physically accurate. Other methods use shadows to infer occluded geometry when training NeRF [19, 20, 38, 44]. However, these methods struggle when the shadow is difficult to detect, such as in ambient light or low albedo backgrounds. In addition, these methods typically...\"}"}
{"id": "CVPR-2024-1865", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"predict relative depth, rather than absolute depth, which is important for many applications. To overcome these limitations, while still enabling physically-accurate reconstruction, we propose using two-bounce light measured with lidar. Single-photon lidar systems, implemented with single-photon avalanche diodes (SPADs), offer an opportunity for accurate single-view 3D reconstruction. Lidar systems typically emit light into the scene and measure the time of flight (ToF) of the light to return to the sensor. As illustrated in Fig. 1a, this light reflects off the scene multiple times \u2014 we refer to each reflection as a \\\"bounce\\\". While traditional lidar systems only exploit the first bounce of light from the scene back to the sensor, providing accurate absolute depth, recent work has shown that two-bounce time of flight, i.e. the time it takes for light to reflect off the scene two times before returning to the sensor, can enable reconstruction of occluded objects [7]. While promising, a limitation of existing methods is generalization to the lower spatial- and temporal-resolutions of lidars found on consumer devices. Our method, called PlatoNeRF, addresses the limitations of single-view NeRF with two-bounce lidar and the limitations of two-bounce lidar with NeRF. The goal of PlatoNeRF is to reconstruct visible and occluded geometry from a single view using ToF measurements of two-bounce light from a single-photon lidar. Like Henley et al. [7], we illuminate individual points in the scene with a pulsed laser. Light is reflected off the illuminated points onto the rest of the scene before reflecting to the sensor. This light, referred to as two-bounce light, contains information about both scene depth and the presence of shadows created by the laser. Our experimental setup is described further in Sec. 3.1. Using the ToF measurements from multiple illumination points, we train NeRF to reconstruct the two-bounce ToF by modeling two-bounce optical paths. The presence or absence of two-bounce light reveals shadows, which allow occluded geometry to be inferred, and its ToF reveals depth. Our method is able to reconstruct 3D geometry with higher accuracy than existing single-view NeRF or lidar methods. Furthermore, using lidar allows our method to operate with higher ambient light and lower scene albedo than RGB methods that exploit shadows. We also demonstrate our method better generalizes to lower spatial- and temporal-resolutions than existing lidar methods due to our use of an implicit representation. To summarize, our contributions are:\\n\\n1. Two-Bounce Lidar NeRF Model: We propose a method to learn 3D geometry by modeling two-bounce light paths and supervising NeRF with lidar transients.\\n2. Single-View 3D Reconstruction: We demonstrate that our method is able to accurately reconstruct scenes from a single-view without hallucinating details of the scene.\\n3. Analysis: We study our method's robustness to ambient light, scene albedo, and spatial- and temporal-resolution. In addition, we prepare a dataset of simulated scenes captured with a single-photon lidar. We use this data to evaluate our method and our baselines. Simulating such data is challenging and requires domain expertise. To lower the barrier to entry for machine learning with single-photon lidars and to drive future research in this direction, we have released this dataset, along with our code and model checkpoints, on our project page: https://platonerf.github.io. Scope of this Work. Our work focuses on reconstruction of Lambertian scenes and we leave non-Lambertian scenes as future work. In addition, we focus on indoor scenes where there are multiple surfaces to reflect light. We assume laser scanning rather than flash illumination.\\n\\n2. Related Work\\n\\nSingle-View Reconstruction. Single-view reconstruction is an ill-posed problem due to missing constraints. To address this, data-driven methods [11, 43, 45] hallucinate the invisible regions using learned 2D or 3D priors. Recently, inspired by the success of diffusion models in generation [8, 36], several methods explore distilling 3D correspondences from pretrained 2D text-to-image models [18, 31, 40, 41]. Others learn 3D priors to produce multi-view-consistent outputs conditioned on the input view [9, 21, 32]. While these methods can generate realistic images, they are unable to ensure physically accurate reconstruction of occluded regions without geometric cues, which is the focus of our work.\\n\\nNeural Shape from Shadow. Shape from shadows (SfS) provides a physically-accurate way to infer occluded geometry based on the shadows it casts. While traditional methods use shadowgrams, space carving, and probabilistic methods to infer SfS [17, 24, 33], in recent years, NeRF has been shown to be an effective representation for learning SfS [14, 19, 20, 38, 44]. These methods leverage volumetric rendering to reconstruct the object or scene based on the observation that pixels in shadow result from geometry between the shadowed point and the light source. However, the performance of these methods degrades when the shadow becomes invisible \u2014 either due to ambient light or low albedo backgrounds. In contrast, our method, while still relying on shadows to reconstruct occluded areas, is more robust to these effects due to our use of lidar rather than RGB sensors.\\n\\n3D Reconstruction with Single-Photon Lidars. Single-photon lidars record time-correlated light intensity and have been widely used for 3D reconstruction. We consider the most common type in our work: single-photon avalanche diodes (SPADs). SPAD-based methods often actively illuminate the scene and record the number of photons arriving at the sensor over time to infer scene geometry. Either visible or non-visible, e.g. near infrared, wavelengths of light can be emitted and detected. Each bounce of light in the scene reveals information about the scene's geometry. First-bounce light encodes scene depth [3, 13], while third-bounce light...\"}"}
{"id": "CVPR-2024-1865", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Problem Definition.\\n\\nWe use a lidar system containing a SPAD at position $x_s$ and a pulsed laser at position $x_l$. The SPAD view is kept constant, while the laser sequentially illuminates different points in the scene, $\\\\{l_1, \\\\ldots, l_K\\\\}$. For each illumination spot, we measure the time of flight for light to travel from $x_l$ to $x_s$, shown by the captured transient.\\n\\nThis encodes partial information about the geometry of objects that are outside the sensor's line of sight, e.g., around corners [16, 39]. NeRF has been used to exploit one- [1, 10, 23, 37] and three-bounce [5, 27, 34] light in lidar/ToF. We focus on two-bounce time of flight, which has recently been shown to encode the geometry of occluded objects [6, 7, 35]. Henley et al. [7] propose a two-step approach, first estimating scene depth from two-bounce returns, and then occluded geometry based on shadows inferred from the presence or absence of two-bounce returns. Inspired by this work, we propose a single, unified pipeline with NeRF to reconstruct both properties. Our work has three main benefits over [7]: (1) a unified approach for both visible and hidden geometry, (2) smoother scene reconstruction, and (3) better generalization to lower spatial (e.g., $32 \\\\times 32$) and temporal resolution regimes, which are key limitations on consumer devices [25].\\n\\n3. NeRF from Single-View Two-Bounce Lidar\\n\\nIn this section, we outline a method to extract 3D geometry from two-bounce transient measurements with NeRF. In Sec. 3.1, we describe the experimental setup, image formation model, and two-bounce transients. In Sec. 3.2, we describe how NeRF is trained with supervision from two-bounce transients. In Sec. 3.3, we provide implementation details that enable replication of our method and results.\\n\\n3.1. Notations and Problem Definition\\n\\nExperimental Setup.\\n\\nFig. 2 shows our experimental setup. The lidar system consists of a SPAD sensor and pulsed laser at known positions $x_s$ and $x_l$ respectively. The laser sequentially points at $K$ different points $\\\\{l_1, \\\\ldots, l_K\\\\}$. For each illumination point $l_k$, an image $i_k$ is captured, resulting in a set of $K$ images $I = \\\\{i_1, \\\\ldots, i_K\\\\}$, as illustrated in Fig. 1.\\n\\nOne-Bounce vs. Two-Bounce Light.\\n\\nSPAD sensors are able to infer properties of a scene by measuring light that has interacted with the scene. In this problem, we are interested in inferring 3D scene geometry from one-bounce and two-bounce light, where \\\"bounce\\\" denotes the number of times light reflects off a scene surface. In Fig. 2, light that travels along the path $x_l \\\\rightarrow l \\\\rightarrow x_p \\\\rightarrow x_s$ is one-bounce light because it undergoes one reflection at $l$. Similarly, light that travels along the path $x_l \\\\rightarrow l \\\\rightarrow x_p \\\\rightarrow x_s$ is referred to as two-bounce light because it undergoes two reflections at $l$ and $x_p$. We refer to each illumination point $l$ as a virtual source because it acts as a point light source. Similarly, we define $x_p$ as a virtual detector because it refers to the scene point that is observing light from $l$. In measurement $i_k$, the pixel observing scene point $l_k$ measures one-bounce signal, and all other pixels (e.g. $x_p$) measure two-bounce signals or shadows. In general, one-bounce light arrives at the sensor earlier in time because it travels a shorter optical pathlength. One-bounce light also generally has higher intensity because light intensity is attenuated after every surface reflection.\\n\\nTransient Measurement.\\n\\nEach image $i \\\\in \\\\mathbb{R}^{N_u \\\\times N_v \\\\times N_t}$ is a transient measurement, where $N_u$ and $N_v$ represent the spatial resolution and $N_t$ denotes the number of timing bins. A transient $i(u, v, t)$ measures the amount of light arriving at every pixel $(u, v)$ at a given time $t$. A pixel measurement $i(u, v, t)$ measures a histogram of light intensity as a function of time. Each bin of the histogram is discretized to a timing resolution, or bin width, of $t_{\\\\text{res}}$. For our experiments, we choose $t_{\\\\text{res}} = 128 \\\\text{ ps}$, meaning that we can resolve the pathlength of light up to a precision of $3.8 \\\\text{ cm}$. An example histogram two-bounce signal is plotted in Fig. 2. The location of the two-bounce peak $t_{\\\\text{peak}}$ in the histogram is directly correlated to the pathlength $d$ that the light travels via the following equation:\\n\\n$$t_{\\\\text{peak}} = \\\\frac{d}{c} = \\\\frac{d_1 + d_2 + d_3}{c},$$\\n\\nwhere $c \\\\approx 3 \\\\cdot 10^8 \\\\text{ m/s}$ is the speed of light, $d_1$ corresponds to the distance between the laser $x_l$ and the virtual source $l$, $d_2$ is the distance between the virtual source $l$ and virtual detector $x_p$, and $d_3$ is the distance between the virtual detector $x_p$ and sensor $x_s$, as shown in Fig. 2.\\n\\nShadow Measurement.\\n\\nEq. (2) assumes that a direct path exists between $x_p$ and $l$. However, if $x_p$ lies in shadow, no two-bounce signal will be measured (i.e., no pulse will be observed). $x_p$ is defined to lie in shadow if an opaque object lies along the ray connecting $l$ and $x_p$. Because $l$ is modeled as a point light source, we neglect any diffraction effects and soft shadows that are common with area sources.\"}"}
{"id": "CVPR-2024-1865", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3.\\n\\n### Method\\n\\nPlatoNeRF learns 3D scene geometry from single-view two-bounce lidar time of flight, modeled with NeRF. Our method consists of three steps.\\n\\n(a) First, we render primary rays from the camera to the scene (Sec. 3.2.1).\\n\\n(b) Second, we model rays that scatter and travel to the virtual light (the point where light rays first hit the scene) (Sec. 3.2.2). Both steps are supervised with transients measured by a single-photon lidar.\\n\\n(c) Third, we find that reconstructing the two-bounce time of flight enables 3D reconstruction (Sec. 3.2.3).\\n\\n### Problem Statement\\n\\nThe resulting transient measurements will contain information about one-bounce signals, two-bounce signals, and shadows. The one-bounce and two-bounce signals provide information about objects that are visible to the sensor, and the shadows provide information about occluded portions of the scene. Using these measurements, we will reconstruct the 3D geometry of visible and occluded portions of the scene. Note that although we capture $N$ measurements, the measurements are captured from the same view with only the laser being scanned.\\n\\n#### 3.2. Two-Bounce Volumetric Lidar Rendering\\n\\nWe parameterize our scene as a neural radiance field (NeRF). The MLP $f_{\\\\theta} : \\\\mathbb{R}^3 \\\\rightarrow \\\\mathbb{R}$ predicts a volume density $\\\\sigma$ for every input 3D scene point $x = (x, y, z)$. The 3D geometry of the scene can then directly be estimated from $\\\\sigma(x)$. The goal of this subsection is to synthesize transient images by developing a renderer that can map densities $\\\\sigma$ to predicted transients $\\\\hat{i}$. These synthesized transient measurements can then be used to train the NeRF in an analysis-by-synthesis framework. Note that, unlike a vanilla NeRF, we are not computing radiance, because we only reconstruct 3D geometry, not texture. Our method is summarized in Fig. 3.\\n\\nTo render two-bounce transients, we must render along two types of rays: (1) primary rays and (2) secondary rays.\\n\\nPrimary rays are defined as $r_p(\\\\lambda) = o_p + \\\\lambda d_p$, where $o_p = x_s$ and $d_p$ is determined by the camera matrix. Secondary rays are defined as $r_s(\\\\lambda) = o_s + \\\\lambda d_s$, where $o_s = x_p$ and $d_s = (l - x_p) / |l - x_p|$. We assume that the position of $l$ is known by using standard time-of-flight techniques [4]. Consistent with NeRF literature, all equations are expressed with respect to a single pixel measurement.\\n\\n#### 3.2.1 Rendering Primary Rays\\n\\nThe goal of rendering along the primary ray is to compute the two-bounce time-of-flight $t_{peak} = d/c$ by determining the depth $d_3$ of $x_p$. Once the location of $x_p$ is known, $d_1$ and $d_2$ can subsequently be computed because $l$, $x_s$, and $x_l$ are already known (Eq. (2)). First, the MLP is queried at $P$ sampled points along the primary ray $r_p$ between the near plane and far plane to output densities $\\\\sigma_1, ..., \\\\sigma_P$. The depth along the ray can be computed from the densities as\\n\\n$$\\\\hat{d}_3(r_p) = \\\\sum_{i=1}^{N} T_i \\\\alpha_i t_i$$\\n\\nwhere\\n\\n$$T_i = i - 1 \\\\sum_{j=1}^{i-1} 1 - \\\\alpha_j$$\\n\\nand\\n\\n$$\\\\alpha_i = 1 - e^{-\\\\sigma_i \\\\delta_i}$$\\n\\nwhere $\\\\delta_i = t_i - t_{i-1}$ is the distance between two samples along a ray. This equation can be interpreted as a discretized expectation integral, where the product $T_i \\\\alpha_i$ is the probability that the ray terminates exactly at $t_i$ (i.e. $d_3 = t_i$). $T_i$ is the transmittance a distance $t_i$ along the ray and models the probability that the ray is not terminated before arriving at $t_i$. $\\\\alpha_i$ denotes the probability of the ray terminating at $t_i$ and is commonly used in graphics for alpha compositing.\\n\\n#### 3.2.2 Rendering Secondary Rays\\n\\nThe goal of rendering secondary rays is to determine if $x_p$ lies in shadow or not. Every primary ray has a corresponding secondary ray, which is determined by (1) computing the depth $d_3$ along the primary ray and (2) connecting the estimated $x_p$ to the virtual light source $l$. The secondary ray connects the virtual source $l$ and the virtual detector $x_p$. Intuitively, if $x_p$ lies in shadow, density along the secondary ray will be high. Otherwise, the density will remain low. The probability that $x_p$ does not lie in shadow is\\n\\n$$p_{\\\\text{shadow}} = \\\\prod_{j=1}^{N-1} 1 - \\\\alpha_j$$\\n\\nThe product integral is effectively the transmittance along the secondary ray, where a low transmittance indicates $x_p$ lies in shadow. Note that, unlike the primary rays, the near and far planes of the secondary rays are known; the near plane is defined as $\\\\lambda_n = 0$ and the far plane as $\\\\lambda_f = d_2$, enabling these rays to be rendered more efficiently.\"}"}
{"id": "CVPR-2024-1865", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2.3 Synthesizing Transient Measurements\\n\\nUsing the synthesized two-bounce time-of-flight and shadow measurements, we can compute a loss based on the input transient measurement. We assume that \\\\( l, x_s, \\\\) and \\\\( x_l \\\\) are known. In addition, we assume that we have a binary mask \\\\( m \\\\in \\\\mathbb{R}^{N_u \\\\times N_v} \\\\) for each measurement \\\\( i \\\\) that segments the transient image into shadowed and unshadowed pixels.\\n\\nDetails on how to compute these quantities using the raw transient measurements are further explained in Sec. 3.3.\\n\\n**Distance Loss.**\\n\\nThe distance loss measures the accuracy of the synthesized two-bounce time of flight. Recall that rendering the primary ray enables estimation of the two-bounce time-of-flight using Eq. (2) and \\\\( x_p \\\\) obtained from Eq. (3). The distance loss is expressed as\\n\\n\\\\[\\nL_{\\\\text{primary}} = \\\\| t_{\\\\text{peak}} - \\\\hat{t}_{\\\\text{peak}} \\\\|_2^2, \\\\tag{6}\\n\\\\]\\n\\nwhere \\\\( t_{\\\\text{peak}} \\\\) is the time of flight observed in the transient measurement, and \\\\( \\\\hat{t}_{\\\\text{peak}} \\\\) is the two-bounce time-of-flight predicted by the NeRF rendering algorithm. Note that the distance loss is only computed on unshadowed pixels because a two-bounce signal will not exist for a shadowed pixel. However, we would still be able to estimate scene depth for these shadowed pixels because it is unlikely that the pixel will be shadowed in all \\\\( N \\\\) images of \\\\( I \\\\) due to illumination diversity.\\n\\n**Shadow Loss.**\\n\\nThe shadow loss determines if \\\\( x_p \\\\) is correctly classified as a shadowed or unshadowed pixel based on the rendered value \\\\( p_{\\\\text{shadow}} \\\\). The shadow loss is computed using the output rendering from the secondary ray in Eq. (5). The shadow loss is expressed as\\n\\n\\\\[\\nL_{\\\\text{secondary}} = \\\\| s - \\\\hat{p}_{\\\\text{shadow}} \\\\|_2^2, \\\\tag{7}\\n\\\\]\\n\\nwhere \\\\( s \\\\in \\\\{0, 1\\\\} \\\\) is a binary value from \\\\( m \\\\) indicating whether the transient measurement observed a shadow at the pixel. Unlike the distance loss, the shadow loss is computed for all pixels, shadowed and unshadowed.\\n\\n**Combined Loss Function.**\\n\\nThe final loss function can be expressed as a weighted sum of the distance and shadow loss\\n\\n\\\\[\\nL = L_{\\\\text{primary}} + \\\\beta L_{\\\\text{secondary}}, \\\\tag{8}\\n\\\\]\\n\\nwhere \\\\( \\\\beta \\\\) is a hyperparameter. Once the MLP is trained on this loss function, the predicted volume density can be extracted by densely sampling 3D scene points and querying the MLP at these points. The resulting densities can be used to render a depth map from any viewpoint or to generate a 3D mesh with marching cubes [22]. The loss, while simple in form, enables reconstruction of both the visible and occluded scene using only physically-based measurements, without data priors.\\n\\n3.3. Implementation Details\\n\\n**Data Pre-Processing.**\\n\\nOur method requires five inputs per pixel: (1) sensor location \\\\( o_p = x_s \\\\) and ray direction \\\\( d_p \\\\), (2) laser location \\\\( x_l \\\\), (3) distance from the laser to the virtual source \\\\( \\\\| l - x_l \\\\| \\\\), (4) two-bounce time of flight \\\\( t_{\\\\text{peak}} \\\\), and (5) if the pixel is in shadow. (1) can be computed using camera matrices and (2) is assumed to be calibrated. We use signal processing to extract (3\u20135). We compute time-of-flight with a template of the laser pulse shape as a match filter, and compute the cross-correlation of the match filter with the histogram at every pixel. The peak of the cross-correlation yields (4) and the maximum value of the cross-correlation yields the confidence that a pulse was measured. We filter out one-bounce returns by finding rays that are close to parallel to \\\\( l \\\\), and setting the corresponding histogram intensities to zero. Then, (5) can be computed by thresholding the confidence map to yield the binary shadow mask. We use a threshold of 0.15 in all simulated experiments, except those with ambient light, which require further threshold finetuning. (3) can be computed by determining the pixel with a one-bounce return and computing the distance as\\n\\n\\\\[\\nd_1 = \\\\frac{c t_1 B}{2},\\n\\\\]\\n\\nwhere \\\\( t_1 B \\\\) is one-bounce time-of-flight, since the laser and sensor are roughly co-located in our experiments.\\n\\n**Training.**\\n\\nFor the first 25,000 iterations of training, \\\\( \\\\beta \\\\) is set to 0. After 25,000 iterations, when an accurate initial estimate of the virtual detector \\\\( x_p \\\\) is obtained, we set \\\\( \\\\beta \\\\) to \\\\( 1/6,000 \\\\) in most experiments to encourage \\\\( L_{\\\\text{primary}} \\\\) to continue to improve after activating the shadow loss. The NeRF MLP is queried twice for each primary ray, once with coarse samples and once with fine samples, followed by coarse and fine sampling along the secondary rays.\\n\\n**Implementation.**\\n\\nPlatoNeRF is implemented in PyTorch [28] and is built off of vanilla NeRF [26]. As in NeRF, we use the Adam optimizer [15] and set an initial learning rate of \\\\( 5 \\\\times 10^{-4} \\\\), which decays exponentially over training.\\n\\n4. Experiments\\n\\nWe validate our method on the task of 3D reconstruction across several scenes. First, we introduce the simulated datasets that we make available to accelerate future work in learning-based methods for single-photon lidars. Then, we share our results, comparisons, and ablations on spatial and temporal resolution, ambient light, low-albedo backgrounds, non-planar backgrounds, and number of illumination points.\\n\\n4.1. Datasets\\n\\nWe validate our method on four simulated datasets and a real dataset, each described below.\\n\\n**Simulated Datasets.**\\n\\nWe create datasets of four scenes of a room with either a chair, bunny, dragon, or occluded bunny in a chair, shown in Fig. 4. The scenes are captured using a...\"}"}
{"id": "CVPR-2024-1865", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Depth evaluation. We compare PlatoNeRF to both lidar- and RGB-based single-view 3D reconstruction methods, BF Lidar [7] and $S^3$-NeRF [44], respectively. Depth metrics are reported (in m for L1 and dB for PSNR) both for the train view and 120 novel test views.\\n\\n| Scene       | Approach | Train View | Test Views |\\n|-------------|----------|------------|------------|\\n|              | L1 Depth | \u2193          | \u2193          |\\n| Chair Scene  | 0.0348   | 0.0233     | 0.0341     |\\n| Dragon Scene | 0.0602   | 0.0619     | 0.0682     |\\n| Bunny Scene  | 0.0222   | 0.0186     | 0.0185     |\\n| Occlusion Scene | 0.0339  | 0.0660     | 0.0601     |\\n\\nFigure 4. Qualitative Depth Results. We provide qualitative results for predicted depth on both train and novel test views, comparing our method, BF Lidar [7], and $S^3$-NeRF [44] to the ground truth across four scenes. Each method is trained from the one train view shown and reconstructs the entire scene.\\n\\n4.2. Results\\n\\nBaselines.\\n\\nWe compare our work with two methods, one that uses two-bounce lidar for single-view 3D reconstruction without learning and one that uses shadows measured by an RGB camera to train NeRF. We note that, to the best of our knowledge, we are the first to model two-bounce lidar with NeRF and so there are not direct comparisons for this task.\\n\\n1. Bounce-Flash Lidar: Our work is inspired by Bounce-Flash (BF) Lidar [7], which models two-bounce lidar analytically to estimate visible depth and occluded geometry from a single view, using geometric constraints and shadow carving [33], respectively. BF Lidar's output is one point cloud (PC) for visible and one for occluded geometry, which we combine for our comparisons.\\n\\n2. $S^3$-NeRF [44] is a recent method for learning neural scene representations using shadows. Using single-view RGB images captured under varying illumination, it trains a neural SDF model by exploiting shadow and shading information. A sphere is initialized at the origin where the object is assumed to be and known camera and light positions are used to model the scene's bidirectional reflectance distribution function. $S^3$-NeRF reconstructs both the object casting shadows and all other background scene geometry, making it a suitable comparison.\\n\\nMetrics.\\n\\nWe use L1 depth error to evaluate our method for 3D reconstruction, as done in past work [14, 19, 44]. In addition, we also report PSNR on reconstructed depth images. Since BF Lidar reconstructs a PC, we also include metrics on Chamfer distance. To convert the BF Lidar PC to depth for depth metrics, we increase the size of each point and project the depth to the test view, taking the smallest depth value along each ray. We find this produces better...\"}"}
{"id": "CVPR-2024-1865", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Real-World Results. (a) Captured scene (stars are illumination spots), (b) BF Lidar result, (c) PlatoNeRF result. Our method yields similar results as BF Lidar, with much fewer artifacts/holes.\\n\\nSimulated Results. Depth metrics for the training and 120 test views for our method and the baseline methods are reported in Tab. 1, and Chamfer distance is reported in Tab. 2. We find that our method consistently outperforms both BF Lidar and S^3-NeRF across both sets of metrics. Qualitative results are shown in Fig. 4. Our method is able to reconstruct the visible and occluded parts of the scene, providing accurate scale and absolute depth. Due to our use of an implicit representation, we achieve much smoother results than BF Lidar. However, because our method uses vanilla NeRF, there are small floaters visible in some results. In contrast, S^3-NeRF uses an SDF representation, reducing floaters, but resulting in overly smoothed results, e.g. the dragon\u2019s head.\\n\\nDespite there being no two-bounce signal for any illumination point in the area directly behind the object, both PlatoNeRF and S^3-NeRF interpolate in this area. Since BF Lidar is not learning-based, the lack of two-bounce signal in this area results in a hole. We do not include this area in our depth metrics. We note that PlatoNeRF also accurately extrapolates beyond the camera\u2019s field of view, in parts of the scene close to the camera. Finally, since RGB-based shape from shadow methods, such as S^3-NeRF, produce relative rather than absolute depth, we tried running iterative closest point [2] on S^3-NeRF\u2019s unprojected depth map, but found it does not improve S^3-NeRF\u2019s metrics. We also note that, as in the original work, we train S^3-NeRF with RGB images rendered with only one bounce, as we found it does not converge when trained on images rendered with multi-bounce.\\n\\nReal-World Results. We show real-world results in Fig. 5 and compare with BF Lidar. PlatoNeRF method achieves competitive performance. While BF Lidar produces many artifacts and holes, especially near edges and specular areas on the mannequin, PlatoNeRF produces far fewer, despite not modeling specular surfaces. In general, PlatoNeRF produces smoother depth, but small floaters are noticeable, especially in the nearby floor region, which is an area for future work.\\n\\n4.3. Ablations\\n\\nWe ablate our method to understand how it is affected by (1) reduced spatial and temporal resolution, (2) ambient light, (3) background albedo, (4) non-planar surfaces, (5) number of illumination points, and (6) shadow mask threshold. (1) is ablated in comparison to BF Lidar to highlight the benefits of our method over related lidar work. (2) and (3) are ablated in comparison to S^3-NeRF to highlight the fundamental advantages of using lidar compared to RGB when measuring shadows. Finally, (4), (5), and (6) are done only on PlatoNeRF. All ablations are done on the chair scene.\\n\\n4.3.1 Spatial and Temporal Resolution\\n\\nIn comparison to research-grade lidars, lidars on consumer devices have lower spatial and temporal resolution. We highlight an advantage of PlatoNeRF over BF Lidar, which is better generalization to low-resolution regimes due to our implicit representation. Quantitative and qualitative results are shown in Tab. 3 and Fig. 6 (rows 1\u20132), respectively.\\n\\nSpatial Resolution. To study spatial resolution, we downsample the number of pixels by four, eight, or sixteen, keeping field of view the same. Resulting spatial resolutions are 128 \u00d7 128, 64 \u00d7 64, and 32 \u00d7 32. We find that BF Lidar\u2019s accuracy degrades more significantly than PlatoNeRF since there is no mechanism to interpolate across missing pixels. Even at 32 \u00d7 32 resolution, PlatoNeRF accurately reconstructs the scene, albeit with slightly more artifacts. More in our supp.\\n\\nTemporal Resolution. We increase the bin size of our transients from 128 ps to 256 ps, 512 ps, and 1024 ps by integrating the intensities within each bin, resulting in fewer bins per transient, and thus less precise depth information. We find that depth error for BF Lidar degrades more than PlatoNeRF. Surfaces predicted by BF Lidar become rough.\"}"}
{"id": "CVPR-2024-1865", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and uneven due to ambiguity in the depth per pixel (see our supplement). In contrast, while PlatoNeRF's predicted depth becomes less accurate as bin size increases, it maintains smooth geometry and consistency between nearby pixels.\\n\\n4.3.2 Ambient Light and Low Albedo Backgrounds\\n\\nIn real-world settings, there may be high ambient light or low scene albedo, both of which make detection of shadows in RGB images challenging. In contrast, lidar-based methods, such as PlatoNeRF, are fundamentally more robust to these low signal-to-noise (SNR) and signal-to-background (SBR) scenarios. While ambient light and low albedo impact both RGB and lidar-based approaches, lidar-based approaches can mitigate their impact with time- and wavelength-gating. Ambient noise is uniformly distributed over time. Time gating enables suppression of ambient noise by only considering timing bins containing the pulse signal. Wavelength gating enables suppression of ambient noise in wavelengths not measured by the SPAD sensor, whereas RGB sensors have broadband sensitivity to visible wavelengths. We provide a detailed explanation of the gating principle in the supp. Quantitative results for these ablations are reported in Tab. 4.\\n\\nAmbient Light.\\nIncreasing ambient light increases the background and noise in the SBR or SNR term. We empirically validate that PlatoNeRF is able to handle ambient light in the scene, while \\\\( S_3 \\\\)-NeRF depth error increases. For this experiment, we render the scene with an added area light to train both methods. The area light intensity is the same in both RGB and lidar rendering. We do not model saturation or pileup distortion effects, since these are not significant indoors. PlatoNeRF's reconstruction under ambient light is shown in Fig. 6 (row 3) and \\\\( S_3 \\\\)-NeRF's is in the supp.\\n\\nAlbedo.\\nReducing albedo reduces the signal in the SNR and SBR term. However, unlike RGB sensors, albedo has a minor effect on lidar. To show its impact on PlatoNeRF and \\\\( S_3 \\\\)-NeRF, we reduce the albedo of all background surfaces. While the shadow is still discernible to the human eye (see supp.), the lowered albedo causes \\\\( S_3 \\\\)-NeRF's depth error to increase, while PlatoNeRF is unaffected. \\\\( S_3 \\\\)-NeRF produces accurate depth from the training view, but geometry in occluded regions is missing due to the weaker shadows.\\n\\n4.3.3 Other Ablations\\n\\nLastly, we study the impact of (a) non-planar background geometry, (b) number of illumination spots, and (c) shadow mask threshold on PlatoNeRF, with additional results for each in the supp. Qualitative results for (a) and (b) are shown in Fig. 6 (row 3). The non-planar scene contains curved background walls. PlatoNeRF's depth L1 is 7.75 cm and PSNR is 27.25 dB across test views, similar to previous experiments. For our illumination spot ablation, we reduce the number of illumination spots from 16 to 8, leading to L1 error of 9.12 cm and PSNR of 26.33 dB across test views, a small drop from when all 16 views are used. For our shadow mask threshold ablation, we vary the threshold between zero and one, resulting in L1 errors between 6.88 and 70 cm, highlighting the importance of the shadow mask threshold.\\n\\n5. Conclusion\\n\\nWe present a method for reconstructing lidar measurements with NeRF, which enables physically-accurate 3D geometry to be learned from a single view. We illuminate the scene with a pulsed laser and record the two-bounce time of flight. This data is used to supervise NeRF, which is trained to learn the optical path of two-bounce light. Our method outperforms related work in single-view 3D reconstruction, reconstructs scenes with fully occluded objects, and learns metric depth from any view. Lastly, we demonstrate generalization to varying sensor parameters and scene properties.\\n\\nLimitations.\\nOur method has a couple limitations. First, we only model Lambertian reflectance. Second, our method is built on top of vanilla NeRF, and, as a result, occasionally has floaters. However, our method is agnostic to the flavor of NeRF and can be integrated into others in the future.\\n\\nFuture Work.\\nWe believe this research is a promising direction as lidar becomes ubiquitous. Future directions enabled by PlatoNeRF include incorporating RGB and lidar with neural rendering to recover textured geometry, using data priors often used in single-view 3D reconstruction \\\\[46\\\\], handling specularities that cause ambiguities in the time of flight, and modeling more than two-bounces of light.\\n\\nAcknowledgements.\\nWe thank Akshat Dave for his paper feedback and insights into time of flight imaging and we thank Wenqi Yang for her guidance with \\\\( S_3 \\\\)-NeRF.\\n\\n14572\"}"}
