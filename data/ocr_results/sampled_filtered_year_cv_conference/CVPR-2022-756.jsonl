{"id": "CVPR-2022-756", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Marcella Astrid, Muhammad Zaigham Zaheer, Jae-Yeong Lee, and Seung-Ik Lee. Learning not to reconstruct anomalies. arXiv preprint arXiv:2110.09742, 2021.\\n\\n[2] Marcella Astrid, Muhammad Zaigham Zaheer, and Seung-Ik Lee. Synthetic temporal anomaly guided end-to-end video anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 207\u2013214, 2021.\\n\\n[3] Arslan Basharat, Alexei Gritai, and Mubarak Shah. Learning object motion patterns for anomaly detection and improved object detection. In 2008 IEEE Conference on Computer Vision and Pattern Recognition, pages 1\u20138. IEEE, 2008.\\n\\n[4] Tanmay Batra and Devi Parikh. Cooperative learning with visual attributes. arXiv preprint arXiv:1705.05512, 2017.\\n\\n[5] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Mvtec ad\u2014A comprehensive real-world dataset for unsupervised anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9592\u20139600, 2019.\\n\\n[6] Paul Bergmann, Michael Fauser, David Sattlegger, and Carsten Steger. Uninformed students: Student-teacher anomaly detection with discriminative latent embeddings. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\\n\\n[7] Raghavendra Chalapathy and Sanjay Chawla. Deep learning for anomaly detection: A survey. arXiv preprint arXiv:1901.03407, 2019.\\n\\n[8] Antoni Chan and Nuno Vasconcelos. Ucsd pedestrian dataset. IEEE Trans. on Pattern Analysis and Machine Intelligence (TPAMI), 30(5):909\u2013926, 2008.\\n\\n[9] Varun Chandola, Arindam Banerjee, and Vipin Kumar. Anomaly detection: A survey. ACM computing surveys (CSUR), 41(3):1\u201358, 2009.\\n\\n[10] MyeongAh Cho, Taeoh Kim, Ig-Jae Kim, and Sangyoun Lee. Unsupervised video anomaly detection via normalizing flows with implicit latent features. arXiv preprint arXiv:2010.07524, 2020.\\n\\n[11] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In Proceedings of the IEEE International Conference on Computer Vision, pages 1705\u20131714, 2019.\\n\\n[12] Dong Gong, Lingqiao Liu, Vuong Le, Budhaditya Saha, Moussa Reda Mansour, Svetha Venkatesh, and Anton van den Hengel. Memorizing normality to detect anomaly: Memory-augmented deep autoencoder for unsupervised anomaly detection. In The IEEE International Conference on Computer Vision (ICCV), October 2019.\\n\\n[13] Kensho Hara, Hirokatsu Kataoka, and Yutaka Satoh. Can spatiotemporal 3d cnns retrace the history of 2d cnns and imagenet? arXiv preprint, arXiv:1711.09577, 2017.\\n\\n[14] Mahmudul Hasan, Jonghyun Choi, Jan Neumann, Amit K Roy-Chowdhury, and Larry S Davis. Learning temporal regularity in video sequences. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 733\u2013742, 2016.\\n\\n[15] Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu, Tie-Yan Liu, and Wei-Ying Ma. Dual learning for machine translation. Advances in neural information processing systems, 29:820\u2013828, 2016.\\n\\n[16] Matth\u00e4us Heer, Janis Postels, Xiaoran Chen, Ender Konukoglu, and Shadi Albarqouni. The ood blind spot of unsupervised anomaly detection. In Medical Imaging with Deep Learning, 2021.\\n\\n[17] Ryota Hinami, Tao Mei, and Shin'ichi Satoh. Joint detection and recounting of abnormal events by learning deep generic knowledge. In Proceedings of the IEEE International Conference on Computer Vision, pages 3619\u20133627, 2017.\\n\\n[18] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\\n\\n[19] John Taylor Jewell, Vahid Reza Khazaie, and Yalda Mohsenzadeh. Oled: One-class learned encoder-decoder network with adversarial context masking for novelty detection. arXiv preprint arXiv:2103.14953, 2021.\\n\\n[20] Shunsuke Kamijo, Yasuyuki Matsushita, Katsushi Ikeuchi, and Masao Sakauchi. Traffic monitoring and accident detection at intersections. IEEE transactions on Intelligent transportation systems, 1(2):108\u2013118, 2000.\\n\\n[21] Jin-Hwa Kim, Do-Hyeong Kim, Saehoon Yi, and Taehoon Lee. Semi-orthogonal embedding for efficient unsupervised anomaly segmentation. arXiv preprint arXiv:2105.14737, 2021.\\n\\n[22] Sangmin Lee, Hak Gu Kim, and Yong Man Ro. Bman: bidirectional multi-scale aggregation networks for abnormal event detection. IEEE Transactions on Image Processing, 29:2395\u20132408, 2019.\\n\\n[23] Tangqing Li, Zheng Wang, Siying Liu, and Wen-Yan Lin. Deep unsupervised anomaly detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3636\u20133645, 2021.\\n\\n[24] Daochang Liu, Tingting Jiang, and Yizhou Wang. Completeness modeling and context separation for weakly supervised temporal action localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1298\u20131307, 2019.\\n\\n[25] Wen Liu, Weixin Luo, Dongze Lian, and Shenghua Gao. Future frame prediction for anomaly detection\u2014a new baseline. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6536\u20136545, 2018.\\n\\n[26] Ziyi Liu, Le Wang, Qilin Zhang, Zhanning Gao, Zhenxing Niu, Nanning Zheng, and Gang Hua. Weakly supervised temporal action localization through contrast based evaluation networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 3899\u20133908, 2019.\\n\\n[27] Cewu Lu, Jianping Shi, and Jiaya Jia. Abnormal event detection at 150 fps in matlab. In Proceedings of the IEEE international conference on computer vision, pages 2720\u20132727, 2013.\"}"}
{"id": "CVPR-2022-756", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Weixin Luo, Wen Liu, and Shenghua Gao. A revisit of sparse coding based anomaly detection in stacked RNN framework. In Proceedings of the IEEE International Conference on Computer Vision, pages 341\u2013349, 2017.\\n\\nSnehashis Majhi, Srijan Das, and Fran\u00e7ois Br\u00e9mont. Dam: Dissimilarity attention module for weakly-supervised video anomaly detection.\\n\\nGerard Medioni, Isaac Cohen, Fran\u00e7ois Br\u00e9mont, Somboon Hongeng, and Ramakant Nevatia. Event detection and analysis from video streams. IEEE Transactions on Pattern Analysis and Machine Intelligence, 23(8):873\u2013889, 2001.\\n\\nSadegh Mohammadi, Alessandro Perina, Hamed Kiani, and Vittorio Murino. Angry crowds: Detecting violent events in videos. In European Conference on Computer Vision, pages 3\u201318. Springer, 2016.\\n\\nAsim Munawar, Phongtharin Vinayavekhin, and Giovanni De Magistris. Limiting the reconstruction capability of generative neural network using negative learning. In 2017 IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP), pages 1\u20136. IEEE, 2017.\\n\\nSanath Narayan, Hisham Cholakkal, Fahad Shahbaz Khan, and Ling Shao. 3c-net: Category count and center loss for weakly-supervised action localization. In Proceedings of the IEEE International Conference on Computer Vision, pages 8679\u20138687, 2019.\\n\\nTrong-Nguyen Nguyen and Jean Meunier. Anomaly detection in video sequence with appearance-motion correspondence. In The IEEE International Conference on Computer Vision (ICCV), October 2019.\\n\\nTrong Nguyen Nguyen and Jean Meunier. Hybrid deep network for anomaly detection. arXiv preprint arXiv:1908.06347, 2019.\\n\\nHyunjong Park, Jongyoun Noh, and Bumsub Ham. Learning memory-guided normality for anomaly detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14372\u201314381, 2020.\\n\\nClaudio Piciarelli, Christian Micheloni, and Gian Luca Foresti. Trajectory-based anomalous event detection. IEEE Transactions on Circuits and Systems for Video Technology, 18(11):1544\u20131554, 2008.\\n\\nJanis Postels, Hermann Blum, Yannick Str\u00fcmpler, Cesar Cadena, Roland Siegwart, Luc Van Gool, and Federico Tombari. The hidden uncertainty in a neural networks activations. arXiv preprint arXiv:2012.03082, 2020.\\n\\nDidik Purwanto, Yie-Tarng Chen, and Wen-Hsien Fang. Dance with self-attention: A new look of conditional random fields on anomaly detection in videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 173\u2013183, October 2021.\\n\\nMahdyar Ravanbakhsh, Moin Nabi, Hossein Mousavi, Enver Sangineto, and Nicu Sebe. Plug-and-play CNN for crowd motion analysis: An application in abnormal event detection. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1689\u20131698. IEEE, 2018.\\n\\nMahdyar Ravanbakhsh, Moin Nabi, Enver Sangineto, Lucio Marcenaro, Carlo Regazzoni, and Nicu Sebe. Abnormal event detection in videos using generative adversarial nets. In 2017 IEEE International Conference on Image Processing (ICIP), pages 1577\u20131581. IEEE, 2017.\\n\\nHuamin Ren, Weifeng Liu, S\u00f8ren Ingvor Olsen, Sergio Escalera, and Thomas B Moeslund. Unsupervised behavior-specific dictionary learning for abnormal event detection. In BMVC, pages 28\u20131, 2015.\\n\\nMohammad Sabokrou, Mahmood Fathy, Guoying Zhao, and Ehsan Adeli. Deep end-to-end one-class classifier. IEEE Transactions on Neural Networks and Learning Systems, 2020.\\n\\nMohammad Sabokrou, Mohsen Fayyaz, Mahmood Fathy, and Reinhard Klette. Deep-cascade: Cascading 3D deep neural networks for fast anomaly detection and localization in crowded scenes. IEEE Transactions on Image Processing, 26(4):1992\u20132004, 2017.\\n\\nZheng Shou, Hang Gao, Lei Zhang, Kazuyuki Miyazawa, and Shih-Fu Chang. Autoloc: Weakly-supervised temporal action localization in untrimmed videos. In Proceedings of the European Conference on Computer Vision (ECCV), pages 154\u2013171, 2018.\\n\\nSorina Smeureanu, Radu Tudor Ionescu, Marius Popescu, and Bogdan Alexe. Deep appearance features for abnormal behavior detection in video. In International Conference on Image Analysis and Processing, pages 779\u2013789. Springer, 2017.\\n\\nFahad Sohrab, Jenni Raitoharju, Moncef Gabbouj, and Alexandros Iosifidis. Subspace support vector data description. In 2018 24th International Conference on Pattern Recognition (ICPR), pages 722\u2013727. IEEE, 2018.\\n\\nJessie James P Suarez and Prospero C Naval Jr. A survey on deep learning techniques for video anomaly detection. arXiv preprint arXiv:2009.14146, 2020.\\n\\nWaqas Sultani, Chen Chen, and Mubarak Shah. Real-world anomaly detection in surveillance videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6479\u20136488, 2018.\\n\\nWaqas Sultani and Jin Young Choi. Abnormal traffic detection using intelligent driver model. In 2010 20th International Conference on Pattern Recognition, pages 324\u2013327. IEEE, 2010.\\n\\nYu Tian, Guansong Pang, Yuanhong Chen, Rajvinder Singh, Johan W Verjans, and Gustavo Carneiro. Weakly-supervised video anomaly detection with robust temporal feature magnitude learning. arXiv preprint arXiv:2101.10030, 2021.\\n\\nLaurens van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of Machine Learning Research, 9(11), 2008.\\n\\nJue Wang and Anoop Cherian. Gods: Generalized one-class discriminative subspaces for anomaly detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8201\u20138211, 2019.\"}"}
{"id": "CVPR-2022-756", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jiang Wang, Yang Song, Thomas Leung, Chuck Rosenberg, Jingbin Wang, James Philbin, Bo Chen, and Ying Wu. Learning fine-grained image similarity with deep ranking. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1386\u20131393, 2014.\\n\\nLimin Wang, Yuanjun Xiong, Dahua Lin, and Luc Van Gool. Untrimmednets for weakly supervised action recognition and detection. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 4325\u20134334, 2017.\\n\\nXuanzhao Wang, Zhengping Che, Bo Jiang, Ning Xiao, Ke Yang, Jian Tang, Jieping Ye, Jingyu Wang, and Qi Qi. Robust unsupervised video anomaly detection by multipath frame prediction. IEEE Transactions on Neural Networks and Learning Systems, 2021.\\n\\nQi Wei, Yinhao Ren, Rui Hou, Bibo Shi, Joseph Y Lo, and Lawrence Carin. Anomaly detection for medical images based on a one-class classification. In Medical Imaging 2018: Computer-Aided Diagnosis, volume 10575, page 105751M. International Society for Optics and Photonics, 2018.\\n\\nPeng Wu, Jing Liu, Yujia Shi, Yujia Sun, Fangtao Shao, Zhaoyang Wu, and Zhiwei Yang. Not only look, but also listen: Learning multimodal violence detection under weak supervision. In European Conference on Computer Vision, pages 322\u2013339. Springer, 2020.\\n\\nYan Xia, Xudong Cao, Fang Wen, Gang Hua, and Jian Sun. Learning discriminative reconstructions for unsupervised outlier removal. In Proceedings of the IEEE International Conference on Computer Vision, pages 1511\u20131519, 2015.\\n\\nDan Xu, Elisa Ricci, Yan Yan, Jingkuan Song, and Nicu Sebe. Learning deep representations of appearance and motion for anomalous event detection. arXiv preprint arXiv:1510.01553, 2015.\\n\\nDan Xu, Yan Yan, Elisa Ricci, and Nicu Sebe. Detecting anomalous events in videos by learning deep representations of appearance and motion. Computer Vision and Image Understanding, 156:117\u2013127, 2017.\\n\\nTan Yu, Zhou Ren, Yuncheng Li, Enxu Yan, Ning Xu, and Junsong Yuan. Temporal structure mining for weakly supervised action detection. In Proceedings of the IEEE International Conference on Computer Vision, pages 5522\u20135531, 2019.\\n\\nMuhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid, and Seung-Ik Lee. Old is gold: Redefining the adversarially learned one-class classifier training paradigm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14183\u201314193, 2020.\\n\\nMuhammad Zaigham Zaheer, Jin-ha Lee, Marcella Astrid, Arif Mahmood, and Seung-Ik Lee. Cleaning label noise with clusters for minimally supervised anomaly detection. In Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2020.\\n\\nMuhammad Zaigham Zaheer, Arif Mahmood, Marcella Astrid, and Seung-Ik Lee. Stabilizing adversarially learned one-class novelty detection using pseudo anomalies, 2022.\\n\\nMuhammad Zaigham Zaheer, Arif Mahmood, Hochul Shin, and Seung-Ik Lee. Claws: Clustering assisted weakly supervised learning with normalcy suppression for anomalous event detection. In European Conference on Computer Vision, pages 358\u2013376. Springer, 2020.\\n\\nMuhammad Zaigham Zaheer, Arif Mahmood, Hochul Shin, and Seung-Ik Lee. Clustering aided weakly supervised training to detect anomalous events in surveillance videos, 2022.\\n\\nJiangong Zhang, Laiyun Qing, and Jun Miao. Temporal convolutional network with complementary inner bag loss for weakly supervised anomaly detection. In 2019 IEEE International Conference on Image Processing (ICIP), pages 4030\u20134034. IEEE, 2019.\\n\\nTianzhu Zhang, Hanqing Lu, and Stan Z Li. Learning semantic scene models by object classification and trajectory clustering. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 1940\u20131947. IEEE, 2009.\\n\\nYing Zhang, Huchuan Lu, Lihe Zhang, Xiang Ruan, and Shun Sakai. Video anomaly detection based on locality sensitive hashing filters. Pattern Recognition, 59:302\u2013311, 2016.\\n\\nYing Zhang, Tao Xiang, Timothy M Hospedales, and Huchuan Lu. Deep mutual learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4320\u20134328, 2018.\\n\\nJia-Xing Zhong, Nannan Li, Weijie Kong, Shan Liu, Thomas H Li, and Ge Li. Graph convolutional label noise cleaner: Train a plug-and-play action classifier for anomaly detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1237\u20131246, 2019.\\n\\nYi Zhu and Shawn Newsam. Motion-aware feature for improved video anomaly detection. arXiv preprint arXiv:1907.10211, 2019.\"}"}
{"id": "CVPR-2022-756", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"different feature vector. In order to extensively explore this concept, we propose the following different types of pseudo targets:\\n\\n1) All Ones Target: The original reconstruction target is replaced by a similar dimensional vector of all 1's.\\n\\n2) Random Normal Target: The original reconstruction target is replaced by a normal labeled feature vector selected arbitrarily.\\n\\n3) Random Gaussian Noise Target: The original reconstruction target is perturbed by adding Gaussian noise.\\n\\n4) No Negative Learning: No negative learning is applied to $G$. Instead only feature vectors pseudo-labeled as normal are used for the training of $G$. Extensive analysis of different pseudo targets is shown in Fig.5. We empirically observe that 'ones' as pseudo target yields more discriminative reconstruction capability, thus better differentiating normal and anomalous inputs. The loss function given by Eq. (1) is modified to include negative learning:\\n\\n$$L_G = \\\\frac{1}{b} \\\\sum_{q=1}^{b} ||t_{q,i,j} - \\\\hat{f}_{q,i,j}||^2,$$\\n\\nwhere the pseudo target $t_{q,i,j}$ is defined as:\\n\\n$$t_{q,i,j} = \\\\begin{cases} f_{q,i,j}, & \\\\text{if } l_{q,D} = 0 \\\\\\\\ 1 & \\\\in \\\\mathbb{R}^d, & \\\\text{if } l_{q,D} = 1 \\\\end{cases}$$\\n\\n3.3. Self-Supervised Pre-training\\n\\nThe proposed GCL is trained using unlabelled videos by utilizing the cooperation of $G$ and $D$. Since anomaly detection is an ill-defined problem, the lack of constraints may affect the convergence and the system may get stuck in a local minima. In order to improve the convergence, we explore to jump-start the training process by pre-training both $G$ and $D$. We empirically observe that using a pre-trained $G$ (based on Eq. (1)) is beneficial for the overall stability of the learning system and it also improves the convergence as well as the performance of the system (Section 4).\\n\\nAutoencoders are known to capture dominant representations of the training data [12, 64]. Despite the fact that anomalies are sparse and normal features are abundant in the training data, we experimentally observe that simply utilizing all training data to pre-train $G$ may not provide an effective jump-start. Using the fact that events in videos happen in temporal sequence and that anomalous frames are usually more eventful than the normal ones, we utilize temporal difference between the consecutive feature vectors as an estimator to initially clean the training dataset for the pre-training of $G$. That is, a feature vector $f_{t+1,i,j}$ will only be used for pre-training if $||f_{t+1,i,j} - f_{t,i,j}||^2 \\\\leq D_{th}$, where the superscripts $t$ and $t+1$ show the temporal order of features in a video and $D_{th}$ is the threshold. This approach does not guarantee complete removal of anomalous events however, it cleans the data for an effective initialization of $G$ to jump-start the training. Once $G$ is pre-trained, it is used to generate pseudo labels which are then used to pre-train the discriminator. In this step, the role of $G$ is similar to a lousy teacher because the generated pseudo-labels are quite noisy and the role of $D$ is like an efficient student because it learns to discriminate normal and anomalous features better even with noisy labels. In the following steps, both pre-trained $G$ and $D$ are plugged into our collaborative learning loop.\\n\\n3.4. Anomaly Scoring\\n\\nIn order to compute final anomaly score at test time, several configurations are possible, i.e., using reconstruction error of $G$ or prediction scores of $D$. We experimentally observed that $G$ remains relatively lousy while $D$ remains efficient across consecutive training iterations. Therefore for simplicity, unless stated otherwise, all results reported in this work are computed using the predictions of $D$.\\n\\n4. Experiments\\n\\nIn this section, we first provide experimental details, then draw comparisons with the existing SOTA methods, and finally study different components of our GCL approach.\\n\\nDatasets.\\n\\nUCF-Crime (UCFC) dataset contains 13 different categories of real-world anomalous events which were captured by CCTV surveillance cameras spanning 128 hours [50]. This dataset is complex because of the unconstrained backgrounds. The training split contains 810 abnormal and 800 normal videos, while the testing split has 140 anomalous and 150 normal videos. In training split, video-level labels are provided while in test split frame-level binary labels are provided. In our unsupervised setting, we discard the training-split labels and train the proposed GCL using unlabelled training videos.\\n\\nShanghaiTech contains staged anomalous events captured in a university campus at 13 different locations spanning 437 videos. This dataset was originally proposed for OCC with only normal videos provided for training. Later, Zhong et al. [74] reorganized this dataset to facilitate training of weakly-supervised algorithms. Normal and anomalous videos were mixed in both the training and the testing splits. The new training split contains 63 anomalous and 175 normal videos whereas, the new testing split contains 44 anomalous and 155 normal videos. In order to train our proposed GCL, we follow the latter split both for training and testing, without using training split video labels.\\n\\nEvaluation Measures.\\n\\nFollowing the existing methods [14, 27, 50, 74], we use area under ROC curve (AUC) for evaluation and comparisons. AUC is computed based on frame-level annotations of the test videos in both datasets.\\n\\nImplementation Details.\\n\\nTo demonstrate the concept of cooperative learning in its true essence, we select fairly simple architectures, without any bells and whistles, as our $G$ and $D$ networks. Architectures of $G$ and $D$ are set as $\\\\text{FC}[2048, 1024, 512, 256, 512, 1024, 2048]$ and $\\\\text{FC}[2048, 14748]$.\"}"}
{"id": "CVPR-2022-756", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Distribution of scores predicted on the test split of UCF-crime dataset by (a) AE trained on all training data, (b) \\\\( G \\\\) trained in GCLB, and (c) \\\\( D \\\\) trained in GCLB. Although \\\\( G \\\\) and \\\\( D \\\\) are trained cooperatively, \\\\( D \\\\) being more robust to noise, demonstrates superior discrimination between normal and anomalous examples.\\n\\nWe train both networks using RMSprop optimizer with a learning rate of 0.00002, momentum 0.60, for 15 epochs on training data with batch size 8192. Thresholds for pseudo label generation are data driven. For \\\\( G \\\\) pseudo labels \\\\( L_{G}^{th} = \\\\mu_{R} + \\\\sigma_{R} \\\\) where \\\\( \\\\mu_{R} \\\\) and \\\\( \\\\sigma_{R} \\\\) are the mean and standard deviation of reconstruction error as given by Eq. (1) for each batch. For \\\\( D \\\\), \\\\( L_{D}^{th} = \\\\mu_{P} + 0.1 \\\\sigma_{P} \\\\), where \\\\( \\\\mu_{P} \\\\) and \\\\( \\\\sigma_{P} \\\\) are the mean and standard deviations of the probabilities \\\\( \\\\hat{p}_{q_{i,j}} \\\\) generated by \\\\( D \\\\) for each batch. The value of \\\\( D_{th} = 0.70 \\\\) is used in unsupervised pre-training. As feature extractor, we use a popular framework ResNext3d proposed by Hara et al.\\\\[13\\\\] in default mode. Segment size for feature extraction is set to 16 non-overlapping frames.\\n\\nAll experiments are performed on NVIDIA RTX 2070 with Intel Core i7, 8th gen and 16GB RAM.\\n\\n4.1. Comparisons with State-Of-The-Art (SOTA)\\n\\nThe proposed GCL approach is trained in an unsupervised fashion without using any kind of annotations. GCL with no pre-training, GCLB, is considered as the baseline. In addition, GCL with pre-training, GCLPT, GCL combined with OCC based pre-trained autoencoder, GCLOCC, and GCL weakly-supervised, GCLWS, are also trained and evaluated on UCFC and ShanghaiTech datasets.\\n\\nAs seen in Table 1, on UCFC dataset, the proposed GCLB obtained an overall AUC of 68.17% which is 11.85% higher than the Autoencoder (AEAllData) trained on complete training data including both normal and anomalous training samples in an unsupervised fashion. Histogram plotted over reconstructions in Fig. 4(a) also provides insights that AEAllData is not able to learn discriminative reconstruction. Also in the GCL, the discrimination ability of \\\\( D \\\\) (Fig. 4(c)) is much enhanced than \\\\( G \\\\) (Fig. 4(b)).\"}"}
{"id": "CVPR-2022-756", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Convergence of $G$ and $D$ in GCL with/without Negative Learning (NL). We test different pseudo reconstruction targets in NL. Best performance is observed for 'ones' NL target.\\n\\nIt is interesting to note that GCL OCC yields comparable performance to the approach proposed by Sultani et al. which utilizes video-level labels for training. Although GCL aims at unsupervised cooperative learning, we also extended it to incorporate weak-supervision. The results for this version are reported as GCL WS in Table 1. Despite using fairly simple networks of $G$ and $D$ without any bells and whistles, GCL WS obtains comparable results to several existing weakly-supervised learning methods.\\n\\nWe also evaluated our approach on ShanghaiTech dataset and the results are compared with the existing SOTA methods in Table 1. On this dataset, our proposed GCL B obtained 72.41% AUC which is more than 10% better than AE AllData showing the effectiveness of the baseline approach. GCL PT obtained 78.93% AUC which is 6.5% better than GCL B demonstrating the importance of unsupervised pre-training for jump-start. Despite unsupervised, GCL PT outperformed all existing OCC methods.\\n\\n4.2. Ablation Study and Analysis\\n\\nAnalysis of different components, design choices, qualitative results and inclusion of supervision are discussed next.\\n\\nComponent Wise Ablation Study. A detailed ablation analysis of GCL with various design choices is reported in Table 2 on UCFC. It can be seen that an autoencoder trained using all training dataset without any supervision AE AllData yields a significantly low performance of 56.32% compared to the one trained on clean normal data in OCC setting AE OCC (65.76%). Training an autoencoder AE TD with our proposed frame temporal difference based unsupervised pre-processing brings the performance closer to AE OCC, which demonstrates effectiveness of our pre-processing approach. Using negative learning enhances the overall performance of GCL B over the counterpart training without negative learning GCL w/oNL by 3.94%. Our complete unsupervised system GCL PT which utilizes negative learning and unsupervised pre-training enhances the overall performance to 71.04%. In addition, in GCL OCC adding one-class supervision improves this performance even further by demonstrating an AUC of 74.20%. This also revalidates our claim of the overall benefit that OCC may have over a completely unsupervised setting, making them different from the unsupervised approaches.\\n\\nEvaluating Negative Learning (NL) Approaches. Experiments are performed with and without NL in GCL framework on UCFC. For the case of NL in GCL B the performances of three different pseudo targets, 'ones' for all ones, 'replace' with random normal, and 'Gaussian' with $\\\\mu=0$ and $\\\\sigma=\\\\{1.5, 6.0\\\\}$ (Section 3.2.5) are compared in Fig. 5. We observe that the 'ones' pseudo target works better than the other approaches. Gaussian perturbations with $\\\\sigma=1.5$ behaved almost identical to the model without any NL (GCL w/oNL). With $\\\\sigma=6.0$, the performance improves but still lower than the 'ones'. It may be attributed to the fixed pseudo target which helps in consistent learning of GCL framework resulting in better discrimination.\\n\\nTo further explore the significance of NL, we provide tSNE visualizations of the reconstructions produced by AE AllData, GCL w/oNL, and GCL B, in Fig. 6. Both AEs, with and without NL, demonstrate superior discrimination over AE AllData. Also, GCL B (Fig. 6(c)), the anomalous features form a distinct cluster which shows that the use of NL is effective than using no NL.\\n\\nQualitative Analysis. A step by step evolution of our GCL approach is visualized in Fig. 7. As the training proceeds, GCL B learns to predict true anomalous portions within the video in a completely unsupervised fashion. Fig. 8 shows final anomaly scores predicted by our GCL PT on four different videos taken from UCFC. In Fig. 8(d), some normal portions are also predicted as anomalous. Inspection of this video reveals that the beginning and ending frames contain floating text, which is unusual in the training data.\"}"}
{"id": "CVPR-2022-756", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7. Evolution of the frame-level anomaly scores in GCL framework during training. Note that our unsupervised approach successfully produces significantly higher scores in the anomalous portions whereas lower scores in the normal portions. Anomaly ground truth is shown as red boxes, and the video is Explosion013 from UCFC. Interestingly, the anomaly score stays higher after the anomalous ground truth is over which is essentially due to aftermath of the explosion that network figures to be anomalous.\\n\\nFigure 8. Anomaly scores by GCL PT are low in normal regions and high in abnormal regions on four different UCFC videos.\\n\\nGCLB and GCLPT obtain an average AUC of $67.09 \\\\pm 0.65$ and $70.13 \\\\pm 0.52$. GCLPT not only improves the overall performance but also reduces the variation over different seeds, demonstrating better convergence.\\n\\nOn Adding Weak Supervision. In a series of experiments using UCFC, weak video-level labels are infused to the GCL ranging from 33% to 100%. Fig. 9 shows that both G and D benefit from the added supervision. Noticeably, there is a significant jump in AUC% upon only providing 33% videos with weak labels which demonstrates that even minimal supervision is quite beneficial for the proposed GCL.\\n\\nOn Training G Using its Own Pseudo Labels. Under this configuration, we observed an AUC of 62.28% on UCFC using ResNext3d features. Although better than 56.32% of AEAllData, it is still lower than 71.04% our GCLPT. This demonstrates that the usage of D for pseudo labeling is critical due to its robust learning under noisy labels [67, 69].\\n\\nOn Using Soft Labels. We explore the usage of soft labels for training D by skipping thresholding (eq. (2)). It resulted in an AUC of 63.58% on UCFC using ResNext3d features, which is almost identical to AE in Table 2. It is because without threshold, D simply starts replicating the output of G, thereby demonstrating identical performance.\\n\\nLimitations. Our unsupervised setting enables an anomaly detection system to start detecting abnormalities just based on the observed data without any human intervention. In case there is no abnormal event so far, the system may consider the rare normal events as abnormal. However, if a system remains operational for a significant time, the probability of having no abnormal event will be rather very small.\\n\\n5. Conclusion\\nWe proposed an unsupervised anomaly detection approach (GCL) using unlabeled training videos, which can be deployed without providing any manual annotations. GCL shows excellent performance on two public benchmark datasets with varying supervision levels, including no-supervision, one class and weak-supervision. Finally, we discussed the limitations of unsupervised settings, i.e., the assumption of having anomalies in the training dataset. However, this is more realistic than OCC methods as it is natural to have anomalies in the real-world scenarios.\\n\\nAcknowledgements: This work was supported by ETRI grant (No. 21YS2700, Development of learning model and data generation/augmentation techniques for data efficient deep learning, 50%) and also by ETRI grant funded by Ulsan Metropolitan City (22AS1600, the development of intelligentization technology for the main industry for manufacturing innovation and Human-mobile-space autonomous collaboration intelligence technology development in industrial sites, 50%). We also acknowledge the generous assistance of Max Plank ETH Center for Learning Systems.\"}"}
{"id": "CVPR-2022-756", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generative Cooperative Learning for Unsupervised Video Anomaly Detection\\n\\nM. Zaigham Zaheer, Arif Mahmood, M. Haris Khan, Mattia Segu, Fisher Yu, Seung-Ik Lee\\n\\nAbstract\\nVideo anomaly detection is well investigated in weakly-supervised and one-class classification (OCC) settings. However, unsupervised video anomaly detection methods are quite sparse, likely because anomalies are less frequent in occurrence and usually not well-defined, which when coupled with the absence of ground truth supervision, could adversely affect the performance of the learning algorithms. This problem is challenging yet rewarding as it can completely eradicate the costs of obtaining laborious annotations and enable such systems to be deployed without human intervention. To this end, we propose a novel unsupervised Generative Cooperative Learning (GCL) approach for video anomaly detection that exploits the low frequency of anomalies towards building a cross-supervision between a generator and a discriminator. In essence, both networks get trained in a cooperative fashion, thereby allowing unsupervised learning. We conduct extensive experiments on two large-scale video anomaly detection datasets, UCF crime and ShanghaiTech. Consistent improvement over the existing state-of-the-art unsupervised and OCC methods corroborate the effectiveness of our approach.\\n\\n1. Introduction\\nIn the real world, learning-based anomaly detection tasks are extremely challenging mainly because of the rare occurrence of such events. The challenge further exacerbates owing to the unconstrained nature of these events. Obtaining sufficient anomaly examples is thus quite cumbersome, while one may safely assume that an exhaustive set, particularly required for training fully-supervised models, will never be collected. To make learning tractable, anomalies have often been attributed as significant deviations from the normal data. Therefore, a popular approach towards anomaly detection is to train a one-class classifier which learns the dominant data representations using only normal training examples [14, 17, 25, 28, 41, 42, 45, 47, 60, 64, 66, 72].\\n\\nA noticeable drawback of one-class classification (OCC) methods is the limited availability of the normal training data, not capturing all normalcy variations [9]. In addition, the OCC approaches are usually unsuitable for complex problems with diverse multiple classes and a wide range of dynamic situations often found in video surveillance. In such cases, an unseen normal activity may deviate significantly from the learned normal representations to be predicted as anomalous, resulting in false alarms [14, 67].\\n\\nRecently, weakly supervised anomaly detection methods have gained significant popularity [24, 26, 34, 46, 56, 63] that reduce the cost of obtaining manual fine-grained annotations by employing video-level labels [50, 65, 67, 69, 74]. Specifically, a video is labeled as anomalous if some of its contents are anomalous and normal if all of its contents are normal, requiring manual inspection of the full videos. Although such annotations are relatively cost-effective, yet remain impractical in many real-world applications. There is a plethora of video data, specifically raw footage, that can be leveraged for anomaly detection training if no annotation cost is incurred. Unfortunately, to the best of our knowledge, there are hardly any notable attempts in leveraging the unlabelled training data for video anomaly detection.\\n\\nIn this work, we explore unsupervised mode for video anomaly detection that is certainly more challenging than fully, weakly or one-class supervision (Fig. 1). However, it is also more rewarding due to minimal assumptions and hence will encourage the development of novel and more practical algorithms. Note that, the term 'unsupervised' in literature often refers to OCC approaches which assume all normal training data [11, 37, 64, 66]. However, it refers to GCL in this work.\"}"}
{"id": "CVPR-2022-756", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ders the overall learning problem partially supervised \\\\[19\\\\]. In approaching unsupervised anomaly detection in videos, we exploit the simple facts that videos are information-rich compared to still images and anomalies are less frequent than the normal happenings \\\\[8, 29, 51, 67\\\\], and attempt to leverage such domain knowledge in a structured manner. To this end, we propose a Generative Cooperative Learning (GCL) method which takes unlabelled videos as input and learns to predict frame-level anomaly score predictions as output. The proposed GCL comprises two key components, a generator and a discriminator, which get trained in a mutually cooperative manner towards improving the anomaly detection performance. The generator not only reconstructs the abundantly available normal representations but also distorts the possible high-confidence anomalous representations by using a novel negative learning (NL) approach. The discriminator instead estimates the probability of an instance to be anomalous. For unsupervised anomaly detection, we create pseudo-labels from the generator and use these to train the discriminator. In the following step, we create pseudo-labels from the trained version of discriminator and then use these to improve the generator. The overall system is trained in an alternate fashion where, in each iteration, both the generator and the discriminator get improved with mutual cooperation.\\n\\nContributions. We propose an anomaly detection approach capable of localizing anomalous events in complex surveillance scenarios without requiring labelled training data. To the best of our knowledge, our method is the first rigorous attempt tackling the surveillance videos anomaly detection in a fully unsupervised mode. A novel Generative Cooperative Learning (GCL) framework is proposed that comprises a generator, a discriminator, and cross-supervision. The generator network is forced not to reconstruct anomalies by using a novel negative learning approach. Extensive experiments on two large-scale complex anomalous event detection datasets, UCF-Crime and ShanghaiTech, show that our method provides visible gains over the baselines and several existing unsupervised as well as OCC methods.\\n\\n2. Related Work\\n\\nAnomaly detection is a widely studied problem both in the image \\\\[7, 16, 39\\\\] and the video \\\\[49, 50, 64, 67, 69\\\\] domains. We review different supervision modes for video anomaly detection and mutual learning strategies.\\n\\nAnomaly Detection as One-Class Classification (OCC). OCC approaches have found their way in a wide range of anomaly detection problems including, medical diagnosis \\\\[58\\\\], cyber security \\\\[11\\\\], surveillance security systems \\\\[20, 29, 32, 64\\\\], and industrial inspection \\\\[5\\\\]. Some of these approaches use hand-crafted features \\\\[3, 31, 38, 55, 71\\\\], while others use deep features extracted using pre-trained models \\\\[42, 47\\\\]. With the advent of generative models, many approaches proposed variants of such networks to learn normal data representations \\\\[12, 35, 36, 43\u201345, 61, 62, 64\\\\]. OCC approaches find it challenging to avoid well-reconstruction of anomalous test inputs. This problem is attributed to the fact that since OCC approaches only use normal class data while training, an ineffective classifier boundary may be achieved which is limited in enclosing normal data while excluding anomalies \\\\[64\\\\]. In an attempt to address this limitation, some researchers recently proposed pseudo-supervised methods in which pseudo-anomaly instances are generated using normal training data \\\\[1, 64\\\\].\\n\\nWeakly Supervised (WS) Anomaly Detection. Video-level binary annotations are used to train WS classifiers capable of predicting frame-level anomaly scores \\\\[40, 50, 52, 65, 67, 69, 74\\\\]. Video-level labels are provided in such a way that a normal labeled video is completely normal whereas an anomalous labeled video contains both normal and anomalous contents without any information about the temporal whereabouts (Fig. 1).\\n\\nUnsupervised Anomaly Detection. Anomaly detection methods using unlabelled training data are quite sparse in literature. According to the nomenclature shown in Fig. 1, most unsupervised methods in the literature actually fall in the category of OCC. For instance, MVTecAD \\\\[5\\\\] benchmark ensures the training data to be only normal, therefore its evaluation protocol is OCC and the methods inheriting this assumption are also essentially one-class classifiers \\\\[6, 12\\\\]. In contrast to these algorithms, our proposed GCL approach is capable of learning from unlabelled training data without assuming any normalcy. The training data in the form of videos conform to several important attributes regarding anomaly detection, such as, anomalies are less frequent than normal events and events are often temporally consistent. We derive our motivation from these clues to carry out the training in a completely unsupervised fashion.\\n\\nTeacher Student Networks. Our proposed GCL shares some similarities with the Teacher Student (TS) frameworks for knowledge distillation \\\\[18\\\\]. GCL is different from TS framework mainly because its aim is not knowledge distillation. Also our generator generates noisy labels while our discriminator, being relatively robust to noise, cleans these labels which is not the case in TS framework.\\n\\nMutual Learning (ML). The GCL framework also shares similarities with the ML algorithms \\\\[73\\\\]. However, the two components of GCL learn different types of information and are trained with cross-supervision in contrast to the supervised learning used by the ML algorithms. Further in GCL, the output of each network is passed through a threshold process to produce pseudo-labels. In ML, the cohort learns to match the distributions of each member while in GCL each member tries to learn from the pseudo-labels generated by the other. A mutual learning of a cohort in unsupervised mode using unlabelled training data is unexplored yet.\"}"}
{"id": "CVPR-2022-756", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Proposed Generative Cooperative Learning (GCL) algorithm introduces cross-supervision for training a Generator $G$ and a Discriminator $D$. The pseudo-labels produced by $G$ are used to compute the $D$ loss and likewise, the pseudo-labels produced by $D$ are utilized to compute the loss of $G$. Both $G$ and $D$ are trained iteratively from unlabelled training data for anomalous events detection.\\n\\nDual Learning. It is also a related method in which two language translation models interactively teach each other [15]. However, the external supervision is provided using pre-trained unconditional language expert models which check the quality of translations. This way, different models have different learning tasks whereas in our proposed GCL approach the learning tasks are identical. Another variant of Cooperative Learning [4] has been previously proposed to learn multiple models jointly for the same task across different domains. For instance, object recognition is formulated by training a model on RGB images and another model on depth images which then communicate the domain invariant object attributes. Whereas, in our GCL approach both models address the same task in the same domain.\\n\\n3. Method\\n\\nOur proposed Generative Cooperative Learning approach for Anomaly Detection (GCL) comprises a feature extractor, a generator network, a discriminator network, and two pseudo-label generators. Fig. 2 shows the overall architecture. Each of the components are discussed next.\\n\\n3.1. Training Data Organization\\n\\nTo minimize the computational complexity and to reduce the training time of GCL, similar to the existing SOTA [50, 52, 65, 67, 69, 74], we utilize a deep feature extractor to convert video data into compact features. All input videos are arranged as segments, features of which are then extracted. Furthermore, these features are randomly arranged as batches. In each iteration a randomly selected batch is used to train the GCL model (Fig. 2). Formally, given a training dataset of $n$ videos, every video is partitioned into non-overlapping segments $S(i,j)$ of $p$ frames each, where $i \\\\in [1,n]$ is the video index and $j \\\\in [1,m_i]$ is the segment index. The segment size $p$ is kept the same across all training and test videos of a dataset. For each $S(i,j)$, a feature vector $f(i,j) \\\\in \\\\mathbb{R}^d$ is computed as $f(i,j) = E(S(i,j))$ using the feature extractor $E(\\\\cdot)$.\\n\\nIn the existing weakly supervised anomaly detection approaches, each training iteration is carried out on one or more complete videos [50, 74]. Recently, CLAWS Net [67] proposed to extract several batches of temporally consistent features, each of which was then randomly input to the network. Such configuration serves the purpose of minimizing correlation between consecutive batches. In these existing approaches, it is important to maintain temporal order at batch or video level. However, in the proposed GCL approach we randomize the order of input features which removes both the intra-batch and inter-batch correlations.\\n\\n3.2. Generative Cooperative Learning\\n\\nOur proposed Generative Cooperative Learning (GCL) approach for anomaly detection consists of a generator $G$ which is an autoencoder (AE) and a discriminator $D$ which is a fully connected (FC) classifier. Both these models are trained in a cooperative fashion without using any data annotations. More specifically, we neither use the normal class annotations as in one class classification (OCC) approaches [12, 37, 54], nor binary annotations used by the weakly supervised anomaly detection systems [50, 67, 69, 74]. As discussed in Section 1, the intuition behind using an AE is that such models can somewhat capture the overall dominant data trends [12]. On the other hand, the FC classification network used as a discriminator is known to be efficient when provided with supervised, albeit noisy, training [67]. In order to carry out the training, first pseudo annotations created using $G$ are used to train $D$. In the next step, pseudo annotations created by using $D$ are used to train $G$.\"}"}
{"id": "CVPR-2022-756", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are used to improve $G$. Thus, each of the two models are\\ntrained by using the annotations created by the other model\\nin an alternate training fashion. The training configuration\\naims that the pseudo-labeling is improved over training it-\\nerations which consequently results in an improved overall\\nanomaly detection performance. Particular architecture de-\\ntails and several design choices are discussed next.\\n\\n3.2.1 Generator Network $G$ takes features as input and produces reconstructions of\\nthose features as output. Typically, $G$ is trained by minimiz-\\ning the reconstruction loss $L_r$ as:\\n\\n$$ L_r = \\\\frac{1}{b} \\\\sum_{q=1}^{b} L_q^G, $$\\n\\nwhere $f_{q,i,j}$ is a feature vector that is input to $G$ and $\\\\hat{f}_{q,i,j}$ is\\nthe corresponding reconstructed vector, $b$ is the batch size.\\n\\n3.2.2 Pseudo Labels from Generator In our proposed collaborative learning, pseudo labels from $G$ are created to train $D$. The labels are created by keep-\\ning in view the distribution of the reconstruction loss $L_q^G$ of\\neach instance $q$ over a batch. The main idea is to consider\\nfeature vectors resulting in higher loss values as anomalous\\nand those generating smaller loss values as normal. In or-\\nder to implement this intuition, one may consider using a\\nthreshold $L_{th}^G$ as:\\n\\n$$ l_q^G = \\\\begin{cases} \\n1, & \\\\text{if } L_q^G \\\\geq L_{th}^G \\\\\\\\\\n0, & \\\\text{otherwise} \\n\\\\end{cases}, $$\\n\\nWe have followed a simple approach for the $L_{th}^G$ selection\\nby considering a fixed percentage of the samples having\\nmaximum reconstruction error as anomalous. In the\\n$L_q^G$ histograms we empirically observed a bigger peak towards\\nminimum error and a smaller peak towards maximum error.\\nDue to the fact that the class boundaries often fall in low\\ndensity regions, error histograms are also an effective tool\\nfor the selection of appropriate $L_{th}^G$. Analysis of different\\nalternates for $L_{th}^G$ selection is given in the Supplementary.\\n\\n3.2.3 Discriminator Network The binary classification network used as the discriminator $D$ is trained using the pseudo annotations from $G$ by mini-\\nmizing the binary cross entropy loss over a batch $b$ as:\\n\\n$$ L_D = -\\\\frac{1}{b} \\\\sum_{q=1}^{b} l_q^G \\\\ln \\\\hat{l}_{q,i,j} + (1 - l_q^G) \\\\ln (1 - \\\\hat{l}_{q,i,j}), $$\\n\\nwhere $l_q^G \\\\in \\\\{0, 1\\\\}$ is the pseudo label generated by $G$ and $\\\\hat{l}_{q,i,j}$ is the output of $D$ when a feature vector $f_{q,i,j}$ is input.\\n\\n3.2.4 Pseudo Labels from Discriminator Pseudo labels from $D$ are used to improve the reconstruc-\\ntion discrimination capability of $G$. The output $\\\\hat{p}_{q,i,j}$ of $D$ is the probability of a feature vector $f_{q,i,j}$ to be anomalous.\\nTherefore, the features obtaining higher probability are con-\\nsidered as anomalous by using a threshold mechanism on\\nthe output $\\\\hat{p}_{q,i,j}$ of $D$. The annotations generated by $D$ are\\nthen used to fine tune $G$ in the next iteration.\\n\\n$$ l_q^D = \\\\begin{cases} \\n1, & \\\\text{if } \\\\hat{p}_{q,i,j} \\\\geq L_{th}^D \\\\\\\\\\n0, & \\\\text{otherwise} \\n\\\\end{cases}, $$\\n\\nwhere the threshold $L_{th}^D$ is computed the same way as the\\nthreshold $L_{th}^G$ is computed.\\n\\n3.2.5 Negative Learning of Generator Network Training of $G$ is carried out by using pseudo labels from $D$ by employing negative learning (NL). In order to increase\\nthe discrimination among reconstructions of normal and\\nanomalous inputs, $G$ is encouraged to poorly reconstruct the\\nsamples which have anomalous pseudo labels whereas, the\\nsamples having normal pseudo labels are aimed to be recon-\\nstructed as usual with minimum error.\\n\\nSome variants of NL have already been explored in the\\nliterature. For instance, Munawar et al. [33] and Astrid\\net al. [1] make the loss negative for a full batch of known\\nanomalous inputs. However, this configuration requires a\\nprior knowledge of the whole dataset and its labels. In the\\nproposed GCL approach, pseudo labels are generated iter-\\natively as the training proceeds, therefore it may encounter\\nboth normal and anomalous samples in the same batch. In\\naddition, instead of making the loss negative, we enforce\\nthe abnormal samples to be poorly reconstructed by using\\na pseudo reconstruction target. Therefore, as illustrated in\\nFig. 3, for each feature vector which is pseudo-labeled as\\nanomalous by $D$, its reconstruction target is replaced by a\"}"}
