{"id": "CVPR-2024-570", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DiffEditor: Boosting Accuracy and Flexibility on Diffusion-based Image Editing\\n\\nChong Mou1,3, Xintao Wang2, Jiechong Song1, Ying Shan2, Jian Zhang1,3*\\n\\n1 School of Electronic and Computer Engineering, Shenzhen Graduate School, Peking University\\n2 ARC Lab, Tencent PCG\\n3 Peking University Shenzhen Graduate School-Rabbitpre AIGC Joint Research Laboratory\\n\\n{eechongm, xintao.alpha}@gmail.com, yingsshan@tencent.com, {songjiechong, zhangjian.sz}@pku.edu.cn\\n\\nAbstract\\n\\nLarge-scale Text-to-Image (T2I) diffusion models have revolutionized image generation over the last few years. Although owning diverse and high-quality generation capabilities, translating these abilities to fine-grained image editing remains challenging. In this paper, we propose DiffEditor to rectify two weaknesses in existing diffusion-based image editing: (1) in complex scenarios, editing results often lack editing accuracy and exhibit unexpected artifacts; (2) lack of flexibility to harmonize editing operations, e.g., imagine new content. In our solution, we introduce image prompts in fine-grained image editing, cooperating with the text prompt to better describe the editing content. To increase the flexibility while maintaining content consistency, we locally combine stochastic differential equation (SDE) into the ordinary differential equation (ODE) sampling. In addition, we incorporate regional score-based gradient guidance and a time travel strategy into the diffusion sampling, further improving the editing quality. Extensive experiments demonstrate that our method can efficiently achieve state-of-the-art performance on various fine-grained image editing tasks, including editing within a single image (e.g., object moving, resizing, and content dragging) and across images (e.g., appearance replacing and object pasting). Our source code is released at https://github.com/MC-E/DragonDiffusion.\\n\\n1. Introduction\\n\\nText-to-image (T2I) diffusion models [29, 32, 33, 35] have become the mainstream of image generation, praised for their high-quality and diverse generation capability. The pre-trained T2I models can serve as a good generation prior and can be used in various ways, e.g., image editing. Since the excellent text-to-image ability, numerous diffusion-based image editing methods are implemented based on the text guidance [5, 6, 11\u201313, 16]. However, the generated results of T2I models are usually sensitive to the quality of text [37]. Therefore, text-guided image editing struggles to achieve fine-grained content manipulation. Recently, DragGAN [30] provides a user-friendly way...\"}"}
{"id": "CVPR-2024-570", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to manipulate the image content by point dragging. However, limited by the capacity of GAN [8] models, DragGAN cannot edit general images. Inspired by this interactive editing mode, DragDiff [39] and DragonDiff [28] are proposed based on the pre-trained T2I diffusion model [33]. Empowered by the diverse generation capabilities of the base model, they can perform fine-grained editing on general images. However, their editing process lacks flexibility, as shown in Fig. 2. Concretely, the image editing operation of transforming a lion from a closed mouth to a widely open mouth conflicts with the LORA [34] in DragDiff, resulting in failure. The visual cross-attention designed in DragonDiff also makes it struggle to imagine new content (e.g., mouth) that is not present in the source image, causing failure too. In addition, these two methods and most diffusion-based image editing methods employ ordinary differential equations (ODE) [41]. Although ODE can better maintain the consistency between the edited results and the source image, its determinacy also limits flexibility during the editing process. Compared to ODE, stochastic differential equations (SDE) [15] is a stochastic sampling process. Some works [48, 49] study the latent space of SDE for accurate image editing. Unlike these works, we aim to utilize the stochasticity in SDE to improve the flexibility of diffusion-based image editing, as shown in the last image of Fig. 2.\\n\\nAnother insight is that although DragDiff and DragonDiff utilize feature correspondence in the pre-trained T2I diffusion model to achieve fine-grained image editing, the role of the text input is ignored in their frameworks. Here, we raise a question: Does the text have no effectiveness in fine-grained image editing, or is there another more suitable form of text input?\\n\\nIn addition to the text prompt, DALL-E2 [32] presents a novel attempt to generate images conditioned on the image prompt, i.e., using images to describe images. Subsequently, some multimodal works [20, 24, 25] and object-customization works [22, 47, 51] are proposed to support image prompts for more detailed content description. Inspired by these works, we introduce image prompts into the fine-grained image editing process, improving editing quality through more detailed content descriptions. In addition, we combine regional score-based gradient guidance and a time travel strategy into diffusion sampling, which further enhances the editing quality.\\n\\nIn summary, this paper has the following contributions:\\n\\n\u2022 We present a novel attempt to introduce the image prompt to fine-grained image editing tasks. In conjunction with the image editing algorithm, this design can provide a more detailed description of the editing content, thus improving the editing quality.\\n\\n\u2022 We consider both the flexibility and content consistency of image editing by proposing regional SDE sampling and regional score-based gradient guidance. We also introduce a time travel strategy in diffusion-based image editing to improve the editing quality further.\\n\\n\u2022 Extensive experiments demonstrate that our method can achieve state-of-the-art performance on various fine-grained image editing tasks (i.e., content dragging, object moving, resizing, pasting, and appearance replacing, as shown in Fig. 1) with attractive complexity.\"}"}
{"id": "CVPR-2024-570", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Overview of our proposed DiffEditor, which is composed of a trainable image prompt encoder and a diffusion sampling with editing guidance that does not require training.\\n\\nFirst attempt to generate images guided by image prompts. ELITE [47], Bilp-Diffusion [22], and IP-Adapter [51] present the learning of image prompts for object customization. However, the effectiveness of image prompts in fine-grained image editing has hardly been studied.\\n\\n2.2. Image Editing\\n\\nThe primary objective of image editing is to manipulate the content of a given image in a controlled manner. Previous methods [1\u20133] usually invert images into the latent space of GANs [8] and then edit the image by manipulating latent vectors. Recently, DragGAN [30] presents a point-dragging formulation for fine-grained image editing. However, limited by the capability of GANs, these methods have weaknesses in model generalization and image quality. Motivated by the success of text-to-image diffusion models [33], various text-guided image editing methods [4, 6, 13, 18, 27] are proposed. The commonly used editing strategies are (1) adding noise and then denoising with target description [7, 18, 21, 27, 46]; (2) using cross-attention maps as an editing medium [11, 13, 16]; (3) using text as editing instructions [6]. However, the correspondence between the text and image in T2I models is weak, making it difficult to achieve fine-grained image editing. Recently, DragDiff [39] and DragonDiff [28] achieve fine-grained image editing based on the feature correspondence [45] in the pre-trained StableDiffusion (SD) [33]. Specifically, DragDiff uses LORA [34] to maintain content consistency and optimizes the latent $z_t$ in a specific diffusion step. DragonDiff is built based on the score-based [44] gradient guidance [9] and a visual cross-attention design for drag-style image editing without model tuning.\\n\\n3. Method\\n\\n3.1. Preliminary: Score-based Editing Guidance\\n\\nFrom the continuous perspective of score-based diffusion [43, 44], the external condition $y$ can be combined in a conditional score function, i.e.,\\n\\n$$\\\\nabla_x t \\\\log q(x_t|y)$$\\n\\nto sample from a more enriched distribution. The conditional score function can be further decomposed as:\\n\\n$$\\\\nabla_x t \\\\log q(x_t|y) = \\\\nabla_x t \\\\log q(y|x_t) + \\\\nabla_x t \\\\log q(x_t) \\\\propto \\\\nabla_x t \\\\log q(x_t) + \\\\eta \\\\cdot \\\\nabla_x t E(x_t, y),$$\\n\\nwhere the first term is the unconditional denoiser, i.e., $\\\\epsilon_t \\\\theta(x_t)$. The second term refers to the conditional gradient produced by an energy function $E(x_t, y) = \\\\log q(y|x_t)$, measuring the distance between current state $x_t$ and condition $y$. Here, we reformulate Eq. 3 as:\\n\\n$$\\\\tilde{\\\\epsilon}_t \\\\theta(x_t) = \\\\epsilon_t \\\\theta(x_t) + \\\\eta \\\\cdot \\\\nabla_x t E(x_t, y),$$\\n\\nwhere $\\\\eta$ refers to the learning rate. Recently, Self-Guidance [11] and DragonDiff [28] convert image editing operations into gradient guidance for image editing tasks. The energy function $E$ in Self-Guidance is built based on the correspondence [13] between image and text features. DragonDiff constructs the energy function based on image feature correspondence [45] in pre-trained SD, which can achieve more accurate drag-style editing tasks. In this paper, we aim to boost the accuracy and flexibility of diffusion-based image editing with the DragonDiff framework.\\n\\n3.2. Overview\\n\\nAn overview of our image editing pipeline is presented in Fig. 3. Specifically, given an image $x_0$ to be edited, we first employ it as image prompts and use an image prompt encoder to embed it. These image embeddings cooperate with text embeddings to form a better description to guide the diffusion process. Then we use DDIM inversion [41] to transform $x_0$ into a Gaussian distribution $z_T$ in the latent space of the pre-trained SD [33]. If the reference image $x_{ref,0}$ exists (i.e., in appearance replacing and object pasting), it will also be involved in the inversion. In this process, we follow the design in DragonDiff [28] to store some intermediate features ($K_{gud,t}, V_{gud,t}, K_{ref,t}, V_{ref,t}$) and latent ($z_{gud,t}, z_{ref,t}$) at each time step in a memory bank, which is\"}"}
{"id": "CVPR-2024-570", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3. Content Description with Image Prompt\\n\\nAlthough several fine-grained image editing methods [28, 39] are based on the T2I diffusion model, the role of prompts is ignored as a simple description. Compared to text prompts, image prompts [22, 32, 51] can provide a more detailed content description. In this paper, we find that the image prompt can improve the quality of fine-grained image editing, especially in some complex scenarios.\\n\\nInspired by IP-Adapter [51], the architecture of our image prompt encoder is shown in Fig. 4. Concretely, given an input image $x_0$, the pre-trained CLIP [31] image encoder embed it to 257 tokens. Then, a linear layer is used to adjust the channel dimension, and a QFormer [23] (without self-attention layer) module is employed to adjust the token numbers to 64 by 64 learnable queries. The QFormer module consists of $N$ (8 by default) submodules, each composed of a cross-attention layer and a feed-forward network (FN). 64 learnable queries serve as queries to extract information from 257 image tokens that act as keys and values. Finally, 257 image tokens are composed into 64 embedding tokens ($c_{im}$), and then they are input into the same cross-attention module as text tokens ($c$) in the SD. To build classifier-free guidance [14] like the text condition, the conditional and unconditional image prompts are jointly trained by randomly dropping (i.e., set image to zero) during training. Finally, image tokens and text tokens are processed separately with the query $Q$ in the cross-attention module, and the results are added together:\\n\\n$$\\\\text{Att}(Q, K', V', K'', V'') = S(Q(K'))^T\\\\sqrt{d}V' + \\\\gamma \\\\cdot S(Q(K''))^T\\\\sqrt{d}V'',$$\\n\\n(5)\\n\\nwhere $(K', V')$ and $(K'', V'')$ refer to the keys and values from the text and image prompt, respectively. $\\\\gamma$ is a weight to balance these two terms. $S$ is the function of Softmax. Note that in tasks with reference images (i.e., object pasting and appearance replacing), $K''$ and $V''$ are formed by the concatenation of image tokens from the source image and the reference image. During training, we fix the parameters in the pre-trained SD and CLIP image encoder, and we only optimize the linear embedding and QFormer by $L^2$ loss:\\n\\n$$\\\\mathbb{E}_{x_0, t, \\\\epsilon_t \\\\sim \\\\mathcal{N}(0, 1)} ||\\\\epsilon_t - \\\\epsilon_t^{\\\\theta}(z_t, c, c_{im})||_2^2.$$ \\n\\n(6)\\n\\nAfter training, this module can be integrated into pre-trained SD for various editing tasks, as demonstrated in this paper.\\n\\n3.4. Sampling with Regional SDE\\n\\nMaintaining consistency between editing results and original images is a great challenge in fine-grained image editing. Most methods adopt a deterministic sampling process (ODE) and utilize DDIM inversion for sampling initialization. In addition, DragDiff [39] uses LORA [34] to constrain the output content, and DragonDiff [28] uses visual cross-attention to maintain content consistency. However, these strategies also compromise editing flexibility, e.g., hindering the imagination of new content to harmonize editing operation as shown in Fig. 2. Our further experiments on DragonDiff show that reducing the content consistency strength can improve editing flexibility. As seen in Fig. 5, the editing flexibility is improved when we randomly initialize the sampling starting point $z_T$ or remove visual cross-attention. When we apply both reductions, the editing objective can be achieved flexibly, but the content consistency is severely compromised. Therefore, in this paper, we explore how to improve editing flexibility without significantly impacting content consistency. In the sampling process (i.e., Eq. 2) of DragonDiff, $\\\\sigma_t = 0$, which is a deterministic ODE sampling. This leads to the final result being highly dependent on $z_T$ and the information injected.\"}"}
{"id": "CVPR-2024-570", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Editing gradient from E edit in different sampling steps.\\n\\nOur solution is to introduce randomness (i.e., $\\\\sigma_t > 0$) during the sampling process, while this randomness is controlled within local editing areas and specific time intervals. Here, we use $z_{t-1} = F(z_t; \\\\sigma_t)$ to simplify Eq. 2. Our regional SDE sampling is defined as:\\n\\n$$z_{t-1} = m_{edit} \\\\cdot F(z_t; \\\\eta_1(t)) + (1 - m_{edit}) \\\\cdot F(z_t; \\\\eta_2(t)),$$\\n\\nwhere $m_{edit}$ locates the editing area. $\\\\tau_{SDE}$ is the time interval for applying regional SDE. After using this sampling strategy, we can accurately inject flexibility to produce satisfactory results, as shown in the last image of Fig. 2.\\n\\n3.5. Editing with Gradient Guidance\\n\\nRegional gradient guidance. In DragonDiff [28], the energy function $E$ consists of two parts, i.e., editing $E_{edit}$ and content consistency $E_{content}$. Although their target areas are independent of each other, the scope of the gradient guidance they generate is global and overlapping, resulting in mutual interference. Concretely, in Fig. 6, we visualize the editing gradient produced by $E_{edit}$ in the object moving task. As can be seen, the gradient guidance gradually converges to the editing area as the diffusion sampling proceeds. During this process, there are some activations outside the editing area, and these imprecise activations can affect the content consistency in these unedited areas (details are presented in Sec. 4.3). To rectify this weakness, we use the editing region mask $m_{edit}$ to locally combine $E_{edit}$ and $E_{content}$. Finally, the conditional term in Eq. 3 is defined as:\\n\\n$$\\\\nabla z_t \\\\log q(y|z_t) = m_{edit} \\\\cdot \\\\nabla x_t E_{edit} + (1 - m_{edit}) \\\\cdot \\\\nabla x_t E_{content},$$\\n\\nwhere $y$ is the editing target. During the sampling, we only add guidance in the first $n$ time steps.\\n\\nTime travel. DragDiff [39] treats $z_t$ as a learnable parameter and iteratively optimizes it within a diffusion step $t$. In contrast, DragonDiff [28] incorporates score-based gradient guidance into each sampling step, i.e., Eq. 4. However, applying editing guidance through Eq. 4 once at each time travel lacks refinement for editing, especially in some complex scenarios.\\n\\nCan we combine the advantages of DragDiff and DragonDiff to build recurrent guidance in the score-based diffusion [44]? To address this issue, we build time travel to perform rollbacks, i.e., $z_t \\\\leftarrow z_{t-1}$, during the sampling process. This strategy has been empirically shown to inhibit the generation of disharmonious results when solving hard generation tasks [26, 52]. However, the rollback strategy (i.e., $z_t \\\\sim N(p_1 - \\\\beta_{t-1} z_{t-1}, \\\\beta_{t-1} I)$) in these works is not suitable in fine-grained image editing tasks. This is because random noise $I$ can introduce significant uncertainty, undermining the content consistency of editing results. To ensure the accuracy of rollback, we use deterministic DDIM inversion [41] to roll back $z_{t-1}$ to $z_t$.\\n\\nDuring sampling, the time travel is performed $U (3$ in our design) times for each guidance step in a time interval $\\\\tau_{TT}$. Due to the guidance enhancement from regional guidance and time travel, we can achieve editing with fewer guidance time steps, i.e., we introduce gradient guidance every two time steps in sampling. Finally, the algorithm logic of our DiffEditor is defined in Alg. 1.\"}"}
{"id": "CVPR-2024-570", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Quantitative evaluation on face manipulation with 68 and 17 points. The accuracy is calculated by MSE distance between edited points and target points. The initial distance (i.e., 57.19 and 36.36) is the upper bound, without editing. FID [38] is utilized to quantize the editing quality of different methods. The time complexity is computed on the \u20181 point\u2019 dragging.\\n\\n| Method          | Inference Complexity | Unaligned 17 Points | Unaligned 68 Points |\\n|-----------------|----------------------|---------------------|---------------------|\\n| UserControllableLT | 1.1s                 | 0.04s               | 32.32%              |\\n| DragGAN         | 50.22s               | 6.28s               | 15.96%              |\\n| DragDiff        | 42.37s               | 19.52s              | 22.95%              |\\n| DragonDiff      | 3.53s                | 15.00s              | 18.51%              |\\n| DiffEditor (Ours) | 3.53s               | 13.88s              | 17.05%              |\\n\\nReference Source\\n\\nUserControllableLT DragGAN DragDiff Ours\\n68 Points\\n\\nMSE=8.53 MSE=11.19 MSE=27.95 MSE=9.54 MSE=32.81\\n\\n68 Points (Unaligned)\\n\\nMSE=13.92 MSE=15.36 MSE=15.49 MSE=15.70 MSE=32.24\\n\\nFigure 7. Qualitative comparison between our DiffEditor and other methods in face manipulation. The current and target points are labeled with red and blue. The white line indicates distance. The MSE distance between the result and the target is labeled in yellow.\\n\\n4. Experiments\\n4.1. Implementation Details\\nWe choose Stable Diffusion V1.5 [33] as the base model for image editing. During image prompt training, we use the training data from LAION [36] and process the image resolution to 512 \u00d7 512. We choose Adam [19] optimizer with an initial learning rate of 1 \u00d7 10^{-5}. The batch size during training is set as 16. The training process iterates 1 \u00d7 10^6 steps on 4 NVIDIA A100 GPUs. We use the same embedding module to process image prompts in different applications. The inference adopts DDIM sampling with 50 steps, and we set the classifier-free guidance scale as 5.\\n\\n4.2. Comparison\\nTime complexity. We divide the time complexity of different methods into preparing and inference stages. The preparing stage involves Diffusion/GAN inversion and model tuning. The inference stage generates the editing result from latent representation. The time complexity for each method is tested on one point dragging, with the image resolution being 512 \u00d7 512. All times are tested on an NVIDIA A100 GPU with Float32 precision. The results in Tab. 1 present the attractive preparing complexity of our method, and the inference complexity is lower than existing diffusion-based methods, i.e., DragDiff and DragonDiff.\\n\\nPerformance. First, we evaluate our method on content dragging by comparing it with some well-known GAN-based methods (i.e., UserControllableLT [10], DragDiff, DragGAN).\"}"}
{"id": "CVPR-2024-570", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Moving\\nReplacing\\nPaint-by-example\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiffusion\\nOurs\\nSelf-guidance\\nDragonDiff"}
{"id": "CVPR-2024-570", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Effectiveness of the regional gradient guidance. In experiment, we remove the content consistency gradient $E_{content}$.\\n\\nVisualization of editing without time travel, with random time travel, and with our accurate time travel.\\n\\nAblation Study\\n\\nImage prompt. Image prompt provides a detailed description of the editing content in our editing pipeline. We conduct an experiment in Fig. 10 to demonstrate its effectiveness. First, we apply it in the pure DDIM inversion and then reconstruction. The result in the first row presents that DDIM inversion based on the text prompt exhibits noticeable distortions and is unstable. After using our image prompt, DDIM inversion can reconstruct stable and high-fidelity results. In the second row of Fig. 10, we show the editing with and without the image prompt. It can be seen that the image prompt provides a better generation prior for editing content, reducing the probability of distortion.\\n\\nRegional gradient guidance. To rectify the interference between different gradient guidance, we use Eq. 8 to produce the guidance. We demonstrate its effectiveness in Fig. 11 by only using $m_{edit} \\\\cdot E_{edit}$ and $E_{edit}$. Note that we remove the content consistency guidance $E_{content}$ to highlight the interference. The results show that if the actuating range of $E_{edit}$ is not constrained, the editing gradient will have an impact on the consistency of some unrelated areas, e.g., distortion of the fingers in the background. After applying regional constraints, the content of the background part has better consistency even without $E_{content}$.\\n\\nTime travel. Time travel is used to build recurrent guidance in a single diffusion time step, thereby refining the editing effect. We present its effectiveness in Fig. 12. As can be seen, the editing result in the complex scenario has distortions without the time travel strategy. If using the random time travel (i.e., $z_t \\\\sim N(p_1 - \\\\beta t - 1 z_{t-1}, \\\\beta t - 1 I)$), the randomness will affect the consistency between the editing result and the original image. After adopting our accurate time travel, the editing quality is improved.\\n\\n5. Conclusion\\n\\nIn this paper, we aim to rectify two issues in diffusion-based fine-grained image editing: (1) in complex scenarios, editing results often lack accuracy and exhibit unexpected artifacts; (2) lack of flexibility to harmonize editing operation, e.g., imagine new content. In our solution, we introduce image prompt into fine-grained image editing, providing a more detailed content description for the edited image. This method can be plugged into various fine-grained image editing tasks without task-specific training. To improve the editing flexibility, we propose a regional SDE strategy to inject randomness into the editing area while maintaining content consistency in other areas. Furthermore, we introduce regional score-based gradient guidance and a time travel strategy to improve the editing quality further. Extensive experiments demonstrate that our method can achieve promising performance in various fine-grained image editing tasks, i.e., object moving, resizing, pasting, appearance replacing, and content dragging. The complexity is also reduced compared with existing diffusion-based methods.\\n\\nLimitations\\n\\nAlthough our method improves the flexibility of diffusion-based image editing and reduces distortion probability, editing difficulties still exist in some scenarios that require a large amount of content imagination, such as rotating a car by dragging its front. We think that this is due to the base model SD. It has a diverse generation space but lacks 3D perception of individual objects. In our future work, we will enhance the editing capabilities of diffusion models in this regard.\"}"}
{"id": "CVPR-2024-570", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan: How to embed images into the stylegan latent space? In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4432\u20134441, 2019.\\n\\n[2] Rameen Abdal, Yipeng Qin, and Peter Wonka. Image2stylegan++: How to edit the embedded images? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8296\u20138305, 2020.\\n\\n[3] Yuval Alaluf, Omer Tov, Ron Mokady, Rinon Gal, and Amit Bermano. Hyperstyle: Stylegan inversion with hypernetworks for real image editing. In Proceedings of the IEEE/CVF conference on computer Vision and pattern recognition, pages 18511\u201318521, 2022.\\n\\n[4] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18208\u201318218, 2022.\\n\\n[5] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, Bryan Catanzaro, et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022.\\n\\n[6] Tim Brooks, Aleksander Holynski, and Alexei A Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18392\u201318402, 2023.\\n\\n[7] Mingdeng Cao, Xintao Wang, Zhongang Qi, Ying Shan, Xiaohu Qie, and Yinqiang Zheng. Masactrl: Tuning-free mutual self-attention control for consistent image synthesis and editing. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22560\u201322570, 2023.\\n\\n[8] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath. Generative adversarial networks: An overview. IEEE signal processing magazine, 35(1):53\u201365, 2018.\\n\\n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:8780\u20138794, 2021.\\n\\n[10] Yuki Endo. User-controllable latent transformer for stylegan image layout editing. In Computer Graphics Forum, pages 395\u2013406. Wiley Online Library, 2022.\\n\\n[11] Dave Epstein, Allan Jabri, Ben Poole, Alexei Efros, and Aleksander Holynski. Diffusion self-guidance for controllable image generation. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[12] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Reddy Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. In Proceedings of the International Conference on Learning Representations, 2022.\\n\\n[13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-or. Prompt-to-prompt image editing with cross-attention control. In Proceedings of the International Conference on Learning Representations, 2022.\\n\\n[14] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022.\\n\\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020.\\n\\n[16] Xuan Ju, Ailing Zeng, Yuxuan Bian, Shaoteng Liu, and Qiang Xu. Direct inversion: Boosting diffusion-based editing with 3 lines of code. arXiv preprint arXiv:2310.01506, 2023.\\n\\n[17] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In Proceedings of the International Conference on Learning Representations, 2018.\\n\\n[18] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6007\u20136017, 2023.\\n\\n[19] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\n[20] Jing Yu Koh, Daniel Fried, and Russ R Salakhutdinov. Generating images with multimodal language models. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[21] Gihyun Kwon and Jong Chul Ye. Diffusion-based image translation using disentangled style and content representation. In Proceedings of the International Conference on Learning Representations, 2022.\\n\\n[22] Dongxu Li, Junnan Li, and Steven Hoi. Blip-diffusion: Pre-trained subject representation for controllable text-to-image generation and editing. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In Proceedings of the International Conference on Machine Learning, pages 19730\u201319742, 2023.\\n\\n[24] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023.\\n\\n[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024.\\n\\n[26] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11461\u201311471, 2022.\\n\\n[27] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In Proceedings of the International Conference on Learning Representations, 2021.\"}"}
{"id": "CVPR-2024-570", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
