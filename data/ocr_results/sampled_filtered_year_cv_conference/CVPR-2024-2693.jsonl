{"id": "CVPR-2024-2693", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"You'll Never Walk Alone: A Sketch and Text Duet for Fine-Grained Image Retrieval\\n\\nSubhadeep Koley\\nAyan Kumar Bhunia\\nAneeshan Sain\\nPinaki Nath Chowdhury\\nTao Xiang\\nYi-Zhe Song\\n\\n1 SketchX, CVSSP, University of Surrey, United Kingdom.\\n2 iFlyTek-Surrey Joint Research Centre on Artificial Intelligence.\\n\\n{s.koley, a.bhunia, a.sain, p.chowdhury, t.xiang, y.song}@surrey.ac.uk\\n\\nhttps://subhadeepkoley.github.io/Sketch2Word\\n\\nFigure 1. (a) Photos retrieved by our method, depicting precise control over both shape and appearance. (b) Unlike baseline sketch+text composed retrieval framework, our method seamlessly composes the structural and contextual cues of sketch and text queries respectively. (c) With a fixed sketch query, our method retrieves different images for different textual descriptions and vice-versa, depicting the complementarity of sketch and text modalities in sketch+text-based composed image retrieval. For a fixed sketch, the visual attributes from different textual descriptions are visibly reflected in the retrieved images while maintaining shape consistency. Similarly, fixing the attributes provided via text, shapes of retrieved images change corresponding to different sketch queries.\\n\\nAbstract\\n\\nTwo primary input modalities prevail in image retrieval: sketch and text. While text is widely used for inter-category retrieval tasks, sketches have been established as the sole preferred modality for fine-grained image retrieval due to their ability to capture intricate visual details. In this paper, we question the reliance on sketches alone for fine-grained image retrieval by simultaneously exploring the fine-grained representation capabilities of both sketch and text, orchestrating a duet between the two. The end result enables precise retrievals previously unattainable, allowing users to pose ever-finer queries and incorporate attributes like colour and contextual cues from text. For this purpose, we introduce a novel compositionality framework, effectively combining sketches and text using pre-trained CLIP models, while eliminating the need for extensive fine-grained textual descriptions. Last but not least, our system extends to novel applications in composed image retrieval, domain attribute transfer, and fine-grained generation, providing solutions for various real-world scenarios.\\n\\n1. Introduction\\n\\nSketch and text represent the two most common in the realm of image retrieval. The choice between these modalities depends on the nature of the retrieval problem, especially when fine-grained distinctions are required. In inter-category retrieval, text dominates as the primary modality, exemplified by widely-used platforms like Google Images. However, when the challenge transitions to fine-grained image retrieval, sketches take the spotlight. Sketches promise to capture fine-grained visual cues that can be cumbersome or even impossible for text to express. Research in this domain predominantly revolves around harnessing the unique qualities of sketches, exploring aspects such as style, abstraction, and more.\\n\\nIn this paper, we question this notion that \\\"sketch is everything\\\" and, for the first time, simultaneously delve into the fine-grained representation capabilities of both sketch and text, and in turn orchestrate a duet between these two modalities for fine-grained image retrieval. The outcome is a novel retrieval experience where a sketch and text work in harmony, enabling users to achieve precise retrievals that were previously unattainable. Now, users can locate \\\"that\\\" specific shoe, considering not only fine-grained pattern cues from sketches but also incorporating attributes like colour and texture from text (Fig. 1a). This synergy extends to scenarios where text offers contextual cues to a given sketch, such as a cat holding a shoe for that matter (Fig. 1b)!\\n\\nWhile sketch and text synergy has been studied before, it has predominantly focused on scene-level/category retrieval, where paired sketch and text descriptions for a given scene/category are readily available in datasets (e.g., FS-COCO, CM-Places). Our contention is that the synergy between sketches and text, while notable, is not as pronounced in category-level retrieval as it is for fine-grained retrieval. Indeed, for category-level retrieval, one might argue the necessity of sketches, as the descriptive power of text might already suffice.\\n\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\"}"}
{"id": "CVPR-2024-2693", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"However, when the desire extends to a horse/cat with a specific pose, an accompanying sketch becomes indispensable (as illustrated in Fig. 1b).\\n\\nOur foremost challenge centres around fine-grained compositionality, specifically investigating how sketches and text can serve as complementary components in fine-grained queries. Our goal is to maintain the semantics of both modalities, ensuring, for example, that a horse in Fig. 1b corresponds precisely to the respective sketch and associated text, rather than any generic horse with a woman riding. To tackle this challenge, we harness the capabilities of CLIP [51], leveraging its implicit grammatical composition capability. We achieve this via CLIP [51] to create a fine-grained textual equivalent of the input sketch, referred to as a \u201cpseudo-word token\u201d. This token, when combined with text input, forms a fine-grained textual query that seamlessly integrates both sketch and text features, allowing them to work in synergy within the text domain.\\n\\nThe second challenge pertains to alleviating the requirement of collecting a dataset of fine-grained sketch and text pairs. We also aim to emulate the inference-time distribution of text input. The key innovation lies in the hypothesis that the fine-grained description embedded in a photo (P) can be approximated by that of a sketch (S) plus text (T), leading to $T = P - S$. This relationship illustrates how the absence of text can be approximated by the difference signal between the photo and the sketch in the latent embedding space. We incorporate this difference signal as a proxy for the missing textual description during training to make it inference-time equivalent through a novel compositionality constraint. Furthermore, we utilise short phrases generated by a lightweight GPT [6] as a neutral-text regulariser to ensure that the synergy works without disrupting the grammatical structure of CLIP\u2019s language manifold.\\n\\nLast but not least, in addition to addressing the challenges in fine-grained image retrieval, our system opens the door to a range of novel applications in the field of composed image retrieval such as object-sketch-based scene image retrieval, domain attribute transfer, and sketch+text-based fine-grained image generation.\\n\\nIn summary, (i) we address the challenge of fine-grained image retrieval by leveraging the synergy between freehand sketches and textual descriptions, extending retrieval beyond traditional category-level distinctions. (ii) we introduce a novel compositionality framework, effectively combining sketches and text using pre-trained CLIP models, eliminating the need for extensive fine-grained textual descriptions. (iii) our system unlocks novel applications like object-sketch-based scene retrieval, domain attribute transfer, and sketch+text-based fine-grained image generation.\\n\\n2. Related Works\\n\\nSketch-Based Image Retrieval (SBIR). Starting at category-level, SBIR is tasked to fetch a photo of the same category as that of a given query sketch. Earlier deep-learning methods [15, 39, 68, 71] generally train Siamese-like networks [15] over a distance-metric in a cross-modal joint embedding space [14]. Moving forward to fine-grained SBIR (FG-SBIR), the aim is to retrieve one particular photo-instance from a gallery of same-category photos corresponding to a query sketch. FG-SBIR has progressed from a deep-triplet ranking-based Siamese network [70] to further enhancements involving higher-order attention [61] or auxiliary losses [37], and local feature alignment [67], to name a few. While most works focus on various applications like early-retrieval [3], cross-category generalisation [5, 49] and even zero-shot FG-SBIR [33, 55], others delved deeper to explore sketch-specific traits, like hierarchy [53], or style-diversity [54], for better retrieval. Recent extensions include retrieving a scene image based on a scene-sketch (i.e., Scene-level FG-SBIR), employing cross-modal region associativity [9], enhanced further with text-query [11]. Unlike existing fully-supervised sketch+text-based methods [11, 59], relying on paired training triplets (i.e., sketch, text, and ground truth photo), our approach alleviates the need for such triplets, simplifying the challenge of collecting fine-grained sketch-text-photo dataset.\\n\\nText-Based Image Retrieval (TBIR). Over the years, much emphasis has been paid to textual query-based image retrieval by learning a joint embedding space via ranking loss [19, 28, 50]. This was further augmented by cross-modal message passing [64], hard triplet mining [20], and one-to-many probabilistic mapping [12], to name a few. Thanks to internet-scale paired image-text datasets TBIR has become highly competitive and one of the most active areas of research [51], leading to expansive techniques like Oscar [36], CLIP [51], ALIGN [27], etc. The recent paradigm of Textual Inversion [13, 21] deals with inverting input image(s) into a pseudo-word token in the language space of pre-trained vision-language models for downstream tasks like personalised image retrieval [13], composed retrieval [2, 57], etc. Composed image retrieval (CIR) aims to retrieve images from a combined query of text and image pairs [1]. Existing CIR methods typically leverage pre-trained CLIP [1, 2, 57], or resort to image-text feature fusion [23, 25, 62, 66]. Textual inversion-based CIR methods [2, 57] either use million-scale image dataset to train inversion networks [57], or uses time-consuming two-stage optimisation-based approach [2]. Nevertheless, these image + text composed retrieval frameworks [2, 57] can not handle the huge domain gap of sparse sketch+text composed image retrieval. Our method, on the other hand, explicitly models the sketch-photo difference within itself for fine-grained sketch+text composed multi-modal retrieval.\\n\\nSketch+Text Joint Multi-Modal Learning. While sketch is a suitable fine-grained query medium for AI systems, certain aspects fall outside its scope like qualitative...\"}"}
{"id": "CVPR-2024-2693", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Graded attributes (e.g., color, shade, etc.) aptly describes these qualitative attributes (e.g., color) which leads to \\\"text as query\\\" being extensively studied [11], albeit largely for category-level tasks (e.g., TBIR). Realizing this potential of sketch-text-photo association, joint multi-modal sketch+text query was heavily used in downstream vision tasks. On generation, sketch-to-image models used text-as-guidance to specify class-label [24]. Simultaneously, text-to-image diffusion models used sketches as semantic-guidance for the encoder [44], or decoder [72], or external classifier guidance [63], NeRF-editing [43, 45] using sketch+text conditioning [42], and sketch-conditioned image captioning [11]. On retrieval, the importance of sketch+text was realized with FS-COCO [10] dataset collecting scene-level sketch-text-photo dataset. Recent works include supervised approaches of Sangkloy et al. [59] using CLIP, and Scenetrilogy [11] using conditional invertible neural networks for sketch+text-based image retrieval. Despite its benefit, collecting fine-grained textual descriptions is quite cumbersome [10], which bottlenecks further exploration of fine-grained sketch-text-photo association for downstream tasks till date.\\n\\n3. Revisiting CLIP\\n\\nCLIP [51] consists of a text encoder and an image encoder, trained with a multi-class N-pair contrastive loss [51] on internet-scale (\u223c400M) image-text pair dataset, with an aim to learn a joint-embedding space that minimizes the cosine similarity between the matching image-text pairs while maximizing the same for random unpaired ones [51]. The image encoder (V) usually employs a Vision Transformer (ViT) [17], to encode an input image \\\\( I \\\\) into a visual feature as \\\\( i = V(I) \\\\in R^{d} \\\\). The text encoder (T) inputs a sequence of words \\\\( W = \\\\{w_0, w_1, \\\\ldots, w_k\\\\} \\\\) and applies a lower-cased byte pair encoding (BPE), followed by a learnable word embedding layer \\\\( T_{\\\\text{w}}(\\\\text{49, 152 vocab size}) \\\\) [51], to convert each word \\\\( w_i \\\\) into a word token embedding \\\\( w_i^e = T_{\\\\text{w}}(w_i) \\\\) of size \\\\( R^{d} \\\\). This sequence of word token embeddings \\\\( W_e = \\\\{w_0^e, w_1^e, \\\\ldots, w_k^e\\\\} \\\\) is then passed via a transformer \\\\( T_{\\\\text{t}} \\\\), to provide the final textual feature as \\\\( w = T_{\\\\text{t}}(W_e) \\\\in R^{d} \\\\), taken from the last hidden state of the final transformer layer.\\n\\n4. Sketch-Based Composed Image Retrieval\\n\\nMotivation. Combining structural cues from sketch with additional textual description results in a powerful query for image retrieval. Existing works [11, 59] on such compositionality usually extract sketch and text features via separate encoders, and add or concatenate (followed by additional learnable layers) them, to obtain the composed query feature. This has two major issues: (i) it needs sketch-associated textual description \u2013 absent from fine-grained SBIR datasets [58, 70], and (ii) combining the two features naively may distort the optimal sketch-text feature correlation needed, to correctly represent a composed semantic. This compositionality is however more explicit in the textual domain [2, 13, 57] where combining individual words/phrases form a composed semantic, e.g., 'a cat' and 'brown' together infers 'a brown cat'. Following CLIP's rise in various downstream tasks [30, 55, 73], we thus aim to leverage its text encoder's input text space to tackle sketch+text compositionality. In particular, inspired by textual inversion literature [21], we aim to represent a sketch as a pseudo-word token that emulates its visual concept in equivalent word-embedding space, and combine its textual description via connecting phrases like 'with', 'in', 'and', etc. (the full list in \u00a7 Suppl.) to obtain \\\"\\\\( \\\\langle \\\\text{pseudo-word token} \\\\rangle \\\\langle \\\\text{connecting phrase} \\\\rangle \\\\langle \\\\text{text description} \\\\rangle \\\\)\\\" as a query. Passing this via CLIP's text encoder would provide a sketch+text composed representation, that can be compared against gallery image features pre-computed via CLIP's vision encoder. The goal here is to learn sketch+text compositionality via CLIP, unsupervised, without any expensive paired textual description.\\n\\nOverall Framework. Here, we aim to design a Sketch-Based Composed Image Retrieval (SBCIR) framework harnessing the Vision-Language (V-L) embedding of pre-trained CLIP [51] using only the sketch-photo pairing readily available in sketch datasets [10, 58, 70], without any annotated textual description. Accordingly, we embed the sketch into a pseudo-word token by first passing it through CLIP's visual encoder \\\\( V \\\\), followed by a visual-to-word converter \\\\( C_{\\\\text{v2w}} \\\\), which is then passed into CLIP's text transformer \\\\( T_{\\\\text{t}} \\\\) along with a few learnable prompts and additional textual descriptions (during inference) to obtain the composed query embedding. Specifically, we have three salient designs \u2013 (i) a novel compositionality constraint imposed via sketch/photo difference-signal to imitate the missing textual description (during training) and neutral text (Sec. 4.2) to preserve the grammatical structure of the input text-space of CLIP's text encoder, (ii) generalisable continuous prompt-learning (Sec. 4.3) over handcrafted textual prompts, and (iii) fine-grained matching (Sec. 4.4) between composed query and paired photo embedding via region-aware triplet loss and an auxiliary generative loss.\\n\\n4.1. Baseline SBCIR\\n\\nConventional CLIP-based SBIR [55] maps both sketch query and its target photo in the joint embedding space using the visual encoder (V). On the contrary, we convert a query sketch to a pseudo-word token and pass it through CLIP's textual encoder (T) to generate its representation. In particular, given a photo \\\\( P \\\\), we generate its latent embedding \\\\( p = V(P) \\\\in R^{1 \\\\times d} \\\\). For a sketch however, we generate its equivalent word embedding as \\\\( s = C_{\\\\text{v2w}}(V(S)) \\\\in R^{1 \\\\times d} \\\\). \\\\( s \\\\) signifies the equivalent language representation of the visual sketch query.\"}"}
{"id": "CVPR-2024-2693", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Consequently, we can leverage the zero-shot compositionality of CLIP's text encoder to append the word embedding of optional textual query (no text was provided during training). During inference, we pose a query embedding, particularly when combined with a reconstruction loss using which we can imitate the effect of adding this pseudo-word token embedding to the sketch embedding.\\n\\nWhile our method does not rely on paired textual descriptions during training, it produces the final sketch embedding as the two-modal matching of the actual photo features. With margin forcing, we retrieve the correct image. We handle this to specify the desired fine-grained matching, particularly when combined with additional information that ensures that the distance between sketch and photo, which ideally would be substituted with real query text during inference. Now, as per our hypothesis, we enforce the compositionality constraint during inference. Thus, to restrict the adverse effect of this additional signal might train a list of neutral text (via GPT) to impose a regularisation loss on image embedding, it may disrupt its grammatical syntax. Furthermore, we mine a set of \\\"neutral text\\\" to impose a regularisation loss for further cross-modal alignment.\\n\\nWe design the visual-to-word converter network \\\\( C_{v2w} \\\\) to finally obtain the final composed query vector. Specifically, we only train the parameters of \\\\( C_{v2w} \\\\) for zero-shot compositionality. We thus set \\\\( P \\\\) as \\\\( \\\\text{Region-aware triplet} \\\\) with \\\\( RT \\\\), as \\\\( \\\\text{unfrozen} \\\\) to get the final composed query vector. To this end, we reposition the query into the output of \\\\( C_{v2w} \\\\), as \\\\( \\\\text{frozen} \\\\). Baseline SBCIR trains over triplet loss to perform cross-modal matching between sketch and paired photo. With margin enforcing, we retrieve the correct image. We handle this to specify the desired fine-grained matching, particularly when combined with additional information that ensures that the distance between sketch and photo, which ideally would be substituted with real query text during inference. Now, as per our hypothesis, we enforce the compositionality constraint during inference. Thus, to restrict the adverse effect of this additional signal might train a list of neutral text (via GPT) to impose a regularisation loss on image embedding, it may disrupt its grammatical syntax. Furthermore, we mine a set of \\\"neutral text\\\" to impose a regularisation loss for further cross-modal alignment.\\n\\nApart from a reconstruction loss \\\\( L_{\\\\text{rec}} \\\\), we use a Stage approach \\\\((2, 57)\\\\), where we compute a reconstruction loss \\\\( L_{\\\\text{rec}} \\\\) and pass this through Frozen CLIP's visual encoder \\\\((1)\\\\). Inspired from prompt learning, we prepend a \\\\( 3\\\\)-Layer MLP with ReLU \\\\((46)\\\\). We only train the \\\\( \\\\text{LayerNorm} \\\\) of \\\\( C_{v2w} \\\\) to obtain the final composed query vector.\\n\\nWe design the visual-to-word converter network \\\\( C_{v2w} \\\\) to finally obtain the final composed query vector. Specifically, we only train the parameters of \\\\( C_{v2w} \\\\) for zero-shot compositionality. We thus set \\\\( P \\\\) as \\\\( \\\\text{Region-aware triplet} \\\\) with \\\\( RT \\\\), as \\\\( \\\\text{unfrozen} \\\\) to get the final composed query vector. To this end, we reposition the query into the output of \\\\( C_{v2w} \\\\), as \\\\( \\\\text{frozen} \\\\). Baseline SBCIR trains over triplet loss to perform cross-modal matching between sketch and paired photo. With margin enforcing, we retrieve the correct image. We handle this to specify the desired fine-grained matching, particularly when combined with additional information that ensures that the distance between sketch and photo, which ideally would be substituted with real query text during inference. Now, as per our hypothesis, we enforce the compositionality constraint during inference. Thus, to restrict the adverse effect of this additional signal might train a list of neutral text (via GPT) to impose a regularisation loss on image embedding, it may disrupt its grammatical syntax. Furthermore, we mine a set of \\\"neutral text\\\" to impose a regularisation loss for further cross-modal alignment.\\n\\nWe design the visual-to-word converter network \\\\( C_{v2w} \\\\) to finally obtain the final composed query vector. Specifically, we only train the parameters of \\\\( C_{v2w} \\\\) for zero-shot compositionality. We thus set \\\\( P \\\\) as \\\\( \\\\text{Region-aware triplet} \\\\) with \\\\( RT \\\\), as \\\\( \\\\text{unfrozen} \\\\) to get the final composed query vector. To this end, we reposition the query into the output of \\\\( C_{v2w} \\\\), as \\\\( \\\\text{frozen} \\\\). Baseline SBCIR trains over triplet loss to perform cross-modal matching between sketch and paired photo. With margin enforcing, we retrieve the correct image. We handle this to specify the desired fine-grained matching, particularly when combined with additional information that ensures that the distance between sketch and photo, which ideally would be substituted with real query text during inference. Now, as per our hypothesis, we enforce the compositionality constraint during inference. Thus, to restrict the adverse effect of this additional signal might train a list of neutral text (via GPT) to impose a regularisation loss on image embedding, it may disrupt its grammatical syntax. Furthermore, we mine a set of \\\"neutral text\\\" to impose a regularisation loss for further cross-modal alignment.\\n\\nWe design the visual-to-word converter network \\\\( C_{v2w} \\\\) to finally obtain the final composed query vector. Specifically, we only train the parameters of \\\\( C_{v2w} \\\\) for zero-shot compositionality. We thus set \\\\( P \\\\) as \\\\( \\\\text{Region-aware triplet} \\\\) with \\\\( RT \\\\), as \\\\( \\\\text{unfrozen} \\\\) to get the final composed query vector. To this end, we reposition the query into the output of \\\\( C_{v2w} \\\\), as \\\\( \\\\text{frozen} \\\\). Baseline SBCIR trains over triplet loss to perform cross-modal matching between sketch and paired photo. With margin enforcing, we retrieve the correct image. We handle this to specify the desired fine-grained matching, particularly when combined with additional information that ensures that the distance between sketch and photo, which ideally would be substituted with real query text during inference. Now, as per our hypothesis, we enforce the compositionality constraint during inference. Thus, to restrict the adverse effect of this additional signal might train a list of neutral text (via GPT) to impose a regularisation loss on image embedding, it may disrupt its grammatical syntax. Furthermore, we mine a set of \\\"neutral text\\\" to impose a regularisation loss for further cross-modal alignment.\\n\\nWe design the visual-to-word converter network \\\\( C_{v2w} \\\\) to finally obtain the final composed query vector. Specifically, we only train the parameters of \\\\( C_{v2w} \\\\) for zero-shot compositionality. We thus set \\\\( P \\\\) as \\\\( \\\\text{Region-aware triplet} \\\\) with \\\\( RT \\\\), as \\\\( \\\\text{unfrozen} \\\\) to get the final composed query vector. To this end, we reposition the query into the output of \\\\( C_{v2w} \\\\), as \\\\( \\\\text{frozen} \\\\). Baseline SBCIR trains over triplet loss to perform cross-modal matching between sketch and paired photo. With margin enforcing, we retrieve the correct image. We handle this to specify the desired fine-grained matching, particularly when combined with additional information that ensures that the distance between sketch and photo, which ideally would be substituted with real query text during inference. Now, as per our hypothesis, we enforce the compositionality constraint during inference. Thus, to restrict the adverse effect of this additional signal might train a list of neutral text (via GPT) to impose a regularisation loss on image embedding, it may disrupt its grammatical syntax. Furthermore, we mine a set of \\\"neutral text\\\" to impose a regularisation loss for further cross-modal alignment.\\n\\nWe design the visual-to-word converter network \\\\( C_{v2w} \\\\) to finally obtain the final composed query vector. Specifically, we only train the parameters of \\\\( C_{v2w} \\\\) for zero-shot compositionality. We thus set \\\\( P \\\\) as \\\\( \\\\text{Region-aware triplet} \\\\) with \\\\( RT \\\\), as \\\\( \\\\text{unfrozen} \\\\) to get the final composed query vector. To this end, we reposition the query into the output of \\\\( C_{v2w} \\\\), as \\\\( \\\\text{frozen} \\\\). Baseline SBCIR trains over triplet loss to perform cross-modal matching between sketch and paired photo. With margin enforcing, we retrieve the correct image. We handle this to specify the desired fine-grained matching, particularly when combined with additional information that ensures that the distance between sketch and photo, which ideally would be substituted with real query text during inference. Now, as per our hypothesis, we enforce the compositionality constraint during inference. Thus, to restrict the adverse effect of this additional signal might train a list of neutral text (via GPT) to impose a regularisation loss on image embedding, it may disrupt its grammatical syntax. Furthermore, we mine a set of \\\"neutral text\\\" to impose a regularisation loss for further cross-modal alignment.\\n\\nWe design the visual-to-word converter network \\\\( C_{v2w} \\\\) to finally obtain the final composed query vector. Specifically, we only train the parameters of \\\\( C_{v2w} \\\\) for zero-shot compositionality. We thus set \\\\( P \\\\) as \\\\( \\\\text{Region-aware triplet} \\\\) with \\\\( RT \\\\), as \\\\( \\\\text{unfrozen} \\\\) to get the final composed query vector. To this end, we reposition the query into the output of \\\\( C_{v2w} \\\\), as \\\\( \\\\text{frozen} \\\\). Baseline SBCIR trains over triplet loss to perform cross-modal matching between sketch and paired photo. With margin enforcing, we retrieve the correct image. We handle this to specify the desired fine-grained matching, particularly when combined with additional information that ensures that the distance between sketch and photo, which ideally would be substituted with real query text during inference. Now, as per our hypothesis, we enforce the compositionality constraint during inference. Thus, to restrict the adverse effect of this additional signal might train a list of neutral text (via GPT) to impose a regularisation loss on image embedding, it may disrupt its grammatical syntax. Furthermore, we mine a set of \\\"neutral text\\\" to impose a regularisation loss for further cross-modal alignment.\\n\\nWe design the visual-to-word converter network \\\\( C_{v2w} \\\\) to finally obtain the final composed query vector. Specifically, we only train the parameters of \\\\( C_{v2w} \\\\) for zero-shot compositionality. We thus set \\\\( P \\\\) as \\\\( \\\\text{Region-aware triplet} \\\\) with \\\\( RT \\\\), as \\\\( \\\\text{unfrozen} \\\\) to get the final composed query vector. To this end, we reposition the query into the output of \\\\( C_{v2w} \\\\), as \\\\( \\\\text{frozen} \\\\). Baseline SBCIR trains over triplet loss to perform cross-modal matching between sketch and paired photo. With margin enforcing, we retrieve the correct image. We handle this to specify the desired fine-grained matching, particularly when combined with additional information that ensures that the distance between sketch and photo, which ideally would be substituted with real query text during inference. Now, as per our hypothesis, we enforce the compositionality constraint during inference. Thus, to restrict the adverse effect of this additional signal might train a list of neutral text (via GPT) to impose a regularisation loss on image embedding, it may disrupt its grammatical syntax. Furthermore, we mine a set of \\\"neutral text\\\" to impose a regularisation loss for further cross-modal alignment.\\n\\nWe design the visual-to-word converter network \\\\( C_{v2w} \\\\) to finally obtain the final composed query vector. Specifically, we only train the parameters of \\\\( C_{v2w} \\\\) for zero-shot compositionality. We thus set \\\\( P \\\\) as \\\\( \\\\text{Region-aware triplet} \\\\) with \\\\( RT \\\\), as \\\\( \\\\text{unfrozen} \\\\) to get the final composed query vector. To this end, we reposition the query into the output of \\\\( C_{v2w} \\\\), as \\\\( \\\\text{frozen} \\\\). Baseline SBCIR trains over triplet loss to perform cross-modal matching between sketch and paired photo. With margin enforcing, we retrieve the correct image. We handle this to specify the desired fine-grained matching, particularly when combined with additional information that ensures that the distance between sketch and photo, which ideally would be substituted with real query text during inference. Now, as per our hypothesis, we enforce the compositionality constraint during inference. Thus, to restrict the adverse effect of this additional signal might train a list of neutral text (via GPT) to impose a regularisation loss on image embedding, it may disrupt its grammatical syntax. Furthermore, we mine a set of \\\"neutral text\\\" to impose a regularisation loss for further cross-modal alignment.\\n\\nWe design the visual-to-word converter network \\\\( C_{v2w} \\\\) to finally obtain the final composed query vector. Specifically, we only train the parameters of \\\\( C_{v2w} \\\\) for zero-shot compositionality. We thus set \\\\( P \\\\) as \\\\( \\\\text{Region-aware triplet} \\\\) with \\\\( RT \\\\), as \\\\( \\\\text{unfrozen} \\\\) to get the final composed query vector. To this end, we reposition the query into the output of \\\\( C_{v2w} \\\\), as \\\\( \\\\text{frozen} \\\\). Baseline SBCIR trains over triplet loss to perform cross-modal matching between sketch and paired photo. With margin enforcing, we retrieve the correct image. We handle this to specify the desired fine-grained matching, particularly when combined with additional information that ensures that the distance between sketch and photo, which ideally would be substituted with real query text during inference. Now, as per our hypothesis, we enforce the compositionality constraint during inference. Thus, to restrict the adverse effect of this additional signal might train a list of neutral text (via GPT) to impose a regularisation loss on image embedding, it may disrupt its grammatical syntax. Furthermore, we mine a set of \\\"neutral text\\\" to impose a regularisation loss for further cross-modal alignment.\\n\\nWe design the visual-to-word converter network \\\\( C_{v2w} \\\\) to finally obtain the final composed query vector. Specifically, we only train the parameters of \\\\( C_{v2w} \\\\) for zero-shot compositionality. We thus set \\\\( P \\\\) as \\\\( \\\\text{Region-aware triplet} \\\\) with \\\\( RT \\\\), as \\\\( \\\\text{unfrozen} \\\\) to get the final composed query vector. To this end, we reposition the query into the output of \\\\( C_{v2w} \\\\), as \\\\( \\\\text{frozen} \\\\). Baseline SBCIR trains over triplet loss to perform cross-modal matching between sketch and paired photo. With margin enforcing, we retrieve the correct image. We handle this to specify the desired fine-grained matching, particularly when combined with additional information that ensures that the distance between sketch and photo, which ideally would be substituted with real query text during inference. Now, as per our hypothesis, we enforce the compositionality constraint during inference. Thus, to restrict the adverse effect of this additional signal might train a list of neutral text (via GPT) to impose a regularisation loss on image embedding, it may disrupt its grammatical syntax. Furthermore, we mine a set of \\\"neutral text\\\" to impose a regularisation loss for further cross-modal alignment.\\n\\nWe design the visual-to-word converter network \\\\( C_{v2w} \\\\) to finally obtain the final composed query vector. Specifically, we only train the parameters of \\\\( C_{v2w} \\\\) for zero-shot compositionality. We thus set \\\\( P \\\\) as \\\\( \\\\text{Region-aware triplet} \\\\) with \\\\( RT \\\\), as \\\\( \\\\text{unfrozen} \\\\) to get the final composed query vector. To this end, we reposition the query into the output of \\\\( C_{v2w} \\\\), as \\\\( \\\\text{frozen} \\\\). Baseline SBCIR trains over triplet loss to perform cross-modal matching between sketch and paired photo. With margin enforcing, we retrieve the correct image. We handle this to specify the desired fine-grained matching, particularly when combined with additional information that ensures that the distance between sketch and photo, which ideally would be substituted with real query text during inference. Now, as per our hypothesis, we enforce the compositionality constraint during inference. Thus, to restrict the adverse effect of this additional signal might train a list of neutral text (via GPT) to impose a regularisation loss on image embedding, it may disrupt its grammatical syntax. Furthermore, we mine a set of \\\"neutral text\\\" to impose a regularisation loss for further cross-modal alignment.\\n\\nWe design the visual-to-word converter network \\\\( C_{v2w} \\\\) to finally obtain the final composed query vector. Specifically, we only train the parameters of \\\\( C_{v2w} \\\\) for zero-shot compositionality. We thus set \\\\( P \\\\) as \\\\( \\\\text{Region-aware triplet} \\\\) with \\\\( RT \\\\), as \\\\( \\\\text{unfrozen} \\\\) to get the final composed query vector. To this end, we reposition the query into the output of \\\\( C_{v2w} \\\\), as \\\\( \\\\text{frozen} \\\\). Baseline SBCIR trains over triplet loss to perform cross-modal matching between sketch and paired photo. With margin enforcing, we retrieve the correct image. We handle this to specify the desired fine-grained matching, particularly when combined with additional information that ensures that the distance between sketch and photo, which ideally would be substituted with real query text during inference. Now, as per our hypothesis, we enforce the compositionality constraint during inference. Thus, to restrict the adverse effect of this additional signal might train a list of neutral text (via GPT) to impose a regularisation loss on image embedding, it may disrupt its grammatical syntax. Furthermore, we mine a set of \\\"neutral text\\\" to impose a regularisation loss for further cross-modal alignment.\\n\\nWe design the visual-to-word converter network \\\\( C_{v2w} \\\\) to finally obtain the final composed query vector."}
{"id": "CVPR-2024-2693", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To enforce region-wise associativity, we use the patch-wise language prompt as a patch-wise feature. This is done via transformer layer to generate patch-wise feature $L \\\\in \\\\mathbb{R}^{d \\\\times b}$ beyond the seen training set. In particular, at every instance, prompts in the text embedding space, so that it generalises to generate the fixed representation $L$. While we are using learnable learnable continuous prompts depict better performance on \"freehand sketch\" to form our optimum neutral text set.\\n\\n4.3. Generalised Prompt Learning\\n\\nPrompt learning literature [7, 73, 74] dictates that handcrafted fixed language prompts (more in \u00a7 Suppl.) describing a generic description of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a photo of a"}
{"id": "CVPR-2024-2693", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in brown suede, ankle length, stretch in black leather, with silver buckle in shiny white with brown legs with brown leather and silver armrests in shiny red paint in navy blue, with orange logo\\n\\nFigure 3. Top-5 fine-grained retrieval result comparison on ShoeV2/ChairV2. GT photos are green-bordered. (Zoom-in for best-view)\\n\\nalso use the ImageNet-R(endition) [26] image-only dataset, which consists of 30,000 images across 200 ImageNet [16] classes and 16 domains with domain annotations.\\n\\nImplementation Details. We use a pre-trained ViT-L/14 CLIP vision encoder [51] in all experiments with an embedding dimension $d = 768$. The prompt vectors are trained with a learning rate of $10^{-5}$, keeping the encoder frozen (except LayerNorm layers). The UNet decoder and the visual-to-word converter are trained with a learning rate of $10^{-4}$ and $10^{-3}$ respectively. We train the model for 100 epochs using AdamW [40] optimiser with $0.09$ weight decay, and a batch size of 128. Values of $\\\\lambda_1, \\\\lambda_2, \\\\lambda_3, \\\\lambda_4, \\\\lambda_5, \\\\lambda_6$ are set to $1, 0.5, 0.1, 0.1, 1, 1$, empirically.\\n\\nCompetitors. We evaluate from three perspectives \u2013\\n\\n(i) Sketch or Text-only Baselines: Here we validate the additional accuracy gain achieved by sketch+text composition over well-studied individual sketch/text-only retrieval paradigms. For sketch-only baselines, we use triplet loss-based frameworks [55] of B-DINOv2 (S) and B-CLIP (S), using DINOv2 [47] and CLIP [51] ViT-L/14 as backbones respectively. Given the de facto usage of pre-trained CLIP [51] in TBIR, for text-only baselines we introduce B-CLIP (T), which employs frozen CLIP [51] vision and language encoder to retrieve by comparing query text feature against gallery image features.\\n\\n(ii) Sketch+Text Composed SoTA: To judge CLIP's off-the-shelf potential in sketch+text composition, we employ B-CLIP (S+T) which uses the mean of sketch and text features from CLIP's frozen vision and language encoders for retrieval. Now, as most supervised sketch+text SoTA models [1, 11, 59] train using paired textual captions, we employ SoTA image captioner BLIP [35] to generate textual captions for training images which however are often noisy, generic and non-discriminative [35]. Among supervised SoTAs, Combiner [1] trains a network to fuse paired image-text features of frozen CLIP encoders [51] to generate multi-modal query features, while TASK-former [59] merges paired sketch and text features via element-wise addition and trains CLIP's vision and language encoders end-to-end. SceneTrilogy [11] models sketch-text-photo joint-embedding by training an invertible neural network.\\n\\n(iii) Unsupervised Sketch+Text Composition: Leveraging unlabelled photos, Pic2Word [57] learns a textual-inversion network to map input visual query into a pseudo-word token in CLIP's textual embedding space for retrieval. Unlike Pic2Word, SEARLE [2] generates a set of pseudo-word tokens from unlabelled photos using optimisation-based textual-inversion, and then uses those image-token pairs to learn a textual-inversion network. While such methods either train a textual-inversion network on a massive 3M image dataset [57] or use time-consuming optimisation-based textual-inversion for image-token pair generation [2], we exploit pre-trained CLIP model to address sketch+text compositionality in an unsupervised manner without any associated textual descriptions. We adapt Combiner [1], Pic2Word [57], and SEARLE [2], by replacing the input image with sketch. For fairness, we keep the same training/testing paradigm for all competing methods.\\n\\nEvaluation Setup. In the fine-grained setup, we aim to retrieve the target image using the composed query formed by an input sketch and text. We use a train:test split of 90:10 for Sketchy [58] (following [56]), 7000:3000 for FS-COCO [10] and 1015:210 for SketchyCOCO [22]. For ShoeV2/ChairV2 we use 1800/300 (6051/1275) photos (sketches) for training and the rest for testing. Notably, our method does not use captions during training. For evaluation, we manually collect fine-grained captions for each of the test-set images of ShoeV2, ChairV2, and Sketchy. We compose the query as $\\\\{P_L; s_w; \\\\text{[text]}\\\\}$, where \\\\text{[text]} denotes the word embedding of textual query with suitable prepositions (e.g., 'with', 'in'). We use Acc.@q to denote the percentage of sketches with true-matched photos in the top-q retrieved images.\\n\\n5.1. Performance Analysis Tab. 1 delineates the quantitative results while the qualitative ones are shown in Fig. 3. In the fine-grained composed retrieval setup (Tab. 1), our method outperforms baselines and SoTAs significantly on all datasets, indicating its efficiency in seamlessly combining fine-grained sketch with textual description. This gain is likely due to the regularisation...\"}"}
{"id": "CVPR-2024-2693", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Results for fine-grained object-level and scene-level composed retrieval.\\n\\n| Methods          | Object-level | Scene-level |\\n|------------------|--------------|-------------|\\n|                  | Acc.@5       | Acc.@10     | Acc.@5       | Acc.@10     | Acc.@5       | Acc.@10     |\\n| ShoeV2 ChairV2   | 9.8          | 17.5        | 16.7         | 18.4        | 6.8          | 11.3        |\\n| Sketchy          | 5.9          | 9.7         | 6.8          | 10.2        |\\n| FS-COCO          | 10.2         | 19.4        | 17.9         | 20.2        | 8.5          | 15.1        |\\n| SketchyCOCO      | 7.6          | 11.4        | 9.4          | 12.2        |\\n| B-CLIP (S)       | 9.1          | 16.6        | 15.4         | 17.8        | 5.9          | 10.2        |\\n| B-DINOv2 (S)     |              |             |              |             |              |             |\\n| B-CLIP (T)       | 9.1          | 16.6        | 15.4         | 17.8        | 5.9          | 10.2        |\\n| B-CLIP (S+T)     | 19.1         | 30.8        | 30.2         | 32.3        | 10.1         | 20.2        |\\n| Combiner [1]     | 24.7         | 40.2        | 35.7         | 39.9        | 15.7         | 33.7        |\\n| TASK-former [59]| 27.7         | 44.1        | 40.7         | 45.2        | 17.8         | 35.2        |\\n| SceneTrilogy [11]| 29.1         | 46.2        | 43.4         | 46.8        | 19.7         | 37.2        |\\n| Pic2Word [57]    | 34.7         | 58.4        | 55.7         | 62.1        | 22.5         | 48.7        |\\n| SEARLE [2]       | 38.4         | 64.8        | 60.8         | 66.4        | 25.3         | 54.2        |\\n| Proposed         | **47.3**     | **79.1**    | **73.5**     | **81.4**    | **30.6**     | **64.2**    |\\n\\nAvg. Improvement: +24.7 +45.5 +38.3 +42.6 +15.9 +34.6 +11.3 +22.4 +17.8 +32.5\\n\\nThe region-aware contrastive loss and generator guidance provided by our method allow it to handle composed retrieval by either training data-hungry textual-inversion networks or using optimisation-based textual-inversion for image-token pair generation followed by training an inversion network. Surprisingly, our method achieves the highest Acc.@5 of 47.3 (73.5) in ShoeV2 (ChairV2) without the data-requirement of Pic2Word [57], or the complicated two-stage approach of SEARLE [2].\\n\\nBeing more challenging than object-level [9], baseline methods perform quite poorly (Tab. 1) for scene-image retrieval. However, thanks to the increased interaction capability (learned via compositionality constraint) of the pseudo-word token with user-given textual queries during inference, we surpass others with an Acc.@5 of 22.7 (33.4).\\n\\n5.2. Downstream Tasks\\n\\nSketch+Text-based Fine-Grained Image Generation. Apart from composed retrieval, our method is also suitable for sketch+text composed object image generation. Here we replace the low-quality UNet decoder with a StyleGAN2 [29] generator (pre-trained on specific classes). The composed query $T,q_L$ (acting as a latent vector [29]) upon passing through the frozen StyleGAN2 generates output photos. We pass $s_T,q_L$ via a learnable FC-layers to convert it to the dimensions required by StyleGAN2's affine transformation layer [29]. Fig. 4 shows a few cases of such fine-grained generation. Notably, the semantic geometry (e.g., shape, structure, etc.) of generated images is driven by input sketches, whereas the high-level appearance (e.g., colour, shade, etc.) is mostly governed by textual descriptions. Overall our method archives a lower FID [29, 34] score of 33.4 (88.5) on ShoeV2 (ChairV2) test-set images compared to 35.85 (90.21) of the current SoTA [31].\\n\\nObject Sketch-based Scene Image Retrieval. It aims to retrieve scene images from a single-object sketch and additional captions. Here, we use 7000 (3000) and 1015 (210) train (test) sketch-photo pairs from FS-COCO and SketchyCOCO respectively. Since they lack single-object sketches, we source them from Sketchy, which is their superset in terms of classes [10]. Here, a retrieval is deemed correct if it contains all objects that were queried in the sketch and text.\\n\\nAs FS-COCO/SketchyCOCO images are derived from MS-COCO [38], we use their segmentation-map labels from MS-COCO to create ground truth object-lists per test-set image. During inference, we use the readily available captions of test-set images of FS-COCO/SketchyCOCO, but remove the object-name queried via sketch (Fig. 5). Here we compose the query like in the fine-grained composed retrieval setup and use Acc.@q as evaluation metric. Tab. 2 shows our method to surpass other baseline and SoTAs with an average Acc.@5 gain of 10.9 on FS-COCO [10].\"}"}
{"id": "CVPR-2024-2693", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"retrieved images in the top-q list to all relevant images for a given query. As strict domain constraints complicate cross-modal composed image retrieval, baseline methods perform poorly here, with B-CLIP (S+T) attaining an average r@50 of 12.9 across four test domains (Tab. 2). Due to their respective CLIP \\\\[51\\\\] feature composition strategies, Combiner \\\\[1\\\\], Pic2Word \\\\[57\\\\] and SEARLE \\\\[2\\\\] depict reasonable performance on ImageNet-R, while our method achieves a notable average r@10 of 15.3.\\n\\nQuery Ours B-CLIP (Sketch + Text)\\n\\nTable 2. Results for domain attribute transfer and object sketch-based scene image retrieval.\\n\\n| Methods       | Domain transfer | Object sketch-based scene retrieval |\\n|---------------|-----------------|-------------------------------------|\\n|               | ImageNet-R      | FS-COCO SketchyCOCO                 |\\n|               | r@10            | Acc.@5 Acc.@10                       |\\n|               | r@50            | Acc.@5 Acc.@10                       |\\n| B-CLIP (S+T)  | 2.2             | 4.3 7.0 10.1 26.7                   |\\n| Combiner \\\\[1\\\\]| 8.3             | 10.8 21.1 16.7 30.9                 |\\n| TASK-former \\\\[59\\\\]| 7.6           | 11.6 23.1 18.2 35.8                 |\\n| SceneTrilogy \\\\[11\\\\]| 9.7           | 14.2 25.3 21.0 40.8                 |\\n| Pic2Word \\\\[57\\\\]| 10.8            | \u2013 \u2013 \u2013 \u2013                              |\\n| SEARLE \\\\[2\\\\]  | 12.1            | \u2013 \u2013 \u2013 \u2013                              |\\n| Proposed      | **15.3**        | **21.2 40.4**                        |\\n| Avg. Improvement | **+6.8**     | **+10.9 +21.2**                      |\\n\\nTable 3. Ablation on design.\\n\\n| Methods        | ShoeV2 | ChairV2 | FS-COCO |\\n|----------------|--------|---------|---------|\\n| w/o LTT        | 41.8   | 71.9    | 68.3    |\\n| w/o Lrec       | 40.7   | 72.8    | 70.9    |\\n| w/o LRT        | 40.2   | 71.6    | 69.1    |\\n| w/o compositionality | 32.5 | 48.2    | 45.7    |\\n| Ours-full      | **47.3**| **79.1**| **73.5**|\\n| Acc.@5         | 98.35% | 98.79%  | 99.11%  |\\n| Acc.@10        | 98.35% | 98.79%  | 99.11%  |\\n\\n5.3. Ablation on Design\\n\\n\u2022 How well does \\\\( s_w \\\\) capture sketch semantics?\\n\\nTo judge the efficacy of pseudo-word token \\\\( s_w \\\\) in representing visual content of a sketch, we evaluate our trained model on retrieving an input sketch solely from its pseudo-word token without additional textual description. Acc.@1 of 98.35\\\\% on ShoeV2 test-set sketches shows the pseudo-word token to capture visual sketch features fairly well. Although representing fine-grained query sketches with one pseudo-word token might be sub-optimal, experimenting with two and three such tokens delivered similar Acc.@1 of 98.79\\\\% and 99.11\\\\% respectively on ShoeV2. We thus stick to a single-word token for computational ease.\\n\\n\u2022 Contribution of \\\\( L_{RT} \\\\) and \\\\( L_{rec} \\\\):\\n\\nRegion-aware local features are pivotal in bridging the huge domain gap between sparse-binary sketches and pixel-dense photos. Although less reflected on ShoeV2/ChairV2 results, a notable Acc.@5 drop of 12.2\\\\% on FS-COCO (Tab. 3) for w/o \\\\( L_{RT} \\\\) verifies its importance in the scene-level setup. Furthermore, a 13.9\\\\% drop in Acc.@5 (ShoeV2) for w/o \\\\( L_{rec} \\\\) shows that fine-grained matching remains incomplete without the proposed generator guidance.\\n\\n\u2022 Impact of \\\\( L_{TT} \\\\):\\n\\nRemoving Text-to-Text loss plummets Acc.@5 by 11.6\\\\% on ShoeV2 (Tab. 3), highlighting its vital role in aligning learned and English language prompts seen by CLIP \\\\[51\\\\] during its training. This removal likely pushes the final language embedding towards sparser parts of CLIP language manifold \\\\[2\\\\], hindering effective communication with real-language tokens during inference \\\\[2\\\\].\\n\\n\u2022 Why Compositionality Constraint?\\n\\nIntroduced via the novel idea of neutral text, compositionality constraint helps preserve internal grammar of CLIP language manifold, to allow optional user-provided query texts during inference. On removing that, Acc.@5 drops the lowest across all datasets (Tab. 3), proving its importance in our framework.\\n\\n\u2022 On combining sketch and text:\\n\\nOur composed retrieval pipeline places the query feature closer to its paired photo in the latent retrieval space (Fig. 7), than only sketch/text-based retrieval. The relative distances (Fig. 7 top insets) depict that the individual text and sketch feature together pushes the composed feature towards the paired photo.\\n\\n6. Conclusion and Future Works\\n\\nIn conclusion, our exploration into the fine-grained representation power of both sketch and text, coupled with the orchestration of their synergistic interplay, marks a significant stride in the realm of image retrieval. By harmonising sketches and text, we offer users a retrieval experience that transcends traditional category-level distinctions. The introduction of a novel compositionality framework, driven by pre-trained CLIP models, eliminates the need for extensive fine-grained textual annotations. Last but not least, our system extends its utility to diverse domains such as sketch+text-based fine-grained image generation, object-sketch-based scene retrieval, domain attribute transfer, etc.\"}"}
{"id": "CVPR-2024-2693", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Alberto Baldrati, Marco Bertini, Tiberio Uricchio, and Alberto Del Bimbo. Effective Conditioned and Composed Image Retrieval Combining CLIP-Based Features. In CVPR, 2022.\\n\\n[2] Alberto Baldrati, Lorenzo Agnolucci, Marco Bertini, and Alberto Del Bimbo. Zero-Shot Composed Image Retrieval with Textual Inversion. In ICCV, 2023.\\n\\n[3] Ayan Kumar Bhunia, Yongxin Yang, Timothy M Hospedales, Tao Xiang, and Yi-Zhe Song. Sketch Less for More: On-the-Fly Fine-Grained Sketch Based Image Retrieval. In CVPR, 2020.\\n\\n[4] Ayan Kumar Bhunia, Subhadeep Koley, Abdullah Faiz Ur Rahman Khilji, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. Sketching without Worrying: Noise-Tolerant Sketch-Based Image Retrieval. In CVPR, 2022.\\n\\n[5] Ayan Kumar Bhunia, Aneeshan Sain, Parth Hiren Shah, Animesh Gupta, Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. Adaptive Fine-Grained Sketch-Based Image Retrieval. In ECCV, 2022.\\n\\n[6] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language Models are Few-Shot Learners. In NeurIPS, 2020.\\n\\n[7] Adrian Bulat and Georgios Tzimiropoulos. LASP: Text-to-Text Optimization for Language-Aware Soft Prompting of Vision & Language Models. In CVPR, 2023.\\n\\n[8] Lluis Castrejon, Yusuf Aytar, Carl Von Dronick, Hamed Pirsiavash, and Antonio Torralba. Learning Aligned Cross-Modal Representations from Weakly Aligned Data. In CVPR, 2016.\\n\\n[9] Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Viswanatha Reddy Gajjala, Aneeshan Sain, Tao Xiang, and Yi-Zhe Song. Partially Does It: Towards Scene-Level FG-SBIR with Partial Input. In CVPR, 2022.\\n\\n[10] Pinaki Nath Chowdhury, Aneeshan Sain, Yulia Gryaditskaya, Ayan Kumar Bhunia, Tao Xiang, and Yi-Zhe Song. FS-COCO: Towards Understanding of Freehand Sketches of Common Objects in Context. In ECCV, 2022.\\n\\n[11] Pinaki Nath Chowdhury, Ayan Kumar Bhunia, Aneeshan Sain, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song. SceneTrilogy: On Human Scene-Sketch and its Complementarity with Photo and Text. In CVPR, 2023.\\n\\n[12] Sanghyuk Chun, Seong Joon Oh, Rafael Sampaio De Rezende, Yannis Kalantidis, and Diane Larlus. Probabilistic Embeddings for Cross-Modal Retrieval. In CVPR, 2021.\\n\\n[13] Niv Cohen, Rinon Gal, Eli A Meirom, Gal Chechik, and Yuval Atzmon. \\\"This is my unicorn, Fluffy\\\": Personalizing frozen vision-language representations. In ECCV, 2022.\\n\\n[14] John Collomosse, Tu Bui, Michael J Wilber, Chen Fang, and Hailin Jin. Sketching with Style: Visual Search with Sketches and Aesthetic Context. In ICCV, 2017.\\n\\n[15] John Collomosse, Tu Bui, and Hailin Jin. LiveSketch: Query Perturbations for Guided Sketch-based Visual Search. In CVPR, 2019.\\n\\n[16] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.\\n\\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR, 2021.\\n\\n[18] Anjan Dutta and Zeynep Akata. Semantically Tied Paired Cycle Consistency for Zero-Shot Sketch-based Image Retrieval. In CVPR, 2019.\\n\\n[19] Aviv Eisenschtat and Lior Wolf. Linking Image and Text with 2-Way Nets. In CVPR, 2017.\\n\\n[20] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja Fidler. VSE++: Improving Visual-Semantic Embeddings with Hard Negatives. In BMVC, 2018.\\n\\n[21] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An Image is Worth One Word: Personalizing Text-to-Image Generation using Textual Inversion. In ICLR, 2023.\\n\\n[22] Chengying Gao, Qi Liu, Qi Xu, Limin Wang, Jianzhuang Liu, and Changqing Zou. SketchyCOCO: Image Generation from Freehand Scene Sketches. In CVPR, 2020.\\n\\n[23] Sonam Goenka, Zhaoheng Zheng, Ayush Jaiswal, Rakesh Chada, Yue Wu, Varsha Hedau, and Pradeep Natarajan. FashionVLP: Vision Language Transformer for Fashion Retrieval with Feedback. In CVPR, 2022.\\n\\n[24] Cusuh Ham, Gemma Canet Tarres, Tu Bui, James Hays, Zhe Lin, and John Collomosse. CoGS: Controllable Generation and Search from Sketch and Style. In ECCV, 2012.\\n\\n[25] Xiao Han, Licheng Yu, Xiatian Zhu, Li Zhang, Yi-Zhe Song, and Tao Xiang. FashionViL: Fashion-Focused Vision-and-Language Representation Learning. In ECCV, 2022.\\n\\n[26] Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadam, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The Many Faces of Robustness: A Critical Analysis of Out-of-Distribution Generalization. In ICCV, 2021.\\n\\n[27] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling Up Visual and Vision-Language Representation Learning With Noisy Text Supervision. In ICML, 2021.\\n\\n[28] Andrej Karpathy and Li Fei-Fei. Deep Visual-Semantic Alignments for Generating Image Descriptions. In CVPR, 2015.\\n\\n[29] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and Improving the Image Quality of StyleGAN. In CVPR, 2020.\\n\\n[30] Muhammad Uzair Khattak, Hanoona Rasheed, Muhammad Maaz, Salman Khan, and Fahad Shahbaz Khan. MaPLe: Multi-modal Prompt Learning. In CVPR, 2023.\"}"}
{"id": "CVPR-2024-2693", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Subhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. Picture that Sketch: Photorealistic Image Generation from Abstract Sketches. In CVPR, 2023.\\n\\nSubhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. How to Handle Sketch-Abstraction in Sketch-Based Image Retrieval? In CVPR, 2024.\\n\\nSubhadeep Koley, Ayan Kumar Bhunia, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. Text-to-Image Diffusion Models are Great Sketch-Photo Match-makers. In CVPR, 2024.\\n\\nSubhadeep Koley, Ayan Kumar Bhunia, Deeptanshu Sekhri, Aneeshan Sain, Pinaki Nath Chowdhury, Tao Xiang, and Yi-Zhe Song. It's All About Your Sketch: Democratising Sketch Control in Diffusion Models. In CVPR, 2024.\\n\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models. arXiv preprint arXiv:2301.12597, 2023.\\n\\nXiujun Li, Xi Yin, Chunyuan Li, Pengchuan Zhang, Xiaowei Hu, Lei Zhang, Lijuan Wang, Houdong Hu, Li Dong, Furu Wei, et al. Oscar: Object-Semantics Aligned Pre-training for Vision-Language Tasks. In ECCV, 2020.\\n\\nHangyu Lin, Yanwei Fu, Peng Lu, Shaogang Gong, Xiangyang Xue, and Yu-Gang Jiang. TC-Net for iSBIR: Triplet Classification Network for Instance-level Sketch Based Image Retrieval. In ACM MM, 2019.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft COCO: Common Objects in Context. In ECCV, 2014.\\n\\nLi Liu, Fumin Shen, Yuming Shen, Xianglong Liu, and Ling Shao. Deep Sketch Hashing: Fast Free-hand Sketch-Based Image Retrieval. In CVPR, 2017.\\n\\nIlya Loshchilov and Frank Hutter. Decoupled Weight Decay Regularization. In ICLR, 2019.\\n\\nHuaishao Luo, Junwei Bao, Youzheng Wu, Xiaodong He, and Tianrui Li. SegCLIP: Patch Aggregation with Learnable Centers for Open-Vocabulary Semantic Segmentation. In ICML, 2023.\\n\\nAryan Mikaeili, Or Perel, Mehdi Safaee, Daniel Cohen-Or, and Ali Mahdavi-Amiri. SKED: Sketch-guided Text-based 3D Editing. In CVPR, 2023.\\n\\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tanick, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis. In ECCV, 2018.\\n\\nChong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, Ying Shan, and Xiaohu Qie. T2I-Adapter: Learning Adapters to Dig out More Controllable Ability for Text-to-Image Diffusion Models. arXiv preprint arXiv:2302.08453, 2023.\\n\\nThomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant Neural Graphics Primitives with a Multiresolution Hash Encoding. ACM TOG, 2022.\\n\\nVinod Nair and Geoffrey E Hinton. Rectified Linear Units Improve Restricted Boltzmann Machines. In ICML, 2010.\\n\\nMaxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. DINOv2: Learning Robust Visual Features without Supervision. arXiv preprint arXiv:2304.07193, 2023.\\n\\nKaiyue Pang, Yi-Zhe Song, Tony Xiang, and Timothy M Hospedales. Cross-domain Generative Learning for Fine-Grained Sketch-Based Image Retrieval. In BMVC, 2017.\\n\\nKaiyue Pang, Ke Li, Yongxin Yang, Honggang Zhang, Timothy M Hospedales, Tao Xiang, and Yi-Zhe Song. Generalising Fine-Grained Sketch-Based Image Retrieval. In CVPR, 2019.\\n\\nBryan A Plummer, Paige Kordas, M Hadi Kiapour, Shuai Zheng, Robinson Piramuthu, and Svetlana Lazebnik. Conditional Image-Text Embedding Networks. In ECCV, 2018.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning Transferable Visual Models From Natural Language Supervision. In ICML, 2021.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional Networks for Biomedical Image Segmentation. In MICCAI, 2015.\\n\\nAneeshan Sain, Ayan Kumar Bhunia, Yongxin Yang, Tao Xiang, and Yi-Zhe Song. Cross-Modal Hierarchical Modelling for Fine-Grained Sketch Based Image Retrieval. In BMVC, 2020.\\n\\nAneeshan Sain, Ayan Kumar Bhunia, Yongxin Yang, Tao Xiang, and Yi-Zhe Song. StyleMeUp: Towards Style-Agnostic Sketch-Based Image Retrieval. In CVPR, 2021.\\n\\nAneeshan Sain, Ayan Kumar Bhunia, Pinaki Nath Chowdhury, Aneeshan Sain, Subhadeep Koley, Tao Xiang, and Yi-Zhe Song. CLIP for All Things Zero-Shot Sketch-Based Image Retrieval, Fine-Grained or Not. In CVPR, 2023.\\n\\nAneeshan Sain, Ayan Kumar Bhunia, Subhadeep Koley, Pinaki Nath Chowdhury, Soumitri Chattopadhyay, Tao Xiang, and Yi-Zhe Song. Exploiting Unlabelled Photos for Stronger Fine-Grained SBIR. In CVPR, 2023.\\n\\nKuniaki Saito, Kihyuk Sohn, Xiang Zhang, Chun-Liang Li, Chen-Yu Lee, Kate Saenko, and Tomas Pfister. Pic2Word: Mapping Pictures to Words for Zero-shot Composed Image Retrieval. In CVPR, 2023.\\n\\nPatsorn Sangkloy, Nathan Burnell, Cusuh Ham, and James Hays. The Sketchy Database: Learning to Retrieve Badly Drawn Bunnies. ACM TOG, 2016.\\n\\nPatsorn Sangkloy, Wittawat Jitkrittum, Diyi Yang, and James Hays. A Sketch Is Worth a Thousand Words: Image Retrieval with Text and Sketch. In ECCV, 2022.\\n\\nJifei Song, Yi-Zhe Song, Tony Xiang, and Timothy M Hospedales. Fine-Grained Image Retrieval: the Text/Sketch Input Dilemma. In BMVC, 2017.\\n\\nJifei Song, Qian Yu, Yi-Zhe Song, Tao Xiang, and Timothy M Hospedales. Deep Spatial-Semantic Attention for Fine-Grained Sketch-Based Image Retrieval. In ICCV, 2017.\"}"}
{"id": "CVPR-2024-2693", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Nam Vo, Lu Jiang, Chen Sun, Kevin Murphy, Li-Jia Li, Li Fei-Fei, and James Hays. Composing Text and Image for Image Retrieval - An Empirical Odyssey. In CVPR, 2019.\\n\\nAndrey Voynov, Kfir Aberman, and Daniel Cohen-Or. Sketch-Guided Text-to-Image Diffusion Models. In SIGGRAPH Asia, 2023.\\n\\nZihao Wang, Xihui Liu, Hongsheng Li, Lu Sheng, Junjie Yan, Xiaogang Wang, and Jing Shao. CAMP: Cross-Modal Adaptive Message Passing for Text-Image Retrieval. In ICCV, 2019.\\n\\nKilian Q Weinberger and Lawrence K Saul. Distance Metric Learning for Large Margin Nearest Neighbor Classification. JMLR, 2009.\\n\\nHui Wu, Yupeng Gao, Xiaoxiao Guo, Ziad Al-Halah, Steven Rennie, Kristen Grauman, and Rogerio Feris. Fashion IQ: A New Dataset Towards Retrieving Images by Natural Language Feedback. In CVPR, 2021.\\n\\nJiaqing Xu, Haifeng Sun, Qi Qi, Jingyu Wang, Ce Ge, Lejian Zhang, and Jianxin Liao. DLA-Net for FG-SBIR: Dynamic Local Aligned Network for Fine-Grained Sketch-Based Image Retrieval. In ACM MM, 2021.\\n\\nShuai Yang, Zhangyang Wang, Jiaying Liu, and Zongming Guo. Deep Plastic Surgery: Robust and Controllable Image Editing with Human-Drawn Sketches. In ECCV, 2020.\\n\\nSasi Kiran Yelamarthi, Shiva Krishna Reddy, Ashish Mishra, and Anurag Mittal. A Zero-Shot Framework for Sketch-based Image Retrieval. In ECCV, 2018.\\n\\nQian Yu, Feng Liu, Yi-Zhe Song, Tao Xiang, Timothy M Hospedales, and Chen-Change Loy. Sketch Me That Shoe. In CVPR, 2016.\\n\\nHua Zhang, Peng She, Yong Liu, Jianhou Gan, Xiaochun Cao, and Hassan Foroosh. Learning Structural Representations via Dynamic Object Landmarks Discovery for Sketch Recognition and Retrieval. IEEE TIP, 2019.\\n\\nLvmin Zhang and Maneesh Agrawala. Adding Conditional Control to Text-to-Image Diffusion Models. In ICCV, 2023.\\n\\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional Prompt Learning for Vision-Language Models. In CVPR, 2022.\\n\\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to Prompt for Vision-Language Models. IJCV, 2022.\"}"}
