{"id": "CVPR-2024-851", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ \\\\Delta \\\\hat{\\\\theta}_E^i = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\Delta \\\\theta_E^i = 1 - \\\\eta \\\\left( \\\\frac{g_i^p}{\\\\hat{g}_i^p} + \\\\frac{g_j^q}{\\\\hat{g}_j^q} \\\\right), \\\\]\\n\\nit means the update for the aggregated encoder mirrors the update of the shared encoder in MTL, if we regard the optimizer as capable of automatically scaling the learning rate \\\\( \\\\eta \\\\) in Eq. (3). While FL typically aggregates client models after several local training epochs in a single communication round, this implies that there can be differences between the learning processes of MTL and FL:\\n\\nTheorem 1 (Difference in optimizing MTL and FL)\\n\\nGiven clients with a shared encoder and multiple task-specific decoders structure, the gradient descent in the shared encoder of Multi-Task Learning is equivalent to averaging parameter aggregation in Federated Learning, adding an extra term \\\\( \\\\nabla \\\\theta (0) \\\\langle \\\\hat{g}_i^p, \\\\hat{g}_j^q \\\\rangle \\\\) that maximizes the inner product of gradients \\\\( \\\\hat{g}_i^p \\\\) and \\\\( \\\\hat{g}_j^q \\\\) between all pairs of tasks \\\\( i \\\\) and \\\\( j \\\\) in each iteration \\\\( p \\\\) and \\\\( q \\\\).\\n\\nWe provide definitions, proofs, and in-depth analysis in Appendix A. As the inner product of gradients is a measure of accordance, maximizing the inner product is equal to reducing the conflict of gradients \\\\[5\\\\]. Hence, Theorem 1 states the necessity of integrating optimization techniques to mitigate gradient conflicts during encoder aggregation in HC-FMTL. Inspired by CAGrad \\\\[39\\\\], for each communication round, we aim to find an optimal aggregated update \\\\( \\\\tilde{U} \\\\) for the encoders that minimizes conflicts while optimizing the main objective with optimization problem:\\n\\n\\\\[\\n\\\\max_{\\\\tilde{U}} \\\\min_i \\\\langle \\\\Delta \\\\theta_E^i, \\\\tilde{U} \\\\rangle \\\\quad s.t. \\\\quad \\\\| \\\\tilde{U} - \\\\Delta \\\\bar{\\\\theta}_E \\\\| \\\\leq c \\\\| \\\\Delta \\\\bar{\\\\theta}_E \\\\|\\n\\\\]\\n\\nwhere \\\\( \\\\Delta \\\\bar{\\\\theta}_E = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\Delta \\\\theta_E^i \\\\) is the average parameter update and \\\\( c \\\\in [0, 1) \\\\) is a hyper-parameter controlling the convergence rate. Here \\\\( \\\\min_i \\\\langle \\\\Delta \\\\theta_E^i, \\\\tilde{U} \\\\rangle \\\\) measures the maximum conflict between client updates and the target update, which is an approximation to the conflict between gradients, as the server only receives parameter updates after several local training epochs rather than the gradients in each iteration. Therefore, maximizing this term can minimize the conflict in parameter optimization, which is consistent with our findings in Theorem 1. With constraint \\\\( \\\\sum_{i=1}^{N} w_i = 1 \\\\), \\\\( w_i \\\\geq 0 \\\\), solving this problem using Lagrangian simplifies to:\\n\\n\\\\[\\n\\\\min w F(w) = U^\\\\top w \\\\Delta \\\\bar{\\\\theta}_E + p \\\\phi \\\\| U w \\\\|_2\\n\\\\]\\n\\nwhere \\\\( U w = \\\\frac{1}{N} \\\\sum_{i=1}^{N} w_i \\\\Delta \\\\theta_E^i \\\\), \\\\( \\\\phi = c^2 \\\\| \\\\Delta \\\\bar{\\\\theta}_E \\\\|_2 \\\\).\\n\\nUpon finding the optimum \\\\( w^* \\\\) and the optimal \\\\( \\\\lambda^* = \\\\| U w^* \\\\|_2 / \\\\phi^{1/2} \\\\), we have the unified aggregated update for encoder parameters:\\n\\n\\\\[\\n\\\\tilde{U} = \\\\Delta \\\\bar{\\\\theta}_E + U w^*/\\\\lambda^* = \\\\Delta \\\\bar{\\\\theta}_E + \\\\sqrt{\\\\phi} \\\\| U w \\\\|_2 U w.\\n\\\\]\\n\\n3.4. Hyper Cross Attention Aggregation\\n\\nThe significance of task interaction in MTL is well-established \\\\[7, 20, 57, 74, 81\\\\], as it allows for exchanging knowledge among tasks and benefiting from complementary information. In representative methods \\\\[57, 81\\\\], task interaction is facilitated by adding the target task's feature with those from source tasks in decoders, formulated as:\\n\\n\\\\[\\nz_{Di} = \\\\frac{1}{N} \\\\sum_{j=1}^{N} \\\\gamma_{i,j}(\\\\theta_{Dj})^\\\\top z_{E}, \\\\quad \\\\forall i \\\\in \\\\{1, \\\\ldots, N\\\\}\\n\\\\]\\n\\nwhere \\\\( z_E \\\\) denotes the output feature of the shared encoder, \\\\( \\\\theta_{Dj} \\\\) represents the decoder of task \\\\( j \\\\), and \\\\( (\\\\theta_{Dj})^\\\\top z_E \\\\) yields task-specific feature from the decoder. The coefficient \\\\( \\\\gamma_{i,j} \\\\) manages the flow of feature from source task \\\\( j \\\\) to target task \\\\( i \\\\) within the interaction and is usually a learnable parameter representing the inter-task correlation. To emulate this task interaction within the FL context, we intuitively aggregate the decoder parameters as follows:\\n\\n\\\\[\\n\\\\tilde{\\\\theta}_{Di} = \\\\frac{1}{N} \\\\sum_{j=1}^{N} \\\\gamma_{i,j} \\\\theta_{Dj}.\\n\\\\]\\n\\nDue to model incongruity in the HC-FMTL environment, the decoder parameters sent to the server originate from diverse clients with heterogeneous tasks. This intricacy leads to a complex landscape where decoders may align or diverge in both data domain and task type, requiring the aggregation process to discern the nuanced relationships among them. Our approach improves the naive decoder aggregation by adopting a cross attention mechanism to further promote the exchange of inter-task knowledge among clients with model incongruity. It calculates dependencies among the local updates of \\\\( K \\\\) decoders, thereby modeling the interplay among tasks.\\n\\nRecognizing that decoders often exhibit varied utilities across different network layers \\\\[6, 21, 47, 76\\\\], we further apply a layer-wise strategy \\\\[51\\\\] to precisely capture the cross-task attention at each decoder layer, allowing for a more fine-grained personalized aggregation that can benefit the transfer of task-specific knowledge. The computation of aggregation based on cross attention is defined as:\\n\\n\\\\[\\nV_l = [\\\\Delta \\\\theta_{D1,l}, \\\\ldots, \\\\Delta \\\\theta_{DK,l}]^\\\\top,\\n\\\\]\\n\\n\\\\[\\n\\\\tilde{A}_{il} = \\\\text{Softmax} \\\\left( \\\\Delta \\\\theta_{Dil} V_l^\\\\top / \\\\sqrt{d} \\\\right) V_l,\\n\\\\]\\n\\nwhere \\\\([\\\\cdot, \\\\cdot]\\\\) indicates concatenation, \\\\( \\\\Delta \\\\theta_{Dil} \\\\) and \\\\( \\\\tilde{A}_{il} \\\\) are the original update and aggregated update for the \\\\( l \\\\)-th layer of the \\\\( i \\\\)-th decoder, with a dimension of \\\\( d \\\\).\\n\\n3.5. Hyper Aggregation Weights\\n\\nAs pointed out by pFL, a unified update for all clients is restricted in addressing client heterogeneity. Hence, we propose Hyper Aggregation Weights, which adaptively assess the importance of the aggregated parameters from peers and...\"}"}
{"id": "CVPR-2024-851", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison to representative methods using PASCAL-Context for five single-task clients and NYUD-v2 for one multi-task client. \u2018\u2191\u2019 means higher is better and \u2018\u2193\u2019 means lower is better. \u2018\u2206m%\u2019 denotes the average performance drop w.r.t. local baseline.\\n\\n| Method      | PASCAL-Context (ST) | NYUD-v2 (MT) | \u2206m% |\\n|-------------|---------------------|--------------|------|\\n|              | mIoU                |               | mIoU |\\n|              | \u2191                   | \u2191             | \u2191    |\\n|              | maxF                |               | maxF |\\n|              | mErr                |               | mErr |\\n|              | RMSE                |               | RMSE |\\n\\n|               |                      |               |      |\\n|               |                      |               |      |\\n\\nLocal 51.69 49.94 80.91 15.76 71.95 41.86 0.6487 20.59 76.46 0.00\\nFedAvg [55] 39.98 37.33 77.56 18.27 69.17 38.94 0.7858 21.62 75.77 -11.76\\nFedProx [35] 44.42 38.10 77.26 18.03 69.39 39.19 0.8068 21.52 76.03 -10.68\\nFedPer [2] 54.51 46.56 78.85 16.95 71.00 44.02 0.6467 21.19 76.61 -1.11\\nDitto [36] 46.23 39.69 77.99 17.52 69.77 41.49 0.6508 20.60 76.45 -5.57\\nFedBABU [59] 52.60 46.41 79.47 16.88 71.13 43.98 0.6361 21.14 76.70 -1.19\\nFedAMP [25] 55.98 52.05 80.79 15.74 72.02 41.67 0.6428 20.54 76.40 1.47\\nFedSTA [60] 52.24 50.68 77.97 16.90 70.07 35.62 0.7384 22.87 75.35 -5.80\\nMaT-FL [8] 57.45 48.63 79.26 17.26 71.23 40.99 0.6352 20.65 76.59 -0.46\\nFedHCA 57.55 52.30 80.71 15.60 72.08 41.47 0.6281 20.53 76.50 2.18\\n\\nEmpower clients with analogous data domains and task objectives to have higher aggregation weights. This enhancement reinforces the mutual contribution from complementary information, thus serving as high-level guidance in harmonizing the local updates with the collaborative updates.\\n\\nSpecifically, the server maintains a dedicated set of weights for each client, which are applied as follows in the personalized aggregation:\\n\\n$$\\\\theta^r_i = \\\\theta^{r-1}_i + \\\\Delta \\\\theta^r_i + \\\\psi_i \\\\tilde{\\\\theta}_i,$$\\n\\nwhere $$\\\\psi_i$$ denotes the hyper weights for client $$C_i$$, i.e., $$\\\\alpha_i$$ for encoder or $$\\\\beta_i$$ for decoder, and $$\\\\tilde{\\\\theta}_i$$ is the aggregated update $$\\\\tilde{U}$$ from Eq. (9) or $$\\\\tilde{A}_{i,l}$$ from Eq. (13). It is worth noting that we implement distinct weights for each decoder layer rather than a single weight value to be consistent with the layer-wise computation of cross attention.\\n\\nFurthermore, we design Hyper Aggregation Weights to be learnable parameters that are dynamically updated throughout the training phase. This adaptability ensures that the weights are optimized in conjunction with the system's overall objective. By employing the chain rule, we can derive the gradient of $$\\\\psi_i$$ as follows:\\n\\n$$\\\\nabla \\\\psi_i L_i = \\\\left( \\\\nabla \\\\psi_i \\\\theta^{r-1}_i \\\\right)^\\\\top \\\\nabla \\\\theta^{r-1}_i L_i = (\\\\tilde{\\\\theta}_i)^\\\\top \\\\nabla \\\\theta^{r-1}_i L_i.$$\\n\\nTo better align this update rule with the FL paradigm, we can reformulate Eq. (15) by substituting gradients with parameter updates, which is the negative accumulation of gradients over batches, resulting in the updating process:\\n\\n$$\\\\alpha^r_i = \\\\alpha^{r-1}_i + \\\\Delta \\\\alpha_i,$nabla \\\\alpha_i \\\\Delta \\\\theta_E,$$\\n\\n$$\\\\beta^r_{i,l} = \\\\beta^{r-1}_{i,l} + \\\\Delta \\\\beta_{i,l},$$\\n\\n$$\\\\nabla \\\\beta_{i,l} \\\\Delta \\\\theta_D,$$\\n\\nIt indicates that the update of Hyper Aggregation Weights can be attained by the alteration in model parameters following local training in subsequent communication rounds.\\n\\n4. Experiments\\n\\n4.1. Experimental Setup\\n\\nDatasets. We conduct experiments with two established benchmark datasets for multi-task dense predictions: PASCAL-Context [58] and NYUD-v2 [68]. The PASCAL-Context dataset contains 4,998 images for training and 5,105 for testing, annotated for five tasks: semantic segmentation ('SemSeg'), human parts segmentation ('Parts'), saliency detection ('Sal'), surface normal estimation ('Normals'), and edge detection ('Edge'). The NYUD-v2 dataset consists of 795 training images and 654 testing images, all depicting indoor scenes, and provides annotations for four tasks: semantic segmentation, depth estimation ('Depth'), surface normal estimation, and edge detection.\\n\\nTo evaluate our algorithm, we configure two HC-FMTL benchmark scenarios: 1) Five single-task clients address five tasks in PASCAL-Context, and one multi-task client addresses four tasks in NYUD-v2; 2) Conversely, four single-task clients address four tasks in NYUD-v2, and one multi-task client addresses five tasks in PASCAL-Context. Following MaT-FL [8], we set an equal number of data samples among the respective clients through random partitioning.\\n\\nImplementation. Our client architecture employs a pre-trained Swin-T [44] backbone coupled with simple FCN decoders and heads. Considering the varying capacities of datasets, we use one local epoch for PASCAL-Context and four for NYUD-v2, setting the total number of communication rounds to 100 and the batch size to 8. We train all models using AdamW optimizer [46] with an initial learning rate and weight decay rate set at 1e-4. We implement all methods with PyTorch [61] and run experiments on two NVIDIA RTX4090 GPUs. To adapt existing methods to the HC-FMTL setting, we decouple the models into encoders and decoders for separate aggregation across all methods.\\n\\nMetrics. We adhere to established evaluation metrics.\"}"}
{"id": "CVPR-2024-851", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Comparison to representative methods using NYUD-v2 for four single-task clients and PASCAL-Context for one multi-task client.\\n\\n| Method          | NYUD-v2 (ST) | PASCAL-Context (MT) |\\n|-----------------|--------------|---------------------|\\n|                 | mIoU \u25b2       | RMSE \u25bc              | mErr \u25bc     | maxF \u25b2     | mIoU \u25b2      | RMSE \u25bc       | mErr \u25bc     | maxF \u25b2     | mIoU \u25b2      | RMSE \u25bc     | mErr \u25bc     | maxF \u25b2     |\\n| Local           | 33.59        | 0.7129              | 23.22      | 75.02      | 65.80      | 55.01       | 83.23      | 14.21      | 71.89      | 0.00       |\\n| FedAvg [55]     | 25.80        | 0.8295              | 24.85      | 75.31      | 64.63      | 52.88       | 81.08      | 15.56      | 68.95      | -7.56      |\\n| FedProx [35]    | 25.96        | 0.8316              | 25.20      | 75.34      | 64.97      | 50.78       | 81.29      | 15.83      | 69.81      | -8.12      |\\n| FedPer [2]      | 35.93        | 0.7460              | 23.75      | 75.53      | 67.78      | 54.75       | 82.50      | 14.75      | 71.90      | -0.16      |\\n| Ditto [36]      | 28.15        | 0.7482              | 23.96      | 75.42      | 65.99      | 51.45       | 81.74      | 15.29      | 69.96      | -4.67      |\\n| FedAMP [25]     | 34.75        | 0.7103              | 23.31      | 75.03      | 66.08      | 54.10       | 83.35      | 14.20      | 71.88      | 0.27       |\\n| MaT-FL [8]      | 35.05        | 0.7504              | 23.39      | 75.33      | 67.90      | 54.78       | 82.84      | 14.58      | 71.94      | -0.16      |\\n\\nTable 3. Ablation study on our proposed aggregation schemes. '+Enc' and '+Dec' denote the integration of Hyper Conflict-Averse Aggregation for the encoders and Hyper Cross Attention Aggregation for the decoders, respectively.\\n\\n| Method          | NYUD-v2 (MT) |\\n|-----------------|--------------|\\n|                 | mIoU \u25b2       | RMSE \u25bc       | mErr \u25bc     | maxF \u25b2     | mIoU \u25b2      | RMSE \u25bc       | mErr \u25bc     | maxF \u25b2     |\\n| Local           | 51.69        | 49.94        | 80.91      | 15.76      | 71.95       | 41.86        | 0.6487     | 20.59      | 76.46      | 0.00       |\\n| +Enc            | 58.38        | 51.64        | 80.44      | 15.65      | 72.09       | 41.21        | 0.6377     | 20.55      | 76.50      | 1.89       |\\n| +Dec            | 57.39        | 51.65        | 80.75      | 15.69      | 72.06       | 41.48        | 0.6344     | 20.56      | 76.41      | 1.80       |\\n| +Enc+Dec        | 57.55        | 52.30        | 80.71      | 15.60      | 72.08       | 41.47        | 0.6281     | 20.53      | 76.50      | 2.18       |\\n\\nSpecifically, we measure semantic segmentation and human parts segmentation using the mean Intersection over Union (mIoU). Saliency detection is evaluated with the maximum F-measure (maxF), while surface normal estimation is assessed by the mean error (mErr). Edge detection utilizes the optimal-dataset-scale F-measure (odsF), and depth estimation uses the Root Mean Square Error (RMSE). To provide an overall evaluation of different algorithms, we calculate the average per-task performance drop relative to the local training baseline, which is trained without aggregation. The formula is as follows:\\n\\n$$\\\\Delta m = \\\\frac{1}{N} \\\\sum_{i=1}^{N} l_i \\\\frac{M_{Fed,i} - M_{Local,i}}{M_{Local,i}},$$\\n\\nwhere $N$ is the count of tasks, $M_{Fed,i}$ and $M_{Local,i}$ correspond to the performance of task $i$ for federated methods and the local baseline, respectively. $l_i = 1$ if a lower metric value is better for task $i$, and $l_i = 0$ otherwise.\\n\\n4.2. Main Results\\n\\nTo evaluate the performance of our method, we compare with representative works including two traditional FL approaches FedAvg [55] and FedProx [35], four pFL methods FedPer [2], Ditto [36], FedBABU [59], FedAMP [25], and two FMTL methods FedSTA [60] and MaT-FL [8]. The results presented in Tab. 1 and Tab. 2 demonstrate that FEDHCA consistently delivers the best performance across most metrics. More importantly, it outperforms all representative methods when considering the average per-task performance drop, which is a widely acknowledged indicator for assessing the overall performance of multiple tasks. In addition, Fig. 4 shows that FEDHCA converges faster to a better result on different tasks.\\n\\n4.3. In-depth Analysis\\n\\nAblation Study.\\n\\nAn ablation study is conducted to discern the individual contributions of each component within Table 4. The results indicate that incorporating either encoder or decoder aggregation enhances performance relative to the baseline. The simultaneous employment of both Hyper Conflict-Averse and Hyper Cross Attention Aggregations enables FEDHCA to achieve optimal performance across the evaluated configurations. This result supports the idea that using these two aggregation schemes together enhances cooperation among different clients while simultaneously reducing negative conflicts between various tasks.\\n\\nImpact of different FMTL scenarios.\\n\\nTo further verify the necessity of introducing our new setting, we conduct experiments comparing two scenarios: 1) each client handles a single task, and 2) HC-FMTL encompasses both single-task and multi-task clients. As Tab. 4 illustrates, while FEDHCA improves upon the local baseline in the single-task client scenario, integrating the multi-task client results in a greater enhancement. This improvement is attributed to the expanded pool of data and the knowledge jointly learned from additional tasks. Further experiments are carried out on another scenario of HC-FMTL setting which exclusively involves multi-task clients.\"}"}
{"id": "CVPR-2024-851", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Impact of the number of clients. To assess the effectiveness of FEDHCA across varying client counts, we conduct tests by scaling the number of clients per task by factors of 2 and 4, with the datasets evenly split. As depicted in Fig. 5, FEDHCA consistently outperforms all comparative methods, exhibiting a positive correlation between the number of clients and performance improvement. This trend contrasts with the performance decline seen with other methods as the client count increases\u2014a result typically attributed to the diminished dataset available to each client and the increased decentralization within the federated learning system. The success of FEDHCA substantiates the efficacy of the Hyper Conflict-Averse Aggregation and Hyper Cross Attention Aggregation schemes, especially in scenarios characterized by pronounced data and task heterogeneity.\\n\\nInteraction between tasks. We investigate the dynamic learning process of Hyper Aggregation Weights for both encoders and decoders, aiming to understand their role in facilitating personalized aggregation for different clients. Fig. 6(a) reveals that the evolution of weights for encoders in single-task clients shows a rising trend, suggesting a consistent uptake of knowledge from peers throughout the training period. In contrast, the encoder weight of the multi-task client, as depicted in Fig. 6(b), exhibits two stages. Initially, the multi-task client mutually assimilates knowledge from single-task clients, a process that is crucial for rapid model convergence. The mutual learning for the multi-task client reaches its peak at about 20 rounds when the encoder weights are comparable. Subsequently, in the second phase, due to the heterogeneity in data and tasks, the multi-task client tends to enhance its feature extraction capabilities specific to its own data domain.\\n\\nWeights for decoders, as shown in Fig. 7, vary significantly across different tasks and decoder layers. From a layer-oriented perspective, the layer closest to the output head, i.e., L6, depends least on cross-task information, which ensures that the final output is finely tuned to the specific task. In terms of task-related differences, a phenomenon markedly distinct from encoders is observed. For decoders of multi-task client, there is a persistent information integration from other tasks until the end of training. This empirical evidence substantiates the significance of employing task interaction in decoder aggregation.\\n\\n5. Conclusion\\nIn conclusion, this paper addresses the challenges of heterogeneity in the novel Hetero-Client Federated Multi-Task Learning (HC-FMTL) setting through the innovative FEDHCA framework. By recognizing and tackling the issues of model incongruity, data heterogeneity, and task heterogeneity, FEDHCA learns personalized models with synergies of the proposed Hyper Conflict-Averse Aggregation, Hyper Cross Attention Aggregation, and Hyper Aggregation Weights. Theoretical insights and extensive experiments confirm the effectiveness of our methodology. Our work opens possibilities for more flexible FL systems in diverse and realistic settings. For future work, we aim to delve into greater model heterogeneity that accommodates varied network structures across clients, and to integrate specific modules into clients to further enhance task interaction, drawing on advancements in MTL.\"}"}
{"id": "CVPR-2024-851", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. In ICLR, 2021.\\n\\n[2] Manoj Ghuhan Arivazhagan, Vinay Aggarwal, Aaditya Kumar Singh, and Sunav Choudhary. Federated learning with personalization layers. arXiv preprint arXiv:1912.00818, 2019.\\n\\n[3] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In AISTATS, pages 2938\u20132948, 2020.\\n\\n[4] Xiang Bai, Hanchen Wang, Liya Ma, Yongchao Xu, et al. Advancing COVID-19 diagnosis with privacy-preserving collaboration in artificial intelligence. Nature Machine Intelligence, 3(12):1081\u20131089, 2021.\\n\\n[5] Cosmin I. Bercea, Benedikt Wiestler, Daniel Rueckert, and Shadi Albarqouni. Federated disentangled representation learning for unsupervised brain anomaly detection. Nature Machine Intelligence, 4(8):685\u2013695, 2022.\\n\\n[6] David Br\u00fcgge, Menelaos Kanakis, Stamatios Georgoulis, and Luc Van Gool. Automated search for resource-efficient branched multi-task networks. In BMVC, page 359, 2020.\\n\\n[7] David Br\u00fcgge, Menelaos Kanakis, Anton Obukhov, Stamatios Georgoulis, and Luc Van Gool. Exploring relational context for multi-task dense prediction. In ICCV, pages 15869\u201315878, 2021.\\n\\n[8] Ruisi Cai, Xiaohan Chen, Shiwei Liu, Jayanth Srinivasa, Myungjin Lee, Ramana Kompella, and Zhangyang Wang. Many-task federated learning: A new problem setting and a simple baseline. In CVPR, pages 5037\u20135045, 2023.\\n\\n[9] Rich Caruana. Multitask learning. Machine learning, 28(1):41\u201375, 1997.\\n\\n[10] Hong-You Chen and Wei-Lun Chao. On bridging generic and personalized federated learning for image classification. In ICLR, 2022.\\n\\n[11] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, pages 801\u2013818, 2018.\\n\\n[12] Yiqiang Chen, Teng Zhang, Xinlong Jiang, Qian Chen, Chenlong Gao, and Wuliang Huang. Fedbone: Towards large-scale federated multi-task learning. arXiv preprint arXiv:2306.17465, 2023.\\n\\n[13] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In ICML, pages 794\u2013803, 2018.\\n\\n[14] Zhao Chen, Jiquan Ngiam, Yanping Huang, Thang Luong, Henrik Kretzschmar, Yuning Chai, and Dragomir Anguelov. Just pick a sign: Optimizing deep multitask models with gradient sign dropout. NeurIPS, 33:2039\u20132050, 2020.\\n\\n[15] Liam Collins, Hamed Hassani, Aryan Mokhtari, and Sanjay Shakkottai. Exploiting shared representations for personalized federated learning. In ICML, pages 2089\u20132099, 2021.\\n\\n[16] Michael Crawshaw. Multi-task learning with deep neural networks: A survey. arXiv preprint arXiv:2009.09796, 2020.\\n\\n[17] Ittai Dayan, Holger R. Roth, Aoxiao Zhong, Ahmed Harouni, Amilcare Gentili, et al. Federated learning for predicting clinical outcomes in patients with COVID-19. Nature Medicine, 27(10):1735\u20131743, 2021.\\n\\n[18] Enmao Diao, Jie Ding, and Vahid Tarokh. Heterofl: Computation and communication efficient federated learning for heterogeneous clients. In ICLR, 2021.\\n\\n[19] Alireza Fallah, Aryan Mokhtari, and Asuman Ozdaglar. Personalized federated learning with theoretical guarantees: A model-agnostic meta-learning approach. NeurIPS, 33:3557\u20133568, 2020.\\n\\n[20] Yuan Gao, Jiayi Ma, Mingbo Zhao, Wei Liu, and Alan L Yuille. Nddr-cnn: Layerwise feature fusing in multi-task cnns by neural discriminative dimensionality reduction. In CVPR, pages 3205\u20133214, 2019.\\n\\n[21] Pengsheng Guo, Chen-Yu Lee, and Daniel Ulbricht. Learning to branch for multi-task learning. In ICML, pages 3854\u20133863, 2020.\\n\\n[22] Chaoyang He, Emir Ceyani, Keshav Balasubramanian, Murali Annavaram, and Salman Avestimehr. Spreadgnn: Decentralized multi-task federated learning for graph neural networks on molecular data. In AAAI, pages 6865\u20136873, 2022.\\n\\n[23] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In CVPR, pages 17853\u201317862, 2023.\\n\\n[24] Suizhi Huang, Shalayiding Sirejiding, Yuxiang Lu, Yue Ding, Leheng Liu, Hui Zhou, and Hongtao Lu. Yolo-med: Multi-task interaction network for biomedical images. In ICASSP, pages 2175\u20132179, 2024.\\n\\n[25] Yutao Huang, Lingyang Chu, Zirui Zhou, Lanjun Wang, Jiangchuan Liu, Jian Pei, and Yong Zhang. Personalized cross-silo federated learning on non-iid data. In AAAI, pages 7865\u20137873, 2021.\\n\\n[26] Yangsibo Huang, Samyak Gupta, Zhao Song, Kai Li, and Sanjeev Arora. Evaluating gradient inversion attacks and defenses in federated learning. NeurIPS, 34:7232\u20137241, 2021.\\n\\n[27] Adri\u00e1n Javaloy and Isabel Valera. Rotograd: Gradient homogenization in multitask learning. In ICLR, 2022.\\n\\n[28] Peter Kairouz, H. Brendan McMahan, Brendan Avent, et al. Advances and open problems in federated learning. Found. Trends Mach. Learn., 14(1-2):1\u2013210, 2021.\\n\\n[29] Menelaos Kanakis, David Br\u00fcgge, Suman Saha, Stamatios Georgoulis, Anton Obukhov, and Luc Van Gool. Reparameterizing convolutions for incremental multi-task learning without task interference. In ECCV, pages 689\u2013707, 2020.\\n\\n[30] Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank J. Reddi, Sebastian U. Stich, and Ananda Theertha Suresh. SCAFFOLD: stochastic controlled averaging for federated learning. In ICML, pages 5132\u20135143, 2020.\"}"}
{"id": "CVPR-2024-851", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alex Kendall, Yarin Gal, and Roberto Cipolla. Multi-task learning using uncertainty to weigh losses for scene geometry and semantics. In CVPR, pages 7482\u20137491, 2018.\\n\\nJakub Kone\u010dn\u00fd, Brendan McMahan, and Daniel Ramage. Federated optimization: Distributed optimization beyond the datacenter. arXiv preprint arXiv:1511.03575, 2015.\\n\\nJakub Kone\u010dn\u00fd, H. Brendan McMahan, Felix X. Yu, Peter Richt\u00e1rik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.\\n\\nQinbin Li, Bingsheng He, and Dawn Song. Model-contrastive federated learning. In CVPR, pages 10713\u201310722, 2021.\\n\\nTian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. In MLSys, 2020.\\n\\nTian Li, Shengyuan Hu, Ahmad Beirami, and Virginia Smith. Ditto: Fair and robust federated learning through personalization. In ICML, pages 6357\u20136368, 2021.\\n\\nXiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. FedBN: Federated learning on non-IID features via local batch normalization. In ICLR, 2021.\\n\\nXin-Chun Li, De-Chuan Zhan, Yunfeng Shao, Bingshuai Li, and Shaoming Song. Fedphp: Federated personalization with inherited private models. In ECML PKDD, pages 587\u2013602, 2021.\\n\\nBo Liu, Xingchao Liu, Xiaojie Jin, Peter Stone, and Qiang Liu. Conflict-averse gradient descent for multi-task learning. NeurIPS, 34:18878\u201318890, 2021.\\n\\nKen Liu, Shengyuan Hu, Steven Z Wu, and Virginia Smith. On privacy and personalization in cross-silo federated learning. NeurIPS, 35:5925\u20135940, 2022.\\n\\nLi Yang Liu, Yi Li, Zhanghui Kuang, Jing-Hao Xue, Yimin Chen, Wenming Yang, Qingmin Liao, and Wayne Zhang. Towards impartial multi-task learning. In ICLR, 2021.\\n\\nShikun Liu, Edward Johns, and Andrew J. Davison. End-to-end multi-task learning with attention. In CVPR, pages 1871\u20131880, 2019.\\n\\nYang Liu, Yan Kang, Tianyuan Zou, Yanhong Pu, Yuanqin He, Xiaozhou Ye, Ye Ouyang, Ya-Qin Zhang, and Qiang Yang. Vertical federated learning: Concepts, advances, and challenges. IEEE TKDE, 2024.\\n\\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, pages 10012\u201310022, 2021.\\n\\nMingsheng Long, Zhangjie Cao, Jianmin Wang, and Philip S Yu. Learning multiple tasks with multilinear relationship networks. NeurIPS, 30, 2017.\\n\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\\n\\nYongxi Lu, Abhishek Kumar, Shuangfei Zhai, Yu Cheng, Tara Javidi, and Rogerio Feris. Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification. In CVPR, pages 5334\u20135343, 2017.\\n\\nYuxiang Lu, Shalayiding Sirejiding, Bayram Bayramli, Suizhi Huang, Yue Ding, and Hongtao Lu. Task indicating transformer for task-conditional dense predictions. In ICASSP, pages 3625\u20133629, 2024.\\n\\nYuxiang Lu, Shalayiding Sirejiding, Yue Ding, Chunlin Wang, and Hongtao Lu. Prompt guided transformer for multi-task dense prediction. IEEE TMM, 2024.\\n\\nLinhao Luo, Yumeng Li, Buyu Gao, Shuai Tang, Sinan Wang, Jiancheng Li, Tanchao Zhu, Jiancai Liu, Zhao Li, and Shirui Pan. MAMDR: A model agnostic learning framework for multi-domain recommendation. In ICDE, pages 3079\u20133092, 2023.\\n\\nXiaosong Ma, Jie Zhang, Song Guo, and Wenchao Xu. Layer-wised model aggregation for personalized federated learning. In CVPR, pages 10092\u201310101, 2022.\\n\\nKevis-Kokitsi Maninis, Ilija Radosavovic, and Iasonas Kokkinos. Attentive single-tasking of multiple tasks. In CVPR, pages 1851\u20131860, 2019.\\n\\nOthmane Marfoq, Giovanni Neglia, Aur\u00e9lien Bellet, Laetitia Kameni, and Richard Vidal. Federated multi-task learning under a mixture of distributions. In NeurIPS, pages 15434\u201315447, 2021.\\n\\nBrendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In AISTATS, pages 1273\u20131282, 2017.\\n\\nBrendan McMahan, Eider Moore, Daniel Ramage, et al. Communication-efficient learning of deep networks from decentralized data. In AISTATS, pages 1273\u20131282, 2017.\\n\\nJed Mills, Jia Hu, and Geyong Min. Multi-task federated learning for personalised deep neural networks in edge computing. TPDS, 33(3):630\u2013641, 2021.\\n\\nIshan Misra, Abhinav Shrivastava, Abhinav Gupta, and Martial Hebert. Cross-stitch networks for multi-task learning. In CVPR, pages 3994\u20134003, 2016.\\n\\nRoozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The role of context for object detection and semantic segmentation in the wild. In CVPR, pages 891\u2013898, 2014.\\n\\nJaehoon Oh, Sangmook Kim, and Se-Young Yun. Fedbabu: Toward enhanced representation for federated image classification. In ECCV, 2022.\\n\\nSangjoon Park, Gwanghyun Kim, Jeongsol Kim, Boah Kim, and Jong Chul Ye. Federated split task-agnostic vision transformer for covid-19 cxr diagnosis. NeurIPS, 34:24617\u201324630, 2021.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, and Luca Antiga. Pytorch: An imperative style, high-performance deep learning library. NeurIPS, 32, 2019.\\n\\nRen\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In ICCV, pages 12179\u201312188, 2021.\\n\\nJiawei Ren, Cunjun Yu, Xiao Ma, Haiyu Zhao, Shuai Yi, et al. Balanced meta-softmax for long-tailed visual recognition. NeurIPS, 33:4175\u20134186, 2020.\"}"}
{"id": "CVPR-2024-851", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sebastian Ruder. An overview of multi-task learning in deep neural networks. arXiv preprint arXiv:1706.05098, 2017.\\n\\nSebastian Ruder, Joachim Bingel, Isabelle Augenstein, and Anders S\u00f8gaard. Latent multi-task architecture learning. In AAAI, pages 4822\u20134829, 2019.\\n\\nOzan Sener and Vladlen Koltun. Multi-task learning as multi-objective optimization. Advances in neural information processing systems, 31, 2018.\\n\\nMehdi Setayesh, Xiaoxiao Li, and Vincent W.S. Wong. Perfedmask: Personalized federated learning with optimized masking vectors. In ICLR, 2023.\\n\\nNathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, pages 746\u2013760, 2012.\\n\\nShalayiding Sirejiding, Yuxiang Lu, Hongtao Lu, and Yue Ding. Scale-aware task message transferring for multi-task learning. In ICME, pages 1859\u20131864, 2023.\\n\\nVirginia Smith, Chao-Kai Chiang, Maziar Sanjabi, and Ameet Talwalkar. Federated multi-task learning. In NeurIPS, pages 4424\u20134434, 2017.\\n\\nGuolei Sun, Thomas Probst, Danda Pani Paudel, Nikola Popovi\u00b4c, Menelaos Kanakis, Jagruti Patel, Dengxin Dai, and Luc Van Gool. Task switching network for multi-task learning. In ICCV, pages 8291\u20138300, 2021.\\n\\nCanh T Dinh, Nguyen Tran, and Josh Nguyen. Personalized federated learning with moreau envelopes. NeurIPS, 33:21394\u201321405, 2020.\\n\\nAlysa Ziying Tan, Han Yu, Lizhen Cui, and Qiang Yang. Towards Personalized Federated Learning. IEEE TNNLS, pages 1\u201317, 2022.\\n\\nSimon Vandenhende, Stamatios Georgoulis, and Luc Van Gool. Mti-net: Multi-scale task interaction networks for multi-task learning. In ECCV, pages 527\u2013543, 2020.\\n\\nSimon Vandenhende, Stamatios Georgoulis, Wouter Van Gansbeke, Marc Proesmans, Dengxin Dai, and Luc Van Gool. Multi-task learning for dense prediction tasks: A survey. IEEE TPAMI, 44(7):3614\u20133633, 2021.\\n\\nMatthew Wallingford, Hao Li, Alessandro Achille, Avinash Ravichandran, Charless Fowlkes, Rahul Bhotika, and Stefano Soatto. Task adaptive parameter sharing for multi-task learning. In CVPR, pages 7561\u20137570, 2022.\\n\\nHongyi Wang, Mikhail Yurochkin, Yuekai Sun, Dimitris S. Papailiopoulos, and Yasaman Khazaeni. Federated learning with matched averaging. In ECCV, 2020.\\n\\nJianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi, and H Vincent Poor. Tackling the objective inconsistency problem in heterogeneous federated optimization. NeurIPS, 33:7611\u20137623, 2020.\\n\\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In ICCV, pages 568\u2013578, 2021.\\n\\nZirui Wang, Yulia Tsvetkov, Orhan Firat, and Yuan Cao. Gradient vaccine: Investigating and improving multi-task optimization in massively multilingual models. In ICLR, 2021.\\n\\nDan Xu, Wanli Ouyang, Xiaogang Wang, and Nicu Sebe. Pad-net: Multi-tasks guided prediction-and-distillation network for simultaneous depth estimation and scene parsing. In CVPR, pages 675\u2013684, 2018.\\n\\nYangyang Xu, Xiangtai Li, Haobo Yuan, Yibo Yang, and Lefei Zhang. Multi-task learning with multi-query transformer for dense prediction. IEEE TCSVT, 2023.\\n\\nQiang Yang, Yang Liu, Tianjian Chen, and Yongxin Tong. Federated machine learning: Concept and applications. ACM Trans. Intell. Syst. Technol., 10(2):12:1\u201312:19, 2019.\\n\\nYuwen Yang, Chang Liu, Xun Cai, Suizhi Huang, Hongtao Lu, and Yue Ding. Unideal: Curriculum knowledge distillation federated learning. In ICASSP, pages 7145\u20137149, 2024.\\n\\nFeiyang Ye, Baijiong Lin, Zhixiong Yue, Pengxin Guo, Qiao Xiao, and Yu Zhang. Multi-objective meta learning. NeurIPS, 34:21338\u201321351, 2021.\\n\\nHanrong Ye and Dan Xu. Inverted pyramid multi-task transformer for dense scene understanding. In ECCV, pages 514\u2013530, 2022.\\n\\nFuxun Yu, Weishan Zhang, Zhuwei Qin, Zirui Xu, Di Wang, Chenchen Liu, Zhi Tian, and Xiang Chen. Fed2: Feature-aligned federated learning. In ACM SIGKDD, pages 2066\u20132074, 2021.\\n\\nTianhe Yu, Saurabh Kumar, Abhishek Gupta, Sergey Levine, Karol Hausman, and Chelsea Finn. Gradient surgery for multi-task learning. NeurIPS, 33:5824\u20135836, 2020.\\n\\nMikhail Yurochkin, Mayank Agarwal, Soumya Ghosh, Kristjan H. Greenewald, Trong Nghia Hoang, and Yasaman Khazaeni. Bayesian nonparametric federated learning of neural networks. In ICML, pages 7252\u20137261, 2019.\\n\\nXiaoya Zhang, Ling Zhou, Yong Li, Zhen Cui, Jin Xie, and Jian Yang. Transfer vision patterns for multi-task pixel learning. In ACM MM, pages 97\u2013106, 2021.\\n\\nZhenyu Zhang, Zhen Cui, Chunyan Xu, Zequn Jie, Xiang Li, and Jian Yang. Joint task-recursive learning for semantic segmentation and depth estimation. In ECCV, pages 235\u2013251, 2018.\\n\\nZhenyu Zhang, Zhen Cui, Chunyan Xu, Yan Yan, Nicu Sebe, and Jian Yang. Pattern-affinitive propagation across depth, surface normal and semantic segmentation. In CVPR, pages 4106\u20134115, 2019.\\n\\nLing Zhou, Zhen Cui, Chunyan Xu, Zhenyu Zhang, Chaoqun Wang, Tong Zhang, and Jian Yang. Pattern-structure diffusion for multi-task learning. In CVPR, pages 4514\u20134523, 2020.\\n\\nLigeng Zhu, Hongzhou Lin, Yao Lu, Yujun Lin, and Song Han. Delayed gradient averaging: Tolerate the communication latency for federated learning. NeurIPS, 34:29995\u201330007, 2021.\\n\\nZhuangdi Zhu, Junyuan Hong, and Jiayu Zhou. Data-free knowledge distillation for heterogeneous federated learning. In ICML, pages 12878\u201312889, 2021.\\n\\nWeiming Zhuang, Yonggang Wen, Lingjuan Lyu, and Shuai Zhang. Mas: Towards resource-efficient federated multiple-task learning. In ICCV, pages 23414\u201323424, 2023.\"}"}
{"id": "CVPR-2024-851", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"FedHCA\u00b2: Towards Hetero-Client Federated Multi-Task Learning\\n\\nYuxiang Lu*, Suizhi Huang*, Yuwen Yang, Shalayiding Sirejiding, Yue Ding\u2020, Hongtao Lu\\nDepartment of Computer Science and Engineering, Shanghai Jiao Tong University\\n\\nAbstract\\nFederated Learning (FL) enables joint training across distributed clients using their local data privately. Federated Multi-Task Learning (FMTL) builds on FL to handle multiple tasks, assuming model congruity that identical model architecture is deployed in each client. To relax this assumption and thus extend real-world applicability, we introduce a novel problem setting, Hetero-Client Federated Multi-Task Learning (HC-FMTL), to accommodate diverse task setups. The main challenge of HC-FMTL is the model incongruity issue that invalidates conventional aggregation methods. It also escalates the difficulties in model aggregation to deal with data and task heterogeneity inherent in FMTL. To address these challenges, we propose the FedHCA\u00b2 framework, which allows for federated training of personalized models by modeling relationships among heterogeneous clients. Drawing on our theoretical insights into the difference between multi-task and federated optimization, we propose the Hyper Conflict-Averse Aggregation scheme to mitigate conflicts during encoder updates. Additionally, inspired by task interaction in MTL, the Hyper Cross Attention Aggregation scheme uses layer-wise cross attention to enhance decoder interactions while alleviating model incongruity. Moreover, we employ learnable Hyper Aggregation Weights for each client to customize personalized parameter updates. Extensive experiments demonstrate the superior performance of FedHCA\u00b2 in various HC-FMTL scenarios compared to representative methods. Code is available at https://github.com/innovator-zero/FedHCA2.\\n\\n1. Introduction\\nFederated Learning (FL) [32] has emerged as a prominent paradigm in distributed training, gaining attention in both academic and industrial fields [4, 5, 17, 28, 83]. The FL framework empowers to collaboratively train models across multiple clients, like mobile devices or distributed data centers, while preserving data privacy and reducing communication costs. The impetus behind FL lies in the recognition that harnessing a broader dataset can improve model performance, but it also introduces the data heterogeneity issue, as clients often collect samples from non-i.i.d. data distributions. Nevertheless, most FL research is centered on single-task scenarios, overlooking applications that demand simultaneous multi-task processing, e.g., autonomous driving [23]. This gap has led to the integration of Multi-Task Learning (MTL) with FL, giving rise to Federated Multi-Task Learning (FMTL) [22, 56]. While existing FMTL approaches primarily address statistical challenges [53, 70], recent studies [8, 12, 60, 96] have highlighted the importance of task heterogeneity, particularly for dense predictions such as semantic segmentation and depth estimation [11, 62, 79].\\n\\nHowever, these FMTL methods often assume model congruity among clients, i.e., all participants either engage in a single task or aggregate with peers handling identical task sets, as shown in Fig. 1(a) and (b). Considering the discrepancy of heterogeneous tasks in practical applications as well as the expensive labor of annotating task-specific labels, clients often have different task setups in different environments. Here task setup describes a set of tasks that...\"}"}
{"id": "CVPR-2024-851", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"can vary in both number and type. We define this as a new problem setting: Hetero-Client Federated Multi-Task Learning (HC-FMTL), as depicted in Fig. 1(c). HC-FMTL relaxes these constraints, facilitating more flexible collaborative learning of various tasks across diverse private data domains and making scenarios more universally applicable.\\n\\nAs a more pervasive setting relaxed from FMTL, HC-FMTL introduces an additional challenge of model incongruity, which exacerbates client heterogeneity. This issue arises from clients having different task setups, coupled with the prevalent use of encoder-decoder architectures in vision tasks, leading to a disparity in multi-task model structures. Model incongruity not only increases the complexity of model aggregation but also coexists with the data and task heterogeneity inherent in FMTL. Data heterogeneity is a consequence of clients encountering distinct data domains, as clients tend to use data from different domains to handle different target tasks without any overlap, which can result in performance degradation of collective learning. Meanwhile, task heterogeneity, which assigns different objectives for each task, could impede joint optimization and magnify the influence of data heterogeneity.\\n\\nIn this paper, we propose a novel framework named FEDHCA$^2$, designed for HC-FMTL. Our goal is to adaptively discern the relationships among heterogeneous clients and learn personalized yet globally collaborative models that benefit from both synergies and distinctions among clients and tasks. Since model incongruity precludes the straightforward application of conventional aggregation methods in FL, our approach involves the server disassembling client models into encoders and decoders for decoupled aggregation. For the encoders, we design the Hyper Conflict-Averse Aggregation scheme to alleviate update conflicts among clients. The motivation behind this is grounded in our theoretical analysis (see Theorem 1) that the optimization processes of MTL and FL are closely connected and share similarities. By incorporating an approximated gradient smoothing technique, we can find an appropriate update direction for all clients that mitigates the negative effects of conflicting parameter updates caused by data and task heterogeneity. When aggregating the decoders, we devise the Hyper Cross Attention Aggregation scheme by drawing inspiration from the modeling of task interaction in MTL [57, 81] and applying it to FL. Specifically, we implement a layer-wise cross attention mechanism to model the interplay between client decoders, enabling the capture of both the commonalities and discrepancies among different tasks in a fine-grained manner and thereby alleviating the incongruity at the model level. In addition, the personalized parameter updates for each client are tailored by learnable Hyper Aggregation Weights, which encourage encoders and decoders to adaptively assimilate knowledge from peers that offer helpful complementary information.\\n\\nOur contributions are summarized as follows:\\n\\n- We introduce a novel setting of Hetero-Client Federated Multi-Task Learning (HC-FMTL) alongside the FEDHCA$^2$ framework. It supports collaborative training across clients, each with its unique task setups, addressing the complexities of data and task heterogeneity, and the newly identified challenge of model incongruity. The relaxed setting broadens the FMTL's applicability to include a wider variety of clients, tasks, and data situations.\\n\\n- We reveal the connection between the optimization of MTL and FL in Theorem 1 and underscore the importance of circumventing update conflicts among clients, which are exacerbated by data and task heterogeneity in HC-FMTL. We propose a Hyper Conflict-Averse Aggregation scheme, designed to alleviate the adverse effects on encoders when absorbing shared knowledge.\\n\\n- We develop a Hyper Cross Attention Aggregation scheme to facilitate task interaction in decoders by modeling the fine-grained cross-task relationships among each decoder layer, tackling both intra- and inter-client heterogeneity.\\n\\n- We evaluate FEDHCA$^2$ using a composite of two benchmark datasets, PASCAL-Context and NYUD-v2, for various HC-FMTL scenarios. Extensive experiments demonstrate that our approach outperforms existing methods.\\n\\n2. Related Work\\n\\n2.1. Personalized Federated Learning\\n\\nFederated Learning (FL) can be broadly classified into traditional and personalized types, depending on the characteristics of data distribution [43, 73]. Traditional Federated Learning, exemplified by the widely used FedAvg [55], has undergone refinements to tackle challenges such as data heterogeneity [1, 34, 35, 77, 78, 87, 89, 95], communication efficiency [18, 30, 33, 54, 94], and privacy concerns [3, 26]. In contrast, personalized Federated Learning (pFL) emerges as a specialized variant designed to cater to individual client needs and address data heterogeneity more effectively [70, 73]. Techniques like meta-learning [19], regularization [25, 36, 72], personalized-head methods [2, 10, 15, 59, 63, 67], and other innovative approaches [37, 38, 84] are widely employed in pFL. In essence, both traditional FL and pFL aim to grapple with the inherent challenge of data heterogeneity.\\n\\n2.2. Multi-Task Learning\\n\\nMulti-Task Learning (MTL) aims to improve overall performance while reducing parameters and speeding up training or inference compared to training individual models for each task in isolation [9, 16, 64, 75]. The main directions of MTL research can be roughly categorized into network architecture design and multi-task optimization strategy [16]. Network structure design employs methods such...\"}"}
{"id": "CVPR-2024-851", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Illustration of the HC-FMTL setting and our proposed FEDHCA framework. HC-FMTL enables clients to have different task setups, from single-task (e.g., client C1, C2, C3) to multi-task (e.g., client Ci, Cn). HC-FMTL faces three main challenges: model incongruity due to different client model structures, data heterogeneity from different local data domains, and task heterogeneity from varied target tasks. The FL system includes a server and several clients. Our framework decomposes model aggregation into two parts: Hyper Conflict-Averse Aggregation for encoders and Hyper Cross Attention Aggregation for decoders. Learnable Hyper Aggregation Weights are employed to customize personalized parameter updates and are iteratively updated by local model updates from clients.\"}"}
{"id": "CVPR-2024-851", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. Architecture Overview\\n\\nThe overall architecture of our proposed FEDHCA is depicted in Fig. 2. It contains a pool of clients that perform local training on their private datasets and a server that coordinates the aggregation of models from these clients. Concerning dense prediction tasks, each client $C_i$ utilizes an encoder-decoder structure consisting of a shared encoder $\\\\theta_E$, task-specific decoders $\\\\{\\\\theta_D, 1^i, \\\\ldots, \\\\theta_D, |T_i|\\\\}$ and prediction heads for each task type they handle. In each communication round $r$, after all clients finish their local training, they send the model parameters of previous round $\\\\theta(r-1)$ and the updates in current round $\\\\Delta\\\\theta(r)$ to the server. The server first disassembles these models into encoders and decoders and then performs independent aggregation processes. The prediction heads, due to their varying parameter dimensions tailored to specific task outputs, are excluded from the aggregation process and remain localized to individual clients. The encoder parameters from all $N$ clients undergo Hyper Conflict-Averse Aggregation. Meanwhile, the server aggregates the parameters of all $K = \\\\sum_{i=1}^{N} |T_i|$ decoders through Hyper Cross Attention Aggregation. The entire pipeline of our framework is outlined in Algorithm 1.\\n\\nAlgorithm 1\\n\\nPseudo-codes for FEDHCA\\n\\nInput: $N$ clients $\\\\{C_1, \\\\ldots, C_N\\\\}$ with private local datasets $\\\\{D_1, \\\\ldots, D_N\\\\}$, client $C_i$ addresses tasks $T_i$, total communication rounds $R$, local epoch $E$, learning rate $\\\\eta$\\n\\nOutput: Trained models $\\\\theta(R) = \\\\{\\\\theta(R)_1, \\\\ldots, \\\\theta(R)_N\\\\}$\\n\\n1: Clients initialize models $\\\\theta(0) = \\\\{\\\\theta(0)_1, \\\\ldots, \\\\theta(0)_N\\\\}$, each model $\\\\theta_i$ consists of a shared encoder $\\\\theta_E$ and $|T_i|$ task-specific decoders $S_{|T_i|j=1} \\\\theta_D, j$ and heads\\n\\n2: Server initializes Hyper Aggregation Weights $\\\\alpha$ and $\\\\beta$\\n\\n3: procedure SERVER UPDATE\\n\\n4: for each communication round $r \\\\in \\\\{1, \\\\ldots, R\\\\}$ do\\n\\n5: for each client $C_i$ in parallel do\\n\\n6: $\\\\Delta\\\\theta(r)_i \\\\leftarrow CLIENT UPDATE(\\\\theta(r-1)_i)$\\n\\n7: end for\\n\\n8: Server gathers updates of client models $\\\\Delta\\\\theta(r)$\\n\\n9: Update $\\\\alpha$, $\\\\beta$ using $\\\\Delta\\\\theta(r)$ with Eq. (17)\\n\\n10: $\\\\theta(r) \\\\leftarrow AGGREGATION(\\\\theta(r-1), \\\\Delta\\\\theta(r), \\\\alpha, \\\\beta)$\\n\\n11: end for\\n\\n12: end procedure\\n\\n13: procedure CLIENT UPDATE($\\\\theta(r-1)_i$)\\n\\n14: $\\\\theta_i \\\\leftarrow \\\\theta(r-1)_i$\\n\\n15: for each local epoch $e \\\\in \\\\{1, \\\\ldots, E\\\\}$ do\\n\\n16: for mini-batch $B_i \\\\subset D_i$ do\\n\\n17: Compute losses $L_i = \\\\sum_{j=1}^{T_i} L_j(i; B_i)$\\n\\n18: Update model $\\\\theta_i \\\\leftarrow \\\\theta_i - \\\\eta \\\\nabla_{\\\\theta_i} L_i$\\n\\n19: end for\\n\\n20: end for\\n\\n21: return $\\\\Delta\\\\theta(r)_i = \\\\theta_i - \\\\theta(r-1)_i$\\n\\n22: end procedure\\n\\n3.3. Hyper Conflict-Averse Aggregation\\n\\nIn MTL, the encoder typically employs a parameter-sharing mechanism to capture common task-agnostic information, thereby serving as a general feature extractor for all tasks and enhancing their generalization capabilities [16, 64]. Within our encoder aggregation process, we anticipate that encoders from various clients, each addressing distinct tasks on different data domains, are able to acquire general knowledge from other client encoders akin to MTL. To elucidate this, we begin with a theoretical analysis of the correlation between the optimization processes of MTL and FL.\\n\\nAs depicted in Fig. 3(a), consider an MTL scenario where $N$ tasks are learned simultaneously using a standard multi-decoder architecture. In each mini-batch, the network backpropagates the loss functions $L_1, \\\\ldots, L_N$ onto the shared encoder parameters $\\\\theta_E$ to calculate its gradient and update:\\n\\n$$g = \\\\sum_{i=1}^{N} g_i = \\\\sum_{i=1}^{N} \\\\nabla_{\\\\theta_E} L_i, \\\\quad (2)$$\\n\\n$$\\\\Delta\\\\theta_E = -\\\\eta g = -\\\\eta \\\\sum_{i=1}^{N} g_i, \\\\quad (3)$$\\n\\nwhere $g$ represents the cumulative gradient on the encoder and $\\\\eta$ signifies the learning rate. By updating through the summation of gradients from diverse tasks, the encoder assimilates knowledge from various task domains, aligning with the objective of MTL. Meanwhile, in an FL setting shown in Fig. 3(b), suppose there are $N$ clients, each using separate networks with the same architectures as the multi-task model but with independent encoders $\\\\theta_E_1, \\\\ldots, \\\\theta_E_N$ to learn the same $N$ tasks. Assuming identical initial weights $\\\\theta_E$ with MTL, and they are trained for only one mini-batch to obtain gradients $g_i = \\\\nabla_{\\\\theta_E} L_i$. FL typically aggregates these encoders by averaging the parameters of all clients:\\n\\n$$\\\\bar{\\\\theta}_E_i = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\theta_E_i, \\\\quad (4)$$\\n\\nwhere $\\\\bar{\\\\theta}_E_i$ is the aggregated encoder parameters.\"}"}
