{"id": "CVPR-2024-961", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gaussian Head Avatar: Ultra High-fidelity Head Avatar via Dynamic Gaussians\\n\\nYuelang Xu, Bengwang Chen, Zhe Li, Hongwen Zhang, Lizhen Wang, Zerong Zheng, Yebin Liu\\n\\n1. Department of Automation, Tsinghua University\\n2. Beijing Normal University\\n3. NNKosmos Technology\\n\\nAbstract\\nCreating high-fidelity 3D head avatars has always been a research hotspot, but there remains a great challenge under lightweight sparse view setups. In this paper, we propose Gaussian Head Avatar represented by controllable 3D Gaussians for high-fidelity head avatar modeling. We optimize the neutral 3D Gaussians and a fully learned MLP-based deformation field to capture complex expressions. The two parts benefit each other, thereby our method can model fine-grained dynamic details while ensuring expression accuracy. Furthermore, we devise a well-designed geometry-guided initialization strategy based on implicit SDF and Deep Marching Tetrahedra for the stability and convergence of the training procedure. Experiments show our approach outperforms other state-of-the-art sparse-view methods, achieving ultra high-fidelity rendering quality at 2K resolution even under exaggerated expressions.\\n\\nProject page: https://yuelangx.github.io/ gaussianheadavatar\\n\\n\u2020Corresponding author.\\n\\n1. Introduction\\nHigh-fidelity 3D human head avatar modeling is of great significance in many fields, such as VR/AR, telepresence, digital human and film production. Automatically creating high-fidelity avatars has been a research hotspot in computer vision for decades. Although some traditional head avatars [37, 39, 41, 54] can realize high-fidelity animation, they typically require accurate geometries reconstructed and tracked from dense multi-view videos, thus limiting their applications in lightweight settings.\\n\\nThanks to the Neural Radiance Fields (NeRF) [43] which show great capability of novel view synthesis in the absence of accurate geometry, recent methods [38, 57] skip the geometry reconstruction and tracking steps but directly learn high-quality NeRF-based head avatars. Other works [42, 48, 71] have verified that NeRF can be applied to either dense or sparse views, which greatly lowers the threshold for head avatar reconstruction. However, it still remains challenging for these NeRF-based approaches to synthesize high-fidelity images at 2K resolutions with pixel-\"}"}
{"id": "CVPR-2024-961", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"level details, including wrinkles and eyes.\\n\\nTo overcome this bottleneck and further improve the avatar quality, we introduce 3D Gaussian splatting [26] for 3D head avatar modeling. This is an explicit discrete representation that can be well adapted to the rasterization-based rendering pipeline. It has been verified that the 3D Gaussian representation is capable of rendering complex scenes with low computational consumption. Compared to NeRF, the reconstruction quality of static and dynamic scenes [40, 59, 67] is much better while rendering time cost has been significantly reduced. Motivated by this progress, we propose Gaussian Head Avatar, a novel representation that utilizes 3D Gaussian splatting for ultra high-fidelity head avatar modeling. Although recent 4D Gaussian works [40, 59, 66, 67] have been proposed to reconstruct dynamic scenes, all of them cannot be animated. For modeling the animatable head avatar, it is crucial but still unexplored how to effectively control the deformation of 3D Gaussians and model the dynamic appearances through expression coefficients.\\n\\nPrevious explicit [74] and implicit [2, 73, 76] head avatars usually formulate the facial deformation via linear blend skinning (LBS) using the skinning weights and blendshapes like the FLAME model [32]. However, such an LBS-based formulation fails to represent exaggerated and fine-grained expressions by simple linear operations, limiting the representation ability of the head avatars. Inspired by NeRSemble [28], we propose a fully learnable expression-conditioned deformation field for the 3D head Gaussians, avoiding the limited capability of the LBS-based formulation. Specifically, we input the positions of the 3D Gaussians with expression coefficients into an MLP to directly predict the displacements from the neutral expression to the target one. Similarly, we control the motion of non-face areas, such as the neck, using the head pose as the condition. 3D Gaussian-based representation has the powerful ability to reconstruct high-frequency details, enabling our method to learn accurate deformation fields. In turn, the learned accurate deformation field facilitates the dynamic Gaussian head model to fit more dynamic details. As a result, our method is able to reconstruct finer-grained dynamic details of expressive human heads.\\n\\nUnfortunately, as a discretized representation, the gradients back-propagated to the 3D Gaussians cannot spread through the whole space. Thus the convergence of training heavily relies on a plausible initialization for both the geometry and the deformation field. However, simply initializing the 3D head Gaussians with a morphable template like FLAME [32] fails to model the long hairstyle and the shoulders. Hence, we further propose an efficient and well-designed geometry-guided initialization strategy. Specifically, instead of starting from stochastic Gaussians or a FLAME model, we initially optimize an implicit signed distance function (SDF) field along with a color field and a deformation MLP for modeling the basic geometry, color, and the expression-conditioned deformations of the head avatar respectively. The SDF field is converted to a mesh through Deep Marching Tetrahedra (DMTet) [52], with the color and deformation of the vertices predicted by the MLPs. Then we render the mesh and optimize them jointly under the supervision of multi-view RGB images. Finally, we use the mesh with per-vertex features from the SDF field to initialize the 3D Gaussians to lie on the basic head surface while the color and deformation MLPs are carried over to the next stage, ensuring stable training for convergence. The entire initialization process takes only around 10 minutes.\\n\\nThe contributions of our method can be summarized as:\\n\\n\u2022 We propose Gaussian Head Avatar, a new head avatar representation that employs controllable dynamic 3D Gaussians to model expressive human head avatars, producing ultra high-fidelity synthesized images at 2K resolutions.\\n\u2022 For modeling high-frequency dynamic details, we employ a fully learned deformation field upon the 3D head Gaussians, which accurately model extremely complex and exaggerated facial expressions.\\n\u2022 We carefully design an efficient initialization strategy that leverages implicit representations to initialize the geometry and deformation, leading to efficient and robust convergence when training the Gaussian Head Avatar.\\n\\nBenefiting from these contributions, our method surpasses recent state-of-the-art methods under lightweight sparse-view setups on the avatar quality by a large margin.\\n\\n2. Related Works\\n\\n3D Head Avatar Reconstruction. Due to the wide application value in the film and digital human industry, 3D head avatar reconstruction from multi-view images has always been a research hotspot. Traditional works [4, 8, 19, 31] reconstruct the scan geometry through multi-view stereo and then register a face mesh template to it. However, such methods usually require heavy computation. With the utilization of deep neural networks, current methods [7, 33, 61, 65] achieve very fast reconstruction, producing even more accurate geometry. Lombardi et al. [37], Bi et al. [5] and Ma et al. [41] represent the full head mesh through a deep neural network and train it with multi-view videos as supervision. However, due to the errors in geometric estimation, mesh-based head avatars typically suffer from texture blur. Therefore, some recent methods [38, 57] utilize NeRF representation [43] to synthesize novel view images without geometry reconstruction, or build NeRF on the head mesh template [39]. Furthermore, the NeRF-based methods are extended to sparse view reconstruction tasks [28, 42, 48, 71] and achieve impressive performance.\\n\\nMethods which focus on generative model [6, 9, 11, 32, 46, 56, 60] are dedicated to learning general mesh\"}"}
{"id": "CVPR-2024-961", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"face templates from large-scale multi-view face images or 3D scans. Recently, implicit SDF-based [68] or NeRF-based [14, 23, 53, 55, 75] methods can learn full-head templates without the limitations of fixed topology, thereby better modeling complex hairstyles and glasses. Cao et al. [14] adopts a hybrid representation of local NeRF built on the mesh surface, which enables high-fidelity rendering and flexible expression control.\\n\\n3D head avatars reconstruction from monocular videos is also a popular yet challenging research topic. Early methods [12, 13, 15, 24, 25, 45] optimize a morphable mesh to fit the training video. Recent methods [20, 27] leverage neural networks to learn non-rigid deformation upon 3DMM face templates [18, 32], thus can recover more dynamic details. Such methods are not flexible enough to handle complex topologies. Therefore, the latest methods explore to construct head avatar models based on implicit SDF [73], point clouds [74] or NeRF [2, 3, 16, 17, 21, 36, 47, 63, 64, 76].\\n\\nPoint-based Rendering.\\n\\nPoint elements as a discrete and unstructured representation can fit geometry with arbitrary topology [69] efficiently. Recent methods [29, 30, 58] open up a differentiable rasterization pipeline, such that the point-based representation is widely used in multi-view reconstruction tasks. Aliev et al. [1] and Ruckert et al. [49] propose to first render the feature map, which is transferred to the images through a convolutional renderer. Xu et al. [62] use neural point cloud associated with neural features to model a NeRF. Recently, 3D Gaussian splatting [26] shows its superior performance, beating NeRF in both novel view synthesis quality and rendering speed. Some approaches [34, 40, 50, 59, 66, 67, 72, 72] extend Gaussian representation to dynamic scene reconstruction. However, these methods can not be migrated to the head avatar reconstruction tasks.\\n\\n3. Overview\\n\\nThe pipeline of the reconstruction of Gaussian Head Avatar is illustrated in Fig. 2, including the initialization stage and the training stage of Gaussian Head Avatar. Before the beginning of the pipeline, we remove the background [35] of each image and jointly estimate the 3DMM model [18], 3D facial landmarks and the expression coefficients for each frame. In the initialization stage (Sec. 4.3), we reconstruct an SDF-based neutral geometry, and optimize a deformation MLP and a color MLP from the training data as the guidance model. Next, we extract the neutral mesh through DMTet to initialize the neutral Gaussians while the deformation and color MLPs are also inherited from the initialization stage. In the training stage of Gaussian Head Avatar (Sec. 4.2), we deform the neutral Gaussians to the target expression through the dynamic generator given the driving expression coefficients as the condition. Finally, given a camera view, the expressive Gaussians are rendered to a feature map, which is then fed into the convolutional super resolution network to generate high-resolution avatar images. The whole model is optimized under the supervision of multi-view RGB videos.\\n\\n4. Method\\n\\n4.1. Avatar Representation\\n\\nGenerally, the static 3D Gaussians [26] with \\\\( N \\\\) points are represented by their positions \\\\( X \\\\), the multi-channel color \\\\( C \\\\), the rotation \\\\( Q \\\\), scale \\\\( S \\\\) and opacity \\\\( A \\\\). The rotation \\\\( Q \\\\) is represented in the form of quaternion. Subsequently, the Gaussians can be rasterized and rendered to the multi-channel image \\\\( I \\\\) given the camera parameters \\\\( \\\\mu \\\\). This process can be formulated as:\\n\\n\\\\[\\nI = R(X, C, Q, S, A; \\\\mu).\\n\\\\]\\n\\nOur task is to reconstruct a dynamic head avatar controlled by expression coefficients. Therefore, we formulate the head avatar as dynamic 3D Gaussians conditioned on expressions. To handle the dynamic changes, we input the expression coefficients with head pose to the head avatar model and output the position and other attributes of the Gaussians as above.\\n\\nSpecifically, we first construct a canonical neutral Gaussian model with expression-independent attributes:\\n\\n\\\\[\\n\\\\{X_0, F_0, Q_0, S_0, A_0\\\\},\\n\\\\]\\n\\nwhich are fully optimizable. \\\\( X_0 \\\\in \\\\mathbb{R}^{N \\\\times 3} \\\\) denotes the positions of the Gaussians with a neutral expression in the canonical space. \\\\( F_0 \\\\in \\\\mathbb{R}^{N \\\\times 128} \\\\) denotes the point-wise feature vectors as their intrinsic properties. \\\\( Q_0 \\\\in \\\\mathbb{R}^{N \\\\times 4}, S_0 \\\\in \\\\mathbb{R}^{N \\\\times 3} \\\\) and \\\\( A_0 \\\\in \\\\mathbb{R}^{N \\\\times 1} \\\\) denote the neutral rotation, scale and opacity respectively. Note that we do not define the neutral color, but directly predict expression-dependent dynamic color from the point-wise feature vectors \\\\( F_0 \\\\). Then, we construct an MLP-based expression conditioned dynamic generator \\\\( \\\\Phi \\\\) to generate all the extra dynamic changes to the neutral model. Overall, the whole Gaussian head avatar can be formulated as:\\n\\n\\\\[\\n\\\\{X, C, Q, S, A\\\\} = \\\\Phi(X_0, F_0, Q_0, S_0, A_0; \\\\theta, \\\\beta),\\n\\\\]\\n\\nwith \\\\( \\\\theta \\\\) denoting expression coefficients and \\\\( \\\\beta \\\\) denoting the head pose. During the training, we optimize all the parameters of the dynamic generator \\\\( \\\\Phi \\\\) and the neutral Gaussian model \\\\( \\\\{X_0, F_0, Q_0, S_0, A_0\\\\} \\\\), which are highlighted in bold in the following.\\n\\nNext, we explain the process of adding expression-related changes to the neutral Gaussian model through the dynamic generator \\\\( \\\\Phi \\\\) as described in Eqn. 2 in detail.\\n\\nPositions \\\\( X' \\\\) of the Gaussians. Expressions bring about the geometric deformation of the neutral model, which is modeled as the displacements of the Gaussian points.\"}"}
{"id": "CVPR-2024-961", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. The overview of the Gaussian Head Avatar rendering and reconstruction. We first optimize the guidance model including a neutral mesh, a deformation MLP and a color MLP in the Initialization stage. Then we use them to initialize the neutral Gaussians and the dynamic generator. Finally, 2K RGB images are synthesized through differentiable rendering and the super-resolution network. The Gaussian Head Avatar are trained under the supervision of multi-view RGB videos.\\n\\nSpecifically, we predict the displacements respectively controlled by the expression and the head pose in the canonical space through two different MLPs: $f_{\\\\text{exp}} \\\\in \\\\Phi$ and $f_{\\\\text{pose}} \\\\in \\\\Phi$. Then, we add them to the neutral positions.\\n\\n$$X' = X_0 + \\\\lambda_{\\\\text{exp}}(X_0)f_{\\\\text{exp}}(X_0, \\\\theta) + \\\\lambda_{\\\\text{pose}}(X_0)f_{\\\\text{pose}}(X_0, \\\\beta).$$ (3)\\n\\n$\\\\lambda_{\\\\text{exp}}(\\\\cdot)$ and $\\\\lambda_{\\\\text{pose}}(\\\\cdot)$ represent the extent to which the points are affected by the expression or the head pose respectively. Without decoupling but just using the expression coefficients as the global condition [16] which also controls the shoulders and upper body, will produce jittering results during the animation. Here, we assume that the Gaussian points closer to 3D landmarks are more affected by the expression coefficients and less affected by the head pose, while the opposite is true for the Gaussian points far away. Specifically, the 3D landmarks $P_0$ of the canonical model are first estimated through the 3DMM model in the data preprocessing and then optimized in the initialization stage 4.3. Then for each Gaussian point, we calculate the above weight $\\\\lambda_{\\\\text{exp}}(\\\\cdot)$ and $\\\\lambda_{\\\\text{pose}}(\\\\cdot)$ as follows:\\n\\n$$\\\\lambda_{\\\\text{exp}}(x) = \\\\begin{cases} 1, & \\\\text{dist}(x, P_0) < t_1 \\\\\\\\ t_2 - \\\\text{dist}(x, P_0), & \\\\text{dist}(x, P_0) \\\\in [t_1, t_2] \\\\\\\\ 0, & \\\\text{dist}(x, P_0) > t_2 \\\\end{cases}$$\\n\\nwith $\\\\lambda_{\\\\text{pose}}(x) = 1 - \\\\lambda_{\\\\text{exp}}(x)$. And $x \\\\in X_0$ denotes the position of one neutral Gaussian.\\n\\ndist$(x, P_0)$ denotes the minimum distance from the point $x$ to the 3D landmarks $P_0$.\\n\\nt_1 = 0.15 and $t_2 = 0.25$ are predefined hyperparameters when the length of the head is set to approximately 1.\\n\\nColor $C'$ of the Gaussians. Modeling the dynamic details typically requires dynamic color that changes with expressions. As we do not pre-define the neutral value in Eqn. 2, the colors are directly predicted by two color MLPs: $f_{\\\\text{exp}} \\\\in \\\\Phi$ and $f_{\\\\text{pose}} \\\\in \\\\Phi$:\\n\\n$$C' = \\\\lambda_{\\\\text{exp}}(X_0)f_{\\\\text{exp}}(F_0, \\\\theta) + \\\\lambda_{\\\\text{pose}}(X_0)f_{\\\\text{pose}}(F_0, \\\\beta).$$ (4)\\n\\nRotation, Scale and Opacity $\\\\{Q', S', A'\\\\}$ of the Gaussians. These three attributes also dynamic, thereby modeling some detailed expressions-related appearance changes. Here, we just use another two attribute MLPs $f_{\\\\text{exp}} \\\\in \\\\Phi$ and $f_{\\\\text{pose}} \\\\in \\\\Phi$ to predict their shift from the neutral value.\\n\\n$$\\\\{Q', S', A'\\\\} = \\\\{Q_0, S_0, A_0\\\\} + \\\\lambda_{\\\\text{exp}}(X_0)f_{\\\\text{exp}}(F_0, \\\\theta) + \\\\lambda_{\\\\text{pose}}(X_0)f_{\\\\text{pose}}(F_0, \\\\beta).$$ (5)\\n\\nFinally, we apply rigid rotations and translations $T(\\\\cdot)$ to the Gaussians, transforming them from the canonical space to the world space. Note, the transformation is only implemented for directional variables: $\\\\{X, Q\\\\}$, while the multi-channel color, the scale and the opacity $\\\\{C, S, A\\\\}$ are not directional thus remain unchanged.\\n\\n$$\\\\{X, Q\\\\} = T(\\\\{X', Q'\\\\}, \\\\beta),$$ (6)\\n\\n$$\\\\{C, S, A\\\\} = \\\\{C', S', A'\\\\}.$$ (7)\\n\\n4.2. Training\\n\\nIn this part, we explain the training pipeline of the Gaussian head avatar 4.1 and the loss function. In each iteration, we first generate the expression conditioned 3D Gaussians as Eqn. 2. Then we render a 32-channel image with 512 resolution $I_C \\\\in \\\\mathbb{R}^{512 \\\\times 512 \\\\times 32}$ referring to Eqn. 1. After that we feed the image to a super-resolution network $\\\\Psi$ to generate a 2048 resolution RGB image $I_{hr} \\\\in \\\\mathbb{R}^{2048 \\\\times 2048 \\\\times 3}$, such that more details are recovered and noise caused by uneven ambient light or camera chromatic aberration in the training data will be filtered out [49, 64].\\n\\nDuring training, we jointly optimize all the learnable parameters mentioned above in bold, including the neutral...\"}"}
{"id": "CVPR-2024-961", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Kara-Ali Aliev, Artem Sevastopolsky, Maria Kolos, Dmitry Ulyanov, and Victor Lempitsky. Neural point-based graph- \\nics. In European Conference on Computer Vision, pages 696\u2013712, 2020.\\n\\n[2] ShahRukh Athar, Zexiang Xu, Kalyan Sunkavalli, Eli Shechtman, and Zhixin Shu. Rignerf: Fully controllable neu- \\nrnal 3d portraits. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) \\n2022.\\n\\n[3] ShahRukh Athar, Zhixin Shu, and Dimitris Samaras. Flame-in-nerf: Neural control of radiance fields for free view face \\nanimation. In IEEE 17th International Conference on Automatic Face and Gesture Recognition (FG), pages 1\u20138, 2023.\\n\\n[4] Thabo Beeler, Bernd Bickel, Paul Beardsley, Bob Sumner, and Markus Gross. High-quality single-shot capture of facial \\ngeometry. ACM Trans. Graph., 29(4), 2010.\\n\\n[5] Sai Bi, Stephen Lombardi, Shunsuke Saito, Tomas Simon, Shih-En Wei, Kevyn Mcphail, Ravi Ramamoorthi, Yaser \\nSheikh, and Jason Saragih. Deep relightable appearance models for animatable faces. ACM Trans. Graph., 40(4), \\n2021.\\n\\n[6] V Blanz and T Vetter. A morphable model for the synthesis of 3d faces. In 26th Annual Conference on Computer Graph- \\ncics and Interactive Techniques (SIGGRAPH 1999), pages 187\u2013194. ACM Press, 1999.\\n\\n[7] Timo Bolkart, Tianye Li, and Michael J. Black. Instant multi-view head capture through learnable registration. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 768\u2013779, 2023.\\n\\n[8] Derek Bradley, Wolfgang Heidrich, Tiberiu Popa, and Alla Sheffer. High resolution passive facial performance capture. \\n29(4), 2010.\\n\\n[9] Alan Brunton, Timo Bolkart, and Stefanie Wuhrer. Multilinear wavelets: A statistical shape space for human faces. In Proceedings of the Proceedings of the European Conference on Computer Vision (ECCV), 2014.\\n\\n[10] Adrian Bulat and Georgios Tzimiropoulos. How far are we from solving the 2d & 3d face alignment problem? (and a \\ndataset of 230,000 3d facial landmarks). In International Conference on Computer Vision, 2017.\\n\\n[11] Chen Cao, Yanlin Weng, Shun Zhou, Y. Tong, and Kun Zhou. Facewarehouse: A 3d facial expression database for \\nvisual computing. In IEEE Transactions on Visualization and Computer Graphics, pages 413\u2013425, 2014.\\n\\n[12] Chen Cao, Derek Bradley, Kun Zhou, and Thabo Beeler. Real-time high-fidelity facial performance capture. ACM \\nTrans. Graph., 34(4), 2015.\\n\\n[13] Chen Cao, Hongzhi Wu, Yanlin Weng, Tianjia Shao, and Kun Zhou. Real-time facial animation with image-based dy- \\nnamic avatars. ACM Trans. Graph., 35(4), 2016.\\n\\n[14] Chen Cao, Tomas Simon, Jin Kyu Kim, Gabe Schwartz, Michael Zollhoefer, Shun-Suke Saito, Stephen Lombardi, \\nShih-En Wei, Danielle Belko, Shoou-I Yu, Yaser Sheikh, and Jason Saragih. Authentic volumetric avatars from a phone \\nscan. ACM Trans. Graph., 41(4), 2022.\\n\\n[15] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. Accurate 3d face reconstruction with \\nweakly-supervised learning: From single image to image set. In Proceedings of the IEEE/CVF Conference on Computer \\nVision and Pattern Recognition (CVPR) Workshops, 2019.\\n\\n[16] Guy Gafni, Justus Thies, Michael Zollhofer, and Matthias Niessner. Dynamic neural radiance fields for monocular 4d \\nfacial avatar reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition \\n(CVPR), pages 8645\u20138654, 2021.\\n\\n[17] Xuan Gao, Chenglai Zhong, Jun Xiang, Yang Hong, Yudong Guo, and Juyong Zhang. Reconstructing person- \\nalized semantic facial nerf models from monocular video. ACM Transactions on Graphics (Proceedings of SIGGRAPH Asia) \\n41(6), 2022.\\n\\n[18] Thomas Gerig, Andreas Forster, Clemens Blumer, Bernhard Egger, Marcel L\u00a8uthi, Sandro Sch\u00a8onborn, and Thomas Vetter. \\nMorphable face models - an open framework. pages 75\u201382, 2017.\\n\\n[19] Abhijeet Ghosh, Graham Fyffe, Borom Tunwattanapong, Jay Busch, Xueming Yu, and Paul Debevec. Multiview face cap- \\nture using polarized spherical gradient illumination. ACM Trans. Graph., 30(6):1\u201310, 2011.\\n\\n[20] Philip-William Grassal, Malte Prinzler, Titus Leistner, Carsten Rother, Matthias Nie\u00dfner, and Justus Thies. Neural \\nhead avatars from monocular rgb videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern \\nRecognition (CVPR), pages 18632\u201318643, 2022.\\n\\n[21] Yudong Guo, Keyu Chen, Sen Liang, Yong-Jin Liu, Hujun Bao, and Juyong Zhang. Ad-nerf: Audio driven neural ra- \\ndiance fields for talking head synthesis. In Proceedings of the IEEE/CVF International Conference on Computer Vision \\n(ICCV), pages 5764\u20135774, 2021.\\n\\n[22] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a \\ntwo time-scale update rule converge to a local nash equilib- \\nrium. In Neural Information Processing Systems, 2017.\\n\\n[23] Yang Hong, Bo Peng, Haiyao Xiao, Ligang Liu, and Juy- \\nong Zhang. Headnerf: A real-time nerf-based parametric \\nhead model. In Proceedings of the IEEE/CVF Conference \\non Computer Vision and Pattern Recognition (CVPR), pages \\n20374\u201320384, 2022.\\n\\n[24] Liwen Hu, Shunsuke Saito, Lingyu Wei, Koki Nagano, Jae- \\nwoo Seo, Jens Fursund, Iman Sadeghi, Carrie Sun, Yen- \\nChun Chen, and Hao Li. Avatar digitization from a single \\nimage for real-time rendering. ACM Trans. Graph., 36(6), \\n2017.\\n\\n[25] Alexandru Eugen Ichim, Sofien Bouaziz, and Mark Pauly. Dynamic 3d avatar creation from hand-held video input. \\nACM Trans. Graph., 34(4), 2015.\\n\\n[26] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk \u00a8uhler, \\nand George Drettakis. 3d gaussian splatting for real-time \\nradiance field rendering. ACM Transactions on Graphics, \\n42(4), 2023.\\n\\n[27] Taras Khakhulin, Vanessa Sklyarova, Victor Lempitsky, and \\nEgor Zakharov. Realistic one-shot mesh-based head avatars. \\nIn Proceedings of the European Conference on Computer Vi- \\nsion (ECCV), 2022.\"}"}
{"id": "CVPR-2024-961", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tobias Kirschstein, Shenhan Qian, Simon Giebenhain, Tim Walter, and Matthias Nie\u00dfner. Nersemble: Multi-view radiance field reconstruction of human heads. ACM Trans. Graph., 42(4), 2023.\\n\\nGeorgios Kopanas, Thomas Leimk\u00fchler, Gilles Rainer, Cl\u00e9ment Jambon, and George Drettakis. Neural point catacaustics for novel-view synthesis of reflections. ACM Transactions on Graphics (TOG), 41(6):1\u201315, 2022.\\n\\nChristoph Lassner and Michael Zollhofer. Pulsar: Efficient sphere-based neural rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1440\u20131449, 2021.\\n\\nMarc Levoy, Kari Pulli, Brian Curless, Szymon Rusinkiewicz, David Koller, Lucas Pereira, Matt Ginzton, Sean Anderson, James Davis, Jeremy Ginsberg, Jonathan Shade, and Duane Fulk. The digital michelangelo project: 3d scanning of large statues. In Proceedings of the 27th Annual Conference on Computer Graphics and Interactive Techniques, page 131\u2013144, USA, 2000. ACM Press/Addison-Wesley Publishing Co.\\n\\nTianye Li, Timo Bolkart, Michael J. Black, Hao Li, and Javier Romero. Learning a model of facial shape and expression from 4d scans. ACM Trans. Graph., 36(6), 2017.\\n\\nTianye Li, Shichen Liu, Timo Bolkart, Jiayi Liu, Hao Li, and Yajie Zhao. Topologically consistent multi-view face inference using volumetric sampling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3824\u20133834, 2021.\\n\\nZhe Li, Zerong Zheng, Lizhen Wang, and Yebin Liu. Animatable gaussians: Learning pose-dependent gaussian maps for high-fidelity human avatar modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\\n\\nShanchuan Lin, Andrey Ryabtsev, Soumyadip SenGupta, Brian Curless, Steve Seitz, and Ira Kemelmacher-Shlizerman. Real-time high-resolution background matting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nXian Liu, Yinghao Xu, Qianyi Wu, Hang Zhou, Wayne Wu, and Bolei Zhou. Semantic-aware implicit neural audio-driven video portrait generation. In Proceedings of the European Conference on Computer Vision (ECCV), 2022.\\n\\nStephen Lombardi, Jason Saragih, Tomas Simon, and Yaser Sheikh. Deep appearance models for face rendering. ACM Trans. Graph., 37(4):68:1\u201368:13, 2018.\\n\\nStephen Lombardi, Tomas Simon, Jason Saragih, Gabriel Schwartz, Andreas Lehrmann, and Yaser Sheikh. Neural volumes: Learning dynamic renderable volumes from images. ACM Trans. Graph., 38(4):65:1\u201365:14, 2019.\\n\\nStephen Lombardi, Tomas Simon, Gabriel Schwartz, Michael Zollhoefer, Yaser Sheikh, and Jason Saragih. Mixture of volumetric primitives for efficient neural rendering. ACM Trans. Graph., 40(4), 2021.\\n\\nJonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis, 2023.\\n\\nShugao Ma, Tomas Simon, Jason Saragih, Dawei Wang, Yuecheng Li, Fernando De La Torre, and Yaser Sheikh. Pixel codec avatars. In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 64\u201373, 2021.\\n\\nMarko Mihajlovic, Aayush Bansal, Michael Zollhoefer, Siyu Tang, and Shunsuke Saito. Keypointnerf: Generalizing image-based volumetric avatars using relative spatial encoding of keypoints. In European conference on computer vision, 2022.\\n\\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.\\n\\nJacob Munkberg, Jon Hasselgren, Tianchang Shen, Jun Gao, Wenzheng Chen, Alex Evans, Thomas M\u00fcller, and Sanja Fidler. Extracting Triangular 3D Models, Materials, and Lighting From Images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8280\u20138290, 2022.\\n\\nKoki Nagano, Jaewoo Seo, Jun Xing, Lingyu Wei, Zimo Li, Shunsuke Saito, Aviral Agarwal, Jens Fursund, and Hao Li. Pagan: Real-time avatars using dynamic textures. ACM Trans. Graph., 37(6), 2018.\\n\\nPascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, and Thomas Vetter. A 3d face model for pose and illumination invariant face recognition. In 2009 Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance, pages 296\u2013301, 2009.\\n\\nMinghan Qin, Yifan Liu, Yuelang Xu, Xiaochen Zhao, Yebin Liu, and Haoqian Wang. High-fidelity 3d head avatars reconstruction through spatially-varying expression conditioned neural radiance field. In AAAI Conference on Artificial Intelligence, 2023.\\n\\nAmit Raj, Michael Zollhoefer, Tomas Simon, Jason Saragih, Shunsuke Saito, James Hays, and Stephen Lombardi. Pva: Pixel-aligned volumetric avatars. In arXiv:2101.02697, 2020.\\n\\nDarius R\u00fcckert, Linus Franke, and Marc Stamminger. Adop: Approximate differentiable one-pixel point rendering. ACM Trans. Graph., 41(4), 2022.\\n\\nRuizhi Shao, Jingxiang Sun, Cheng Peng, Zerong Zheng, Boyao Zhou, Hongwen Zhang, and Yebin Liu. Control4d: Efficient 4d portrait editing with text. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2024.\\n\\nTianchang Shen, Jun Gao, Kangxue Yin, Ming-Yu Liu, and Sanja Fidler. Deep marching tetrahedra: a hybrid representation for high-resolution 3d shape synthesis. In Advances in Neural Information Processing Systems (NeurIPS), 2021.\\n\\nJingxiang Sun, Xuan Wang, Lizhen Wang, Xiaoyu Li, Yong Zhang, Hongwen Zhang, and Yebin Liu. Next3d: Generative neural texture rasterization for 3d-aware head avatars. 2024.\"}"}
{"id": "CVPR-2024-961", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2024-961", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Gaussians: \\\\{X_0, F_0, Q_0, S_0, A_0\\\\}, the dynamic generator: \\\\{f_{\\\\text{exp, col}}, f_{\\\\text{pose, col}}, f_{\\\\text{exp, def}}, f_{\\\\text{pose, def}}, f_{\\\\text{exp, att}}, f_{\\\\text{pose, att}}\\\\}, and the super resolution network \u03a8. For the loss function, we only use the foreground RGB images \\\\(I_{gt}\\\\) as supervision to construct an L1 loss and a VGG perceptual loss \\\\[70\\\\] between the generated images \\\\(I_{hr}\\\\) and the ground truth \\\\(I_{gt}\\\\). Besides, we encourage the first three channels of the 32-channel feature image \\\\(I_C\\\\) to be RGB channels, which is ensured by a L1 loss term. The total loss is:\\n\\n\\\\[L = ||I_{hr} - I_{gt}||_1 + \\\\lambda_{vgg} \\\\cdot \\\\text{VGG}(I_{hr}, I_{gt}) + \\\\lambda_{lr} ||I_{lr} - I_{gt}||_1,\\\\]\\n\\nwith \\\\(I_{lr}\\\\) denoting the first three channels of the 32-channel image \\\\(I_C\\\\). We set the weights \\\\(\\\\lambda_{vgg} = 0.1\\\\) and \\\\(\\\\lambda_{lr} = 1.4\\\\).\\n\\n4.3. Geometry-guided Initialization\\nUnlike neural networks, the Gaussians act as an unordered and unstructured representation. Random initialization leads to failure to converge while naively using the FLAME model to initialize will significantly reduce the reconstruction quality. In this section, we describe in detail how to optimize a mesh guidance model to provide reliable initialization for the Gaussians in Sec. 4.1.\\n\\nMesh Guidance Model. Specifically, we first construct a MLP \\\\(f_{\\\\text{sd f}}\\\\) to represent a signed distance field. In addition, this network will also output the corresponding feature vector of each point, which is used for predicting the point color. It can be formulated as:\\n\\n\\\\[s, \\\\eta = f_{\\\\text{sd f}}(x),\\\\]\\n\\nwith \\\\(s\\\\) denotes the SDF value, \\\\(\\\\eta\\\\) denotes the feature vector and \\\\(x\\\\) denotes the point position. Then through \\\\((\\\\text{DMTet}) [51]\\\\), we can differentially extract a mesh with vertices \\\\(X\\\\), per-vertex feature vectors \\\\(F\\\\) and its faces. We also predict the per-vertex 32-channel color as Eqn. 4 by the two color MLPs \\\\(f_{\\\\text{exp, col}}\\\\) and \\\\(f_{\\\\text{pose, col}}\\\\). In parallel, we construct the two deformation MLPs: \\\\(f_{\\\\text{exp, def}}\\\\) and \\\\(f_{\\\\text{pose, def}}\\\\) as described in Sec. 4.1 to predict the displacements and add them to the vertex positions. This process is similar to Eqn 3 above, with the Gaussian positions \\\\(X_0\\\\) replaced by the vertex positions \\\\(X\\\\). Finally we also apply rigid rotations and translations to the deformed mesh, transforming it to the world space and render the deformed mesh into an image \\\\(I\\\\) and a mask \\\\(M\\\\) through differentiable rasterization \\\\[44\\\\] according to the camera parameters \\\\(\\\\mu\\\\).\\n\\nLoss Function and Training. Next, we can construct the RGB loss and the silhouette loss to train the guidance model:\\n\\n\\\\[L_{\\\\text{RGB}} = ||I_{r,g,b} - I_{gt}||_1,\\\\]\\n\\n\\\\[L_{\\\\text{sil}} = \\\\text{IOU}(M, M_{gt}),\\\\]\\n\\nwith \\\\(I_{gt}\\\\) and \\\\(M_{gt}\\\\) denote the ground truth RGB image and mask, respectively. \\\\(\\\\text{IOU}(\\\\cdot)\\\\) denotes Intersection over Union metrics. Note that only the first three channels \\\\(R, G, B\\\\) of the 32-channel image \\\\(I\\\\) are supervised by the ground truth RGB images. We also use the estimated 3D facial landmarks \\\\(P_{gt}\\\\) as described in Sec. 3 to provide rough guidance for the expression deformation MLP. Specifically, we input the neutral 3D landmarks \\\\(P_0\\\\) into the expression deformation MLP to predict the expression conditioned landmarks \\\\(P\\\\):\\n\\n\\\\[P = P_0 + f_{\\\\text{exp, def}}(P_0, \\\\theta).\\\\]\\n\\nThen we construct the loss function with 3D facial landmarks \\\\(P_{gt}\\\\) as the supervision:\\n\\n\\\\[L_{\\\\text{def}} = ||P - P_{gt}||_2,\\\\]\\n\\nBesides, we introduce three constraints: (1) a regular term \\\\(L_{\\\\text{offset}}\\\\) to punish all non-zero displacements to prevent the two deformation MLPs from learning a global constant offset \\\\[63\\\\], (2) a regular term \\\\(L_{\\\\text{lmk}}\\\\) to limit the SDF value at the 3D landmarks to be close to zero, such that the landmarks are located on the surface of the mesh, (3) a Laplacian term \\\\(L_{\\\\text{lap}}\\\\) for maintaining the extracted mesh smooth to a certain extent. Overall, the total loss function is formulated as:\\n\\n\\\\[L = L_{\\\\text{RGB}} + \\\\lambda_{\\\\text{sil}} \\\\cdot L_{\\\\text{sil}} + \\\\lambda_{\\\\text{def}} \\\\cdot L_{\\\\text{def}} + \\\\lambda_{\\\\text{offset}} \\\\cdot L_{\\\\text{offset}} + \\\\lambda_{\\\\text{lmk}} \\\\cdot L_{\\\\text{lmk}} + \\\\lambda_{\\\\text{lap}} \\\\cdot L_{\\\\text{lap}},\\\\]\\n\\nwith \\\\(\\\\lambda\\\\) denoting the weights of each term, which are set as follows:\\n\\n\\\\(\\\\lambda_{\\\\text{sil}} = 0.1\\\\), \\\\(\\\\lambda_{\\\\text{def}} = 1\\\\), \\\\(\\\\lambda_{\\\\text{offset}} = 0.01\\\\), \\\\(\\\\lambda_{\\\\text{lmk}} = 0.1\\\\) and \\\\(\\\\lambda_{\\\\text{lap}} = 100\\\\). We jointly optimize the MLPs mentioned above with the neutral 3D landmarks \\\\(P_0\\\\) jointly until all MLPs are converged.\\n\\nParameters Transfer. Finally, we use the roughly trained mesh guidance model to initialize the Gaussian model. Specifically, we extract the neutral mesh with vertices \\\\(X\\\\) and per-vertex features \\\\(F\\\\) through DMTet, and directly assign their values to the neutral positions \\\\(X_0 = X\\\\) and the per-vertex feature vectors \\\\(F_0 = F\\\\) of the neutral Gaussians respectively. Then, we retain all the four optimized MLPs: \\\\{\\\\(f_{\\\\text{exp, col}}, f_{\\\\text{pose, col}}, f_{\\\\text{exp, def}}, f_{\\\\text{pose, def}}\\\\\\\\} for the Gaussian model. For the other neutral attributes: rotation, scale and opacity, we adopt the original initialization strategy in Gaussian Splatting \\\\[26\\\\]. And the parameters of the two attribute MLPs: \\\\{\\\\(f_{\\\\text{exp, att}}, f_{\\\\text{pose, att}}\\\\\\\\} and the super resolution network \\\\(\\\\Psi\\\\) are just randomly initialized.\\n\\n5. Experiments\\n5.1. Implementation Details\\nIn the experiment we use 12 sets of data, 10 of which are from NeRSemble \\\\[28\\\\], and the other 2 are multi-view video data from HAvatar \\\\[71\\\\]. For the 10 identities from NeRSemble, each set contains 2500 to 3000 frames, 16\"}"}
{"id": "CVPR-2024-961", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Qualitative comparisons of different methods on self reenactment task. From left to right: NeRFBlendShape [17], NeRFace [16], HAvatar [71] and Ours. Our method can reconstruct details like beards, teeth, etc. with high quality.\\n\\nCameras are distributed about 120 degrees in front, and simultaneously capture 2K resolution video. For each identity, we use the sequences marked with \u201cFREE\u201d as the evaluation data, and the rest as the training data. For the 2 identities from HAvatar, each set contains 3000 frames, 8 cameras are distributed about 120 degrees in front, and 4K resolution videos are collected simultaneously. Later, we crop the face area and resize to 2K resolution.\\n\\nFor data preprocessing, we first remove the background [35] and extract 68 2D facial landmarks [10] for all the images. Then, for each frame, we use multi-view images to estimate the corresponding 3D landmarks, expression coefficients and head pose by fitting the Basel Face Model (BFM) [18]. Note that we define the 3D landmarks as the usual 68 landmarks with vertices indexed as multiples of 100 in the BFM vertices.\\n\\n5.2. Results and Comparisons\\n\\nSelf Reenactment. In this section, we first compare our method with existing SOTA methods in qualitative experiments on self reenactment task. Specifically, NeRFace [16] uses a deep MLP to fit an expression conditioned dynamic NeRF. The current SOTA method HAvatar [71] introduces 3DMM template prior and uses a deep convolutional network to generate a human head NeRF represented by three planes from a mesh template with expression. Note, HAvatar leverages the GAN framework using the adversarial loss function to force the network to generate details that are not view-consistent. For a fair comparison, we remove this part and use VGG perceptual loss as Sec. 4.2 instead.\\n\\nQualitative results on self reenactment task are shown in the Fig. 3. Our method can accurately reconstruct pixel-level high-frequency details such as beards, teeth, and hair. Besides, our method can achieve expression transfer more accurately, such as eye movements in the figure.\\n\\nNext, we conduct a quantitative evaluation for the four methods on 5 identities and 6 cameras using the evaluation split. The evaluation metrics include: Peak Signal-to-Noise Ratio (PSNR), Structure Similarity Index (SSIM), Learned Perceptual Image Patch Similarity (LPIPS) [70] and Fr\u00e9chet Inception Distance (FID) [22]. Note, we calculate FID by comparing the distribution of all the training images and all the rendered images. As the task mainly focuses on the reconstruction of the head, we use face parsing to remove the body parts in the image to eliminate their impact in the experiment. As shown in Tab. 1, our method demonstrates a slight improvement in PSNR and SSIM compared with\"}"}
{"id": "CVPR-2024-961", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Quantitative evaluation results of NeRFBlendShape [17], NeRFace [16], HAvatar [71], our method without super resolution and our full method on self reenactment task.\\n\\n| Method          | PSNR  | SSIM  | LPIPS (512) | LPIPS (2K) | FID (2K) |\\n|-----------------|-------|-------|-------------|------------|----------|\\n| NeRFBlendShape  | 25.91 | 0.836 | 0.123       | 0.229      | 54.80    |\\n| NeRFace         | 27.14 | 0.849 | 0.147       | 0.234      | 65.11    |\\n| HAvatar         | 27.19 | 0.883 | 0.064       | 0.209      | 31.06    |\\n| Ours (w/o SR)   | 27.82 | 0.887 | 0.080       | 0.202      | 45.50    |\\n| Ours            | 27.70 | 0.883 | 0.056       | 0.098      | 18.50    |\\n\\nTable 2. Quantitative evaluation results of the other SOTA methods and our method on 3D consistency.\\n\\n| Method          | PSNR  | SSIM  | LPIPS |\\n|-----------------|-------|-------|-------|\\n| NeRFBlendShape  | 25.43 | 0.812 | 0.148 |\\n| NeRFace         | 26.65 | 0.825 | 0.151 |\\n| HAvatar         | 27.13 | 0.880 | 0.65  |\\n| Ours            | 27.58 | 0.882 | 0.059 |\\n\\nCross-Identity Reenactment. We also qualitatively compare our method with the above SOTA methods on cross-identity reenactment task. As shown in the results, our method is able to synthesize higher-fidelity images with more accurate expression transfer and richer emotions.\\n\\nNovel View Synthesis. In this section, we show the results of novel view synthesis as shown in Fig. 5. In this case, we use video data from 8 views for training and render the image at a new viewpoint. Next, we quantitatively evaluate the 3D consistency of our method and compare it with other SOTA methods mentioned above. Specifically, we select the 5 identities as above and use the video data from 8 cameras for training while the other 8 holdout cameras for evaluation. The evaluation metrics include PSNR, SSIM and LPIPS (512 resolution). The results are shown in Tab. 2. Our method outperforms other methods in 3D consistency.\\n\\n5.3. Ablation Study\\n\\nTable 3. Quantitative evaluation results of the other two ablation baselines and ours on self reenactment task.\\n\\n| Method          | PSNR  | SSIM  | LPIPS |\\n|-----------------|-------|-------|-------|\\n| FLAME-Init      | 28.73 | 0.875 | 0.123 |\\n| Mesh-Deform     | 28.83 | 0.874 | 0.116 |\\n| Ours            | 28.94 | 0.876 | 0.108 |\"}"}
{"id": "CVPR-2024-961", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Novel view synthesis results of our method. We use 8-view synchronized videos for training the avatar.\\n\\nFigure 6. Ablation study on the initialization strategies: FLAME-initialization and our geometry-guided initialization. Our strategy ensures the hair strands away from the head are well reconstructed.\\n\\nExpression deformation MLP is optimized to learn the displacement of FLAME vertices. We set the per-vertex feature to zeros, while randomly initialize the parameters of the expression color MLP. The initialization of other variables is the same as our strategy. Qualitative results are shown in the Fig. 6. Due to the lack of initialization for the hair and shoulders in FLAME-Init, the points to model these parts are offset from nearby vertices, which leads to sparseness of the Gaussians, resulting in blurring.\\n\\nAblation on Deformation Modeling Approaches. We compare our fully learned deformation field with the previous mesh-based deformation (Mesh-Deform). Specifically, we migrate the method in INSTA [76] for controlling the NeRF deformation to our Gaussians. First we fit a 3DMM mesh template. Then, for each Gaussian point, find the closest face on the mesh, and calculate the deformation gradient to estimate the displacement. Qualitative results are shown in the Fig. 7. For some expressions that cannot be captured well by the 3DMM mesh template, our method can learn accurate deformation, thereby achieving the modeling of complex expressions. Quantitave results are shown in the Fig. 3.\\n\\nOur method outperforms both the two ablation baselines on PSNR, SSIM and LPIPS metrics.\\n\\nFigure 7. Ablation study on the deformation modeling: mesh LBS-based deformation and our fully learned deformation. Our approach can learn complex and exaggerated expressions.\\n\\n6. Discussion and Conclusion\\n\\nEthical Considerations. Our method is capable of creating artificial portrait videos, which have the potential to disseminate misinformation, influence public perceptions, and erode confidence in media sources.\\n\\nLimitation. For the tongue and teeth inside the mouth or long hair, blurring is sometimes produced in our method due to the lack of tracking methods.\\n\\nConclusion. In this paper, we propose Gaussian Head Avatar, a novel representation for head avatar reconstruction, which leverages dynamic 3D Gaussians controlled by a fully learned expression deformation. Experiments demonstrate our method can synthesize ultra high-fidelity images while modeling exaggerated expressions. In addition, we propose a well-designed minute-level initialization strategy to ensure the training convergence. We believe our Gaussian Head Avatar will become the mainstream direction for head avatar reconstruction in the future.\\n\\nAcknowledgement. This paper is supported by National Key R&D Program of China (2022YFF0902200), the NSFC project No.62125107, the Beijing Municipal Science & Technology Z231100005923030.\"}"}
