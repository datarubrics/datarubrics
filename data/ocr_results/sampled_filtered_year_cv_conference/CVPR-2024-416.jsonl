{"id": "CVPR-2024-416", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\( \\\\mathcal{P}_k \\\\mathcal{C}_k a \\\\mathcal{C}_k m, t \\\\mathcal{C}_k a, t Sigmoid \\\\mathcal{C}_k a ... \\\\)\\n\\n\\\\( s_t^k = \\\\text{Sigmoid} (\\\\text{MLP} (P_k)) \\\\)\\n\\n\\\\( s_{s_t}^k(i) \\\\) is the more relevant the instance feature in frame \\\\( i \\\\).\\n\\n\\\\( C_{a,t}^k = \\\\text{HF}_{t} (C_a^k, s_t^k, \\\\theta_t) \\\\)\\n\\n\\\\( C_{m,t}^k = \\\\text{HF}_{t} (C_m^k, s_t^k, \\\\theta_t) \\\\)\\n\\n\\\\( \\\\theta_t \\\\) is a threshold operation that passes instance context features of \\\\( C_a^k \\\\) and \\\\( C_m^k \\\\) with confidence scores greater than \\\\( \\\\theta_t \\\\).\\n\\n\\\\( s_s^k \\\\) are spatial confidence scores which are measured using the predicted IoU confidence [14], originally used for detection.\"}"}
{"id": "CVPR-2024-416", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"vIoU score compared to CSDVL [21]. Compared to our tIoU represents vIoU and vIoU@R heads is set to 8 are applied to all training videos. The number of attention methods such as random resizing and random cropping 256 and decoder is v1, 32 L cross-entropy loss are used as follows,\\n\\nFor spatial grounding, smooth L1 loss, IoU loss and binary cross-entropy are used as the loss function and the losses of the bounding box sequence B H groundtruth start timestamps s and e H predict: (1) start timestamps s, (2) temporal and spatial confidence scores C, (3) temporal and spatial confidence scores C\\n\\ns and e H predict: (1) start timestamps s, (2) temporal and spatial confidence scores C, (3) temporal and spatial confidence scores C\\n\\n4. Experiments\\n\\nThe initial learning rate for three backbones is set to 2\\n\\nThe batch size is set to 16 and data augmentations for test set are not publicly available, we present the results based on validation set as existing methods [21, 35].\\n\\nFollowing [16, 21, 35], VidSTG is divided into datasets, i.e. HCSTVG, VidSTG [41]. HCSTVG, focusing solely on humans in videos, is available in two versions: HCSTVG-v1 and HCSTVG-v2. Following [16, 31, 35], we divide the HCSTVG-v1 into 160 distinct videos, and HCSTVG-v2 further expands HCSTVG-v1 and vIoU is calculated as vIoU represents the average vIoU score over all testing videos.\\n\\nTo validate the effectiveness of CG-STVG, we compare it with other state-of-the-art methods also achieves SOTA in 3 out of 4 metrics as shown results on the HCSTVG-v1 test set, and our proposed method achieves state-of-the-art performance in 3 out of 4 metrics. Specifically, our method improves the temporal localization performance, while m\\n\\n10 and K\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0\\n\\n9\\n\\n408\\n\\n413\\n\\n|P\\n\\ns\\n\\ni\\n\\nu\\n\\n2\\n\\n4\\n\\n4\\n\\n4\\n\\n3\\n\\n4\\n\\n0"}
{"id": "CVPR-2024-416", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1. Comparison with others on HCSTVG-v1 test set (%).\\n\\n| Methods      | mtIoU | mvIoU | vIoU@0.3 | vIoU@0.5 |\\n|--------------|-------|-------|----------|----------|\\n| STGVT        | -     | 18.2  | 9.5      |          |\\n| [TCSVT21]    |       |       |          |          |\\n| STVGBert     | -     | 20.4  | 11.3     |          |\\n| [ICCV21]     |       |       |          |          |\\n| TubeDETR     | 43.7  | 32.4  | 49.8     | 23.5     |\\n| [CVPR22]     |       |       |          |          |\\n| STCAT        | 49.4  | 35.1  | 57.7     | 30.1     |\\n| [NeurIPS22]  |       |       |          |          |\\n| CSDVL        | -     | 36.9  | 62.2     | 34.8     |\\n| [CVPR23]     |       |       |          |          |\\n| Baseline     | 50.4  | 36.5  | 58.6     | 32.3     |\\n| CG-STVG      | 52.8  | 38.4  | 61.5     | 36.3     |\\n|             | (+2.4)| (+1.9)| (+2.9)   | (+4.0)   |\\n\\n### Table 2. Comparison with others on HCSTVG-v2 val. set (%).\\n\\n| Methods      | mtIoU | mvIoU | vIoU@0.3 | vIoU@0.5 |\\n|--------------|-------|-------|----------|----------|\\n| STGRN        | 48.5  | 19.8  | 25.8     | 14.6     |\\n| [CVPR20]     |       |       |          |          |\\n| OMRN         | 50.7  | 23.1  | 32.6     | 16.4     |\\n| [IJCAI20]    |       |       |          |          |\\n| STGVT        | -     | 21.6  | 29.8     | 18.9     |\\n| [TCSVT21]    |       |       |          |          |\\n| STVGBert     | -     | 24.0  | 30.9     | 18.4     |\\n| [ICCV21]     |       |       |          |          |\\n| TubeDETR     | 48.1  | 30.4  | 42.5     | 28.2     |\\n| [CVPR22]     |       |       |          |          |\\n| STCAT        | 50.8  | 33.1  | 46.2     | 32.6     |\\n| [NeurIPS22]  |       |       |          |          |\\n| CSDVL        | -     | 33.7  | 47.2     | 32.8     |\\n| [CVPR23]     |       |       |          |          |\\n| Baseline     | 49.7  | 32.4  | 45.0     | 31.4     |\\n| CG-STVG      | 51.4  | 34.0  | 47.7     | 33.1     |\\n|             | (+1.7)| (+1.6)| (+2.7)   | (+1.7)   |\\n\\n### Table 3. Comparison with existing state-of-the-art methods on VidSTG test set (%).\\n\\n| Methods      | mtIoU | mvIoU | vIoU@0.3 | vIoU@0.5 |\\n|--------------|-------|-------|----------|----------|\\n| STGRN        | 48.5  | 19.8  | 25.8     | 14.6     |\\n| [CVPR20]     |       |       |          |          |\\n| OMRN         | 50.7  | 23.1  | 32.6     | 16.4     |\\n| [IJCAI20]    |       |       |          |          |\\n| STGVT        | -     | 21.6  | 29.8     | 18.9     |\\n| [TCSVT21]    |       |       |          |          |\\n| STVGBert     | -     | 24.0  | 30.9     | 18.4     |\\n| [ICCV21]     |       |       |          |          |\\n| TubeDETR     | 48.1  | 30.4  | 42.5     | 28.2     |\\n| [CVPR22]     |       |       |          |          |\\n| STCAT        | 50.8  | 33.1  | 46.2     | 32.6     |\\n| [NeurIPS22]  |       |       |          |          |\\n| CSDVL        | -     | 33.7  | 47.2     | 32.8     |\\n| [CVPR23]     |       |       |          |          |\\n| Baseline     | 49.7  | 32.4  | 45.0     | 31.4     |\\n| CG-STVG      | 51.4  | 34.0  | 47.7     | 33.1     |\\n|             | (+1.7)| (+1.6)| (+2.7)   | (+1.7)   |\\n\\n### Table 4. Ablation study of ICG and ICR on HCSTVG-v1 test set.\\n\\n| Methods | mtIoU | mvIoU | vIoU@0.3 | vIoU@0.5 |\\n|---------|-------|-------|----------|----------|\\n| \u2713       | 50.42 | 36.52 |          |          |\\n| \u2713 T     | 51.07 | 37.42 | 59.48    | 32.93    |\\n| \u2713 S     | 51.26 | 37.86 | 60.95    | 33.28    |\\n| \u2713 S+T   | 52.80 | 38.04 | 60.90    | 35.40    |\\n\\n### Table 5. Ablation of thresholds in ICR on HCSTVG-v1 test set.\\n\\n| Threshold | mtIoU | mvIoU | vIoU@0.5 |\\n|-----------|-------|-------|----------|\\n| 0.3       | 52.82 | 38.19 | 35.34    |\\n| 0.5       | 52.80 | 38.29 | 35.43    |\\n| 0.7       | 52.84 | 38.42 | 36.29    |\\n| 0.9       | 52.84 | 38.27 | 36.12    |\\n\\n### Table 6. Ablation of usage of temporal and spatial confidence.\\n\\n| Usage     | mtIoU | mvIoU | vIoU@0.3 | vIoU@0.5 |\\n|-----------|-------|-------|----------|----------|\\n| Two-level (ours) | 38.42 | 61.47 | 36.29    |          |\\n| One-level w/ \\\"s+t\\\" | 38.31 | 61.12 | 35.69    |          |\\n| One-level w/ \\\"s\u00d7t\\\" | 38.25 | 61.07 | 35.52    |          |\\n\\n### Table 7. Ablation on applying instance context to TDB.\\n\\n| Method          | mtIoU | mvIoU | vIoU@0.3 | vIoU@0.5 |\\n|-----------------|-------|-------|----------|----------|\\n| w/o Instance Context | 52.84 | 38.42 | 61.47    | 36.29    |\\n| w/ Instance Context | 52.61 | 38.01 | 61.03    | 35.78    |\\n\\n### 4.3. Ablation Study\\n\\n#### Impact of ICG and ICR\\n\\nThe key of CG-STVG lies in two simple yet effective modules, including ICG and ICR, for instance context learning. To verify their effectiveness, we conduct ablation experiments on HCSTVG-v1 in Tab. 4. As in Tab. 4, our baseline achieves a mvIoU score of 36.52 without ICG and ICR. After incorporating ICG for instance context, the mvIoU score is increased to 37.42, demonstrating that the visual context from ICG helps improve the grounding performance. To enhance the quality of instance context, we use a spatial-temporal joint refinement mechanism in ICR module. When we apply temporal refinement alone, we observe that the mvIoU score is improved by 1.9.\\n\\n---\\n\\nUsage of \\\\(s_t\\\\) and \\\\(s_s\\\\) in Tab. 2. CSDVL [21] won the first place in the HCSTVG track of the 4-th Person in Context Challenge. Compared to the CSDVL, our approach outperforms it by 1.9, 0.8, and 2.5 scores on mtIoU, mvIoU, and vIoU@0.5 metrics, respectively. The significant improvement in metric vIoU@0.5 across two datasets indicates that instance context excels at refining bounding boxes with an IoU under 0.5.\\n\\n---\\n\\nThe experimental results further evidence the effectiveness of our method, showing that instance context information helps ground the target.\"}"}
{"id": "CVPR-2024-416", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The man in blue clothes speaks, and the blue man follows him and walks forward.\\n\\nThe woman in the white dress to the right of the woman in the gray dress hands the document to the gray woman.\\n\\nImpact of temporal and spatial confidence score usage. Temporal and spatial confidence scores $s_{t,k}$ and $s_{s,k}$ are crucial for instance context refinement. In this work, we adopt a two-level method to separately use $s_{t,k}$ and $s_{s,k}$ for refinement. To further study the impact of different methods for the usage of temporal and spatial confidence scores, we design two additional one-level methods for refinement: one is to add $s_{t,k}$ and $s_{s,k}$ and then apply a single fused confidence for refinement (one-level with $s_{t,k} + s_{s,k}$), and the other is to multiply $s_{t,k}$ and $s_{s,k}$ for refinement (one-level with $s_{t,k} \\\\times s_{s,k}$). We show the architectures of these two variants and comparison with our strategy in supplementary material. We conducted experiments as in Tab. 6, and we can see that our two-level method achieves better performance.\\n\\nImpact of applying instance context to TDB. From the Tab.4, it can be seen that as the spatial grounding improves with the help of context, the temporal grounding is also improving, $50.42$ vs $52.84$. To explore the impact of applying the instance context to the TDB on model performance, we conduct ablation study as shown in Tab. 7. There is a slight drop in model performance after employing context to the TDB. We believe the temporal branch is mainly used to determine the boundaries of events, and the context from the spatial branch has a gap with the temporal branch. Directly using context in temporal branch may cause boundary blur.\"}"}
{"id": "CVPR-2024-416", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. VQA: Visual question answering. In ICCV, 2015. 3\\n\\n[2] Wayner Barrios, Mattia Soldan, Fabian Caba Heilbron, Alberto Mario Ceballos-Arroyo, and Bernard Ghanem. Localizing moments in long video via multimodal guidance. In ICCV, 2023. 3\\n\\n[3] Meng Cao, Tianyu Yang, Junwu Weng, Can Zhang, Jue Wang, and Yuexian Zou. Locvtp: Video-text pre-training for temporal localization. In ECCV, 2022. 3\\n\\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020. 2, 3, 4\\n\\n[5] Long Chen, Yuhang Zheng, and Jun Xiao. Rethinking data augmentation for robust visual question answering. In ECCV, 2022. 3\\n\\n[6] Yi-Wen Chen, Yi-Hsuan Tsai, and Ming-Hsuan Yang. End-to-end multi-modal video temporal grounding. In NeurIPS, 2021. 3\\n\\n[7] Xin Gu, Guang Chen, Yufei Wang, Libo Zhang, Tiejian Luo, and Longyin Wen. Text with knowledge graph augmented transformer for video captioning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023. 3\\n\\n[8] Mingzhe Guo, Zhipeng Zhang, Heng Fan, and Liping Jing. Divert more attention to vision-language tracking. NeurIPS, 2022. 3\\n\\n[9] Mingfei Han, Yali Wang, Zhihui Li, Lina Yao, Xiaojun Chang, and Yu Qiao. Html: Hybrid temporal-scale multimodal learning framework for referring video object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. 3\\n\\n[10] Jiachang Hao, Haifeng Sun, Pengfei Ren, Jingyu Wang, Qi Qi, and Jianxin Liao. Can shuffling video benefit temporal bias problem: A novel training framework for temporal grounding. In ECCV, 2022. 3\\n\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 3, 6\\n\\n[12] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask R-CNN. In ICCV, 2017. 5\\n\\n[13] Vladimir Iashin and Esa Rahtu. Multi-modal dense video captioning. In CVPR, 2020. 3\\n\\n[14] Borui Jiang, Ruixuan Luo, Jiayuan Mao, Tete Xiao, and Yunjing Jiang. Acquisition of localization confidence for accurate object detection. In ECCV, 2018. 5\\n\\n[15] Huaizu Jiang, Ishan Misra, Marcus Rohrbach, Erik Learned-Miller, and Xinlei Chen. In defense of grid features for visual question answering. In CVPR, 2020. 3\\n\\n[16] Yang Jin, Yongzhi Li, Zehuan Yuan, and Yadong Mu. Embracing consistency: A one-stage approach for spatio-temporal video grounding. In NeurIPS, 2022. 1, 2, 3, 4, 6, 7\\n\\n[17] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetr-modulated detection for end-to-end multi-modal understanding. In ICCV, 2021. 2, 6\\n\\n[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv, 2014. 6\\n\\n[19] Thao Minh Le, Vuong Le, Svetha Venkatesh, and Truyen Tran. Hierarchical conditional relation networks for video question answering. In CVPR, 2020. 3\\n\\n[20] Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip Torr. Controllable text-to-image generation. NeurIPS, 2019. 3\\n\\n[21] Zihang Lin, Chaolei Tan, Jian-Fang Hu, Zhi Jin, Tiancai Ye, and Wei-Shi Zheng. Collaborative static and dynamic vision-language streams for spatio-temporal video grounding. In CVPR, 2023. 1, 2, 3, 4, 6, 7\\n\\n[22] Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach. arXiv, 2019. 3, 6\\n\\n[23] Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video Swin Transformer. In CVPR, 2022. 3, 6\\n\\n[24] Jonghwan Mun, Minsu Cho, and Bohyung Han. Local-global video-text interactions for temporal grounding. In CVPR, 2020. 3\\n\\n[25] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, 2021. 3\\n\\n[26] Paul Hongsuck Seo, Arsha Nagrani, Anurag Arnab, and Cordelia Schmid. End-to-end generative pretraining for multimodal video captioning. In CVPR, 2022. 3\\n\\n[27] Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large language models with answer heuristics for knowledge-based visual question answering. In CVPR, 2023. 3\\n\\n[28] Yaojie Shen, Xin Gu, Kai Xu, Heng Fan, Longyin Wen, and Libo Zhang. Accurate and fast compressed video captioning. In ICCV, 2023. 3\\n\\n[29] Rui Su, Qian Yu, and Dong Xu. Stvgbert: A visual-linguistic transformer based framework for spatio-temporal video grounding. In ICCV, 2021. 1, 2, 3, 4, 6, 7\\n\\n[30] Chaolei Tan, Zihang Lin, Jian-Fang Hu, Xiang Li, and Wei-Shi Zheng. Augmented 2d-tan: A two-stage approach for human-centric spatio-temporal video grounding. arXiv, 2021. 7\\n\\n[31] Zongheng Tang, Yue Liao, Si Liu, Guanbin Li, Xiaojie Jin, Hongxu Jiang, Qian Yu, and Dong Xu. Human-centric spatio-temporal video grounding with visual transformers. IEEE TCSVT, 32(12):8238\u20138249, 2021. 1, 2, 6, 7\\n\\n[32] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, 2017. 4\"}"}
{"id": "CVPR-2024-416", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2024-416", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Context-Guided Spatio-Temporal Video Grounding\\n\\nXin Gu1,3\u2217, Heng Fan2\u2217, Yan Huang2, Tiejian Luo1, Libo Zhang1,3\u2020\\n\\n1University of Chinese Academy of Sciences, Beijing, China\\n2Department of Computer Science and Engineering, University of North Texas, Denton, USA\\n3Institute of Software, Chinese Academy of Sciences, Beijing, China\\n\\nAbstract\\n\\nSpatio-temporal video grounding (or STVG) task aims at locating a spatio-temporal tube for a specific instance given a text query. Despite advancements, current methods easily suffer the distractors or heavy object appearance variations in videos due to insufficient object information from the text, leading to degradation. Addressing this, we propose a novel framework, context-guided STVG (CG-STVG), which mines discriminative instance context for object in videos and applies it as a supplementary guidance for target localization. The key of CG-STVG lies in two specially designed modules, including instance context generation (ICG), which focuses on discovering visual context information (in both appearance and motion) of the instance, and instance context refinement (ICR), which aims to improve the instance context from ICG by eliminating irrelevant or even harmful information from the context. During grounding, ICG, together with ICR, are deployed at each decoding stage of a Transformer architecture for instance context learning. Particularly, instance context learned from one decoding stage is fed to the next stage, and leveraged as a guidance containing rich and discriminative object feature to enhance the target-awareness in decoding feature, which conversely benefits generating better new instance context to improve localization finally. Compared to existing methods, CG-STVG enjoys object information in text query and guidance from mined instance visual context for more accurate target localization. In experiments on HCSTVG-v1/-v2 and VidSTG, CG-STVG sets new state-of-the-arts in mIoU and mvIoU on all of them, showing efficacy. Code is released at https://github.com/HengLan/CGSTVG.\\n\\n1. Introduction\\n\\nSpatio-temporal video grounding task, or STVG, is recently introduced in [41] and aims to localize the object of interest in an untrimmed video with a spatio-temporal tube (formed by a sequence of bounding boxes) given a free-form textual query. It is a challenging multimodal task which is involved with learning and understanding spatio-temporal visual representations in videos and their connections to the linguistic representation of text. Due to the importance in multimodal video understanding, STVG has drawn increasing attention in recent years (e.g., [16, 21, 29, 31, 35, 40, 41]).\\n\\nCurrent methods usually use the given textual expression as the only cue for retrieving object in videos (see Fig.1 (a)). Despite progress, they may degrade in complex scenes (e.g., in presence of distractors, or severe appearance changes, or both in videos), because text query is insufficient to describe and distinguish the foreground object in these cases. To alleviate this problem, one straightforward solution is to enhance the textual query by including more fine-grained linguistic description. However, there may exist several issues. First, this needs reconstruction of text queries for all objects with longer detailed descriptions, which is laborious as well as expensive. Second, longer text query will result in more computational overheads for training and inference. Third, although the text query can be enhanced with more details, it might still be hard to comprehensively describe certain visual details [43]. Thus, it is natural to ask: Is there any other way, besides enhancing text query, that improves efficiently, effectively, and friendly spatio-temporal video ground?\\n\\nWe answer yes! Instead of enhancing the text query, we propose to exploit visual information of the object to offer a guidance, directly from the vision perspective, for improving STVG. As indicated in the famous saying, \\\"A Picture Is Worth a Thousand Words\\\", visual cues can provide richer information with description granularity about the target object. Nevertheless, for the STVG task, there is no additional external visual information allowed, besides the text query, for target localization. So, where to acquire the desired visual information for improving STVG? From the video itself! In this paper, we introduce a novel framework, context-guided STVG or CG-STVG, that mines internally discriminative visual context information from a video.\"}"}
{"id": "CVPR-2024-416", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Existing Methods\\n\\nOur Context-Guided STVG\\n\\nQuery\\n\\nInstance Context\\n\\nVideo\\n\\nOutput\\n\\n(a) Existing STVG methods (b) Our Context-Guided STVG\\n\\nThe boy in striped clothes raises his finger forward, then steps back against the door.\\n\\nFigure 1. Comparison between (a) existing methods that localize the target using object information from text query and (b) our Context-Guided STVG that enjoys object information from text query and guidance from mined instance context for STVG.\\n\\nBest viewed in color for all figures.\\n\\nvideo for the object, and uses it as a supplementary guidance to improve target localization (see Fig. 1 (b)). The crux of CG-STVG lies in two crucial modules, including instance context generation (or ICG) and instance context refinement (or ICR). ICG focuses on discovering visual information of the object. Specifically, ICG first estimates potential regions for the foreground and then uses them to extract contextual information of both appearance and motion from the visual features. Considering there might exist noises in contextual features that are irrelevant or even harmful for the localization due to inaccurate foreground region estimation, ICR is leveraged to eliminate the useless information. Concretely, it adopts a joint temporal-spatio filtering way based on the temporal and spatio relevance scores to suppress irrelevant features, greatly enhancing the context for localization. In this work, we adopt DETR-similar architecture [4] to implement CG-STVG. During video grounding, ICG, together with the ICR, are deployed at each of the decoding stage for instance context learning. Particularly, the instance context learned from one decoding stage is fed to the next stage, and used as a supplementary guidance containing rich and discriminative object information to enhance target-awareness of decoding feature, which in turn benefits generating better new instance context for improving the localization finally.\\n\\nFig. 2 illustrates the architecture of CG-STVG. To our best knowledge, CG-STVG is the first to mine instance visual context from the videos to guide STVG. Compared with existing approaches, CG-STVG can leverage the object information from both text query, as in current methods, and guidance from its mined instance context for more accurate target localization. To validate its effectiveness, we conduct extensive experiments on three datasets, including HCSTVG-v1/-v2 [31] and VidSTG [42], CG-STVG outperforms existing methods and sets new state-of-the-arts in mIoU and mvIoU on all of these benchmarks, evidencing the efficacy of guidance from instance context for STVG.\\n\\nIn summary, the main contributions are as follows:\\n\\n\u2660 We introduce CG-STVG, a novel and simple approach for improving STVG via mining instance visual context from the video to guide target localization.\\n\\n\u2661 We propose an instance context generation module (ICG) to discover visual context information of the object.\\n\\n\u2663 An instance context refinement (ICR) module is presented to improve the context of object by eliminating irrelevant contextual features, greatly enhancing the performance.\\n\\n\u2662 In extensive experiments on three benchmarks, including HCSTVG-v1/-v2 [31] and VidSTG [42], CG-STVG sets new state-of-the-arts, showing the effectiveness.\\n\\n2. Related Work\\n\\nSpatio-temporal video grounding.\\n\\nSpatio-temporal video grounding [31] aims to generate a spatio-temporal tube for a target given its text query. Early methods (e.g., [31, 40, 41]) mainly follow a two-stage paradigm, which leverages a pre-trained detector to obtain the candidate region proposals and then finds the correct region proposals through the designed network. The main issue of these methods is the heavy reliance on pre-trained detectors, and the performance is restricted by a detector's own limitations. Differently, recent works (e.g., [16, 21, 29, 35]) adopt a one-stage paradigm, directly generating spatio-temporal object proposals without relying on any pre-trained object detectors. The method of [29] is the first of this kind, which leverages the visual-linguistic transformer to generate a spatio-temporal object tube corresponding to the textual sentence. Inspired by the success of the model for text-conditioned object detection [17], the method in [35] introduces a spatio-temporal transformer decoder together with a video-text encoder for STVG. The approach of [16] utilizes a multi-modal template as the global objective to deal with the inconsistency issue for improvement. The work of [21] proposes to explore static appearance and dynamic motion cues collaboratively for target localization, showing promising results. In this paper, we exploit visual context from videos and...\"}"}
{"id": "CVPR-2024-416", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"There is a white dog pushing a white and black dog.\\n\\nFigure 2. Overview of our method, which consists of a multimodal encoder for feature extraction and a context-guided decoder by cascading a set of decoding stages for grounding. In each decoding stage, instance context is mined to guide query learning for better localization.\\n\\nDifferent from existing approaches (e.g., [16, 21, 29, 35]) which explore object information only from the text query for localization, the proposed CG-STVG is able to leverage both textual cue and object guidance from the mined instance context, significantly enhancing the STVG performance and outperforming other methods, particularly in complicated scenarios with similar distractors or large appearance changes.\\n\\nTemporal Grounding. Temporal grounding aims at locating and understanding specific objects or events in a video. Relevant to but different than the STVG, temporal grounding does not require bounding box localization of the target. Numerous approaches (e.g., [2, 3, 6, 10, 24, 33, 39]) have been introduced recently. For example, the algorithm of [2] proposes an effective strategy to avoid the long-form burden by applying a guidance model for grounding time. The approach of [3] leverages cross-modal contrastive learning at coarse-grained (video-sentence) and fine-grained (clip-word) levels for grounding. The work in [6] designs a multimodal framework to learn complementary features from images, flow, and depth for the temporal grounding. Different than these methods, we focus on the more challenging STVG that spatially and temporally localizes the object.\\n\\nVision-Language Modeling. Vision-language modeling is to simultaneously process visual and linguistic information for joint multimodal understanding in various tasks such as visual question answering [1, 5, 15, 19, 27, 38], video captioning [7, 13, 26, 28, 36, 44], text-to-image generation [20, 25], visual-language tracking [8, 45], refer video segmentation [9], etc. Differently, we focus on modeling vision and language for spatio-temporal target localization.\\n\\n3. The Proposed Method\\n\\nOverview. In this work, we present CG-STVG to mine discriminative visual context of object and use it as a guidance to improve localization. Inspired by DETR [4], CG-STVG employs an encoder-decoder architecture, which comprises a multimodal encoder (Sec. 3.1) and a context-guided decoder (Sec. 3.2). As in Fig. 2, the encoder aims at generating multimodal visual-linguistic feature that contains object information from text query, which is sent to the context-guided decoder for target localization guided by instance context learned with ICG (Sec. 3.3) and ICR (Sec. 3.4).\\n\\n3.1. Multimodal Encoder\\n\\nThe multimodal encoder is to generate a robust multimodal feature for the target localization in decoder, and consists of visual and textual feature extraction and fusion as follows.\\n\\nVisual Feature Extraction. To leverage rich cues from the videos, we extract both the appearance and motion features. In specific, we first sample a set of frames \\\\( F = \\\\{ f_i \\\\}_{i=1}^{N_v} \\\\) of length \\\\( N_v \\\\) from the video, and then utilize ResNet-101 [11] for appearance feature extraction and VidSwin [23] for motion feature extraction, respectively. We denote the appearance feature as \\\\( V_a = \\\\{ v_a_i \\\\}_{i=1}^{N_v} \\\\), where \\\\( v_a_i \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C_a} \\\\) with \\\\( H, W, \\\\) and \\\\( C_a \\\\) the height, width, and channel dimensions.\\n\\nSimilarly, we denote the motion feature as \\\\( V_m = \\\\{ v_m_i \\\\}_{i=1}^{N_v} \\\\), where \\\\( v_m_i \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C_m} \\\\) with \\\\( C_m \\\\) the channel dimension.\\n\\nTextual Feature Extraction. We adopt RoBERTa [22] for textual feature extraction. We first tokenize query to obtain a word sequence \\\\( W = \\\\{ w_i \\\\}_{i=1}^{N_t} \\\\) and then apply RoBERTa to produce an embedding sequence \\\\( T = \\\\{ t_i \\\\}_{i=1}^{N_t} \\\\), where \\\\( t_i \\\\in \\\\mathbb{R}^{C_t} \\\\) with \\\\( C_t \\\\) the word embedding dimension.\\n\\nMultimodal Feature Fusion. STVG is a multimodal task. To enhance feature representation, we perform multimodal fusion of the appearance feature \\\\( V_a \\\\), motion feature \\\\( V_m \\\\), and text feature \\\\( T \\\\). Specifically, we first map \\\\( V_a, V_m \\\\) and \\\\( T \\\\) to the same channel number through linear projection and then concatenate corresponding features to obtain the representation.\\n\\n3.2. Context-Guided Decoder\\n\\nPredicted Tube. Grounding applies a guidance model for grounding time. The approach of [3] leverages cross-modal contrastive learning at coarse-grained (video-sentence) and fine-grained (clip-word) levels for grounding. The work in [6] designs a multimodal framework to learn complementary features from images, flow, and depth for the temporal grounding. Different than these methods, we focus on the more challenging STVG that spatially and temporally localizes the object.\\n\\nVision-Language Modeling. Vision-language modeling is to simultaneously process visual and linguistic information for joint multimodal understanding in various tasks such as visual question answering [1, 5, 15, 19, 27, 38], video captioning [7, 13, 26, 28, 36, 44], text-to-image generation [20, 25], visual-language tracking [8, 45], refer video segmentation [9], etc. Differently, we focus on modeling vision and language for spatio-temporal target localization.\"}"}
{"id": "CVPR-2024-416", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to learn query feature $Q_k$ during decoding stage (following DETR [4]). Then, in decoding stage $k$, $Q_k$ features are sent to the $N_k$ features for query feature learning, which is in turn used to generate new instance context. As a guidance with rich visual cue to enhance the query feature, which is in turn used to generate new instance context.\\n\\n3.2. Context-Guided Decoder for Grounding\\n\\nSupplementary material for architecture of SAEncoder and $E_{pos}$ position embedding where $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2$, ..., $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1$, $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i$, $v_a$ appears features $i$, $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal feature in frame $i\", $v_a$ appears features $i\", $v_m$ is the enhanced multimodal feature for decoding, $v_1\", $v_2\", $v_{N_k}$ is learned as follows, $\\\\tilde{x}$ for learning object position information from supplementary material, where $\\\\tilde{x}$ with $\\\\tilde{z}$ is the multimodal"}
