{"id": "CVPR-2024-572", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"which follows from Eq. (4). Images and corresponding views, and incorporate rendering of an intermediate volume to rate probabilistic scene illumination estimation via Eqn. 9. Nonetheless, all these approaches involve rendering of a neural implicit surface during denoising. We also incorporate a neural implicit surface as an intermediate 3D representation in every single denoising step, which is computationally expensive during inference. Consequently, this enables us to sample from a learned distribution that all 3D samples from our predicted distribution are consistent with the conditioning image after rendering and shading. Please refer to the Suppl. Mat. for details.\\n\\nThe above formulation of diffusion via rendering is similar to [57, 59]. These approaches implement diffusion over S by the final reconstruction S \u223c render(x). The networks \u0398 and \u0398(t) are trained by minimising the following DDPM loss in each training iteration:\\n\\n\\\\[ \\\\text{VLB}(\\\\Theta) = \\\\mathbb{E}_{x \\\\sim p_{data}} \\\\left[ \\\\log p_{model}(x) \\\\right] \\\\]\\n\\nFurthermore, we can obtain a shaded image C of this surface. This enables us to sample from a learned reconstruction and obtain the denoised estimate via rendering incorporating a neural implicit surface as an intermediate 3D representation [39] into the reverse process. Our method considers various pixel-aligned observations of a 3D human from the same view and reconstructs intermediate implicit surfaces during denoising. We also incorporate an additional \u201cgenerator\u201d neural network h to estimate the \u201cclean\u201d observation set x continue to be features that validly\\n\\n\\\\[ \\\\hat{x} \\\\sim p_{data}(x|y) \\\\]\\n\\nOnce h is trained, we can ancestrally sample reverse process trajectories over observation sets I0: T \u223c pobserve(x) = VLb(\u0398, x), which is given by adding noise-dependence to \\\\( p_{model}(x|y) \\\\) and \\\\( p_{data}(x) \\\\). Moreover, we can supervise on \\\\( p_{model}(x|y) \\\\) and \\\\( p_{data}(x) \\\\) using generated denoised estimates \\\\( x \\\\sim p_{model}(x|y) \\\\). Noised observations are sampled from \\\\( \\\\bar{x} \\\\sim p_{data}(x) \\\\) and subsequently rasterised. Sphere tracing requires excessive evaluations of \\\\( f(x) \\\\) and subsequently rasterised. Sphere tracing requires K number of tracing steps until a surface is found or the ray is terminated; and subsequently rasterised. Marching Cubes is simply applied once to extract the final mesh. In contrast, our method is memory- and time-intensive, both when an explicit mesh is extracted with Marching Cubes (Eq. (8)).\\n\\nSec. 3.2 involves rendering an implicit surface in every denoising step. This is memory- and time-intensive, both when an explicit mesh is extracted with Marching Cubes and also when denoising step. This is memory- and time-intensive, both when an explicit mesh is extracted with Marching Cubes and also when an explicit mesh is extracted with Marching Cubes is simply applied once to extract the final mesh. In contrast, our method is memory- and time-intensive, both when an explicit mesh is extracted with Marching Cubes (Eq. (8)).\"}"}
{"id": "CVPR-2024-572", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Qualitative comparison against deterministic monocular 3D human reconstruction methods that predict only surface geometry: PIFuHD [50], ICON [63] and ECON [64]. Samples from our method generally exhibit greater geometric detail in uncertain regions, while maintaining a high level of consistency with the input image in shaded renders. Moreover, deterministic methods often fall back towards the mean of the training data distribution when faced with ambiguous and challenging inputs [8, 11, 37]; e.g., predicting trousers from the back instead of a long skirt in row 3. This can be mitigated by learning to predict a distribution over reconstructions instead.\\n\\n3.4. Implementation Details\\nOur networks are trained with a synthetic training dataset, consisting of HDRI-based illuminated renders of real body scans from [45] and our own captured data. We use \u223c5.9 \u00d7 10^5 scans of \u223c1.1 \u00d7 10^3 identities to render \u223c450 K training examples, each consisting of a 512 \u00d7 512 px image (with masked-out background) and an observation set x_0. The back-views in x_0 are created by inverting the z-buffer, and thus rendering the scans back-to-front. [45] provides ground-truth albedo textures, which we use to generate A_F and A_B in x_0. Our scans approximately capture albedo using even ambient lighting. However, this is not perfect and causes our model to sometimes yield shading artefacts in albedo predictions.\\n\\nIn addition to L^VLB, we use 3D losses on d_p, a_p and n_p to improve training stability (see Suppl. Mat. for details).\\n\\nDuring training, we render 32 \u00d7 32 px patches using differentiable ray tracing [68] to form \u02c6x(t)_0^\u0398 for L^VLB. We apply L^generate on the full resolution generation x(t)_0^\u0398. g^\u0398 and h^\u0398 are U-Nets [47] with 13 encoder-decoder layers each and skip connections. Both networks double the filter size in each encoder layer, starting from 64 up to 512 for g^\u0398 and up to 128 for h^\u0398. g^\u0398 outputs a pixel-aligned feature map in R^{512 \u00d7 512 \u00d7 256}. f^\u0398 and s^\u0398 are MLPs, following [5].\\n\\nAt test time, we perform 100 DDIM [56] denoising steps. At each step, we can choose to denoise via render or generate and we ablate different strategies in Sec. 4.1. For faster inference, we use Marching Cubes and rasterisation, instead of sphere tracing, in render. If we render at higher noise (large t), we run Marching Cubes on a 256^3 grid. For small t, we use 512^3. The final denoising step always has to be a render step, since generate does not produce 3D geometry. However, we do not perform a full step of render at t = 1, but only reconstruct S and omit rasterisation of x_0.\\n\\n4. Experiments\\nThis section quantitatively compares DiffHuman with the state-of-the-art photorealistic human reconstruction methods, and visually demonstrates the quality of 3D samples conditioned on internet images. Furthermore, we experimentally ablate a number of crucial design choices. Please see the Suppl. Mat. for additional results and experiments.\\n\\nTest dataset and metrics.\\nWe use the test set of [5] for numerical evaluation, and report both 3D metrics and image-based (pixel-aligned) metrics.\\n\\nThe 3D metrics consist of bi-directional Chamfer distance \u00d7 10^\u22123 (Ch. \u2193), Normal Consistency (NC \u2191), and Volumetric Intersection over Union (IoU \u2191). Iterative Closest Points is used to first align 3D predictions with the ground-truth.\\n\\n3D metrics are sensitive to the assumed camera model. In contrast, image-based metrics partially ignore errors due to an incorrect camera, instead focusing on surface structure. This is better correlated with perceived quality. Image-based metrics are computed by rendering 3D predictions with each model\u2019s assumed camera, and comparing the resulting images against ground-truth 2D renders. Specifically, we report Structural Similarity Index (SSIM \u2191), Learned Perceptual Image Patch Similarity (LPIPS \u2193) [73], and Peak Signal-to-Noise Ratio (PSNR \u2191) for albedo and shaded colour renders. For normal renders, we report the angular error in degrees.\"}"}
{"id": "CVPR-2024-572", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Visualisation of the reverse process. The denoising trajectory shows noisy samples $x_t$ and generated clean predictions $\\\\bar{x}(t)$ at each timestep. Clean predictions are initially very simple, akin to many deterministic approaches, and become detailed over time. The heatmaps show sample diversity, computed as the per-pixel variance of the observations in $\\\\bar{x}(t)$ over 10 samples. Diversity is low at the start of the denoising process ($t = 1000$), but increases gradually as the samples diverge. Back diversity is, intuitively, greater than the front.\\n\\nTable 1. Ablation of hybrid implicit surface diffusion. $N = 5$ samples are obtained using 100 DDIM [56] steps. We periodically render every 1, 10 or 25 steps, or only in the final step. All other denoising steps use generate. While per-step render performs best on 3D metrics, predominantly using generate results in better colour. The best and second best results are marked. (Ang. \u2193) and LPIPS. SSIM and LPIPS evaluate structure rather than pixel-to-pixel errors. The latter can be misleadingly low for over-smooth reconstructions; SSIM and LPIPS, in our experience, better capture the perceived quality.\\n\\nFor our method, we report metrics using the best-of-$N \\\\in \\\\{1, 5, 10\\\\}$ reconstructions, following [7, 29, 53]. Specifically, we obtain $N$ different 3D samples for each test image, and aggregate metrics using the numerical best reconstruction. The Suppl. Mat. additionally reports mean metrics, and discusses the use of best-of-$N$ vs. mean metrics for this task.\\n\\nBaselines. We compare with a large number of recent approaches to monocular 3D human reconstruction. Only PHORHUM [5], S3F [10], and PIFu [49] reconstruct surface color, but the latter does not decompose albedo and shading. ARCH [22] and ARCH++ [17] do not reconstruct true surface details but use normal mapping to enhance the visual fidelity of results. For fairness, we evaluate the estimated normals instead of true surface normals for these methods. We also compare against a version of PHORHUM retrained with our larger dataset, resulting in a strong baseline method.\\n\\n4.1. Ablation Studies\\nSince we retrained PHORHUM [5] on our larger synthetic dataset, we consider it as an ablation of our main design choice: probabilistically modelling the reconstruction process using a diffusion model that predicts distributions over 3D human reconstructions. Even though the retrained PHORHUM model turned out to be a very strong baseline, DiffHuman is able to produce reconstructions with higher visual fidelity and better numerical performance, especially for unseen regions. Better performance for unseen regions can be explained by our probabilistic approach being less prone to averaging effects caused by the inherent aleatoric uncertainty [26] in an ill-posed problem.\\n\\nAdditionally, we ablate our hybrid diffusion strategy: the generator network $h(t)$ and denoising via generate instead of render. In Tab. 1 we compare the use of render in every denoising step, every 10, every 25, and only at the final step (to extract the final mesh with Marching Cubes). We use generate for all other steps. The performances of all variants are comparable, suggesting that $h(t)$ has learned to imitate render sufficiently well. Nonetheless, Ch. is slightly better when using render in every step, while colour metrics are improved with lower render frequency. This is expected: render actually reconstructs 3D geometry whereas generate only synthesises observations. Errors in this approximate synthesis operation may accumulate over the course of the denoising process. On the other hand, generate may also \u201cfix\u201d inconsistent colour extracted from the signed-distance and colour field. Crucially, the use of generate results in an up to $55 \\\\times$ speed up at comparable quality. We use the \u201cfinal step\u201d strategy for all remaining experiments. The Suppl. Mat. shows that 3D samples obtained via per-step and final step rendering are visually similar.\\n\\n4.2. Reconstruction Accuracy\\nTab. 2 and Tab. 3 compare recent methods in terms of image-based metrics and 3D metrics respectively. DiffHuman yields improved performance for image-based back metrics, especially with growing number of samples $N$, while front metrics are stable for all $N$. This is because the back is unobserved \u2013 more samples means a higher chance of finding the correct reconstruction \u2013 while the front is visible and thus variation is lower. We do not split observed and unobserved parts for 3D metrics, but also find a global improvement with higher $N$. Our retrained PHORHUM also performs well, and often slightly better than $N = 1$. This is again expected, as DiffHuman is conducting a much harder task: while PHORHUM outputs a single solution, DiffHuman models the distribution over possible 3D reconstructions.\"}"}
{"id": "CVPR-2024-572", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Quantitative comparison against other monocular 3D human reconstruction methods in terms of pixel-aligned metrics. Since DiffHuman predicts a distribution over 3D reconstructions, we report metrics using the best of $N = 1$, 5 and 10 samples drawn for each test image. We render only in the final denoising step, and use generate otherwise. The best and second best results are marked.\\n\\n| Method          | Ch. | IoU  | NC  |\\n|-----------------|-----|------|-----|\\n| PIFu [49]       |     | 3.21 | 0.61| 0.77|\\n| PIFuHD [50]     |     | 4.54 | 0.62| 0.78|\\n| Geo-PIFu [16]   |     | 4.98 | 0.54| 0.72|\\n| ARCH [22]       |     | 3.58 | 0.57| 0.75|\\n| ARCH++ [17]     |     | 3.48 | 0.59| 0.77|\\n| PaMIR [74]      |     | 2.88 | 0.61| 0.77|\\n| PHORHUM [5]     |     | 1.29 | 0.73| 0.85|\\n| ICON [63]       |     | 2.44 | 0.62| 0.78|\\n| ECON [64]       |     | 3.48 | 0.61| 0.76|\\n| D-IF [67]       |     | 2.97 | 0.58| 0.78|\\n| S3F [10]        |     | 2.35 | 0.63| 0.80|\\n| PHORHUM (retrained) |   | 1.10 | 0.73| 0.87|\\n| DiffHuman: $N = 1$ |   | 1.98 | 0.69| 0.83|\\n| DiffHuman: $N = 5$ |   | 1.16 | 0.72| 0.86|\\n| DiffHuman: $N = 10$ |   | 1.09 | 0.73| 0.86|\\n\\nTable 3. Quantitative comparison against other monocular 3D human reconstruction methods in terms of 3D metrics. Since DiffHuman predicts a distribution over 3D reconstructions, we report metrics using the best of $N = 1$, 5 and 10 samples drawn for each test image. We render only in the final denoising step, and use generate otherwise. The best and second best results are marked. Methods marked with \u2020 use a parametric body model. Furthermore, the 3D ground-truth is but one plausible reconstruction in a monocular setting. Our method is able to yield other 3D solutions that are input-consistent, but differ from the ground-truth resulting in worse metrics. Nevertheless, DiffHuman is competitive even for $N = 1$, and produces qualitatively better reconstructions, as discussed below.\\n\\n4.3. Qualitative Results and Diversity\\nWe show the qualitative performance of DiffHuman in Figs. 3 and 4 side-by-side with state-of-the-art approaches. All competing methods only return one solution, while DiffHuman allows us to sample multiple diverse results. We show two reconstructions per image. Consistent with the numerical results in Tab. 3, DiffHuman can shine the most when reconstructing unobserved back-sides. Despite performing well numerically, our retrained PHORHUM baseline does not produce good reconstructions of uncertain regions, with blurry colours and a lack of geometric detail. Methods that explicitly estimate a back normal map [50, 63, 64] produce over-smooth back reconstructions. In contrast, samples from DiffHuman exhibit fine wrinkles and details both for observed and unobserved regions, as shown by rows 1 and 2 in Fig. 4. PHORHUM and S3F [10] tend to simply clone front colours to the person\u2019s back-side \u2013 a reasonable approach for some but not all garments. E.g., in the 3rd row of Fig. 3, the subject\u2019s shirt is cloned onto the jacket in the back. In contrast, DiffHuman reliably colours unobserved regions without such artefacts. Moreover, we can obtain diverse reconstructions, shown by the different dresses and hairstyles in Fig. 1 and row 2 of Fig. 3. We visualise diversity over the denoising process in Fig. 5. The Suppl. Mat. contains additional qualitative results, as well as unconditional and edge-conditioned samples from DiffHuman. These exhibit greater diversity than image-conditioned samples, as RGB images are strong conditioning signals.\\n\\n5. Conclusion\\nWe presented DiffHuman, a probabilistic method for photorealistic 3D human reconstruction from a single RGB image. We build on top of recent advances in diffusion-based generative modelling and propose a novel pipeline for fast sampling of 3D human shapes. Our model is numerically competitive with the state-of-the-art, while improving the visual fidelity and the level of detail of unseen surfaces. Furthermore, we can sample multiple input-consistent but diverse 3D human reconstructions. Our novel hybrid implicit surface diffusion speeds up 3D sampling at test time compared with diffusion-via-rendering [57, 59], giving a general framework for computationally cheaper diffusion over implicit 3D representations. A limitation of our method is that it currently requires examples with known 3D geometry for training, which constrains the amount of data that can be used. In future work, we plan to overcome this by leveraging data with partial 2D and 2.5D supervision.\"}"}
{"id": "CVPR-2024-572", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, and Gerard Pons-Moll. Detailed human avatars from monocular video. In Proceedings of the International Conference on 3D Vision (3DV), 2018.\\n\\n[2] Thiemo Alldieck, Marcus Magnor, Weipeng Xu, Christian Theobalt, and Gerard Pons-Moll. Video based reconstruction of 3D people models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\n[3] Thiemo Alldieck, Marcus Magnor, Bharat Lal Bhatnagar, Christian Theobalt, and Gerard Pons-Moll. Learning to reconstruct people in clothing from a single rgb camera. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\n[4] Thiemo Alldieck, Gerard Pons-Moll, Christian Theobalt, and Marcus Magnor. Tex2shape: Detailed full human body geometry from a single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2293\u20132303, 2019.\\n\\n[5] Thiemo Alldieck, Mihai Zanfir, and Cristian Sminchisescu. Photorealistic monocular 3D reconstruction of humans wearing clothing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\n[6] Dragomir Anguelov, Praveen Srinivasan, Daphne Koller, Sebastian Thrun, Jim Rodgers, and James Davis. Scape: shape completion and animation of people. SIGGRAPH, 2005.\\n\\n[7] Benjamin Biggs, David Novotny, Sebastien Ehrhardt, Hanbyul Joo, Ben Graham, and Andrea Vedaldi. 3d multi-bodies: Fitting sets of plausible 3d human models to ambiguous image data. Advances in Neural Information Processing Systems, 33:20496\u201320507, 2020.\\n\\n[8] Christopher M. Bishop. Mixture Density Networks. Technical report, Aston University, 1994.\\n\\n[9] Federica Bogo, Angjoo Kanazawa, Christoph Lassner, Peter Gehler, Javier Romero, and Michael J. Black. Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image. In Proceedings of the European Conference on Computer Vision (ECCV), 2016.\\n\\n[10] Enric Corona, Mihai Zanfir, Thiemo Alldieck, Eduard Gabriel Bazavan, Andrei Zanfir, and Cristian Sminchisescu. Structured 3d features for reconstructing relightable and animatable avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[11] Qiongjie Cui, Huaijiang Sun, and Fei Yang. Learning dynamic relationships for 3d human motion prediction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6519\u20136527, 2020.\\n\\n[12] Ziya Erkok, Fangchang Ma, Qi Shan, Matthias Nie\u00dfner, and Angela Dai. Hyperdiffusion: Generating implicit neural fields with weight-space diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023.\\n\\n[13] Jia Gong, Lin Geng Foo, Zhipeng Fan, Qiuhong Ke, Hossein Rahmani, and Jun Liu. Diffpose: Toward more reliable 3d pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13041\u201313051, 2023.\\n\\n[14] Riza Alp Guler and Iasonas Kokkinos. Holopose: Holistic 3D human reconstruction in-the-wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\n[15] John C Hart. Sphere tracing: A geometric method for the antialiased ray tracing of implicit surfaces. The Visual Computer, 12(10):527\u2013545, 1996.\\n\\n[16] Tong He, John Collomosse, Hailin Jin, and Stefano Soatto. Geo-pifu: Geometry and pixel aligned implicit functions for single-view human reconstruction. Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\n[17] Tong He, Yuanlu Xu, Shunsuke Saito, Stefano Soatto, and Tony Tung. Arch++: Animation-ready clothed human reconstruction revisited. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arxiv:2006.11239, 2020.\\n\\n[19] Shoukang Hu, Fangzhou Hong, Liang Pan, Haiyi Mei, Lei Yang, and Ziwei Liu. Sherf: Generalizable human nerf from a single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.\\n\\n[20] Yangyi Huang, Hongwei Yi, Weiyang Liu, Haofan Wang, Boxi Wu, Wenxiao Wang, Binbin Lin, Debing Zhang, and Deng Cai. One-shot implicit animatable avatars with model-based priors. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 8974\u20138985, 2023.\\n\\n[21] Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Jiaxiang Tang, Deng Cai, and Justus Thies. TeCH: Text-guided Reconstruction of Lifelike Clothed Humans. In Proceedings of the International Conference on 3D Vision (3DV), 2024.\\n\\n[22] Zeng Huang, Yuanlu Xu, Christoph Lassner, Hao Li, and Tony Tung. Arch: Animatable reconstruction of clothed humans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\n[23] Mustafa Isik, Martin Runz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes Agapito, and Matthias Nie\u00dfner. Humanrf: High-fidelity neural radiance fields for humans in motion. ACM Transactions on Graphics (TOG), 42(4):1\u201312, 2023.\\n\\n[24] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel, and Anurag Ranjan. Neuman: Neural human radiance field from a single video. In Proceedings of the European Conference on Computer Vision (ECCV), 2022.\\n\\n[25] Angjoo Kanazawa, Michael J Black, David W Jacobs, and Jitendra Malik. End-to-end recovery of human shape and pose. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7122\u20137131, 2018.\\n\\n[26] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? In Advances in Neural Information Processing Systems (NeurIPS), 2017.\\n\\n[27] Muhammed Kocabas, Chun-Hao P. Huang, Otmar Hilliges, and Michael J. Black. PARE: Part attention regressor for 3D human pose estimation. In Proceedings of the International Conference on 3D Vision (3DV), 2018.\"}"}
{"id": "CVPR-2024-572", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[28] Nikos Kolotouros, Georgios Pavlakos, Michael J Black, and Kostas Daniilidis. Learning to reconstruct 3d human pose and shape via model-fitting in the loop. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2252\u20132261, 2019.\\n\\n[29] Nikos Kolotouros, Georgios Pavlakos, Dinesh Jayaraman, and Kostas Daniilidis. Probabilistic modeling for human mesh recovery. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 11605\u201311614, 2021.\\n\\n[30] Verica Lazova, Eldar Insafutdinov, and Gerard Pons-Moll. 360-degree textures of people in clothing from a single image. In Proceedings of the International Conference on 3D Vision (3DV), pages 643\u2013653. IEEE, 2019.\\n\\n[31] Chen Li and Gim Hee Lee. Generating multiple hypotheses for 3d human pose estimation with mixture density network. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9887\u20139895, 2019.\\n\\n[32] Zhe Li, Tao Yu, Zerong Zheng, Kaiwen Guo, and Yebin Liu. Posefusion: Pose-guided selective fusion for single-view human volumetric capture. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14162\u201314172, 2021.\\n\\n[33] Zhe Li, Zerong Zheng, Yuxiao Liu, Boyao Zhou, and Yebin Liu. Posevocab: Learning joint-structured pose embeddings for human avatar modeling. In ACM SIGGRAPH Conference Proceedings, 2023.\\n\\n[34] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J Black. Smpl: A skinned multi-person linear model. ToG, 2015.\\n\\n[35] William E Lorensen and Harvey E Cline. Marching cubes: A high resolution 3D surface construction algorithm. SIGGRAPH, 1987.\\n\\n[36] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2837\u20132845, 2021.\\n\\n[37] Michael Mathieu, Camille Couprie, and Yann LeCun. Deep multi-scale video prediction beyond mean square error. arXiv preprint arXiv:1511.05440, 2015.\\n\\n[38] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3D reconstruction in function space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\n[39] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.\\n\\n[40] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\\n\\n[41] Norman M\u00fcller, Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bul\u00f2, Peter Kontschieder, and Matthias Nie\u00dfner. Diffrf: Rendering-guided 3d radiance field diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4328\u20134338, 2023.\\n\\n[42] Mohamed Omran, Christoph Lassner, Gerard Pons-Moll, Peter Gehler, and Bernt Schiele. Neural body fitting: Unifying deep learning and model based human pose and shape estimation. In Proceedings of the International Conference on 3D Vision (3DV), 2018.\\n\\n[43] Jeong Joon Park, Peter Florence, Julian Straub, Richard Newcombe, and Steven Lovegrove. Deepsdf: Learning continuous signed distance functions for shape representation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 165\u2013174, 2019.\\n\\n[44] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas Daniilidis. Learning to estimate 3d human pose and shape from a single color image. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 459\u2013468, 2018.\\n\\n[45] RenderPeople Dataset. Renderpeople dataset. https://renderpeople.com/.\\n\\n[46] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conf. Comput. Vis. Pattern Recog., pages 10684\u201310695, 2022.\\n\\n[47] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image Computing and Computer-Assisted Intervention, pages 234\u2013241. Springer, 2015.\\n\\n[48] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Adv. Neural Inform. Process. Syst., 35:36479\u201336494, 2022.\\n\\n[49] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2019.\\n\\n[50] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixel-aligned implicit function for high-resolution 3D human digitization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\n[51] Akash Sengupta, Ignas Budvytis, and Roberto Cipolla. Synthetic training for accurate 3d human pose and shape estimation in the wild. In Proceedings of the British Machine Vision Conference (BMVC), 2020.\\n\\n[52] Akash Sengupta, Ignas Budvytis, and Roberto Cipolla. Hierarchical kinematic probability distributions for 3D human shape and pose estimation from images in the wild. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 11219\u201311229, 2021.\\n\\n[53] Akash Sengupta, Ignas Budvytis, and Roberto Cipolla. Humaniflow: Ancestor-conditioned normalising flows on SO(3).\"}"}
{"id": "CVPR-2024-572", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"manifolds for human pose and shape distribution estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4779\u20134789, 2023.\\n\\n[54] Wenkang Shan, Zhenhua Liu, Xinfeng Zhang, Zhao Wang, Kai Han, Shanshe Wang, Siwei Ma, and Wen Gao. Diffusion-based 3d human pose estimation with multi-hypothesis aggregation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.\\n\\n[55] Ruizhi Shao, Zerong Zheng, Hongwen Zhang, Jingxiang Sun, and Yebin Liu. Diffustereo: High quality human reconstruction via diffusion-based stereo using sparse cameras. In Proceedings of the European Conference on Computer Vision (ECCV), 2022.\\n\\n[56] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In Proceedings of the International Conference on Learning Representations (ICLR), 2020.\\n\\n[57] Stanislaw Szymanowicz, Christian Rupprecht, and Andrea Vedaldi. Viewset diffusion: (0-)image-conditioned 3D generative models from 2D data. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.\\n\\n[58] Vince J. K. Tan, Ignas Budvytis, and Roberto Cipolla. Indirect deep structured learning for 3D human shape and pose prediction. In Proceedings of the British Machine Vision Conference (BMVC), 2017.\\n\\n[59] Ayush Tewari, Tianwei Yin, George Cazenavette, Semon Rezchikov, Joshua B. Tenenbaum, Fr\u00e9do Durand, William T. Freeman, and Vincent Sitzmann. Diffusion with forward models: Solving stochastic inverse problems without direct supervision. Advances in Neural Information Processing Systems (NeurIPS), 2023.\\n\\n[60] Tom Wehrbein, Marco Rudolph, Bodo Rosenhahn, and Bastian Wandt. Probabilistic monocular 3D human pose estimation with normalizing flows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 11199\u201311208, 2021.\\n\\n[61] Chung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira Kemelmacher-Shlizerman. HumanNeRF: Free-viewpoint rendering of moving people from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16210\u201316220, 2022.\\n\\n[62] Donglai Xiang, Fabian Prada, Chenglei Wu, and Jessica Hodgins. Monoclothcap: Towards temporally coherent clothing capture from monocular rgb video. In Proceedings of the International Conference on 3D Vision (3DV), pages 322\u2013332. IEEE, 2020.\\n\\n[63] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael J Black. Icon: Implicit clothed humans obtained from normals. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\n[64] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael J. Black. ECON: Explicit clothed humans Optimized via Normal integration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[65] Hongyi Xu, Eduard Gabriel Bazavan, Andrei Zanfir, William T Freeman, Rahul Sukthankar, and Cristian Sminchisescu. Ghum & ghuml: Generative 3D human shape and articulated pose models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\n[66] Hongyi Xu, Thiemo Alldieck, and Cristian Sminchisescu. H-nerf: Neural radiance fields for rendering and temporal reconstruction of humans in motion. Advances in Neural Information Processing Systems (NeurIPS), 2021.\\n\\n[67] Xueting Yang, Yihao Luo, Yuliang Xiu, Wei Wang, Hao Xu, and Zhaoxin Fan. D-if: Uncertainty-aware human digitization via implicit distribution field. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 9122\u20139132, 2023.\\n\\n[68] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\n[69] Andrei Zanfir, Eduard Gabriel Bazavan, Mihai Zanfir, William T Freeman, Rahul Sukthankar, and Cristian Sminchisescu. Neural descent for visual 3D human pose and shape. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14484\u201314493, 2021.\\n\\n[70] Mihai Zanfir, Andrei Zanfir, Eduard Gabriel Bazavan, William T Freeman, Rahul Sukthankar, and Cristian Sminchisescu. Thundr: Transformer-based 3D human reconstruction with markers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\n[71] Xiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3D shape generation. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\n[72] Hongwen Zhang, Jie Cao, Guo Lu, Wanli Ouyang, and Zhenan Sun. Danet: Decompose-and-aggregate network for 3D human shape and pose estimation. In Proceedings of the 27th ACM International Conference on Multimedia, pages 935\u2013944, 2019.\\n\\n[73] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In IEEE Conf. Comput. Vis. Pattern Recog., 2018.\\n\\n[74] Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai. Pamir: Parametric model-conditioned implicit representation for image-based human reconstruction. PAMI, 2021.\\n\\n[75] Hao Zhu, Xinxin Zuo, Sen Wang, Xun Cao, and Ruigang Yang. Detailed human shape estimation from a single image by hierarchical mesh deformation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\"}"}
{"id": "CVPR-2024-572", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We present DiffHuman, a probabilistic method for photorealistic 3D human reconstruction from a single RGB image. Despite the ill-posed nature of this problem, most methods are deterministic and output a single solution, often resulting in a lack of geometric detail and blurriness in unseen or uncertain regions. In contrast, DiffHuman predicts a probability distribution over 3D reconstructions conditioned on an input 2D image, which allows us to sample multiple detailed 3D avatars that are consistent with the image. DiffHuman is implemented as a conditional diffusion model that denoises pixel-aligned 2D observations of an underlying 3D shape representation. During inference, we may sample 3D avatars by iteratively denoising 2D renders of the predicted 3D representation. Furthermore, we introduce a generator neural network that approximates rendering with considerably reduced runtime, resulting in a novel dual-branch diffusion framework. Our experiments show that DiffHuman can produce diverse and detailed reconstructions for the parts of the person that are unseen or uncertain in the input image, while remaining competitive with the state-of-the-art when reconstructing visible surfaces.\"}"}
{"id": "CVPR-2024-572", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Method overview. We use a diffusion probabilistic model [18] to predict a distribution over plausible 3D reconstructions conditioned on a single RGB image. During training, we predict noise-dependent pixel-aligned features $g(t)$ given a noisy observation set $x_t$ consisting of front/back albedo, depth and normal renders, and an RGB image $I$. These features condition an SDF $f(t)$ which is dependent on both $x_t$ and $I$. $f(t)$ and $g(t)$ are neural networks that define an implicit surface $S(t)$ given $x_t$ and $I$. Then, we obtain an estimate of the denoised observation set $x_0$ by rendering $S(t)$. We may additionally produce a shaded image $C(t)$ by applying a pixel-wise noise-dependent shading network $s(t)$. During inference, we can sample trajectories over observation sets $x_0:T \\\\sim p(x_0:T | I)$ by computing and rendering $S(t)$ in each denoising step. Our final 3D samples $S \\\\sim p(S | I)$ are obtained as the final reconstruction $S(1)$.\\n\\nTo mitigate the computational cost of rendering an implicit surface in every step, we train a \u201cgenerator\u201d network $h(t)$ that imitates rendering by directly mapping $g(t)$ to $x_0$. During inference, we denoise using $h(t)$ and only explicitly compute the 3D reconstruction in the last step.\\n\\nTo enable full 3D reconstruction, we take inspiration from recent work [57, 59] and integrate rendering of an intermediate implicit representation into the model\u2019s denoising step. This allows, at test time, to reconstruct 3D meshes from a signed distance and colour field defined by this same intermediate representation.\\n\\nHowever, diffusion-via-rendering is notoriously slow. Thus, we develop a hybrid solution which replaces the expensive implicit surface rendering with a single forward pass through an additional generator network, resulting in a $55 \\\\times$ speed up at test time. Our probabilistic approach enables us to sample multiple input-consistent reconstructions and visualise prediction uncertainty, while significantly improving the quality of unseen surfaces. In summary, our contributions are:\\n\\n- We present a probabilistic diffusion model for photorealistic 3D human reconstruction that predicts a distribution of plausible reconstructions conditioned on an input image.\\n- We propose a novel dual-branch framework that utilises an image generation network, alleviating the need for expensive implicit surface rendering at every denoising step.\\n- We show that our model produces 3D reconstructions with greater levels of geometric detail and colour sharpness in uncertain regions than the current state-of-the-art.\\n\\n2. Related Work\\n\\nWe give an overview of related work on photorealistic and probabilistic 3D human reconstruction.\\n\\nPhotorealistic 3D human reconstruction. Methods for human reconstruction in 3D can be broadly categorised into three classes: mesh-based, implicit, and NeRF-based. Several methods [9, 14, 25, 27, 28, 42, 44, 51, 58, 69, 70, 72] attempt to reconstruct humans in 3D by leveraging parametric body models [6, 34, 65]. However, these approaches only reconstruct the body geometry under clothing and do not predict texture. Others focus on learning deformations on top of parametric models to model hair and clothing [1\u20134, 30, 62, 75]. These methods are generally fast to render, but have the drawback of working with low resolution meshes that cannot capture fine geometric and texture details, and more importantly cannot handle garments with topologies that deviate from the body, such as skirts or dresses.\\n\\nImplicit methods model 3D surface geometry as the level-set of a signed-distance function [43] or occupancy field [38]. They are able to model surfaces of arbitrary topology, which makes them suitable for representing clothed humans. PIFu [49] and PIFuHD [50] predict occupancy and colour fields directly from an input image using pixel-aligned features. Geo-PIFu [16] and PaMIR [74] use a combination of pixel-aligned features and sampled features from a voxel grid to mitigate depth ambiguity issues. PHORHUM [5] replaces the occupancy field with a signed distance function and decouples albedo and shading. ARCH [22], ARCH++ [17], and S3F [10] leverage a human body prior and reconstruct animatable avatars. ICON [63] only predicts surface geometry, using a normal refinement procedure that alternates between normal prediction and body pose refinement. ECON [64] independently reconstructs front and back surfaces, which are then fused using a body model prior. TECH [21] is a concurrent optimisation-based method that uses guidance from a text-to-image diffusion model to reconstruct invisible surfaces. DiffuStereo [55] reconstructs detailed 3D human geometry using a multi-view stereo setup, whereas POSE-Fusion [32] uses a single RGB-D camera. D-IF [67] models uncertainty in occupancy field predictions based on the distance of a point from the surface. In contrast, our method learns a distribution over plausible implicit surfaces for a given image from which we can sample at test time.\\n\\nAnother line of work for photorealistic 3D human reconstruction uses Neural Radiance Fields (NeRFs) [40] as the underlying representation. However, these often require\"}"}
{"id": "CVPR-2024-572", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This section details our method for predicting diffusion-\\n\\nTowards the task of monocular reconstruction, SHERF [19]\\nvolumetric radiance fields, but denoising 3D voxel grids is\\n\\nA forward Markov chain\\n\\ndata distribution\\n\\nare generative models that learn to sample from a target\\n\\nDenoising Diffusion Probabilistic Models.\\n\\n3.1. Background\\n\\nof denoising diffusion models and implicit surfaces.\\n\\nman geometry and appearance. We begin with an overview\\n\\nbased distributions over implicit surfaces representing hu-\\n\\n3. Method\\n\\nrendering using a 2D generator neural network.\\n\\nmethod is similar, but we mitigate the cost of diffusion-via-\\n\\nin the weight space of occupancy networks. However, this\\n\\na method for 3D shape generation that performs diffusion\\n\\ncomputationally expensive. HyperDiffusion [12] presents\\n\\nthis task is the choice of an appropriate 3D representation.\\n\\nthat apply these to 3D generation. A pertinent challenge in\\n\\n2D image synthesis [46, 48] has motivated a few methods\\n\\n3D diffusion models.\\n\\ncorresponding to clothed human geometry and appearance.\\n\\na much more expressive distribution over detailed surfaces\\n\\nor body model parameters. In contrast, our method learns\\n\\ndict distributions over sparse 3D landmarks, joint rotations\\n\\nSO\\n\\n3D rotation group\\n\\nconditioned joint rotations, which respect the structure of the\\n\\n[53] predicts normalising flow distributions over ancestor-\\n\\ntions that exploit the SMPL kinematic tree. HuManiFlow\\n\\net al. [29] utilises conditional normalising flows to this end. Sen-\\n\\nhypotheses conditioned on an input image, while ProHMR\\n\\npredicts a categorical distribution over SMPL [34] parameter\\n\\nlearning a distribution over 3D poses. 3D Multibodies [7]\\n\\nods, such as [13] and [54], employ diffusion models for\\n\\ndensity networks with normalising flows. More recent meth-\\n\\non observed 2D keypoint locations. [60] replaces mixture\\n\\nto estimate a distribution over 3D keypoints conditioned\\n\\nput image. For example, [31] uses mixture density networks\\n\\nestimate distributions over 3D poses conditioned on an in-\\n\\nProbabilistic 3D human reconstruction.\\n\\nSeveral methods\\n\\nThe success of diffusion models for\\n\\n. [52] output hierarchical matrix-Fisher distribu-\\n\\n(x0 via a learned iterative denoising pro-\\n\\n(x0:\\n\\nT\\n\\nprogressively adds\\n\\nconcretely obtained as\\n\\nthe distribution parameters are typically predicted\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the image\\n\\n\u0398\\n\\non the"}
{"id": "CVPR-2024-572", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Qualitative comparison against deterministic monocular 3D human reconstruction methods [5, 10] that predict geometry, surface albedo and shaded colour. PHORHUM [5] (retrained on our dataset) outputs good front predictions, but exhibits over-smooth, flat geometry and blurry colours on the back. S3F [10] yields more detailed geometry, but colours are still often blurry. Moreover, both these methods occasionally paste the front colour predictions onto the back incorrectly (see row 3). Our method outputs multiple diverse samples, with a greater level of geometric detail and colour sharpness in uncertain regions, that are consistent with the input image after shading.\\n\\nTo decouple unshaded albedo colour and illumination-dependent shading, an additional neural network $\\\\Theta$ may be used to estimate a shading coefficient $s_p$ at each surface point $p$. This is obtained using $s_p = s_{\\\\Theta}(n_p, l(I))$, where $n_p = \\\\nabla_p d_p$ is the surface normal at $p$ and $l(I)$ is a scene illumination code estimated from the input image. The latter may be computed using the bottleneck of $g_{\\\\Theta}(I)$, as in [5]. Then, the shaded colour at a point $p$ is given by $c_p = a_p \\\\odot s_p$, where $\\\\odot$ denotes element-wise multiplication.\\n\\nGiven an image-conditioned SDF $f_{\\\\Theta}$, various methods exist to extract and/or render the corresponding surface $S$. An explicit mesh approximation of $S$ is typically generated by running Marching Cubes [35] in a densely sampled 3D bounding box [5, 49, 50]. Standard graphics pipelines can be used to render various properties of the mesh, such as surface albedo, shaded colour, surface normal or depth maps. In addition, $S$ may be directly rendered using sphere tracing [15], without generating an explicit mesh. Sphere tracing can be formulated as a differentiable operation [68], enabling the use of 2D rendering losses during training.\\n\\n3.2. Implicit Surface Diffusion via Rendering\\n\\nOur aim is to estimate the 3D surface geometry and appearance $S$ of a human subject given a single RGB image $I$. This is an ill-posed problem, since multiple 3D reconstructions can plausibly explain a 2D input image, e.g. due to occlusion or depth ambiguity. Thus, we seek to predict a probability distribution over 3D geometry and appearance conditioned on the RGB image, $p_{\\\\Theta}(S|I)$. Our method, DiffHuman, implements $p_{\\\\Theta}(S|I)$ using the framework of DDPMs [18], and enables us to sample multiple plausible 3D reconstructions. We represent $S$ as an implicit surface $S_{\\\\Theta}(I)$ defined by the corresponding neural networks $f_{\\\\Theta}$ and $g_{\\\\Theta}$, as detailed in Eq. (5). If we wish to directly apply a DDPM to learn $p_{\\\\Theta}(S|I)$, we need to define forward and reverse diffusion processes over $S$. This requires a suitable representation of $S$ that can be noised and denoised. In the framework of implicit surfaces, the neural network $f_{\\\\Theta}$ is fixed and acts as a decoder for the pixel-aligned features $g_{\\\\Theta}(I)$. The latter could be an adequate choice for a representation; however, they are unknown before the network is trained. Specifically, we do not have access to ground-truth pixel aligned features $g_{\\\\Theta}(I)$ a priori, and thus cannot add noise to or denoise them directly.\\n\\nInstead, we model a distribution over image-based, pixel-aligned observations of $S$ that cover the true $S$ well. Specifically, we consider three types of observations of the front and back of $S$ with respect to a fixed camera $\\\\pi$: (i) unshaded albedo colour images $A_F$ and $A_B$, (ii) surface normal images $N_F$ and $N_B$, and (iii) depth maps $D_F$ and $D_B$. These are concatenated together to form an observation set $x_0 = \\\\{A_F, A_B, N_F, N_B, D_F, D_B\\\\}$. In practice, this observation set is represented as an array $x_0 \\\\in [-1, 1]^H \\\\times W \\\\times C$. Given a surface $S$, the corresponding $x_0 = render(S, \\\\pi)$ can be obtained via rendering. Since $x_0$ is effectively a multichannel image, a conventional image-based DDPM may be directly applied to learn $p_{\\\\Theta}(x_0|I)$. This would involve training a neural network $\\\\hat{x}_0(t)_{\\\\Theta}(x_t, I)$ to $1442^2$\"}"}
