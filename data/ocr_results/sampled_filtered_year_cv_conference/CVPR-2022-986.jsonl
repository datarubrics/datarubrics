{"id": "CVPR-2022-986", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Referring video object segmentation aims to predict foreground labels for objects referred by natural language expressions in videos. Previous methods either depend on 3D ConvNets or incorporate additional 2D ConvNets as encoders to extract mixed spatial-temporal features. However, these methods suffer from spatial misalignment or false distractors due to delayed and implicit spatial-temporal interaction occurring in the decoding phase. To tackle these limitations, we propose a Language-Bridged Duplex Transformer (LBDT) module which utilizes language as an intermediary bridge to accomplish explicit and adaptive spatial-temporal interaction earlier in the encoding phase. Concretely, cross-modal attention is performed among the temporal encoder, referring words and the spatial encoder to aggregate and transfer language-relevant motion and appearance information. In addition, we also propose a Bilateral Channel Activation (BCA) module in the decoding phase for further denoising and highlighting the spatial-temporal consistent features via channel-wise activation. Extensive experiments show our method achieves new state-of-the-art performances on four popular benchmarks with 6.8% and 6.9% absolute AP gains on A2D Sentences and J-HMDB Sentences respectively, while consuming around 7\u00d7 less computational overhead.\\n\\n1. Introduction\\n\\nReferring video object segmentation (RVOS), which aims to segment the target object referred by a natural language expression in video frames, is an emerging task at the intersection of computer vision and natural language processing. Different from semi-automatic video object segmentation (SVOS) [3, 7, 31, 38], where the target object is referred by the manually annotated mask in the first frame, RVOS is more challenging for identifying the targets due to the variance of free-form expressions. Providing a more natural way for human-computer interaction, RVOS opens up a wide range of applications including language-based video editing [5], language-guided video summarization [29], and video question answering [14, 39], etc.\\n\\nThe keys to solving RVOS are spatial-temporal interaction and cross-modal alignment [12, 41]. Existing methods mainly focus on the latter and design several mechanisms (e.g., cross-modal attention [30, 37], capsule routing [25], and dynamic convolution [6, 36]) to mine the semantic correspondence between vision and language modalities. However, all these methods have limitations on spatial-temporal interaction which can be improved by our LBDT module.\"}"}
{"id": "CVPR-2022-986", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"temporal interaction due to the reliance on 3D ConvNets (e.g., I3D [1]). Concretely, since the poses and locations of moving objects vary in adjacent frames, aggregating spatially misaligned multi-frame features via 3D operators (e.g., 3D convolution and 3D pooling) may confuse the original appearance information in the target frame, leading to inaccurate segmentation results.\\n\\nTo alleviate this phenomenon, CSTM [12] introduces an additional 2D spatial encoder (e.g., ResNet [8]) to extract undisturbed appearance information of the target frame, which is fused with features of the temporal encoder in the later decoding phase. However, the spatial encoder of CSTM lacks motion information since it doesn't explicitly interact with the temporal encoder, making it hard to distinguish among objects with similar appearances while performing different actions. Thus, it tends to generate high responses on false objects and introduces noises inevitably.\\n\\nIn this paper, we argue that an explicit interaction between spatial and temporal features should be established earlier in the encoding phase, forming a more sufficient and effective information exchange process between encoders. Moreover, naive spatial-temporal interaction still tends to introduce noises due to redundant information contained in language-irrelevant distractors. Therefore, we believe the language expression can be exploited as the medium to bridge spatial and temporal interaction, where only language-relevant information can be transferred between encoders for valid context aggregation. To this end, we propose a novel Language-Bridged Duplex Transfer (LBDT) module for effective spatial-temporal interaction in the encoding phase. As illustrated in Figure 1, motion information from the temporal encoder is first aggregated to the referring words by cross-modal attention. Then, the spatial encoder can obtain language-relevant motion clues from the referring words by reversed cross-modal attention, which assists in identifying the referred object by recognizing correct actions (Figure 1 top). Similarly, appearance information from the spatial encoder is also transferred to the temporal encoder through the language bridge, which facilitates the temporal encoder to distinguish language-relevant foreground objects from complex backgrounds (Figure 1 bottom). In addition, we also remove the dependence on 3D ConvNets and approximate motion information with frame difference processed by a 2D ConvNet. By this means, the model complexity is significantly reduced as 2D ConvNet occupies nearly $30 \\\\times$ less computational overhead compared to 3D ConvNet (e.g., 3.6 vs $107.9$ GFLOPs) [2].\\n\\nTo exploit rich multi-scale contexts of hierarchical visual features for finer mask predictions, we also propose a Bilateral Channel Activation (BCA) module to adjust different feature channels in the decoding phase. Concretely, we first upsample and add multi-level features together in temporal and spatial decoders respectively to obtain the decoded features, on which linguistic features are utilized to filter out language-irrelevant motion and appearance information by channel-wise activation. Meanwhile, the global contexts of the decoded features are further extracted to activate the spatial-temporal consistent channels for highlighting features of the referred object.\\n\\nIn a nutshell, our contributions are three-fold: 1) We propose a Language-Bridged Duplex Transfer (LBDT) module to conduct spatial-temporal interaction explicitly between two independent 2D ConvNets in the encoding phase for RVOS, where we use referring words as the medium to transfer language-relevant motion and appearance information. 2) In the decoding phase, we propose a Bilateral Channel Activation (BCA) module to obtain the language-denoised spatial-temporal consistent features for segmenting the referred object. 3) Extensive experiments show that our proposed method outperforms previous methods on four popular RVOS benchmarks, with significant AP gains of 6.8% on A2D Sentences and 6.9% on J-HMDB Sentences, while consuming around $7 \\\\times$ less computational overhead.\\n\\n2. Related Work\\n2.1. Referring Image Segmentation\\nThe goal of referring image segmentation (RIS) is to segment the corresponding object referred by a natural language expression in a static image. The task is first proposed by Hu et al. [10], where visual features extracted by FCN [23] and language features extracted by LSTM [9] are directly concatenated and fused to form the cross-modal features, based on which the segmentation mask of the referred object is predicted. Most recent methods follow this one-stage paradigm and design sophisticated ways of cross-modal interaction involving fine-grained dependency modeling and structural analysis [11, 13, 15, 20, 41]. Moreover, as RIS and referring expression comprehension (i.e., predicting the bounding box for the referred object instead of mask) are highly related, MCN [24] proposes a multi-task collaborative network to achieve joint learning of the two tasks. In this paper, we also follow the one-stage paradigm but focus more on realizing effective and efficient spatial-temporal interaction under the mediation of language.\\n\\n2.2. Referring Video Object Segmentation\\nReferring video object segmentation (RVOS) can be regarded as an extension of RIS, where both motion and appearance information is required to segment the correct object in a dynamic video. With the availability of diverse benchmarks [6,16,34], RVOS has achieved notable progress recently. Most existing methods mainly feed a video clip centered on the target frame to a 3D ConvNet (e.g., I3D [1]), and then obtain mixed spatial-temporal features of the target frame via 3D convolution and pooling. Similar to RIS\"}"}
{"id": "CVPR-2022-986", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Overview of our proposed method. We feed the target frame $I_t$ and its frame difference $I_d$ into the spatial encoder (bottom) and temporal encoder (top) respectively. And in the LBDT module, we stack several LBDT layers to conduct spatial-temporal interaction with referring words as the medium. In the decoding phase, we denoise the language-irrelevant motion and appearance information and activate the spatial-temporal consistent channels for the decoded spatial features $D_S$ and temporal features $D_T$ respectively in the proposed BCA module. Finally, we apply convolutions and sigmoid function on the outputs of BCA module to get the prediction $P$.\\n\\nMethods, they focus on designing different mechanisms for better mining the semantic correspondence between video and language features [6, 25, 30, 36, 37, 41], while neglecting the spatial misalignment issue caused by 3D operators. Accordingly, CSTM [12] leverages an additional 2D spatial encoder to complement undisturbed spatial information of the target frame with temporal features in the decoding phase, but still introduce noises with the absence of motion clues in the encoding phase. In this paper, we propose to establish an explicit spatial-temporal interaction earlier in the encoding phase, where language is exploited as a bridge between spatial and temporal encoders to transfer only language-relevant motion and appearance information while suppressing other irrelevant distractors.\\n\\n2.3. Spatial-Temporal Interaction\\n\\nRecently, the improvement of spatial-temporal interaction has been widely witnessed in the field of unsupervised video object segmentation [4, 19, 21, 33, 40, 42]. For example, Zhou et al. [42] propose a motion-attentive transition to reinforce spatial-temporal object representations with motion information. Ren et al. [33] propose a reciprocal transformation network to discover primary objects by correlating both motion and appearance clues. However, these methods only consider spatial-temporal interaction but ignore the vision-language alignment, while the latter is also crucial for RVOS. In this paper, we exploit language as a medium to bridge spatial-temporal interaction for extracting comprehensive multimodal representations.\\n\\n3. Method\\n\\nThe overall architecture of our model is shown in Figure 2. For the input video clip, we feed its target frame $I_t$ annotated with ground-truth mask and the computed frame difference $I_d$ into two independent ResNet-50 [8] backbones respectively, which are denoted as spatial encoder and temporal encoder in the following. For the input referring expression, we extract language features from the pretrained GloVe embeddings [32] with LSTM [9], which are denoted as $R = \\\\{r_n\\\\}_{n=1}^N$ where $N$ is the length of referring expression. To explicitly transfer the language-relevant motion and appearance information between the two encoders, we propose a LBDT module and insert it into different encoder stages. In the decoding phase, we integrate multi-scale contexts and propose a BCA module to denoise the language-irrelevant information and activate the spatial-temporal consistent features via channel-wise activation.\\n\\n3.1. Visual and Linguistic Feature Extraction\\n\\nGiven a video clip, we feed the target frame $I_t \\\\in \\\\mathbb{R}^3 \\\\times H_0 \\\\times W_0$ and the frame difference $I_d = |I_t - I_{t-\\\\delta}| \\\\in \\\\mathbb{R}^3 \\\\times H_0 \\\\times W_0$ to the spatial encoder and temporal encoder respectively, where $\\\\delta$ is the interval between the target frame and the previous frame for calculating the frame difference. Instead of using I3D [1] as the temporal encoder, we build our spatial and temporal encoders upon the 2D ResNet-50 [8]. We denote features of the five stages as $\\\\{S_s\\\\}_{s=1}^5, S_s \\\\in \\\\mathbb{R}^{C_s \\\\times H_s \\\\times W_s}$ and $\\\\{T_s\\\\}_{s=1}^5, T_s \\\\in \\\\mathbb{R}^{C_s \\\\times H_s \\\\times W_s}$.\"}"}
{"id": "CVPR-2022-986", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Illustration of the temporal\u2192language\u2192spatial information transfer process in our proposed LBDT module. The language-relevant motion information from the temporal features $T$ is aggregated into the language medium $R \\\\rightarrow T \\\\rightarrow S$, from which each pixel in the spatial features $S$ can select the cross-modal motion information according to the semantic relevance. The spatial\u2192language\u2192temporal information transfer process is conducted similarly.\\n\\nFor the referring expression, we embed each word as a 300-dimensional vector [32] and use LSTM [9] as the text encoder to extract the words features $R = \\\\{r_n\\\\}_{n=1}^N \\\\in \\\\mathbb{R}^{N \\\\times C_m}$, where $N$ is the max length of the referring expressions and $C_m$ is the channel number.\\n\\n3.2. Language-Bridged Duplex Transfer\\n\\nOur LBDT module aims to explicitly transfer the language-relevant motion and appearance information between temporal and spatial encoders with language as the bridge, where we stack $L$ layers of LBDT module to conduct this duplex transfer approach (Figure 3). To clearly elaborate the transfer process in the LBDT module, we take the $s$-th stage of the encoders as an example and omit the superscript $s$ for simplicity. We change the channel numbers of both spatial features $S \\\\in \\\\mathbb{R}^{C_s \\\\times H_s \\\\times W_s}$ and temporal features $T \\\\in \\\\mathbb{R}^{C_t \\\\times H_t \\\\times W_t}$ to $C_m$ via linear transformation:\\n\\n$$T_1 = \\\\text{Linear}(T), \\\\quad S_1 = \\\\text{Linear}(S),$$\\n\\n(1)\\n\\nwhere $S_1 \\\\in \\\\mathbb{R}^{C_m \\\\times H_s \\\\times W_s}$ and $T_1 \\\\in \\\\mathbb{R}^{C_m \\\\times H_t \\\\times W_t}$ are the visual inputs to the 1-st LBDT layer.\\n\\nFor the linguistic inputs, we first enhance the words features $R \\\\in \\\\mathbb{R}^{N \\\\times C_m}$ by the self-attention mechanism [35] and denote the enhanced words features as $R' \\\\in \\\\mathbb{R}^{N \\\\times C_m}$, which can be formatted as follows:\\n\\n$$R' = \\\\text{Softmax}(RQ(TK)T/\\\\sqrt{C_m})RV + R,$$\\n\\n(2)\\n\\nwhere $P_R \\\\in \\\\mathbb{R}^{N \\\\times C_m}$ is the sinusoids positional encoding [35].\\n\\nOur LBDT module follows the implementation practice of Transformer [35] and revises it to a cross-modal version. In each LBDT layer, we feed the enhanced language features $R'$ and the outputs from the previous layer to it as inputs:\\n\\n$$S_{l+1}, T_{l+1} = \\\\text{LBDT}(S_l, T_l, R'),$$\\n\\n(3)\\n\\nAs the duplex transfer process happens in a symmetrical way, we take the temporal\u2192language\u2192spatial transfer process in the $l$-th LBDT layer as an example (Figure 3). For motion aggregation, we first add the 2D sinusoids positional encoding $P_T \\\\in \\\\mathbb{R}^{C_m \\\\times H_T \\\\times W_T}$ to the temporal features $T_l$, and then reshape it to $T_l' \\\\in \\\\mathbb{R}^{H_T W_T \\\\times C_m}$. We obtain the attention map $A_{R \\\\times T} \\\\in \\\\mathbb{R}^{N \\\\times H_T W_T}$ by calculating the similarity between each word and each pixel:\\n\\n$$T_l' = \\\\text{Reshape}(T_l + P_T),$$\\n\\n(4)\\n\\n$$R_Q = \\\\text{Linear}(R'), \\\\quad T_K = \\\\text{Linear}(T_l'), \\\\quad A_{R \\\\times T} = \\\\text{Softmax}(RQ(TK)T/\\\\sqrt{C_m}),$$\\n\\n(5)\\n\\nwhere $A_{R \\\\times T} = \\\\{A_i_{R \\\\times T}\\\\}_{i=1}^N$ and $A_i_{R \\\\times T} \\\\in \\\\mathbb{R}^{H_T W_T}$ is the attention map for the $i$-th word. We use $A_{R \\\\times T}$ to adaptively aggregate the language-relevant motion information from the reshaped temporal features $T_l'$, and then add it to the words features $R'$ to obtain the language medium $R' \\\\rightarrow T \\\\rightarrow S \\\\in \\\\mathbb{R}^{N \\\\times C_m}$ with multimodal representations:\\n\\n$$T_V = \\\\text{Linear}(T_l'), \\\\quad R' \\\\rightarrow T \\\\rightarrow S = A_{R \\\\times T}T_V + R'.$$\\n\\n(6)\\n\\nFor motion transfer, we let the spatial features to adaptively select cross-modal motion information from the...\"}"}
{"id": "CVPR-2022-986", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Jo\u02dcao Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In CVPR, 2017.\\n\\n[2] Yunpeng Chen, Yannis Kalantidis, Jianshu Li, Shuicheng Yan, and Jiashi Feng. Multi-fiber networks for video recognition. In ECCV, 2018.\\n\\n[3] Brendan Duke, Abdalla Ahmed, Christian Wolf, Parham Aarabi, and Graham W Taylor. SSTVS: Sparse spatiotemporal transformers for video object segmentation. In CVPR, 2021.\\n\\n[4] Katerina Fragkiadaki, Pablo Arbelaez, Panna Felsen, and Jitendra Malik. Learning to segment moving objects in videos. In CVPR, 2015.\\n\\n[5] Tsu-Jui Fu, Xin Eric Wang, Scott T Grafton, Miguel P Eckstein, and William Yang Wang. Language-based video editing via multi-modal multi-level transformer. arXiv preprint arXiv:2104.01122, 2021.\\n\\n[6] Kirill Gavrilyuk, Amir Ghodrati, Zhenyang Li, and Cees G. M. Snoek. Actor and action video segmentation from a sentence. In CVPR, 2018.\\n\\n[7] Wenbin Ge, Xiankai Lu, and Jianbing Shen. Video object segmentation using global and instance embedding learning. In CVPR, 2021.\\n\\n[8] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\n[9] Sepp Hochreiter and J\u00a8urgen Schmidhuber. Long short-term memory. Neural computation, 1997.\\n\\n[10] Ronghang Hu, Marcus Rohrbach, and Trevor Darrell. Segmentation from natural language expressions. In ECCV, 2016.\\n\\n[11] Shaofei Huang, Tianrui Hui, Si Liu, Guanbin Li, Yunchao Wei, Jizhong Han, Luoqi Liu, and Bo Li. Referring image segmentation via cross-modal progressive comprehension. In CVPR, 2020.\\n\\n[12] Tianrui Hui, Shaofei Huang, Si Liu, Zihan Ding, Guanbin Li, Wenguan Wang, Jizhong Han, and Fei Wang. Collaborative spatial-temporal modeling for language-queried video actor segmentation. In CVPR, 2021.\\n\\n[13] Tianrui Hui, Si Liu, Shaofei Huang, Guanbin Li, Sansi Yu, Faxi Zhang, and Jizhong Han. Linguistic structure guided context modeling for referring image segmentation. In ECCV, 2020.\\n\\n[14] Yunseok Jang, Yale Song, Youngjae Yu, Youngjin Kim, and Gunhee Kim. Tgif-qa: Toward spatio-temporal reasoning in visual question answering. In CVPR, 2017.\\n\\n[15] Ya Jing, Tao Kong, Wei Wang, Liang Wang, Lei Li, and Tie-niu Tan. Locate then segment: A strong pipeline for referring image segmentation. In CVPR, 2021.\\n\\n[16] Anna Khoreva, Anna Rohrbach, and Bernt Schiele. Video object segmentation with language referring expressions. In ACCV, 2018.\\n\\n[17] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\n[18] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. NeurIPS, 2012.\\n\\n[19] Haofeng Li, Guanqi Chen, Guanbin Li, and Yizhou Yu. Motion guided attention for video salient object detection. In ICCV, 2019.\\n\\n[20] Ruiyu Li, Kaican Li, Yi-Chun Kuo, Michelle Shu, Xiaojuan Qi, Xiaoyong Shen, and Jiaya Jia. Referring image segmentation via recurrent refinement networks. In CVPR, 2018.\\n\\n[21] Siyang Li, Bryan Seybold, Alexey Vorobyov, Xuejing Lei, and C-C Jay Kuo. Unsupervised video object segmentation with motion-based bilateral networks. In ECCV, 2018.\\n\\n[22] Si Liu, Tianrui Hui, Shaofei Huang, Yunchao Wei, Bo Li, and Guanbin Li. Cross-modal progressive comprehension for referring segmentation. IEEE TPAMI, 2021.\\n\\n[23] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.\\n\\n[24] Gen Luo, Yiyi Zhou, Xiaoshuai Sun, Liujuan Cao, Chenglin Wu, Cheng Deng, and Rongrong Ji. Multi-task collaborative network for joint referring expression comprehension and segmentation. In CVPR, 2020.\\n\\n[25] Bruce McIntosh, Kevin Duarte, Yogesh S Rawat, and Mubarak Shah. Visual-textual capsule routing for text-based video segmentation. In CVPR, 2020.\\n\\n[26] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 3DV, 2016.\\n\\n[27] Varun K Nagaraja, Vlad I Morariu, and Larry S Davis. Modeling context between objects for referring expression understanding. In ECCV, 2016.\\n\\n[28] Vinod Nair and Geoffrey E Hinton. Rectified linear units improve restricted boltzmann machines. In ICML, 2010.\\n\\n[29] Medhini Narasimhan, Anna Rohrbach, and Trevor Darrell. Clip-it! language-guided video summarization. arXiv preprint arXiv:2107.00650, 2021.\\n\\n[30] Ke Ning, Lingxi Xie, Fei Wu, and Qi Tian. Polar relative positional encoding for video-language segmentation. In IJCAI, 2020.\\n\\n[31] Hyojin Park, Jayeon Yoo, Seohyeong Jeong, Ganesh Venkatesh, and Nojun Kwak. Learning dynamic network using a reuse gate function in semi-supervised video object segmentation. In CVPR, 2021.\\n\\n[32] Jeffrey Pennington, Richard Socher, and Christopher D Manning. Glove: Global vectors for word representation. In EMNLP, 2014.\\n\\n[33] Sucheng Ren, Wenxi Liu, Yongtuo Liu, Haoxin Chen, Guoqiang Han, and Shengfeng He. Reciprocal transformations for unsupervised video object segmentation. In CVPR, 2021.\\n\\n[34] Seonguk Seo, Joon-Young Lee, and Bohyung Han. Urvos: Unified referring video object segmentation network with a large-scale benchmark. In ECCV, 2020.\\n\\n[35] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\"}"}
{"id": "CVPR-2022-986", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hao Wang, Cheng Deng, Fan Ma, and Yi Yang. Context modulated dynamic networks for actor and action video segmentation with language queries. In AAAI, 2020.\\n\\nHao Wang, Cheng Deng, Junchi Yan, and Dacheng Tao. Asymmetric cross-guided attention network for actor and action video segmentation from natural language query. In ICCV, 2019.\\n\\nHaochen Wang, Xiaolong Jiang, Haibing Ren, Yao Hu, and Song Bai. Swiftnet: Real-time video object segmentation. In CVPR, 2021.\\n\\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In CVPR, 2016.\\n\\nPengxiang Yan, Guanbin Li, Yuan Xie, Zhen Li, Chuan Wang, Tianshui Chen, and Liang Lin. Semi-supervised video salient object detection using pseudo-labels. In ICCV, 2019.\\n\\nLinwei Ye, Mrigank Rochan, Zhi Liu, Xiaoqin Zhang, and Yang Wang. Referring segmentation in images and videos with cross-modal self-attention network. IEEE TPAMI, 2021.\\n\\nTianfei Zhou, Shunzhou Wang, Yi Zhou, Yazhou Yao, Jianwu Li, and Ling Shao. Motion-attentive transition for zero-shot video object segmentation. In AAAI, 2020.\"}"}
{"id": "CVPR-2022-986", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"avgpooling\\n\\n\\\\[ \\\\mathbf{D}! \\\\]\\n\\nC \\\\mathbf{F}\\n\\n\\\\( t \\\\)\\n\\n\\\\( s \\\\) \\\\( f \\\\)\\n\\nLinear transform\\n\\nConcat\\n\\nElement-wise product\\n\\nSigmoid\\n\\nConv3x3\\n\\n\\\\( r \\\\)\\n\\nDecoded Temporal\\n\\nDecoded Spatial\\n\\n\\\\[ (7) \\\\]\\n\\n\\\\[ (11) \\\\]\\n\\n\\\\[ (10) \\\\]\\n\\n\\\\[ (13) \\\\]\\n\\n\\\\[ (12) \\\\]\\n\\n([40])\\n\\n\\\\[ d \\\\]\\n\\n\\\\[ s \\\\]\\n\\n\\\\[ \\\\mathbf{t} \\\\]\\n\\n\\\\[ \\\\mathbf{s} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{t\"}\\n\\nLinear\\n\\n\\\\[ \\\\mathbf{C} \\\\]\\n\\n\\\\[ \\\\mathbf{T} \\\\]\\n\\n\\\\[ \\\\mathbf{V} \\\\]\\n\\n\\\\[ \\\\mathbf{P} \\\\]\\n\\n\\\\[ \\\\mathbf{R} \\\\]\\n\\n\\\\[ \\\\mathbf{A} \\\\]\\n\\n\\\\[ \\\\mathbf{M} \\\\]\\n\\n\\\\[ \\\\mathbf{L} \\\\]\\n\\n\\\\[ \\\\mathbf{F} \\\\]\\n\\n\\\\[ \\\\mathbf{S} \\\\]\\n\\n\\\\[ \\\\mathbf{D} \\\\]\\n\\n\\\\[ \\\\mathbf{r} \\\\]\\n\\n\\\\[ \\\\mathbf{s} \\\\]\\n\\n\\\\[ \\\\mathbf{t} \\\\]\\n\\n\\\\[ \\\\mathbf{t\"}\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n\\n\\\\[ \\\\mathbf{f} \\\\]\\n"}
{"id": "CVPR-2022-986", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method                  | Pub.   | J & F       | J & F       |\\n|------------------------|--------|-------------|-------------|\\n| URVOS \u2020 [34]           | ECCV20 | 41.34       | -           |\\n| CMPC-V [22]            | TPAMI21| 45.64       | 49.32       |\\n| Ours (LBDT-4)          | -      | 48.18 (+2.54)| 50.57 (+1.25)| 49.38 (+1.90) |\\n\\nTable 3. Comparison with state-of-the-art methods on the Refer-Youtube-VOS validation set.\\n\\n\u2020 indicates removing multiple iterations of the second stage inference step.\"}"}
{"id": "CVPR-2022-986", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3. Comparison with State-of-the-Art Methods\\n\\nWe compare our method with previous state-of-the-art methods on four popular benchmarks mentioned before. As shown in Table 1, our method outperforms previous works by large margins on the A2D Sentences test set [6]. Compared with CSTM [12], our LBDT-1 model achieves 5.7%, 3.9%, and 3.9% absolute improvements on AP, Overall IoU, and Mean IoU respectively, indicating that using language as the medium to conduct explicit spatial-temporal interaction in the encoding phase is superior to existing methods using 3D ConvNets and implicit interaction in the decoding phase. By stacking LBDT layers, spatial and temporal features can be iteratively optimized, and the best performance is obtained using the LBDT-4 model with 4 layers.\\n\\nWe further verify the generalization ability of our method on J-HMDB Sentences dataset [6]. Following prior works [12, 30, 36], we use the best model trained on A2D Sentences to directly evaluate all the samples in J-HMDB Sentences without finetuning. As shown in Table 2, our method accomplishes significant performance gains over previous state-of-the-arts, indicating that our method can obtain more robust multi-modal representations and generalize the learned knowledge to unseen datasets.\\n\\nWe also conduct experiments on the newly proposed Refer-YouTube-VOS benchmark [34] with richer object categories and denser annotated frames. As shown in Table 3, our method outperforms URVOS [34] and CMPC-V [22] by 2.15% and 1.90% on the J & F metrics respectively, demonstrating that our approach can perform well even in complex scenarios. Moreover, following URVOS [34], we use the best model trained on the Refer-YouTube-VOS and finetune it on the Refer-DA VIS17 dataset [16], where we also achieve the best performance as shown in Table 4.\\n\\n4.4. Ablation Studies\\n\\nWe conduct ablation studies on the A2D Sentences dataset to evaluate the different designs of our model. All experiments are based on our LBDT-1 model.\\n\\nComponent Analysis. We summarize the ablation results of our proposed components in Table 5. The 1-st row is the baseline method, where we first fuse the language features with the visual features and then conduct the duplex spatial-temporal interaction with the cross-attention mechanism [35] directly between spatial and temporal features without the bridging of language, whose computational complexity is $O((HW)^2C)$. As shown in the 2-nd and 3rd rows, both motion transfer and appearance transfer can bring notable improvements, validating the effectiveness of the language-bridged duplex strategy. Moreover, the complexity of our LBDT module is $O(HWNC)$, which is more lightweight as $N \\\\ll HW$. As for the BCA module, we conduct ablation experiments on two key components: language denoiser and spatial-temporal consistency activator. It shows the decoded spatial and temporal features can benefit each other by activating the spatial-temporal consistent channels (the 5-th row), and reducing the language-irrelevant motion and appearance information can improve the vanilla feature fusion in the decoder (the 6-th row).\\n\\nInterval Metrics\\n\\nWe demonstrate the influence of the interval value $\\\\delta$ for calculating the frame difference in Table 6. We found that the best performance is achieved when the interval is 6, which achieves a balance of modeling short and long actions.\\n\\nInserting Stages of LBDT Module.\\n\\nWe evaluate different inserting positions of LBDT module and summarize the results in Table 7. Inserting LBDT into the 4-th and 5-th stages of our spatial and temporal encoders can bring significant improvements, but the performance decreases as we\"}"}
{"id": "CVPR-2022-986", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.5. Computational Overhead\\n\\nWe compare the computational overhead of our method and previous ones in Table 8. Without the dependency on 3D ConvNet, our model outperforms existing methods by significant margins while consuming around 7 \u00d7 less GFLOPs and a much smaller input size. Moreover, we evaluate the FPS of these methods on a single NVIDIA 1080Ti GPU. It shows that our method is more efficient, which increases the possibility of practical applications for RVOS.\\n\\n4.6. Qualitative Analysis\\n\\nFigure 5 presents the predictions of our method and CSTM [12] in the complex scenes. As the spatial encoder in CSTM lacks motion information, it tends to generate masks on false objects (2nd column). By explicitly conducting spatial-temporal interaction in the encoding phase with language as the bridge, our methods can obtain the accurate masks of the referred objects (3rd column). We further visualize the attended regions of the referring words in our LBDT module in Figure 6. Taking the 1st row as an example, the motion-related word \\\"jumping\\\" attends to the region of the jumping girl, and the appearance-related word \\\"white\\\" and \\\"blue\\\" has the highest responses on the two people in the corresponding colors.\\n\\n5. Conclusion and Discussion\\n\\nIn this paper, we reconsider the way of spatial-temporal interaction for RVOS and propose a Language-Bridged DuFrame Transfer (LBDT) module to explicitly conduct spatial-temporal interaction in the encoding phase with language as the medium for transferring the language-relevant information. A Bilateral Channel Activation (BCA) module is also introduced in the decoding phase to denoise and activate the spatial-temporal consistent features via channel activation. Experiments show that our methods outperform previous methods by large margins on four popular benchmarks with much less computational overhead.\\n\\nLimitation.\\n\\nThe limitation of our paper is that static language descriptions may not always match the dynamic objects whose location and pose are various in continuous frames. In the future, we hope to address the mentioned mismatch problem by exploring the temporal coherence between the masks in different video frames, which is complementary to our focus in this paper.\\n\\nAcknowledgments\\n\\nThis research is partly supported by National Natural Science Foundation of China (Grant 62122010, 61876177), Fundamental Research Funds for the Central Universities, and Key R&D Program of Zhejiang Province (2022C01082).\"}"}
