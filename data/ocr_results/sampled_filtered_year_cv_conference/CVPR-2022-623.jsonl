{"id": "CVPR-2022-623", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Exploring Denoised Cross-video Contrast for Weakly-supervised Temporal Action Localization\\n\\nJingjing Li\\n\\nTianyu Yang\\n\\nWei Ji\\n\\nJue Wang\\n\\nLi Cheng\\n\\n1 University of Alberta, Canada\\n\\n2 Tencent AI Lab, Shenzhen, China\\n\\njingjin1, wji3, lcheng5@ualberta.ca, tianyu-yang@outlook.com, arphid@gmail.com\\n\\nAbstract\\n\\nWeakly-supervised temporal action localization aims to localize actions in untrimmed videos with only video-level labels. Most existing methods address this problem with a \\\"localization-by-classification\\\" pipeline that localizes action regions based on snippet-wise classification sequences. Snippet-wise classifications are unfortunately error prone due to the sparsity of video-level labels. Inspired by recent success in unsupervised contrastive representation learning, we propose a novel denoised cross-video contrastive algorithm, aiming to enhance the feature discrimination ability of video snippets for accurate temporal action localization in the weakly-supervised setting. This is enabled by three key designs: 1) an effective pseudo-label denoising module to alleviate the side effects caused by noisy contrastive features, 2) an efficient region-level feature contrast strategy with a region-level memory bank to capture \\\"global\\\" contrast across the entire dataset, and 3) a diverse contrastive learning strategy to enable action-background separation as well as intra-class compactness & inter-class separability. Extensive experiments on THUMOS14 and ActivityNet v1.3 demonstrate the superior performance of our approach.\\n\\n1. Introduction\\n\\nAs a fundamental yet challenging computer vision task, temporal action localization aims to localize the occurrences of prescribed action categories in untrimmed videos. It has received extensive research attention due to its wide applications in surveillance [49], video summarization [32], and highlight detection [55], etc. Many existing methods [4, 7, 28, 43, 56, 66, 68] are based on fully-supervised training, which rely heavily on densely annotated frame labels that are typically laborious and time-consuming to acquire. On the other hand, it is much easier for users to provide video-level tags describing scene context and content. This naturally gives rise to the weakly-supervised temporal action localization, or WS-TAL, where cheap video-level tags are utilized as an alternative supervision signal [38, 41, 50]. Most existing WS-TAL methods [18, 25, 38, 39, 41, 50, 60, 64] follow a \\\"localization-by-classification\\\" pipeline: a snippet-wise classification is carried out over time to generate the Temporal Class Activation Sequence, also called T-CAS or T-CAM [38, 41]; this is followed by selecting snippets with high responses to localize the plausible action regions. Given the sparsity nature of video-level labels, however, snippet-wise classifications are often error-prone, which may severely damage the final localization performance.\\n\\nTo learn a good T-CAS for action localization, it becomes crucial to enhance the feature discrimination ability of various video snippets in snippet-wise classification. Generally, the snippet feature embedding space is expected to satisfy two properties: 1) action snippets should be separable from the background snippets that do not belong to any action classes, i.e., action-background separation; 2) action snippets from a same class should be closer than those from different classes, i.e., intra-class compactness & inter-class separability. This has led to several prior studies [36, 41, 64] exploring deep metric learning [15, 26] or contrastive learning [5] to foster learning discriminative features. As illustrated in Fig. 1 (a) & (b), their focus is mostly on action-background separation, by pushing action features of a specific class to be close and pulling action features away from the background ones, either within individual videos [64], or within a carefully-designed mini-batch [36, 41]. They unfortunately fail to capture the inter-class separability, and ignore the useful \\\"global\\\" contrast across training videos in the entire dataset. Given the lack of frame-level annotations, snippet-wise pseudo-labels [64] or attention-based mechanisms [36, 41] are often used internally as a substitute. As illustrated in Fig. 1 (a), action-background separation is performed based on pseudo-labels over the snippets of each video. In Fig. 1 (b), attention-pooled video-level features from a mini-batch are engaged in the feature contrastive training process. Due to the noisy pseudo-labels or false activations in the learned attention sequence...\"}"}
{"id": "CVPR-2022-623", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Different contrastive learning schemes. (a) Exploiting snippet-wise contrastive learning within single video to separate snippet-wise actions from backgrounds with pseudo-labels (e.g., [64]). (b) Exploiting deep metric learning within mini-batch to separate video-level actions from backgrounds with attention-weighted pooling (e.g., [36, 41]). (c) Our denoised cross-video contrastive algorithm with 1) pseudo-label denoising module, 2) region-level feature contrastive learning across entire dataset, and 3) action-background separation, as well as intra-class compactness & inter-class separability.\\n\\nThese strategies would inevitably give rise to noisy contrastive features. Incorporating these noisy contrastive features may unnecessarily complicate the snippet feature training, and result in suboptimal performance of action localization.\\n\\nThe above observations motivate us to propose a novel Denoised Cross-video Contrastive (DCC) algorithm tailored for weakly-supervised temporal action localization. As illustrated in Fig. 1 (c), it contains three key ideas. First, to account for the pseudo-label noises that are ubiquitous in weakly-supervised TAL, a pseudo-label denoising (PLD) module is devised to reduce the negative impacts of noisy contrastive features. By down-weighting the confidence scores of incorrect pseudo-labels, more accurate contrastive features can be generated. Second, to capture \u201cglobal\u201d contrast across the entire dataset, we propose a region-level feature contrast strategy which, together with a region-level memory bank, allow our learned model to preserve \u201cglobal\u201d informative features across the entire dataset. Third, a diverse contrastive training strategy is proposed to enforce contrasts between actions and backgrounds, and between different action classes. It is capable of promoting action-background separation, inter-class separability as well as intra-class compactness. Note that our DCC algorithm is performed only during training, so it does not incur additional computational cost in testing.\\n\\nHere we summarize our main contributions. (1) A novel denoised cross-video contrastive algorithm is proposed for weakly-supervised TAL. It reduces the influence of noisy contrastive features; it also captures \u201cglobal\u201d contrast across the entire dataset, and simultaneously promotes action-background separation, inter-class separation as well as intra-class compactness. As a result, the discrimination ability of snippet features is significantly enhanced. (2) Extensive experiments on THUMOS14 and ActivityNet v1.3 datasets demonstrate the superior performance of our approach over the state-of-the-art methods. Specifically, we observe a 16.7% improvement over the baseline in terms of average mAP of IoU thresholds from 0.1 to 0.7 on THUMOS14, a significant amount without incurring extra computation cost in inference.\\n\\n2. Related Work\\n\\nTemporal action localization (TAL). Fully-supervised TAL has been extensively studied over the years. They can be roughly classified into two categories, namely two-stage methods and one-stage methods. Two-stage models [4, 7, 10, 21, 22, 24, 43, 45, 56, 62, 68] first generate action proposals, then classify them by temporal boundary regression. One-stage methods [1, 23, 28, 65, 66], on the contrary, directly predict frame-level action labels. The fully-supervised paradigm unfortunately relies on densely annotated labels at the frame-level, which may be prohibitively expensive to acquire.\\n\\nWeakly-supervised TAL is drawing increasing attention, as the video-level labels are comparably at low cost. UntrimmedNet [50] performs per-clip classification, then selects important clips for video label generation via a soft or hard attention. STPN [38] introduces sparsity loss to assist sparse selection of video snippets. To facilitate the detection of complete actions, [33, 46, 69] propose to remove the discriminative action parts or randomly hide...\"}"}
{"id": "CVPR-2022-623", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"video snippets to press the models in exploring complementary action regions. Liu et al. [25] design a multi-branch network and a diversity loss to discover distinct temporal snippets. To improve feature discriminability, deep metric learning algorithms are explored in [33, 37, 41] to encourage action features of the same class to stay similar and to distinguish the activity-related snippets from the backgrounds. CoLA [64] proposes a snippet contrast loss to refine the hard snippet representation in feature space and make them more distinguishable. Meanwhile, explicit background modeling is introduced in [18, 39] with an auxiliary background class. Nguyen et al. [39] generate background attention from the foreground attention in order to pool background frames for training the background class; BaSNet [18] designs an asymmetrical training strategy to suppress background snippet activations. In [19], background frames are modeled as out-of-distribution samples. The action-context separation problem has been considered in DGAM [42] and CMCS [25]. More recently, attempts are made in [30, 40, 58, 63] to generate frame-level pseudo-labels for iterative network training. The pioneer work of [40] proposes an iterative refinement approach by estimating and training with pseudo frame-level ground-truth at each iteration. Zhai et al. [63] generate frame-level pseudo-labels by considering two-stream consensus and designing an attention normalization loss to promote polarizing the attention predictions. Expectation-Maximization [34] is employed in [30] to alternatively train key instance assignment module and foreground classification module. Yang et al. [58] train RGB and optical flow streams using pseudo-labels generated from each other, with an uncertainty-aware learning module to alleviate noises in pseudo-labels. Our approach also tackles pseudo-label noise issue, while it is based on clustering-based confidence voting and is used to generate more accurate contrastive features. Action-foreground consistency is explored in [13] with a hybrid attention to improve boundary accuracy. Lou et al. [29] propose an action unit memory bank to learn action units specific classifiers. The differences of our approach with existing methods are discussed in Sec. 3.4.\\n\\nContrastive Learning. As an important branch of deep metric learning [15], contrastive learning [5, 9, 11, 12, 53] have recently made impressive progress in unsupervised representation learning. These approaches learn representations in a discriminative manner by contrasting positive pairs against negative ones: two augmentations of the same image may be viewed as a positive pair, while two different images are considered as a negative pair. However, false negative samples are inevitably brought in [5] due to the lack of label information [6]. Prannay et al. [16] introduce supervised contrastive loss for image classification, showcasing the benefits of engaging label information in constructing positive and negative pairs. Moreover, several latest studies extend contrastive loss to a variety of downstream tasks, e.g., semantic segmentation [51, 67] and object detection [47, 52, 54], and lead to new state-of-the-art performance.\\n\\n3. Methodology\\n\\nIn this section, we first describe our baseline method in Sec. 3.1, then detail the proposed Denoised Cross-video Contrastive (DCC) algorithm in Sec. 3.2. This is followed by introducing the overall training objective and our inference process in Sec. 3.3. Finally, we discuss the differences with existing works in Sec. 3.4.\\n\\n3.1. Baseline Setup\\n\\nFig. 2 (upper) presents the pipeline of our baseline algorithm. Given a training video sample \\\\( \\\\{v, y\\\\} \\\\), where \\\\( y \\\\in \\\\mathbb{R}^C \\\\) stands for the action label of video \\\\( v \\\\) and \\\\( C \\\\) is the number of action categories, we sample a fixed number of \\\\( T \\\\) non-overlapping snippets, each with 16 frames, for each video and then extract snippet-wise features using the pre-trained feature extractor (e.g., I3D [3]). Next, we apply several layers of temporal convolution layers on the pre-trained features to introduce some temporal involvement between snippets and output the base Temporal Class Activation Sequence (T-CAS) \\\\( A_b \\\\in \\\\mathbb{R}^{T \\\\times (C+1)} \\\\) using a classification head. Here we additionally predict a background class for each snippet to better model background. Following BaSNet [18], a parallel branch termed as foreground selection module is introduced to learn the class-agnostic foreground probability \\\\( Q \\\\in \\\\mathbb{R}^{T \\\\times 1} \\\\), which can be regarded as temporal attention for actions. By multiplying \\\\( Q \\\\) with \\\\( A_b \\\\) temporally, we obtain the T-CAS \\\\( A_f \\\\in \\\\mathbb{R}^{T \\\\times (C+1)} \\\\), which filters out non-action predictions. Following multiple instance learning [8], we apply a temporal top-k pooling followed by a softmax on both \\\\( A_b \\\\) and \\\\( A_f \\\\) to generate the video level prediction \\\\( p_b, p_f \\\\in \\\\mathbb{R}^{C+1} \\\\), respectively. By using snippet-wise binary cross entropy loss, we calculate the MIL loss as,\\n\\n\\\\[\\nL_{MIL} = -\\\\sum_{c=1}^{C+1} \\\\left( y_{bc} \\\\log p_{bc} + (1 - y_{bc}) \\\\log (1 - p_{bc}) \\\\right) + y_{fc} \\\\log p_{fc} + (1 - y_{fc}) \\\\log (1 - p_{fc}) ,\\n\\\\]\\n\\nwhere \\\\( y_{bc}, y_{fc} \\\\) are the corresponding labels of \\\\( p_b, p_f \\\\) by introducing the background labels. Concretely, \\\\( y_{bc} = y_{fc} = y_c \\\\) for \\\\( 1 \\\\leq c \\\\leq C \\\\). \\\\( y_{b C+1} \\\\) is set to 1 because that all training videos contain background snippets and \\\\( y_{f C+1} \\\\) is set to 0 since background snippets are filtered in \\\\( A_f \\\\). To enforce the foreground scores to be more polarized, we also apply\\n\\n1. We use two modalities, i.e., RGB and optical flow, as the input of feature extractor.\"}"}
{"id": "CVPR-2022-623", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"3.2. Denoised Cross-video Contrastive Algorithm\\n\\nAn overview of our DCC is illustrated in Fig. 2 (bottom). Our pipeline includes three components which are Snippet-wise Pseudo-label Generation (SPG), Pseudo-label Denoising (PLD) and Denoised Contrastive Learning (DCL). SPG aims to estimate the snippet-wise label for action and background region extraction in videos and PLD is designed to emphasize confident video regions while suppress unreliable ones to alleviate noisy issue of snippet-wise label. DCL is in charge of constructing denoised contrastive features and generating positive and negative feature pairs for contrastive learning.\\n\\nSnippet-wise Pseudo-label Generation. To determine the required action or background portions under the weakly-supervised setting, we opt to generate pseudo-label $\\\\hat{A}$ by thresholding $A_b$ as in [30]. A softmax function along the category dimension $\\\\epsilon(\\\\cdot)$ is first applied on $A_b$ to map the logits to probability scores. This process is formulated as $\\\\hat{A}_{t,c} = \\\\Phi(\\\\epsilon(\\\\hat{A}_b)_{t,c}; \\\\theta_c)$, (3) where $\\\\theta_c$ is the threshold value for class c and is set to the mean value of $\\\\epsilon(\\\\hat{A}_b)_c$ along temporal dimension; $\\\\Phi$ is the thresholding operation where $\\\\hat{A}_{t,c}$ is 1 if $\\\\epsilon(\\\\hat{A}_b)_{t,c} \\\\geq \\\\theta_c$, and 0 otherwise.\\n\\nPseudo-label Denoising. To address the issue of noisy estimated snippet-wise pseudo-label $\\\\hat{A}$, we design a Pseudo-label Denoising (PLD) module aiming to assign each video snippet a confidence score that estimates the probability of its pseudo-label being a trustworthy true label. Intuitively, video snippets within the same cluster are more likely to maintain the same category label; so the outliers, i.e., video snippets whose pseudo-labels are inconsistent with the majority in each cluster, have a high probability of being misclassified and should be assigned with lower confidence scores.\\n\\nConcretely, we cluster the embedding features using the basic K-means algorithm [20] with the number of cluster center set to $K$. After feature clustering, each snippet will be assigned to a cluster center, which is denoted by $\\\\{E_t\\\\}_{t=1}^T$ where $E_t \\\\in [1, K]$. The confidence score of pseudo-label $\\\\hat{A}_{t,c}$ can be calculated by a confidence voting strategy, $S_{t,c} = \\\\frac{\\\\sum_{k=1}^T 1(E_t = E_k \\\\land \\\\hat{A}_{t,c} = \\\\hat{A}_k,c)}{\\\\sum_{k=1}^T 1(E_t = E_k)}$, (4) where $1(condition)$ is the indicator function, i.e., a function that returns 1 if the condition is satisfied, and 0 otherwise. $\\\\land$ means the and operation. This strategy takes as confidence score the percentage of snippets in cluster center $E_t$ that have the same pseudo-label as the $t$-th snippet.\"}"}
{"id": "CVPR-2022-623", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Denoised Contrastive Learning.\\n\\nThe estimated pseudo-labels and the confidence scores computed in PLD module are then engaged to generate contrastive features. To capture \\\"global\\\" contrast across the entire dataset, we propose a region-level feature contrast strategy which, together with a region-level memory bank, allow our learned model to preserve \\\"global\\\" informative features across the entire dataset.\\n\\nAs illustrated in Fig. 2, following [5], we first append a projection head after the embedded features to get more compact representation, termed as $X \\\\in \\\\mathbb{R}^{T \\\\times d}$, where $d$ is the dimension of projected feature, for contrastive learning. Then we compute the denoised action video feature $F$ by multiplying the projected feature $X$ by pseudo-label $\\\\hat{A}_c$ and its corresponding confidence score in an element-wise manner:\\n\\n$$F_{t,i} = \\\\hat{A}_{t,c} \\\\times S_{t,c} \\\\times X_{t,i}, \\\\quad (5)$$\\n\\nwhere $c$ is the video label of encoded $X$. For background feature $F'$, we alter the pseudo-label $\\\\hat{A}_{t,c}$ with $1 - \\\\hat{A}_{t,c}$ accordingly,\\n\\n$$F'_{t,i} = (1 - \\\\hat{A}_{t,c}) \\\\times S_{t,c} \\\\times X_{t,i}. \\\\quad (6)$$\\n\\nNext, we evenly divide the denoised action video feature $F$ into $M$ action region features along temporal dimension, denoted as $F \\\\Rightarrow \\\\{R_m\\\\}_{M=m=0}^M$, where we set $R_0 = F$ by treating video feature as a relatively large region feature. Finally, we temporally average pooling these region features to obtain their corresponding vectors $\\\\{r_m\\\\}_{M=m=0}^M$ for contrastive learning. Similarly, the background region features $\\\\{r'_m\\\\}_{M=m=0}^M$ are also generated. At the same time, a region-level memory bank is introduced to store the region features of all training videos, which enables our model to learn \\\"global\\\" contrast from the entire dataset.\\n\\nGiven these denoised region-level features, we then apply a diverse contrastive training strategy to both enforce contrasts between actions and backgrounds, and between different action classes. The positive/negative sample pairs are constructed from two sources, i.e., within video and cross video. In detail, given a denoised action region feature $r_m$, its positive sample set $P_m$ includes: 1) action region features from the same video with the same class label; 2) action region features from other videos with the same class label. Its negative sample set $N_m$ consists of: 1) background region features from the same video; 2) background region features from other videos; 3) action region features from other videos but with different class label.\\n\\nEquipped with the InfoNCE [12] loss, we can formulate the contrastive learning as,\\n\\n$$L_{dcc} = -\\\\frac{1}{M} \\\\sum_{m=0}^{M} \\\\log \\\\sum_{r_m \\\\in P_m} \\\\exp \\\\left( \\\\frac{r_m \\\\cdot r_m}{\\\\tau} \\\\right) \\\\sum_{r_{\\\\pm} \\\\in P_m \\\\cup N_m} \\\\exp \\\\left( \\\\frac{r_m \\\\cdot r_{\\\\pm}}{\\\\tau} \\\\right), \\\\quad (7)$$\\n\\nwhere $\\\\tau$ is the temperature parameter. Note that all the embeddings in the loss function are $l_2$-normalized. With $L_{dcc}$, the model is able to capture action-background separation, intra-class compactness, and inter-class separability.\\n\\n3.3. Overall Training Objective and Inference\\n\\nThe overall training objective of our model is\\n\\n$$L_{final} = L_{base} + \\\\beta L_{dcc}, \\\\quad (8)$$\\n\\nwhere $\\\\beta$ is a balancing factor. Since the contrastive features are less informative in the early training stage, we gradually increase $\\\\beta$ from 0.1 to 10000 during network training to focus more on the MIL loss at the early training stage and regularize the feature space learning at later stage. We note that the DCC algorithm is only applied during training and will be removed at inference time. Thus it does not introduce any extra computation at deployment stage.\\n\\nIn the inference stage, we first threshold on the video-level prediction $p_f$ with threshold $\\\\theta_v$ to determine the action categories to be localized. For each selected category, we threshold the T-CAS with $\\\\theta_l$ to obtain candidate action proposals. To enrich the proposal pool, multiple thresholds are applied and Non-Maximum Suppression (NMS) is used to remove duplicated proposals.\\n\\n3.4. Discussion\\n\\nDeep metric learning [15,26] and contrastive learning [5] are also explored in [33, 36, 37, 41, 64] for temporal action localization, and the differences are discussed as follows:\\n\\n1. [64] designs a snippet-wise contrastive loss to refine the hard action or background snippet features. They only consider the action-background separation within single videos. While in our DCC, a diverse contrastive learning strategy is proposed to simultaneously contrast action-background, and different classes. Moreover, our region-level feature contrast enables the model to learn \\\"global\\\" contrast across the entire dataset.\\n\\n2. [36, 41] exploit deep metric learning techniques to enforce action-background separation across video-level features in a mini-batch while our method captures the region-level contrast in the entire dataset and also learns inter-class separability.\\n\\n3. [41, 64] fail to address the noisy contrastive feature issue, whereas in our method, a novel pseudo-label denoising module is designed to generate better contrastive features.\\n\\n4. In [33, 37], contrasts are made between noisy attention-pooled video features and the class-specific central features, while in our DCC, contrasts are made between the denoised region-level features and abundant \\\"global\\\" features in a novel region-level memory bank.\\n\\n4. Experiments\\n\\n4.1. Datasets and Evaluation Metrics\\n\\nEmpirical analysis are carried out on two popular benchmark datasets including THUMOS14 [14] and ActivityNet 19918.\"}"}
{"id": "CVPR-2022-623", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"THUMOS14 includes untrimmed videos with 20 categories. The videos are densely annotated with frame-wise labels, in which their temporal lengths vary greatly. Note that we only use the video-level labels in WS-TAL. By convention [18, 41], we use 200 videos in the validation set for training and 213 videos in the test set for evaluation.\\n\\nActivityNet v1.3 [2] is a superset of ActivityNet v1.2, which consists of 10024 training videos, 4926 validation videos and 5044 testing videos belonging to 200 action categories. Since the annotations for the test set are not released, following the common practice [33,63], we train our model on the training set and evaluate it on the validation set. Following the standard protocol, the mAP (mean Average Precision) under different IoU (Intersection-over-Union) is used together with the benchmark code provided by ActivityNet for evaluation.\\n\\n4.2. Implementation Details\\n\\nThe network is implemented in PyTorch toolbox on a PC with a single Tesla P40 GPU. Optical flow frames are generated using TV-L1 algorithm [61]. Following [18, 64], the number of sampled snippets \\\\( T \\\\) is set to 750 and 50 for THUMOS14 and ActivityNet v1.3, respectively. For fair comparisons, the I3D [3] feature extractor is not fine-tuned. The foreground selection module contains two fully-connected layers with ReLU [35] activation. The projection head [5] is implemented in a similar way with the output dimension \\\\( d \\\\) set to 512. We use Adam optimizer [17] with learning rate 0.0001. \\\\( \\\\tau \\\\) is set to 0.1 following [5]. The cluster center \\\\( K \\\\) and region number \\\\( M \\\\) are both set to 5 for THUMOS14, and 2 for ActivityNet v1.3. The training takes 4 hours for THUMOS14 and 15 hours for ActivityNet v1.3. The GPU memory consumption for THUMOS14 is about 3.5GB. Empirically, to avoid model collapse where all snippets are classified to be background, we adopt a two-stage training mode, i.e., the baseline network is first trained to generate pseudo-labels, with which we then optimize the whole network from scratch. \\\\( \\\\theta_v \\\\) is set to 0.2. \\\\( \\\\theta_l \\\\) spans from 0 to 0.9 with a step size of 0.025.\\n\\n4.3. Ablation Studies\\n\\nIn this section, we provide detailed analysis on the effectiveness of our core model designs, using THUMOS14.\\n\\nEffect of each component.\\n\\nTable 1 presents the comparison results of eliminating different modules of DCC. The DCC model without denoising improves the baseline performance greatly by 12.2% (from 37.7% to 42.3%) in terms of average mAP of IoU thresholds from 0.1 to 0.7, verifying the effectiveness of our method to improve feature discriminability. More detailed analyses and visualizations of cross-video contrastive algorithm are in the following subsections. When equipped with PLD module, our DCC further improves the action localization performance by 4%.\\n\\nThe average confidence scores for both correct and incorrect labels are shown in Table 2. It is observed that, the correct pseudo-label achieves higher average confidence score than these incorrect ones, verifying the effectiveness of our PLD module to distinguish correct pseudo-labels from incorrect ones.\\n\\nIn Table 3, we experiment with different number of cluster \\\\( K \\\\) using K-means. It is observed that under a wide range of \\\\( K \\\\), the results are all better than the model without considering the noisy issue, which further demonstrates the usefulness and robustness of our proposed pseudo-label denoising module.\\n\\nAction-background separation.\\n\\nTo study the effectiveness of our model in capturing action-background separation, we conduct a comparison experiment with results presented in Table 4. It is observed that the action localization performance is significantly improved by an absolute value of 1.1% for average mAP@0.1:0.7, verifying the effectiveness of our model to learn action-background separation. Moreover, in Fig. 3, we visualize the embedded features of a video example from the THUMOS14 test set for baseline and our DCC, respectively. The embeddings are projected to 2-dimensional space using t-SNE tool [48] for visualization. As we can see, our method can better separate actions from backgrounds than the baseline model.\\n\\nIntra-class compactness & inter-class separability.\\n\\nTo investigate the importance of modeling intra-class compactness & inter-class separability, we further introduce the contrast between different action classes to enforce inter-class separability (3rd row in Table 4). These results show that...\"}"}
{"id": "CVPR-2022-623", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Ablation studies of different contrastive learning designs on THUMOS14. \\\"Intra&Inter\\\": intra-class compactness and inter-class separability. \\\"Act-bkg\\\": action-background separation.\\n\\n| Contrastive features | Contrastive strategies | mAP@IoU(%) |\\n|----------------------|------------------------|------------|\\n| Video-level          |                        |            |\\n| Region-level         |                        |            |\\n| Act-bkg              | Intra & Inter          | 0.5        |\\n|                      | Avg                    | 0.5        |\\n\\nTable 5. Analysis on region number M on THUMOS14. We report the average mAP under IoU thresholds from 0.1 to 0.7. We also show the number of features in the memory bank, where \\\\( N_v \\\\) is the total number of training videos.\\n\\n| Region number | Number of features | mAP@Avg |\\n|---------------|-------------------|---------|\\n| 1             | 2                 | 39.6    |\\n| 3             | 8                 | 41.2    |\\n| 5             | 12                | 42.3    |\\n| 10            | 22                | 41.7    |\\n\\nTable 6. Analysis on contrastive features from different videos on THUMOS14.\\n\\n| Ablation Models | Avg mAP(0.1:0.7) |\\n|-----------------|------------------|\\n| Baseline (w/o contrast) | 37.7      |\\n| Intra-video Contrast | 38.5      |\\n| Inter-video Contrast w/o memory (Mini-batch) | 39.7      |\\n| Inter-video Contrast w/ memory (Entire dataset) | 42.3      |\\n\\nThe performance of the average mAP@0.1:0.7 is further improved by an absolute value of 0.8% thanks to modeling intra-class compactness & inter-class separability. We then visualize the learned feature distribution of various classes in Fig. 4, where the left part shows the feature space of the model trained with baseline MIL loss and the right part shows the feature space of our DCC model. It is observed that the snippet embeddings of our model are more compact and well separated, which can produce more discriminative features and improve action localization performance.\\n\\nDifferent level of contrastive features. When using only video-level features (3rd row in Table 4) for contrastive learning, the model gets 39.6% average mAP@0.1:0.7. With our region-level features (4th row), we achieve significant performance gain (39.6% \u2192 42.3% for average mAP), which strongly verifies the effectiveness of our region-level contrastive feature design. In addition, in Table 5, we evaluate the effect of different region number \\\\( M \\\\), which represents the granularity for segmenting videos. The larger the \\\\( M \\\\) value is, the finer the feature granularity is. Experimental results in Table 5 suggest that: (1) within a relatively coarse granularity, larger \\\\( M \\\\) usually leads to higher mAP score since more features are retained in the contrastive training process; (2) too fine-grained granularity (\\\\( M > 5 \\\\)) does not further improve the performance. We conjecture that this is because too fine-grained features are prone to introduce noisy contrastive features and lead to suboptimal contrastive training.\\n\\nGeneralization analysis. We verify the generalization ability of our DCC algorithm by applying it to two recent baseline models, STPN [38] and BaSNet [18]. Experimental results are presented in Table 7. After integrating with DCC, the performances of these two methods are significantly improved by 20.9% and 16.7% on the average mAP@0.1:0.7 scores. This verifies the good generalization ability of our approach on different backbones.\"}"}
{"id": "CVPR-2022-623", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 8. Performance comparison on THUMOS14 testing set. The 'Avg' columns show average mAP under IoU thresholds of 0.1:0.5 and 0.1:0.7. \u2020 indicates access to newly-collected data or additional annotations. \u2217 means using I3D features.\\n\\n| Supervision | Methods          | Publication | 0.1     | 0.2     | 0.3     | 0.4     | 0.5     | 0.6     | 0.7     | (0.1:0.5) | (0.1:0.7) |\\n|-------------|------------------|-------------|---------|---------|---------|---------|---------|---------|---------|-----------|-----------|\\n| Full        | S-CNN [45]       | CVPR'16     | 47.7    | 43.5    | 36.3    | 28.7    | 19.0    | 10.3    | 5.3     | 35.0      | 27.3      |\\n|             | SSN [68]         | ICCV'17     | 66.0    | 59.4    | 51.9    | 41.0    | 29.8    | -       | -       | 49.6      | -         |\\n|             | TAL-Net [4]      | CVPR'18     | 59.8    | 57.1    | 53.2    | 48.5    | 42.8    | 33.8    | 20.8    | 52.3      | 45.1      |\\n|             | BSN [24]         | ECCV'18     | -       | -       | 53.5    | 45.0    | 36.9    | 28.4    | 20.0    | -         | -         |\\n|             | GTAN [28]        | CVPR'19     | 69.1    | 63.7    | 57.8    | 47.2    | 38.8    | -       | -       | 55.3      | -         |\\n| Weak \u2020      | BM \u2217 [39]        | ICCV'19     | 64.2    | 59.5    | 49.1    | 38.4    | 27.5    | 17.3    | 8.6     | 29.8      | 37.8      |\\n|             | 3C-Net \u2217 [37]    | ICCV'19     | 59.1    | 53.5    | 44.2    | 34.1    | 26.6    | 8.1     | 43.5    | -         | -         |\\n|             | STAR \u2217 [57]      | AAAI'19     | 68.8    | 60.0    | 48.7    | 34.7    | 23.0    | -       | -       | 47.0      | -         |\\n|             | SF-Net \u2217 [31]    | ECCV'20     | 71.0    | 63.4    | 53.2    | 40.7    | 29.3    | 18.4    | 9.6     | 51.5      | 40.8      |\\n| Weak        | UntrimNet [50]   | CVPR'17     | 44.4    | 37.7    | 28.2    | 21.1    | 13.7    | -       | -       | 29.0      | -         |\\n|             | STPN \u2217 [38]      | CVPR'18     | 52.0    | 44.7    | 35.5    | 25.8    | 16.9    | 9.9     | 4.3     | 35.0      | 27.0      |\\n|             | W-TALC \u2217 [41]   | ECCV'18     | 55.2    | 49.6    | 40.1    | 31.1    | 22.8    | 7.6     | 39.8    | -         | -         |\\n|             | AutoLoc \u2217 [44]   | ECCV'18     | -       | -       | 35.8    | 29.0    | 21.2    | 13.4    | 5.8     | -         | -         |\\n|             | CleanNet \u2217 [27]  | ICCV'19     | -       | -       | 37.0    | 30.9    | 23.9    | 13.9    | 7.1     | -         | -         |\\n|             | Liu et al. \u2217 [25]| CVPR'19     | 57.4    | 50.8    | 41.2    | 32.1    | 23.1    | 15.0    | 7.0     | 40.9      | 32.4      |\\n|             | BaSNet \u2217 [18]    | AAAI'20     | 58.2    | 52.3    | 44.6    | 36.0    | 27.0    | 18.6    | 10.4    | 43.6      | 35.3      |\\n|             | DGAM \u2217 [42]      | CVPR'20     | 60.0    | 56.0    | 46.6    | 37.5    | 26.8    | 17.6    | 9.0     | 45.6      | 37.0      |\\n|             | EMMIL \u2217 [30]     | ECCV'20     | 59.1    | 52.7    | 45.5    | 36.8    | 30.5    | 22.7    | 16.4    | 45.0      | 37.7      |\\n|             | TSCN \u2217 [63]      | ECCV'20     | 63.4    | 57.6    | 47.8    | 37.7    | 28.7    | 19.4    | 10.2    | 47.0      | 37.8      |\\n|             | A2CL-PT \u2217 [33]   | ECCV'20     | 61.2    | 56.1    | 48.1    | 39.0    | 30.1    | 19.2    | 10.6    | 46.9      | 37.8      |\\n|             | UM \u2217 [19]        | AAAI'21     | 67.5    | 61.2    | 52.3    | 43.4    | 33.7    | 22.9    | 12.1    | 51.6      | 41.9      |\\n|             | CoLA \u2217 [64]      | CVPR'21     | 66.2    | 59.5    | 51.5    | 41.9    | 32.2    | 22.0    | 13.1    | 50.3      | 40.9      |\\n|             | AUMN \u2217 [29]      | CVPR'21     | 66.2    | 61.9    | 54.9    | 44.4    | 33.3    | 20.5    | 9.0     | 52.1      | 41.5      |\\n|             | FAC-Net \u2217 [13]   | ICCV'21     | 67.6    | 62.1    | 52.6    | 44.3    | 33.4    | 22.5    | 12.7    | 52.0      | 42.2      |\\n|             | D2Net \u2217 [36]     | ICCV'21     | 65.7    | 60.2    | 52.3    | 43.4    | 36.0    | -       | -       | 51.5      | -         |\\n|             | DCC (Ours)       |            | 69.0    | 63.8    | 55.9    | 45.9    | 35.7    | 24.3    | 13.7    | 54.1      | 44.0      |\\n\\nTable 9. Performance comparison on ActivityNet v1.3 dataset. The average mAP is computed on thresholds 0.5:0.05:0.95.\\n\\n| Supervision | Methods    | 0.5     | 0.75    | 0.95    | Avg     |\\n|-------------|------------|---------|---------|---------|---------|\\n| Weak        | STPN [38]  | 29.3    | 16.9    | 2.6     | 16.3    |\\n|             | CMCS [25]  | 34.0    | 20.9    | 5.7     | 21.2    |\\n|             | BM [39]    | 36.4    | 19.2    | 2.9     | 19.5    |\\n|             | TSM [59]   | 30.3    | 19.0    | 4.5     | -       |\\n|             | BaSNet [18]| 34.5    | 22.5    | 4.9     | 22.2    |\\n|             | TSCN [63]  | 35.3    | 21.4    | 5.3     | 21.7    |\\n|             | A2CL-PT [33]| 36.8   | 22.0    | 5.2     | 22.5    |\\n|             | AUMN [29]  | 38.3    | 23.5    | 5.2     | 23.5    |\\n|             | DCC (Ours) | 38.8    | 24.2    | 5.7     | 24.3    |\\n\\n4.4. Comparison with State-of-the-Arts\\n\\nWe compare our method with state-of-the-art approaches under different level of supervisions on THUMOS14 test set in Table 8. Note that \\\"Full\\\" means training using frame-wise annotations; \\\"Weak \u2020\\\" indicates using newly collected data [39] or additional annotations [31,37,57]. Our method outperforms recently proposed weakly-supervised methods, e.g. UM [19] and FAC-Net [13], with a large margin. The average mAP of IoU thresholds from 0.1 to 0.7 even reaches 44.0%, bringing the state-of-the-art to a new level. Our method also outperforms weak \u2020 approaches at almost all IoU thresholds, and obtains competitive results even compared with fully-supervised methods, which substantially closes the gap between weakly-supervised TAL and fully-supervised one. Evaluation on ActivityNet v1.3 benchmark is displayed in Table 9. We report the mAP score at various IoU thresholds and report the average mAP for IoU thresholds from 0.5 to 0.95 with a step size of 0.05. As can be seen, our method performs favourably compared with state-of-the-art approaches.\\n\\n5. Conclusion\\n\\nIn this paper, we propose a novel denoised cross-video contrastive algorithm tailored for weakly-supervised temporal action localization. Our key insight is to enhance the feature discrimination ability by three critical ingredients, namely pseudo-label denoising module in addressing noisy contrastive features, region-level feature contrast strategy and region-level memory bank to capture \\\"global\\\" cross-video contrast, and a diverse contrastive learning strategy to regularize the snippet embedding representation. Extensive experiments on two benchmarks demonstrate the superior performance of our approach.\\n\\nAcknowledgement.\\n\\nThis research was supported by CCF-Tencent Open Fund, the University of Alberta Start-up Grant, UAHJIC Grants, and NSERC Discovery Grants (No. RGPIN-2019-04575).\"}"}
{"id": "CVPR-2022-623", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Shyamal Buch, Victor Escorcia, Bernard Ghanem, Li Fei-Fei, and Juan Carlos Niebles. End-to-end, single-stream temporal action detection in untrimmed videos. In Proceedings of the British Machine Vision Conference (BMVC), 2019.\\n\\n[2] Fabian Caba Heilbron, Victor Escorcia, Bernard Ghanem, and Juan Carlos Niebles. Activitynet: A large-scale video benchmark for human activity understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 961\u2013970, 2015.\\n\\n[3] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6299\u20136308, 2017.\\n\\n[4] Yu-Wei Chao, Sudheendra Vijayanarasimhan, Bryan Seybold, David A Ross, Jia Deng, and Rahul Sukthankar. Rethinking the faster r-cnn architecture for temporal action localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1130\u20131139, 2018.\\n\\n[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In Proceedings of the 33rd International Conference on Machine Learning (ICML), pages 1597\u20131607, 2020.\\n\\n[6] Tsai-Shien Chen, Wei-Chih Hung, Hung-Yu Tseng, Shao-Yi Chien, and Ming-Hsuan Yang. Incremental false negative detection for contrastive learning. arXiv preprint arXiv:2106.03719, 2021.\\n\\n[7] Xiyang Dai, Bharat Singh, Guyue Zhang, Larry S Davis, and Yan Qiu Chen. Temporal context network for activity localization in videos. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 5793\u20135802, 2017.\\n\\n[8] Thomas G Dietterich, Richard H Lathrop, and Tom Lozano-P\u00e9rez. Solving the multiple instance problem with axis-parallel rectangles. Artificial Intelligence, 89(1-2):31\u201371, 1997.\\n\\n[9] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. Advances in Neural Information Processing Systems, 27:766\u2013774, 2014.\\n\\n[10] Jiyang Gao, Zhenheng Yang, and Ram Nevatia. Cascaded boundary regression for temporal action detection. arXiv preprint arXiv:1705.01180, 2017.\\n\\n[11] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant mapping. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1735\u20131742, 2006.\\n\\n[12] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 9729\u20139738, 2020.\\n\\n[13] Linjiang Huang, Liang Wang, and Hongsheng Li. Foreground-action consistency network for weakly supervised temporal action localization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 8002\u20138011, 2021.\\n\\n[14] Yu-Gang Jiang, Jingen Liu, A Roshan Zamir, George Toderici, Ivan Laptev, Mubarak Shah, and Rahul Sukthankar. Thumos challenge: Action recognition with a large number of classes, 2014.\\n\\n[15] Mahmut Kaya and Hasan \u015eakir Bilge. Deep metric learning: A survey. Symmetry, 11(9):1066, 2019.\\n\\n[16] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. arXiv preprint arXiv:2004.11362, 2020.\\n\\n[17] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\n[18] Pilhyeon Lee, Youngjung Uh, and Hyeran Byun. Background suppression network for weakly-supervised temporal action localization. Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), 34(7):11320\u201311327, 2020.\\n\\n[19] Pilhyeon Lee, Jinglu Wang, Yan Lu, and Hyeran Byun. Weakly-supervised temporal action localization by uncertainty modeling. arXiv preprint arXiv:2006.07006, 2020.\\n\\n[20] Aristidis Likas, Nikos Vlassis, and Jakob J Verbeek. The global k-means clustering algorithm. Pattern Recognition, 36(2):451\u2013461, 2003.\\n\\n[21] Chuming Lin, Jian Li, Yabiao Wang, Ying Tai, Donghao Luo, Zhipeng Cui, Chengjie Wang, Jilin Li, Feiyue Huang, and Rongrong Ji. Fast learning of temporal action proposal via dense boundary generator. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), pages 11499\u201311506, 2020.\\n\\n[22] Tianwei Lin, Xiao Liu, Xin Li, Errui Ding, and Shilei Wen. Bmn: Boundary-matching network for temporal action proposal generation. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 3889\u20133898, 2019.\\n\\n[23] Tianwei Lin, Xu Zhao, and Zheng Shou. Single shot temporal action detection. In Proceedings of the 25th ACM International Conference on Multimedia (ACM MM), pages 988\u2013996, 2017.\\n\\n[24] Tianwei Lin, Xu Zhao, Haisheng Su, Chongjing Wang, and Ming Yang. Bsn: Boundary sensitive network for temporal action proposal generation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 3\u201319, 2018.\\n\\n[25] Daochang Liu, Tingting Jiang, and Yizhou Wang. Completeness modeling and context separation for weakly supervised temporal action localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1298\u20131307, 2019.\\n\\n[26] Weiyang Liu, Yandong Wen, Zhiding Yu, and Meng Yang. Large-margin softmax loss for convolutional neural networks. In Proceedings of the 33rd International Conference on Machine Learning (ICML), pages 507\u2013516, 2016.\"}"}
{"id": "CVPR-2022-623", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ziyi Liu, Le Wang, Qilin Zhang, Zhanning Gao, Zhenxing Niu, Nanning Zheng, and Gang Hua. Weakly supervised temporal action localization through contrast based evaluation networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3899\u20133908, 2019.\\n\\nFuchen Long, Ting Yao, Zhaofan Qiu, Xinmei Tian, Jiebo Luo, and Tao Mei. Gaussian temporal awareness networks for action localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 344\u2013353, 2019.\\n\\nWang Luo, Tianzhu Zhang, Wenfei Yang, Jingen Liu, Tao Mei, Feng Wu, and Yongdong Zhang. Action unit memory network for weakly supervised temporal action localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 9969\u20139979, 2021.\\n\\nZhekun Luo, Devin Guillory, Baifeng Shi, Wei Ke, Fang Wan, Trevor Darrell, and Huijuan Xu. Weakly-supervised action localization with expectation-maximization multi-instance learning. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.\\n\\nFan Ma, Linchao Zhu, Yi Yang, Shengxin Zha, Gourab Kundu, Matt Feiszli, and Zheng Shou. Sf-net: Single-frame supervision for temporal action localization. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.\\n\\nYu-Fei Ma, Xian-Sheng Hua, Lie Lu, and Hong-Jiang Zhang. A generic framework of user attention model and its application in video summarization. IEEE Transactions on Multimedia, 7(5):907\u2013919, 2005.\\n\\nKyle Min and Jason J. Corso. Adversarial background-aware loss for weakly-supervised temporal activity localization. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.\\n\\nTodd K Moon. The expectation-maximization algorithm. IEEE Signal processing magazine, 13(6):47\u201360, 1996.\\n\\nVinod Nair and Geoffrey E. Hinton. Rectified linear units improve restricted Boltzmann machines. In Proceedings of the 27th International Conference on Machine Learning (ICML), pages 807\u2013814, 2010.\\n\\nSanath Narayan, Hisham Cholakkal, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. D2-net: Weakly-supervised action localization via discriminative embeddings and denoised activations. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 13608\u201313617, 2021.\\n\\nSanath Narayan, Hisham Cholakkal, Fahad Shahbaz Khan, and Ling Shao. 3c-net: Category count and center loss for weakly-supervised action localization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 8679\u20138687, 2019.\\n\\nPhuc Nguyen, Bohyung Han, Ting Liu, and Gautam Prasad. Weakly supervised action localization by sparse temporal pooling network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6752\u20136761, 2018.\\n\\nPhuc Nguyen, Deva Ramanan, and Charless Fowlkes. Weakly-supervised action localization with background modeling. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 5502\u20135511, 2019.\\n\\nAlejandro Pardo, Humam Alwassel, Fabian Caba, Ali Thabet, and Bernard Ghanem. Refineloc: Iterative refinement for weakly-supervised action localization. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 3319\u20133328, 2021.\\n\\nSujoy Paul, Sourya Roy, and Amit K. Roy-Chowdhury. W-talic: Weakly-supervised temporal activity localization and classification. In Proceedings of the European Conference on Computer Vision (ECCV), pages 588\u2013607, 2018.\\n\\nBaifeng Shi, Qi Dai, Yadong Mu, and Jingdong Wang. Weakly-supervised action localization by generative attention modeling. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1009\u20131019, 2020.\\n\\nZheng Shou, Jonathan Chan, Alireza Zareian, Kazuyuki Miyazawa, and Shih-Fu Chang. Cdc: Convolutional-deconvolutional networks for precise temporal action localization in untrimmed videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5734\u20135743, 2017.\\n\\nZheng Shou, Hang Gao, Lei Zhang, Kazuyuki Miyazawa, and Shih-Fu Chang. Autoloc: Weakly-supervised temporal action localization in untrimmed videos. In Proceedings of the European Conference on Computer Vision (ECCV), pages 162\u2013179, 2018.\\n\\nZheng Shou, Dongang Wang, and Shih-Fu Chang. Temporal action localization in untrimmed videos via multi-stage cnns. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1049\u20131058, 2016.\\n\\nKrishna Kumar Singh and Yong Jae Lee. Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 3544\u20133553, 2017.\\n\\nPeng Tang, Chetan Ramaiah, Yan Wang, Ran Xu, and Caiming Xiong. Proposal learning for semi-supervised object detection. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 2291\u20132301, 2021.\\n\\nLaurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.\\n\\nSarvesh Vishwakarma and Anupam Agrawal. A survey on activity recognition and behavior understanding in video surveillance. The Visual Computer, 29(10):983\u20131009, 2013.\\n\\nLimin Wang, Yuanjun Xiong, Dahua Lin, and Luc Van Gool. Untrimmednets for weakly supervised action recognition and detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 6402\u20136411, 2017.\"}"}
{"id": "CVPR-2022-623", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wenguan Wang, Tianfei Zhou, Fisher Yu, Jifeng Dai, Elnder Konukoglu, and Luc Van Gool. Exploring cross-image pixel contrast for semantic segmentation. arXiv preprint arXiv:2101.11939, 2021.\\n\\nFangyun Wei, Yue Gao, Zhirong Wu, Han Hu, and Stephen Lin. Aligning pretraining for detection via object-level contrastive learning. arXiv preprint arXiv:2106.02637, 2021.\\n\\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3733\u20133742, 2018.\\n\\nEnze Xie, Jian Ding, Wenhai Wang, Xiaohang Zhan, Hang Xu, Peize Sun, Zhenguo Li, and Ping Luo. Detco: Unsupervised contrastive learning for object detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 8392\u20138401, 2021.\\n\\nBo Xiong, Yannis Kalantidis, Deepti Ghadiyaram, and Kristen Grauman. Less is more: Learning highlight detection from video duration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1258\u20131267, 2019.\\n\\nHuijuan Xu, Abir Das, and Kate Saenko. R-c3d: Region convolutional 3d network for temporal activity detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 5783\u20135792, 2017.\\n\\nYunlu Xu, Chengwei Zhang, Zhanzhan Cheng, Jianwen Xie, Yi Niu, Shiliang Pu, and Fei Wu. Segregated temporal assembly recurrent networks for weakly supervised multiple action detection. In Proceedings of the AAAI Conference on Artificial Intelligence (AAAI), volume 33, pages 9070\u20139078, 2019.\\n\\nWenfei Yang, Tianzhu Zhang, Xiaoyuan Yu, Tian Qi, Yongdong Zhang, and Feng Wu. Uncertainty guided collaborative training for weakly supervised temporal action detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 53\u201363, 2021.\\n\\nTan Yu, Zhou Ren, Yuncheng Li, Enxu Yan, Ning Xu, and Junsong Yuan. Temporal structure mining for weakly supervised action detection. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 5522\u20135531, 2019.\\n\\nYuan Yuan, Yueming Lyu, Xi Shen, Ivor W Tsang, and Dit-Yan Yeung. Marginalized average attentional network for weakly-supervised learning. arXiv preprint arXiv:1905.08586, 2019.\\n\\nChristopher Zach, Thomas Pock, and Horst Bischof. A duality based approach for realtime tv-l 1 optical flow. In Joint Pattern Recognition Symposium, pages 214\u2013223, 2007.\\n\\nRunhao Zeng, Wenbing Huang, Mingkui Tan, Yu Rong, Peilin Zhao, Junzhou Huang, and Chuang Gan. Graph convolutional networks for temporal action localization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 7094\u20137103, 2019.\\n\\nYuanhao Zhai, Le Wang, Wei Tang, Qilin Zhang, Junsong Yuan, and Gang Hua. Two-stream consensus network for weakly-supervised temporal action localization. In Proceedings of the European Conference on Computer Vision (ECCV), 2020.\\n\\nCan Zhang, Meng Cao, Dongming Yang, Jie Chen, and Yuexian Zou. Cola: Weakly-supervised temporal action localization with snippet contrastive learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nDa Zhang, Xiyang Dai, Xin Wang, and Yuan-Fang Wang. S3d: Single shot multi-span detector via fully 3d convolutional networks. arXiv preprint arXiv:1807.08069, 2018.\\n\\nPeisen Zhao, Lingxi Xie, Chen Ju, Ya Zhang, Yanfeng Wang, and Qi Tian. Bottom-up temporal action localization with mutual regularization. In Proceedings of the European Conference on Computer Vision (ECCV), pages 539\u2013555, 2020.\\n\\nXiangyun Zhao, Raviteja Vemulapalli, Philip Andrew Manfield, Boqing Gong, Bradley Green, Lior Shapira, and Ying Wu. Contrastive learning for label efficient semantic segmentation. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 10623\u201310633, 2021.\\n\\nYue Zhao, Yuanjun Xiong, Limin Wang, Zhirong Wu, Xiaouou Tang, and Dahua Lin. Temporal action detection with structured segment networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 2914\u20132923, 2017.\\n\\nJia-Xing Zhong, Nannan Li, Weijie Kong, Tao Zhang, Thomas H Li, and Ge Li. Step-by-step erasion, one-by-one collection: a weakly supervised temporal action detector. In Proceedings of the 26th ACM International Conference on Multimedia (ACM MM), pages 35\u201344, 2018.\"}"}
