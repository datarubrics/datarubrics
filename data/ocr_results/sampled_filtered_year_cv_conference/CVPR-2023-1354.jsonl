{"id": "CVPR-2023-1354", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nIn this paper, we propose multi-agent automated machine learning (MA2ML) with the aim to effectively handle joint optimization of modules in automated machine learning (AutoML). MA2ML takes each machine learning module, such as data augmentation (AUG), neural architecture search (NAS), or hyper-parameters (HPO), as an agent and the final performance as the reward, to formulate a multi-agent reinforcement learning problem. MA2ML explicitly assigns credit to each agent according to its marginal contribution to enhance cooperation among modules, and incorporates off-policy learning to improve search efficiency. Theoretically, MA2ML guarantees monotonic improvement of joint optimization. Extensive experiments show that MA2ML yields the state-of-the-art top-1 accuracy on ImageNet under constraints of computational cost, e.g., 79.7% / 80.5% with FLOPs fewer than 600M / 800M. Extensive ablation studies verify the benefits of credit assignment and off-policy learning of MA2ML.\\n\\n1. Introduction\\nAutomated machine learning (AutoML) aims to find high-performance machine learning (ML) pipelines without human effort involvement. The main challenge of AutoML lies in finding optimal solutions in huge search spaces. In recent years, reinforcement learning (RL) has been validated to be effective to optimize individual AutoML modules, such as data augmentation (AUG) [4], neural architecture search (NAS) [25, 53, 54], and hyper-parameter optimization (HPO) [38]. However, when facing the huge search space (Figure 1 left) and the joint optimization of these modules, the efficiency and performance challenges remain.\\n\\nThrough experiments, we observed that among AutoML modules there exists a cooperative relationship that facilities the joint optimization of modules. For example, a small network (ResNet-34) with specified data augmentation and optimized hyper-parameters significantly outperforms a large one (ResNet-50) with default training settings (76.8% vs. 76.1%). In other words, good AUG and HPO alleviate the need for NAS to some extent. Accordingly, we propose multi-agent automated machine learning (MA2ML), which explores the cooperative relationship towards joint optimization of ML pipelines. In MA2ML, ML modules are defined as RL agents (Figure 1 mid), which take actions to jointly maximize the reward, so that the training efficiency and test accuracy are significantly improved. Especially, we introduce credit assignment to differentiate the contribution of each module, such that all modules can be simultaneously updated. To handle both continuous (e.g., learning rate) and discrete (e.g., architecture) action spaces, MA2ML employs a multi-agent actor-critic method, where a centralized Q-function is learned to evaluate the joint action. Besides, to further improve search efficiency, MA2ML adopts off-policy learning to exploit historical samples for policy updates.\\n\\nMA2ML is justified theoretically and experimentally. Theoretically, we prove that MA2ML guarantees monotonic policy improvement (Figure 1 right), i.e., the performance of the searched pipeline monotonically improves in expectation. This enables MA2ML to fit the joint optimization problem and be adaptive to all modules in the ML pipeline, potentially achieving full automation. Experimentally, we take the combination of individual RL-based modules to form MA2ML-Lite, and compare their performance with AutoML and other RL-based AutoML methods.\"}"}
{"id": "CVPR-2023-1354", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To better balance performance and computational cost, we add constraints of FLOPs in the experiment on ImageNet. Experiments show that MA2ML substantially outperforms MA2ML-Lite w.r.t. both accuracy and sample efficiency, and MA2ML achieves remarkable accuracy compared with recent methods.\\n\\nOur contributions are summarized as follows:\\n\\n- We propose MA2ML, which utilizes credit assignment to differentiate the contributions of ML modules, providing a systematic solution for the joint optimization of AutoML modules.\\n- We prove the monotonic improvement of module policies, which enables MA2ML to fit the joint optimization problem and be adaptive to various modules in the ML pipeline.\\n- MA2ML yields the state-of-the-art performance under constraints of computational cost, e.g., 79.7% / 80.5% on ImageNet, with FLOPs fewer than 600M/800M, validating the superiority of the joint optimization of MA2ML.\\n\\n2. Related work\\n\\nSingle-module Optimization.\\n\\nAutoML has been applied to different modules in the ML pipeline, such as AUG, NAS, and HPO. For AUG, AutoAugment [4] is an RL-based method to search augmentation policies. Fast AA [21] speeds up the search with less computing cost through Bayesian optimization. Faster AA [10] is a differentiable method to optimize augmentation policy, which reduces computing cost further. PBA [12] also reduces search cost by population-based training. Adversarial AA [49] uses the idea of GAN [8] to generate the hard augmentation policy for the model to improve robustness.\\n\\nFor NAS, there are three main types of methods, evolutionary methods [29, 30, 45], RL-based methods [25, 27, 35, 53, 54], and differentiable gradient-based methods [3, 11, 22, 44]. Evolutionary methods optimize the search through selection, recombination, and mutation in the neural architecture population. RL-based methods regard the neural architecture as a black box and the final accuracy as the reward. They typically use RL algorithms to solve the optimization problem. Differentiable gradient-based methods design a continuous representation for neural architecture space and make NAS a differentiable problem, which greatly improves search efficiency, compared with the other two types.\\n\\nFor HPO, black-box methods [1, 15] have been utilized for a long time. Meanwhile, some methods like [16, 41] use multi-fidelity ways to accelerate optimization through the evaluation on proxy tasks. Recently, gradient-based methods [23, 24, 26, 32] optimize hyper-parameters by calculating gradient respect to them, which reduces computing cost substantially.\\n\\nThe above methods for single module optimization leave other modules of the ML pipeline fixed during search, either by expert knowledge or empirical setting, which may not be optimal when combining them. Joint optimization of multiple modules is a more plausible way to advance the ML pipeline.\\n\\nJoint Optimization.\\n\\nRecent studies [5, 6, 19, 46] search for NAS and HPO jointly through RL or training an accuracy predictor, or [17, 40] consider the joint optimization of AUG and NAS, and obtain convincing results by bi-level gradient-based optimization. DHA [52] explores the joint optimization for AUG, NAS, and HPO through one-level optimization, which is achieved in a differentiable manner by optimizing a compressed lower dimensional feature space for NAS. However, the alternate optimization of the gradient-based method may get stuck at the non-stationary point with limited-order gradient descent.\\n\\nUnlike existing joint optimization methods for AutoML, our MA2ML aims to update all modules, as well as guarantee the convergence of joint optimization.\\n\\n3. The Proposed Method\\n\\nMA2ML is a general framework for joint optimization in AutoML and can be applied to any arbitrary combination of different modules. For ease of presentation, in the following, we use the joint optimization of AUG, NAS, and HPO for image classification tasks as an example to elaborate MA2ML.\\n\\n3.1. Action Space\\n\\nBefore formulating the joint optimization as an MARL problem, we first define an action space for AUG, NAS, and HPO. In AUG, we adopt the setting in AutoAugment [4]. In NAS, we utilize the search space in NASNet [54] and FBNetV3 [5] on different datasets to show that MA2ML is naturally agnostic to search spaces. In HPO, we design a search space for hyper-parameters, including learning rate, weight decay, etc.\\n\\nFor each module, we implement a controller to sample different choices from the action space sequentially. Note that MA2ML can be easily extended to support more modules in the ML pipeline, such as loss function search and mix-precision configuration search, by simply implementing the module by a controller and adding it to the framework.\"}"}
{"id": "CVPR-2023-1354", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Acc\\nsample a mini-batch of Centralized Critic \\nstore \\nTD Error\\nupdate \\nAgent 1\\ncredit assignment\\nAgent 2\\nAgent 3\\ndivergence\\nData\\ntrain & evaluate\\nArchitecture\\nAugmentation\\nupdate update update \\nAutoML Training\\nMA2ML Training\\nupdate update update \\ndivergence\\nHyperparameters\\nReplay\\nBuffer\\nFigure 2. The framework of joint optimization with MA2ML. For AutoML training (lower panel), the ML pipeline is formed by the actions sampled from the policies of agents, then it is deployed for training on the dataset and to obtain the accuracy (reward). After that, the tuple $\\\\langle S, A, R \\\\rangle$ is stored in the replay buffer. For MA2ML training (upper panel), a mini-batch of $\\\\langle S, A, R \\\\rangle$ are sampled from the replay buffer to update the critic, policies, and target policies.\\n\\nlected action $A_i$ in the action space sampled from its policy $\\\\pi_i$, which is denoted as $A_i \\\\sim \\\\pi_{\\\\theta_i}(\\\\cdot|S_i)$ where the policy is parameterized by $\\\\theta_i$. After all actions are determined, we train the network with the determined pipeline setting on the image classification task for epochs and evaluate top-1 accuracy on the validation set. We take the top-1 accuracy as the reward $R$ for all the agents. The objective is to maximize the expected reward $R$, represented by $J(\\\\Theta)$:\\n\\n$$J(\\\\Theta) = \\\\mathbb{E}_{\\\\pi_{\\\\Theta}(A|S)}[R],$$\\n\\n(1)\\n\\nwhere $\\\\pi_{\\\\Theta}(A|S) \\\\equiv Q_{n i=1}^\\\\pi \\\\theta_i(A_i|S_i)$ is the joint policy, and $\\\\Theta, S, A$ respectively denote the gather of $\\\\theta_i, S_i$ and action $A_i$ of all agents (we may drop $\\\\Theta$ or $\\\\theta_i$ for brevity if there is no confusion). Consequently, we transform the joint optimization of the ML pipeline as an MARL problem, then we can rely on MARL methods to solve the problem.\\n\\n3.3. MA2ML-Lite\\n\\nFor the MARL problem defined in (1), one method to solve it is to learn a policy individually for each module of AUG, NAS, and HPO. In the AutoML training phase, we sample an action according to the policy of each agent, and train the model according to the searched ML pipeline to calculate the reward $R$(top-1 accuracy). In the MA2ML training phase, we use the reward $R$ to directly guide the update of each agent's policy. REINFORCE [34] which can handle both discrete and continuous action, is used to calculate policy gradient, which is formulated as\\n\\n$$\\\\nabla_{\\\\theta_i} J(\\\\Theta) = \\\\mathbb{E}_{\\\\pi}(A|S)[\\\\nabla_{\\\\theta_i} \\\\log \\\\pi_{\\\\theta_i}(A_i|S_i)(R - b)],$$\\n\\n(2)\\n\\nwhere $b$ is an exponential moving average of the previous rewards. The moving average is beneficial for reducing the variance of gradient estimate. Then, the policy of each agent $i$ is updated by gradient ascent using (2). Although $J(\\\\Theta)$ depends on the joint policy of all agents, their policies are not really jointly optimized by (2). In other words, for the obtained $R$, it is not able to tell the contribution of each agent, and thus cannot explicitly update their policies towards better ones. We term this RL method MA2ML-Lite, which is the lite version of MA2ML.\\n\\n3.4. Credit Assignment\\n\\nAs AUG, NAS, and HPO jointly determine the final performance, if we use the reward $R$ to directly feedback to each agent as MA2ML-Lite does, each agent cannot distinctly determine whether the performed action is good or not. This poses a great challenge to the learning of policies. To solve the challenge, we train a centralized critic to learn the action-value function (Q-function) and add a counterfactual baseline, inspired by [7]. The counterfactual baseline marginalizes out an agent's action, while keeping other agents' actions fixed. The difference between the value of taken actions and the counterfactual baseline measures the marginal contribution of each agent. The difference is used to update each agent's policy.\\n\\nThe centralized critic is denoted as $Q(S, A)$ or $Q(S, A_i, A_{-i})$, where $A_{-i}$ represents the joint action of all agents except agent $i$. The counterfactual baseline denoted as $b(S, A_{-i})$ for each agent $i$ is calculated as\\n\\n$$b(S, A_{-i}) = \\\\mathbb{E}_{A_i \\\\sim \\\\pi_{\\\\theta_i}}[Q(S, A_i, A_{-i})].$$\\n\\n(3)\\n\\n11962\"}"}
{"id": "CVPR-2023-1354", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In AutoML, considering the large action space of different modules and to reduce the computation cost, we can use the sampled action $A_i$ as input, while keeping other actions fixed, to approximate the counterfactual baseline.\\n\\n3.5. Off-Policy Learning\\n\\nREINFORCE is an on-policy method, meaning that current policy can only be updated using the experiences obtained by itself. Considering it takes much time to train the model to receive top-1 accuracy, on-policy methods are very inefficient. Thereby, we incorporate off-policy learning, such that the current policy can also use historical experiences generated during learning for updates, which helps a lot in improving search efficiency.\\n\\nTo enable off-policy learning for actor-critic methods, a popular method is maximum-entropy RL, like SAC [9], which adds an entropy regularization in the objective as\\n\\n$$J(\\\\Theta) = E_{\\\\pi}(A|S)\\\\left[Q(S, A) - \\\\lambda \\\\log \\\\pi(A|S)\\\\right] = E_{\\\\pi}(A|S)\\\\left[Q(S, A)\\\\right] + \\\\lambda H(\\\\pi(A|S)),$$\\n\\nwhere $H(\\\\pi(A|S))$ represents the entropy of $\\\\pi(A|S)$, and $\\\\lambda$ is the coefficient of entropy regularization. However, the entropy regularization biases the converged policy, i.e., the converged policy maximizes (4) instead of the original objective (1).\\n\\nTo eliminate the bias of the converged policy as well as implement off-policy learning, inspired by [33], we maintain a target policy $\\\\rho_i$ for each agent, in addition to the original policy $\\\\pi_i$, and add the divergence regularization between $\\\\pi_i$ and $\\\\rho_i$ in the reward,\\n\\n$$J(\\\\Theta) = E_{\\\\pi}(A|S)\\\\left[Q(S, A) - \\\\lambda \\\\log \\\\pi(A|S)\\\\rho(A|S)\\\\right] = E_{\\\\pi}(A|S)\\\\left[Q(S, A)\\\\right] - \\\\lambda D_{KL}(\\\\pi(\u00b7|S)||\\\\rho(\u00b7|S)),$$\\n\\nwhere $\\\\rho \\\\triangleq \\\\sum_{i=1}^{n} \\\\rho_i$ denotes the joint target policy and $D_{KL}(\\\\pi(\u00b7|S)||\\\\rho(\u00b7|S))$ denotes the KL divergence between two distributions, $\\\\pi(\u00b7|S)$ and $\\\\rho(\u00b7|S)$. The divergence regularization is beneficial for exploration and stable policy improvement. More importantly, based on divergence policy iteration [33] we can further derive the theoretical result as follows.\\n\\n**Theorem 1.** By iteratively applying divergence policy iteration and taking $\\\\pi_k$ as the joint target policy $\\\\rho_{k+1}$, the policy sequence of $\\\\{\\\\pi_k\\\\}$ converges and monotonically improves upon the original optimization problem.\\n\\n**Proof.** Let $J_{\\\\text{init}}(\\\\pi)$ be the original optimization objective and $J_{\\\\text{reg}}(\\\\pi, \\\\rho)$ be the optimization objective with divergence regularization given the fixed target policy $\\\\rho$. Then they have the following definitions and relations.\\n\\n$$J_{\\\\text{init}}(\\\\pi) = E_{\\\\pi}(A|S)\\\\left[R\\\\right],$$\\n\\n$$J_{\\\\text{reg}}(\\\\pi, \\\\rho) = E_{\\\\pi}(A|S)\\\\left[R - \\\\lambda \\\\log \\\\pi(A|S)\\\\rho(A|S)\\\\right],$$\\n\\n$$J_{\\\\text{reg}}(\\\\pi, \\\\rho) = J_{\\\\text{init}}(\\\\pi) - \\\\lambda D_{KL}(\\\\pi(\u00b7|S)||\\\\rho(\u00b7|S)).$$\\n\\nIteratively applying divergence policy iteration and taking $\\\\pi_k$ as the joint target policy $\\\\rho_{k+1}$ can be formulated as\\n\\n$$\\\\pi_{k+1} = \\\\arg \\\\max_{\\\\pi} J_{\\\\text{reg}}(\\\\pi, \\\\rho_{k+1}) = \\\\arg \\\\max_{\\\\pi} J_{\\\\text{reg}}(\\\\pi, \\\\pi_k).$$\\n\\nSubstituting (9) to (8), we have\\n\\n$$J_{\\\\text{init}}(\\\\pi_k+1) \\\\geq J_{\\\\text{init}}(\\\\pi_k) - \\\\lambda D_{KL}(\\\\pi_{k+1}(\u00b7|S)||\\\\pi_k(\u00b7|S)) = J_{\\\\text{reg}}(\\\\pi_k, \\\\pi_k) \\\\geq J_{\\\\text{reg}}(\\\\pi_k, \\\\pi_k) = J_{\\\\text{init}}(\\\\pi_k).$$\\n\\nThe first inequality is from the non-negativity of the KL-divergence and the second inequality is from the definition of $\\\\pi_{k+1}$ in (9).\\n\\nThe conclusion $J_{\\\\text{init}}(\\\\pi_k+1) \\\\geq J_{\\\\text{init}}(\\\\pi_k)$ (defined by (10)) means the original objective monotonically improves though we optimize the objective with divergence regularization.\\n\\nMoreover, the reward $R$ is bounded, so the original objective $J_{\\\\text{init}}(\\\\pi)$ is bounded. The boundness and monotonic improvement of the sequence $J_{\\\\text{init}}(\\\\pi_k)$ guarantees the convergence of the original objective sequence.\\n\\n**Remark 1.** Theorem 1 guarantees the monotonic policy improvement and sub-optimum in the original optimization objective (1), not just the divergence-regularized objective as in [33]. This is crucial for AutoML as we can control the search of MA2ML by $\\\\lambda$, without deteriorating the optimality of the searched ML pipeline. Therefore, Theorem 1 lays the theoretical foundation of MA2ML.\\n\\n3.6. Training\\n\\nIn the AutoML training phase, we sample actions $A$ from $\\\\pi$ to obtain the ML pipeline and train the model according to the pipeline to receive top-1 accuracy as the reward $R$. After that, we store the experience $\\\\langle S, A, R \\\\rangle$ in the replay buffer $D$.\\n\\nIn the MA2ML training phase, we sample a mini-batch $B$ of experiences from the replay buffer $D$. We first update the centralized critic, parametrized by $\\\\phi$, by gradient descent of the loss function $L_Q$ as\\n\\n$$L_Q = E\\\\left[\\\\left(Q_{\\\\phi}(S, A) - R\\\\right)^2\\\\right],$$\\n\\n$$\\\\phi = \\\\phi - \\\\eta \\\\phi \\\\nabla \\\\phi L_Q,$$\"}"}
{"id": "CVPR-2023-1354", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] James Bergstra and Yoshua Bengio. Random search for hyper-parameter optimization. *Journal of Machine Learning Research*, 13(2), 2012.\\n\\n[2] Han Cai, Chuang Gan, Tianzhe Wang, Zhekai Zhang, and Song Han. Once for all: Train one network and specialize it for efficient deployment. In *International Conference on Learning Representations (ICLR)*, 2020.\\n\\n[3] Xiangxiang Chu, Xiaoxing Wang, Bo Zhang, Shun Lu, Xiaolin Wei, and Junchi Yan. Darts-: Robustly stepping out of performance collapse without indicators. In *International Conference on Learning Representations (ICLR)*, 2020.\\n\\n[4] Ekin D Cubuk, Barret Zoph, Dandelion Man\u00e9, Vijay Vasudevan, and Quoc V Le. Autoaugment: Learning augmentation strategies from data. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2019.\\n\\n[5] Xiaoliang Dai, Alvin Wan, Peizhao Zhang, Bichen Wu, Zhijian He, Zhen Wei, Kan Chen, Yuandong Tian, Matthew Yu, Peter Vajda, et al. Fbnetv3: Joint architecture-recipe search using predictor pretraining. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2021.\\n\\n[6] Xuanyi Dong, Mingxing Tan, Adams Wei Yu, Daiyi Peng, Bogdan Gabrys, and Quoc V Le. Autohas: Efficient hyperparameter and architecture search. *arXiv preprint arXiv:2006.03656*, 2020.\\n\\n[7] Jakob Foerster, Gregory Farquhar, Triantafyllos Afouras, Nantas Nardelli, and Shimon Whiteson. Counterfactual multi-agent policy gradients. In *AAAI Conference on Artificial Intelligence (AAAI)*, 2018.\\n\\n[8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In *Advances in Neural Information Processing Systems (NeurIPS)*, 2014.\\n\\n[9] Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor. In *International Conference on Machine Learning (ICML)*, 2018.\\n\\n[10] Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Faster autoaugment: Learning augmentation strategies using backpropagation. In *European Conference on Computer Vision (ECCV)*, 2020.\\n\\n[11] Chaoyang He, Haishan Ye, Li Shen, and Tong Zhang. Milenas: Efficient neural architecture search via mixed-level reformulation. In *IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020.\\n\\n[12] Daniel Ho, Eric Liang, Xi Chen, Ion Stoica, and Pieter Abbeel. Population based augmentation: Efficient learning of augmentation policy schedules. In *International Conference on Machine Learning (ICML)*, 2019.\\n\\n[13] Andrew Howard, Andrey Zhmoginov, Liang-Chieh Chen, Mark Sandler, and Menglong Zhu. Inverted residuals and linear bottlenecks: Mobile networks for classification, detection and segmentation. In *IEEE/CVF International Conference on Computer Vision (CVPR)*, 2018.\\n\\n[14] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q Weinberger. Deep networks with stochastic depth. In *European Conference on Computer Vision (ECCV)*, 2016.\\n\\n[15] Frank Hutter, Holger H Hoos, and Kevin Leyton-Brown. Sequential model-based optimization for general algorithm configuration. In *International Conference on Learning and Intelligent Optimization (LION)*, 2011.\\n\\n[16] Kirthevasan Kandasamy, Gautam Dasarathy, Jeff Schneider, and Barnab\u00e1s P\u00f3czos. Multi-fidelity bayesian optimisation with continuous approximations. In *International Conference on Machine Learning (ICML)*, 2017.\\n\\n[17] Taiga Kashima, Yoshihiro Yamada, and Shunta Saito. Joint search of data augmentation policies and network architectures, 2021.\\n\\n[18] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In *International Conference on Learning Representations (ICLR)*, 2015.\\n\\n[19] Aaron Klein and Frank Hutter. Tabular benchmarks for joint architecture and hyperparameter optimization. *arXiv preprint arXiv:1905.04970*, 2019.\\n\\n[20] Alex Krizhevsky. Learning multiple layers of features from tiny images. 2009.\\n\\n[21] Sungbin Lim, Ildoo Kim, Taesup Kim, Chiheon Kim, and Sungwoong Kim. Fast autoaugment. In *Advances in Neural Information Processing Systems (NeurIPS)*, 2019.\\n\\n[22] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. In *International Conference on Learning Representations (ICLR)*, 2018.\\n\\n[23] Jonathan Lorraine, Paul Vicol, and David Duvenaud. Optimizing millions of hyperparameters by implicit differentiation. In *International Conference on Artificial Intelligence and Statistics (AISTATS)*, 2020.\\n\\n[24] Dougal Maclaurin, David Duvenaud, and Ryan Adams. Gradient-based hyperparameter optimization through reversible learning. In *International Conference on Machine Learning (ICML)*, 2015.\\n\\n[25] Keith G Mills, Fred X Han, Mohammad Salameh, Seyed Saeed Changiz Rezaei, Linglong Kong, Wei Lu, Shuo Lian, Shangling Jui, and Di Niu. L2nas: Learning to optimize neural architectures via continuous-action reinforcement learning. In *ACM International Conference on Information & Knowledge Management (CIKM)*, 2021.\\n\\n[26] Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In *International Conference on Machine Learning (ICML)*, 2016.\\n\\n[27] Hieu Pham, Melody Guan, Barret Zoph, Quoc Le, and Jeff Dean. Efficient neural architecture search via parameters sharing. In *International Conference on Machine Learning (ICML)*, 2018.\\n\\n[28] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Designing network design spaces. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, 2020.\\n\\n[29] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V Le. Regularized evolution for image classifier architecture search. In *AAAI Conference on Artificial Intelligence (AAAI)*, 2019.\"}"}
{"id": "CVPR-2023-1354", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Esteban Real, Sherry Moore, Andrew Selle, Saurabh Saxena, Yutaka Leon Suematsu, Jie Tan, Quoc V Le, and Alexey Kurakin. Large-scale evolution of image classifiers. In International Conference on Machine Learning (ICML), 2017.\\n\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015.\\n\\nAmirreza Shaban, Ching-An Cheng, Nathan Hatch, and Byron Boots. Truncated back-propagation for bilevel optimization. In International Conference on Artificial Intelligence and Statistics (AISTATS), 2019.\\n\\nKefan Su and Zongqing Lu. Divergence-regularized multi-agent actor-critic. In International Conference on Machine Learning (ICML), 2022.\\n\\nRichard S Sutton, David A McAllester, Satinder P Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. In Advances in Neural Information Processing Systems (NeurIPS), 2000.\\n\\nMingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V. Le. Mnasnet: Platform-aware neural architecture search for mobile. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nMingxing Tan, Bo Chen, Ruoming Pang, Vijay Vasudevan, Mark Sandler, Andrew Howard, and Quoc V Le. Mnasnet: Platform-aware neural architecture search for mobile. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, 2019.\\n\\nMingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning (ICML), 2019.\\n\\nH. Tobias, NG Nicolas, M. Sven, and W. Stefan. Speeding up the hyperparameter optimization of deep convolutional neural networks. International Journal of Computational Intelligence and Applications, 17(02):1850008\u2013, 2018.\\n\\nXiaoxing Wang, Xiangxiang Chu, Yuda Fan, Zhexi Zhang, Xiaolin Wei, Junchi Yan, and Xiaokang Yang. Rome: robustifying memory-efficient nas via topology disentanglement and gradients accumulation. arXiv preprint arXiv:2011.11233, 2020.\\n\\nXiaoxing Wang, Xiangxiang Chu, Junchi Yan, and Xiaokang Yang. Daas: Differentiable architecture and augmentation policy search, 2021.\\n\\nJian Wu, Saul Toscano-Palmerin, Peter I Frazier, and Andrew Gordon Wilson. Practical multi-fidelity bayesian optimization for hyperparameter tuning. In Uncertainty in Artificial Intelligence (UAI), 2020.\\n\\nHan Xiao, Ziwei Wang, Zheng Zhu, Jie Zhou, and Jiwen Lu. Shapley-nas: Discovering operation contribution for neural architecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11892\u201311901, June 2022.\\n\\nYuhui Xu, Lingxi Xie, Xiaopeng Zhang, Xin Chen, Guo-Jun Qi, Qi Tian, and Hongkai Xiong. Pc-darts: Partial channel connections for memory-efficient architecture search. In International Conference on Learning Representations (ICLR), 2020.\\n\\nYibo Yang, Hongyang Li, Shan You, Fei Wang, Chen Qian, and Zhouchen Lin. Ista-nas: Efficient and consistent neural architecture search by sparse coding. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nZhaohui Yang, Yunhe Wang, Xinghao Chen, Boxin Shi, Chao Xu, Chunjing Xu, Qi Tian, and Chang Xu. Cars: Continuous evolution for efficient neural architecture search. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\nArber Zela, Aaron Klein, Stefan Falkner, and Frank Hutter. Towards automated deep learning: Efficient joint neural architecture and hyperparameter search. arXiv preprint arXiv:1807.06906, 2018.\\n\\nHongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In International Conference on Learning Representations (ICLR), 2018.\\n\\nMiao Zhang, Shirui Pan, Xiaojun Chang, Steven Su, Jilin Hu, Gholamreza (Reza) Haffari, and Bin Yang. Balenas: Differentiable architecture search via the bayesian learning rule. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11871\u201311880, June 2022.\\n\\nXinyu Zhang, Qiang Wang, Jian Zhang, and Zhao Zhong. Adversarial autoaugment. In International Conference on Learning Representations (ICLR), 2019.\\n\\nXiawu Zheng, Xiang Fei, Lei Zhang, Chenglin Wu, Fei Chao, Jianzhuang Liu, Wei Zeng, Yonghong Tian, and Rongrong Ji. Neural architecture search with representation mutual information. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11912\u201311921, June 2022.\\n\\nXiawu Zheng, Rongrong Ji, Yuhang Chen, Qiang Wang, Baochang Zhang, Jie Chen, Qixiang Ye, Feiyue Huang, and Yonghong Tian. Migo-nas: Towards fast and generalizable neural architecture search. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(9):2936\u20132952, 2021.\\n\\nKaichen Zhou, Lanqing Hong, Shoukang Hu, Fengwei Zhou, Binxin Ru, Jiashi Feng, and Zhenguo Li. Dha: End-to-end joint optimization of data augmentation policy, hyper-parameter and architecture. arXiv preprint arXiv:2109.05765, 2021.\\n\\nBarret Zoph and Quoc V Le. Neural architecture search with reinforcement learning. In International Conference on Learning Representations (ICLR), 2017.\\n\\nBarret Zoph, Vijay Vasudevan, Jonathon Shlens, and Quoc V Le. Learning transferable architectures for scalable image recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\"}"}
{"id": "CVPR-2023-1354", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1 MA2ML\\n\\n1: initialize the critic $\\\\phi$ and the policy $\\\\theta_i$ and target policy $\\\\bar{\\\\theta}_i$ for each agent $i$\\n\\n2: for $\\\\text{iter} = 1$ to $\\\\text{max}\\\\text{iter}$ do\\n\\n3: receive initial state $S_i$ for each agent $i$\\n\\n4: sample $A_i \\\\sim \\\\pi_{\\\\theta_i}(\\\\cdot | S_i)$ for each agent $i$\\n\\n5: determine the pipeline setting according to $A_i$\\n\\n6: train the pipeline for a number of epochs on dataset\\n\\n7: obtain the top-1 accuracy on the validation set as $R$\\n\\n8: store $S_i, A, R$ in the replay buffer $D$\\n\\n9: sample a mini-batch $B$ from $D$\\n\\n10: update the critic $\\\\phi$ by (11)\\n\\n11: update each agent's policy $\\\\theta_i$ by (13)\\n\\n12: update each agent's target policy $\\\\bar{\\\\theta}_i$ by (14)\\n\\n13: end for\\n\\n14: choose top-k pipelines in terms of top-1 accuracy\\n\\n15: retrain the pipelines till convergence\\n\\n16: obtain the highest accuracy on the test set\\n\\nwhere $\\\\eta_\\\\phi$ denotes the learning rate of $\\\\phi$. Then, we update the policy and the target policy for each agent. The counterfactual baseline with divergence regularization is modified as $b(S_i, A - i) = E_{A_i \\\\sim \\\\pi_i}[Q(S, A_i, A - i) - \\\\lambda \\\\log \\\\pi(A_i | S) \\\\rho_i(A_i | S)]$.\\n\\n(12)\\nThe gradient for the policy of each agent is calculated and the policy is updated by gradient ascent as $\\\\nabla_{\\\\theta_i} L_{\\\\pi_{\\\\theta_i}} = E[\\\\nabla_{\\\\theta_i} \\\\log \\\\pi_{\\\\theta_i}(A_i | S_i)(Q(S, A) - \\\\lambda \\\\log \\\\pi_{\\\\theta_i}(A_i | S_i) \\\\rho_i(A_i | S_i) - E_{A_i \\\\sim \\\\pi_i}[Q(S, A_i)] + \\\\lambda D_{\\\\text{KL}}(\\\\pi_{\\\\theta_i}(\\\\cdot | S_i) \\\\parallel \\\\rho_i(\\\\cdot | S_i))]$, (13)\\n\\nwhere $\\\\eta_{\\\\theta_i}$ is the learning rate of the policy. The target policy of each agent is parameterized by $\\\\bar{\\\\theta}_i$, and is updated as $\\\\bar{\\\\theta}_i = (1 - \\\\tau)\\\\bar{\\\\theta}_i + \\\\tau \\\\theta_i$, (14)\\n\\nwhere $\\\\tau$ is an empirically determined parameter.\\n\\nFor completeness, the framework of MA2ML is depicted in Figure 2 and the learning algorithm of MA2ML is summarized in Algorithm 1.\\n\\n4. Experiments\\n\\nImage classification experiments of MA2ML and MA2ML-Lite are performed on ImageNet while ablation studies are carried on CIFAR-10/100.\\n\\nDetailed experiment settings, results, and the search cost are available in the supplementary material.\"}"}
{"id": "CVPR-2023-1354", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Top-1 accuracy (%) and FLOPs of state-of-the-art AutoML methods on ImageNet, where NARS denotes neural architecture-recipe search. All compared models have computational cost close to 600M FLOPs for a fair comparison.\\n\\n| Model          | Acc (%) | FLOPs (M) | Method      | Search Modules |\\n|----------------|---------|-----------|-------------|----------------|\\n| DARTS [22]     | 73.3    | 574       | gradient NAS |                |\\n| NASNet [54]    | 74.0    | 564       | RL NAS      |                |\\n| MiLeNAS [11]   | 75.3    | 584       | gradient NAS |                |\\n| RMI-NAS [50]   | 75.3    | 657       | Random Forest NAS |          |\\n| RegNetY [28]   | 75.5    | 600       | pop. param. |                |\\n| \u2217 NAS           |         |           |             |                |\\n| ROME [39]      | 75.5    | 556       | gradient NAS |                |\\n| AmoebaNet-C [29] | 75.7  | 570       | evolution NAS |                |\\n| PC-DARTS [43]  | 75.8    | 597       | gradient NAS |                |\\n| BaLeNAS [48]   | 75.8    | 597       | gradient NAS |                |\\n| ISTA-NAS [44]  | 76.0    | 638       | gradient NAS |                |\\n| Shapley-NAS [42] | 76.1 | 582       | gradient NAS |                |\\n| DAAS [40]      | 76.6    | 698       | AUG+NAS     |                |\\n| DHA [52]       | 77.4    | -         | AUG+NAS+HPO |                |\\n| MIGO-NAS [51]  | 78.3    | 595       | MIGO NAS    |                |\\n| \u2217 Population parameterization. |      |           |             |                |\\n| OFA \u2020 [2]      | 79.0    | 595       | gradient NAS |                |\\n| EfficientNet-B1 [37] | 79.1  | 700       | RL NAS      |                |\\n| FBNetV3 \u2021 [5]  | 79.2    | 550       | NARS NAS+HPO |                |\\n| L2NAS [25]     | 79.3    | 618       | RL NAS      |                |\\n| MA2ML-A        | 79.3    | 490       | MARL AUG+NAS+HPO |            |\\n| MA2ML-B        | 79.7    | 596       | MARL AUG+NAS+HPO |            |\\n| \u2217 Results are given in [25] without distillation. |      |           |             |                |\\n| \u2217 Results are reproduced according to 600M FLOPs constraint without distillation. |      |           |             |                |\\n\\nTable 2. Comparison of MA2ML-A/B/C with MA2ML-Lite-A/B/C on ImageNet.\\n\\n| Model        | Acc (%) | FLOPs (M) |\\n|--------------|---------|-----------|\\n| MA2ML-Lite   | 78.6/79.1/79.5 | 498/597/697 |\\n| MA2ML        | 79.3/79.7/80.1 | 490/596/694 |\\n\\nEach model was trained for 100 epochs on ImageNet-200 to achieve top-1 accuracy. For one trial, 1992 pipelines were searched on ImageNet-200 in total. After the search, 20 pipelines with the highest accuracy were retrained under the given constraint on the entire ImageNet dataset for 400 epochs. The pipeline with the highest accuracy was selected as the search result. For the search results obtained by MA2ML and MA2ML-Lite with FLOPs constraint = 600M, we retrained those with FLOPs fewer than 500M/600M/700M and picked the best as MA2ML-A/B/C and MA2ML-Lite-A/B/C. For the search results obtained with FLOPs constraint = 900M, we retrained those with FLOPs fewer than 800M/900M/1G and picked the best as MA2ML-D/E/F.\\n\\nPerformance. On ImageNet, MA2ML-A/B/C/D/E/F achieves 79.3%, 79.7%, 80.1%, 80.5%, 80.7%, and 81.1% top-1 accuracy. Figure 3 gives the accuracy under different FLOPs of MA2ML and other methods. Since we do not use distillation in our experiments, results achieved by implementing distillation are not drawn here. The curve of MA2ML is plotted with the results achieved by MA2ML-A/B/C/D/E/F, which is the highest among all the compared methods with a large margin. Figure 3 also indicates that MA2ML can always perform better with larger constraints of computation cost.\\n\\nIn Table 1, we compared MA2ML-A, MA2ML-B, and other methods with similar FLOPs. MA2ML-A outperforms all listed AutoML methods with only 490M FLOPs (L2NAS has the same performance but with much larger FLOPs), and MA2ML-B with 596M FLOPs obtains the performance with large improvement compared with the results of other methods close to 600M FLOPs.\\n\\nIn Table 2, MA2ML-Lite-A/B/C achieves 78.6%, 79.1%, and 79.5%. The comparison shows that MA2ML outperforms MA2ML-Lite by 0.6%, which is attributed to credit assignment and off-policy learning of MA2ML.\\n\\nFigure 4 presents the average reward curves of top-20 pipelines and the scatter plot of average reward in each batch of MA2ML and MA2ML-Lite on ImageNet-200 (FLOPs constraint = 600M). The curves illustrate that the superiority of MA2ML at the early stage is not significant.\"}"}
{"id": "CVPR-2023-1354", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"monotonic policy improvement\\n\\nRight age reward curves of top-20 pipelines in terms of patch numbers. the benefit of\\n\\nthe batch-level average reward of MA2ML is also more sta-\\n\\ntic, but surpasses MA2ML-Lite a lot in the mid-term of\\n\\nMA2ML also greatly\\n\\nthe search and keeps the lead till the end. The scatter depicts\\n\\nFLOPs\\n\\nImageNet-200 (600M).\\n\\nWe also use the action space mentioned in Section\\n\\nSetting.\\n\\ning for ML modules is necessary.\\n\\neffectiveness and generality of MA2ML, and jointly search-\\n\\ncomputation cost. The performance on ImageNet proves the\\n\\nmance gain of MA2ML over MA2ML-Lite takes no more\\n\\nsearch cost is the training time to receive top-1 accuracy and\\n\\nLite and other RL-based methods, because the major part of\\n\\n25 augmentation sub-policies, which is the only difference\\n\\n10\\n\\nerase. Overall, AUG, NAS and HPO space contains\\n\\nconcrete values and use 10 steps to determine them on av-\\n\\nexperience. We discretize the search space of them into 4\\n\\nthese two hyper-parameters affect much the performance by\\n\\nwe choose SGD as the optimizer, fix the batch size, and\\n\\nA cell is composed of 5 sequential blocks. To create a new\\n\\ncells, and the 5th and 11th are designed as reduction cells.\\n\\na reduction cell. We construct the whole network with 17\\n\\nsearch space in NASNet, and search for a normal cell and\\n\\nfrom AUG module on ImageNet. For NAS, we utilize the\\n\\n25 augmentation sub-policies, which is the only difference\\n\\n3.1 on CIFAR-10 and CIFAR-100. For AUG, we generate\\n\\n10\\n\\ncandidates, respectively.\\n\\nresults achieved by MA2ML, MA2ML-Lite, and other\\n\\n0\\n\\n84\\n\\n07%\\n\\n\u00b1 0.12%\\n\\n85\\n\\n\u00b1 0.08\\n\\n0\\n\\n12%\\n\\n14%\\n\\n10%\\n\\nand\\n\\nResults on CIFAR-10/100.\\n\\nThe learning pattern of MA2ML and MA2ML-Lite on CIFAR-100 is\\n\\nThe scatter diagram, one can see that MA2ML outperforms\\n\\nMA2ML-Lite by a large margin from the start. Compared\\n\\nhigh sample efficiency of MA2ML on CIFAR-10. From\\n\\nMA2ML surpasses MA2ML-Lite at early stages and keeps\\n\\nMA2ML on-policy and MA2ML-Lite in the late search stages indi-\\n\\nBesides, the performance gap between the MA2ML\\n\\nin each batch of MA2ML, MA2ML-Lite, and MA2ML on-\\n\\nof top-30 pipelines and the scatter plot for average accuracy\\n\\nCIFAR-10. Figure 5 illustrates the average accuracy curves\\n\\nwe implement MA2ML on-policy version as an ablation on\\n\\nAblation on off-policy learning and credit assignment.\\n\\nOutperforming MA2ML-Lite and other RL-based methods, MA2ML achieves very competi-\\n\\ntive performance.\"}"}
{"id": "CVPR-2023-1354", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Top-1 accuracy (%) and parameter size (MB) of compared AutoML methods on CIFAR-10 and CIFAR-100. The results of MA2ML and MA2ML-Lite are mean and standard deviation of 5 trials of the best ML pipeline.\\n\\n| Model       | CIFAR-10 Acc (%) | CIFAR-10 Param (M) | CIFAR-100 Acc (%) | CIFAR-100 Param (M) |\\n|-------------|------------------|--------------------|-------------------|--------------------|\\n| NASNet-A    | 97.60            | 0.60               | 27.60             | 0.60               |\\n| ENAS        | 97.11            | 0.11               | 80.57             | 0.57               |\\n| L2NAS       | 97.51 \u00b1 0.12     | 3.80               | 82.24 \u00b1 0.19      | 3.50               |\\n| AmoebaNet   | 97.45 \u00b1 0.05     | 2.80               | 81.07 \u00b1 0.14      | 3.10               |\\n| DARTS       | 97.24 \u00b1 0.09     | 3.30               | 82.64 \u00b1 0.44      | 3.30               |\\n| DARTS-     | 97.41 \u00b1 0.08     | 3.50               | 82.49 \u00b1 0.25      | 3.30               |\\n| MiLeNAS     | 97.49 \u00b1 0.11     | 3.90               | -                 | -                  |\\n| ISTA-NAS    | 97.64 \u00b1 0.06     | 3.40               | 83.10 \u00b1 0.11      | 3.40               |\\n| AutoHAS     | 95.00             | -78.40             | -                 | -                  |\\n| Joint Search| 97.46 \u00b1 0.09     | -                  | 83.81 \u00b1 0.49      | -                  |\\n| DAAS        | 97.76 \u00b1 0.10     | 4.00               | 84.63 \u00b1 0.31      | 3.80               |\\n| DHA         | 98.11 \u00b1 0.26     | -                  | 83.93 \u00b1 0.23      | -                  |\\n| MA2ML-Lite  | 97.70 \u00b1 0.10     | 7.80               | 84.80 \u00b1 0.12      | 9.00               |\\n| MA2ML       | 97.77 \u00b1 0.07     | 9.00               | 85.08 \u00b1 0.14      | 7.70 MARL          |\\n\\nTable 4. Comparison of NAS+AUG+HPO, AUG+HPO, and HPO on CIFAR-100.\\n\\n| Module       | Baseline | HPO | AUG+HPO | NAS+AUG+HPO |\\n|--------------|----------|-----|---------|-------------|\\n| Acc(%)       | 73.50    | 74.39 | 77.37   | 85.08       |\\n\\nAblation on ML Modules.\\nOn CIFAR-100, the ablation studies for ML modules are performed. We fix the network architecture as ResNet-56 and run the optimization of HPO and AUG+HPO. In Table 4, one can see that HPO improves 0.89% accuracy (74.39% vs. 73.50%) over the baseline (ResNet-56 using default hyper-parameters) while AUG and NAS respectively improve 2.98% and 7.71%, which are all significant margins.\\n\\n5. Conclusion\\nWe proposed MA2ML, a general framework for the joint optimization of ML pipelines. MA2ML transforms the joint optimization of modules as an MARL problem, and provides a method with theoretical guarantee. Empirically, we investigated the joint optimization of AUG, NAS, and HPO for image classification tasks on ImageNet and CIFAR-10/100. MA2ML yields state-of-the-art top-1 accuracy on ImageNet under constraints of computational cost with similar settings, e.g., 79.7% / 80.5% with FLOPs fewer than 600M/800M. MA2ML substantially outperforms MA2ML-Lite and MA2ML on-policy, which empirically verified the benefit of credit assignment and off-policy learning of MA2ML.\\n\\nMA2ML has several advantages over other AutoML methods. First, MA2ML can be applied to the joint optimization of any arbitrary combination of different ML modules. Second, MA2ML is agnostic to search spaces, so we can choose suitable search spaces for different datasets. Third, MA2ML can also be easily generalized to different tasks by taking the evaluation (e.g., mIOU and pixel accuracy for semantic segmentation, and mAP for object detection) as the reward. Fourth, MA2ML is an RL-based method, which optimizes reward instead of training loss. Consequently, the incompatibility of different ML modules that may exist in the methods based on training loss, e.g., gradient-based methods, is naturally avoided. Last but not the least, the search of MA2ML can be easily controlled without affecting the optimality of the performance.\\n\\nMA2ML may have some limitations. From the experiments, we can find that there exists a gap between the rank of short-term and final long-term training accuracy, which causes the superiority of MA2ML's final performance over MA2ML-Lite not as large as in the search. Moreover, although MA2ML provides theoretical guarantee of the performance, as an MARL-based method, the search cost is large as other RL-based AutoML methods, since receiving the final reward costs much. How to address these two limitations is left as future work.\\n\\nAcknowledgement\\nThis work was supported in part by National Natural Science Foundation of China (NSFC) under Grant 62225208, 62171431, 62250012, and 62250068, the Strategic Priority Research Program of Chinese Academy of Sciences under Grant No. XDA27000000, and Huawei Technologies Co., Ltd.\"}"}
