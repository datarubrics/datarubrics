{"id": "CVPR-2022-871", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\\\[ \\\\Phi(Z | X = x) = \\\\mathbb{E}_z h p(Z | X = x)(z) - p(Z | X = x)(z)^2 \\\\]\\n\\nThe term \\\\( \\\\Phi(Z | X = x) \\\\) in Eq. (3, 4) can be regarded as the sensitivity of the latent distribution \\\\( p(Z | X = \\\\cdot | x) \\\\), with respect to changes at the input \\\\( x \\\\). Therefore, optimizing the Fisher information, \\\\( \\\\Phi(Z | X) \\\\), will minimize the average sensitivity of the latent distribution with respect to change of inputs \\\\( X \\\\). As shortcuts are generated by data artefacts that are transient in nature, they are sensitive to perturbations of input data \\\\[12\\\\]. As such, minimizing the Fisher information is a step towards promoting the learning of shortcut-invariant features. Our conjecture is supported by the results of the toy experiment included in Tab. 1. The DRNs constrained by the Fisher information (RIB) achieved better performance than the IB networks in the target domain.\\n\\nIn order to minimize the Fisher information expressed in Eq. (4), one has to compute second order derivatives such as \\\\( \\\\nabla^2 \\\\theta \\\\nabla^2 x \\\\log p(Z | X = \\\\cdot | x) \\\\), which is computationally prohibitive for tasks with large dimensional inputs such as stereo matching, semantic segmentation, etc. \\\\[39\\\\]. To overcome this issue, we propose ITSA, a simple yet computationally feasible approach to promote the learning of shortcut-invariant features.\\n\\n### 3.3.2 Approximating Fisher information\\n\\nOptimizing the Fisher information \\\\( \\\\Phi(Z | X) \\\\) measure defined in Eq. (3) is related to minimizing \\\\( \\\\Phi(Z | X = x) \\\\).\\n\\nBy adding a regularization term such as \\\\( \\\\Phi(Z | X = x) \\\\) to the loss function, we can penalize the transient features and discourage networks from learning shortcuts. To calculate this term, we employ a first order approximation as described below.\\n\\n**Lemma 3.1.** If \\\\( \\\\epsilon > 0 \\\\), \\\\( u \\\\) is a unit vector (i.e. \\\\( \\\\| u \\\\| = 1 \\\\)), we refer to as the shortcut perturbation and \\\\( x^* = x + \\\\epsilon u \\\\), then, subject to first order approximation:\\n\\n\\\\[\\n\\\\Phi(Z | X = x) = \\\\mathbb{E}_z h p(Z | X = x^*)(z) - p(Z | X = x)(z)^2 \\\\epsilon^2 \\\\cos^2 \\\\psi + V h \\\\nabla^2 x \\\\log p(Z | X = x)(z)^2 i \\\\epsilon\\n\\\\]\\n\\nwhere \\\\( \\\\mathbb{E}_z \\\\) and \\\\( V \\\\) are the expectation and variance of \\\\( \\\\epsilon \\\\), and \\\\( \\\\psi \\\\) is the angle between \\\\( u \\\\) and \\\\( \\\\nabla^2 x p(Z | X = x)(z) \\\\).\\n\\nProof is given in the supplementary material.\\n\\nThe first term in the RHS of Eq. (5) will be minimized when the divergence (distance) between the two distributions, \\\\( p(Z | X = x) \\\\) and \\\\( p(Z | X = x + \\\\epsilon u) \\\\), is reduced. There are many popular divergence measures between distributions, such as Kullback-Leibler divergence, Jensen-Shannon divergence, Total Variation, the Wasserstein distance, etc. In this work, we choose the Wasserstein distance: as the distributions \\\\( p(Z | X = x) \\\\) and \\\\( p(Z | X = x + \\\\epsilon u) \\\\) may not have common supports and it leads to a simpler loss function.\\n\\nIn the case of a deterministic feature extractor, which is common in stereo matching networks, the distributions \\\\( p(Z | X = x) \\\\) and \\\\( p(Z | X = x^*) \\\\) can be seen as two degenerate distributions (i.e. Dirac delta distributions) located at points \\\\( z = f(x) \\\\) and \\\\( z^* = f(x^*) \\\\). Furthermore, the \\\\( V \\\\) in Eq. (5) will be zero. In this case, the Wasserstein-distance can be simplified as:\\n\\n\\\\[\\nW_p(p(Z | X = x^*), p(Z | X = x)) = \\\\frac{z^* - z}{p^2(1/p^2)}.\\n\\\\]\\n\\nUsing the above insights, we can see that minimizing \\\\( \\\\| z^* - z \\\\|^2 \\\\) is a step towards minimizing \\\\( \\\\Phi(Z | X = x) \\\\) for \\\\( p = 1 \\\\). Thus, we propose to promote the learning of robust and shortcut-invariant features in stereo matching networks, by optimizing the overall loss function defined below:\\n\\n\\\\[\\nL = L_{smooth} L_1(\\\\hat{y}, y) + \\\\lambda^2 L_{FI z_l, z^*_l} + L_{FI z_r, z^*_r}\\n\\\\]\\n\\nwhere \\\\( \\\\hat{y} \\\\) and \\\\( y \\\\) are the estimated and ground-truth disparity maps, \\\\( L_{FI} \\\\) is our proposed Fisher information loss function defined as:\\n\\n\\\\[\\nL_{FI} = \\\\sum_{i=1}^{n} (z(i) - z^*(i))^2\\n\\\\]\\n\\nand \\\\( L_{smooth} L_1 \\\\) is the smooth-L1 loss function commonly employed for optimizing stereo matching networks \\\\[6, 16, 52, 53\\\\].\\n\\n### 3.3.3 Shortcut Perturbation (SCP)\\n\\nIn order to compute \\\\( L_{FI} \\\\), we need to define \\\\( u \\\\) (refer to as shortcut perturbation and is introduced in Lemma 3.1):\\n\\n\\\\[\\nu = \\\\frac{\\\\nabla^2 x z(i)}{\\\\| \\\\nabla^2 x z(i) \\\\|^2} \\\\]\\n\\nwhere \\\\( \\\\nabla^2 x z(i) \\\\) is the gradient of the extracted features \\\\( z \\\\) with respect to input. The shortcut-perturbed image can then be expressed as:\\n\\n\\\\[\\nx^*(i) = x(i) + \\\\epsilon \\\\nabla^2 x z(i) \\\\epsilon^2 \\\\]\\n\\nThe above perturbation will put more weight on pixels that are sensitive to changes in the input. Intuitively, pixels with large absolute value of \\\\( \\\\nabla^2 x z \\\\) will have significant impact in altering the statistics of encoded latent distributions and the extracted latent feature representations. Moreover, these pixels are also likely to include shortcuts as shortcuts are highly sensitive to perturbations of the input \\\\[12\\\\].\\n\\nTo examine the accuracy of the above approximations, we trained the digit recognition network of our toy experiment with the proposed SCP and \\\\( L_{FI} \\\\) (ITSA). As the proposed method is specifically designed for domain generalization, our method can effectively generalize the network.\"}"}
{"id": "CVPR-2022-871", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Examples of shortcuts in stereo matching networks. The left and right input images are included in the top two rows. The disparity maps estimated by the baseline PSMNet [6] are included in the third row and ITSA-PSMNet in the bottom row. The performance of the baseline PSMNet deteriorates substantially when the shortcut attributes are distorted or removed from the input stereo images. The corresponding EPE is displayed on the estimated disparity map. Best viewed in color and zoom in for details.\\n\\n4. Experiments\\n\\n4.1. Experimental Settings\\n\\nDatasets and Metrics: Scene Flow [25] is a large collection of synthetic stereo images with dense disparity ground truth. It contains FlyingThings3D, Driving and Monkaa subsets, and provides 35,454 training and 4,370 testing images. In our experiments, all stereo matching networks are trained on the Scene Flow dataset only.\\n\\nThe realistic datasets used in our experiments include KITTI2012 [11] and KITTI2015 [26] containing 193 and 200 stereo images of outdoor driving scenes, Middlebury [35] containing 15 images of high resolution indoor scenes, and ETH3D [36] containing 27 low resolution, greyscale stereo images of both indoor and outdoor scenes. Furthermore, datasets covering different weather conditions provided by the DrivingStereo [47] dataset, and night-time provided by Oxford Robotcar [24]) were also included to evaluate the robustness of our proposed method. All the above datasets come with with sparse ground truth.\\n\\nWe evaluated the performance of disparity estimation using the D1 error rate (%), with different pixel threshold. The D1 metric computes the percentage of bad pixels (disparity end-point error larger than the threshold) in the left frame. Following the advice of data originators, a threshold of 3 pixels is selected for KITTI and DrivingStereo, 2 pixels for Middlebury, and 1 pixel for ETH3D.\\n\\nBaselines & Implementation Details: We have selected three popular and top-performing stereo matching networks namely PSMNet [6], GwcNet [16] and CFNet [37] as the baseline networks for our experiments. We have selected these networks mainly due to the fact that PSMNet and GwcNet are well-studied, and commonly employed as a baseline in many prior works [44, 50, 54]; and CFNet is one of the recently proposed state-of-the-art stereo matching networks. The networks are implemented using PyTorch framework and are trained end-to-end with Adam (\\\\(\\\\beta_1 = 0.9, \\\\beta_2 = 0.999\\\\)) optimizer. Similar to the original implementations of the selected networks, our data processing includes color normalization and random cropping the input images to size \\\\(H = 256\\\\) and \\\\(W = 512\\\\). Following the original implementation of CFNet, asymmetric chromatic augmentation and asymmetric occlusion [46] are also employed for data augmentation in CFNet. The maximum disparity for PSMNet and GwcNet is set to 192, and for CFNet is set to 256. All models are trained from scratch for 20 epochs with learning rate set to 0.001 for the first 10 epochs and decreased by half for another 10 epochs. The batch size is set to 12 for training on 2 NVIDIA RTX 8000 Quadro GPUs. The models are trained using synthetic data only and directly tested using data from different realistic datasets. For all experiments included in the following sections, the hyper-parameters \\\\(\\\\lambda\\\\) and \\\\(\\\\epsilon\\\\) were set to 0.1 and 0.5 respectively. The hyper-parameter tuning experiments are detailed in the supplementary document. The code of our implementations is available at: https://github.com/waychin-weiqin/ITSA\\n\\n4.2. Shortcuts in stereo matching networks\\n\\nOur hypothesis is that the baseline stereo matching networks naively trained on synthetic data only, learn to exploit common artefacts of synthetic stereo images as shortcut features. These artefacts include (1) consistent local statistics (RGB color features) between the left and right stereo images and (2) over-reliance on local chromaticity features of the reference stereo viewpoint.\\n\\nTo empirically verify the above, we tested three baseline networks trained only with synthetic data (i.e. Scene flow), using augmented stereo inputs images. The augmented stereo images were derived from the Scene Flow dataset.\"}"}
{"id": "CVPR-2022-871", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Ablation results on PSMNet [6] and GwcNet [16]. SCP is the proposed shortcut perturbations and L^FI is the proposed loss function in Eq. (7). The D1 metric was used for evaluation.\\n\\n| Method          | KITTI-2012 | KITTI-2015 |\\n|-----------------|------------|------------|\\n|                 | HD         | Full       | Half       | Quarter   |\\n| CasStereo [15]  | 23.6       | 26.5       | 50.3       | 37.9      | 20.3       | 54.2       |\\n| GwcNet [16]     | 11.7       | 12.8       | 45.5       | 18.1      | 10.9       | 9.0        |\\n| PSMNet [6]      | 27.4       | 29.3       | 60.4       | 29.1      | 19.6       | 16.1       |\\n| MS-GCNet [3]    | 5.5        | 6.2        | 19.8       | -         | -          | 8.8        |\\n| MS-PSMNet [3]   | 14.0       | 7.8        | -          | 19.8      | -          | 16.8       |\\n| DSMNet [53]     | 6.2        | 6.5        | 21.8       | 13.8      | -          | 8.1        |\\n| ITSA-GwcNet     | 4.9        | 5.4        | 26.8       | 11.4      | 9.3        | 7.1        |\\n| ITSA-CFNet      | 4.2        | 4.7        | 20.7       | 10.4      | -          | 8.5        |\\n| ITSA-PSMNet     | 5.2        | 5.8        | 28.4       | 12.7      | 9.6        | 9.8        |\\n| CFNet RVC       | (1.6)      | (2.0)      | (16.1)     | (10.1)    | -          | (3.7)      |\\n\\nTable 4. Synthetic-to-realistic domain generalization evaluation using KITTI, Middlebury and ETH3D training sets. All methods are trained on the Scene Flow dataset and directly tested on the three real datasets. Pixel error rate with different threshold are employed: KITTI 3-pixel, Middlebury 2-pixel and ETH3D 1-pixel.\\n\\n4.4. Synthetic-to-Realistic Domain Generalization Evaluation\\n\\nIn Tab. 4, we compare the synthetic-to-realistic domain generalization performance of our method with the state-of-the-art stereo matching networks [6,15,16,37,49,52,53] on the four realistic datasets. All networks are trained on the synthetic Scene Flow training set only. We found that the proposed ITSA substantially improved the domain generalization performance (6.8% - 23.5%) of the selected stereo networks (PSMNet [6] and GwcNet [16]), outperforming the state-of-the-art stereo matching networks in the realistic datasets. The improved networks also outperform DSMNet [53] on the KITTI 2012 [11] and KITTI 2015 [26] datasets, and achieve comparable performance as the CFNet on the Middlebury [35] datasets. In addition, we show that ITSA is even capable of further enhancing the robustness and cross-domain performance of CFNet [37], which was the best performing stereo matching networks in the Robust Vision Challenge 2020. The results of fine-tuned CFNet (CFNet RVC) are also included in Tab. 4 to show the upper bound performance of CFNet. Qualitative results comparison of the baseline and ITSA are included in Fig. 4.\\n\\n4.5. Robustness to Anomalous Scenarios\\n\\nHere, we analyze the robustness to anomalous conditions of a network trained on synthetic data with the proposed ITSA. The anomalous conditions include night-time, foggy and rainy weather conditions. In this comparison, we train the same network twice: (1) pre-train using synthetic data followed by fine-tuning on realistic KITTI 2015 dataset (common strategy), (2) train only using synthetic data with the proposed SCP and L^FI (ITSA). We also included the pre-trained counterpart of CFNet [37] to illustrate the efficacy of our method in further enhancing the network robustness.\\n\\nIn Tab. 5, we show that the fine-tuned (FT) networks generated...\"}"}
{"id": "CVPR-2022-871", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Qualitative results on KITTI 2015 stereo data. For each example, the results of the baseline networks are presented on the top row and the results from our method are included in the bottom row. The corresponding left image and ground truth are included in column (a). Our method can significantly improve the stereo matching performance even in scenario with poor lighting condition. Best viewed in color and zoom in for details.\\n\\n| Model     | FT | ITSA | Avg D1 | Avg D2 | Avg D3 |\\n|-----------|----|------|--------|--------|--------|\\n| PSMNet    | \u2713  | \u2717    | 3.94   | 2.82   | 11.51  |\\n| GwcNet    | \u2713  | \u2717    | 3.10   | 2.46   | 12.34  |\\n| CFNet     | \u2713  | \u2717    | 4.89   | 4.64   | 10.74  |\\n\\nTable 5. Robustness evaluation on anomalous scenarios. Our method (ITSA) consistently enhances the robustness of selected stereo matching networks and outperform the fine-tuned (FT) models in the real-world anomalous scenarios including rainy and foggy weather and night-time. The performances were evaluated using the D1 metric.\\n\\nGenerally has better performance when tested on data similar to the KITTI training data (sunny and cloudy). In contrast, our method (ITSA) can substantially improve the robustness and overall performance of the PSMNet and GwcNet, without using the real-world data. The overall performance of fine-tuned CFNet is slightly better than its ITSA counterpart. However, as mentioned earlier, the proposed ITSA improves CFNet performance when only using synthetic data for training. The results demonstrate that our method could effectively improve the robustness and performance of existing stereo matching networks, and extends these networks to real-world applications, without using the real data for fine-tuning.\\n\\n4.6. Extension to Semantic Segmentation\\n\\nSimilar to stereo matching networks, semantic segmentation networks trained on synthetic data also fail to generalize to realistic data. Here, we show that the proposed ITSA can easily be extended to the semantic segmentation task to promote the learning of shortcut-invariant feature and enhance domain generalization. We have selected the commonly employed FCN paired with ResNet-50 as the baseline network. The network was trained on synthetic GTA V dataset only and evaluated on real Cityscapes dataset. The mean intersection over union (mIoU) metric was employed for evaluation.\\n\\n| Method   | GTA V mIoU | Cityscapes mIoU |\\n|----------|------------|-----------------|\\n| IBN-Net  | 22.17      | 32.45           |\\n| ISW      | 28.95      | 32.71           |\\n| DRPC     | 35.36      | 36.58           |\\n\\nTable 6. Synthetic-to-realistic domain generalization performance comparison on semantic segmentation task. All networks were trained on the GTA V synthetic dataset only and evaluated on the Cityscapes validation set (G \u2192 C). The mean intersection over union (mIoU) metric was employed for evaluation. As shown in Tab. 6, the proposed method (ITSA) can also improve the synthetic-to-realistic domain generalization performance of semantic segmentation networks and achieve comparable performance with the existing domain generalization methods (IBN-Net, ISW and DRPC). This further demonstrates the effectiveness of our proposed method in promoting shortcut-invariant features and enhancing the performance of domain generalization. The implementation details and qualitative results of our method are included in the supplementary document.\\n\\n5. Limitations\\n\\nAlthough the proposed method can significantly improve the performance of stereo matching networks without fine-tuning and even outperform their fine-tuned counterpart when tested in unseen challenging environments (e.g. rain and night-time), its performance remains fragile under extreme conditions (e.g. heavy rain and extreme low light), which may occur in real scenarios. This is reflected by the large errors reported in Tab. 5. By looking at samples with large errors, we noticed that those inaccuracies are largely due to having insufficient light source, lens glare/flares and reflection on specular surfaces (wet grounds).\\n\\n6. Conclusion\\n\\nIn this work, we have presented ITSA: a novel information theory-based approach for domain generalization in stereo matching networks. To address the shortcut learning challenge, we propose to minimize the sensitivity of the extracted feature representations to the input perturbations, measured via the Fisher information. We further proposed an efficient algorithm to optimize the Fisher information objective. Experimental results show that the proposed method consistently promotes the learning of robust and shortcut-invariant features, and substantially enhances the performance of existing stereo matching networks in cross-domain generalization, even outperforming their fine-tuned counterparts in challenging scenarios. We also show that the proposed method can be easily extended for non-geometry based vision problems such as semantic segmentation.\\n\\nAcknowledgements\\n\\nD. Suter acknowledges funding under Australian Research Council grant DP200103448.\"}"}
{"id": "CVPR-2022-871", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nState-of-the-art stereo matching networks trained only on synthetic data often fail to generalize to more challenging real data domains. In this paper, we attempt to unfold an important factor that hinders the networks from generalizing across domains: through the lens of shortcut learning. We demonstrate that the learning of feature representations in stereo matching networks is heavily influenced by synthetic data artefacts (shortcut attributes). To mitigate this issue, we propose an Information-Theoretic Shortcut Avoidance (ITSA) approach to automatically restrict shortcut-related information from being encoded into the feature representations. As a result, our proposed method learns robust and shortcut-invariant features by minimizing the sensitivity of latent features to input variations. To avoid the prohibitive computational cost of direct input sensitivity optimization, we propose an effective yet feasible algorithm to achieve robustness. We show that using this method, state-of-the-art stereo matching networks that are trained purely on synthetic data can effectively generalize to challenging and previously unseen real data scenarios. Importantly, the proposed method enhances the robustness of the synthetic trained networks to the point that they outperform their fine-tuned counterparts (on real data) for challenging out-of-domain stereo datasets.\\n\\n1. Introduction\\nStereo matching is a fundamental task in computer vision and is widely used for depth sensing in various applications such as augmented reality (AR), robotics and autonomous driving. In recent years, end-to-end trained Convolutional Neural Networks (CNNs) have achieved impressive results for this task as quantified by the performance on several publicly available stereo-matching benchmarks [6, 16, 18, 45, 52]. Generally, end-to-end stereo-matching networks require a large amount of labelled data for training. To overcome this challenge, many state-of-the-art networks are initially trained on labelled synthetic data, commonly generated using game engines. However, models trained using synthetic data do not generalize well to unseen realistic domains. For example, the PSMNet [6] pre-trained on the Scene Flow dataset [25] performs poorly when tested on unseen realistic domains as illustrated in Fig. 1. Therefore, in practice, the networks trained with synthetic data are fine-tuned using labelled data from the relevant target domain. However, collecting even a relatively small amount of dense ground truth data in the real-world can be challenging for tasks like stereo-matching [22, 41]. Furthermore, to be practically useful in many applications, a stereo-matching model should be able to generalize effortlessly to different domains.\\n\\nFigure 1. Comparison of disparity maps estimated by PSMNet [6] when it is trained under different settings and across multiple domains. Each column shows the results for a realistic domain namely: KITTI 2015 [26], DrivingStereo [47], Oxford Robotcar [24] and Middlebury [35]. Rows from top to bottom show a sample image (I), the prediction for the Scene Flow pre-trained model (II), KITTI-15 fine-tuned model (III), and the proposed ITSA optimized method (IV). Comparing these figures shows that PSMNet trained solely on synthetic data performs poorly on real data and fine tuning only improves the result for KITTI dataset (still fails to generalize for other scenarios). The proposed method performs well across the board (best viewed in color).\"}"}
{"id": "CVPR-2022-871", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"mains like day and night times, varying weather conditions, etc. Collecting data for fine-tuning that cover all possible situations is both difficult and expensive. It is therefore highly desirable to remove the fine-tuning requirement.\\n\\nIt is known that neural networks, including stereo matching networks, can learn superficial shortcut features (or spurious correlations with the target labels), which prevent them from generalizing across different domains [2,12]. We found that stereo matching networks trained on synthetic data are susceptible to exploiting shortcuts in synthetic data such as (1) consistent local statistics (RGB color features) between the left and right stereo images and (2) over-reliance on local chromaticity features (e.g. color, illumination, texture) of the reference stereo viewpoint. Detailed analysis and discussion are included in Sec. 4.2. Dependency on these shortcut cues, instead of the desirable semantic and structural representations, means that these networks would fail drastically when the spurious correlations between shortcuts and labels do not exist in a new (unseen) domain [33]. While several shortcut-removal approaches have been previously proposed [4,17,38], most of these methods are manually designed (e.g. carefully selected data augmentations [4, 17]) and rely on the assumption that the shortcuts could be identified in advance. However, shortcuts can be non-intuitive, task-specific, and difficult to identify [9, 27].\\n\\nOur goal is to train a stereo matching network on synthetic data that can generalize to realistic scenes without the need for fine-tuning. To achieve this, we propose an information-theoretic approach to automatically restrict the shortcut-related information from being encoded from the input into the feature representations. The approach is based on the well known information bottleneck (IB) principle that proposes to optimize the following objective [1, 40]:\\n\\n$$\\\\arg\\\\max_{\\\\theta} I(Y; Z) - \\\\beta I(X; Z)$$\\n\\nwhere $Z$ is the encoding of input $X$, $Y$ is the target, $I$ is mutual information and $\\\\beta \\\\in [0, 1]$ is the hyperparameter that controls the size of the information bottleneck. While optimizing the IB objective leads to compressed feature representations, our empirical experiments showed that these compressed features are neither robust nor shortcut-invariant (details are provided in Sec. 3.3.1). Consequently, the IB optimized networks may still incorporate shortcuts and remain fragile when tested in unseen domains. The recently introduced robust IB criterion [31] encourages the learning of both robust and compressive features by replacing the mutual information in IB with statistical Fisher information. Robust IB is presented in the context of learning features that are robust to adversarial attacks and to the best of our knowledge it has not been used for domain generalization.\\n\\nIn our approach, we combine the task loss (e.g. smooth L1 loss) with Fisher information to learn a generalizable stereo matching model. Although such an objective can work in theory, straightforward optimization of the Fisher information by gradient descent requires computation of the second-order derivatives and is therefore computationally expensive for tasks with high dimensional inputs such as stereo matching and semantic segmentation. To overcome this shortcoming, we propose ITSA which consists of a novel loss term and perturbation technique to approximate the optimization of the Fisher information loss. The proposed ITSA is computationally efficient, and as we show by extensive experiments, it can promote the learning of shortcut-invariant features. Unlike the existing domain-invariant stereo matching networks [37, 53], the proposed ITSA does not involve significant network alteration and is model-agnostic. Therefore, as shown in the experiments section, it can be easily integrated with different stereo matching networks.\\n\\nThe empirical results show that stereo-matching networks trained on synthetic data, with the proposed ITSA, can generalize to realistic data without fine-tuning. Additional experiments on challenging out-of-domain stereo datasets (e.g. different adverse weathers and night scenes) show that our method also improves the overall robustness of the stereo matching networks and importantly even outperforms the networks fine-tuned on realistic domains when tested on these challenging datasets. The main contributions of this paper include:\\n\\n\u2022 We show that learning feature representations that are less sensitive to input variations can significantly enhance the synthetic to realistic domain generalization, and robustness in stereo matching networks.\\n\u2022 We introduce a novel loss function that enables us to minimize the Fisher information, without computing the second-order derivatives.\\n\u2022 We also show that the application of the proposed framework is not limited to stereo matching task, and can be used in training models for non-geometry based vision problems such as semantic segmentation.\\n\\nThe rest of the paper is organized as follows. Sec. 2 describes the related work in the field of learning-based stereo matching networks, domain generalization and shortcut learning. Sec. 3 presents the proposed method for automatic shortcut avoidance and domain generalization. Experimental results and discussions are presented in Sec. 4, and Sec. 6 concludes the paper.\"}"}
{"id": "CVPR-2022-871", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ing networks have excelled in most datasets and benchmarks [6, 18, 45, 52]. These networks generally have three sub-modules (1) feature extraction sub-network, (2) cost-volume generator, and (3) cost aggregation and refinement sub-network. There are two main types of stereo matching networks based on how the cost volume is generated. Correlation-based stereo matching networks construct the cost volume by correlating the features extracted from the two views. Previously proposed correlation-based methods include DispNetC [25], iResNet [21], CRL [30], SegStereo [48], and AANet [45]. Although these methods are usually computationally efficient, semantics and structural information in the feature representations are lost due to the correlation operation [16]. As a result, the correlation-based stereo matching methods usually have inferior performance compared to the concatenated-based methods.\\n\\nConcatenation-based methods use a cost volume that is a simple assembly of features extracted from the two views. Examples of the state-of-the-art concatenation-based stereo matching networks include PSMNet [6], GANet [52], GCNet [18], StereoDrNet [5] and EMCUA [28]. While these networks can achieve superior performance in stereo matching, they require labelled samples from the target environments, for fine-tuning. Without fine-tuning, these networks cannot generalize to unseen test data.\\n\\nTo overcome this problem, Zhang et al. [53] proposed DSMNet, which employs Domain Normalization and non-local graph-based filtering layers to enforce the learning of structural features that are domain-invariant. Similarly, Shen et al. [37] introduced CFNet, an efficient network architecture with multi-scale cost volume fusion and refinement, to enforce the learning of robust structural representation for stereo matching. In contrast, we have identified shortcut learning [13] as a major factor that hinders stereo matching networks from generalizing across domains. In this work, we show that avoiding shortcut learning can effectively enhance the robustness of the stereo matching networks and enables a model to generalize across domains. This is evidenced by showing networks' superior performance on challenging realistic data without fine-tuning.\\n\\nSingle Domain Generalization\\n\\nDomain generalization typically involves forcing DNNs to learn domain-invariant features, using data sampled from multiple source domains [20, 32]. On the other hand, single domain generalization is a more challenging problem because only one source domain is available for training. To solve this problem, Volpi et al. [42] proposed adversarial data augmentation (ADA), which aims to expand and diversify the distribution of training data. Specifically, ADA creates \u201cfictitious\u201d yet \u201cchallenging\u201d new populations, simulating data sampled from novel domains, using adversarial training. In a similar fashion, Qiao et al. [32] proposed a novel framework that employs ADA and meta-learning to enforce the learning of domain-invariant features. While these works focus on minimizing the domain differences, we are interested in learning robust and shortcut-invariant features that are transferable across different domains. To this end, we propose ITSA, an information-theoretic approach to prevent shortcut learning (see next section), particularly in the stereo matching networks.\\n\\nShortcut Learning\\n\\nGeirhos et al. [12] coined the term shortcut learning as a phenomenon where DNNs learn trivial solutions by relying on superficial features (shortcuts). These features are spuriously correlated with the target labels, without contributing to transferability across contexts. For example, image classification networks tend to rely on shortcuts such as backgrounds [2, 12] and textures [14, 43] to improve their performance. However, these networks fail to generalize to unseen domains, where the spurious correlations between shortcuts and labels are violated [33]. Similarly, we observed that stereo matching networks trained on synthetic data also have a tendency to exploit shortcuts to produce accurate depth results in synthetic domains. Consequently, these networks fail drastically when tested in unseen realistic environments.\\n\\nSeveral attempts have been made to restrict the learning of identified shortcuts and generalize DNNs across domains [4, 7, 17, 38, 43]. These methods rely on having some shortcut-related prior knowledge and usually include data augmentations [4, 17], whitening transformation [7] or dropout-based regularization [38] as part of their solutions. However, shortcuts are non-trivial, task-specific, and are often difficult to be identified a priori [9, 27]. In contrast, our proposed method automatically avoids shortcut learning without requiring shortcut-related knowledge in advance.\\n\\n3. Methodology\\n\\n3.1. Problem Definition\\n\\nIn this work, we focus on the synthetic-to-realistic domain generalization for stereo matching. Given a synthetic stereo data set $D_{syn}$ consisting of stereo image pairs $x^{(i)}_{syn,l}, x^{(i)}_{syn,r}$, with corresponding ground-truth disparity $y^{(i)}_{syn}$, the goal is to design a robust and shortcut-invariant stereo matching network that can accurately predict disparity map $\\\\hat{y}^{(i)}$ for unseen realistic environments $D_{real}$.\\n\\nOur approach to achieve synthetic-to-realistic domain generalization is to use an information-theoretic measure to automatically restrict the shortcut-related information from being included in feature representations.\"}"}
{"id": "CVPR-2022-871", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. Model\\nA typical stereo-matching network can be represented by the following equation:\\n\\\\[\\n\\\\hat{y}(i) = m_\\\\psi C\\\\ f_\\\\theta x(i) l, f_\\\\theta x(i) r!\\n\\\\] (2)\\nwhere \\\\( f_\\\\theta (... \\\\) is the feature extraction sub-network, \\\\( C(... \\\\) the cost volume and \\\\( m_\\\\psi (...) \\\\) the cost aggregation and refinement sub-network. The refined cost volumes are converted to disparity maps via the soft argmin [18] operation.\\n\\nOur proposed method (ITSA) can be applied to any stereo-matching network that has the above structure. In the experiments section, we show the result of applying the proposed algorithm to different stereo-matching networks with concatenation cost (we observed similar results with correlation-based methods) volumes [6, 16, 37]. The high-level structure of the network including the proposed shortcut avoidance strategy is shown in Fig. 2.\\n\\n3.3. Loss function\\nOur main contribution is the loss function devised to automatically restrict the shortcut-related information from being encoded in the learning process. As we explained earlier, the information bottleneck (IB) principle [1, 40] is typically used to compress features and would be a natural choice to achieve this objective.\\n\\nThe standard \\\\( L^{IB} \\\\) loss defined in Eq. (1), which uses mutual information to quantify information content, was designed to extract features that are both concise and relevant for prediction. However, models trained by this loss are not robust to existence of artefacts that can generate shortcuts (similar to adversarial distortions mentioned in [31]). To demonstrate the above point, we conducted a toy experiment. In this experiment, we investigated the efficacy of using IB loss for helping digit recognition networks (DRNs) to generalize from MNIST (source) [19] to MNIST-M [10] (target) dataset. The former contains images of handwritten digits with black background, and the latter is created by combining the MNIST digits with randomly extracted color patches as their background. All networks were trained on the MNIST training set only and the top-1 accuracy (\\\\% \\\\) was employed for evaluation. The details of the experiment are included in the supplementary document. As shown in Tab. 1, the standard IB can effectively reduce over fitting, and achieves the best performance in the source domain. However, it fails to generalize its performance to the unseen domain. Importantly, it even performs worse than the baseline networks in the unseen target domain.\\n\\n### Table 1. Performance comparison of digit recognition networks optimized via empirical risk minimization (ERM), the information bottleneck (IB) [1], its robust variant (RIB) [31] and our proposed method (ITSA). While IB performs well in the in-domain tests, it performs poorly on out-of-domain tests.\\n\\n| Method | MNIST [19] | MNIST-M [10] |\\n|--------|-----------|--------------|\\n| ERM    | 97.9 \u00b1 0.14 | 40.9 \u00b1 2.95  |\\n| IB     | 99.0 \u00b1 0.47 | 21.8 \u00b1 0.21  |\\n| RIB    | 98.3 \u00b1 0.13 | 52.8 \u00b1 1.04  |\\n| ITSA   | 98.1 \u00b1 0.38 | 56.9 \u00b1 1.23  |\\n\\nWhile IB performs well in the in-domain tests, it performs poorly on out-of-domain tests.\"}"}
{"id": "CVPR-2022-871", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Alex Alemi, Ian Fischer, Josh Dillon, and Kevin Murphy. Deep variational information bottleneck. In ICLR, 2017.\\n\\n[2] Sara Beery, Grant Van Horn, and Pietro Perona. Recognition in terra incognita. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.\\n\\n[3] Changjiang Cai, Matteo Poggi, Stefano Mattoccia, and Philippos Mordohai. Matching-space stereo networks for cross-domain generalization. In 2020 International Conference on 3D Vision (3DV), pages 364\u2013373, 2020.\\n\\n[4] Fabio M Carlucci, Antonio D'Innocente, Silvia Bucci, Barbara Caputo, and Tatiana Tommasi. Domain generalization by solving jigsaw puzzles. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2229\u20132238, 2019.\\n\\n[5] Rohan Chabra, Julian Straub, Christopher Sweeney, Richard Newcombe, and Henry Fuchs. Stereodrnet: Dilated residual stereonet. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 11786\u201311795, 2019.\\n\\n[6] Jia-Ren Chang and Yong-Sheng Chen. Pyramid stereo matching network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5410\u20135418, 2018.\\n\\n[7] Sungha Choi, Sanghun Jung, Huiwon Yun, Joanne T Kim, Seungryong Kim, and Jaegul Choo. Robustnet: Improving domain generalization in urban-scene segmentation via instance selective whitening. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11580\u201311590, 2021.\\n\\n[8] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\\n\\n[9] Nikolay Dagaev, Brett D Roads, Xiaoliang Luo, Daniel N Barry, Kaustubh R Patil, and Bradley C Love. A too-good-to-be-true prior to reduce shortcut reliance. arXiv preprint arXiv:2102.06406, 2021.\\n\\n[10] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The journal of machine learning research, 17(1):2096\u20132030, 2016.\\n\\n[11] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\\n\\n[12] Robert Geirhos, J\u00f6rn-Henrik Jacobsen, Claudio Michaelis, Richard Zemel, Wieland Brendel, Matthias Bethge, and Felix A Wichmann. Shortcut learning in deep neural networks. arXiv preprint arXiv:2004.07780, 2020.\\n\\n[13] Robert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.\\n\\n[14] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2495\u20132504, 2020.\\n\\n[15] Xiaoyang Guo, Kai Yang, Wukui Yang, Xiaogang Wang, and Hongsheng Li. Group-wise correlation stereo network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3273\u20133282, 2019.\\n\\n[16] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019.\\n\\n[17] Alex Kendall, Hayk Martirosyan, Saumitro Dasgupta, Peter Henry, Ryan Kennedy, Abraham Bachrach, and Adam Bry. End-to-end learning of geometry and context for deep stereo regression. In Proceedings of the IEEE International Conference on Computer Vision, pages 66\u201375, 2017.\\n\\n[18] Yann LeCun, L\u00e9on Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\\n\\n[19] Haoliang Li, Sinno Jialin Pan, Shiqi Wang, and Alex C Kot. Domain generalization with adversarial feature learning. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5400\u20135409, 2018.\\n\\n[20] Zhengfa Liang, Yiliu Feng, Yulan Guo, Hengzhu Liu, Wei Chen, Linbo Qiao, Li Zhou, and Jianfeng Zhang. Learning for disparity estimation through feature constancy. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2811\u20132820, 2018.\\n\\n[21] Rui Liu, Chengxi Yang, Wenxiu Sun, Xiaogang Wang, and Hongsheng Li. Stereogan: Bridging synthetic-to-real domain gap by joint optimization of domain translation and stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12757\u201312766, 2020.\\n\\n[22] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431\u20133440, 2015.\\n\\n[23] Will Maddern, Geoff Pascoe, Chris Linegar, and Paul Newman. 1 Year, 1000km: The Oxford RobotCar Dataset. The International Journal of Robotics Research (IJRR), 36(1):3\u201315, 2017.\\n\\n[24] Nikolaus Mayer, Eddy Ilg, Philip Hausser, Philipp Fischer, Daniel Cremers, Alexey Dosovitskiy, and Thomas Brox. A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation. In Proceedings of the 13030.\"}"}
{"id": "CVPR-2022-871", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[26] Moritz Menze, Christian Heipke, and Andreas Geiger. Object scene flow. ISPRS Journal of Photogrammetry and Remote Sensing (JPRS), 2018.\\n\\n[27] Matthias Minderer, Olivier Bachem, Neil Houlsby, and Michael Tschannen. Automatic shortcut removal for self-supervised representation learning. In International Conference on Machine Learning, pages 6927\u20136937. PMLR, 2020.\\n\\n[28] Guang-Yu Nie, Ming-Ming Cheng, Yun Liu, Zhengfa Liang, Deng-Ping Fan, Yue Liu, and Yongtian Wang. Multi-level context ultra-aggregation for stereo matching. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3283\u20133291, 2019.\\n\\n[29] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In Proceedings of the European Conference on Computer Vision (ECCV), pages 464\u2013479, 2018.\\n\\n[30] Jiahao Pang, Wenxiu Sun, Jimmy SJ Ren, Chengxi Yang, and Qiong Yan. Cascade residual learning: A two-stage convolutional neural network for stereo matching. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 887\u2013895, 2017.\\n\\n[31] Ankit Pensia, Varun Jog, and Po-Ling Loh. Extracting robust and accurate features via a robust information bottleneck. IEEE Journal on Selected Areas in Information Theory, 1(1):131\u2013144, 2020.\\n\\n[32] Fengchun Qiao, Long Zhao, and Xi Peng. Learning to learn single domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12556\u201312565, 2020.\\n\\n[33] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International Conference on Machine Learning, pages 5389\u20135400. PMLR, 2019.\\n\\n[34] Stephan R Richter, Vibhav Vineet, Stefan Roth, and Vladlen Koltun. Playing for data: Ground truth from computer games. In European conference on computer vision, pages 102\u2013118. Springer, 2016.\\n\\n[35] Daniel Scharstein, Heiko Hirschm\u00fcller, York Kitajima, Greg Krathwohl, Nera Ne\u0161i\u0107, Xi Wang, and Porter Westling. High-resolution stereo datasets with subpixel-accurate ground truth. In German conference on pattern recognition, pages 31\u201342. Springer, 2014.\\n\\n[36] Thomas Sch\u00f6ps, Johannes L. Sch\u00f6nberger, Silvano Galliani, Torsten Sattler, Konrad Schindler, Marc Pollefeys, and Andreas Geiger. A multi-view stereo benchmark with high-resolution images and multi-camera videos. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\n[37] Zhelun Shen, Yuchao Dai, and Zhibo Rao. Cfnet: Cascade and fused cost volume for robust stereo matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13906\u201313915, June 2021.\\n\\n[38] Baifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, and Jingdong Wang. Informative dropout for robust representation learning: A shape-bias perspective. In International Conference on Machine Learning, pages 8828\u20138839. PMLR, 2020.\\n\\n[39] Yuge Shi, Jeffrey Seely, Philip HS Torr, N Siddharth, Awni Hannun, Nicolas Usunier, and Gabriel Synnaeve. Gradient matching for domain generalization. arXiv preprint arXiv:2104.09937, 2021.\\n\\n[40] Naftali Tishby and Noga Zaslavsky. Deep learning and the information bottleneck principle. In 2015 IEEE Information Theory Workshop (ITW), pages 1\u20135. IEEE, 2015.\\n\\n[41] Alessio Tonioni, Oscar Rahnama, Thomas Joy, Luigi Di Stefano, Thalaiyasingam Ajanthan, and Philip HS Torr. Learning to adapt for stereo. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9661\u20139670, 2019.\\n\\n[42] Riccardo Volpi, Hongseok Namkoong, Ozan Sener, John C. Duchi, Vittorio Murino, and Silvio Savarese. Generalizing to unseen domains via adversarial data augmentation. In Samy Bengio, Hanna M. Wallach, Hugo Larochelle, Kristen Grauman, Nicol\u00f2 Cesa-Bianchi, and Roman Garnett, editors, Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 5339\u20135349, 2018.\\n\\n[43] Haohan Wang, Zexue He, Zachary L. Lipton, and Eric P. Xing. Learning robust representations by projecting superficial statistics out. In International Conference on Learning Representations, 2019.\\n\\n[44] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar from visual depth estimation: Bridging the gap in 3D object detection for autonomous driving. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8445\u20138453, 2019.\\n\\n[45] Haofei Xu and Juyong Zhang. Aanet: Adaptive aggregation network for efficient stereo matching. arXiv preprint arXiv:2004.09548, 2020.\\n\\n[46] Gengshan Yang, Joshua Manela, Michael Happold, and Deva Ramanan. Hierarchical deep stereo matching on high-resolution images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5515\u20135524, 2019.\\n\\n[47] Guorun Yang, Xiao Song, Chaoqin Huang, Zhidong Deng, Jianping Shi, and Bolei Zhou. Drivingstereo: A large-scale dataset for stereo matching in autonomous driving scenarios. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 899\u2013908, 2019.\\n\\n[48] Guorun Yang, Hengshuang Zhao, Jianping Shi, Zhidong Deng, and Jiaya Jia. Segstereo: Exploiting semantic information for disparity estimation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 636\u2013651, 2018.\\n\\n[49] Zhichao Yin, Trevor Darrell, and Fisher Yu. Hierarchical discrete distribution decomposition for match density estimation. In Proceedings of the IEEE/CVF Conference on...\"}"}
{"id": "CVPR-2022-871", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[50] Yurong You, Yan Wang, Wei-Lun Chao, Divyansh Garg, Geoff Pleiss, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar++: Accurate depth for 3d object detection in autonomous driving. arXiv preprint arXiv:1906.06310, 2019.\\n\\n[51] Xiangyu Yue, Yang Zhang, Sicheng Zhao, Alberto Sangiovanni-Vincentelli, Kurt Keutzer, and Boqing Gong. Domain randomization and pyramid consistency: Simulation-to-real generalization without accessing target domain data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2100\u20132110, 2019.\\n\\n[52] Feihu Zhang, Victor Prisacariu, Ruigang Yang, and Philip HS Torr. Ga-net: Guided aggregation net for end-to-end stereo matching. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 185\u2013194, 2019.\\n\\n[53] Feihu Zhang, Xiaojuan Qi, Ruigang Yang, Victor Prisacariu, Benjamin Wah, and Philip Torr. Domain-invariant stereo matching networks. In European Conference on Computer Vision, pages 420\u2013439. Springer, 2020.\\n\\n[54] Youmin Zhang, Yimin Chen, Xiao Bai, Jun Zhou, Kun Yu, Zhiwei Li, and Kuiyuan Yang. Adaptive unimodal cost volume filtering for deep stereo matching. arXiv preprint arXiv:1909.03751, 2019.\"}"}
