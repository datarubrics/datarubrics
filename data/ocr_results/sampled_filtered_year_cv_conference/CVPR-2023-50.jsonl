{"id": "CVPR-2023-50", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Sameer Agarwal, Keir Mierle, and Others. Ceres solver. http://ceres-solver.org\\n\\n[2] Michael Burri, Janosch Nikolic, Pascal Gohl, Thomas Schneider, Joern Rehder, Sammy Omari, Markus W Achtelik, and Roland Siegwart. The euroc micro aerial vehicle datasets. The International Journal of Robotics Research, 35(10):1157\u20131163, 2016.\\n\\n[3] Qi Cai, Lilian Zhang, Yuanxin Wu, Wenxian Yu, and Dewen Hu. A pose-only solution to visual reconstruction and navigation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\\n\\n[4] Carlos Campos, Richard Elvira, Juan J G\u00f3mez Rodr\u00edguez, Jos\u00e9 MM Montiel, and Juan D Tard\u00f3s. Orb-slam3: An accurate open-source library for visual, visual\u2013inertial, and multimap slam. IEEE Transactions on Robotics, 2021.\\n\\n[5] Carlos Campos, Jos\u00e9 MM Montiel, and Juan D Tard\u00f3s. Inertial-only optimization for visual-inertial initialization. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 51\u201357. IEEE, 2020.\\n\\n[6] Andrea Censi. An icp variant using a point-to-line metric. In 2008 IEEE International Conference on Robotics and Automation, pages 19\u201325. Ieee, 2008.\\n\\n[7] Nikolaus Demmel, David Schubert, Christiane Sommer, Daniel Cremers, and Vladyslav Usenko. Square root marginalized for sliding-window bundle adjustment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13260\u201313268, 2021.\\n\\n[8] Javier Dom\u00ednguez-Conti, Jianfeng Yin, Yacine Alami, and Javier Civera. Visual-inertial slam initialization: A general linear formulation and a gravity-observing non-linear optimization. In 2018 IEEE International Symposium on Mixed and Augmented Reality (ISMAR), pages 37\u201345. IEEE, 2018.\\n\\n[9] Tue-Cuong Dong-Si and Anastasios I Mourikis. Estimator initialization in vision-aided inertial navigation with unknown camera-imu calibration. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 1064\u20131071. IEEE, 2012.\\n\\n[10] Jakob Engel, Vladlen Koltun, and Daniel Cremers. Direct sparse odometry. IEEE transactions on pattern analysis and machine intelligence, 40(3):611\u2013625, 2017.\\n\\n[11] Georgios Evangelidis and Branislav Micusik. Revisiting visual-inertial structure-from-motion for odometry and slam initialization. IEEE Robotics and Automation Letters, 6(2):1415\u20131422, 2021.\\n\\n[12] Christian Forster, Luca Carlone, Frank Dellaert, and Davide Scaramuzza. On-manifold preintegration for real-time visual\u2013inertial odometry. IEEE Transactions on Robotics, 33(1):1\u201321, 2016.\\n\\n[13] Christian Forster, Matia Pizzoli, and Davide Scaramuzza. Svo: Fast semi-direct monocular visual odometry. In 2014 IEEE international conference on robotics and automation (ICRA), pages 15\u201322. IEEE, 2014.\\n\\n[14] Patrick Geneva, Kevin Eckenhoff, Woosik Lee, Yulin Yang, and Guoquan Huang. Openvins: A research platform for visual-inertial estimation. In 2020 IEEE International Conference on Robotics and Automation (ICRA), pages 4666\u20134672. IEEE, 2020.\\n\\n[15] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003.\\n\\n[16] Jacques Kaiser, Agostino Martinelli, Flavio Fontana, and Davide Scaramuzza. Simultaneous state initialization and gyroscope bias calibration in visual inertial aided navigation. IEEE Robotics and Automation Letters, 2(1):18\u201325, 2016.\\n\\n[17] Laurent Kneip and Simon Lynen. Direct optimization of frame-to-frame rotation. In Proceedings of the IEEE International Conference on Computer Vision, pages 2352\u20132359, 2013.\\n\\n[18] Seong Hun Lee and Javier Civera. Rotation-only bundle adjustment. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 424\u2013433, 2021.\\n\\n[19] Vincent Lepetit, Francesc Moreno-Noguer, and Pascal Fua. Epnp: An accurate o (n) solution to the pnp problem. International journal of computer vision, 81(2):155\u2013166, 2009.\\n\\n[20] Stefan Leutenegger, Simon Lynen, Michael Bosse, Roland Siegwart, and Paul Furgale. Keyframe-based visual\u2013inertial odometry using nonlinear optimization. The International Journal of Robotics Research, 34(3):314\u2013334, 2015.\\n\\n[21] Xin Li, Yijia He, Jinlong Lin, and Xiao Liu. Leveraging planar regularities for point line visual-inertial odometry. In 2020 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 5120\u20135127. IEEE, 2020.\\n\\n[22] Haomin Liu, Mingyu Chen, Guofeng Zhang, Hujun Bao, and Yingze Bao. Ice-ba: Incremental, consistent and efficient bundle adjustment for visual-inertial slam. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1974\u20131982, 2018.\\n\\n[23] Bruce D Lucas, Takeo Kanade, et al. An iterative image registration technique with an application to stereo vision, volume 81. Vancouver, 1981.\\n\\n[24] Agostino Martinelli. Vision and imu data fusion: Closed-form solutions for attitude, speed, absolute scale, and bias determination. IEEE Transactions on Robotics, 28(1):44\u201360, 2011.\\n\\n[25] Agostino Martinelli. Closed-form solution of visual-inertial structure from motion. International journal of computer vision, 106(2):138\u2013152, 2014.\\n\\n[26] Dominik Muhle, Lukas Koestler, Nikolaus Demmel, Florian Bernard, and Daniel Cremers. The probabilistic normal epipolar constraint for frame-to-frame rotation optimization under uncertain feature positions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1819\u20131828, 2022.\\n\\n[27] Raul Mur-Artal and Juan D Tard\u00f3s. Orb-slam2: An open-source slam system for monocular, stereo, and rgb-d cameras. IEEE transactions on robotics, 33(5):1255\u20131262, 2017.\"}"}
{"id": "CVPR-2023-50", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ra\u00fal Mur-Artal and Juan D Tard\u00f3s. Visual-inertial monocular slam with map reuse. IEEE Robotics and Automation Letters, 2(2):796\u2013803, 2017.\\n\\nDavid Nister. An efficient solution to the five-point relative pose problem. IEEE transactions on pattern analysis and machine intelligence, 26(6):756\u2013770, 2004.\\n\\nTong Qin, Peiliang Li, and Shaojie Shen. Vins-mono: A robust and versatile monocular visual-inertial state estimator. IEEE Transactions on Robotics, 34(4):1004\u20131020, 2018.\\n\\nTong Qin and Shaojie Shen. Robust initialization of monocular visual-inertial estimation on aerial robots. In 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 4225\u20134232. IEEE, 2017.\\n\\nRen\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12179\u201312188, 2021.\\n\\nRen\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 2020.\\n\\nJianbo Shi et al. Good features to track. In 1994 Proceedings of IEEE conference on computer vision and pattern recognition, pages 593\u2013600. IEEE, 1994.\\n\\nShinji Umeyama. Least-squares estimation of transformation parameters between two point patterns. IEEE Transactions on Pattern Analysis & Machine Intelligence, 13(04):376\u2013380, 1991.\\n\\nVladyslav Usenko, Nikolaus Demmel, David Schubert, J\u00f6rg St\u00fcckler, and Daniel Cremers. Visual-inertial mapping with non-linear factor recovery. IEEE Robotics and Automation Letters, 5(2):422\u2013429, 2019.\\n\\nKejian Wu, Ahmed M Ahmed, Georgios A Georgiou, and Stergios I Roumeliotis. A square root inverse filter for efficient vision-aided inertial navigation on mobile devices. In Robotics: Science and Systems, volume 2. Rome, Italy, 2015.\\n\\nJi Zhao. An efficient solution to non-minimal case essential matrix estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(4):1777\u20131792, 2020.\\n\\nYunwen Zhou, Abhishek Kar, Eric Turner, Adarsh Kowdle, Chao X Guo, Ryan C DuToit, and Konstantine Tsotsos. Learned monocular depth priors in visual-inertial initialization. arXiv preprint arXiv:2204.09171, 2022.\\n\\nDavid Zu\u00f1iga-No\u00ebl, Francisco-Angel Moreno, and Javier Gonzalez-Jimenez. An analytical solution to the imu initialization problem for visual-inertial systems. IEEE Robotics and Automation Letters, 6(3):6116\u20136122, 2021.\"}"}
{"id": "CVPR-2023-50", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2.1 Tightly-Coupled Solution\\n\\nAssuming that the first frame of a multi-frame sequence is the world coordinate system, the position of each frame in the world coordinate system can be solved efficiently by directly using the linear global translation constraint [3].\\n\\nSuppose there are three keyframes and the index of these keyframes is $r$, $i$, and $l$, respectively. The LiGT constraint can be expressed as:\\n\\n$$B p_{rc1} + C p_{ci1} + D p_{cl1} = 0,$$\\n\\nwhere\\n\\n$$B = \\\\lfloor f_k i \\\\rfloor \\\\times R_{ci1} l_k a^T l_r R_{cr1},$$\\n$$C = \\\\theta_2 l_r \\\\lfloor f_k i \\\\rfloor \\\\times R_{ci1},$$\\n$$D = -(B + C) a^T l_r = \\\\lfloor R_{cr1} l_k \\\\rfloor \\\\times f_k r^T l_r R_{ci1} \\\\theta_l r.$$\\n\\nWhen we have $m$ ($m > 3$) keyframes, there are multiple $B$, $C$, $D$. We can concatenate them and define as the coefficient matrix $L$ which containing only visual observations and global rotations. Let $P = p^T c_{r1}, ..., p^T c_{n1}$, then Eq. (9) can be written as\\n\\n$$L \\\\cdot P = 0$$\\n\\nThe positions of all cameras concerning the first keyframe can be solved by Eq. (11). Since the translation vectors of Eq. (9) are all about the camera coordinate system, we substitute Eq. (5) into Eq. (9) to transform the camera coordinate system into IMU coordinate system.\\n\\n$$B' u_r + C' u_i + D' u_l = 0,$$\\n\\nwhere\\n\\n$$B' = BR_{bc}^T,$$\\n$$C' = CR_{bc}^T,$$\\n$$D' = DR_{bc}^T.$$\\n\\nAll global translations in the above formulation can be replaced using IMU integration formulation in Eq. (1). Therefore, the system of linear equations (11) for solving the global position is transformed into the following equations for solving the initial velocity and gravity vector,\\n\\n$$A_1 A_2 v_{b1}^T b_1 g_{b1} = d$$\\n\\nwhere\\n\\n$$A_1 = B' \\\\Delta t_1 r + C' \\\\Delta t_1 i + D' \\\\Delta t_1 l,$$\\n$$A_2 = 2 B' \\\\Delta t_2 r + C' \\\\Delta t_2 i + D' \\\\Delta t_2 l,$$\\n$$d = -B' s_r - C' s_i - D' s_l,$$\\n$$s_m = \\\\alpha_{b1} b_m + R_{b1} b_m p_{bc} - p_{bc},$$\\n\\nFor detailed derivation, please refer to the supplement material Sec. 2. Since the norm of the gravity vector is constant, the Lagrange multiplier method [6] is used to find the optimal solution for the constrained least squares problem.\\n\\n4.2.2 Loosely-Coupled Solution\\n\\nThe loosely-coupled approach requires computing the camera translation first, which is then combined with the IMU measurements to compute the initial state variables. The camera pose can be calculated by Eq. (9), and if monocular camera is used, the position obtained from LiGT constraint is up to scale, then the metric scale factor $s$ needs to be computed explicitly during initialization. Define $X$ as a vector of initial state variables, $v_{b0}^T b_0$ means the velocity of body in $F_{b0}$, $g_{c0}$ is the gravity in $F_{c0}$ and $s$ is the metric scale.\\n\\n$$X = h v_{b0}^T b_0, v_{b1}^T b_1, ..., v_{bn}^T b_n, s, g_{c0}^T i \\\\in \\\\mathbb{R}^{3(n+1)+1+3}$$\\n\\nAssume that the body coordinate systems corresponding to two keyframes are $F_{bi}$ and $F_{bk}$, The following constraints [31] exist between IMU and visual measurements\\n\\n$$\\\\hat{\\\\alpha}_{bi} = R_{bi c0} s (p_{c0 bk} - p_{c0 bi}) + 1/2 g_{c0} \\\\Delta t_{2ik} - v_{bi} \\\\Delta t_{2ik}$$\\n$$\\\\hat{\\\\beta}_{bi} = R_{bi c0} R_{c0 bk} v_{bk} + g_{c0} \\\\Delta t_{2ik} - v_{bi} \\\\Delta t_{2ik},$$\\n\\nThe residual is the difference between the estimated and measured values, it can be parameterized as\\n\\n$$r(X) = \\\\hat{\\\\alpha}_{bi} - \\\\hat{\\\\alpha}_{bi}, \\\\hat{\\\\beta}_{bi} - \\\\hat{\\\\beta}_{bi}$$\\n\\nIn the presence of noise this system have no exact solution, so we still use the least squares solution:\\n\\n$$H^T b = \\\\hat{H}$$\\n\\nFinally, we stack $H$ generated by multiple adjacent frames into a coefficient matrix $H$, and the same is true for $b$ and $\\\\hat{b}$. Solving the least squares solution $H X = b$, $X$ can be obtained. Please refer to the supplement material Sec. 2 for the specific forms of $H$ and $b$.\\n\\n5. Experiments\\n\\nIn the section, simulation experiments are first used to verify the effectiveness of our method, and then the evaluation on real datasets demonstrates the accuracy, robustness and computational efficiency. Gyroscope bias error,\"}"}
{"id": "CVPR-2023-50", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"velocity error, gravity direction error, and scale factor error are used to evaluate the performance of each algorithm. We perform a Sim(3) alignment [35] against the ground truth trajectory to get scale error. For the gyroscope bias error, let $b_g$ be the mean of all biases in the GT trajectory, and the percent of the relative error is computed with $|b_g| - |b_g|/|b_g|$. We divided all the datasets collected by continuous motion in different scenarios into several data segments. Each data segment used for initialization consists of 10 keyframes, where the keyframes are obtained by sampling image frames at a frequency of $4\\\\, \\\\text{Hz}$ as used in other works [5, 40]. In all quantitative experiments, only datasets with successful initialization, i.e., scale error less than 1 ($s - 1 < 1$), were used for statistics, and the Root Mean Square Error (RMSE) is used for evaluation. All the experiments were conducted on a computer with Intel i7-9750H@ 2.6GHz CPU.\\n\\nThe loosely-coupled VI-initialization methods used for comparison include the VINS-Mono initialization (denoted as VINS-Mono) [30] and the analytical-solution [40] which is an improved work of the ORB-SLAM3 initialization [5] (denoted as AS-MLE). The code for the tightly-coupled initialization (denoted as CS-VISfM) [25] used for comparison is from the open-sourced SLAM OpenVINS [14]. We denote our method of solving the velocity and gravity vectors in a tightly-coupled or loosely-coupled manner as DRT-t and DRT-l, respectively. To verify the necessity of a gyroscope bias estimator (GBE), we evaluate the method combining our GBE with CS-VISfM (denoted as CS-VISfM-GBE) and the DRT-l method without GBE (denoted as DRT-l-wo-GBE) as ablation experiments.\\n\\n### 5.1. Simulation Experiments\\n\\nWe simulated camera motion with $20\\\\, \\\\text{Hz}$ and IMU measurements with $200\\\\, \\\\text{Hz}$, forming an ellipse trajectory with sinusoidal vertical motion. The long-semi axis and short-semi axis of the ellipse trajectory are $4\\\\, \\\\text{m}$ and $3\\\\, \\\\text{m}$, respectively. The number of observed feature points in each frame is limited to 150, and the Gaussian noise with standard deviation $\\\\delta_{\\\\text{pix}} = 1\\\\, \\\\text{pixel}$ is added to the landmark observations.\\n\\nThe simulated acceleration and gyroscope measurements are computed from the analytic derivation of the parametric trajectory and additionally corrupted by white noise and slowly time-varying bias terms $1$. To verify the convergence of the gyroscope bias estimator, we set different gyroscope biases from 0.02 rad/s to 0.18 rad/s during simulation.\\n\\n### Table 1. Initialization accuracy in gyroscope bias error (\\\\%), gravity direction error (\\\\degree), velocity error (m/s) and scale error metrics with DRT-t method.\\n\\n| Gyroscope Bias (rad/s) | Bg  | G.Dir  | Vel  | Scale |\\n|------------------------|-----|--------|------|-------|\\n| 0.02                   | 28.50 | 0.58  | 0.09 | 0.08  |\\n| 0.04                   | 12.56 | 0.60  | 0.09 | 0.08  |\\n| 0.06                   | 7.23  | 0.58  | 0.09 | 0.08  |\\n| 0.08                   | 5.82  | 0.59  | 0.09 | 0.08  |\\n| 0.10                   | 4.02  | 0.60  | 0.09 | 0.08  |\\n| 0.12                   | 3.53  | 0.59  | 0.09 | 0.08  |\\n| 0.14                   | 2.48  | 0.61  | 0.09 | 0.08  |\\n| 0.16                   | 2.52  | 0.59  | 0.09 | 0.08  |\\n| 0.18                   | 2.03  | 0.61  | 0.09 | 0.08  |\\n\\nAs shown in Tab. 1, the percentage of the gyro bias error decreases with the increase in the gyro bias magnitude, which is caused by the estimated absolute error of gyro bias all being about 0.01 rad/s. Meanwhile, the gravity direction errors obtained by DRT-t are about 0.6 \\\\degree, and the trajectory scale errors are 0.08, indicating the effectiveness of our initialization algorithm. The simulation results show that our method can not only converge on different gyroscope bias magnitude, but also accurately initialize the state variables.\\n\\n### 5.2. Real Experiments\\n\\nThe popular EuRoC dataset [2] from a micro air vehicle (MAV) is used to verify the algorithms. This dataset contains 11 sequences of different motion patterns collected in two scenes. We sampled 1422 data segments with sufficient motion excitations to exhaustively evaluate the accuracy, robustness, and time-consuming of each algorithm.\\n\\nIn the experiments, all algorithms use the same image processing operations, existing features are tracked by the KLT sparse optical flow algorithm [23], and new corner features are detected [34] to maintain 150 points for each image. The outliers are culled using RANSAC with a fundamental matrix model [15] for 1 pixel re-projection error. For the loosely-coupled algorithms, we adopt a general SfM framework to estimate camera motion, which first estimates the initial camera pose with the 5-point algorithm [29] and the PnP solver [19], and then uses bundle adjustment to optimize all poses and point clouds. The max running time of BA is set to 0.2s to fulfill real-time commands [30].\\n\\n### 5.2.1 Accuracy evaluation\\n\\nTo verify the accuracy and robustness of our gyroscope bias estimation algorithm, our method is compared with two loosely-coupled methods, VINS-Mono and AS-MLE. Fig. 4 shows that our method significantly outperforms previous methods in almost all sequences. Specifically, the loosely-coupled methods have no results on the V103 and V203 sequences because they are successfully initialized on too few data segments (less than 5) to be statistically significant. This also illustrates the robustness of our gyroscope bias estimation method.\"}"}
{"id": "CVPR-2023-50", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Exhaustive initialization results for 10KFs setting in low, medium, and high angular velocity datasets from EuRoC. For each metric, the best in red, the second best in blue.\\n\\n| Scale RMSE | Velocity RMSE | G.Dir RMSE |\\n|------------|---------------|------------|\\n| (m/s)      | (\u25e6)           |            |\\n| Low        | Medium        | High       |\\n| Mean       | Mean          | Mean       |\\n| AS-MLE     | 0.28          | 0.35       | 0.25       | 0.31       | 0.16       | 0.21       | 0.23       | 0.18       |\\n| CS-VISfM   | 0.53          | 0.50       | 0.41       | 0.51       | 0.23       | 0.24       | 0.30       | 0.24       |\\n| CS-VISfM-GBE | 0.23         | 0.23       | 0.07       | 0.22       | 0.13       | 0.13       | 0.06       | 0.12       |\\n| VINS-Mono  | 0.19          | 0.23       | 0.16       | 0.20       | 0.11       | 0.13       | 0.16       | 0.12       |\\n| DRT-t      | 0.25          | 0.22       | 0.06       | 0.23       | 0.13       | 0.13       | 0.06       | 0.13       |\\n| DRT-l      | 0.15          | 0.15       | 0.07       | 0.15       | 0.09       | 0.10       | 0.07       | 0.09       |\\n| DRT-l-wo-GBE | 0.48       | 0.46       | 0.51       | 0.48       | 0.22       | 0.24       | 0.28       | 0.23       |\\n\\nSequences: MH01, MH02, MH03, MH04, MH05, V101, V102, V103, V201, V202, V203\\n\\nGyroscope Bias Error (%)\\n\\nFigure 4. Gyroscope bias errors on EuRoC sequences. Loosely-coupled methods are difficult to initialize successfully on V103 and V203.\\n\\nIn the error statistics of other initial state variables such as scale factor and gravity vector, we classified the 1422 data segments according to the magnitude of angular velocity, including 638 low-speed data segments ($|\\\\omega| < 15$ \u25e6/s), 327 high-speed data segments ($|\\\\omega| > 30$ \u25e6/s), and 457 medium-speed data segments. Please refer to the supplementary material Sec.3 for the results of separate statistics for the 11 sequences. From Tab. 2, it can be seen that DRT-l significantly outperforms state-of-the-art initialization methods on almost all the motion scenarios, which verifies the effectiveness of our proposed framework. Specifically, comparing the two tightly coupled methods CS-VISfM-GBE and DRT-t, it can be found that the accuracy difference is marginal. In fact, the difference between the two methods is whether to introduce 3D point coordinates when constructing the constraint equation. Comparing VINS-Mono and DRT-t, it can be seen that the method of decoupling rotation and translation can estimate the gravity vector more accurately. However, in the tightly coupled method, the velocity and pose of each keyframe are calculated by integrating the accelerometer data from the initial moment, and the noise accumulated by the integration makes the accuracy of the scale factor and the velocity lower than that of the loosely coupled method. DRT-l combines the advantages of high rotation accuracy obtained by the decoupling method and the advantage of not requiring long-time integration of accelerometer data by the loosely coupled method, making it the best overall performance. It should be noted that compared with VINS-Mono, the LiGT constraint used to solve translation in DRT-l is not more accurate than the pose solved by SfM [3], so the core of the accuracy improvement is the higher rotation accuracy estimated by the decoupling method. The main contribution of the LiGT constraint is computational efficiency. Finally, by comparing the results of CS-VISfM against CS-VISfM-GBE and DRT-l-wo-GBE against DRT-l, we can find that their performance degrades significantly when the visual information is not used to remove the gyroscope bias. This validates the necessity of estimating the gyroscope bias and also illustrates the importance of accurate rotation estimation. For qualitative analysis, we visualize a dataset with the trajectory in Fig. 5. The successful initialization rates and accuracy of DRT outperform the other algorithms, which intuitively illustrates the superiority of our algorithm in different motion modes (e.g., rapid rotation).\\n\\n5.2.2 Robustness evaluation\\n\\nThe robustness experiments are divided into two categories. One is the histogram distribution of the error, the more statistics on small errors, the better the robustness of the system. The other is the proportion of successful initialization on low-latency data segments. For the evaluation of lower latency initialization, the number of keyframes is reduced from 10 KFs to 5 KFs ($\\\\approx 1$ s), so as to reduce image observations and motion excitation. In Fig. 6, we plot the distribution of the percents for the scale error, velocity error, and gravity error metrics. It can be seen that no matter the 10KFs test results in the first row or the 5KFs test results in 745...\"}"}
{"id": "CVPR-2023-50", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Angular velocity and scale error visualizations for the V202 dataset.\\n\\nLeft: Trajectory colored by angular velocity magnitude.\\nRight: Segments of poses colored by scale error magnitude for each initialization window in the dataset (lighter is better). Segments colored black indicate failed initializations for the respective methods.\\n\\nFigure 6. Distribution plots of successful percentages for primary error metrics.\\n\\nFirst row: Results with 10 keyframes.\\nSecond row: Results with 5 keyframes. For each plot, the X axis denotes the threshold for the error metric and the Y axis shows the fraction of initialization sequences with the respective error metric belonging to the threshold boundary on the X axis.\\n\\nThe second row, the number of initialization sequences with DRT-l is the largest in the small-errors range, which demonstrates the robustness of our rotation and translation decoupled method in different scenarios (e.g., fast motion and low latency). The results of CS-VISfM are significantly better than CS-VISfM-GBE, which shows that our rotation-only optimizer plays a corner-stone role in robustness and accuracy. Comparing CS-VISfM-GBE and DRT-t, their performance difference in robustness is still tiny as they are in the accuracy evaluation.\\n\\n5.2.3 Running time evaluation\\n\\nTo show the time-consuming details of each algorithm, the time-consuming of each module is counted separately, such as SfM, gyroscope bias optimizer, velocity and gravity estimator, and point triangulation. Since our method does not need to compute point clouds, in order to be consistent with other methods, we triangulate all observations after initializing the IMU variables. The point cloud triangulation module of the loosely-coupled method refers to scaling all point clouds with the estimated scale factor.\\n\\nThe computation cost (in milliseconds) of different initialization methods for the 10KFs setting is shown in Tab. 3. We can observe that the initialization speed of DRT-l is the fastest, and it only takes 3.89 ms, which is 72 times faster than the tightly-coupled method CS-VISfM and 8 times faster than the loosely-coupled method VINS-Mono. CS-VISfM needs to solve the large dimensional matrix containing the initial variables and the position of the points. The loosely-coupled method uses the SfM module to estimate the visual poses, which is the source of the most time-consuming in the initialization process.\\n\\n6. Conclusion\\n\\nThis paper proposes a rotation and translation decoupled solution for visual-inertial initialization. A new formulation for optimizing gyroscope bias directly using visual observations and inertial information is derived, and a globally optimal solver for initial velocity and gravity vectors without estimating 3D point clouds is proposed. Extensive experiments demonstrate that our method is computationally efficient while achieving significant improvements in accuracy and robustness. However, our method ignores the effect of accelerometer bias, and modeling the accelerometer bias in translation constraints is our future work.\"}"}
{"id": "CVPR-2023-50", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A Rotation-Translation-Decoupled Solution for Robust and Efficient Visual-Inertial Initialization\\n\\nYijia He\\nChinese Academy of Sciences\\nBo Xu\\nWuhan University\\nZhanpeng Ouyang\\nShanghaiTech University\\nHongdong Li\\nAustralian National University\\n\\nAbstract\\nWe propose a novel visual-inertial odometry (VIO) initialization method, which decouples rotation and translation estimation, and achieves higher efficiency and better robustness. Existing loosely-coupled VIO-initialization methods suffer from poor stability of visual structure-from-motion (SfM), whereas those tightly-coupled methods often ignore the gyroscope bias in the closed-form solution, resulting in limited accuracy. Moreover, the aforementioned two classes of methods are computationally expensive, because 3D point clouds need to be reconstructed simultaneously. In contrast, our new method fully combines inertial and visual measurements for both rotational and translational initialization. First, a rotation-only solution is designed for gyroscope bias estimation, which tightly couples the gyroscope and camera observations. Second, the initial velocity and gravity vector are solved with linear translation constraints in a globally optimal fashion and without reconstructing 3D point clouds. Extensive experiments have demonstrated that our method is 8-72 times faster (w.r.t. a 10-frame set) than the state-of-the-art methods, and also presents significantly higher robustness and accuracy. The source code is available at https://github.com/boxuLibrary/drt-vio-init.\\n\\n1. Introduction\\nVisual-inertial odometry (VIO) aims to estimate camera motion and recover 3D scene structure by fusing both image and IMU measurements. The low-cost and compactness of the camera module and IMU sensors make VIO widely used in virtual or augmented reality systems (VR/AR) and various autonomous navigation systems. Currently, most VIO systems track camera motion by minimizing nonlinear visual re-projection errors [14, 30], so the accuracy of the initial value will affect the convergence. In addition, the robustness and lower latency of the initialization are also very important for the downstream application, e.g. AR developers need accurate camera tracking within a few hundred milliseconds after launching VIO, regardless of the use case. For the sensor that has calibrated intrinsic and extrinsic parameters, the initial variables for VIO include the gravity vector, initial velocity, gyroscope and accelerometer biases. Many VIO systems are initialized by setting the initial velocity to zero, then calculating the gravity vector and gyroscope bias with IMU measurements [14,20,36]. However, this method only works when the system is strictly static. For sensors in motion, loosely-coupled and tightly-coupled initialization methods are widely studied. As shown in Fig. 2, the loosely-coupled methods [5,28,30] combine the camera poses estimated by visual SfM and the IMU measurements to estimate the initial state variables. However, visual SfM is prone to inaccuracy or failure when co-viewed.\\n\\nFigure 1. Comparison of computational cost and scale factor errors on EuRoC dataset. Different colors indicate different types of methods. Our proposed initialization method for decoupling rotation and translation (DRT) is accurate and computationally efficient.\"}"}
{"id": "CVPR-2023-50", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2. Comparison between our method and previous VIO initialization methods. Different colored arrows indicate different information flows for VI fusion. Our method takes full advantage of the complementary information between vision and IMU. In contrast, previous loosely-coupled methods do not incorporate IMU information into visual SfM, and previous tightly-coupled methods do not use visual observations to remove gyroscope bias, either of which affects the robustness and accuracy of VIO initialization.\\n\\nFrames are insufficient or the camera rotates rapidly. The motion information measured by IMU is not used to improve the robustness of visual SfM. The tightly-coupled methods [8, 9, 24, 25] firstly use gyroscope measurements and calibrated extrinsic parameters to estimate camera rotation, then use closed-form solution constructed with vision and accelerometer observations to solve for the initial velocity and gravity vector. However, this type of method has poor accuracy on systems equipped with inexpensive and noisy IMU (e.g. cell phones), because no visual observations are used to estimate the gyroscope bias. Moreover, the three-dimensional coordinates of point clouds are obtained with the closed-form solution, resulting in a large and time-consuming solution matrix. Both the above two kinds of methods under-utilize the complementary advantages between visual and inertial sensors, resulting in limited accuracy and robustness.\\n\\nAccording to [17, 18, 26, 38], image observations could be directly used to optimize frame-to-frame rotation and camera poses could be efficiently solved with linear global translation constraints [3]. Inspired by this, we propose a novel rotation-translation-decoupled VIO initialization framework. Gyroscope measurements are directly integrated into the camera rotation estimation, which greatly improves the robustness of initialization, and the translation related initial variables are solved efficiently without estimating the 3D structure. As shown in Fig. 1, our method achieves the lowest scale error and is significantly faster than previous methods. The scale factor error is one of the metrics for evaluating the initialization. Our main contributions are:\\n\\n- We propose a rotation-only solution to directly optimize gyroscope bias using image observations, which can obtain camera rotation more efficiently and more robustly compared to vision-only methods.\\n- We propose a globally optimal solution for estimating the initial velocity and gravity vector based on linear translation constraints. Its linearity and independence of scene structure significantly benefit computational efficiency.\\n- Our proposed initialization framework outperforms the state-of-the-art in both accuracy and robustness on public datasets while being $8 \\\\sim 72$ times faster in calculation time for a 10-frame set. We published our code to facilitate communication.\\n\\n2. Related Work\\n\\nVisual-inertial odometry has been widely studied in terms of reducing time consumption or improving accuracy [7, 21, 22, 37]. A robust and accurate initialization method is indispensable for VIO. Many influential VIO systems [4, 14, 28, 30] have their own designed initialization methods.\\n\\nMartinelli [25] proposed an impressive tightly-coupled closed-form solution that uses tracked visual features and accelerometer measurements to jointly estimate initial state.\"}"}
{"id": "CVPR-2023-50", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"variables and features depth. But the gyroscope is assumed to be unbiased. [16] and [8] demonstrated that the gyroscope bias will significantly affect the accuracy of the closed-form solution, and proposed a method to iteratively optimize the gyroscope bias. Recently, [11] reduces the computational complexity by using the projection matrix to eliminate the variables of the solution matrix, and the gyroscope bias is obtained with a global bundle adjustment optimization. However, these methods of estimating gyroscope bias by minimizing the nonlinear loss function are sensitive to the results of closed-form solutions and are computationally time-consuming.\\n\\nWith the development of visual odometry or SfM [10, 13, 27], loosely-coupled methods for estimating VIO initial variables with high-precision camera trajectories as measurements were naturally proposed [28, 31]. Recently, Campos et al. [5] pointed out that the previous method did not consider the IMU measurement uncertainty, and proposed to use the maximum a posteriori to optimize the initial variables. Zu\u00f1iga-\u02dcNo\u00a8el et al. [40] extended this method to a non-iterative efficient analytical solution.\\n\\nBenefiting from deep learning-based monocular depth estimation [32, 33], Zhou et al. [39] used the learned monocular depth as input to improve the robustness of VIO initialization in scenarios with small parallax and low motion excitation. However, this method is limited by the generalization ability of the learning-based model and the large computational cost of the convolutional network.\\n\\n3. Notations and Preliminaries\\n\\nIn this section, notations are defined and IMU motion model is given. Let $F_c^i$ and $F_b^i$ denote the camera frame and IMU frame at time-index $i$. $T_{b^i b^j}$ to be the Euclidean transformation that take 3D points from IMU frame at time-index $j$ to the one at time-index $i$, which consisted of translation $p_{b^i b^j}$ and rotation $R_{b^i b^j}$. The calibrated extrinsic transformation from $F_b$ to $F_c$ is denoted by $T_{cb}$. $\\\\lfloor \\\\cdot \\\\rfloor \\\\times$ and $\\\\| \\\\cdot \\\\|$ are skew-symmetric operator and Euclidean norm operator, respectively.\\n\\nThe IMU integration follows the standard approach on $SO(3)$ manifold as proposed in [12].\\n\\n$p_{b^1 b^j} = p_{b^1 b^i} + v_{b^1 b^1} \\\\Delta t_{ij} - \\\\frac{1}{2} g_{b^1} \\\\Delta t_{ij}^2 + R_{b^1 b^i} \\\\alpha_{b^i b^j}$ (1)\\n\\n$v_{b^1 b^j} = v_{b^1 b^i} - g_{b^1} \\\\Delta t_{ij} + R_{b^1 b^i} \\\\beta_{b^i b^j}$\\n\\n$R_{b^1 b^j} = R_{b^1 b^i} \\\\gamma_{b^i b^j}$\\n\\nwhere $v_{b^1 b^1}$ and $g_{b^1}$ represent the initial velocity and gravity vector in $F_b$ which are need to be estimated. $\\\\alpha_{b^i b^j}$, $\\\\beta_{b^i b^j}$, $\\\\gamma_{b^i b^j}$ are defined as the pre-integration of translation, velocity, and rotation, respectively. $\\\\Delta t_{ij}$ is the time interval from time $i$ to time $j$.\\n\\n$\\\\alpha_{b^i b^j} = j - 1 \\\\sum_{k=i}^{j} R_{b^i b^f} a_m f \\\\Delta t_{kj} + 1 \\\\frac{1}{2} R_{b^i b^k} a_m k \\\\Delta t_{kj}^2$ (2)\\n\\nNote that the pre-integration formula does not take into account the bias of the measurement. Considering the acceleration bias and the gravity vector are coupled together and cannot be distinguished in a small motion, ignoring the acceleration bias will not greatly affect the initialization results [8, 31]. In this work, the acceleration bias is assumed to be zero, and the effect of gyroscope bias on the $\\\\gamma_{b^i b^j}$ can be represented by a first-order Taylor approximation [12].\\n\\n$\\\\hat{\\\\gamma}_{b^i b^j} = \\\\gamma_{b^i b^j} \\\\exp J_{\\\\gamma_{bi bj}} b_g$ (3)\\n\\nwhere $b_g$ is gyroscope bias which to be estimated, $\\\\hat{\\\\gamma}_{b^i b^j}$ is the updated $\\\\gamma_{b^i b^j}$, $J_{\\\\gamma_{bi bj}} b_g$ is the Jacobian of the derivative of $\\\\gamma_{b^i b^j}$ with respect to $b_g$ and is a constant can be calculated [12].\\n\\n4. Our Initialization Framework\\n\\nAn accurate and robust rotation estimation is crucial for improving the trajectory accuracy of the system since the rotation will affect the accumulation of translation vectors. In this section, we first introduce our method for robust estimation of gyroscope bias using at least two images. Then, we derive two linear solutions for the initial velocity and gravity vector after the rotation is obtained by gyroscope integration.\\n\\n4.1. Gyroscope Bias Optimizer\\n\\nThe rotation between the two cameras can be directly iteratively optimized using the geometric constraints constructed by feature correspondences [17], but this method requires the initial value of the rotation to be close to the ground truth. We extend this method to visual-inertial systems to avoid the above problem and extend it to solving rotations between multiple views.\\n\\nIn this paragraph, we revisit the main idea of directly optimizing frame-to-frame rotation [17]. As shown in Fig. 3,\"}"}
{"id": "CVPR-2023-50", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Geometric relationships of unit feature observation vectors and gyroscope bias. The normal vectors (yellow and blue) perpendicular to the corresponding epipolar plane (yellow and blue) should be coplanar (purple plane), which can form a constraint for solving the relative rotation (gray). The gyroscope bias optimizer converts the value to be solved into gyroscope bias (orange) by the known extrinsic rotation (green).\\n\\nIf a 3D point \\\\( p_1 \\\\) can be observed by two cameras, the two camera centers \\\\( F_{ci} \\\\) and \\\\( F_{cj} \\\\) and the 3D point construct an epipolar plane. Define \\\\( f_{ki} \\\\) and \\\\( f_{kj} \\\\) represent the unit vectors pointing from \\\\( F_{ci} \\\\) and \\\\( F_{cj} \\\\) to \\\\( p_1 \\\\), respectively. The normal vector of the epipolar plane can be calculated by cross product, \\\\( n_k = \\\\langle f_{ki} \\\\rangle \\\\times Rc_{ij} \\\\langle f_{kj} \\\\rangle \\\\). The normal vectors of all epipolar planes will be perpendicular to the translation vector, which means these normal vectors need to be coplanar. Suppose we have \\\\( n \\\\) 3D points observed in two frames, stacking all normal vectors into a matrix \\\\( N = [n_1, \\\\ldots, n_n] \\\\), then coplanarity is algebraically equivalent to the minimum eigenvalue of the matrix \\\\( M = NN^\\\\top \\\\) equal to zero. The final problem of calculating the relative rotation \\\\( Rc_{ij} \\\\) is parameterized as\\n\\n\\\\[\\nR^*_{cij} = \\\\arg \\\\min_{R_{cij}} \\\\lambda_{M_{ij}},\\n\\\\]\\n\\nwhere \\\\( \\\\lambda_{M_{ij}}, \\\\min \\\\) is the smallest eigenvalue of \\\\( M_{ij} \\\\).\\n\\nFor the visual and inertial system with known extrinsic parameters \\\\( R_{bc}, p_{bc} \\\\), the relative motion between cameras can be represented in the IMU body frame:\\n\\n\\\\[\\nR_{cij} = R_{bc}^\\\\top R_{bi} R_{bj} R_{bc},\\n\\\\]\\n\\n\\\\[\\np_{cij} = R_{bc}^\\\\top p_{bi} + R_{bi} R_{bj} p_{bc} - p_{bc},\\n\\\\]\\n\\nwhere \\\\( R_{bi} \\\\) represents the rotation from \\\\( F_{bj} \\\\) to \\\\( F_{bi} \\\\), which can be obtained by integrating the gyroscope measurements between time \\\\( i \\\\) and time \\\\( j \\\\) using Eq. (3). Combining Eq. (3), (4) and (5), \\\\( M_{ij} \\\\) can be represented as a new matrix related to gyroscope information\\n\\n\\\\[\\nM'_{ij} = \\\\sum_{k=1}^n \\\\langle f_{ki} \\\\rangle \\\\times R_{bc}^\\\\top \\\\exp J_{\\\\gamma_{bi} \\\\gamma_{bj} \\\\gamma_{bg} \\\\gamma_{bg}} R_{bc} \\\\langle f_{kj} \\\\rangle^\\\\top\\n\\\\]\\n\\nwhere only \\\\( \\\\gamma_g \\\\) needs to be estimated. Using the properties of the rotation matrix and vector cross product, Eq. (6) can be further simplified to the following form\\n\\n\\\\[\\nM'_{ij} = \\\\sum_{k=1}^n R_{\\\\langle f_{ki}' \\\\rangle \\\\times \\\\exp J_{\\\\gamma_{bi} \\\\gamma_{bj} \\\\gamma_{bg} \\\\gamma_{bg}} \\\\langle f_{kj}' \\\\rangle^\\\\top}\\n\\\\]\\n\\nwhere \\\\( R = R_{bc}^\\\\top \\\\gamma_{bi} \\\\gamma_{bj} \\\\gamma_{bg} \\\\gamma_{bg}, f_{ki}' = R_{\\\\langle f_{ki} \\\\rangle}, \\\\) and \\\\( f_{kj}' = R_{bc} \\\\langle f_{kj} \\\\rangle \\\\).\\n\\nPlease refer to the supplement material Sec.1 for details.\\n\\nLet \\\\( E \\\\) denote the set of keyframe pairs that observe enough common features. Since the gyroscope bias is slowly time-varying, it can be assumed to be a constant during VIO initialization, so any keyframe pair \\\\( (i, j) \\\\in E \\\\) can be used to estimate the gyroscope bias. To fully utilize all visual observations, multiple keyframe pairs are combined to optimize the solution\\n\\n\\\\[\\nb^*_g = \\\\arg \\\\min_{b_g} \\\\lambda \\\\cdot \\\\sum_{(i,j) \\\\in E} \\\\lambda_{M'_{ij}}, \\\\min\\n\\\\]\\n\\nEq. (8) is the cornerstone and one of the main contributions of this paper. To solve Eq. (8), quaternions are used as minimal rotation parameterization, and the Levenberg-Marquardt strategy with automatic differentiation in ceres [1] is used to iteratively optimize the solution [17]. Since the gyroscope bias is small (usually less than 0.1 rad/s), we can set the initial value \\\\( b_g = 0 \\\\) during iterative optimization.\\n\\nIn addition, after solving \\\\( b_g \\\\), we can calculate the rotation matrices between all cameras by integrating the bias-removed gyroscope measurements. Although there are multiple keyframes, only a three-dimensional variable \\\\( b_g \\\\) needs to be solved.\\n\\n4.2. Velocity And Gravity Estimator\\n\\nAfter the rotation is calculated, the initial velocity and gravity vector of the system can be solved efficiently without estimating 3D point clouds. In this section, tightly-coupled and loosely-coupled solvers based on linear translation constraints are presented separately.\"}"}
