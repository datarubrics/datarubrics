{"id": "CVPR-2023-1351", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Nir Aharo, Roy Orfaig, and Ben-Zion Bobrovsky. BoT-SORT: Robust associations multi-pedestrian tracking. arXiv preprint arXiv:2206.14651, 2022.\\n\\n[2] Mykhaylo Andriluka, Stefan Roth, and Bernt Schiele. People-tracking-by-detection and people-detection-by-tracking. In CVPR, pages 1\u20138, 2008.\\n\\n[3] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe. Tracking without bells and whistles. In ICCV, pages 941\u2013951, 2019.\\n\\n[4] Keni Bernardin and Rainer Stiefelhagen. Evaluating multiple object tracking performance: the clear mot metrics. JIVP, 2008:1\u201310, 2008.\\n\\n[5] Alex Bewley, Zongyuan Ge, Lionel Ott, Fabio Ramos, and Ben Upcroft. Simple online and realtime tracking. In ICIP, pages 3464\u20133468, 2016.\\n\\n[6] Guillem Bras\u00f3 and Laura Leal-Taixe. Learning a neural solver for multiple object tracking. In CVPR, pages 6247\u20136257, 2020.\\n\\n[7] Jiarui Cai, Mingze Xu, Wei Li, Yuanjun Xiong, Wei Xia, Zhuowen Tu, and Stefano Soatto. MeMOT: Multi-object tracking with memory. In CVPR, pages 8090\u20138100, 2022.\\n\\n[8] Chenyi Chen, Ari Seff, Alain Kornhauser, and Jianxiong Xiao. DeepDriving: Learning affordance for direct perception in autonomous driving. In ICCV, pages 2722\u20132730, 2015.\\n\\n[9] Patrick Dendorfer, Hamid Rezatofighi, Anton Milan, Javen Shi, Daniel Cremers, Ian Reid, Stefan Roth, Konrad Schindler, and Laura Leal-Taix\u00e9. MOT20: A benchmark for multi object tracking in crowded scenes. arXiv preprint arXiv:2003.09003, 2020.\\n\\n[10] Patrick Dendorfer, Vladimir Yugay, Aljo\u0161a O\u0161ep, and Laura Leal-Taix\u00e9. Quo vadis: Is trajectory forecasting the key towards long-term multi-object tracking? 2022.\\n\\n[11] Xiaohan Ding, Yuchen Guo, Guiguang Ding, and Jungong Han. ACNet: Strengthening the kernel skeletons for powerful cnn via asymmetric convolution blocks. In ICCV, pages 1911\u20131920, 2019.\\n\\n[12] Yunhao Du, Yang Song, Bo Yang, and Yanyun Zhao. StrongSORT: Make deepsort great again. arXiv preprint arXiv:2202.13514, 2022.\\n\\n[13] Yunhao Du, Junfeng Wan, Yanyun Zhao, Binyu Zhang, Zhihang Tong, and Junhao Dong. GIAOTracker: A comprehensive framework for mcmot with global information and optimizing strategies in visdrone 2021. In ICCV, pages 2809\u20132819, 2021.\\n\\n[14] Zheng Ge, Songtao Liu, Feng Wang, Zeming Li, and Jian Sun. YOLOX: Exceeding yolo series in 2021. arXiv preprint arXiv:2107.08430, 2021.\\n\\n[15] Song Guo, Jingya Wang, Xinchao Wang, and Dacheng Tao. Online multiple object tracking with cross-task synergy. In CVPR, pages 8136\u20138145, 2021.\\n\\n[16] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable trajectories with generative adversarial networks. In CVPR, pages 2255\u20132264, 2018.\\n\\n[17] Shoudong Han, Piao Huang, Hongwei Wang, En Yu, Donghaisheng Liu, and Xiaofeng Pan. MAT: Motion-aware multi-object tracking. Neurocomputing, 476:75\u201386, 2022.\\n\\n[18] Jiawei He, Zehao Huang, Naiyan Wang, and Zhaoxiang Zhang. Learnable graph matching: Incorporating graph partitioning with deep feature learning for multiple object tracking. In CVPR, pages 5299\u20135309, 2021.\\n\\n[19] Rudolph Emil Kalman. A new approach to linear filtering and prediction problems. 1960.\\n\\n[20] Tarasha Khurana, Achal Dave, and Deva Ramanan. Detecting invisible people. In ICCV, pages 3174\u20133184, 2021.\\n\\n[21] Chanho Kim, Fuxin Li, Arridhana Ciptadi, and James M Rehg. Multiple hypothesis tracking revisited. In CVPR, pages 4696\u20134704, 2015.\\n\\n[22] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.\\n\\n[23] Chao Liang, Zhipeng Zhang, Xue Zhou, Bing Li, and Weiming Hu. One more check: making \u201cfake background\u201d be tracked again. In AAAI, pages 1546\u20131554, 2022.\\n\\n[24] Chao Liang, Zhipeng Zhang, Xue Zhou, Bing Li, Shuyuan Zhu, and Weiming Hu. Rethinking the competition between detection and reid in multiobject tracking. IEEE T-IP, 31:3182\u20133196, 2022.\\n\\n[25] Jonathon Luiten, Aljosa Osep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taix\u00e9, and Bastian Leibe. HOTA: A higher order metric for evaluating multi-object tracking. IJCV, 129(2):548\u2013578, 2021.\\n\\n[26] Tim Meinhardt, Alexander Kirillov, Laura Leal-Taixe, and Christoph Feichtenhofer. TrackFormer: Multi-object tracking with transformers. In CVPR, pages 8844\u20138854, 2022.\\n\\n[27] Anton Milan, Laura Leal-Taixe, Ian Reid, Stefan Roth, and Konrad Schindler. MOT16: A benchmark for multi-object tracking. arXiv preprint arXiv:1603.00831, 2016.\\n\\n[28] James Munkres. Algorithms for the assignment and transportation problems. Journal of the Society for Industrial and Applied Mathematics, 5(1):32\u201338, 1957.\\n\\n[29] Sangmin Oh, Anthony Hoogs, Amitha Perera, Naresh Cuntoor, Chia-Chih Chen, Jong Taek Lee, Saurajit Mukherjee, JK Aggarwal, Hyungtae Lee, Larry Davis, et al. A large-scale benchmark dataset for event recognition in surveillance video. In CVPR, pages 3153\u20133160, 2011.\\n\\n[30] Jiangmiao Pang, Linlu Qiu, Xia Li, Haofeng Chen, Qi Li, Trevor Darrell, and Fisher Yu. Quasi-dense similarity learning for multiple object tracking. In CVPR, pages 164\u2013173, 2021.\\n\\n[31] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An imperative style, high-performance deep learning library. In NeurIPS, pages 8026\u20138037, 2019.\\n\\n[32] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NIPS, pages 91\u201399, 2015.\"}"}
{"id": "CVPR-2023-1351", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"multi-target, multi-camera tracking. In ECCV, pages 17\u201335, 2016.\\n\\n[34] Fatemeh Saleh, Sadegh Aliakbarian, Hamid Rezatofighi, Mathieu Salzmann, and Stephen Gould. Probabilistic tracklet scoring and inpainting for multiple object tracking. In CVPR, pages 14329\u201314339, 2021.\\n\\n[35] Liushuai Shi, Le Wang, Chengjiang Long, Sanping Zhou, Mo Zhou, Zhenxing Niu, and Gang Hua. Sgcn: Sparse graph convolution network for pedestrian trajectory prediction. In CVPR, pages 8994\u20139003, 2021.\\n\\n[36] Daniel Stadler and J\u00fcrgen Beyerer. Modelling ambiguous assignments for multi-person tracking in crowds. In WACV, pages 133\u2013142, 2022.\\n\\n[37] Pavel Tokmakov, Jie Li, Wolfram Burgard, and Adrien Gaidon. Learning to track with object permanence. In ICCV, pages 10860\u201310869, 2021.\\n\\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, pages 6000\u20136010, 2017.\\n\\n[39] Xingyu Wan, Jiakai Cao, Sanping Zhou, Jinjun Wang, and Nanning Zheng. Tracking beyond detection: learning a global response map for end-to-end multi-object tracking. IEEE T-IP, 30:8222\u20138235, 2021.\\n\\n[40] Xingyu Wan, Sanping Zhou, Jinjun Wang, and Rongye Meng. Multiple object tracking by trajectory map regression with temporal priors embedding. In ACM MM, pages 1377\u20131386, 2021.\\n\\n[41] Qiang Wang, Yun Zheng, Pan Pan, and Yinghui Xu. Multiple object tracking with correlation learning. In CVPR, pages 3876\u20133886, 2021.\\n\\n[42] Qiang Wang, Yun Zheng, Pan Pan, and Yinghui Xu. Multiple object tracking with correlation learning. In CVPR, pages 3876\u20133886, 2021.\\n\\n[43] Zhongdao Wang, Liang Zheng, Yixuan Liu, Yali Li, and Shengjin Wang. Towards real-time multi-object tracking. In ECCV, pages 107\u2013122, 2020.\\n\\n[44] Nicolai Wojke, Alex Bewley, and Dietrich Paulus. Simple online and realtime tracking with a deep association metric. In ICIP, pages 3645\u20133649, 2017.\\n\\n[45] Fan Yang, Xin Chang, Sakriani Sakti, Yang Wu, and Satoshi Nakamura. ReMOT: A model-agnostic refinement for multiple object tracking. Image Vis. Comput., 106:104091, 2021.\\n\\n[46] En Yu, Zhuoling Li, and Shoudong Han. Towards discriminative representation: Multi-view trajectory contrastive learning for online multi-object tracking. In CVPR, pages 8834\u20138843, 2022.\\n\\n[47] En Yu, Zhuoling Li, and Shoudong Han. Towards discriminative representation: Multi-view trajectory contrastive learning for online multi-object tracking. In CVPR, pages 8834\u20138843, 2022.\\n\\n[48] En Yu, Zhuoling Li, Shoudong Han, and Hongwei Wang. RelationTrack: Relation-aware multiple object tracking with decoupled representation. IEEE T-MM, 2022.\\n\\n[49] Jiahui Yu, Yuning Jiang, Zhangyang Wang, Zhimin Cao, and Thomas Huang. UnitBox: An advanced object detection network. In ACM MM, pages 516\u2013520, 2016.\\n\\n[50] Fangao Zeng, Bin Dong, Tiancai Wang, Xiangyu Zhang, and Yichen Wei. Motr: End-to-end multiple-object tracking with transformer. arXiv preprint arXiv:2105.03247, 2021.\\n\\n[51] Jimuyang Zhang, Sanping Zhou, Xin Chang, Fangbin Wan, Jinjun Wang, Yang Wu, and Dong Huang. Multiple object tracking by flowing and fusing. arXiv preprint arXiv:2001.11180, 2020.\\n\\n[52] Yifu Zhang, Peize Sun, Yi Jiang, Dongdong Yu, Fucheng Weng, Zehuan Yuan, Ping Luo, Wenyu Liu, and Xinggang Wang. ByteTrack: Multi-object tracking by associating every detection box. In ECCV, pages 1\u201321, 2022.\\n\\n[53] Yifu Zhang, Chunyu Wang, Xinggang Wang, Wenjun Zeng, and Wenyu Liu. Fairmot: On the fairness of detection and re-identification in multiple object tracking. IJCV, 129(11):3069\u20133087, 2021.\\n\\n[54] Zelin Zhao, Ze Wu, Yueqing Zhuang, Boxun Li, and Jiaya Jia. Tracking objects as pixel-wise distributions. In ECCV, pages 76\u201394, 2022.\\n\\n[55] Linyu Zheng, Ming Tang, Yingying Chen, Guibo Zhu, Jinqiao Wang, and Hanqing Lu. Improving multiple object tracking with single object tracking. In CVPR, pages 2453\u20132462, 2021.\\n\\n[56] Sanping Zhou, Fei Wang, Zeyi Huang, and Jinjun Wang. Discriminative feature learning with consistent attention regularization for person re-identification. In ICCV, pages 8040\u20138049, 2019.\\n\\n[57] Sanping Zhou, Jinjun Wang, Deyu Meng, Yudong Liang, Yihong Gong, and Nanning Zheng. Discriminative feature learning with foreground attention for person re-identification. IEEE T-IP, 28:4671\u20134684, 2019.\\n\\n[58] Sanping Zhou, Jinjun Wang, Jiayun Wang, Yihong Gong, and Nanning Zheng. Point to set similarity based deep feature learning for person re-identification. In CVPR, pages 3741\u20133750, 2017.\\n\\n[59] Xingyi Zhou, Vladlen Koltun, and Philipp Kr\u00e4henb\u00fchl. Tracking objects as points. In ECCV, pages 474\u2013490, 2020.\"}"}
{"id": "CVPR-2023-1351", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Illustration of Correlation Calculation. We compute a correlation matrix to represent the association probabilities between lost tracklets and detections.\\n\\n\u03be \u2208 [0, 1] is a threshold. Finally, we normalize all the non-zero elements in A_{adjc}.\\n\\nMotion Prediction. After extracting the interaction matrix between tracklets, we use a graph convolution [22] to fuse the interactions for each tracklet and get the prediction using a multi-layer perceptron (MLP):\\n\\nP_{offs} = MLP_\u03b4 \u03d5 (A_{adjc} \u00b7 O_{t}, W_{G}), (4)\\n\\nwhere W_{G} is the weight of the linear transformation. The prediction P_{offs} \u2208 \\\\mathbb{R}^{M \\\\times 4} will be used with D_t for IoU-based association.\\n\\n3.4. Refind Module\\n\\nTo refind the lost tracklet, we first identify its matched detection in the correlation calculation step and then refine the occluded trajectory in the error compensation step.\\n\\nCorrelation Calculation. After IoU-based association, there are U unmatched detections D_{rest} \u2208 \\\\mathbb{R}^{U \\\\times 5}, and S lost tracklets T_{lost} \u2208 \\\\mathbb{R}^{S \\\\times 30 \\\\times 5}, in which we record the last thirty alive locations for each lost tracklet. D_{rest} and T_{lost} include both time and locations, i.e., (t, x, y, w, h), and they are the inputs to the Refind Module. As shown in Figure 4, we first normalize D_{rest} and T_{lost} in the last dimension and then extract features from them separately. We apply the asymmetric convolution to T_{lost} in the second and third dimensions, respectively, and then pool them into feature vectors F_{traj} \u2208 \\\\mathbb{R}^{S \\\\times D_t}:\\n\\nT_l = \u03b4 (conv(T_{l-1}, K_{\u03ba \\\\times 1})),\\nF_{traj} = pool (\u03b4 (conv(T_L, K_{1 \\\\times \u03ba}))), (5)\\n\\nwhere T_0 is initialized as T_{lost} and there are L convolution layers. We calculate the difference between the detection and the last alive location, and concatenate it with the detection as \u0302D_{rest} \u2208 \\\\mathbb{R}^{U \\\\times 10}. Then, we map it to high-dimensional features F_{dete} \u2208 \\\\mathbb{R}^{U \\\\times D} as follows:\\n\\nF_{dete} = \u03d5 ( \u0302D_{rest}, W_{D}), (6)\\n\\nwhere W_{D} is the weight of the linear transformation. Afterward, we combine F_{traj} and F_{dete} into a feature matrix F \u2208 \\\\mathbb{R}^{(S \\\\times U) \\\\times 2D}, which models the spatial distribution pattern and velocity-time correlation, etc. A fully connected layer and a sigmoid function are then applied to yield the correlation score. Here, we obtain the correlation matrix C_{corr} \u2208 \\\\mathbb{R}^{S \\\\times U} reflecting the association probabilities between lost tracklets and unmatched detections. Finally, we use the greedy algorithm to pick the matched pairs with high correlation score and initialize the remaining unmatched detections as new tracklets.\\n\\nError Compensation. After re-identifying the lost tracklet with its matched detection, we need to fill the trajectory during the long-time occlusion. Unlike other interpolation methods, we correct the predicted trajectory instead of generating a new one. We use the error between the matched detection d_t and the prediction p_t of the lost tracklet to infer the errors during occlusion and refine the prediction:\\n\\nd_t = p_t + d_t \u2212 p_t, t_1 < t < t_2, (7)\\n\\nwhere the tracklet becomes lost after frame t_1 and is refound at frame t_2. When the tracklet is occluded, it still exists and interacts with other tracklets, which can provide some information to support the prediction of the lost one. Related experiments are in the supplementary material.\\n\\n3.5. Training\\n\\nThe output of the Interaction Module is a set of offsets P_{offs}. We convert it to location coordinates P_{coor} based on location in the previous frame. Each coordinate in P_{coor} is composed of four values (x, y, w, h), which are not independent. These four variables affect each other, so they should not be supervised individually but combined to drive the training of the Interaction Module. Inspired by the training process of the detector [14], we employ the IoU loss [49] to supervise the Interaction Module as follows:\\n\\nL_{INTR} = 1 \u2212 IoU(P_{coor}, P_{gt}), (8)\\n\\nwhere IoU denotes the Intersection over Union. We take three consecutive frames as a training sample. The offset between the first two frames is taken as input, and the last frame is used as supervision P_{gt}.\\n\\nFor the training of Refind Module, we extract the data samples and labels from the tracking dataset. We first extract all the trajectories in a complete video, and then randomly couple them in pairs, each being a training set. For each training set, we sample the tracklet and detection, and label positive or negative by whether they are from the same trajectory. Then, we supervise the correlation calculation in Refind Module with a binary cross-entropy loss function:\\n\\nL_{CORR} = 1/n \\\\sum_i (-y_i \\\\log(c_i) + (1 \u2212 y_i) \\\\log(1 \u2212 c_i)), (9)\\n\\nwhere c_i is the predicted correlation score.\"}"}
{"id": "CVPR-2023-1351", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison with the state-of-the-art methods under the \\\"private detector\\\" protocol on the MOT17 test set.\\n\\n| Method         | Protocol | MOT17 | MOT20 |\\n|----------------|----------|-------|-------|\\n| ReMOT [45]     | IVC'21   | 72.0  | 77.0  |\\n| QuasiDense [30] | CVPR'21  | 66.3  | 68.7  |\\n| SOTMOT [55]    | CVPR'21  | 71.9  | 71.0  |\\n| SiamMOT [23]   | CVPR'21  | 72.3  | 76.3  |\\n| CorrTracker [42] | CVPR'21 | 73.6  | 76.5  |\\n| PermaTrackPr [37] | ICCV'21 | 68.9  | 73.8  |\\n| FairMOT [53]   | IJCV'21  | 72.3  | 73.7  |\\n| CSTrack [24]   | TIP'22   | 72.6  | 74.9  |\\n| RelationTrack [48] | TMM'22 | 74.7  | 73.8  |\\n| TrackFormer [26] | CVPR'22 | 68.0  | 74.1  |\\n| MeMOT [7]      | CVPR'22  | 69.0  | 72.5  |\\n| MTrack [46]    | CVPR'22  | 73.5  | 72.1  |\\n| MOTR [50]      | ECCV'22  | 68.6  | 73.4  |\\n| ByteTrack [52] | ECCV'22  | 77.3  | 80.3  |\\n| P3AFormer (+W&B) [54] | ECCV'22 | 78.1 | 81.2 |\\n| MotionTrack(ours) | -       | 80.1  | 81.1  |\\n\\n\u2191 means higher is better, \u2193 means lower is better. The best results for each metric are bolded.\\n\\nwhere $c_i$ denotes the predicted correlation score, and $y_i$ indicates the ground truth correlation label, in which 1 and 0 represent the positive and negative correlation, respectively.\\n\\n4. Experiments\\n4.1. Setting\\nDatasets. We evaluate our MotionTrack under the \\\"private detection\\\" protocol on the MOT17 [27] and MOT20 [9] datasets. For fair comparisons, we directly apply the publicly available detector of YOLOX [14], trained by ByteTrack [52] on MOT17 and MOT20. For the training of the Interaction Module and Refind Module, we use only half the training set of MOT17 and MOT20.\\n\\nMetrics. We employ CLEAR metrics (MOTA, FP, FN, IDs, etc.), IDF1 [33], and HOTA [25] to evaluate different aspects of tracking performance. In particular, IDF1 focuses more on association performance, MOTA is computed based on FP, FN, and IDs, which mainly rely on the detection performance because the number of FPs and FNs is larger than IDs. HOTA is a unified metric that balances the effectiveness of detection and association.\\n\\nImplementation Details. We implemented our MotionTrack in PyTorch [31], and performed all experiments on one NVIDIA GeForce RTX 3090 Ti GPU. For fair comparisons, we directly apply the publicly available detector of YOLOX [14], trained by ByteTrack [52] for MOT17, MOT20. For the Interaction Module, the threshold used by the signal function is set to 0.6. For Refind Module, pairs with correlation scores less than 0.9 were rejected. For the tracking process, we make full use of all detections with double matching, following [52]. The default high and low thresholds are 0.6 and 0.1, respectively. Unless otherwise specified, the new tracklet initialization score is 0.7. In the IoU-based association, we reject the matching if the IoU score is smaller than 0.2, and we use global motion compensation for steadier tracking. For the lost tracklets, we keep 60 frames for MOT17 and 120 frames for MOT20, respectively.\\n\\n4.2. Comparison with the State-of-the-Art Methods\\nMOT17. As shown in Table 1, our MotionTrack outperforms the state-of-the-art methods in most key metrics, i.e., ranks first for metrics IDF1, HOTA, AssA, DetA, IDs, Frag and ranks second for MOTA. Our approach focuses on solving dense crowds and extreme occlusion to enable the tracker with a more robust ability for identity preservation over a short to long range. Consequently, it produces more accurate associations and vastly outperforms the second-performance tracker in metrics reflecting the association ability (i.e., +2.0 IDF1 and +3.1 AssA). It should be pointed out that P3AFormer is based on segmentation, while our MotionTrack is based on detection. Even in this situation, we are only 0.1 lower than P3AFormer in MOTA, but IDF1 surpasses it by 2.0. We are the highest on HOTA, demonstrating the robustness and comprehensive tracking capability of our MotionTrack. For the IDs metric, we are 40% less than P3AFormer, which shows the strong ability of our association component.\\n\\nMOT20. As shown in Table 2, our MotionTrack still achieves state-of-the-art results on the MOT20 dataset. Even though ByteTrack uses duplicate detections as our method, our MotionTrack has made significant progress in several core metrics (i.e., +1.3 IDF1, +0.2 MOTA, and +1.5 HOTA). Besides, we still achieve the highest HOTA in this more challenging scenario. Moreover, our MotionTrack achieves the least IDs, which is 13% less than the second P3AFormer. The underlying reason is that we infer the occluded trajectory in the Refind Module, which is able to inference.\\n\\n1 We ranked second among all methods on the official MOT Challenge evaluation server and first among all online methods.\"}"}
{"id": "CVPR-2023-1351", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2. Comparison with the state-of-the-art methods under the \u201cprivate detector\u201d protocol on the MOT20 test set.\\n\\n| Setting       | IDF1 | MOTA | HOTA | AssA | DetA | IDs | Frag |\\n|---------------|------|------|------|------|------|-----|------|\\n| FairMOT [53]  | 67.3 | 61.8 | 54.6 | 54.7 | 54.7 | 103440 | 88901 |\\n| CorrTracker [42] | 69.1 | 65.2 | -    | -    | -    | 79429 | 95855 |\\n| SiamMOT [23]  | 69.1 | 67.1 | -    | -    | -    | -   | -    |\\n| SOTMOT [55]   | 71.4 | 68.6 | 57.4 | 57.3 | 57.7 | 57064 | 101154 |\\n| CSTrack [24]  | 68.6 | 66.6 | 54.0 | 50.0 | 54.2 | 25404 | 144358 |\\n| RelationTrack [48] | 70.5 | 67.2 | 56.5 | 56.4 | 56.8 | 61134 | 104597 |\\n| MeMOT [7]     | 66.1 | 63.7 | 54.1 | 55.0 | -    | 47882 | 137982 |\\n| MTrack [46]   | 69.2 | 63.5 | -    | -    | -    | 96123 | 86964 |\\n| ByteTrack [52] | 75.2 | 77.8 | 61.3 | 59.6 | 63.4 | 26249 | 87594 |\\n| P3AFormer(+W&B) [54] | 76.4 | 78.1 | -    | -    | -    | 25413 | 86510 |\\n| MotionTrack (ours) | 76.5 | 78.0 | 62.8 | 61.8 | 64.0 | 28629 | 84152 |\\n\\n\u2191 means higher is better, \u2193 means lower is better. The best results for each metric are bolded.\\n\\n### Table 3. Ablation studies on Interaction Module (I) and Refind Module (R) on the MOT17 validation set.\\n\\n| Setting       | # | IDF1 | MOTA | HOTA | AssA | DetA |\\n|---------------|---|------|------|------|------|------|\\n| Baseline      | 30 | 80.9 | 79.8 | 69.0 | 70.6 | 68.1 |\\n| Baseline+I    | 120| 80.1 | 77.6 | 68.4 | 70.5 | 66.9 |\\n| Baseline+I+R  | 30 | 77.2 | 77.0 | 66.4 | 67.1 | 66.3 |\\n| IoU-based     | 120| 70.4 | 67.5 | 60.6 | 60.3 | 61.6 |\\n| ReID-based    | 30 | 82.6 | 80.4 | 70.2 | 72.4 | 68.7 |\\n| Ours          | 120| 83.3 | 80.7 | 70.7 | 73.2 | 68.8 |\\n\\n\u2206: Increases and decreases in metrics are marked in green and red, respectively.\\n\\n### Table 4. Comparison with other methods for handling occlusions on the MOT17 validation set. We raise the upper limit of occlusion time from 30 to 120 frames to reflect the ability to deal with long-term occlusion. Increases and decreases in metrics are marked in green and red, respectively.\\n\\n| Setting       | \u2a7e20 | \u2a7e40 | \u2a7e60 | \u2a7e80 | \u2a7e100 |\\n|---------------|-----|-----|-----|-----|------|\\n| Baseline      | 77.2 | 73.6 | 75.2 | 73.3 | 71.5 |\\n| Ours          | 78.3 | 75.1 | 76.8 | 74.1 | 72.4 |\\n\\nImprovement: +1.1 +1.5 +1.6 +0.8 +0.9\\n\\n### Table 5. Evaluation of MOTA for crowd and occlusion cases on the MOT17 validation set. We set visibility <0.25 and define the minimum time constants for occlusion or crowds as 20 to 100, respectively. Increases and decreases in metrics are marked in green and red, respectively.\\n\\n| Setting       | \u2a7e20 | \u2a7e40 | \u2a7e60 | \u2a7e80 | \u2a7e100 |\\n|---------------|-----|-----|-----|-----|------|\\n| Baseline      | 77.2 | 73.6 | 75.2 | 73.3 | 71.5 |\\n| Ours          | 78.3 | 75.1 | 76.8 | 74.1 | 72.4 |\\n\\nImprovement: +1.1 +1.5 +1.6 +0.8 +0.9\\n\\n### Analysis of Interaction Module.\\n\\nThe IoU-based trackers predict the location of tracklets in the next frame, and then compute the IoU between the detection boxes and the predicted boxes to conduct data association. These methods heavily rely on accurate predictions to maintain a high-quality data association. As shown in Figure 5, we compare the prediction accuracy, in which we take the average IoU of all targets to count the total IoU score. The results show that the introduction of interaction yields more accurate predictions than the traditional Kalman Filter. Meanwhile, steady improvements in IDF1 and AssA indicate that Interaction Module improves prediction accuracy and leads to stronger association capability.\\n\\n### Advantage of Refind Module.\\n\\nIn practice, the IoU-based and Re-ID-based approaches are often taken to deal with \\\\[17945\\\\] occlusion cases. Our Refind Module can improve all the metrics, indicating that the learned history trajectory-based motions are effective for the long-range data association problem.\"}"}
{"id": "CVPR-2023-1351", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the occlusion problem, while our method learns the discriminative motion patterns between the history trajectories and current detection to address the extreme occlusion problem. As shown in Table 4, we compare the classical approach [44, 52] with our method in terms of its ability to handle long-term occlusion. The results show that the first two methods cause dramatic performance decreases due to a large number of incorrect associations, while our approach method achieves a consistent improvement in dealing the long-term occlusion.\\n\\nExtra Evaluation of Crowd and Occlusion. Extreme occlusion and dense crowds are not always present in the tracking scene. Therefore evaluating the entire dataset cannot reflect our ability to directly solve the two challenging problems. To address this issue, we propose to modify MOTA to crowdMOTA, which separates people in the dataset who are likely to be occluded or crowded, and evaluate the tracking performance of these people. In particular, we select people by visibility label in the dataset, i.e., considering people being crowded or occluded when their visibility is below 0.25. We set the minimum number of frames from 20 to 100 for continuous occlusion or crowding to validate the solution for samples with different difficulty levels. As shown in Table 5, the improvement in samples of different difficulties illustrates the effectiveness of our method in solving dense crowds and extreme occlusion.\\n\\n4.4. Visualization\\n\\nVisualization of Directed Interaction. The directed interaction is visualized in Figure 6, from which we find that our method possesses the ability to capture effective interaction in different scenarios. In particular, (a) shows that the stride forward movement pattern of the pedestrian is affected by the pedestrians coming towards him. In (b), a cyclist walking toward the left will affect two oncoming pedestrians. As shown in (c), even if the tracklet is in the LOST state, he is still influenced by the pedestrians around him. Although he cannot be seen, he is still actually in the crowd. As a result, considering the interaction of nearby people helps describe the movement of the target during occlusion.\\n\\nVisualization of Refinding Targets. As shown in Figure 7, two cases of refinding targets are given in the red box. When the target is occluded, our Interaction Module still iteratively infers its location. Therefore, our Refind Module can accurately re-identify the lost targets through correlation calculation and refine the predicted trajectory by error compensation.\\n\\n5. Conclusion\\n\\nWe propose MotionTrack for online MOT, which introduces an Interaction Module and Refind Module to address the short-term and long-term association problems in MOT. Our results on the MOT benchmark datasets have shown the benefits of our method. The application of interactions can lead to more accurate location prediction, yielding more robust data association. Even though the targets have been occluded for a long period, they can still be refound by learning discriminative motion patterns between the history trajectory and current detection. Notably, without using any complex components, such as person Re-ID, our tracker achieves state-of-the-art performance.\\n\\nLimitation and Future Work. One major limitation of our method is that we only considered the motion patterns and relationships between pedestrians while ignoring the drivable information in interaction, which may weaken its performance in motion prediction. In the future, we plan to explore this prior information in our MotionTrack to further improve the tracking performance. Besides, two modules we proposed can be further combined to support each other for achieving better results.\\n\\nAcknowledgement\\n\\nThis work was supported in part by National Key R&D Program of China under Grant 2021YFB1714700, NSFC under Grants 62088102 and 62106192, Natural Science Foundation of Shaanxi Province under Grants 2022JC-41, China Postdoctoral Science Foundation under Grant 2022T150518, and Fundamental Research Funds for the Central Universities under Grants XTR042021005 and XTR072022001.\"}"}
{"id": "CVPR-2023-1351", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MotionTrack: Learning Robust Short-term and Long-term Motions for Multi-Object Tracking\\n\\nZheng Qin\u2020\\nSanping Zhou\u2020\\nLe Wang\u2217\\nJinghai Duan\\nGang Hua\\nWei Tang\\n\\n1National Key Laboratory of Human-Machine Hybrid Augmented Intelligence, National Engineering Research Center for Visual Information and Applications, Institute of Artificial Intelligence and Robotics, Xi'an Jiaotong University\\n2School of Software Engineering, Xi'an Jiaotong University\\n3Wormpex AI Research\\n4University of Illinois at Chicago\\n\\nAbstract\\n\\nThe main challenge of Multi-Object Tracking (MOT) lies in maintaining a continuous trajectory for each target. Existing methods often learn reliable motion patterns to match the same target between adjacent frames and discriminative appearance features to re-identify the lost targets after a long period. However, the reliability of motion prediction and the discriminability of appearances can be easily hurt by dense crowds and extreme occlusions in the tracking process. In this paper, we propose a simple yet effective multi-object tracker, i.e., MotionTrack, which learns robust short-term and long-term motions in a unified framework to associate trajectories from a short to long range. For dense crowds, we design a novel Interaction Module to learn interaction-aware motions from short-term trajectories, which can estimate the complex movement of each target. For extreme occlusions, we build a novel Refind Module to learn reliable long-term motions from the target's history trajectory, which can link the interrupted trajectory with its corresponding detection. Our Interaction Module and Refind Module are embedded in the well-known tracking-by-detection paradigm, which can work in tandem to maintain superior performance. Extensive experimental results on MOT17 and MOT20 datasets demonstrate the superiority of our approach in challenging scenarios, and it achieves state-of-the-art performances at various MOT metrics. Code is available at https://github.com/qwomeng/MotionTrack.\\n\\n1. Introduction\\n\\nMulti-Object Tracking (MOT) is a fundamental task in computer vision, which has a wide range of applications, such as autonomous driving [8] and intelligent surveillance [29]. It aims at jointly locating targets through bounding boxes and recognizing their identities throughout a whole video [40]. Though great progress has been made in the past few years, MOT still remains a challenging task due to the dynamic environment, such as dense crowds and extreme occlusions, in the tracking scenario.\\n\\nIn general, the existing MOT methods either follow the tracking-by-detection [2] or tracking-by-regression [39, 40, 59], paradigm. The former methods first detect objects in each video frame and then associate detections between adjacent frames to create individual object tracks over time. The latter methods conduct tracking differently: the object detector not only provides frame-wise detections but also replaces the data association with a continuous regression of each tracklet to its new position. Regardless of the\\n\\nTarget Tracked trajectory\\nOccluded trajectory\\nVideo sequence\\n(b)\\n(a)\\nFigure 1. Illustration of challenging scenarios in different videos. (a) Dense crowds. Pedestrians do not move independently in this situation. They will be affected by their surrounding neighbors to avoid collisions which will make their motion patterns hard to learn in practice. (b) Extreme occlusion. Pedestrians are easily occluded by fixed facilities for a long period, such as billboard and sunshade, in which the dynamic environment will make them undergone a large appearance variation.\"}"}
{"id": "CVPR-2023-1351", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"paradigm, all methods need to address the short-range and long-range association problems, i.e., how to associate the alive tracklets with detections in a short time, and how to re-identify the lost tracklets with detections after a long period.\\n\\nFor the short-range association problem, discriminative motion patterns and appearance features [5, 44] are often learned to conduct data association between adjacent frames. However, as shown in Figure 1 (a), it is tough to learn discriminative representations in the dense crowd scenario. On the one hand, the bounding boxes of detections are too small to be distinguished by their appearances. On the other hand, different targets need to plan suitable paths to avoid collisions, which makes the resulting motions very complex in the tracking process. For the long-range association problem, prior works [24, 43, 44] usually learn discriminative appearance features to re-identify the lost targets after long occlusion [56\u201358]. As shown in Figure 1 (b), the main bottleneck of these methods is how to keep the robustness of features against different poses, low resolution, and poor illumination for the same target. To alleviate this issue, the memory technology [7, 46] is widely applied to store diverse features for each target to match different targets in a multi-query manner. Moreover, a lot of memories and time will be consumed by the memory module and multi-query regime, unfriendly to real-time tracking.\\n\\nIn this paper, we propose a simple yet effective object tracker, i.e., MotionTrack, to address the short-range and long-range association problems in MOT. In particular, our MotionTrack follows the tracking-by-detection paradigm, in which both interaction-aware and history trajectory-based motions are learned to associate trajectories from a short to long range. To deal with the short-range association problem, we design a novel Interaction Module to model all interactions between targets, which can predict their complex motions to avoid collisions. The Interaction Module uses an asymmetric adjacency matrix to represent the interaction between targets, and obtains the prediction after the information fusion by a graph convolution network. Thanks to the captured target interaction, those short-term occluded targets can be successfully tracked in dense crowds. To deal with the long-range association problem, we design a novel Refind Module based on the history trajectory of each target. It can effectively re-identify the lost targets through two steps: correlation calculation and error compensation. For the lost tracklets and the unmatched detections, the correlation calculation step takes the features of history trajectories and current detections as input, and computes a correlation matrix to represent the possibility that they are associated. Afterward, the error compensation step is further taken to revise the occluded trajectories. Extensive experiments on two benchmark datasets (MOT17 and MOT20) demonstrate that our proposed MotionTrack outperforms the previous state-of-the-art methods.\\n\\nThe main contribution of this work can be highlighted as follows: (1) We propose a simple yet effective multi-object tracker, MotionTrack, to address the short-range and long-range association problems; (2) We design a novel Interaction Module to model the interaction between targets, which can handle complex motions in dense crowds; (3) We design a novel Refind Module to learn discriminative motion patterns, which can re-identify the lost tracklets with current detections.\\n\\n2. Related Work\\n\\nTracking-by-Detection. The recent advancement of object detection brings remarkable improvement to the tracking-by-detection paradigm [2]. In this framework, an existing object detector [14, 32] generates detections in the current frame, then a matching algorithm, e.g., the Hungarian algorithm [28], builds tracklets by associating the detections across different frames. Many efforts have been made in different aspects to improve the effectiveness of this paradigm. For example, SORT [5] adds the Kalman Filter (KF) [19] to approximate the inter-frame displacements, while other models [30, 41, 44, 47] focus on distinguishing appearance features to improve the matching accuracy. Some other works [6, 18, 21] formulate data association as a graph optimization problem by considering each detection as a graph node. For example, ByteTrack [52] enhances tracking performance by fully using detections. We follow this tracking-by-detection paradigm and propose a novel framework to extract interaction-aware and history trajectory-based motions for more accurate short and long range association.\\n\\nMotion Models. The motion models can be divided into filter-based and model-based methods in MOT. The filter-based methods mainly consider motion prediction as state estimation. For example, the well-known SORT [5] introduces KF [19] into MOT as a linear constant velocity model based on the assumption of independence across the objects and the camera motion, which inspires a series of works [17, 20, 53] to improve the motion prediction in different aspects. Later, many works [1, 3, 12, 36] consider the camera motion compensation of the tracker for robust tracking. Meanwhile, some works [12, 13] using KF variants to further improve prediction accuracy.\\n\\nRecently, model-based methods combine motion and visual information to provide better predictions based on a data-driven paradigm. For example, Tracktor [3] adopts the regression part from Faster R-CNN [32] to predict the displacement of targets between adjacent frames. FFT [51] further adds optical flow to help regress the displacement. Besides, CenterTrack [59] builds a tracking branch to predict the motion specifically. ArTIST [34] considers motion as a probability distribution and implicitly models all\"}"}
{"id": "CVPR-2023-1351", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Interaction extraction\\nMotion prediction\\nIoU-based association\\nOffsets\\nDetection\\nCorrelation calculation\\n\\n0.1 ...\\n\\n\u017d'\u2022\u2013\u0b35 \u017d'\u2022\u2013\u0b36 \u0b35 \u0b36\\n\\nPrediction\\nTracked target\\nLost target\\nDetection\\nInteraction Module\\nRefind Module\\nUnmatched detection\\nLost tracklets\\nShort-range association\\nCorrelation matrix\\n.adj\\n\\nFigure 2.\\nOverview of our MotionTrack. The Interaction Module captures the directional interaction relationship between tracklets, then fuses the interaction information and predicts the location in the next frame for the short-range association. The Refind Module analyzes the correspondence between unmatched lost tracklets and detections by correlation calculation, then the matched pairs are further chosen to complete the long-range association via an additional error compensation. Finally, the short-range and long-range associations are combined to generate complete tracking results.\\n\\nSurrounding interactions with max-pooling to one feature. However, most motion models do not consider explicit interactions between targets, especially in dense crowd scenes. As a result, they cannot accurately estimate the complex movements of nearby targets.\\n\\nOcclusion. How to deal with occlusions has been a long-standing challenge in MOT. In particular, occlusions can be divided into short-term and long-term occlusions. The short-term occlusion means the target is incomplete in a frame as some other objects occlude it. It prevents the extraction of high-quality detection features for the association. To address this issue, some works try to separate the unoccluded and occluded targets [15, 46].\\n\\nThe long-term occlusion occurs when the tracking target is lost for a long period due to obstacles. To alleviate this issue, DeepSORT [44] proposes a cascaded matching strategy that first matches the detection boxes to the alive tracklets and then to the lost targets based on appearance features. MeMOT [7] builds a memory bank to store the appearance features of the tracklets for retrieval. QuoVadis [10] convert trajectories [16, 35] to a bird's eye view. ByteTrack [52] re-identifies the lost tracklet using the IoU score between the tracklet's iterative prediction and detection. However, these methods will become less reliable as the occlusion time becomes longer. We propose a new representation of the tracklet based on its history trajectory, so that the model can still provide a reliable matching strategy in extremely occluded scenes.\\n\\n3. Method\\n3.1. Notation\\nAs shown in Figure 2, our MotionTrack follows a well-known tracking-by-detection paradigm [52]. We first process each frame with YOLOX [14] to obtain the detection results. The detections are denoted as $D_t = \\\\{d_{ti}\\\\}_{i=1}^{N}$ containing $N$ detections in frame $t$. A detection $d_{ti} \\\\in \\\\mathbb{R}^4$ is represented as $(x, y, w, h)$, where $(x, y)$ means the bounding box center, and $w$ and $h$ indicate its width and height, respectively. We denote the set of $M$ tracklets by $T = \\\\{T_j\\\\}_{j=1}^{M}$. $T_j$ is a tracklet with identity $j$ and is defined as $T_j = \\\\{l_{t0j}, l_{t0j+1}, ..., l_{tj}\\\\}$, where $l_{tj}$ is the location in frame $t$, and $t_0$ is the initialized moment. When tracking begins, we initialize the set of tracklets $T$ with $D_1$. For the subsequent video frames, we assign the new detections to their corresponding tracklets, and update $T$ at each time step. Throughout the whole video sequence, new tracklets are constantly initialized and incorporated into $T$. Meanwhile, the existing tracklets may be terminated and removed from $T$. In the tracking process, the tracklet may be interrupted, for example due to dense crowds and extreme occlusion, therefore we further divide $T$ into two parts, i.e., $T_{alive}$ and $T_{lost}$, which denote the set of tracked tracklets and the set of lost but not yet removed tracklets, respectively. A tracklet in $T_{alive}$ is represented as $T_k = \\\\{d_{t0k}, d_{t0k+1}, ..., d_{tk}\\\\}$, and a tracklet in $T_{lost}$ is represented as $T_s = \\\\{d_{t0s}, d_{t0s+1}, ..., p_{tlost}, p_{tlost+1}, ..., \\\\}$, where $t_{lost}$ denotes the moment it is lost, and $p_{t_{lost}}$ is the prediction during occlusion calculated in Section 3.3 below.\"}"}
{"id": "CVPR-2023-1351", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. Overview of MotionTrack\\n\\nOur MotionTrack mainly executes two steps for each video frame:\\n\\n- Step 1: Short-range association. Modeling the inter-tracklet interaction to obtain more accurate predictions and the short-range tracking results.\\n\\n- Step 2: Long-range association. Re-identifying lost tracklets based on the history trajectory and unmatched detections and then compensating the trajectory during occlusion.\\n\\nAll tracks go through three states from birth to death: alive, lost, and dead. The tracklet being tracked is called alive, and its state is converted to lost upon interruption by accident, such as extreme occlusion. When the interruption occurs for a long time, the tracklet becomes dead, and we remove it from the tracklets set $T$.\\n\\nShort-range Association. Given the input video frame $t$, we first obtain its detections $D_t$. In addition, we have obtained the set of $M$ tracklets $T_{t-1}$ up to frame $t-1$, with $S$ lost tracklets $T_{lost}$. Then, we construct the directed interactions between tracklets to get the predictions at frame $t$. As shown in Figure 2, we first calculate the iterative offsets $O_t \\\\in \\\\mathbb{R}^{M \\\\times 4}$, where each row $o_t_j = (\\\\Delta x_t, \\\\Delta y_t, \\\\Delta w_t, \\\\Delta h_t)$ and $\\\\Delta$ denotes the offset from frame $t-2$ to frame $t-1$. The Interaction Module concatenates absolute coordinates and offsets as input, denote as $I_t \\\\in \\\\mathbb{R}^{M \\\\times 8}$. An asymmetric interaction matrix $A_{adjc} \\\\in [0, 1]^{M \\\\times M}$ is obtained through interaction extraction, where each element indicates the impact of one tracklet on the other. Afterward, $A_{adjc}$ is used to estimate an accurate offset in the motion prediction step. Finally, we follow the association policy in ByteTrack [52] to update the alive tracklets $T_{alive}$ with matched detections and record the predictions for lost tracklets $T_{lost}$.\\n\\nLong-range Association. Suppose that there are $S$ lost tracklets and $U$ unmatched detections after the short-range association. Some unmatched detections could share the identities with those lost tracklets, which motivates us to learn history trajectory-based representations for the long-range association. In particular, we first calculate the correlation between the $S$ lost tracklets and the $U$ unmatched detections based on the spatial distribution of trajectories and the velocity-time relationship to obtain the correlation matrix $C_{corre} \\\\in [0, 1]^{S \\\\times U}$. Then, we retain highly relevant pairs and utilize error compensation to refine the trajectory. Finally, we combine the results of short-range and long-range associations to generate the complete tracking results at frame $t$.\\n\\n3.3. Interaction Module\\n\\nTo obtain more accurate tracklets, we capture the directed interactions between tracklets in the interaction extraction step and then use them to estimate the offsets between two consecutive frames in the motion prediction step.\\n\\nInteraction Extraction. As shown in Figure 3, we first obtain the attention matrix $A_{atte} \\\\in \\\\mathbb{R}^{M \\\\times M}$, which measures the interaction magnitude between every pair of tracklets, using the self-attention mechanism [38]:\\n\\n$$\\nE_t = \\\\phi(I_t, W_E),\\nQ_t = \\\\phi(E_t, W_Q),\\nK_t = \\\\phi(E_t, W_K),\\nA_{atte} = \\\\text{Softmax}(Q_t K_t^T) / \\\\sqrt{d},\\n$$\\n\\nwhere $\\\\phi(\\\\cdot, \\\\cdot)$ denotes linear transformation (multiplying a weight matrix and adding a bias), $E_t$ is a higher-dimension embedding mapped from $I_t$. $Q_t \\\\in \\\\mathbb{R}^{M \\\\times D}$ and $K_t \\\\in \\\\mathbb{R}^{M \\\\times D}$ are the query and key of the self-attention mechanism. $W_E$, $W_Q$, and $W_K$ are the weights of the linear transformation, and $\\\\sqrt{d} = \\\\sqrt{D}$ is the scaling factor [38].\\n\\nWe use the attention matrix $A_{atte}$ to express the asymmetric interaction between different tracklets instead of the undirected spatial distance. The $i, j$-th element in $A_{atte}$ represents the influence of tracklet $i$ on tracklet $j$. To further consider the whole scene, such as group behavior, we model higher levels of interaction via a cascade of asymmetric convolution [11]:\\n\\n$$\\nA_l = \\\\delta(\\\\text{conv}(A_{l-1}, K_1 \\\\times \\\\kappa) + \\\\text{conv}(A_{l-1}, K_\\\\kappa \\\\times 1)),\\n$$\\n\\nwhere $K_1 \\\\times \\\\kappa$ and $K_\\\\kappa \\\\times 1$ are the asymmetric convolution kernels, $\\\\delta$ denotes the PReLU, and $A_0$ is initialized as $A_{atte}$.\\n\\nThere are $L$ convolution layers. Afterward, to capture significant interactions between tracklets, we retain only high attention values in $A_{adjc} \\\\in [0, 1]^{M \\\\times M}$:\\n\\n$$\\nA_{mask} = \\\\text{sgn}(\\\\phi(A_L) - \\\\xi),\\nA_{adjc} = A_{mask} \\\\odot A_{atte},\\n$$\\n\\nwhere $\\\\phi$ and $\\\\odot$ denote the sigmoid function and element-wise multiplication, respectively, $\\\\text{sgn}$ is a sign function, and $\\\\xi$ is a threshold.\"}"}
