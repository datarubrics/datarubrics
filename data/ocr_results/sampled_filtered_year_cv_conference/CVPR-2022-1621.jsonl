{"id": "CVPR-2022-1621", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krystian Mikolajczyk. Hpatches: A benchmark and evaluation of handcrafted and learned local descriptors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5173\u20135182, 2017. 8\\n\\n[2] Daniel Barath, Jiri Matas, and Jana Noskova. MagSAC: Marginalizing sample consensus. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10197\u201310205, 2019. 6\\n\\n[3] Axel Barroso-Laguna, Edgar Riba, Daniel Ponsa, and Krystian Mikolajczyk. Key.net: Keypoint detection by handcrafted and learned CNN filters. In Proceedings of the IEEE International Conference on Computer Vision, 2019. 1, 2, 5, 6, 8\\n\\n[4] Axel Barroso-Laguna, Yannick Verdie, Benjamin Busam, and Krystian Mikolajczyk. HDD-net: Hybrid detector descriptor with mutual interactive learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2020. 1, 2\\n\\n[5] Eric Brachmann and Carsten Rother. Neural-guided RANSAC: Learning where to sample model hypotheses. In Proceedings of the IEEE/CVF Conference on Computer Vision, pages 4322\u20134331, 2019. 2\\n\\n[6] Mathias B\u00fcrki, Lukas Schaupp, Marcin Dymczyk, Renaud Dub\u00e9, Cesar Cadena, Roland Siegwart, and Juan Nieto. Vizard: Reliable visual localization for autonomous vehicles in urban outdoor environments. In 2019 IEEE Intelligent Vehicles Symposium (IV), pages 1124\u20131130. IEEE, 2019. 1\\n\\n[7] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834\u2013848, 2017. 3\\n\\n[8] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017. 3\\n\\n[9] J. Deng, W. Dong, R. Socher, L.-J. Li, K. Li, and L. Fei-Fei. ImageNet: A Large-Scale Hierarchical Image Database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2009. 6\\n\\n[10] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point detection and description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 224\u2013236, 2018. 1, 2, 5, 6\\n\\n[11] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-net: A trainable CNN for joint detection and description of local features. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2019. 1, 2, 4, 5, 7\\n\\n[12] Mihai Dusmanu, Johannes L. Sch\u00f6nberger, and Marc Pollefeys. Multi-view optimization of local feature geometry. In Proceedings of the European Conference on Computer Vision, 2020. 7\\n\\n[13] Patrick Ebel, Anastasiia Mishchuk, Kwang Moo Yi, Pascal Fua, and Eduard Trulls. Beyond cartesian representations for local descriptors. In Proceedings of the IEEE International Conference on Computer Vision, pages 253\u2013262, 2019. 2\\n\\n[14] Hugo Germain, Guillaume Bourmaud, and Vincent Lepetit. S2dnet: Learning accurate correspondences for sparse-to-dense feature matching. In European Conference on Computer Vision, 2020. 2, 5\\n\\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016. 6\\n\\n[16] Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang, Tobias Weyand, Marco Andreetto, and Hartwig Adam. Mobilenets: Efficient convolutional neural networks for mobile vision applications. CoRR, abs/1704.04861, 2017. 8\\n\\n[17] Max Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In Advances in neural information processing systems, pages 2017\u20132025, 2015. 3\\n\\n[18] Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasacchi, and Kwang Moo Yi. Cotr: Correspondence transformer for matching across images. arXiv preprint arXiv:2103.14167, 2021. 2\\n\\n[19] Karel Lenc and Andrea Vedaldi. Large scale evaluation of local image feature detectors on homography datasets. In The British Machine Vision Conference (BMVC), 2018. 8\\n\\n[20] Zhengqi Li and Noah Snavely. Megadepth: Learning single-view depth prediction from internet photos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3302\u20133312, 2018. 5, 6\\n\\n[21] Philipp Lindenberger, Paul-Edouard Sarlin, Viktor Larsson, and Marc Pollefeys. Pixel-perfect structure-from-motion with featuremetric refinement. arXiv preprint arXiv:2108.08291, 2021. 7\\n\\n[22] Yuan Liu, Zehong Shen, Zhixuan Lin, Sida Peng, Hujun Bao, and Xiaowei Zhou. Gift: Learning transformation-invariant dense visual descriptors via group CNNs. arXiv preprint arXiv:1911.05932, 2019. 2\\n\\n[23] David G. Lowe. Distinctive image features from scale-invariant keypoints. In International booktitle of computer vision, 2004. 1, 2, 6, 8\\n\\n[24] Zixin Luo, Lei Zhou, Xuyang Bai, Hongkai Chen, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan. Aslfeat: Learning local features of accurate shape and localization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2020. 2\\n\\n[25] Iaroslav Melekhov, Aleksei Tiulpin, Torsten Sattler, Marc Pollefeys, Esa Rahtu, and Juho Kannala. DGC-Net: Dense geometric correspondence network. In Proceedings of the IEEE Winter Conference on Applications of Computer Vision (WACV), 2019. 1, 2, 5, 6\\n\\n[26] Krystian Mikolajczyk and Cordelia Schmid. Scale & affine invariant interest point detectors. International journal of computer vision, 60(1):63\u201386, 2004. 1\\n\\n[27] Krystian Mikolajczyk and Cordelia Schmid. A performance evaluation of local descriptors. In IEEE Transactions on Pattern Analysis and Machine Intelligence, number 10, pages 1615\u20131630. IEEE, 2005. 7\"}"}
{"id": "CVPR-2022-1621", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2022-1621", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Olivia Wiles, Sebastien Ehrhardt, and Andrew Zisserman. Co-attention for conditioned image matching. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2021.\\n\\nKwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal Fua. Lift: Learned invariant feature transform. In European Conference on Computer Vision, pages 467\u2013483. Springer, 2016.\\n\\nKwang Moo Yi, Yannick Verdie, Pascal Fua, and Vincent Lepetit. Learning to assign orientations to feature points. CoRR, abs/1511.04273, 2015.\\n\\nGuoshen Yu and Jean-Michel Morel. Asift: An algorithm for fully affine invariant comparison. Image Processing On Line, 1:11\u201338, 2011.\\n\\nJin Yuhe, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas, Pascal Fua, Kwang Moo Yi, and Eduard Trulls. Image matching across wide baselines: From paper to practice. In International Journal of Computer Vision, 2020.\\n\\nLei Zhou, Siyu Zhu, Tianwei Shen, Jinglu Wang, Tian Fang, and Long Quan. Progressive large scale-invariant image matching in scale space. In Proceedings of the IEEE International Conference on Computer Vision, pages 2362\u20132371, 2017.\"}"}
{"id": "CVPR-2022-1621", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this paper, we address the problem of estimating scale factors between images. We formulate the scale estimation problem as a prediction of a probability distribution over scale factors. We design a new architecture, ScaleNet, that exploits dilated convolutions as well as self- and cross-correlation layers to predict the scale between images. We demonstrate that rectifying images with estimated scales leads to significant performance improvements for various tasks and methods. Specifically, we show how ScaleNet can be combined with sparse local features and dense correspondence networks to improve camera pose estimation, 3D reconstruction, or dense geometric matching in different benchmarks and datasets. We provide an extensive evaluation on several tasks, and analyze the computational overhead of ScaleNet. The code, evaluation protocols, and trained models are publicly available at https://github.com/axelBarroso/ScaleNet.\\n\\n1. Introduction\\n\\nEstablishing correspondences is the very first step in many different 3D pipelines. Advancing on this task will have a direct impact on the performance of downstream applications such as camera pose estimation [39], autonomous driving [6], or 3D reconstructions [41]. However, methods that search for correspondences between images face significant challenges, and although some solutions are more mature than others, the task still is far from being solved. As the field advances, even though the intermediate tasks in the correspondence search remain the same, their methods are being revisited and redesigned, e.g., keypoint detectors/descriptors [10, 11, 37], dense geometric matchers [25, 53], or geometric verification techniques [31, 40, 46]. These new approaches have shown that the downstream tasks can be pushed to new performance levels through robust correspondences. The key objective of these new methods is to handle more and more extreme cases where previous pipelines failed, and although some methods are arguably application-specific [60], their robustness to extreme conditions is the main reason for success.\\n\\nInspired by the previous methods targeting visual robustness [29, 30, 58], we address the problem of handling the scale change between images, which is a long-standing challenge in computer vision [23, 26]. Scale robustness and estimation have been the focus of much research in the area of handcrafted feature extraction [23, 54, 60] as a reliable solution that can significantly boost the performance of existing methods. Moreover, the scale change is arguably the most challenging, and the most important parameter to estimate compared to rotation, translation, or even local affine deformations [54]. There are several strategies to deal with scale changes, with the multi-scale pyramid being one of the most popular solutions [3, 4, 11, 23, 37, 52, 54]. Although the multi-scale pyramid mitigates the problem of different scale conditions is the main reason for success.\"}"}
{"id": "CVPR-2022-1621", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"scales, it increases complexity and ambiguity as the matcher needs to establish correspondences among multiple scale levels. Figure 1 (bottom) shows an example of extreme pairs for which R2D2 [37] with multi-scale pyramid can only get a high number of correct matches once we rectify the scales. Besides the added complexity, a multi-scale pyramid is not always a straightforward solution to incorporate in some methods, such as dense correspondence networks [18,45,53]. In contrast to multi-scale pyramids, some works aim at being invariant to different scales through their learning process [10, 34], however, as a side effect, they become progressively less discriminative [55]. Another possible direction, and popular strategy, is to estimate the local or global transformations and rectify the images prior to establishing the correspondences [30, 36, 50, 58].\\n\\nThe scale factor characterizes the relationship between pairs of images and, in general, an accurate estimate can only be achieved when considering both images at the same time. Using pairs of images as input may increase the complexity beyond acceptable in some applications such as large-scale retrieval and localization unless used in their final verification stage. Nonetheless, solving the scale before the main analysis improves the discriminative power and allows less robust but more efficient methods to be used in challenging scenarios [34]. Hence, we propose a new approach that estimates and corrects the scale factor between a given pair of images before the correspondence search, which is illustrated at the top of figure 1. Our scale predictor network, termed ScaleNet, is inspired by dense geometric methods [25, 51, 53] and conditioned local features extractors [14, 34, 55]. ScaleNet extracts features from two low-resolution images and exploits CNN correlation layers to predict the scale factor. Due to the non-linear nature of scale changes, we formulate the scale regression problem as the estimation of a probability distribution in logarithmic space. We show how ScaleNet can be combined with different methods and demonstrate the improvements on different tasks and datasets.\\n\\nOur contributions include: 1) a scale-aware matching system based on a novel scale predictor architecture, 2) a strategy to measure and label the scale factor between two images, and 3) a learning scheme that tackles the non-linear nature of scale changes.\\n\\n2. Related work\\nRecent works have allowed significant progress in establishing good correspondences between images. Although many works have focused on solving entire tasks in an end-to-end manner [5, 39], there are lots of efforts focused on identifying limitations and improving the robustness of individual steps in modern pipelines [3, 28, 40].\\n\\nImage rectification consists of predicting or applying a set of transformations to the images so that the search for correspondences is done in an optimum setting. Pioneering work on this area is ASIFT [58], which applies multiple affine transformations to find less challenging image pairs for matching with SIFT [23]. MODS [29] investigated this line of work by introducing an iterative scheme to generate intermediate synthetic views between images. MODS also proposed an adaptive system to avoid applying synthetic transformations to easy-to-match pairs, being faster and more versatile than previous ASIFT. Closer to our work is [60], which computes the scale factor between a pair of images by detecting and matching exhaustively SIFT features on multiple scale levels. Although it shows that they can deal with strong scale changes, their method is tied to the need of visiting all possible scaled images before the actual local feature matching stage. One negative aspect of these methods is that they still require a blind exploration of synthetic views to find the optimal image transformations. A few works have tried to overcome the previous limitation and proposed to learn such parameterizations directly from the images. One of the first attempts is [57], where authors introduced a neural network to assign a canonical orientation to every input image patch before the descriptor architecture. In AffNet [30], authors follow this trend and learn a full affine shape estimator to geometrically align input patches before the descriptor. Moreover, [36] proposed a scene-specific overlapping estimator and showed that scale rectification based on image overlapping can improve feature matching. Unlike previously learned methods, our labeling strategy is based on keypoint distance ratios, resulting in a scene-agnostic scale predictor that uses pairs of images as input. Even though our ScaleNet tackles only the scale factor out of several possible transformation parameters, we show in section 5 that the scale factor is crucial for boosting the performance of current methods.\\n\\nVisual robustness has been the focus of numerous works in the field of correspondence search [4, 13, 24]. The rotation has been addressed by correcting input patches [13, 23, 30] before extracting local descriptors [28,48,49], or by designing robust architectures [4, 22, 24]. However, in the context of scale changes, the standard strategy is the well-known multi-scale (M-S) pyramid approach, which applies methods at different re-scaled versions of the image [11, 37, 47]. Even though M-S pyramids offer a versatile solution for many applications, it does not provide a suitable approach for extreme scale changes (cf. section 5), and thus, both, single and multi-scale feature extractors, benefit from correcting the scale factor before extracting features. Moreover, recent works show that there is growing interest in pair-wise methods, i.e., that use two input images at the same time to establish the local or dense correspondences [14, 18, 25, 35, 45, 53, 55], but, in that scenario, there is no...\"}"}
{"id": "CVPR-2022-1621", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. ScaleNet uses a multi-scale (M-S) feature extractor and a combination of self- and cross-correlation layers to calculate the relationship within each image and between them. The M-S extractor is composed of a common CNN and an ASPP block [7, 8]. The ASPP module consists of three 3x3 convolutional layers with different dilation rates. Conv blocks perform 1x1 convolutions to learn the cross-channel interactions that go into the self- and cross-correlation layers. Correlation volumes' dimensionality is reduced through a CNN, and its results are concatenated into a flat feature vector. Finally, consecutive fully connected layers calculate the scale distribution.\\n\\nHence, given that such methods already take two images as inputs, ScaleNet rectification offers a more natural and intuitive process towards visual robustness than M-S pyramids.\\n\\n3. Method\\n\\nScaleNet embraces several key concepts to deliver good performance in practical settings. The first key aspect is that it is effective for low-resolution images, which makes it more efficient. Another important idea is the formulation of the scale estimation as a distribution prediction in logarithmic space rather than a regression problem [17, 30, 56]. This allows using a simple and shallow, yet effective architecture. Figure 2 presents our ScaleNet architecture, and the following sections detail each of the aspects of ScaleNet and its learning scheme.\\n\\n3.1. ScaleNet architecture\\n\\nConsider A and B as input images to ScaleNet. Low-resolution images A and B are processed by a multi-scale feature extractor block, which is composed of a generic network, e.g., VGG, or ResNet, followed by the atrous spatial pyramid pooling (ASPP) [7, 8]. The ASPP block achieves multi-scale robustness by applying to the feature map 3\u00d73 dilated convolutions, each with a different dilation rate, e.g., 2, 3, and 4. Thus, the ASPP block allows the network to compute and fuse features from different receptive fields. M-S features are concatenated and fed into a final 1\u00d71 convolution, which combines the features from local and global areas at a minimum cost. We then apply self- and cross-correlation layers to multi-scale features fA and fB, and obtain the correlation maps, cA, cB, and cA\u2212B, which contain the self- and cross-pairwise similarities. As in [38], ReLU and L2-normalization are applied to the correlation maps before the feature reduction blocks, which are composed of four Conv-Batch-ReLU layers each. Finally, c\u2032A, c\u2032B, and c\u2032A\u2212B maps are flattened, concatenated, and fed into a set of fully connected layers to predict the scale distribution PA\u2192B, with a final softmax activation layer.\\n\\n3.2. Predicting scale distributions\\n\\nScaleNet outputs a scale distribution rather than a regressed single scale factor, which helps the network to converge to a reliable model. In contrast, when tackling the problem as a regression task, the same network cannot predict an accurate scale (cf. appendix C.1). We attribute this to the fact that the network can learn and interpret the relationships between the quantized scale ranges and solve an easier classification task, which requires it to assign the weights to the predefined scale factors instead of predicting its actual value.\\n\\nWe formulate the scale estimation as a problem of predicting the probability distribution in a scale-space quantized into L bins. Given images A and B, our objective function measures the distance between our computed scale distribution, PA\u2192B, and the ground-truth distribution PgtA\u2192B:\\n\\n\\\\[\\n\\\\text{Loss}(A, B) = KL(P_{A \\\\rightarrow B}, P_{gtA \\\\rightarrow B}),\\n\\\\]\\n\\nwhere KL(\u00b7, \u00b7) is the Kullback-Leibler divergence loss. To obtain the scale factor from the probability distribution PA\u2192B, we combine all scale levels using a soft-scale computation. It enables the network to output scale factors that interpolate between the quantized scale values, thus covering all possible scales between images A and B.\\n\\nSoft-assignment gives the architecture further flexibility and...\"}"}
{"id": "CVPR-2022-1621", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. In the dataset generation, we randomly pick pairs of corresponding points in both views and compute their distances in the image planes. The final scale ground-truth is the average of the distance ratios of all picked keypoints.\\n\\nThe scale factor is a relative ratio operator, hence, it is non-linear. To avoid a bias by high scale values when computing the soft-scale, we transform the quantized scale classes, $s_i$, to logarithmic space. The logarithmic transformation allows to calculate the soft-scale, $\\\\bar{S}_{A \\\\to B}$, as a linear combination of the logarithmic scales, $\\\\ln(s_i)$, weighted by the predicted scale probabilities from softmax output $p_i$. The global scale factor $\\\\bar{S}_{A \\\\to B}$ in log-scale is given as:\\n\\n$$\\\\bar{S}_{A \\\\to B} = L^{-1} \\\\sum_{i=0}^{L-1} p_i \\\\cdot \\\\ln(s_i),$$\\n\\nwhere $s_i = \\\\sigma_t$ corresponds to the quantized scale for bin $i$, $\\\\sigma$ is our predefined base scale factor, integer $t \\\\in [-L/2, \\\\ldots, 0, \\\\ldots, +L/2]$, and $L = 2^t + 1$ is the total number of scale bins. Moreover, we improve the robustness of our scale estimator by a simple yet effective consistency check trick, where we compute the scale factor, $\\\\bar{S}_{A \\\\to B}$, and its inverse, $\\\\bar{S}_{B \\\\to A}$, and combine them as:\\n\\n$$\\\\hat{S}_{A \\\\to B} = \\\\bar{S}_{A \\\\to B} - \\\\bar{S}_{B \\\\to A},$$\\n\\nand\\n\\n$$S_{A \\\\to B} = e^{\\\\hat{S}_{A \\\\to B}},$$\\n\\nwith $S_{A \\\\to B}$ as the final scale factor between the images.\\n\\n3.3. Dataset generation\\nScaleNet is trained with synthetically generated image pairs as well as images from real scenes.\\n\\nSynthetic pairs. We define a set of planar affine transformations to map one image into another. Such image pairs are easy to generate on-demand for any ground-truth scale $S_{gt}$, however, they do not include the real noise from different imaging conditions.\\n\\nReal pairs present more challenging conditions than synthetic pairs, i.e., non-planar viewpoint changes, weather/illumination conditions, or occlusions, among others. In addition, in contrast to the previous synthetic global transformations, the scale between two real images may spatially vary, and areas of different depth or strong perspective changes may include various scale factors. Thus, we introduce a new approach for obtaining training data by estimating the scale factors between real images. We use 3D reconstruction datasets, where 3D point clouds and their corresponding projected 2D positions on the images are available. First of all, given the 3D model, we find pairs of images with an overlap higher than 10% computed as in [11, 32]. For each pair of images $A$ and $B$, we identify the 3D points of the model that are visible in both images.\\n\\nUsing the registered 3D points from the model as opposed to sampling random positions ensures that the regions used for computing the scale are discriminative. Thus, given the covisible 3D points, we query its projected 2D positions, $k_A$ and $k_B$, on image $A$ and $B$. We randomly sample pairs of points, $k_A$ and $k_B$, and compute their distances as shown in figure 3.\\n\\nThe scale factor between two images with keypoints $i$ and $j$ is calculated as the ratio of their distances in image $A$ and $B$:\\n\\n$$S_{i-j} = \\\\frac{\\\\|k_{Bi} - k_{Bj}\\\\|}{\\\\|k_{Ai} - k_{Aj}\\\\|}$$\\n\\nand $i, j \\\\in \\\\{1, \\\\ldots, K\\\\}$, with $K$ as the total number of covisible 3D points between images $A$ and $B$. As different regions may exhibit different scale factors, we compute the global scale factor as the average of ratios in logarithmic scale after sampling $R$ different pairs of points:\\n\\n$$S_{gt A \\\\to B} = e^{\\\\bar{S}_{A \\\\to B}}$$\\n\\nwhere\\n\\n$$\\\\bar{S}_{A \\\\to B} = \\\\frac{1}{R} \\\\sum_{i \\\\neq j} \\\\ln(S_{i-j}).$$\\n\\nAs detailed in section 3.2, the ScaleNet learning scheme minimizes the K-L divergence between the predicted scales and ground-truth distributions. We, therefore, build the ground-truth scale distribution, $P_{gt A \\\\to B}$, such as it satisfies:\\n\\n$$\\\\ln(S_{gt A \\\\to B}) = \\\\bar{S}_{gt A \\\\to B} = L^{-1} \\\\sum_{i=0}^{L-1} p_{gt i} \\\\cdot \\\\ln(s_i),$$\\n\\nwhere $s_i$ are the quantized scale factors, and $P_{gt A \\\\to B} = [p_{gt 0}, \\\\ldots, p_{gt (L-1)}]$ is the ground-truth distribution of the scale estimated as a normalized histogram of point pairs.\\n\\n4. Implementation notes\\nScaleNet details. ScaleNet predicts a scale distribution with $L=13$ bins and $\\\\sigma=\\\\sqrt{2}$, giving possible scale factors in range $S \\\\in [0.16, \\\\ldots, 6]$. ASPP module has 3 levels with dilation rates 2, 3, and 4. During training, ScaleNet uses Adam Optimizer with a learning rate of $10^{-4}$ and a decay factor of 0.1 every ten epochs. The training takes on average 40 epochs, 20 hours on a machine.\"}"}
{"id": "CVPR-2022-1621", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Ablation study of the different ScaleNet's design choices. Baseline refers to SuperPoint [10] without scale correction.\\n\\n| Method                      | 5\u00b0  | 10\u00b0 | 20\u00b0 | Ours + Consistency check |\\n|-----------------------------|-----|-----|-----|--------------------------|\\n| Baseline                   | 4.8 | 7.4 | 10.4|                          |\\n| VGG-16                     | 5.4 | 8.3 | 11.8|                          |\\n| ResNet-50                  | 5.6 | 8.8 | 12.7|                          |\\n| VGG w/ self-corr.          | 6.2 | 9.3 | 12.5|                          |\\n| VGG w/ ASPP                | 7.3 | 11.1| 15.7|                          |\\n| Ours (VGG+self-corr+ASPP)  | 8.4 | 12.3| 17.6|                          |\\n| Ours + Consistency check   | 8.7 | 13.4| 19.5|                          |\\n\\nDataset details.\\n\\nWe use Megadepth dataset [20] for generating our custom training and testing dataset. We discard scenes that are in the PhotoTourism test from our training set as in [40] to avoid overlap. We keep 10% of the training scenes as our validation set. During data generation, we sample \\\\( R = 200 \\\\) random pairs of 3D points (cf. equation 5) for a robust scale estimation between the two images. We sample pairs of images with scale factors \\\\( S \\\\in [0.16, \\\\ldots, 6] \\\\) and create a collection of 250,000 training and 25,000 validation pairs where all scale factors are well represented. Synthetic pairs are generated on the fly from the Megadepth training images during training. We include more details and examples of our training set in appendix A.\\n\\n5. Experiments\\n\\nThis section presents results for ScaleNet integrated with state-of-the-art methods on several datasets and tasks. Refer to appendix for more experiments and qualitative examples.\\n\\n5.1. Preliminaries\\n\\nMulti-scale pyramid & ScaleNet.\\n\\nMulti-scale (M-S) pyramids and ScaleNet aim at making methods more robust against arbitrary scale changes. Although both approaches tackle the same problem, each has its strengths, e.g., M-S pyramids can compute a higher number of features by visiting multiple resized images, and ScaleNet offers a more natural integration into tasks where two images are given as input [14, 25, 53, 55]. Besides, we claim that ScaleNet improves not only single-scale feature extractors but also multi-scale ones. We analyze in figure 4a the robustness of methods against synthetic scale transformations and show how the combination with ScaleNet benefits them. In this experiment, we use 2,000 random images from the Megadepth dataset and scale them to create the pairs. We measure the mean matching accuracy (MMA) computed as in [37]. As expected, results indicate that the performance of single-scale methods, Key.Net [3] and R2D2 [37], drop notably even when images present small scale perturbations (\\\\( s > 1.5 \\\\)). Meanwhile, M-S pyramid or ScaleNet strategies mitigate the effect and lead to a better approach. Moreover, we observe that the combination of M-S pyramids and ScaleNet achieves the top performance, and proves that both strategies contribute and work well together.\\n\\nLocal vs global scale estimation.\\n\\nScaleNet can predict a scale for each point within the image, however, it is not straightforward to correct the scale factor locally for networks that process the whole image, e.g., dense correspondence networks [25, 51, 53], or dense local feature extractors [10, 11, 37]. Figure 4b shows the histogram of the average scale ratios between the globally and the locally estimated scales per image. To compute the local scale values, we restrict the random sampling to spatially neighboring keypoints in equation 4 rather than points sampled across the whole image. Figure 4b shows that in the majority of images the differences between global and local estimations are small and within 1.0 and 1.2. A ratio \\\\( r \\\\) of 1.0 indicates that the local and the global scales between the two images are the same, and therefore, the global scale is valid across the whole image. Based on results in figure 4b, we argue that although local scales could bring an extra benefit in scenes with strong viewpoint or perspective changes, a single global scale will significantly contribute towards correcting images.\\n\\nAblation study in table 1 displays the contributions of each ScaleNet's block towards robust scale estimation.\"}"}
{"id": "CVPR-2022-1621", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Relative camera pose results on custom Megadepth split with and without ScaleNet correction.\\n\\n| Method        | 5\u00b0   | 10\u00b0  | 20\u00b0  |\\n|---------------|------|------|------|\\n| Pose estimation (AUC) w/ ScaleNet | 12.9 (+32%) | 19.5 (+27%) | 27.4 (+25%) |\\n| SIFT          | 9.8  | 15.4 | 22.0 |\\n| R2D2-SS      | 3.1  | 4.4  | 5.6  |\\n| R2D2-MS      | 10.5 | 13.1 | 18.9 |\\n| Key.Net-SS   | 4.7  | 6.9  | 9.3  |\\n| Key.Net-MS   | 14.0 | 22.1 | 31.6 |\\n| SuperPoint   | 5.4  | 8.2  | 11.0 |\\n| SP+SuperGlue | 16.5 | 25.3 | 35.0 |\\n\\nWe first compare the effect of different pre-trained feature extractors on ImageNet (cf. figure 2), VGG-16 [44], and ResNet-50 [15], and see that the more complex ResNet representation contributes towards better poses with the downside of a higher computational cost. Thus, to keep our method light and fast, and given the similarity of the AUC scores, we use the VGG feature extractor for following experiments. In addition to the extractors, we analyze the effect of the self-correlation layers and the multi-scale ASPP component and observe that both boost the performance. Self-correlations capture the intra-image relationships and, therefore, give a better awareness of the global content. Additionally, ASPP offers a mechanism to extract more global features, thus, address larger scale changes.\\n\\nFurther, we show that the consistency check offers a higher AUC at a low computation cost. Note that M-S features and correlation layers only need to be computed once in inference and, hence, the extra cost for consistency check is small and proportional to running the dense layers twice.\\n\\n5.2. Relative camera pose Protocol.\\n\\nWe first evaluate ScaleNet on the camera pose estimation task due to its natural integration into the existing pipelines. Similar to the previous ablation experiment, given a collection of image pairs, we calculate the AUC of the pose errors at 5\u00b0, 10\u00b0, and 20\u00b0, where the error is calculated as the maximum of the rotation and translation angular errors.\\n\\nWe sample 2,000 image pairs with scale factors between 0.16 and 6 from an independent validation set. Besides the AUC, we report the overhead inference time of each design.\\n\\nWe first compare the effect of different pre-trained feature extractors on ImageNet (cf. figure 2), VGG-16 [44], and ResNet-50 [15], and see that the more complex ResNet representation contributes towards better poses with the downside of a higher computational cost. Thus, to keep our method light and fast, and given the similarity of the AUC scores, we use the VGG feature extractor for following experiments. In addition to the extractors, we analyze the effect of the self-correlation layers and the multi-scale ASPP component and observe that both boost the performance. Self-correlations capture the intra-image relationships and, therefore, give a better awareness of the global content. Additionally, ASPP offers a mechanism to extract more global features, thus, address larger scale changes.\\n\\nFurthermore, we show that the consistency check offers a higher AUC at a low computation cost. Note that M-S features and correlation layers only need to be computed once in inference and, hence, the extra cost for consistency check is small and proportional to running the dense layers twice.\\n\\n5.3. Geometric matching Protocol.\\n\\nWe also evaluate ScaleNet on the geometric correspondence task by integrating it into the popular DGC-Net [25] and GLU-Net [53]. Note that ScaleNet can also be combined with other recent methods [45, 51, 52]. ScaleNet rescales one of the images before the dense correspondence network estimates the dense flow fields between the two images. Due to the lack of dense annotations on real image pairs with large viewpoint and illumination changes, we evaluate ScaleNet using sparse correspondences available in the Megadepth [20] dataset. Specifically, we follow the protocol of 1,600 image pairs introduced in [43] to compute the percentage of correct keypoints (PCK) under a 5 pixel acceptance threshold. The experiment is extended with multiple acceptance thresholds in appendix C.4.\\n\\nResults in table 3 show the PCK scores obtained with and without scale correction before the dense architectures, DGC-Net and GLU-Net. Moreover, to highlight the benefit of ScaleNet for images at different scale factors, besides 128x128, we also report results at 128x13, 512x512, and 1024x1024.\"}"}
{"id": "CVPR-2022-1621", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. 3D reconstruction results on the 10 test scenes (100 images each) from the IMC dataset [59] with and without ScaleNet correction.\\n\\n| Method       | Images | Obs. | Length | Error |\\n|--------------|--------|------|--------|-------|\\n| SIFT         | 98.5   | 93k  | 7.26   | 0.80  |\\n| SuperPoint   | 99.0   | 78k  | 10.47  | 1.17  |\\n| w/ ScaleNet  | 98.6   | 126k | 10.20  | 0.87  |\\n| w/ D-ScaleNet| 99.5   | 135k | 12.71  | 1.26  |\\n| IMC Dataset  |        |      |        |       |\\n| R2D2-SS      | 96.0   | 52k  | 8.35   | 0.93  |\\n| R2D2-MS      | 97.9   | 86k  | 14.19  | 1.05  |\\n| w/ ScaleNet  | 97.2   | 89k  | 10.71  | 1.02  |\\n| w/ D-ScaleNet| 99.0   | 114k | 15.35  | 1.13  |\\n| Key.Net-SS   | 99.1   | 82k  | 11.05  | 1.06  |\\n| Key.Net-MS   | 99.5   | 114k | 16.23  | 0.96  |\\n| w/ ScaleNet  | 99.6   | 148k | 14.22  | 1.16  |\\n| w/ D-ScaleNet| 99.2   | 126k | 12.71  | 1.10  |\\n\\n5.4. 3D reconstruction Protocol.\\n\\nScaleNet can be easily integrated into geometric correspondence or relative camera pose pipelines, which are often a part of a more general 3D reconstruction system. To evaluate ScaleNet in this scenario, we follow the protocol proposed in the Local Feature Evaluation Benchmark [42] for building 3D reconstruction models. As ScaleNet can upsample one of the images, and that could result in a higher number of candidate keypoints, we limit the number of features to the top 2,048 keypoints based on the protocol proposed in [59]. We present results for the test split from the IMC dataset [59], which includes ten different scenes with 100 images each. IMC images pose significant challenges, e.g., weather/illumination, perspective, scale, as well as strong occlusions.\\n\\nAs ScaleNet is applied to image pairs before the feature extraction, the detectors and descriptors are recomputed every time the scale is corrected. Hence, to reduce the computation time, we propose a discrete variant of ScaleNet (D-ScaleNet) that makes the extraction process more efficient. D-ScaleNet implements a hard-assignment by selecting the maximum scale instead of the soft-scale and consistency check from equation 2 and 3. Analogous to multi-scale pyramid approaches, we run the detectors/descriptors at multiple resized images but then select the optimal set of pre-computed features for matching based on D-ScaleNet estimation.\\n\\nTable 5. 3D reconstruction times on the British Museum scene (100 images) from IMC dataset [59].\\n\\n| Method       | Time (s) |\\n|--------------|----------|\\n| SuperPoint   | 10.3     |\\n| w/ ScaleNet  | 980.5    |\\n| w/ D-ScaleNet| 141.8    |\\n\\nResults in table 4 show the 3D reconstruction metrics of state-of-the-art methods with and without ScaleNet. We notice that image rectification by both, D-ScaleNet and ScaleNet, increases the number of registered images and the total number of observations in the 3D models. Track length is especially boosted by scale correction, meaning that the model was able to match the same keypoint simultaneously in more images. This increase of track length is particularly important since it proves that ScaleNet helps current methods distinguish and link points that were not possible without it, due to extreme view differences. On average, improvements added by ScaleNet are greater than those produced by D-ScaleNet, however, D-ScaleNet still brings a notable boost over baselines. On the opposite side, ScaleNet increases the reprojection error (Rep. Error) of the reconstructions by 0.09 points on average. We attribute this to their longer track lengths since more points are triangulated throughout the images, and thus, the reprojection error increases, which has also been reported in [42].\\n\\nLonger tracks will benefit works that rely on complete and long tracks to refine the point positions and reduce their reprojection errors [12, 21]. In table 5, we show a comparison of the times taken to generate a 3D model when using ScaleNet and D-ScaleNet, and display the benefits in terms of computational time that D-ScaleNet provides.\\n\\n5.5. Image matching Protocol.\\n\\nWe compute the Mean Matching Accuracy (MMA) [27] as the ratio of correctly matched features within a threshold (5 pixels) and the total number of features following the benchmark proposed in [11]. We report\"}"}
{"id": "CVPR-2022-1621", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. MMA on the full Viewpoint HPatches (left), scenes with easy (middle) and hard (right) scale transformations. Dashed lines (- -) show ScaleNet combined with other methods.\\n\\nResults in figure 5 (left) show that ScaleNet improves the robustness of current methods overall, and excels for single-scale methods, e.g., Key.Net-SS [3], or R2D2-SS [37]. However, in contrast to images used for statistics in figure 4b, most of the scenes in the HPatches Viewpoint contain strong perspective changes, i.e., display different scale factors within the scene. Hence, to highlight the effect of ScaleNet, we show results for the subset of scenes with affine transformations, i.e., scenes with stretch and skew transformations in addition to global scale and rotation. We select the splits such that the Easy has $s \\\\in [1, 2, \\\\ldots, 1.8]$ and the Hard has $s > 1.8$. When the scale factor is global across the image, ScaleNet correction can deal with planar scenes and improve the matching accuracy of single and multi-scale methods. As expected, results show that ScaleNet scaling is more critical for stronger scale changes in figure 5 (right). Only SIFT [23], which is specially designed to be robust against scale changes on planar scenes, does not benefit from ScaleNet correction in the Hard split. Moreover, to deal with scenes with possible strong perspective scale changes, we introduce in appendix Local-ScaleNet, which infers local scale factors and offers a more robust and functional alternative for scenes with perspective changes.\\n\\n6. Discussion\\n\\nLimitations. ScaleNet deals with arbitrary scale changes and, hence, it only brings improvements if such changes are present in the images. This makes ScaleNet useful in applications where those viewpoint changes prevent a successful matching of images, e.g., extreme and sparse collection of images, or ground-aerial applications. Nevertheless, even though ScaleNet does not boost performance when there is no scale change, it does not hurt either (cf. appendix C.3). Another limitation comes when ScaleNet needs to be applied to a large collection of images, e.g., 3D tasks. ScaleNet works with pairs of images, hence, features can not be stored but need to be computed every time a new image is presented, increasing the feature extraction time as seen in table 5. Although we propose D-ScaleNet, which mitigates the complexity time for such tasks, ScaleNet can be further optimized for faster processing by replacing VGG with more compact models such as MobileNet [16], or by only using ScaleNet for computing camera pose after a restrictive retrieval search.\\n\\nSocietal impact. Image matching is a pivotal but small component within large systems that facilitate technologies like AR, 3D reconstruction, navigation, modeling, SLAM, among others. Hence, as we contribute towards more robust matching pipelines, ScaleNet\u2019s societal impact is tied to the applications that rely on such technologies. Some applications may include smartphone apps, AR headsets, or autonomous cars. However, as our method cannot work independently of a larger system, the negative or ethical issues are not directly associated with our approach but rather with the specific business and final application where image matching may be used.\\n\\nReproducibility. The experiments are computed on standard and public datasets and tasks, and hence, they can be reproduced. Moreover, we made public the evaluation and training scripts, as well as our custom training dataset. In addition, to encourage the research on scale estimation, we published the test set of section 5.2 and splits of section 5.3 for easier comparison and support of future works.\\n\\n7. Conclusions\\n\\nWe introduced ScaleNet, an approach that estimates the scale change between images and improves the performance of methods that search for correspondences throughout different views of the same scene. We proposed a novel learning scheme that formulates the problem of scale estimation as a prediction of a probability distribution of scales. We demonstrated how to make use of images from non-planar scenes to generate the training data. In addition to ScaleNet, we also introduced D-ScaleNet, a discrete variant of the proposed approach, and demonstrated its effectiveness in 3D-related tasks as well as computational time. We proved that ScaleNet can improve the results of popular pipelines in image matching for relative camera pose or 3D reconstruction while not being limited only to these tasks.\\n\\nAcknowledgements. This project was supported by Chist-Era EPSRC IPALM EP/S032398/1 grant.\"}"}
