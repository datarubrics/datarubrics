{"id": "CVPR-2024-2046", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Regions obtained for Example 3.3. On the left, the constraint \\\\( x_1^2 + 2x_2^2 - 4 = 0 \\\\) is depicted in blue, the purple region is obtained from Equation (42) and it is contained in the orange region coming from Equation (35). On the right, the level sets for the ratio \\\\( \\\\| \\\\epsilon_G \\\\| / \\\\| \\\\epsilon_S \\\\| \\\\), and the constraint depicted in blue. Observe that, although the ratio changes, it is bounded by 2 for every point in the colored regions.\\n\\nFrom Proposition 3.2, we have that \\\\( \\\\| \\\\epsilon_G \\\\| \\\\leq 2 \\\\| \\\\epsilon_S \\\\| \\\\) if \\\\( z \\\\) is in the colored region of Figure 2 (both the orange and purple region). Finally, the relaxed condition from (42) gives a smaller region, potentially easier to work with, where the previous bounds also hold (purple region in Figure 2).\\n\\n3.2. One Polynomial Constraint\\n\\nTo understand the Sampson error for a model defined by a single polynomial constraint of any degree \\\\( C : \\\\mathbb{R}^n \\\\to \\\\mathbb{R} \\\\), we extend the method presented previously and consider Taylor approximations of degree \\\\( d \\\\):\\n\\n\\\\[\\nC(z + \\\\epsilon) = C(z) + d \\\\sum_{i=1}^n \\\\frac{1}{i!} \\\\epsilon_i \\\\times T_i, \\\\tag{46}\\n\\\\]\\n\\nwhere \\\\( T_i \\\\) is a symmetric \\\\( n \\\\times \\\\cdots \\\\times n \\\\) tensor of order \\\\( i \\\\), and \\\\( \\\\epsilon \\\\times T_i = \\\\sum_{j_1, \\\\ldots, j_i \\\\in [n]} (T_i)_{j_1, \\\\ldots, j_i} \\\\epsilon_{j_1} \\\\cdots \\\\epsilon_{j_i} \\\\). \\\\tag{47}\\n\\nFor example, \\\\( T_1 = J \\\\) is the Jacobian and \\\\( T_2 = H \\\\) the Hessian.\\n\\nProposition 3.4. When the optimization problem (5) only has one polynomial constraint of degree \\\\( d \\\\), \\\\( J \\\\neq 0 \\\\) and \\\\( |C(z)| \\\\geq d \\\\sum_{i=2}^n \\\\frac{(-1)^i}{i!} C_i \\\\), then \\\\( \\\\| \\\\epsilon_G \\\\| \\\\leq 2 \\\\| \\\\epsilon_S \\\\| \\\\).\\n\\nProof. The proof is exactly the same as for Proposition 3.2; one checks that \\\\( C(z) \\\\leq 0 \\\\) and \\\\( C(z + \\\\epsilon(\\\\lambda)) \\\\geq 0 \\\\) (or the other way around) for \\\\( \\\\lambda = -2C(z)/\\\\|J\\\\| \\\\).\\n\\n3.3. Multiple Polynomial Constraints\\n\\nFor mathematical models defined by multiple constraints, the Sampson error and its relation to the geometric error are more involved. Here, we give an overview of our approach to computing Sampson errors in practice and studying its relation to the geometric error for polynomial constraints, but leave mathematical statements and proofs in the Supplementary Material.\\n\\nAssume that \\\\( C(z) = (C_1(z), \\\\ldots, C_N(z)) \\\\) are \\\\( N \\\\) polynomial constraints. The model \\\\( C(z) = 0 \\\\), for \\\\( z \\\\in \\\\mathbb{R}^n \\\\), is an algebraic variety \\\\( X \\\\) (a zero set of a system of polynomials) of some dimension \\\\( m \\\\leq n \\\\), which depends on the constraints. For generic constraints of fixed degrees \\\\( m = n - N \\\\), i.e. each constraint lowers the dimension of the model by one, however, for specific systems of polynomials this equality does not necessarily hold. In both cases, the intuitive behaviour is that the more constraints we have, the smaller \\\\( m \\\\) is and our data is harder to fit to the model. The same phenomenon occurs with the linearized constraints for the Sampson error: The more constraints we have, the smaller is the affine linear space defined by \\\\( C(z) + J\\\\epsilon = 0 \\\\).\\n\\nTherefore, if the number of constraints \\\\( N \\\\) is much greater than the codimension \\\\( n - m \\\\), the Sampson approximation is likely to be poor. To remedy this, we propose to use the fact that locally around a generic point of \\\\( X \\\\), the model is described by precisely \\\\( n - m \\\\) polynomials whose Jacobian has full rank. This is a result coming from algebraic geometry. Our proposal is to perform Sampson approximation by choosing a subset of \\\\( n - m \\\\) constraints whose Jacobian is full-rank and linearizing those constraints. In Section 4.2 we try this approach for the Three-View Sampson error and the results suggest that it is also beneficial to choose these constraints with smallest degree possible.\\n\\nIn the Supplementary Material, we generalize Proposition 3.1 and find an upper bound for \\\\( \\\\| \\\\epsilon_S \\\\| \\\\) in terms of \\\\( \\\\| \\\\epsilon_G \\\\| \\\\) for multiple constraints of any degrees. We also generalize Proposition 3.2 for quadratic constraints and data points \\\\( z \\\\) such that the Jacobian \\\\( J \\\\) at \\\\( z \\\\) has linearly independent rows. In this case, under appropriate conditions, we find a \\\\( \\\\lambda^* \\\\) such that \\\\( C_i(z + \\\\|J\\\\| J^\\\\dagger \\\\lambda^*) = 0 \\\\) for \\\\( i = 1, \\\\ldots, n \\\\). We get \\\\( \\\\| \\\\epsilon_G \\\\| \\\\leq \\\\| \\\\|J\\\\| J^\\\\dagger \\\\lambda^* \\\\| \\\\leq \\\\|J\\\\| \\\\|J^\\\\dagger\\\\| \\\\|\\\\lambda^*\\\\| \\\\), \\\\tag{49}\\n\\nand by construction we get an upper bound for this \\\\( \\\\|\\\\lambda^*\\\\| \\\\) expressed in terms of \\\\( C \\\\), its Jacobian and its Hessian.\\n\\n4. Experimental Evaluation\\n\\nIn the following sections we evaluate the Sampson approximation for different geometric estimation problems. First, in Section 4.1 we evaluate the classical Sampson error for two-view geometry. Next, in Section 4.2 we consider the analogous error in the three-view setting. Section 4.3 considers line segment to vanishing point errors. Finally, Section 4.4 show an application from absolute pose estimation with uncertainties applied in both 2D and 3D.\"}"}
{"id": "CVPR-2024-2046", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.1. Application: Two-view Relative Pose\\n\\nWe first consider the classical setting where the Sampson approximation is applied, two-view relative pose estimation. For the experiments we use image pairs from the IMC Phototourism 2021 [9] (SIFT), MegaDepth-1500 [25] (SP+LG [13]) and ScanNet-1500 [22] (SP+SG [22]). For each image pair we estimate an initial essential matrix using DLT [7] applied to the ground truth inliers (from [14]).\\n\\nResults and Discussion.\\n\\nFigure 3 shows the distribution of the difference between the Sampson approximation and the true error, and in Table 1 shows the Area-Under-Curve up to 1 pixel, on the British Museum scene from IMC-PT. For comparison we also include the symmetric epipolar error (distances to the epipolar lines computed in both images) which is another popular choice in practice. The Sampson error provides a very accurate approximation of the true reprojection error. As discussed in Section 3 the quality of the approximation depends on how close the initial point is to the constraint set (small \\\\( C(z) \\\\)) and the curvature (small Hessian). In Figure 4 we plot the approximation error \\\\(|E_S - E_G|\\\\) against \\\\( \\\\rho |C(z)| / \\\\|J\\\\|_2 \\\\), where \\\\( \\\\rho \\\\) is the spectral norm of the Hessian. Consistent with the theory, the figure shows a clear trend where correspondences with smaller \\\\( \\\\rho |C(z)| / \\\\|J\\\\|_2 \\\\) have smaller errors.\\n\\nFinally we also evaluate the error in the context of pose refinement on all three datasets. Table 2 shows the resulting pose errors (max of rotation and translation error) after non-linear refinement of the initial essential matrix using different error functions.\\n\\n### Table 2\\n\\n|                  | IMC-PT | MD1.5k | SN1.5k |\\n|------------------|--------|--------|--------|\\n| Initial estimate (DLT) | 0.361  | 0.321  | 0.274  |\\n| Algebraic        | 0.580  | 0.515  | 0.485  |\\n| Cosine           | 0.654  | 0.689  | 0.600  |\\n| Sym. Epipolar    | 0.673  | 0.728  | 0.654  |\\n| Sampson          | 0.678  | 0.732  | 0.657  |\\n\\n4.2. Application: Three-view Sampson Error\\n\\nIn this section we evaluate different error formulations for 3-view point matches. The naive baseline is to simply average the two-view Sampson errors (4),\\n\\n\\\\[\\nE_{\\\\text{pair}} = E_S(x, x', E_{12}) + E_S(x, x'', E_{13}) + E_S(x', x'', E_{23})\\n\\\\]\\n\\nArea under the CDF up to 1 px error as a ratio of the complete square.\"}"}
{"id": "CVPR-2024-2046", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"alternative is to only consider the three pairwise constraints,\\n\\\\[ C_3(x, x', x'') = x' \\\\top E_{12} x, x' \\\\top E_{13} x, x' \\\\top E_{23} x \\\\] (53)\\n\\nNote that applying the Sampson approximation to \\n\\\\[ C_3 \\\\] yields a different error compared to \\n\\\\[ E_{pair} \\\\] (50), as the constraints are considered jointly.\\n\\nIn this section we evaluate the following errors\\n- \\\\( E_{pair} \\\\) - averaging the pairwise Sampson errors (50)\\n- \\\\( E_9 \\\\) - applying Sampson approximation to \\\\( C_9 \\\\) (51)\\n- \\\\( E_4 \\\\) - applying Sampson approximation to \\\\( C_4 \\\\) (52)\\n- \\\\( E_3 \\\\) - applying Sampson approximation to \\\\( C_3 \\\\) (53)\\n\\nWe also include two combinations of the above. First taking 3 out of the 4 constraints from \\\\( C_4 \\\\), denoted \\\\( C_4:3 \\\\), and one where we combine \\\\( C_3 \\\\) and \\\\( C_4 \\\\) by taking two quadratic and one cubic constraint, denoted \\\\( C_4:1 \\\\), \\\\( 3:2 \\\\). We also consider a set of pseudo-Sampson approximations which take the form\\n\\\\[ \\\\| C \\\\|_2 / \\\\| J \\\\|_2 \\\\], and thus avoid computing matrix inverses as in Section 2.2. This can be seen as a naive extension of the 1-dimensional Sampson approximation (8) to the multi-dimensional case. We denote these as \\\\( \\\\hat{E}_9 \\\\), \\\\( \\\\hat{E}_4 \\\\), \\\\( \\\\hat{E}_3 \\\\).\\n\\nExperiment setup.\\nTo compare the approximations, we generate synthetic camera triplets (70\u00b0 field-of-view, 1000x1000 pixel images) observing a 3D point. To the three projections we add normally distributed noise with standard deviation \\\\( \\\\sigma \\\\in \\\\{1, 5, 10\\\\} \\\\) px. We obtain the reference ground-truth reprojection error by directly optimizing over the 3D point. For each of the evaluated approximated error functions, we compute the difference to the ground-truth.\\n\\nResults and Discussion.\\nTable 3 shows the errors for 100k synthetic instances. We compute the Area-Under-Curve up to 1 pixel deviation from the reference error. The Sampson approximation of the epipolar constraints \\\\( C_3 \\\\) (53) yields the best approximation. In particular, we can see that the naive approximation \\\\( E_{pair} \\\\) that averages the pairwise Sampson errors, is significantly worse. Interestingly, we can also see that \\\\( C_4 \\\\) performs much worse compared to both \\\\( C_3 \\\\) and the mixed variants \\\\( C_4:3 \\\\) and \\\\( C_4:1 \\\\), \\\\( 3:2 \\\\). This is consistent with the discussion in Sec. 3.3.\\n\\n4.3. Application: Vanishing Point Estimation\\nWe now show another example with a 1-dimensional quadratic constraint. Consider a line-segment \\\\( (x_1, x_2) \\\\) \u2208 \\\\( S^2 \\\\times S^2 \\\\) and a vanishing point \\\\( v \\\\) \u2208 \\\\( S^2 \\\\). Assuming we want to refine \\\\( v \\\\), it is reasonable to consider what is the smallest pertubation of the line endpoints such that the line passes through the vanishing point, i.e. satisfy the constraint\\n\\\\[ C(x_1, x_2) = v \\\\top (x_1 \\\\times v) \\\\] (54)\\n\\nDifferentiating with respect to the image points we get,\\n\\\\[ J = x_2 \\\\times v, v \\\\times x_1 S \\\\] (55)\\nwhere \\\\( S = I_2 0 \\\\), and we can directly setup a Sampson approximation of the line-segment to vanishing point distance as\\n\\\\[ |C(x_1, x_2)| / \\\\| J \\\\| \\\\]. For this problem the ground-truth error \\\\( E_G \\\\) can be computed in closed form using SVD.\\n\\nIn Section 3.1 we derived bounds that relate the true geometric error \\\\( \\\\epsilon_G \\\\) and the Sampson approximation \\\\( \\\\epsilon_S \\\\), \\\\( B_l := 1 / \\\\tau \\\\leq \\\\| \\\\epsilon_S \\\\| / \\\\| \\\\epsilon_G \\\\| \\\\leq 1 + \\\\rho^2 \\\\| J \\\\| \\\\| \\\\epsilon_G \\\\| =: B_u \\\\) (56)\\n\\nNext we evaluate how tight these bounds are on real data.\\n\\nExperiment Setup.\\nFor the experiment, we consider circa 350k pairs of line segments and vanishing points collected from the YUB+ [5] and NYU VP [10, 17]. The line segments are detected using DeepLSD [18] and using Progressive-X [1] we estimate a set of vanishing points.\\n\\nResults and Discussion.\\nFigure 5 shows \\\\( B_u \\\\), \\\\( B_l \\\\) for each of line-vanishing point pairs and in Figure 6 we show the distribution of the difference between the bounds. As can be seen in the figures the approximation works extremely well for this setting. We also experimented with refining the vanishing points using the Sampson error but found that the results are very similar to minimizing the mid-point error (as was done in [18]). The full results and details can be found in the Supplementary Material.\\n\\n4.4. Application: 2D/3D Reprojection Error\\nMinimizing the square reprojection error, i.e. deviation between the observed 2D point and the projection of the 3D point, assumes a Gaussian noise model on the 2D observations. However, in many scenarios, we also have noise in the 3D points. In this section we consider the case where we have a known covariance for both the 2D and 3D points.\"}"}
{"id": "CVPR-2024-2046", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u03c4 = 5 px\\n\\n\u03c4 = 10 px\\n\\n\u03c4 = 20 px\\n\\nAUC (\u2191) \u03b5 R (\u2193) \u03b5 t (\u2193) AUC (\u2191) \u03b5 R (\u2193) \u03b5 t (\u2193) RT\\n\\nReprojection error (60) 0.358 1.03 3.20 0.350 1.06 3.24 0.311 1.21 3.63 0.7 ms\\n\\nReprojection error + Cov. (61) 0.367 1.01 3.10 0.379 0.99 2.99 0.378 0.99 3.00 10.1 ms\\n\\nSampson approximation 0.367 1.01 3.10 0.379 0.98 2.99 0.375 1.00 3.02 2.4 ms\\n\\nTable 4. Pose refinement on 7Scenes. Table shows the Area-Under-Curve (AUC) @ 5cm position errors and the median errors in rotation \u03b5 R (deg.) and translation \u03b5 t (cm). Results are reported for different inlier thresholds \u03c4. For large thresholds, more uncertain points are included and the improvement from the covariance weighting is more significant. However, larger errors also make the linearization point (the original correspondence) worse, degrading the Sampson approximation. For the two lower thresholds, the Sampson approximation of 2D-3D covariance weighted reprojection error gives almost identical results as performing the full (expensive) optimization.\\n\\nFigure 5. Evaluation of the bounds for VP-line error. The lower bound \\\\( B_l \\\\) and upper bound \\\\( B_u \\\\) for the \\\\( \\\\approx 350k \\\\) VP-line pairs in the combined dataset. For illustration the pairs are sorted w.r.t. the tightness of the bound.\\n\\nFigure 6. Evaluation of the bounds for VP-line error. Distribution of the log \\\\( 10 \\\\) differences between the upper and lower bounds.\\n\\nGiven a point correspondence \\\\((x, X) \\\\in \\\\mathbb{R}^2 \\\\times \\\\mathbb{R}^3\\\\), together with covariances \\\\( \\\\Sigma_2^D \\\\in \\\\mathbb{R}^{2 \\\\times 2} \\\\) and \\\\( \\\\Sigma_3^D \\\\in \\\\mathbb{R}^{3 \\\\times 3} \\\\), the maximum likelihood estimate is then given by\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\min_{\\\\epsilon_2, \\\\epsilon_3} & \\\\quad \\\\|\\\\epsilon_2\\\\|^2 \\\\Sigma_2^D + \\\\|\\\\epsilon_3\\\\|^2 \\\\Sigma_3^D \\\\\\\\\\n\\\\text{s.t.} & \\\\quad C(x + \\\\epsilon_2, X + \\\\epsilon_3) = 0\\n\\\\end{align*}\\n\\\\]\\n\\nwhere \\\\( C(x, X) \\\\) encodes the reprojection equations, i.e.\\n\\n\\\\[\\nC(x, X) = \\\\begin{bmatrix}\\nI_{2 \\\\times 2},\\n-x\\n\\\\end{bmatrix} (R X + t) = 0\\n\\\\]\\n\\nExperiment Setup. For the experiment we consider the visual localization benchmark setup on 7Scenes [23] dataset. Using HLoc [21] we establish 2D-3D matches and estimate an initial camera pose for each query image. For the experiment we then refine this camera pose, including the uncertainty in both the 2D and 3D points. The 2D covariances are assumed to be unit gaussians and to obtain the 3D covariances we propagate the 2D covariances from the mapping images used to triangulate the 3D point.\\n\\nResults and Discussion. Table 4 shows the average pose error across all scenes (per-scene results are available in the Supplementary Material). We compare only minimizing the reprojection error (only 2D noise)\\n\\n\\\\[\\n\\\\min_{R, t} \\\\|x_k - \\\\pi(R X_k + t)\\\\|^2 \\\\Sigma_2\\n\\\\]\\n\\nwith optimizing over the 3D points as well (2D/3D noise),\\n\\n\\\\[\\n\\\\min_{R, t, \\\\{\\\\hat{X}_k\\\\}} \\\\|x_k - \\\\pi(R \\\\hat{X}_k + t)\\\\|^2 \\\\Sigma_2 + \\\\|\\\\hat{X}_k - X_k\\\\|^2 \\\\Sigma_3\\n\\\\]\\n\\nand applying the Sampson approximation to (59). As shown in Table 4, including the uncertainty of the 3D point can greatly improve the pose accuracy. Further, the Sampson approximation works well in this setting and it is only when we include matches with very large errors (20 pixels) that performance degrades.\\n\\nNote that the optimization problem in (61) requires parameterizing each individual 3D point, potentially leading to hundreds or thousands of extra parameters compared to (60) that only optimize over the 6-DoF in the camera pose. Since the Sampson approximation eliminates the extra unknowns, it also allows us to only optimize over the camera pose while modelling the 3D uncertainty. Table 4 also shows the average runtime in milliseconds for the query images. Minimizing the Sampson approximation is significantly faster compared to (61).\\n\\n5. Conclusions\\n\\nThe Sampson approximation, originally applied to compute conic-point distances, has shown itself to surprisingly versatile in the context of robust model fitting. While it has been known that it works extremely well in practice, we provide the first theoretical bounds on the approximation error. In multiple experiments on real data in different application contexts we have validated our theory and highlighted the usefulness of the approximation.\\n\\nAcknowledgments: Viktor Larsson was supported by the strategic research project ELLIIT and the Swedish Research Council (grant no. 2023-05424). Felix Rydell was supported by the Knut and Alice Wallenberg Foundation within their WASP (Wallenberg AI, Autonomous Systems and Software Program) AI/Math initiative. Ang\u00e9lica Torres was supported by DFG grant 464109215 within the priority programme SPP 2298 \\\"Theoretical Foundations of Deep Learning\\\".\"}"}
{"id": "CVPR-2024-2046", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Daniel Barath and Jiri Matas. Progressive-X: Efficient, anytime, multi-model fitting algorithm. In International Conference on Computer Vision (ICCV), 2019.\\n\\n[2] Paul Breiding and Sascha Timme. Homotopycontinuation.jl: A package for homotopy continuation in Julia. In James H. Davenport, Manuel Kauers, George Labahn, and Josef Urban, editors, Mathematical Software \u2013 ICMS 2018, pages 458\u2013465, Cham, 2018. Springer International Publishing.\\n\\n[3] Wojciech Chojnacki, Michael J. Brooks, Anton Van Den Hengel, and Darren Gawley. On the fitting of surfaces to data with covariances. IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI), 2000.\\n\\n[4] Ond\u0159ej Chum, Tom\u00e1\u0161 Pajdla, and Peter Sturm. The geometric error for homographies. Computer Vision and Image Understanding (CVIU), 2005.\\n\\n[5] Patrick Denis, James H Elder, and Francisco J Estrada. Efficient edge-based methods for estimating manhattan frames in urban imagery. In European Conference on Computer Vision (ECCV), 2008.\\n\\n[6] Jan Draisma, Emil Horobet\u02d8, Giorgio Ottaviani, Bernd Sturmfels, and Rekha R Thomas. The euclidean distance degree of an algebraic variety. Foundations of computational mathematics, 16:99\u2013149, 2016.\\n\\n[7] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university press, 2003.\\n\\n[8] Richard I Hartley and Peter Sturm. Triangulation. Computer Vision and Image Understanding (CVIU), 1997.\\n\\n[9] Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas, Pascal Fua, Kwang Moo Yi, and Eduard Trulls. Image Matching Across Wide Baselines: From Paper to Practice. International Journal of Computer Vision (IJCV), 129(2):517\u2013547, Feb. 2021.\\n\\n[10] Florian Kluger, Eric Brachmann, Hanno Ackermann, Carsten Rother, Michael Ying Yang, and Bodo Rosenhahn. CONSAC: Robust multi-model fitting by conditional sample consensus. In Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\n[11] Viktor Larsson, Kalle Astrom, and Magnus Oskarsson. Efficient solvers for minimal problems by syzygy-based reduction. In Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\n[12] Spyridon Leonardos, Roberto Tron, and Kostas Daniilidis. A metric parametrization for trifocal tensors with non-colinear pinholes. In Computer Vision and Pattern Recognition (CVPR), 2015.\\n\\n[13] Philipp Lindenberger, Paul-Edouard Sarlin, and Marc Pollefeys. Lightglue: Local feature matching at light speed. In International Conference on Computer Vision (ICCV), 2023.\\n\\n[14] Peter Lindstrom. Triangulation made easy. In Computer Vision and Pattern Recognition (CVPR), 2010.\\n\\n[15] Quan-Tuan Luong and Olivier D Faugeras. The fundamental matrix: Theory, algorithms, and stability analysis. International Journal of Computer Vision (IJCV), 1996.\\n\\n[16] Laurentiu G Maxim, Jose I Rodriguez, and Botong Wang. Euclidean distance degree of the multiview variety. SIAM Journal on Applied Algebra and Geometry, 4(1):28\u201348, 2020.\\n\\n[17] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In European Conference on Computer Vision (ECCV), 2012.\\n\\n[18] R\u00e9mi Pautrat, Daniel Barath, Viktor Larsson, Martin R Oswald, and Marc Pollefeys. Deeplsd: Line segment detection and refinement with deep image gradients. In Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[19] Felix Rydell, Elima Shehu, and Angelica Torres. Theoretical and numerical analysis of 3d reconstruction using point and line incidences. arXiv preprint arXiv:2303.13593, 2023.\\n\\n[20] Paul D Sampson. Fitting conic sections to \u201cvery scattered\u201d data: An iterative refinement of the bookstein algorithm. Computer graphics and image processing, 1982.\\n\\n[21] Paul-Edouard Sarlin, Cesar Cadena, Roland Siegwart, and Marcin Dymczyk. From coarse to fine: Robust hierarchical localization at large scale. In Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\n[22] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superglue: Learning feature matching with graph neural networks. In Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\n[23] Jamie Shotton, Ben Glocker, Christopher Zach, Shahram Izadi, Antonio Criminisi, and Andrew Fitzgibbon. Scene coordinate regression forests for camera relocalization in RGB-D images. In Computer Vision and Pattern Recognition (CVPR), 2013.\\n\\n[24] Peter Sturm. Vision 3D non calibr\u00e9e: contributions \u00e0 la reconstruction projective et \u00e9tude des mouvements critiques pour l'auto-calibrage. PhD thesis, Institut National Polytechnique de Grenoble-INPG, 1997.\\n\\n[25] Jiaming Sun, Zehong Shen, Yuang Wang, Hujun Bao, and Xiaowei Zhou. Loftr: Detector-free local feature matching with transformers. In Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\n[26] Zygmunt L Szpak, Wojciech Chojnacki, and Anton Van Den Hengel. Guaranteed ellipse fitting with the sampson distance. In European Conference on Computer Vision (ECCV), 2012.\\n\\n[27] Gabriel Taubin. Estimation of planar curves, surfaces, and nonplanar space curves defined by implicit equations with applications to edge and range image segmentation. IEEE Trans. Pattern Analysis and Machine Intelligence (PAMI), 1991.\\n\\n[28] Mikhail Terekhov and Viktor Larsson. Tangent sampson error: Fast approximate two-view reprojection error for central camera models. In International Conference on Computer Vision (ICCV), 2023.\"}"}
{"id": "CVPR-2024-2046", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Revisiting Sampson Approximations for Geometric Estimation Problems\\n\\nFelix Rydell\\nKTH Royal Institute of Technology\\ngfelixry@kth.se\\n\\nAngelica Torres\\nMax Planck Institute for Mathematics in the Sciences\\nangelica.torres@mis.mpg.de\\n\\nViktor Larsson\\nLund University\\nviktor.larsson@math.lth.se\\n\\nAbstract\\n\\nMany problems in computer vision can be formulated as geometric estimation problems, i.e. given a collection of measurements (e.g. point correspondences) we wish to fit a model (e.g. an essential matrix) that agrees with our observations. This necessitates some measure of how much an observation \\\"agrees\\\" with a given model. A natural choice is to consider the smallest perturbation that makes the observation exactly satisfy the constraints. However, for many problems, this metric is expensive or otherwise intractable to compute. The so-called Sampson error approximates this geometric error through a linearization scheme. For epipolar geometry, the Sampson error is a popular choice and in practice known to yield very tight approximations of the corresponding geometric residual (the reprojection error).\\n\\nIn this paper we revisit the Sampson approximation and provide new theoretical insights as to why and when this approximation works, as well as provide explicit bounds on the tightness under some mild assumptions. Our theoretical results are validated in several experiments on real data and in the context of different geometric estimation tasks.\\n\\n1. Introduction\\n\\nEstimating a geometric model from a collection of measurements is a common task in computer vision pipelines. Prerequisite for any such estimation is the ability to check whether a measurement is consistent with a given model. This can be used both to filter outlier measurements or as a loss for non-linear model refinement. For generative models, i.e. models that can produce the idealized measurements, it is straightforward to check this consistency, e.g. computing the difference between the projection and observed 2D keypoint. However, in many cases the relation between model and data is implicitly encoded through a set of geometric constraints. One such example is the epipolar constraint in two-view geometry,\\n\\n\\\\[ C(x_1, x_2, E) = (x_2; 1)^\\\\top E(x_1; 1) = 0 \\\\]\\n\\nrelating the essential (or fundamental) matrix \\\\( E \\\\) with the point correspondence \\\\((x_1, x_2) \\\\in \\\\mathbb{R}^2 \\\\times \\\\mathbb{R}^2\\\\). For a noisy correspondence the constraint will not be satisfied exactly, so it becomes natural to ask: How close is this match to agreeing with the model? Mathematically, this can be formulated as\\n\\n\\\\[\\n\\\\min \\\\hat{x}_1, \\\\hat{x}_2 \\\\quad \\\\| \\\\hat{x}_1 - x_1 \\\\|^2 + \\\\| \\\\hat{x}_2 - x_2 \\\\|^2 \\\\quad \\\\text{s.t.} \\\\quad C(\\\\hat{x}_1, \\\\hat{x}_2, E) = 0\\n\\\\]\\n\\ni.e., what is the smallest perturbation to the points such that they exactly satisfy the constraint. This minimum distance gives a measure of the consistency between the model (the essential matrix \\\\( E \\\\)) and the measurements (\\\\( x_1 \\\\) and \\\\( x_2 \\\\)).\\n\\n\"}"}
{"id": "CVPR-2024-2046", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For the specific case of the epipolar constraint, the optimal solution to (2) can be computed \\\\[8, 14\\\\] by finding the roots to degree 6 univariate polynomial. As this procedure is relatively expensive, and difficult to integrate into non-linear refinement methods, it is common in practice to instead use some approximation of the error in (2). One example of this is the Sampson error \\\\[15, 20\\\\], defined as\\n\\\\[\\nE^2_S = \\\\frac{(x_1^2; 1)^\\\\top E(x_1; 1)}{\\\\|E_1\\\\|_2} + \\\\frac{\\\\|E_1^\\\\top (x_2^2; 1)\\\\|_2}{\\\\|E_1\\\\|_2}.\\n\\\\]\\nThis expression is derived by linearizing the constraint in (2), i.e. by replacing \\\\(C(\\\\hat{x}_1, \\\\hat{x}_2, E) = 0\\\\) by the constant and linear terms of its Taylor expansion around the data point \\\\((x_1, x_2)\\\\). It provides a good approximation of (2) if the curvature of the constraint at the data point is small enough in relation to the size of \\\\(C(x_1, x_2, E)\\\\), while being cheap to compute and easy to optimize in non-linear refinement. This approach was originally used by Sampson \\\\[20\\\\] to approximate distances between conics and points, but has since been applied to many other geometric models.\\n\\nIn this paper we revisit the Sampson approximation and provide new theoretical insights into why (and when) the approximation works well. Under relatively mild assumptions we derive explicit bounds on the tightness of the approximation. These bounds are experimentally validated on real data and showcased in multiple applications from geometric computer vision (two- and three-view estimation, vanishing point estimation and resectioning).\\n\\nThe paper is organized as follows: Section 1.1 discusses the related work. In Section 2 we first present the classical derivation of the Sampson error, along with some geometrical interpretations of the approximation. In Section 3 we present our bounds on the approximation. Finally, in Section 4 we provide some experimental evaluation.\\n\\n1.1. Related Work\\n\\nThe Geometric Error. In applied algebraic geometry, fitting noisy data points to a mathematical model defined by polynomials has recently seen a lot of interest \\\\[6\\\\], and more specifically for 3D reconstruction \\\\[16, 19\\\\]. One of the main contributions is the development and computation of the so called Euclidean distance degree, which is the number of critical points to the closest point optimization problem given generic data. It expresses the algebraic complexity of fitting data to a model; the higher the Euclidean distance degree is, the more computationally expensive this optimization is. It is used to implement efficient solvers in Homotopy Continuation \\\\[2\\\\] or for solving the associated polynomials systems via specialized symbolic solvers \\\\[11\\\\].\\n\\nThe Sampson Error. The Sampson approximation was first proposed in \\\\[20\\\\] to approximate the point-conic distance. It was also later derived independently by Taubin \\\\[27\\\\]. Since then it has appeared in numerous papers for different problems. Luong and Faugeras \\\\[15\\\\] introduced it for approximating the reprojection error in epipolar geometry. The corresponding geometric error for homographies was first introduced by Sturm \\\\[24\\\\] and later revisited by Chum et al. \\\\[4\\\\] who also derived the Sampson approximation in this setting. Leonardos et al. \\\\[12\\\\] used the Sampson error for point-line-line constraint from the trifocal tensor. Chojnacki et al. \\\\[3\\\\] considered how to integrate known measurement covariances, and recently Terekhov et al. \\\\[28\\\\] generalized the Sampson error in epipolar geometry to handle arbitrary central camera model. In \\\\[26\\\\] it was used in an optimization method for conic fitting that guarantees convergence to an ellipse. This extensive use of the Sampson approximation for geometric problems shows its versatility, which motivates a deeper theoretical study in a more general setting. This is precisely our goal in this work.\\n\\n2. The Sampson Approximation\\n\\nWe consider geometric residuals analogous to (2) for general models, i.e. problems of the form\\n\\\\[\\nE^2_G(z, \\\\theta) = \\\\min_{\\\\varepsilon} \\\\|\\\\varepsilon\\\\|_2\\n\\\\]\\n\\\\[\\n\\\\text{s.t. } C(z + \\\\varepsilon, \\\\theta) = 0\\n\\\\]\\nwhere \\\\(z \\\\in \\\\mathbb{R}^n\\\\) is our measurement, \\\\(\\\\theta\\\\) our model parameters, and \\\\(C(z, \\\\theta)\\\\) our geometric constraint. In the special case of epipolar geometry, we have that \\\\(z = [x_1, x_2]\\\\) consists of the two matching image points, the model parameters are the entries of the essential matrix \\\\(E\\\\), and the constraint is,\\n\\\\[\\nC(z, E) = (x_2^2; 1)^\\\\top E(x_1; 1) = 0.\\n\\\\]\\nIn the rest of the paper we consider a general polynomial constraint \\\\(C(z, \\\\theta)\\\\) and will for notational convenience drop the dependence on the model parameters \\\\(\\\\theta\\\\) in most places.\\n\\nSince (5) often does not admit a simple closed form solution, the idea in \\\\[20\\\\] is to linearize the constraint, \\\\(C(z + \\\\varepsilon) = 0\\\\), at the original measurement \\\\(z\\\\), i.e.\\n\\\\[\\nE^2_S(z, \\\\theta) = \\\\min_{\\\\varepsilon} \\\\|\\\\varepsilon\\\\|_2\\n\\\\]\\n\\\\[\\n\\\\text{s.t. } C(z) + J\\\\varepsilon = 0\\n\\\\]\\nwhere \\\\(J = \\\\partial C(z)/\\\\partial z\\\\) is the Jacobian of the constraint, evaluated at \\\\(z\\\\). Introducing a Lagrangian for (8),\\n\\\\[\\nL(\\\\varepsilon, \\\\alpha) = \\\\frac{1}{2} \\\\|\\\\varepsilon\\\\|_2^2 + \\\\alpha (C(z) + J\\\\varepsilon)\\n\\\\]\\nwe get the first-order constraints as\\n\\\\[\\n\\\\varepsilon + \\\\alpha J^\\\\top = 0,\\\\quad C(z) + J\\\\varepsilon = 0.\\n\\\\]\"}"}
{"id": "CVPR-2024-2046", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Inserting the first equation into the second yields\\n\\\\[ C(z) - \\\\alpha \\\\|J\\\\|_2^2 = 0 = \\\\Rightarrow \\\\varepsilon = -C(z) \\\\|J\\\\|_2^2J^\\\\top. \\\\] (12)\\n\\nThus the minimum in (8) is given by\\n\\\\[ E_2 S(z, \\\\theta) = C(z) \\\\|J\\\\|_2^2J^\\\\top = C(z)\\\\|J\\\\|_2^2. \\\\] (13)\\n\\nIn the special case of epipolar geometry, replacing \\\\( C(z) \\\\) with the epipolar constraint, we arrive at the classical formula for the Sampson error (4).\\n\\n2.1. Multiple Constraints and Covariances\\nThe Sampson error for a single constraint, discussed previously, can easily be generalized to the case where we measure the deviation in the Mahalanobis distance from a model defined by multiple constraints. See [7, p128] for more details. Assume that the \\\\( N \\\\) constraints are given by\\n\\\\[ C(z) = (C_1(z), C_2(z), \\\\ldots, C_N(z))^\\\\top = 0, \\\\] (14)\\nand we want to measure the deviation \\\\( \\\\varepsilon \\\\) in Mahalanobis distance for a given positive definite covariance \\\\( \\\\Sigma \\\\). This can be formulated as\\n\\\\[ E_2 G(z, \\\\theta) = \\\\min_{\\\\varepsilon} \\\\|\\\\varepsilon\\\\|_2^2 \\\\Sigma \\\\] s.t. \\\\[ C(z + \\\\varepsilon) = 0, \\\\] (15)\\nwhere \\\\( \\\\|\\\\varepsilon\\\\|_2^2 \\\\Sigma = \\\\varepsilon^\\\\top \\\\Sigma^{-1/2} \\\\varepsilon \\\\). The Lagrangian of the linearized constraint then becomes\\n\\\\[ L(\\\\varepsilon, \\\\alpha) = \\\\frac{1}{2} \\\\|\\\\varepsilon\\\\|_2^2 \\\\Sigma + \\\\alpha^\\\\top (C(z) + J\\\\varepsilon), \\\\] (17)\\nwhere \\\\( J \\\\in \\\\mathbb{R}^{N \\\\times n} \\\\) is the Jacobian of \\\\( C \\\\) evaluated at \\\\( z \\\\in \\\\mathbb{R}^n \\\\).\\n\\nFirst order conditions again yield\\n\\\\[ \\\\Sigma^{-1/2} \\\\varepsilon + J^\\\\top \\\\alpha = 0, \\\\] (18)\\n\\\\[ C(z) + J\\\\varepsilon = 0. \\\\] (19)\\nWe get \\\\( \\\\varepsilon = -\\\\Sigma J^\\\\top \\\\alpha \\\\), allowing us to solve for \\\\( \\\\alpha \\\\) as\\n\\\\[ C(z) - J \\\\Sigma J^\\\\top \\\\alpha = 0 = \\\\Rightarrow \\\\alpha = (J \\\\Sigma J^\\\\top)^{-1} C(z), \\\\] (19)\\nwhen \\\\( J \\\\Sigma J^\\\\top \\\\) is invertible (in Section 2.2 we consider the general case). Let \\\\( \\\\varepsilon_S \\\\) denote the argmin of the linearized optimization problem. From this we get\\n\\\\[ \\\\varepsilon_S = -\\\\Sigma J^\\\\top (J \\\\Sigma J^\\\\top)^{-1} C(z), \\\\] (20)\\nand finally,\\n\\\\[ \\\\|\\\\varepsilon_S\\\\|_2^2 \\\\Sigma = \\\\varepsilon_S \\\\Sigma^{-1/2} \\\\varepsilon_S, \\\\] which simplifies to\\n\\\\[ C(z)^\\\\top (J \\\\Sigma J^\\\\top)^{-1} C(z) = \\\\|C(z)\\\\|_2^2 J \\\\Sigma J^\\\\top. \\\\] (21)\\n\\n2.2. General Case\\nLet \\\\((\\\\cdot)^\\\\dagger\\\\) denote the Moore-Penrose psuedo-inverse of a matrix. Assuming that the linearized equation \\\\( C(z) + J\\\\varepsilon = 0 \\\\) has at least one solution, meaning that \\\\( C(z) \\\\in \\\\text{Im} J = \\\\text{Im} J \\\\Sigma^{1/2} \\\\), then\\n\\\\[ (J \\\\Sigma^{1/2})^\\\\dagger (J \\\\Sigma^{1/2}) C(z) = C(z). \\\\] In this case, all solutions to the linearized equation can be written as\\n\\\\[ \\\\varepsilon(\\\\mu) = -\\\\Sigma^{-1/2} (J \\\\Sigma^{1/2})^\\\\dagger C(z) + M\\\\mu, \\\\mu \\\\in \\\\mathbb{R}^N \\\\] (22)\\nwhere the columns of \\\\( M \\\\) is a basis for the nullspace of \\\\( J \\\\).\\n\\nIn order to find \\\\( \\\\varepsilon_S \\\\) we consider\\n\\\\[ \\\\|\\\\varepsilon_S\\\\|_2^2 \\\\Sigma = \\\\min_\\\\mu \\\\| - (J \\\\Sigma^{1/2})^\\\\dagger C(z) + \\\\Sigma^{-1/2} M\\\\mu \\\\| \\\\] (23)\\nHere we note that \\\\( M^\\\\top \\\\Sigma^{-1/2} (J \\\\Sigma^{1/2})^\\\\dagger = 0 \\\\), which implies that the optimal choice is \\\\( \\\\mu = 0 \\\\). To be precise, we get\\n\\\\[ \\\\varepsilon_S = -\\\\Sigma^{1/2} (J \\\\Sigma^{1/2})^\\\\dagger C(z). \\\\] (25)\\nand thus\\n\\\\[ \\\\|\\\\varepsilon_S\\\\|_2^2 \\\\Sigma = \\\\| (J \\\\Sigma^{1/2})^\\\\dagger C(z) \\\\|_2^2. \\\\] (26)\\n\\nRemark 2.1.\\nFor the case of \\\\( \\\\Sigma = I \\\\), we note that \\\\( \\\\|\\\\varepsilon_S\\\\|_2 = \\\\|J^\\\\dagger C(z_0)\\\\| \\\\) is the length of the Gauss-Newton step, \\\\( z_1 = z_0 - (J^\\\\top J)^{-1} J^\\\\top C(z_0) \\\\) applied to solving \\\\( C(x) = 0 \\\\) starting from the point \\\\( z_0 \\\\).\\n\\nFigure 1 visualizes this for one constraint in two variables.\\n\\n3. Bounding the Approximation Error\\nIn this section we investigate the Sampson approximation (8) in terms of how well it approximates the geometric error (5). We do this by constructing explicit bounds. We start by studying one quadratic constraint and then we generalize our methods by considering constraints of higher degrees. Finally, we discuss the case of multiple constraints, but leave the details in the Supplementary Material.\\n\\n3.1. One Quadratic Constraint\\nConsider the case where we have one quadratic constraint \\\\( C(z) \\\\). The classical Sampson error (4) falls into this category as the epipolar constraint is a quadratic polynomial in terms of the image points.\\n\\nWe write \\\\( \\\\varepsilon_G \\\\) for the argmin of (5) and \\\\( \\\\varepsilon_S \\\\) for the argmin of (8). Given a fixed data point \\\\( z \\\\in \\\\mathbb{R}^n \\\\), we write \\\\( H \\\\) for the Hessian of \\\\( C \\\\) at \\\\( z \\\\). Since \\\\( C \\\\) is a quadratic polynomial,\\n\\\\[ C(z + \\\\varepsilon) = C(z) + J\\\\varepsilon + \\\\frac{1}{2} \\\\varepsilon^\\\\top H\\\\varepsilon. \\\\] (28)\"}"}
{"id": "CVPR-2024-2046", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use that any vector $\\\\varepsilon$ satisfies the inequality\\n$$|\\\\varepsilon^\\\\top H\\\\varepsilon| \\\\leq \\\\rho \\\\|\\\\varepsilon\\\\|_2$$\\n(29)\\nwhere $\\\\rho$ is the spectral radius of the Hessian of $C(z)$, that is, the maximum of the absolute values of its eigenvalues.\\n\\nFirst we present an upper bound on the Sampson approximation $\\\\|\\\\varepsilon_S\\\\|$ in terms of the geometric residual $\\\\|\\\\varepsilon_G\\\\|$.\\n\\n**Proposition 3.1.** When the optimization problem (5) only has one quadratic constraint and $J \\\\neq 0$, then\\n$$\\\\|\\\\varepsilon_S\\\\| \\\\leq \\\\|\\\\varepsilon_G\\\\| + \\\\frac{\\\\rho}{2} \\\\|J\\\\| \\\\|\\\\varepsilon_G\\\\|^2.$$ \\n(30)\\n\\nUnder the assumption that $\\\\frac{\\\\rho}{2} \\\\|J\\\\|$ is reasonably small (i.e. function is approximately linear), we can interpret this proposition as saying that if $\\\\varepsilon_S$ is big, then $\\\\varepsilon_G$ is also big.\\n\\nThe condition $J \\\\neq 0$ is reasonable, as the approximation is undefined if the linearized constraint set is empty.\\n\\n**Proof.** Since $\\\\varepsilon_G$ satisfies the constraint (28), we have that\\n$$0 = C(z) + J\\\\varepsilon_G + \\\\frac{1}{2}(\\\\varepsilon_G^\\\\top H\\\\varepsilon_G)$$\\n(31)\\n\\nIn other words, $\\\\varepsilon_G + J^\\\\top \\\\|J\\\\| 2(\\\\varepsilon_G)^\\\\top H\\\\varepsilon_G$ satisfies the linearized constraint. Since $\\\\varepsilon_S$ is by definition the smallest vector that satisfies the linearized constraint, we get that\\n$$\\\\|\\\\varepsilon_S\\\\| \\\\leq \\\\|\\\\varepsilon_G\\\\| + \\\\frac{1}{2} \\\\|J\\\\| \\\\|(\\\\varepsilon_G)^\\\\top H\\\\varepsilon_G\\\\|.$$ \\n(33)\\nFinally, using the inequality of the spectral radius mentioned above, we get the upper bound.\\n\\nNext we give an upper bound of $\\\\|\\\\varepsilon_G\\\\|$ in terms of $\\\\|\\\\varepsilon_S\\\\|$.\\n\\n**Proposition 3.2.** If the minimization problem (5) only has one quadratic constraint, $J \\\\neq 0$ and $\\\\|J\\\\|_4 \\\\geq 2 |C(z)| \\\\|JHJ^\\\\top|$, then\\n$$\\\\|\\\\varepsilon_G\\\\| \\\\leq 2 \\\\|\\\\varepsilon_S\\\\|.$$ \\n(35)\\n\\nThe assumption should be intuitively understood as the model being close enough to linear in the direction of $J$ locally around the input $z$, relative to the size of $C(z)$. Note that from the Proposition, we also have\\n$$\\\\|\\\\varepsilon_S - \\\\varepsilon_G\\\\| \\\\leq \\\\|\\\\varepsilon_S\\\\| + \\\\|\\\\varepsilon_G\\\\| \\\\leq 3 \\\\|\\\\varepsilon_S\\\\|.$$ \\n(36)\\n\\n**Proof.** Define $\\\\varepsilon(\\\\lambda) := \\\\lambda \\\\|J\\\\| J^\\\\top$ for $\\\\lambda \\\\in \\\\mathbb{R}$ and consider $C(z + \\\\varepsilon(\\\\lambda)) = C(z) + \\\\|J\\\\| \\\\lambda + JHJ^\\\\top \\\\lambda^2$, \\n(37)\\nwhich is a quadratic polynomial in $\\\\lambda$. Evaluating this polynomial at $\\\\lambda = -\\\\frac{2}{\\\\|J\\\\|} |C(z)|$, we get\\n$$-C(z) + 2 JHJ^\\\\top \\\\|J\\\\| C(z) \\\\lambda^2.$$ \\n(38)\\nThe absolute value of the second term can by assumption be bounded from above by $|C(z)|$. This means that either $C(z) \\\\geq 0$ and $C(z + \\\\varepsilon(\\\\lambda)) \\\\leq 0$ or the other way around. By continuity of polynomials, there must exist a solution $\\\\lambda^*$ to (37) in the interval $(0, -\\\\frac{2}{\\\\|J\\\\|} |C(z)|)$. Since $\\\\varepsilon_G$ is the smallest vector satisfying this, we have\\n$$\\\\|\\\\varepsilon_G\\\\| \\\\leq \\\\|\\\\varepsilon(\\\\lambda^*)\\\\| = |\\\\lambda^*| \\\\leq 2 |C(z)| / \\\\|J\\\\| = 2 \\\\|\\\\varepsilon_S\\\\|.$$ \\n(39)\\n\\nIn order to get a sharper bound in the case that $JHJ^\\\\top \\\\neq 0$, we may instead solve the quadratic equation (37) directly:\\n$$\\\\lambda^* = -\\\\|J\\\\|^3 \\\\pm \\\\|J\\\\|^p \\\\|J\\\\|^4 - 2 |C(z)| JHJ^\\\\top JHJ^\\\\top JHJ^\\\\top.$$ \\n(40)\\nLet $\\\\lambda^*$ be the solution with smallest absolute value, then\\n$$\\\\|\\\\varepsilon_G\\\\| \\\\leq |\\\\lambda^*|$$\\nand, following from the proof of Proposition 3.2, $|\\\\lambda^*| \\\\leq 2 \\\\|\\\\varepsilon_S\\\\|$. Note that if $JHJ^\\\\top = 0$, then $\\\\lambda^* = -\\\\frac{2}{\\\\|J\\\\|} |C(z)|$ and we directly get $\\\\|\\\\varepsilon_G\\\\| \\\\leq \\\\|\\\\varepsilon_S\\\\|$.\\n\\nTo summarize we have the following inequalities\\n$$\\\\frac{1}{2} \\\\|\\\\varepsilon_G\\\\| \\\\leq \\\\frac{1}{2} \\\\tau \\\\|\\\\varepsilon_G\\\\| \\\\leq \\\\|\\\\varepsilon_S\\\\| \\\\leq \\\\|\\\\varepsilon_G\\\\| + \\\\rho \\\\|J\\\\| \\\\|\\\\varepsilon_G\\\\|^2,$$\\n(41)\\nwhere $\\\\tau = |\\\\lambda^*/C(z)| \\\\|J\\\\|$ and $\\\\lambda^*$ is given by (40).\\n\\nThe hypothesis in Proposition 3.2 defines a region where the geometric error is bounded linearly by the Sampson error. We highlight that this region contains the region defined by $\\\\|\\\\varepsilon_S\\\\| = |C(z)| \\\\|J\\\\| \\\\leq \\\\|J\\\\|^2 \\\\rho$.\\n\\n**Example 3.3.** Consider as a constraint the conic $C(x) = x_1^2 + 2 x_2^2 - 4$ in $\\\\mathbb{R}^2$. For fixed $z = (z_1, z_2)$, the minimization problem is\\n$$\\\\min \\\\|\\\\varepsilon\\\\|_2$$\\n(43)\\nsubject to\\n$$(z_1 + \\\\varepsilon_1)^2 + 2(z_2 + \\\\varepsilon_2)^2 - 4 = 0$$\\n(44)\\nand the linearized constraint is\\n$$z_1^2 + 2 z_2^2 - 4 + 2 z_1 \\\\varepsilon_1 2 z_2 \\\\varepsilon_2 = 0.$$ \\n(45)\"}"}
