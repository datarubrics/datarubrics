{"id": "CVPR-2023-481", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. The pipeline of our method. We train two individual models with Focal loss and Center loss. In inference, we calculate MaxNorm on the second model and MaxCosine on the first model. Then we couple the two scores into DML to detect OOD samples.\\n\\n4.4. Decoupling MaxLogit + (DML+)\\n\\nDML is a post-hoc scoring function that is model agnostic and training scheme agnostic. In this section, we improve DML based on the key insights which make MaxCosine and MaxNorm effective in practice.\\n\\nTo further improve DML, a robust method is to apply MaxCosine on the cosine classifier model trained with Focal loss and apply MaxNorm on the cosine classifier model trained with Center loss. This method does not require tuning $\\\\lambda$ in DML on each model. As we explained, the MaxNorm and MaxCosine are complementary. Thus, as shown in Fig. 3, we extend DML to DML+ as\\n\\n$$DML^+ = \\\\lambda \\\\cdot \\\\text{MaxCosine}_F + \\\\text{MaxNorm}_C,$$\\n\\nwhere MaxCosine$_F$ means MaxCosine applied on the Focal model and MaxNorm$_C$ means MaxNorm applied on the Center model. We use MCF to represent MaxCosine$_F$ and MNC to represent MaxNorm$_C$ for convenience.\\n\\nNotably, DML+ offers several compelling advantages:\\n\\n1. The MCF model and MNC model have similar performance. Thus, DML+ is free from hyper-parameter tuning, where we set $\\\\lambda = 1$ for all experiments.\\n2. DML+ is robust to different OOD samples because DML+ draws on the strengths of both models.\\n3. DML+ is OOD agnostic and does not rely on the information of OOD samples.\\n\\n5. Experiments\\n\\n5.1. Experimental Settings\\n\\nIn-distribution datasets. We use three common benchmarks CIFAR-10 and CIFAR-100 [19] and ImageNet [5].\\n\\nOut-of-distribution datasets. For CIFAR, we use six benchmarks as OOD datasets following the work [37] including Textures [4], SVHN [24], LSUN-Crop and LSUN-Resize [43], iSUN [40] and Places365 [46]. For ImageNet, we use four common benchmarks as OOD datasets following the work [30] including iNaturalist [32], SUN [39], Places365, and Textures. The evaluations include a diverse range of domains. Details of datasets can be found in supp.\\n\\nEvaluation metrics. We use two commonly-adopted metrics to evaluate our methods. The AUROC is a threshold-free metric to describe the performance of a model. Notably, a higher AUROC is better. FPR95 is the false-positive rate of OOD data when the true-positive rate of ID data is 95%, and a smaller FPR95 is better. For all experiments, we report both scores in percentage. All reported scores are averaged over ten runs.\\n\\nHyper-parameters. In DML, we tune $\\\\lambda$ based on the OOD detection performance on Gaussian noise. In DML+, we set $\\\\lambda = 1$ for all experiments.\\n\\nTraining details. The baseline methods we reproduced are all experimented on the model trained with CE loss and linear classifier, as in the original paper report. The scale parameter of the cosine classifier is set to 40 for all experiments. We set $\\\\gamma = 2$ for Focal loss.\\n\\nWe experiment several model architectures including WRN-40-2 [45], ResNet34 and ResNet50 [9] and DenseNet [13]. For CIFAR-10 and CIFAR-100, all the models are trained for 200 epochs using SGD optimizer with a momentum of 0.9 and the cosine learning rate scheduler, which gradually decays the learning rate from 0.1 to 0. The weight decay is $5 \\\\times 10^{-4}$, and the batch size is 128. For ImageNet, we train the ResNet50 from scratch for 90 epochs using the same SGD and learning rate scheduler as on CIFAR. The weight decay is $5 \\\\times 10^{-4}$, and the batch size is 256.\\n\\n5.2. Comparison with SOTA\\n\\nYang et al. [41] experiment on 22 OOD detection methods with identical settings. We choose the SOTA methods based on that result for different datasets. For OOD detection methods without extra data and with training, we choose LogitNorm [37] and GODIN [12]. For OOD detection methods without extra data and without training, we choose LogitNorm [37] and GODIN [12].\"}"}
{"id": "CVPR-2023-481", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 4. OOD detection for our methods and baseline methods on CIFAR-100. AUR represents AUROC and FPR95 for FPR, and all values are percentages. The best model is emphasized in bold, while the 2nd and 3rd are underlined. * means the results are from [37].\\n\\n| Method       | CIFAR-100 AUROC (%) | CIFAR-100 FPR95 (%) |\\n|--------------|---------------------|---------------------|\\n| MSP          | 74.24               | 84.43               |\\n| ODIN*        | 75.60               | 80.23               |\\n| Energy       | 75.80               | 82.23               |\\n| ViM          | 91.25               | 38.65               |\\n| MaxLogit     | 76.55               | 82.30               |\\n| ours (DML)   | 79.57               | 82.63               |\\n| LogitNorm*   | 86.38               | 60.76               |\\n| ours (MCF)   | 91.74               | 40.15               |\\n| ours (MNC)   | 85.52               | 58.41               |\\n| ours (DML+)  | 88.56               | 49.24               |\\n\\n### Table 5. Results on CIFAR-10, and different model architectures on CIFAR-100. All numbers are average and in percentage.\\n\\n| Model       | CIFAR-100 AUROC (%) | CIFAR-100 FPR95 (%) |\\n|-------------|---------------------|---------------------|\\n| WRN-40-2    | 90.47               | 51.60               |\\n| R34         | 90.48               | 35.85               |\\n| DenseNet    | 94.98               | 23.01               |\\n| MaxLogit    | 90.45               | 36.35               |\\n| ours (DML)  | 92.22               | 37.02               |\\n| LogitNorm   | 96.91               | 15.65               |\\n| ours (MCF)  | 97.25               | 13.49               |\\n| ours (MNC)  | 97.26               | 13.85               |\\n| ours (DML+) | 98.00               | 10.08               |\\n\\nOur methods rank top on the near-OOD datasets too and complete results including the near-OOD results are available in supplementary materials. Results on CIFAR. We present our results of CIFAR-100 in Table 4. We emphasize the best model in bold while the second and third ones are in underlines. On all six OOD datasets, our methods perform the best on AUROC. On three datasets, including SVHN, LSUN-R and iSUN, our methods achieve the best, second and third AUROC and FPR95. On Textures, ViM [36] which is a SOTA post-hoc method, performs the best on FPR95. On LSUN-C and Places365, LogitNorm [37] takes the second or third place on AUROC. LogitNorm is a training method for OOD detection which uses LogitNorm loss. LogitNorm performs best in our baselines, whose AUROC is 85.73%, 3.04% higher than SOTA post-hoc method ViM, which also illustrates the necessity of focusing on model training.\\n\\nWe also observe that MCF and MNC are complementary: on Textures, LSUN-R and iSUN, MCF outperforms MNC by around 7% on AUROC while MNC performs better on the other three datasets. As a result, our DML is more robust on different OOD datasets, and the average AUROC is 5.84% higher than the prior SOTA method LogitNorm. In addition, the left part of Table 5 shows the results of CIFAR-10 and our methods outperform ViM and LogitNorm by more than 1% in terms of AUROC.\\n\\nResults on ImageNet. ImageNet is a large-scale dataset and the OOD detection performance degrades as explained in [15]. The results are shown in Table 6. Our methods perform best on three datasets, excluding Textures. On Textures, feature distance-based methods perform better, including ViM, KNN and Mahalanobis. We hypothesize that our non-best performance relates to the feature norm not being distinguishable for the OOD samples, because MNC has the smallest AUROC among four OOD datasets. For the other three datasets, our methods take the first, second and third places on AUROC, excluding iNaturalist where KNN (w/ CL) takes the third place. KNN (w/ CL) is the KNN OOD detection method applied on the ResNet50 trained with the SupCon loss [18]. However, KNN used on the conventional model (w/o CL) performs worse than MaxLogit. Thus, the performance boost mainly comes from the better model. More details of SupCon will be discussed next. Our method has 93.16% AUROC and 29.79% FPR95, surpassing the second place (excluding KNN with CL) by 5.51% and 10.91%, respectively.\\n\\n5.3. Ablation Study\\n\\nDifferent architectures. In the right part of Table 5, we show that our methods are effective on different model architectures. We choose DenseNet and ResNet34 which are commonly used in CIFAR-100. It is shown that our methods outperform ViM and LogitNorm by more than 1% in terms of AUROC.\"}"}
{"id": "CVPR-2023-481", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6. OOD detection performance comparison with various methods on ImageNet. We train ResNet50 for 90 epochs from scratch for all models. KNN (w/o CL) means KNN method tested on ResNet50 trained with CE loss, while (w/ CL) means the ResNet50 trained with SupCon [18]. * means the results are from [30]. The methods above the line are post-hoc while under the line are with improved training.\\n\\n| Methods          | AUROC 87.36 | FPR 59.29 | TPR 79.92 | ACCAUR 73.42 | FPR 79.82 | ACCAUR 73.88 | FPR 80.75 | ACCAUR 68.48 |\\n|------------------|-------------|-----------|-----------|--------------|-----------|--------------|-----------|--------------|\\n| MSP              | 81.81       | 68.77     | 72.18     |              |           |              |           |              |\\n| Energy           | 91.02       | 55.10     | 85.58     | 62.11        | 83.98     | 65.34        | 87.68     | 52.25        |\\n| GradNorm         | 91.79       | 31.24     | 88.87     | 38.53        | 86.28     | 46.29        | 83.66     | 46.76        |\\n| ViM              | 88.40       | 67.95     | 72.65     | 91.87        | 71.47     | 91.09        | 97.52     | 12.40        |\\n| KNN (w/o CL)*    | 86.20       | 59.08     | 80.10     | 69.53        | 74.87     | 77.09        | 97.18     |              |\\n| MaxLogit         | 91.05       | 54.49     | 84.96     | 65.45        | 83.69     | 67.60        | 86.71     | 57.09        |\\n| ours (DML)       |              |           |           |              |           |              |           |              |\\n| ours (MCF)       | 93.77       | 36.29     | 89.50     | 51.18        | 86.78     | 57.38        | 94.35     | 28.46        |\\n| ours (MNC)       |              |           |           |              |           |              |           |              |\\n| ours (DML+)      | 97.50       | 13.57     | 94.01     | 30.21        | 91.42     | 39.06        | 89.70     | 36.31        |\\n\\nTable 7. The results of different coupling methods of DML+.\\n\\n| Methods          | AUROC 87.89 | FPR 89.70 | TPR 89.82 |\\n|------------------|-------------|-----------|-----------|\\n| Original         |             |           |           |\\n| Norm+Uniform     |             |           |           |\\n\\nFigure 4. Effect of $\\\\lambda$ in DML with ImageNet. Our model is robust to model architecture changes. For example, DML+ has 92.19% AUROC, which surpasses the second place by 8.82%.\\n\\nDifferent $\\\\lambda$ for DML+.\\n\\nWe experiment with three coupling methods for DML+ on Textures as shown in Table 7. Original means we use the MCF and MNC scores without normalization. However, the cosine similarity has different norms from the feature norm. Thus, the coupling performance is affected mainly by the one with larger norms. We normalize the MCF and MNC by the ID information. We accumulate the score of ID training data and divide the score of test data with the accumulated number. The uniform in the table means $\\\\lambda = 1$, and greedy means we set $\\\\lambda$ based on the OOD detection performance on Gaussian noise. For instance, if MCF performs better than MNC on Gaussian noise, $\\\\lambda$ would be larger than 1. Due to the extra training cost and marginal improvement, we choose norm+uniform for all experiments, which is fast and convenient.\\n\\nDifferent $\\\\lambda$ for DML.\\n\\nIn Fig. 4, we ablate how the hyperparameter $\\\\lambda$ in DML affects the OOD detection performance. When $\\\\lambda = 0$, DML equals to MaxNorm and performs the worst. When $\\\\lambda > 0$, the performance increases.}\\n\\nFigure 5. (a) shows how the average feature norm for ID train/test and OOD data evolve as training proceeds. (b) shows the classifier weight (blue line) and average feature norm (bars) for different channels. We choose the 48th class data as the ID data and sort the channels by the descending 48th classifier weight. Much by over 2%, which indicates that MaxCosine and MaxNorm are complementary.\\n\\nVisualization.\\n\\nWe visualize the feature norm of training, test and OOD data as training proceeds in Fig. 5 (a). The feature norm of OOD data is always less than that of test data, which guarantees MaxNorm's effectiveness. As Fig. 5 (b) shows, the channel activation of ID data is larger when the corresponding classifier weight is larger. As the product of the feature and classifier, the cosine similarity of ID data is larger than the OOD data.\\n\\n5.4. Discussion\\n\\nOur model can improve various scoring methods. In the above experiments, we test the existing scoring methods with linear classifier and CE loss because the methods do not pay attention to this area. In Fig. 6, we show that our model (Focal(N) and Center(N)) not only improves MaxCosine and MaxNorm but also greatly boosts the performance of existing methods. Our MNC model (Center(N)) improves all the scoring functions by more than 9%. For GradNorm, the cosine classifier and center loss facilitate the AUROC from 52.8% to 90.8%, which is higher than 3394.\"}"}
{"id": "CVPR-2023-481", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. We report the mean AUROC of WRN-40-2 on CIFAR-100 with different models and methods. (a), (b) and (c) show the performance of existing methods on different models. (U) stands for linear classifier and (N) means the cosine classifier. (d) shows the in-distribution classification accuracy. MLS: MaxLogit, Maha: Mahalanobis, GradN: GradNorm, Foc: Focal and Cen: Center.\\n\\nThe SOTA method LogitNorm by 5%. We also observe that MaxCosine and cosine classifiers with CE loss (in Table 3) achieve better results than ViM (84.1% vs. 82.7%), which shows the effectiveness of the cosine classifier on OOD detection. Also, we notice that Focal(U) and Center(U) have similar OOD detection performance with CE(U).\\n\\nHowever, when trained with a cosine classifier, different scoring functions gain a significant performance boost, especially Center(N). As a result, the simple training scheme could serve as the future baseline for OOD detection.\\n\\nThe ID classification accuracy is at least maintained. The ID classification accuracy on ImageNet and CIFAR is shown in Table 6 and Fig. 6 (d). The MNC model outperforms the linear classifier with CE loss on both datasets. MCF model achieves comparable ID performance on CIFAR-10 and ImageNet. As the coupling form of MNC and MCF, DML+ chooses the MNC model to output ID prediction. Overall, our DML+ has comparable or better classification performance on the ID data while substantially improving the OOD detection performance.\\n\\nHow to choose the methods between DML+, MCF and MNC. DML, as the coupling form, performs the best on average. However, DML+ needs to train two models that cost double the training and memory resource. The MNC model is a better choice when the training resource is limited. Compared to the baseline, it has similar training costs but higher OOD detection performance with different scoring functions, as shown in Fig. 6. In addition, the in-distribution classification accuracy of MNC is higher, as explained.\\n\\nRelations to better ID performance: The recent work [33] shows that the closed-set and open-set performance are strongly correlated. The MSP with a stronger model achieves SOTA on open-set benchmarks, including longer training time, stronger model architecture, and better augmentations. We also focus on the model training on OOD detection, but our conclusion is orthogonal. Our MCF and MNC model take similar training costs as the baseline and achieve similar ID classification accuracy. But the OOD detection performance is much higher than the baseline. Similarly, KNN [30] explores SupCon [18] which has better ID classification accuracy (79.10% vs. 72.18%) as shown in Table 6. SupCon takes more training cost than baseline (20\u00d7GPU training cost). However, the performance increment is less than our MCF and MNC models. Our DML+ method has two times the training cost but has much higher OOD performance.\\n\\nRelations to other methods:\\n\\n1. MaxCosine and GODIN [12]. GODIN decomposes the logit as \\\\( f_i(x) = h_i(x) + g(x) \\\\) where \\\\( h_i(x) \\\\) is the cosine similarity and \\\\( g(x) \\\\) can be viewed as temperature scaling. In inference, the data with perturbation is fed into the model and the output maximum cosine similarity is used for OOD detection. MaxCosine is different: (1) we do not use the perturbation process; (2) we do not contain \\\\( g(x) \\\\) item.\\n\\n2. MaxNorm and Objecto [6]. Objecto [6] uses the regularization method to decrease the unknown feature magnitude to increase separation in deep feature space. Our method is different: (1) Objecto [6] needs unknown data during training, ours is OOD-agnostic; (2) we use MaxNorm as OOD scoring function.\\n\\n3. Our methods and LogitNorm [37]. As explained in [37], the normalization of logits and features is different. Also, we set \\\\( s = 40 \\\\) for the cosine classifier, while LogitNorm needs to tune the parameter on OOD dataset several times. Our AUROC is higher than LogitNorm (91.57% vs. 85.73%) and we can further facilitate the existing scoring functions (MSP AUROC 89.60% vs. 81.41%).\\n\\n6. Conclusion\\n\\nThis paper presents the limitations of the simple logit-based scoring function MaxLogit. To overcome the limitations, we propose Decoupling MaxLogit which decouples MaxLogit for flexibility to balance MaxCosine and MaxNorm. Unlike prior works, we provide important insights that fewer hard samples and compact feature space are two key components to make logit-based methods effective. We extend DML to DML+ which changes the standard training to achieve the ideal feature space. The method can be easily implemented with existing networks and does not require sophisticated changes to the training scheme. Extensive experiments show that DML+ can greatly improve OOD detection and maintain in-distribution classification accuracy. We hope our work will inspire future logit-based research, and more training methods will be explored.\\n\\nAcknowledgement. This research was supported by HUST Independent Inno. Res. Fund (2021XXJS096), and Alibaba Inno. Res. program (CRAQ7WHZ11220001-20978282).\"}"}
{"id": "CVPR-2023-481", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Julian Bitterwolf, Alexander Meinke, and Matthias Hein. Certifiably adversarially robust detection of out-of-distribution data. Advances in Neural Information Processing Systems, 33:16085\u201316095, 2020.\\n\\n[2] Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha. Atom: Robustifying out-of-distribution detection using outlier mining. In Joint European Conference on Machine Learning and Knowledge Discovery in Databases, pages 430\u2013445. Springer, 2021.\\n\\n[3] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. In International Conference on Machine Learning, pages 1597\u20131607. PMLR, 2020.\\n\\n[4] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3606\u20133613, 2014.\\n\\n[5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 248\u2013255. IEEE, 2009.\\n\\n[6] Akshay Raj Dhamija, Manuel G\u00fcnther, and Terrance Boult. Reducing network agnostophobia. Advances in Neural Information Processing Systems, 31, 2018.\\n\\n[7] Xin Dong, Junfeng Guo, Ang Li, Wei-Te Ting, Cong Liu, and HT Kung. Neural mean discrepancy for efficient out-of-distribution detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19217\u201319227, 2022.\\n\\n[8] Xuefeng Du, Zhaoning Wang, Mu Cai, and Yixuan Li. V os: Learning what you don't know by virtual outlier synthesis. arXiv preprint arXiv:2202.01197, 2022.\\n\\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.\\n\\n[10] Dan Hendrycks, Steven Basart, Mantas Mazeika, Mostajabi, Jacob Steinhardt, and Dawn Song. Scaling out-of-distribution detection for real-world settings. arXiv preprint arXiv:1911.11132, 2019.\\n\\n[11] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassified and out-of-distribution examples in neural networks. Proceedings of International Conference on Learning Representations, 2017.\\n\\n[12] Yen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized odin: Detecting out-of-distribution image without learning from out-of-distribution data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10951\u201310960, 2020.\\n\\n[13] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4700\u20134708, 2017.\\n\\n[14] Rui Huang, Andrew Geng, and Yixuan Li. On the importance of gradients for detecting distributional shifts in the wild. Advances in Neural Information Processing Systems, 34:677\u2013689, 2021.\\n\\n[15] Rui Huang and Yixuan Li. Mos: Towards scaling out-of-distribution detection for large semantic space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8710\u20138719, 2021.\\n\\n[16] Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang. Curricularface: adaptive curriculum learning loss for deep face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5901\u20135910, 2020.\\n\\n[17] K J Joseph, Salman Khan, Fahad Shahbaz Khan, and Vineeth N Balasubramanian. Towards open world object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5830\u20135840, June 2021.\\n\\n[18] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:18661\u201318673, 2020.\\n\\n[19] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\\n\\n[20] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple unified framework for detecting out-of-distribution samples and adversarial attacks. Advances in neural information processing systems, 31, 2018.\\n\\n[21] Shiyu Liang, Yixuan Li, and R Srikant. Enhancing the reliability of out-of-distribution image detection in neural networks. In International Conference on Learning Representations, 2018.\\n\\n[22] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.\\n\\n[23] Weitang Liu, Xiaoyun Wang, John Owens, and Yixuan Li. Energy-based out-of-distribution detection. Advances in Neural Information Processing Systems, 33:21464\u201321475, 2020.\\n\\n[24] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bisacco, Bo Wu, and Andrew Y Ng. Reading digits in natural images with unsupervised feature learning. 2011.\\n\\n[25] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High confidence predictions for unrecognizable images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 427\u2013436, 2015.\\n\\n[26] Aristotelis-Angelos Papadopoulos, Mohammad Reza Rajati, Nazim Shaikh, and Jiamian Wang. Outlier exposure with confidence control for out-of-distribution detection. Neurocomputing, 441:138\u2013150, 2021.\\n\\n[27] Vardan Papyan, XY Han, and David L Donoho. Prevalence of neural collapse during the terminal phase of deep learning training. Proceedings of the National Academy of Sciences, 117(40):24652\u201324663, 2020.\"}"}
{"id": "CVPR-2023-481", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[31] Sunil Thulasidasan, Gopinath Chennupati, Jeff A Bilmes, Tanmoy Bhattacharya, and Sarah Michalak. On mixup training: Improved calibration and predictive uncertainty for deep neural networks. Advances in Neural Information Processing Systems, 32, 2019.\\n\\n[32] Grant Van Horn, Oisin Mac Aodha, Yang Song, Yin Cui, Chen Sun, Alex Shepard, Hartwig Adam, Pietro Perona, and Serge Belongie. The inaturalist species classification and detection dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8769\u20138778, 2018.\\n\\n[33] Sagar Vaze, Kai Han, Andrea Vedaldi, and Andrew Zisserman. Open-set recognition: A good closed-set classifier is all you need. arXiv preprint arXiv:2110.06207, 2021.\\n\\n[34] Feng Wang, Jian Cheng, Weiyang Liu, and Haijun Liu. Additive margin softmax for face verification. IEEE Signal Processing Letters, 25(7):926\u2013930, 2018.\\n\\n[35] Feng Wang, Xiang Xiang, Jian Cheng, and Alan Loddon Yuille. Normface: L2 hypersphere embedding for face verification. In Proceedings of the 25th ACM international conference on Multimedia, pages 1041\u20131049, 2017.\\n\\n[36] Haoqi Wang, Zhizhong Li, Litong Feng, and Wayne Zhang. Vim: Out-of-distribution with virtual-logit matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4921\u20134930, 2022.\\n\\n[37] Hongxin Wei, Renchunzi Xie, Hao Cheng, Lei Feng, Bo An, and Yixuan Li. Mitigating neural network overconfidence with logit normalization. 2022.\\n\\n[38] Yandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach for deep face recognition. In European Conference on Computer Vision, pages 499\u2013515. Springer, 2016.\\n\\n[39] Jianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 3485\u20133492. IEEE, 2010.\\n\\n[40] Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R Kulkarni, and Jianxiong Xiao. Turkergaze: Crowdsourcing saliency with webcam based eye tracking. arXiv preprint arXiv:1504.06755, 2015.\\n\\n[41] Jingkang Yang, Pengyun Wang, Dejian Zou, Zitang Zhou, Kunyuan Ding, Wenxuan Peng, Haoqi Wang, Guangyao Chen, Bo Li, Yiyou Sun, et al. Openood: Benchmarking generalized out-of-distribution detection. arXiv preprint arXiv:2210.07242, 2022.\\n\\n[42] Jingkang Yang, Kaiyang Zhou, Yixuan Li, and Ziwei Liu. Generalized out-of-distribution detection: A survey. arXiv preprint arXiv:2110.11334, 2021.\\n\\n[43] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of a large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015.\\n\\n[44] Qing Yu and Kiyoharu Aizawa. Unsupervised out-of-distribution detection by maximum classifier discrepancy. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9518\u20139526, 2019.\\n\\n[45] Sergey Zagoruyko and Nikos Komodakis. Wide residual networks. In British Machine Vision Conference 2016. British Machine Vision Association, 2016.\\n\\n[46] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):1452\u20131464, 2017.\\n\\n[47] Xiong Zhou, Xianming Liu, Deming Zhai, Junjun Jiang, Xin Gao, and Xiangyang Ji. Learning towards the largest margins. In International Conference on Learning Representations, 2022.\\n\\n[48] Zhihui Zhu, Tianyu Ding, Jinxin Zhou, Xiao Li, Chong You, Jeremias Sulam, and Qing Qu. A geometric analysis of neural collapse with unconstrained features. Advances in Neural Information Processing Systems, 34:29820\u201329834, 2021.\"}"}
{"id": "CVPR-2023-481", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Decoupling MaxLogit for Out-of-Distribution Detection\\n\\nZihan Zhang\\n\\nand Xiang Xiang\\n\\nKey Lab of Image Processing and Intelligent Control, Ministry of Education\\nSchool of Artificial Intelligence and Automation\\nHuazhong University of Science and Technology, China\\n\\nAbstract\\n\\nIn machine learning, it is often observed that standard training outputs anomalously high confidence for both in-distribution (ID) and out-of-distribution (OOD) data. Thus, the ability to detect OOD samples is critical to the model deployment. An essential step for OOD detection is post-hoc scoring. MaxLogit is one of the simplest scoring functions which uses the maximum logits as OOD score. To provide a new viewpoint to study the logit-based scoring function, we reformulate the logit into cosine similarity and logit norm and propose to use MaxCosine and MaxNorm. We empirically find that MaxCosine is a core factor in the effectiveness of MaxLogit. And the performance of MaxLogit is encumbered by MaxNorm. To tackle the problem, we propose the Decoupling MaxLogit (DML) for flexibility to balance MaxCosine and MaxNorm. To further embody the core of our method, we extend DML to DML+ based on the new insights that fewer hard samples and compact feature space are the key components to make logit-based methods effective. We demonstrate the effectiveness of our logit-based OOD detection methods on CIFAR-10, CIFAR-100 and ImageNet and establish state-of-the-art performance.\\n\\n1. Introduction\\n\\nIn real-world applications, the closed-world assumption does not always hold where all the classes in the test phase would be available in the training phase. Out-of-distribution (OOD) detection [11] is a natural and challenging setting, and there is an open space containing outliers not belonging to any training classes. When the model is deployed in practice, OOD data often come from the open world [17]. Thus, it is crucial for a trustworthy model to not only produce accurate predictions on in-distribution (ID) data, but also distinguish the OOD data and reject them. However, the machine-learning model easily produces over-confident wrong predictions on OOD data [25]. For instance, a model may wrongly detect the zebra as a horse with high confidence when the zebra is not in the training set.\\n\\nA key of the OOD detection algorithm is a scoring function that maps the input to the OOD score, indicating to what extent the sample is an OOD sample. Various scoring functions have been proposed to seek the properties that better distinguish OOD samples. The OOD score is calculated mainly from the output of the model, including features [20, 30, 36], logits [10, 11, 23]. For example, MSP [11] uses the maximum Softmax probabilities, and MaxLogit [10] uses the maximum logits as the OOD score. MaxLogit and MSP are two simplest scoring functions that do not require extra computational costs. In contrast, other methods require extra storage [30], or extra computational cost [14]. However, the logit-based methods MSP and MaxLogit are not state-of-the-art (SOTA). Intuitively, the simple logit-based method could achieve comparable performance as other complex scoring methods, because logit contains high-level semantic information. We hypothesize some underlying reasons limit the performance.\\n\\nTo revitalize the simple logit-based method, we start the work by analyzing the reasons which cause the performance gap between MSP and MaxLogit. The gap may be due to this CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\\n\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\"}"}
{"id": "CVPR-2023-481", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the softmax operation normalizing out much feature norm information of the logits. To delve into the effect of feature norm, we divide the logit into two parts: (1) the cosine similarity between the features and classifier; (2) the feature norm. We discard the classifier weight norm because the norm is identical after the model coverage [27]. We use the top value of the two as the OOD score, named MaxCosine and MaxNorm. Therefore, MaxLogit is a coupled form.\\n\\nWe find that MaxCosine outperforms MaxLogit with the same model and MaxNorm performs much worse than MaxLogit. Thus, MaxLogit (1) is encumbered by MaxNorm, (2) suppresses the effectiveness of MaxCosine, and (3) restricts the flexibility to balance MaxCosine and MaxNorm. The three problems are the bottleneck of MaxLogit. To tackle the problem, we propose Decoupling MaxLogit (DML) for flexibility to balance MaxCosine and MaxNorm. DML decouples the MaxCosine from the equal coefficient with MaxNorm by replacing it with a constant. The decoupling method solves the second and third problems but still leaves the first problem unsolved. Although MaxNorm helps DML to outperform MaxCosine, the improvement is marginal due to the low performance of MaxNorm. Therefore, we study the role of model training and show that a simple modification to standard training could significantly boost MaxNorm and MaxCosine for OOD detection. Specifically, a feature space with fewer hard samples benefits MaxCosine and a compact feature space benefits MaxNorm. Also, the normalized feature and classifier are the key to the success of the logit-based methods. These findings are not discussed in prior works. We extend DML to DML+ based on the above new insights to further boost the DML performance as shown in Fig. 1.\\n\\nWe summarize our contributions as follows.\\n\\n\u2022 To overcome the limitations of MaxLogit, we propose a post-hoc scoring method DML, which decouples MaxLogit for flexibility to balance MaxCosine and MaxNorm. DML outperforms MaxLogit and achieves comparable performance with SOTA methods.\\n\\n\u2022 We offer new insights into the key components to make MaxCosine, MaxNorm and DML effective, including replacing the standard linear classifier with a cosine classifier and different training losses. The findings are supported by empirical results and theoretical analysis. We also prove that the findings could greatly boost the performance of existing OOD scoring methods.\\n\\n\u2022 Based on the insights, we extend DML to DML+ which changes the standard training. Significant improvements on CIFAR and ImageNet have shown its effectiveness.\\n\\n2. Related Work\\n\\nOOD Detection has attracted growing research attention for the safe deployment of the model in the real world. The major branch of OOD detection is classification-based methods [41, 42] including confidence enhancement methods [1,31], outlier exposure [2,7,26,44] and post-hoc detection [10\u201312,14,21,23,29,36]. The outlier exposure methods require auxiliary OOD data as the outlier to help the model learn the ID or OOD discrepancy. For example, NMD [7] finds that activation means of OOD data and ID data are deviated and trains the OOD detector with OOD and ID data. An active line of research of OOD detection without extra data is post-hoc detection. It is easy to use the post-hoc methods when given a trained model. MaxLogit [10] experiments with the maximum logit value of the samples. ODIN [21] finds that large temperature scaling and input perturbation help distinguish the OOD samples. Energy [23] proposes using an energy score based on the logsumexp of the logits for OOD detection. GradNorm [14] uses the gradients of KL divergence between the softmax output and a uniform distribution as evidence for ID and OOD distinction. A recent work ViM [36] combines the information from both the feature space and the logits for OOD detection.\\n\\nAnother line of work that detects OOD without extra data involves training. VOS [8] adaptively synthesizes virtual outliers from the low-likelihood region in the feature space to reduce the confidence of the model for the detection task. LogitNorm [37] fixes the widely used cross-entropy loss by normalizing the logits in the training phase.\\n\\nNormalization in deep learning. Normalization is widely adopted in deep learning including face recognition [16, 34, 35, 47], self-supervised learning [3, 18], etc. NormFace [35] normalizes the feature and classifier to boost face verification performance. SupCon [18] and SimCLR [3] use cosine similarity to measure the similarity between different samples. In the literature on OOD detection, LogitNorm [37] normalizes the logit value during training time and uses the unnormalized logits to calculate the OOD score. Normalization on logits differs from the cosine similarity because of the cosine value difference. And LogitNorm can be easily combined with common OOD scoring functions.\\n\\n3. Preliminaries\\n\\nWe consider a neural network for the $K$-class classification task, which can be represented as $f(x; W_{\\\\text{full}}) = b_L + W_L \\\\delta(\\\\cdots \\\\delta(b_1 + W_1 x) \\\\cdots)$, (1) where $W_{\\\\text{full}} = \\\\{W_1, \\\\cdots, W_L\\\\}$ denotes the weights of the $L$ layers, $\\\\{b_1, \\\\cdots, b_L\\\\}$ denotes the biases, and $\\\\delta(\\\\cdot)$ is the nonlinear activation function. Given the data $x_{k,i}$ belonging to class $k$, we define the last-layer features as $h_{k,i} \\\\in \\\\mathbb{R}^d$, $f(x; W_{\\\\text{full}}) = b_L + W_L h_{k,i}$. The later analysis does not include the bias term for simplicity. Then, the logit is $z_{k,i} = W_L h_{k,i}$ where $W_L = [w_1, \\\\cdots, w_K]^\\\\top$. Given a training set $D_{\\\\text{tr}} = \\\\{ (x_{k,i}, k) \\\\}_{i=1}^{N_{\\\\text{training}}}$ with $N_{\\\\text{training}}$ training samples and $K$ classes from an underlying distribution.\"}"}
{"id": "CVPR-2023-481", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We first train a model on the training set. The goal of OOD detection is to discriminate if a given sample is from $P_{tr}$ or another data distribution. Therefore, two keys of OOD detection are (1) training a model robust to OOD samples, i.e., easier to distinguish the ID and OOD samples and (2) designing a score function so that samples with a smaller score are classified as OOD.\\n\\nWe define two metrics to measure feature collapse [48]: Within-class Feature Convergence (WFC) and Class mean Feature Convergence to the corresponding classifier (CFC).\\n\\n$WFC := \\\\text{trace} (\\\\Sigma W \\\\Sigma B \\\\dagger)$,\\n\\n$CFC := \\\\frac{1}{K} \\\\sum_{k=1}^{K} \\\\frac{|h_k|^F}{|w|^F}$,\\n\\nwhere $\\\\dagger$ denotes the pseudo-inverse, $h$ is the feature matrix of all samples. $h_k$ and $h$ are the mean of class $k$ features and all features respectively, $\\\\Sigma W = \\\\frac{1}{Kn} \\\\sum_{k=1}^{K} \\\\sum_{i=1}^{P} \\\\sum_{j=1}^{n} (h_{k,i} - h_k)(h_{k,i} - h_k)^\\\\dagger$ and $\\\\Sigma B = \\\\frac{1}{K} \\\\sum_{k=1}^{K} (h_k - h)(h_k - h)^\\\\dagger$.\\n\\nMore details are included in the supplementary materials.\\n\\n4. Methods\\n4.1. Rethinking MaxLogit\\n\\nThe MSP score of a sample is the maximum Softmax value of the sample: $\\\\max(\\\\text{Softmax}(z_{k,i}))$. The MaxLogit score of a sample is the maximum logit value of the sample: $\\\\max(z_{k,i})$. Recall that $z_{k,i} = [z_{i1}, z_{i2}, ..., z_{iK}]$.\\n\\nMaxLogit outperforms MSP on different datasets. The monotonically-increasing function transforms on the score function (e.g., $\\\\log(\\\\cdot)$ and $\\\\exp(\\\\cdot)$) do not affect the OOD detection performance. Therefore, the only difference between MSP and MaxLogit is the sum item $\\\\sum_{j=1}^{K} \\\\exp(z_{ij})$.\\n\\nAfter the model coverage, the sum item is primarily affected by the feature norm. Therefore, the difference between MSP and MaxLogit mainly comes from the feature norm. This motivates us to investigate how the cosine similarity and the feature norm affect OOD detection performance.\\n\\nWe decouple the MaxLogit into two parts: MaxCosine and MaxNorm. Given a sample $x_{k,i}$, MaxCosine and MaxNorm can be formulated as:\\n\\n$\\\\text{MaxCosine} := \\\\max(\\\\cos < h_{k,i}, w_j>)$, $K \\\\sum_{j=1}^{K}$,\\n\\n$\\\\text{MaxNorm} := \\\\frac{|h_{k,i}|}{|w|}$.\\n\\nThe MaxLogit score equals the MaxCosine score multiplied by the MaxNorm score. As we explained, applying an increasing function transform on the score does not affect the OOD detection performance. Thus, MaxLogit can be formulated with two individual parts $\\\\log(\\\\max(\\\\text{Softmax}(z_{k,i}))) = \\\\log(\\\\max(\\\\cos < h_{k,i}, w_j>)) + \\\\log(|h_{k,i}|) + \\\\log(|w|)$, which is a coupled form of MaxCosine and MaxNorm. Note that $|w|$ is almost identical after model coverage [27], so we use a constant $|w|$ to replace it.\\n\\nBy the reformulation of MaxLogit, we empirically find that MaxCosine outperforms MaxLogit with the same model and MaxNorm performs much worse than MaxLogit. As Table 1 shows, MaxCosine outperforms MaxLogit by around 0.8% on different models. Meanwhile, MaxNorm performs worse than MaxCosine by around 30%. As a coupled form, MaxLogit is dragged down by MaxNorm. When the MaxNorm performs worse on Model2, MaxCosine outperforms MaxLogit by over 1.3%, which is the largest performance gap between the three models. When MaxNorm performs better on Model3, MaxLogit outperforms MaxCosine by 0.03%, which indicates that MaxNorm is complementary to MaxCosine.\\n\\nBased on the above analysis, we propose a new OOD detection method termed Decoupling MaxLogit (DML). DML can be written as:\\n\\n$DML = \\\\lambda \\\\text{MaxCosine} + \\\\text{MaxNorm}$,\\n\\nwhere $\\\\lambda$ is a hyper-parameter. We normalize MaxCosine and MaxNorm scores by their sum on ID data respectively so that their importance can be separately considered. The weight $\\\\lambda$ is tuned on Gaussian noise. As Table 1 shows, DML outperforms MaxLogit by over 1.36%.\\n\\n4.2. Improving MaxCosine and MaxNorm\\n\\nAlthough MaxNorm helps DML to outperform MaxCosine, the improvement is marginal due to the low performance of MaxNorm. Therefore, we study the role of model training and show that a simple modification to standard training could significantly boost the performance of MaxNorm and MaxCosine. The experimental setting is the same as in Table 4. We report the mean area under the ROC curve (AUROC) on six OOD test datasets.\"}"}
{"id": "CVPR-2023-481", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. The AUROC of OOD detection and CFC / WFC on CIFAR-100 with WRN-40-2 and CE loss. MaxL: MaxLogit, MaxC: MaxCosine and MaxN: MaxNorm.\\n\\n| Loss   | AUROC | MaxL | MaxC | MaxN |\\n|--------|-------|------|------|------|\\n| Linear | 82.34 | 80.96| 81.78| 56.93|\\n| Cosine | 85.34 | 82.53| 84.14| 76.39|\\n\\nTable 3. The AUROC of OOD detection of and CFC/WFC of different loss and classifiers on CIFAR-100 with WRN-40-2. CLS: classifier, L: linear classifier, C: cosine classifier.\\n\\nWe also analyze it from a statistical view. As Table2 and 3 show, both the WFC and CFC scores decrease when using the cosine classifier. WFC indicates the with-class feature convergence and WFC is lower when the features are more compact. CFC measures the extent of the features' convergence to the corresponding classifier. Intuitively, the features having similar norms could lead to better MaxNorm. And the features closer to the corresponding classifier could lead to better MaxCosine. All the WFC and CFC are calculated on the training set.\\n\\nLower WFC leads to better MaxNorm. One approach to improve the WFC of the model is Center loss [38], naturally. The Center loss can be formulated as\\n\\n$$L_{center} = \\\\frac{1}{K} \\\\sum_{k=1}^{K} \\\\sum_{i=1}^{n} || h_{k,i} - C_k ||^2,$$\\n\\n(7)\\n\\nwhere $C_k$ is the mean feature of corresponding class $k$. Center loss is combined with CE loss with a weighting hyper-parameter as in [38]. Center loss urges the features to be more clustered and the features of identical classes have more similar feature norms. We use WFC to quantify that and suppose the model with lower WFC has better MaxNorm performance. To verify the assumption, we take different loss functions with different hyper-parameter and train the WRN-40-2 on CIFAR-100.\\n\\nWe can improve MaxNorm's performance by decreasing the WFC. Fig. 2 (a) shows the correlation between WFC and MaxNorm. For example, CE loss (scatter point 6); Focal loss with different $\\\\gamma$ (5,7,8); Center loss with different weight (250). The figure also shows that Center loss helps the model to gain a lower WFC score than both CE and Focal loss. Details can be found in supplementary materials.\\n\\nLower CFC leads to better MaxCosine. MaxCosine uses cosine similarity to identify OOD samples. Thus, when there are fewer ID samples in the low-likelihood region (i.e., fewer hard samples), the performance of MaxCosine could be better. A method to tackle hard samples is hard sample mining. The Focal loss [22] is one of the leading works in this area and can be written as\\n\\n$$L_{focal} = - \\\\sum_{k=1}^{K} \\\\sum_{i=1}^{n} (1 - p_{k,i})^\\\\gamma \\\\log(p_{k,i}),$$\\n\\n(8)\\n\\nwhere $\\\\gamma$ is a hyper-parameter and $p_{k,i}$ is the softmax score.\\n\\nFig. 2 (b) shows the correlation between CFC and MaxCosine. For example, CE loss (scatter point 3); Focal loss with different $\\\\gamma$ (1,2); Center loss with different weight (4-8). The figure shows that lower CFC leads to better MaxCosine. Also, training with Focal loss could lead to smaller CFC than both Center loss and CE loss.\\n\\nIn general, we can conclude: (1) compared with a linear classifier, a cosine classifier is more robust to OOD samples; (2) there exists correlations between WFC and MaxNorm, and between CFC and MaxCosine; (3) training with Center loss leads to larger CFC and smaller WFC, while the opposite is true for Focal loss.\\n\\n4.3. Theoretical analysis of WFC and CFC\\n\\nBelow, we reveal the lower bound of WFC and CFC along with the loss optimization.\\n\\nProposition 1 (Lower Bound of WFC and CFC)\\n\\nFor the normalized $w_1, w_2, \\\\ldots, w_K$ and $h \\\\in \\\\mathbb{R}^d$, $(z_{k,i})_j = w_j^\\\\top h_{k,i} \\\\in \\\\mathbb{R}$. CE loss is bounded by\\n\\n$$L_{CE} \\\\geq n \\\\log \\\\frac{1}{1 + \\\\sqrt{K} \\\\sqrt{\\\\frac{1}{K-1} ||W||_F ||h||_2}}.$$ \\n\\nWhen the equality holds, WFC and CFC reach the lower bound: $WFC, CFC \\\\geq 0$. The proof is provided in supplementary materials.\"}"}
