{"id": "CVPR-2023-834", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Michael A. Alcorn, Qi Li, Zhitao Gong, Chengfei Wang, Long Mai, Wei-Shinn Ku, and Anh Nguyen. Strike (with) a pose: Neural networks are easily fooled by strange poses of familiar objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4845\u20134854, 2019.\\n\\n[2] Amr Alexandari, Anshul Kundaje, and Avanti Shrikumar. Maximum likelihood with bias-corrected calibration is hard-to-beat at label shift adaptation. In International Conference on Machine Learning, pages 222\u2013232. PMLR, 2020.\\n\\n[3] Kamyar Azizzadenesheli, Anqi Liu, Fanny Yang, and Animashree Anandkumar. Regularized learning for domain adaptation under label shifts. arXiv preprint arXiv:1903.09734, 2019.\\n\\n[4] Shai Ben-David, John Blitzer, Koby Crammer, Alex Kulesza, Fernando Pereira, and Jennifer Wortman Vaughan. A theory of learning from different domains. Machine Learning, 79(1-2):151\u2013175, 2010.\\n\\n[5] Shai Ben-David, John Blitzer, Koby Crammer, and Fernando Pereira. Analysis of representations for domain adaptation. Advances in neural information processing systems, 19:137\u2013144, 2006.\\n\\n[6] Ryan Y. Benmalek, Sabhya Chhabria, Pedro O. Pinheiro, Claire Cardie, and Serge Belongie. Learning to adapt to semantic shift. 2021.\\n\\n[7] Lucas Beyer, Olivier J. H. Henaff, Alexander Kolesnikov, Xiaohua Zhai, and A. Aron van den Oord. Are we done with imagenet? arXiv preprint arXiv:2006.07159, 2020.\\n\\n[8] Konstantinos Bousmalis, Nathan Silberman, David Dohan, Dumitru Erhan, and Dilip Krishnan. Unsupervised pixel-level domain adaptation with generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3722\u20133731, 2017.\\n\\n[9] Konstantinos Bousmalis, George Trigeorgis, Nathan Silberman, Dilip Krishnan, and Dumitru Erhan. Domain separation networks. In Advances in neural information processing systems, pages 343\u2013351, 2016.\\n\\n[10] Tianle Cai, Ruiqi Gao, Jason Lee, and Qi Lei. A theory of label propagation for subpopulation shift. In International Conference on Machine Learning, pages 1170\u20131182. PMLR, 2021.\\n\\n[11] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. Advances in Neural Information Processing Systems, 33:9912\u20139924, 2020.\\n\\n[12] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9650\u20139660, 2021.\\n\\n[13] Chaoqi Chen, Weiping Xie, Wenbing Huang, Yu Rong, Xinghao Ding, Yue Huang, Tingyang Xu, and Junzhou Huang. Progressive feature alignment for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 627\u2013636, 2019.\\n\\n[14] Gongwei Chen, Xinhang Song, Bohan Wang, and Shuqiang Jiang. See more for scene: Pairwise consistency learning for scene classification. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 4987\u20134999. Curran Associates, Inc., 2021.\\n\\n[15] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.\\n\\n[16] Xinlei Chen and Abhinav Gupta. Webly supervised learning of convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 1431\u20131439, 2015.\\n\\n[17] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. pages 9640\u20139649, 2021.\\n\\n[18] Yi-Hsin Chen, Wei-Yu Chen, Yu-Ting Chen, Bo-Cheng Tsai, Yu-Chiang Frank Wang, and Min Sun. No more discrimination: Cross city adaptation of road scene segmenters. In Proceedings of the IEEE International Conference on Computer Vision, pages 1992\u20132001, 2017.\\n\\n[19] Jinwoo Choi, Chen Gao, Joseph CE Messou, and Jia-Bin Huang. Why can't i dance in the mall? learning to mitigate scene bias in action recognition. Advances in Neural Information Processing Systems, 32, 2019.\\n\\n[20] Myung Jin Choi, Antonio Torralba, and Alan S. Willsky. Context models and out-of-context objects. Pattern Recognition Letters, 33(7):853\u2013862, 2012.\\n\\n[21] Elijah Cole, Xuan Yang, Kimberly Wilber, Oisin Mac Aodha, and Serge J. Belongie. When does contrastive visual representation learning work? In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1\u201310. IEEE, 2022.\\n\\n[22] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.\\n\\n[23] Zhijie Deng, Yucen Luo, and Jun Zhu. Cluster alignment with a teacher for unsupervised domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages 9944\u20139953, 2019.\\n\\n[24] Terrance DeVries, Ishan Misra, Changhan Wang, and Laurens van der Maaten. Does object recognition work for everyone? CoRR, abs/1906.02659, 2019.\\n\\n[25] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n\\n[26] Abhimanyu Dubey, Vignesh Ramanathan, Alex Pentland, and Dhruv Mahajan. Adaptive methods for real-world domain generalization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14340\u201314349, 2021.\"}"}
{"id": "CVPR-2023-834", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180\u20131189. PMLR, 2015.\\n\\nSaurabh Garg, Yifan Wu, Sivaraman Balakrishnan, and Zachary Lipton. A unified view of label shift estimation. Advances in Neural Information Processing Systems, 33:3290\u20133300, 2020.\\n\\nRobert Geirhos, Patricia Rubisch, Claudio Michaelis, Matthias Bethge, Felix A Wichmann, and Wieland Brendel. Imagenet-trained cnns are biased towards texture; increasing shape bias improves accuracy and robustness. arXiv preprint arXiv:1811.12231, 2018.\\n\\nKristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18995\u201319012, 2022.\\n\\nXiang Gu, Jian Sun, and Zongben Xu. Spherical space domain adaptation with robust pseudo-label loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9101\u20139110, 2020.\\n\\nKaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009, 2022.\\n\\nKaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729\u20139738, 2020.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\nDan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: A critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8340\u20138349, 2021.\\n\\nJudy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In International conference on machine learning, pages 1989\u20131998. PMLR, 2018.\\n\\nChao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International Conference on Machine Learning, pages 4904\u20134916. PMLR, 2021.\\n\\nYing Jin, Ximei Wang, Mingsheng Long, and Jianmin Wang. Minimum class confusion for versatile domain adaptation. In European Conference on Computer Vision, pages 464\u2013480. Springer, 2020.\\n\\nTarun Kalluri and Manmohan Chandraker. Cluster-to-adapt: Few shot domain adaptation for semantic segmentation across disjoint labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4121\u20134131, 2022.\\n\\nTarun Kalluri, Astuti Sharma, and Manmohan Chandraker. Mem sac: Memory augmented sample consistency for large scale domain adaptation. arXiv preprint arXiv:2207.12389, 2022.\\n\\nTarun Kalluri, Girish Varma, Manmohan Chandraker, and CV Jawahar. Universal semi-supervised semantic segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pages 5259\u20135270, 2019.\\n\\nGuoliang Kang, Lu Jiang, Yi Yang, and Alexander G Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4893\u20134902, 2019.\\n\\nDonghyun Kim, Kaihong Wang, Stan Sclaroff, and Kate Saenko. A broad study of pre-training for domain generalization and adaptation. arXiv preprint arXiv:2203.11819, 2022.\\n\\nAbhishek Kumar, Prasanna Sattigeri, Kahini Wadhawan, Leonid Karlinsky, Rogerio Feris, Bill Freeman, and Gregory Wornell. Co-regularized alignment for unsupervised domain adaptation. In Advances in Neural Information Processing Systems, pages 9345\u20139356, 2018.\\n\\nJogendra Nath Kundu, Suvaansh Bhambri, Akshay Kulkarni, Hiran Sarkar, Varun Jampani, and R Venkatesh Babu. Subsidiary prototype alignment for universal domain adaptation. arXiv preprint arXiv:2210.15909, 2022.\\n\\nWen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database: Visual learning and understanding from web data. arXiv preprint arXiv:1708.02862, 2017.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.\\n\\nZachary Lipton, Yu-Xiang Wang, and Alexander Smola. Detecting and correcting for label shift with black box predictors. In International conference on machine learning, pages 3122\u20133130. PMLR, 2018.\\n\\nMingsheng Long, Yue Cao, Jianmin Wang, and Michael Jordan. Learning transferable features with deep adaptation networks. In International conference on machine learning, pages 97\u2013105. PMLR, 2015.\\n\\nMingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation. In Advances in Neural Information Processing Systems, pages 1640\u20131650, 2018.\\n\\nMingsheng Long, Han Zhu, Jianmin Wang, and Michael I Jordan. Deep transfer learning with joint adaptation networks. In International conference on machine learning, pages 2208\u20132217. PMLR, 2017.\"}"}
{"id": "CVPR-2023-834", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"adversaries for semantics consistent domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2507\u20132516, 2019.\\n\\nDhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European conference on computer vision (ECCV), pages 181\u2013196, 2018.\\n\\nChanghwa Park, Jonghyun Lee, Jaeyoon Yoo, Minhoe Hur, and Sungroh Yoon. Joint contrastive learning for unsupervised domain adaptation. arXiv preprint arXiv:2006.10297, 2020.\\n\\nZhongyi Pei, Zhangjie Cao, Mingsheng Long, and Jianmin Wang. Multi-adversarial domain adaptation. arXiv preprint arXiv:1809.02176, 2018.\\n\\nXingchao Peng, Qinxun Bai, Xide Xia, Zijun Huang, Kate Saenko, and Bo Wang. Moment matching for multi-source domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1406\u20131415, 2019.\\n\\nXingchao Peng, Ben Usman, Neela Kaushik, Judy Hoffman, Dequan Wang, and Kate Saenko. Visda: The visual domain adaptation challenge. arXiv preprint arXiv:1710.06924, 2017.\\n\\nViraj Prabhu, Ramprasaath R Selvaraju, Judy Hoffman, and Nikhil Naik. Can domain adaptation make object recognition work for everyone? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3981\u20133988, 2022.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\\n\\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In Kamalika Chaudhuri and Ruslan Salakhutdinov, editors, Proceedings of the 36th International Conference on Machine Learning, volume 97 of Proceedings of Machine Learning Research, pages 5389\u20135400. PMLR, 09\u201315 Jun 2019.\\n\\nWilliam A Gaviria Rojas, Sudnya Diamos, Keertan Ranjan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track.\\n\\nAmir Rosenfeld, Richard Zemel, and John K Tsotsos. The elephant in the room. arXiv preprint arXiv:1808.03305, 2018.\\n\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015.\\n\\nKate Saenko, Brian Kulis, Mario Fritz, and Trevor Darrell. Adapting visual category models to new domains. In European conference on computer vision, pages 213\u2013226. Springer, 2010.\\n\\nKuniaki Saito, Donghyun Kim, Stan Sclaroff, and Kate Saenko. Universal domain adaptation through self-supervision. arXiv preprint arXiv:2002.07953, 2020.\\n\\nKuniaki Saito and Kate Saenko. Ovanet: One-vs-all network for universal domain adaptation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9000\u20139009, 2021.\\n\\nKuniaki Saito, Yoshitaka Ushiku, Tatsuya Harada, and Kate Saenko. Adversarial dropout regularization. arXiv preprint arXiv:1711.01575, 2017.\\n\\nKuniaki Saito, Kohei Watanabe, Yoshitaka Ushiku, and Tatsuya Harada. Maximum classifier discrepancy for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3723\u20133732, 2018.\\n\\nSwami Sankaranarayanan, Yogesh Balaji, Carlos D Castillo, and Rama Chellappa. Generate to adapt: Aligning domains using generative adversarial networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8503\u20138512, 2018.\\n\\nChristoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021.\\n\\nD. Sculley, Eric Breck, Igor Ivanov, James Atwood, Miha Skalic, Pallavi Baljekar, Pavel Ostyakov, Roman Solovyev, Weimin Wang, and Yoni Halpern. The Inclusive Images Competition. 2019.\\n\\nShreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and D Sculley. No classification without representation: Assessing geodiversity issues in open data sets for the developing world. arXiv preprint arXiv:1711.08536, 2017.\\n\\nAstuti Sharma, Tarun Kalluri, and Manmohan Chandraker. Instance level affinity-based transfer for unsupervised domain adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5361\u20135371, 2021.\\n\\nKrishna Kumar Singh, Dhruv Mahajan, Kristen Grauman, Yong Jae Lee, Matt Feiszli, and Deepti Ghadiyaram. Don't judge an object by its context: learning to overcome contextual bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11070\u201311078, 2020.\\n\\nMannat Singh, Laura Gustafson, Aaron Adcock, Vinicius de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv Mahajan, Ross Girshick, Piotr Doll\u00e1r, and Laurens van der Maaten. Revisiting Weakly Supervised Pre-Training of Visual Perception Models. In CVPR, 2022.\"}"}
{"id": "CVPR-2023-834", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Baochen Sun and Kate Saenko. Deep coral: Correlation alignment for deep domain adaptation. In European conference on computer vision, pages 443\u2013450. Springer, 2016.\\n\\nShuhan Tan, Xingchao Peng, and Kate Saenko. Class-imbalanced domain adaptation: an empirical odyssey. In European Conference on Computer Vision, pages 585\u2013602. Springer, 2020.\\n\\nRohan Taori, Achal Dave, Vaishaal Shankar, Nicholas Carlini, Benjamin Recht, and Ludwig Schmidt. Measuring robustness to natural distribution shifts in image classification. Advances in Neural Information Processing Systems, 33, 2020.\\n\\nAntonio Torralba and Alexei A Efros. Unbiased look at dataset bias. In CVPR 2011, pages 1521\u20131528. IEEE, 2011.\\n\\nEric Tzeng, Judy Hoffman, Trevor Darrell, and Kate Saenko. Simultaneous deep transfer across domains and tasks. In Proceedings of the IEEE international conference on computer vision, pages 4068\u20134076, 2015.\\n\\nEric Tzeng, Judy Hoffman, Kate Saenko, and Trevor Darrell. Adversarial discriminative domain adaptation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7167\u20137176, 2017.\\n\\nHemanth Venkateswara, Jose Eusebio, Shayok Chakraborty, and Sethuraman Panchanathan. Deep hashing network for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5018\u20135027, 2017.\\n\\nRui Wang, Zuxuan Wu, Zejia Weng, Jingjing Chen, Guo-Jun Qi, and Yu-Gang Jiang. Cross-domain contrastive learning for unsupervised domain adaptation. arXiv preprint arXiv:2106.05528, 2021.\\n\\nYan Wang, Xiangyu Chen, Yurong You, Li Erran Li, Bharath Hariharan, Mark Campbell, Kilian Q Weinberger, and Weilun Chao. Train in germany, test in the usa: Making 3d object detectors generalize. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11713\u201311723, 2020.\\n\\nGuoqiang Wei, Cuiling Lan, Wenjun Zeng, Zhizheng Zhang, and Zhibo Chen. Toalign: Task-oriented alignment for unsupervised domain adaptation. In NeurIPS, 2021.\\n\\nJianxiong Xiao, James Hays, Krista A Ehinger, Aude Oliva, and Antonio Torralba. Sun database: Large-scale scene recognition from abbey to zoo. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 3485\u20133492. IEEE, 2010.\\n\\nShaoan Xie, Zibin Zheng, Liang Chen, and Chuan Chen. Learning semantic representations for unsupervised domain adaptation. In International Conference on Machine Learning, pages 5423\u20135432, 2018.\\n\\nRuijia Xu, Guanbin Li, Jihan Yang, and Liang Lin. Larger norm more transferable: An adaptive feature norm approach for unsupervised domain adaptation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1426\u20131435, 2019.\\n\\nKaichao You, Mingsheng Long, Zhangjie Cao, Jianmin Wang, and Michael I Jordan. Universal domain adaptation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2720\u20132729, 2019.\\n\\nKun Zhang, Bernhard Sch\u00f6lkopf, Krikamol Muandet, and Zhikun Wang. Domain adaptation under target and conditional shift. In International conference on machine learning, pages 819\u2013827. PMLR, 2013.\\n\\nWeichen Zhang, Wanli Ouyang, Wen Li, and Dong Xu. Collaborative and adversarial network for unsupervised domain adaptation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3801\u20133809, 2018.\\n\\nYuchen Zhang, Tianle Liu, Mingsheng Long, and Michael I Jordan. Bridging theory and algorithm for domain adaptation. arXiv preprint arXiv:1904.05801, 2019.\\n\\nBingchen Zhao, Shaozuo Yu, Wufei Ma, Mingxin Yu, Shenxiao Mei, Angtian Wang, Ju He, Alan Yuille, and Adam Kortylewski. Ood-cv: A benchmark for robustness to individual nuisances in real-world out-of-distribution shifts. In ICML 2022 Shift Happens Workshop, 2022.\\n\\nBolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A 10 million image database for scene recognition. IEEE transactions on pattern analysis and machine intelligence, 40(6):1452\u20131464, 2017.\\n\\nBolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Torralba, and Aude Oliva. Learning deep features for scene recognition using places database. Advances in neural information processing systems, 27, 2014.\"}"}
{"id": "CVPR-2023-834", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nIn recent years, several efforts have been aimed at improving the robustness of vision models to domains and environments unseen during training. An important practical problem pertains to models deployed in a new geography that is under-represented in the training dataset, posing a direct challenge to fair and inclusive computer vision.\\n\\nIn this paper, we study the problem of geographic robustness and make three main contributions. First, we introduce a large-scale dataset GeoNet for geographic adaptation containing benchmarks across diverse tasks like scene recognition (GeoPlaces), image classification (GeoImNet) and universal adaptation (GeoUniDA). Second, we investigate the nature of distribution shifts typical to the problem of geographic adaptation and hypothesize that the major source of domain shifts arise from significant variations in scene context (context shift), object design (design shift) and label distribution (prior shift) across geographies. Third, we conduct an extensive evaluation of several state-of-the-art unsupervised domain adaptation algorithms and architectures on GeoNet, showing that they do not suffice for geographical adaptation, and that large-scale pre-training using large vision models also does not lead to geographic robustness. Our dataset is publicly available at https://tarun005.github.io/GeoNet.\\n\\n1. Introduction\\nIn recent years, domain adaptation has emerged as an effective technique to alleviate dataset bias [80] during training and improve transferability of vision models to sparsely labeled target domains [27, 36, 40, 42, 49\u201351, 68, 69, 86, 89]. While being greatly instrumental in driving research forward, methods and benchmark datasets developed for domain adaptation [56, 57, 64, 83] have been restricted to a narrow set of divergences between domains. However, the geographic origin of data remains a significant source of bias, attributable to several factors of variation between train and test data. Training on geographically biased datasets may cause a model to learn the idiosyncrasies of their geographies, preventing generalization to novel domains with significantly different geographic and demographic composition. Besides robustness, this may have deep impact towards fair and inclusive computer vision.\\n\\nFigure 1. Summary of our contributions (a): Training computer vision models on geographically biased datasets suffers from poor generalization to new geographies. We propose a new dataset called GeoNet to study this problem and take a closer look at the various types of domain shifts induced by geographic variations. (b) Prior unsupervised adaptation methods that efficiently handle other variations do not suffice for improving geographic transfer. (c) We highlight the limitations of modern convolutional and transformer architectures in addressing geographic bias, exemplified here by USA \u2192 Asia transfer on GeoImNet.\"}"}
{"id": "CVPR-2023-834", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"computer vision, as most modern benchmark datasets like ImageNet [63] and COCO [47] suffer from a significant US or UK-centric bias in data [24, 73], with poor representation of images from various other geographies like Asia. In this paper, we study the problem of geographic adaptation by introducing a new large-scale dataset called GeoNet, which constitutes three benchmarks \u2013 GeoPlaces for scene classification, GeoImNet for object recognition and GeoUniDA for universal domain adaptation. These benchmarks contain images from USA and Asia, which are two distinct geographical domains separated by various cultural, economic, demographic and climatic factors. We additionally provide rich metadata associated with each image, such as GPS location, captions and hashtags, to facilitate algorithms that leverage multimodal supervision.\\n\\nGeoNet captures the multitude of novel challenges posed by varying image and label distributions across geographies. We analyze GeoNet through new sources of domain shift caused by geographic disparity, namely (i) context shift, where the appearance and composition of the background in images changes significantly across geographies, (ii) design shift, where the design and make of various objects changes across geographies, and (iii) prior shift, caused by different per-category distributions of images in both domains. We illustrate examples of performance drop caused by these factors in Fig. 1a, where models trained on images from USA fail to classify common categories such as running track and mailbox due to context and design shifts, respectively.\\n\\nGeoNet is an order of magnitude larger than previous datasets for geographic adaptation [58, 61], allowing the training of modern deep domain adaptation methods. Importantly, it allows comparative analysis of new challenges posed by geographic shifts for algorithms developed on other popular adaptation benchmarks [56, 57, 64, 83]. Specifically, we evaluate the performance of several state-of-the-art unsupervised domain adaptation algorithms on GeoNet, and show their limitations in bridging domain gaps caused by geographic disparities. As illustrated in Fig. 1b for the case of DomainNet [56] vs. GeoNet, state-of-the-art models on DomainNet often lead to accuracies even worse than a source only baseline on GeoNet, resulting in negative relative gain in accuracy (defined as the gain obtained by an adaptation method over a source-only model as a percentage of gap between a source-only model and the target-supervised upper bound). Furthermore, we also conduct a study of modern architectures like vision transformers and various pre-training strategies, to conclude that larger models with supervised and self-supervised pre-training offer improvements in accuracy, which however are not sufficient to address the domain gap (Fig. 1c). This highlights that the new challenges introduced by geographic bias such as context and design shift are relatively under-explored, where our dataset may motivate further research towards this important problem.\\n\\nIn summary, our contribution towards geographic domain adaptation is four-fold:\\n\\n\u2022 A new large-scale dataset, GeoNet, with benchmarks for diverse tasks like scene classification and object recognition, with labeled images collected from geographically distant locations across hundreds of categories (Sec. 3).\\n\u2022 Analysis of domain shifts in geographic adaptation, which may be more complex and subtle than style or appearance variations (Sec. 3.4).\\n\u2022 Extensive benchmarking of unsupervised adaptation algorithms, highlighting their limitations in addressing geographic shifts (Sec. 4.2).\\n\u2022 Demonstration that large-scale pretraining and recent advances like vision transformers do not alleviate these geographic disparities (Sec. 4.3).\\n\\n2. Related Works\\n\\nDomain Adaptation\\n\\nUnsupervised domain adaptation enables training models on a labeled source domain along with unlabeled samples from a different target domain to improve the target domain accuracy. A large body of prior works aim to minimize some notion of divergence [4, 5] between the source and target distributions based on MMD [49, 51, 77, 78], adversarial [9, 13, 27, 50, 68, 81, 82, 92], generative [8, 36, 70], class-level [31, 44, 52, 55, 69, 88] or instance-level alignment [74, 84, 86] techniques. Clustering [23, 39, 41, 42, 54] and memory-augmentation approaches [40] have also been shown to be effective. However, most of these works are shown to improve performance using standard datasets such as Office-31 [64], visDA [57], OfficeHome [83] or DomainNet [56], where the distribution shifts typically arise from unimodal variations in style or appearance between source and target. While prior works also study semantic shift [6] and sub-population shift [10], we aim to address a more practical problem of geographic domain adaptation with more complex variations not covered by prior works.\\n\\nGeographic Robustness\\n\\nMany prior works study biases of CNNs towards 3D poses [1, 94], textures [29], styles [35], natural variations [7, 60, 79] and adversarial inputs [35], but robustness of computer vision towards shift induced by geography is relatively under-explored. While algorithms for bridging geographic domain gaps have been proposed in [18, 41, 85], they are restricted to road scenes with limited number of classes. A major hindrance has been the lack of suitable benchmark datasets for geographic adaptation, so several datasets have been recently proposed to address this issue [24, 58, 61, 72]. Datasets based on dollar street images [61] highlight the geographic differences induced by income disparities between various countries, Ego4D [30] contains egocentric videos with actions from various geographies, while researchers in [58] design an adaptation dataset with images from YFCC-100M [26] to analyze geographic.\"}"}
{"id": "CVPR-2023-834", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"while also providing the appropriate level of abstraction and categories from each of our settings. In this paper, we present the overall summary of various datasets in Table 1, including the number of images per class from USA and Asia domains shown for the GeoPlaces benchmark in (a) and GeoImNet benchmark in (b). The label distributions are long-tailed in both, and the dominant and tail classes are widely different across geographies in each setting.\\n\\nClass distribution in GeoNet\\n\\nTable 1. Summary of GeoNet universal domain adaptation across geographies.\\n\\n| Split          | GeoPlaces | GeoImNet | GeoUniDA |\\n|----------------|-----------|----------|----------|\\n| USA Train      | 178110    | 154816   | 100136   |\\n| Test           | 26923     | 9636     | 8478     |\\n| USA Test       | 17234     | 16784    | 25034    |\\n| Asia Train     | 15370     | 15010    | 100136   |\\n| Test           | 26923     | 9636     | 8478     |\\n| Asia Test      | 17234     | 16784    | 25034    |\\n\\nWe propose GeoPlaces to study geographic adaptation in contrast to object classification, it is necessary to accurately identify and understand various interactions and relationships between the objects and people in the scene to predict and contrast to object classification, it is necessary to accurately identify and understand various interactions and relationships between the objects and people in the scene to predict.\"}"}
{"id": "CVPR-2023-834", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Context Shift in GeoNet\\n\\nA few examples showing the nature of context shifts across categories from GeoPlaces benchmark in (a), and GeoImNet benchmark in (a), arising due to a variety of differences between geographical disparity. For example, outdoor scenes (shopfront, marketplace) reflect the demographics across geographies, indoor-scenes (living rooms, cafeteria) reflect cultural and economic variations and wildlife images reflect the habitat and climatic variations.\\n\\nFigure 4. Design Shift in GeoNet\\n\\nWe show examples illustrating the design shifts for the cases of castle from GeoPlaces and candle from GeoImNet. Note that differences in designs of castles as well as the variety of objects like candles found across geographies lead to design shifts between the domains.\\n\\nAdditional Data\\n\\nDue to the inherent US-centric bias of photo-sharing websites like Flickr, a major portion of images are US-based. In order to collect more images from the Asia domain, we directly scrape images from Flickr using the 205 category names from Places-205 as the seed concepts. As many Asian users often post descriptions and tags for pictures in languages other than English, we use translations of these seed concepts in English to 6 Asian languages, namely {Hindi, Korean, Japanese, Chinese, Russian, Hebrew}, and use these along with the original concepts, as the augmented or expanded concepts. Then, we search Flickr for images which match the criterion that (i) they are geotagged in Asia, and (ii) the tags associated with the image match with exactly one of the categories in the expanded concept list (which we assign as the label). We collect around 190k images this way, and use this as the training set. Since images collected from web tend to be nosier than human labeled ones, we use the manually labeled 27k images from Places-205 as the test set for Asia domain to ensure robust benchmarking.\\n\\n3.2. GeoImNet\\n\\nWe propose the GeoImNet benchmark to investigate the domain shift due to geographical disparities on object classification. Different from existing object-level datasets for domain adaptation [56, 57, 64, 83], GeoImNet provides domain shifts induced by geographic disparities.\\n\\nDataset curation\\n\\nWe collect images in the GeoImNet benchmark from the WebVision dataset [46], which itself is scraped from Flickr using queries generated from 5000 concepts in the Imagenet-5k dataset [22]. We then follow the same pipeline as explained above for GeoPlaces benchmark, and identify the GPS coordinates of each images using its Flickr-id.\\n\\nConcept Selection\\n\\nAlthough the original dataset contains 5000 classes, many of these classes are indigenous to a particular geography. For example, Bengal Tiger are found in Indian subcontinent, and Bald Eagle is a North-American bird. Since unsupervised domain adaptation typically demands matching label spaces across source and target, we select 600 categories out of the original 5000 with at least 20 images in each domain from each category. We then assign roughly 15% of images from each domain into the test set and use the remaining as the training images.\"}"}
{"id": "CVPR-2023-834", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dataset filtering\\n\\nWebVision is webly supervised [16], which does not guarantee object-centric images or clean labels. Therefore, we remove all the images from the dataset which have more than one tag that match our selected concepts (the 600 chosen categories) to handle multi-labeled images. Furthermore, we manually quality-check all the test images and remove all the images with noisy labels. Finally, we perform de-duplication to remove images from the training set which are very similar to those in the test set. More insights into each step of our data collection and filtering process is provided in the supplementary material. The final label distribution for both US and Asia domains in both our benchmarks is shown in Fig. 2.\\n\\n3.3. GeoUniDA\\n\\nUniversal Domain Adaptation (UniDA) [90] facilitates domain adaptation between source and target domains that have few private classes, in addition to shared classes which are common to both. While this is a realistic problem, prior works [45, 65, 67, 90] use benchmarks created from existing UDA datasets for evaluation. However, our proposed geographical adaptation setting gives us an unique opportunity to design benchmarks for UniDA such that the private categories from the source and the target are a natural reflection of the presence or absence of these categories in the respective geographical domains. In order to select the shared and private categories for our Geo-UniDA benchmark, we first start with the 1000 categories in the original Imagenet-1k dataset [63], and select top 200 categories each in the USA and Asia domains that have the most number of images from the WebVision dataset. Out of these, we use the 62 common classes as the shared categories, and the remaining 138 as the private classes in each domain.\\n\\n3.4. Analysis of Distribution Shifts\\n\\nWe denote the source dataset using $D_s = \\\\{X_s, Y_s\\\\}$, and assume that $X_s \\\\sim P_s(x)$ and $(X_s, Y_s) \\\\sim P_s(x, y)$ where $P_s(x)$ and $P_s(x, y)$ are the image marginal and image-label joint distribution respectively. Target dataset $D_t = \\\\{X_t, Y_t\\\\}$ and target distributions $P_t(x)$ and $P_t(x, y)$ are defined similarly, and the domain discrepancy assumption states that $P_s(x, y) \\\\neq P_t(x, y)$. In order to formulate domain shift across geographies, we define $f_x$ as the part of image referring to the foreground objects (corresponds to the salient objects in a scene) and $b_x$ to be the rest of the image corresponding to the background regions (corresponding to the surrounding regions or context). For example, for the task of classifying living room in Fig. 3a from GeoPlaces, common objects like sofa and table are foreground, while floor, roof and walls are backgrounds. We make a simplifying assumption that an image is completely explainable using its foreground and background and replace the class-conditional distribution of the images $P(x|y)$ with the joint class-conditional $P(b_x, f_x|y)$. Further, we also assume that given a class label, the background is conditionally independent of the foreground. Then,\\n\\n$$P(x, y) = P(x|y) \\\\cdot P(y) = P(b_x|y) \\\\cdot P(f_x|b_x, y) \\\\cdot P(y) = \\\\Rightarrow P(x, y) = P(b_x|y) \\\\cdot P(f_x|b_x, y) \\\\cdot P(y).$$\\n\\nWe define the class-conditional background distribution $P(b_x|y)$ as context, class-conditional object distribution $P(f_x|y)$ as design and the label distribution $P(y)$ as prior. Note that standard covariate shift assumption [4] assumes uniform domain discrepancy across all the images ($P_s(x) \\\\neq P_t(x)$), which does not hold for geographic adaptation due to the diverse source of variations. We analyze each of these from a geographic adaptation perspective next.\\n\\nContext Shift\\n\\nWe define context shift to be the changes in the context around an object or scene given by $P_s(b_x|y) \\\\neq P_t(b_x|y)$. Deep learning models are generally sensitive to object contexts and backgrounds, and learn spurious correlations that impede their ability to recognize objects and scenes in novel contexts [19, 20, 62, 75]. In geographic adaptation, context shift can be caused by differences in cultural or economic factors across geographies, and few examples illustrating context shift from GeoPlaces and GeoImNet are shown in Fig. 3. While prior works already introduce context shift for domain adaptation [58], a key difference lies in their modeling assumption that the context is irrelevant while training, while in our case context might play a key role in improving scene classification on GeoPlaces.\\n\\nDesign Shift\\n\\nWe define \u201cdesign\u201d shift as the change in object structure, shape and appearance, where the foreground objects belonging to the same semantic category look different across geographies, given by $P_s(f_x|y) \\\\neq P_t(f_x|y)$. Few examples are shown in Fig. 4, where categories like castle from GeoPlaces and candle from GeoImNet datasets look widely different due to high intra-class variance, although they belong to the same semantic category. It is important to note that context and design shifts might also occur within a domain or within a geography. However, it is easier to account for intra-domain variations on labeled source datasets than ensuring robustness to new and unlabeled geographies.\\n\\nPrior Shift\\n\\nThe label distributions across the domains in our benchmarks widely differ due to natural prominence or rarity of the classes according to the geography, as shown in Fig. 2, where the head classes of one domain might be tail classes in another. This leads to a prior shift where $P_s(y) \\\\neq P_t(y)$. For example, categories like railway station, outdoor markets, monasteries are common in Asia while baseball stadiums are more common in USA. Prior works examining prior shift or label shift across domains [2, 3, 28, 48, 91] generally\"}"}
{"id": "CVPR-2023-834", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 2. Top-1/Top-5 accuracies of Resnet-50 models across geographically different train and test domains. Note the significant drop in accuracies caused by the geographical domain shifts in each setting.\\n\\n|       | GeoPlaces |       | GeoImNet |       |\\n|-------|-----------|-------|----------|-------|\\n|       | Original  | USA   | Asia     | USA   | Asia   |\\n|       | Balanced | Drop  | Drop     | Drop  | Drop   |%\\n| USA   | 56.35/77.95 | 36.98/63.42 | -19.37/-14.53 | 56.35/77.95 | 36.98/63.42 | -19.37/-14.53 |\\n| Asia  | 40.43/64.60 | 60.37/80.22 | -19.94/-15.62 | 40.43/64.60 | 60.37/80.22 | -19.94/-15.62 |\\n\\nTable 3. USA \u2192 Asia comparison between GeoNet and its label-balanced version. Non-trivial gaps between the geographies still exist even after accounting for prior shift between the domains.\\n\\nMeta-category wise error analysis for GeoImNet\\nWe relate the drop in performances across geographies to the proposed notions of domain discrepancy in geographic adaptation like context and design shifts in Fig. 5. Specifically, since the concepts in GeoImNet are sourced from ILSVRC, we leverage the Wordnet hierarchy to group our 600 classes into 9 meta-labels. We then average the accuracy within each meta-class from USA \u2192 Asia domain transfer, and plot the difference in accuracy across domains per meta-label in Fig. 5. We note that categories in the meta-label \u201canimals\u201d have minimum design-shift across domains, but suffer from context shift due to shifts in weather and habitats across geographies leading to significant drop in accuracy. On the other hand, many categories in \u201cequipment\u201d and \u201cobject\u201d (like candle, broom, sewing machine) have prominent design shifts (Fig. 4) leading to notable performance drop. Finally, categories in \u201cfood\u201d (like bottled water, ice-cream) have minimum change in both design and context and hence suffer the least fall in accuracy across domains.\\n\\nGradCAM visualization of the failure cases\\nWe present few examples in Fig. 6 of predictions made on Asia test images by a model trained on USA, along with their Grad-CAM visualizations. As shown, when the model focuses on the context and background, it fails to generalize to new scenes from target geographies with notable shifts in context (kitchen classified as art studio). Even in cases when the model accurately focuses on the foreground object, it sometimes leads to incorrect predictions due to design shifts between geographies, where oil lamp is accurately localized, but predicted as bouquet.\\n\\nSeparating the prior shift\\nTo further delineate prior shift from context and design shifts, we curate a balanced subset out of GeoNet such that each category has about 200-300 images, and drop categories which have fewer images (about 3/4th of the categories remain). From Tab. 3, the drop in accuracy after addressing the prior shift is 12.9% on GeoPlaces and 15.4% on GeoImNet, compared to 20.08% and 19.37% on the original datasets, showing that non-trivial accuracy drop still exists.\"}"}
{"id": "CVPR-2023-834", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. UDA on GeoNet\\n\\n| Method       | Top-1 | Top-5 |\\n|--------------|-------|-------|\\n| Source Only  | 36.27 | 63.27 |\\n| DANN [27]    | 29.58 | 55.23 |\\n| CDAN [50]    | 30.48 | 55.94 |\\n| MCC [38]     | 30.09 | 55.85 |\\n| SAFN [89]    | 32.50 | 57.93 |\\n| MDD [93]     | 34.18 | 59.10 |\\n| MCD [69]     | 33.49 | 59.41 |\\n| ToAlign [86] | 29.86 | 56.16 |\\n| MemSAC [40]  | 34.68 | 60.52 |\\n\\nTable 5. Universal domain adaptation methods on Geo-UniDA\\n\\n| Method       | closed-set | open-set | H-Score          |\\n|--------------|------------|----------|------------------|\\n| UniDA [90]   | 27.64      | 43.93    | 33.93            |\\n| DANCE [66]   | 38.54      | 78.73    | 51.75            |\\n| OV ANet [67] | 36.54      | 66.89    | 47.26            |\\n\\nTable 4. UDA on GeoNet\\n\\nTop-1 and Top-5 accuracies of various unsupervised adaptation methods on GeoNet. Most of the methods fail to sufficiently handle cross-geography transfer on both GeoPlaces and GeoImNet benchmarks and often give lower accuracies even compared to a baseline model trained only using source data, calling attention to the need for novel methods that can handle domain shifts beyond style and appearance.\\n\\nTable 5. Universal domain adaptation methods on Geo-UniDA.\\n\\n| Method       | closed-set | open-set | H-Score          |\\n|--------------|------------|----------|------------------|\\n| UniDA [90]   | 27.64      | 43.93    | 33.93            |\\n| DANCE [66]   | 38.54      | 78.73    | 51.75            |\\n| OV ANet [67] | 36.54      | 66.89    | 47.26            |\\n\\nWe study the effectiveness of prior unsupervised adaptation algorithms in bridging novel notions of domain gaps like context shift and design shift on GeoNet. We review various standard as well as current state-of-the-art domain adaptation methods to examine their geographical robustness.\\n\\nArchitecture and training details\\n\\nWe follow the standard protocol established in prior works [40, 50, 69] and use an ImageNet pre-trained Resnet-50 [34] as the feature extractor backbone and a randomly initialized classifier layer. We use a batch size of 32 and SGD with a learning rate of 0.01 for the classifier head and 0.001 for the already pretrained backbone. We report the top-1 and top-5 accuracy numbers using the test splits from each benchmarks. We perform comparisons between traditional adversarial methods (DANN [27], CDAN [50]), class-aware adaptation methods (MCC [38], MDD [93]), non-adversarial methods (SAFN [89], MCD [69]) as well as recent state-of-the-art (ToAlign [86], MemSAC [40]). We train prior works using their publicly available code and adopt all hyper-parameters as recommended in the respective papers.\\n\\nExisting UDA methods do not suffice on GeoNet\\n\\nWe show the Top-1 and Top-5 accuracies of all the transfer settings from GeoNet in Tab. 4. A key observation is that most of the domain adaptation approaches are no better, or sometimes even worse, than the baseline model trained only using source domain data, indicating their limitations for geographic domain adaptation. For example, on GeoPlaces, training using data from USA achieves a top-1 accuracy of 36.27% on test data from Asia test images, while the best adaptation method (MemSAC) obtains lesser accuracy of 34.7%, indicating negative transfer. Likewise, on GeoImNet, a USA-trained source model achieves 36.98% on test images from Asia which is comparable to the best adaptation accuracy of 36.71%. To further illustrate this, we define relative accuracy gain as the improvement in accuracy obtained by a method over a source-only model as a percentage of gap between a source-only model and the target-supervised upper bound (which is 100% if the method achieves the target-supervised upper bound). From Fig. 1b, it is notable that the same adaptation methods that yield significantly high relative accuracy gains on DomainNet [56] yield negative relative accuracy gains on GeoNet, highlighting the unique nature of distribution shifts in real-world settings like geographic adaptation that challenge existing methods. These observations also suggest that future research should focus on context-aware and object-centric representations in addition to domain invariant features to improve cross-domain transfer amidst context and design shifts.\\n\\nUniversal domain adaptation on Geo-UniDA\\n\\nWe run SOTA universal domain adaptation methods (You et.al. [90], DANCE [66] and OvaNET [67]) on the Geo-UniDA benchmark of GeoNet. Following prior works [67], we adopt the H-score metric which is a harmonic mean of closed-set and open-set accuracies giving equal importance to closed set transfer as well as open set accuracy. In Tab. 5, we show that DANCE [66] outperforms both You et.al. [90] and Ov ANet [67] on the Geo-UniDA benchmark. We also show...\"}"}
{"id": "CVPR-2023-834", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7. We show that most architectures and pre-training strategies exhibit significant cross-domain drops when fine-tuned on geographically biased datasets. Shown for USA \u2192 Asia on GeoPlaces, refer Fig. 1c for the plot on GeoImNet and supplementary material for other transfer settings.\\n\\n4.3. Large-scale pre-training and architectures\\n\\nIt is common to use large scale self-supervised [11, 12, 15, 17, 32, 33] and weakly-supervised [37, 53, 76] pre-trained models as starting points in various downstream applications. While recent works explored role of pre-training on domain robustness [43], we are interested in the extent to which large scale pre-training effectively preserved robustness when fine-tuned on geographically under-represented datasets. We investigate the performance of a variety of methods on GeoNet in terms of backbone architectures, pre-training strategies and supervision.\\n\\nExperimental setup\\n\\nOur backbone architectures include Resnet50 [34] as well as the small (ViT-S), base (ViT-B) and large (ViT-L) vision transformers [25]. In terms of supervision, in addition to the standard supervised pre-training on ImageNet-1k, we also consider self-supervised methods MoCo-V3 [17], SwAV [11], DINO [12], MAE [32] trained on ImageNet-1k, the weakly supervised SWAG [76] trained on 3.6B uncurated instagram images and CLIP [59] trained on 400M image-language pairs [71]. We denote {Backbone-Supervision-Data} for different model choices (for example, Resnet50-sup-IN1k indicates a Resnet50 pre-trained on supervised data from ImageNet-1k).\\n\\nFor evaluating geographic robustness of these models, we first take the pre-trained model and fine-tune it on training data from a \u201csource\u201d geography, then evaluate the performance on test data from the \u201ctarget\u201d geography. We show the results using USA as the source and Asia as the target from the GeoPlaces benchmark in Fig. 7, and GeoImNet benchmark in Fig. 1c. For reference, we also report accuracy after fine-tuning on labeled data from the target geography using the same {Backbone-Supervision-Data} pair (denoted as target-supervised), which serves as an upper bound for the transfer performance.\\n\\nLarge-scale pretraining is not geographically robust\\n\\nFrom Fig. 7, we make a few observations. Firstly, comparison between Resnet50 and ViT-S which have roughly the same number of parameters suggests the superiority of the vision transformer architectures over CNNs. For example, ViT-S-sup-IN1k is better than Resnet50-sup-IN1k, and ViT-S-moco-IN1k is better than Resnet50-moco-IN1k, indicating that global reasoning using self-attention layers in vision transformers benefits context-dependent tasks like GeoPlaces. Next, comparing different pre-training strategies, we observe that MoCo gives best accuracy on ViT-S and ViT-B, while supervised pre-training outperforms other approaches on large models like ViT-L. However, the gap between target supervised accuracy and the best adaptation accuracy achieved using either Resnet50 or any of the vision transformers is still high, highlighting the need for better transfer strategies. In terms of data, weakly-supervised pre-training using billion-scale dataset IG3.6B (ViT-B-swag-3B) shows significant improvements over self-supervised training methods like MAE (ViT-B-mae-IN1k) and DINO (ViT-B-dino-IN1k). But despite training on massive-scale data, ViT-L-swag-3B and ViT-L-clip-2B are still inferior to the target supervised accuracies, revealing the limitations of current pre-training strategies towards robust cross-geography transfer after fine-tuning. While the success of large-scale pre-training strategies are well-documented on popular datasets like ImageNet, our results indicate that similar benefits might not be observed when application domains significantly differ from pre-training or fine-tuning datasets [21].\\n\\n5. Conclusion\\n\\nWe introduce a new dataset called GeoNet for the problem of geographic adaptation with benchmarks covering the tasks of scene and object classification. In contrast to existing datasets for domain adaptation [56,57,64,83], our dataset with images collected from different locations contains domain shifts captured by natural variations due to geographies, cultures and weather conditions from across the world, which is a novel and understudied direction in domain adaptation. Through GeoNet, we analyze the sources of domain shift caused by changes in geographies such as context and design shift. We conduct extensive benchmarking on GeoNet and highlight the limitations of current domain adaptation methods as well as large-scale pretraining methods towards geographical robustness. Finally, in spite of geographical diversity in GeoNet, we note a possible limitation of indirect bias towards USA as the user-base on photo-sharing sites like Flickr is dominated by the US. Creating datasets that are a more natural reflection of cultures and trends from diverse geographies and devising learning algorithms robust to those variations is an exciting proposition for the future.\\n\\nAcknowledgements\\n\\nWe thank NSF CAREER 1751365, NSF Chase-CI 1730158 and Google Award for Inclusion Research.\"}"}
