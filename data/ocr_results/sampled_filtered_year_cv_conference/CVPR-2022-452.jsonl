{"id": "CVPR-2022-452", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] R. Arandjelovi\u0107 and Andrew Zisserman. Three things everyone should know to improve object retrieval. 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 2911\u20132918, 2012.\\n\\n[2] Relja Arandjelovi\u0107, Petr Gronat, Akihiko Torii, Tom\u00e1\u0161 Pajdla, and Josef Sivic. NetVLAD: CNN architecture for weakly supervised place recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(6):1437\u20131451, 2018.\\n\\n[3] Relja Arandjelovi\u0107 and Andrew Zisserman. Dislocation: Scalable descriptor distinctiveness for location recognition. In Daniel Cremers, Ian D. Reid, Hideo Saito, and Ming-Hsuan Yang, editors, Computer Vision - ACCV 2014 - 12th Asian Conference on Computer Vision, Singapore, Singapore, November 1-5, 2014, Revised Selected Papers, Part IV, volume 9006 of Lecture Notes in Computer Science, pages 188\u2013204. Springer, 2014.\\n\\n[4] Hossein Azizpour, Ali Razavian, Josephine Sullivan, Atsuto Maki, and Stefan Carlsson. Factors of transferability for a generic convnet representation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38, 11 2015.\\n\\n[5] Artem Babenko and Victor Lempitsky. Aggregating deep convolutional features for image retrieval. ICCV, 10 2015.\\n\\n[6] Artem Babenko and Victor S. Lempitsky. The inverted multi-index. In CVPR, pages 3069\u20133076. IEEE Computer Society, 2012.\\n\\n[7] Artem Babenko, Anton Slesarev, A. Chigorin, and V. Lempitsky. Neural codes for image retrieval. ArXiv, abs/1404.1777, 2014.\\n\\n[8] Herbert Bay, Andreas Ess, Tinne Tuytelaars, and Luc Van Gool. Speeded-up robust features (surf). Computer Vision and Image Understanding, 110:346\u2013359, 06 2008.\\n\\n[9] Gabriele Berton, Carlo Masone, Valerio Paolicelli, and Barbara Caputo. Viewpoint Invariant Dense Matching for Visual Geolocalization. In IEEE International Conference on Computer Vision, 2021.\\n\\n[10] Gabriele Moreno Berton, Valerio Paolicelli, Carlo Masone, and Barbara Caputo. Adaptive-attentive geolocalization from few queries: A hybrid approach. In IEEE Winter Conference on Applications of Computer Vision, pages 2918\u20132927, January 2021.\\n\\n[11] B. Cao, A. Araujo, and J. Sim. Unifying deep local and global features for image search. In European Conference on Computer Vision, pages 726\u2013743. Springer Int. Publishing, 2020.\\n\\n[12] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In IEEE International Conference on Computer Vision, pages 9650\u20139660, October 2021.\\n\\n[13] D. M. Chen, G. Baatz, K. K\u00f6ser, S. S. Tsai, R. Vedantham, T. Pylv\u00e4n\u00e4inen, K. Roimela, X. Chen, J. Bach, M. Pollefeys, B. Girod, and R. Grzeszczuk. City-scale landmark identification on mobile devices. In IEEE Conference on Computer Vision and Pattern Recognition, pages 737\u2013744, 2011.\\n\\n[14] Zetao Chen, Adam Jacobson, Niko Sunderhauf, Ben Upcroft, Lingqiao Liu, Chunhua Shen, Ian Reid, and Michael Milford. Deep learning features at scale for visual place recognition. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 3223\u20133230, 2017.\\n\\n[15] Gabriela Csurka, Christopher Dance, Lixin Fan, Jutta Willamowski, and C\u00e9dric Bray. Visual categorization with bags of keypoints. In European Conference on Computer Vision, volume Vol. 1, 01 2004.\\n\\n[16] M. Cummins and P. Newman. Highly scalable appearance-only slam - FAB-MAP 2.0. In Robotics: Science and Systems, 2009.\\n\\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. ArXiv, abs/2010.11929, 2021.\\n\\n[18] Alaaeldin El-Nouby, Natalia Neverova, Ivan Laptev, and Herv\u00e9 Jegou. Training vision transformers for image retrieval. ArXiv, abs/2102.05644, 2021.\\n\\n[19] Matthew Gadd, D. Martini, and P. Newman. Look around you: Sequence-based radar place recognition with learned rotational invariance. 2020 IEEE/ION Position, Location and Navigation Symposium (PLANS), pages 270\u2013276, 2020.\\n\\n[20] Sourav Garg, Tobias Fischer, and Michael Milford. Where is your place, visual place recognition? In Zhi-Hua Zhou, editor, Proceedings of the Thirtieth International Joint Conference on Artificial Intelligence, IJCAI-21, pages 4416\u20134425. International Joint Conferences on Artificial Intelligence Organization, 8 2021. Survey Track.\\n\\n[21] Sourav Garg, Ben Harwood, G. Anand, and Michael Milford. Delta descriptors: Change-based place representation for robust visual localization. IEEE Robotics and Automation Letters, 5:5120\u20135127, 2020.\\n\\n[22] Sourav Garg and Michael Milford. Seqnet: Learning descriptors for sequence-based hierarchical place recognition. IEEE Robotics and Automation Letters, 6:4305\u20134312, 2021.\\n\\n[23] Yixiao Ge, Haibo Wang, Feng Zhu, Rui Zhao, and Hongsheng Li. Self-supervising fine-grained region similarities for large-scale image localization. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael Frahm, editors, Computer Vision \u2013 ECCV 2020, pages 369\u2013386, Cham, 2020. Springer International Publishing.\\n\\n[24] Albert Gordo, Jon Almaz\u00e1n, J\u00e9r\u00f4me Revaud, and Diane Larlus. Deep image retrieval: Learning global representations for image search. In ECCV, 2016.\\n\\n[25] A. Gordo, J. Almazan, J. Revaud, and D. Larlus. End-to-end learning of deep visual representations for image retrieval. IJCV, 2017.\\n\\n[26] Ali Hassani, Steven Walton, Nikhil Shah, Abulikemu Abuduweili, Jiachen Li, and Humphrey Shi. Escaping the Big Data Paradigm with Compact Transformers. ArXiv, abs/2104.05704, 2021.\\n\\n[27] Stephen Hausler, Sourav Garg, Ming Xu, Michael Milford, and Tobias Fischer. Patch-netvlad: Multi-scale fusion of\"}"}
{"id": "CVPR-2022-452", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"locally-global descriptors for place recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14141\u201314152, 2021.\\n\\n[28] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.\\n\\n[29] Ziyang Hong, Yvan Petillot, David Lane, Yishu Miao, and Sen Wang. Textplace: Visual place recognition and topological localization through reading scene texts. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.\\n\\n[30] H. J\u00e9gou and Andrew Zisserman. Triangulation embedding and democratic aggregation for image search. In 2014 IEEE Conference on Computer Vision and Pattern Recognition, pages 3310\u20133317, 2014.\\n\\n[31] H. J\u00e9gou, M. Douze, and C. Schmid. Hamming embedding and weak geometric consistency for large scale image search. In D. Forsyth, P. Torr, and A. Zisserman, editors, European Conference on Computer Vision, pages 304\u2013317, Berlin, Heidelberg, 2008. Springer Berlin Heidelberg.\\n\\n[32] Herv\u00e9 J\u00e9gou, Matthijs Douze, and Cordelia Schmid. Product quantization for nearest neighbor search. IEEE Trans. Pattern Anal. Mach. Intell., 33(1):117\u2013128, 2011.\\n\\n[33] Herv\u00e9 J\u00e9gou, Matthijs Douze, Jorge S\u00e1nchez, Patrick Perez, and Cordelia Schmid. Aggregating local image descriptors into compact codes. IEEE transactions on pattern analysis and machine intelligence, 34, 12 2011.\\n\\n[34] A. Khaliq, S. Ehsan, Z. Chen, M. Milford, and K. McDonald-Maier. A holistic visual place recognition approach using lightweight CNNs for significant viewpoint and appearance changes. IEEE Transactions on Robotics, 36(2):561\u2013569, 2020.\\n\\n[35] Hyo Jin Kim, Enrique Dunn, and Jan-Michael Frahm. Learned contextual feature reweighting for image geolocalization. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3251\u20133260, 2017.\\n\\n[36] Diederik Kingma and Jimmy Ba. Adam: A method for stochastic optimization. International Conference on Learning Representations, 12 2014.\\n\\n[37] Giorgos Kordopatis-Zilos, Panagiotis Galopoulos, S. Papadopoulos, and Y. Kompatsiaris. Leveraging efficientnet and contrastive learning for accurate global-scale location estimation. ACM International Conference on Multimedia Retrieval, 2021.\\n\\n[38] Yunpeng Li, Noah Snavely, Daniel Huttenlocher, and Pascal Fua. Worldwide Pose Estimation using 3D Point Clouds. In European Conference on Computer Vision, 2012.\\n\\n[39] Dongfang Liu, Yiming Cui, Liqi Yan, Christos Mousas, Baijian Yang, and Yingjie Chen. DenserNet: Weakly supervised visual localization using multi-scale feature aggregation. Proceedings of the AAAI Conference on Artificial Intelligence, pages 6101\u20136109, May 2021.\\n\\n[40] Liu Liu, Hongdong Li, and Yuchao Dai. Stochastic Attraction-Repulsion Embedding for Large Scale Image Localization. In IEEE International Conference on Computer Vision, 2019.\\n\\n[41] David G. Lowe. Distinctive image features from scale-invariant keypoints. Int. J. Comput. Vision, 60(2):91\u2013110, 2004.\\n\\n[42] Stephanie Lowry, Niko Sunderhauf, Paul Newman, John J. Leonard, David Cox, Peter Corke, and Michael J. Milford. Visual place recognition: A survey. IEEE Transactions on Robotics, 32(1):1\u201319, 2016.\\n\\n[43] Yu A. Malkov and D. A. Yashunin. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42:824\u2013836, 2020.\\n\\n[44] Carlo Masone and Barbara Caputo. A survey on deep visual place recognition. IEEE Access, 9:19516\u201319547, 2021.\\n\\n[45] Michael Milford and G. Wyeth. Mapping a suburb with a single camera using a biologically inspired slam system. IEEE Transactions on Robotics, 24:1038\u20131053, 2008.\\n\\n[46] Eva Mohedano, Kevin McGuinness, Xavier Giro i Nieto, and N. O'Connor. Saliency weighted convolutional features for instance search. 2018 International Conference on Content-Based Multimedia Indexing (CBMI), pages 1\u20136, 2018.\\n\\n[47] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand, and Bohyung Han. Large-scale image retrieval with attentive deep local features. In IEEE International Conference on Computer Vision, 2017.\\n\\n[48] A. Oliva and A. Torralba. Building the gist of a scene: the role of global image features in recognition. Progress in brain research, 155:23\u201336, 2006.\\n\\n[49] Eng-Jon Ong, Sameed Husain, and Miroslaw Bober. Siamese network of deep fisher-vector descriptors for image retrieval. CoRR, abs/1702.00338, 2017.\\n\\n[50] Guohao Peng, Yufeng Yue, Jun Zhang, Zhenyu Wu, Xiaoyu Tang, and Danwei Wang. Semantic reinforced attention learning for visual place recognition. In IEEE International Conference on Robotics and Automation, ICRA 2021, Xi'an, China, May 30 - June 5, 2021, pages 13415\u201313422. IEEE, 2021.\\n\\n[51] Guohao Peng, Jun Zhang, Heshan Li, and Danwei Wang. Attentional pyramid pooling of salient visual residuals for place recognition. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 885\u2013894, October 2021.\\n\\n[52] Florent Perronnin, Yan Liu, Jorge S\u00e1nchez, and Herv\u00e9 Poirier. Large-scale image retrieval with compressed fisher vectors. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3384\u20133391, 06 2010.\\n\\n[53] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Object retrieval with large vocabularies and fast spatial matching. In IEEE Conference on Computer Vision and Pattern Recognition. IEEE Computer Society, 2007.\\n\\n[54] James Philbin, Ondrej Chum, Michael Isard, Josef Sivic, and Andrew Zisserman. Lost in quantization: Improving particular object retrieval in large scale image databases. In IEEE Conference on Computer Vision and Pattern Recognition, June 2008.\"}"}
{"id": "CVPR-2022-452", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"On the benefit of heterogeneous data.\\n\\nPattern Recognition, 74:90\u2013109, 2018.\\n\\nNo\u00e9 Pion, Martin Humenberger, Gabriela Csurka, Yohann Cabon, and Torsten Sattler. Benchmarking image retrieval for visual localization. In 2020 International Conference on 3D Vision (3DV), pages 483\u2013494, 2020.\\n\\nFilip Radenovi\u0107, Giorgos Tolias, and O. Chum. CNN Image Retrieval Learns from BoW: Unsupervised Fine-Tuning with Hard Examples. In ECCV, 2016.\\n\\nF. Radenovi\u0107, G. Tolias, and O. Chum. Fine-tuning CNN Image Retrieval with No Human Annotation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2018.\\n\\nA. Razavian, Hossein Azizpour, J. Sullivan, and S. Carlsson. Cnn features off-the-shelf: An astounding baseline for recognition. 2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 512\u2013519, 2014.\\n\\nA. Razavian, J. Sullivan, A. Maki, and S. Carlsson. Visual Instance Retrieval with Deep Convolutional Networks. CoRR, abs/1412.6574, 2015.\\n\\nJer\u00f4me Revaud, Jon Almaz\u00e1n, R. S. Rezende, and C\u00e9sar Roberto de Souza. Learning with average precision: Training image retrieval with a listwise loss. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 5106\u20135115, 2019.\\n\\nTorsten Sattler, Will Maddern, Carl Toft, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, Fredrik Kahl, and Tomas Pajdla. Benchmarking 6DOF outdoor visual localization in changing conditions. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8601\u20138610, 2018.\\n\\nGrant Schindler, Matthew Brown, and Richard Szeliski. City-Scale Location Recognition. In IEEE Conference on Computer Vision and Pattern Recognition, 2007.\\n\\nZachary Seymour, Karan Sikka, Han-Pang Chiu, S. Samaratunga, and Rakesh Kumar. Semantically-aware attentional neural embeddings for image-based visual localization. ArXiv, abs/1812.03402, 2018.\\n\\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In International Conference on Learning Representations, 2015.\\n\\nJosef Sivic and Andrew Zisserman. Video google: A text retrieval approach to object matching in videos. In ICCV, pages 1470\u20131477. IEEE Computer Society, 2003.\\n\\nElena Stumm, Christopher Mei, and Simon Lacroix. Probabilistic place recognition with covisibility maps. In IEEE/RSJ International Conference on Intelligent Robots and Systems, pages 4158\u20134163. IEEE, 2013.\\n\\nGiorgos Tolias, R. Sicre, and H. J\u00e9gou. Particular object retrieval with integral max-pooling of CNN activations. CoRR, abs/1511.05879, 2016.\\n\\nA. Torii, R. Arandjelovi\u0107, J. Sivic, M. Okutomi, and T. Pajdla. 24/7 place recognition by view synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(2):257\u2013271, 2018.\\n\\nA. Torii, J. Sivic, M. Okutomi, and T. Pajdla. Visual place recognition with repetitive structures. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(11):2346\u20132359, 2015.\\n\\nA. Torii, Hajime Taira, Josef Sivic, M. Pollefeys, M. Okutomi, T. Pajdla, and Torsten Sattler. Are large-scale 3d models really necessary for accurate visual localization? IEEE Transactions on Pattern Analysis and Machine Intelligence, 43:814\u2013829, 2021.\\n\\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In Marina Meila and Tong Zhang, editors, Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 10347\u201310357. PMLR, July 2021.\\n\\nO. Vysotska and C. Stachniss. Effective visual place recognition using multi-sequence maps. IEEE Robotics and Automation Letters, 4:1730\u20131736, 2019.\\n\\nZ. Wang, J. Li, S. Khademi, and J. van Gemert. Attention-aware age-agnostic visual place recognition. In The IEEE International Conference on Computer Vision (ICCV) Workshops, Oct 2019.\\n\\nFrederik Warburg, Soren Hauberg, Manuel Lopez-Antequera, Pau Gargallo, Yubin Kuang, and Javier Civera. Mapillary street-level sequences: A dataset for lifelong place recognition. In IEEE Conference on Computer Vision and Pattern Recognition, June 2020.\\n\\nIsaac Ronald Ward, M. Jalwana, and M. Bennamoun. Improving image-based localization with deep learning: The impact of the loss function. In PSIVT Workshops, 2019.\\n\\nTobias Weyand, A. Ara\u00fajo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2 \u2013 a large-scale benchmark for instance-level recognition and retrieval. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2572\u20132581, 2020.\\n\\nZhe Xin, Xiaoguang Cui, Jixiang Zhang, Yiping Yang, and Yanqing Wang. Visual place recognition with cnns: From global to partial. In 2017 Seventh International Conference on Image Processing Theory, Tools and Applications (IPTA), pages 1\u20136, 2017.\\n\\nMubariz Zaffar, Shoaib Ehsan, Michael Milford, and K. Mcdonald-Maier. Cohog: A light-weight, compute-efficient, and training-free visual place recognition technique for changing environments. IEEE Robotics and Automation Letters, 5:1835\u20131842, 2020.\\n\\nMubariz Zaffar, Sourav Garg, Michael Milford, Julian Kooij, David Flynn, Klaus McDonald-Maier, and Shoaib Ehsan. VPR-Bench: An open-source visual place recognition evaluation framework with quantifiable viewpoint and appearance change. International Journal of Computer Vision, 129(7):2136\u20132174, 2021.\\n\\nAmir R. Zamir, Asaad Hakeem, Luc Van Gool, Mubarak Shah, and Richard Szeliski, editors. Large-Scale Visual Geo-localization. Advances in Computer Vision and Pattern Recognition. Springer, 2016.\"}"}
{"id": "CVPR-2022-452", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Xiwu Zhang, Lei Wang, and Yan Su. Visual place recognition: A survey from deep learning perspective. Pattern Recognition, 113, 2021.\\n\\nY. Zhu, J. Wang, L. Xie, and L. Zheng. Attention-based pyramid aggregation network for visual place recognition. In Proc. of the 26th ACM Int. Conf. on Multimedia, MM '18, page 99\u2013107, New York, NY, USA, 2018. Association for Computing Machinery.\"}"}
{"id": "CVPR-2022-452", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 3. Results and computational requirements with different convolutional backbones\\n\\n| Backbone | Aggregation Method | Features Dim | FLOPs (GF) | Training on Pitts30k | Training on MSLSR@1Pitts30k | R@1MSLSR@1Tokyo 24/7 | R@1R-SFR@1Eynsham | R@1St Lucia | R@1Pitts30k | R@1MSLSR@1Tokyo 24/7 | R@1R-SFR@1Eynsham | R@1St Lucia | Average |\\n|----------|-------------------|--------------|-----------|---------------------|----------------------------|-----------------------|-------------------|------------|------------|---------------------|-------------------|------------|---------|\\n| ResNet-50 | GeM               | 1024         | 40.61     | 82.0                | 38.0                       | 41.5                  | 45.4              | 66.3       | 59.0       | 77.4                | 72.0              | 55.4       | 45.7    |\\n|           | NetVLAD           | 1024         | 86.29     | 86.5                | 51.8                       | 72.2                  | 67.5              | 74.0       | 63.6       | 80.8                | 77.7              | 62.8       | 56.1    |\\n\\n### Table 4. Aggregation methods:\\n\\nWe report results with different aggregation methods downscaled or upscaled to equivalent dimensionality. ResNet-50 GeM + PCA and NetVLAD end up being significantly outclassed on Tokyo and R-SF by GeM, making it a more compelling choice. Furthermore, the dimensionality reduction via PCA yields a significant drop in performance for NetVLAD and CRN, while adding a fully connected layer on top of GeM gives best results when trained on a large scale dataset, which is the type of scenario for which GeM was proposed [58]. Note that while the CRN aggregator yields the most robust results, it has the drawbacks of requiring a two-stage training process that almost doubles the training time and three times more hyperparameters w.r.t. NetVLAD. In addition, depending on the initialization of its modulation layer, training does not always converge.\\n\\n### 4.3. Visual Transformers\\n\\nIn this section we investigate how Visual Transformers compare to more traditional CNN-based methods in VG. For this analysis we use two popular Transformer architectures, the Vision Transformer (ViT) [17], which processes the images by splitting them into sequences of flattened 2D patches, and the Compact Convolutional Transformer (CCT) [26], which incorporates convolutional layers to insert the inductive bias of CNNs. Following [18], we use as a global descriptor the CLS token, which is the output state of the prepended learnable embedding to the sequence of patches [17]. Moreover, we test the use of CCT in conjunction with traditional aggregation methods, such as GeM [58] and NetVLAD [2], and with SeqPool, which was specifically introduced in [26] for Transformers.\\n\\n| Backbone | Aggregation Method | Features Dim | FLOPs (GF) | Training on MSLSR@1Pitts30k | Training on MSLSR@1Tokyo 24/7 | R@1R-SFR@1Eynsham | R@1St Lucia | Average |\\n|----------|-------------------|--------------|-----------|---------------------|----------------------------|-------------------|------------|---------|\\n| ResNet-18 | GeM               | 256          | 17.29     | 71.6                | 65.3                       | 42.8              | 30.5       | 80.3    |\\n|           | NetVLAD           | 16384        | 17.27     | 81.6                | 75.8                       | 62.3              | 55.1       | 87.1    |\\n| ResNet-50 | GeM               | 1024         | 40.51     | 77.4                | 72.0                       | 55.4              | 45.7       | 83.9    |\\n|           | NetVLAD           | 65536        | 86.0      | 80.9                | 76.9                       | 62.3              | 51.5       | 87.2    |\\n| ViT CLS  |                  | 768          | 82.31     | 82.9                | 73.5                       | 59.9              | 65.0       | 84.5    |\\n| CCT CLS  |                  | 384          | 22.34     | 79.6                | 71.1                       | 52.0              | 49.9       | 85.6    |\\n| CCT SeqPool |                 | 384         | 26.19     | 81.4                | 71.0                       | 59.1              | 60.5       | 86.1    |\\n| CCT GeM  |                  | 384          | 22.36     | 78.7                | 72.0                       | 48.8              | 48.6       | 83.9    |\\n| ResNet-18 | NetVLAD           | 16384        | 17.27     | 81.6                | 75.8                       | 62.3              | 55.1       | 87.1    |\\n| ResNet-50 | NetVLAD           | 65536        | 40.51     | 80.9                | 76.9                       | 62.3              | 51.5       | 87.2    |\\n| ViT CLS  |                  | 768          | 82.31     | 82.9                | 73.5                       | 59.9              | 65.0       | 84.5    |\\n\\n### Table 5. Transformers Comparison of traditional CNN architectures with novel Transformers-based approaches.\\n\\nDiscussion.\\n\\nTable 5 compares traditional CNN-based methods with novel Visual Transformer based approaches, never used before specifically for VG. The main findings of this set of experiments is that they represent a viable alternative to CNN-based backbones even without an additional aggregation steps using directly the compact and robust representation provided by the CLS token. Further improvements can be obtained when combined with aggregators such as GeM, SeqPool, NetVLAD, as shown in the table. Overall, the results show that these architectures possess better generalization capabilities than their CNN counterparts, and ViT proves to be competitive even with the much bigger NetVLAD descriptors, albeit with higher computational requirements. As for CCT, despite being incredibly lightweight, with a cost comparable to a ResNet-18, consistently outperforms the ResNet-18 and, in many cases, also the ResNet-50, which has roughly double the computational cost. Concluding, it seems that the SeqPool aggregator enhances the robustness of the CCT descriptors, providing better generalization and that NetVLAD coupled with CCT outperforms CNN-based methods. We observe similar behaviors when trained on Pitts30k (see supp. material).\"}"}
{"id": "CVPR-2022-452", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6. Negative mining methods.\\n\\n\\\"Space & Time Complexity\\\" refers to the complexity of building the cache, which normally is done after iterating over 1000 triplets [2, 75]. \\n\\n#db and #q are the numbers of database and query images, kdb and kq are chosen constants (usually set to 1000), and #pos is the number of positives for the considered queries, which depends on the queries and database density.\\n\\nDiscussion. As expected, both full and partial database mining outperform the random negative sampling. The latter, in spite of its low cost yields in average 5% lower results on Pitts30k, due to the low variability of the dataset. Indeed, on the larger MSLS results drop of 10% or more. On the other hand, full database mining does not provide always best performance and on average its gain over partial mining is around 1%. Furthermore, on large scale datasets such as MSLS full mining is not feasible in a reasonable time. These results clearly show that partial mining is, in general, a great compromise between cost and accuracy.\\n\\n4.5. Data Augmentation\\n\\nHere we investigate if and which data augmentation are beneficial for VG methods, and if the improvements are domain-specific or can generalize to diverse datasets. We apply data augmentation to the query, with the sole exception of random horizontal flipping, for which we either flip or not flip the whole triplet. We run experiments with many popular augmentation techniques, training a ResNet-18 with NetVLAD on Pitts30k.\\n\\nDiscussion. Plots of the results are in Fig. 2 (shown in higher resolution in the supp. material). Depending on the test dataset, we observe different impact of these augmentations. On one hand, on Pitts30k augmentation only worsens results, probably due to dataset homogeneity between train and test. On the other hand, we see that some techniques can improve robustness on unseen datasets, in particular color jittering methods that change brightness, contrast and saturation. As an example, setting contrast up to 2 can improve recall@1 by more than 3% on MSLS, 5% on Tokyo 24/7, 5% on St Lucia, with a less than 1% drop on Pitts30k and Eynsham. Although most augmentations fail to produce consistent improvements, two notable exceptions are random horizontal flipping (with probability 50%) and random resized cropping, where crops are as small as 50% of the image size (and then resized to full resolution).\\n\\n4.6. Resize\\n\\nWhile common VG datasets have images of resolutions around 480x640 pixels, it is interesting to investigate how resizing them can affect the results. To this end, we perform experiments by training and testing models on images of lower resolution, by reducing both sides of the images from 80% to 20% of their original size, both at train and test time on Pitts30k. We conduct this analysis with CNNs\"}"}
{"id": "CVPR-2022-452", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Changing the images' resolution. On the x-axis is the train and test resolution (N%), on the y-axis is the recall@1. Regarding the curves, red refers to ResNet-50 + NetVLAD, orange to ResNet18 + NetVLAD, green to ResNet-50 + GeM, and blue to ResNet-18 + GeM. In many cases, full resolution is not the optimal choice. NetVLAD's initial clusters computation breaks with low resolutions, followed by GeM or NetVLAD, since such architectures do not require a fixed input image resolution.\\n\\nDiscussion. Interestingly, it can be seen in Figure 3 that using the highest available resolution is in most cases superfluous, and often even detrimental. On average, NetVLAD's descriptors seem to better handle higher resolutions than their GeM counterparts. Lower resolutions, as low as 40%, show improved results especially when there is a wide domain gap between train and test sets: this is exemplified by the results on the St Lucia dataset, which is very different from Pitts30k (the former has only forward views) and shows best R@1 performance when using 40% of the original resolution. This behaviour can be explained by the disappearance of domain specific low-level patterns (e.g., texture and foliage) when the size of the image is reduced. In general 60% is a good compromise, suggesting that for geo-localization, which is strongly related to appearance-based retrieval, fine details are not too important.\\n\\nFinally, note that 40% resolution means reducing it to 192x256, with FLOPs going down to (40%)$^2 = 16\\\\%$ w.r.t. full resolution images. Storage needs also decrease in the same fashion as FLOPs, and although images are not directly needed in a retrieval system (only descriptors and coordinates are used for kNN), they can be used useful for post-processing, e.g., spatial verification, or to generate a visual response for users.\\n\\n4.7. Nearest Neighbor Search and Inference Time\\n\\nIn practical applications, one of the most relevant factors for a VG system is inference time ($t_i$). Once the application is deployed and has to serve the user's needs, the perceived delay depends only on $t_i$. Inference time can be divided into: i) extraction time ($t_e$), defined as the elapsed time to extract the features of an image, which solely depends on model and resolution; ii) matching time ($t_m$), i.e., duration of the kNN to find the best matches in the database, which depends on the parameter $k$ (i.e., number of candidates), the size of the database, the dimension of the descriptors, and the type of searching algorithm.\\n\\nIn Fig. 4a, we report a plot on how matching time linearly depends on the sizes of the database and descriptors. Figure 4b shows how the use of efficient nearest neighbor search algorithm impacts computation and memory footprint. Besides exhaustive kNN, we investigate the use of (a) (b)\\n\\nFigure 4. (a) Matching time for one query. The plot shows, with exact search, linear dependency on database size and features dimensionality. The red line marks the extraction time of an image for ResNet-101 + GeM; above, the bottleneck is matching time, below it is extraction time. As a rule of thumb, kNN is the bottleneck if database size times the features dimension exceeds 200M.\\n\\n(b) Analysis of the Recall-Speed-Memory trade-off using optimized indexing techniques for neighbor search. Dots refer to a ResNet-50 + GeM (feat. dim. 1024) trained on Pitts30k. On the x axis is matching time in seconds for all queries in the dataset, on the y axis recall@1. The numbers next to the dots represent the RAM requirements in MB.\\n\\ninverted file indexes (IVF) [66], product quantization with and without IVF (PQ and IVFPQ) [32], inverted multi index (MultiIndex) [6] and hierarchical navigable small world graphs (HNSW) [43]. In Fig. 4b we report results computed with a ResNet-50 + GeM descriptors on R-SF. See more experiments and thorough discussions in the supp. material.\\n\\nDiscussion Figure 4a shows that as the database grows, inference time is dominated by matching time whereas the extraction time is generally fixed at around 10 milliseconds (see Tab. 3 and supp. material). On the other hand, Fig. 4b shows that the choice of neighbor search algorithm can bring huge benefits on time and memory footprint, with little to no loss in recalls. Among the most interesting results, IVFPQ reduces both matching time and memory footprint by 98.5%, with a drop in accuracy from 45.4% to 41.4%. Note that memory footprint is an important factor in image retrieval, since for fast computation all vectors should be kept in RAM, making large scale VG application expensive in terms of memory. For example, R-SF dataset's descriptors, with a ResNet-50 + NetVLAD, require roughly $1.05 \\\\cdot 65536 \\\\cdot 4B = 256$ GB of memory, thus making a RAM-efficient search technique (e.g., product quantization) very useful. When memory is not a critical constraint, using\"}"}
{"id": "CVPR-2022-452", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7. Comparison between recent SOTA methods, and a simple ResNet-18+NetVLAD where we use all the insight gained from the benchmark to find its optimal configuration: training with data augmentation, resize 80%, and majority voting post-processing for Tokyo 24/7 (since queries have different resolutions).\\n\\nInference time and kNN search. We propose an extensive study for VG, unique in its kind, comparing advanced kNN search algorithms and compact representations. This study has shown that the choice of a good neighbor search algorithm can have a huge impact on time and memory footprint, with little impact on the performance. Furthermore, we observe that advanced kNN methods might nullify the gap in terms of both memory footprint and matching time between larger and smaller descriptors.\\n\\nFinal remarks. All the above insights are important to design and optimize VG architectures depending on one\u2019s use case and requirements. For instance, consider again the example from Tab. 1. In light of the lessons learned, we can carefully optimize the same simple architecture to get results that are comparable with much more complex (yet not optimized) methods (see Tab. 7).\\n\\nLimitations. Despite its modularity and versatility, our framework has also some limitations, e.g., it is focused on VG methods in outdoor urban environments, it only addresses the task of Visual Geo-localization from a single image, it does not try to analyze the viewpoint and luminosity invariance of the methods (as done in [80]). Furthermore, some recent SOTA works [23, 51] are not implemented yet, and some newer losses not yet compared [40]. However, we plan to continue supporting the software and website, expanding them to evaluate more techniques and use-cases and investigate additional elements in a VG pipeline.\"}"}
{"id": "CVPR-2022-452", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nIn this paper, we propose a new open-source benchmarking framework for Visual Geo-localization (VG) that allows to build, train, and test a wide range of commonly used architectures, with the flexibility to change individual components of a geo-localization pipeline. The purpose of this framework is twofold: i) gaining insights into how different components and design choices in a VG pipeline impact the final results, both in terms of performance (recall@N metric) and system requirements (such as execution time and memory consumption); ii) establish a systematic evaluation protocol for comparing different methods.\\n\\nUsing the proposed framework, we perform a large suite of experiments which provide criteria for choosing backbone, aggregation and negative mining depending on the use-case and requirements. We also assess the impact of engineering techniques like pre/post-processing, data augmentation and image resizing, showing that better performance can be obtained through somewhat simple procedures: for example, downscaling the images' resolution to 80% can lead to similar results with a 36% savings in extraction time and dataset storage requirement. Code and trained models are available at https://deep-vg-bench.herokuapp.com/.\\n\\n1. Introduction\\n\\nThe task of coarsely estimating the place where a photo was taken based on a set of previously visited locations is called Visual (Image) Geo-localization (VG) [35, 40, 81] or Visual Place Recognition (VPR) [20,42] and it is addressed using image matching and retrieval methods on a database of images of known locations. We are witnessing a rapid growth of this field of research, as demonstrated by the increasing number of publications [2,10,14,21\u201323,27,29,34,35,40,42,44,55,58,69,70,73\u201376,78,82], but this expansion is accompanied by two major limitations:\\n\\ni) A focus on single metric optimization, as it is common practice to compare results solely based on the recall on chosen datasets and ignoring other factors such as execution time, hardware requirements, and scalability. All these aspects are important constraints in the design of a real-world VG system. For instance, one might gladly accept a 5% drop in accuracy if this leads to a 90% decrease of descriptors size as the resulting reduction in memory requirements enables a better scalability. Similarly, computational time and descriptor dimensionality are crucial constraints in real-time applications, given a target hardware platform.\\n\\nii) A lack of a standardized framework to train and test VG models. It is common practice to perform direct comparisons among off-the-shelf methods that use different setups (e.g., data augmentation, initialization, training dataset, etc.) [35, 64, 80], which can hide the improvement (or lack thereof) obtained by algorithmic changes and it does not allow to pinpoint the impact of each individual component. Table 1 shows how some simple engineering choices can have big effects on the recall metric.\\n\\nAlthough previous benchmarks for VPR [80] and the related task of Visual Localization [56, 62] offer interesting insights, they do not address the aforementioned issues. For these reasons, we propose a new open-source benchmark that provides researchers with an all-inclusive tool to build, train, and test a wide range of commonly used VG architectures, offering the flexibility to change each component of a geo-localization pipeline. This allows to rigorously examine how each element of the system influences the final results.\"}"}
{"id": "CVPR-2022-452", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"results while providing information computed on-the-fly regarding the number of parameters, FLOPs, descriptors dimensionality, etc.\\n\\nUsing our framework, we run numerous experiments aiming to understand which components are the most suitable for a real-world application, and derive good practices depending on the target dataset and one\u2019s hardware availability. For example, we find that ResNet-50 [28] provides a good trade-off between accuracy, FLOPs and model size, and that Visual Transformers can successfully replace the CNN backbones and achieve better geo-localization performances when trained on larger datasets. Furthermore, we observed that partial negative mining and reduced resolution yield important decrease in computations without significantly compromising the performance, or even yielding gains in some cases.\\n\\nThe benchmark\u2019s software and models are hosted at https://deep-vg-bench.herokuapp.com/.\\n\\n2. Related Work\\n\\nRepresentation learning for visual retrieval and localization.\\n\\nVisual Geo-localization (VG), Visual Localization (VL), and Landmark Retrieval (LR) are three well-known Computer Vision tasks that try to establish a mapping between an image and a spatial location, albeit with some nuances. In VG the goal is to find the geographical location of a given query image and the predicted coordinates are considered correct if they are roughly close to ground truth position [2, 10, 23, 35, 39, 40, 74, 75]. VL focuses on precisely estimating the 6 DoF camera pose of a query image within a known scene. VG methods can be used as a part of a VL pipeline, combined with other processing stages that reduce the differences when used in a VL task. Therefore the evaluation papers on VL [56,62,71] might not be indicative of VG performance, justifying a separate benchmark on the latter. LR is a particular case of Image Retrieval (IR) in which queries contain some landmark, and the goal is to identify all database instances depicting the same landmark, regardless of their visually overlap with the query photo. Since VG is usually addressed as a retrieval problem where the query position is estimated using the GPS tags of the top retrieved image, several methods originally proposed for LR (or IR in general) have carried over to VG. LR datasets, both on a city-scale (Oxford and Paris Buildings [53, 54]) and on a global scale (Google Landmarks [47,77]), consists of a discrete set of landmarks, whereas VG datasets usually cover a continuous geographical area.\\n\\nIR [3,16,67] is traditionally performed via nearest neighbors search using fixed-size image representations [15, 30, 31,33,52,63,69] obtained from the aggregation of highly informative local [1, 8, 41] or global [48, 79] features. Convolutional neural networks (CNNs) have become the de-facto standard to extract the features for IR, using various methods to concatenate them [7, 59] or pool them [4, 5, 68] to create image descriptors. Among the deep learning representation methods, one that has proven very effective for VG is NetVLAD [2], a differentiable implementation of VLAD [33] trained end-to-end with the CNN backbone directly for place recognition. The layer has since been used in numerous works [10,19,22,27,40,74,75]. One downside of NetVLAD is that it outputs high-dimensional descriptors, leading to steep memory requirements for VG systems. This problem has inspired research on more compact descriptors, either using dimensionality reduction techniques [7, 11, 24, 49, 57, 83] or replacing NetVLAD with lighter pooling layers, such as GeM [58] and R-MAC [25]. It has also been shown that attention modules can be used to focus feature extraction and aggregation towards the most salient parts of the scene for the geo-localization task [11, 35, 39, 46]. The Contextual Reweighting Network (CRN) [35] is a variation of NetVLAD that adds a contextual modulation to produce a weighting mask based on semi-global context. Visual Transformers based on self-attention such as ViT [17] and DeiT [72] have also been used in IR [12,18], but not yet in VG. All these architectures used in VG to learn image representations are trained with metric embedding objectives commonly used in learning-to-rank problems, such as the contrastive loss [49, 57, 58], the triplet loss [2, 25, 35] and the SARE loss [40].\\n\\nOur benchmark analyzes how the combination of popular backbone networks, pooling strategies, data augmentation, and engineering choices impacts geo-localization performance and other aspects, such as memory and computational requirements.\"}"}
{"id": "CVPR-2022-452", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Throughout this work, we rigorously and fairly analyze each component of a visual geo-localization system (the light blue blocks) comparing a variety of different implementations, both for train and test time.\\n\\n3. Methodology\\n\\nThis section describes the VG pipeline used in our benchmark (cf. Fig. 1) and our experimental setup.\\n\\n3.1. Visual Geo-localization System\\n\\nThe VG task is commonly tackled using an image retrieval pipeline: given a new photo (query) to be geo-localized, its location is estimated by matching it to a database of geo-tagged images. A VG system is thus an algorithm that first extracts descriptors for the database images (offline) and for the query photo (online), then it applies a nearest neighbors search in the descriptor space. The orange blocks in Fig. 1 show that a VG system is built through several design choices, including network architectures, negative mining methods, and engineering aspects such as image sizes and data augmentation. All of these choices impact the behavior of the system, both in terms of performance and required resources. We propose a new benchmark to systematically investigate the impact of the components of VG systems, using the modular architecture shown in Fig. 1 as a canvas to reproduce most VG methods based on CNN backbones and to develop new models based on Visual Transformers.\\n\\nThis abstract model contains several components that can be modified, both during training and test time: the backbone (Sec. 4.1); feature aggregation (Sec. 4.2); mining training examples (Sec. 4.4); image resizing (Sec. 4.6); data augmentation (Sec. 4.5). We conduct a series of tests focused individually on each of these elements, to systematically show each component\u2019s influence. Due to limited space, we only summarize here the results of some experiments, while detailed results and additional experiments on pre/post-processing methods and predictions refinement, effect of pre-training and many other aspects are provided in the supp. material.\\n\\nThe code of the benchmark follows the modular structure shown in Fig. 1, where each component can be modified. We further provide scripts to download and format a number of datasets, and to train and test the models making easy to perform a large number of experiments while ensuring consistency and reproducibility of results. Our codebase allows to easily reproduce the architectures used in a wide range of works [2, 25, 35, 40, 58, 61, 68, 75] and commonly used training protocols [2, 40, 75]. More details on the software are provided in the supp. material.\\n\\n3.2. Datasets\\n\\nWe use six highly heterogeneous datasets (see Tab. 2 and maps in the supp. material), which together cover a variety of real-world scenarios: different scales, degree of inter-image variability, different camera types. For training, we use Pitts30k [2] and Mapillary Street-Level Sequences (MSLS) [75] datasets, as they provide a small and large amount of images, respectively. While Pitts30k is very homogeneous, i.e. all images share the same resolution, weather conditions and camera, MSLS represents a wide range of conditions from very diverse cities. Regarding MSLS, given the lack of labels for the test set, we follow [27] and report validation recalls computed on the validation set. To assess inter-dataset robustness, we also test all models on four other datasets: Tokyo 24/7 [69], Revised San Francisco (R-SF) [13, 38, 71], Eynsham [16] and St Lucia [45]. Further details on these datasets, such as their geographical coverage, are included in the supp. material.\\n\\n3.3. Benchmark Protocol\\n\\nIn all experiments, unless otherwise specified, we use the metric of recall@N (R@N) measuring the percentage of queries for which one of the top-N retrieved images was taken within a certain distance of the query location. We mostly focus on R@1 and, following common practice in...\"}"}
{"id": "CVPR-2022-452", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Train/Validation Datasets\\n\\n| Dataset   | Databasetype | Databaseimg. size | Queriestype |\\n|-----------|--------------|-------------------|-------------|\\n| Pitts30k  | 20K / 15K    | 10K / 6.8K        | panorama    |\\n|           |              |                   | panorama    |\\n|           |              |                   | MSLS        |\\n|           |              |                   | front-view  |\\n| Tokyo 24/7 | 0 / 0        | 75K / 315         | phone       |\\n|           |              |                   |             |\\n| R-SF      | 0 / 0        | 1.05M / 598       | phone       |\\n|           |              |                   |             |\\n| Eynsham   | 0 / 0        | 24K / 24K         | panorama    |\\n|           |              |                   |             |\\n| St Lucia  | 0 / 0        | 1.5K / 1.5K       | front-view  |\\n\\n### Table 2. Summary of the Datasets:\\n\\n- \\\"panorama\\\" means images are cropped from a 360\u00b0 panorama (including undistortion);\\n- \\\"front-view\\\" means that only one (forward facing) view is available;\\n- \\\"phone\\\" means photos were collected with a smartphone.\\n\\nPanorama and front-view images were taken with car-rooftop cameras.\\n\\n### 4. Results\\n\\nThroughout this section, we explore how each block from Fig. 1 influences the results. Specifically, we first investigate the use of different architectures, with a focus on backbones (Sec. 4.1), aggregation methods (Sec. 4.2) and Transformers-based networks (Sec. 4.3). We then move to train-time components (i.e., negative mining Sec. 4.4 and data augmentation Sec. 4.5), to understanding how the resolution of the images influences a VG system (Sec. 4.6), and finally we explore the use of efficient nearest neighbor search algorithms (Sec. 4.7). Given the limited amount of space in the manuscript, a thorough extension over each one of these sections can be found in the supp. material, as well as further experiments on various metrics and more.\\n\\n#### 4.1. CNN Backbones\\n\\nTasked with extracting highly informative feature maps from images, the CNN backbone represents a fundamental component of any VG system. To understand its impact, we experiment with four CNN backbones (VGG16 [65], ResNet-18, ResNet-50 and ResNet-101 [28]), combined with two popular aggregation methods, GeM [58] and NetVLAD [2]. Note that this seemingly limited number of backbones covers several state-of-the-art architectures in VG and image retrieval [2, 25, 35, 40, 50, 51, 58, 61, 68, 75].\\n\\nFor all ResNets, we use the feature maps extracted from the conv4x1 layer. For VGG16, we use all the convolutional layers, excluding the last pooling before the classifier part.\\n\\nTable 3 shows the results of our experiments.\\n\\n### Discussion.\\n\\nWe can see that deeper ResNets, such as ResNet-50 and ResNet-101, achieve better results w.r.t. their shallower counterparts. In particular, ResNet-50 shows recalls on par with ResNet-101, but with the advantage of less than half the FLOPs and model size, making the former a more practically relevant option than the latter. ResNet-18 performs worse, but allows for much faster and lighter computation, making it the most efficient, lightweight backbone. Moreover, results considerably depend on the training data: as an example, training the same network on Pitts30k or MSLS yields a 30% gap testing the model on St. Lucia, as well as a noticeable difference on other datasets too. This effect demonstrates that comparing models trained on different datasets, as done in [80], can be misleading.\\n\\n#### 4.2. Aggregation and Descriptor Dimensionality\\n\\nAggregation methods are layers tasked with processing the output features of the backbone. Over the years, a number of such methods have been proposed, from shallow pooling layers [5, 60] to more complex modules [2, 35].\\n\\nOur framework allows to compute results with a number of them, namely SPOC [5], MAC [60], R-MAC [68], RRM [37], GeM [58], NetVLAD [2] and CRN [35]. While a complete list of results with all aggregation methods is shown in the supp. material, in Tab. 4 we report the performance of the best performing aggregators: GeM, NetVLAD and CRN. Given the difference in size of the outputted descriptors, we apply PCA or a fully connected (FC) layer to even their dimensionality.\\n\\n### Discussion.\\n\\nThe results in Tab. 4 show that performance strongly depends on the training set. When training on the small Pitts30k, the best results are obtained globally with CRN, even when reducing its dimension to be the same as GeM. However, when training on the much larger MSLS, the advantage of CRN is reduced, and both CRN and NetVLAD perform similarly. This demonstrates the importance of using the right aggregation method for the specific task and dataset at hand.\"}"}
