{"id": "CVPR-2022-181", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Iro Armeni, Sasha Sax, Amir R. Zamir, and Silvio Savarese. Joint 2D-3D-semantic data for indoor scene understanding. arXiv preprint arXiv:1702.01105, 2017.\\n\\n[2] Wei-Lun Chang, Hui-Po Wang, Wen-Hsiao Peng, and Wei-Chen Chiu. All about structure: Adapting structural information across domains for boosting semantic segmentation. In CVPR, 2019.\\n\\n[3] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018.\\n\\n[4] Minghao Chen, Hongyang Xue, and Deng Cai. Domain adaptation for semantic segmentation with maximum squares loss. In ICCV, 2019.\\n\\n[5] Shoufa Chen, Enze Xie, Chongjian Ge, Ding Liang, and Ping Luo. CycleMLP: A MLP-like architecture for dense prediction. In ICLR, 2022.\\n\\n[6] Ting Chen, Simon Kornblith, Kevin Swersky, Mohammad Norouzi, and Geoffrey Hinton. Big self-supervised models are strong semi-supervised learners. In NeurIPS, 2020.\\n\\n[7] Zhiyang Chen, Yousong Zhu, Chaoyang Zhao, Guosheng Hu, Wei Zeng, Jinqiao Wang, and Ming Tang. DPT: Deformable patch-based transformer for visual recognition. In MM, 2021.\\n\\n[8] Yiting Cheng, Fangyun Wei, Jianmin Bao, Dong Chen, Fang Wen, and Wenqiang Zhang. Dual path learning for domain adaptation of semantic segmentation. In ICCV, 2021.\\n\\n[9] Sungha Choi, Joanne T. Kim, and Jaegul Choo. Cars can't fly up in the sky: Improving urban-scene segmentation via height-driven attention networks. In CVPR, 2020.\\n\\n[10] Taco Cohen, Maurice Weiler, Berkay Kicanaoglu, and Max Welling. Gauge equivariant convolutional networks and the icosahedral CNN. In ICML, 2019.\\n\\n[11] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016.\\n\\n[12] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In ICCV, 2017.\\n\\n[13] Gr\u00e9goire Payen de La Garanderie, Amir Atapour Abarghouei, and Toby P. Breckon. Eliminating the blind spot: Adapting 3D object detection and monocular depth estimation to 360\u00b0 panoramic imagery. In ECCV, 2018.\\n\\n[14] Liuyuan Deng, Ming Yang, Hao Li, Tianyi Li, Bing Hu, and Chunxiang Wang. Restricted deformable convolution-based road scene semantic segmentation using surround view cameras. T-ITS, 2020.\\n\\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, ThomasUnterthiner, Mostafa Dehghani, Matthias Minderer, GeorgHeigold, Sylvain Gelly, JakobUszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\\n\\n[16] Marc Eder, Mykhailo Shvets, John Lim, and Jan-Michael Frahm. Tangent images for mitigating spherical distortion. In CVPR, 2020.\\n\\n[17] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In CVPR, 2019.\\n\\n[18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In NeurIPS, 2014.\\n\\n[19] Qiqi Gu, Qianyu Zhou, Minghao Xu, Zhengyang Feng, Guangliang Cheng, Xuequan Lu, Jianping Shi, and Lizhuang Ma. PIT: Position-invariant transform for cross-FoV domain adaptation. In ICCV, 2021.\\n\\n[20] Julia Guerrero-Viu, Clara Fernandez-Labrador, C\u00e9dric Dehounceaux, and Jose J. Guerrero. What's in my room? Object recognition on indoor panoramic images. In ICRA, 2020.\\n\\n[21] Xiaoqing Guo, Chen Yang, Baopu Li, and Yixuan Yuan. MetaCorrection: Domain-aware meta loss correction for unsupervised domain adaptation in semantic segmentation. In CVPR, 2021.\\n\\n[22] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. CyCADA: Cycle-consistent adversarial domain adaptation. In ICML, 2018.\\n\\n[23] Qibin Hou, Li Zhang, Ming-Ming Cheng, and Jiashi Feng. Strip pooling: Rethinking spatial pooling for scene parsing. In CVPR, 2020.\\n\\n[24] Ping Hu, Federico Perazzi, Fabian Caba Heilbron, Oliver Wang, Zhe Lin, Kate Saenko, and Stan Sclaroff. Real-time semantic segmentation with fast attention. RA-L, 2021.\\n\\n[25] Jiaxing Huang, Shijian Lu, Dayan Guan, and Xiaobing Zhang. Contextual-relation consistent domain adaptation for semantic segmentation. In ECCV, 2020.\\n\\n[26] Zilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. CCNet: Criss-cross attention for semantic segmentation. In ICCV, 2019.\\n\\n[27] Alexander Jaus, Kailun Yang, and Rainer Stiefelhagen. Panoramic panoptic segmentation: Towards complete surrounding understanding via unsupervised contrastive learning. In IV, 2021.\\n\\n[28] Chiyu Max Jiang, Jingwei Huang, Karthik Kashinath, Prabhat, Philip Marcus, and Matthias Nie\u00dfner. Spherical CNNs on unstructured grids. In ICLR, 2019.\\n\\n[29] Zhenchao Jin, Tao Gong, Dongdong Yu, Qi Chu, Jian Wang, Changhu Wang, and Jie Shao. Mining contextual information beyond image for semantic segmentation. In ICCV, 2021.\\n\\n[30] Tarun Kalluri, Girish Varma, Manmohan Chandraker, and C. V. Jawahar. Universal semi-supervised semantic segmentation. In ICCV, 2019.\\n\\n[31] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR, 2015.\\n\\n[32] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Panoptic feature pyramid networks. In CVPR, 2019.\"}"}
{"id": "CVPR-2022-181", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yeonkun Lee, Jaeseok Jeong, Jongseob Yun, Wonjune Cho, and Kuk-Jin Yoon. SpherePHD: Applying CNNs on a spherical PolyHeDron representation of 360\u00b0 images. In CVPR, 2019.\\n\\nGuangrui Li, Guoliang Kang, Wu Liu, Yunchao Wei, and Yi Yang. Content-consistent matching for domain adaptive semantic segmentation. In ECCV, 2020.\\n\\nYunsheng Li, Lu Yuan, and Nuno Vasconcelos. Bidirectional learning for domain adaptation of semantic segmentation. In CVPR, 2019.\\n\\nDongze Lian, Zehao Yu, Xing Sun, and Shenghua Gao. AS-MLP: An axial shifted MLP architecture for vision. arXiv preprint arXiv:2107.08391, 2021.\\n\\nQing Lian, Lixin Duan, Fengmao Lv, and Boqing Gong. Constructing self-motivated pyramid curriculums for cross-domain semantic segmentation: A non-adversarial approach. In ICCV, 2019.\\n\\nTsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017.\\n\\nHanxiao Liu, Zihang Dai, David R. So, and Quoc V. Le. Pay attention to MLPs. In NeurIPS, 2021.\\n\\nMengyi Liu, Shuhui Wang, Yulan Guo, Yuan He, and Hui Xue. Pano-SfMLearner: Self-Supervised multi-task learning of depth and semantics in panoramic videos. SPL, 2021.\\n\\nYahao Liu, Jinhong Deng, Xinchen Gao, Wen Li, and Lixin Duan. BAPA-net: Boundary adaptation and prototype alignment for cross-domain semantic segmentation. In ICCV, 2021.\\n\\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.\\n\\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.\\n\\nYawei Luo, Liang Zheng, Tao Guan, Junqing Yu, and Yi Yang. Taking a closer look at domain shift: Category-level adversaries for semantics consistent domain adaptation. In CVPR, 2019.\\n\\nChaoxiang Ma, Jiaming Zhang, Kailun Yang, Alina Roitberg, and Rainer Stiefelhagen. DensePASS: Dense panoramic semantic segmentation via unsupervised domain adaptation with attention-augmented context exchange. In ITSC, 2021.\\n\\nHaoyu Ma, Xiangru Lin, Zifeng Wu, and Yizhou Yu. Coarse-to-fine domain adaptive semantic segmentation with photometric alignment and category-center regularization. In CVPR, 2021.\\n\\nSemih Orhan and Yalin Bastanlar. Semantic segmentation of outdoor panoramic images. SIVP, 2021.\\n\\nMarin Orsic, Ivan Kreso, Petra Bevandic, and Sinisa Segvic. In defense of pre-trained ImageNet architectures for real-time semantic segmentation of road-driving images. In CVPR, 2019.\\n\\nFei Pan, Inkyu Shin, Francois Rameau, Seokju Lee, and In So Kweon. Unsupervised intra-domain adaptation for semantic segmentation through self-supervision. In CVPR, 2020.\\n\\nLorenzo Porzi, Samuel Rota Bul\u00f2, Aleksander Colovic, and Peter Kontschieder. Seamless scene segmentation. In CVPR, 2019.\\n\\nRudra P. K. Poudel, Stephan Liwicki, and Roberto Cipolla. Fast-SCNN: Fast semantic segmentation network. In BMVC, 2019.\\n\\nEduardo Romera, Jose M. Alvarez, Luis Miguel Bergasa, and Roberto Arroyo. ERFNet: Efficient residual factorized ConvNet for real-time semantic segmentation. T-ITS, 2018.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: convolutional networks for biomedical image segmentation. In MICCAI, 2015.\\n\\nAhmed Rida Sekkat, Yohan Dupuis, Pascal Vasseur, and Paul Honeine. The OmniScape dataset. In ICRA, 2020.\\n\\nMehran Shakerinava and Siamak Ravanbakhsh. Equivariant networks for pixelized spheres. In ICML, 2021.\\n\\nPrabhu Teja Sivaprasad and Fran\u00e7ois Fleuret. Uncertainty reduction for model adaptation in semantic segmentation. In CVPR, 2021.\\n\\nRobin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for semantic segmentation. In ICCV, 2021.\\n\\nCheng Sun, Min Sun, and Hwann-Tzong Chen. HoHoNet: 360 indoor holistic understanding with latent horizontal features. In CVPR, 2021.\\n\\nKeisuke Tateno, Nassir Navab, and Federico Tombari. Distortion-aware convolutional filters for dense prediction in panoramic images. In ECCV, 2018.\\n\\nIlya O. Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. MLP-mixer: An all-MLP architecture for vision. In NeurIPS, 2021.\\n\\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In ICML, 2021.\\n\\nYi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In CVPR, 2018.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\\n\\nTuan-Hung Vu, Himalaya Jain, Maxime Bucher, Matthieu Cord, and Patrick P\u00e9r\u00e8z. ADVENT: Adversarial entropy minimization for domain adaptation in semantic segmentation. In CVPR, 2019.\\n\\nWenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In ICCV, 2021.\\n\\nXiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In CVPR, 2018.\"}"}
{"id": "CVPR-2022-181", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhonghao Wang, Mo Yu, Yunchao Wei, Rogerio Feris, Jinjun Xiong, Wen-mei Hwu, Thomas S. Huang, and Honghui Shi. Differential treatment for stuff and things: A simple unsupervised domain adaptation method for semantic segmentation. In CVPR, 2020.\\n\\nEnze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M. Alvarez, and Ping Luo. SegFormer: Simple and efficient design for semantic segmentation with transformers. In NeurIPS, 2021.\\n\\nMai Xu, Yuhang Song, Jianyi Wang, MingLang Qiao, Liangyu Huo, and Zulin Wang. Predicting head movement in panoramic video: A deep reinforcement learning approach. TPAMI, 2019.\\n\\nYuanyou Xu, Kaiwei Wang, Kailun Yang, Dongming Sun, and Jia Fu. Semantic segmentation of panoramic images using a synthetic dataset. In SPIE, 2019.\\n\\nYanyu Xu, Ziheng Zhang, and Shenghua Gao. Spherical DNNs and their applications in 360\u00b0 images and videos. TPAMI, 2021.\\n\\nKailun Yang, Xinxin Hu, Luis Miguel Bergasa, Eduardo Romera, and Kaiwei Wang. PASS: Panoramic annular semantic segmentation. T-ITS, 2020.\\n\\nKailun Yang, Xinxin Hu, Hao Chen, Kaite Xiang, Kaiwei Wang, and Rainer Stiefelhagen. DS-PASS: Detail-sensitive panoramic annular semantic segmentation through SwaftNet for surrounding sensing. In IV, 2020.\\n\\nKailun Yang, Xinxin Hu, and Rainer Stiefelhagen. Is context-aware CNN ready for the surroundings? Panoramic semantic segmentation in the wild. TIP, 2021.\\n\\nKailun Yang, Jiaming Zhang, Simon Rei\u00df, Xinxin Hu, and Rainer Stiefelhagen. Capturing omni-range context for omnidirectional segmentation. In CVPR, 2021.\\n\\nYanchao Yang and Stefano Soatto. FDA: Fourier domain adaptation for semantic segmentation. In CVPR, 2020.\\n\\nYaozu Ye, Kailun Yang, Kaite Xiang, Juan Wang, and Kaiwei Wang. Universal semantic segmentation for fisheye urban driving images. In SMC, 2020.\\n\\nMinghao Yin, Zhuliang Yao, Yue Cao, Xiu Li, Zheng Zhang, Stephen Lin, and Han Hu. Disentangled non-local neural networks. In ECCV, 2020.\\n\\nSenthil Kumar Yogamani, Christian Witt, Hazem Rashed, Sanjaya Nayak, Saquib Mansoor, Padraig Varley, Xavier Perrotton, Derek O\u2019Dea, Patrick P\u00e9rez, Ciar\u00e1n Hughes, Jonathan Horgan, Ganesh Sistu, Sumanth Chennupati, Michal Uric\u00e1r, Stefan Milz, Martin Simon, and Karl Amende. WoodScape: A multi-task, multi-camera fisheye dataset for autonomous driving. In ICCV, 2019.\\n\\nChangqian Yu, Jingbo Wang, Changxin Gao, Gang Yu, Chunhua Shen, and Nong Sang. Context prior for scene segmentation. In CVPR, 2020.\\n\\nYuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation. In ECCV, 2020.\\n\\nYuhui Yuan, Rao Fu, Lang Huang, Weihong Lin, Chao Zhang, Xilin Chen, and Jingdong Wang. HRFormer: High-resolution transformer for dense prediction. In NeurIPS, 2021.\\n\\nXiaoyu Yue, Shuyang Sun, Zhanghui Kuang, Meng Wei, Philip H. S. Torr, Wayne Zhang, and Dahua Lin. Vision transformer with progressive sampling. In ICCV, 2021.\\n\\nXiangyu Yue, Zangwei Zheng, Shanghang Zhang, Yang Gao, Trevor Darrell, Kurt Keutzer, and Alberto Sangiovanni Vincentelli. Prototypical cross-domain self-supervised learning for few-shot unsupervised domain adaptation. In CVPR, 2021.\\n\\nCheng Zhang, Zhaopeng Cui, Cai Chen, Shuaicheng Liu, Bing Zeng, Hujun Bao, and Yinda Zhang. DeepPanoContext: Panoramic 3D scene understanding with holistic scene context graph and relation-based optimization. In ICCV, 2021.\\n\\nChao Zhang, Stephan Liwicki, William Smith, and Roberto Cipolla. Orientation-aware semantic segmentation on icosahedron spheres. In ICCV, 2019.\\n\\nHang Zhang, Chongruo Wu, Zhongyue Zhang, Yi Zhu, Zhi Zhang, Haibin Lin, Yue Sun, Tong He, Jonas Mueller, R. Manmatha, Mu Li, and Alexander J. Smola. ResNeSt: Split-attention networks. arXiv preprint arXiv:2004.08955, 2020.\\n\\nJiaming Zhang, Chaoxiang Ma, Kailun Yang, Alina Roitberg, Kunyu Peng, and Rainer Stiefelhagen. Transfer beyond the field of view: Dense panoramic semantic segmentation via unsupervised domain adaptation. T-ITS, 2021.\\n\\nJiaming Zhang, Kailun Yang, Angela Constantinescu, Kunyu Peng, Karin M\u00fcller, and Rainer Stiefelhagen. Trans4Trans: Efficient transformer for transparent object segmentation to help visually impaired people navigate in the real world. In ICCVW, 2021.\\n\\nJiaming Zhang, Kailun Yang, and Rainer Stiefelhagen. ISSAFE: Improving semantic segmentation in accidents by fusing event-based data. In IROS, 2020.\\n\\nPan Zhang, Bo Zhang, Ting Zhang, Dong Chen, Yong Wang, and Fang Wen. Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation. In CVPR, 2021.\\n\\nYang Zhang, Philip David, and Boqing Gong. Curriculum domain adaptation for semantic segmentation of urban scenes. In ICCV, 2017.\\n\\nHengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017.\\n\\nSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H. S. Torr, and Li Zhang. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In CVPR, 2021.\\n\\nZhedong Zheng and Yi Yang. Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation. IJCV, 2021.\\n\\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: Deformable transformers for end-to-end object detection. In ICLR, 2021.\\n\\nYang Zou, Zhiding Yu, Xiaofeng Liu, B. V. K. Vijaya Kumar, and Jinsong Wang. Confidence regularized self-training. In ICCV, 2019.\"}"}
{"id": "CVPR-2022-181", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mutual Pan(oramic) Pin(hole) Images in D get domain with semantics from the source domain and transfer it to the target (panoramic) dataset. The goal of domain adaptation is to learn feature maps of semantic similarities in feature space.\\n\\nComplementary direct targets and (2) it performs pseudo-labels by using them in feature space instead of as softens, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it softens space, which brings two benefits: (1) it"}
{"id": "CVPR-2022-181", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Performance gaps of CNN- and transformer-based models from Cityscapes (CS) @1024 \u00d7 512 to DensePASS (DP).\\n\\n| Network Backbone | SPin | SPan | mIoU | Gaps |\\n|------------------|------|------|------|------|\\n| SwiftNet         | 42.28| 34.95| -7.87|\\n| DANet            | 43.33| 37.76| -5.57|\\n| Trans4PASS-T     | 41.28| 24.45| -16.83|\\n| Trans4PASS-S     | 44.47| 23.11| -21.36|\\n\\nTable 2. Performance gaps from Stanford2D3D-Pinhole (SPin) to Stanford2D3D-Panoramic (SPan) dataset on fold-1.\\n\\n| Setting          | Gaps |\\n|------------------|------|\\n| Pinhole          | 44.8 |\\n| Panoramic        | 44.8 |\\n\\n4.2. PIN2PAN Gaps\\n\\nDomain gap in outdoor scenarios. To quantify the PIN2PAN domain gap in outdoor scenarios, we evaluate over 15 off-the-shelf segmentation models trained on Cityscapes. Table 1 summarizes the results tested on Cityscapes and DensePASS validation sets. Although previous transformers reduce the mIoU gap from \u223c50% of CNN-based counterparts to \u223c40%, the PIN2PAN gap remains large. The proposed Trans4PASS architecture has a high performance on pinhole image segmentation and also outperforms other methods on panoramic segmentation with 44.8% mIoU without any adaptation strategy. It indicates that distortion-aware features and long-range cues maintained in both low and high levels of Transformers as opposed to the context learned in higher-levels of CNNs, are important for wide-FoV panoramic segmentation.\\n\\nDomain gap in indoor scenarios. Table 2 shows PIN2PAN domain gaps in indoor scenarios. As pinhole and panoramic images from Stanford2D3D are captured under the same setting, the PIN2PAN gap is smaller compared to the outdoor scenario. Still, in light of other CNN- and transformer-based methods, the small Trans4PASS version achieves 50.2% and 48.34% mIoU in pinhole- and panoramic image segmentation, yielding the smallest performance drop.\\n\\n4.3. Trans4PASS Structural Analysis\\n\\nEffect of DPE. We compare DPE against DePatch from DPT [7]. While the object-aware offsets and scales in DPT make patches shift around the object, our DPE is flexible to split image patches and is decoupled from object proposals. As shown in the first group of Table 3, compared with DPT, our DPE-based Trans4PASS adds +3.50% and +9.39% mIoU on Cityscapes and DensePASS, respectively.\\n\\nEffect of DMLP. To ablate the effect of different MLP-like modules embedded in the decoder of Trans4PASS, we substitute DMLP by CycleMLP [5] and ASMLP [36] modules. DMLP is lighter than ASMLP with fewer GFLOPs, parameters and it is more adaptive as opposed to the fixed offsets in CycleMLP. The first group of Table 3 shows that DMLP outperforms both modules with 3% to 5% in mIoU.\\n\\nEffect of encoders and decoders. With the same encoder as PVT, a DMLP-based decoder brings a +3.98% improvement compared to the FPN- and MLP-based decoders, as shown in the second group of Table 3. When our DPE is applied in the early stage of the PVT encoder, further improvements of +5.30% can be made. Similar improvement results (+6.12% and +6.87%) are evident in experiments with a SegFormer encoder. Overall, these results show that DPE and DMLP can be integrated into diverse backbones, significantly improving distortion-adaptability for panoramic scene segmentation.\"}"}
{"id": "CVPR-2022-181", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method                      | 0\u00b0   | 45\u00b0   | 90\u00b0   | 135\u00b0  | 180\u00b0  | 225\u00b0  | 270\u00b0  | 315\u00b0  |\\n|-----------------------------|------|-------|-------|-------|-------|-------|-------|-------|\\n| MPA                        | 54.72| 55.25 | 55.25 | 54.72 | 52.91 | 54.72 | 52.91 | 54.72 |\\n| P2PDA                      | 52.91| 52.91 | 52.91 | 52.91 | 52.91 | 52.91 | 52.91 | 52.91 |\\n\\nMPA brings uniform improvement to omnidirectional segmentation. Apart from benefiting the stuff classes (vegetation, traffic sign, building, fence), MPA improves the segmentation of object classes, such as person, motorcycle, and truck. Due to the captured long-range contexts and distortion-aware features, Trans4PASS-T and -S match up against each other. Individually, MPA plus SSL, Trans4Pass-S obtains new state-of-the-art performance on DensePASS, reaching 79.91% and 81.72% in mIoU.\\n\\n**Comparison with state-of-the-art panoramic segmentation** [72, 75], domain adaptation [44, 67, 84, 88, 97], and multi-supervision methods [30, 50, 90]. * denotes performing multi-scale (MS) evaluation.\\n\\n| Network                  | 0\u00b0   | 10\u00b0  | 20\u00b0  | 30\u00b0  | 40\u00b0  | 50\u00b0  | 60\u00b0  |\\n|--------------------------|------|------|------|------|------|------|------|\\n| FANet                    | 50.8 | 53.0 | 54.6 | 56.4 | 58.2 | 60.0 | 61.8 |\\n| DANet                    | 48.7 | 51.5 | 54.2 | 56.9 | 59.6 | 61.4 | 63.2 |\\n\\nOmnidirectional segmentation. To showcase the effectiveness of MPA on omnidirectional segmentation, the polar diagram in Fig. 5 demonstrates that MPA works collaboratively with pseudo labels and provides a complementary feature alignment incentive. **Table 4a** compares our solution with recent panoramic segmentation [72, 75] and domain adaptation [44, 67, 84, 169]. In indoor and outdoor scenarios, MPA surpasses ERFNet [52] and CLAN (Adversarial) [44] with multi-scale evaluation. This verifies that the advantage of a superior network architecture, MPA achieves state-of-the-art performance on DensePASS, reaching 93% and 97% in mIoU.\\n\\n**Comparison on SPan**\\n\\n| Method                      | 0\u00b0   | 10\u00b0  | 20\u00b0  | 30\u00b0  | 40\u00b0  | 50\u00b0  | 60\u00b0  |\\n|-----------------------------|------|------|------|------|------|------|------|\\n| UGSCNN (PVT-Tiny)           | 66.18| 69.64| 68.49| 67.25| 66.01| 64.77| 63.53|\\n| HexRUNet (PVT-Small)        | 71.01| 31.85| 76.79| 12.13| 23.61| 11.93| 3.23  |\\n| CRST (Self-training)        | 68.86| 18.22| 47.01| 9.45 | 12.79| 17.00| 8.12  |\\n| P2PDA (Adversarial)         | 65.39| 21.14| 69.10| 17.29| 25.49| 11.17| 3.14  |\\n| ERFNet (MPA+MS)             | 81.60| 67.84| 59.69| 19.96| 29.41| 8.26 | 4.54  |\\n\\nAdaptation results on SPan\\n\\n| Method                      | 0\u00b0   | 10\u00b0  | 20\u00b0  | 30\u00b0  | 40\u00b0  | 50\u00b0  | 60\u00b0  |\\n|-----------------------------|------|------|------|------|------|------|------|\\n| UGSCNN (PVT-Tiny)           | 88.62| 86.04| 60.85| 39.20| 55.36| 29.47| 79.20|\\n| HexRUNet (PVT-Small)        | 78.10| 70.24| 26.72| 28.44| 14.02| 11.67| 5.79  |\\n| CRST (Self-training)        | 68.18| 15.72| 76.78| 14.06| 26.11| 9.90 | 0.82  |\\n| P2PDA (Adversarial)         | 50.71| 46.39| 12.98| 8.92 | 0.44 | 12.77| 8.77  |\\n\\n| Method                      | 0\u00b0   | 10\u00b0  | 20\u00b0  | 30\u00b0  | 40\u00b0  | 50\u00b0  | 60\u00b0  |\\n|-----------------------------|------|------|------|------|------|------|------|\\n| UGSCNN (PVT-Tiny)           | 88.62| 86.04| 60.85| 39.20| 55.36| 29.47| 79.20|\"}"}
{"id": "CVPR-2022-181", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6: Qualitative comparisons, DPE and DMLP visualizations. (a) and (c) show segmentation comparisons, where the baseline has neither DPE/DMLP nor MPA. The dots in (b) and (d) are sampling points shifted by learned offsets w.r.t. the patch center of DPE (from decoder). (e) and (f) show the #75 channel maps of stage-3 before and after DMLP. Zoom in for better view.\\n\\n5. Conclusion\\n\\nTo revitalize 360\u00b0 scene understanding, we introduce a universal framework with a Transformer for PAnoramic Semantic Segmentation (Trans4PASS) model and a Mutual Prototypical Adaptation (MPA) method for transferring semantic information from the label-rich pinhole domain to the label-scarce panoramic domain. The Deformable Patch Embedding (DPE) and the Deformable MLP (DMLP) module endow Trans4PASS with distortion awareness. The framework elevates state-of-the-art performances on the competitive Stanford2D3D and DensePASS benchmarks.\\n\\nLimitations. We note that the accuracy of some classes are still impacted by the partition boundary of panoramas at 180\u00b0. Transferring models between pinhole-, fisheye-, and panoramic domains, fusing modalities, and solving various tasks of 360\u00b0 imagery are opportunities for further research.\"}"}
{"id": "CVPR-2022-181", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bending Reality: Distortion-aware Transformers for Adapting to Panoramic Semantic Segmentation\\n\\nJiaming Zhang\\nKailun Yang\\nChaoxiang Ma\\nSimon Rei\u00df\\nKunyu Peng\\nRainer Stiefelhagen\\n\\n1 CV:HCI Lab, Karlsruhe Institute of Technology\\n2 ByteDance Inc.\\n3 Carl Zeiss AG\\n\\nAbstract\\nPanoramic images with their 360\u00b0 directional view encompass exhaustive information about the surrounding space, providing a rich foundation for scene understanding. To unfold this potential in the form of robust panoramic segmentation models, large quantities of expensive, pixel-wise annotations are crucial for success. Such annotations are available, but predominantly for narrow-angle, pinhole-camera images which, off the shelf, serve as sub-optimal resources for training panoramic models. Distortions and the distinct image-feature distribution in 360\u00b0 panoramas impede the transfer from the annotation-rich pinhole domain and therefore come with a big dent in performance. To get around this domain difference and bring together semantic annotations from pinhole- and 360\u00b0 surround-visuals, we propose to learn object deformations and panoramic image distortions in the Deformable Patch Embedding (DPE) and Deformable MLP (DMLP) components which blend into our Transformer for PAnoramic Semantic Segmentation (Trans4PASS) model. Finally, we tie together shared semantics in pinhole- and panoramic feature embeddings by generating multi-scale prototype features and aligning them in our Mutual Prototypical Adaptation (MPA) for unsupervised domain adaptation. On the indoor Stanford2D3D dataset, our Trans4PASS with MPA maintains comparable performance to fully-supervised state-of-the-arts, cutting the need for over 1,400 labeled panoramas. On the outdoor DensePASS dataset, we break state-of-the-art by 14.39% mIoU and set the new bar at 56.38%.\\n\\n1. Introduction\\nPanoramic 360\u00b0 cameras have received an increasing amount of attention in fields, such as omnidirectional sensing in automated vehicles [13, 75] and bringing immersive viewing experiences to augmented- and virtual reality [69, 71]. Opposed to images captured with pinhole cameras, that occupy narrow Fields of View (FoV), panoramic images offer omni-range perception, benefiting the detection of road scene objects and indoor scene elements [13, 20]. In particular, dense semantic segmentation on panoramic images, facilitates a high-level holistic pixel-wise understanding of surrounding environments [45, 73].\\n\\nPanoramic semantic segmentation is usually performed on 2D panoramas that were transformed using equirectangular projection [58, 75], which is accompanied by image distortions and object deformations (see Fig. 1). Further, in the 360\u00b0 image domain, labeled data is scarce which necessitates model training to be carried out on semantically matching narrow-FoV pinhole datasets. These two circumstances culminate in a significantly degraded performance on panoramic segmentation as compared to the pinhole counterpart [72] and as such they have to be adequately addressed. Considering the intricacies of panoramas, convolution variants [10, 55, 59] and attention-augmented models [75] were proposed to mitigate image distortions and enlarge receptive fields of Convolutional Neural Networks (CNNs). However, they remain sub-optimal in handling the severe deformations from pinhole- to panoramic data, and fail in establishing long-range contextual dependencies in the ultra-wide 360\u00b0 images, which prove essential for accurate semantic segmentation [17, 94].\\n\\n*Corresponding author (e-mail: kailun.yang@kit.edu).\\n\\n1 Code will be made publicly available at https://github.com/jamycheung/Trans4PASS.\\n\\nFigure 1: Semantic segmentation of (a) narrow-angle pinhole image and (b) 360\u00b0 panoramic image. Compared to (c) standard Patch Embeddings, our (d) Deformable Patch Embedding partitions 360\u00b0 images while considering distortions, e.g. in sidewalks.\\n\\ndisplays [69, 71].\"}"}
{"id": "CVPR-2022-181", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In light of these challenges, we propose a Transformer for Panoramic Semantic Segmentation (Trans4PASS) architecture, and overcome image distortions and object deformations with two novel design choices: Our Deformable Patch Embedding (DPE) is located at the early image sequentialization- and intermediate feature interpretation stages empowering the model to learn characteristic panoramic image distortions and preserve semantics. Secondly, with the Deformable MLP (DMLP) module in the feature parsing stage, we mix patches with learned spatial offsets to enhance global context modeling.\\n\\nThe challenging mismatch between the label-rich pinhole- and the label-scarce panoramic domain can also be addressed by unsupervised domain adaptation (UDA), considering labeled 2D Pinhole images as source- and 360\u00b0 Panoramas as target domain. Following previous works [45, 75], we refer to this scenario as PIN2PAN. Taking this view on the learning problem, shows to be a vital ingredient for circumventing the expensive panoramic image annotation process while satisfying the need for large-scale annotated data [94] to train robust segmentation transformers. Unlike common adversarial-learning [44] and pseudo-label self-learning [97] methods for UDA, we put forward Mutual Prototypical Adaptation (MPA), which generates mutual prototypes for pinhole- and panoramic multi-scale feature embeddings, distilling prototypical knowledge of both domains, which proves advantageous to domain-separate distillation [84]. On top, we show MPA works with pseudo-labels in a joint manner and provides a complementary alignment incentive in the feature space.\\n\\nTo verify the capability for generalization to diverse scenarios of our solution, we evaluate Trans4PASS on both indoor- and outdoor panoramic-view datasets, i.e., Stanford2D3D [1] and DensePASS [45] benchmarks. On DensePASS, it outperforms the previous best result [88] by >10.0% in mIoU. Our solution achieves top performance among unsupervised methods on Stanford2D3D and even ranks higher than many competing supervised methods.\\n\\nIn summary, we deliver the following contributions:\\n\\n1. We consider panoramic deformations in our distortion-aware Transformer for Panoramic Semantic Segmentation (Trans4PASS) with deformable patch embedding- and deformable MLP modules.\\n2. We present Mutual Prototypical Adaptation to transfer models via distilling dual-domain prototypical knowledge, boosting performance by coupling it with pseudo-labels in feature- and output space.\\n3. Our framework for transferring models from PIN2PAN yields excellent results on two competitive benchmarks: On Stanford2D3D we circumvent using 1/400 expensive panorama labels while achieving comparable results and on DensePASS we boost state-of-the-art performance by an absolute 14.39% in mIoU.\\n\\n2. Related Work\\n\\nSemantic- and panoramic segmentation.\\n\\nDense semantic segmentation is experiencing steep progress since FCN [43] addressed it end-to-end. Following works built upon FCN to improve performance by enlarging receptive fields [23, 93] and refining context priors [29, 80]. Driven by non-local blocks [66], self-attention [63] is integrated to learn long-range dependencies [17, 26] within FCNs. Currently, architectures which replace convolutional- with transformer-based backbones [15, 61] emerge. Then, image perception is viewed from the lens of sequence-to-sequence learning with dense prediction transformers [42, 82] and semantic segmentation transformers [57, 94]. Recently, MLP-like architectures [36, 39, 60] which alternate spatial- and channel mixing sparked interest for recognition tasks. Most methods are designed for narrow-FoV images and often have large accuracy drops in the 360\u00b0 domain. In this work, we address panoramic segmentation, with a novel Transformer architecture which considers a broad FoV already in its design and handles the panorama-specific semantic distribution via MLP-based mixing.\\n\\nBy capturing wide-FoV scenes, panoramic images can serve as starting point for a more holistic scene understanding. Outdoor panorama segmentation works rely on fish-eye cameras [14, 54, 77, 79] or panoramic images [27, 47, 70, 74] for seamless 360\u00b0 parsing. Indoor methods on the other hand focus on either distortion-mitigated representations [28, 33, 55] or multi-task schemes [40, 58, 85]. Most of these works assume that labeled images are available in the target panorama domain. We cut this requirement for labeled target data and circumvent the prohibitively expensive annotation process of determining pixel-wise semantics in complex real-world surroundings. Therefore, unlike previous works, we look through the lens of unsupervised transfer learning and introduce a pinhole- to panorama (PIN2PAN) adaptation method to profit from rich, readily available annotated pinhole datasets. In experiments, our panoramic segmentation transformer architecture generalizes to both indoor and outdoor scenes.\\n\\nUnsupervised domain adaptation.\\n\\nDomain adaptation has been thoroughly investigated to enhance model generalization to unseen domains, with two predominant paradigms based either on self-training [8, 21, 92] or adversarial learning [2, 22, 62]. Self-training methods generally create pseudo-labels to gradually adapt through iterative improvement [37], whereas adversarial solutions leverage the idea of GANs [18] to perform image translation [22, 35], or enforce alignment in layout matching [25, 34] and feature agreement [44, 45]. Further adaptation flavors, consider uncertainty reduction [56, 95], model ensembling [4, 76], category-level alignment [41, 46], or adversarial entropy minimization [49, 64]. Relevant to our work, PIT [19] addresses the camera gap with FoV-based adaptation, whereas...\"}"}
{"id": "CVPR-2022-181", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2: Comparison of segmentation transformers.\\n\\nTransformers (a) borrow a FPN-like decoder [94] from CNN counterparts or (b) adopt a vanilla-MLP decoder [68] for feature fusion, which lacks patch mixing. (c) Trans4PASS integrates Deformable Patch Embeddings (DPE) and the Deformable MLP (DMLP) module for capabilities to handle distortions (see warped terrain) and mix patches.\\n\\nP2PDA [45] first tackles P\\\\textsuperscript{IN}2\\\\textsuperscript{AN} transfer by learning attention correspondences. Aside from distortion-adaptive architecture design, we revisit P\\\\textsuperscript{IN}2\\\\textsuperscript{AN} segmentation from a feature prototype adaptation-based perspective where we distill panoramic knowledge through class-wise prototypes. Different from methods using individual prototypes for source and target domains [84, 91], we present mutual prototypical adaptation, which jointly exploits source and target feature embeddings to boost transfer beyond the FoV.\\n\\n3. Methodology\\n\\nHere, we put forward our panoramic semantic segmentation framework. In Sec. 3.1, we introduce the Trans4PASS architecture for capturing distortion-aware features and long-range dependencies, with detailed descriptions of deformable patch embeddings and the deformable MLP module in Sec. 3.2 and 3.3. Finally, we outline our domain adaptation method using mutual prototype features in Sec. 3.4.\\n\\n3.1. Trans4PASS Architecture\\n\\nTo investigate the transformer model on panoramic semantic segmentation, we create two versions of Trans4PASS models (T: Tiny and S: Small). We build both with four stages, where for the tiny model, each stage encompasses 2 layers, for the small version the stages have 3, 4, 6, and 3 layers. As shown in Fig. 2, the pyramidal stages are inspired by recent transformers [65, 68], which reduce the feature scales in deeper layers. Given an input image with $H \\\\times W \\\\times 3$, Trans4PASS makes use of a Patch Embedding (PE) module [68] to split the image into patches.\\n\\nTo deal with the severe distortions in panoramas, a special Deformable Patch Embedding (DPE) module is proposed and applied in the encoder and decoder (Fig. 2c). In the encoder, each feature map $f_l \\\\in \\\\{f_1, f_2, f_3, f_4\\\\}$ in the $l$th stage is down-sampled by the $l$th stride $\\\\in \\\\{4, 8, 16, 32\\\\}$. The channel dimensions $C_l \\\\in \\\\{64, 128, 320, 512\\\\}$ grow successively. Different from the FPN-like decoder [94] and vanilla-MLP based decoder [68] in Fig. 2, we propose the Deformable MLP (DMLP) decoder structure, which mixes feature patches extracted via DPE. Given the extracted feature hierarchy in multiple scales from the encoder, four deformable decoder layers process the feature hierarchy into a consistent shape of $H_4 \\\\times W_4 \\\\times C_{\\\\text{emb}}$, where we set the number of resulting embedding channels $C_{\\\\text{emb}} = 128$. An ensuing linear layer transforms the 128 channel output to contain the number of semantic classes of the respective task.\\n\\n3.2. Deformable Patch Embedding\\n\\nSpherical topological images captured by 360\u00b0 cameras occupy a polar coordinate system with $\\\\theta \\\\in [0, 2\\\\pi)$ and $\\\\phi \\\\in [0, \\\\pi]$. To represent it in 2D space, the spherical data is usually converted into a panoramic format in euclidean-like space through the equirectangular projection. This process leads to severe shape distortions in the projected panoramic image, as seen in Fig. 1. Therefore, a common PE module with fixed sampling positions does not respect these shape distortions of objects and the overall scene. Inspired by deformable convolution [12] and overlapping PE [68], we propose Deformable Patch Embeddings (DPE) and employ them on the input to the encoder and the decoder, splitting panoramic images and features. Given an input image or feature map $f \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C}$ in $f$, a standard PE module [15, 68] splits it into a flattened 2D patch sequence $z \\\\in \\\\mathbb{R}^{(HW_s^2) \\\\times (s^2 \\\\cdot C_{\\\\text{in}})}$, where $HW_s^2$ is the number of patches and $s$ is the width and height of each patch. Each element in this sequence is passed through a linear projection layer transforming it into $C_{\\\\text{out}}$-dimensional embeddings.\\n\\nConsider a single patch in $z$ representing a rectangle of size $s \\\\times s$ with $s^2$ positions. We can define a position offset relative to a location $(i, j)$ $\\\\in [1, s]$ as $\\\\Delta(i,j) \\\\in \\\\mathbb{N}_2^2$. In standard PE, these offsets are fixed and lie in $\\\\Delta(i,j) \\\\in [\\\\lfloor-\\\\frac{s}{2}\\\\rfloor, \\\\lfloor\\\\frac{s}{2}\\\\rfloor]^2$. Take e.g. a $3 \\\\times 3$ patch, offsets $\\\\Delta(i,j)$ relative to the center will lie in $[-1, 1] \\\\times [-1, 1]$.\\n\\nAs we want to process panoramic images, which inherit distortions from the equirectangular projection, we can directly address this degradation in the PE. To this end, in our Deformable Patch Embedding (DPE), we enable the model to learn a data-dependent offset $\\\\Delta_{\\\\text{DPE}} \\\\in \\\\mathbb{N}^{H \\\\times W \\\\times 2}$ that can better cope with the spatial connections of objects, as present in distorted patches. DPE is learnable and pre-\"}"}
{"id": "CVPR-2022-181", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The offset \\\\( \\\\Delta DPE(i,j) \\\\) is calculated as depicted in Eq. (1).\\n\\n\\\\[\\n\\\\Delta DPE(i,j) = \\\\min(\\\\max(-Hr, g(f)(i,j)), Hr)\\n\\\\]\\n\\nwhere \\\\( g(\\\\cdot) \\\\) is the offset prediction function, which we implement via the deformable convolution operation \\\\[12\\\\]. The hyperparameters put a constraint onto the offsets and is set as 4 in our experiments. The learned offsets make DPE adaptive and as a result distortion-aware.\\n\\nIn earlier works, DPT \\\\[7\\\\] applies non-overlapping PE with anchor-based offsets at later stages, PS-ViT \\\\[83\\\\] uses a progressive sampling module coupled with previous iterations, and Deformable DETR \\\\[96\\\\] leverages deformable attention to enhance feature maps. Unlike these previous works, our proposed DPE is designed for pixel-dense prediction tasks and is flexible to replace the raw PE without having to couple previous iterations. Intuitively, a model supplied with DPE, can profit from pinhole images and better adapt to distortions in panoramic images by learning to counteract severe deformations in the data.\\n\\n3.3. Deformable MLP\\n\\nApart from the specific design of the encoder, the decoder with an adaptive feature parsing capacity is crucial in segmentation transformers \\\\[68, 89\\\\]. As shown in Fig. 2a, some transformers \\\\[94\\\\] borrow a FPN-like decoder from the CNN counterpart \\\\[38\\\\], whose receptive field is limited to the feature resolution in its final stage \\\\[65\\\\]. SegFormer \\\\[68\\\\] takes inspiration from Multilayer Perceptron-based (MLP) models \\\\[60\\\\] and integrates a vanilla MLP to combine features (Fig. 2b), but does not consider potential distortions in the imaging data. Next, we propose a mechanism to associate self-attention in Transformers and deformation-properties in \\\\(360^\\\\circ\\\\) imagery. Linking both of these enables profiting from long-range dependencies for dense scene parsing and keeping this improvement when processing panoramic scenes. Achieving this distortion-aware property at manageable computational complexity, we put forward the Deformable MLP (DMLP) module. Within each stage of the decoder, DMLP mixes patches across the channel dimension, but with a particularly large receptive field, which improves the interpretation of features delivered by the aforementioned DPE.\\n\\nFig. 3 shows the difference in MLP-based modeling: while the vanilla MLP (see Fig. 3a) performs traditional linear projection without learning any spatial context, CycleMLP (see Fig. 3b) has a limited spatial receptive field by hand-crafted, fixed offsets in mixing patches and their channels. In Fig. 3c, the proposed DMLP generates a learned spatial offset (top) in a wider range and an adaptive manner.\\n\\nGiven the input feature map \\\\( f \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C} \\\\) in, the spatial offset \\\\( \\\\Delta DMLP(i,j,c) \\\\) is predicted channel-wise as in Eq. (1) and is flattened as \\\\( \\\\Delta DMLP(k,c) \\\\), where \\\\( k \\\\in HW \\\\) and \\\\( c \\\\in C \\\\) in, for mixing the flattened patch features \\\\( z \\\\in \\\\mathbb{R}^{HW \\\\times C} \\\\) in, as:\\n\\n\\\\[\\n\\\\hat{z}(k,c) = \\\\sum_{k=1}^{HW} \\\\sum_{c=1}^{C} w^T(k,c) \\\\cdot z(k+\\\\Delta DMLP(k,c),c),\\n\\\\]\\n\\nwhere \\\\( w \\\\in \\\\mathbb{R}^{C_{in} \\\\times C_{out}} \\\\) is the weight matrix of a fully-connected (FC) layer. As shown in Fig 2c, the decoder has a similar structure as a MLP-Mixer block \\\\[60\\\\], consisting of DPE, DMLP, and MLP modules. The residual connections are kept. Formally, the four-stage decoder is denoted as:\\n\\n\\\\[\\n\\\\hat{z}_l = DPE(C_l, C_{emb})(z_l), \\\\quad \\\\forall l \\\\in \\\\{1,2,3,4\\\\}\\n\\\\]\\n\\n\\\\[\\n\\\\hat{z}_l = DMLP(C_{emb}, C_{emb})(\\\\hat{z}_l) + \\\\hat{z}_l, \\\\quad \\\\forall l \\\\in \\\\{1,2,3,4\\\\}\\n\\\\]\\n\\n\\\\[\\n\\\\hat{z}_l = MLP(C_{emb}, C_{emb})(\\\\hat{z}_l) + \\\\hat{z}_l, \\\\quad \\\\forall l \\\\in \\\\{1,2,3,4\\\\}\\n\\\\]\\n\\n\\\\[\\n\\\\hat{z}_l = Up(H/4, W/4)(\\\\hat{z}_l), \\\\quad \\\\forall l \\\\in \\\\{1,2,3,4\\\\}\\n\\\\]\\n\\nwhere \\\\( Up(\\\\cdot) \\\\) and \\\\( LN(\\\\cdot) \\\\) refer to the Upsample- and Layer-Norm operations, and \\\\( p \\\\) is the prediction of \\\\( K \\\\) classes.\\n\\n3.4. Mutual Prototypical Adaptation\\n\\nDue to the lack of large-scale training data in panoramas, we look into PIN2PAN domain adaptation from a perspective of semantic prototypes \\\\[91\\\\]. We propose the Mutual Prototypical Adaptation (MPA) method to enable distilling knowledge via prototypes which we cultivate through source ground truth labels and target pseudo labels. Pseudo-labels depend on the few remaining mutual properties from pinhole and panoramic images, e.g., scene distribution at the frontal viewing angle \\\\[9, 75\\\\]. While the related PCS \\\\[84\\\\] performs inter- and intra-domain instance-prototype learning, our mutual prototypes are learned from source- and target feature embeddings \\\\( f_s \\\\) and \\\\( f_t \\\\), projected to a shared latent space, and stored in a dynamic bank, as shown in Fig. 4. The key differences to PCS lie in that (1) the mutual prototypes are built by joining embeddings from both domains, and (2) our method leverages multi-scale pyramidal features using different input scales in computing the embeddings which yields more robust prototypes.\"}"}
