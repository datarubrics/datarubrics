{"id": "CVPR-2024-2648", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Thus, we aim to enlarge the high-dimension feature discrepancy between different bases. To this end, we design a regularization loss $L_{\\\\text{reg}}$ to enlarge the feature discrepancy between different bases $z$.\\n\\nFirst, given projected bases $q_i$, we also calculate the cosine similarity $s_{ij}$ between different $q_i$ and $q_j$ as follows:\\n\\n$$s_{ij} = \\\\text{CosSim}(q_i, q_j) = \\\\frac{q_i \\\\cdot q_j}{\\\\|q_i\\\\| \\\\|q_j\\\\|}, \\\\quad i, j \\\\in n, i \\\\neq j,$$\\n\\nwhere we aim to regularize $s_{ij}$ to 0, enforcing feature discrepancy between different bases. Thus, the loss function $L_{\\\\text{reg}}$ is formulated as:\\n\\n$$L_{\\\\text{reg}} = 2 \\\\frac{n(n-1)}{n} \\\\sum_{i,j \\\\in n, i \\\\neq j} |s_{ij}|,$$\\n\\nwhere $|.|$ denotes the absolute value. With loss $L_{\\\\text{reg}}$, we aim to optimize $q$ as linearly independent bases:\\n\\n$$q_i \\\\perp q_j, \\\\quad i \\\\neq j, \\\\quad i, j \\\\in n.$$\\n\\nWith the regularization loss function $L_{\\\\text{reg}}$, we aim to learn a group of linearly independent bases to represent all directions of high-dimension features. In this way, we can learn a group of more discriminative class assignments to supervise the final position predictions.\\n\\n### Overall loss function\\n\\nThus, the total loss function $L$ is the combination of $L_{\\\\text{reg}}$ and $L_{\\\\text{pred}}$:\\n\\n$$L = L_{\\\\text{pred}} + \\\\lambda L_{\\\\text{reg}},$$\\n\\nwhere $\\\\lambda$ is used to balance the relative contributions of these two loss terms and set to 1.0 in experiments empirically since we consider their importance equally. The ablation studies of $\\\\lambda$ are provided in the supplementary materials.\\n\\n## 4. Experiments\\n\\nIn this section, we first describe the datasets used in the pre-training and downstream tasks. Then, we briefly introduce the implementation details of VoCo. Finally, we report detailed experiment results of our proposed VoCo compared with other state-of-the-art SSL methods in 3D medical images. More details are in the supplementary materials.\\n\\n### 4.1. Datasets\\n\\n#### Pre-training datasets\\n\\nAiming to conduct fair comparisons with the previous works [51, 55, 70, 71, 13, 73], we also carry out pre-training experiments on the same three public datasets, i.e., BTCV [35], TCIA Covid19 [14], and LUNA [48] datasets, including about totally 1.6k CT scans for pre-training. It is worth noting that, aiming to conduct fair comparisons with previous works [73, 13], we only use BTCV [35] and TCIA Covid-19 [14] for pre-training in the downstream experiments of BTCV [35]. For the other downstream tasks, we use all three datasets for pre-training. Details are provided in the supplementary materials.\\n\\n#### Downstream datasets\\n\\nTo evaluate the effectiveness of our VoCo, we conduct downstream experiments on six public datasets, i.e., BTCV [35], LiTs [4], MSD Spleen [1], MM-WHS [74], BraTS 21 [49], and CC-CCII [67], including segmentation and classification tasks. The first five datasets are developed for segmentation, while CC-CCII [67] is for COVID-19 classification. Note that only BTCV [35] is used in pre-training, the other datasets are unseen in pre-training.\\n\\nIn addition, to evaluate the cross-modality generalization ability, we transfer the model pre-trained on the CT dataset to the MRI dataset BraTS 21 [49]. We adopt consistent settings as previous works [13, 73, 26, 55, 32]. We also evaluate the performance on 2D medical dataset. Details are provided in the supplementary materials.\\n\\n### 4.2. Implementation details\\n\\nFollowing previous works [51, 55], we use SwinUNETR [26] for both pre-training and downstream tasks. We use AdamW [38] optimizer and cosine learning rate scheduler for all experiments. We set 100K training steps in the pre-training process and applied a slicing window inference for fair comparisons with previous works [51, 55, 13, 73]. Aiming to evaluate the pure effectiveness, we do not use foundation models or post-processing [34, 36]. Details are provided in the supplementary materials.\\n\\n### Comparison methods\\n\\nWe compare our VoCo with both General and Medical SSL methods. First, we compare with the typical SSL methods MAE [28, 13] and MoCo v3 [29, 12], since they represent the two mainstream SSL paradigms, i.e., mask-autoencoder and contrastive learning. Since it is not practical to set a large batch size for 3D medical images due to computation cost, for fair comparisons, we adopt consistent settings with other methods in MAE [28, 13] and MoCo v3 [29, 12]. We also report the results of SimCLR [10] and SimMIM [63] according to [13].\\n\\nWe further evaluate the performance of Jiasaw [9] and PositionLabel [68], since they are related to our position-aware methods. Most existing state-of-the-art medical SSL methods are compared in our experiments.\\n\\n### 4.3. Experiments on downstream tasks\\n\\n#### Outperform existing methods on the BTCV dataset\\n\\nWe first conduct experiments on BTCV [35], as shown in Table 1. Specifically, among the comparison methods, MAE3D [28, 13], SimCLR [10], SimMIM [63], MoCo v3 [29, 12], and GL-MAE [73] use UNETR [27]. Other methods including our VoCo adopt Swin-UNETR [26] as the default settings of the previous work [51].\\n\\nRemark. It can be seen in Table 1 that the general SSL methods perform worse than most medical SSL methods.\"}"}
{"id": "CVPR-2024-2648", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method          | Network | Dice Score (%) | From Scratch |\\n|-----------------|---------|----------------|--------------|\\n|                 |         |                | 3D UNet [46] | 90.70        |\\n|                 |         |                | UNETR [27]   | 93.25        |\\n|                 |         |                | Swin-UNETR [26] | 93.42      |\\n|                 |         |                | MAE3D [28, 13] | UNETR 94.02 |\\n|                 |         |                | MoCo v3 [29, 12] | UNETR 93.86 |\\n|                 | Swin-UNETR |              | Jigsaw [9] | 95.24        |\\n|                 | Swin-UNETR |              | PositionLabel [68] | 94.13       |\\n|                 | 3D UNet |                | MG [72] | 91.30        |\\n|                 | 3D UNet |                | TransVW [24] | 91.42        |\\n|                 | 3D UNet |                | ROT [50] | 94.49        |\\n|                 | 3D UNet |                | PCRLv1 [70] | 93.87        |\\n|                 | 3D UNet |                | PCRLv2 [71] | 94.50        |\\n|                 | 3D UNet |                | Rubik [75] | 94.93        |\\n|                 | 3D UNet |                | Rubik++ [52] | 95.72        |\\n|                 | 3D UNet |                | Swin-UNETR [51, 26] | 95.33      |\\n|                 | 3D UNet |                | SwinMM [55] | 95.52        |\\n|                 | VoCo    |                | VoCo    | 96.03        |\\n|                 | VoCo    |                | Swin-UNETR | 96.52        |\\n\\nTable 1. Experimental results on BTCV [35]. The best results are bolded. 'From Scratch' denotes the supervised baseline without self-supervised pre-training. \u2020 denotes we re-implement the approach. Most results are drawn from [13, 65, 73] or their own papers.\\n\\n| Method          | Network | Dice Score (%) | From Scratch |\\n|-----------------|---------|----------------|--------------|\\n|                 |         |                | 3D UNet [46] | 93.71        |\\n|                 |         |                | UNETR [27]   | 94.20        |\\n|                 |         |                | Swin-UNETR [26] | 94.63        |\\n|                 |         |                | MAE3D [28, 13] | UNETR 95.20 |\\n|                 |         |                | MoCo v3 [29, 12] | UNETR 94.32 |\\n|                 |         |                | Jigsaw [9] | 94.29        |\\n|                 |         |                | PositionLabel [68] | 94.16       |\\n|                 | 3D UNet |                | MG [72] | 94.40        |\\n|                 | 3D UNet |                | VicRegl [3] | 94.12        |\\n|                 | 3D UNet |                | UniMiss [62] | 95.09        |\\n|                 | 3D UNet |                | PCRLv1 [70] | 94.32        |\\n|                 | 3D UNet |                | PCRLv2 [71] | 94.94        |\\n|                 | 3D UNet |                | Rubik++ [52] | 95.11        |\\n|                 | 3D UNet |                | Swin-UNETR [51, 26] | 95.02      |\\n|                 | 3D UNet |                | SwinMM [55] | 95.34        |\\n|                 | 3D UNet |                | JSSL [42] | 94.92        |\\n|                 | 3D UNet |                | GVSL [32] | 95.47        |\\n\\nTable 2. Experimental results on LiTs [4]. We report the Dice Scores of liver segmentation. \u2020 denotes we re-implement the approach. Most results are drawn from [65, 71].\\n\\nSpecifically, MoCo v3 [29, 12] can only achieve 79.54% Dice Score. Since MoCo v3 [29, 12] heavily relies on a large batch size to acquire adequate negative samples, which is not practical in 3D medical images due to the huge computation burden. In addition, the negative relation between different images used in MoCo [29, 12] is not appropriate in medical images. MAE [28, 13], SimCLR [10], and SimMIM [63] (results from [13]) also gain limited performance. Our VoCo also outperforms the position-based methods Jigsaw [9] and PositionLabel [68] by a clear margin. Thus, we conclude that general SSL methods are not very suitable for 3D medical images. It is crucial to consider the characteristics of medical images in medical SSL.\\n\\nThe scratch Swin-UNETR [26] only achieves 80.53% Dice Score.\"}"}
{"id": "CVPR-2024-2648", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method          | Network  | Accuracy (%) |\\n|-----------------|----------|--------------|\\n| From Scratch    |          |              |\\n| UNETR [27]      |          | 81.62        |\\n| Swin-UNETR [26] |          | 81.28        |\\n| With General SSL|          |              |\\n| MAE3D [28, 13]  | UNETR    | 82.34        |\\n| SimMIM [63]     | UNETR    | 84.06        |\\n| SimCLR [10]     | UNETR    | 83.13        |\\n| MoCo v3 [29, 12]| UNETR    | 82.60        |\\n| Jigsaw [9]      | Sw-UNE.  | 81.62        |\\n| PositionLabel [68]| Sw-UNE. | 81.35        |\\n| With Medical SSL|          |              |\\n| PCRLv1 [70]     | Swin-UNETR| 81.96        |\\n| PCRLv2 [71]     | Swin-UNETR| 82.13        |\\n| Rubik++ [52]    | Swin-UNETR| 84.32        |\\n| Swin-UNETR [51, 26]| Swin-UNETR | 82.51        |\\n| SwinMM [55]     | Swin-UNETR| 83.48        |\\n| VoCo            | Swin-UNETR| 85.27        |\\n\\nTable 4. Experimental results on BraTS 21 [49]. WT, TC, and ET denote the whole tumor, tumor core, and enhancing tumor, respectively. \u2020 denotes we re-implement the approach.\\n\\n| Method          | Network  | Dice Score (%) |\\n|-----------------|----------|----------------|\\n| From Scratch    |          |                |\\n| UNETR [27]      |          | 88.92          |\\n| Swin-UNETR [26] |          | 88.04          |\\n| With General SSL|          |                |\\n| MAE3D [28, 13]  | UNETR    | 89.47          |\\n| MoCo v3 [29, 12]| UNETR    | 84.95          |\\n| Jiasaw [9]      | Swin-UNETR| 86.88          |\\n| PositionLabel [68]| Swin-UNETR | 87.54          |\\n| With Medical SSL|          |                |\\n| PCRLv1 [70]     | Swin-UNETR| 88.72          |\\n| PCRLv2 [71]     | Swin-UNETR| 89.15          |\\n| Rubik++ [52]    | Swin-UNETR| 89.23          |\\n| Swin-UNETR [51, 26]| Swin-UNETR | 89.45          |\\n| SwinMM [55]     | Swin-UNETR| 89.61          |\\n| VoCo            | Swin-UNETR| 90.83          |\\n\\nTable 5. Experimental results of CC-CCII [67] classification. With VoCo pre-training, we gain 3.32% improvements with 83.85% Dice Score, which also outperforms existing methods by a clear margin. Among the compared methods, GL-MAE [73] achieves the highest Dice Score (82.01%). Our VoCo surpasses it by 1.84% Dice Score, which is a clear improvement in this dataset.\\n\\nPromising performance on Unseen datasets. We further conduct experiments on unseen datasets in pre-training, i.e., LiTs [4], MSD Spleen [1], and MM-WHS [74]. The results on LiTs [4] are shown in Table 2. We report the results of compared methods according to [70, 71, 65]. Since the scratch Swin-UNETR [26] can obtain a higher Dice Score (93.42%), we further pre-train a 3D UNet [46] based on VoCo, aiming to conduct fair comparisons. It can be seen that with VoCo pre-training, Swin-UNETR [26] gains 3.10% improvements and achieves 96.52% Dice Score. With 3D UNet [46] as the backbone, VoCo also achieves 96.03% Dice Score, proving the effectiveness of VoCo with different network architectures.\\n\\nFigure 5. Overall comparisons with state-of-the-art methods on six different datasets.\\n\\n| Loss Functions | BTCV | MM-WHS |\\n|----------------|------|--------|\\n| $L_{pred}$     | 80.53| 82.96  |\\n| $L_{reg}$      | 86.11| 88.82  |\\n\\nTable 6. Evaluation of loss functions $L_{pred}$ and $L_{reg}$. We report the average Dice Score on BTCV [35] and MM-WHS [74].\\n\\n| Number of bases | BTCV | MM-WHS |\\n|-----------------|------|--------|\\n| 2 \u00d7 2 \u00d7 1       | 81.56| 86.73  |\\n| 3 \u00d7 3 \u00d7 1       | 82.77| 89.31  |\\n| 4 \u00d7 4 \u00d7 1       | 83.85| 90.54  |\\n| 5 \u00d7 5 \u00d7 1       | 83.60| 90.49  |\\n| 3 \u00d7 3 \u00d7 2       | 82.86| 89.14  |\\n| 4 \u00d7 4 \u00d7 2       | 83.47| 90.52  |\\n\\nTable 7. Evaluation of the value of bases $n$. We report the average Dice Score on BTCV [35] and MM-WHS [74].\"}"}
{"id": "CVPR-2024-2648", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Qualitative visualization of segmentation results for the BTCV [35] dataset. We compare VoCo with MoCo v3 [12], PositionLabel [68], SwinUNETR [51, 26], and GVSL [32].\\n\\n4.4. Ablation Study\\n\\nWe further conduct ablation studies to evaluate the loss functions and the settings in VoCo, which are verified on the BTCV [35] and MM-WHS [74] datasets.\\n\\nLoss functions. We first study the importance of the two loss functions, i.e., $L_{\\\\text{pred}}$ and $L_{\\\\text{reg}}$, as shown in Table 6. It can be seen that with our proposed $L_{\\\\text{pred}}$ loss function, the performance is significantly improved, i.e., 80.53% to 82.96% on BTCV, 86.11% to 88.82% on MM-WHS. These results demonstrate the effectiveness of our proposed position prediction pretext task. In addition, with the proposed regularization loss $L_{\\\\text{reg}}$, the performance can be further improved. Thus, we can see that it is crucial to learn discriminative bases in VoCo.\\n\\nNumber of bases. We further evaluate different settings of the number of bases $n$ in VoCo. We compare with different settings of $n$ in the ablation studies, as shown in Table 7. It is worth noting that due to the ROI size inconsistency in the Z direction, it is not practical to crop multiple bases in the Z direction, since we have to resize the volume after crops, which will result in inconsistent volume scales. In addition, due to the computation limitation, it is costly to increase the values of $n$. As shown in Table 7, with $n = 2 \\\\times 2 \\\\times 1$, the VoCo only achieves 81.56% and 86.73% Dice Score on BTCV and MM-WHS, respectively. When we increase the values of $n$ to $3 \\\\times 3 \\\\times 1$ and $4 \\\\times 4 \\\\times 1$, the performances are improved obviously. Specifically, with $n$ as $4 \\\\times 4 \\\\times 1$, we achieve 83.85% and 90.54% on BTCV and MM-WHS, respectively. However, we observe that higher $n$ (e.g., $5 \\\\times 5 \\\\times 1$) cannot further bring higher performance. We further verify the performance of generating base crops in the Z direction. It can be seen that $3 \\\\times 3 \\\\times 2$ and $4 \\\\times 4 \\\\times 2$ cannot yield improvements. Thus, aiming to balance the performance and efficiency, we set $n$ as $4 \\\\times 4 \\\\times 1$ in VoCo. It can be seen that the setting of $n$ is crucial to VoCo.\\n\\nVisualization results on BTCV [35] are shown in Fig. 6. It can be seen that VoCo can yield improved segmentation accuracy and completeness. More visualization results are in the supplementary materials.\\n\\n5. Conclusion and Future Directions\\n\\nIn this paper, we develop a simple-yet-effective SSL framework VoCo for 3D medical image analysis. Motivated by the observation that 3D medical images contain relatively consistent contextual positions between different organs, we propose to leverage the contextual position priors to learn consistent semantic representations in pre-training. Specifically, we crop volumes from different positions in an input volume and represent them as a group of bases to represent features in different directions. Then, we predict the contextual position of a randomly cropped volume by contrasting its similarity to different bases. In this way, VoCo effectively encodes the contextual position priors into model representations, enabling us to effectively improve the performance of downstream tasks that require high-level semantics. Extensive experiments demonstrate that VoCo achieves superior performance.\\n\\nWe will further consider several ways of extension: (1) Scale up the pre-training dataset to evaluate the upper performance of VoCo. (2) Experiments on more downstream datasets. (3) Evaluate the label-efficient performance of VoCo (e.g., semi-supervised learning). (4) Explore the capacity of VoCo in mining inter-volume relationships.\\n\\nAcknowledgments\\n\\nThis work was supported by Hong Kong Innovation and Technology Fund (Project No. ITS/028/21FP and No. MHP/002/22), and Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. T45-401/22-N).\"}"}
{"id": "CVPR-2024-2648", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Michela Antonelli et al. The medical segmentation de-\\n[2] cathlon. Nature Commun., 13(1):4128, 2022. 5, 6, 7\\n\\n[2] Shekoofeh Azizi et al. Big self-supervised models advance\\n[1] medical image classification. In ICCV, pages 3478\u20133488,\\n[2] 2021. 1\\n\\n[3] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicregl: Self-\\n[1] supervised learning of local visual features. NIPS,\\n[2] 35:8799\u20138810, 2022. 6\\n\\n[4] Patrick Bilic et al. The liver tumor segmentation benchmark\\n[1] (lits). Medical Image Analy., 84:102680, 2023. 5, 6, 7\\n\\n[5] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and\\n[1] Matthijs Douze. Deep clustering for unsupervised learning\\n[2] of visual features. In ECCV, pages 132\u2013149, 2018. 2\\n\\n[6] Mathilde Caron et al. Unsupervised learning of visual fea-\\n[1] tures by contrasting cluster assignments. NIPS,\\n[2] 33:9912\u20139924, 2020. 1, 2, 4, 5\\n\\n[7] Mathilde Caron et al. Emerging properties in self-supervised\\n[2] vision transformers. In ICCV, pages 9650\u20139660, 2021. 2\\n\\n[8] Mathilde Caron, Neil Houlsby, and Cordelia Schmid. Location-aware self-supervised transformers.\\n[2] arXiv preprint arXiv:2212.02400, 2022. 3\\n\\n[9] Pengguang Chen, Shu Liu, and Jiaya Jia. Jigsaw clustering\\n[3] for unsupervised visual representation learning. In CVPR,\\n[3] pages 11526\u201311535, 2021. 3, 5, 6, 7\\n\\n[10] Ting Chen et al. A simple framework for contrastive learning\\n[1] of visual representations. In ICML, pages 1597\u20131607, 2020.\\n[1] 1, 2, 3, 4, 5, 6, 7\\n\\n[11] Xinlei Chen and Kaiming He. Exploring simple siamese rep-\\n[1] resentation learning. In CVPR, pages 15750\u201315758, 2021. 1,\\n[1] 2, 3, 4\\n\\n[12] Xinlei Chen, Saining Xie, and Kaiming He. An empirical\\n[3] study of training self-supervised vision transformers.\\n[2] arXiv preprint arXiv:2104.02057, 2021. 5, 6, 7, 8\\n\\n[13] Zekai Chen et al. Masked image modeling advances 3d med-\\n[1] ical image analysis. In WACV, pages 1970\u20131980, 2023. 1, 3,\\n[1] 5, 6, 7\\n\\n[14] Kenneth Clark and Bruceand others Vendt. The cancer imag-\\n[5] ing archive (tcia): maintaining and operating a public infor-\\n[5] mation repository. Jour. of Dig. Imag., 26:1045\u20131057, 2013.\\n[5]\\n\\n[15] Jiequan Cui et al. Parametric contrastive learning. In ICCV,\\n[2] pages 715\u2013724, 2021. 2\\n\\n[16] Jiequan Cui et al. Generalized parametric contrastive learn-\\n[1] ing. IEEE Trans. Pattern Analy. Mach. Intell., 2023. 2\\n\\n[17] Carl Doersch, Abhinav Gupta, and Alexei A Efros. Unsuper-\\n[3] vised visual representation learning by context prediction. In ICCV,\\n[3] pages 1422\u20131430, 2015. 3\\n\\n[18] Alexey Dosovitskiy et al. An image is worth 16x16 words:\\n[3] Transformers for image recognition at scale. ICLR,\\n[3] 2020. 3\\n\\n[19] Hao Du, Qihua Dong, Yan Xu, and Jing Liao. Weakly-\\n[3] supervised 3d medical image segmentation using geometric\\n[3] prior and contrastive similarity. IEEE Trans. Medi. Imag.,\\n[3] 2023. 3\\n\\n[20] Yuting Gao, Jia-Xin Zhuang, et al. Disco: Remedying self-\\n[1] supervised learning on lightweight models with distilled con-\\n[1] trastive learning. In ECCV, pages 237\u2013253, 2022. 1\\n\\n[21] Florin-Cristian Ghesu et al. Multi-scale deep reinforcement\\n[1] learning for real-time 3d-landmark detection in ct scans.\\n[1] IEEE Trans. Pattern Anal. Mach. Intell., 41(1):176\u2013189,\\n[1] 2017. 1\\n\\n[22] Jean-Bastien Grill et al. Bootstrap your own latent-a new ap-\\n[2] proach to self-supervised learning. NIPS,\\n[2] 33:21271\u201321284,\\n[2] 2020. 2\\n\\n[23] Katharina Gr\u00a8unberg et al. Annotating medical image data.\\n[1] Medical Image Analy., pages 45\u201367, 2017. 1\\n\\n[24] Fatemeh Haghighi et al. Transferable visual words: Exploit-\\n[6] ing the semantics of anatomical patterns for self-supervised\\n[6] learning. IEEE Trans. Medical Imag., 40(10):2857\u20132868,\\n[6] 2021. 6\\n\\n[25] Fatemeh Haghighi et al. Dira: Discriminative, restorative,\\n[3] and adversarial learning for self-supervised medical image\\n[3] analysis. In CVPR, pages 20824\u201320834, 2022. 1, 3\\n\\n[26] Ali Hatamizadeh et al. Swin unetr: Swin transformers for\\n[5] semantic segmentation of brain tumors in mri images. In MICCAIW,\\n[5] pages 272\u2013284, 2021. 5, 6, 7, 8\\n\\n[27] Ali Hatamizadeh et al. Unetr: Transformers for 3d medical\\n[5] image segmentation. In WACV, pages 574\u2013584, 2022. 5, 6,\\n[5] 7\\n\\n[28] Kaiming He et al. Masked autoencoders are scalable vision\\n[1] learners. In CVPR, pages 16000\u201316009, 2022. 1, 5, 6, 7\\n\\n[29] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross\\n[5] Girshick. Momentum contrast for unsupervised visual rep-\\n[5] resentation learning. In CVPR, pages 9729\u20139738, 2020. 2,\\n[5] 3, 5, 6, 7\\n\\n[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\n[3] Deep residual learning for image recognition. In CVPR,\\n[3] pages 770\u2013778, 2016. 3\\n\\n[31] Xingxin He, Leyuan Fang, Mingkui Tan, and Xiangdong\\n[3] Chen. Intra-and inter-slice contrastive learning for point su-\\n[3] pervised oct fluid segmentation. IEEE Trans. Image Pro-\\n[3] cess., 31:1870\u20131881, 2022. 3\\n\\n[32] Yuting He et al. Geometric visual similarity learning in 3d\\n[1] medical image self-supervised pre-training. In CVPR,\\n[1] pages 9538\u20139547, 2023. 1, 3, 5, 6, 7, 8\\n\\n[33] Fabian Isensee, Paul F Jaeger, Simon AA Kohl, Jens Pe-\\n[5] tersen, and Klaus H Maier-Hein. nnu-net: a self-configuring\\n[5] method for deep learning-based biomedical image segmen-\\n[5] tation. Nature Methods, 18(2):203\u2013211, 2021. 1\\n\\n[34] Yankai Jiang et al. Anatomical invariance modeling and se-\\n[5] mantic alignment for self-supervised learning in 3d medical\\n[5] image analysis. In ICCV, pages 15859\u201315869, 2023. 1, 5\\n\\n[35] Bennett Landman et al. Miccai multi-atlas labeling beyond\\n[5] the cranial vault\u2013workshop and challenge. In MICCAIW,\\n[5] volume 5, page 12, 2015. 5, 6, 7, 8\\n\\n[36] Jie Liu et al. Clip-driven universal model for organ segmen-\\n[5] tation and tumor detection. In ICCV, pages 21152\u201321164,\\n[5] 2023. 1, 5\\n\\n[37] Qiang Liu et al. A multi-level label-aware semi-supervised\\n[3] framework for remote sensing scene classification. IEEE\\n[3] Trans. Geosci. Remote Sens., 2023. 3\\n\\n[38] Ilya Loshchilov and Frank Hutter. Decoupled weight decay\\n[5] regularization. arXiv preprint arXiv:1711.05101, 2017. 5\"}"}
{"id": "CVPR-2024-2648", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Xiangde Luo et al. WORD: A large scale dataset, benchmark and clinical applicable study for abdominal organ segmentation from CT image. Medical Image Analysis, 82:102642, 2022.\\n\\nJun Ma et al. AbdomenCT-1K: Is abdominal organ segmentation a solved problem? IEEE Trans. Pattern Anal. Mach. Intell., 44(10):6695\u20136714, 2021.\\n\\nT Nathan Mundhenk, Daniel Ho, and Barry Y Chen. Improvements to context based self-supervised learning. In CVPR, pages 9339\u20139348, 2018.\\n\\nNguyen et al. Joint self-supervised image-volume representation learning with intra-inter contrastive clustering. In AAAI, pages 14426\u201314435, 2023.\\n\\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In ECCV, pages 69\u201384. Springer, 2016.\\n\\nAaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\\n\\nMaxime Oquab et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In MICCAI, pages 234\u2013241, 2015.\\n\\nRodrigo Santa Cruz et al. Deeppermnet: Visual permutation learning. In CVPR, pages 3949\u20133957, 2017.\\n\\nArnaud Arindra Adiyoso Setio et al. Validation, comparison, and combination of algorithms for automatic detection of pulmonary nodules in computed tomography images: the Luna16 challenge. Medical Image Analysis, 42:1\u201313, 2017.\\n\\nAmber L Simpson et al. A large annotated medical image dataset for the development and evaluation of segmentation algorithms. arXiv preprint arXiv:1902.09063, 2019.\\n\\nAiham Taleb et al. 3D self-supervised methods for medical imaging. NIPS, 33:18158\u201318172, 2020.\\n\\nYucheng Tang et al. Self-supervised pre-training of Swin Transformers for 3D medical image analysis. In CVPR, pages 20730\u201320740, 2022.\\n\\nXing Tao et al. Revisiting Rubik\u2019s Cube: Self-supervised learning with volume-wise transformation for 3D medical image segmentation. In MICCAI, pages 238\u2013248, 2020.\\n\\nGuotai Wang et al. Deepigeos: A deep interactive geodesic framework for medical image segmentation. IEEE Trans. Pattern Anal. Mach. Intell., 41(7):1559\u20131572, 2018.\\n\\nXiaosong Wang et al. ChestX-ray8: Hospital-scale chest x-ray database and benchmarks on weakly-supervised classification and localization of common thorax diseases. In CVPR, pages 2097\u20132106, 2017.\\n\\nYiqing Wang et al. SwinMM: Masked multi-view with Swin Transformers for 3D medical image segmentation. In MICCAI, 2023.\\n\\nXin Wen et al. Self-supervised visual representation learning with semantic grouping. NIPS, 35:16423\u201316438, 2022.\\n\\nLinshan Wu et al. Deep bilateral filtering network for point-supervised semantic segmentation in remote sensing images. IEEE Trans. Image Process., 31:7419\u20137434, 2022.\\n\\nLinshan Wu et al. Modeling the label distributions for weakly-supervised semantic segmentation, 2024.\\n\\nLinshan Wu, Leyuan Fang, Xingxin He, Min He, Jiayi Ma, and Zhun Zhong. Querying labeled for unlabeled: Cross-image semantic consistency guided semi-supervised semantic segmentation. IEEE Trans. Pattern Anal. Mach. Intell., 45(7):8827\u20138844, Jul. 2023.\\n\\nLinshan Wu, Ming Lu, and Leyuan Fang. Deep covariance alignment for domain adaptive remote sensing image segmentation. IEEE Trans. Geosci. Remote Sens., 60:1\u201311, 2022.\\n\\nLinshan Wu, Zhun Zhong, Leyuan Fang, Xingxin He, Qiang Liu, Jiayi Ma, and Hao Chen. Sparsely annotated semantic segmentation with adaptive Gaussian mixtures. In CVPR, pages 15454\u201315464, 2023.\\n\\nYutong Xie, Jianpeng Zhang, Yong Xia, and Qi Wu. Unimiss: Universal medical self-supervised learning via breaking dimensionality barrier. In ECCV, pages 558\u2013575, 2022.\\n\\nZhenda Xie et al. Simmim: A simple framework for masked image modeling. In CVPR, pages 9653\u20139663, 2022.\\n\\nShuangfei Zhai et al. Position prediction as an effective pre-training strategy. arXiv preprint arXiv:2207.07611, 2022.\\n\\nChuyan Zhang, Hao Zheng, and Yun Gu. Dive into the details of self-supervised learning for medical image analysis. Medical Image Analysis, 89:102879, 2023.\\n\\nHongyi Zhang et al. Mixup: Beyond empirical risk minimization. In ICLR, 2018.\\n\\nKang Zhang et al. Clinically applicable AI system for accurate diagnosis, quantitative measurements, and prognosis of COVID-19 pneumonia using computed tomography. Cell, 181(6):1423\u20131433, 2020.\\n\\nZhemin Zhang and Xun Gong. Positional label for self-supervised vision transformer. In AAAI, pages 3516\u20133524, 2023.\\n\\nHong-Yu Zhou et al. Comparing to learn: Surpassing imagenet pretraining on radiographs by comparing image representations. In MICCAI, pages 398\u2013407, 2020.\\n\\nHong-Yu Zhou et al. Preservational learning improves self-supervised medical image models by reconstructing diverse contexts. In ICCV, pages 3499\u20133509, 2021.\\n\\nHong-Yu Zhou et al. A unified visual information preservation framework for self-supervised pre-training in medical image analysis. IEEE Trans. Pattern Anal. Mach. Intell., 2023.\\n\\nZongwei Zhou et al. Models genesis. Medical Image Analysis, 67:101840, 2021.\\n\\nJia-Xin Zhuang, Luyang Luo, and Hao Chen. Advancing volumetric medical image segmentation via global-local masked autoencoder. arXiv preprint arXiv:2306.08913, 2023.\\n\\nXiahai Zhuang. Multivariate mixture model for myocardial segmentation combining multi-source images. IEEE Trans. Pattern Anal. Mach. Intell., 41(12):2933\u20132946, 2018.\\n\\nXinrui Zhuang et al. Self-supervised feature learning for 3D medical images by playing a Rubik\u2019s Cube. In MICCAI, pages 420\u2013428, 2019.\"}"}
{"id": "CVPR-2024-2648", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VoCo: A Simple-yet-Effective Volume Contrastive Learning Framework for 3D Medical Image Analysis\\n\\nLinshan Wu Jiaxin Zhuang\\nHong Kong University of Science and Technology\\nHao Chen*\\n\\nAbstract\\nSelf-Supervised Learning (SSL) has demonstrated promising results in 3D medical image analysis. However, the lack of high-level semantics in pre-training still heavily hinders the performance of downstream tasks. We observe that 3D medical images contain relatively consistent contextual position information, i.e., consistent geometric relations between different organs, which leads to a potential way for us to learn consistent semantic representations in pre-training. In this paper, we propose a simple-yet-effective VoCo framework to leverage the contextual position priors for pre-training. Specifically, we first generate a group of base crops from different regions while enforcing feature discrepancy among them, where we employ them as class assignments of different regions. Then, we randomly crop sub-volumes and predict them belonging to which class (located at which region) by contrasting their similarity to different base crops, which can be seen as predicting contextual positions of different sub-volumes. Through this pretext task, VoCo implicitly encodes the contextual position priors into model representations without the guidance of annotations, enabling us to effectively improve the performance of downstream tasks that require high-level semantics. Extensive experimental results on six downstream tasks demonstrate the superior effectiveness of VoCo. Code will be available at https://github.com/Luffy03/VoCo.\\n\\n1. Introduction\\nDeep learning has demonstrated outstanding achievements in 3D medical image analysis [53, 21, 40, 33, 39], yet is heavily hampered by the expensive cost of the required expert annotations [50, 23]. To address this problem, Self-Supervised Learning (SSL) has received significant attention due to its promising ability to learn representations without annotations [10, 11, 6, 28, 20], which has become an important label-efficient solution in 3D medical image analysis [71, 51, 32, 2, 34, 36].\\n\\nExisting methods [50, 75, 71, 13] are mostly based on information reconstructions to learn augment-invariant representations of 3D medical images, which first employ strong data augmentation to the images and then reconstruct the raw information. Specifically, rotate-and-reconstruct [50, 51, 75, 52] proposed to randomly rotate the 3D volumetric images and learn to recover them, which encourages models to learn rotational invariant features. Recent methods [70, 71, 32, 25, 62] further proposed to restore information among different views of the image. PCRL [70, 71] cropped global and local patches then conducted multi-scale restorations. GVSL [32] further explored the geometric similarity between multi-scans by affine augmentation and matching. Mask-reconstruct methods [13, 73, 55] are also widely used, which are introduced from MAE [28] and aim to learn representations by masking images and reconstructing the missing pixels. Although promising results have been demonstrated, previous works [52, 32] have...\"}"}
{"id": "CVPR-2024-2648", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Typical contrastive learning frameworks. (a) Instance-level contrastive learning [10, 11, 29, 22, 7] employs strong data augmentation or model perturbation on input data to acquire different views of instance, then regularizes their consistency. (b) Prototype-level contrastive learning [5, 6, 56, 45, 15, 16] conducts (1) online clustering or (2) randomly initialize then online update process to obtain prototypes as class assignments, then leverage the prototypes to contrast each input image. (c) Our VoCo follows the idea of prototype-level contrastive learning. Specifically, instead of using time-consuming online clustering and updating procedures, we leverage the valuable contextual position priors of 3D medical images and leverage the base crops to generate prototypes (bases).\\n\\nProved that the lack of high-level semantics in pre-training will heavily hinder the performance of downstream tasks. To address this challenge, we argue that stronger high-level semantics should be further involved into 3D medical image pre-training.\\n\\nTo this end, we argue that the contextual position priors of 3D medical images should be further exploited. As shown in Fig. 1(a), we observe that in 3D medical images, different organs (semantic regions) contain relatively consistent contextual positions with relatively consistent anatomic characteristics (shapes). Thus, the consistency of geometric relations between different organs leads to a potential way for us to learn consistent semantic representations for 3D medical images pre-training. In this paper, we propose a pretext task for contextual position predictions, which aims to encode contextual position priors into model representations and enables us to effectively improve the performance of downstream tasks that require high-level semantics.\\n\\nIn this paper, we propose a simple-yet-effective Volume Contrast (VoCo) framework for 3D medical image analysis, as shown in Fig. 1(b). Specifically, we first crop a group of non-overlap volumes from different positions while enforcing feature discrepancy among them. We represent these volumes as a group of bases in the learned high-dimension space, where we employ them as class assignments of different positions. Then, we randomly crop sub-volumes and predict them belonging to which class (located at which position) by contrasting their similarity to different bases, which can be seen as predicting contextual positions of different sub-volumes. In this way, we formulate a contextual position prediction pretext task for 3D medical image SSL. Through learning to predict contextual positions, we implicitly involve the high-level semantic priors into the model representations, which enables us to significantly improve the performance of downstream tasks. Extensive experimental results on six downstream tasks demonstrate that our proposed VoCo clearly outperforms existing state-of-the-art 3D medical image SSL methods.\\n\\n2. Related Works\\n\\nIn this section, we first introduce the previous mainstream contrastive learning paradigms. Then, we survey the existing SSL methods for medical image analysis, especially for 3D medical images. Finally, we review the position-related SSL methods for comparisons with our method and highlight the differences.\\n\\nContrastive learning. Contrastive learning is one of the mainstream paradigms in SSL, which aims to learn consistent representations by contrasting positive and negative pairs of samples without extra annotations [10, 6, 29, 11]. According to [6], instance- and prototype-level contrastive learning are two typical types of contrastive learning, as shown in Fig. 2.\\n\\nInstance-level contrastive learning [10, 11, 29, 22, 7] transforms input images with different augmentations or model perturbations, aiming to compare the features from each other. Prototype-level contrastive learning [5, 6, 56, 45, 15, 16] proposes to generate prototypes (also called clusters or bases) for contrasting each input image. Specifically, there are two typical ways to generate prototypes. First, Caron et al. proposed DeepCluster [5] to conduct online clustering on the whole dataset to generate prototypes. However, it is very time-consuming to calculate clusters on a large dataset. Thus, some recent works [6, 56, 15, 16] propose to randomly initialize a group of prototypes and then update them through back-propagation during training, which has demonstrated promising results. However, there is still no explicit guarantee that these randomly initialized prototypes can be updated well during training.\\n\\nOur VoCo follows the primary idea of prototype-level contrastive learning. As shown in Fig. 2(c), to address the existing problems mentioned above, instead of randomly initializing prototypes, we leverage the valuable contextual position priors of 3D medical images and leverage the base crops to generate prototypes (bases).\"}"}
{"id": "CVPR-2024-2648", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VoCo leverages the valuable contextual position priors of 3D medical images to generate base crops as prototypes, which also requires no time-consuming clustering on a large dataset. Due to the high potential in label-efficient learning, SSL has also received significant attention in the field of medical image analysis. Existing methods are mainly based on comparative SSL. Specifically, Zhou et al. combined Mixup into MoCo to learn the diversity of positive and negative samples in InfONCE. Azizi et al. used multi-instance learning to compare multiple views of images from each patient. There are also a number of approaches that supervising the models via restoring low-level information from raw images.\\n\\nIn 3D medical image analysis, reconstructing raw information is a popular pretext task for learning representations. Existing methods are mainly based on reconstructing information from augmented images. These previous methods first conducted strong data augmentation, e.g., rotate, multi-view crops, and mask, then supervised the model by reconstructing raw 3D information. Although promising results have been demonstrated, most of these methods still largely ignore the importance of integrating high-level semantics into model representations, which heavily hinders the performance of downstream tasks.\\n\\nPosition-related SSL methods are also explored in a number of previous works in the field of natural images. Noroozi et al. proposed to predict the order of a set of shuffled patches. Zhai et al. and Caron et al. proposed to train a ViT to predict the locations of each input patch. However, since the geometric relations of different objects are not very consistent in natural images, it is still difficult to effectively learn consistent position representations given visual appearance only. In addition, previous works mainly trained a linear layer to output the positions directly, which works in a black-box manner.\\n\\nIn this paper, we introduce the pretext task of contextual position prediction into the field of 3D medical images, where the geometric relations between different organs are relatively consistent, which guides us to learn consistent semantic representations in pre-training. Different from the previous methods, in this paper, we introduce a totally different position prediction paradigm. Specifically, instead of using a linear layer to output positions directly, we predict the contextual positions based on volume contrast, which is more intuitive and effective.\\n\\n### 3.1. Overall Framework\\n\\nThe overall framework of our proposed VoCo is presented in Fig. 3, which contains a contextual position prediction branch and a regularization branch. The prediction branch is used to predict the contextual positions between different cropped volumes. Specifically, given an input volume, we first crop it into non-overlap base volumes, which cover the whole input volume. Then, we randomly crop a volume and transform it into the high-dimension feature space using a typical backbone (CNN or Transformer). The goal is to predict the contextual positions between the randomly cropped volumes and base volumes.\\n\\nIn this paper, instead of training a linear classifier to predict positions as in previous works, we propose to establish this goal by volume contrast. We develop a loss function $L_{\\\\text{pred}}$ to supervise the final predictions. In addition, we further use a loss function $L_{\\\\text{reg}}$ to regularize the feature discrepancy from different bases by enlarging their distance, aiming to learn more discriminative class assignments. The details are presented in Section 3.2 and 3.3.\\n\\n### 3.2. Contextual Position Prediction\\n\\n**Base and random crops.** Given an input volume, we first crop it into $n$ non-overlap base volumes, which cover the whole input volume. We then employ the extracted features $z$ as class assignments (we call them bases), which present the prototype-level features from different positions. Then, following previous SSL works, a projector with linear layers is used to project $z$ into latent features $q$. Then, we randomly crop a volume and transform it into high-dimension feature space as $p$. The backbone and projector are also used to project the features from the randomly cropped volumes.\\n\\n**Volume contrast for contextual position prediction.** With features extracted from the backbone and projector, following previous SSL works, we first conduct 3D adaptive average pooling to resize them to one dimension, i.e., $p \\\\in \\\\mathbb{R}^{1 \\\\times C}$ and $q \\\\in \\\\mathbb{R}^{1 \\\\times C}$, where $C$ is the number of channels. Specifically, we empirically set $C$ to 2048 as in [10, 11, 29]. Then, we calculate the similarity logits $l_i$ between $p$ and $q_i$. Specifically, we use cosine similarity to compute $l$ as follows:\\n\\n$$l_i = \\\\text{CosSim}(p, q_i) = \\\\frac{p \\\\cdot q_i}{\\\\|p\\\\| \\\\|q_i\\\\|},$$\\n\\nwhere $i \\\\in n$. In this paper, instead of training a linear classifier to predict positions as in previous works, we propose to establish this goal by volume contrast. We develop a loss function $L_{\\\\text{pred}}$ to supervise the final predictions. In addition, we further use a loss function $L_{\\\\text{reg}}$ to regularize the feature discrepancy from different bases by enlarging their distance, aiming to learn more discriminative class assignments. The details are presented in Section 3.2 and 3.3.\"}"}
{"id": "CVPR-2024-2648", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. The overall framework of VoCo. VoCo contains a prediction branch and a regularization branch. The prediction branch is responsible for predicting contextual positions between different sub-volumes. The regularization branch is employed to enforce the feature discrepancy between different bases, which aims to learn more discriminative class assignments.\\n\\nwhere \\\\( q_i \\\\) is the projected feature of each base crop. \\\\( l_i \\\\) denotes the similarity between \\\\( p \\\\) in \\\\( q_i \\\\), which ranges from 0 to 1. It is worth noting that, we stop the gradients of \\\\( q \\\\) when computing Eq. 1, which aims to avoid feature collapse [10, 11, 6].\\n\\nIntuitively, higher \\\\( l_i \\\\) represents that \\\\( p \\\\) has higher probabilities to share overlap regions with \\\\( q_i \\\\). In this way, we can explicitly associate the similarity value with the position information, i.e., \\\\( p \\\\) with higher \\\\( l_i \\\\) is more likely to be located in the region of the \\\\( i \\\\)th base. Thus, instead of training a black-box linear layer, we predict the contextual positions by volume contrast, which is more intuitive and effective.\\n\\nPosition labels generation. The process of generating position labels is shown in Fig. 4. As shown in Fig. 4, when we generate \\\\( n = 4 \\\\times 4 \\\\) base crops, there will be \\\\( n \\\\) class assignments. Then we calculate the overlap area between a randomly cropped volume and \\\\( n \\\\) base crops. The proportions of the overlap area are then assigned as position labels \\\\( y \\\\), which also range from 0 to 1. Thus, we can easily supervise the model by calculating the distance between the prediction logits \\\\( l \\\\) and position labels \\\\( y \\\\). The setting of the number \\\\( n \\\\) of base crops will be discussed in Section 4.4.\\n\\nLoss function for contextual position prediction. The formulation of prediction loss function \\\\( L_{\\\\text{pred}} \\\\) is based on entropy. Specifically, we first calculate the distance \\\\( d \\\\) between prediction logits \\\\( l \\\\) and position labels \\\\( y )$\\n\\n$$d_i = |y_i - l_i|, i \\\\in n,$$\\n\\n(2)\\n\\nwhere \\\\(|\\\\cdot|\\\\) denotes the absolute value. Then, \\\\( L_{\\\\text{pred}} \\\\) is formulated as follows:\\n\\n$$L_{\\\\text{pred}} = -\\\\frac{1}{n} \\\\sum_{i \\\\in n} \\\\log (1 - d_i).$$\\n\\n(3)\\n\\nIt is worth noting that VoCo predicts contextual positions of a volume (high similarities with all its contextual overlapped volumes), thus don't need one-to-one correspondence: e.g., in Fig. 4, high-value \\\\( l_i \\\\) pertain to \\\\( y_i > 0 \\\\), \\\\( i = 5, 6, 9, 10 \\\\) simultaneously. Then we calculate the distance between \\\\( l_i \\\\) and \\\\( y_i \\\\) (Eq. 2).\\n\\n3.3. Volume Contrast for Regularization\\n\\nWe aim to learn more discriminative class assignments (bases) for volume contrast. Since intuitively, different sub-volumes tend to contain different organs (semantic discrepancies...\"}"}
