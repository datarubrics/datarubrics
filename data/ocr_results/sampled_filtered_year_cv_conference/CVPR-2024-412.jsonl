{"id": "CVPR-2024-412", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Constructing and Exploring Intermediate Domains in Mixed Domain Semi-supervised Medical Image Segmentation\\n\\nQinghe Ma\\\\textsuperscript{1}, Jian Zhang\\\\textsuperscript{1}, Lei Qi\\\\textsuperscript{2}, Qian Yu\\\\textsuperscript{3}, Yinghuan Shi\\\\textsuperscript{1},* Yang Gao\\\\textsuperscript{1}\\n\\n\\\\textsuperscript{1}Nanjing University \\\\textsuperscript{2}Southeast University \\\\textsuperscript{3}Shandong Women's University\\n\\nAbstract\\n\\nBoth limited annotation and domain shift are prevalent challenges in medical image segmentation. Traditional semi-supervised segmentation and unsupervised domain adaptation methods address one of these issues separately. However, the coexistence of limited annotation and domain shift is quite common, which motivates us to introduce a novel and challenging scenario: Mixed Domain Semi-supervised medical image Segmentation (MiDSS). In this scenario, we handle data from multiple medical centers, with limited annotations available for a single domain and a large amount of unlabeled data from multiple domains. We found that the key to solving the problem lies in how to generate reliable pseudo labels for the unlabeled data in the presence of domain shift with labeled data. To tackle this issue, we employ Unified Copy-Paste (UCP) between images to construct intermediate domains, facilitating the knowledge transfer from the domain of labeled data to the domains of unlabeled data. To fully utilize the information within the intermediate domain, we propose a symmetric Guidance training strategy (SymGD), which additionally offers direct guidance to unlabeled data by merging pseudo labels from intermediate samples. Subsequently, we introduce a Training Process aware Random Amplitude MixUp (TP-RAM) to progressively incorporate style-transition components into intermediate samples. Compared with existing state-of-the-art approaches, our method achieves a notable 13.57% improvement in Dice score on Prostate dataset, as demonstrated on three public datasets. Our code is available at https://github.com/MQinghe/MiDSS\\n\\n*Corresponding author. Qinghe Ma, Jian Zhang, Yinghuan Shi and Yang Gao are with the State Key Laboratory for Novel Software Technology and National Institute of Healthcare Data Science, Nanjing University, China. This work was supported by the NSFC Program (62222604, 62206052, 62192783), Jiangsu Natural Science Foundation (BK20210224), and Shandong Natural Science Foundation (ZR2023MF037).\\n\\nLimited labeled data Domain shift Both challenge\\n\\nSSMS UDA MiDSS (ours)\\n\\nLabeled data Unlabeled data Testing data Domain shift\\n\\nFixMatch BCP\\n\\n100 80 60\\n\\nDice (%)\\n\\nSIFA UDA-VAE++ Ours\\n\\nTest performance of labeled domain Average test performance of other domains\\n\\nFigure 1. The upper figure illustrates SSMS, UDA, and MiDSS. The lower figure shows the comparison between different methods on the labeled domain (BIDMC) \\\\cite{23} and other domains.\\n\\n1. Introduction\\n\\nSemi-supervised medical image segmentation (SSMS) has aroused lots of attention in recent years, due to its advantage of improving performance while reducing the labeling burden \\\\cite{13, 20, 21, 43}. Surprisingly, the performance of semi-supervised methods \\\\cite{12, 22} is sometimes close to or even better than that of its upper bound\u2014supervised methods \\\\cite{8, 32}. With 1) only a few labeled data as guidance, and 2) the development of unsupervised learning techniques, e.g., contrastive learning \\\\cite{18}, SSMS is becoming a promising direction to utilize unlabeled data. In traditional SSMS tasks, it is typically assumed that labeled and unlabeled data come from the same distribution \\\\cite{1, 3, 42}. In this case, the model applies the information gained from labeled data directly to predict pseudo labels for unlabeled data.\\n\\nDespite their success, we notice a significant yet easily underestimated issue: During clinical data collection, do we really check and guarantee, whether each unlabeled sample belongs to the same distribution as labeled samples? For example, given a certain amount of labeled data collected and well annotated from a typical center, since unlabeled data are easily accessed and collected, when the\"}"}
{"id": "CVPR-2024-412", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"physicians have a massive number of unlabeled data, they possibly lack sufficient time to verify whether the data belongs to the same distribution \\\\[10, 29\\\\].\\n\\nAs this dilemma mentioned above, which refers to \\\"domain shift\\\" \\\\[16, 38\\\\], could cause a large performance degeneration, we should pay more attention to this issue during our development of a general SSMS model. A more practical setting could be: given labeled data from a consistent distribution, the unlabeled data is allowed to come from the same distribution or even multiple different distributions. It is important to note that this issue could not be directly solved by current unsupervised domain adaptation (UDA) methods. Although UDA is promising to address domain shift, these methods \\\\[6, 11, 30, 41\\\\] demand abundant labeled source domain data, which is the primary reason for the existence of SSMS.\\n\\nDespite the significance of the domain shift issue in SSMS, according to our best knowledge, our work is first to attempt to investigate this issue, which we termed as Mixed Domains Semi-supervised medical image Segmentation (abreviated as MiDSS in our paper). As illustrated in Fig. 1, confronted with the coexistence of limited annotation and domain shift, it is challenging to effectively utilize a large quantity of unlabeled medical images from multiple domains. Additionally, from the lower figure in Fig. 1, we found that achieving satisfactory results in our setting is difficult when relying solely on SSMS or UDA methods.\\n\\nThe key to addressing the problem lies in how to generate reliable pseudo labels for unlabeled data, especially in the presence of domain shift. However, the domain gap among training samples results in the model producing poor pseudo labels for unlabeled data. Thus, our key insight is first to construct several intermediate domains among original domains to narrow the domain gap, and then emphasize knowledge transfer approaches upon these constructed domains. CutMix \\\\[44\\\\], also referred to as Copy-Paste (CP), is a simple yet highly effective technique for generating intermediate samples by embedding patches from labeled samples into unlabeled samples along a single direction. To fully leverage the data generation capability of CP, we employ a Unified Copy-Paste (UCP) approach. UCP generates a great quantity of diverse intermediate samples by unified applying both embedding directions between labeled and unlabeled samples, narrowing the domain gap and effectively mitigating the issue of error accumulation.\\n\\nBesides, the intermediate domains may not align perfectly with the domains of the unlabeled data. Solely training within the intermediate domains neglects the model performance on the unlabeled data, which are of great interest. The information from intermediate domains should be harnessed to promote the model to generate high quality pseudo labels for unlabeled data. Moreover, medical images of the same organ or lesion often exhibit similar structures \\\\[37\\\\], with stylistic differences being the primary source of domain shift \\\\[24\\\\]. Local semantic mixing methods (e.g., CP) fail to address these differences, leading to a lack of stylistic transition in intermediate domains. Additionally, it is crucial that an aggressive stylistic transition hinders the gradual construction of intermediate domains. Therefore, to fully harness the information from intermediate domains and ensure comprehensive and stable knowledge transfer, we propose a Mixed Domains Semi-supervised medical image segmentation approach. In terms of training strategy, we design Symmetric Guidance training strategy (SymGD), which is composed of guidance from unlabeled data to intermediate samples and vice versa. This dual-perspective integration of pseudo labels enhances the precision of guidance. Concerning intermediate samples, we introduce a Training Process aware Random Amplitude MixUp module (TP-RAM) to promote smooth stylistic knowledge transfer. Our main contributions are three folds:\\n\\n\u2022 We investigate a new yet underestimated semi-supervised medical image segmentation setting (namely MiDSS).\\n\u2022 A novel SymGD training strategy based on UCP, promotes the training on the unlabeled data with intermediate domains information.\\n\u2022 A TP-RAM module to make domain knowledge transfer comprehensive and stable.\\n\\nExtensive experiments validate the effectiveness of our method on three public datasets. For example, on Prostate dataset, we achieved a remarkable Dice score improvement of 13.57% compared with all state-of-the-art methods.\\n\\n2. Related Work\\n\\nSemi-supervised Medical Image Segmentation.\\nManually annotating medical images is challenging and costly \\\\[49\\\\]. Semi-supervised medical image segmentation methods have shown promise in addressing limited-label segmentation tasks. Entropy minimization and consistency regularization are widely adopted techniques in this context. Yu et al. \\\\[43\\\\] encouraged consistent predictions under different perturbations. Luo et al. \\\\[26\\\\] proposed a dual-task network that predicts pixel-wise segmentation maps and geometry-aware level set representations. Li et al. \\\\[19\\\\] introduced a shape-aware strategy using a multi-task deep network that jointly predicts semantic segmentation and signed distance maps. Wu et al. \\\\[40\\\\] addressed challenges in SSMS by simultaneously enforcing pixel-level smoothness and inter-class separation. Miao et al. \\\\[27\\\\] pointed out the importance of algorithmic independence between two networks or branches in SSMS. Unfortunately, existing methods often face challenges when domain shift occurs, causing decreased performance due to the shared distribution assumption between labeled and unlabeled data.\\n\\nUnsupervised Domain Adaptation.\\nUDA methods train the model by leveraging abundant labeled source domain data.\"}"}
{"id": "CVPR-2024-412", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. The overall framework of our method emphasizes domain knowledge transfer through data augmentation and training strategy. We generate intermediate samples through UCP between labeled data and unlabeled data. During training, we gradually introduce style transfer components to the intermediate samples, constructing intermediate domains at both semantic and stylistic levels. Further details about UCP and TP-RAM are provided in Fig. 3 and Fig. 5. Moreover, we design a symmetric guidance for model training. In addition to guiding from unlabeled data to intermediate samples, we merge unlabeled regions of intermediate samples to obtain the pseudo label of unlabeled data from another perspective. The integration of pseudo labels through ensemble from two perspectives guides the prediction of unlabeled data. Best viewed in color.\\n\\nIn medical image segmentation tasks, domain shift issues arise from variations in device parameters, disease severity, imaging principles, etc. Thus, UDA plays a crucial role in addressing these issues. Adversarial learning based UDA methods achieve alignment at multiple levels to narrow domain gap, including input alignment [5, 30] and feature alignment [11, 15, 35, 47]. Self-training based UDA methods [46, 48] train the model by generating reliable pseudo labels for unlabeled data. Methods like [7, 41] promote domain knowledge transfer by generating intermediate domains. These methods typically rely on a sufficient amount of source domain data and focus on a single target domain, making it challenging to be effective in the MiDSS scenario.\\n\\nData Augmentation via Copy-Paste. Copy-paste (CP) [44] involves pasting the content of a particular region of one image onto the corresponding region of another image, creating a new image that retains the semantic information from both original images. Compared to other pixel-level fusion strategies, such as MixUp [45], CP excels in preserving and blending semantic information from source images. In contrast, MixUp combines global source images proportionally, potentially leading to ambiguity issues when pixels from different classes are blended. Therefore, CP is a more suitable augmentation technique for medical image segmentation tasks. In line with this research direction, BCP [2] takes into account the dual embedding directions between unlabeled and labeled samples, randomly selecting one of them to generate intermediate samples. To mitigate domain shift, previous works [2, 14] employed CP to transfer knowledge from labeled data to unlabeled data by generating intermediate samples. Despite the progress, they fall short in fully leveraging information from intermediate domains, leading to sub-optimal transfer effects.\\n\\n3. Method\\n\\nIn the MiDSS scenario, the domain gap exists among images originating from $K$ data centers $\\\\{D_i\\\\}_{i=1}^{K}$. The training set comprises $N$ labeled images $\\\\{(x_i, y_i)\\\\}_{i=1}^{N}$ from a single domain $D_j \\\\in \\\\{D_i\\\\}_{i=1}^{K}$ and $M$ unlabeled images $\\\\{u_i\\\\}_{i=1}^{M}$ from multiple domains $D_1, \\\\ldots, D_K$, where $M > N$. The image resolution of $H \\\\times W \\\\times D$ sequentially represents height and width and channel, and $y_i \\\\in \\\\{0, 1\\\\}$ is the ground truth of $x_i$, where $C$ is the number of class. We aim to train a model based on the aforementioned training set, capable of delivering outstanding segmentation performance across all domains with or without labeled data.\"}"}
{"id": "CVPR-2024-412", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. The illustration of UCP between images. Cut refers to splitting the image into two parts according to $M\\\\alpha$, while Mix implies merging two parts of images back together.\\n\\nBased on the mean teacher framework [34], for each labeled data $x$, teacher model $f_t$ generates probability map $p_w$ and pseudo label $\\\\hat{p}$ for weakly augmented unlabeled data $u_w$, guiding the student model $f_s$ in making predictions $p_s$ for strongly augmented unlabeled data $u_s$:\\n\\n$$x_w = A_w(x); u_w = A_w(u); u_s = A_s(u),$$\\n\\n$$p_w = f_t(u_w); p_s = f_s(u_s); \\\\hat{p} = \\\\arg \\\\max(p_w),$$\\n\\nwhere $\\\\arg \\\\max(\\\\cdot)$ applied to probability distributions produces valid \u201cone-hot\u201d probability distributions, and $f_t$ is updated by the EMA of $f_s$ [34]. The weak augmentation $A_w$ includes cropping, rotation, flip, and elastic distortion, while the strong augmentation $A_s$ builds upon $A_w$ by incorporating non-geometric operations like color jitter and Gaussian blurring [33, 36]. The overall framework of our method is shown in Fig. 2.\\n\\n3.1. Intermediate Samples Generation by UCP\\n\\nGiven a pair of labeled data $(x_w, y_w)$ and unlabeled data $(u_s, p_w, \\\\hat{p})$, we simultaneously apply both embedding directions of CP. This enables unified Copy-Paste (UCP) between labeled and unlabeled data with distribution discrepancy. Through this approach, we obtain intermediate samples along with their pseudo-labels and probability maps:\\n\\n$$u_s^{\\\\text{in}} = x_w \\\\odot M \\\\alpha + u_s \\\\odot (1 - M \\\\alpha),$$\\n\\n$$p_w^{\\\\text{in}} = y_w \\\\odot M \\\\alpha + p_w \\\\odot (1 - M \\\\alpha),$$\\n\\n$$\\\\hat{p}^{\\\\text{in}} = y_w \\\\odot M \\\\alpha + \\\\hat{p} \\\\odot (1 - M \\\\alpha),$$\\n\\n$$u_s^{\\\\text{out}} = u_s \\\\odot M \\\\alpha + x_w \\\\odot (1 - M \\\\alpha),$$\\n\\n$$p_w^{\\\\text{out}} = p_w \\\\odot M \\\\alpha + y_w \\\\odot (1 - M \\\\alpha),$$\\n\\n$$\\\\hat{p}^{\\\\text{out}} = \\\\hat{p} \\\\odot M \\\\alpha + y_w \\\\odot (1 - M \\\\alpha),$$\\n\\nwhere $M \\\\alpha \\\\in \\\\{0, 1\\\\}^{W \\\\times H}$ is a randomly generated one-centered mask, indicating the region for CP operation. In this context, 1 indicates an all-one matrix, and $\\\\odot$ means element-wise multiplication. Referring to Fig. 3, the illustration explains UCP between images, with the same operation applied to both probability maps and pseudo labels.\\n\\n(a) SSMS (b) SSMS with UCP\\n\\nFigure 4. The results depict the quality of pseudo labels in a SSMS method (FixMatch) with and without UCP, utilizing 40 labeled data from the BIDMC domain in Prostate dataset [23]. Each colored bar represents samples from different domains, with the bar height indicating the quality of pseudo labels generated by the model for unlabeled data from that domain.\\n\\n3.2. Symmetric Guided Training with UCP\\n\\nIn the case of a labeled data $(x_w, y_w)$ and unlabeled data $(u_w, u_s)$ pair, we aim to facilitate the transfer of knowledge from the labeled domain while maximizing the extraction of latent information from the unlabeled data. During the training phase, the teacher model firstly predicts $u_w$ to generate probability map $p_w$ and pseudo label $\\\\hat{p}$ for $u_s$. Then we employ UCP between $(x_w, y_w)$ and $(u_s, p_w, \\\\hat{p})$ to generate intermediate samples $(u_s^{\\\\text{in}}, u_s^{\\\\text{out}})$, probability maps $(p_w^{\\\\text{in}}, p_w^{\\\\text{out}})$ and pseudo labels $(\\\\hat{p}^{\\\\text{in}}, \\\\hat{p}^{\\\\text{out}})$ by Eq. (2). We set weight map $w$ to indicate whether the pseudo label is reliable:\\n\\n$$w_i = 1(\\\\max(p_w_i) \\\\geq \\\\tau),$$\\n\\nwhere $w_i$ is the $i$th pixel of $w$, and $\\\\tau$ is a pre-defined confidence threshold used to filter noisy labels. The indicator function is denoted as $1(\\\\cdot)$. The pseudo labels $\\\\hat{p}^{\\\\text{in}}$ and $\\\\hat{p}^{\\\\text{out}}$ will be used as the supervision to guide the student model in predicting $p_s^{\\\\text{in}}$ and $p_s^{\\\\text{out}}$ for $u_s^{\\\\text{in}}$ and $u_s^{\\\\text{out}}$:\\n\\n$$L^{\\\\text{in}} = L_{\\\\text{ce}}(\\\\hat{p}^{\\\\text{in}}, p_s^{\\\\text{in}}, w^{\\\\text{in}}) + L_{\\\\text{dice}}(\\\\hat{p}^{\\\\text{in}}, p_s^{\\\\text{in}}, w^{\\\\text{in}}),$$\\n\\n$$L^{\\\\text{out}} = L_{\\\\text{ce}}(\\\\hat{p}^{\\\\text{out}}, p_s^{\\\\text{out}}, w^{\\\\text{out}}) + L_{\\\\text{dice}}(\\\\hat{p}^{\\\\text{out}}, p_s^{\\\\text{out}}, w^{\\\\text{out}}),$$\\n\\nwhere $L_{\\\\text{ce}}$ and $L_{\\\\text{dice}}$ respectively represent the cross-entropy loss and dice loss, which are formulated as:\\n\\n$$L_{\\\\text{ce}}(y, p, w) = -\\\\frac{1}{H \\\\times W} \\\\sum_{i=1}^{H \\\\times W} w_i y_i \\\\log p_i,$$\\n\\n$$L_{\\\\text{dice}}(y, p, w) = 1 - \\\\frac{2 \\\\times P}{H \\\\times W} \\\\sum_{i=1}^{H \\\\times W} w_i p_i y_i \\\\left( 1 + (p_i^2 + y_i^2) \\\\right),$$\\n\\nwhere $p_i, y_i$ denote the probability of foreground and pseudo label of the $i$th pixel, respectively. The supervision provided by the fusion of $\\\\hat{p}$ with $y_w$ to $p_s^{\\\\text{in}}$ and $p_s^{\\\\text{out}}$ effectively promotes the model training in the intermediate domains, facilitating adaptation to the domains.\"}"}
{"id": "CVPR-2024-412", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FFT\\n\\nPhase\\n\\nAmplitude\\n\\n\u03a6 \\\\sim (0, t/total)\\n\\n1 - \u03a6\\n\\nM \u03b2\\n\\nIFFT\\n\\nTP-RAM\\n\\nx w\\n\\nu w\\n\\nx u\\n\\nFigure 5. The illustration of TP-RAM. Fast Fourier transform (FFT) extracts the amplitude and phase maps of \\\\( x_w \\\\) and \\\\( y_w \\\\). The low-frequency regions (determined by \\\\( M \u03b2 \\\\)) of the two phase maps are mixed. Through inverse Fast Fourier transform (IFFT), \\\\( x_u \\\\) is synthesized, preserving consistent semantics while introducing a different style compared to \\\\( x_w \\\\).\\n\\nof unlabeled data. With the assistance of UCP, the quality of pseudo labels for unlabeled data in each domain is significantly improved as shown in Fig. 4. However, this single-direction supervision lacks further utilization of information acquired in the intermediate domains, resulting in suboptimal performance in the domains of unlabeled data. To address such limitation, we design a Symmetric Guidance training strategy (SymGD) to fully utilize the information from the intermediate domains. In addition to the consistency mentioned above, we also explore the direct guidance of unlabeled data through intermediate domain information.\\n\\nSimilarly, we employ \\\\( M \u03b1 \\\\) to mix \\\\( x_w \\\\) and \\\\( u_w \\\\), generating intermediate samples \\\\( u_w \\\\) in and \\\\( u_w \\\\) outside. The teacher model predicts them to generate pseudo labels \\\\( \\\\hat{p}_w \\\\) in and \\\\( \\\\hat{p}_w \\\\) outside. We merge the unlabeled regions of the pseudo labels:\\n\\n\\\\[\\n\\\\hat{p}_{mg} = \\\\hat{p}_w \\\\odot (1 - M \u03b1) + \\\\hat{p}_w \\\\odot M \u03b1.\\n\\\\]\\n\\nWe obtain the pseudo label for \\\\( u_s \\\\) from the pseudo labels of intermediate samples, then integrate it with \\\\( p_w \\\\) to provide more accurate guidance for the prediction of the student model on \\\\( u_s \\\\). The weight map \\\\( w_{ens} \\\\) of \\\\( \\\\hat{p}_{mg} \\\\) is determined as follows:\\n\\n\\\\[\\nw_{ens} = (1 - (\\\\hat{p} \\\\oplus \\\\hat{p}_{mg})) \\\\odot w \\\\odot w_{mg},\\n\\\\]\\n\\nwhere \\\\( \\\\oplus \\\\) is the pixel-wise XOR operator to indicate the consistency between pseudo labels from two perspectives, and \\\\( w \\\\) and \\\\( w_{mg} \\\\) represent the weight map of \\\\( \\\\hat{p} \\\\) and \\\\( \\\\hat{p}_{mg} \\\\) respectively. The loss of such direction can be defined as:\\n\\n\\\\[\\nL_{sym} = L_{ce}(\\\\hat{p}_{mg}, p_s, w_{mg}) + L_{dice}(\\\\hat{p}_{mg}, p_s, w_{mg}),\\n\\\\]\\n\\nTo generate reliable pseudo labels for unlabeled data from the perspective of intermediate samples, it is necessary that the model performs well in the intermediate domain. Therefore, the guidance from \\\\( \\\\hat{p}_{mg} \\\\) to \\\\( p_s \\\\) maintains a relatively low weight in the early stages of training and rapidly increases in the middle to later stages.\\n\\n3.3. Style Transition in Intermediate Samples\\n\\nThe intermediate sample generation method based on UCP primarily focuses on local semantic blending while overlooking the transition of stylistic differences between different domains. Distribution information (i.e., style) is typically represented in low-frequency components, while edge information is often found in high-frequency components. By interpolating low-frequency information between data from different domains, amplitude MixUp generates samples with a new style. Then we mix these samples with \\\\( u_w \\\\) by UCP to introduce style-transition components in the intermediate domains.\\n\\nHowever, arbitrary style transfer is not conducive to the stable model learning of the intermediate domain. We aim for the intermediate samples to exhibit a gradual transition in style between domains during the training process without compromising the diversity introduced by random MixUp. Therefore, we propose Training Progress aware Random Amplitude Mixup (TP-RAM).\\n\\nOverall, as exhibited in Fig. 5, we obtain the frequency space signals of each channel of the image \\\\( x \\\\) through Fast Fourier transformation \\\\( F \\\\) as follows:\\n\\n\\\\[\\nF(x)_{(u, v)} = H_{-1} \\\\sum_{h=0}^{W-1} W_{-1} \\\\sum_{w=0}^{H-1} x_{(h, w)} e^{-j2\\\\pi(hH/u + wW/v)},\\n\\\\]\\n\\nLet \\\\( F_A, F_P \\\\) be the amplitude and phase components of \\\\( F \\\\). For a pair of labeled and unlabeled data pair \\\\( (x_w, u_w) \\\\), we obtain their amplitude \\\\( (F_A(x_w), F_A(u_w)) \\\\) and phase \\\\( (F_P(x_w), F_P(u_w)) \\\\). After that, we blend the low-frequency information of \\\\( F_A(u_w) \\\\) into \\\\( F_A(x_w) \\\\), and obtain a new image \\\\( x_u \\\\) through inverse Fast Fourier transformation \\\\( F^{-1} \\\\) as follows:\\n\\n\\\\[\\nx_u = F^{-1}[M_\u03b2 \\\\odot F_A(u_w) + (1 - M_\u03b2) \\\\odot F_A(x_w), F_P(x_w)],\\n\\\\]\\n\\nwhere \\\\( M_\u03b2 \\\\) is a mask with values of 0 except in the central \\\\( 2\u03b2W \\\\times 2\u03b2H \\\\) region, where the values range from 0 to \\\\( \u03a6 \\\\), indicating the mixing ratio for low-frequency amplitude. \\\\( \u03a6 \\\\) is a parameter that increases during model training:\\n\\n\\\\[\\n\u03a6(t) = t/t_{total},\\n\\\\]\\n\\nwhere \\\\( t \\\\) denotes the current training iteration and \\\\( t_{total} \\\\) is the maximum training step, controlling the gradual enhancement of low-frequency components of unlabeled data in the intermediate samples.\\n\\n3.4. Loss Function\\n\\nFor each pair of labeled and unlabeled data, the overall training objective function consists of two parts. The supervised loss is trivial, while the unsupervised loss is calculated through consistency regularization of symmetric guidance:\\n\\n\\\\[\\nL_{total} = L_s + \\\\lambda(L_{in} + L_{out} + \\\\lambda L_{sym}),\\n\\\\]\\n\\nwhere \\\\( \\\\lambda \\\\) is a weight coefficient decided by a time-dependent Gaussian warming-up function:\\n\\n\\\\[\\n\\\\lambda(t) = e^{-5(1 - t/t_{total})}.\\n\\\\]\"}"}
{"id": "CVPR-2024-412", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison of different methods on Fundus dataset. #L represents the number of labeled samples. In the line of Upper bound, * denotes the Upper bound using all training samples in a domain as labeled data. \u2191 indicates that a higher value corresponds to better performance, while \u2193 suggests the opposite. The best performance is marked as bold, and the second-best is underlined.\\n\\n| Task          | Method   | #L | Domain 1         | Domain 2         | Domain 3         | Domain 4         | Avg. | Avg. | Avg. | Avg. |\\n|---------------|----------|----|------------------|------------------|------------------|------------------|------|------|------|------|\\n| Optic Cup / Disc Segmentation | U-Net    | 20 | 59.54 / 73.89    | 71.28 / 74.23    | 50.87 / 64.29    | 35.61 / 63.30    | 61.63 | 52.65 | 48.28 | 28.86 |\\n|               | UA-MT    |    | 59.35 / 78.46    | 63.08 / 74.45    | 35.24 / 47.73    | 36.18 / 55.43    | 56.24 | 47.00 | 48.64 | 31.35 |\\n|               | FDA      |    | 76.99 / 89.94    | 77.69 / 89.63    | 78.27 / 90.96    | 64.52 / 74.29    | 80.29 | 71.05 | 16.23 | 8.44  |\\n|               | SIFA     |    | 50.67 / 75.30    | 64.44 / 80.69    | 61.67 / 83.77    | 55.07 / 70.67    | 67.78 | 54.77 | 20.16 | 10.93 |\\n|               | FixMatch |    | 81.18 / 91.29    | 72.04 / 87.60    | 80.41 / 92.95    | 74.58 / 87.07    | 83.39 | 73.48 | 11.77 | 5.60  |\\n|               | CPS      |    | 64.53 / 86.25    | 70.26 / 86.97    | 42.92 / 54.94    | 36.98 / 46.70    | 61.19 | 52.69 | 34.44 | 26.79 |\\n|               | CoraNet  |    | 61.64 / 87.32    | 65.56 / 87.05    | 66.12 / 83.54    | 49.01 / 77.73    | 72.25 | 60.50 | 20.52 | 10.44 |\\n|               | UDA-V AE++ |   | 55.01 / 80.76    | 68.87 / 85.94    | 63.23 / 84.92    | 68.42 / 80.89    | 73.51 | 61.40 | 17.60 | 9.86  |\\n|               | SS-Net   |    | 59.42 / 78.15    | 67.32 / 85.05    | 45.69 / 69.91    | 38.76 / 61.13    | 63.18 | 53.49 | 44.90 | 25.73 |\\n|               | BCP      |    | 71.65 / 91.10    | 77.19 / 92.00    | 72.63 / 90.77    | 77.67 / 91.42    | 83.05 | 73.66 | 11.05 | 5.80  |\\n|               | CauSSL   |    | 63.38 / 80.60    | 67.52 / 80.72    | 49.53 / 63.88    | 39.43 / 49.43    | 61.81 | 51.80 | 41.25 | 23.94 |\\n|               | Ours     |    | 83.71 / 92.96    | 80.47 / 89.93    | 84.18 / 92.97    | 83.71 / 93.38    | 87.66 | 79.10 | 8.21  | 3.89  |\\n\\nUpper bound: *\\n\\n![Figure 6](image_url)\"}"}
{"id": "CVPR-2024-412", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2. Comparison of different methods on Prostate dataset.\\n\\n| Method          | $\\\\#L$ | DC   | JC   | HD  | ASD | Avg.  | Avg.  | Avg.  | Avg.  |\\n|-----------------|-------|------|------|-----|-----|-------|-------|-------|-------|\\n| RUNMC           | 40    | 31.11| 20.04| 38.18| 19.41| 26.62 | 23.24 | 95.11 | 65.84 |\\n| UA-MT           | 40    | 29.44| 12.49| 39.42| 17.94| 18.22 | 14.88 | 112.07| 77.58 |\\n| FDA             | 40    | 47.44| 24.54| 61.01| 28.19| 40.51 | 32.17 | 76.67 | 47.87 |\\n| SIFA            | 40    | 72.67| 64.08| 73.49| 71.62| 65.16 | 56.78 | 29.43 | 13.03 |\\n| FixMatch        | 40    | 83.58| 73.63| 79.21| 56.07| 84.78 | 74.41 | 65.96 | 24.18 |\\n| CPS             | 40    | 29.83| 11.84| 43.84| 13.51| 14.56 | 20.47 | 115.96| 78.51 |\\n| CoraNet         | 40    | 69.43| 16.29| 69.33| 24.66| 22.16 | 38.84 | 31.48 | 67.91 |\\n| UDA-V AE++      | 40    | 68.73| 65.49| 67.19| 63.29| 65.15 | 66.54 | 52.80 | 34.20 |\\n| SS-Net          | 40    | 29.10| 14.20| 51.96| 23.83| 13.23 | 24.30 | 18.74 | 109.54|\\n| BCP             | 40    | 70.15| 46.15| 58.93| 74.21| 67.47 | 64.81 | 55.17 | 27.22 |\\n| CauSSL          | 40    | 24.10| 16.94| 27.23| 15.28| 14.56 | 20.93 | 15.48 | 114.62|\\n| Ours            | 40    | 88.76| 87.61| 88.34| 88.62| 88.20 | 87.98 | 80.21 | 4.20  |\\n| Upper bound     |       | 88.52| 85.71| 88.61| 88.98| 89.49 | 88.32 | 80.71 | 4.12  |\\n\\n### Table 3. Comparison of different methods on M&Ms dataset.\\n\\n| Method          | $\\\\#L$ | DC   | JC   | HD  | ASD | Avg.  | Avg.  | Avg.  | Avg.  |\\n|-----------------|-------|------|------|-----|-----|-------|-------|-------|-------|\\n| U-Net           | 20    | 57.29| 55.83| 63.85| 53.01| 44.30 | 38.07 | 22.88 |       |\\n| UA-MT           | 20    | 38.02| 43.13| 41.89| 37.96| 29.14 | 72.35 | 40.84 |       |\\n| FDA             | 20    | 61.66| 73.80| 77.23| 62.28| 53.33 | 25.99 | 16.10 |       |\\n| SIFA            | 20    | 63.97| 56.52| 59.57| 47.14| 34.96 | 25.01 | 11.45 |       |\\n| FixMatch        | 20    | 87.26| 91.06| 90.86| 82.96| 73.99 | 6.21  | 3.51  |       |\\n| CPS             | 20    | 46.40| 44.38| 47.71| 42.54| 33.82 | 58.30 | 34.94 |       |\\n| CoraNet         | 20    | 65.70| 64.89| 68.38| 50.33| 40.54 | 32.98 | 19.22 |       |\\n| UDA-V AE++      | 20    | 51.14| 57.88| 31.71| 39.28| 28.82 | 53.90 | 24.94 |       |\\n| SS-Net          | 20    | 48.98| 54.92| 55.04| 44.07| 35.89 | 49.50 | 32.55 |       |\\n| BCP             | 20    | 85.91| 85.66| 61.61| 71.65| 62.67 | 30.91 | 18.22 |       |\\n| CauSSL          | 20    | 40.20| 41.05| 53.78| 35.44| 26.73 | 72.90 | 37.99 |       |\\n| Ours            | 20    | 87.77| 91.48| 89.25| 84.31| 75.18 | 5.15  | 2.42  |       |\\n| Upper bound     |       | 91.37| 92.33| 92.20| 86.82| 78.64 | 4.32  | 2.14  |       |\\n\\n$\\\\tau = 0.95$ as the default value in experiments. We use Stochastic Gradient Descent (SGD) with a momentum of 0.9 and weight decay of 0.0001 as optimizer, with an initial learning rate of 0.03. The batch size is set as 8, including 4 labeled data and 4 unlabeled data. The iteration is set to 30,000 for Fundus dataset and 60,000 for Postate and M&Ms dataset. In the testing stage, the final segmentation results are determined by the student model. We compare our method with other state-of-the-art (SOTA) methods, including supervised methods such as UA-MT [43], FixMatch [33], CPS [9], CoraNet [31], SS-Net [40], BCP [2], and CauSSL [27] as well as domain unsupervised adaptation methods like FDA [41], SIFA [6] and UDA-V AE++ [25]. In each experiment, a small amount of data from one domain is labeled (e.g., Domain 1 in Tab. 1), while the remainder serves as unlabeled data. For the upper bound, we employ UCP in FixMatch, and use all available training data from a certain domain as labeled data, providing the model with sufficient source domain information. We adopt the Dice coefficient (DC), Jaccard coefficient (JC), 95% Hausdorff Distance (HD), and Average Surface Distance (ASD) as evaluation metrics. Except for SIFA, which uses ResNet blocks [17] for the generator and decoder, all other methods use U-Net [28] as the backbone.\"}"}
{"id": "CVPR-2024-412", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Ablation experiments on Fundus dataset.\\n\\n| Task            | Method       | Domain 1 | Domain 2 | Domain 3 | Domain 4 | Avg. DC |\\n|-----------------|--------------|----------|----------|----------|----------|---------|\\n| Optic Cup / Disc Segmentation | UCP VanillaGD SymGD TP-RAM RAM DC | 82.67 / 93.11 | 72.08 / 88.86 | 82.97 / 92.78 | 80.84 / 92.94 | 85.78   |\\n|                 |              | 83.19 / 93.45 | 73.57 / 89.48 | 82.16 / 92.91 | 80.42 / 93.35 | 86.07   |\\n|                 |              | 83.21 / 93.48 | 76.13 / 89.06 | 83.04 / 92.87 | 83.63 / 93.69 | 86.89   |\\n|                 |              | 83.41 / 93.54 | 77.18 / 88.96 | 82.69 / 92.88 | 83.54 / 93.39 | 86.95   |\\n|                 |              | 83.27 / 93.41 | 76.14 / 88.46 | 83.27 / 92.90 | 83.64 / 93.40 | 86.82   |\\n|                 |              | 83.71 / 92.96 | 80.47 / 89.93 | 84.18 / 92.97 | 83.71 / 93.38 | 87.66   |\\n\\nTable 5. Varying the low-frequency region size parameter $\\\\beta$ on Fundus dataset.\\n\\n| $\\\\beta$ | Avg. DC |\\n|---------|---------|\\n| 0.1     | 87.32   |\\n| 0.05    | 87.49   |\\n| 0.01    | 87.66   |\\n| 0.005   | 87.52   |\\n\\nGiven the larger differences between domains in Prostate dataset, SSMS methods are notably influenced by more severe error accumulation. UDA methods also demonstrate poor performance in this scenario. The segmentation result examples are presented in the supplementary material.\\n\\nResults on M&Ms dataset.\\n\\nWe conducted experiments on M&Ms dataset using 20 labeled data. The results in Tab. 3 demonstrate that our approach is effective in multi-object segmentation tasks, achieving optimal performance. Due to the abundance of samples in M&Ms dataset, the substantial increase in labeled data significantly enhances the performance of the upper bound. The segmentation result examples are presented in the supplementary material.\\n\\nIt is worth noting that while BCP considers the distribution difference between labeled and unlabeled data in SSMS, our method still exhibits significant advantages in the scenario where there is distribution difference within unlabeled data and between unlabeled and labeled data. We also compare our method with more SSMS methods aimed at mitigating distribution differences, and detailed analysis results can be found in the supplementary material.\\n\\n4.4. Ablation Study\\n\\nIn this paragraph, to validate the effectiveness of each component, we conduct a series of ablation experiments on Fundus dataset. The investigated settings are introduced as follows: 1) UCP: Generating intermediate samples based on UCP, 2) VanillaGD: Guiding $p_s$ with $\\\\hat{p}$, 3) SymGD: Guiding $p_s$ with mergence of $\\\\hat{p}_{out}$ and $\\\\hat{p}_{in}$, 4) TP-RAM: Gradually increasing the range of value for $\\\\beta$, 5) RAM: $\\\\beta$ is uniformly sampled from the distribution in the range from 0 to 1. The results are shown in Tab. 4.\\n\\nThe Efficiency of SymGD.\\n\\nCompared to #1, #3 introduces an additional guidance branch that guides the predictions of unlabeled data by integrating pseudo labels from dual perspectives. SymGD effectively leverages the intermediate domain information, aiding the model in making more accurate predictions for unlabeled data. In #2, we use the pseudo label $\\\\hat{p}$ generated from the teacher model to guide the student model's prediction $p_s$, and this straightforward additional consistency constraint has a limited impact on the model performance improvement.\\n\\nThe Efficiency of TP-RAM.\\n\\nMoreover, in #4, we gradually introduce stylistic transition components to intermediate samples, not limited to local semantic fusion. The comprehensive and stable construction of intermediate domains facilitates the transfer of domain knowledge from the domain of labeled data to the domains of unlabeled data. In #5, we replace the TP-RAM in our method with RAM, introducing stylistic transition arbitrarily for intermediate samples. Training with significant stylistic differences from labeled data in the early stages is detrimental to the stable construction of the intermediate domains, and compared to #2, it might even harm the model's performance. In #6, we incorporate two new modules to form a unified domain knowledge transfer framework, ultimately achieving the best performance in the MiDSS scenarios.\\n\\nSize of Low-frequency Region in $M_\\\\beta$.\\n\\nWe also explore the relationship between the selection of low-frequency region size and model performance. Here, $M_\\\\beta$ represents the low-frequency region of an image with a size of $2^{\\\\beta}W \\\\times 2^{\\\\beta}H$, $\\\\beta \\\\in \\\\{0.1, 0.05, 0.01, 0.005\\\\}$. Tab. 5 reveals that the model is quite robust to the choice of $\\\\beta$. Overall, there is a slight trend of performance improvement followed by a decline, with the best performance achieved when $\\\\beta = 0.01$.5.\\n\\nConclusion\\n\\nIn this paper, we explore a novel and practical setting, MiDSS, which involves the simultaneous challenges of limited labeled data and domain shift. To address the issue of declining pseudo label quality due to domain shift, we construct intermediate domains by UCP. Additionally, we introduce SymGD to enhance the utilization of intermediate domains information. Considering the stylistic differences between different domains, we design TP-RAM to introduce comprehensive and stable style transition components to intermediate domains. Extensive results on three datasets demonstrate the effectiveness of our method.\"}"}
{"id": "CVPR-2024-412", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Wenjia Bai, Ozan Oktay, Matthew Sinclair, Hideaki Suzuki, Martin Rajchl, Giacomo Tarroni, Ben Glocker, Andrew King, Paul M Matthews, and Daniel Rueckert. Semi-supervised learning for network-based cardiac MR image segmentation. In Medical Image Computing and Computer-Assisted Intervention, pages 253\u2013260. Springer, 2017.\\n\\n[2] Yunhao Bai, Duowen Chen, Qingli Li, Wei Shen, and Yan Wang. Bidirectional copy-paste for semi-supervised medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11514\u201311524, 2023.\\n\\n[3] Heng Cai, Shumeng Li, Lei Qi, Qian Yu, Yinghuan Shi, and Yang Gao. Orthogonal annotation benefits barely-supervised medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3302\u20133311, 2023.\\n\\n[4] Victor M Campello, Polyxeni Gkontra, Cristian Izquierdo, Carlos Martin-Isla, Alireza Sojoudi, Peter M Full, Klaus Maier-Hein, Yao Zhang, Zhiqiang He, Jun Ma, et al. Multi-centre, multi-vendor and multi-disease cardiac segmentation: the m&ms challenge. IEEE Transactions on Medical Imaging, 40(12):3543\u20133554, 2021.\\n\\n[5] Agisilaos Chartsias, Thomas Joyce, Rohan Dharmakumar, and Sotirios A Tsaftaris. Adversarial image synthesis for unpaired multi-modal cardiac data. In Simulation and Synthesis in Medical Imaging: Second International Workshop, SASHIMI, pages 3\u201313. Springer, 2017.\\n\\n[6] Cheng Chen, Qi Dou, Hao Chen, Jing Qin, and Pheng Ann Heng. Unsupervised bidirectional cross-modality adaptation via deeply synergistic image and feature alignment for medical image segmentation. IEEE Transactions on Medical Imaging, 39(7):2494\u20132505, 2020.\\n\\n[7] Lin Chen, Zhixiang Wei, Xin Jin, Huaian Chen, Miao Zheng, Kai Chen, and Yi Jin. Deliberated domain bridging for domain adaptive semantic segmentation. Advances in Neural Information Processing Systems, 35:15105\u201315118, 2022.\\n\\n[8] Xuming Chen, Shanlin Sun, Narisu Bai, Kun Han, Qianqian Liu, Shengyu Yao, Hao Tang, Chupeng Zhang, Zhipeng Lu, Qian Huang, et al. A deep learning-based auto-segmentation system for organs-at-risk on whole-body computed tomography images for radiation therapy. Radiotherapy and Oncology, 160:175\u2013184, 2021.\\n\\n[9] Xiaokang Chen, Yuhui Yuan, Gang Zeng, and Jingdong Wang. Semi-supervised semantic segmentation with cross-pseudo supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2613\u20132622, 2021.\\n\\n[10] Sabyasachi Dash, Sushil Kumar Shakyawar, Mohit Sharma, and Sandeep Kaushik. Big data in healthcare: management, analysis and future prospects. Journal of Big Data, 6(1):1\u201325, 2019.\\n\\n[11] Qi Dou, Cheng Ouyang, Cheng Chen, Hao Chen, and Pheng-Ann Heng. Unsupervised cross-modality domain adaptation of convnets for biomedical image segmentations with adversarial loss. arXiv preprint arXiv:1804.10916, 2018.\\n\\n[12] Yue Duan, Zhen Zhao, Lei Qi, Luping Zhou, Lei Wang, and Yinghuan Shi. Towards semi-supervised learning with non-random missing labels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16121\u201316131, 2023.\\n\\n[13] Deng-Ping Fan, Tao Zhou, Ge-Peng Ji, Yi Zhou, Geng Chen, Huazhu Fu, Jianbing Shen, and Ling Shao. Inf-net: Automatic COVID-19 lung infection segmentation from CT images. IEEE Transactions on Medical Imaging, 2020.\\n\\n[14] Jiashuo Fan, Bin Gao, Huan Jin, and Lihui Jiang. Ucc: Uncertainty-guided cross-head co-training for semi-supervised semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9947\u20139956, 2022.\\n\\n[15] Yaroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Fran\u00e7ois Laviolette, Mario Marchand, and Victor Lempitsky. Domain-adversarial training of neural networks. The Journal of Machine Learning Research, 17(1):2096\u20132030, 2016.\\n\\n[16] Hao Guan and Mingxia Liu. Domain adaptation for medical image analysis: a survey. IEEE Transactions on Biomedical Engineering, 69(3):1173\u20131185, 2021.\\n\\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.\\n\\n[18] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9729\u20139738, 2020.\\n\\n[19] Shuailin Li, Chuyu Zhang, and Xuming He. Shape-aware semi-supervised 3D semantic segmentation for medical images. In Medical Image Computing and Computer-Assisted Intervention, pages 552\u2013561. Springer, 2020.\\n\\n[20] Shumeng Li, Heng Cai, Lei Qi, Qian Yu, Yinghuan Shi, and Yang Gao. PLN: Parasitic-like network for barely-supervised medical image segmentation. IEEE Transactions on Medical Imaging, 42(3):582\u2013593, 2022.\\n\\n[21] Xiaomeng Li, Lequan Yu, Hao Chen, Chi-Wing Fu, Lei Xing, and Pheng-Ann Heng. Transformation-consistent self-ensembling model for semi-supervised medical image segmentation. IEEE Transactions on Neural Networks and Learning Systems, 32(2):523\u2013534, 2020.\\n\\n[22] Zekun Li, Lei Qi, Yinghuan Shi, and Yang Gao. IOMatch: Simplifying open-set semi-supervised learning with joint inliers and outliers utilization. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15870\u201315879, 2023.\\n\\n[23] Quande Liu, Qi Dou, and Pheng-Ann Heng. Shape-aware meta-learning for generalizing prostate MRI segmentation to unseen domains. In Medical Image Computing and Computer-Assisted Intervention, pages 475\u2013485. Springer, 2020.\\n\\n[24] Quande Liu, Cheng Chen, Jing Qin, Qi Dou, and Pheng-Ann Heng. FedDgD: Federated domain generalization on medical image segmentation via episodic learning in continuous frequency space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\"}"}
{"id": "CVPR-2024-412", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[25] Changjie Lu, Shen Zheng, and Gaurav Gupta. Unsupervised domain adaptation for cardiac segmentation: Towards structure mutual information maximization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2588\u20132597, 2022.\\n\\n[26] Xiangde Luo, Jieneng Chen, Tao Song, and Guotai Wang. Semi-supervised medical image segmentation through dual-task consistency. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 8801\u20138809, 2021.\\n\\n[27] Juzheng Miao, Cheng Chen, Furui Liu, Hao Wei, and Pheng-Ann Heng. Caussl: Causality-inspired semi-supervised learning for medical image segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 21426\u201321437, 2023.\\n\\n[28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention, pages 234\u2013241. Springer, 2015.\\n\\n[29] Vivek A Rudrapatna, Atul J Butte, et al. Opportunities and challenges in using real-world data for health care. The Journal of Clinical Investigation, 130(2):565\u2013574, 2020.\\n\\n[30] Paolo Russo, Fabio M Carlucci, Tatiana Tommasi, and Barbara Caputo. From source to target and back: symmetric bidirectional adaptive gan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8099\u20138108, 2018.\\n\\n[31] Yinghuan Shi, Jian Zhang, Tong Ling, Jiwen Lu, Yefeng Zheng, Qian Yu, Lei Qi, and Yang Gao. Inconsistency-aware uncertainty estimation for semi-supervised medical image segmentation. IEEE Transactions on Medical Imaging, 41(3):608\u2013620, 2021.\\n\\n[32] Zhao Shi, Chongchang Miao, U Joseph Schoepf, Rock H Savage, Danielle M Dargis, Chengwei Pan, Xue Chai, Xiu Li Li, Shuang Xia, Xin Zhang, et al. A clinically applicable deep-learning model for detecting intracranial aneurysm in computed tomography angiography images. Nature Communications, 11(1):6090, 2020.\\n\\n[33] Kihyuk Sohn, David Berthelot, Nicholas Carlini, Zizhao Zhang, Han Zhang, Colin A Raffel, Ekin Dogus Cubuk, Alexey Kurakin, and Chun-Liang Li. Fixmatch: Simplifying semi-supervised learning with consistency and confidence. Advances in Neural Information Processing Systems, 33:596\u2013608, 2020.\\n\\n[34] Antti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in Neural Information Processing Systems, 30, 2017.\\n\\n[35] Yi-Hsuan Tsai, Wei-Chih Hung, Samuel Schulter, Kihyuk Sohn, Ming-Hsuan Yang, and Manmohan Chandraker. Learning to adapt structured output space for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7472\u20137481, 2018.\\n\\n[36] Pratima Upretee and Bishesh Khanal. Fixmatchseg: Fixing fixmatch for semi-supervised semantic segmentation. arXiv preprint arXiv:2208.00400, 2022.\\n\\n[37] Jiacheng Wang, Xiaomeng Li, Yiming Han, Jing Qin, Liansheng Wang, and Zhou Qichao. Separated contrastive learning for organ-at-risk and gross-tumor-volume segmentation with limited annotation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2459\u20132467, 2022.\\n\\n[38] Qin Wang, Wen Li, and Luc Van Gool. Semi-supervised learning by augmented distribution alignment. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1466\u20131475, 2019.\\n\\n[39] Shujun Wang, Lequan Yu, Kang Li, Xin Yang, Chi-Wing Fu, and Pheng-Ann Heng. Dofe: Domain-oriented feature embedding for generalizable fundus image segmentation on unseen datasets. IEEE Transactions on Medical Imaging, 39(12):4237\u20134248, 2020.\\n\\n[40] Yicheng Wu, Zhonghua Wu, Qianyi Wu, Zongyuan Ge, and Jianfei Cai. Exploring smoothness and class-separation for semi-supervised medical image segmentation. In Medical Image Computing and Computer-Assisted Intervention, pages 34\u201343. Springer, 2022.\\n\\n[41] Yanchao Yang and Stefano Soatto. Fda: Fourier domain adaptation for semantic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4085\u20134095, 2020.\\n\\n[42] Chenyu You, Yuan Zhou, Ruihan Zhao, Lawrence Staib, and James S Duncan. Simcvd: Simple contrastive voxel-wise representation distillation for semi-supervised medical image segmentation. IEEE Transactions on Medical Imaging, 41(9):2228\u20132237, 2022.\\n\\n[43] Lequan Yu, Shujun Wang, Xiaomeng Li, Chi-Wing Fu, and Pheng-Ann Heng. Uncertainty-aware self-ensembling model for semi-supervised 3d left atrium segmentation. In Medical Image Computing and Computer-Assisted Intervention, pages 605\u2013613. Springer, 2019.\\n\\n[44] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6023\u20136032, 2019.\\n\\n[45] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. arXiv preprint arXiv:1710.09412, 2017.\\n\\n[46] Yifan Zhang, Ying Wei, Qingyao Wu, Peilin Zhao, Shuaicheng Niu, Junzhou Huang, and Mingkui Tan. Collaborative unsupervised domain adaptation for medical image diagnosis. IEEE Transactions on Image Processing, 29:7834\u20137844, 2020.\\n\\n[47] Ziyuan Zhao, Fangcheng Zhou, Kaixin Xu, Zeng Zeng, Cuntai Guan, and S Kevin Zhou. Le-uda: Label-efficient unsupervised domain adaptation for medical image segmentation. IEEE Transactions on Medical Imaging, 42(3):633\u2013646, 2022.\\n\\n[48] Zhedong Zheng and Yi Yang. Rectifying pseudo label learning via uncertainty estimation for domain adaptive semantic segmentation. International Journal of Computer Vision, 129(4):1106\u20131120, 2021.\\n\\n[49] Xiahai Zhuang et al. Challenges and methodologies of fully automatic whole heart segmentation: a review. Journal of Healthcare Engineering, 4:371\u2013407, 2013.\"}"}
