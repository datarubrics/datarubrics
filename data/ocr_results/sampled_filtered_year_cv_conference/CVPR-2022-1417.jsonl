{"id": "CVPR-2022-1417", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Prarthana Bhattacharyya, Chengjie Huang, and Krzysztof Czarnecki. Sa-det3d: Self-attention based context-aware 3d object detection. arXiv preprint, 2021.\\n\\n[2] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. ECCV, 2020.\\n\\n[3] Xiaozhi Chen, Kaustav Kundu, Yukun Zhu, Andrew G Berneshawi, Huimin Ma, Sanja Fidler, and Raquel Urtasun. 3d object proposals for accurate object class detection. NeurIPS, 2015.\\n\\n[4] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wengang Zhou, Yanyong Zhang, and Houqiang Li. Voxel r-cnn: Towards high performance voxel-based 3d object detection. AAAI, 2021.\\n\\n[5] Runzhou Ge, Zhuangzhuang Ding, Yihan Hu, Yu Wang, Sijia Chen, Li Huang, and Yuan Li. Afdet: Anchor free one stage 3d object detection. arXiv preprint, 2020.\\n\\n[6] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. CVPR, 2012.\\n\\n[7] Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3d semantic segmentation with submanifold sparse convolutional networks. CVPR, 2018.\\n\\n[8] Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua, and Lei Zhang. Structure aware single-stage 3d object detection from point cloud. CVPR, 2020.\\n\\n[9] Pedro Hermosilla, Tobias Ritschel, Pere-Pau V\u00e1zquez, \u00c1lvar Vinacua, and Timo Ropinski. Monte carlo convolution for learning on non-uniformly sampled point clouds. ACM Transactions on Graphics (TOG), 2018.\\n\\n[10] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint, 2014.\\n\\n[11] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. CVPR, 2019.\\n\\n[12] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. CVPR, 2017.\\n\\n[13] Ilya Loshchilov and Frank Hutter. Sgdr: Stochastic gradient descent with warm restarts. arXiv preprint, 2016.\\n\\n[14] Jiageng Mao, Minzhe Niu, Haoyue Bai, Xiaodan Liang, Hang Xu, and Chunjing Xu. Pyramid r-cnn: Towards better performance and adaptability for 3d object detection. ICCV, 2021.\\n\\n[15] Jiageng Mao, Yujing Xue, Minzhe Niu, Haoyue Bai, Jiashi Feng, Xiaodan Liang, Hang Xu, and Chunjing Xu. Voxel transformer for 3d object detection. ICCV, 2021.\\n\\n[16] Xuran Pan, Zhuofan Xia, Shiji Song, Li Erran Li, and Gao Huang. 3d object detection with pointformer. CVPR, 2021.\\n\\n[17] Emanuel Parzen. On Estimation of a Probability Density Function. The Annals of Mathematical Statistics, 1962.\\n\\n[18] Charles R. Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J. Guibas. Frustum pointnets for 3d object detection from rgb-d data. CVPR, 2018.\\n\\n[19] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. CVPR, 2017.\\n\\n[20] Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. NeurIPS, 2017.\\n\\n[21] Murray Rosenblatt. Remarks on Some Nonparametric Estimates of a Density Function. The Annals of Mathematical Statistics, 1956.\\n\\n[22] Hualian Sheng, Sijia Cai, Yuan Liu, Bing Deng, Jianqiang Huang, Xian-Sheng Hua, and Min-Jian Zhao. Improving 3d object detection with channel-wise transformer. ICCV, 2021.\\n\\n[23] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection. CVPR, 2020.\\n\\n[24] Shaoshuai Shi, Li Jiang, Jiajun Deng, Zhe Wang, Chaoxu Guo, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn++: Point-voxel feature set abstraction with local vector representation for 3d object detection. arXiv preprint, 2021.\\n\\n[25] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr-cnn: 3d object proposal generation and detection from point cloud. CVPR, 2019.\\n\\n[26] Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network. TPAMI, 2020.\\n\\n[27] Weijing Shi and Raj Rajkumar. Point-gnn: Graph neural network for 3d object detection in a point cloud. CVPR, 2020.\\n\\n[28] Leslie N Smith. A disciplined approach to neural network hyper-parameters: Part 1\u2013learning rate, batch size, momentum, and weight decay. arXiv preprint, 2018.\\n\\n[29] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. CVPR, 2020.\\n\\n[30] OpenPCDet Development Team. Openpcdet: An open-source toolbox for 3d object detection from point clouds. https://github.com/open-mmlab/OpenPCDet, 2020.\\n\\n[31] Hugues Thomas, Fran\u00e7ois Goulette, Jean-Emmanuel Deschaud, Beatriz Marcotegui, and Yann LeGall. Semantic classification of 3d point clouds with multiscale spherical neighborhoods. 3DV, 2018.\\n\\n[32] Hugues Thomas, Charles R Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Fran\u00e7ois Goulette, and Leonidas J Guibas. Kpconv: Flexible and deformable convolution for point clouds. ICCV, 2019.\\n\\n[33] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017.\\n\\n[34] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. CVPR, 2018.\"}"}
{"id": "CVPR-2022-1417", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yue Wang, Alireza Fathi, Abhijit Kundu, David A Ross, Caroline Pantofaru, Tom Funkhouser, and Justin Solomon. Pillar-based object detection for autonomous driving. ECCV, 2020.\\n\\nWenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3D point clouds. CVPR, 2019.\\n\\nYan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embedded convolutional detection. Sensors, 2018.\\n\\nZetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3DSSD: Point-based 3D single stage object detector. CVPR, 2020.\\n\\nZetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya Jia. STD: Sparse-to-dense 3D object detector for point cloud. ICCV, 2019.\\n\\nWu Zheng, Weiliang Tang, Sijin Chen, Li Jiang, and Chi-Wing Fu. CIA-SSD: Confident IoU-aware single-stage object detector from point cloud. AAAI, 2021.\\n\\nWu Zheng, Weiliang Tang, Li Jiang, and Chi-Wing Fu. SSD: Self-ensembling single-stage object detector from point cloud. CVPR, 2021.\\n\\nYin Zhou, Pei Sun, Yu Zhang, Dragomir Anguelov, Jiyang Gao, Tom Ouyang, James Guo, Jiquan Ngiam, and Vijay Vasudevan. End-to-end multi-view fusion for 3D object detection in lidar point clouds. CoRL, 2020.\\n\\nYin Zhou and Oncel Tuzel. VoxelNet: End-to-end learning for point cloud based 3D object detection. CVPR, 2018.\"}"}
{"id": "CVPR-2022-1417", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Density-aware RoI grid pooling performs self-attention on the grid point features (green) to capture long-range dependencies. Point density positional encoding uses the relative offset of the grid point and number of points (blue) in each grid voxel as inputs.\\n\\n\\\\( T_{g_i} \\\\) is the transformer encoder layer output for \\\\( f_{g_i} \\\\) and \\\\( \\\\tilde{f}_{g_i} \\\\) is the output grid feature. Empty grid point features \\\\( |N(g_i)| = 0 \\\\) are untouched by the self-attention module and are kept as their original feature encoding.\\n\\n**Point Density Positional Encoding.** We add positional encoding to the self-attention module by using the local grid point positions and the number of points in the box proposal. The bounding box proposal is divided into voxels \\\\( V_{g_j} \\\\) using the same \\\\( U \\\\times U \\\\times U \\\\) grid resolution to establish voxels for each grid point. The positional encoding for each grid feature is then calculated as\\n\\n\\\\[\\nPE(f_{g_j}) = FFN(\\\\delta_{g_j}, \\\\log|N(V_{g_j})| + \\\\epsilon)\\n\\\\]\\n\\nwhere \\\\( \\\\delta_{g_j} = x_{g_j} - c_b \\\\) is the relative position of \\\\( g_j \\\\) from the bounding box proposal centroid \\\\( c_b \\\\), \\\\( |N(V_{g_j})| \\\\) is the number of points in each grid point voxel \\\\( V_{g_j} \\\\), and \\\\( \\\\epsilon \\\\) is a constant offset. By leveraging the local offsets and number of points within each voxel, density-aware RoI grid pooling is able to capture point densities within each region proposal.\\n\\n### 3.4. Density Confidence Prediction\\n\\nPDV also uses the relationship between the distance and the number of LiDAR points on scanned objects to predict the confidence of the final bounding box predictions. A shared FFN first encodes the flattened features from the density-aware RoI grid pooling module. Then, two separate FFN branches encode the features for the box refinement and box confidence outputs. In the box confidence branch, we additionally append two features to predict the output confidence \\\\( p_{\\\\tilde{b}} \\\\) for the final bounding box \\\\( \\\\tilde{b} \\\\):\\n\\n\\\\[\\np_{\\\\tilde{b}} = FFN(f_s_{\\\\tilde{b}}, c_{\\\\tilde{b}}, \\\\log|N(\\\\tilde{b})|)\\n\\\\]\\n\\nwhere \\\\( f_s_{\\\\tilde{b}} \\\\) is the output feature vector from the shared FFN, \\\\( c_{\\\\tilde{b}} \\\\) is the centroid of the final bounding box, and \\\\( |N(\\\\tilde{b})| \\\\) is the number of raw points in the final bounding box.\\n\\n### 3.5. Training Losses\\n\\nWe use an end-to-end training strategy for PDV with a region proposal loss \\\\( L_{RPN} \\\\) and proposal refinement loss \\\\( L_{RCNN} \\\\) that are trained jointly. The \\\\( L_{RPN} \\\\) is calculated as\\n\\n\\\\[\\nL_{RPN} = L_{\\\\text{cls}}(y_b, y_\\\\star_b) + \\\\beta L_{\\\\text{reg}}(r_b, r_\\\\star_b)\\n\\\\]\\n\\nwhere \\\\( L_{\\\\text{cls}} \\\\) is the focal loss [12], \\\\( L_{\\\\text{reg}} \\\\) is the smooth-L1 loss, \\\\( y_b \\\\) is the predicted class vector, \\\\( y_\\\\star_b \\\\) is the ground truth class, \\\\( r_b \\\\) is the predicted RoI anchor residual, \\\\( r_\\\\star_b \\\\) is the ground truth anchor residual, and \\\\( \\\\beta \\\\) is a scaling factor.\\n\\n\\\\( L_{RCNN} \\\\) is composed of\\n\\n\\\\[\\nL_{\\\\text{IoU}} = -p_\\\\star_{\\\\tilde{b}} \\\\log(p_{\\\\tilde{b}}) - (1 - p_\\\\star_{\\\\tilde{b}}) \\\\log(1 - p_{\\\\tilde{b}})\\n\\\\]\\n\\nwhere \\\\( p_\\\\star_{\\\\tilde{b}} \\\\) is the confidence training target scaled by the 3D RoI and their associated ground truth bounding box as done in PV-RCNN [23]. Thus\\n\\n\\\\[\\nL_{RCNN} = L_{\\\\text{IoU}} + L_{\\\\text{reg}}(r_{\\\\tilde{b}}, r_\\\\star_{\\\\tilde{b}})\\n\\\\]\\n\\nwhere \\\\( r_{\\\\tilde{b}} \\\\) is the predicted bounding box residual and \\\\( r_\\\\star_{\\\\tilde{b}} \\\\) is the ground truth residual. A smooth-L1 loss is used to regress the bounding box residuals. We use the same confidence and regression targets as PV-RCNN [23].\\n\\n### 4. Experimental Results\\n\\n**Datasets.** We evaluate PDV on the Waymo Open Dataset [29] and the KITTI 3D Object Detection benchmark [6]. The Waymo Open Dataset is one of the biggest and most diverse autonomous driving datasets available, containing 798 training sequences (approximately 158k point cloud samples) and 202 validation sequences (approximately 40k point cloud samples), with annotations for objects in full 360\u00b0 field of view. The Waymo Open Dataset uses standard mean average precision (mAP) as well as mAPH, which factors in heading angle. The predictions are split into LEVEL 1, which only includes 3D labels with more than five LiDAR points, and LEVEL 2, which also includes 3D labels with at least one LiDAR point. The KITTI dataset contains 7,481 training samples and 7,518 testing samples and uses standard average precision (AP) on easy, moderate and hard difficulties. We adopt the standard training and val split sets from Chen et al. [3].\\n\\n**Input Parameters.** For the Waymo Open Dataset [29], the detection range is \\\\([-75.2m, 75.2m]\\\\) for the X and Y axes, and \\\\([-2m, 4m]\\\\) for the Z axis. We divide the raw point cloud into voxels of size \\\\((0.1m, 0.1m, 0.15m)\\\\). Since the KITTI dataset [6] only provides annotations in front camera's field of view, its detection range is set to be \\\\([0, 70.4m]\\\\) for the X axis, \\\\([-40m, 40m]\\\\) for the Y axis, and \\\\([-3m, 1m]\\\\) for the Z axis. We set the voxel size to be \\\\((0.05m, 0.05m, 0.1m)\\\\).\\n\\n**Network Architecture.** PDV uses the last two voxel layers \\\\( l = 3, 4 \\\\) for voxel point centroid localization. Density-aware RoI grid pooling uses a grid size of \\\\( U = 6 \\\\). Each\"}"}
{"id": "CVPR-2022-1417", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method            | LEVEL 1 | LEVEL 2 |\\n|------------------|---------|---------|\\n|                  | mAP     | mAPH    | mAP     | mAPH    |\\n|                  | 0-30m   | 30-50m  | 50m-Inf | 0-30m   | 30-50m  | 50m-Inf |\\n| PV-RCNN \u22c6        | 91.92/91.34 | 69.21/68.53 | 42.17/41.31 | 91.58/91.00 | 65.13/64.49 | 36.46/35.70 |\\n| Voxel-RCNN       | 92.49/- | 74.09/- | 53.15/- | 91.74/- | 67.89/- | 40.80/- |\\n| CT3D             | 92.51/- | 75.07/ | 55.36/ | 91.76/- | 68.93/ | 42.60/ |\\n| VoTr-TSD         | 92.28/91.73 | 73.36/72.56 | 51.09/50.01 | 92.41/91.99 | 69.36/68.81 | 42.16/41.48 |\\n| Pyramid-PV       | 92.67/92.20 | 74.91/74.21 | 54.54/53.45 | 92.67/92.20 | 74.91/74.21 | 54.54/53.45 |\\n| PDV (Ours)       | 93.13/92.71 | 75.49/74.91 | 54.75/53.90 | 93.13/92.71 | 75.49/74.91 | 54.75/53.90 |\\n\\nTable 2. Performance comparison on the Waymo Open Dataset with 202 validation sequences for 3D vehicle detection across distance.\\n\\n| Method            | LEVEL 2 mAPH |\\n|------------------|--------------|\\n|                  | 0-30m        | 30-50m      | 50m-Inf     |\\n| PV-RCNN \u22c6        | 68.41        | 57.61       | 63.98       |\\n| PV-RCNN++         | 69.71        | 59.72       | 65.17       |\\n| PDV (Ours)        | 69.98        | 60.00       | 67.88       |\\n\\nTable 3. Performance comparison on the Waymo validation set for 3D multi-class with first and second LiDAR return.\\n\\n4.1. Waymo Dataset Results\\n\\nWe report the multi-class Waymo Open Dataset results on the validation set in Table 1. PDV achieves state-of-the-art results on all classes on both LEVEL 1 and LEVEL 2 mAP/mAPH metrics. We outperform methods that use PV-RCNN as a base architecture [14,15,23] by at least +2.13% on vehicle LEVEL 2 mAPH. PDV performs well on the other classes as well, increasing performance by +2.99% and +1.88% on pedestrian and cyclist LEVEL 2 3D mAPH, respectively, compared to PV-RCNN. We also outperform PV-RCNN++ [24] by +1.25%, +0.46%, and +0.71% on vehicle, pedestrian, and cyclist LEVEL 2 3D mAPH, respectively. PDV effectively captures fine point details lost in the voxel backbone through point density for accurate bounding box refinement in the second stage.\"}"}
{"id": "CVPR-2022-1417", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. 3D detection results on the KITTI val set for car, pedestrian, and cyclist classes using AP$|_{R \\\\leq 40}$. $\\\\star$: Results are taken from publicly released models [22, 23, 30].\\n\\n| Method       | Car 3D (IoU=0.7) | Pedestrian 3D (IoU=0.5) | Cyclist 3D (IoU=0.5) |\\n|--------------|------------------|-------------------------|----------------------|\\n| SECOND [37]  | 83.34            | 72.55                   | 65.82                |\\n| PointPillar [11] | 82.58          | 74.31                   | 68.99                |\\n| STD [39]     | 87.95            | 79.71                   | 75.09                |\\n| 3DSSD [38]   | 88.36            | 79.57                   | 74.55                |\\n| SA-SSD [8]   | 88.75            | -                       | -                    |\\n| SE-SSD [41]  | 91.49            | -                       | -                    |\\n| PV-RCNN [23] | 90.25            | 81.43                   | 76.82                |\\n| PV-RCNN++ [24] | 90.14          | 81.88                   | 77.15                |\\n| CT3D [22]    | 87.83            | 81.77                   | 77.16                |\\n| VoxTr-TSD [15] | 89.90          | 82.09                   | 79.14                |\\n| Pyramid-PV [14] | 88.39          | -                       | 77.49                |\\n| PDV (Ours)   | 92.56            | 85.29                   | 83.05                |\\n\\nTable 5. 3D detection results on the KITTI test set for car and cyclist using AP$|_{R \\\\leq 40}$. Bolded and underlined values are best and second-best performance, respectively.\\n\\nWaymo Open Dataset for the vehicle class. PDV outperforms all methods by +0.46%/+0.51% and +0.42%/+0.70% at 0-30m and 30-50m for LEVEL 1 mAP/mAPH, respectively, and +0.65%/+0.99% and +0.43%/+4.32% for LEVEL 2 mAP/mAPH, respectively, better utilizing Waymo's close-range LiDAR data for accurate detection. We also report the results on the Waymo validation set using first and second LiDAR return in Table 3, surpassing both PV-RCNN and PV-RCNN++. Detailed second LiDAR return results can be found in the supplementary materials.\\n\\n4.2. KITTI Dataset Results\\n\\nTable 4 shows the results on the KITTI val set. PDV achieves state-of-the-art multi-class results, improving 3D AP$|_{R \\\\leq 40}$ performance by +0.32%, +4.13%, and +2.28% on car, pedestrian, and cyclist classes, respectively, on the moderate difficulty. As shown in Table 5, PDV also obtains competitive results on the KITTI test set, showing improvements over PV-RCNN [23], Voxel-RCNN [4], and CT3D [22] with an increase of at least +0.09% on the moderate 3D AP$|_{R \\\\leq 40}$ car class, but falling short compared to other methods [14, 15, 24, 41]. We hypothesize that since detection architectures can use a higher voxel resolution for the truncated KITTI point cloud, PDV's modules provide smaller improvements compared to other methods.\\n\\n4.3. Ablation Studies\\n\\nWe provide ablation studies to study the effects of each component in PDV. All models and variations are trained on 10% of the Waymo training dataset for 60 epochs.\\n\\nComponents.\\n\\nTable 6 shows the relative performance gain of each component on LEVEL 2 mAPH on the Waymo Open Dataset validation set. Experiment 1 uses voxel centers to localize voxel features. Experiment 2 provides an improvement of +0.83%, +3.86%, and +3.67% for the vehicle, pedestrian, and cyclist classes, respectively. Voxel feature localization is extremely beneficial for smaller objects since voxel centers do not have proper alignment with the point cloud. By localizing the features closer to the object's scanned surface, voxel point centroids contain meaningful geometric shape information for proposal refinement.\\n\\nLocal feature density estimation improves scores by +0.07%, 4.45%, +1.78% as shown in Experiment 3. By capturing feature density relationships via KDE, local feature density estimation provides a large improvement for the deformable pedestrian class, where encoded features have variable spatial configurations. Experiment 4 shows attention improves results by +0.45%, +0.61%, +1.43% providing a large increase in the pedestrian and cyclist class by establishing long-range dependencies between RoI grid points. Finally, Experiment 5 shows the effect of density confidence prediction, improving performance on the pedestrian and cyclist classes by +0.23% and +0.74%, respectively, with a small drop on the vehicle class of -0.01%.\\n\\nPoint Density Positional Encoding.\\n\\nTable 7 shows the effects of point density positional encoding in density-aware RoI pooling. Experiment 1 shows the baseline performance without any positional encoding. Experiment 2 uses a sinusoidal encoding similar to DETR [2, 33] where each spatial dimension is encoded with independent sine and cosine functions at different frequencies, showing an improvement on the pedestrian and cyclist by +0.14% and +0.50%, but a reduction on vehicle by -0.25%. A FFN with spatial grid point coordinates as input provides an overall increase in performance of +0.17%, +0.18%, and +0.27% on vehicles, pedestrians, and cyclists, respectively, as shown in Experiment 3. In Experiment 4, the density feature, in the form of...\"}"}
{"id": "CVPR-2022-1417", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6. PDV ablation experiments trained on 10% of the Waymo training set. VC indicates voxel point centroids, LD indicates local feature density estimation, and GA indicates grid point self-attention.\\n\\n| Exp. | PE          | Veh. Ped. Cyc. | LEVEL 2 mAPH |\\n|------|-------------|----------------|--------------|\\n| 1    | None        | 65.20          | 51.23        | 62.21        |\\n| 2    | Sinusoidal  | 64.95          | 51.37        | 62.71        |\\n| 3    | FFN (XYZ)   | 65.37          | 51.41        | 62.48        |\\n| 4    | FFN (D)     | 65.42          |              |              |\\n| 5    | FFN (XYZD)  | 65.35          | 52.30        | 64.03        |\\n\\nTable 7. Positional encoding (PE) ablation experiments trained on 10% of the Waymo training dataset. XYZ indicates spatial grid locations and D indicates number of points in each grid voxel.\\n\\n| Exp. | PE          | Veh. Ped. Cyc. | LEVEL 2 mAPH |\\n|------|-------------|----------------|--------------|\\n| 1    | None        | 65.20          | 51.23        | 62.21        |\\n| 2    | Sinusoidal  | 64.95          | 51.37        | 62.71        |\\n| 3    | FFN (XYZ)   | 65.37          | 51.41        | 62.48        |\\n| 4    | FFN (D)     | 65.42          |              |              |\\n| 5    | FFN (XYZD)  | 65.35          | 52.30        | 64.03        |\\n\\nTable 8 shows the runtime comparisons of PDV. We train the models for 60 epochs on 10% of the training data for Waymo and 80 epochs on the training set for KITTI. Each model is evaluated with an Intel i7-6850K processor, a single Titan Xp GPU, and a batch size of 1. For a fair comparison, we use the same number of $K$ proposals from the RPN ($K = 275$ for Waymo and $K = 100$ for KITTI) for all models. PDV outperforms PV-RCNN on inference speed and performance, reducing runtime by 14% and 5% for Waymo and KITTI, respectively, and improving mAPH by +2.42% and mAP by +1.51% on Waymo and KITTI, respectively.\\n\\nTable 8 also shows the issue with voxel-based methods on larger input spaces. Although Voxel-RCNN is computationally efficient and performs well on the KITTI dataset, its performance degrades significantly, performing worse than PV-RCNN by -0.67% on Waymo. On the other hand, voxel point centroids provide an efficient alternative that retains spatial fidelity across the two datasets. Using voxel point centroids only (VC Only) results in an efficient architecture with a decrease in runtime by 61% and better performance on LEVEL 2 mAPH by +1.22% compared to PV-RCNN.\\n\\n4.4. Runtime Analysis\\n\\n4.5. False Positives across Distance\\n\\nFigure 6 shows the number of false positives (IoU < 0.7) predicted by PDV and PV-RCNN across distance for vehicles on the Waymo dataset. As the distance from the sensor increases, the gap between PDV and PV-RCNN increases. We attribute the increased differential to using point density to refine bounding box regression and confidence values, which is more beneficial at detecting challenging objects at farther ranges.\\n\\n5. Conclusion\\n\\nWe present PDV, a novel LiDAR 3D object detection method that uses voxel features and raw point cloud data to account for point density variations in LiDAR point clouds. PDV is particularly useful on large input spaces where point cloud sampling is expensive and voxel resolutions are low, resulting in state-of-the-art performance on the Waymo dataset and competitive results for the KITTI dataset.\"}"}
{"id": "CVPR-2022-1417", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Point Density-Aware Voxels for LiDAR 3D Object Detection\\n\\nJordan S. K. Hu Tianshu Kuai Steven L. Waslander\\nUniversity of Toronto Robotics Institute\\n{jordan.hu, tianshu.kuai}@mail.utoronto.ca, steven.waslander@robotics.utias.utoronto.ca\\n\\nAbstract\\nLiDAR has become one of the primary 3D object detection sensors in autonomous driving. However, LiDAR\u2019s diverging point pattern with increasing distance results in a non-uniform sampled point cloud ill-suited to discretized volumetric feature extraction. Current methods either rely on voxelized point clouds or use inefficient farthest point sampling to mitigate detrimental effects caused by density variation but largely ignore point density as a feature and its predictable relationship with distance from the LiDAR sensor. Our proposed solution, Point Density-Aware Voxel network (PDV), is an end-to-end two stage LiDAR 3D object detection architecture that is designed to account for these point density variations. PDV efficiently localizes voxel features from the 3D sparse convolution backbone through voxel point centroids. The spatially localized voxel features are then aggregated through a density-aware RoI grid pooling module using kernel density estimation (KDE) and self-attention with point density positional encoding. Finally, we exploit LiDAR\u2019s point density to distance relationship to re\ufb01ne our \ufb01nal bounding box con\ufb01dences. PDV outperforms all state-of-the-art methods on the Waymo Open Dataset and achieves competitive results on the KITTI dataset.\\n\\n1. Introduction\\n3D object detection is one of the key perception problems in the autonomous vehicle space as object pose estimation directly impacts the effectiveness of downstream tasks in the perception pipeline. Within the autonomous driving sensor stack, LiDAR has become one of the most popular sensors used for 3D object detection [23,25,37], because of the accurate 3D point cloud it produces through laser light. However, the reliance on LiDAR data comes at the cost of point density variations across distance. Other factors such as occlusion play a role, but the primary reason is the natural divergence of points from the LiDAR with increasing distance due to the angular offsets between the LiDAR lasers. Thus, objects located at farther distances return fewer points than objects located closer to the LiDAR.\\n\\nVoxel-based methods [4,37,40,43] typically ignore point density, solely relying on the quantized representation of the point cloud. When a high voxel resolution is afforded, as is the case on the KITTI dataset [6], voxel-based methods [41] have outperformed point-based and point-voxel-based methods. However, on datasets with larger input spaces such as the Waymo Open Dataset [29], the voxel resolution is limited due to memory constraints. Fine object details are therefore lost due to spatial misalignment between the voxel features and the point cloud as shown in Figure 1 (a), resulting in a degradation in performance.\\n\\nOther methods [23, 25] attempt to remedy point density variations through farthest point sampling (FPS) as seen in Figure 1 (b). Although effective at sampling locations on non-uniformly distributed point clouds, the computation scales poorly as a function of the number of points in the point cloud, increasing runtime and limiting the number of sampled points for second-stage proposal refinement.\\n\\nPoint density also affects detection of smaller objects such as pedestrians and cyclists. These objects have less surface area to intersect the LiDAR\u2019s laser beams, resulting in poorer object localization. Perhaps informatively, current state-of-the-art methods have largely ignored detection performance for pedestrians and cyclists, focusing solely on the car or vehicle class [4, 14, 15, 41]. As we move towards...\"}"}
{"id": "CVPR-2022-1417", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"datasets with higher environment coverage, it is necessary for architectures to be scalable to larger input spaces and to serve as a multi-class solution for 3D object detection. We therefore propose Point Density-Aware Voxel network (PDV) to resolve these identified issues by leveraging voxel point centroid localization and feature encodings that directly account for point density in multi-class 3D object detection. We summarize our approach with the following three contributions.\\n\\n(1) Voxel Point Centroid Localization. PDV partitions the LiDAR points in each non-empty voxel to calculate a point centroid for each voxel feature, as shown in Figure 1 (c). By localizing the voxel features with point centroids for second-stage proposal refinement, PDV uses the point density distributions to retain fine-grained position information in the feature encodings without requiring an expensive point cloud sampling method such as FPS.\\n\\n(2) Density-Aware RoI Grid Pooling. We augment region of interest (RoI) grid pooling [23] to encode local point density as an additional feature. First, we use kernel density estimation (KDE) [17, 21] to encode local voxel feature density at each grid point ball query, followed by self-attention [33] between grid points with a novel point density positional encoding. Density-aware RoI grid pooling captures localized point density information in the context of the whole region proposal for second-stage refinement.\\n\\n(3) Density Confidence Prediction. We further refine our bounding box confidence predictions by using the final bounding box centroid location and the number of raw LiDAR points within the final bounding box as additional features. Thus, we use the inherent relationship between distance and point density established by LiDAR for more informed confidence predictions.\\n\\nPDV outperforms all current state-of-the-art methods on the Waymo Open Dataset [29] with an increase of +0.65%/+1.25%, +0.53%/+0.46%, and +0.49%/+0.71% on the vehicle, pedestrian, and cyclist LEVEL 1/LEVEL 2 mAPH classes, respectively, and achieves competitive performance on the KITTI dataset [6].\\n\\n2. Related Work\\n\\nPoint-based LiDAR 3D Object Detection. Point-based methods use the raw point cloud to extract point-level features for bounding box predictions. F-PointNet [18] applies PointNet [19, 20] on the point cloud, segmented via image-based 2D object detections. PointRCNN [25] directly generates RoIs at the point-level through a PointNet++ [20] backbone and uses point-level features for bounding box refinement. STD [39] proposes PointsPool for RoI feature extraction while 3DSSD [38] adopts a new sampling strategy on the raw point cloud to preserve enough interior points for objects in the downsampled point cloud. Point-GNN [27] constructs a graph using the raw point cloud and aggregates node-level features to generate predictions. Point-based methods utilize expensive point cloud sampling and grouping, which inevitably require long inference times.\\n\\nVoxel-based LiDAR 3D Object Detection. Voxel-based methods divide the point cloud into a voxel grid to directly apply 3D and 2D convolutions for generating predictions [11, 37, 43]. CIA-SSD [40] adopts a light network on the bird's eye view (BEV) grid to extract robust spatial-semantic features with a confidence rectification module for better post-processing. Voxel-RCNN [4] proposes Voxel RoI pooling to generate RoI features by aggregating voxel features. VoxelTr [15] proposes a transformer-based 3D backbone as an alternative to the standard sparse convolution layers [7, 37]. The performance of voxel-based methods is limited by the quantized point cloud as fine-grained point-level information is lost from the voxelization process.\\n\\nPoint-Voxel-based LiDAR 3D Object Detection. Point-voxel-based methods utilize both voxel and point representations of the point cloud. SA-SSD [8] uses an auxiliary network during training that interpolates point-level features from intermediate voxel layers. PV-RCNN [23] adopts RoI grid pooling to effectively aggregate FPS-sampled keypoint features on evenly spaced grids inside each bounding box proposal. PV-RCNN++ [24] proposes a modified version of FPS for faster point sampling and VectorPool aggregation for RoI grid pooling. CT3D [22] constructs a cylindrical-shaped RoI at each bounding box proposal. It adopts a transformer-based encoder-decoder architecture to extract RoI features directly from the nearby points without using intermediate voxel features. Pyramid-RCNN [14] extends the idea of RoI grid pooling over a single set of evenly spaced grids to multiple sets of grids at different scales with adaptive ball query radius at a significantly higher computation cost. Current point-voxel based methods do not explicitly account for variations in point cloud density within each RoI and typically require long inference times due to the dependence on point cloud sampling.\\n\\nPoint Density Estimation. KDE [17, 21] estimates a probability distribution function of a random variable using a finite set of samples and a chosen kernel function and bandwidth. A couple methods have used KDE for feature encoding in point clouds. MC Convolution [9] uses a Monte Carlo estimation of the convolution integral to handle non-uniform sampled point clouds and uses KDE to estimate the likelihood of a point within a local convolution. PointConv [36] also uses KDE, but estimates the likelihood of each sample with an additional feed-forward network (FFN). Rather than restricting the density estimate for reweighting, we use KDE as an additional feature within each grid point ball query in density-aware RoI grid pooling.\"}"}
{"id": "CVPR-2022-1417", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. PDV architecture. The input point cloud is first voxelized and processed through 3D sparse convolutions and a RPN head to produce initial bounding box proposals. Voxel features in each layer are then localized via voxel point centroids, which are then aggregated through density-aware RoI grid pooling. The RoI grid features are used to refine the bounding box proposals and their associated confidence, with additional adjustments from density confidence prediction.\\n\\n3. Methodology\\n\\nPDV uses a two-stage approach with a 3D sparse convolution backbone for initial bounding box proposals which are then refined in a second stage through voxel features in each voxel layer and the raw point cloud data. Figure 2 shows an overview of the PDV framework.\\n\\n3.1. 3D Voxel Backbone\\n\\nWe use a similar voxel backbone for initial bounding box proposals to SECOND [37]. The input to PDV is a point cloud which is defined as a set of 3D points \\\\( \\\\{ p_i = \\\\{ x_p, f_p \\\\} \\\\mid i = 1, \\\\ldots, N_p \\\\} \\\\) where \\\\( x_p \\\\in \\\\mathbb{R}^3 \\\\) are the \\\\( xyz \\\\) spatial coordinates, \\\\( f_p \\\\in \\\\mathbb{R}^F \\\\) are additional features such as the intensity or elongation of each point, and \\\\( N_p \\\\) is the number of points in the point cloud. First, the point cloud is voxelized and subsequently encoded using a series of 3D sparse convolutions [7, 37], followed by a region proposal network (RPN) for initial bounding box proposals. Each voxel layer has a different spatial resolution with 1x, 2x, 4x, and 8x downsampled resolutions based on the original voxel grid size. The voxel features in each layer are used for bounding box refinement in the second stage.\\n\\n3.2. Voxel Point Centroid Localization\\n\\nInspired by grid-subsampling in KPConv [31, 32], the voxel point centroid localization module spatially locates non-empty voxel features for aggregation in density-aware RoI grid pooling.\\n\\nLet \\\\( V_l = \\\\{ V_l_k = \\\\{ h_{V_l_k}, f_{V_l_k} \\\\} \\\\mid k = 1, \\\\ldots, N_l \\\\} \\\\) be the set of non-empty voxels in the \\\\( l \\\\)-th voxel layer where \\\\( h_{V_l_k} \\\\) is the 3D voxel index, \\\\( f_{V_l_k} \\\\) is the associated voxel feature vector, and \\\\( N_l \\\\) is the number of non-empty voxels for voxel layers \\\\( l = 1, \\\\ldots, L \\\\). First, points that are within the same voxel are grouped together into a set \\\\( N(V_l_k) \\\\) by calculating their voxel index \\\\( h_{V_l_k} \\\\) from their spatial coordinates \\\\( x_i \\\\) and voxel grid dimensions. The point centroid of each voxel feature is then calculated as\\n\\n\\\\[\\n\\\\mathbf{c}_{V_l_k} = \\\\frac{1}{|N(V_l_k)|} \\\\sum_{x_p \\\\in N(V_l_k)} x_p.\\n\\\\]\\n\\nSince the voxels in the convolution layers are in a sparse format, an intermediate hash table is used to efficiently map each calculated voxel point centroid to its corresponding feature vector. As shown in Figure 3, both voxel point centroids and sparse voxel features are associated with a shared voxel index. The intermediate hash table links the centroid \\\\( \\\\mathbf{c}_{V_l_k} \\\\) with \\\\( V_l_k \\\\) using the matching voxel index \\\\( h_{V_l_k} \\\\).\\n\\nAn advantage with using voxels is that we can use the previous voxel layer centroid calculations to efficiently compute the subsequent voxel point centroids based on the stride, padding, and kernel size of the convolution block.\\n\\nLet \\\\( C_{l+1}^k = \\\\{ \\\\mathbf{c}_{V_{l+1}^j} \\\\mid K_{l+1}(h_{V_{l+1}^j}) = h_{V_{l+1}^k} \\\\} \\\\) be the set of voxel point centroids where \\\\( K_{l+1} \\\\) is the convolution block that maps voxel index \\\\( h_{V_{l+1}^j} \\\\) to \\\\( h_{V_{l+1}^k} \\\\). We can then perform\"}"}
{"id": "CVPR-2022-1417", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Voxel point centroids (red) are assigned to their respective voxel grid index. A 3D hash table maps the voxel index to the associated voxel feature in the sparse convolution layer.\\n\\nBy avoiding recomputing centroids using the entire point cloud for each layer, voxel point centroid localization scales to larger point clouds more efficiently.\\n\\n3.3. Density-aware RoI Grid Pooling\\n\\nDensity-aware RoI grid pooling builds upon RoI grid pooling [23] by augmenting the pooling method with a combination of KDE and self-attention to encode point density features into each proposal. First, uniform grid points \\\\( G_b = \\\\{g_1, \\\\ldots, g_{U^3} \\\\} \\\\) are sampled for each bounding box proposal \\\\( b \\\\).\\n\\nLocal Feature Density. We use KDE to estimate local feature density within each grid point ball query. Rather than limiting the estimated density as a feature reweighting like MC Convolution [9] and PointConv [36], density-aware RoI grid pooling encodes the estimated probability density as an additional feature in the ball query for a more implicit feature encoding. First, we aggregate neighbouring features near each grid point where \\\\( N(g_j) \\\\) is the set of voxel point centroids in a sphere of radius \\\\( r \\\\) centered around \\\\( g_j \\\\):\\n\\n\\\\[\\n\\\\Psi_l g_j = \\\\begin{cases} \\n\\\\begin{bmatrix} f V_l k c V_l k - g_j \\\\end{bmatrix}^T, \\\\\\\\\\n\\\\forall c V_l k \\\\in N(g_j)\\n\\\\end{cases}\\n\\\\]\\n\\nwhere the local offset \\\\( c V_l k - g_j \\\\) and likelihood \\\\( p(c V_l k | g_j) \\\\) are appended as additional features, as shown in Figure 4.\\n\\n![Figure 4. Two types of features are added to each grid point ball query: (a) the relative offset from the ball query center, and (b) the probability density of each point calculated through KDE. Red and blue indicate higher and lower probability density, respectively.](image)\\n\\nThe likelihood is calculated for each grid point using KDE:\\n\\n\\\\[\\np(c V_l k | g_j) \\\\approx \\\\frac{1}{|N(g_j)| \\\\sigma^3} \\\\sum_{c V_l i \\\\in N(g_j)} W(c V_l k, c V_l i)\\n\\\\]\\n\\nwhere \\\\( \\\\sigma \\\\) is the bandwidth and \\\\( W \\\\) is\\n\\n\\\\[\\nW(c V_l k, d) = \\\\prod_{d=1}^3 w_{c V_l k, d - c V_l i, d} \\\\sigma\\n\\\\]\\n\\nwith an independent kernel \\\\( w \\\\) on each \\\\( xyz \\\\) dimension \\\\( d \\\\).\\n\\nOnce the features are appended, a PointNet multi-scale grouping (MSG) module [19, 20] is used to obtain a feature vector \\\\( f l g_j \\\\) for each grid point \\\\( g_j \\\\):\\n\\n\\\\[\\nf l g_j = \\\\text{maxpool}(\\\\text{FFN}(\\\\Psi_l g_j))\\n\\\\]\\n\\nWe use multiple radii \\\\( r \\\\) to capture feature density at different scales for each grid point and concatenate the output features together. Finally, features are appended from different voxel layers to obtain the final features for each grid point:\\n\\n\\\\[\\nf g_j = f 1 g_j, \\\\ldots, f L g_j\\n\\\\]\\n\\nGrid Point Self-Attention. The features encoded at each RoI grid point are localized to the size of the ball query but lack interdependent relationships between different grid points. An easy solution is to use self-attention [33] to capture long-range dependencies between the grid points but simply adding an attention module lacks the geometric information of the LiDAR point cloud. Thus, we also introduce a novel type of positional encoding that takes into consideration the point density within the point cloud.\\n\\nAs shown in Figure 5, the self-attention module performs self-attention between the non-empty grid point features \\\\( f G b = n f g_i | N(g_i) > 0, \\\\forall g_i \\\\in G_b \\\\) using a standard transformer encoder layer [33] and a residual connection similar to a non-local neural network block [34]:\\n\\n\\\\[\\n\\\\tilde{f} g_i = T g_i (f G b) + f g_i\\n\\\\]\"}"}
