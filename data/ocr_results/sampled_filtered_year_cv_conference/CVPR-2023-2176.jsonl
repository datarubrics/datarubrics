{"id": "CVPR-2023-2176", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Turning Strengths into Weaknesses: A Certified Robustness Inspired Attack Framework against Graph Neural Networks\\n\\nBinghui Wang\\\\textsuperscript{\u00a7}, Meng Pang\\\\textsuperscript{\u2020}, and Yun Dong\\\\textsuperscript{\u2021}\\n\\n\\\\textsuperscript{\u00a7}Department of Computer Science, Illinois Institute of Technology\\n\\\\textsuperscript{\u2020}School of Mathematics and Computer Sciences, Nanchang University\\n\\\\textsuperscript{\u2021}Department of Visual and Performing Arts, Education, and Sciences, Waubonsee Community College\\n\\nEmail: \\\\textsuperscript{\u00a7}bwang70@iit.edu, \\\\textsuperscript{\u2020}mengpang@ncu.edu.cn, \\\\textsuperscript{\u2021}ydong@waubonsee.edu\\n\\nAbstract\\n\\nGraph neural networks (GNNs) have achieved state-of-the-art performance in many graph learning tasks. However, recent studies show that GNNs are vulnerable to both test-time evasion and training-time poisoning attacks that perturb the graph structure. While existing attack methods have shown promising attack performance, we would like to design an attack framework to further enhance the performance. In particular, our attack framework is inspired by certified robustness, which was originally used by defenders to defend against adversarial attacks. We are the first, from the attacker perspective, to leverage its properties to better attack GNNs. Specifically, we first derive nodes' certified perturbation sizes against graph evasion and poisoning attacks based on randomized smoothing, respectively. A larger certified perturbation size of a node indicates this node is theoretically more robust to graph perturbations. Such a property motivates us to focus more on nodes with smaller certified perturbation sizes, as they are easier to be attacked after graph perturbations. Accordingly, we design a certified robustness inspired attack loss, when incorporated into (any) existing attacks, produces our certified robustness inspired attack counterpart. We apply our framework to the existing attacks and results show it can significantly enhance the existing base attacks' performance.\\n\\n1. Introduction\\n\\nLearning with graphs, such as social networks, citation networks, chemical networks, has attracted significant attention recently. Among many methods, graph neural networks (GNNs)\\\\textsuperscript{14,33,38,41,44} have achieved state-of-the-art performance in graph related tasks such as node classification, graph classification, and link prediction. However, recent studies\\\\textsuperscript{8,19,20,23,30,34,36,37,39,40,50,51} show that GNNs are vulnerable to both test-time graph evasion attacks and training-time graph poisoning attacks. Take GNNs for node classification as an instance, graph evasion attacks mean that, given a learnt GNN model and a (clean) graph, an attacker carefully perturbs the graph structure (i.e., inject new edges to or remove the existing edges from the graph) such that as many testing nodes as possible are misclassified by the GNN model. Whereas, graph poisoning attacks mean that, given a GNN algorithm and a graph, an attacker carefully perturbs the graph structure in the training phase, such that the learnt GNN model misclassifies as many testing nodes as possible in the testing phase. While existing methods have shown promising attack performance, we want to ask: Can we design a general attack framework that can further enhance both the existing graph evasion and poisoning attacks to GNNs? The answer is yes. We design an attack framework inspired by certified robustness. Certified robustness was originally used by defenders to guarantee the robustness of classification models against evasion attacks. Generally speaking, a testing example (e.g., an image or a node) with a better certified robustness guarantee indicates this example is theoretically more robust to adversarial (e.g., pixel or graph) perturbations. While certified robustness is mainly derived for doing the good, attackers, on the other hand, can also leverage its property to do the bad. For instance, when an attacker knows the certified robustness of nodes in a graph, he can base on nodes' certified robustness to reversely reveal the vulnerable region of the graph and leverage this vulnerability to design better attacks. We are inspired by such property of certified robustness and design the first certified robustness inspired attacks to GNNs.\\n\\nOur attack framework consists of three parts: i) Inspired by the state-of-the-art randomized smoothing based certified robustness against evasion attacks to image models\\\\textsuperscript{7,28} and GNN models\\\\textsuperscript{35}, we first propose to generalize randomized smoothing and derive the node's certified robustness guarantee against graph evasion attacks.\\n\\n1 We mainly consider the graph structure attack in the paper, as it is more effective than the feature attack. However, our attack framework can be easily extended to the feature attack.\\n\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\"}"}
{"id": "CVPR-2023-2176", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.1. Graph Neural Networks (GNNs)\\n\\nGraph Neural Networks (GNNs) are a class of deep learning models that operate on graph data, which is a collection of nodes and edges. They are designed to handle data that has a natural graph structure, such as social networks, molecular structures, and recommendation systems. GNNs are parameterized by a set of learnable parameters and can be trained to perform tasks such as node classification, link prediction, and graph classification.\\n\\nEach node in a graph is associated with a feature vector, and the edges between nodes carry information about the relationship between them. GNNs aggregate information from neighboring nodes to update the features of a node, capturing the graph structure and the node's position within the graph.\\n\\n2.2. Adversarial Attacks to GNNs\\n\\nAdversarial attacks to GNNs aim to mislead the model's predictions by perturbing the graph data. These attacks can be categorized into two types: evasion and poisoning. In evasion attacks, the attacker modifies the graph to mislead the model's prediction for a specific node. In poisoning attacks, the attacker modifies the graph to affect the model's predictions for multiple nodes.\\n\\nWe propose a certified robustness inspired attack framework to design adversarial graph perturbations to GNNs, based on loss with certified perturbation size defined node weights, which takes the graph $G$ as the base attack in our framework. Any existing graph evasion or poisoning attack method can be used as the base attack.\\n\\nWe emphasize that, as our new attack loss only modifies the existing attack loss to the state-of-the-art graph evasion and poisoning attacks, our contributions are as follows:\\n\\n1. We design the certified robustness inspired attack framework to generate adversarial graph perturbations to GNNs, based on loss with certified perturbation size defined node weights. In doing so, losses for nodes with a larger/smaller certified perturbation size will be assigned a smaller/larger weight. In doing so, losses for nodes with a larger/smaller certified perturbation size will be assigned a smaller/larger weight.\\n\\n2. We modify the classic node-wise loss by assigning each node a certified robustness inspired attack loss. Specifically, we modify the classic node-wise loss by assigning each node a certified robustness inspired attack loss. Specifically, we define the following bilevel optimization problem:\\n\\n$$\\\\min_{X} \\\\max_{\\\\Delta A} X^T \\\\Delta A \\\\Delta A^T X$$\\n\\nsubject to:\\n\\n$$X^T \\\\Delta A \\\\Delta A^T X \\\\leq \\\\epsilon$$\\n\\nwhere $X$ is an $n \\\\times d$ matrix, $\\\\Delta A$ is a $d \\\\times d$ matrix, and $\\\\epsilon$ is the perturbation budget.\\n\\n3. For instance, we aim to maximize the following 0-1 indicator function, which is hard to be optimized. In practice, an attacker will solve an alternative optimize problem as below:\\n\\n$$\\\\max_{\\\\Delta A} X^T \\\\Delta A \\\\Delta A^T X$$\\n\\nsubject to:\\n\\n$$X^T \\\\Delta A \\\\Delta A^T X \\\\leq \\\\epsilon$$\\n\\nwhere $X$ is an $n \\\\times d$ matrix, $\\\\Delta A$ is a $d \\\\times d$ matrix, and $\\\\epsilon$ is the perturbation budget.\\n\\n4. Moreover, we denote $\\\\Delta A_{uv}$ as the element-wise XOR operator. For instance, if there is an edge $(u, v)$ in the graph, we add a new edge $(u, v)$ in the perturbed graph. We also denote $\\\\Delta A_{uv} = 1$ if the edge status between nodes $u$ and $v$ is perturbed (or kept) and $\\\\Delta A_{uv} = 0$ if the edge status is not perturbed (or kept).\\n\\n5. The adversarial attack is done by solving Equation (1) under the perturbation budget $\\\\epsilon$.\\n\\n6. Graph poisoning attacks and evasion attacks can be plugged into any existing attack framework when applied to the existing attacks to GNNs. Evaluation results validate the effectiveness of our attack framework when applied to the existing attacks to GNNs.\"}"}
{"id": "CVPR-2023-2176", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Attacks via Randomized Smoothing\\n\\n**3. Certified Robustness to Graph Poisoning Attacks**\\n\\nExisting randomized smoothing mainly certifies robustness to poisoning attacks using the smoothed function. Our key idea is to extend randomized smoothing from the binary space \\\\( \\\\{0, 1\\\\} \\\\) to the continuous space of graphs. Formally, given a node classifier \\\\( f \\\\) and a graph \\\\( G \\\\), we construct a smoothed node classifier \\\\( \\\\tilde{f} \\\\) that is provably more robust to adversarial graph perturbation. Specifically, \\\\( \\\\tilde{f} \\\\) can estimate the certified perturbation size, which is the maximum perturbation that can be applied to the graph without changing the label of any node. This size is bounded by the radius of the \\\\( \\\\beta \\\\)-quantile of the Beta distribution with shape parameters \\\\( a \\\\) and \\\\( b \\\\), where \\\\( a = 1 \\\\) and \\\\( b \\\\) is determined by the number of training nodes.\\n\\nIn other words, if a node has a larger \\\\( \\\\beta \\\\)-quantile value, the certified perturbation size is smaller. This quantile can be estimated via the Monte Carlo algorithm. When an attacker aims to maximize the loss on the training graph and minimize the loss on the noisy graph, the certified perturbation size can be maximized. In the worst case, the certified perturbation size is at most 35 times the radius of the \\\\( \\\\beta \\\\)-quantile. This bound is tight, meaning that it is hard to optimize. A common strategy to address this issue is by instead maximizing the loss on the training nodes and minimizing the loss on the noisy nodes.\\n\\nBuilding a base function.\\n\\n**2.3. Certified Robustness to Graph Evasion Attacks**\\n\\nWe introduce certified robustness achieved via the state-of-the-art randomized smoothing. It consists of the following three steps.\\n\\n1. **Attacks via Randomized Smoothing**\\n\\n2. **Certified Robustness to Graph Poisoning Attacks**\\n\\n3. **Certified Robustness to Graph Evasion Attacks**\\n\\nIn this section, we generalize it to the continuous space of graphs. Specifically, we construct a smoothed node classifier that is provably more robust to evasion attacks. This classifier can be estimated via the Monte Carlo algorithm. The certified perturbation size is bounded by the radius of the \\\\( \\\\beta \\\\)-quantile of the Beta distribution with shape parameters \\\\( a \\\\) and \\\\( b \\\\), where \\\\( a = 1 \\\\) and \\\\( b \\\\) is determined by the number of testing nodes.\\n\\nIn other words, if a node has a larger \\\\( \\\\beta \\\\)-quantile value, the certified perturbation size is smaller. This quantile can be estimated via the Monte Carlo algorithm. When an attacker aims to maximize the loss on the training graph and minimize the loss on the noisy graph, the certified perturbation size can be maximized. In the worst case, the certified perturbation size is at most 35 times the radius of the \\\\( \\\\beta \\\\)-quantile. This bound is tight, meaning that it is hard to optimize. A common strategy to address this issue is by instead maximizing the loss on the training nodes and minimizing the loss on the noisy nodes.\"}"}
{"id": "CVPR-2023-2176", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Constructing a smoothed function.\\n\\nIn graph poisoning attacks, an attacker aims to perturb the graph in the training phase. To apply randomized smoothing, we first add a random noise matrix $\\\\mathcal{E}$ to the graph, where each entry $\\\\mathcal{E}_{s,t}$ is drawn from a discrete distribution, e.g., defined in Equation 7. As we add random noise $\\\\mathcal{E}$ to the graph $G$, the output of the base function $\\\\tilde{f}$ is also random. Then, inspired by Equation 6, we define the smoothed function $\\\\tilde{g}$ as follows:\\n\\n$$\\\\tilde{g}(A, V_{Tr}; v) = \\\\arg\\\\max_{c} \\\\mathbb{P}(\\\\tilde{f}(A, V_{Tr}; v) = c),$$\\n\\n(10)\\n\\nwhere $\\\\mathbb{P}(\\\\tilde{f}(A, V_{Tr}; v) = c)$ is the probability that $v$ is predicted to be a label $c$ by a GNN model trained on a noisy graph $A$ using training nodes $V_{Tr}$. $\\\\tilde{g}(A, V_{Tr}; v)$ is the predicted label for $v$ by the smoothed function $\\\\tilde{g}$.\\n\\n4. Certified Robustness Inspired Attack Framework against GNNs\\n\\nIn this section, we will design our attack framework to GNNs inspired by certified robustness. Our attack framework can be seamlessly plugged into the existing graph evasion and poisoning attacks to design more effective attacks.\\n\\n4.1. Motivation and Observation\\n\\nCertified robustness, more specifically, certified perturbation size derived in Section 2.3 and Section 3, was used by defenders to defend GNN models against attacks. On the other hand, from the attacker perspective, he can leverage the properties of certified robustness to better attack GNN models. Specifically, certified perturbation size of a node characterizes the extent to which the GNN model provably and accurately predicts this node against the worst-case graph perturbation. An attacker can base on nodes' certified perturbation sizes to reversely reveal the vulnerable region of the graph and leverage this vulnerability to design better attacks. In particular, we have the following observation that reveals the inverse relationship between a node's certified perturbation size and the perturbation associated with this node when designing the attack.\\n\\nObservation 1: A node with a larger (smaller) certified perturbation size should be disrupted with a smaller (larger) number of perturbed edges.\\n\\nIf a node has a larger (smaller) certified perturbation size, it means this node is more (less) robust to graph perturbations. To misclassify this node, an attacker should allocate a larger (smaller) number of perturbed edges. Thus, to design more effective attacks (i.e., misclassify more nodes) with a perturbation budget, an attacker should avoid disrupting nodes with larger certified perturbation sizes, but focus on nodes with smaller certified perturbation sizes.\\n\\nBased on the above observation, our attack needs to solve three correlated problems: i) How to obtain the node's certified perturbation size for both graph evasion and poisoning attacks? ii) How to allocate the perturbation budget in order to disrupt the nodes with smaller certified perturbation sizes? iii) How to generate the adversarial graph perturbation for both evasion and poisoning attacks? To address i), we adopt the derived node's certified perturbation size against graph evasion attacks (Section 2.3) and graph poisoning attacks (Section 3). To address ii), we design a certified robustness inspired loss, by maximizing which an attacker will put more effort into disrupting nodes with smaller certified perturbation sizes. To address iii), we design a certified robustness inspired attack framework, where any existing graph evasion/poisoning attacks to GNNs can be adopted as the base attack in our framework.\\n\\n4.2. Certified Robustness Inspired Loss Design\\n\\nSuppose we have obtained nodes' certified perturbation sizes. To perform a more effective attack, a naive solution is that the attacker sorts all nodes' certified perturbation sizes in an ascending order, and then carefully perturbs the edges to misclassify the sorted nodes one-by-one until reaching the perturbation budget. However, this solution is both computationally intensive\u2014as it needs to solve an optimization...\"}"}
{"id": "CVPR-2023-2176", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Aleksandar Bojchevski and Stephan G\\\"unnemann. Adversarial attacks on node embeddings via graph poisoning. In ICML, 2019.\\n\\n[2] Aleksandar Bojchevski, Johannes Klicpera, and Stephan G\\\"unnemann. Efficient robustness certificates for discrete data: Sparsity-aware randomized smoothing for graphs, images and more. In ICML, 2020.\\n\\n[3] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In IEEE S&P, 2017.\\n\\n[4] Heng Chang, Yu Rong, Tingyang Xu, Wenbing Huang, Honglei Zhang, Peng Cui, Wenwu Zhu, and Junzhou Huang. A restricted black-box adversarial framework towards attacking graph embedding models. In AAAI, 2020.\\n\\n[5] Jinyin Chen, Yangyang Wu, Xuanheng Xu, Yixian Chen, Haibin Zheng, and Qi Xuan. Fast gradient attack on network embedding. arXiv, 2018.\\n\\n[6] Yizheng Chen, Yacin Nadji, Athanasios Kountouras, and more. Practical attacks against graph-based clustering. In CCS, 2017.\\n\\n[7] Jeremy M Cohen, Elan Rosenfeld, and J Zico Kolter. Certified adversarial robustness via randomized smoothing. In ICML, 2019.\\n\\n[8] Hanjun Dai, Hui Li, Tian Tian, and more. Adversarial attack on graph structured data. In ICML, 2018.\\n\\n[9] Quanyu Dai, Qiang Li, Jian Tang, and Dan Wang. Adversarial network embedding. In AAAI, 2018.\\n\\n[10] Negin Entezari, Saba A Al-Sayouri, Amirali Darvishzadeh, and Evangelos E Papalexakis. All you need is low (rank) defending against adversarial attacks on graphs. In WSDM, 2020.\\n\\n[11] Simon Geisler, Tobias Schmidt, Hakan S\u0131r\u0131n, Daniel Z\\\"uger, Aleksandar Bojchevski, and Stephan G\\\"unnemann. Robustness of graph neural networks at scale. In NeurIPS, 2021.\\n\\n[12] Jinyuan Jia, Binghui Wang, Xiaoyu Cao, and Neil Gong. Certified robustness of community detection against adversarial structural perturbation via randomized smoothing. In WWW, 2020.\\n\\n[13] Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, and Jiliang Tang. Graph structure learning for robust graph neural networks. In KDD, 2020.\\n\\n[14] Thomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In ICLR, 2017.\\n\\n[15] Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, and Suman Jana. Certified robustness to adversarial examples with differential privacy. In IEEE S & P, 2019.\\n\\n[16] Alexander Levine and Soheil Feizi. Robustness certificates for sparse adversarial attacks by randomized ablation. In AAAI, 2020.\\n\\n[17] Bai Li, Changyou Chen, Wenlin Wang, and Lawrence Carin. Certified adversarial robustness with additive noise. 2019.\\n\\n[18] Jia Li, Honglei Zhang, Zhichao Han, and et al. Adversarial attack on community detection by hiding individuals. In WWW, 2020.\\n\\n[19] Xuanqing Liu, Si Si, Xiaojin Zhu, Yang Li, and Cho-Jui Hsieh. A unified framework for data poisoning attack to graph-based semi-supervised learning. In NeurIPS, 2019.\\n\\n[20] Jiaqi Ma, Shuangrui Ding, and Qiaozhu Mei. Towards more practical adversarial attacks on graph neural networks. In NeurIPS, 2020.\\n\\n[21] Yao Ma, Suhang Wang, Tyler Derr, Lingfei Wu, and Jiliang Tang. Attacking graph convolutional networks via rewiring. arXiv, 2019.\\n\\n[22] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In ICLR, 2018.\\n\\n[23] Jiaming Mu, Binghui Wang, Qi Li, Kun Sun, Mingwei Xu, and Zhuotao Liu. A hard label black-box adversarial attack to graph neural networks. In CCS, 2021.\\n\\n[24] Felix Mujkanovic, Simon Geisler, Stephan G\\\"unnemann, and Aleksandar Bojchevski. Are defenses for graph neural networks robust? In NeurIPS, 2022.\\n\\n[25] Mark Newman. Networks. Oxford university press, 2018.\\n\\n[26] Jerzy Neyman and Egon Sharpe Pearson. Ix. on the problem of the most efficient tests of statistical hypotheses. 1933.\\n\\n[27] Ryan Rossi and Nesreen Ahmed. The network data repository with interactive graph analytics and visualization. In AAAI, 2015.\\n\\n[28] Hadi Salman, Jerry Li, Ilya Razenshteyn, and more. Provably robust deep learning via adversarially trained smoothed classifiers. In NeurIPS, 2019.\\n\\n[29] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-Rad. Collective classification in network data. AI magazine, 29(3):93\u201393, 2008.\\n\\n[30] Yiwei Sun, Suhang Wang, Xianfeng Tang, Tsung-Yu Hsieh, and Vasant Honavar. Adversarial attacks on...\"}"}
{"id": "CVPR-2023-2176", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"graph neural networks via node injections: A hierarchical reinforcement learning approach. In The Web Conference, 2020.\\n\\nTsubasa Takahashi. Indirect adversarial attacks via poisoning neighbors for graph convolutional networks. In 2019 IEEE International Conference on Big Data (Big Data), 2019.\\n\\nMohamad Ali Torkamani and Daniel Lowd. Convex adversarial collective classification. In ICML, 2013.\\n\\nPetar Veli\u010dkovi\u0107, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. In ICLR, 2018.\\n\\nBinghui Wang and Neil Gong. Attacking graph-based classification via manipulating the graph structure. In CCS, 2019.\\n\\nBinghui Wang, Jinyuan Jia, Xiaoyu Cao, and Neil Gong. Certified robustness of graph neural networks against adversarial structural perturbation. In KDD, 2021.\\n\\nBinghui Wang, Youqi Li, and Pan Zhou. Bandits for black-box attacks to graph neural networks with structure perturbation. In CVPR, 2022.\\n\\nBinghui Wang, Tianxiang Zhou, Minhua Lin, Pan Zhou, Ang Li, Meng Pang, Cai Fu, Hai Li, and Yiran Chen. Evasion attacks to graph neural networks via influence function. arXiv, 2020.\\n\\nFelix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu, and Kilian Q Weinberger. Simplifying graph convolutional networks. In ICML, 2019.\\n\\nHuijun Wu, Chen Wang, Yuriy Tyshetskiy, Andrew Docherty, Kai Lu, and Liming Zhu. Adversarial examples on graph data: Deep insights into attack and defense. In IJCAI, 2019.\\n\\nKaidi Xu, Hongge Chen, Sijia Liu, and more. Topology attack and defense for graph neural networks: An optimization perspective. In IJCAI, 2019.\\n\\nKeyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks? In ICLR, 2019.\\n\\nGreg Yang, Tony Duan, J Edward Hu, Hadi Salman, Ilya Razenshteyn, and Jerry Li. Randomized smoothing of all shapes and sizes. In ICML, 2020.\\n\\nRuntian Zhai, Chen Dan, Di He, Huan Zhang, Boqing Gong, Pradeep Ravikumar, Cho-Jui Hsieh, and Liwei Wang. Macer: Attack-free and scalable robust training via maximizing certified radius. In ICLR, 2020.\\n\\nMuhan Zhang and Yixin Chen. Link prediction based on graph neural networks. In NIPS, 2018.\\n\\nXiang Zhang and Marinka Zitnik. GnnGuard: Defending graph neural networks against adversarial attacks. In NeurIPS, 2020.\\n\\nZaixi Zhang, Jinyuan Jia, Binghui Wang, and Neil Gong. Backdoor attacks to graph neural networks. 2021.\\n\\nZijie Zhang, Zeru Zhang, Yang Zhou, Yelong Shen, Ruoming Jin, and Dejing Dou. Adversarial attacks on deep graph matching. 2020.\\n\\nDingyuan Zhu, Ziwei Zhang, Peng Cui, and Wenwu Zhu. Robust graph convolutional networks against adversarial attacks. In KDD, 2019.\\n\\nJun Zhuang and Mohammad Al Hasan. Defending graph convolutional networks against dynamic graph perturbations via Bayesian self-supervision. In AAAI, 2022.\\n\\nDaniel Z\u00fcgner, Amir Akbarnejad, and Stephan Gnnemann. Adversarial attacks on neural networks for graph data. In KDD, 2018.\\n\\nDaniel Z\u00fcgner and Stephan Gnnemann. Adversarial attacks on graph neural networks via meta learning. In ICLR, 2019.\"}"}
{"id": "CVPR-2023-2176", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"problem for each node; and suboptimal\u2014as all nodes and the associated edges collectively make predictions and perturbing an edge could affect predictions of many nodes.\\n\\nWe design a certified perturbation size inspired loss that assists to automatically seeking the \u201cideal\u201d edges to be perturbed for both evasion attacks and poisoning attacks. Particularly, we notice that the loss function of evasion attacks in Equation 3 or poisoning attacks in Equation 5 is defined per node. Then, we propose to modify the loss function in Equation 3 or Equation 5 by assigning each node with a weight and multiplying each node loss with the corresponding weight, where the node weight has a strong connection with the node\u2019s certified perturbation size. Formally, we design the certified perturbation size inspired loss as follows:\\n\\n\\\\[ L_{CR}(f, A, V_T) = \\\\sum_{u} V_T w(u) \\\\cdot \\\\` (f(\\\\hat{A}; u), y_u), \\\\]\\n\\nwhere \\\\( V_T = V_Te \\\\) for evasion attacks and \\\\( V_T = V_Tr \\\\) for poisoning attacks; and \\\\( w(u) \\\\) is the weight of the node \\\\( u \\\\).\\n\\nNote that when setting all nodes with an equal weight, our certified perturbation size inspired loss reduces to the conventional loss in Equation 3 or Equation 5. Next, we show the inverse relationship between the node\u2019s certified perturbation size and the assigned weight.\\n\\nObservation 2: A node with a larger (smaller) certified perturbation size is assigned a smaller (larger) weight. As shown in Observation 1, we should disrupt more nodes with smaller certified perturbation sizes, as these nodes are more vulnerable. In other words, we should put more weights on nodes with smaller certified perturbation sizes to enlarge these nodes\u2019 losses\u2014making these nodes easier to be misclassified with graph perturbations. In contrast, we should put smaller weights on nodes with larger certified perturbation sizes, in order to save the usage of the perturbation budget. Formally, we assign the node weight such that \\\\( w(u) \\\\sim \\\\frac{1}{1 + e^{a \\\\cdot K(p_y u)}} \\\\).\\n\\nThere are many ways to assign node weights satisfying the inverse relationship. In this paper, for instance, we propose to define node weights as \\\\( w(u) = \\\\frac{1}{1 + e^{a \\\\cdot K(p_y u)}} \\\\), where \\\\( a \\\\) is a tunable hyperparameter. We can observe that the node weight is exponentially decreased as the node\u2019s certified perturbation size increases. Such a property ensures that the majority of perturbed edges are used for disrupting nodes with smaller certified perturbation sizes (See Figures 3) when performing the attack.\\n\\n4.3. Certified Robustness Inspired Attack Design\\n\\nBased on the derived certified perturbation size and our certified robustness inspired loss, we now propose to generate graph perturbations against GNNs with both graph evasion and poisoning attacks.\\n\\nCertified robustness inspired graph evasion attacks to generate graph perturbations. We can choose any graph evasion attack to GNNs as the base evasion attack. In particular, given the attack loss from any existing evasion attack, we only need to modify the loss by multiplying it with our certification perturbation sizes defined node weights. For instance, we can use the PGD attack \\\\([40]\\\\) as the base evasion attack. We replace its attack loss by our certified robustness inspired loss \\\\( L_{CR} \\\\) in Equation 12. Then, we have our certified robustness inspired PGD (CR-PGD) evasion attack that iteratively generates graph perturbations as follows:\\n\\n\\\\[\\n\\\\hat{\\\\vx} = \\\\text{Proj}_B(\\\\vx + \\\\tau \\\\cdot r \\\\cdot L_{CR}(f, A, V_{Te})),\\n\\\\]\\n\\nwhere \\\\( \\\\tau \\\\) is the learning rate in PGD, \\\\( B = \\\\left\\\\{ \\\\vx : 1^T \\\\vx \\\\leq \\\\alpha \\\\right\\\\} \\\\) is the allowable perturbation set, and \\\\( \\\\text{Proj}_B(\\\\vx) = \\\\left( \\\\frac{0}{1^T \\\\vx}, \\\\frac{1}{1^T \\\\vx} \\\\right) \\\\cdot \\\\vx, \\\\) if \\\\( 1^T \\\\vx = \\\\alpha \\\\), \\\\( \\\\frac{0}{1^T \\\\vx} \\\\cdot \\\\vx, \\\\) if \\\\( 1^T \\\\vx \\\\leq \\\\alpha \\\\), \\\\( \\\\frac{1}{1^T \\\\vx} \\\\cdot \\\\vx, \\\\) if \\\\( 1^T \\\\vx > \\\\alpha \\\\).\\n\\nThe final graph perturbation is used to perform the evasion attack.\\n\\nCertified robustness inspired graph poisoning attacks to generate graph perturbations. Likewise, we can choose any graph poisoning attack to GNNs as the base poisoning attack. Given the bilevel loss from any existing poisoning attack, we simply modify each loss by multiplying it with our certified perturbation sizes' defined node weights. Specifically, we have\\n\\n\\\\[\\n\\\\max_{\\\\vx} L_{CR}(f, A, V_{Tr}),\\n\\\\]\\n\\ns.t.\\n\\n\\\\[\\n\\\\vx = \\\\arg \\\\min_{\\\\vx} L_{CR}(f, A, V_{Tr}),\\n\\\\]\\n\\nwhere \\\\( L_{CR}(f, A, V_{Tr}) = \\\\sum_{v} V_{Tr} w(v) \\\\cdot \\\\` (f(A; v), y_v) \\\\).\\n\\nThen, solving Equation 16 and Equation 17 produces the poisoning attack graph perturbations with our framework.\\n\\nAlgorithm 1 and Algorithm 2 in Appendix show two instances of applying our CR inspired attack framework to the PGD evasion attack and Minmax [40] poisoning graph, respectively. To save time, we calculate nodes\u2019 certified perturbation sizes per \\\\( \\\\text{INT} \\\\) iterations. Then, comparing with PGD, the computational overhead of our CR-PGD is calculating the node\u2019s certified perturbation size with a set of \\\\( N \\\\) sampled noises every \\\\( \\\\text{INT} \\\\) iterations, which only involves making predictions on \\\\( N \\\\) noisy matrices and is efficient.\\n\\nNote that the predictions are independent and can be also parallelized. Comparing with Minmax, the computational overhead of our CR-Minmax is to independently train (a small number of) \\\\( N \\\\) models every \\\\( \\\\text{INT} \\\\) iterations, which can be implemented in parallel.\"}"}
{"id": "CVPR-2023-2176", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. Experiments\\n\\n5.1. Setup\\n\\nDatasets and GNN models. Following [40, 51], we evaluate our attacks on benchmark graph datasets, i.e., Cora, Citeseer [29], and BlogCatalogs [27]. Table 3 in Appendix shows basic statistics of these graphs. We choose Graph Convolutional Network (GCN) [14] as the targeted GNN model, also following [40, 51].\\n\\nBase attack methods. For graph evasion attacks, we choose the PGD attack [40] that uses the cross-entropy loss and CW loss [3] as the base attack methods, and denote the two attacks as CE-PGD and CW-PGD, respectively. For graph poisoning attacks, we choose the Minmax attack [40] and MetaTrain attack [51] as the base attack methods. We apply our CR inspired attack framework to these evasion and poisoning attacks and denote them as CR-CE-PGD, CR-CW-PGD, CR-Minmax, and CR-MetaTrain, respectively. All attacks are implemented in PyTorch and run on a Linux server with 96 core 3.0GHz CPU, 768GB RAM, and 8 Nvidia A100 GPUs.\\n\\nTraining and testing. Following [51], we split the datasets into 10% training nodes, 10% validation nodes, and 80% testing nodes. The validation nodes are used to tune the hyperparameters, and the testing nodes are used to evaluate the attack performance. We repeat all attacks on 5 different splits of the training/evaluation/testing nodes and report the mean attack accuracy on testing nodes, i.e., fraction of testing nodes are misclassified after the attack.\\n\\nParameter settings. Without otherwise mentioned, we set the perturbation budget as 20% of the total number of edges in a graph (before attack). We set the parameter $a=0.999$ in the noise distribution Equation 7, the confidence level $\\\\alpha=0.9$, the number of samples $N$ in Monte Carlo sampling to calculate node's certified perturbation size is set to be 200 and 20 in evasion attacks and poisoning attacks, respectively, and $a=1$ in Equation 13. The number of iterations $T$ is 100 and 10, and the interval is set to be $\\\\text{INT}=10$ and $\\\\text{INT}=2$ in evasion attacks and poisoning attacks, respectively. The other hyperparameters in CE-PGD, CW-PGD, Minmax, and MetaTrain are selected based on their source code, and we set equal values in our CR inspired attack counterparts. We also study the impact of the important hyperparameters that could affect our attack performance: $a$, $N$, $\\\\alpha$, and $\\\\alpha$. When studying the impact of a hyperparameter, we fix the other hyperparameters to be their default values.\\n\\n5.2. Attack Results\\n\\nOur attack framework is effective. Figure 1 and Figure 2 show the evasion attack accuracy and poisoning attack accuracy of the base attacks and those with our attack framework vs. perturbation budget, respectively. We can observe that our certified robustness inspired attack framework can enhance the base attack performance in all datasets. For instance, when attacking GCN on Cora and the perturbation ratio is 20%, our CR-CE-PGD and CR-CW-PGD have a relative 7.0% and 5.6% gain over the CE-PGD and CW-PGD evasion attacks. Moreover, CR-Minmax and CR-MetaTrain have a relative 12.2% and 10.3% gain over the Minmax and MetaTrain poisoning attacks. These results demonstrate that the node's certified robustness can indeed guide our attack framework to find the more vulnerable region in the graph to be perturbed, which helps to better allocate the perturbation budget, and thus makes the base attacks with our attack framework misclassify more nodes.\"}"}
{"id": "CVPR-2023-2176", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":false,\"rotation_correction\":90,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Certified perturbation size\\n\\nPercentage of perturbed edges\\n\\n(a) CW-PGD vs. CR-CW-PGD\\n\\n(b) CE-PGD vs. CR-CE-PGD\\n\\n(c) Minmax vs. CR-Minmax\\n\\n(d) MetaTrain vs. CR-MetaTrain\\n\\nFigure 3. Distribution of the perturbed edges vs. node's certified perturbation size on Citeseer.\\n\\nTo further understand the effectiveness of our framework, we visualize the distribution of the perturbed edges vs. node's certified perturbation size. Specifically, we first obtain the perturbed edges via the base attacks and our CR inspired attacks, and calculate testing/training nodes' certified perturbation sizes for evasion/poisoning attacks, respectively. Then we plot the distribution of the perturbed edges vs node's certified perturbation size. Specifically, if a perturbed edge is connected with a testing/training node in the evasion/poisoning attack, then we map this perturbed edge to this node's certified perturbation size. Our intuition is that a perturbed edge affects its connected node the most. Figure 3 shows the results on Citeseer (We observe that the conclusions on the other datasets are similar). We can see that a majority number of the perturbed edges connect with testing/training nodes that have relatively smaller certified perturbation sizes in our CR inspired attacks. In contrast, a significant number of the perturbed edges in the base attacks connect with nodes with relatively larger certified perturbation sizes. Hence, under a fixed perturbation budget, our attacks can misclassify more nodes.\\n\\nComparing with other weight design strategies. Recall that our weight design is based on node's certified robustness: nodes less provably robust to graph perturbations are assigned larger weights, in order to enlarge these node attack losses. Here, we consider three other possible strategies to design node weights that aim to empirically capture this property: 1) Random, where we uniformly assign node weights between \\\\([0, 1]\\\\) at random; 2) Node degree, where a node with a smaller degree might be less robust to graph perturbations, and we set a larger weight. Following our weight design, we set \\\\(w_{\\\\text{deg}}(u) = \\\\frac{1}{1 + \\\\exp(a \\\\cdot \\\\text{deg}(u))}\\\\); 3) Node centrality, where a node with a smaller centrality might be less robust to graph perturbations, and we set a larger weight. Similarly, we set \\\\(w_{\\\\text{cen}}(u) = \\\\frac{1}{1 + \\\\exp(a \\\\cdot \\\\text{cen}(u))}\\\\).\\n\\nAs a baseline, we also consider no node weights. Table 1 shows the attack results by applying these weight design strategies to the existing graph evasion and poisoning attacks. We have the following observations: 1) Random obtains the attack performance even worse than No weight's. This indicates an inappropriate weight design can be harmful to the attack. 2) Node Degree and Centrality perform slightly better than No weight. One possible reason is that nodes with larger degree and centrality are empirically more robust to perturbations, which are also observed in previous works, e.g., [34, 50]. 3) Our weight design strategy performs the best. This is because our weight design intrinsically captures nodes' certified robustness and thus yields more effective attacks.\\n\\nAblation study. In this experiment, we study the impact of hyperparameters: \\\\(\\\\alpha\\\\) in Equation 7, confidence level \\\\(\\\\varepsilon\\\\) in Equation 9, and \\\\(N\\\\) in Equation 9, \\\\(a\\\\) in Equation 13, as well the running time vs. \\\\(N\\\\). Figure 4 shows the results of \\\\(\\\\varepsilon\\\\), \\\\(\\\\alpha\\\\), and \\\\(N\\\\), and running time vs. \\\\(N\\\\) on our attack. We observe that: 1) Our attack is not sensitive to \\\\(\\\\varepsilon\\\\); 2) Our attack slightly becomes worse as the confidence level \\\\(\\\\varepsilon\\\\) increases. Such an observation can guide an attacker to set a relatively small \\\\(\\\\varepsilon\\\\) in practice. 3) Our attack becomes better as \\\\(N\\\\) increases, but already works well with a relatively smaller \\\\(N\\\\). From this observation, an attacker can choose a small \\\\(N\\\\) in practice to save the time and cost when performing the attack. 4) Running time does not increase too much with \\\\(N\\\\) on the evasion attacks and is linear to \\\\(N\\\\) on poisoning attacks, consistent with our analysis in Section 4.3.\\n\\nTable 2 shows the impact of \\\\(a\\\\). We see the performances are stable across different \\\\(a\\\\). This is largely because our weight design already ensures the node weight is inversely and exponentially to the node's certified perturbation size.\"}"}
{"id": "CVPR-2023-2176", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4. Impact of (a) Noise parameter, (b) Confidence level, (c) Number of samples, and (d) Running time vs. $N$ on Citeseer. Note that (c): \u201cNo evasion attack\u201d and \u201cNo poisoning attack\u201d curves are overlapped; (d) INT = 10 (2) for our evasion (poisoning) attacks.\\n\\n6. Discussion\\n\\nEvaluations on other GNNs.\\n\\nWe mainly follow existing attacks \\\\[40,51\\\\], which only evaluate GCN. Here, we also test SGC \\\\[38\\\\] on Cora and results show our CR-based GNN attacks also have a 6%-12% gain over the base attacks. This validates our strategy is generic to design better attacks.\\n\\nTransferability between different GNNs.\\n\\nWe evaluate the transferability of the graph perturbations generated by our CR-based attacks on GCN to SGC on Cora, when the attack budget is 15. Accuracy on SGC under the 4 attacks are: 73%, 76%, 66%, and 67%, while accuracy on GCN are 71%, 73%, 63%, and 65%, respectively. This implies a promising transferability between GCN and SGC.\\n\\nDefenses against our attacks.\\n\\nAlmost all existing empirical defenses \\\\[10,11,13,39,45,48,49\\\\] are ineffective to adaptive attacks \\\\[24\\\\]. We adopt adversarial training \\\\[22\\\\], which is the only known effective empirical defense. Specifically, we first generate graph perturbations for target nodes via our attack and use the perturbed graph to retrain GNN with true node labels. The trained GNN is used for evaluation. We test on Cora and show this defense is effective to some extent, but has a nonnegligible utility loss. For instance, when budget=15, the accuracy under the CR-CW-PGD (CR-CE-PGD) attack increases from 73% (71%) to 76% (73%), but the normal accuracy reduces from 84% to 73% (72%).\\n\\n7. Related Work\\n\\nAttacking graph neural networks (GNNs).\\n\\nWe classify the existing attacks to GNNs as evasion attacks \\\\[8,20,21,23,36,39,40,50\\\\] and poisoning attacks \\\\[8,19,30,31,40,46,50,51\\\\]. E.g., Xu et al. \\\\[40\\\\] proposed an untargeted PGD graph evasion attack to the GCN. The PGD attack leverages first-order optimization and generates discrete graph perturbations via convexly relaxing the binary graph structure, and obtains the state-of-the-art attack performance. Regarding graph poisoning attacks, Zugner et al. \\\\[51\\\\] proposed a graph poisoning attack, called Metattack, that perturbs the whole graph based on meta-learning. Our attack framework can be seamlessly plugged into these graph evasion and poisoning attacks and enhance their attack performance.\\n\\nAttacking other graph-based methods.\\n\\nBesides attacking GNNs, other adversarial attacks against graph data include attacking graph-based clustering \\\\[6\\\\], graph-based collective classification \\\\[32,34\\\\], graph embedding \\\\[1,4,5,9\\\\], community detection \\\\[18\\\\], graph matching \\\\[47\\\\], etc. For instance, Chen et al. \\\\[6\\\\] proposed a practical attack against spectral clustering, which is a well-known graph-based clustering method. Wang and Gong \\\\[34\\\\] designed an attack to the collective classification method, called linearized belief propagation, by modifying the graph structure.\\n\\nCertified robustness and randomized smoothing.\\n\\nRandomized smoothing \\\\[7,15\u201317,42,43\\\\] was the first method to obtain certified robustness of large models and achieved state-of-the-art performance. For instance, Cohen et al. \\\\[7\\\\] leveraged the Neyman-Pearson Lemma \\\\[26\\\\] to obtain a tight $l_2$ certified robustness for randomized smoothing with Gaussian noise on normally trained image models. Salman et al. \\\\[28\\\\] improved the certified robustness by combining the design of an adaptive attack against smoothed soft image classifiers and adversarial training on the attacked classifiers. \\\\[12,35,2\\\\] applied randomized smoothing in the graph domain and derived certified robustness for community detection and node/graph classifications methods against graph perturbations. In this paper, we use randomized smoothing to design better attacks against GNNs.\\n\\n8. Conclusion\\n\\nWe study graph evasion and poisoning attacks to GNNs and propose a novel attack framework motivated by certified robustness. We are the first work that uses certified robustness for an attack purpose. In particular, we first derive the node\u2019s certified perturbation size, by extending randomized smoothing from the classifier perspective to a general function perspective. Based on it, we design certified robustness inspired node weights, which can be seamlessly plugged into the existing graph perturbation attacks\u2019 loss and produce our certified robustness inspired attack loss and attack framework. Evaluations on multiple datasets demonstrate that existing attacks\u2019 performance can be significantly enhanced by applying our attack framework.\\n\\nAcknowledgments.\\n\\nThis work was supported by Wang\u2019s startup funding, the Cisco Research Award, and the National Science Foundation under grant No. 2216926. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the funding agencies.\"}"}
