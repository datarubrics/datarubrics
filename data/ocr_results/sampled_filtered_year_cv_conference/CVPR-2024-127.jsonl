{"id": "CVPR-2024-127", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and generative models for 3D point clouds. In International conference on machine learning, pages 40\u201349. PMLR, 2018.\\n\\n[2] Martin Arjovsky and Leon Bottou. Towards principled methods for training generative adversarial networks. In International Conference on Learning Representations, 2017.\\n\\n[3] Martin Arjovsky, Soumith Chintala, and L\u00e9on Bottou. Wasserstein generative adversarial networks. In International conference on machine learning, pages 214\u2013223. PMLR, 2017.\\n\\n[4] Mohammadreza Armandpour, Huangjie Zheng, Ali Sadeghian, Amir Sadeghian, and Mingyuan Zhou. Reimagine the negative prompt algorithm: Transform 2d diffusion into 3d, alleviate janus problem and beyond. arXiv preprint arXiv:2304.04968, 2023.\\n\\n[5] David Bau, Jun-Yan Zhu, Hendrik Strobelt, Bolei Zhou, Joshua B Tenenbaum, William T Freeman, and Antonio Torralba. Gan dissection: Visualizing and understanding generative adversarial networks. arXiv preprint arXiv:1811.10597, 2018.\\n\\n[6] Andrew Brock, Theodore Lim, James M Ritchie, and Nick Weston. Neural photo editing with introspective adversarial networks. arXiv preprint arXiv:1609.07093, 2016.\\n\\n[7] Rui Chen, Yongwei Chen, Ningxin Jiao, and Kui Jia. Fantasia3d: Disentangling geometry and appearance for high-quality text-to-3d content creation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22246\u201322256, 2023.\\n\\n[8] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai, Gang Yu, Lei Yang, and Guosheng Lin. It3d: Improved text-to-3d generation with explicit view synthesis. arXiv preprint arXiv:2308.11473, 2023.\\n\\n[9] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 8789\u20138797, 2018.\\n\\n[10] Brian Dolhansky and Cristian Canton Ferrer. Eye in-painting with exemplar generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7902\u20137911, 2018.\\n\\n[11] Ali Edalati, Marzieh Tahaei, Ivan Kobyzev, Vahid Partovi Nia, James J Clark, and Mehdi Rezagholizadeh. Krona: Parameter efficient tuning with kronecker adapter. arXiv preprint arXiv:2212.10650, 2022.\\n\\n[12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Haim Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. In International Conference on Learning Representations, 2022.\\n\\n[13] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.\\n\\n[14] Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron C Courville. Improved training of wasserstein gans. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2017.\\n\\n[15] Yuan-Chen Guo, Ying-Tian Liu, Ruizhi Shao, Christian Laforte, Vikram Voleti, Guan Luo, Chia-Hao Chen, Zixin Zou, Chen Wang, Yan-Pei Cao, and Song-Hai Zhang. threestudio: A unified framework for 3d content generation. https://github.com/threestudio-project/threestudio, 2023.\\n\\n[16] Zhenliang He, Wangmeng Zuo, Meina Kan, Shiguang Shan, and Xilin Chen. Attgan: Facial attribute editing by only changing what you want. IEEE transactions on image processing, 28(11):5464\u20135478, 2019.\\n\\n[17] Amir Hertz, Kfir Aberman, and Daniel Cohen-Or. Delta denoising score. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2328\u20132337, 2023.\\n\\n[18] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.\\n\\n[19] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2021.\\n\\n[20] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In Proceedings of the European conference on computer vision (ECCV), pages 172\u2013189, 2018.\\n\\n[21] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Globally and locally consistent image completion. ACM Transactions on Graphics (ToG), 36(4):1\u201314, 2017.\\n\\n[22] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1125\u20131134, 2017.\\n\\n[23] Abdul Jabbar, Xi Li, and Bourahla Omar. A survey on generative adversarial networks: Variants, applications, and training. ACM Computing Surveys (CSUR), 54(8):1\u201349, 2021.\\n\\n[24] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):1\u201314, 2023.\\n\\n[25] Subin Kim, Kyungmin Lee, June Suk Choi, Jongheon Jeong, Kihyuk Sohn, and Jinwoo Shin. Collaborative score distillation for consistent visual synthesis. arXiv preprint arXiv:2307.04787, 2023.\\n\\n[26] Taeksoo Kim, Moonsu Cha, Hyunsoo Kim, Jung Kwon Lee, and Jiwon Kim. Learning to discover cross-domain relations with generative adversarial networks. In International conference on machine learning, pages 1857\u20131865. PMLR, 2017.\\n\\n[27] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse image-to-image translation via disentangled representations. In Proceedings of the European conference on computer vision (ECCV), pages 8139.\"}"}
{"id": "CVPR-2024-127", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[28] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 4582\u20134597, 2021.\\n\\n[29] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 300\u2013309, 2023.\\n\\n[30] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Advances in Neural Information Processing Systems, 35:1950\u20131965, 2022.\\n\\n[31] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. Advances in neural information processing systems, 30, 2017.\\n\\n[32] Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, and Jie Tang. Gpt understands, too. AI Open, 2023.\\n\\n[33] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diff-instruct: A universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[34] Anton Mallasto, Guido Mont\u00fafar, and Augusto Gerolin. How well do wgangs estimate the Wasserstein metric? arXiv preprint arXiv:1910.03875, 2019.\\n\\n[35] Xudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2017.\\n\\n[36] Gal Metzer, Elad Richardson, Or Patashnik, Raja Giryes, and Daniel Cohen-Or. Latent-nerf for shape-guided generation of 3d shapes and textures. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12663\u201312673, 2023.\\n\\n[37] Ben Mildenhall, Pratul P Srinivasan, Matthew Tancik, Jonathan T Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99\u2013106, 2021.\\n\\n[38] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM transactions on graphics (TOG), 41(4):1\u201315, 2022.\\n\\n[39] Seonghyeon Nam, Yunji Kim, and Seon Joo Kim. Text-adaptive generative adversarial networks: manipulating images with natural language. Advances in neural information processing systems, 31, 2018.\\n\\n[40] Xingang Pan, Bo Dai, Ziwei Liu, Chen Change Loy, and Ping Luo. Do 2d {gan}s know 3d shape? unsupervised 3d shape reconstruction from 2d image {gan}s. In International Conference on Learning Representations, 2021.\\n\\n[41] Ben Poole, Ajay Jain, Jonathan T Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In International Conference on Learning Representations, 2022.\\n\\n[42] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022.\\n\\n[43] Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023.\\n\\n[44] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In International Conference on Learning Representations, 2023.\\n\\n[45] Edward J Smith and David Meger. Improved adversarial systems for 3d object generation and reconstruction. In Conference on Robot Learning, pages 87\u201396. PMLR, 2017.\\n\\n[46] Kunpeng Song, Ligong Han, Bingchen Liu, Dimitris Metaxas, and Ahmed Elgammal. Diffusion guided domain adaptation of image generators. arXiv preprint arXiv:2212.04473, 2022.\\n\\n[47] StabilityAI. Stable diffusion 2-1. https://huggingface.co/stabilityai/stable-diffusion-2-1, 2023.\\n\\n[48] StabilityAI. Stable diffusion 2-1-base. https://huggingface.co/stabilityai/stable-diffusion-2-1-base, 2023.\\n\\n[49] Jan Stanczuk, Christian Etmann, Lisa Maria Kreusser, and Carola-Bibiane Sch\u00f6nlieb. Wasserstein gans work because they fail (to approximate the Wasserstein distance). arXiv preprint arXiv:2103.01678, 2021.\\n\\n[50] Chaoyue Wang, Chang Xu, Chaohui Wang, and Dacheng Tao. Perceptual adversarial networks for image-to-image transformation. IEEE Transactions on Image Processing, 27(8):4066\u20134079, 2018.\\n\\n[51] Haochen Wang, Xiaodan Du, Jiahao Li, Raymond A Yeh, and Greg Shakhnarovich. Score jacobian chaining: Lifting pretrained 2d diffusion models for 3d generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12619\u201312629, 2023.\\n\\n[52] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36, 2024.\\n\\n[53] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling. Advances in neural information processing systems, 29, 2016.\\n\\n[54] Jiqing Wu, Zhiwu Huang, Janine Thoma, Dinesh Acharya, and Luc Van Gool. Wasserstein divergence for gans. In Proceedings of the European Conference on Computer Vision (ECCV), 2018.\"}"}
{"id": "CVPR-2024-127", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bo Yang, Hongkai Wen, Sen Wang, Ronald Clark, Andrew Markham, and Niki Trigoni. 3d object reconstruction from a single depth view with adversarial learning. In Proceedings of the IEEE International Conference on Computer Vision (ICCV) Workshops, 2017.\\n\\nZili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dual-gan: Unsupervised dual learning for image-to-image translation. In Proceedings of the IEEE international conference on computer vision, pages 2849\u20132857, 2017.\\n\\nChaohui Yu, Qiang Zhou, Jingliang Li, Zhe Zhang, Zhibin Wang, and Fan Wang. Points-to-3d: Bridging the gap between sparse points and shape-controllable text-to-3d generation. In Proceedings of the 31st ACM International Conference on Multimedia, pages 6841\u20136850, 2023.\\n\\nXin Yu, Yuan-Chen Guo, Yangguang Li, Ding Liang, Song-Hai Zhang, and Xiaojuan Qi. Text-to-3d with classifier score distillation. arXiv preprint arXiv:2310.19415, 2023.\\n\\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A. Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 2223\u20132232, 2017.\\n\\nJun-Yan Zhu, Philipp Kr\u00e4henb\u00fchl, Eli Shechtman, and Alexei A. Efros. Generative visual manipulation on the natural image manifold, 2018.\\n\\nYuanzhi Zhu. Prolific dreamer 2d. https://github.com/yuanzhi-zhu/prolific_dreamer2d, 2023.\"}"}
{"id": "CVPR-2024-127", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Workflow of ASD. Green lines show the pipeline of generator optimization. Orange lines show the pipeline of discriminator optimization. The avatar of NeRF is adapted from [37]. See the supplementary for the algorithm description.\\n\\n\\\\[ \\\\tilde{x} \\\\text{ comes from a mixture distribution } \\\\tilde{\\\\mu}, \\\\text{ so it can be replaced by a combination of } x_r t \\\\text{ and } x_g t. \\\\]\\n\\n\\\\[ \\\\eta \\\\text{ and } \\\\gamma \\\\text{ are adjustable hyperparameters, to keep the penalty term positive, they are subject to } \\\\gamma \\\\geq -\\\\frac{1}{\\\\eta} \\\\left\\\\| \\\\epsilon x_r t ; y, t - \\\\epsilon x_r t ; \\\\phi, t \\\\right\\\\|_2^2 \\\\]\\n\\nIn practice, we find \\\\( \\\\eta = \\\\frac{1}{2}, \\\\gamma \\\\in [-1, 0) \\\\) works for most cases. See supplementary materials for the derivation details of Eq. (9). In SDS [41], this discriminator optimization is completely ignored, as there is no optimizable \\\\( \\\\phi \\\\).\\n\\nVSD [52] uses the LoRA or simple UNet branch to implement \\\\( \\\\phi \\\\), and updates it using \\\\( \\\\nabla \\\\phi \\\\left\\\\| \\\\epsilon x_g t ; \\\\phi, t - \\\\epsilon \\\\right\\\\|_2^2 \\\\), which is just a part of Eq. (9).\\n\\nWe propose Adversarial Score Distillation (ASD) based on the WGAN paradigm. Figure 3 illustrates the workflow of ASD. ASD uses Eq. (7) to update the parameters \\\\( \\\\theta \\\\) of a generator. This generator could be a NeRF [37, 38] in the text-to-3d task, or a general generator similar to that in [33], or just image pixels in the 2D distillation and image editing tasks. ASD maintains an optimizable discriminator by implementing \\\\( \\\\phi \\\\) with an optimizable conditional embedding [12] or LoRA [19], and updates this discriminator using the complete \\\\( L_D \\\\).\\n\\nHowever, in Eq. (9), the noisy real sample \\\\( x_r t \\\\) is unavailable for text-conditioned cases. For these cases, we can use the upper bound of \\\\( L_D \\\\), which does not contain \\\\( x_r t \\\\) terms.\\n\\n\\\\[ L_D' \\\\text{ for text-conditioned distillation.} \\\\]\\n\\nWhen \\\\( \\\\eta = \\\\frac{1}{2} \\\\), based on triangle and Cauchy\u2013Schwarz inequality, we have\\n\\n\\\\[ \\\\frac{1}{2} \\\\left\\\\| \\\\epsilon x_r t ; y, t - \\\\epsilon \\\\right\\\\|_2^2 \\\\leq \\\\left\\\\| \\\\epsilon x_r t ; y, t - \\\\epsilon \\\\phi, t \\\\right\\\\|_2^2 + \\\\left\\\\| \\\\epsilon x_r t ; \\\\phi, t - \\\\epsilon \\\\right\\\\|_2^2. \\\\]\\n\\nThus, we can rewrite Eq. (9) as\\n\\n\\\\[ \\\\nabla \\\\phi L_D \\\\leq \\\\nabla \\\\phi E_t, \\\\epsilon \\\\left[ \\\\left\\\\| \\\\epsilon x_g t ; \\\\phi, t - \\\\epsilon \\\\right\\\\|_2^2 + \\\\gamma \\\\left\\\\| \\\\epsilon x_g t ; y, t - \\\\epsilon \\\\phi, t \\\\right\\\\|_2^2 \\\\right] := \\\\nabla \\\\phi L_D' \\\\quad (10) \\\\]\\n\\nFor the case where real samples are unavailable, we use Eq. (10) to update the prompt \\\\( \\\\phi \\\\) of ASD. For cases where real samples are available, like image editing or image / 3D-conditioned distillation, we can continue to use Eq. (9).\"}"}
{"id": "CVPR-2024-127", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Optimization Steps\\n\\nFigure 5. Score distillation results with the prompt \u201ca photograph of a fox\u201d, \u201ca front view of an owl\u201d in 2D, and \u201ca colorful rooster\u201d, \u201ca plush dragon toy\u201d in 3D. Only using the classifier term is equivalent to using SDS with a huge CFG scale, which tends to get over-saturated results in both 2D and 3D distillation. ASD can recover from over-saturated initialization.\\n\\nClassifier Score Distillation.\\n\\nVery recently, the concurrent work Classifier Score Distillation (CSD) [58] highlights the importance of the classifier term in the SDS gradient. We observe that only using the classifier term is similar to using SDS with a very huge CFG scale (as the learning rate can be adjusted dynamically by the optimizer, the percentage of the gradient component is the key factor), thereby prone to over-saturation in both 2D and 3D distillation. As a very important trick, CSD proposes to use negative prompts to alleviate over-saturation, which is similar to the discriminator in Eq (5) with fixed \\\\( \\\\phi \\\\). Thanks to the adversarial capabilities, ASD can recover from over-saturation even with the over-saturated initialization. See Figure 5 for visualization.\\n\\nDiscussion of \\\\( \\\\gamma \\\\).\\n\\nWhen \\\\( \\\\gamma = 0 \\\\), Eq (10) becomes VSD, which can be considered as the WGAN [3, 54] discriminator loss with fixed penalty term for Lipschitz constraint. For \\\\( \\\\gamma < 0 \\\\), we decrease the weight of the penalty term, which can enhance the ability of the discriminator. For \\\\( \\\\gamma > 0 \\\\), we...\\n\\n| SDS@100 | VSD@7.5 | ASD@7.5 |\\n|---------|---------|---------|\\n| Figure 6. Quality comparisons of distillation results among SDS, VSD, and ASD. For the text-to-3D task, the effects of different representations vary widely, so we all compared the results from NeRF distillation for fair play. \"@7.5\" denotes \u201cat the CFG scale 7.5\u201d. \u201cT2I\u201d denotes text-to-image results. |\"}"}
{"id": "CVPR-2024-127", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Optimization Steps\\n\\nVSD\\n\\nASD\\n\\na realistic happy dog playing in the grass\\n\\nVSD\\n\\nASD\\n\\na pineapple\\n\\nFigure 7. Stability comparisons of the distillation progress between VSD and SDS in both 2D and 3D. As can be seen, ASD constantly keeps the main object and image / NeRF structures unchanged.\\n\\nincrease the weight of the penalty term. Specifically, when $\\\\gamma \\\\gg 0$, the penalty term will dominate Eq (10), which leads to $\\\\epsilon_{x^g t, y, t} \\\\approx \\\\epsilon_{x^g t, \\\\phi, t}$. Then the discriminator fails to recognize fake samples, and the gradient for generators (6) mainly comes from the implicit classifier term. Only using the classifier term can be considered as a special case of our paradigm in this situation.\\n\\n6. Experiments\\n\\nImplementation Details.\\n\\nWe choose ProlificDreamer-2D [61] as the code base for 2D score distillation, Three-studio [15] as the code base for the text-to-3D task, and DDS [17] as the code base for image editing. We employ Stable Diffusion v2-1-base [48] as the pretrained diffusion model except for the text-to-3D task, where the LoRA branch is initialized from Stable Diffusion v2-1 [42, 47], following the configuration of VSD [52] in Three-studio. We set $\\\\lambda$ in Eq. (7) to 7.5 by default and use Eq. (10) for discriminator optimization in both 2D score distillation and text-to-3D tasks. We set $\\\\gamma$ in Eq. (10) to -1 for 2D score distillation, and set to -0.5 for the text-to-3D task. Although here we use fixed $\\\\gamma$ for simplicity, we think that a dynamic $\\\\gamma$ based on prompts and optimization steps may yield better results. In the text-to-3D task, all other configurations are inherited from the default configuration of VSD in Three-studio [15].\\n\\nFor image editing, we feed the source image into the second discriminator $D(x_t; z)$ following DDS [17]. In such case, we use Eq. (10) to optimize the first discriminator $D(x_t; y)$ and employ Eq. (9) to optimize the second discriminator $D(x_t; z)$, where $\\\\gamma$ in Eq. (10) is set to -1, $\\\\eta$ and $\\\\gamma$ in Eq. (9) are set to 0 and 1, respectively. The source image of image editing is generated from Stable Diffusion v2-1 [47].\\n\\nQuality.\\n\\nWe first compare the distillation quality among SDS [41], VSD [52], and ASD. Figure 6 shows the visual results obtained by different distillation methods in 2D distillation and text-to-3D tasks. In 2D distillation, Figure 6 (a) demonstrates that ASD can consistently achieve better results than SDS and VSD for three types of images: portraits, realistic images, and caricatures. In comparison, VSD results are slightly more saturated (as shown in the bear image) and contain artifacts due to instability in the distillation process (as shown in the flamingo image). Both phenomena are also seen in the text-to-3D task, where the VSD results are slightly more saturated in color and have more structural artifacts. Thanks to the complete discriminator optimization, the proposed ASD can obtain clean 3D NeRF compared to VSD.\\n\\nStability.\\n\\nWe further evaluate the stability of ASD by comparing the distillation process with VSD. Figure 7 shows the distillation progress in 2D and 3D tasks. In 2D distillation, ASD always keeps the main objects and image structures unchanged. The gradient of the ASD generator loss is focused on color and details, where the color of the dog gradually changes from black to yellow. However, VSD is not that stable. Both the main object, the image structure,\"}"}
{"id": "CVPR-2024-127", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9. Comparisons between DDS and ASD on 2D image editing tasks. We test our ASD in three image editing situations: simplification, refinement, and replacement. For each case, we use three images to show the optimization progress.\\n\\nThe color of the dog, and other details are constantly changing during the VSD distillation process. We note that this instability has less impact on the text-to-3D task, as occupancy loss and NeRF help maintain structural stability. Despite this, the structures distilled by VSD still change faster than those distilled by ASD, resulting in structural artifacts (see the pineapple leaves). It is worth noting that both ASD and VSD will change objects or structures eventually, due to their adversarial nature. ASD can slow down and stabilize this process, thereby reducing artifacts.\\n\\nDiversity. By switching seeds, we can easily get various distillation results. Figure 8 exhibits several distillation results using the same text prompt but different seeds. It can be seen that ASD can always obtain high-quality results in both 2D and 3D tasks.\\n\\nImage Editing. In addition to typical score distillation tasks, we further extend ASD to zero-shot image editing. Figure 9 shows how ASD simplifies the source image, refines the source image, and replaces objects in the source image. For the simplification tasks, our ASD can remove the most unnecessary high-frequency details and retain the main content based on the target prompt. ASD can also add realistic details back to cartoon-style images, which is significantly better than the recent zero-shot image editing method DDS [17]. We also challenged the object replacement task. Our ASD method produces more reasonable, detailed, and realistic results based on semantic information instead of the rigid fashion of copying fixed objects and then pasting them at fixed locations. See Figure 9, the squirrel's tail still appears in the results of DDS. Besides reasonable object replacement, ASD can maintain the details of the source image as well.\\n\\nQuantitative Results. Table 1 presents quantitative evaluations of CLIP scores and user studies for 3D generation tasks, demonstrating that ASD performs favorably against SDS and VSD. See the supplementary for more discussion about CLIP scores.\\n\\n| Method        | CLIP B/32 | CLIP B/16 | CLIP L/14 |\\n|---------------|-----------|-----------|-----------|\\n| DreamFusion (SDS) | 0.3282    | 0.3290    | 0.2889    |\\n| ProlificDreamer (VSD) | 0.3203    | 0.3258    | 0.2789    |\\n| Ours (ASD)     | 0.3314    | 0.3376    | 0.2854    |\\n\\n| Method        | User Study | Details (%) | Semantic Alignment (%) | Overall Quality (%) |\\n|---------------|------------|-------------|------------------------|---------------------|\\n| ProlificDreamer (VSD) | 35.3       | 45.7        | 34.5                   |\\n| Ours (ASD)     | 64.7       | 54.3        | 65.5                   |\\n\\nTable 1. Quantitative Results. CLIP scores are the cosine similarity between multiple randomly rendered views of the 3D model and the given text prompt. User evaluation is conducted between VSD and ASD from three perspectives.\\n\\n7. Limitation\\nDespite showing favorable capabilities in terms of distillation quality, stability, and diversity, the speed of ASD is similar to VSD. We find that sharing the same added noise between generator optimization and discriminator optimization can speed up distillation by 30\u201350%, with a little quality sacrifice. However, distilling a 3D NeRF still takes more than 5 hours on a single A800 GPU. Using faster 3D representation [24], or faster PEFT [11, 19, 28, 30, 32] methods to update the discriminator, or designing forward-only methods to approximate $\\\\epsilon_x^t; \\\\phi^t$ may be promising solutions to this limitation.\\n\\n8. Conclusion\\nIn this paper, we proposed the Adversarial Score Distillation (ASD) under the WGAN paradigm. Compared to existing methods, our ASD exploits a complete discriminator loss with a principled discussion, which alleviates the CFG-sensitive issue and leads to impressive performances. More importantly, we elucidated the relationship between score distillation and GAN, which not only explains methodological issues of existing score distillation, but also enables the pretrained model to extend to downstream tasks by various GAN paradigms.\"}"}
{"id": "CVPR-2024-127", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adversarial Score Distillation: When score distillation meets GAN\\n\\nMin Wei 1\\nJingkai Zhou 2*\\nJunyao Sun 3\\nXuesong Zhang 1\\n\\n1 Beijing University of Posts and Telecommunications\\n2 Independent Researcher\\n3 South China University of Technology\\n\\n{mw, xuesongzhang}@bupt.edu.cn fs.jingkaizhou@gmail.com 201810108390@mail.scut.edu.cn\\n\\nAbstract\\nExisting score distillation methods are sensitive to classifier-free guidance (CFG) scale, manifested as over-smoothness or instability at small CFG scales, while over-saturation at large ones. To explain and analyze these issues, we revisit the derivation of Score Distillation Sampling (SDS) and decipher existing score distillation with the Wasserstein Generative Adversarial Network (WGAN) paradigm. With the WGAN paradigm, we find that existing score distillation either employs a fixed sub-optimal discriminator or conducts incomplete discriminator optimization, resulting in the scale-sensitive issue. We propose the Adversarial Score Distillation (ASD), which maintains an optimizable discriminator and updates it using the complete optimization objective. Experiments show that the proposed ASD performs favorably in 2D distillation and text-to-3D tasks against existing methods. Furthermore, to explore the generalization ability of our paradigm, we extend ASD to the image editing task, which achieves competitive results.\\n\\n1. Introduction\\nScore distillation is a rapidly growing technique attracting a lot of attention [17, 25, 33, 41, 51, 52, 58]. It transfers knowledge from a pretrained 2D diffusion model to downstream tasks, such as image editing [17, 25], video editing [25], diffusion model distillation [33, 43], and text-to-3D [4, 7, 8, 29, 36, 41, 44, 51, 52, 57, 58], with no downstream data required.\\n\\nHowever, as a milestone in score distillation methods, Score Distillation Sampling (SDS) [41] is very sensitive to the classifier-free guidance (CFG) scale. Figure 1 takes 2D score distillation as an example. When setting a small CFG scale, SDS tends to obtain over-smoothing results, whereas when setting a large one, the generated images become over-saturated. Recently, Variational Score Distillation (VSD) [52] alleviates the over-smoothing problem at small CFG scales, but we observe that VSD is a bit unstable during the distillation progress at small CFG scales, as shown in Figure 1. These phenomena make us conjecture that there are still some unrevealed methodological issues behind SDS and VSD.\\n\\nWe start from revisiting the derivation of SDS. In [41], the gradient of SDS is derived from the L2 loss of the diffusion model. This derivation holds only when the vanilla-predicted noise is used in the gradient. However, in practice, the SDS gradient exploits the CFG noise rather than the vanilla-predicted one. This little trick changes the whole thing. It means that, in practice, the real SDS loss is no longer the L2 loss of the diffusion model, but some other loss associated with the implicit classifier or discriminator.\\n\\n* Equal contribution\\nCorresponding authors: Jingkai Zhou, Xuesong Zhang\\n\\nSDS (CFG = 7.5) SDS (CFG = 100)\\n\\n(a) SDS is sensitive to the CFG scale.\\n\\nUnstable distillation progress of VSD.\\n\\nFigure 1. 2D score distillation examples with the prompts \\\"a photograph of an astronaut riding a horse\\\" and \\\"exterior frontal perspective shot of resort villa inspired by Mykonos architecture\\\".\\n\\nSDS is very sensitive to the CFG scales while VSD exhibits fluctuation of generated contents during distillation at small CFG scales. These phenomena make us conjecture that there are still some unrevealed methodological issues behind SDS and VSD.\\n\\nWe start from revisiting the derivation of SDS. In [41], the gradient of SDS is derived from the L2 loss of the diffusion model. This derivation holds only when the vanilla-predicted noise is used in the gradient. However, in practice, the SDS gradient exploits the CFG noise rather than the vanilla-predicted one. This little trick changes the whole thing. It means that, in practice, the real SDS loss is no longer the L2 loss of the diffusion model, but some other loss associated with the implicit classifier or discriminator.\\n\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\\n\\n8131\"}"}
{"id": "CVPR-2024-127", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"(a) ASD can generate photorealistic images through 2D score distillation. A dog with its reflection below a DSLR photo of a hamburger inside a restaurant.\\n\\n(b) ASD can generate high quality 3D NeRFs from scratch (only stage 1 of VSD [52]). A freshly baked loaf of sourdough bread on a cutting board, a ficus planted in a pot, a roast turkey on a platter, a pineapple.\\n\\n(c) ASD can extend to image editing with caption modification. A cat is posing next to a laptop computer, a dog is posing next to a laptop computer, a cartoon elephant.\\n\\nFigure 2. Examples generated by ASD in 2D distillation, image editing, and text-to-3D tasks. Stable Diffusion [47, 48] is used as the pretrained diffusion model. For more results please refer to our project page.\\n\\nTo figure out where the real SDS gradient comes from, we connect SDS to Generative Adversarial Networks (GANs) [3, 13, 54]. By carefully designing the discriminator using the diffusion model, we can naturally derive the SDS gradient from the generator loss in the Wasserstein GAN (WGAN) [3, 54]. This means that SDS inherently comes from WGAN. However, when we employ the WGAN paradigm to explain and analyze SDS, we find that the discriminator optimization in WGAN is ignored by SDS. More specifically, SDS only optimizes the generator loss in WGAN by using a fixed sub-optimal discriminator implemented with added noise. Similarly, VSD can also be represented with the WGAN paradigm, in which case the discriminator is optimizable but the optimization objective is incomplete compared to the WGAN discriminator loss, making the distillation progress unstable.\"}"}
{"id": "CVPR-2024-127", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this work, we propose Adversarial Score Distillation (ASD) based on the WGAN paradigm. ASD maintains an optimizable discriminator and optimizes it using the complete WGAN discriminator loss, thereby improving distillation stability and quality. Specifically, the discriminator can be implemented using the combination of diffusion models, where textual-inversion embedding [12] or LoRA [19] make it optimizable. During discriminator optimization, we propose two kinds of optimization objectives derived from the complete WGAN discriminator loss. One exploits both real sample distribution and a pretrained diffusion model and can therefore be applied to the image-conditioned task, such as image editing. The other only requires a pretrained diffusion model, which is suitable for text-conditioned cases. Experiments on 2D distillation and text-to-3D tasks demonstrate the superior performance of ASD compared to existing score distillation methods. See Figure 2 for some examples. In addition, we extend ASD to image editing via discriminator transformations and show that the Delta Denoising Score (DDS) [17] is a special case in our paradigm. Figure 2 and experiments show some competitive editing results. This demonstrates that our paradigm can be generalized to more GAN paradigms for various tasks.\\n\\nBeyond methodological improvements, the more important significance of bridging score distillation and GANs is to enable GAN paradigms to exploit powerful diffusion models in the form of score distillation. More specifically, we can extend a general diffusion model to various downstream tasks (e.g., text-to-3D, image editing, diffusion distillation) by designing GAN paradigms, which eliminates the need to redesign specific diffusion models or collect large amounts of fine-tuning samples.\\n\\nIn short, we make the following contributions:\\n\\n\u2022 Bridged score distillation and WGAN to explain the methodological issues of existing score distillation, which also enables the pretrained model to extend to various downstream tasks by designing GAN paradigms.\\n\u2022 Proposed ASD based on the WGAN paradigm that employs the complete WGAN discriminator loss, resulting in better distillation stability and quality.\\n\u2022 Comprehensive experiments show that the proposed ASD performs favorably against existing methods in 2D distillation, text-to-3D, and image editing tasks.\\n\\n2. Related Work\\n\\nScore Distillation. Score distillation, first proposed by [41] and [51], is a method to optimize the parameter space via distilling a pretrained diffusion model. In text-to-3D generation, Score Distillation Sampling (SDS) shows great potential for optimizing 3D representations based on differentiable rendering. However, SDS often suffers from problems such as over-saturation, over-smoothing, and low diversity. The recent work Variational Score Distillation (VSD) [52] has made tremendous progress in solving these problems, which optimizes a 3D distribution by employing the Wasserstein gradient flow. However, VSD is a bit unstable during the optimization progress at small CFG scales, which will be discussed later. Score distillation can also be used in image editing [17, 46]. Delta Denoising Score (DDS) [17] proposes to modify the image in a zero-shot way by only editing its caption. Moreover, StyleGAN-Fusion [46] and Diff-Instruct [33] apply score distillation to knowledge distillation between generators in a data-free manner. Our work will trace the origin of SDS and connect it with GAN to reveal its methodological issues.\\n\\nGenerative Adversarial Networks. GANs [2, 3, 13, 14, 23, 35, 54] have achieved great success in generating realistic and clear images, such as image in-painting [10, 21], image manipulation applications [5, 6, 16, 39, 60], image-to-image translation [9, 20, 22, 26, 27, 31, 50, 56, 59]. Among these 2D GANs, Wasserstein GAN (WGAN) [3] is a remarkable study that minimizes the Wasserstein distance instead of the Jensen-Shannon divergence to better handle the case when the generated sample distribution is far away from the real distribution. In 3D modeling, a lot of work on 3D GANs [1, 45, 53, 55] has been proposed. However, the generation of 3D representations from low-dimensional latent spaces needs expensive high-quality 3D data for training. GAN2Shape [40] demonstrates that conventional 2D GANs, when exclusively trained on images, contain rich 3D knowledge and can faithfully reconstruct intricate 3D shapes without 3D annotations. In this work, we only use the form of WGAN to explain and analyze score distillation methods, such as SDS and VSD.\\n\\n3. Preliminaries\\n\\nScore Distillation Sampling. Score Distillation Sampling (SDS) [41] proposes to use 2D diffusion priors to optimize the parameters $\\\\theta$ of 3D representations. It first employs a differentiable renderer $g(\\\\theta, c)$ parameterized by $\\\\theta$ to generate an image $x_g^0 = g(\\\\theta, c)$ at a random view $c$, then adds noise to this image $x_g^t$ to obtain $x_g^t$, and uses a pretrained diffusion model to predict noise conditioned on a given text $y$. In the SDS paper [41], the gradient is directly derived from the L2 loss of the diffusion model, which can be written as\\n\\n$$\\\\nabla_\\\\theta L_{\\\\text{diff}} = \\\\nabla_\\\\theta E_{t,\\\\epsilon}[\\\\omega(t)\\\\|\\\\epsilon_{x_g^t}; y, t - \\\\epsilon\\\\|^2] = E_{t,\\\\epsilon}[\\\\omega(t)(\\\\epsilon_{x_g^t}; y, t - \\\\epsilon)\\\\partial\\\\epsilon_{x_g^t}; y, t] \\\\approx E_{t,\\\\epsilon}[\\\\omega(t)\\\\partial x_g^0; \\\\partial x_g^0, \\\\partial x_g^0, \\\\partial x_g^0, \\\\partial \\\\theta]. (1)$$\\n\\nwhere $\\\\omega(t)$ is a weight based on the time $t$, and $\\\\epsilon_{x_g^t}; y, t$ is the vanilla-predicted noise with input $x_g^t$ conditioned on $y$.8133\"}"}
{"id": "CVPR-2024-127", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In other words, $p$ where $L$ which cannot be derived from and which can be written as $\\\\text{GAN (WGAN)}$ [3, 54] reformulates the original GAN with when two distributions are very different. The Wasserstein network $D$ is reformulated by a zero-sum competition between a discriminator model, we can derive the SDS gradient directly from the generator loss in the WGAN. We define a discriminator in the form as $\\\\text{GAN [13]}$ essentially minimizes the Jensen-Shannon loss related to the implicit classifier or discriminator. Here, we consider $\\\\phi$ as the prompt for fake samples. The original form of GAN [13] ensures the vanishing neural network $\\\\text{D}$. In Eq. (4), we have $\\\\text{D}$ trained diffusion model, thereby achieving better quality. One goal of this work is to unravel the methodological issues of first two, loss as the penalty weight. $\\\\text{\u03d5}$ comes from the distribution $\\\\mu$ of generated samples, $\\\\tilde{\\\\mu}$ of first two, $\\\\text{\u03d5}$ as the inversion embedding [12] or LoRA [19]. Thus, $\\\\text{\u03d5}$ so they are not optimizable.\\n\\nIn Eq. (7), $\\\\text{\u03d5}$ is that the SDS gradient uses the added noise $\\\\omega$ where $\\\\text{\u03d5}$ is a weight based on the time $\\\\sigma$ of real samples, $\\\\tilde{\\\\sigma}$ is a weight based on the time $\\\\tau$. $\\\\text{\u03d5}$ comes from the diffusion model, thereby achieving better quality. From another intuitive perspective, $\\\\text{\u03d5}$ also contains the inductive bias of the pretrained diffusion model while $\\\\text{\u03d5}$ does not. Recently, VSD [52] uses the LoRA branch to predict $\\\\text{\u03d5}$.\\n\\nIn Eq. (8), $\\\\text{\u03d5}$, $\\\\text{\u03d5}$ makes the derivation of Eq. 1 no longer valid. This approximation may not be optimal, which can be implemented using an textual bias of the pretrained diffusion model while $\\\\text{\u03d5}$ is not optimizable.\\n\\nIn Eq. (9), $\\\\text{\u03d5}$ is a loss $\\\\text{\u03d5}$ based on the given pretrained diffusion model, $\\\\text{\u03d5}$ does not. In Eq. (10), $\\\\text{\u03d5}$ is a loss $\\\\text{\u03d5}$ with $\\\\text{\u03d5}$ corresponding $\\\\text{\u03d5}$ in $\\\\text{\u03d5}$ inversion embedding [12] or LoRA [19]. Thus, $\\\\text{\u03d5}$ optimizable, which can be implemented using an textual bias of the pretrained diffusion model while $\\\\text{\u03d5}$ is not optimizable.\"}"}
