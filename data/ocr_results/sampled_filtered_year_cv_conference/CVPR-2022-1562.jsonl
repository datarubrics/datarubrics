{"id": "CVPR-2022-1562", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 9157\u20139166, 2019.\\n\\n[2] Chris Burges, Tal Shaked, Erin Renshaw, Ari Lazier, Matt Deeds, Nicole Hamilton, and Greg Hullender. Learning to rank using gradient descent. In Proc. Int. Conf. Machine Learning (ICML), pages 89\u201396, 2005.\\n\\n[3] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 6154\u20136162, 2018.\\n\\n[4] Kean Chen, Jianguo Li, Weiyao Lin, John See, Ji Wang, Lingyu Duan, Zhibo Chen, Changwei He, and Junni Zou. Towards accurate one-stage object detection with ap-loss. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 5119\u20135127, 2019.\\n\\n[5] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, et al. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.\\n\\n[6] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-level face localisation in the wild. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 5203\u20135212, 2020.\\n\\n[7] Shanghua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and Philip HS Torr. Res2net: A new multi-scale backbone architecture. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(2):652\u2013662, 2021.\\n\\n[8] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Nas-fpn: Learning scalable feature pyramid architecture for object detection. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 7036\u20137045, 2019.\\n\\n[9] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pages 2961\u20132969, 2017.\\n\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 770\u2013778, 2016.\\n\\n[11] Borui Jiang, Ruixuan Luo, Jiayuan Mao, Tete Xiao, and Yuning Jiang. Acquisition of localization confidence for accurate object detection. In Proc. Eur. Conf. Comput. Vis. (ECCV), pages 784\u2013799, 2018.\\n\\n[12] Wei Ke, Tianliang Zhang, Zeyi Huang, Qixiang Ye, Jianzhuang Liu, and Dong Huang. Multiple anchor learning for visual object detection. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 10206\u201310215, 2020.\\n\\n[13] Kang Kim and Hee Seok Lee. Probabilistic anchor assignment with iou prediction for object detection. In Proc. Eur. Conf. Comput. Vis. (ECCV), pages 355\u2013371, 2020.\\n\\n[14] Buyu Li, Yu Liu, and Xiaogang Wang. Gradient harmonized single-stage detector. In Proc. AAAI Conf. Artificial Intelligence (AAAI), pages 8577\u20138584, 2019.\\n\\n[15] Hengduo Li, Zuxuan Wu, Chen Zhu, Caiming Xiong, Richard Socher, and Larry S Davis. Learning from noisy anchors for one-stage object detection. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 10588\u201310597, 2020.\\n\\n[16] Xiang Li, Wenhai Wang, Lijun Wu, Shuo Chen, Xiaolin Hu, Jun Li, Jinhui Tang, and Jian Yang. Generalized focal loss: Learning qualified and distributed bounding boxes for dense object detection. In Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), pages 21002\u201321012, 2020.\\n\\n[17] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Microsoft coco: Common objects in context. In Proc. Eur. Conf. Comput. Vis. (ECCV), pages 2117\u20132125, 2014.\\n\\n[18] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 2117\u20132125, 2017.\\n\\n[19] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Focal loss for dense object detection. In Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pages 2117\u20132125, 2017.\\n\\n[20] Ji Liu, Dong Li, Rongzhang Zheng, Lu Tian, and Yi Shan. Rankdetnet: Delving into ranking constraints for object detection. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 264\u2013273, 2021.\\n\\n[21] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In Proc. Eur. Conf. Comput. Vis. (ECCV), pages 21\u201337, 2016.\\n\\n[22] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan Kalkan. A ranking-based, balanced loss function unifying classification and localisation in object detection. In Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), 2020.\\n\\n[23] Kemal Oksuz, Baris Can Cam, Emre Akbas, and Sinan Kalkan. Rank & sort loss for object detection and instance segmentation. In Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pages 3009\u20133018, 2021.\\n\\n[24] Qi Qian, Lei Chen, Hao Li, and Rong Jin. Dr loss: Improving object detection by distributional ranking. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 12164\u201312172, 2020.\\n\\n[25] Han Qiu, Yuchen Ma, Zeming Li, Songtao Liu, and Jian Sun. Borderdet: Border feature for dense object detection. In Proc. Eur. Conf. Comput. Vis. (ECCV), pages 549\u2013564, 2020.\\n\\n[26] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 779\u2013788, 2016.\\n\\n[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Proc. Adv. Neural Inf. Process. Syst. (NeurIPS), pages 91\u201399, 2015.\"}"}
{"id": "CVPR-2022-1562", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Hamid Rezatofighi, Nathan Tsoi, Junyoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union: A metric and a loss for bounding box regression. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 658\u2013666, 2019.\\n\\nZhiyu Tan, Xuecheng Nie, Qi Qian, Nan Li, and Hao Li. Learning to rank proposals for object detection. In Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pages 8273\u20138281, 2019.\\n\\nZhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS: Fully convolutional one-stage object detection. In Proc. IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pages 9627\u20139636, 2019.\\n\\nXinlong Wang, Tao Kong, Chunhua Shen, Yuning Jiang, and Lei Li. Solo: Segmenting objects by locations. In Proc. Eur. Conf. Comput. Vis. (ECCV), pages 649\u2013665, 2020.\\n\\nShengkai Wu, Jinrong Yang, Xinggang Wang, and Xiaoping Li. Iou-balanced loss functions for single-stage object detection. arXiv preprint arXiv:1908.05641, 2019.\\n\\nEnze Xie, Peize Sun, Xiaoge Song, Wenhai Wang, Xuebo Liu, Ding Liang, Chunhua Shen, and Ping Luo. Polarmask: Single shot instance segmentation with polar representation. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 12193\u201312202, 2020.\\n\\nSaining Xie, Ross Girshick, Piotr Dollar, Zhuowen Tu, and Kaiming He. Aggregated residual transformations for deep neural networks. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 1492\u20131500, 2017.\\n\\nYunyang Xiong, Hanxiao Liu, Suyog Gupta, Berkin Akin, Gabriel Bender, Yongzhe Wang, Pieter-Jan Kindermans, Mingxing Tan, Vikas Singh, and Bo Chen. Mobiledets: Searching for object detection architectures for mobile accelerators. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 3825\u20133834, 2021.\\n\\nDongli Xu, Jian Guan, Pengming Feng, and Wenwu Wang. Association loss for visual object detection. IEEE Signal Processing Letters, 27:1435\u20131439, 2020.\\n\\nHaoyang Zhang, Ying Wang, Feras Dayoub, and Niko Sunderhauf. Varifocalnet: An iou-aware dense object detector. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 8514\u20138523, 2021.\\n\\nShifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Z Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 9759\u20139768, 2020.\\n\\nXiangyu Zhang, Xinyu Zhou, Mengxiao Lin, and Jian Sun. Shufflenet: An extremely efficient convolutional neural network for mobile devices. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 6848\u20136856, 2018.\\n\\nChenchen Zhu, Yihui He, and Marios Savvides. Feature selective anchor-free module for single-shot object detection. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 840\u2013849, 2019.\\n\\nXizhou Zhu, Han Hu, Stephen Lin, and Jifeng Dai. Deformable convnets v2: More deformable, better results. In Proc. IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 9308\u20139316, 2019.\"}"}
{"id": "CVPR-2022-1562", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. (a) Different positions in the ground truth box may have the same IoU between their preset anchor box and ground truth. The blue boxes are used to denote anchor boxes, while two-way arrows are used to highlight two ranking pairs. (b) Dense object detection can predict many candidate boxes from positive positions. The box that perfectly predicts this cat is clearly performing better than those that ignore the cat's foot. (c) Left: The distribution of original ranking scores and localization scores. Although they are in the range of $[0, 1]$, they have different distributions. Right: The distribution of $0$-$1$ normalized ranking scores and localization scores.\\n\\nAlgorithm 1: Adaptive Ranking Pair Selection\\n\\nInput: $P$ is a set of positive positions $N$ is a set of negative positions $\\\\hat{I}$ are the IoUs between boxes $\\\\hat{B}$ predicted at the corresponding position and ground truth\\n\\nOutput: $A$ is a set of adaptive negative position sets\\n\\n1. $A \\\\leftarrow \\\\emptyset$\\n2. For all $u \\\\in P$ do\\n3. Build an empty negative sample set for each positive sample position $u$: $A_u \\\\leftarrow \\\\emptyset$\\n4. For all $v \\\\in P$, $v \\\\neq u$ do\\n5. If $\\\\hat{I}_v < \\\\hat{I}_u$ then\\n6. If its IoU is smaller than that of $p$, this sample should be considered an adaptive False Positive (aFP) sample in original $P$ set $A_u = A_u \\\\cup v$\\n7. End\\n8. End\\n9. $A_u = A_u \\\\cup N$\\n10. $A = A \\\\cup \\\\{A_u\\\\}$\\n11. End\\n12. Return $A$.\\n\\n6. Adaptive Ranking Pair Selection\\n\\nWe propose ARPS to calculate Adaptive Pairwise Error between positive samples. Our method focuses on dynamically selecting negative samples $A_u$ in positive set $P$ according to their localization qualities. Notably, each positive sample $u$ is assigned with a different set $A_u$ to replace $N$ in Eq. (8). Here, APE loss $L_{APE}$ can be written as follows:\\n\\n$$L_{APE}(\\\\hat{P}_u, \\\\hat{P}_v) = -\\\\frac{1}{BC} \\\\sum_{v \\\\in A_u} D(\\\\hat{P}_v - \\\\hat{P}_u) \\\\tag{9}$$\\n\\nwhere the balance constant $BC$ is set as $\\\\text{rank}_+^{+}(u) + \\\\text{rank}_-^{+}(u)$, while the distance function is set as cross entropy with sigmoid $CE(S(\\\\cdot), 0)$. In a departure from previous works [11, 16, 30, 36], adaptive pairwise error integrates localization quality-related information into extra ranking pairs between positive samples rather than regressing a certain value (e.g., center-ness in [30] and IoU in [16]). Our method is adaptive and does not bring any hyperparameters, which eases training overloads for different applications. With the dynamic expansion of the negative sample set, our proposed APE loss considers the ranking pairs in the positive samples and further boosts the detection accuracy.\"}"}
{"id": "CVPR-2022-1562", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Alg. 1 describes how the proposed method works for an existing negative sample set and dynamically expands it. For each positive position $u \\\\in P$ with IoU $\\\\hat{I}_u$, we add the samples $v$ with lower IoU $\\\\hat{I}_v$ than $\\\\hat{I}_u$ into a dynamically expanded negative sample set $A_u$ as shown in Alg. 1. After we store all additional negative samples from $P$, we merge $A_u$ with $N$ in lines 9 to 10. After we employ the ARPS to calculate adaptive pairwise error in Eq. (9), it would be interesting to explore the obstacle part in the original AP metric. Hence, we re-write APE loss in AP formula and analyse which parts of AP loss are modified by ARPS.\\n\\n$$L_{APE}(\\\\hat{P}_u, \\\\hat{P}_v) = -\\\\frac{1}{BC} \\\\sum_{v \\\\in A_u} D(\\\\hat{P}_v - \\\\hat{P}_u) = \\\\sum_{v \\\\in N} D(\\\\hat{P}_v - \\\\hat{P}_u) + \\\\sum_{v \\\\in P, \\\\hat{I}_v < \\\\hat{I}_u} D(\\\\hat{P}_v - \\\\hat{P}_u) + \\\\sum_{v \\\\in P, v \\\\neq u} D(\\\\hat{P}_v - \\\\hat{P}_u) + \\\\sum_{v \\\\in N} D(\\\\hat{P}_v - \\\\hat{P}_u) = FP + aFP (TP - aFP) + (FP + aFP)$$\\n\\nwhere $BC$ is also set as $\\\\text{rank}(u) + (\\\\text{rank}(u) - 1)$; moreover, a sample should be considered as an adaptive False Positive (aFP) sample if its IoU is smaller than that of $p$.\\n\\nIn conclusion, the question of how to define TPs and FPs is an important one for dense object detection, as not all positive samples are the best detection results. ARPS accordingly identifies some adaptive False Positive (aFP) samples in the original TP set, and can thus be considered to provide a more accurate AP metric. Additionally, to reduce the inaccurate ranking pairs in ARPS that are selected by the preset anchor\u2019s IoU, we focus on image contents by investigating ranking scores and localization scores. Considering there are massive numbers of candidate boxes before the NMS algorithm, positive samples around objects can receive comparatively higher localization scores while the IoUs of negative samples are normally close to 0. This means that a large localization score gap exists between positive and negative samples. Instead, it can be inferred from Fig.3 (b) that the ranking scores of the positive and negative samples only have a small gap between them (e.g., the value of the distance function will be close to 0 when $\\\\hat{P}_u - \\\\hat{P}_v$ goes to 1, which means that after sigmoid function, their gap $S(\\\\hat{P}_u) - S(\\\\hat{P}_v)$ is far smaller than 1).\\n\\nWe here employ PAA [13] to exploit the information of these pair gaps. Specifically, we first use 0-1 normalization for each instance to reduce the scale differences between the ranking score and localization score. We can then observe from Fig. 3 (c) that samples naturally gather close to two ends (i.e., (0, 0) and (1, 1)), and the blank area between the blue and red shades is an ideal separation that we expected. However, there are still many outliers of the two clusters, i.e., the small blue and red dots in the gap. If the ranking pairs are formulated as every different potential combination of two samples from the two clusters (the bottom-left and top-right region in Fig. 3 (c)), the pairwise error will in turn enlarge this gap and control the outliers as the training progresses. Therefore, we convert the ranking pair selection problem into a two-cluster discrimination problem on the two normalized scores. Following PAA [13], Gaussian Mixture Model (GMM) is used to construct positive and negative clusters.\\n\\n### 7. Experiments\\n\\n#### 7.1. Dataset and Implementation Details\\n\\nExperiments are conducted on the MS-COCO 2017 dataset [17]. We use a split set train-2017 (115k images) for training and validation set val-2017 (5k images) for the ablation study and compare with other losses. test-dev split is used to test advanced additions. The COCO-style average precision (AP) is employed as the performance metric. The implementation is based on Pytorch 1.4, CUDA 10 and mmdetection [5]. Unless otherwise specified, all hyper-parameters are set to default, as in RetinaNet [19] and FCOS [30]. The ranking score threshold of NMS is 0.15 and IoU threshold of NMS is 0.6. The batch size is 16 and 4 GPUs are used for training (we use 4 \u00d7 GTX 2080Ti-11G for training on ResNet-50 [10] and 4 \u00d7 RTX Titan-24G for training with advanced methods, i.e., advanced backbone and multi-scale training). SGD is used as an optimizer and warmed up for the first 500 iterations. SGD is used as an optimizer and warmed up for the first 500 iterations. RetinaNet 512: Here, we follow the baseline, i.e., A P loss, and employ SSD-style [21] data augmentations (including Photo Metric Distortion and Min IoU Random Crop [5]) for comparable evaluations. Moreover, to improve this baseline, we employ GIoU [28] loss to obtain more accurate localization results. Here, the max pair number $Q$ (see appendix) here is $1000000$. The reso\"}"}
{"id": "CVPR-2022-1562", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Overall contribution of the proposed methods over baseline.\\n\\n| Ranking Loss | Sampling Strategy |\\n|--------------|-------------------|\\n|              | AP Loss           |\\n|              | IoU threshold     |\\n|              | 37.3              |\\n|              | 57.4              |\\n|              | 38.9              |\\n|              | Adaptive Pairwise Error |\\n|              | IoU threshold     |\\n|              | 38.3              |\\n|              | 55.8              |\\n|              | 41.1              |\\n|              | PAA*              |\\n|              | 41.1              |\\n|              | 59.6              |\\n|              | 43.7              |\\n\\nTable 5. Contribution of APE loss in association with localization scores.\\n\\n| Method                        | PCC        | SCC        | KCC         |\\n|-------------------------------|------------|------------|-------------|\\n| Pairwise Error                | 0.3742     | 0.3790     | 0.2538      |\\n| Adaptive Pairwise Error       | 0.4940     | 0.5034     | 0.3449      |\\n\\nThe resolution of the input images is 512 \u00d7 512 and the initial learning rate is 0.004. For 48-epochs training, we divide the learning rate by 10 at 32 epochs and again at 44 epochs.\\n\\nFCOS 800: We here omit the center-ness branch and make the pipeline more elegant. To ensure focus on more important samples, we also use category prediction to weight the GIoU loss following [16], and use IoU to weight APE loss following [32]. The weight of ranking loss is set to 1.0.\\n\\nThe max pair number Q here is 100,000 (see appendix).\\n\\nThe resolution of the input images is 800 \u00d7 1333 with the same ratio of raw images. The initial learning rate is 0.01. For 24-epochs training, we divide the learning rate by 10 at 18 and again at 22 epochs.\\n\\n7.2. Ablation Study\\n\\nWe conduct ablation experiments on ResNet-50-FPN [10, 18] to evaluate the effect of each component of our proposed method. To begin with, we implement our approach on RetinaNet to evaluate individual components relative to the baseline method, i.e., AP loss [4]. However, RetinaNet is an obsolete detector with lower performance than the currently most popular method, i.e., FCOS. Therefore, we also test our method on the state-of-the-art detector FCOS (A TSS) to illustrate the superiority of APE loss over recent classification losses.\\n\\nEffect of Adaptive Pairwise Error: We conduct experiments with our implementation of Pairwise Error on RetinaNet, where \\\\( \\\\lambda \\\\) is set to 8 (see appendix for the varying of \\\\( \\\\lambda \\\\)). As shown in Table 4, by replacing AP loss with APE loss, we can improve the AP and \\\\( \\\\text{AP}_{75} \\\\) by 1.0 and 2.2, respectively. This demonstrates that our proposed APE loss improves detection accuracy. Moreover, to better understand the reasons why APE loss is effective, we adopt IoUs and ranking scores of detection results with higher localization scores (i.e., IoUs larger than 0.5) as two lists of data and calculate their Pearson Correlation Coefficient (PCC), Spearman Correlation Coefficient (SCC) and Kendall rank Correlation Coefficient (KCC). The results are presented in Table 5. From the table, it can be seen that APE loss promotes PCC by 0.12, SCC by 0.12 and KCC by 0.09. As quantitative measurements of correlation, the three coefficients are promoted significantly, indicating that APE loss can associate the category predictions with localization qualities and thereby boost NMS accuracy.\\n\\nEffect of Normalized Scores Input of PAA: To study the necessity of two normalized scores for PAA, we test the original PAA [13] that adopts Focal loss [19] and GIoU loss [28] for clustering. As the results in Table 6 show, when we replace normalized ranking and classification scores with Focal loss and GIoU loss, the performance drops from 41.1 to 39.4. We contend that it is because the scales of ranking and localization gap in these losses are not well aligned; localization loss would be smaller, and its information could be easily ignored. However, PAA works well in classification methods owing to the fact that these two scores are trained to be equal [16, 37] and could have similar distribution scales. We additionally test different clustering input features; normalized ranking scores and localization scores independently achieve 38.9 and 33.9 AP, illustrating the association of two scores brings 2.2 AP performance gain. Moreover, as shown in Table 6, when normalization processing on the two scores is omitted, the performance also drops to 40.5, illustrating the normalization brings 0.6 AP performance gain.\\n\\nEffect of Higher Resolution and Learning Rate: To facilitate comparative evaluation of our method with state-of-the-art losses, we incorporate it into the most popular detector with the higher input resolution, i.e., FCOS(800). As the results in Table 7 show, training on FCOS(800) can improve our method by 0.4 AP. Note that we here discard SSD-style augmentation. We argue that this promotion occurs due to the higher input resolution, since the main performance gap between RetinaNet and FCOS is addressed by A TSS [38]. We also record the comparison of training time in Table 8.\"}"}
{"id": "CVPR-2022-1562", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Loss Function | Method | Data Aug. | Epoch | Backbone | AP 50 | AP 75 | AP S | AP M | AP L |\\n|---------------|--------|-----------|-------|----------|-------|-------|------|------|------|\\n| Classification Loss | Focal Loss | Retina(500)/enc-37 | 12 | ResNet-50-FPN | 32.5 | 50.9 | 34.8 |\\n|                | GHM Loss | Retina(800)/enc-37 | 12 | ResNet-50-FPN | 35.8 | 55.5 | 38.1 |\\n|                | Focal Loss | A TSS(800)/enc-37 | 12 | ResNet-50-FPN | 39.2 | 56.5 | 40.7 |\\n|                | Quality Focal Loss | A TSS(800)/enc-37 | 12 | ResNet-50-FPN | 39.9 | 58.5 | 43.0 |\\n|                | Varifocal Loss | A TSS(800)/enc-37 | 12 | ResNet-50-FPN | 40.2 | 58.2 | 44.0 |\\n|                | Focal Loss | PAA(800)/enc-37 | 24 | ResNet-50-FPN | 41.1 | 59.4 | 44.3 |\\n| Ranking Loss | Average Precision Loss | Retina(512)/enc-34 | 100 | ResNet-50-FPN | 35.4 | 58.1 | 37.0 |\\n|                | Distributional Ranking Loss | Retina(800)/enc-37 | 12 | ResNet-50-FPN | 37.4 | 56.0 | 40.0 |\\n|                | aLRP Loss | A TSS(512)/enc-34 | 100 | ResNet-50-FPN | 41.0 | 61.2 | 43.3 |\\n|                | RankDetNet Losses | A TSS(800)/enc-37 | 24 | ResNet-50-FPN | 40.7 | 58.4 | 44.0 |\\n|                | Rank & Sort Loss | PAA(800)/enc-37 | 12 | ResNet-50-FPN | 41.0 | 59.1 | 44.5 |\\n| Ours | Adaptive Pairwise Error | Retina(512)/enc-34 | 48 | ResNet-50-FPN | 38.3 | 55.8 | 41.1 |\\n|                | Adaptive Pairwise Error | A TSS(512)/enc-34 | 48 | ResNet-50-FPN | 39.9 | 56.3 | 43.6 |\\n|                | Adaptive Pairwise Error | A TSS(800)/enc-37 | 24 | ResNet-50-FPN | 40.8 | 57.7 | 44.0 |\\n|                | Adaptive Pairwise Error | PAA*(512)/enc-34 | 48 | ResNet-50-FPN | 41.1 | 59.2 | 44.5 |\\n|                | Adaptive Pairwise Error | PAA*(800)/enc-37 | 24 | ResNet-50-FPN | 41.5 | 60.7 | 44.8 |\\n\\n7.3. Comparison with State-of-the-Art Losses\\n\\nFinally, we compare APE loss with state-of-the-art loss functions on val-2017, as shown in Table 9. Here, every method only uses single-model and single-scale testing, omits 1.5 stage regression (as in [25, 37]) and utilizes ResNet-50-FPN [10, 18]. APE loss achieves 41.5 on the FCOS [30] detector without center-ness branch. We also verify our method on A TSS, PAA and RetinaNet. The performances of APE loss are superior to that of recent state-of-the-art losses. Furthermore, we test our method with advanced additions on test-dev, i.e., multi-scale training and stronger backbones (ResNext [34], Res2Net [7]) with Deformable ConvNets v2 [41]. As shown in Table 9, those additions boost our methods to 50.0 AP.\"}"}
{"id": "CVPR-2022-1562", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Revisiting AP Loss for Dense Object Detection: Adaptive Ranking Pair Selection\\n\\nDongli Xu, Jinhong Deng, Wen Li*\\nSchool of Computer Science and Engineering & Shenzhen Institute for Advanced Study\\nUniversity of Electronic Science and Technology of China\\n{dongliixu,jhdeng1997,liwenbnu}@gmail.com\\n\\nAbstract\\nAverage precision (AP) loss has recently shown promising performance on the dense object detection task. However, a deep understanding of how AP loss affects the detector from a pairwise ranking perspective has not yet been developed. In this work, we revisit the average precision (AP) loss and reveal that the crucial element is that of selecting the ranking pairs between positive and negative samples. Based on this observation, we propose two strategies to improve the AP loss. The first of these is a novel Adaptive Pairwise Error (APE) loss that focusing on ranking pairs in both positive and negative samples. Moreover, we select more accurate ranking pairs by exploiting the normalized ranking scores and localization scores with a clustering algorithm. Experiments conducted on the MS-COCO dataset support our analysis and demonstrate the superiority of our proposed method compared with current classification and ranking loss. The code is available at https://github.com/Xudangliatiger/APE-Loss.\\n\\n1. Introduction\\nObject detection is one of the fundamental computer vision tasks and aims to predict both the category labels and the bounding-box coordinates of all objects in a given image [3, 19, 21, 27, 30]. It also plays an important role in many down-stream applications such as instance segmentation [1, 9, 31, 33] and face detection [6]. Modern object detectors can be divided into two-stage approaches [3, 27] and one-stage approaches [35, 39]. One-stage detectors adopt dense prediction without a region proposal phase; thus they are also known as dense object detectors. One-stage object detectors are also naturally faster than two-stage detectors and popular in real-world applications such as that on edge devices [35, 39]. Most one-stage object detectors [16, 19, 21, 26, 30, 38, 40] rely on a classification task to discriminate the category of objects, i.e., directly predicting the category probability for every patch in an image. However, it often suffers from an extreme imbalance problem, as huge numbers of background patches (i.e., negative samples) can overwhelm the sufficient loss gradients for foreground patches (i.e., positive samples). Recently, to alleviate this sensitivity to the ratio of negative and positive samples, Average Precision Loss [4] converts the classification task into a ranking task by explicitly modeling sample relationships that are calculated by comparing all sample pairs.\\n\\nDespite AP loss performing well at addressing the sample imbalance problem, the essential mechanism of how AP loss affects the detector is still veiled. Accordingly, in this paper, we delve into AP loss from a pairwise ranking perspective [2]; specifically, the crucial part of AP loss is the minimizing of the pairwise error between the positive and negative samples. To find out the essential properties of AP loss, we analyze components of pairwise error separately and reveal that the key factor is the accurate and complete ranking pair selection. Consequently, we improve the AP loss by adjusting the way in which ranking pairs are constructed and selected. Proper ranking pair selection achieves notable performance gains for object detection. More specifically, we first derive a reformulation of the AP loss and show that it contains three main components of generalized pairwise error: distance function, balance constant and ranking pair selection. We conduct detailed experiments to verify these different components' effects and determine that proper ranking pair selection plays a vital role in producing accurate detection results. We then investigate the barriers to good performance encountered by existing ranking pair selection strategies and identify the following issues: (1) Traditional strategies [13, 19, 30, 38] ignore the ranking pairs between positive samples, which may lead to inaccurate category probability prediction, and harms the Non-Maximum Suppression (NMS) result; (2) The probability of image content in the ranking task and accuracy in the localization task, i.e., ranking score and localization score have different distributions, which brings imbalanced attentions to those two tasks during the pair selection process.\"}"}
{"id": "CVPR-2022-1562", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Comparison between our ranking pair selection and a traditional method. Here, the large red and blue dots are used to highlight positive and negative samples, respectively, while the two-way arrows are used to highlight typical ranking pairs. (a) Current pairwise ranking methods ignore the pair between positive positions and focus on the anchor-based IoU to select ranking pairs. The red and white boxes are the ground truth and preset anchor. (b) FCOS detector (this paper omits the center-ness branch). (c) Our method can make up for the loss of ranking pairs between positive samples and focus on the distribution of prediction scores to adaptively select more accurate ranking pairs.\\n\\nThe contributions of this work are three-fold and can be summarized as follows: 1) We conduct thorough experiments to verify each part of AP loss from a pairwise perspective and reveal that inappropriate ranking pair selection is the main obstacle; 2) we propose a ranking pair selection algorithm, i.e., ARPS, to exploit complete and more accurate ranking pairs automatically; 3) our method achieves competitive performance when evaluated against all other existing classification and ranking methods.\"}"}
{"id": "CVPR-2022-1562", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"on the model's learning status.\u9002\\nadaptively separate positive and negative sets depending on\\nprovides an adaptive IoU threshold. Recent studies [12, 13,\\njusted IoU threshold of preset anchor box, while A TSS [38]\\nnaNet [19] defines positive samples using a manually ad-\\ntwo sample sets; i.e.\\n\\nthat the ranking pairs are formulated as different pairs from\\nsample selection\\\" interchangeably, with the understanding\\nwill use the terms \\\"ranking pair selection\\\" and \\\"training\\nThroughout this paper, we\\nTraining Sample Selection:\\n\\n3. Preliminary\\n\\nposes to leverage the information among positive samples\\nanchor prediction, respectively; moreover, 4 denotes the en-\\nbox prediction, respectively; moreover, 4 denotes the en-\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\nL\\n\\n"}
{"id": "CVPR-2022-1562", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison of distance functions.\\n\\n| Distance Function | 50 | 75 |\\n|------------------|----|----|\\n| $H(\\\\cdot)$/enc-34 | 37.4 | 57.5 |\\n| $S(\\\\cdot)$/enc-34 | 37.3 | 57.4 |\\n| CE($S(\\\\cdot)$, 0) | 37.3 | 57.2 |\\n\\n\\\\[\\n\\\\hat{P}_u) / (rank^+ (i) + rank^- (u)), \\\\text{ where } g_v \\\\text{ is used in the same way as } \\\\partial L_\\\\text{p}(\\\\hat{P}_u, \\\\hat{P}_v) / \\\\partial \\\\hat{P}_v \\\\text{ in back propagation. It is worth noting that } \\\\sum v \\\\in N g_v = -g_u; \\\\text{ thus, pairwise design can alleviate the imbalance of positive and negative samples.}\\n\\\\]\\n\\nIn fact, the denominator term of the backpropagation gradient $g_u$ shown in Eq. (7) is the same as the one of precision loss in Eq. (5) and can be considered as a balance constant. It is evident that optimizing AP loss is equivalent to minimizing the numerator in Eq. (5) \\\\[22\\\\], which can also be considered as the sum of the distance error between each pair of prediction scores, i.e., pairwise error. Therefore, to reduce the redundancy of analyzing ranking tasks, we can rewrite the precision loss as pairwise error loss with the unified formalized description following \\\\[2\\\\]. We here present this unified pairwise error loss $L_{PE}$ as follows:\\n\\n\\\\[\\nL_{PE}(\\\\hat{P}_u, \\\\hat{P}_v) = -1_{BC} \\\\sum v \\\\in N D(\\\\hat{P}_v - \\\\hat{P}_u)\\n\\\\]\\n\\nwhere $BC$ denotes the balance constant, while $D(\\\\cdot)$ denotes the distance function adopted as the pairwise error. Finally, we have our ranking loss $L_{rank}$ for all positive samples in a mini-batch:\\n\\n\\\\[\\nL_{rank} = \\\\sum u L_{PE}(\\\\hat{P}_u, \\\\hat{P}_v) / N_{pos}.\\n\\\\]\\n\\n5. What Contributes to Pairwise Error?\\n\\nAs shown in Eq. (8), the reformulation of the pairwise ranking loss consists of three essential parts: (1) the distance function $D(\\\\cdot)$; (2) the balance constant $BC$; (3) the pair selection of $\\\\hat{P}_v$ and $\\\\hat{P}_u$. To find out the essential properties of different designs in ranking-based losses, we analyze each part separately and verify the effect of different strategies for pairwise ranking methods \\\\[2, 22, 29\\\\]:\\n\\nDistance Function: \\\\[2, 29\\\\] adopt sigmoid $S(\\\\cdot)$ as distance function, which can be described as follows:\\n\\n\\\\[\\nS(\\\\hat{P}_v - \\\\hat{P}_u) = 1 / (1 + \\\\exp(\\\\lambda(\\\\hat{P}_u - \\\\hat{P}_v))), \\\\text{ where } \\\\lambda \\\\text{ is a tuning hyper-parameter that controls the slope of the sigmoid function near the origin.}\\n\\\\]\\n\\nBy contrast, AP loss uses a non-differentiable function $H(\\\\cdot)$, described in Eq. (6) and employs Error-Driven Update for backpropagation. However, as shown in the first and second rows of Table 1, when $H(\\\\cdot)$ in AP loss is replaced with $S(\\\\cdot)$, the influence on performance is minor. Additionally, we also use a cross entropy loss $CE(\\\\cdot, 0)$ to replace Error-Driven Update (see appendix for more details). The results are shown in Table 2, where it can be seen that the usage of Error-Driven Update provides the same performance as CE loss (i.e., 37.3 vs. 37.3). These results illustrate that the influence on the performance of different distance functions employed in recent pairwise loss \\\\[2,4,29\\\\] is minor.\\n\\nBalance Constant: Consequently, we need a balance constant to normalize the sum of all pairwise errors. AP loss uses $\\\\sum v \\\\in N D(\\\\hat{P}_v - \\\\hat{P}_u)$ in Eq. (7) as its balance constant. While \\\\[2\\\\] sets the balance constant as the number of valid negative samples $N_{neg}$, which can keep the loss function simple. Note that, only if $N_{neg}$ is small enough, we can replace balance constant with $N_{neg}$. Hence, we use a threshold to define which sample is valid (for more details, please refer to the appendix). We conduct empirical studies with these two balance constants. As shown in Table 1, the two different balance constants also yield similar performances (37.3 vs. 37.3). Although the results suggest that these two balance constant choices are robust, the balance constant design of AP loss is more adaptive and does not require a valid sample threshold hyper-parameter.\\n\\nRanking Pair Selection: As for the selection of ranking pairs, i.e., $(\\\\hat{P}_v, \\\\hat{P}_u)$, ALRP loss \\\\[22\\\\] adopts A TSS \\\\[38\\\\] while AP loss adopts an IoU threshold, following \\\\[19\\\\]. When we use the A TSS \\\\[38\\\\] to select ranking pairs, the performance of pairwise loss is significantly promoted; specifically, it is boosted from 37.3 to 39.2. This result may be caused by some important ranking pairs being ignored or many of the selected ranking pairs being inaccurate. Thus, inappropriate ranking pair selection is a significant barrier to higher accuracy.\\n\\nAnalysis of Inappropriate Ranking Pair Selection: Current state-of-the-art pair selection strategies \\\\[4, 22, 24\\\\] only depend on IoUs between preset anchor boxes and ground truths; thus, there is a lack of exploiting image content. As can be seen from Fig. 2 (a), the background and foreground may have the same priority, since the distance to the centre of this ground truth box is the same for both of them, leading to an inferior ranking pair selection (e.g., there will be a ranking pair between the two grass positions in Fig. 2 (a)). Recent studies on adaptive sampling selection \\\\[12, 13, 15\\\\] integrate both the predicted category probabilities and localization results into positive sample selection, since the category predictions can represent the image content, while accurate localization results can also achieve better detection accuracy. To address this barrier, we employ PAA \\\\[13\\\\] which utilize both classification loss and localization loss.\"}"}
