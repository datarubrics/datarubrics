{"id": "CVPR-2022-1158", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to dynamically adjust the weights of two kinds of perturbation, and achieve lower queries. For example, if there is very small motion between two frames, intensity based noise can be more effective. However, in the case of large temporal movements of objects or camera, the rotational noise can be useful. We propose a Siamese network based architecture, named DifferenceNet, to predict the weight of each perturbation for a frame.\\n\\n**DifferenceNet**\\n\\nThe proposed DifferenceNet model is a 3D CNN model (with I3D pipeline) trained to calculate semantic difference between input video $V$ and adversarial video $V_{adv}$. The task of DifferenceNet is to provide a low difference score to videos which are semantically similar otherwise a high score. This is achieved by training the network with a dual margin contrastive loss function [48]. The network is trained over positive pairs which have the camera rotations between the frames corresponding to actual videos and negative pairs having abrupt rotations between the frames. To create positive and negative pairs, real Homographies ($H_{real}$), between the frames from the given dataset $D$ and random/fake Homographies have been generated. Application of $H_{real}$, $H_{rand}$ on a video segment $V$, gives us ($V_p$, $V_n$) constituting a positive and negative pair as ($V$, $V_p$), ($V$, $V_n$) respectively. Finally, the trained network is utilized for gradient composition as described below.\\n\\n**Gradient Composition**\\n\\nFor a given input $V$, intensity based perturbation, and camera based perturbations are combined as:\\n\\n$$\\\\hat{V} = \\\\text{Pert}_{in}(V, \\\\alpha \\\\gamma \\\\theta_{in})$$\\n\\n$$V_{adv} = \\\\text{Pert}_{cr}(\\\\hat{V}, \\\\alpha (1 - \\\\gamma) \\\\theta_{cr})$$\\n\\n(10)\\n\\nwhere, $\\\\gamma \\\\in [0, 1]$ $T \\\\times 1$ is the composition parameter and $\\\\alpha$ is a small constant. Since $\\\\gamma$ depends on semantic difference between ($V$, $V_{adv}$), we have utilized DifferenceNet to predict its value:\\n\\n$$d = \\\\text{DifferenceNet}(V, V_{adv})$$\\n\\n$$\\\\gamma = \\\\gamma - \\\\alpha \\\\times \\\\delta d$$\\n\\n(11)\\n\\nwhere, $\\\\sigma$ is a small constant.\\n\\n**3.4. Projected Gradient Descent**\\n\\nFinally, projection gradient descent (PGD) has been utilized to translate gradient estimation and its combination into an efficient Adversarial Example Optimization (AEO). We update intensity based perturbation ($\\\\text{Pert}_{in}(V, \\\\theta_{in})$), camera rotational perturbation ($\\\\text{Pert}_{cr}(\\\\hat{V}, \\\\theta_{cr})$), and composition parameter ($\\\\gamma$) in every iteration of PGD. The complete procedure is shown in Algorithm 1.\\n\\n**Algorithm 1:** Adversarial Example Optimization (AEO)\\n\\nInput: Original video $V$, its label $y$, learning rate $\\\\alpha$ for updating adversarial video.\\n\\n1. Initialise $g_{in} = 0$, $g_{cr} = 0$, $\\\\theta_{in} = 0$, $\\\\theta_{cr} = 0$ and $\\\\gamma = 0$.\\n\\n2. while $\\\\text{arg max } [f(V)] = y$ do\\n\\n3. $\\\\Delta_{in} = \\\\text{GE}(V, y, \\\\theta_{in}, g_{in})$ // Eq 8\\n\\n4. $\\\\Delta_{cr} = \\\\text{GE}(V, y, \\\\theta_{cr}, g_{cr})$ // Eq 8\\n\\n5. $g_{in} = g_{in} - \\\\eta \\\\Delta_{in}$ // Grad. Update\\n\\n6. $g_{cr} = g_{cr} - \\\\eta \\\\Delta_{cr}$ // Grad. Update\\n\\n7. $\\\\theta_{in} = \\\\theta_{in} - g_{in}$ // Param. Update\\n\\n8. $\\\\theta_{cr} = \\\\theta_{cr} - g_{cr}$ // Param. Update\\n\\n9. $\\\\hat{V} = \\\\text{Pert}_{in}(V, \\\\theta_{in} \\\\gamma \\\\alpha)$ // Grad. Composition\\n\\n10. $V_{adv} = \\\\text{Pert}_{cr}(\\\\hat{V}, \\\\theta_{cr} (1 - \\\\gamma) \\\\alpha)$ // Grad. Composition\\n\\n11. $d = \\\\text{DifferenceNet}(V, V_{adv})$\\n\\n12. $\\\\gamma = \\\\gamma - \\\\alpha \\\\times \\\\delta d \\\\delta \\\\gamma$\\n\\n13. $V = V_{adv}$\\n\\nOutput: $V_{adv}$\\n\\n**4. Experiments and Results**\\n\\nIn this section, we provide the details of the experimental analysis performed to validate the efficacy of the proposed method. We start with the details of the experimental setup, including details about the datasets used, target DNN models attacked, attack setting, and evaluation metrics. Finally, we show the comparative analysis and ablation study using both quantitative and qualitative experiments.\\n\\n**4.1. Dataset and Evaluation**\\n\\n**Datasets:** We perform video attacks on three video tasks: third-person action recognition using Kinetics-400 [2] dataset, first-person activity recognition via Epic-Kitchens [5] dataset, and first-person wearer recognition using IITMD-WFP [38] dataset. Kinetics-400 is a large-scale dataset that has around 300K videos in 400 classes. Epic-Kitchens is a first-person activity recognition dataset that consists of 55 hours of egocentric videos from 32 subjects and contains 125 labeled activities performed by the subjects. IITMD-WFP dataset [38] consists of 3.1 hours of videos captured from 31 different subjects. The dataset has been captured under indoor and outdoor scenarios.\\n\\n**DNN Video Analysis Models Used for Experiments:** For third-person video action recognition, we follow the experimental setup of [49]. We choose video action recognition model I3D [2] as our black-box model. For I3D training on Kinetics-400, we train it from ImageNet initialized weights. For first-person activity recognition, we choose...\"}"}
{"id": "CVPR-2022-1158", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Dataset Method ANQ SR%\\n\\n#### Kinetics-400\\n- **V-Bad** [20]: ANQ 4,047, SR% 99.75\\n- **ME-Sampler** [49]: ANQ 2,717, SR% 99.00\\n- **Proposed**: ANQ 1,257, SR% 99.33\\n\\n#### Epic-Kitchens\\n- **V-Bad** [20]: ANQ 8,483, SR% 99.71\\n- **ME-Sampler** [49]: ANQ 7,326, SR% 100.00\\n- **Proposed**: ANQ 3,564, SR% 100.00\\n\\n#### IITMD-FPR\\n- **V-Bad** [20]: ANQ 5,480, SR% 94.67\\n- **ME-Sampler** [49]: ANQ 6,025, SR% 92.62\\n- **Proposed**: ANQ 3,487, SR% 96.33\\n\\n### Table 1. Untargeted attacks on Kinetics-400, Epic-Kitchens, and IITMD-FPR. The attacked models are I3D, Rolling-Unrolling LSTM, and EgoGaitNet respectively.\\n\\n#### Kinetics-400\\n- **V-Bad** [20]: ANQ 23,182, SR% 92.95\\n- **ME-Sampler** [49]: ANQ 11,120, SR% 94.67\\n- **Proposed**: ANQ 6,234, SR% 95.82\\n\\n#### Epic-Kitchens\\n- **V-Bad** [20]: ANQ 44,326, SR% 84.23\\n- **ME-Sampler** [49]: ANQ 22,541, SR% 89.12\\n- **Proposed**: ANQ 15,283, SR% 91.56\\n\\n#### IITMD-FPR\\n- **V-Bad** [20]: ANQ 34,382, SR% 82.19\\n- **ME-Sampler** [49]: ANQ 18,759, SR% 86.67\\n- **Proposed**: ANQ 9,910, SR% 87.33\\n\\n### Table 2. Targeted attacks on Kinetics-400, Epic-Kitchens, and IITMD-FPR. The attacked models are I3D, Rolling-Unrolling LSTM, and EgoGaitNet respectively.\\n\\n#### Kinetics-400\\n- **V-Bad** [20]: ANQ 23,182, SR% 92.95\\n- **ME-Sampler** [49]: ANQ 11,120, SR% 94.67\\n- **Proposed**: ANQ 6,234, SR% 95.82\\n\\n#### Epic-Kitchens\\n- **V-Bad** [20]: ANQ 44,326, SR% 84.23\\n- **ME-Sampler** [49]: ANQ 22,541, SR% 89.12\\n- **Proposed**: ANQ 15,283, SR% 91.56\\n\\n#### IITMD-FPR\\n- **V-Bad** [20]: ANQ 34,382, SR% 82.19\\n- **ME-Sampler** [49]: ANQ 18,759, SR% 86.67\\n- **Proposed**: ANQ 9,910, SR% 87.33\\n\\n### Table 3. Ablation study on Kinetics-400, Epic-Kitchens, and IITMD-FPR. The attacked models are I3D, Rolling-Unrolling LSTM, and EgoGaitNet respectively.\\n\\n#### Kinetics-400\\n- **Only Intensity**: ANQ 3,569, SR% 99.0\\n- **Only Rotation**: ANQ 1,067, SR% 38.19\\n- **Manual Composition**: ANQ 1,884, SR% 62.50\\n- **Proposed**: ANQ 1,257, SR% 99.33\\n\\n#### Epic-Kitchens\\n- **Only Intensity**: ANQ 8,238, SR% 100.00\\n- **Only Rotation**: ANQ 3,286, SR% 62.81\\n- **Manual Composition**: ANQ 4,467, SR% 79.67\\n- **Proposed**: ANQ 3,564, SR% 100.00\\n\\n#### IITMD-FPR\\n- **Only Intensity**: ANQ 6,356, SR% 95.23\\n- **Only Rotation**: ANQ 3,286, SR% 58.42\\n- **Manual Composition**: ANQ 4,019, SR% 72.48\\n- **Proposed**: ANQ 3,487, SR% 96.33\\n\\n### Evaluation Metric [49]\\n\\n- **ANQ**: Measures the average number of queries made in attacking across all videos.\\n- **SR**: Gives the overall success rate in attacking within a query budget Q.\\n\\n### 4.2. Quantitative Comparison\\n\\n#### Untargeted Attacks\\nWe report the effectiveness of our proposed method compared to SOTA in Tab. 1. We compare with V-BAD [20], and ME-Sampler [49]. To the best of our knowledge these are the only two video based adversarial attack models with the source code available. We see that our technique achieves comparable SR as the SOTA, while taking a fraction of query budget in comparison. We also report the comparative performance on top-5 performing classes of each of the attacked model in Tab. 4.\\n\\n#### Targeted Attack\\nWe report the results of the targeted attacks in Tab. 2. We also report the results of top-5 performing classes of each attacked model in Tab. 5. Similar to untargeted attacks, here also we observe similar SR performance and a large improvement in query budget. For example, on Epic-Kitchens, our method consumes only 15,283 queries, in comparison to 44,326 by V-BAD and 22,541 by ME-Sampler, an improvement of almost $3 \\\\times$. Even for Kinetics dataset, we outperform V-BAD and ME-Sampler by saving 16,948 and 4,886 queries, respectively, and achieve a comparable success rate.\"}"}
{"id": "CVPR-2022-1158", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Untargeted attacks on top-4 performing classes of Kinetics-400, Epic-Kitchens, and IITMD-FPR. The attacked models are I3D, Rolling-Unrolling LSTM, and EgoGaitNet respectively.\\n\\n| Dataset          | Method   | Class 1 | Class 2 | Class 3 | Class 4 |\\n|------------------|----------|---------|---------|---------|---------|\\n| Kinetics-400     | V-Bad    | 4,618   | 99.54   | 4,975   | 99.57   |\\n|                  | ME-Sampler | 2,246   | 99.32   | 2,554   | 98.71   |\\n|                  | Proposed | 1,851   | 99.35   | 1,719   | 99.40   |\\n\\n| Epic-Kitchens    | V-Bad    | 8,421   | 99.61   | 8,156   | 99.72   |\\n|                  | ME-Sampler | 7,672   | 100.00  | 7,914   | 100.00  |\\n|                  | Proposed | 6,496   | 100.00  | 6,944   | 100.00  |\\n\\n| IITMD-FPR        | V-Bad    | 5,836   | 94.11   | 5,706   | 94.51   |\\n|                  | ME-Sampler | 5,720   | 92.53   | 5,661   | 91.34   |\\n|                  | Proposed | 3,531   | 95.97   | 3,718   | 96.38   |\\n\\nTable 5. Targeted attacks on top-4 performing classes of Kinetics-400, Epic-Kitchens, and IITMD-FPR. The attacked models are I3D, Rolling-Unrolling LSTM, and EgoGaitNet respectively.\\n\\n| Dataset          | Method   | Class 1 | Class 2 | Class 3 | Class 4 |\\n|------------------|----------|---------|---------|---------|---------|\\n| Kinetics-400     | V-Bad    | 23,059  | 91.74   | 27,234  | 93.14   |\\n|                  | ME-Sampler | 11,217  | 95.24   | 11,181  | 94.62   |\\n|                  | Proposed | 6,414   | 95.87   | 6,037   | 95.65   |\\n\\n| Epic-Kitchens    | V-Bad    | 43,646  | 82.55   | 43,436  | 83.15   |\\n|                  | ME-Sampler | 22,040  | 87.96   | 22,159  | 88.94   |\\n|                  | Proposed | 15,037  | 92.34   | 15,071  | 91.49   |\\n\\n| IITMD-FPR        | V-Bad    | 30,338  | 82.01   | 35,508  | 81.36   |\\n|                  | ME-Sampler | 18,553  | 86.26   | 18,888  | 87.11   |\\n|                  | Proposed | 9,908   | 87.23   | 10,471  | 87.81   |\\n\\n4.3. Qualitative Analysis\\n\\nThe comparative qualitative analysis of the proposed framework with ME-Sampler is shown in Fig. 3. We have shown the analysis for three video segments, choosing the middle frame from each video. For detailed analysis, please refer to the supplementary material. The first column shows the original frame, the second column shows the attacked frame using ME-Sampler, and the third column shows the attacked frame using our proposed technique. We have also mentioned the number of queries required for the successful attack for each frame. It is evident from the figure that our proposed framework, similar to ME-Sampler, produces imperceptible perturbation to the video frame. However, our proposed framework requires substantially smaller number of queries for successful attack.\\n\\n4.4. Ablation Study\\n\\nIntensity based Vs Geometric Perturbation: We have conducted ablation study to understand importance of various components of the proposed architecture. Our method introduces a mix of intensity based and geometric noise. In Tab. 3 we show the results, when only one of the noise type is used for perturbation. We see that only intensity based attack causes much more query to generate the perturbation, whereas rotation based attacks require much lesser queries but also a much lower success rate. Combining the both as in the proposed framework, achieves high success rate at a lower query budget.\\n\\nManual Vs Learnt $\\\\gamma$: The composition factor to combine the intensity based and geometric perturbation is automatically learnt by our model using DifferenceNet. In Tab. 3 we also show the results after setting composition weight manually. One can see that similar to geometric perturbation, the configuration achieves low success rate, at a low query budget. Automated learning of composition weight gives best results, thus validating the need of DifferenceNet.\\n\\nDistribution of $\\\\gamma$: One of the key components of our model is the distribution of $\\\\gamma$. We have observed that for different datasets and models, the distribution of $\\\\gamma$ varies. This highlights the importance of learning the composition weight automatically to achieve optimal performance.\"}"}
{"id": "CVPR-2022-1158", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Comparative Qualitative Analysis of the proposed system. The detailed analysis is in the supplementary material. The first column shows the original frame, the second column shows the attacked frame using ME-Sampler [49], and the third column shows the attacked frame using our proposed technique.\\n\\nFigure 4. Histogram of the learned composition parameter on Epic-Kitchens dataset. The minimum and maximum values of $\\\\gamma$ are 0.07 and 0.96, respectively. Given such a variability of $\\\\gamma$, learnable gradient composition is required for successful attacks.\\n\\nRelationship between $\\\\gamma$ and Video Content: To understand the relationship between $\\\\gamma$ value and the corresponding video, we chose few videos having low, middle and high $\\\\gamma$ values. A few representation frames of these videos are shown in Fig. 5. We observe that the videos having small spatio-temporal variation, results in higher $\\\\gamma$. Conversely, large variations results in smaller $\\\\gamma$. This is expected, since in the videos where spatio-temporal variation is small, intensity-based noise has more affect rather than geometric noise. Hence, the proposed framework favors intensity noise by learning a high $\\\\gamma$ value.\\n\\n5. Conclusion\\n\\nBlack-Box adversarial attacks on DNNs for videos analysis have utilized intensity-based noise for adversarial perturbation. However, such frameworks, require a large number of queries for estimating the perturbation. To overcome that, we propose a parametric noise based adversarial attack. It utilizes both intensity-based noise and camera rotational noise for generating the adversarial video. Gradient estimation has been done over both noises and are merged using a learnable novel gradient composition framework. We have shown the efficacy of the proposed framework on both first-person and third-person video analysis tasks.\\n\\n6. Acknowledgement\\n\\nThis work was supported in part by the DST, Government of India, under project id T-138.\"}"}
{"id": "CVPR-2022-1158", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Bharat Lal Bhatnagar, Suriya Singh, Chetan Arora, and CV Jawhar. Unsupervised learning of deep feature representation for clustering egocentric actions. In Proceedings of the 26th International Joint Conference on Artificial Intelligence, pages 1447\u20131453, 2017.\\n\\n[2] Joao Carreira and Andrew Zisserman. Quo vadis, action recognition? A new model and the kinetics dataset. In proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6299\u20136308, 2017.\\n\\n[3] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801\u2013818, 2018.\\n\\n[4] Zhikai Chen, Lingxi Xie, Shanmin Pang, Yong He, and Qi Tian. Appending adversarial frames for universal video attack. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 3199\u20133208, 2021.\\n\\n[5] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. Scaling egocentric vision: The epic-kitchens dataset. In European Conference on Computer Vision (ECCV), 2018.\\n\\n[6] Jiawei Du, Hu Zhang, Joey Tianyi Zhou, Yi Yang, and Jiashi Feng. Query-efficient meta attack to deep neural networks. arXiv preprint arXiv:1906.02398, 2019.\\n\\n[7] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and Kaiming He. Slowfast networks for video recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6202\u20136211, 2019.\\n\\n[8] Jessica Finocchiaro, Aisha Urooj Khan, and Ali Borji. Egocentric height estimation. In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1142\u20131150. IEEE, 2017.\\n\\n[9] Antonino Furnari and Giovanni Farinella. Rolling-unrolling lstms for action anticipation from first-person video. IEEE transactions on pattern analysis and machine intelligence, 2020.\\n\\n[10] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. arXiv preprint arXiv:1412.6572, 2014.\\n\\n[11] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.\\n\\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\n[13] Joel A Hesch and Stergios I Roumeliotis. Consistency analysis and improvement for single-camera localization. In 2012 IEEE Computer Society Conference on Computer Vision and Pattern Recognition Workshops, pages 15\u201322. IEEE, 2012.\\n\\n[14] Yedid Hoshen and Shmuel Peleg. An egocentric look at video photographer identity. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4284\u20134292, 2016.\\n\\n[15] Yifei Huang, Minjie Cai, Zhenqiang Li, Feng Lu, and Yoichi Sato. Mutual context network for jointly estimating egocentric gaze and action. IEEE Transactions on Image Processing, 29:7795\u20137806, 2020.\\n\\n[16] Andrew Ilyas, Logan Engstrom, Anish Athalye, and Jessy Lin. Black-box adversarial attacks with limited queries and information. In International Conference on Machine Learning, pages 2137\u20132146. PMLR, 2018.\\n\\n[17] Andrew Ilyas, Logan Engstrom, and Aleksander Madry. Prior convictions: Black-box adversarial attacks with bandits and priors. arXiv preprint arXiv:1807.07978, 2018.\\n\\n[18] Nathan Inkawhich, Matthew Inkawhich, Yiran Chen, and Hai Li. Adversarial attacks for optical flow-based action recognition classifiers. arXiv preprint arXiv:1811.11875, 2018.\\n\\n[19] Shuiwang Ji, Wei Xu, Ming Yang, and Kai Yu. 3d convolutional neural networks for human action recognition. IEEE transactions on pattern analysis and machine intelligence, 35(1):221\u2013231, 2012.\\n\\n[20] Linxi Jiang, Xingjun Ma, Shaoxiang Chen, James Bailey, and Yu-Gang Jiang. Black-box adversarial attacks on video recognition models. In Proceedings of the 27th ACM International Conference on Multimedia, pages 864\u2013872, 2019.\\n\\n[21] Kris M Kitani, Takahiro Okabe, Yoichi Sato, and Akihiro Sugimoto. Fast unsupervised ego-action learning for first-person sports videos. In CVPR 2011, pages 3241\u20133248. IEEE, 2011.\\n\\n[22] Shasha Li, Ajaya Neupane, Sujoy Paul, Chengyu Song, Srikanth V Krishnamurthy, Amit K Roy Chowdhury, and Ananthram Swami. Adversarial perturbations against real-time video classification systems. arXiv preprint arXiv:1807.00458, 2018.\\n\\n[23] Ana Cristina Murillo, Daniel Guti\u00e9rrez-G\u00f3mez, Alejandro Rituerto, Luis Puig, and Josechu J Guerrero. Wearable omnidirectional vision system for personal localization and guidance. In 2012 IEEE computer society conference on computer vision and pattern recognition workshops, pages 8\u201314. IEEE, 2012.\\n\\n[24] Pravin Nagar, Mansi Khemka, and Chetan Arora. Concept drift detection for multivariate data streams and temporal segmentation of daylong egocentric videos. In Proceedings of the 28th ACM International Conference on Multimedia, pages 1065\u20131074, 2020.\\n\\n[25] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia conference on computer and communications security, pages 506\u2013519, 2017.\\n\\n[26] Hyun Park, Eakta Jain, and Yaser Sheikh. 3d social saliency from head-mounted cameras. Advances in Neural Information Processing Systems, 25:422\u2013430, 2012.\\n\\n[27] Hyun Soo Park, Eakta Jain, and Yaser Sheikh. Predicting primary gaze behavior using social saliency fields. In Proceedings of the 27th IEEE Conference on Computer Vision and Pattern Recognition, pages 1065\u20131074, 2020.\"}"}
{"id": "CVPR-2022-1158", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[28] Hamed Pirsiavash and Deva Ramanan. Detecting activities of daily living in first-person camera views. In 2012 IEEE conference on computer vision and pattern recognition, pages 2847\u20132854. IEEE, 2012.\\n\\n[29] Yair Poleg, Chetan Arora, and Shmuel Peleg. Temporal segmentation of egocentric videos. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2537\u20132544, 2014.\\n\\n[30] Yair Poleg, Ariel Ephrat, Shmuel Peleg, and Chetan Arora. Compact cnn for indexing egocentric videos. In 2016 IEEE winter conference on applications of computer vision (WACV), pages 1\u20139. IEEE, 2016.\\n\\n[31] Zhaofan Qiu, Ting Yao, and Tao Mei. Learning spatio-temporal representation with pseudo-3d residual networks. In proceedings of the IEEE International Conference on Computer Vision, pages 5533\u20135541, 2017.\\n\\n[32] Anuj Rathore, Pravin Nagar, Chetan Arora, and CV Jawahar. Generating 1 minute summaries of day long egocentric videos. In Proceedings of the 27th ACM International Conference on Multimedia, pages 2305\u20132313, 2019.\\n\\n[33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28:91\u201399, 2015.\\n\\n[34] Karen Simonyan and Andrew Zisserman. Two-stream convolutional networks for action recognition in videos. arXiv preprint arXiv:1406.2199, 2014.\\n\\n[35] Suriya Singh, Chetan Arora, and CV Jawahar. First person action recognition using deep learned descriptors. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2620\u20132628, 2016.\\n\\n[36] Suriya Singh, Chetan Arora, and CV Jawahar. Trajectory aligned features for first person action recognition. Pattern Recognition, 62:45\u201355, 2017.\\n\\n[37] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus. Intriguing properties of neural networks. arXiv preprint arXiv:1312.6199, 2013.\\n\\n[38] Daksh Thapar, Chetan Arora, and Aditya Nigam. Is sharing of egocentric video giving away your biometric signature? In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XVII, pages 399\u2013416. Springer, 2020.\\n\\n[39] Daksh Thapar, Aditya Nigam, and Chetan Arora. Recognizing camera wearer from hand gestures in egocentric videos. In Proceedings of the 28th ACM International Conference on Multimedia, pages 2095\u20132103, 2020.\\n\\n[40] Du Tran, Lubomir Bourdev, Rob Fergus, Lorenzo Torresani, and Manohar Paluri. Learning spatiotemporal features with 3d convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 4489\u20134497, 2015.\\n\\n[41] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 6450\u20136459, 2018.\\n\\n[42] Sagar Verma, Pravin Nagar, Divam Gupta, and Chetan Arora. Making third person techniques recognize first-person actions in egocentric videos. In 2018 25th IEEE International Conference on Image Processing (ICIP), pages 2301\u20132305. IEEE, 2018.\\n\\n[43] Xiaolong Wang, Ross Girshick, Abhinav Gupta, and Kaiming He. Non-local neural networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7794\u20137803, 2018.\\n\\n[44] Xingxing Wei, Jun Zhu, Sha Yuan, and Hang Su. Sparse adversarial perturbations for videos. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 8973\u20138980, 2019.\\n\\n[45] Zhipeng Wei, Jingjing Chen, Xingxing Wei, Linxi Jiang, Tat-Seng Chua, Fengfeng Zhou, and Yu-Gang Jiang. Heuristic black-box adversarial attacks on video recognition models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 12338\u201312345, 2020.\\n\\n[46] Jia Xu, Lopamudra Mukherjee, Yin Li, Jamieson Warner, James M Rehg, and Vikas Singh. Gaze-enabled egocentric video summarization via constrained submodular maximization. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2235\u20132244, 2015.\\n\\n[47] Huanqian Yan, Xingxing Wei, and Bo Li. Sparse black-box video attack with reinforcement learning. arXiv preprint arXiv:2001.03754, 2020.\\n\\n[48] Zhao Yang, Tie Liu, Jiehao Liu, Li Wang, and Sai Zhao. A novel soft margin loss function for deep discriminative embedding learning. IEEE Access, 8:202785\u2013202794, 2020.\\n\\n[49] Hu Zhang, Linchao Zhu, Yi Zhu, and Yi Yang. Motion-excited sampler: Video adversarial attack with sparked prior. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XX, pages 240\u2013256. Springer, 2020.\\n\\n[50] Linchao Zhu, Du Tran, Laura Sevilla-Lara, Yi Yang, Matt Feiszli, and Heng Wang. Faster recurrent networks for efficient video classification. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13098\u201313105, 2020.\\n\\n[51] Linchao Zhu and Yi Yang. Label independent memory for semi-supervised few-shot video classification. IEEE Transactions on Pattern Analysis & Machine Intelligence, (01):1\u20131, 2020.\"}"}
{"id": "CVPR-2022-1158", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Merry Go Round: Rotate a Frame and Fool a DNN\\n\\nDaksh Thapar, Aditya Nigam\\nIndian Institute of Technology Mandi\\ndakshthapar.github.io, faculty.iitmandi.ac.in/\u02dcaditya\\n\\nChetan Arora\\nIndian Institute of Technology Delhi\\nwww.cse.iitd.ac.in/\u02dcchetan\\n\\nAbstract\\n\\nA large proportion of videos captured today are first person videos shot from wearable cameras. Similar to other computer vision tasks, Deep Neural Networks (DNNs) are the workhorse for most state-of-the-art (SOTA) egocentric vision techniques. On the other hand DNNs are known to be susceptible to Adversarial Attacks (AAs) which add imperceptible noise to the input. Both black-box, as well as white-box attacks on image as well as video analysis tasks have been shown. We observe that most AAs techniques basically add intensity perturbation to an image. Even for videos, the same process is essentially repeated for each frame independently. We note that definition of imperceptibility used for images may not be applicable for videos, where a small intensity change happening randomly in two consecutive frames may still be perceptible. In this paper we make a key novel suggestion to use perturbation in optical flow to carry out AAs on a video analysis system. Such perturbation is especially useful for egocentric videos, because there is lot of shake in the egocentric videos anyways, and adding a little more, keeps it highly imperceptible. In general our idea can be seen as adding structured, parametric noise as the adversarial perturbation. Our implementation of the idea by adding 3D rotations to the frames, reveal that using our technique, one can mount a black-box AA on an egocentric activity detection system in one-third of the queries compared to the SOTA AA technique.\\n\\n1. Introduction\\n\\nDespite achieving superior performance on a variety of computer vision tasks [3,11,12,33], Deep Neural Networks (DNNs) remain remarkably susceptible to imperceptible adversarial perturbations [37]. The goal of an adversarial attack (AA) is, given a clean image, $I$, create an adversarial perturbation ($P$), which when added to the clean image, generates an adversarial sample $I_{adv} = I + P$, which tricks a DNN model into producing an incorrect prediction. Since the purpose is to attack a system, the perturbation should be imperceptible to the humans.\\n\\nIntensity-based noise\\n\\nIntensity-based Perturbation\\n\\nIterative Optimization\\n\\nCamera Rotational noise\\n\\nCamera Rotational Perturbation\\n\\nIterative Optimization\\n\\nClean video segment\\n\\nIntensity-based attack.\\n\\nHigh SR, High ANQ\\n\\nCamera Rotational attack.\\n\\nLow SR, Low ANQ\\n\\nApplying Perturbation\\n\\nCombined attack.\\n\\nHigh SR, Low ANQ\\n\\nCurrent Techniques\\n\\nProposed Technique\\n\\nFigure 1. A brief pipeline for the proposed system.\\n\\nSR denote Success Rate, and ANQ denotes Average Number of Queries. Successful Attack demands high SR and low ANQ. For a given input video of size $T \\\\times H \\\\times W \\\\times C$, where $T$ is the number of frames, and $H$, $W$, and $C$ are height, width, and channel respectively of each frame, an intensity based attack needs to predict $T \\\\times H \\\\times W \\\\times C$ parameters. Whereas, our proposed parametric perturbation attack, using rotation based transformation, predicts only $T \\\\times 3$ parameters. This reduces the query budget to predict the parameters. Geometric transformations are natural perturbations and do not disturb semantic integrity of an image or a video.\\n\\nThe simplest setting to mount such an AA is when the adversary gets full access to the model ($M$), including input ($X$)/output ($Y$), and the exact gradients ($G$). One can, then, simply backpropagate the loss corresponding to the desired (incorrect) output, and use it guide the perturbation in the input [16, 25, 37]. The setting is called white box attacks, but is usually impractical in real life, due to unavailability of the full access to the model. The alternate setting is the black box setting when an adversary has access to $X$, and $Y$, but not $G$. In this formulation the primary challenge becomes estimating the gradient at the input without having access to $G$ [6, 16, 17]. The quality of an AA technique is usually determined by how imperceptible the $P$ is, and additionally in case of black box attacks, how many ($X$, $Y$) pairs a technique needs to find a $P$ corresponding to a particular $I$.\"}"}
{"id": "CVPR-2022-1158", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Researchers have shown both white box and black box attacks for a variety of DNN models across a range of tasks [37]. Further, relevant to our context, the attacks have been shown when the input to the model is an image [16,17], or a video [20,49]. Our focus in this paper is on mounting black box adversarial attacks on video analysis (VA) systems.\\n\\nWe note that most of the techniques for AA on a VA system trivially extend the black box pipeline from images to videos. The videos are broken down into frames, and adversarial examples are created by adding random perturbations in the pixel intensities [20,49]. For a successful attack, these methods require a large number of queries on the target model. For example [20] requires 23K queries on an average for generating a single adversarial sample. We would like to emphasize that a frame-wise attack, using intensity-based noise, do not coordinate the adversarial perturbations between consecutive frames. While a change in intensity level of a few individual pixels may be imperceptible in an individual frame, when played as a video, such random flashes are easily detected by a human being.\\n\\nOne of the key ideas of this paper is to parameterize the perturbation. The parameterization has two advantages, (1) it is easier to regularize within, and across the frames, and (2) one can perturb a large number of pixels, by estimating only a few parameters, thus reducing the query budget, an important consideration in a black box attack. While, the idea of parametric perturbation is generic and can be used in a variety of settings, given our focus to videos, we consider it for attack on VA systems, and even more specifically, on egocentric VA systems.\\n\\nWe observe that one of the simplest ways to perform coordinated change in intensity levels of large number of pixels, across frames of a video, is by geometrically transforming each frame. The transformation will cause change in the optical flow, which is an important cue for many VA tasks. At the same time, performing frame-wise geometric transformation maintains semantic integrity of frame contents, keeping it imperceptible to human beings.\\n\\nContributions: The key contributions of this work are:\\n1. We propose to add novel parametric perturbations to mount an AA attack against a computer vision system.\\n2. For a VA system, we suggest use of geometric transformations to implement such parametric perturbations.\\n3. We propose a novel DNN architecture for predicting a mix of intensity, and geometric perturbations which can successfully fool a VA system to carry out black box AA attack.\\n4. Our exhaustive experiments on multitude of benchmark datasets, and VA tasks for egocentric, and third person videos show that our proposed architecture outperforms SOTA techniques, managing to fool a DNN in one-third of the queries as needed by the SOTA.\"}"}
{"id": "CVPR-2022-1158", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3. Proposed Methodology\\n\\n3.1. Gradient Estimation\\n\\nWe consider a DNN model \\\\( f \\\\), which has been pre-trained for some VA task. The model takes as input a video \\\\( V \\\\in \\\\mathbb{R}^{T \\\\times H \\\\times W \\\\times C} \\\\), where \\\\( T, H, W, \\\\) and \\\\( C \\\\) represent video length, height, width, and number of channels (in each frame) respectively. Assuming a video classification model, the output of \\\\( f \\\\) is a label \\\\( y \\\\in \\\\{1, \\\\ldots, K\\\\} \\\\), where \\\\( K \\\\) is the number of classes. The goal of an adversarial attack is, given an input video \\\\( V \\\\), generate an adversarial video \\\\( V_{\\\\text{adv}} \\\\) which minimises the loss function:\\n\\n\\\\[\\nL = \\\\max(l_y - \\\\max_{k \\\\neq y}(l_k), 0).\\n\\\\]\\n\\nHere \\\\( l \\\\) is the logit vector corresponding to input \\\\( V_{\\\\text{adv}} \\\\), and \\\\( l_i \\\\) is the value of the \\\\( i \\\\)th element (corresponding to class \\\\( i \\\\)) of the vector. Minimizing \\\\( L \\\\) confuses the model with the second most confident class prediction for the untargeted adversarial attack. For the targeted attack \\\\( \\\\max_k \\\\neq y(l_k) \\\\) can be replaced by the logit of the corresponding class. To simplify the notation, in the rest of the paper we simply use \\\\( L(V, y) \\\\) instead of \\\\( L(f(V), y) \\\\). The adversarial video \\\\( V_{\\\\text{adv}} \\\\) is chosen as:\\n\\n\\\\[\\nV_{\\\\text{adv}} = \\\\arg \\\\min V_{\\\\text{adv}} L(f(V_{\\\\text{adv}}), y) \\\\quad \\\\text{s.t. } \\\\text{dist}(V_{\\\\text{adv}}, V) \\\\leq \\\\max \\\\text{dist}, \\\\text{ and } \\\\#\\\\text{queries} \\\\leq Q.\\n\\\\]\\n\\nWe can model \\\\( V_{\\\\text{adv}} \\\\) using any perturbation parameterized by \\\\( \\\\theta \\\\in \\\\mathbb{R}^{T \\\\times d} \\\\), where \\\\( d \\\\) is the dimension of \\\\( \\\\theta \\\\), s.t. \\\\( V_{\\\\text{adv}} = \\\\text{Pert}(V, \\\\theta) \\\\). Here, the function \\\\( \\\\text{Pert}(V, \\\\theta) \\\\) applies the perturbation parameterized by \\\\( \\\\theta \\\\) on the video \\\\( V \\\\). The function \\\\( \\\\text{Pert} \\\\) will be dependent upon the type of perturbation and is defined in detail in Sec. 3.2. To generate an adversarial video \\\\( V_{\\\\text{adv}} \\\\), we need to find an optimal perturbation \\\\( \\\\theta^* \\\\) s.t.:\\n\\n\\\\[\\n\\\\theta^* = \\\\arg \\\\min_{\\\\theta} L(\\\\text{Pert}(V, \\\\theta), y) \\\\quad \\\\text{s.t. } \\\\|\\\\theta\\\\|_2 \\\\leq k, \\\\text{ and } \\\\#\\\\text{queries} \\\\leq Q.\\n\\\\]\"}"}
{"id": "CVPR-2022-1158", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The above perturbation framework allows us to generalize the adversarial attacks additive, multiplicative, or even some complex non-differentiable perturbations. Moreover, it allows us to design a parametric perturbation of a very low dimension $d$ which is easier to compute in limited query budget.\\n\\nThe key challenge in black-box adversarial attacks is to estimate the gradient of a model. It is because for this setting, the model is not accessible (beyond input, output), and the gradient $\\\\nabla_\\\\theta L(Pert(V, \\\\theta), y)$ required for generating $V_{adv}$ cannot be directly computed. Hence, we adopt an iterative optimization strategy suggested in [49] for estimating $\\\\nabla_\\\\theta L(Pert(V, \\\\theta), y)$.\\n\\nIt is important to note that for an iterative optimization, we are only interested in the direction of $\\\\nabla_\\\\theta L(Pert(V, \\\\theta), y)$ rather than its exact value which also includes the magnitude. Hence, we learn a vector $g \\\\in \\\\mathbb{R}^{T \\\\times d}$ whose direction $\\\\|g\\\\|$ aligns with $\\\\nabla_\\\\theta L(Pert(V, \\\\theta), y)$. In order to estimate such a $g$, we use the following loss function [17]:\\n\\n$$l(g) = -\\\\langle \\\\nabla_\\\\theta L(Pert(V, \\\\theta), y), g \\\\|g\\\\| \\\\rangle,$$\\n\\n(4)\\n\\nwhich is the inverse of directional derivative of $L$, in the direction of the vector $g$. The inverse direction of directional derivative provides the direction of $g$'s movement to optimize $l(g)$ and get closer to the desired gradient $\\\\nabla_\\\\theta L(Pert(V, \\\\theta), y)$ as:\\n\\n$$g^* = \\\\arg \\\\min_g (l(g)).$$\\n\\n(5)\\n\\nIn order to compute $g^*$, we compute the gradient $\\\\nabla g l(g)$, denoted as $\\\\Delta$. We perform a two-query estimation to the expectation and apply the authentic sampling [17] to get:\\n\\n$$\\\\Delta = l(g + \\\\delta r) - l(g - \\\\delta r) \\\\delta r,$$\\n\\n(6)\\n\\nwhere $r \\\\in \\\\mathbb{R}^{T \\\\times d} \\\\in \\\\mathbb{N}(0, 1)$ is the Gaussian noise, and $\\\\delta$ is a small number scaling the magnitude of loss variation. In two-query estimation, $r$ vector acts as a directional candidate for the update of $g$. We query in the direction of $r$ and in its opposite direction. This gives us a scalar indicating of how good the candidate $r$ is. We scale accordingly to form our update of $g$.\\n\\nFinally, Eq. (4) can be approximated as [17]:\\n\\n$$l(g) = -\\\\langle \\\\nabla_\\\\theta L(Pert(V, \\\\theta), y), g \\\\|g\\\\| \\\\rangle \\\\approx -L(Pert(V, \\\\theta + \\\\epsilon g), y) - L(Pert(V, \\\\theta), y) \\\\epsilon,$$\\n\\n(7)\\n\\nwhere $\\\\epsilon$ is a small approximation constant. Substituting Eq. (7) into Eq. (6), we get $GE(V, y, \\\\theta, g)$ as:\\n\\n$$\\\\Delta = GE(V, y, \\\\theta, g) = L(Pert(V, \\\\theta + \\\\epsilon g + \\\\delta r), y) - L(Pert(V, \\\\theta + \\\\epsilon g - \\\\delta r), y) \\\\delta r,$$\\n\\n(8)\\n\\nwhere $g^+ = g + \\\\delta r$ and $g^- = g - \\\\delta r$.\\n\\n3.2. Parametric Noise\\n\\nIt can be observed from Eq. (8), that in order to estimate the gradient, we have utilized a random noise ($r$). For intensity-based noise, $r \\\\in \\\\mathbb{R}^{T \\\\times H \\\\times W \\\\times C}$ is used for estimating the gradient $g \\\\in \\\\mathbb{R}^{T \\\\times H \\\\times W \\\\times C}$[49]. This requires one to estimate $T \\\\times H \\\\times W \\\\times C$ parameters for the adversarial attack, which may lead to a high number of queries [49], making such attacks unrealistic in practice.\\n\\nTo overcome these limitations, we have proposed a parametric noise (camera rotational noise $r_{cr}$) which can suitably alter the geometrical properties of a video for an attack. Since, rotation of the camera can be represented as a 3D vector in Euler space, the proposed noise $r_{cr} \\\\in \\\\mathbb{R}^{T \\\\times 3}$ requires only $T \\\\times 3$ parameters to be predicted for an adversarial attack. This significantly reduces the number of queries required to predict it in comparison to an intensity-based noise.\\n\\nWe estimate the camera rotational gradient $g_{cr} \\\\in \\\\mathbb{R}^{T \\\\times 3}$ from $r_{cr}$ using gradient estimation, as discussed in the previous section. This allows us to find a new perturbation vector $\\\\theta$ for each frame. Recall, that $\\\\theta_i \\\\in \\\\mathbb{R}^3$ corresponds to a 3D rotation for the frame. We compute an Homography using the 3D rotation as $H_i = K \\\\cdot \\\\theta_i K^{-1}$, where $K$ is the camera internal matrix (assumed identity in our case). The perturbation can be applied on the video as:\\n\\n$$Pert_{cr}(V, \\\\theta) = \\\\forall i (H_i * V_i),$$\\n\\n(9)\\n\\nwhere, $V_i$ is the $i$th frame in the video $V$ and $*$ denotes the geometric transformation of each frame using the Homography $H_i$. To ensure that the perturbations are small, we have clipped the magnitude of $r_{cr}$ to $0.18$ radians.\\n\\nWe observe that in our experiments the number of queries required to render a successful black-box attack gets substantially reduced by using parametric noise, but at the expense of success rate (refer Sec. 4.2). Hence, we propose to mix it with intensity based perturbation, using a learnable composition parameter, as described in the next section.\\n\\n3.3. Gradient Composition\\n\\nIn order to address the issue of low success rate using parametric noise, we propose a novel learnable gradient composition framework which suitably combines intensity-based, and parametric perturbations. Such fusion exploits spatio-temporal properties of a particular segment in a video.\"}"}
