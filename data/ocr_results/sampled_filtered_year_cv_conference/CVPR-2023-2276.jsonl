{"id": "CVPR-2023-2276", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nThe last several years have witnessed remarkable progress in video-and-language (VidL) understanding. However, most modern VidL approaches use complex and specialized model architectures and sophisticated pretraining protocols, making the reproducibility, analysis and comparisons of these frameworks difficult. Hence, instead of proposing yet another new VidL model, this paper conducts a thorough empirical study demystifying the most important factors in the VidL model design. Among the factors that we investigate are (i) the spatiotemporal architecture design, (ii) the multimodal fusion schemes, (iii) the pretraining objectives, (iv) the choice of pretraining data, (v) pretraining and finetuning protocols, and (vi) dataset and model scaling. Our empirical study reveals that the most important design factors include: temporal modeling, video-to-text multimodal fusion, masked modeling objectives, and joint training on images and videos. Using these empirical insights, we then develop a step-by-step recipe, dubbed VINDLU, for effective VidL pretraining. Our final model trained using our recipe achieves comparable or better than state-of-the-art results on several VidL tasks without relying on external CLIP pretraining. In particular, on the text-to-video retrieval task, our approach obtains 61.2% on DiDeMo, and 55.0% on ActivityNet, outperforming current SOTA by 7.8% and 6.1% respectively. Furthermore, our model also obtains state-of-the-art video question-answering results on ActivityNet-QA, MSRVTT-QA, MSRVTT-MC and TVQA. Our code and pretrained models are publicly available at: https://github.com/klauscc/VindLU.\"}"}
{"id": "CVPR-2023-2276", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. An overview of the existing VidL methods. Significant differences exist among these methods, making it challenging to reproduce, analyze and compare these methods. This motivates us to answer the question \u201cWhat are the key steps to build a highly performant VidL framework\u201d by investigating various components in the VidL framework design.\\n\\nRecent VidL frameworks have become significantly more complex and specialized over the last several years. As a result, it is increasingly difficult to reproduce, analyze and compare most recent VidL frameworks. For example, several recent approaches [25, 32, 66] propose new architectures, new initialization strategies, pretraining objectives, pretraining datasets, and optimization protocols. Due to the large computational cost of ablating all these factors, it is difficult to understand which components are critical to the success of the proposed frameworks. Similarly, the key success factors of many other recent VidL approaches [6, 16, 32, 57] are also often obfuscated, which hinders future research.\\n\\nIn Table 1, we illustrate the complexity of modern VidL frameworks by dissecting them along multiple dimensions, including temporal modeling schemes, multimodal fusion modules, pretraining objectives, the source of the pretraining data, and the number of frames for pretraining, fine-tuning and inference. Based on this analysis, we observe that there exist significant differences among these VidL methods. Unfortunately, it\u2019s not clear which differences are important for the overall VidL performance and which are not.\\n\\nThe recent METER [13] work studies a subset of these components in the context of image-language modeling. However, their analysis is limited to images and, thus, ignores various aspects related to video modeling, such as spatiotemporal architecture design, video pretraining objectives, video pretraining data, and video-specific fine-tuning/evaluation protocols such as the number of frames. As we will show in our experimental section, many of the findings presented in the image-based studies [13] do not hold for video. Beyond image-based analysis, we note that the concurrent work in [17] conducts an empirical study of VidL transformers. However, unlike our work, which covers a broad range of VidL design factors, their analysis is focused predominantly on masked visual modeling objectives, which we also study in this work.\\n\\nOur main objective in this work is to answer the question \u201cWhat are the key steps needed to build a highly performant VidL framework?\u201d To do this, we conduct a thorough empirical study that demystifies the importance of various VidL design choices and ultimately leads to a VidL framework that achieves state-of-the-art results on various VidL benchmarks. Using our empirical insights, we then develop a step-by-step recipe for effective VidL pretraining. Our recipe, dubbed VINDLU (VIdeo aND Language Understanding), starts from a standard Vision Transformer (ViT) [12] and uses a simple progressive expansion scheme where at each step, we investigate a particular aspect of VidL framework design (e.g., architecture, pretraining objective, pretraining data, etc.), and choose the best performing option. In particular, we study the following VidL design components: (i) the spatiotemporal architecture design,\"}"}
{"id": "CVPR-2023-2276", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. We progressively expand an image transformer baseline (e.g., ViT) to a performant video-and-language (VidL) model. We do so by investigating the importance of many VidL design choices such as (i) temporal modeling, (ii) multimodal fusion modules, (iii) pretraining objectives, (iv) the source of the pretraining data, (v) the number of pre-training frames, (vi) multi-stage pretraining, and (vii) scaling of the data and model. Each bar depicts an average text-to-video retrieval Recall@1,5,10 accuracy across MSR-VTT, DiDeMo, ActivityNet-Captions. The red bars denote the best-performing design choice in each subgroup. Our final VidL framework, dubbed VINDLU, outperforms our initial image Transformer baseline by 23.2%.\\n\\nThe key findings of our empirical study include:\\n\\n1. Contrary to the conclusions of several prior works [6, 25] that a single frame is sufficient for VidL modeling, we discover that temporal modeling using multiple frames leads to a significant improvement over the spatial-only baselines (+6% averaged video retrieval accuracy on MSR-VTT, DiDeMo, and ActivityNet).\\n\\n2. Multimodal fusion module incorporating video features into text is critical for good VidL performance (+3.6%). Conversely, adding text features to the video representation is not useful.\\n\\n3. Masked language modeling objective significantly improves performance (+6.2%) while masked video modeling objective brings an additional +1% improvement.\\n\\n4. Pretraining jointly on images and videos is beneficial (+2.7%). Also, contrary to prior methods [2, 57], we find multi-stage training unnecessary.\\n\\n5. Pretraining with a small number of frames (e.g., 4) is sufficient and it can significantly reduce the computational cost of large-scale pretraining. Pretraining with more frames does not lead to a substantial performance boost.\\n\\n6. Compared to many recent CLIP-based [45] VidL approaches [3, 40, 66], our recipe achieves comparable or even better performance with 20\u00d7 less pretraining data.\\n\\nOur final model, trained using our VINDLU recipe, achieves state-of-the-art results on several VidL benchmarks. Specifically, on the video retrieval task, our method achieves 46.5%, 61.2%, 55.0% R@1 accuracy on MSR-VTT, DiDeMo, and ActivityNet outperforming the state-of-the-art by 7.8% and 6.1% on the latter two datasets. Also, our approach obtains state-of-the-art video question-answering results on ActivityNet-QA, MSRVTT-QA, MSRVTT-MC and TVQA, where we achieve top-1 accuracy of 44.7%, 44.6%, 97.1%, and 79.0% respectively.\\n\\nWe want to make it clear that, in this paper, we do not claim technical novelty behind any of the individual design choices (i.e., different subsets of these design choices were already used by prior VidL methods as shown in Table 1). Instead, our main contribution, which we believe might be equally if not more important than proposing yet another specialized or obfuscated VidL model, is to investigate these components collectively and validate their importance. We also do not claim superiority over previous methods (despite better results). Due to the implementation complexities of each method, fair and complete comparisons are difficult and not our intent. Instead, we hope that our recipe for building an effective VidL framework will provide useful insights for future research on VidL understanding. To enable the VidL community to build on our work, we release our code and pretrained models.\"}"}
{"id": "CVPR-2023-2276", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"these findings generalize to video. In comparison, our work thoroughly investigates various video-specific design choices for effective video-language pretraining.\\n\\nVideo-and-Language Pretraining. In recent years, the large-scale VidL pretraining [6,16,26,32,57,58] has shown strong transfer learning ability to downstream VidL tasks such as text-to-video retrieval [1,23,26,35,40,65,72], video question answering [63, 72, 73], video captioning [21, 23, 50, 56, 81], etc. Several methods [3, 18, 40, 66] achieve impressive results by building on the popular image-language pretrained model CLIP [45]. Additionally, several recent approaches [25, 32, 57] propose more sophisticated VidL frameworks to achieve comparable performance as CLIP-based methods without large-scale CLIP pretraining. However, with the impressive results, these methods also require more complex architectures and specialized video pretraining protocols (as shown in Table 1). The complexity of these frameworks and the large computational cost of VidL pretraining makes it challenging to decipher which VidL framework components are truly needed for good performance. Moreover, unlike in the image-language domain, there are few empirical studies investigating various VidL design components collectively. For instance, the concurrent work of Fu [17] only studies masked video modeling pretraining objectives and is based on a slightly older VIOLET [16] method. Furthermore, the recent works [6,25] focus predominantly on spatial biases in modern VidL benchmarks. In contrast to these approaches, our work investigates the importance of various factors in VidL framework design. We then use our empirical insights to provide a detailed step-by-step recipe for effective VidL pretraining.\\n\\n3. A Recipe for Video-Language Pretraining\\n\\nIn this section, we describe our recipe for video-and-language (VidL) pretraining. We begin with a standard image transformer (e.g., ViT [12]) and progressively expand it to a model that achieves state-of-the-art results on various VidL datasets and tasks. At each step of our recipe, we study how various design choices affect VidL performance. In particular, we are interested in answering the following questions about the VidL pretraining design:\\n\\n\u2022 Does a VidL model need temporal modeling, especially since most VidL benchmarks are spatially biased [6,25]? If so, what is the best temporal modeling scheme?\\n\u2022 What is the most effective way to do multimodal fusion? Some approaches [16, 32, 55] use bidirectional while others [25, 57] employ unidirectional multimodal fusion modules. Which of these schemes works the best?\\n\u2022 Which pretraining objectives are most useful for VidL representation learning? Prior methods use video-text contrastive (VTC) [28], video-text matching (VTM) [28, 31, 39], masked-language-modeling (MLM) [11], or masked-video-modeling (MVM) [52]. Are all of these objectives needed for the best performance?\\n\u2022 What pretraining data is most useful for training VidL models (e.g., video-only or images and videos)? Is it necessary to use curriculum learning [2, 55, 57] or is single-stage pretraining sufficient?\\n\u2022 How many frames are needed for pretraining, fine-tuning, and inference? Several approaches [6, 25] claimed that single frame pretraining is sufficient while others [57,66] pretrained their models with 8 or even more frames. Should we finetune the pretrained VidL models using the same number of frames as during pretraining or is it helpful to use more frames during fine-tuning and inference?\\n\\nMotivated by these questions, we next present our recipe while also studying these questions in more detail.\\n\\nStep 0: Starting Ingredients\\n\\nImage Transformer Baseline. We start with a standard ViT-B/16 [12] transformer trained on single frames of WebVid-2M [2]. We use BERT [11] as our text encoder for all experiments. Formally, given the paired video and text input \\\\((v, t)\\\\), The image transformer randomly selects a single frame from the video as input to extract the video embeddings. A text encoder encodes the text \\\\(t\\\\) to extract the text embeddings. We then use a video-text contrastive (VTC) loss to maximize the agreement between the paired video and text embeddings as in [2, 45]. Following [25], we use BEiT [4] initialization for our image transformer, whereas the text encoder is initialized with BERT base.\\n\\nExperimental Setup. As our initial pretraining data, we use WebVid-2M [2] unless noted otherwise. We then fine-tune and evaluate our pretrained model on the three popular text-to-video retrieval datasets: MSR-VTT [65], DiDeMo [1], and ActivityNet-Captions [23], which include short and long videos. We report the averaged top-1, top-5, and top-10 text-to-video retrieval accuracies across these datasets as our evaluation metric. As shown in Fig. 2, our Image Transformer baseline achieves an average accuracy of 50.4%. Over the next several subsections, we progressively expand this baseline by adding more components of increasing complexity. In particular, we start by incorporating (i) temporal modeling blocks, (ii) a multimodal fusion encoder, and (iii) additional pretraining objectives. Afterward, we investigate the choice for the (iv) pretraining data, (v) fine-tuning and inference protocols, and (vi) dataset and model scaling schemes. We would like to note that due to the large computational cost, we cannot ablate the order of the steps in our recipe. Thus, the order of the steps is primarily determined by the computational cost (i.e., the steps that can be implemented most efficiently are studied first then, moving to the more computationally costly steps).\"}"}
{"id": "CVPR-2023-2276", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Lisa Anne Hendricks, Oliver Wang, Eli Shechtman, Josef Sivic, Trevor Darrell, and Bryan Russell. Localizing moments in video with natural language. In Proceedings of the IEEE international conference on computer vision, pages 5803\u20135812, 2017. 1, 4\\n\\n[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in time: A joint video and image encoder for end-to-end retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1728\u20131738, 2021. 1, 2, 3, 4, 5, 6, 8\\n\\n[3] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. A clip-hitchhiker's guide to long video retrieval. arXiv preprint arXiv:2205.08508, 2022. 2, 3, 4, 8\\n\\n[4] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021. 4\\n\\n[5] Gedas Bertasius, Heng Wang, and Lorenzo Torresani. Is space-time attention all you need for video understanding? In Proceedings of the International Conference on Machine Learning (ICML), July 2021. 2, 5\\n\\n[6] Shyamal Buch, Crist\u00f3bal Eyzaguirre, Adrien Gaidon, Jiajun Wu, Li Fei-Fei, and Juan Carlos Niebles. Revisiting the\\\"video\\\" in video-language understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2917\u20132927, 2022. 2, 3, 4, 5\\n\\n[7] Jaeseok Byun, Taebaek Hwang, Jianlong Fu, and Taesup Moon. Grit-vlp: Grouped mini-batch sampling for efficient vision and language pre-training. In European Conference on Computer Vision, pages 395\u2013412. Springer, 2022. 3\\n\\n[8] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3558\u20133568, 2021. 1\\n\\n[9] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantham, Saurabh Gupta, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 1\\n\\n[10] Yen-Chun Chen, Linjie Li, Licheng Yu, Ahmed El Kholy, Faisal Ahmed, Zhe Gan, Yu Cheng, and Jingjing Liu. Uniter: Universal image-text representation learning. In European conference on computer vision, pages 104\u2013120. Springer, 2020. 3\\n\\n[11] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. 4\\n\\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 1, 2, 4\\n\\n[13] Zi-Yi Dou, Yichong Xu, Zhe Gan, Jianfeng Wang, Shuohang Wang, Lijuan Wang, Chenguang Zhu, Pengchuan Zhang, Lu Yuan, Nanyun Peng, et al. An empirical study of training end-to-end vision-and-language transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18166\u201318176, 2022. 2, 3\\n\\n[14] Christoph Feichtenhofer. X3d: Expanding architectures for efficient video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 203\u2013213, 2020. 5\\n\\n[15] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaiming He. Masked autoencoders as spatiotemporal learners. arXiv preprint arXiv:2205.09113, 2022. 6\\n\\n[16] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. Violet: End-to-end video-language transformers with masked visual-token modeling. arXiv preprint arXiv:2111.12681, 2021. 2, 4, 5, 6, 8\\n\\n[17] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. An empirical study of end-to-end video-language transformers with masked visual modeling. arXiv preprint arXiv:2209.01540, 2022. 2, 4\\n\\n[18] Zijian Gao, Jingyu Liu, Sheng Chen, Dedan Chang, Hao Zhang, and Jinwei Yuan. Clip2tv: An empirical study on transformer-based methods for video-text retrieval. arXiv preprint arXiv:2111.05610, 2021. 2, 4\\n\\n[19] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009, 2022. 6\\n\\n[20] Xiaowei Hu, Zhe Gan, Jianfeng Wang, Zhengyuan Yang, Zicheng Liu, Yumao Lu, and Lijuan Wang. Scaling up vision-language pre-training for image captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17980\u201317989, 2022. 3\\n\\n[21] Vladimir Iashin and Esa Rahtu. Multi-modal dense video captioning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 958\u2013959, 2020. 4\\n\\n[22] Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International Conference on Machine Learning, pages 5583\u20135594. PMLR, 2021. 3\\n\\n[23] Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, and Juan Carlos Niebles. Dense-captioning events in videos. In Proceedings of the IEEE international conference on computer vision, pages 706\u2013715, 2017. 1, 3, 4\\n\\n[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. International journal of computer vision, 123(1):32\u201373, 2017. 1, 2\\n\\n[25] Jie Lei, Tamara L Berg, and Mohit Bansal. Revealing single frame bias for video-and-language learning. arXiv preprint arXiv:2206.03428, 2022. 2, 3, 4, 5, 6, 8\\n\\n[26] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7331\u20137341, 2021. 1, 2, 4, 8\"}"}
{"id": "CVPR-2023-2276", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2023-2276", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ers for self-supervised video pre-training.\\n\\n[53] Du Tran, Heng Wang, Lorenzo Torresani, and Matt Feiszli. Video classification with channel-separated convolutional networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.\\n\\n[54] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\n[55] Alex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-training. arXiv preprint arXiv:2203.07303, 2022.\\n\\n[56] Bairui Wang, Lin Ma, Wei Zhang, and Wei Liu. Reconstruction network for video captioning. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7622\u20137631, 2018.\\n\\n[57] Junke Wang, Dongdong Chen, Zuxuan Wu, Chong Luo, Luowei Zhou, Yucheng Zhao, Yujia Xie, Ce Liu, Yu-Gang Jiang, and Lu Yuan. Omnivl: One foundation model for image-language and video-language tasks. arXiv preprint arXiv:2209.07526, 2022.\\n\\n[58] Jinpeng Wang, Yixiao Ge, Guanyu Cai, Rui Yan, Xudong Lin, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Object-aware video-language pre-training for retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3313\u20133322, 2022.\\n\\n[59] Jianfeng Wang, Xiaowei Hu, Zhe Gan, Zhengyuan Yang, Xiyang Dai, Zicheng Liu, Yumao Lu, and Lijuan Wang. Ufo: A unified transformer for vision-language representation learning. arXiv preprint arXiv:2111.10023, 2021.\\n\\n[60] Teng Wang, Wenhao Jiang, Zhichao Lu, Feng Zheng, Ran Cheng, Chengguo Yin, and Ping Luo. Vlmixer: Unpaired vision-language pre-training via cross-modal cutmix. In International Conference on Machine Learning, pages 22680\u201322690. PMLR, 2022.\\n\\n[61] Wenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, et al. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. arXiv preprint arXiv:2208.10442, 2022.\\n\\n[62] Saining Xie, Chen Sun, Jonathan Huang, Zhuowen Tu, and Kevin Murphy. Rethinking spatiotemporal feature learning: Speed-accuracy trade-offs in video classification. In Proceedings of the European conference on computer vision (ECCV), pages 305\u2013321, 2018.\\n\\n[63] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video question answering via gradually refined attention over appearance and motion. In Proceedings of the 25th ACM international conference on Multimedia, pages 1645\u20131653, 2017.\\n\\n[64] Hu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. arXiv preprint arXiv:2109.14084, 2021.\\n\\n[65] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large video description dataset for bridging video and language. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5288\u20135296, 2016.\\n\\n[66] Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua Song, Houqiang Li, and Jiebo Luo. Clip-vip: Adapting pre-trained image-text model to video-language representation alignment. arXiv preprint arXiv:2209.06430, 2022.\\n\\n[67] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer questions from millions of narrated videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1686\u20131697, 2021.\\n\\n[68] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Zero-shot video question answering via frozen bidirectional language models. arXiv preprint arXiv:2206.08155, 2022.\\n\\n[69] Jianwei Yang, Chunyuan Li, Pengchuan Zhang, Bin Xiao, Ce Liu, Lu Yuan, and Jianfeng Gao. Unified contrastive learning in image-text-label space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19163\u201319173, 2022.\\n\\n[70] Xu Yang, Hanwang Zhang, Guojun Qi, and Jianfei Cai. Causal attention for vision-language tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9847\u20139857, 2021.\\n\\n[71] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. arXiv preprint arXiv:2205.01917, 2022.\\n\\n[72] Youngjae Yu, Jongseok Kim, and Gunhee Kim. A joint sequence fusion model for video question answering and retrieval. In Proceedings of the European Conference on Computer Vision (ECCV), pages 471\u2013487, 2018.\\n\\n[73] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. Activitynet-qa: A dataset for understanding complex web videos via question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 9127\u20139134, 2019.\\n\\n[74] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella, Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang, Boxin Li, Chunyuan Li, et al. Florence: A new foundation model for computer vision. arXiv preprint arXiv:2111.11432, 2021.\\n\\n[75] Rowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. Advances in Neural Information Processing Systems, 34:23634\u201323651, 2021.\\n\\n[76] Yan Zeng, Xinsong Zhang, and Hang Li. Multi-grained vision language pre-training: Aligning texts with visual concepts. arXiv preprint arXiv:2111.08276, 2021.\\n\\n[77] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.\"}"}
{"id": "CVPR-2023-2276", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18123\u201318133, 2022.\\n\\n[78] Pengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5579\u20135588, 2021.\\n\\n[79] Luowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, James Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning and vqa. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 13041\u201313049, 2020.\\n\\n[80] Linchao Zhu and Yi Yang. Actbert: Learning global-local video-text representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8746\u20138755, 2020.\\n\\n[81] Mohammadreza Zolfaghari, Kamaljeet Singh, and Thomas Brox. Eco: Efficient convolutional network for online video understanding. In Proceedings of the European conference on computer vision (ECCV), pages 695\u2013712, 2018.\"}"}
{"id": "CVPR-2023-2276", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Step 1: Temporal Modeling\\n\\nIn the first step of our recipe, we extend our initial image transformer to video via a temporal modeling mechanism, which enables training our model on multiple frames. We experiment with several temporal modeling schemes:\\n\\n- **Mean Pooling (MP).** In this variant, the visual encoder processes input frames independently and averages their frame-wise scores for the video-level score as in [40].\\n\\n- **Late Temporal Attention (L-TA).** Following [25,40,42] we use a late temporal modeling scheme by attaching 2 Transformer layers to an image encoder, which then aggregates temporal information across all input frames.\\n\\n- **Temporal Convolution (TC).** Many prior methods [14, 44, 62] used 3D convolutions for temporal modeling. To validate its effectiveness, we inject 3D convolution [53] before the spatial attention to each Transformer Layer.\\n\\n- **Temporal Attention (TA).** Inspired by TimeSformer [5], we experiment with divided space-time attention, which we insert before spatial attention as in [5].\\n\\nAs shown in the upper part of Fig. 2 and the Table below, the temporal modeling capability is critical for good VidL performance. This is indicated by a +6.3% accuracy boost of our temporal attention variant (TA) over the spatial-only baseline. We also observe that late temporal modeling (L-TA) has nearly no effect. We conjecture that this is due to the limited temporal modeling capacity (i.e., only two layers) and the lack of temporal fusion in the early layers. Lastly, our results suggest that TA outperforms TC by 2.1%, which might indicate that long-range temporal attention is more useful than local 3D convolutions.\\n\\n| Mean Pooling | L-TA | TC | TA |\\n|-------------|------|----|----|\\n| acc.(%)     | 49.8 | 50.2 | 54.6 | 56.7 |\\n\\nInterestingly, we note that our findings contradict the conclusions of several recent methods [6, 25], claiming that temporal modeling is not needed for many VidL tasks. We hypothesize that even on the spatially-biased datasets, temporal modeling is useful for resolving spatial ambiguities caused by appearance variations across different frames.\\n\\nTakeaway #1: We adopt Temporal Attention (TA) as our temporal modeling mechanism and pretrain our model with 4-frame inputs unless otherwise noted.\\n\\nStep 2: Multimodal Fusion Encoder\\n\\nBuilding on the model from Step 1, we next analyze the role of multimodal fusion modules. The multimodal fusion encoder aims to fuse multimodal cues from video and language for a more discriminative VidL feature representation. As shown in Fig. 3, we experiment with several variants of multi-modal fusion encoders:\\n\\n- **Video-to-Text Multimodal Fusion (V2T-MF).** As illustrated in Fig. 3a, V2T-MF injects relevant video cues into the textual features using Cross-Attention. For a fair comparison with previous baselines [2,25], we do not add any extra layers but instead re-purpose the last m layer of our text encoder for V2T fusion. Specifically, a cross-attention operation is inserted into each of the m last layers in the text encoder between Self-Attention and MLP. This scheme was also previously used by [25, 57].\\n\\n- **Text-to-Video Multimodal Fusion (T2V-MF).** Similar to V2T-MF, we build T2V-MF (Fig. 3b) by re-purposing the last m layers of the vision encoder and using cross-attention to incorporate text cues into the video features.\\n\\n- **Bidirectional Multimodal Fusion (B-MF).** Prior approaches [16,32,55,75] feed the concatenated visual and textual features to a m-layer Transformer. However, this is often computationally infeasible in the video domain due to many input frames. Instead, we implement B-MF by combining T2V-MF and V2T-MF.\\n\\nTo train each variant, we add the video-text matching (VTM) loss (see Sec. 3) as in [16,55,57]. In the table below and Figure 2, we report that the V2T-MF scheme performs the best (i.e., +3.6% improvement). Surprisingly, the reverse T2V-MF scheme substantially decreases performance (-1.3%). We conjecture that predicting the matching video-text pairs using a pretrained language rather than a visual representation is easier. We also note that the B-MF scheme yields no improvement compared to V2T-MF.\\n\\n| w/o. MF | T2V-MF | V2T-MF | B-MF |\\n|--------|--------|--------|------|\\n| acc.(%)| 56.7   | 55.4   | 60.3 | 60.3 |\\n\\nTakeaway #2: For our remaining experiments, we use V2T-MF as our multimodal fusion encoder.\\n\\nStep 3: Pretraining Objectives\\n\\nBuilding on Step 2, we next study the following pretraining objectives:\\n\\n- **Visual-Text Contrastive Learning (VTC).** VTC aims to learn independent representations for video and text by maximizing the agreement between positive (visual, text) pairs while minimizing the agreement between negative pairs. Note that this objective is already used in previous steps, and thus, not included in Figure 2.\"}"}
{"id": "CVPR-2023-2276", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Visual-Text Matching (VTM). VTM objective is implemented as a standard cross-entropy loss that encourages a VidL model to produce binary predictions indicating whether a given video-text pair matches. Following [30], we attach this loss to our multimodal fusion encoder and use hard negative mining during training as in [25]. The VTM objective is already used in Step 2 (i.e., the multimodal fusion step) and thus, not included in Figure 2.\\n\\nMasked Language Modeling (MLM). MLM objective aims to predict the masked words by leveraging information from both visual and textual features. We mask 50% text tokens using the same masking strategy as in BERT [80] and attach a linear layer to our multimodal fusion encoder (T2V-MF) to predict the masked words.\\n\\nMasked Video Modeling (MVM). The MVM objective aims to recover the masked video tokens [15, 19, 37, 52]. To implement MVM, we apply a linear layer on the vision encoder and predict the masked tokens as in [46].\\n\\nIn the Table below and Fig. 2, we report that the MLM pretraining objective leads to a substantial boost in performance (+6.2%). Furthermore, adding MVM loss further improves the accuracy by 1%. However, adding the MVM objective slows the training by about 40% (due to additional forward and backward passes). Thus, to speed up the training, we don\u2019t use MVM loss in our remaining experiments.\\n\\n|                  | acc. (%) |\\n|------------------|---------|\\n| VTC (Step 1)     | 56.7    |\\n| VTC+VTM (Step 2) | 60.3    |\\n| VTC+VTM+MLM      | 66.5    |\\n| VTC+VTM+MLM+MVM  | 67.5    |\\n\\nTakeaway #3: For the remaining experiments, we use VTC, VTM, MLM as our pretraining objectives.\\n\\nStep 4: Pretraining Data\\n\\nIn this section, we analyze the effect of (i) the pretraining data, and (ii) pretraining protocols.\\n\\nDatasets. Recent methods [2, 16] suggest that jointly pretraining on images and videos leads to better performance. To investigate this, we consider an additional image-based CC3M [48] consisting of 3M image-text pairs. Specifically, we experiment with pretraining our model on the (i) image-only (CC3M), (ii) video-only (WebVid2M), and (iii) joint image and video (CC3M + WebVid2M) datasets. When pretraining on images, we replace our previously introduced temporal attention module with an identity connection. As shown in the Table below and Fig. 2, training on videos is more beneficial than training on images (+2.7%). Furthermore, jointly pretraining on images and videos leads to an additional 2.7% boost, which suggests that a stronger spatial representation is useful for VidL modeling.\\n\\n|                  | acc. (%) |\\n|------------------|---------|\\n| Images Videos    |         |\\n| Images-only      | 64.8    |\\n| Videos-only      | 67.5    |\\n| Images+Videos    | 70.2    |\\n\\nThe Number of Input Frames for Pretraining. Prior approaches [2, 16, 57, 75] use a different number of input frames for pretraining (i.e., from 1 to 16). Thus, we next study how many frames are needed for effective VidL pretraining. From the Table below and Fig. 2, we observe that multi-frame pretraining using 4 frames leads to 1.7% improvement compared to a single-frame pretraining. However, we also observe that the performance saturates with 4-frame inputs while the computational cost of pretraining with more frames increases significantly, i.e., pre-training with 4 frames is 2.5 \u00d7 faster than pretraining with 16 frames.\\n\\n|                  | acc. (%) |\\n|------------------|---------|\\n| 1 frame          | 68.5    |\\n| 4 frames         | 70.2    |\\n| 8 frames         | 70.2    |\\n| 16 frames        | 70.2    |\\n\\nMulti-stage Curriculum Pretraining. Lastly, we validate the necessity of multi-stage curriculum pretraining, which was used in several prior VidL approaches [2, 57]. Specifically, we experiment with two different pretraining protocols: (i) a two-stage pretraining that first trains a model for 10 epochs using single frames, and then for 5 additional epochs using 4-frame inputs, and (ii) a three-stage pretraining that builds on (i) by adding a third stage where the model is trained for additional 3 epochs using 8-frame inputs. Our results in the Table below and Figure 2, indicate that multi-stage pretraining does not lead to any significant performance boost, contrary to the findings of prior approaches [2, 57]. We believe that this happens because prior approaches [2, 57] train their model for only several epochs at each stage, whereas we train it until convergence. We also note that compared to the 4-frame one-stage pretraining, the two-stage 1 \u2192 4 \u2192 4 has a comparable pretraining cost as the latter model is trained for more epochs.\\n\\n|                  | acc. (%) |\\n|------------------|---------|\\n| 1 frame          | 69.6    |\\n| 4 frame          | 69.5    |\\n| 8 frame          | 70.4    |\\n\\nTakeaway #4: We adopt a single-stage pretraining on joint image and video datasets while using 4-frame inputs.\\n\\nStep 5: Finetuning & Inference\\n\\nExisting methods typically use the same number of frames either between pretraining and finetuning [2, 25, 75] or finetuning and inference [16, 57, 75]. Here, we study using a different number of frames at different phases.\\n\\nFinetuning. We experiment with finetuning our 4-frame pretrained model with $K = 1, 4, 8, 12, 24, 32$-frame inputs while using $M$ frames during inference. We use $M = 12$ for all $K \\\\leq 12$ and $M = K$ for $K > 12$. Based on the results in the Table below, we observe that while finetuning with more frames leads to higher accuracy (70.5%), the performance saturates with about 12 frames. We also note that finetuning with a single-frame input is 22.4 \u00d7 faster than with 32 frames but has a 5% lower accuracy. On the other hand, finetuning with 32-frame inputs is 1.7 \u00d7 faster than with 4-frame inputs but has a 5% lower accuracy.\\n\\n|                  | acc. (%) |\\n|------------------|---------|\\n| Finetuning       |         |\\n| 1 frame          | 70.5    |\\n| 4 frame          | 70.5    |\\n| 8 frame          | 70.5    |\\n| 12 frame         | 70.5    |\\n| 24 frame         | 70.5    |\\n| 32 frame         | 70.5    |\\n\\nSpeedup: 1.7 \u00d7 1.7 \u00d7 1.2 \u00d7 1 \u00d7 1 \u00d7 1 \u00d7 1.\"}"}
{"id": "CVPR-2023-2276", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"hand, finetuning with 12 frames yields only 0.3% lower accuracy but 2.6\u00d7 speedup compared to finetuning with 32 frames. Therefore, due to the favorable accuracy-cost trade-off, we finetune most of our models with 12-frame inputs.\\n\\n| # frames | acc.(%) | speedup |\\n|----------|---------|---------|\\n| 1        | 65.5    | 22.4\u00d7   |\\n| 4        | 68.1    | 7.1\u00d7    |\\n| 8        | 69.2    | 3.9\u00d7    |\\n| 12       | 70.2    | 2.6\u00d7    |\\n| 24       | 70.1    | 1.5\u00d7    |\\n| 32       | 70.5    | 1.0\u00d7    |\\n\\nInference. Next, we experiment with 12, 24, 32, 64 frames for testing our 4-frame pretrained and 12-frame finetuned model. We report the averaged accuracies on DiDeMo (D) / ActivityNet (A), which contain longer videos. Using more frames for inference helps, but the accuracy saturates quickly, and the inference cost becomes large.\\n\\n| # frames | D/A acc.(%) | speedup |\\n|----------|-------------|---------|\\n| 12       | 73.4/70.4   | 10.6\u00d7   |\\n| 24       | 73.0/72.1   | 3.1\u00d7    |\\n| 32       | 72.7/72.6   | 2.1\u00d7    |\\n| 64       | 73.8/72.8   | 1\u00d7       |\\n\\nTakeaway #5: Considering the trade-off between computational cost and accuracy, we use 12 frames for finetuning and inference on all datasets except ActivityNet. On ActivityNet, we use 12 and 32 frames for finetuning and inference.\\n\\nStep 6: Scaling Up\\nLastly, we scale up the pretraining data and the model.\\n\\nPretraining Data. For the pre-training data, we experiment with (a) adding 12M images from CC12M for a 17M Corpus, and (b) additional 10M videos from WebVid10M for a 25M Corpus. The results in the Table below and in Fig. 2 indicate that scaling our corpus from 5M \u2192 17M improves the performance by 2.2%. Furthermore, scaling the corpus from 17M \u2192 25M leads to an additional boost of 1.2%.\\n\\n| # corpus | acc.(%) |\\n|----------|---------|\\n| 5M       | 70.2    |\\n| 17M      | 72.4    |\\n| 25M      | 73.6    |\\n\\nModel Size. We also experiment with scaling the video encoder (ViT\\_base \u2192 ViT\\_large) or text encoder (BERT\\_base \u2192 BERT\\_large). Due to the large computational cost, we only conduct these experiments on the 5M corpus. We report that scaling the vision encoder brings larger improvement (+3.0%) than scaling the text encoder (+1.0%).\\n\\n| encoders | acc.(%) |\\n|----------|---------|\\n| base     | 70.2    |\\n| ViT      | 73.2    |\\n| BERT     | 71.2    |\\n\\nFinal Takeaway: Our final scaled-up VINDLU model improves the initial image transformer baseline by 23.2%.\\n\\n4. Experimental Results\\nWe validate our VINDLU recipe on two mainstream VidL tasks. See implementation details and dataset descriptions in the supplementary material.\\n\\nText-to-Video Retrieval. We compare our results with existing methods on three spatially-biased datasets MSR-VTT, DiDeMo, and ActivityNet and two temporally-heavy datasets, SSv2-label, and SSv2-template as shown in Tab. 2 and Tab. 3 respectively. Our method outperforms previous methods by a large margin on multiple datasets, achieving averaged accuracies of 79.3% (+5.6%), 75.4% (+4.7%), 84.6% (+4.6%) on DiDeMo, ActivityNet-Captions and SSv2 respectively. Our results on MSR-VTT are worse (66.5% vs. 68.6%) than OmniVL [57] but our pretraining framework is significantly cheaper (i.e., 82 vs. 169 V100 GPU days). We also note that our method is significantly cheaper than other top-performing approaches including LA VENDER [32], All-in-one [55], and CLIP-ViP [66] (82 vs. 640, 448, 984 V100 GPU days for pretraining respectively). Additionally, our cheapest VINDLU variant requires only 15 V100 GPU days for pre-training, which is the second cheapest model among all listed approaches, and it still achieves competitive results on all three benchmarks. Furthermore, compared to the other leading VidL approaches such as OmniVL and Singularity, which rely on a multi-stage curriculum pretraining, our framework is simpler since it can be trained in a single stage. We also include the results of our scaled up variant VINDLU-L that uses ViT\\_large as its video encoder, and report that it achieves 74.5% averaged retrieval accuracy, thus, outperforming all other approaches. Lastly, our results on the SSv2 dataset in Table 3 indicate that VINDLU performs well not only on spatially-biased datasets but also on temporally-heavy datasets, which require sophisticated temporal modeling capabilities. For fairer comparisons, we de-emphasize CLIP-based methods since they use a lot more pre-training data.\\n\\nVideo Question-Answering. In Table 4, we also present our results for the video question-answering task on ActivityNet-QA [73], MSRVTT-QA [63], MSRVTT-MC [72] and TVQA [27]. Our results indicate that compared to prior state-of-the-art approaches, VINDLU achieves competitive results across all four of these datasets. In particular, our method outperforms existing approaches by 0.6% on ActivityNet-QA, 0.3% on MSRVTT-QA, 3.4% on MSRVTT-MC and 0.3% on TVQA. For fair comparison, we de-emphasize FrozenBiLM [68], since it is a lot larger than our model (1.2B vs. 201M parameters) and uses a lot more pretraining data (400M vs. 25M).\\n\\n5. Conclusion\\nIn this work, we demystify the importance of various components used in modern VidL framework design. Throughout our empirical study, we find that temporal modeling, multimodal fusion, masked modeling pretraining objectives, and joint training on images and videos are critical for good performance on the downstream VidL under-\"}"}
{"id": "CVPR-2023-2276", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Method          | #Data | #Frames | Time | R1  | R5  | R10 | Avg  | R1  | R5  | R10 | Avg  | R1  | R5  | R10 | Avg  |\\n|-----------------|-------|---------|------|-----|-----|-----|------|-----|-----|-----|------|-----|-----|-----|------|\\n| ClipBERT        | 5.4M  | 1       | 32   | 22.0| 46.8| 59.9| 42.9 | 20.4| 48.0| 60.8| 43.1 | 21.3| 49.0| 63.5| 44.6 |\\n| VideoCLIP       | 136M  | 960     | 8    | 30.9| 55.4| 66.8| 51.0 | -   | -   | -   | -    | -   | -   | -   | -    |\\n| Frozen          | 5M    | 1-4     | 35   | 31.0| 59.5| 70.5| 53.7 | 34.6| 65.0| 74.7| 58.1 | -   | -   | -   | -    |\\n| ALPRO           | 5M    | 8       | 24   | 33.9| 60.7| 73.2| 55.9 | 35.9| 67.5| 78.8| 60.7 | -   | -   | -   | -    |\\n| VIOLET          | 138M  | 4       | 83   | 34.5| 63.0| 73.4| 57.0 | 32.6| 62.8| 74.7| 56.7 | -   | -   | -   | -    |\\n| All-in-one      | 138M  | 3       | 448  | 37.9| 68.1| 77.1| 61.0 | 32.7| 61.4| 73.5| 55.9 | 22.4| 53.7| 67.7| 47.9 |\\n| LA VENDER       | 30M   | 4       | 640  | 40.7| 66.9| 77.6| 61.7 | 53.4| 78.6| 85.3| 72.4 | -   | -   | -   | -    |\\n| Singularity     | 17M   | 1-4     | 29   | 42.7| 69.5| 78.1| 63.4 | 53.1| 79.9| 88.1| 73.7 | 48.9| 77.0| 86.3| 70.7 |\\n| OmniVL          | 17M   | 1-8     | 169  | 47.8| 74.2| 83.8| 68.6 | -   | -   | -   | -    | 52.4| 79.5| 85.4| 72.4 |\\n| CLIP4Clip       | 400M  | 1-768   | -    | -   | -   | -   | -    | 44.5| 71.4| 81.6| 65.8 | 42.8| 68.5| 79.2| 63.5 |\\n| ECLIPSE         | 400M  | 1-768   | -    | -   | -   | -   | 44.2 | -   | -   | -   | 75.7| 86.2| -   | -   | -    |\\n| CLIP-Hhiker     | 400M  | 1-768   | -    | -   | -   | -   | 47.7| 74.1| 82.9| 68.6 | -   | -   | 44.0| 74.9| 86.1| 68.3 |\\n| CLIP-ViP        | 500M  | 1-12    | 984  | 54.2| 77.2| 84.8| 72.1 | 50.5| 78.4| 87.1| 72.0 | 53.4| 81.4| 90.0| 74.9 |\\n| VINDLU          | 5M    | 4       | 15   | 43.8| 70.3| 79.5| 64.5 | 54.6| 81.3| 89.0| 75.0 | 51.1| 79.2| 88.4| 72.9 |\\n| VINDLU-L        | 25M   | 4       | 178  | 48.8| 72.4| 82.2| 67.8 | 59.8| 86.6| 91.5| 79.3 | 55.9| 82.3| 90.9| 76.4 |\\n\\nTable 2. Comparison to the state-of-the-art text-to-video retrieval methods on MSRVTT, DiDeMo and ActivityNet-Captions. Pretraining time is measured in V100 GPU days, where * means our estimated time based on FLOPs, pretraining data, and the number of epochs for the methods that do not report their pretraining time. VINDLU uses ViT-B/16 while VINDLU-L uses ViT-L/16 as video encoders. For fair comparisons, we de-emphasize the CLIP-based methods since they use a lot more pretraining data than all other approaches. Our results indicate that VINDLU achieves competitive or even better than state-of-the-art results while also being simple and efficient.\\n\\n| Method            | #PT  | SSv2-label | SSv2-template | Avg  |\\n|-------------------|------|------------|---------------|------|\\n| CLIP4Clip         | 400M | 43.1       | 71.4          | 77.0 |\\n| Singularity       | 17M  | 47.4       | 75.9          | 77.6 |\\n| VINDLU            | 5M   | 51.2       | 78.8          | 82.2 |\\n| VINDLU-L          | 17M  | 53.0       | 80.8          | 86.2 |\\n| VINDLU            | 25M  | 53.1       | 81.8          | 83.3 |\\n\\nTable 3. Comparison with state-of-the-art text-to-video retrieval methods on the temporally-heavy SSv2-Label and SSv2-Template datasets. #PT denotes the amount of pretraining data. Averaged numbers are the average of Recal@{1,5,10} on these two datasets. CLIP-based models are de-emphasized for fairer comparisons. We observe that VINDLU achieves the best performance, which demonstrates its ability to reason about complex temporal dependencies in the video data.\\n\\nTable 4. Comparison with state-of-the-art video answering methods on ActivityNet-QA (ANet), MSRVTT-QA (MSR-QA), MSRVTT-MC (MSR-MC) and TVQA. #PT denotes the amount of pretraining data. We gray out FrozenBiLM as it is much larger than our model (1.2B vs 207M parameters). VINDLU achieves competitive results across all four datasets.\\n\\nAcknowledgements. We thank Y. Lin, M. Islam, A. Madasu and M. Gramopadhye for helpful discussions. This work was supported by the Sony Faculty Innovation award, Lilly Endowment Inc. via Indiana University Pervasive Technology Institute, Laboratory for Analytic Sciences via NC State University, and NSF-AI Institute DRL211263.\"}"}
