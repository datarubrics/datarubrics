{"id": "CVPR-2023-322", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] A Bansal, S. S Rambhatla, A Shrivastava, and R Chellappa. Detecting human-object interactions via functional generalization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 10460\u201310469, 2020.\\n\\n[2] N Carion, F Massa, G Synnaeve, N Usunier, A Kirillov, and S Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, pages 213\u2013229. Springer, 2020.\\n\\n[3] Y.-W Chao, Y Liu, X Liu, H Zeng, and J Deng. Learning to detect human-object interactions. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 381\u2013389. IEEE, 2018.\\n\\n[4] M Chen, Y Liao, S Liu, Z Chen, F Wang, and C Qian. Reformulating hoi detection as adaptive set prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9004\u20139013, 2021.\\n\\n[5] B Cheng, I Misra, A. G Schwing, A Kirillov, and R Girshar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1290\u20131299, 2022.\\n\\n[6] L Dong, Z Li, K Xu, Z Zhang, L Yan, S Zhong, and X Zou. Category-aware transformer network for better human-object interaction detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19538\u201319547, 2022.\\n\\n[7] C Gao, J Xu, Y Zou, and J.-B Huang. DRG: Dual relation graph for human-object interaction detection. In European Conference on Computer Vision, pages 696\u2013712. Springer, 2020.\\n\\n[8] C Gao, Y Zou, and J.-B Huang. ICAN: Instance-centric attention network for human-object interaction detection. arXiv preprint arXiv:1808.10437, 2018.\\n\\n[9] G Gkioxari, R Girshick, P Doll\u00e1r, and K He. Detecting and recognizing human-object interactions. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8359\u20138367, 2018.\\n\\n[10] S Gupta and J Malik. Visual semantic role labeling. arXiv preprint arXiv:1505.04474, 2015.\\n\\n[11] T Gupta, A Schwing, and D Hoiem. No-frills human-object interaction detection: Factorization, layout encodings, and training techniques. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9677\u20139685, 2019.\\n\\n[12] H He, Y Yuan, X Yue, and H Hu. Rankseg: Adaptive pixel classification with image category ranking for segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 682\u2013700. Springer Nature Switzerland Cham, 2022.\\n\\n[13] A Iftekhar, H Chen, K Kundu, X Li, J Tighe, and D Modolo. What to look at and where: Semantic and spatial refined transformer for detecting human-object interactions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5353\u20135363, 2022.\\n\\n[14] B Kim, T Choi, J Kang, and H. J Kim. Uniondet: Union-level detector towards real-time human-object interaction detection. In European Conference on Computer Vision, pages 498\u2013514. Springer, 2020.\\n\\n[15] B Kim, J Lee, J Kang, E.-S Kim, and H. J Kim. HOTR: End-to-end human-object interaction detection with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 74\u201383, 2021.\\n\\n[16] B Kim, J Mun, K.-W On, M Shin, J Lee, and E.-S Kim. MSTR: Multi-scale transformer for end-to-end human-object interaction detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19578\u201319587, 2022.\\n\\n[17] D.-J Kim, X Sun, J Choi, S Lin, and I. S Kweon. Detecting human-object interactions with action co-occurrence priors. In European Conference on Computer Vision, pages 718\u2013736. Springer, 2020.\\n\\n[18] Y.-L Li, X Liu, X Wu, Y Li, and C Lu. HOI analysis: Integrating and decomposing human-object interaction. Advances in Neural Information Processing Systems, 33:5011\u20135022, 2020.\\n\\n[19] Y.-L Li, L Xu, X Liu, X Huang, Y Xu, S Wang, H.-S Fang, Z Ma, M Chen, and C Lu. Pastanet: Toward human activity knowledge engine. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 382\u2013391, 2020.\\n\\n[20] Y.-L Li, S Zhou, X Huang, L Xu, Z Ma, H.-S Fang, Y Wang, and C Lu. Transferable interactiveness knowledge for human-object interaction detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3585\u20133594, 2019.\\n\\n[21] Y Liao, S Liu, F Wang, Y Chen, C Qian, and J Feng. PPDM: Parallel point detection and matching for real-time human-object interaction detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 482\u2013490, 2020.\\n\\n[22] Y Liao, A Zhang, M Lu, Y Wang, X Li, and S Liu. GEN-VLKT: Simplify association and enhance interaction understanding for hoi detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20123\u201320132, 2022.\\n\\n[23] T.-Y Lin, P Goyal, R Girshick, K He, and P Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 2980\u20132988, 2017.\\n\\n[24] T.-Y Lin, M Maire, S Belongie, J Hays, P Perona, D Ramanan, P Doll\u00e1r, and C. L Zitnick. Microsoft COCO: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.\\n\\n[25] S Liu, L Zhang, X Yang, H Su, and J Zhu. Query2label: A simple transformer way to multi-label classification. arXiv preprint arXiv:2107.10834, 2021.\\n\\n[26] X Liu, Y.-L Li, X Wu, Y.-W Tai, C Lu, and C.-K Tang. Interactiveness field in human-object interactions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20113\u201320122, 2022.\\n\\n[27] Y Liu, Q Chen, and A Zisserman. Amplifying key cues for human-object-interaction detection. In European Conference on Computer Vision, pages 248\u2013265. Springer, 2020.\"}"}
{"id": "CVPR-2023-322", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"J. Park, S. Lee, H. Heo, H. K. Choi, and H. J. Kim. Consistency learning via decoding path augmentation for transformers in human object interaction detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1019\u20131028, 2022.\\n\\nS. Qi, W. Wang, B. Jia, J. Shen, and S.-C. Zhu. Learning human-object interactions by graph parsing neural networks. In Proceedings of the European Conference on Computer Vision (ECCV), pages 401\u2013417, 2018.\\n\\nX. Qu, C. Ding, X. Li, X. Zhong, and D. Tao. Distillation using oracle queries for transformer-based human-object interaction detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19558\u201319567, 2022.\\n\\nA. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language super-vision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\\n\\nT. Ridnik, E. Ben-Baruch, N. Zamir, A. Noy, I. Friedman, M. Protter, and L. Zelnik-Manor. Asymmetric loss for multi-label classification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 82\u201391, 2021.\\n\\nM. Tamura, H. Ohashi, and T. Yoshinaga. Qpic: Query-based pairwise human-object interaction detection with image-wide contextual information. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10410\u201310419, 2021.\\n\\nO. Ulutan, A. Iftekhar, and B. S. Manjunath. Vsgnet: Spatial attention network for detecting human object interactions using graph convolutions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13617\u201313626, 2020.\\n\\nA. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, \u0141. Kaiser, and I. Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017.\\n\\nB. Wan, D. Zhou, Y. Liu, R. Li, and X. He. Pose-aware multi-level feature network for human object interaction detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9469\u20139478, 2019.\\n\\nH. Wang, W.-s. Zheng, and L. Yingbiao. Contextual heterogeneous graph network for human-object interaction detection. In European Conference on Computer Vision, pages 248\u2013264. Springer, 2020.\\n\\nT. Wang, T. Yang, M. Danelljan, F. S. Khan, X. Zhang, and J. Sun. Learning human-object interaction detection using interaction points. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4116\u20134125, 2020.\\n\\nX. Wu, Y. -L. Li, X. Liu, J. Zhang, Y. Wu, and C. Lu. Mining cross-person cues for body-part interactiveness learning in hoi detection. In Proceedings of the European Conference on Computer Vision (ECCV), pages 121\u2013136. Springer, 2022.\\n\\nH. Yuan, J. Jiang, S. Albanie, T. Feng, Z. Huang, D. Ni, and M. Tang. Rlip: Relational language-image pre-training for human-object interaction detection. In Advances in Neural Information Processing Systems, 2022.\\n\\nA. Zhang, Y. Liao, S. Liu, M. Lu, Y. Wang, C. Gao, and X. Li. Mining the benefits of two-stage and one-stage hoi detection. In Thirty-Fifth Conference on Neural Information Processing Systems, 2021.\\n\\nF. Z. Zhang, D. Campbell, and S. Gould. Spatially conditioned graphs for detecting human-object interactions. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13319\u201313327, 2021.\\n\\nF. Z. Zhang, D. Campbell, and S. Gould. Efficient two-stage detection of human-object interactions with a novel unary-pairwise transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20104\u201320112, 2022.\\n\\nY. Zhang, Y. Pan, T. Yao, R. Huang, T. Mei, and C.-W. Chen. Exploring structure-aware transformer over interaction proposals for human-object interaction detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19548\u201319557, 2022.\\n\\nX. Zhong, C. Ding, Z. Li, and S. Huang. Towards hard-positive query mining for detr-based human-object interaction detection. In Proceedings of the European Conference on Computer Vision (ECCV), pages 444\u2013460. Springer, 2022.\\n\\nX. Zhong, C. Ding, X. Qu, and D. Tao. Polysemy deciphering network for robust human\u2013object interaction detection. International Journal of Computer Vision, 129(6):1910\u20131929, 2021.\\n\\nX. Zhong, X. Qu, C. Ding, and D. Tao. Glance and gaze: Inferring action-aware points for one-stage human-object interaction detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13234\u201313243, 2021.\\n\\nD. Zhou, Z. Liu, J. Wang, L. Wang, T. Hu, E. Ding, and J. Wang. Human-object interaction detection via disentangled transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19568\u201319577, 2022.\\n\\nC. Zou, B. Wang, Y. Hu, J. Liu, Q. Wu, Y. Zhao, B. Li, C. Zhang, C. Zhang, Y. Wei, et al. End-to-end human object interaction detection with hoi transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11825\u201311834, 2021.\"}"}
{"id": "CVPR-2023-322", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. The performance numbers of three different baseline HOI methods with and without integration of our method, on two datasets.\\n\\n| Method       | E2E | Full | Rare | Non-Rare | #Params | FPS |\\n|--------------|-----|------|------|----------|---------|-----|\\n| QPIC [33]    | \u2713   | 28.93| 31.12| 61.39    | 41M     | 19.5|\\n| Ours         | \u2713   | 31.08| 33.22| 65.49    | 46M (+5M) | 18.3(-6.2%) |\\n| SCG [42]     | \u2717   | 31.28| 33.40| 66.74    | 57M     | 4.5 |\\n| Ours         | \u2717   | 32.74| 34.68| 65.61    | 64M (+7M) | 4.1(-8.9%) |\\n| GEN-VLKT [22]| \u2717   | 33.69| 34.81| 66.40    | 42M     | 21.7|\\n| Ours         | \u2717   | 35.36| 36.07| 69.17    | 47M (+5M) | 20.6(-5.1%) |\\n\\n\"E2E\" denotes whether a HOI detector is end-to-end. All models are tested on Tesla V100.\\n\\nQPIC uses transformer. It uses a multi-stream graph neural network (GNN) for interaction classification. In our experiment, the detection boxes are from a fine-tuned detector provided by DRG [7] for HICO-DET and a fine-tuned DETR for V-COCO.\\n\\nDuring our integration, the CNN feature map in the backbone of SCG is used as image feature \\\\( I \\\\). The human-object feature \\\\( \\\\{ F_i \\\\} \\\\) is generated through RoI pooling with detected human and object boxes and fused with the GNN.\\n\\nGEN-VLKT is also transformer-based, but not end-to-end as pairwise NMS [41] is used for post-processing. It is the current state-of-the-art method. It uses two parallel decoders for object detection and interaction classification, namely instance decoder and interaction decoder.\\n\\nDuring our integration, the feature map in its transformer encoder is used as image feature \\\\( I \\\\). The query feature in the interaction decoder is used as the human-object feature \\\\( \\\\{ F_i \\\\} \\\\). Note that, unlike the majority of HOI detection methods, the original GEN-VLKT uses HOI categories rather than interaction categories during interaction classification. Our experiments still use interaction categories, in order to be consistent with most other methods.\\n\\n5. Experiments\\n\\nIn this section, we verify the applicability and effectiveness of the proposed method through experiments. In Sec. 5.1, we introduce the experimental settings. Then we demonstrate the effectiveness of the proposed method over 3 baselines in Sec. 5.2, and show it achieves SOTA results on major benchmarks in Sec. 5.3. Next, in Sec. 5.4 we conduct comprehensive ablation studies on the key components as well as detailed technical designs. Lastly, we provide some analysis and visualization in Sec. 5.5.\\n\\n5.1. Datasets\\n\\nHICO-DET [3] and V-COCO [10] are two widely-used HOI benchmarks. HICO-DET contains 47,776 images, with 38,118 for training and 9,658 for testing. There are 600 HOI categories in HICO-DET, consisting of 117 interaction classes and 80 object classes. Each HOI category is composed of an interaction and an object. V-COCO is a subset of MS-COCO [24] with HOI annotations, including 10,346 images (2,533 for training, 2,867 for validation and 4,946 for testing). It has 80 object categories same with HICO-DET and 29 interaction categories.\\n\\nEvaluation metrics.\\n\\nFor HICO-DET, we adopt the commonly used mAP metric [3]. Each prediction is a \\\\( \\\\langle \\\\text{human}, \\\\text{interaction}, \\\\text{object} \\\\rangle \\\\) triplet. A prediction is a true positive only when the human and object bounding boxes both have IoU > 0.5 w.r.t. ground truth and the interaction classification result is correct. We evaluate the performance in two different settings following [3]. In the known object setting, for each HOI category, we evaluate the prediction only on the images containing the target object category. In default setting, the detection result of each category is evaluated on the full test set. In each setting, we report the mAP over (1) all 600 HOI categories (Full), (2) 138 categories with less than 10 training samples (Rare), and (3) the remaining 462 categories (Non-rare). For V-COCO, we use the role mAP following [10], under both scenario #1 (including objects) and #2 (ignoring objects). The performance is evaluated using its official evaluation toolkit.\\n\\n5.2. Improvement on Three Different Baselines\\n\\nTab. 1 summarizes the performance of the three baseline HOI methods before and after integration of our method. Backbone is ResNet50. All these methods are significantly improved. Specifically, QPIC [33] is improved by 2.15 mAP, making it competitive with those more recent works [16, 41, 43]. SCG [42] is improved by 1.46 mAP, demonstrating that our method is not limited to transformer-based baselines. The current SOTA method GEN-VLKT [22] is improved by 1.67 mAP, producing the new SOTA result (also refer to Tab. 2).\\n\\nOn the V-COCO dataset [10], the performance improvement is similar, which is 2.28, 2.21 and 1.51 mAP on QPIC, SCG and GEN-VLKT, respectively.\\n\\nNotably, for SCG [42], as the object detector is fixed during the training of its interaction classification network, the improvement by our method is purely due to better interaction classification, not a better fine-tuned CNN backbone or a better object detector. This further consolidates that the category query learning is effective.\"}"}
{"id": "CVPR-2023-322", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. The proposed method achieves state-of-the-art on HICO-DET [3]. The best results are marked in bold.\\n\\n| Method   | Backbone | Scenario #1 | Scenario #2 |\\n|----------|----------|-------------|-------------|\\n| DRG [7]  | R50      | -           | 51.0        |\\n| SCG [42] | R50      | 54.2        | 60.9        |\\n| GG-Net [47] | HG104   | 54.7        | -           |\\n| QPIC [33] | R50      | 58.8        | 61.0        |\\n| HQM [45]  | R50      | -           | 63.6        |\\n| CDN [41]  | R50      | 61.7        | 63.8        |\\n| GEN-VLKT-B [22] | R50  | 62.4        | 64.5        |\\n| GEN-VLKT-M [22] | R101  | 63.3        | 65.6        |\\n| GEN-VLKT-L [22] | R101  | -           | 63.6        |\\n| MSTR [16] | R50      | 62.0        | 65.2        |\\n| BodyPartMap [39] | R50 | 63.0        | 65.1        |\\n| IF [26]   | R50      | -           | 63.0        |\\n| DT [48]   | R50      | -           | 66.2        |\\n| STIP [44] | R50      | 65.1        | 69.7        |\\n| GEN-VLKT-B + Ours | R50   | 66.4        | 69.2        |\\n| GEN-VLKT-M + Ours | R101 | 66.8        | -           |\\n| GEN-VLKT-L + Ours | R101 | 66.5        | -           |\\n\\nTable 3. Comparison with state-of-the-art methods on V-COCO [10] dataset. The best results are marked in bold.\\n\\nTo verify that the performance improvement is not due to a larger model, we also compare the model size and running speed. Our method increases the model parameters by a few millions, which is small compared to the original model size. The running speed, measured by FPS, is only decreased by a few percent. The marginal additional cost shows that our method is quite lightweight.\\n\\n5.3. Comparison with State-of-the-art\\n\\nTab. 2 and Tab. 3 compare our method with many previous methods for HICO-DET and V-COCO datasets, respectively. GEN-VLKT [22] is used as our baseline.\\n\\nOn HICO-DET, our result with ResNet50 backbone already outperforms all previous methods under both default and known object settings. With the stronger ResNet101 backbone, our method achieves the new state-of-the-art 36.03 full mAP under default settings and 38.82 under known object settings.\\n\\nOn V-COCO dataset, our method achieves the new state-of-the-art performance on Scenario 1, with an AP of 66.4 for ResNet50, surpassing [48]. For scenario 2, it is comparable with the state-of-the-art [44].\\n\\n5.4. Ablation Experiments\\n\\nWe perform various ablation experiments to validate the effectiveness of different components in our method. HICO-DET dataset and GEN-VLKT [22] baseline are used. The proposed method can be divided into 3 components: C1 means applying Query2Label [25] to the baseline detector as a multi-task learning (with feature extractor shared). In detail, it adds the queries, the decoder and image classification loss (Eq. (1), Eq. (2) and Eq. (5)).\\n\\nC2 means using the learned query in C1 as adaptive interaction classification.\"}"}
{"id": "CVPR-2023-322", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. Ablation on the type and weight $\\\\lambda$ of the image classification loss. The best results are marked in bold.\\n\\n| Layer structure          | Full Rare | Non-Rare |\\n|-------------------------|-----------|----------|\\n| $S \\\\rightarrow C \\\\rightarrow F$ | 34.73     | 32.09    |\\n| $C \\\\rightarrow S \\\\rightarrow F$ | 34.98     | 31.73    |\\n| $C \\\\rightarrow F$       | 34.63     | 31.34    |\\n\\nTable 6. Ablation on structure of each layer in the category decoder. Here \\\"S\\\", \\\"C\\\" and \\\"F\\\" stands for the self-attention, cross-attention and FFN in a standard transformer.\\n\\n| L   | Full Rare | Non-Rare |\\n|-----|-----------|----------|\\n| 1   | 34.66     | 31.47    |\\n| 2   | 34.98     | 31.73    |\\n| 3   | 34.86     | 31.56    |\\n\\nTable 7. Ablation on the number of layers in the category decoder.\\n\\nTo validate our approach in Fig. 2, several variants with these components in our approach are experimented and summarized in Tab. 4. First, (b) is a simple combination of Query2label [25] and the baseline detector (a) in a multi-task setting. (b) is only slightly better than (a), showing that simply applying Query2Label to HOI is barely helpful. Second, variant (c) significantly boosts (b), indicating using the queries as adaptive classification weights is the key to the performance improvement. This shows the effectiveness of our most crucial technical design in C2: applying the learned category queries as adaptive interaction classification weights. Finally, the complete approach is (d). It further adds the integration step of image classification and instance classification score on (c). This technique produces moderate improvement over (c), i.e., 1.67 mAP vs. 1.29 mAP.\\n\\nImage classification loss. Tab. 5 compares different loss functions and weights. First, when $\\\\lambda = 0$, which means no image-level supervision is applied, the improvement over the baseline (33.69 mAP) drops to only 0.52 mAP. This demonstrates the image classification supervision is essential to make the category query learning effective, with either focal loss or ASL. Additionally, ASL is slightly better than focal loss. By default, $\\\\lambda = 1.0$ is adopted.\\n\\nIs Asymmetric Loss the Key? In the ablation above, we can see that ASL does help our image-level query learning. However, it is not the major reason for the performance improvement. To figure out this, we replace focal loss in the plain baseline GEN-VLKT with ASL and the result is only slightly better by 0.08 mAP.\\n\\nDecoder Structure. Tab. 6 compares several structures of the decoder. Compared to the standard decoder [2, 35] ($S \\\\rightarrow C \\\\rightarrow F$ in the table), putting cross-attention first ($C \\\\rightarrow S \\\\rightarrow F$) is slightly better (by 0.25 mAP) without extra computation. If we remove self-attention ($C \\\\rightarrow F$), the performance drops by 0.35 mAP compared with the $S \\\\rightarrow C \\\\rightarrow F$ setting. This is probably because self-attention helps to learn the dependencies between different category queries.\\n\\nTab. 7 compares different numbers of decoder layers, denoted as $L$. We find that $L = 2$ is sufficient. More layers do not help the performance.\"}"}
{"id": "CVPR-2023-322", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Performance evaluated on image partitions with different n = 1, 2, 3, 4, 5, > 5 instances for an interaction category. Top: mAP numbers for three baselines (dashed) as well as our integration (solid). Bottom: relative mAP ratios for improvement produced by our method (solid) and between two arbitrary previous methods (dashed).\\n\\nThe attention maps in the cross-attention layer of the decoder is visualized in Fig. 3. For different category queries, the corresponding attention maps show they learn to capture the semantics of the category, while being adaptive to different images. For example, in Fig. 3a, the broken part of the bat in the air is highlighted for \u201cbreak\u201d while the left part in the hand is highlighted for \u201chold\u201d. This is similar for Fig. 3b. In Fig. 3c, the attention map highlights many instances with corresponding action \u201chold\u201d and \u201cride\u201d.\\n\\nTo qualitatively demonstrate how our method helps, we visualize some cases of the baseline and the proposed method in Fig. 4. In the first case, the baseline predicts a TP of \u201cperson sit on bus\u201d with a low score 0.15, and a FP of \u201cperson board bus\u201d with a high score 0.29. Our model also predicts the same TP and FP, but lowers the FP score to 0.07 and lifts the TP score to 0.38. Besides, our model correctly predicts image-level scores: the wrong category \u201cboard\u201d is given a low score of 0.06 while the four correct categories are given high scores. In the second case, our model successfully predicts the TP \u201cperson dribble\u201d missed by the baseline and suppress the FP \u201cperson hold sports\u201d from 0.6 to 0.11 with the help of correct image-level classification result. This is similar for the third case.\\n\\nLast, as our category query learning is performed on image level, we conjecture that it is more helpful for images with dense human-object interactions. In such images, an human-object instance is relatively small and hard to learn good feature on its own. However, it may benefit more from the global image level category query feature, which aggregates more information from other similar instances in this image. To validate this conjecture, we partition the images according to their \u201cinteraction density\u201d and check whether our method produces larger improvement on images that are \u201cdenser\u201d. Specifically, for each interaction category, its mAP is evaluated separately on six different image partition subsets, where each image contains different numbers (n=1, 2, 3, 4, 5 and > 5) of human-object instances of this category. The mAP results of the three baseline methods and their integrated versions (as in Tab. 1) are shown in Fig. 5 (top, dashed vs. solid lines). It shows that: 1) the mAP is lower for larger n, indicating the \u201cdenser\u201d images are more challenging; 2) our method improves the baselines consistently on all different partitions.\\n\\nTo check whether the proposed method is more effective on \u201cdenser\u201d images, we use the relative mAP improvement, which is a ratio, \\\\( \\\\frac{\\\\text{mAP}_{\\\\text{ours}} - \\\\text{mAP}_{\\\\text{baseline}}}{\\\\text{mAP}_{\\\\text{baseline}}} \\\\), for analysis. The ratio curves of the three baselines are shown in Fig. 5 (bottom, solid lines). It is clear that the relative improvement becomes larger for larger n. This indicates that the image-level category query learning is more effective on these challenging dense images.\\n\\nTo further verify that this behavior is not commonly true, we also compute the relative ratio of three more comparisons, \u201cgen-vlkt vs. qpic\u201d, \u201cgen-vlkt vs. scg\u201d and \u201cscg vs. qpic\u201d, in which the former outperforms the latter. These curves are also shown in Fig. 5 (bottom, dashed lines). There is no clear pattern in these curves, indicating that the performance gap between two arbitrary HOI methods are in general not related to the image \u201cdensity\u201d.\\n\\n6. Conclusion\\n\\nThis work proposes a novel approach for the human-object interaction classification sub-task in HOI detection. We study the problem of interaction category modeling, in contrast to most previous methods focusing on human-object feature learning. We adopt the concept of category query in a previous method [25] for HOI, for the first time, and show that it is simple, general and highly effective.\\n\\nClearly, this idea of category query modeling is not limited to multi-label image classification and HOI detection. We hope it is useful for other vision tasks.\\n\\nAcknowledgment. This work was supported in part by the National Natural Science Foundation of China under Grant 62076183, 61936014 and 61976159, in part by the Natural Science Foundation of Shanghai under Grant 20ZR1473500, in part by the Shanghai Science and Technology Innovation Action Project of under Grant 20511100700 and 22511105300, in part by the Shanghai Municipal Science and Technology Major Project under Grant 2021SHZDZX0100, and in part by the Fundamental Research Funds for the Central Universities. The authors would also like to thank the anonymous reviewers for their careful work and valuable suggestions.\"}"}
{"id": "CVPR-2023-322", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nUnlike most previous HOI methods that focus on learning better human-object features, we propose a novel and complementary approach called category query learning. Such queries are explicitly associated to interaction categories, converted to image specific category representation via a transformer decoder, and learnt via an auxiliary image-level classification task. This idea is motivated by an earlier multi-label image classification method, but is for the first time applied for the challenging human-object interaction classification task. Our method is simple, general and effective. It is validated on three representative HOI baselines and achieves new state-of-the-art results on two benchmarks. Code will be available at https://github.com/charles-xie/CQL.\\n\\n1. Introduction\\n\\nHuman-Object Interaction (HOI) detection has attracted a lot of interests in recent years [3, 8\u201310, 21, 33]. The task consists of two sub-tasks. The first is human and object detection. It is usually performed by common object detection methods. The second is interaction classification of each human-object (HO) pair. This sub-task is very challenging due to the complex appearance variations in the interaction categories. See Fig. 1 for examples. It is the focus of most previous HOI methods, as well as this work.\\n\\nMost previous HOI methods focus on learning better human-object features, including modeling relation and context via GNN [7, 29, 34, 37] or attention mechanism [8, 34, 46], decoupling localization and classification [22, 41, 48], leveraging vision-language knowledge [6, 22] and introducing multi-scale feature to transformer [16]. However, for interaction classification they all adopt the simple linear classifier that performs the dot product of the human-object feature and a static weight vector, which represents an interaction category.\\n\\nIn this work, we propose a new approach that enhances the above paradigm and complements most previous HOI methods. It is motivated by the recent work Query2label [25], a transformer-based classification network. It proposes a new concept we call category-specific query. Unlike the queries in other transformer methods, each query is associated to a specific and fixed image category during training and inference. This one-to-one binding makes the query learn to model each category more effectively. The queries are converted to image specific category representations via a transformer decoder. This method achieves excellent performance on multi-label image classification task.\\n\\nWe extend this approach for human-object interaction classification. Essentially, our approach replaces traditional category representation as a static weight vector in previous HOI methods with category queries learnt as described above. The same linear classifier is adopted. Such category queries achieve excellent performance on the HOI benchmarks.\\n\\n1. Introduction\\n\\nHuman-Object Interaction (HOI) detection has attracted a lot of interests in recent years [3, 8\u201310, 21, 33]. The task consists of two sub-tasks. The first is human and object detection. It is usually performed by common object detection methods. The second is interaction classification of each human-object (HO) pair. This sub-task is very challenging due to the complex appearance variations in the interaction categories. See Fig. 1 for examples. It is the focus of most previous HOI methods, as well as this work.\\n\\nMost previous HOI methods focus on learning better human-object features, including modeling relation and context via GNN [7, 29, 34, 37] or attention mechanism [8, 34, 46], decoupling localization and classification [22, 41, 48], leveraging vision-language knowledge [6, 22] and introducing multi-scale feature to transformer [16]. However, for interaction classification they all adopt the simple linear classifier that performs the dot product of the human-object feature and a static weight vector, which represents an interaction category.\\n\\nIn this work, we propose a new approach that enhances the above paradigm and complements most previous HOI methods. It is motivated by the recent work Query2label [25], a transformer-based classification network. It proposes a new concept we call category-specific query. Unlike the queries in other transformer methods, each query is associated to a specific and fixed image category during training and inference. This one-to-one binding makes the query learn to model each category more effectively. The queries are converted to image specific category representations via a transformer decoder. This method achieves excellent performance on multi-label image classification task.\\n\\nWe extend this approach for human-object interaction classification. Essentially, our approach replaces traditional category representation as a static weight vector in previous HOI methods with category queries learnt as described above. The same linear classifier is adopted. Such category queries achieve excellent performance on the HOI benchmarks.\"}"}
{"id": "CVPR-2023-322", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"gory queries are more effective, and adaptive for different images, giving rise to better modeling of the complex variations in each interaction category. This is the crucial difference between this work and a simple adaption of [25] to HOI. Notably, this work is the first to address the category weight representation problem in the HOI community.\\n\\nNote that our proposed category specific query is different and not related to those queries in other transformer-based HOI methods [4, 15, 33, 49]. Specifically, category queries extract image-level features as the category representation. The queries in other methods are human-object instance-level features and category-agnostic.\\n\\nOur method is simple, lightweight and general. The overview is in Fig. 2. It is complementary to any off-the-shelf HOI method that provides human-object features. The modification of both inference and training is small. The incurred additional cost is marginal.\\n\\nIn experiments, our approach is validated on three representative and strong HOI baseline methods, two transformer-based methods [22, 33] and a traditional two-stage method [42]. They are all significantly improved by our approach. New state-of-the-art results are obtained on two benchmarks. Specifically, we obtain 36.03 mAP on HICO-DET. Comprehensive ablation studies and in-depth discussion are also provided to verify the effectiveness of implementation details in our approach. It turns out that our method is more effective on challenging images that contain more human-object instances, a property that is rarely discussed by previous HOI methods.\\n\\n2. Related Work\\n\\n2.1. Instance Query Learning in HOI Detection\\n\\nDETR [2] firstly proposes the concept of object instance query for object detection task. Such queries essentially learn the priors of both object appearance and spatial location. DETR leverages those queries to probe image features through a transformer [35] and localize unique objects in the image. Motivated by its great success, many works [15, 22, 33, 41, 48, 49] adapt such detection transformer framework to HOI detection by simply treating the HOI triplet [33, 49] or H-O pair [4, 15] as an object. A few of them [6, 30, 45] pay attention to adapting the plain query to this task. DOQ [30] proposes a knowledge distillation model using oracle queries to facilitate the representation learning of a transformer-based detector; HQM [45] explicitly constructs hard positive queries from ground truth to train the model to be less vulnerable to spacial variations; CATN [6] utilizes the object category prior generated from external object detector and language model for query initialization. In summary, these transformer-based methods use each query to aggregate context information not restricted to one interaction category, in order to predict a potential HOI instance at a specific location.\\n\\nIn DETR [2] and its variants [4,15,22,33,41,49] in HOI, as the queries are category-agnostic, their association to object categories are dynamic and unstable during training. This could be problematic. For example, it is well known that the convergence of DETR training is slow. In contrast, our proposed query is category-specific. The learning is guided by image-level classification task and stable. Such queries learn category-specific priors and are good representation for interaction categories.\\n\\n2.2. Feature Learning in HOI Detection\\n\\nEarly methods. Based on two-stage detection framework, early works make many efforts to help feature learning, including employing architectures effective in modeling relation and context like GNN [7, 29, 34, 37] and attention module [8, 34, 46], leveraging fine-grained visual features [11, 17, 19, 20, 36] like human pose and introducing language prior [1, 7, 17, 27, 46].\\n\\nTransformer-based methods. Motivated by DETR [49], many methods [4, 15, 33, 49] leverage transformer architecture [35] and extend the object query in DETR to HOI query. With the help of HOI query and transformer's built-in attention mechanism, those methods learn effective feature representation for HOI triplet or H-O pair. Based on those pioneer transformer-based methods, recently, many methods are proposed to further help feature learning, by decoupling H-O pair localization and interaction classification [22, 41, 48] or exploiting multi-scale feature in transformer architecture [16]. Some works [6, 22] leverage vision-language knowledge in CLIP [31] or design a pretrained model [40] specifically for HOI; others utilize information like human poses [39] or spatial configurations [13] that has been used in early HOI detectors.\\n\\nRelation to the proposed method. Previous methods learn the H-O feature while ours learns the category query as the category representation feature. Thus, they are complementary. The interaction classification is simply by measuring the similarity between the two types of features. The integration of our method to previous HOI methods is simple.\\n\\n3. Our Method\\n\\nThe overview of our method is in Fig. 2. It consists of two components, the image-level category query learning (top block) and human-object interaction classification (bottom right block).\\n\\nThe first component is detailed in Sec. 3.1. It is briefly summarized here. A number of queries (embedding vectors) are associated to human-object interaction category, in a one-to-one manner. Such queries interact with image features (provided by a baseline HOI method) through a transformer decoder [35] and become image-specific queries.\"}"}
{"id": "CVPR-2023-322", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Overview of our method. It consists of two components (top and bottom right). It can be integrated with any baseline HOI method (bottom left) that provides image feature $I$ and human-object instance features $F_i$. See Sec. 3 for details.\\n\\nLearning of both the queries and decoder weights is supervised by an auxiliary image-level classification task. In this way, the queries are learnt to capture category-specific feature and become good feature representation for these categories. Besides some minor details, this step is the same as the previous work Query2label [25], which is for multi-label image classification task.\\n\\nThe second component is detailed in Sec. 3.2. For the first time, we adopt the category query learning method for human-object interaction classification tasks. The cosine similarity between the category query and human-object feature is used for interaction classification. Thus, it works with any HOI method that provides human-object features. Besides, the image-level classification results turn out moderately helpful in an score integration step, which is an extra technique that benefits the performance.\\n\\nOverall, the proposed method is simple, effective, lightweight and general. It can be combined with most previous HOI detection methods (bottom left block in Fig. 2), with small modification, as elaborated in Sec. 4.\\n\\n3.1. Image-level Category Query Learning\\n\\nSimilar as in Query2Label [25], for $K$ human-object interaction categories, we define their one-to-one corresponding category queries, which are learnable embedding vectors, $\\\\{Q_k\\\\} \\\\in \\\\mathbb{R}^{K \\\\times D}$, where $D$ is the vector dimension.\\n\\nEach query $Q_k$ aggregates image features $I \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times D}$ through a transformer decoder and is updated to image specific query $Q_k'$, $\\\\{Q_k'\\\\} = decoder(\\\\{Q_k\\\\}, I)$. (1)\\n\\nNote that the decoder structure has several variants, which are studied in [5]. Our experiments show that the structure is of minor importance, as discussed in Sec. 5.4. Specifically, our decoder consists of two layers, each of which consisting of a cross-attention layer, a self-attention layer and a FFN layer, in order.\\n\\nThen, each image-level classification probabilities $p_k$ is computed by applying a category-specific fully-connected layer and a sigmoid activation on the updated query $Q_k'$. $p_k = sigmoid(FC(Q_k'))$. (2)\\n\\nLearning of the category query $\\\\{Q_k\\\\}$ and the decoder weights is supervised by common image classification losses. To deal with the label imbalance problem, focal loss [23] and asymmetric loss (ASL) [32] are used. Asymmetric loss is a variant of focal loss. It is more robust for high label imbalance and noises. Our experiments (see Sec. 5.4) show that it is slightly better.\\n\\nSpecifically, with the classification probability $p_k$ and the shifted probability $p_k' = max(p_k - m, 0)$, the asymmetric loss is\\n\\n$$L_{img} = \\\\frac{1}{K} \\\\sum_{k=1}^{K} \\\\left[ (1 - p_k) \\\\gamma^+ + \\\\log(p_k) y_k \\\\right]^{\\\\gamma^-} \\\\left[ -\\\\log(1 - p_k') y_k \\\\right]^{\\\\gamma^-},$$\\n\\nwhere the binary label $y_k$ indicates the existence of category $k$ in the image, and $\\\\gamma^+$, $\\\\gamma^-$ as well as $m$ are hyper-parameters. We use the default values in ASL [32], $\\\\gamma^+ = 0$, $\\\\gamma^- = 4$ and $m = 0$.\\n\\nIn this way, the category queries are learnt to encode the category priors. Figure 3 is the visualization of the heatmaps in the cross-attention layer of the decoder. Each category query learns to locate the human body parts related to discriminative feature of its corresponding interaction category, e.g., in Fig. 3c, the query of \u201chold\u201d highlights...\"}"}
{"id": "CVPR-2023-322", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The hand region, while in the same image the query of \\\"ride\\\" highlights the foot region. It qualitatively demonstrates that the query learning is effective in encoding category-specific information.\\n\\nThe updated queries adaptively extract category-related features for each image, with the help of the transformer decoder's built-in multi-head cross-attention layer.\\n\\n3.2. Interaction Classification with Category Query\\n\\nIn this step, we apply the updated category queries $\\\\{Q'_k\\\\}$ as the weights for interaction classification. Given the $i$-th human-object instance, its classification probability score for $k$-th interaction category is simply the cosine similarity between its feature $F_i$ and the category query feature $Q'_k$.\\n\\n$$s_{i,k} = \\\\text{sigmoid}(Q'_k \\\\cdot F_i | Q'_k | \\\\times |F_i|).$$ (4)\\n\\nThere are no restrictions or assumptions on the human-object feature $F_i$. Most previous HOI methods should be applicable here.\\n\\nIn this step, the traditional classification weight from static parameters is replaced with the category query adaptive on each image. This behavior is essentially different from [25], which uses category queries as image feature. By \\\"adaptive\\\", we mean that the queries are updated dynamically according to the image contents. As exemplified in Fig. 3a and Fig. 3c, the queries of \\\"hold\\\" learn to highlight different interactive areas in different images and update themselves with features from these areas via attention. This shows the query learning is adaptive to image.\\n\\nAs discussed in Sec. 5.4, the classification step in Eq. (4) is crucial to make the category query learning effective on human-object interaction classification. Without this step, the image-level category query learning using only image-level classification from Query2label [25] is of little use.\\n\\nScore integration step. The segmentation method in [12] discards certain categories during pixel classification that have low image classification scores. Motivated by this method, we take a similar score integration step. The image-level classification score $\\\\{p_i\\\\}$ is used to enhance the human-object instance classification. The idea is that, the instance score should be higher if the image-level probability is higher. Our implementation is similar as in [12]. During both training and testing, for each image, the top-$\\\\kappa$ categories ($\\\\kappa = 70$ in this work) with higher image classification scores $\\\\{p_i\\\\}$ are selected. The instance score $s_{i,k}$ is slightly modulated such that it becomes higher if the rank of category $k$ is higher. This strategy gives rise to moderate improvement, as verified in Tab. 4. We left the implementation details in the supplementary materials.\\n\\nFigure 3. From left to right: image, attention maps of the cross-attention layer in the decoder for different interaction categories.\\n\\n4. Integration to Off-the-shelf HOI Detectors\\n\\nAs shown in Fig. 2, our method is ready to integrate with any baseline HOI method that provides image feature $I$ and human-object instance feature $\\\\{F_i\\\\}$. The integration is simple. During inference, the human-object instance interaction classification part is replaced by our method in Sec. 3.2, the top and bottom right block in Fig. 2.\\n\\nDuring training, the original loss in the baseline HOI method $L_{base}$ is added to our image classification loss in Eq. (3). The final loss $L$ for training is\\n\\n$$L = L_{base} + \\\\lambda \\\\times L_{img},$$ (5)\\n\\nwhere the weight $\\\\lambda$ is 1.0 by default. All other hyper parameters and details during training remain the same as in the baseline HOI method.\\n\\nThus, our method is general and applicable to most existing HOI methods. In this work, we select three representative yet different baseline methods to verify the effectiveness of our approach, as described below.\\n\\nQPIC [33] is the first to introduce transformer method into HOI task. It is also the baseline for many recent works [13, 28, 41, 45, 48]. Its performance is much better than early one-stage [14, 21, 38] and two-stage [7, 11, 17] methods while keeping a simple and end-to-end architecture. It consists of a CNN backbone as well as a transformer encoder and decoder.\\n\\nDuring our integration, the feature map in its transformer encoder is used as the image feature $I$. The human-object feature $\\\\{F_i\\\\}$ is the query feature in its decoder.\"}"}
