{"id": "CVPR-2022-1487", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Pipeline of the fine registration and fusion module (F2M). \u201cConv\u201d: convolution layer with kernel size as $3 \\\\times 3$ and stride as 1. As the receptive fields grow with depth increases, a pixel in deeper feature maps corresponds to a larger region in the image, which are not conducive to improving the registration accuracy. Thus, we use the shallow features to explore and generate spatial deformation. The nonlinear mapping of the first convolution layer eliminates the pixel intensity differences between $I_C$ and $I_y$. The deformation block generates the deformation fields (refer to Supplementary Material for details). Resampling, batch normalization, and residual blocks are used to apply to different deformations.\\n\\nFor texture preservation, we introduce the gradient channel attention block as in Fig. 5. We aggregate the absolute gradients as they are a better representation of information richness in feature maps. The information is aggregated by jointly using max-pooling and average-pooling operations. Then, the two-branch results are added and fed into two individual multi-layer perceptrons to generate shared channel-wise attention weights. Then, several convolution layers map the features back to generate $I_f$.\\n\\n4. Experiments\\n\\nImplementation Details. The code of our method is implemented in TensorFlow. Experiments are conducted on an NVIDIA GeForce GTX Titan X GPU and 2.4 GHz Intel Core i5-1135 CPU. The parameters in all networks are updated with the Adam Optimizer. The epoch of training coarse registration network is set to 100, and that of training F2Net is set to 30. The batch size is 4. The learning rate is set to 0.0004 with exponential decay. The hyper-parameters are set as: $\\\\eta = 2$, $\\\\delta = 100$, $\\\\gamma = 0$, $\\\\lambda = 0.1$. We build the training and test datasets based on a publicly available VIS/NIR Scene dataset. Images are cropped into patches of size $384 \\\\times 384$ and flipped for more training data.\\n\\n4.1. Multi-modal Image Registration\\n\\nWe compare our coarse registration module with SOTA multi-modal registration methods, including traditional ones (i.e., MI, DASC, NTG, SCB) and a deep learning-based method NeMAR. For NeMAR, we retrain the model on our training dataset for 800 epoches.\\n\\nQualitative Results. Qualitative results are shown in Fig. 6. In the first two groups, the proposed RFNet and NTG show more accurate registration results than others. MI and SCB perform almost exactly on the first pair while suffer a large registration error in the second pair. DASC shows severe geometric distortions, especially in the un-overlapped regions of two source images. NeMAR shows a slight improvement than unaligned images. In the third group, source images exhibit high structure similarity and repeatability in different regions. In this case, the proposed RFNet shows higher registration accuracy than comparative methods, including NTG. These results demonstrate that our method can outperform the SOTA methods.\\n\\nQuantitative Evaluation. For quantitative evaluation, we build 5 pairs of point landmarks in each image pair (see Supplementary Material for illustration). The points in the deformed VIS image are expected to be in the same positions as those in the NIR image. Thus, we measure the Euclidean distance between the deformed source points and target points. We count the distances from three aspects, including root mean square error (RMSE), max square error (MAE) and median square error (MEE). Furthermore, we measure the image-level similarity between the deformed VIS and NIR images with peak signal-to-noise ratio (PSNR) and structural similarity index measure (SSIM). All the metrics are tested on 45 unaligned multi-modal image pairs and reported in Table 1. The coarse registration module of RFNet achieves the optimal results on RMSE, MAE and MEE. By comparison, MI and NTG achieve low means but high standard deviations as they perform well in some scenarios while not in others. DASC shows the optimal results in SSIM and PSNR because the results contain some incorrect information in un-overlapped regions. However, ...\"}"}
{"id": "CVPR-2022-1487", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Quantitative comparisons of registration accuracy (mean and standard variation, red: best, blue: second best, cyan: third best).\\n\\n| Methods              | RMSE      | MAE       | MEE       | SSIM      | PSNR     |\\n|----------------------|-----------|-----------|-----------|-----------|----------|\\n| unaligned VIS MI     | 47.223 \u00b1 12.729 | 74.652 \u00b1 17.786 | 66.199 \u00b1 18.287 | 0.292 \u00b1 0.105 | 11.768 \u00b1 1.996 |\\n| DASC                 | 15.756 \u00b1 53.284  | 25.347 \u00b1 79.450  | 22.102 \u00b1 76.505  | 0.505 \u00b1 0.177 | 14.837 \u00b1 3.672 |\\n| NTG                  | 17.214 \u00b1 10.115  | 40.752 \u00b1 22.688  | 15.989 \u00b1 14.240  | 0.726 \u00b1 0.064 | 17.582 \u00b1 3.052 |\\n| SCB                  | 9.942 \u00b1 23.807   | 18.221 \u00b1 42.164  | 13.430 \u00b1 35.279  | 0.611 \u00b1 0.164 | 15.349 \u00b1 3.453 |\\n| NeMAR                | 37.384 \u00b1 25.209  | 67.760 \u00b1 39.807  | 49.605 \u00b1 36.250  | 0.324 \u00b1 0.177 | 12.784 \u00b1 3.429 |\\n| coarse registration  module of RFNet | 43.747 \u00b1 12.426  | 69.616 \u00b1 17.498  | 61.059 \u00b1 17.910  | 0.345 \u00b1 0.115 | 12.179 \u00b1 2.139 |\\n\\nIn other results, the un-overlapped regions are black with little similarity with NIR images. In general, our method exhibits comparative registration performances.\\n\\n4.2. Multi-modal Fusion and Our Fine Registration\\n\\nThis section focuses on evaluating the fusion and fine registration performances of our F2M. As the state-of-the-art (SOTA) fusion methods cannot deal with unaligned data, the registration method NTG is utilized as the pre-registration operation for them as it ranks second in Sec. 4.1.\\n\\nIn other words, we compare RFNet with the combination of NTG and SOTA fusion methods to evaluate the fusion performance and observe how significant the registration is for existing fusion methods. These fusion methods include DenseFuse, IFCNN, U2Fusion, PMGI and MDLatLRR.\\n\\nQualitative Results.\\n\\nQualitative results on six typical unaligned image pairs are shown in Fig. 7. We analyze the results from three aspects. First, our method can register multi-modal images well as well as fuse their complementary information. As shown in the first two examples, the registration method fails to completely eliminate the parallax in two source images. The misalignments remain in the fusion results and result in disorganized scene content. By comparison, the joint coarse-to-fine registration in our method and the feedback of image fusion help correct the misalignments and improve the fusion performance.\\n\\nSecond, our method can remove the overlapping shadows to present clear textures. As shown in the third and fourth rows, the slightly deficient registration accuracy causes the overlapping shadows and blurs the fusion results. By comparison, our method can finely remove the overlapping shadows and preserve more sharp edges.\\n\\nThird, our fusion results exhibit the most abundant and natural textures. In the last two examples, NIR images contain richer content than corresponding VIS images. In competitors, the blurred texture details in the VIS images affect the clarity of fusion results more or less. And in the fourth row, the trees in the result of IFCNN are closer to those in the NIR image than natural ones. By comparison, our results are suitable for the human visual perception system.\\n\\nQuantitative Evaluation.\\n\\nWe perform the quantitative evaluation of image fusion from two aspects. First, we assess the characteristics of fused images with average gradient (AG), entropy (EN) and standard deviation (STD). Second, we measure the similarity between the fused image and two source images with PSNR. It is worth noting that if the source images are unaligned, the fused images will suffer misalignments while the quantitative results may show fake improvements (e.g., average gradients). To avoid the negative influence of this situation, we selected 35 image pairs that do not have noticeable misalignments after being processed by NTG/coarse registration module. The results are reported in Table 2. Our optimal results on AG, EN and STD show that our results contain the most texture details, the most amount of information and the most obvious contrast, respectively. Besides, our optimal result on PSNR indicates that the proposed fusion method produces least distortion and our fused images are closest to the source images.\\n\\nExternal Verification on Object Detection.\\n\\nTo evaluate the practical benefits of image fusion and its improved performances, an external verification is further performed. We compare the detection results by using YOLOv5 as the detector. As shown in Fig. 8, we perform the detections both on an unaligned image pair to validate the effect of registration accuracy and on an aligned image pair to validate the effect of fusion performance. In the first example, the misalignments in fusion results negatively impact the detections of the cars. When the images are well registered, the merged information from two modalities plays a positive role in promoting the detection result, as shown in the detection result on our registered and fused image.\\n\\nIn the second example, the images are aligned. In this case, other fusion methods reduce the accuracy of detecting the stop sign compared with that in the VIS image. By comparison, our method increases the detection accuracy by fusing the information in the NIR image.\\n\\n4.3. Ablation Study\\n\\nEssential Factors in Coarse Registration Module.\\n\\nThe essential factors in this module lie in three aspects,\"}"}
{"id": "CVPR-2022-1487", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7. Fusion results on six unaligned multi-modal pairs.\\n\\nTable 2. Quantitative comparisons of fusion performance (mean and standard variation, red: best, blue: second best, cyan: third best).\\n\\n| Methods          | AG  | EN  | STD | PSNR |\\n|------------------|-----|-----|-----|------|\\n| DenseFuse        | 6.565\u00b11.677 | 7.859\u00b11.816 | 8.862\u00b12.206 | 64.678\u00b12.011 |\\n| IFCNN            | 7.290\u00b10.372 | 7.105\u00b10.403 | 7.104\u00b10.324 | 65.359\u00b12.376 |\\n| U2Fusion         | 6.906\u00b11.489 | 7.222\u00b10.276 | 7.325\u00b10.288 | 63.712\u00b11.729 |\\n| PMGI             | 7.092\u00b10.372 | 7.290\u00b10.295 | 7.104\u00b10.324 | 64.641\u00b11.666 |\\n| MDLatLRR         | 6.122\u00b11.421 | 7.222\u00b10.276 | 7.325\u00b10.288 | 66.269\u00b12.334 |\\n| RFNet            | 10.217\u00b12.516 | 7.325\u00b10.288 | 10.064\u00b10.877 | 66.363\u00b12.150 |\\n\\nFigure 8. Detection results on source images and different registration and fusion. The detector is YOLOv5.\\n\\nWe design three comparison experiments to separately validate their effectivenesses. The registration accuracy is uniformly evaluated by the NCC loss.\\n\\ni) We change the input of AffineNet and the loss is defined according to the inputs. We separately feed the descriptors defined in SCB, \\\\{I_x, I_y\\\\} without translation, and \\\\{I_x\u2192y, I_y\\\\} generated by our TransNet. The changes in losses shown in Fig. 9 demonstrate that the image-level inputs outperform the sparse descriptors. And the same-domain inputs further promote the convergence speed and performance.\\n\\nii) The deformable convolution layers in AffineNet are replaced with traditional ones while traditional ones lead to a gradient explosion.\\n\\niii) We compare the effect of NCC/L_1/L_2 loss as the metric. L_2 loss encounters a gradient explosion and Fig. 9 shows that NCC loss is superior to L_1 loss.\\n\\nFine Registration Performance of F2M. To validate the effectiveness of the fine registration in F2M on eliminating local misalignments, we perform two experiments by comparing F2M with two different competitors. One situation is that there are merely local parallaxes in source...\"}"}
{"id": "CVPR-2022-1487", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9. Changes in registration losses during training process.\\n\\nFigure 10. Qualitative results to validate the effectiveness of the fine registration in F2M through two experiments.\\n\\n(a) Dealing with images with small parallaxes without pre-registration\\n\\n(b) Comparing the registration accuracy before and after F2M\\n\\nFigure 11. Qualitative comparison results with (w/) and without (w/o) texture preservation strategies.\\n\\nFigure 12. Some examples of scenarios where the performance of the coarse registration module is prone to degrade.\\n\\nTexture Preservation Strategies. We adapt the gradient channel attention mechanism, introduce the gradient loss, and set $\\\\gamma$ to a relatively high value to preserve texture details. To validate their effectiveness, we remove the attention mechanism, remove the gradient loss ($\\\\delta = 0$) and set $\\\\gamma$ to a lower value to de-preserve texture. Comparison results are shown in Fig. 11. The results with texture preservation exhibit more texture details than those without using these strategies.\\n\\nLimitations. It is typically difficult to establish strict correspondences between multi-modal images. In some cases, the scenes may show obvious cross-modal structure differences, such as field and forest (e.g., the first example of Fig. 12). The image translation mainly adjusts the intensity, but rarely changes the scene content or structures (few edges are generated or eliminated). In other words, it is difficult for image translation to reduce the cross-modal structural differences. Moreover, in some other cases, the scenes may lack salient structures, such as water (e.g., the second example of Fig. 12). These factors bring challenges to the coarse registration module which is based on image translation and NCC loss. Thus, in these cases, the registration accuracy of the coarse registration module is prone to degrade, as shown in the last column of Fig. 12.\\n\\n5. Conclusion\\n\\nIn this paper, a new unsupervised multi-modal image registration and fusion method is proposed by mutually reinforcing the two individual tasks. The registration is handled in a coarse-to-fine approach. The coarse registration is modeled as an affine transformation and realized through an deformable convolution-based network and an image translation-based image-level loss function. The fine registration relies on the feedback of fusion. The fine-registered results further improve the fusion results. Also, we focus on texture preservation for both the feedback of fusion and image fusion itself. Experiments validate the effectiveness of the proposed method and mutually reinforcing framework.\"}"}
{"id": "CVPR-2022-1487", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Moab Arar, Yiftach Ginger, Dov Danon, Amit H Bermano, and Daniel Cohen-Or. Unsupervised multi-modal image registration via geometry preserving image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 13410\u201313419, 2020.\\n\\n[2] Matthew Brown and Sabine S\u00fcsstrunk. Multi-spectral sift for scene category recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 177\u2013184, 2011.\\n\\n[3] Si-Yuan Cao, Hui-Liang Shen, Shu-Jie Chen, and Chenguang Li. Boosting structure consistency for multispectral and multimodal image registration. IEEE Transactions on Image Processing, 29:5147\u20135162, 2020.\\n\\n[4] Chen Chen, Yeqing Li, Wei Liu, and Junzhou Huang. Sirf: Simultaneous satellite image registration and fusion in a unified framework. IEEE Transactions on Image Processing, 24(11):4213\u20134224, 2015.\\n\\n[5] Shu-Jie Chen, Hui-Liang Shen, Chunguang Li, and John H Xin. Normalized total gradient: a new measure for multispectral image registration. IEEE Transactions on Image Processing, 27(3):1297\u20131310, 2017.\\n\\n[6] Guangmang Cui, Huajun Feng, Zhihai Xu, Qi Li, and Yueling Chen. Detail preserved fusion of visible and infrared images using regional saliency extraction and multi-scale image decomposition. Optics Communications, 341:199\u2013209, 2015.\\n\\n[7] Yu Fu, Xiao-Jun Wu, and Tariq Durrani. Image fusion based on generative adversarial network consistent with perception. Information Fusion, 72:110\u2013125, 2021.\\n\\n[8] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 1501\u20131510, 2017.\\n\\n[9] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In Proceedings of the International Conference on Machine Learning (ICML), pages 448\u2013456, 2015.\\n\\n[10] Seungryong Kim, Dongbo Min, Bumsub Ham, Minh N Do, and Kwanghoon Sohn. Dasc: Robust dense descriptor for multi-modal and multi-spectral correspondence estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(9):1712\u20131729, 2017.\\n\\n[11] Seungryong Kim, Dongbo Min, Bumsub Ham, Seungchul Ryu, Minh N Do, and Kwanghoon Sohn. Dasc: Dense adaptive self-correlation descriptor for multi-modal and multi-spectral correspondence. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2103\u20132112, 2015.\\n\\n[12] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\n[13] Hui Li and Xiao-Jun Wu. Densefuse: A fusion approach to infrared and visible images. IEEE Transactions on Image Processing, 28(5):2614\u20132623, 2018.\\n\\n[14] Hui Li, Xiao-Jun Wu, and Josef Kittler. Mdlatlrr: A novel decomposition method for infrared and visible image fusion. IEEE Transactions on Image Processing, 29:4733\u20134746, 2020.\\n\\n[15] Yu Liu, Xun Chen, Juan Cheng, and Hu Peng. A medical image fusion method based on convolutional neural networks. In Proceedings of the International Conference on Information Fusion (Fusion), pages 1\u20137, 2017.\\n\\n[16] Yu Liu, Shuping Liu, and Zengfu Wang. A general framework for image fusion based on multi-scale transform and sparse representation. Information Fusion, 24:147\u2013164, 2015.\\n\\n[17] Jiayi Ma, Pengwei Liang, Wei Yu, Chen Chen, Xiaojie Guo, Jia Wu, and Junjun Jiang. Infrared and visible image fusion via detail preserving adversarial learning. Information Fusion, 54:85\u201398, 2020.\\n\\n[18] Jiayi Ma, Han Xu, Junjun Jiang, Xiaoguang Mei, and Xiaoping Zhang. Ddcgan: A dual-discriminator conditional generative adversarial network for multi-resolution image fusion. IEEE Transactions on Image Processing, 29:4980\u20134995, 2020.\\n\\n[19] Jiayi Ma, Wei Yu, Pengwei Liang, Chang Li, and Junjun Jiang. Fusiongan: A generative adversarial network for infrared and visible image fusion. Information Fusion, 48:11\u201326, 2019.\\n\\n[20] Dwarikanath Mahapatra, Bhavna Antony, Suman Sedai, and Rahil Garnavi. Deformable medical image registration using generative adversarial networks. In Proceedings of the IEEE International Symposium on Biomedical Imaging (ISBI), pages 1449\u20131453, 2018.\\n\\n[21] Fanjie Meng, Baolong Guo, Miao Song, and Xu Zhang. Image fusion with saliency map and interest points. Neurocomputing, 177:1\u20138, 2016.\\n\\n[22] J Wesley Roberts, Jan A van Aardt, and Fethi Babikker Ahmed. Assessment of image fusion procedures using entropy, image quality, and multispectral classification. Journal of Applied Remote Sensing, 2(1):023522, 2008.\\n\\n[23] Colin Studholme, Corina Drapaca, Bistra Iordanova, and Valerie Cardenas. Deformation-based mapping of volume change from serial brain MRI in the presence of local tissue contrast change. IEEE Transactions on Medical Imaging, 25(5):626\u2013639, 2006.\\n\\n[24] Carlo Tomasii and Roberto Manduchi. Bilateral filtering for gray and color images. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 839\u2013846, 1998.\\n\\n[25] Paul Viola and William M Wells III. Alignment by maximization of mutual information. International Journal of Computer Vision, 24(2):137\u2013154, 1997.\\n\\n[26] Christian Wachinger and Nassir Navab. Structural image representation for image registration. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition-Workshops, pages 23\u201330, 2010.\\n\\n[27] Chengjia Wang, Giorgos Papanastasiou, Agisilaos Chartsias, Grzegorz Jacenkow, Sotirios A Tsaftaris, and Heye Zhang. Fire: unsupervised bi-directional inter-modality registration using deep networks. arXiv preprint arXiv:1907.05062, 2019.\"}"}
{"id": "CVPR-2022-1487", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Qi Wei, Jos\u00e9 Bioucas-Dias, Nicolas Dobigeon, and Jean-Yves Tourneret. Hyperspectral and multispectral image fusion based on a sparse representation. IEEE Transactions on Geoscience and Remote Sensing, 53(7):3658\u20133668, 2015.\\n\\nQi Xie, Minghao Zhou, Qian Zhao, Deyu Meng, Wangmeng Zuo, and Zongben Xu. Multispectral and hyperspectral image fusion by ms/hs fusion net. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1585\u20131594, 2019.\\n\\nHan Xu, Jiayi Ma, Junjun Jiang, Xiaojie Guo, and Haibin Ling. U2fusion: A unified unsupervised image fusion network. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(1):502\u2013518, 2022.\\n\\nYang Yang, Jiaxiang Liu, Shuying Huang, W eiguo Wan, Wenying Wen, and Juwei Guan. Infrared and visible image fusion via texture conditional generative adversarial network. IEEE Transactions on Circuits and Systems for Video Technology, 31(12):4771\u20134783, 2021.\\n\\nHao Zhang, Zhuliang Le, Zhenfeng Shao, Han Xu, and Jiayi Ma. Mff-gan: An unsupervised generative adversarial network with adaptive and gradient joint constraints for multi-focus image fusion. Information Fusion, 66:40\u201353, 2021.\\n\\nHao Zhang, Han Xu, Yang Xiao, Xiaojie Guo, and Jiayi Ma. Rethinking the image fusion: A fast unified image fusion network based on proportional maintenance of gradient and intensity. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 12797\u201312804, 2020.\\n\\nYu Zhang, Yu Liu, Peng Sun, Han Yan, Xiaolin Zhao, and Li Zhang. Ifcnn: A general image fusion framework based on convolutional neural network. Information Fusion, 54:99\u2013118, 2020.\"}"}
{"id": "CVPR-2022-1487", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"RFNet: Unsupervised Network for Mutually Reinforcing Multi-modal Image Registration and Fusion\\n\\nHan Xu, Jiayi Ma, Jiteng Yuan, Zhuliang Le, and Wei Liu\\n\\n1 Wuhan University, China\\n2 Tencent Data Platform, China\\n\\n{xuhan,yuanjiteng,lezhuliang}@whu.edu.cn, jyma2010@gmail.com, wl2223@columbia.edu\\n\\nAbstract\\n\\nIn this paper, we propose a novel method to realize multi-modal image registration and fusion in a mutually reinforcing framework, termed as RFNet. We handle the registration in a coarse-to-fine fashion. For the first time, we exploit the feedback of image fusion to promote the registration accuracy rather than treating them as two separate issues. The fine-registered results also improve the fusion performance. Specifically, for image registration, we solve the bottlenecks of defining registration metrics applicable for multi-modal images and facilitating the network convergence. The metrics are defined based on image translation and image fusion respectively in the coarse and fine stages. The convergence is facilitated by the designed metrics and a deformable convolution-based network. For image fusion, we focus on texture preservation, which not only increases the information amount and quality of fusion results but also improves the feedback of fusion results. The proposed method is evaluated on multi-modal images with large global parallaxes, images with local misalignments and aligned images to validate the performances of registration and fusion. The results in these cases demonstrate the effectiveness of our method.\\n\\n1. Introduction\\n\\nMulti-modal image fusion aims to merge the information from different imaging modalities to generate a single image with rich information and high quality. Because the fused images can describe scenes comprehensively by merging the complementary information, image fusion serves as a powerful tool for wide applications, such as security, remote sensing, clinical treatment, etc.\\n\\nAs multi-modal images are taken from different devices/sensors, it inevitably leads to parallaxes due to based positions, angles, etc. However, almost all the fusion methods fail to consider parallaxes. They require an accurate registration before fusion can take place, as shown in Fig. 1 (a). Unfortunately, the diversity between different modalities poses a great challenge to improve the registration accuracy, still resulting in mitigated misalignments in the pre-registered images. When registration and fusion are separate issues, existing fusion methods have to \u201ctolerate\u201d rather than \u201cfight\u201d the pre-registration misalignments. Thus, multi-modal image registration and fusion become an urgent issue for the practical application of image fusion. Meanwhile, in existing separate branches, image fusion is a downstream task of registration, and fails to provide feedbacks to improve registration accuracy. Nevertheless, considering the characteristics of fused images, it is possible for image fusion to inversely eliminate misalignments. First, the fused images integrate the information from both modalities. When the fused image is registered with either source image, the alleviated modal diversity reduces the difficulty of registration. Second, the misalignments in fused images undoubtedly lead to more but repeated salient structures, i.e., dense gradients. By comparison, an accurate registration encourages the sparseness of gradients. Thus, the gradient sparsity of fusion results can act as a criterion to improve registration accuracy in a feedback fashion without losing the scene information in source images. Third, the fused images retain the obvious salient structures in a single image and discard some superfluous and useless information during the fusion process. It reduces the negative impact of superfluous information on image registration.\"}"}
{"id": "CVPR-2022-1487", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"When image fusion helps to eliminate misalignments, the more precisely aligned data further promotes fusion results. As a result, these two tasks can be mutually reinforced in this way, as illustrated in Fig. 1.\\n\\nSpecifically to the individual solution of each task, either image registration or fusion has its own bottlenecks. For image registration, it is a difficult problem to develop appropriate registration metrics or evaluation ways adaptive for multi-modal data. The other important issue is to ensure that the designed registration constraints should be practical for deep network optimization though gradient descent.\\n\\nFor image fusion, a general purpose is to enable fused images present the most amount of information, partly represented by gradients. Moreover, as stated above, the gradients of fused images play a critical role in eliminating misalignments. Combining these two aspects, fusion methods should be dedicated to the retention of texture information, which is consistent with both the fusion target and the feedback function of image fusion to image registration.\\n\\nTo address the limitations of prior works and unexplored issues, we explore multi-modal image registration and fusion in a mutually reinforcing framework. We propose an unsupervised network to realize it, termed as RFNet. The proposed framework is summarized as Fig. 1(b). The registration is handled in a coarse-to-fine approach. The coarse stage corrects the global parallaxes through an evaluation metric based on image translation. The coarse-registered results help generate meaningful but rough fused images. Image fusion and fine registration are integrated in a single network. Then, to correct local misalignments, we rely on the characteristics of fused images to optimize the deformation-related parts in this network. Finally, the network generates the fine-registered and fused image.\\n\\nThe main contributions of RFNet are summarized as follows:\\n\\ni) The problems of multi-modal image registration and fusion are mutually reinforced in our work. It is the first time that image fusion is exploited to promote multi-modal image registration accuracy through a deep neural network.\\n\\nii) We focus on designing constraints to optimize the multi-modal registration performance. In the coarse stage, we apply image translation to build an image-level evaluation metric. An improved network architecture is proposed to help facilitate the network convergence. In the fine stage, the metric is designed based on the fusion results.\\n\\niii) Considering the texture retention in image fusion, we adapt a gradient channel attention mechanism to adaptively adjust the channel-wise contributions of features. Besides, we design a gradient loss with bias. The network architecture and loss function are both based on the texture richness.\\n\\n2. Related Works\\n\\nMulti-modal Image Registration.\\n\\nTraditional registration methods include transformation- and measure-based ones. Transformation-based ones transfer images into a common space to exhibit better consistency [3, 10, 11, 26]. They manually analyse multi-modal characteristics and design constraints to impose consistency. Nevertheless, the optimization in these methods is thorny. Measure-based ones aim to measure the similarity with low sensitivity to modal variations. Representative methods utilize mutual information (MI), regional MI [23], etc., which are computationally intractable and not suitable for gradient descent [5]. Recently, deep learning-based methods have been proposed. For instance, Wang et al. [27] use a network to create modal-independent features while drawbacks of sparsity still exist. Closest to our work, Arar et al. learn a cross-modality translation [1]. However, the cooperative training of translation and registration networks increases the difficulty of optimizing registration network. In our work, we find that feeding translated images in the same domain into the network can improve the registration accuracy and speed up the convergence simultaneously. Besides, compared with existing registration networks [1, 20], we employ deformable convolution in our network as it refers to the deformation in unregistered images for higher registration accuracy and stronger robustness. Most relevant to our work, SIRF [4] confirms that joint registration and fusion can definitely improve the results if they are combined properly. However, this work is realized in a tradition vectorial total variation model and designed for remote sensing images with restrictive local misalignments.\\n\\nMulti-modal Image Fusion.\\n\\nExisting fusion methods are tailored to aligned images without regard to parallaxes. Focusing on fusion itself, traditional methods include six categories: methods based on multi-scale transform [16], sparse representation [28], subspace, saliency [21], hybrid methods, and others. They are devoted to designing decomposition ways and fusion strategies in a manual way while detailed and diverse designs make them more and more complex. To solve it, some deep learning-based methods are proposed [13, 15, 29]. Some of them do not pay attention to texture preservation and some generative adversarial network-based methods [7, 18, 19, 32] suffer from generating fake and blurred details. Even some methods concern the textures [17, 31], they preserve the textures according to the image modality rather than the actual textures of specific regions. In this work, we adapt a gradient-based attention mechanism and a gradient loss with bias to enhance texture retention. Moreover, the network blends the deformation which enables misalignment correction based on the preserved textures.\\n\\n3. Proposed Method\\n\\nWe design an unsupervised network for mutually reinforcing multi-modal image registration and fusion, term as RFNet. The overall procedure is shown in Fig. 2.\"}"}
{"id": "CVPR-2022-1487", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Overall pipeline of our RFNet.\\n\\n3.1. Coarse Registration Module\\n\\nAffineNet takes images into the same domain (i.e., encoder to embed domain\\ninformation, we map it back to domains while removing the domain information. To ensure that the content information is kept, we use an AffineNet whose output is the coarse aligned image. Then, multi-modal images are roughly aligned except for some local parallaxes, where an affine transformation model performs the global correction based on the affine transformation parameters.\\n\\n3.1.1 Image Translation Network\\n\\nTransNet is aimed at learning the image translation function to narrow modal differences. AffineNet is a network to generate the affine transformation parameters. The first layers in decoders are shared. The result of mapping parameters to generate the deformation field for $D$ through the decoders contains the content information; we map it back to domains to a same content space, in addition to the designed construction loss and the translation loss are defined as:\\n\\n$$L_{trans} = \\\\sum_{i,j} \\\\left\\\\| E_{trans}(x, y) \\\\right\\\\|_1$$\\n\\n$$L_{con} = \\\\sum_{i,j} \\\\left\\\\| E_{con}(x, y) \\\\right\\\\|_1$$\\n\\nWhen the optimal deformation field $\\\\phi$ of size $HW\\\\times HW$ is obtained, we perform the same spatial transform on the coarse aligned image $C$ formed by shifting the fine registration and fusion are realized in a unified module/network, termed as fine registration and fusion module (F2M). Therefore, the loss function of AffineNet is defined to control the trade-off: $\\\\eta L_{trans} + (1-\\\\eta) L_{con}$ with a hyper-parameter $\\\\eta$ controlling the trade-off.\\n\\nAccording to [19681], multi-modal images such as visible (VIS) and near-infrared (NIR) images. We use in-stance normalization rather than batch normalization as it is applied for the betterment of this step.\\n\\nWhen feeding a pair of unaligned images $x$ and $y$ to TransNet, it outputs the affine parameters $p_\\\\theta(x, y)$ to perform the spatial transform on $x$ and $y$. TransNet firstly transfers multi-modal images into the content space as $x \\\\rightarrow c$, translating and reconstructing $x \\\\rightarrow y$. Similarly, for $y$ we have $x \\\\rightarrow y$. According to [19681], $x \\\\rightarrow y$.\\n\\nBy retaining content information, we use an AffineNet that is the final aligned and fused image. TransNet is a network to translate and reconstruct domain-related information, the registration loss is thus defined as:\\n\\n$$L_{trans} = \\\\sum_{i,j} \\\\left\\\\| E_{trans}(x, y) \\\\right\\\\|_1$$\\n\\n$$L_{con} = \\\\sum_{i,j} \\\\left\\\\| E_{con}(x, y) \\\\right\\\\|_1$$\\n\\n$\\\\eta L_{trans} + (1-\\\\eta) L_{con}$ with a hyper-parameter $\\\\eta$ controlling the trade-off. When the optimal deformation field $\\\\phi$ is obtained, we perform the same spatial transform on the coarse aligned image $C$ formed by shifting the fine registration and fusion are realized in a unified module/network, termed as fine registration and fusion module (F2M). Therefore, the loss function of AffineNet is defined to control the trade-off:\\n\\n$$L_{trans} = \\\\sum_{i,j} \\\\left\\\\| E_{trans}(x, y) \\\\right\\\\|_1$$\\n\\n$$L_{con} = \\\\sum_{i,j} \\\\left\\\\| E_{con}(x, y) \\\\right\\\\|_1$$\\n\\n$\\\\eta L_{trans} + (1-\\\\eta) L_{con}$ with a hyper-parameter $\\\\eta$ controlling the trade-off. Where $x$ and $y$ are a paired image set and the mapping result to domain $y$ is shown in Fig. 3.1. Coarse Registration Module\\n\\nThe pipeline of the proposed coarse registration module is shown in Fig. 3.1 to ensure that the content information is kept, we use an AffineNet whose output is the coarse aligned image. Then, multi-modal images are roughly aligned except for some local parallaxes, where an affine transformation model performs the global correction based on the affine transformation parameters. When the optimal deformation field $\\\\phi$ is obtained, we perform the same spatial transform on the coarse aligned image $C$ formed by shifting the fine registration and fusion are realized in a unified module/network, termed as fine registration and fusion module (F2M). Therefore, the loss function of AffineNet is defined to control the trade-off:\\n\\n$$L_{trans} = \\\\sum_{i,j} \\\\left\\\\| E_{trans}(x, y) \\\\right\\\\|_1$$\\n\\n$$L_{con} = \\\\sum_{i,j} \\\\left\\\\| E_{con}(x, y) \\\\right\\\\|_1$$\\n\\n$\\\\eta L_{trans} + (1-\\\\eta) L_{con}$ with a hyper-parameter $\\\\eta$ controlling the trade-off. Where $x$ and $y$ are a paired image set and the mapping result to domain $y$ is shown in Fig. 3.1. Coarse Registration Module\\n\\nThe pipeline of the proposed coarse registration module is shown in Fig. 3.1 to ensure that the content information is kept, we use an AffineNet whose output is the coarse aligned image. Then, multi-modal images are roughly aligned except for some local parallaxes, where an affine transformation model performs the global correction based on the affine transformation parameters. When the optimal deformation field $\\\\phi$ is obtained, we perform the same spatial transform on the coarse aligned image $C$ formed by shifting the fine registration and fusion are realized in a unified module/network, termed as fine registration and fusion module (F2M). Therefore, the loss function of AffineNet is defined to control the trade-off:\\n\\n$$L_{trans} = \\\\sum_{i,j} \\\\left\\\\| E_{trans}(x, y) \\\\right\\\\|_1$$\\n\\n$$L_{con} = \\\\sum_{i,j} \\\\left\\\\| E_{con}(x, y) \\\\right\\\\|_1$$\\n\\n$\\\\eta L_{trans} + (1-\\\\eta) L_{con}$ with a hyper-parameter $\\\\eta$ controlling the trade-off. Where $x$ and $y$ are a paired image set and the mapping result to domain $y$ is shown in Fig. 3.1. Coarse Registration Module\\n\\nThe pipeline of the proposed coarse registration module is shown in Fig. 3.1 to ensure that the content information is kept, we use an AffineNet whose output is the coarse aligned image. Then, multi-modal images are roughly aligned except for some local parallaxes, where an affine transformation model performs the global correction based on the affine transformation parameters. When the optimal deformation field $\\\\phi$ is obtained, we perform the same spatial transform on the coarse aligned image $C$ formed by shifting the fine registration and fusion are realized in a unified module/network, termed as fine registration and fusion module (F2M). Therefore, the loss function of AffineNet is defined to control the trade-off:\\n\\n$$L_{trans} = \\\\sum_{i,j} \\\\left\\\\| E_{trans}(x, y) \\\\right\\\\|_1$$\\n\\n$$L_{con} = \\\\sum_{i,j} \\\\left\\\\| E_{con}(x, y) \\\\right\\\\|_1$$\\n\\n$\\\\eta L_{trans} + (1-\\\\eta) L_{con}$ with a hyper-parameter $\\\\eta$ controlling the trade-off. Where $x$ and $y$ are a paired image set and the mapping result to domain $y$ is shown in Fig. 3.1. Coarse Registration Module\\n\\nThe pipeline of the proposed coarse registration module is shown in Fig. 3.1 to ensure that the content information is kept, we use an AffineNet whose output is the coarse aligned image. Then, multi-modal images are roughly aligned except for some local parallaxes, where an affine transformation model performs the global correction based on the affine transformation parameters. When the optimal deformation field $\\\\phi$ is obtained, we perform the same spatial transform on the coarse aligned image $C$ formed by shifting the fine registration and fusion are realized in a unified module/network, termed as fine registration and fusion module (F2M). Therefore, the loss function of AffineNet is defined to control the trade-off:\\n\\n$$L_{trans} = \\\\sum_{i,j} \\\\left\\\\| E_{trans}(x, y) \\\\right\\\\|_1$$\\n\\n$$L_{con} = \\\\sum_{i,j} \\\\left\\\\| E_{con}(x, y) \\\\right\\\\|_1$$\\n\\n$\\\\eta L_{trans} + (1-\\\\eta) L_{con}$ with a hyper-parameter $\\\\eta$ controlling the trade-off. Where $x$ and $y$ are a paired image set and the mapping result to domain $y$ is shown in Fig. 3.1. Coarse Registration Module\\n\\nThe pipeline of the proposed coarse registration module is shown in Fig. 3.1 to ensure that the content information is kept, we use an AffineNet whose output is the coarse aligned image. Then, multi-modal images are roughly aligned except for some local parallaxes, where an affine transformation model performs the global correction based on the affine transformation parameters. When the optimal deformation field $\\\\phi$ is obtained, we perform the same spatial transform on the coarse aligned image $C$ formed by shifting the fine registration and fusion are realized in a unified module/network, termed as fine registration and fusion module (F2M). Therefore, the loss function of AffineNet is defined to control the trade-off:\\n\\n$$L_{trans} = \\\\sum_{i,j} \\\\left\\\\| E_{trans}(x, y) \\\\right\\\\|_1$$\\n\\n$$L_{con} = \\\\sum_{i,j} \\\\left\\\\| E_{con}(x, y) \\\\right\\\\|_1$$\\n\\n$\\\\eta L_{trans} + (1-\\\\eta) L_{con}$ with a hyper-parameter $\\\\eta$ controlling the trade-off. Where $x$ and $y$ are a paired image set and the mapping result to domain $y$ is shown in Fig. 3.1. Coarse Registration Module\\n\\nThe pipeline of the proposed coarse registration module is shown in Fig. 3.1 to ensure that the content information is kept, we use an AffineNet whose output is the coarse aligned image. Then, multi-modal images are roughly aligned except for some local parallaxes, where an affine transformation model performs the global correction based on the affine transformation parameters. When the optimal deformation field $\\\\phi$ is obtained, we perform the same spatial transform on the coarse aligned image $C$ formed by shifting the fine registration and fusion are realized in a unified module/network, termed as fine registration and fusion module (F2M). Therefore, the loss function of AffineNet is defined to control the trade-off:\\n\\n$$L_{trans} = \\\\sum_{i,j} \\\\left\\\\| E_{trans}(x, y) \\\\right\\\\|_1$$\\n\\n$$L_{con} = \\\\sum_{i,j} \\\\left\\\\| E_{con}(x, y) \\\\right\\\\|_1$$\\n\\n$\\\\eta L_{trans} + (1-\\\\eta) L_{con}$ with a hyper-parameter $\\\\eta$ controlling the trade-off. Where $x$ and $y$ are a paired image set and the mapping result to domain $y$ is shown in Fig. 3.1. Coarse Registration Module\\n\\nThe pipeline of the proposed coarse registration module is shown in Fig. 3.1 to ensure that the content information is kept, we use an AffineNet whose output is the coarse aligned image. Then, multi-modal images are roughly aligned except for some local parallaxes, where an affine transformation model performs the global correction based on the affine transformation parameters. When the optimal deformation field $\\\\phi$ is obtained, we perform the same spatial transform on the coarse aligned image $C$ formed by shifting the fine registration and fusion are realized in a unified module/network, termed as fine registration and fusion module (F2M). Therefore, the loss function of AffineNet is defined to control the trade-off:\\n\\n$$L_{trans} = \\\\sum_{i,j} \\\\left\\\\| E_{trans}(x, y) \\\\right\\\\|_1$$\\n\\n$$L_{con} = \\\\sum_{i,j} \\\\left\\\\| E_{con}(x, y) \\\\right\\\\|_1$$\\n\\n$\\\\eta L_{trans} + (1-\\\\eta) L_{con}$ with a hyper-parameter $\\\\eta$ controlling the trade-off. Where $x$ and $y$ are a paired image set and the mapping result to domain $y$ is shown in Fig. 3.1. Coarse Registration Module\\n\\nThe pipeline of the proposed coarse registration module is shown in Fig. 3.1 to ensure that the content information is kept, we use an AffineNet whose output is the coarse aligned image. Then, multi-modal images are roughly aligned except for some local parallaxes, where an affine transformation model performs the global correction based on the affine transformation parameters. When the optimal deformation field $\\\\phi$ is obtained, we perform the same spatial transform on the coarse aligned image $C$ formed by shifting the fine registration and fusion are realized in a unified module/network, termed as fine registration and fusion module (F2M). Therefore, the loss function of AffineNet is defined to control the trade-off:\\n\\n$$L_{trans} = \\\\sum_{i,j} \\\\left\\\\| E_{trans}(x, y) \\\\right\\\\|_1$$\\n\\n$$L_{con} = \\\\sum_{i,j} \\\\left\\\\| E_{con}(x, y) \\\\right\\\\|_1$$\\n\\n$\\\\eta L_{trans} + (1-\\\\eta) L_{con}$ with a hyper-parameter $\\\\eta$ controlling the trade-off. Where $x$ and $y$ are a paired image set and the mapping result to domain $y$ is shown in Fig. 3.1. Coarse Registration Module\\n\\nThe pipeline of the proposed coarse registration module is shown in Fig. 3.1 to ensure that the content information is kept, we use an AffineNet whose output is the coarse aligned image. Then, multi-modal images are roughly aligned except for some local parallaxes, where an affine transformation model performs the global correction based on the affine transformation parameters. When the optimal deformation field $\\\\phi$ is obtained, we perform the same spatial transform on the coarse aligned image $C$ formed by shifting the fine registration and fusion are realized in a unified module/network, termed as fine registration and fusion module (F2M). Therefore, the loss function of AffineNet is defined to control the trade-off:\\n\\n$$L_{trans} = \\\\sum_{i,j} \\\\left\\\\| E_{trans}(x, y) \\\\right\\\\|_1$$\\n\\n$$L_{con} = \\\\sum_{i,j} \\\\left\\\\| E_{con}(x, y) \\\\right\\\\|_1$$\\n\\n$\\\\eta L_{trans} + (1-\\\\eta) L_{con}$ with a hyper-parameter $\\\\eta$ controlling the trade-off. Where $x$ and $y$ are a paired image set and the mapping result to domain $y$ is shown in Fig. 3.1. Coarse Registration Module\\n\\nThe pipeline of the proposed coarse registration module is shown in Fig. 3.1 to ensure that the content information is kept, we use an AffineNet whose output is the coarse aligned image. Then, multi-modal images are roughly aligned except for some local parallaxes, where an affine transformation model performs the global correction based on the affine transformation parameters. When the optimal deformation field $\\\\phi$ is obtained, we perform the same spatial transform on the coarse aligned image $C$ formed by shifting the fine registration and fusion are realized in a unified module/network, termed as fine registration and fusion module (F2M). Therefore, the loss function of AffineNet is defined to control the trade-off:\\n\\n$$L_{trans} = \\\\sum_{i,j} \\\\left\\\\| E_{trans}(x, y) \\\\right\\\\|_1$$\\n\\n$$L_{con} = \\\\sum_{i,j} \\\\left\\\\| E_{con}(x, y) \\\\right\\\\|_1$$\\n\\n$\\\\eta L_{trans} + (1-\\\\eta) L_{con}$ with a hyper-parameter $\\\\eta$ controlling the trade-off. Where $x$ and $y$ are a paired image set and the mapping result to domain $y$ is shown in Fig. 3.1. Coarse Registration Module\\n\\nThe pipeline of the proposed coarse registration module is shown in Fig. 3.1 to ensure that the content information is kept, we use an AffineNet whose output is the coarse aligned image. Then, multi-modal images are roughly aligned except for some local parallaxes, where an affine transformation model performs the global correction based on the affine transformation parameters. When the optimal deformation field $\\\\phi$ is obtained, we perform the same spatial transform on the coarse aligned image $C$ formed by shifting the fine registration and fusion are realized in a unified module/network, termed as fine registration and fusion module (F2M). Therefore, the loss function of AffineNet is defined to control the trade-off:\\n\\n$$L_{trans} = \\\\sum_{i,j} \\\\left\\\\| E_{trans}(x, y) \\\\right\\\\|_1$$\\n\\n$$L_{con} = \\\\sum_{i,j} \\\\left\\\\| E_{con}(x, y) \\\\right\\\\|_1$$\\n\\n$\\\\eta L_{trans} + (1-\\\\eta) L_{con}$ with a hyper-parameter $\\\\eta$ controlling the trade-off. Where $x$ and $y$ are a paired image set and the mapping result to domain $y$ is shown in Fig. 3.1. Coarse Registration Module\\n\\nThe pipeline of the proposed coarse registration module is shown in Fig. 3.1 to ensure that the content information is kept, we use an AffineNet whose output is the coarse aligned image. Then, multi-modal images are roughly aligned except for some local parallaxes, where an affine transformation model performs"}
{"id": "CVPR-2022-1487", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Procedure of the coarse registration module to generating the coarse deformation field.\\n\\nFigure 4. Multi-modal image translation configurations.\\n\\nto alleviate this problem. Thus, deformable convolution layers are applied to replace the regular receptive fields in traditional convolution layers. Deformable convolution layers augment receptive fields with offsets, which are learned from additional convolution layers from preceding feature maps. Thus, it refers to deformations in unregistered images for higher registration accuracy and stronger robustness.\\n\\n3.2. Mutually Reinforcing Fine Registration and Fusion Module (F2M)\\n\\nIn the first phase, F2M realizes the texture-focused image fusion, which is also the foundation of fine registration. The pipeline is shown in Fig. 5. We optimize the parameters in F2M for image fusion except those in the deformation block. The deformation block depends on initialized parameters to generate the deformation field, which automatically tends to be identical. In this case, \\\\( I_f \\\\) combines the scene information of \\\\( I_C \\\\) and \\\\( I_y \\\\), and renders their paralaxs in a single image. Loss function is defined as:\\n\\n\\\\[\\nL_{\\\\text{fus}} = L_{\\\\text{content}} + \\\\delta L_{\\\\text{gradient}},\\n\\\\]\\n\\nwhere \\\\( \\\\delta \\\\) controls the trade-off between these two terms.\\n\\n\\\\[\\nL_{\\\\text{content}} = (1 - \\\\gamma) \\\\| I_f - I_C \\\\|_1 + \\\\gamma \\\\| I_f - I_y \\\\|_1.\\n\\\\]\\n\\nAs the NIR images usually contain more texture details than RGB images, \\\\( \\\\gamma \\\\) is set to a value between 0.5 and 1. As salient structures are usually presented in larger gradients, the gradient loss \\\\( L_{\\\\text{gradient}} \\\\) is defined as:\\n\\n\\\\[\\nL_{\\\\text{gradient}} = \\\\left\\\\| \\\\nabla I_f - \\\\nabla I_C + \\\\nabla I_y \\\\right\\\\|_2.\\n\\\\]\\n\\nwhere \\\\( \\\\nabla \\\\) denotes the gradient of an image.\\n\\nIn the second phase, F2M realizes the fine registration based on the characteristics of fused images. In this phase, we fix the fusion-related parameters which have been optimized in the first phase and train the deformation block. The loss function considers the following three aspects. First, \\\\( I_y \\\\) is the fixed image providing the reference texture information. \\\\( I_f \\\\) retains the deformed gradients of \\\\( I_C \\\\). After the correct deformation, \\\\( \\\\nabla I_f \\\\) should demonstrate high consistency with \\\\( \\\\nabla I_y \\\\). Thus, the first term constrains the consistency with the reference information. Second, it is easy to observe that any misalignments in \\\\( I_f \\\\) will decrease the sparsity of gradients. We use the second term to encourage the sparsity of \\\\( \\\\nabla I_f \\\\) and penalize the salient gradients that should be corrected. Third, it is clear that neighboring pixels should have similar deformations, intuitively represented by the smoothness of the deformation field. Otherwise, the scene structure will be distorted. We deploy a regularization term to prevent the deformation block from generating non-smooth deformation fields. Therefore, the loss function contains the following three terms:\\n\\n\\\\[\\nL_{\\\\text{defor}} = \\\\| \\\\nabla I_f - \\\\nabla I_y \\\\|_1 + \\\\| \\\\nabla I_f \\\\|_1 + \\\\lambda L_{\\\\text{smooth}},\\n\\\\]\\n\\nwhere we use the \\\\( l_1 \\\\)-norm as it encourages sparsity. Specifically to \\\\( L_{\\\\text{smooth}} \\\\), denoting the deformation as \\\\( \\\\phi_f \\\\), the first order gradients of \\\\( \\\\phi_f \\\\) reflect the abrupt changes of the deformation. Besides, to avoid over-smoothing, inspired by \\\\( [1] \\\\), a bilateral filter \\\\( [24] \\\\) is used to assign variable weights to different first-order changes, defined as:\\n\\n\\\\[\\nL_{\\\\text{smooth}} = \\\\sum_{p \\\\in R} e^{-\\\\alpha |I_f(p) - I_f(p_n)|} \\\\cdot |\\\\phi_f(p) - \\\\phi_f(p_n)|,\\n\\\\]\\n\\nwhere \\\\( p \\\\) is the position index of a pixel in \\\\( I_f \\\\) or \\\\( \\\\phi_f \\\\). \\\\( R \\\\) denotes a set of neighbors of \\\\( p \\\\). \\\\( p_n \\\\) represents the position index in this set. \\\\( \\\\alpha \\\\) is a coefficient and set to 0.5.\\n\\nWhen the deformation block has been optimized, we once again perform the forward process of F2M entirely to generate the final aligned and fused image \\\\( I_f \\\\).\\n\\nNetwork Architecture. As shown in Fig. 5, we share the weights of the first three layers to ensure the intensity consistency of feature types from different modalities. It avoids the attenuation and diffusion of information in one source image compared to the other one. Otherwise, the attenuation and diffusion will cause the fake gradient sparsity and affect the improvement of registration performance.\"}"}
