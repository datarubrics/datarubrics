{"id": "CVPR-2024-1141", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"IS-FUSION: Instance-Scene Collaborative Fusion for Multimodal 3D Object Detection\\n\\nJunbo Yin, Jianbing Shen, Runnan Chen, Wei Li*, Ruigang Yang, Pascal Frossard, Wenguan Wang*\\n\\n1. School of Computer Science and Technology, Beijing Institute of Technology\\n2. SKL-IOTSC, CIS, University of Macau\\n3. The University of Hong Kong\\n4. Inceptio\\n5. \u00b4Ecole Polytechnique F\u00e9d\u00e9rale de Lausanne (EPFL)\\n6. ReLER, CCAI, Zhejiang University\\n\\n{yinjunbo.cn, wenguanwang.ai}@gmail.com\\n\\nAbstract\\n\\nBird's eye view (BEV) representation has emerged as a dominant solution for describing 3D space in autonomous driving scenarios. However, objects in the BEV representation typically exhibit small sizes, and the associated point cloud context is inherently sparse, which leads to great challenges for reliable 3D perception. In this paper, we propose IS-FUSION, an innovative multimodal fusion framework that jointly captures the Instance- and Scene-level contextual information.\\n\\nIS-FUSION essentially differs from existing approaches that only focus on the BEV scene-level fusion by explicitly incorporating instance-level multimodal information, thus facilitating the instance-centric tasks like 3D object detection. It comprises a Hierarchical Scene Fusion (HSF) module and an Instance-Guided Fusion (IGF) module. HSF applies Point-to-Grid and Grid-to-Region transformers to capture the multimodal scene context at different granularities. IGF mines instance candidates, explores their relationships, and aggregates the local multimodal context for each instance. These instances then serve as guidance to enhance the scene feature and yield an instance-aware BEV representation. On the challenging nuScenes benchmark, IS-FUSION outperforms all the published multimodal works to date. Code is available at: https://github.com/yinjunbo/IS-Fusion.\\n\\n1. Introduction\\n\\n3D object detection [23, 49, 53, 82, 88, 93] is a critical task in various applications such as autonomous driving and robotics. Over the past few years, tremendous progress has been achieved in point cloud-based 3D object detection, due to the effective 3D neural network models [5, 17, 18, 19, 55, 87]. While point clouds, typically captured by depth-aware sensors such as LiDAR, provide valuable geometric information about the 3D space, they often lack detailed texture descriptions and are sparsely distributed over long distances, e.g., beyond 100 meters in outdoor scenarios like nuScenes [2]. To tackle these limitations, a recent trend is to perform multimodal 3D object detection [33, 34, 40, 46, 77] by fusing information from both point clouds and synchronized multi-view images. The image modality provides detailed texture and dense semantic information [20, 70], which complements the sparse point cloud and thus enhances the 3D perception capacity.\\n\\nTo handle heterogeneous data from different modalities, existing approaches [40, 42, 46] typically pre-define a unified space that is compatible with both modalities (i.e., the bird's eye view (BEV) in the ego-vehicle coordinate system), and then perform feature alignment and fusion on this shared space. BEV representation simplifies the complex 3D space into a 2D plane, making it easier to understand the scene. However, performing fusion from the entire BEV scene level ignores the inherent difference between the foreground instances and background regions, which may undermine the performance. For example, object instances represented in the BEV often exhibit smaller sizes compared to those observed in natural images. Additionally, the number of BEV grid cells occupied by foreground instances is significantly lower than those occupied by background regions, leading to an inefficient utilization of the scene representation.\"}"}
{"id": "CVPR-2024-1141", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ground ones, leading to a severe imbalance between foreground and background samples. As a result, the above approaches struggle to capture local context around the object instances, or largely rely on additional networks in the decoding stage to iteratively refine the detections [1, 77].\\n\\nWhile a few methods [3,71] aim to perform object-level encoding, they ignore the potential collaboration between the scene and instance features. For example, a false negative object in a scene can be potentially rectified by enhancing its feature through interactions with the instances sharing similar semantic information. Therefore, it remains an open question how to simultaneously formulate the instance-level and scene-level context, as well as elegantly integrate them by leveraging multimodal fusion.\\n\\nIn this work, we present a new multimodal detection framework, IS-FUSION, to tackle the above challenge. As shown in Fig. 1, IS-FUSION explores both the Instance-level and Scene-level Fusion, as well as encourages the interaction between the instance and scene features to strengthen the overall representation. It consists of two crucial components: the Hierarchical Scene Fusion (HSF) module and the Instance-Guided Fusion (IGF) module. HSF aims to capture scene features at various granularities by utilizing Point-to-Grid and Grid-to-Region transformers. This also enables the generation of high-quality instance-level features that are vital for IGF. In IGF, the foreground instance candidates are determined by the heatmap scores of the scene feature; meanwhile, an inter-instance self-attention is employed to capture the instance relationships. These instances then aggregate essential semantic information from the multimodal context through deformable attention. Furthermore, we incorporate an Instance-to-Scene transformer attention to enforce the local instance features to collaborate with the global scene feature. This yields an enhanced BEV representation that is better suited for instance-aware tasks like 3D object detection.\\n\\nIn summary, IS-FUSION provides a new insight into existing multimodal 3D detection approaches that focus on scene-level fusion. By incorporating HSF and IGF, it explicitly promotes collaboration between scene- and instance-level features, thereby ensuring comprehensive representation and yielding improved detection results. Extensive experiments on the competitive nuScenes [2] dataset demonstrate that IS-FUSION attains the best performance among all the published 3D object detection works. For example, it achieves 72.8% mAP on the nuScenes validation set, outperforming prior art BEVFusion [46] by 4.3% mAP. It also surpasses concurrent works like CMT [74] and SparseFusion [71] by 2.5% and 1.8% mAP, respectively.\\n\\n2. Related Work\\n\\nLiDAR-based 3D Object Detection.\\n\\nLiDAR sensors are essential for advanced autonomous driving due to their capacity of perceiving objects in 3D space even in adverse illumination and weather environments, where they are usually more reliable than camera sensors [16, 21, 30, 37, 44, 67, 68]. Current LiDAR-based detection approaches can be broadly classified into three categories according to the various encoding formats of point cloud: point-based [15, 52, 58, 59, 78, 83, 89], voxel-based [22, 29, 32, 69, 81, 90] and point-voxel fusion networks [10, 25, 50, 57, 79]. Shi et al. [58] propose an early work for point-wise 3D detection by extending PointNet [54, 55] backbone with a two-stage proposal refinement network. Due to the huge computation overhead in large-scale scenes with more than 100k points, point downsampling operations [78] have to be applied. A more popular solution is to use the voxel-based representation, where the point clouds are quantified by regular grids such that standard convolutional networks can be directly applied. VoxelNet [90] is the seminal work that exploits the 3D convolutional network that is later optimized in [75] with sparse convolution. In addition, there are also some works like [57] exploring joint point-voxel representation by enhancing the region proposals with the raw points information, while they often require multiple stages to refine the 3D proposals. These LiDAR-only 3D detectors usually operate with sparse and noisy context provided by point cloud data. However, in challenging scenarios where objects have low reflectivity, small sizes or are heavily occluded, relying solely on point cloud data may lead to inaccurate detection [4, 43, 48]. Therefore, our focus is to explore the multimodal context by incorporating the merits of both geometry-aware point clouds and semantic-rich images to guarantee advanced 3D object detection capability.\\n\\nLiDAR-camera Fusion for 3D Object Detection.\\n\\nMultimodal 3D object detection [6, 38, 39, 72] has recently received considerable attention. It also has been proven that multimodal learning can yield a more accurate latent space representation [26] compared to the unimodal learning. Multimodal fusion approaches for 3D object detection basically comprise early fusion [12, 63, 63, 73, 85], middle fusion [34, 35, 36, 40, 46, 51, 77, 86] and late fusion [1, 6, 27, 31, 38, 76], which are categorized based on the stages at which data fusion occurs. The works in [63, 64] are pioneering efforts of early fusion that enhance input points with corresponding image pixel features. Later, Chen et al. [12] propose to fuse point cloud and image features at the voxel level and aggregate information from multiple sampling points. However, the early fusion approaches are more sensitive to potential calibration errors. Late fusion approaches like [6, 27] typically fuse multimodal information at the region proposal level, where the region proposals are usually generated separately by modality-specific encoders. These approaches may result in limited interactions between modalities during detection.\"}"}
{"id": "CVPR-2024-1141", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Overview of our IS-FUSION framework. Multimodal inputs including a point cloud and multi-view images are first processed by modality-specific encoders to obtain initial features. Then, the HSF module, equipped with Point-to-Grid and Grid-to-Region transformers, utilizes these features to generate a scene-level feature with hierarchical context. Furthermore, the IGF module identifies the top-K salient instances and aggregates the multimodal context for each instance. Finally, these instances are employed by the Instance-to-Scene transformer to propagate valuable information to the scene, producing the final BEV representation with improved instance awareness.\\n\\n3. Methodology\\n\\nWe first introduce the general overview of the proposed IS-FUSION in Sec. 3.1. Next, we delve into the details of the HSF module in Sec. 3.2. After that, in Sec. 3.3, we elaborate on the crucial design steps of the IGF module.\\n\\n3.1. Overall Framework\\n\\nAs illustrated in Fig. 2, each scene is represented by a LiDAR point cloud $P$, along with synchronized RGB images $I = \\\\{I_1, I_2, \\\\ldots, I_N\\\\}$ captured by $N$ cameras that are well-calibrated with the LiDAR sensor. Our goal is to devise a detection model capable of producing precise 3D bounding boxes $Y$, given multimodal inputs $(P, I)$. Formally, the proposed IS-FUSION model is defined by:\\n\\n$$Y = f_{dec}(f_{enc}(f_{point}(P), f_{img}(I)))$$\\n\\nwhere $f_{point}(\\\\cdot)$ and $f_{img}(\\\\cdot)$ serve as the input encoding modules, $f_{enc}(\\\\cdot)$ denotes the multimodal encoder (formed by HSF and IGF) and $f_{dec}(\\\\cdot)$ is the decoder.\\n\\nMultimodal Input Encoding.\\n\\nTo handle inputs from heterogeneous modalities, we first utilize modality-specific encoders to get their respective initial representation, i.e., $B_P = f_{point}(P)$ and $F_I = f_{img}(I)$. Following [46, 77], we instantiate $f_{point}(\\\\cdot)$ with VoxelNet [90], and $f_{img}(\\\\cdot)$ by Swin-Transformer [45]. This yields the point cloud BEV feature $B_P$ and the image Perspective-View (PV) features $F_I$. In particular, $B_P \\\\in \\\\mathbb{R}^{W \\\\times H \\\\times C}$ is obtained by compressing the height dimension of the 3D voxel feature as in [90], where $W$ and $H$ are the numbers of BEV grid cells along the $x$ and $y$ axes, and $C$ denotes the channel dimension.\\n\\nMultimodal Encoder.\\n\\nThe multimodal encoder $f_{enc}(\\\\cdot)$ conducts cross-modality feature fusion between $B_P$ and $F_I$ to yield a fused BEV feature $\\\\hat{B}_F \\\\in \\\\mathbb{R}^{W \\\\times H \\\\times C}$. In contrast to previous multimodal encoders that only focus on fusion at the entire scene level [46, 77], we develop both instance-level and scene-level representations. To this end, we design $f_{enc}(\\\\cdot)$ using two modules, namely, HSF module $f_{HSF}(\\\\cdot)$ and IGF module $f_{IGF}(\\\\cdot)$:\\n\\n$$\\\\hat{B}_F = f_{enc}(B_P, F_I) = f_{IGF}(f_{HSF}(B_P, F_I))$$\\n\\nwhere $f_{HSF}(\\\\cdot)$ generates multi-granularity scene feature, while $f_{IGF}(\\\\cdot)$ further integrates crucial information about foreground instances. We will elaborate on $f_{HSF}(\\\\cdot)$ and $f_{IGF}(\\\\cdot)$ in Sec. 3.2 and Sec. 3.3, respectively.\\n\\nMultimodal Decoder.\\n\\nThe multimodal decoder aims to yield the final 3D detections $Y$ based on the BEV representation $\\\\hat{B}_F$, given by $Y = f_{dec}(\\\\hat{B}_F)$. In our work, $f_{dec}(\\\\cdot)$ is...\"}"}
{"id": "CVPR-2024-1141", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. Hierarchical Scene Fusion\\n\\nLet us denote \\\\( I_{\\\\text{grid}} \\\\) as the BEV grid cells obtained by dividing the point cloud \\\\( \\\\mathcal{P} \\\\) into a set of grid-wise features \\\\( \\\\{ g_{\\\\text{grid}} \\\\} \\\\), given by:\\n\\n\\\\[\\nI_{\\\\text{grid}} = f_{\\\\text{P2G}}(\\\\mathcal{P})\\n\\\\]\\n\\nEach grid cell \\\\( g_{\\\\text{grid}} \\\\) contains a subset of points within a pillar. This enables each point to consider the inter-point/pixel correlations in its local region. In this way, we get the point-wise feature \\\\( f_{\\\\text{point}}(p_{\\\\text{point}}) \\\\) of the points within a pillar, which further mines the inter-grid point-level features into the grid-level features with the Point-to-Grid transformer.\\n\\nTo handle the potential calibration noise between LiDAR sensors and the image plane, and to simultaneously consider the inter-view context, we merge the point-wise information \\\\( f_{\\\\text{point}}(p_{\\\\text{point}}) \\\\) and the distribution of objects. HSF fully leverages various representation granularities, as illustrated in Fig. 3.\\n\\nGiven the point cloud BEV feature \\\\( \\\\mathcal{P} \\\\) and the image PV, we suggest a Hierarchical Scene Fusion (HSF) module to integrate different feature granularities and obtain the fused BEV feature \\\\( I_{\\\\text{BEV}} \\\\), and its corresponding PV feature \\\\( I_{\\\\text{PV}} \\\\), as follows:\\n\\n\\\\[\\nI_{\\\\text{BEV}} = f_{\\\\text{HSF}}(\\\\mathcal{P}, \\\\mathcal{I}_{\\\\text{PV}})\\n\\\\]\\n\\nIt first aggregates the point-wise features \\\\( f_{\\\\text{point}}(p_{\\\\text{point}}) \\\\) into a BEV grid-wise feature. For each BEV grid, while \\\\( f_{\\\\text{MSA}}(g_{\\\\text{grid}}) \\\\) is the multi-head self-attention [62], and \\\\( f_{\\\\text{proj}}(g_{\\\\text{grid}}) \\\\) is the projection process from point to grid, we further explore the inter-grid and inter-region multimodal scene context. The intuition is that different feature granularities capture scene context at different levels. For example, at the point level, each element \\\\( p_{\\\\text{point}} \\\\) is a point, while at the grid level, each element \\\\( g_{\\\\text{grid}} \\\\) is a grid-wise feature that will be assigned to the image BEV feature \\\\( I_{\\\\text{BEV}} \\\\). To be specific, \\\\( f_{\\\\text{R2G}}(g_{\\\\text{grid}}) \\\\) is the enhanced BEV feature.\\n\\n\\\\[\\n\\\\begin{align*}\\nI_{\\\\text{BEV}} & = f_{\\\\text{HSF}}(\\\\mathcal{P}, \\\\mathcal{I}_{\\\\text{PV}}) \\\\\\\\\\n\\\\mathcal{I}_{\\\\text{BEV}} & = f_{\\\\text{R2G}}(g_{\\\\text{grid}})\\n\\\\end{align*}\\n\\\\]\\n\\nHere, \\\\( f_{\\\\text{HSF}}(\\\\mathcal{P}, \\\\mathcal{I}_{\\\\text{PV}}) \\\\) considers the inter-point/pixel correlations in \\\\( \\\\mathcal{P} \\\\) and the inter-grid/pixel correlations in \\\\( \\\\mathcal{I}_{\\\\text{PV}} \\\\). It aggregates them into a BEV grid-wise feature. Each grid cell \\\\( g_{\\\\text{grid}} \\\\) is a grid-wise feature that will be assigned to the image BEV feature by applying a max pooling operation. Afterwards, we merge the point-wise information \\\\( f_{\\\\text{point}}(p_{\\\\text{point}}) \\\\) and obtain the fused BEV feature \\\\( \\\\mathcal{I}_{\\\\text{BEV}} \\\\) and Grid-to-Region Transformer.\\n\\nIn addition to the Point-to-Grid transformer that models the inter-point dependencies, we further explore the inter-grid and inter-region relationships via the Grid-to-Region transformer to capture the global scene context. This can be denoted as \\\\( \\\\mathcal{I}_{\\\\text{grid}} \\\\). To be specific, we project the point cloud BEV feature \\\\( \\\\mathcal{P} \\\\) and retrieve their pixel-level features:\\n\\n\\\\[\\n\\\\mathcal{I}_{\\\\text{grid}} = f_{\\\\text{G2R}}(\\\\mathcal{P})\\n\\\\]\\n\\nHere, \\\\( f_{\\\\text{G2R}}(\\\\mathcal{P}) \\\\) is a grid-wise feature that will be assigned to the image BEV feature \\\\( I_{\\\\text{BEV}} \\\\) and obtained by applying global bilinear interpolation function computing features at non-integer coordinates. In this way, we get the point-wise feature \\\\( f_{\\\\text{point}}(p_{\\\\text{point}}) \\\\) of the points within a pillar. This enables each point to consider the inter-grid point-level features into the grid-level features with the Point-to-Grid transformer.\"}"}
{"id": "CVPR-2024-1141", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Instance candidates are selected as the keypoint detection head on the scene feature. We implement Instance Candidates Selection and explain feature context for each instance and yield a set of instance features \\\\( \\\\{ f \\\\} \\\\) for \\\\( q = [ f ] \\\\). An additional linear layer is employed to embed each instance, to represent the corresponding instances. Meanwhile, we keep the top-\\\\( K \\\\) object with the highest centerness scores to optimize this prediction head. During inference, we get, and the peak location is determined by the BEV projection of Gaussian distribution is defined for each instance as the target grid cells. Then, we capture the interactions between different regions before the shift, thus rearranging all the grids coming from various regions after the shift. This allows each grid to interact with the relevant instances. Formally, given the scene feature, we can rectify this by comparing it with all the enriched BEV feature map \\\\( \\\\tilde{B} \\\\) of \\\\( \\\\triangle_{ins} \\\\) in [45], and \\\\( f \\\\) where \\\\( \\\\hat{f} \\\\) is incorrectly categorized as part of the background in the scene feature, we can rectify this by comparing it with all the attentive grid features \\\\( g \\\\). After flattening \\\\( \\\\tilde{B} \\\\), the enriched BEV feature map is \\n\\nIllustration of IGF module. Figure 4.\\n\\n Instance Context Aggregation.\\n\\nFinally, we capture the interactions between different regions by merging the augmented \\\\( \\\\hat{F} \\\\) of \\\\( \\\\hat{f} \\\\). This enables the propagation of information from individual points to different BEV regions. This facilitates the integration of both local and global multimodal scene contexts.\\n\\nCross-Attention\\nAdd & Norm\\nSelf-Attention\\nInstance Context\\nAggregation\\nInstance Candidates...\"}"}
{"id": "CVPR-2024-1141", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method          | Modality | mAP (L+C) | Construction Vehicle | Trailer | Barrier | Motorcycle | Pedestrian | Traffic Cone |\\n|-----------------|----------|-----------|----------------------|---------|---------|------------|------------|--------------|\\n| CenterPoint     |          | 84.6      | 51.0                 | 17.5    | 60.2    | 53.2       | 70.9       | 53.7         |\\n| Focals Conv     |          | 86.7      | 56.3                 | 23.8    | 67.7    | 59.5       | 74.1       | 64.5         |\\n| VoxelNeXt      |          | 84.6      | 53.0                 | 28.7    | 64.7    | 55.8       | 74.6       | 73.2         |\\n| TransFusion-L   |          | 86.2      | 56.7                 | 28.2    | 66.3    | 58.8       | 78.2       | 68.3         |\\n| PillarNet-34    |          | 87.6      | 57.5                 | 27.9    | 63.6    | 63.1       | 77.2       | 70.1         |\\n| FocalFormer3D   |          | 87.2      | 57.1                 | 34.4    | 69.6    | 64.9       | 77.8       | 76.2         |\\n| MVP             |          | 86.8      | 58.5                 | 26.1    | 67.4    | 57.3       | 74.8       | 70.0         |\\n| GraphAlign      |          | 87.6      | 57.7                 | 26.1    | 66.2    | 57.8       | 74.1       | 72.5         |\\n| PointAug.       |          | 87.5      | 57.3                 | 28.0    | 65.2    | 60.7       | 72.6       | 74.3         |\\n| UVTR            |          | 87.5      | 56.0                 | 33.8    | 67.5    | 59.5       | 73.0       | 73.4         |\\n| AutoAlignV2     |          | 88.2      | 59.0                 | 33.1    | 69.3    | 59.3       | 73.9       | 74.3         |\\n| TransFusion-LC  |          | 87.1      | 60.0                 | 33.1    | 68.3    | 60.8       | 78.1       | 73.6         |\\n| BEVFusion       |          | 88.1      | 60.9                 | 34.4    | 69.3    | 62.1       | 80.0       | 74.1         |\\n| BEVFusion\u2020      |          | 88.6      | 60.1                 | 39.3    | 69.8    | 63.8       | 80.0       | 75.0         |\\n| DeepInteraction |          | 87.9      | 60.2                 | 37.5    | 70.8    | 63.8       | 80.4       | 75.4         |\\n| UniTR           |          | 89.4      | 60.2                 | 39.2    | 72.2    | 65.1       | 76.8       | 75.8         |\\n| ObjectFusion    |          | 90.7      | 59.0                 | 40.5    | 71.8    | 63.1       | 80.0       | 78.1         |\\n| MSMDFusion      |          | 90.6      | 61.0                 | 35.2    | 71.4    | 64.2       | 80.7       | 76.9         |\\n| FocalFormer3D\u2020  |          | 85.3      | 61.4                 | 35.9    | 71.7    | 66.4       | 79.3       | 80.3         |\\n| SparseFusion    |          | 87.9      | 60.2                 | 38.7    | 72.0    | 64.9       | 79.2       | 78.5         |\\n| CMT             |          | 84.7      | 63.3                 | 37.3    | 75.4    | 65.4       | 78.2       | 79.1         |\\n| IS-FUSION       |          | 89.2      | 62.7                 | 38.4    | 74.9    | 67.3       | 78.1       | 82.4         |\\n| IS-FUSION\u2020      |          | 91.1      | 67.8                 | 44.5    | 77.6    | 68.3       | 81.8       | 85.3         |\\n\\nTable 1. 3D Object Detection Performance on the nuScenes test set. 'L' is the LiDAR and 'C' denotes the camera. 'C.V.', 'T.L.', 'B.R.', 'M.T.', 'Ped.', and 'T.C.' indicate the construction vehicle, trailer, barrier, motorcycle, pedestrian, and traffic cone, respectively. \u2020 denotes the model with test-time augmentation and model ensemble techniques. The best results in each column are marked in bold font. IS-FUSION achieves superior performance compared to all the other published 3D detection works.\\n\\nall the grid cells, we rearrange the obtained grid features \\\\{\\\\^g_1, \\\\^g_2, \\\\ldots, \\\\^g_{W \\\\times H}\\\\} back into a BEV feature \\\\^B \u2208 R_{W \\\\times H \\\\times C}, which will be employed in the subsequent decoding stage to produce the final 3D detections.\\n\\n4. Experiments\\n\\n4.1. Experimental Setup\\n\\nDataset. We evaluate the 3D object detection performance of the proposed IS-FUSION by comparing it with other state-of-the-art approaches on the nuScenes benchmark [2]. nuScenes is a very challenging large-scale autonomous driving dataset that is widely used for evaluating multi-modality 3D object detectors. It provides 700, 150, and 150 scene sequences for training, validation, and testing, respectively. Each sequence in the dataset consists of approximately 40 frames of annotated LiDAR point cloud data, and each point cloud data is accompanied by six calibrated image data covering 360\u00b0 field of view. It requires detecting 10 object categories that are commonly observed in driving scenarios. The evaluation of 3D object detection is based on two key metrics: mean Average Precision (mAP) and nuScenes detection scores (NDS). In particular, NDS is a comprehensive metric that consolidates object translation, scale, orientation, velocity and attribute.\\n\\nNetwork Architecture. Our implementation follows the open-source framework MMDetection3D [13]. Specifically, the point cloud covers [-54 m, 54 m] along the X and Y axes, and [-5 m, 3 m] along the Z axis, with a voxel size of (0.075 m, 0.075 m, 0.2 m). In the Point-to-Grid transformer, we set the pillar size to (0.6 m, 0.6 m, 8.0 m). The input resolution of multi-view images is set to 384 \u00d7 1024. The BEV feature map is of size 180 \u00d7 180. In HSF, we define the point number L as 20 and the region size M as 6. In IGF, the number of instance candidates K is set to 200. The number of sampling locations D on the multimodal feature is set to 16. For the model ensemble, multiple models are utilized with voxel sizes ranging from (0.05m, 0.05m, 0.2m) to (0.125m, 0.125m, 0.2m) with intervals of 0.025m. For the test-time augmentation, we apply double flipping and rotations (i.e., \\\\{0\u00b0, \u00b122.5\u00b0, \u00b145\u00b0, \u00b1180\u00b0\\\\}) on the input point clouds.\\n\\nTraining. The image encoder is pre-trained on the nuImage dataset [2] following current approaches [1, 46, 77]. The full model is trained end-to-end for 10 epochs with the AdamW optimizer [47]. Meanwhile, the once-cycle learning policy [60] is employed with a maximum learning rate of 1e\u22123. The class-balanced sampling strategy from CBGS [91] and the cross-modal data augmentation from AutoAlignV2 [12] are adopted during training. The design of the 3D decoder follows the common practices of leading approaches, such as TransFusion-L [1] and BEVFusion [46], where we decode the top 200 bounding boxes.\"}"}
{"id": "CVPR-2024-1141", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Examples of 3D object detections on nuScenes validation set. We visualize the 3D bounding boxes of car, pedestrian and bicycle with orange, blue and red colors in the multi-view images. In the point cloud, the predictions are in gray and GTs are in green.\\n\\n| Method       | Image Encoder | mAP  | NDS  | FPS  |\\n|--------------|---------------|------|------|------|\\n| FUTR3D [7]   | ResNet-101    | 64.2 | 68.0 | 2.3  |\\n| TransFusion-LC [1] | ResNet-50     | 67.5 | 71.3 | 3.2  |\\n| BEVFusion [46] | Swin-T        | 68.5 | 71.4 | 4.2  |\\n| DeepInteraction [77] | Swin-T      | 69.9 | 72.6 | 2.6  |\\n| CMT [74]     | VoV-99        | 70.3 | 72.9 | 3.8  |\\n| SparseFusion [71] | Swin-T       | 71.0 | 73.1 | 5.3  |\\n| IS-FUSION (Ours) | Swin-T       | 72.8 | 74.0 | 3.2  |\\n\\nTable 2. Performance comparison on the nuScenes validation set. IS-FUSION achieves superior 3D detection performance while maintaining a comparable inference speed.\\n\\n4.2. Performance Benchmarking\\n\\nIn Table 1, we benchmark the performance of our model against current leading LiDAR-based (indicated by 'L') and multimodal (indicated by 'L+C') 3D object detectors on the nuScenes test set. It demonstrates that IS-FUSION outperforms all existing state-of-the-art (SOTA) 3D detection algorithms. Specifically, the LiDAR-only baseline of IS-FUSION is built upon TransFusion-L [1]. By exploring instance-scene collaborative fusion, IS-FUSION significantly improves it by 7.5% in mAP and 5.0% in NDS, respectively. Furthermore, IS-FUSION demonstrates superior performance compared to some very recent multimodal detection works such as FocalFormer3D [11], SparseFusion [71] and CMT [74], outperforming them by 1.4%, 1.0% and 1.0% in mAP, respectively. Notably, IS-FUSION obtains the highest results in some categories with fewer labeled instances, i.e., motorcycle and trailers (constituting only 1.08% and 2.13% of the dataset). This suggests that IS-FUSION captures essential information even from limited instances. By applying test-time augmentation and model ensemble, IS-FUSION\u2020 achieves a new SOTA on the highly competitive nuScenes leaderboard.\\n\\nAs shown in Table 2, IS-FUSION also obtains the best detection accuracy on the nuScenes validation set, meanwhile keeping a comparable inference speed. In particular, it significantly surpasses the SOTA detectors like CMT and SparseFusion by 2.5% and 1.8% in mAP, respectively. In Fig. 5, we additionally present some qualitative detection results on the nuScenes validation set to showcase the performance of IS-FUSION. The visualization reveals that IS-FUSION is capable of accurately detecting objects of various classes, even at distant ranges and with varying scales. Overall, the promising performance of IS-FUSION can be attributed to the joint modeling of the multimodal instance-level and scene-level contexts, as well as their effective collaboration in enhancing the BEV representation.\\n\\n4.3. Ablation Studies\\n\\n4.3.1 Component-wise Ablation\\n\\nIn this section, we investigate the contribution of each component in our model. We begin by introducing the baseline frameworks of IS-FUSION. Concretely, our LiDAR-only baseline derives from Transfusion-L [1], which is reimplemented here as Baseline-L. For the multimodal baseline, denoted as Baseline-LC, we adopt a straightforward approach that combines the point cloud and image BEV features via a convolutional layer (see Eq. (6)). To obtain the image BEV features, we summarize the point features in each pillar through summation operation, where the point features are determined by the image features as introduced in Eq. (4). According to Table 3 (a)-(b), this intuitive fusion solution achieves 69.4% mAP and 71.6% NDS, outperforming Baseline-L by 4.0% in mAP and 1.5% in NDS. By leveraging HSF to enhance the scene feature, it improves 2.2% in mAP and 1.5% in NDS. However, it also slightly reduces the FPS to 3.2 from 3.3. The trade-off is justified due to the significant performance gain.\\n\\nTable 3. Ablation studies for each module in IS-FUSION on the nuScenes validation set. Baseline-L indicates the LiDAR-only baseline, while Baseline-LC refers to a simple variant of IS-FUSION without employing the HSF or IGF modules.\"}"}
{"id": "CVPR-2024-1141", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.3.2 Analysis of HSF\\n\\nThe HSF module is designed to hierarchically extract multimodal features at various granularities, facilitating a comprehensive description of the scene context. Therefore, we examine the effectiveness of using different feature granularities in HSF. According to Table 4(b), the Point-to-Grid transformer, focusing on point-wise and grid-wise features, shows an improvement over Baseline-LC by 0.5% in mAP and 0.3% in NDS. The Grid-to-Region transformer improves Baseline-LC by 1.8% in mAP and 1.2% in NDS, by exploring the inter-grid and inter-region features. It suggests that a larger receptive field is more crucial for 3D object detection. The full HSF results in an improvement of 2.2% in mAP and 1.6% in NDS, highlighting the benefits of feature integration across different granularities.\\n\\n4.3.3 Analysis of IGF\\n\\nThe IGF module aggregates the local multimodal feature around each instance, and incorporates necessary instance-level information into the BEV scene feature. In IGF, there are two hyper-parameters that need to be determined, namely, the instance number ($K$) and the sampled neighbor number ($D$) in the multimodal feature. According to Table 4(c), we found that setting $K = 200$ and $D = 16$ yields better performance, achieving 70.9% mAP and 72.8% NDS. Further increasing $K$ or $D$ does not lead to additional improvement, which suggests that the self-attention between instances has effectively explored a suitable receptive field.\\n\\nAdditionally, we provide visualization of the BEV feature maps for models with and without IGF. As shown in Fig. 6, the feature maps without IGF tend to exhibit incomplete patterns and lower responses, while the IGF module significantly enhances the quality of the feature map, due to the interactive collaboration with the instance-level feature.\"}"}
{"id": "CVPR-2024-1141", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Xuyang Bai, Zeyu Hu, Xinge Zhu, Qingqiu Huang, Yilun Chen, Hongbo Fu, and Chiew-Lan Tai. Transfusion: Robust lidar-camera fusion for 3D object detection with transformers. In CVPR, 2022.\\n\\n[2] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multimodal dataset for autonomous driving. In CVPR, 2020.\\n\\n[3] Qi Cai, Yingwei Pan, Ting Yao, Chong-Wah Ngo, and Tao Mei. Objectfusion: Multi-modal 3D object detection with object-centric fusion. In ICCV, 2023.\\n\\n[4] Runnan Chen, Youquan Liu, Lingdong Kong, Nenglun Chen, Xinge Zhu, Yuexin Ma, Tongliang Liu, and Wenping Wang. Towards label-free scene understanding by vision foundation models. In NeurIPS, 2023.\\n\\n[5] Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, and Wenping Wang. Clip2scene: Towards label-efficient 3D scene understanding by clip. In CVPR, 2023.\\n\\n[6] Xiaozhi Chen, Huimin Ma, Ji Wan, Bo Li, and Tian Xia. Multi-view 3D object detection network for autonomous driving. In CVPR, 2017.\\n\\n[7] Xuanyao Chen, Tianyuan Zhang, Yue Wang, Yilun Wang, and Hang Zhao. Futr3d: A unified sensor fusion framework for 3D detection. In CVPRW, 2023.\\n\\n[8] Yukang Chen, Yanwei Li, Xiangyu Zhang, Jian Sun, and Jiaya Jia. Focal sparse convolutional networks for 3D object detection. In CVPR, 2022.\\n\\n[9] Yukang Chen, Jianhui Liu, Xiangyu Zhang, Xiaojuan Qi, and Jiaya Jia. Voxelnex: Fully sparse voxelnet for 3D object detection and tracking. In CVPR, 2023.\\n\\n[10] Yilun Chen, Shu Liu, Xiaoyong Shen, and Jiaya Jia. Fast point r-cnn. In ICCV, 2019.\\n\\n[11] Yilun Chen, Zhiding Yu, Yukang Chen, Shiyi Lan, Animashree Anandkumar, Jiaya Jia, and Jose Alvarez. Focalformer3d: Focusing on hard instance for 3D object detection. In ICCV, 2023.\\n\\n[12] Zehui Chen, Zhenyu Li, Shiquan Zhang, Liangji Fang, Qin-hong Jiang, and Feng Zhao. Autoalignv2: Deformable feature aggregation for dynamic multi-modal 3D object detection. In ECCV, 2022.\\n\\n[13] MMDetection3D Contributors. MMDetection3D: OpenMMLab next-generation platform for general 3D object detection. https://github.com/open-mmlab/mmdetection3d, 2020.\\n\\n[14] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\\n\\n[15] Tuo Feng, Ruijie Quan, Xiaohan Wang, Wenguan Wang, and Yi Yang. Interpretable3d: An ad-hoc interpretable classifier for 3D point clouds. In AAAI, 2024.\\n\\n[16] Tuo Feng, Wenguan Wang, Fan Ma, and Yi Yang. Lsknet: Towards effective and efficient 3D perception with large sparse kernels. In CVPR, 2024.\\n\\n[17] Tuo Feng, Wenguan Wang, Xiaohan Wang, Yi Yang, and Qinghua Zheng. Clustering based point cloud representation learning for 3D analysis. In ICCV, 2023.\\n\\n[18] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural message passing for quantum chemistry. In ICML, 2017.\\n\\n[19] Benjamin Graham, Martin Engelcke, and Laurens Van Der Maaten. 3D semantic segmentation with submanifold sparse convolutional networks. In CVPR, 2018.\\n\\n[20] Wencheng Han, Junbo Yin, Xiaogang Jin, Xiangdong Dai, and Jianbing Shen. Brnet: Exploring comprehensive features for monocular depth estimation. In ECCV, 2022.\\n\\n[21] Wencheng Han, Junbo Yin, and Jianbing Shen. Self-supervised monocular depth estimation by direction-aware cumulative convolution network. In ICCV, 2023.\\n\\n[22] Chenhang He, Ruihuang Li, Shuai Li, and Lei Zhang. Voxelset transformer: A set-to-set approach to 3D object detection from point clouds. In CVPR, 2022.\\n\\n[23] Chenhang He, Ruihuang Li, Yabin Zhang, Shuai Li, and Lei Zhang. Msf: Motion-guided sequential fusion for efficient 3D object detection from point cloud sequences. In CVPR, 2023.\\n\\n[24] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\n[25] Jordan SK Hu, Tianshu Kuai, and Steven L Waslander. Point density-aware voxels for lidar 3D object detection. In CVPR, 2022.\\n\\n[26] Yu Huang, Chenzhuang Du, Zihui Xue, Xuanyao Chen, Hang Zhao, and Longbo Huang. What makes multi-modal learning better than single (provably). In NeurIPS, 2021.\\n\\n[27] Jason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, and Steven L Waslander. Joint 3D proposal generation and object detection from view aggregation. In IROS, 2018.\\n\\n[28] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83\u201397, 1955.\\n\\n[29] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In CVPR, 2019.\\n\\n[30] Peiliang Li, Xiaozhi Chen, and Shaojie Shen. Stereo r-cnn based 3D object detection for autonomous driving. In CVPR, 2019.\\n\\n[31] Xin Li, Tao Ma, Yuenan Hou, Botian Shi, Yucheng Yang, Youquan Liu, Xingjiao Wu, Qin Chen, Yikang Li, Yu Qiao, et al. Logonet: Towards accurate 3D object detection with local-to-global cross-modal fusion. In CVPR, 2023.\\n\\n[32] Xiang Li, Junbo Yin, Wei Li, Cheng-Zhong Xu, Ruigang Yang, and Jianbing Shen. Di-v2x: Learning domain-invariant representation for vehicle-infrastructure collaborative 3D object detection. In AAAI, 2024.\\n\\n[33] Xiang Li, Junbo Yin, Botian Shi, Yikang Li, Ruigang Yang, and Jianbing Shen. Lwsis: Lidar-guided weakly supervised instance segmentation for autonomous driving. In AAAI, 2023.\\n\\n[34] Yanwei Li, Yilun Chen, Xiaojuan Qi, Zeming Li, Jian Sun, and Jiaya Jia. Unifying voxel-based representation with transformer for 3D object detection. In NeurIPS, 2022.\\n\\n[35] Yanwei Li, Xiaojuan Qi, Yukang Chen, Liwei Wang, Zeming Li, Jian Sun, and Jiaya Jia. Voxel field fusion for 3D object detection.\"}"}
{"id": "CVPR-2024-1141", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yingwei Li, Adams Wei Yu, Tianjian Meng, Ben Caine, Ji-quant Ngiam, Daiyi Peng, Junyang Shen, Yifeng Lu, Denny Zhou, Quoc V Le, et al. Deepfusion: Lidar-camera deep fusion for multi-modal 3d object detection. In CVPR, 2022.\\n\\nZhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Yu Qiao, and Jifeng Dai. Bevformer: Learning bird's-eye-view representation from multi-camera images via spatiotemporal transformers. In ECCV, 2022.\\n\\nMing Liang, Bin Yang, Yun Chen, Rui Hu, and Raquel Urtasun. Multi-task multi-sensor fusion for 3d object detection. In CVPR, 2019.\\n\\nMing Liang, Bin Yang, Shenlong Wang, and Raquel Urtasun. Deep continuous fusion for multi-sensor 3d object detection. In ECCV, 2018.\\n\\nTingting Liang, Hongwei Xie, Kaicheng Yu, Zhongyu Xia, Zhiwei Lin, Yongtao Wang, Tao Tang, Bing Wang, and Zhi Tang. Bevfusion: A simple and robust lidar-camera fusion framework. In NeurIPS, 2022.\\n\\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In ICCV, 2017.\\n\\nRui Liu, Xiaohan Wang, Wenguan Wang, and Yi Yang. Bird's-eye-view scene graph for vision-language navigation. In ICCV, 2023.\\n\\nYouquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wenwei Zhang, Liang Pan, Kai Chen, and Ziwei Liu. Segment any point cloud sequences by distilling vision foundation models. In NeurIPS, 2023.\\n\\nYingfei Liu, Tiancai Wang, Xiangyu Zhang, and Jian Sun. Petr: Position embedding transformation for multi-view 3d object detection. In ECCV, 2022.\\n\\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.\\n\\nZhijian Liu, Haotian Tang, Alexander Amini, Xingyu Yang, Huizi Mao, Daniela Rus, and Song Han. Bevfusion: Multi-task multi-sensor fusion with unified bird's-eye view representation. In ICRA, 2023.\\n\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\\n\\nYuhang Lu, Qi Jiang, Runnan Chen, Yuenan Hou, Xinge Zhu, and Yuexin Ma. See more and know more: Zero-shot point cloud segmentation via multi-modal visual data. In ICCV, 2023.\\n\\nQinghao Meng, Wenguan Wang, Tianfei Zhou, Jianbing Shen, Yunde Jia, and Luc Van Gool. Towards a weakly supervised framework for 3d point cloud object detection and annotation. IEEE TPAMI, 44(8):4454\u20134468, 2021.\\n\\nZhenwei Miao, Jikai Chen, Hongyu Pan, Ruiwen Zhang, Kaixuan Liu, Peihan Hao, Jun Zhu, Yang Wang, and Xin Zhan. Pvgnet: A bottom-up one-stage 3d object detector with integrated multi-level features. In CVPR, 2021.\\n\\nAJ Piergiovanni, Vincent Casser, Michael S Ryoo, and Anelia Angelova. 4d-net for learned multi-modal alignment. In ICCV, 2021.\\n\\nCharles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In ICCV, 2019.\\n\\nCharles R Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas J Guibas. Frustum pointnets for 3d object detection from rgb-d data. In CVPR, 2018.\\n\\nCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In CVPR, 2017.\\n\\nCharles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. In NeurIPS, 2017.\\n\\nGuangsheng Shi, Ruifeng Li, and Chao Ma. Pillarnet: Real-time and high-performance pillar-based 3d object detection. In ECCV, 2022.\\n\\nShaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection. In CVPR, 2020.\\n\\nShaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointrcnn: 3d object proposal generation and detection from point cloud. In CVPR, 2019.\\n\\nWeijing Shi and Raj Rajkumar. Point-gnn: Graph neural network for 3d object detection in a point cloud. In CVPR, 2020.\\n\\nLeslie N Smith. Cyclical learning rates for training neural networks. In WACV, 2017.\\n\\nZiying Song, Haiyue Wei, Lin Bai, Lei Yang, and Caiyan Jia. Graphalign: Enhancing accurate feature alignment by graph matching for multi-modal 3d object detection. In ICCV, 2023.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\\n\\nSourabh Vora, Alex H Lang, Bassam Helou, and Oscar Beijbom. Pointpainting: Sequential fusion for 3d object detection. In CVPR, 2020.\\n\\nChunwei Wang, Chao Ma, Ming Zhu, and Xiaokang Yang. Pointaugmenting: Cross-modal augmentation for 3d object detection. In CVPR, 2021.\\n\\nChien-Yao Wang, Hong-Yuan Mark Liao, Yueh-Hua Wu, Ping-Yang Chen, Jun-Wei Hsieh, and I-Hau Yeh. Cspnet: A new backbone that can enhance learning capability of cnn. In CVPR, 2020.\\n\\nHaiyang Wang, Hao Tang, Shaoshuai Shi, Aoxue Li, Zheguo Li, Bernt Schiele, and Liwei Wang. Unitr: A unified and efficient multi-modal transformer for bird's-eye-view representation. In ICCV, 2023.\\n\\nLi Wang, Liang Du, Xiaoqing Ye, Yanwei Fu, Guodong Guo, Xiangyang Xue, Jianfeng Feng, and Li Zhang. Depth-conditioned dynamic message propagation for monocular 3d object detection. In CVPR, 2021.\\n\\nYan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Q Weinberger. Pseudo-lidar from visual depth estimation: Bridging the gap in 3d object detection for autonomous driving. In ECCV, 2019.\\n\\nYan Wang, Junbo Yin, Wei Li, Pascal Frossard, Ruigang Yang, and Jianbing Shen. Ssda3d: Semi-supervised domain adaptation for 3d object detection from point cloud. In AAAI, 2023.\"}"}
{"id": "CVPR-2024-1141", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Wang. Virtual sparse convolution for multimodal 3D object detection. In CVPR, 2023.\\n\\nYichen Xie, Chenfeng Xu, Marie-Julie Rakotosaona, Patrick Rim, Federico Tombari, Kurt Keutzer, Masayoshi Tomizuka, and Wei Zhan. Sparsefusion: Fusing multi-modal sparse representations for multi-sensor 3D object detection. In ICCV, 2023.\\n\\nDanfei Xu, Dragomir Anguelov, and Ashesh Jain. Pointsion: Deep sensor fusion for 3D bounding box estimation. In CVPR, 2018.\\n\\nShaoqing Xu, Dingfu Zhou, Jin Fang, Junbo Yin, Zhou Bin, and Liangjun Zhang. Fusionpainting: Multimodal fusion with adaptive attention for 3D object detection. In IEEE International Intelligent Transportation Systems Conference (ITSC), 2021.\\n\\nJunjie Yan, Yingfei Liu, Jianjian Sun, Fan Jia, Shuailin Li, Tiancai Wang, and Xiangyu Zhang. Cross modal transformer via coordinates encoding for 3D object detection. In ICCV, 2023.\\n\\nYan Yan, Yuxing Mao, and Bo Li. Second: Sparsely embedded convolutional detection. Sensors, 18(10):3337, 2018.\\n\\nHonghui Yang, Zili Liu, Xiaopei Wu, Wenxiao Wang, Wei Qian, Xiaofei He, and Deng Cai. Graph R-CNN: Towards accurate 3D object detection with semantic-decorated local graph. In ECCV, 2022.\\n\\nZeyu Yang, Jiaqi Chen, Zhenwei Miao, Wei Li, Xiatian Zhu, and Li Zhang. Deepinteraction: 3D object detection via modality interaction. In NeurIPS, 2022.\\n\\nZetong Yang, Yanan Sun, Shu Liu, and Jiaya Jia. 3DSSD: Point-based 3D single stage object detector. In CVPR, 2020.\\n\\nZetong Yang, Yanan Sun, Shu Liu, Xiaoyong Shen, and Jiaya Jia. STD: Sparse-to-dense 3D object detector for point cloud. In ICCV, 2019.\\n\\nZhuyu Yao, Jiangbo Ai, Boxun Li, and Chi Zhang. Efficient DETR: Improving end-to-end object detector with dense prior. arXiv preprint arXiv:2104.01318, 2021.\\n\\nJunbo Yin, Jin Fang, Dingfu Zhou, Liangjun Zhang, Chengzhong Xu, Jianbing Shen, and Wenguan Wang. Semi-supervised 3D object detection with proficient teachers. In ECCV, 2022.\\n\\nJunbo Yin, Jianbing Shen, Xin Gao, David J Crandall, and Ruigang Yang. Graph neural network and spatiotemporal transformer attention for 3D video object detection from point clouds. IEEE TPAMI, 45(8):9822\u20139835, 2021.\\n\\nJunbo Yin, Dingfu Zhou, Liangjun Zhang, Jin Fang, Chengzhong Xu, Jianbing Shen, and Wenguan Wang. Proposal-contrast: Unsupervised pre-training for lidar-based 3D object detection. In ECCV, 2022.\\n\\nTianwei Yin, Xingyi Zhou, and Philipp Krahenbuhl. Center-based 3D object detection and tracking. In CVPR, 2021.\\n\\nTianwei Yin, Xingyi Zhou, and Philipp Kr\u00e4henb\u00fchl. Multi-modal virtual point 3D detection. In NeurIPS, 2021.\\n\\nYanan Zhang, Jiaxin Chen, and Di Huang. CAT-DET: Contrastively augmented transformer for multi-modal 3D object detection. In CVPR, 2022.\\n\\nHengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In ICCV, 2021.\\n\\nChao Zhou, Yanan Zhang, Jiaxin Chen, and Di Huang. Octr: Octree-based transformer for 3D object detection. In CVPR, 2023.\\n\\nDingfu Zhou, Jin Fang, Xibin Song, Liu Liu, Junbo Yin, Yuchao Dai, Hongdong Li, and Ruigang Yang. Joint 3D instance segmentation and object detection for autonomous driving. In CVPR, 2020.\\n\\nYin Zhou and Oncel Tuzel. Voxelnet: End-to-end learning for point cloud based 3D object detection. In CVPR, 2018.\\n\\nBenjin Zhu, Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu. Class-balanced grouping and sampling for point cloud 3D object detection. arXiv preprint arXiv:1908.09492, 2019.\\n\\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable DETR: Deformable transformers for end-to-end object detection. In ICLR, 2021.\\n\\nZiyue Zhu, Qiang Meng, Xiao Wang, Ke Wang, Liujiang Yan, and Jian Yang. Curricular object manipulation in lidar-based object detection. In CVPR, 2023.\"}"}
