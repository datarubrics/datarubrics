{"id": "CVPR-2024-2593", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, and Richard Szeliski. Building rome in a day. Communications of the ACM, 2011.\\n\\n[2] Jonathan T Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields. In ICCV, 2021.\\n\\n[3] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In CVPR, 2022.\\n\\n[4] Jonathan T Barron, Ben Mildenhall, Dor Verbin, Pratul P Srinivasan, and Peter Hedman. Zip-nerf: Anti-aliased grid-based neural radiance fields. In ICCV, 2023.\\n\\n[5] Piotr Bojanowski, Armand Joulin, David Lopez-Pas, and Arthur Szlam. Optimizing the latent space of generative networks. In ICML, 2018.\\n\\n[6] Ilker Bozcan and Erdal Kayacan. Au-air: A multi-modal unmanned aerial vehicle dataset for low altitude traffic surveillance. In ICRA, 2020.\\n\\n[7] Ang Cao and Justin Johnson. Hexplane: A fast representation for dynamic scenes. In CVPR, 2023.\\n\\n[8] Junli Cao, Huan Wang, Pavlo Chemerys, Vladislav Shakhrai, Ju Hu, Yun Fu, Denys Makoviichuk, Sergey Tulyakov, and Jian Ren. Real-time neural light field on mobile devices. In CVPR, 2023.\\n\\n[9] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In ECCV, 2022.\\n\\n[10] Xingyu Chen, Qi Zhang, Xiaoyu Li, Yue Chen, Ying Feng, Xuan Wang, and Jue Wang. Hallucinated neural radiance fields in the wild. In CVPR, 2022.\\n\\n[11] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. Mobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. In CVPR, 2023.\\n\\n[12] Zilong Chen, Feng Wang, and Huaping Liu. Text-to-3d using gaussian splatting. arXiv preprint arXiv:2309.16585, 2023.\\n\\n[13] Dawei Du, Yuankai Qi, Hongyang Yu, Yifan Yang, Kaiwen Duan, Guorong Li, Weigang Zhang, Qingming Huang, and Qi Tian. The unmanned aerial vehicle benchmark: Object detection and tracking. In ECCV, 2018.\\n\\n[14] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In CVPR, 2022.\\n\\n[15] Sara Fridovich-Keil, Giacomo Meanti, Frederik Rahb\u00e6k Warburg, Benjamin Recht, and Angjoo Kanazawa. K-planes: Explicit radiance fields in space, time, and appearance. In CVPR, 2023.\\n\\n[16] Christian Fr\u00fch and Avideh Zakhor. An automated method for large-scale, ground-based city model acquisition. IJCV, 2004.\\n\\n[17] Yasutaka Furukawa, Brian Curless, Steven M Seitz, and Richard Szeliski. Towards internet-scale multi-view stereo. In CVPR, 2010.\\n\\n[18] Hang Gao, Ruilong Li, Shubham Tulsiani, Bryan Russell, and Angjoo Kanazawa. Monocular dynamic view synthesis: A reality check. In NeurIPS, 2022.\\n\\n[19] Michael Goesele, Noah Snavely, Brian Curless, Hugues Hoppe, and Steven M Seitz. Multi-view stereo for community photo collections. In ICCV, 2007.\\n\\n[20] Peter Hedman, Pratul P Srinivasan, Ben Mildenhall, Jonathan T Barron, and Paul Debevec. Baking neural radiance fields for real-time view synthesis. In ICCV, 2021.\\n\\n[21] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM ToG, 2023.\\n\\n[22] Wei Li, CW Pan, Rong Zhang, JP Ren, YX Ma, Jin Fang, FL Yan, QC Geng, XY Huang, HJ Gong, et al. Aads: Augmented autonomous driving simulation using data-driven algorithms. Science robotics, 2019.\\n\\n[23] Xiaowei Li, Changchang Wu, Christopher Zach, Svetlana Lazebnik, and Jan-Michael Frahm. Modeling and recognition of landmark image collections using iconic scene graphs. In ECCV, 2008.\\n\\n[24] Zhaoshuo Li, Thomas M\u00fcller, Alex Evans, Russell H Taylor, Mathias Unberath, Ming-Yu Liu, and Chen-Hsuan Lin. Neuralangelo: High-fidelity neural surface reconstruction. In CVPR, 2023.\\n\\n[25] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia, 2022.\\n\\n[26] Liqiang Lin, Yilin Liu, Yue Hu, Xingguang Yan, Ke Xie, and Hui Huang. Capturing, reconstructing, and simulating: the urbanscene3d dataset. In ECCV, 2022.\\n\\n[27] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In CVPR, 2023.\\n\\n[28] Jonathon Luiten, Georgios Kopanas, Bastian Leibe, and Deva Ramanan. Dynamic 3d gaussians: Tracking by persistent dynamic view synthesis. arXiv preprint arXiv:2308.09713, 2023.\\n\\n[29] Ricardo Martin-Brualla, Noha Radwan, Mehdi SM Sajjadi, Jonathan T Barron, Alexey Dosovitskiy, and Daniel Duckworth. Nerf in the wild: Neural radiance fields for unconstrained photo collections. In CVPR, 2021.\\n\\n[30] Moustafa Meshry, Dan B Goldman, Sameh Khamis, Hugues Hoppe, Rohit Pandey, Noah Snavely, and Ricardo Martin-Brualla. Neural rerendering in the wild. In CVPR, 2019.\\n\\n[31] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In ECCV, 2020.\\n\\n[32] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM ToG, 2022.\\n\\n[33] Julian Ost, Fahim Mannan, Nils Thuerey, Julian Knodt, and Felix Heide. Neural scene graphs for dynamic scenes. In CVPR, 2021.\"}"}
{"id": "CVPR-2024-2593", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Marc Pollefeys, David Nister, J-M Frahm, Amir Akbarzadeh, Philippos Mordohai, Brian Clipp, Chris Engels, David Gallup, S-J Kim, Paul Merrell, et al. Detailed real-time urban 3d reconstruction from video. *IJCV*, 2008.\\n\\nRavi Ramamoorthi. Nerfs: The search for the best 3d representation. *arXiv preprint arXiv:2308.02751*, 2023.\\n\\nChristian Reiser, Songyou Peng, Yiyi Liao, and Andreas Geiger. Kilonerf: Speeding up neural radiance fields with thousands of tiny mlps. In *ICCV*, 2021.\\n\\nChristian Reiser, Rick Szeliski, Dor Verbin, Pratul Srinivasan, Ben Mildenhall, Andreas Geiger, Jon Barron, and Peter Hedman. Merf: Memory-efficient radiance fields for real-time view synthesis in unbounded scenes. *ACM ToG*, 2023.\\n\\nJohannes Lutz Sch\u00f6nherr and Jan-Michael Frahm. Structure-from-motion revisited. In *CVPR*, 2016.\\n\\nNoah Snavely, Steven M Seitz, and Richard Szeliski. Photo tourism: exploring photo collections in 3d. In *SIGGRAPH*.\\n\\nCheng Sun, Min Sun, and Hwann-Tzong Chen. Direct voxel grid optimization: Super-fast convergence for radiance fields reconstruction. In *CVPR*, 2022.\\n\\nMatthew Tancik, Vincent Casser, Xinchen Yan, Sabeek Pradhan, Ben Mildenhall, Pratul P Srinivasan, Jonathan T Barron, and Henrik Kretzschmar. Block-nerf: Scalable large scene neural view synthesis. In *CVPR*, 2022.\\n\\nJiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. *arXiv preprint arXiv:2309.16653*, 2023.\\n\\nJiaxiang Tang, Hang Zhou, Xiaokang Chen, Tianshu Hu, Eruru Ding, Jingdong Wang, and Gang Zeng. Delicate textured mesh recovery from nerf via adaptive surface refinement. In *ICCV*, 2023.\\n\\nHaithem Turki, Deva Ramanan, and Mahadev Satyanarayanan. Mega-nerf: Scalable construction of large-scale nerfs for virtual fly-throughs. In *CVPR*, 2022.\\n\\nDor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler, Jonathan T Barron, and Pratul P Srinivasan. Ref-nerf: Structured view-dependent appearance for neural radiance fields. In *CVPR*, 2022.\\n\\nHuan Wang, Jian Ren, Zeng Huang, Kyle Olszewski, Minglei Chai, Yun Fu, and Sergey Tulyakov. R2l: Distilling neural radiance field to neural light field for efficient novel view synthesis. In *ECCV*, 2022.\\n\\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In *NeurIPS*, 2021.\\n\\nPeng Wang, Yuan Liu, Zhaoxi Chen, Lingjie Liu, Ziwei Liu, Taku Komura, Christian Theobalt, and Wenping Wang. F2-nerf: Fast neural radiance field training with free camera trajectories. In *CVPR*, 2023.\\n\\nYiming Wang, Qin Han, Marc Habermann, Kostas Daniilidis, Christian Theobalt, and Lingjie Liu. Neus2: Fast learning of neural implicit surfaces for multi-view reconstruction. In *ICCV*, 2023.\\n\\nChung-Yi Weng, Brian Curless, Pratul P. Srinivasan, Jonathan T. Barron, and Ira Kemelmacher-Shlizerman. Hummernerf: Free-viewpoint rendering of moving people from monocular video. In *CVPR*, 2022.\\n\\nGuanjun Wu, Taoran Yi, Jiemin Fang, Lingxi Xie, Xiaopeng Zhang, Wei Wei, Wenyu Liu, Qi Tian, and Xinggang Wang. 4d gaussian splatting for real-time dynamic scene rendering. *arXiv preprint arXiv:2310.08528*, 2023.\\n\\nYuanbo Xiangli, Linning Xu, Xingang Pan, Nanxuan Zhao, Anyi Rao, Christian Theobalt, Bo Dai, and Dahua Lin. Bungeenerf: Progressive neural radiance field for extreme multi-scale scene rendering. In *ECCV*, 2022.\\n\\nLinning Xu, Yuanbo Xiangli, Sida Peng, Xingang Pan, Nanxuan Zhao, Christian Theobalt, Bo Dai, and Dahua Lin. Grid-guided neural radiance fields for large urban scenes. In *CVPR*, 2023.\\n\\nZhenpei Yang, Yuning Chai, Dragomir Anguelov, Yin Zhou, Pei Sun, Dumitru Erhan, Sean Rafferty, and Henrik Kretzschmar. Surfelgan: Synthesizing realistic sensor data for autonomous driving. In *CVPR*, 2020.\\n\\nZiyi Yang, Xinyu Gao, Wen Zhou, Shaohui Jiao, Yuqing Zhang, and Xiaogang Jin. Deformable 3d gaussians for high-fidelity monocular dynamic scene reconstruction. *arXiv preprint arXiv:2309.13101*, 2023.\\n\\nZeyu Yang, Hongye Yang, Zijie Pan, Xiatian Zhu, and Li Zhang. Real-time photorealistic dynamic scene representation and rendering with 4d gaussian splatting. *arXiv preprint arXiv:2310.10642*, 2023.\\n\\nLior Yariv, Jiatao Gu, Yoni Kasten, and Yaron Lipman. Volume rendering of neural implicit surfaces. In *NeurIPS*, 2021.\\n\\nLior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P Srinivasan, Richard Szeliski, Jonathan T Barron, and Ben Mildenhall. Bakedsdf: Meshing neural sdfs for real-time view synthesis. *arXiv preprint arXiv:2302.14859*, 2023.\\n\\nTaoran Yi, Jiemin Fang, Guanjun Wu, Lingxi Xie, Xiaopeng Zhang, Wenyu Liu, Qi Tian, and Xinggang Wang. Gaussian-dreamer: Fast generation from text to 3d gaussian splatting with point cloud priors. *arXiv preprint arXiv:2310.08529*, 2023.\\n\\nAlex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. In *ICCV*, 2021.\\n\\nMI Zhenxing and Dan Xu. Switch-nerf: Learning scene decomposition with mixture of experts for large-scale neural radiance fields. In *ICLR*, 2022.\\n\\nSiyu Zhu, Runze Zhang, Lei Zhou, Tianwei Shen, Tian Fang, Ping Tan, and Long Quan. Very large-scale global sfm by distributed motion averaging. In *CVPR*, 2018.\\n\\nMatthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. In *VIS*, 2001.\"}"}
{"id": "CVPR-2024-2593", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2. Decoupled Appearance Modeling\\n\\nThere are obvious appearance variations in the images taken in uneven illumination, and 3DGS tends to produce floaters to compensate for these variations across different views, as shown in Fig. 2(a\u2013d).\\n\\nTo address this problem, some NeRF-based methods concatenate an appearance embedding to point-based features in pixel-wise ray-marching, and feed them into the radiance MLP to obtain the final color. This is not suitable for 3DGS, whose rendering is performed by frame-wise rasterization without MLPs. Instead, we introduce decoupled appearance modeling into the optimization process, which produces a transformation map to adjust the rendered image to fit the appearance variations in the training image, as shown in Fig. 4. Specifically, we first downsample the rendered image $I_r^i$ to not only prevent the transformation map from learning high-frequency details, but also reduce computation burden and memory consumption. We then concatenate an appearance embedding $\\\\ell^i$ of length $m$ to every pixel in the three-channel downsampled image, and obtain a 2D map $D^i$ with $3 + m$ channels. $D^i$ is fed into a convolutional neural network (CNN), which progressively upsamples $D^i$ to generate $M^i$ that is of the same resolution as $I_r^i$. Finally, the appearance-variant image $I_a^i$ is obtained by performing a pixel-wise transformation $T$ on $I_r^i$ with $M^i$:\\n\\n$$I_a^i = T(I_r^i; M^i).$$\\n\\nIn our experiments, a simple pixel-wise multiplication works well on the datasets we use. The appearance embeddings and CNN are optimized along with the 3D Gaussians, using the loss function modified from Eq. (1):\\n\\n$$L = (1 - \\\\lambda) L_1(I_a^i, I_i) + \\\\lambda L_{D-SSIM}(I_r^i, I_i).$$\\n\\nSince $L_{D-SSIM}$ mainly penalizes the structural dissimilarity, applying it between $I_r^i$ and the ground truth $I_i$ makes the structure information in $I_r^i$ close to $I_i$, leaving the appearance information to be learned by $\\\\ell^i$ and the CNN. The loss $L_1$ is applied between the appearance-variant rendering $I_a^i$ and $I_i$, which is used to fit the ground truth image $I_i$ that may have appearance variations from other images. After training, $I_r^i$ is expected to have a consistent appearance with other images, from which the 3D Gaussians can learn an average appearance and correct geometry of all the input views. This appearance modeling can be discarded after optimization, without slowing down the real-time rendering speed.\"}"}
{"id": "CVPR-2024-2593", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Quantitative evaluation of our method compared to previous work on five large scenes. We report SSIM\u2191, PSNR\u2191 and LPIPS\u2193 on test views. The best and second best results are highlighted. \u201c\u2013\u201d denotes missing data from the Grid-NeRF paper.\\n\\n5. Experiments\\n5.1. Experimental Setup\\nImplementation. We evaluate our model with 8 cells in our main experiments. The visibility threshold is 25%. The rendered images are downsampled by 32x before being concatenated with the appearance embeddings of length 64. Each cell is optimized for 60,000 iterations. The densification [21] starts at the 1,000th iteration and ends at the 30,000th iteration, with an interval of 200 iterations. The other settings are identical to those of 3DGS [21]. Both the appearance embeddings and the CNN use a learning rate of 0.001. We perform Manhattan world alignment to make the y-axis of the world coordinate perpendicular to the ground plane. We describe the CNN architecture in the supplement.\\n\\nDatasets. The experiments are conducted on five large-scale scenes: Rubble and Building from the Mill-19 dataset [44], and Campus, Residence, and Sci-Art from the UrbanScene3D dataset [26]. Each scene contains thousands of high-resolution images. We downsample the images by 4x times for training and validation, following previous methods [44, 61] for fair comparison.\\n\\nMetrics. We evaluate the rendering quality using three quantitative metrics: SSIM, PSNR, and AlexNet-based LPIPS. The aforementioned photometric variation makes the evaluation difficult, as it is uncertain which photometric condition should be replicated. To address this issue, we follow Mip-NeRF 360 [3] to perform color correction on the rendered images before evaluating the metrics of all the methods, which solves a per-image least squares problem to align the RGB values between the rendered image and its corresponding ground truth. We also report the rendering speed at 1080p resolution, average training time, and video memory consumption.\\n\\nCompared methods. We compare our VastGaussian with four methods: Mega-NeRF [44], Switch-NeRF [61], Grid-NeRF [53], and 3DGS [21]. For 3DGS, we need to increase optimization iterations to make it comparable in our main experiments, but naively doing that causes out-of-memory errors. Therefore, we increase the densification interval accordingly to build a feasible baseline (termed Modified 3DGS). The other configurations are the same as those in the original 3DGS paper. For Grid-NeRF, its code is released without rendered images and carefully tuned configuration files due to its confidentiality requirements. These unavailable files are critical to its performance, making its results not reproducible. Therefore, we use its code to evaluate only its training time, memory and rendering speed, while the quality metrics are copied from its paper.\\n\\n5.2. Result Analysis\\nReconstruction quality. In Tab. 1, we report the mean SSIM, PSNR, and LPIPS metrics in each scene. Our VastGaussian outperforms the compared methods in all the SSIM and LPIPS metrics by significant margins, suggesting that it reconstructs richer details with better rendering in perception. In terms of PSNR, VastGaussian achieves better or comparable results. We also show visual comparison in Fig. 5. The NeRF-based methods fall short of details and produce blurry results. Modified 3DGS has sharper rendering but produces unpleasant floaters. Our method achieves clean and visually pleasing rendering. Note that due to the noticeable over-exposure or under-exposure in some test images, VastGaussian exhibits slightly lower PSNR values, but produces significantly better visual quality, sometimes even being more clear than the ground truth, such as the example on the 3rd row in Fig. 5. The high quality of VastGaussian is partly thanks to its large number of 3D Gaussians. Take the Campus scene for example, the number of 3D Gaussians in Modified 3DGS is 8.9 million, while for VastGaussian the number is 27.4 million.\"}"}
{"id": "CVPR-2024-2593", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Qualitative comparison between VastGaussian and previous work. Floaters are pointed out by green arrows.\\n\\nEfficiency and memory. In Table 2, we report the training time, video memory consumption during optimization, and rendering speed. Mega-NeRF, Switch-NeRF and VastGaussian are trained on 8 Tesla V100 GPUs, while GridNeRF and Modified 3DGS on a single V100 GPU as they do not perform scene decomposition. The rendering speeds are tested on a single RTX 3090 GPU. Our VastGaussian requires much shorter time to reconstruct a scene with photorealistic rendering. Compared to Modified 3DGS, VastGaussian greatly reduces video memory consumption on a single GPU. Since VastGaussian has more 3D Gaussians in the merged scene than Modified 3DGS, its rendering speed is slightly slower than Modified 3DGS, but is still much faster than the NeRF-based methods, achieving real-time rendering at 1080p resolution.\\n\\n5.3. Ablation Study\\nWe perform ablation study on the Sci-Art scene to evaluate different aspects of VastGaussian. Data partition. As shown in Figure 6 and Table 3, both visibility-based camera selection (VisCam) and coverage-based point selection (CovPoint) can improve visual quality. Without each or both of them, floaters can be created in the airspace of a cell to fit the views observing regions out.\"}"}
{"id": "CVPR-2024-2593", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. The visibility-based camera selection and coverage-based point selection can reduce floaters in the airspace.\\n\\n| Model setting | SSIM | PSNR | LPIPS |\\n|---------------|------|------|-------|\\n| 1) w/o VisCam | 0.694 | 20.05 | 0.261 |\\n| 2) w/o CovPoint | 0.874 | 26.14 | 0.128 |\\n| 3) w/o VisCam & CovPoint | 0.699 | 20.35 | 0.253 |\\n| 4) airspace-aware | 0.855 | 24.54 | 0.128 |\\n| 5) w/o Decoupled AM | 0.858 | 25.08 | 0.148 |\\n| Full model | 0.885 | 26.81 | 0.121 |\\n\\nTable 3. Ablation on data partition, visibility calculation and decoupled appearance modeling (Decoupled AM).\\n\\nAs shown in Fig. 7, the visibility-based camera selection can eliminate the appearance jumping on the cell boundaries.\\n\\nAirspace-aware visibility calculation. As illustrated in the 4th row of Tab. 3 and Fig. 8, the cameras selected based on the airspace-aware visibility calculation provide more supervision for the optimization of a cell, thus not producing floaters that are presented when the visibility is calculated in the airspace-agnostic way.\\n\\nDecoupled appearance modeling. As shown in Fig. 2 and the 5th row of Tab. 3, our decoupled appearance modeling reduces the appearance variations in the rendered images. Therefore, the 3D Gaussians can learn consistent geometry and colors from training images with appearance variations, instead of creating floaters to compensate for these variations. Please also see the videos in the supplement.\\n\\nDifferent number of cells. As shown in Tab. 4, more cells reconstruct better details in VastGaussian, leading to better SSIM and LPIPS values, and shorter training time when the cells are optimized in parallel. However, when the cell number reaches 16 or bigger, the quality improvement becomes marginal, and PSNR slightly decreases because there may be gradual brightness changes in a rendered image from cells that are far apart.\\n\\n6. Conclusion and Limitation\\nIn this paper, we propose VastGaussian, the first high-quality reconstruction and real-time rendering method on large-scale scenes. The introduced progressive data partitioning strategy allows for independent cell optimization and seamless merging, obtaining a complete scene with sufficient 3D Gaussians. Our decoupled appearance modeling decouples appearance variations in the training images, and enables consistent rendering across different views. This module can be discarded after optimization to obtain faster rendering speed. While our VastGaussian can be applied to spatial divisions of any shape, we do not provide an optimal division solution that should consider the scene layout, the cell number and the training camera distribution. In addition, there are a lot of 3D Gaussians when the scene is huge, which may need a large storage space and significantly slow down the rendering speed.\"}"}
{"id": "CVPR-2024-2593", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 1. Renderings of three state-of-the-art methods and our VastGaussian from the Residence scene in the UrbanScene3D dataset [26].\\n\\n(a, b) Mega-NeRF [44] and Switch-NeRF [61] produce blurry results with slow rendering speeds. (c) We modify 3D Gaussian Splatting (3DGS) [21] so that it can be optimized for enough iterations on a 32 GB GPU. The rendered image is much sharper, but with a lot of floaters. (d) Our VastGaussian achieves higher quality and much faster rendering than state-of-the-art methods in large scene reconstruction, with much shorter training time.\\n\\nAbstract\\n\\nExisting NeRF-based methods for large scene reconstruction often have limitations in visual quality and rendering speed. While the recent 3D Gaussian Splatting works well on small-scale and object-centric scenes, scaling it up to large scenes poses challenges due to limited video memory, long optimization time, and noticeable appearance variations. To address these challenges, we present VastGaussian, the first method for high-quality reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting. We propose a progressive partitioning strategy to divide a large scene into multiple cells, where the training cameras and point cloud are properly distributed with an airspace-aware visibility criterion. These cells are merged into a complete scene after parallel optimization. We also introduce decoupled appearance modeling into the optimization process to reduce appearance variations in the rendered images. Our approach outperforms existing NeRF-based methods and achieves state-of-the-art results on multiple large scene datasets, enabling fast optimization and high-fidelity real-time rendering. Project page: https://vastgaussian.github.io.\"}"}
{"id": "CVPR-2024-2593", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"dataset with an area of less than 100 m^2 already requires approximately 5.8 million 3D Gaussians for a high-fidelity reconstruction. Second, it requires sufficient iterations to optimize an entire large scene as a whole, which could be time-consuming, and unstable without good regularizations. Third, the illumination is usually uneven in a large scene, and there are noticeable appearance variations in the captured images, as shown in Fig. 2(a). 3DGS tends to produce large 3D Gaussians with low opacities to compensate for these disparities across different views. For example, bright blobs tend to come up close to the cameras with images of high exposure, and dark blobs are associated with images of low exposure. These blobs turn to be unpleasing floaters in the air when observed from novel views, as shown in Fig. 2(b, d).\\n\\nTo address these issues, we propose Vast 3D Gaussians (VastGaussian) for large scene reconstruction based on 3D Gaussian Splatting. We reconstruct a large scene in a divide-and-conquer manner: Partition a large scene into multiple cells, optimize each cell independently, and finally merge them into a full scene. It is easier to optimize these cells due to their finer spatial scale and smaller data size. A natural and naive partitioning strategy is to distribute training data geographically based on their positions. This may cause boundary artifacts between two adjacent cells due to few common cameras, and can produce floaters in the air without sufficient supervision. Thus, we propose visibility-based data selection to incorporate more training cameras and point clouds progressively, which ensures seamless merging and eliminates floaters in the air. Our approach allows better flexibility and scalability than 3DGS. Each of these cells contains a smaller number of 3D Gaussians, which reduces the memory requirement and optimization time, especially when optimized in parallel with multiple GPUs. The total number of 3D Gaussians contained in the merged scene can greatly exceed that of the scene trained as a whole, improving the reconstruction quality. Besides, we can expand the scene by incorporating new cells or fine-tune a specific region without retraining the entire large scene.\\n\\nTo reduce the floaters caused by appearance variations, Generative Latent Optimization (GLO) [5] with appearance embeddings [29] is proposed for NeRF-based methods [41, 61]. This approach samples points through ray-marching, and the point features are fed into an MLP along with appearance embeddings to obtain the final colors. The rendering process is the same as the optimization, which still requires appearance embeddings as input. It is not suitable for 3DGS as its rendering is performed by frame-wise rasterization without MLPs. Therefore, we propose a novel decoupled appearance modeling that is applied only in the optimization. We attach an appearance embedding to the rendered image pixel-by-pixel, and feed them into a CNN to obtain a transformation map for applying appearance adjustment on the rendered image. We penalize the structure dissimilarities between the rendered image and its ground truth to learn constant information, while the photometric loss is calculated on the adjusted image to fit the appearance variations in the training image. Only the consistent rendering is what we need, so this appearance modeling module can be discarded after optimization, thus not slowing down the real-time rendering speed.\\n\\nExperiments on several large scene benchmarks confirm the superiority of our method over NeRF-based methods. Our contributions are summarized as follows:\\n\\n\u2022 We present VastGaussian, the first method for high-fidelity reconstruction and real-time rendering on large scenes based on 3D Gaussian Splatting.\\n\u2022 We propose a progressive data partitioning strategy that assigns training views and point clouds to different cells, enabling parallel optimization and seamless merging.\\n\u2022 We introduce decoupled appearance modeling into the optimization process, which suppresses floaters due to appearance variations. This module can be discarded after optimization to obtain the real-time rendering speed.\"}"}
{"id": "CVPR-2024-2593", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Related Work\\n\\n2.1. Large Scene Reconstruction\\n\\nThere is significant progress in image-based large scene reconstruction over the past decades. Some works \\\\[1, 16, 23, 34, 38, 39, 62\\\\] follow a structure-from-motion (SfM) pipeline to estimate camera poses and a sparse point cloud. The following works \\\\[17, 19\\\\] produce a dense point cloud or triangle mesh from the SfM output based on multi-view stereo (MVS). As NeRF \\\\[31\\\\] becomes a popular 3D representation for photo-realistic novel-view synthesis in recent years \\\\[35\\\\], many variants are proposed to improve quality \\\\[2\u20134, 24, 45, 47\u201349, 57\\\\], increase speed \\\\[8, 9, 11, 14, 20, 32, 36, 37, 40, 43, 46, 58, 60\\\\], extend to dynamic scenes \\\\[7, 15, 18, 25, 27, 50\\\\], and so on. Some methods \\\\[41, 44, 52, 53, 61\\\\] scale it to large scenes. Block-NeRF \\\\[41\\\\] divides a city into multiple blocks and distributes training views according to their positions. Mega-NeRF \\\\[44\\\\] uses grid-based division and assigns each pixel in an image to different grids through which its ray passes. Unlike these heuristics partitioning strategies, Switch-NeRF \\\\[61\\\\] introduces a mixture-of-NeRF-experts framework to learn scene decomposition. Grid-NeRF \\\\[53\\\\] does not perform scene decomposition, but rather uses an integration of NeRF-based and grid-based methods. While the rendering quality of these methods is significantly improved over traditional ones, they still lack details and render slowly. Recently, 3D Gaussian Splatting \\\\[21\\\\] introduces an expressive explicit 3D representation with high-quality and real-time rendering at 1080p resolution. However, it is non-trivial to scale it up to large scenes. Our VastGaussian is the first one to do so with novel designs for scene partitioning, optimizing, and merging.\\n\\n2.2. Varying Appearance Modeling\\n\\nAppearance variation is a common problem in image-based reconstruction under changing lighting or different camera setting such as auto-exposure, auto-white-balance and tone-mapping. NRW \\\\[30\\\\] trains an appearance encoder in a data-driven manner with a contrastive loss, which takes a deferred-shading deep buffer as input and produces an appearance embedding (AE). NeRF-W \\\\[29\\\\] attaches AEs to point-based features in ray-marching, and feeds them into an MLP to obtain the final colors, which becomes a standard practice in many NeRF-based methods \\\\[41, 44, 61\\\\]. Ha-NeRF \\\\[10\\\\] makes AE a global representation across different views, and learns it with a view-consistent loss. In our VastGaussian, we concatenate AEs with rendered images, feed them into a CNN to obtain transformation maps, and use the transformation maps to adjust the rendered images to fit the appearance variations.\\n\\n3. Preliminaries\\n\\nIn this paper, we propose VastGaussian for large scene reconstruction and rendering based on 3D Gaussian Splatting (3DGS) \\\\[21\\\\]. 3DGS represents the geometry and appearance via a set of 3D Gaussians $G$. Each 3D Gaussian is characterized by its position, anisotropic covariance, opacity, and spherical harmonic coefficients for view-dependent colors. During the rendering process, each 3D Gaussian is projected to the image space as a 2D Gaussian. The projected 2D Gaussians are assigned to different tiles, sorted and alpha-blended into a rendered image in a point-based volume rendering manner \\\\[63\\\\].\\n\\nThe dataset used to optimize a scene contains a sparse point cloud $P$ and training views $V$ = \\\\{(C_i, I_i)\\\\}, where $C_i$ is the $i$-th camera, and $I_i$ is the corresponding image. $P$ and $\\\\{C_i\\\\}$ are estimated by Structure-from-Motion (SfM) from $\\\\{I_i\\\\}$. $P$ is used to initialize 3D Gaussians, and $V$ is used for differentiable rendering and gradient-based optimization of 3D Gaussians. For camera $C_i$, the rendered image $I_r = R(G, C_i)$ is obtained by a differentiable rasterizer $R$. The properties of 3D Gaussians are optimized with respect to the loss function between $I_r$ and $I_i$ as follows:\\n\\n$$L = (1 - \\\\lambda)L_1(I_r, I_i) + \\\\lambda L_{D-SSIM}(I_r, I_i), \\\\quad (1)$$\\n\\nwhere $\\\\lambda$ is a hyper-parameter, and $L_{D-SSIM}$ denotes the D-SSIM loss \\\\[21\\\\]. This process is interleaved with adaptive point densification, which is triggered when the cumulative gradient of the point reaches a certain threshold.\\n\\n4. Method\\n\\n3DGS \\\\[21\\\\] works well on small and object-centric scenes, but it struggles when scaled up to large environments due to video memory limitation, long optimization time, and appearance variations. In this paper, we extend 3DGS to large scenes for real-time and high-quality rendering. We propose to partition a large scene into multiple cells that are merged after individual optimization. In Sec. 4.1, we introduce a progressive data partitioning strategy with airspace-aware visibility calculation. Sec. 4.2 elaborates how to optimize individual cells, presenting our decoupled appearance modeling to capture appearance variations in images. Finally, we describe how to merge these cells in Sec. 4.3.\\n\\n4.1. Progressive Data Partitioning\\n\\nWe partition a large scene into multiple cells and assign parts of the point cloud $P$ and views $V$ to these cells for optimization. Each of these cells contains a smaller number of 3D Gaussians, which is more suitable for optimization with lower memory capacity, and requires less training time when optimized in parallel. The pipeline of our progressive data partitioning strategy is shown in Fig. 3.\"}"}
{"id": "CVPR-2024-2593", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Camera-position-based region division. As shown in Fig. 3(a), we partition the scene based on the projected camera positions on the ground plane, and make each cell contain a similar number of training views to ensure balanced optimization between different cells under the same number of iterations. Without loss of generality, assuming that a grid of \\\\( m \\\\times n \\\\) cells fits the scene in question well, we first partition the ground plane into \\\\( m \\\\) sections along one axis, each containing approximately \\\\( \\\\frac{|V|}{m} \\\\) views. Then each of these sections is further subdivided into \\\\( n \\\\) segments along the other axis, each containing approximately \\\\( \\\\frac{|V|}{(m \\\\times n)} \\\\) views. Although here we take grid-based division as an example, our data partitioning strategy is also applicable to other geography-based division methods, such as sectorization and quadtrees.\\n\\nPosition-based data selection. As illustrated in Fig. 3(b), we assign part of the training views \\\\( V \\\\) and point cloud \\\\( P \\\\) to each cell after expanding its boundaries. Specifically, let the \\\\( j \\\\)-th region be bounded in a \\\\( \\\\ell_{h} \\\\times \\\\ell_{w} \\\\) rectangle; the original boundaries are expanded by a certain percentage, 20\\\\% in this paper, resulting in a larger rectangle of size \\\\( (\\\\ell_{h} + 0.2 \\\\ell_{h}) \\\\times (\\\\ell_{w} + 0.2 \\\\ell_{w}) \\\\). We partition the training views \\\\( V \\\\) into \\\\( \\\\{V_j\\\\}_{j=1}^{mn} \\\\) based on the expanded boundaries, and segment the point cloud \\\\( P \\\\) into \\\\( \\\\{P_j\\\\}_{j=1}^{mn} \\\\) in the same way.\\n\\nVisibility-based camera selection. We find that the selected cameras in the previous step are insufficient for high-fidelity reconstruction, which can lead to poor detail or floater artifact. To solve this problem, we propose to add more relevant cameras based on a visibility criterion, as shown in Fig. 3(c). Given a yet-to-be-selected camera \\\\( C_i \\\\), let \\\\( \\\\Omega_{ij} \\\\) be the projected area of the \\\\( j \\\\)-th cell in the image \\\\( I_i \\\\), and let \\\\( \\\\Omega_i \\\\) be the area of \\\\( I_i \\\\); visibility is defined as \\\\( \\\\frac{\\\\Omega_{ij}}{\\\\Omega_i} \\\\). Those cameras with a visibility value greater than a predefined threshold \\\\( T_h \\\\) are selected. Note that different ways of calculating \\\\( \\\\Omega_{ij} \\\\) result in different camera selections. As illustrated in Fig. 3(e), a natural and naive solution is based on the 3D points distributed on the object surface. They are projected on \\\\( I_i \\\\) to form a convex hull of area \\\\( \\\\Omega_{surf_{ij}} \\\\). This calculation is airspace-agnostic because it takes only the surface into account. Therefore some relevant cameras are not selected due to its low visibility on the \\\\( j \\\\)-th cell in this calculation, which results in under-supervision for airspace, and cannot suppress floaters in the air.\\n\\nWe introduce an airspace-aware visibility calculation, as shown in Fig. 3(f). Specifically, an axis-aligned bounding box is formed by the point cloud in the \\\\( j \\\\)-th cell, whose height is chosen as the distance between the highest point and the ground plane. This calculation takes into account both the surface and the airspace, resulting in a more accurate visibility metric for camera selection.\\n\\nFloaters caused by depth ambiguity with improper point initialization, which cannot be eliminated without sufficient supervision from training cameras.\"}"}
