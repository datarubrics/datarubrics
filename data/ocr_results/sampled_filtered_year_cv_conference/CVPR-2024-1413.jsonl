{"id": "CVPR-2024-1413", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] 3d scan store. https://www.3dscanstore.com/. Accessed: 2023-11-13.\\n\\n[2] Mohammadamin Aliari, Tiberiu Beauchamp, Andre Popa, and Eric Paquette. Face editing using part-based optimization of the latent space. Computer Graphics Forum, 2023.\\n\\n[3] Tristan Aumentado-Armstrong, Stavros Tsogkas, Allan Jepson, and Sven Dickinson. Geometric disentanglement for generative latent shape models. In 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 8180\u20138189, 2019.\\n\\n[4] Volker Blanz and Thomas Vetter. A morphable model for the synthesis of 3d faces. In Proceedings of the 26th Annual Conference on Computer Graphics and Interactive Techniques, page 187\u2013194, USA, 1999. ACM Press/Addison-Wesley Publishing Co.\\n\\n[5] James Booth, Anastasios Roussos, Stefanos Zafeiriou, Allan Ponniah, and David Dunaway. A 3d morphable model learnt from 10,000 faces. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5543\u20135552, 2016.\\n\\n[6] James Booth, Epameinondas Antonakos, Stylianos Ploumpis, George Trigeorgis, Yannis Panagakis, and Stefanos Zafeiriou. 3d face morphable models\u201c in-the-wild\u201d. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 48\u201357, 2017.\\n\\n[7] Giorgos Bouritsas, Sergiy Bokhnyak, Stylianos Ploumpis, Michael Bronstein, and Stefanos Zafeiriou. Neural 3d morphable models: Spiral convolutional networks for 3d shape representation learning and generation. In The IEEE International Conference on Computer Vision (ICCV), 2019.\\n\\n[8] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In Computer Vision \u2013 ECCV 2020, pages 213\u2013229, Cham, 2020. Springer International Publishing.\\n\\n[9] Lucy Chai, Richard Tucker, Zhengqi Li, Phillip Isola, and Noah Snavely. Persistent nature: A generative model of unbounded 3d worlds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20863\u201320874, 2023.\\n\\n[10] Richard J Chen, Ming Y Lu, Tiffany Y Chen, Drew F K Williamson, and Faisal Mahmood. Synthetic data in machine learning for medicine and healthcare. Nat. Biomed. Eng., 5(6):493\u2013497, 2021.\\n\\n[11] Blender Online Community. Blender - a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018.\\n\\n[12] Enric Corona, Mihai Zanfir, Thiemo Alldieck, Eduard Gabriel Bazavan, Andrei Zanfir, and Cristian Sminchisescu. Structured 3d features for reconstructing relightable and animatable avatars. In CVPR, 2023.\\n\\n[13] Micha\u00ebl Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks on graphs with fast localized spectral filtering. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2016.\\n\\n[14] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 4171\u20134186, Minneapolis, Minnesota, 2019. Association for Computational Linguistics.\\n\\n[15] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\\n\\n[16] Bernhard Egger, William A. P. Smith, Ayush Tewari, Stefanie Wuhrer, Michael Zollhoefer, Thabo Beeler, Florian Bernard, Timo Bolkart, Adam Kortylewski, Sami Romdhani, Christian Theobalt, Volker Blanz, and Thomas Vetter. 3D Morphable Face Models \u2013 Past, Present and Future. arXiv e-prints, 2019.\\n\\n[17] Simone Foti, Bongjin Koo, Danail Stoyanov, and Matthew J. Clarkson. 3d shape variational autoencoder latent disentanglement via mini-batch feature swapping for bodies and faces. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18730\u201318739, 2022.\\n\\n[18] Simone Foti, Bongjin Koo, Danail Stoyanov, and Matthew J. Clarkson. 3d generative model latent disentanglement via local eigenprojection. Computer Graphics Forum, 42(6):e14793, 2023.\\n\\n[19] Baris Gecer, Stylianos Ploumpis, Irene Kotsia, and Stefanos Zafeiriou. Ganfit: Generative adversarial network fitting for high fidelity 3d face reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1155\u20131164, 2019.\\n\\n[20] Donya Ghafourzadeh, Sahel Fallahdoust, Cyrus Rahgoshay, Andre Beauchamp, Adeline Aubame, Tiberiu Popa, and Eric Paquette. Local control editing paradigms for part-based 3D face morphable models. Computer Animation and Virtual Worlds, 32(6):e2028.\\n\\n[21] Shunwang Gong, Lei Chen, Michael Bronstein, and Stefanos Zafeiriou. Spiralnet++: A fast and highly efficient mesh convolution operator. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 0\u20130, 2019.\\n\\n[22] Dan Hendrycks and Kevin Gimpel. Bridging nonlinearities and stochastic regularizers with gaussian error linear units. CoRR, abs/1606.08415, 2016.\\n\\n[23] Loshchilov Ilya, Hutter Frank, et al. Decoupled weight decay regularization. Proceedings of ICLR, 2019.\\n\\n[24] Firas Khader, Gustav M\u00fcller-Franzes, Soroosh Tayebi Arasteh, Tianyu Han, Christoph Haarburger, Maximilian Schulze-Hagen, Philipp Schad, Sandy Engelhardt, Bettina 1875.\"}"}
{"id": "CVPR-2024-1413", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bae\u00dfler, Sebastian Foersch, Johannes Stegmaier, Christiane Kuhl, Sven Nebelung, Jakob Nikolas Kather, and Daniel Truhn. Denoising diffusion probabilistic models for 3D medical image generation. *Scientific Reports*, 13:7303, 2023.\\n\\n[25] Alexandros Lattas, Stylianos Moschoglou, Stylianos Ploumpis, Baris Gecer, Jiankang Deng, and Stefanos Zafeiriou. Fitme: Deep photorealistic 3d morphable model avatars. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 8629\u20138640, 2023.\\n\\n[26] Tianye Li, Timo Bolkart, Michael. J. Black, Hao Li, and Javier Romero. Learning a model of facial shape and expression from 4D scans. *ACM Transactions on Graphics*, (Proc. SIGGRAPH Asia), 36(6):194:1\u2013194:17, 2017.\\n\\n[27] Isaak Lim, Alexander Dielen, Marcel Campen, and Leif Kobbelt. A simple approach to intrinsic correspondence learning on unstructured 3d meshes. In *Computer Vision \u2013 ECCV 2018 Workshops: Munich, Germany, September 8-14, 2018, Proceedings, Part III*, page 349\u2013362, Berlin, Heidelberg, 2019. Springer-Verlag.\\n\\n[28] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. In *International Conference on Learning Representations*, 2017.\\n\\n[29] Foivos Paraperas Papantoniou, Alexandros Lattas, Stylianos Moschoglou, and Stefanos Zafeiriou. Relightify: Relightable 3d faces from a single image via diffusion models. In *Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV)*, 2023.\\n\\n[30] Pascal Paysan, Reinhard Knothe, Brian Amberg, Sami Romdhani, and Thomas Vetter. A 3d face model for pose and illumination invariant face recognition. In *2009 Sixth IEEE International Conference on Advanced Video and Signal Based Surveillance*, pages 296\u2013301, 2009.\\n\\n[31] Stylianos Ploumpis, Haoyang Wang, Nick Pears, William AP Smith, and Stefanos Zafeiriou. Combining 3d morphable models: A large scale face-and-head model. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 10934\u201310943, 2019.\\n\\n[32] Stylianos Ploumpis, Evangelos Ververas, Eimear O'Sullivan, Stylianos Moschoglou, Haoyang Wang, Nick Pears, William AP Smith, Baris Gecer, and Stefanos Zafeiriou. Towards a complete 3d morphable model of the human head. *IEEE transactions on pattern analysis and machine intelligence*, 43(11):4142\u20134160, 2020.\\n\\n[33] Stylianos Ploumpis, Stylianos Moschoglou, Vasileios Triantafyllou, and Stefanos Zafeiriou. 3d human tongue reconstruction from single in-the-wild images. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 2771\u20132780, 2022.\\n\\n[34] Rolandos Alexandros Potamias, Stylianos Ploumpis, Stylianos Moschoglou, Vasileios Triantafyllou, and Stefanos Zafeiriou. Handy: Towards a high fidelity 3d hand shape and appearance model. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 4670\u20134680, 2023.\\n\\n[35] Dafei Qin, Jun Saito, Noam Aigerman, Groueix Thibault, and Taku Komura. Neural face rigging for animating and retargeting facial meshes in the wild. In *SIGGRAPH 2023 Conference Papers*, 2023.\\n\\n[36] Anurag Ranjan, Timo Bolkart, Soubhik Sanyal, and Michael J. Black. Generating 3D faces using convolutional mesh autoencoders. In *European Conference on Computer Vision (ECCV)*, pages 725\u2013741, 2018.\\n\\n[37] Michael Stengel, Koki Nagano, Chao Liu, Matthew Chan, Alex Trevithick, Shalini De Mello, Jonghyun Kim, and David Luebke. Ai-mediated 3d video conferencing. In *ACM SIGGRAPH Emerging Technologies*, 2023.\\n\\n[38] Michail Tarasiou, Erik Chavez, and Stefanos Zafeiriou. Vits for sits: Vision transformers for satellite image time series. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 10418\u201310428, 2023.\\n\\n[39] Ilya O Tolstikhin, Neil Houlsby, Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Thomas Unterthiner, Jessica Yung, Andreas Steiner, Daniel Keysers, Jakob Uszkoreit, Mario Lucic, and Alexey Dosovitskiy. Mlp-mixer: An all-mlp architecture for vision. In *Advances in Neural Information Processing Systems*, pages 24261\u201324272. Curran Associates, Inc., 2021.\\n\\n[40] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herve Jegou. Training data-efficient image transformers & distillation through attention. In *International Conference on Machine Learning*, pages 10347\u201310357, 2021.\\n\\n[41] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In *Advances in Neural Information Processing Systems 30*, pages 5998\u20136008. Curran Associates, Inc., 2017.\\n\\n[42] Tengfei Wang, Bo Zhang, Ting Zhang, Shuyang Gu, Jianmin Bao, Tadas Baltrusaitis, Jingjing Shen, Dong Chen, Fang Wen, Qifeng Chen, and Baining Guo. Rodin: A generative model for sculpting 3d digital avatars using diffusion. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 4563\u20134573, 2023.\\n\\n[43] Peizhi Yan, James Gregson, Qiang Tang, Rabab Ward, Zhan Xu, and Shan Du. Neo-3df: Novel editing-oriented 3d face creation and reconstruction. In *Proceedings of the Asian Conference on Computer Vision*, pages 486\u2013502, 2022.\\n\\n[44] Haotian Zhang, Ye Yuan, Viktor Makoviychuk, Yunrong Guo, Sanja Fidler, Xue Bin Peng, and Kayvon Fatahalian. Learning physically simulated tennis skills from broadcast videos. *ACM Trans. Graph.* 2023.\"}"}
{"id": "CVPR-2024-1413", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nWe present the Locally Adaptive Morphable Model (LAMM), a highly flexible Auto-Encoder (AE) framework for learning to generate and manipulate 3D meshes. We train our architecture following a simple self-supervised training scheme in which input displacements over a set of sparse control vertices are used to overwrite the encoded geometry in order to transform one training sample into another. During inference, our model produces a dense output that adheres locally to the specified sparse geometry while maintaining the overall appearance of the encoded object. This approach results in state-of-the-art performance in both disentangling manipulated geometry and 3D mesh reconstruction. To the best of our knowledge LAMM is the first end-to-end framework that enables direct local control of 3D vertex geometry in a single forward pass. A very efficient computational graph allows our network to train with only a fraction of the memory required by previous methods and run faster during inference, generating 12k vertex meshes at >60fps on a single CPU thread.\\n\\nWe further leverage local geometry control as a primitive for higher level editing operations and present a set of derivative capabilities such as swapping and sampling object parts. Code and pretrained models can be found at https://github.com/michaeltrs/LAMM.\\n\\n1. Introduction\\n\\nThe capacity to generate and manipulate digital 3D objects lies at the core of a multitude of applications related to the entertainment and media industries [9, 44], virtual and augmented reality [12, 25, 29, 37, 42] and healthcare [10, 24]. In particular, the ability to manually shape virtual humans can result in highly realistic avatars, while granting ultimate creative control to individual users.\\n\\nHowever, achieving fine control in mesh manipulation necessitates the learning of a disentangled representation of 3D shapes which is still an open research problem [16\u201318]. Recently proposed methods based on Graph Convolutional Network (GCN)-based Auto-Encoders (AEs) [21, 27, 36] have demonstrated impressive performance in dimensionality reduction but typically learn a highly entangled latent space making them unsuitable for detailed shape manipulation. Additionally, despite having a low parameter count, these methods struggle to handle high-resolution meshes, limiting their applicability. Few works [17, 18] have dealt with this challenge.\\n\\nFigure 1. Overview of the Locally Adaptive Morphable Model (LAMM) use during inference. (top) Our trained decoder $f_{dec}$ receives as inputs a latent code $z$ and displacements $\\\\delta V_C$ over a set of sparse control points (red vertices). Here displacements for control points in the nose region are shown with green arrows. The decoder generates the shape of the object encoded in $z$, overwriting local geometry to respect $\\\\delta V_C$. (bottom) We can sample latent codes to generate new instances of human heads in the (a) identity, (b) expression space. Similarly, we can either randomly sample or provide vertex displacements manually at control points to manipulate (c) local identity features (ears, nose, mouth shown here) and (d) add expressions while retaining identity features unchanged.\"}"}
{"id": "CVPR-2024-1413", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"with the disentanglement of local identity attributes, however these methods still rely on GCNs and opt for controlling manipulations through the state of the latent code which is partitioned and assigned to predefined object regions. Using the latent code to drive shape manipulation requires the use of explicit optimization objectives to learn a disentangled latent space. Moreover, partitioning its state is critically suboptimal for learning compressed representations of 3D objects. We propose a different paradigm which does not involve partitioning the latent code or relying on its state to drive changes in shape, resulting in state-of-the-art (SOTA) disentanglement and reconstruction capabilities in a unified architecture. Instead, we use a global latent code for 3D object unconditional generation and utilise additional inputs to jointly train our generative model to locally overwrite the latent encoded geometry. Our main contributions are as follows:\\n\\n\u2022 We present the Locally Adaptive Morphable Model (LAMM), a general framework for manipulating the geometry of registered meshes. To the best of our knowledge, this is the first method that allows direct shape control with a single forward pass. Applied on human 3D heads, LAMM exhibits SOTA disentanglement properties and allows for very fine geometric control over both facial identities and expressions.\\n\\n\u2022 Our models, trained for manipulation, concurrently exhibit SOTA performance in mesh dimensionality reduction compared against methods trained exclusively on this task. As a result, a single model can be used to generate entirely new shapes and apply both localized and global modifications to their geometry.\\n\\n\u2022 We show how our framework can leverage direct control as a primitive to achieve higher level editing operations such as region swapping and sampling.\\n\\n\u2022 By deviating from GCN-based AE design, LAMM can scale to much larger meshes, needs only a fraction of GPU memory to train and can be significantly faster during inference compared to competing methods. For example, trained on 72k vertex meshes with batch size 32, our model requires 7.5GB of GPU memory and runs at 0.045s on a single CPU thread. This model outperforms SpiralNet++ [21] which equivalently requires >40GB of memory and runs \u00d713 slower at 0.58s.\\n\\n\u2022 We release training and evaluation codes to support future research. Additionally we will release pre-trained models and a Blender add-on [11] to facilitate intuitive mesh manipulation and fine-grained control.\\n\\n2. Related Work\\n\\n3D Morphable Models. Blanz and Vetter pioneered an innovative approach for synthesis and reconstruction of 3D human faces through their seminal 3D Morphable Model (3DMM) [30]. First, they establish a fixed mesh topology, ensuring that each face vertex is semantically significant and could be uniquely identified, e.g. the \u201ctip of the nose\u201d. Subsequently, Principal Component Analysis (PCA) is employed to map 3D shapes and textures into a lower dimensional latent code. By adjusting these codes the PCA-based 3DMM can seamlessly generate new facial instances. Such 3DMMs are simple and very robust in terms of performance. They are well established as exceptionally reliable frameworks for 3D face/body analysis, finding applications in numerous human centric models [5, 16, 19, 25, 26, 30\u201333] which remain relevant to this date.\\n\\nHowever, the linear formulation of PCA limits its expressivity and capacity to capture high frequency details. In an effort to capture nonlinear variations in the shape of human faces performing expressions [36] were the first to utilise Graph Convolutional Networks (GCN) for 3D dimensionality reduction. Their Convolutional Mesh AutoEncoder (COMA) makes use of fast and efficient spectral convolutions applied on a mesh surface [13] to compress and generate 3D meshes. An inductive bias more suitable for registered meshes is proposed in [7] who incorporate the spiral operator [27] in a GCN-based AE. This operator uses a spiral sequence to define an explicit local ordering for aggregating node features, thus giving the convolution kernels a sense of orientation across the mesh surface and greater expressive power. Following a similar principle, [21] further improve the performance of this architecture by enabling context aggregation across larger areas through dilated spiral operators. The deep GCN-based family of models discussed above have been consistently shown to outperform PCA at high levels of compression. However, when increasing the size of the latent space to values that are practical for applications, e.g. monocular 3D face reconstruction [6, 19], PCA is shown to match and quickly surpass GCN architectures. We argue that the translation equivariant property of GCNs is not optimal for modelling registered meshes as it does not account for the distinct semantics of each vertex. In contrast, by employing fully connected layers to encode input shapes into tokens and a global receptive field for downstream processing, our framework respects this attribute of the data, having the capacity to treat similar features differently depending on their location.\\n\\nFace Manipulation. All statistical models of 3D meshes discussed above can be used to manipulate mesh geometry, e.g. the optimal fit for a given shape can be discerned through latent code optimization. The main issue with this approach is that latent spaces learnt for generative modelling are typically highly entangled [3, 16, 17, 35]. This attribute discards the possibility for finely-tuned control of local geometry as varying any one dimension of the latent code can concurrently impact multiple properties of the reconstructed data. Numerous studies have been proposed to specifically address the challenge of entanglement.\"}"}
{"id": "CVPR-2024-1413", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2. Architecture overview. A source mesh $V_s$ is encoded into latent code $z$ and decoded using additional displacements at control points $\\\\delta V_C$. (top) Tokenization and encoder modules. A 3D input mesh $V_s$ is split into regions $V_s[R_i]$, each region is tokenized via region-specific linear weights $W_{in}^i$ and compressed by the encoder module into latent code $z$. (bottom) Displacement control, decoder and inverse tokenization modules. User displacements at control points are split into corresponding regions $C_i$, processed by region specific control networks $f_{\\\\delta_i}$ and used by the decoder to overwrite the geometry encoded in $z$. The inverse tokenization module translates decoder outputs into region geometries, via region-specific linear weights $W_{out}^i$, which are merged together, using $R_i$, into the target estimate $\\\\hat{V}_t$.}\\n\\nTangled representations of 3D face data for shape manipulation [16]. They predominantly utilize face regions to disentangle a latent representation and achieve local control through optimization, either by employing linear models [20] or GCN [2, 17, 18, 43]. Notably, works by [17, 18] adeptly deform 3D face geometry to manipulate an individual's identity. Both works harness the backbone architecture from [21], along with various face regions, each governed by non-overlapping parts of the latent code. In [17] their AE learns a disentangled latent space by creating a composite batch through swapping arbitrary features across different samples. This enables the definition of a contrastive loss that leverages known differences and similarities in the latent representations. In a related approach, [18] also segment their latent codes to region specific parts and use a loss component forcing segments to adhere to the local eigenprojections of identity attributes. Similar to these works, we also employ predefined regions to segment our inputs. Importantly, we do not partition our latent codes into region-specific segments, which is beneficial for retaining strong performance in mesh reconstruction. Furthermore, we do not enforce spatial disentanglement through explicitly defined loss components (we train only with $L_1$ loss) but rather reach SOTA disentanglement performance solely as an emerging property of our architecture.\\n\\n3. Method\\n\\nIn this section we present the LAMM framework for the disentangled direct manipulation of 3D shapes. First, we describe the architecture, followed by a self-supervised training scheme for learning direct local control over mesh geometry. In the following, we assume a 3D mesh is represented as $M = \\\\{V, F\\\\}$, where $V \\\\in \\\\mathbb{R}^{N \\\\times 3}$ is a set of $N$ vertices represented as points in 3D space, and $F$ is a set of faces defining a shared topology.\\n\\n3.1. Architecture\\n\\nWe leverage the AE framework for mesh representation learning, with targeted modifications in the decoder architecture to facilitate direct manipulation of mesh geometry. First, a source mesh $V_s$ is encoded into a latent space $z = f_{enc}(V_s) \\\\in \\\\mathbb{R}^D$ and subsequently decoded into an estimated target geometry $\\\\hat{V}_t = f_{dec}(z, \\\\delta V_C)$ using control vertex displacements $\\\\delta V_C$ as additional input to the decoder. Both encoder and decoder modules utilise patch-based backbones, namely the Transformer [41] and MLP-Mixer [39], that operate on sets of tokens and exhibit a homogeneous feature space, i.e. feature dimensions stay constant throughout the network. An overview of the proposed architecture is presented schematically in Fig.2.\"}"}
{"id": "CVPR-2024-1413", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tokenization. For deriving the inputs to the encoder we first split each source object into $K$ non-overlapping dense regions, $R_i \\\\subseteq \\\\{1, 2, \\\\ldots, N\\\\}$ (Fig. 3). We further flatten each region into a vector of size $v_{s_i} \\\\in \\\\mathbb{R}^{3N_i}$ and tokenize it as a linear projection $W_{\\\\text{in}} v_{s_i}$, where $W_{\\\\text{in}} \\\\in \\\\mathbb{R}^{D \\\\times 3N_i}$. The proposed tokenization scheme differs significantly from the one used in [15, 39] that operate on fixed-size square patches. First, no parameter sharing takes place during this step. Instead each tokenization weight is uniquely assigned to and solely applied to a corresponding input vertex coordinate, thus, respecting the semantic meaning of each vertex. This allows us to define regions with varying number of vertices that are designed to include distinct object parts based on the template mesh, e.g. a \u201cwhole eye\u201d in human face meshes. Finally, our regions are typically much larger than commonly employed in computer vision [15, 38, 39], which results in a small number of tokens. For example, while the original implementation of ViT [15] used 256 tokens for image classification, we use as few as 11 tokens for 3D face modelling. Since the Transformer space and time complexities are both quadratic to the number of tokens, our models are very efficient in terms of memory consumption, can scale to very large meshes and can run significantly faster than GCNs especially on a CPU.\\n\\nEncoder. Our fixed size tokenized representation for region $i$ is $x_0^i = W_{\\\\text{in}} v_{s_i}$. To these features we prepend a learnable identity token $x_0 \\\\in \\\\mathbb{R}^D$ [14, 40] which is independent of the input. After processing by the $L$-layer homogeneous encoder, we retain only the state of the first index token $x_L^0 \\\\in \\\\mathbb{R}^D$ that forms the encoded representation of the source geometry. To obtain our latent code we linearly project this vector into a space of desired dimensionality $z = W_{\\\\text{down}} x_L^0 \\\\in \\\\mathbb{R}^d$. The tokenization and encoder modules are illustrated in the top part of Fig. 2.\\n\\nDecoder. In the decoder module, we first linearly project the latent code back into a $D$-dimensional vector $y_0 = W_{\\\\text{up}} z$. Analogously to the encoder, we build our decoder inputs by appending $K$ additional learned tokens $y_0^i \\\\in \\\\mathbb{R}^D$ to the projected latent code $y_0$. We then select a set of non-overlapping, sparse vertex indices $C_i \\\\subseteq R_i$ that are used as control points (Fig. 3). We gather all desired displacements at control points per region $\\\\delta V_{C_i} = (V_t - V_s)|_{C_i} \\\\in \\\\mathbb{R}^{3|C_i|}$ and process each with a fully connected network $f_{\\\\delta i}$ the output of which is added to respective learned tokens $y_0^i, i > 0$. We design $f_{\\\\delta i}$ with GELU activations [22] and no bias terms. In this manner, for the special case of $\\\\delta V_{C_i} = 0$ we get $f_{\\\\delta i}(0) = 0$ and LAMM seamlessly reproduces the behaviour of an AE.\\n\\nTokenization\u22121. Following the inverse process of region splitting and tokenization steps we now project each token independently to a vector $\\\\hat{v}_t^i = W_{\\\\text{out}} y_L^i \\\\in \\\\mathbb{R}^{3N_i}$, reshape its dimensions to $N_i \\\\times 3$ and use $R_i$ to merge regions into a unified 3D template mesh, $\\\\hat{V}_t$. The decoder and inverse tokenization modules are presented schematically in the bottom part of Fig. 2.\\n\\n3.2. Training\\n\\nSelf-supervised objective. At each step we sample $B$ source and target elements $V_s, V_t \\\\in \\\\mathbb{R}^{B \\\\times N \\\\times 3}$ from the training set and construct our source and target batches respectively as $\\\\{V_s, V_s\\\\}$ and $\\\\{V_s, V_t\\\\}$. In this manner, the first half of our batch is used for AE training and the second half for learning source-to-target transformations. For identity manipulation we use a random permutation of $V_s$ as $V_t$. For expression manipulation, $V_s$ and $V_t$ include the same identities with neutral and non-neutral expressions. Training with direct supervision from real target data has the benefit of providing realistic examples for the target geometry space. However, there are a few issues with this approach. In practice, interactive vertex control is an iterative process. As a result typical user input displacements are expected to be smaller and significantly sparser than the ones that globally transform one sample into another. To reduce the effect of displacement size discrepancy we train with a linear combination of source and target samples $\\\\alpha V_s + (1 - \\\\alpha) V_t, \\\\alpha \\\\sim U(\\\\alpha_{\\\\text{min}}, 1)$ as the new target values. This effectively reduces the absolute values of displacements encountered during training in addition to augmenting the target space. The sparsity discrepancy is a trickier problem as it is not trivial to find realistic training pairs with only localized differences. We find that this issue is solved to a satisfactory degree implicitly as the proposed architecture naturally propagates deformations only to non-zero-displacement regions. We believe this attribute to be an intrinsic property of our architecture. In support of this claim, we show in the supplementary material that only through AE training, i.e. $\\\\delta V_{C_i} = 0$, corrupting the values of learnable decoder tokens $y_0^i, i > 0$ influences only their corresponding region geometries.\\n\\nMultilayer Loss. To adequately train our architecture, a distance-based loss between the target and output geometries is applied which suits our fundamental learning objective. In all experiments we utilize the $L_1$ distance metric.\"}"}
{"id": "CVPR-2024-1413", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"calculated as $||V_t - V_{t-1}||_1$. We introduce an extension to this loss, applied at every level of the model, which increases the encoder's capacity to encode input-related information and guides the decoder transformation from mean to target shape. Both employed backbones [39, 41] produce homogeneous feature spaces of size $D$ across all encoder and decoder layers. At every layer $l$ we decode region features $x_l$, $y_l$, $i \\\\geq 1$ using $W_{out}$ and merge regions together to obtain a multilayer output geometry.\\n\\nThe objective of the encoder is to obtain a compressed representation of the input in the form of the latent code $z$. At input, we want our tokens to retain as much information about the input shape to begin with, so we add a loss component $||W_{out}x_0 - vs_i||_1$ that encourages the input tokens to decode close to the source geometry. Furthermore, we provide some guidance to our encoder to progressively discard input related information from the region features through a loss component $||W_{out}x_L - \\\\bar{v}_i||_1$ that encourages the final encoder region tokens to match the mean shape for each region $\\\\bar{v}_i$ over the training data. Similarly, we expand this pattern for all intermediate layers in a linear fashion.\\n\\n$L_{enc} = \\\\sum_{l=0}^{K} \\\\lambda_l \\\\sum_{i=1}^{K} ||W_{out}x_l - 1||_1 (l vs_i + (L - l) \\\\bar{v}_i)$ (1)\\n\\n$L_{dec} = \\\\sum_{l=0}^{K} \\\\lambda_l \\\\sum_{i=1}^{K} ||W_{out}y_l - 1||_1 (l vt_i + (L - l) \\\\bar{v}_i)$ (2)\\n\\nIn both eqs. 1, 2 $\\\\lambda_l$ is a weight for the loss component at layer $l$. In practice we opt for a simple solution of keeping all $\\\\lambda_l = 1$ which leads to stable training and good generalization performance. We note that in both equations the inverse tokenization weights $W_{out}$ are shared in every loss term, forcing a common geometric representation throughout the network which progressively transforms from source to mean to target.\\n\\nAdditionally, our data are typically mean centered prior to any processing, thus $\\\\bar{v}_i$ reduces to the zero vector. In this case, our multilayer loss can be viewed as a form of regularization forcing the values of $W_{out}$ to be either small in values or orthogonal to the learnt tokens $y_0$.\\n\\n4. Experiments\\n\\n4.1. Implementation details\\n\\nDatasets. For effectively learning disentangled human head identity manipulations we need a large scale dataset of 3D heads in neutral expression. For this reason, we register 6k heads from the original data used in the UHM model [32] into a new quads-based template which consists of 12k vertices, is more friendly towards rigging and is animation ready. We will refer to this data as UHM12k. To create our synthetic expression data we applied a set of 28 blendshapes [1] to the UHM12k data. To experiment with high resolution meshes, we utilise the linear models provided by the UHM [32] and sample 10k identities consisting of 72k vertices. To evaluate our method on hand data, we utilized Handy [34], a large-scale hand model, trained from more than 1k distinct identities. In particular, to generate our hand dataset, we randomly sampled 10k hands from the Handy latent space. All datasets were split into training and evaluation sets at a 9:1 ratio.\\n\\nArchitecture. We optimize our architecture through an ablation study on autoencoding using the UHM12k data, presented in the supplementary material. Overall, our architecture consists of five encoder and three decoder layers with 512 feature dimension. For manipulation experiments we split the UHM12k and the Handy templates into 11 and 9 regions and utilise 73 (101 for expression modelling) and 72 control points respectively, all of which are illustrated in Figs. 3, 5, 6. For identity control with the UHM data we use the same face regions as [17, 18] and 91 control points.\\n\\nTraining. Our proposed architecture can train for significantly longer than GCN baselines without observing a drop in evaluation set performance. We train all networks (including baselines) in mesh reconstruction and manipulation for 1,500 epochs on a single Nvidia Titan X GPU, with batch size 32 (adjusted for GCNs according to memory). We use the AdamW optimizer [23] starting with a 10 epoch learning rate warm up to a $10^{-4}$ which is decayed via a 1-cycle cosine decay [28] to $10^{-6}$. For local control experiments, we find it is crucial to initialize our models from checkpoints pre-trained exclusively for autoencoding. Further details are provided in the supplementary material.\\n\\n4.2. 3D shape reconstruction\\n\\nIn Table 1 we compare the reconstruction performance of our proposed framework for autoencoding against PCA [4] and state of the art GCNs [7, 21, 36]. Overall, our method is shown to significantly outperform others applied in head and hand data with improvements being more pronounced on UHM12k. Comparing the two employed backbones, the MLPMixer is a consistently strong performer, surpassing baselines in all cases tested. Transformer-based backbones are found to be more suitable for the expression encoding.\"}"}
{"id": "CVPR-2024-1413", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Qualitative assessment of trained AE in UHM12k. (top) New shape generation through sampling of the latent space $z$, (bottom) Interpolation between latent codes for two samples from the evaluation data shows a smooth transition between identities.\\n\\nTable 1. Quantitative evaluation of 3D shape reconstruction for models trained exclusively in autoencoding with latent size 256. Presented values are mean Euclidean distances ($\\\\times 10^{-2}$ mm).\\n\\n| Model                  | UHM12k  | HMM12k+expr. |\\n|------------------------|---------|--------------|\\n| PCA                    | 10.42   | 11.49        |\\n| COMA [36]              | 13.11   | 14.20        |\\n| Neural3DMM [7]         | 11.82   | 12.97        |\\n| SpilarNet++ [21]       | 11.53   | 12.58        |\\n| LAMM-Transformer       | 9.08    | 9.09         |\\n| LAMM-MLPMixer          | 7.97    | 9.51         |\\n\\nEnhanced data, but weaker than the MLPMixer for the UHM and Handy data. In Fig. 4 (top) we present generated samples drawn from our head model. Only the decoder part of our architecture is used here. Similar to previous works [4, 7, 21, 36] sampling new instances of global shape can be achieved by fitting a Gaussian probability distribution to $z$ values collected over the training data. In the bottom part of Fig. 4 we choose two highly diverse identities from the evaluation set, encode both and present geometries generated by linear interpolation of their latent codes, noting a smooth transition between generated shapes.\\n\\n4.3. Local control\\n\\nGlobal manipulation. We present a quantitative evaluation of models trained in shape manipulation in Table 2. First, we note that AE performance is very similar to the values presented in Table 1 where models are trained exclusively in mesh reconstruction. In fact, AE performance is found to improve in many cases suggesting the complementarity of the two tasks. We also find that expression training can increase AE performance significantly. If we train for expression manipulation but select our model based on its AE performance our Transformer based model achieves a distance error of $7.79 \\\\times 10^{-2}$ mm, which is 14% less compared to the best value we obtained from AE training in Table 1 ($9.08 \\\\times 10^{-2}$ mm). Figs. 5, 6 illustrate per-vertex mean prediction errors plotted over respective templates for identity and expression manipulations. In Fig. 5 these are shown to be clearly smaller close to control points verifying the metrics reported in Table 2. For human heads, errors are mostly concentrated in the frontal region of the face. For hands, errors are significantly larger and more evenly spread. However, there are clear concentrations near the wrist which can be attributed to the absence of control points in these regions. We additionally show examples of source to target generations from the evaluation data, observing how predicted geometries match the appearance of target for both datasets. For expression manipulation, we can generally observe from Fig. 6 and Table 2 that errors are smaller compared to identity manipulation. As expected, we observe that errors are also concentrated in the face region where most of the deformation takes place and particularly small in head regions that do not participate in facial expressions such as the ears and skull. We also note that, in contrast to identity manipulation, errors are significantly lower here at non-control points which can be attributed to the fact that $/ \\\\in C_i$ includes many vertices with very small deformations, e.g. skull.\\n\\nRegion swap. For swapping regions among meshes it is critical to find a set of control point displacements that best transform the source region $V_s$ to a desired target shape $V_t$.\\n\\nHowever, calculating displacements directly results in non-realistic geometries as the two regions may be offset by a significant amount. To resolve this issue, we first rigidly align regions by their common outer edge indices. Due to the simplicity of applied transformations, rigid alignment is found not to disturb the appearance of the target shape, keeping it realistic. The outcome of this operation is reduced offsets at the boundaries, however, there are still discontinuities due to the restricted nature of rigid alignment. It is critical to note, however, that the region's control vertices, sparsely located in its interior, represent a valid sparse geometry since there exists a valid dense shape that fits these vertices while respecting the region's boundary conditions. After finding the optimal rigid transformation, it...\"}"}
{"id": "CVPR-2024-1413", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Identity manipulation performance under direct vertex control for UHM12k (left) and Handy (right) data. We observe that errors are typically reduced close to control points. For both datasets our method learns to produce highly realistic output geometries.\\n\\nFigure 6. Expression manipulation performance under direct vertex control for UHM12k data. (left) We observe that per-vertex errors are very small and that $\\\\hat{V}_t$ closely resembles the appearance of $V_t$. (right) Linear interpolation in expression intensities for the two samples. We observe a smooth transition between facial expressions and the ability to perform highly localized edits by manipulating eyes independently.\\n\\nFigure 7. Head region swapping for UHM data. We can apply to the control points of the target region to obtain the updated target locations of the control vertices, which in turn are used to calculate $\\\\delta V_{Ci}$. We employ this methodology for region swapping in human heads. Fig.7 demonstrates swapping the eyes, ears, nose and mouth regions between two diverse meshes from the UHM data.\\n\\nRegion sampling and disentanglement. We show it is possible to generate new dense region shapes by sampling valid displacements for the control vertices. Using the same approach described for region swapping, we randomly select data pairs from the training data and collect a set of valid control vertex displacements for every region. To these, we fit a Gaussian distribution from which we can sample from to produce new valid control point displacements. We present samples of generated face regions in the top part of Fig.8 using the UHM data. In the bottom part, we utilise UHM12k to illustrate face part interpolation for the ears and nose regions. We evaluate the capacity of our model to have only localized effects in manipulated geometry in Fig.9 compared to SOTA methods SD-V AE [17] and LED-V AE [18]. Consistently, we find that our method leads to more localized effects for all face regions and improved disentanglement over generated geometries. Importantly, we note that in contrast to [17], our framework is trained without any loss component aimed towards disentanglement which indicates this to be an intrinsic property of our method. We provide additional evidence to support this claim in the supplementary material. Furthermore, we find that SD-V AE has low reconstruction performance at $18.90 \\\\times 10^{-2}$ mm, more than double the reconstruction error of our method from Table 2 and more than SpiralNet++ [21] which is the backbone employed by SD-V AE. As we discuss in the supplementary material, this is mainly attributed to the choice of partitioning the latent code made by SD-V AE to control region geometries.\\n\\n5. Conclusion\\nWe have introduced LAMM, a comprehensive framework for learning 3D mesh representations and achieving spatially disentangled shape manipulation. To our knowledge, LAMM is the first end-to-end trainable method that is capable of directly manipulating mesh shape, by using desired displacements as inputs, in a single forward pass.\"}"}
{"id": "CVPR-2024-1413", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8. Random generation and interpolation of head regions. (top) Random head region generation on UHM (bottom) Region interpolation for ears and nose using UHM12k. We observe a smooth transition between generated regions while remaining areas are unaffected.\\n\\nFigure 9. Evaluation of disentanglement performance compared to SD-V AE [17], LED-V AE [18]. For all data in the evaluation set we randomly sample 20 shapes for each head region and show per-vertex mean Euclidean distances between input and output shapes.\\n\\nExtensive experiments with 3D human head and hand mesh data demonstrate that LAMM simultaneously attains state-of-the-art performance in both disentanglement and reconstruction within a unified architecture. Notably, it scales more effectively to high-resolution data, requires less memory for training, and offers faster inference speeds compared to existing methods. When implemented within the identity space of human heads, our method exhibits significant potential for applications in face sculpting. Additionally, when trained in the expression space of human heads, LAMM demonstrates considerable promise for manually enhancing digital characters with expressive details and for its application in the field of animation. Fast CPU inference makes LAMM an energy efficient and economically viable option, democratizing access to advanced 3D modeling.\\n\\nAcknowledgements\\nRP and SZ were supported by GNOMON (EP/X011364). SZ was supported by EPSRC Project DEFORM (EP/S010203/1).\"}"}
