{"id": "CVPR-2024-708", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Each Test Image Deserves A Specific Prompt: Continual Test-Time Adaptation for 2D Medical Image Segmentation\\n\\nZiyang Chen\\nYongsheng Pan\\nYiwen Ye\\nMengkang Lu\\nYong Xia\\n\\n1 School of Computer Science and Engineering, Northwestern Polytechnical University, China\\n2 Research & Development Institute of Northwestern Polytechnical University in Shenzhen, China\\n3 Ningbo Institute of Northwestern Polytechnical University, China\\n\\nzychen@mail.nwpu.edu.cn, yspan@nwpu.edu.cn, ywye@mail.nwpu.edu.cn\\nlmk@mail.nwpu.edu.cn, yxia@nwpu.edu.cn\\n\\nAbstract\\nDistribution shift widely exists in medical images acquired from different medical centres and poses a significant obstacle to deploying the pre-trained semantic segmentation model in real-world applications. Test-time adaptation has proven its effectiveness in tackling the cross-domain distribution shift during inference. However, most existing methods achieve adaptation by updating the pre-trained models, rendering them susceptible to error accumulation and catastrophic forgetting when encountering a series of distribution shifts (i.e., under the continual test-time adaptation setup). To overcome these challenges caused by updating the models, in this paper, we freeze the pre-trained model and propose the Visual Prompt-based Test-Time Adaptation (VPTTA) method to train a specific prompt for each test image to align the statistics in the batch normalization layers. Specifically, we present the low-frequency prompt, which is lightweight with only a few parameters and can be effectively trained in a single iteration. To enhance prompt initialization, we equip VPTTA with a memory bank to benefit the current prompt from previous ones. Additionally, we design a warm-up mechanism, which mixes source and target statistics to construct warm-up statistics, thereby facilitating the training process. Extensive experiments demonstrate the superiority of our VPTTA over other state-of-the-art methods on two medical image segmentation benchmark tasks. The code and weights of pre-trained source models are available at https://github.com/Chen-Ziyang/VPTTA.\\n\\n\u2020 Yong Xia and Yongsheng Pan are the corresponding authors. This work was supported in part by the National Natural Science Foundation of China under Grants 62171377, in part by STI2030-Major Project under Grants 2022ZD0213100, in part by Shenzhen Science and Technology Program under Grants JCYJ20220530161616036, and in part by the Ningbo Clinical Research Center for Medical Imaging under Grant 2021L003 (Open Project 2022LYKFZD06).\"}"}
{"id": "CVPR-2024-708", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Introduction\\nSemantic segmentation serves a pivotal role in medical image processing to outline specific anatomical structures. However, the segmentation model pre-trained on the source dataset may be hindered from deploying in real-world applications due to the commonly existing distribution shift, which is typically caused by the variations in imaging protocols, operators, and scanners [16, 34]. Test-time adaptation (TTA) has emerged as a promising paradigm to replace domain adaptation (DA) [5, 43, 46, 53], as TTA requires only test data during the inference phase, while DA necessitates a sufficiently large and representative target dataset [45]. The mainstream TTA methods [29\u201331, 37, 40, 41, 47, 48] leverage the self-supervised tasks to construct auxiliary losses, such as entropy minimization [41] and self-training [37], to update the parameters of pre-trained models. Although these TTA methods successfully alleviate the domain gap and performance degradation, they are based on the assumption that the target domains exhibit static distributions. Unfortunately, the target domains are continuously changing rather than static in most real-world scenarios. This was first presented as the continual test-time adaptation (CTTA) setup [42] that addresses TTA on a series of distribution shifts. Under this setup, pre-trained models are required to adapt to several distinct domains and therefore are more prone to suffer error accumulation and catastrophic forgetting. As mentioned above, many TTA methods rely on self-supervised tasks and may be affected by the noisy losses due to unreliable supervision, leading to error accumulation [2, 23]. Catastrophic forgetting typically occurs when the model is continually trained on new domains, resulting in unexpected performance degradation [21, 30]. Some research has been studied in addressing these two challenges based on optimizing the loss functions for better supervision [6, 31, 42, 42, 45] (see Figure 1 (a)) or prevent the model parameters from excessive alterations by model resetting or introducing regularizers [30, 31, 39, 42] (see Figure 1 (b)). Despite their efforts, loss optimization cannot ensure a robust training process to avoid catastrophic forgetting during the long-term inference, insufficient or excessive updating of the model parameters may lead to inadequate adaptation or error accumulation. Since the pre-trained model is updated by the self-supervised loss where the noise cannot be completely removed, these methods may still suffer from these two issues.\\n\\nConsidering the above limitations in CTTA, updating the pre-trained model during inference seems to be inappropriate. To this end, we propose the Visual Prompt-based Test-Time Adaptation (VPTTA) method to avoid error accumulation and catastrophic forgetting associated with updating model by freezing the model parameters and conduct adaptation by learning a specific visual prompt for each test image (see Figure 1 (c)). The prompt can adapt each test image to the frozen pre-trained model to take into account both adaptation ability and knowledge retaining. We attempt to achieve this by answering the following three questions.\\n\\n\u2022 How to design the prompt? Each prompt is restricted to a limited number of training iterations (sometimes just one) to satisfy the demands of online inference, requiring the prompt to be lightweight. Based on this principle, we introduce the prompt for only the low-frequency components of each sample, resulting in the low-frequency prompt. Since the low-frequency components are strongly associated with style textures and can serve as the primary source of distribution shift [46], our low-frequency prompt allows for maintaining lightweight and being able to reduce distribution shifts by modifying the low-frequency components.\\n\\n\u2022 How to initialize the prompt? It is well known that model training can benefit from proper parameter initialization, resulting in not only fewer training iterations but also improved performance [3, 52]. We argue that this is also applicable to prompt training and therefore construct a memory bank to store the prompts and low-frequency components of previous test images. By comparing the similarity between the low-frequency components of the current and previous images, we select several historical prompts of the most similar images to initialize the current prompt.\\n\\n\u2022 How to train the prompt? Since the statistics mismatch between the source and target domains in the batch normalization (BN) layers is a significant cause of distribution shift [33, 35, 44], we optimize the prompt with the objective of minimizing the distances between BN statistics and the statistics extracted from the features of test data for statistics alignment [19]. To further address the training difficulty posed by the large distribution shift at the beginning of the inference phase, we devise a statistics-fusion-based warm-up mechanism to assist in learning the prompt from easy to hard.\\n\\nTo summarise, our contributions are three-fold: (1) We present the low-frequency prompt to adjust the test image to adapt the model instead of updating the model and therefore prevent the model from error accumulation and catastrophic forgetting. (2) We propose the novel VPTTA framework to alleviate the distribution shift by training a sample-specific visual prompt, which is lightweight, well-initialized, and follows a gradual learning process. (3) The powerful framework we provide can be effectively deployed, requiring only a single iteration and a few dozen parameters for adaptation.\\n\\n2. Related Work\\n2.1. Test-time Adaptation (TTA)\\nTTA aims to adapt the model pre-trained on the source domain to the test data during inference in a source-free\"}"}
{"id": "CVPR-2024-708", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"and online manner [40, 41]. The mainstream TTA methods are based on model updating, which constructs self-supervised auxiliary tasks to guide the model to learn on the test data [4, 13, 26, 29, 37, 40, 41, 48, 49]. Sun et al. [40] utilized an auxiliary task to predict the rotation to assist the model learning on the test data. Wang et al. [41] presented a test-time entropy minimization scheme to reduce generalization error by reducing the entropy of model predictions on test data. Zhang et al. [49] introduced a supplementary network to fit the test data, which can work in a plug-and-play fashion, necessitating minimal hyperparameter tuning. Some other methods are based on BN statistics, which alleviate the distribution shifts by modifying the statistics within BN layers [25, 27, 44]. Mirza et al. [25] constantly adapted the statistics of the BN layers to modify the feature representations of the model. These TTA methods improve the adaptation performance on the static target domain but neglect the continually changing target domains in most real-world scenarios.\\n\\nRecently, the continual test-time adaptation (CTTA) [42] is proposed to tackle TTA under a series of continually changing target domains. Due to long-term inference, both error accumulation [2, 23] and catastrophic forgetting [21, 30] are more prone to occur in this setup. Many methods [6, 30, 31, 39, 42, 45, 47] optimize the self-supervised losses or prevent excessive alterations to the model parameters via model resetting or regularizer to improve this situation but are still impacted by these issues. Wang et al. [42] refined pseudo labels by the weight- and augmentation-averaged predictions and randomly restored a small part of model parameters. Niu et al. [31] removed partial noisy samples with large gradients and encouraged model weights toward a flat minimum. Yang et al. [45] designed a dynamic strategy to adjust the learning rate for each test image. Zhang et al. [47] presented to adaptively fuse source and test statistics and improve the entropy minimization loss to a generalized one. In contrast, our proposed VPTTA employs a learnable prompt for each test image to mitigate distribution shifts during inference, requiring no changes to the pre-trained model and therefore alleviating the error accumulation and catastrophic forgetting.\\n\\n2.2. Prompt Learning\\nPrompt learning was initially employed to devise additional text instructions for input text to fine-tune the large-scale Natural Language Processing (NLP) models on downstream tasks [24]. With remarkable success in NLP, it has also been involved in computer vision and greatly benefited visual models in recent years [12, 15, 19, 51]. Jia et al. [15] introduced a small number of trainable parameters in the input space while keeping the model backbone frozen to fine-tune the model with high quality and efficiency. Zhou et al. [51] presented the conditional context optimization to generate an input-conditional token for each image to generalize the pre-trained vision-language model to unseen classes. Hu et al. [19] employed the visual prompt at the pixel level and added it to the input images to explicitly minimize the domain discrepancy. Gan et al. [12] learned a domain-specific and domain-agnostic visual prompt for adaptation by using the exponential-moving-average strategy under the guidance of self-training. Different from them, our VPTTA presents the low-frequency prompt to address a series of changing distribution shifts with a few dozen parameters. We further consider optimizing the initialization and warm-up mechanism, enabling our visual prompts to be effectively trained within a single iteration to satisfy the demands of CTTA.\\n\\n3. Methodology\\n3.1. Problem Definition and Method Overview\\nLet the labeled source domain dataset and unlabeled testing target dataset be $D_s = \\\\{X_s^i, Y_s^i\\\\}_{i=1}^N$ and $D_t = \\\\{X_t^i\\\\}_{i=1}^N$, respectively, where $X^*_i \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C}$ is the $i$-th image with $C$ channels and size of $H \\\\times W$, and $Y^*_i$ is its label. Our goal is to learn a low-frequency prompt $P_i$ for each test image $X_t^i$ to obtain the modified image $\\\\tilde{X}_t^i$ that can be adapted to the model $f_\\\\theta: X \\\\rightarrow Y$ pre-trained on $D_s$. To accomplish this goal, our VPTTA considers three key facets: prompt design, prompt initialization, and prompt training. For prompt design, the prompt is designed to change the low-frequency component of the amplitude of the test image. For prompt initialization, we adopt a memory bank to initialize the current prompt with previous ones. For prompt training, we employ the absolute distance to train the prompt to align the statistics between features extracted from test images and the batch normalization (BN) layers and also present a statistics-fusion-based warm-up mechanism. An overview of our VPTTA is illustrated in Figure 2. We now delve into its details.\\n\\n3.2. Prompt Design: Low-frequency Prompt\\nLet $F(\\\\cdot)$ and $F^{-1}(\\\\cdot)$ be the Fast Fourier Transform (FFT) and Inverse Fast Fourier Transform (IFFT) operations [10], respectively. The amplitude component and phase component are denoted by $F_A(\\\\cdot)$ and $F_P(\\\\cdot)$ respectively. Note that the low-frequency component in $F_A(\\\\cdot)$ is shifted to the center. Utilizing the prompt $P_i \\\\in \\\\mathbb{R}^{(\\\\alpha \\\\times H) \\\\times (\\\\alpha \\\\times W) \\\\times C}$ to obtain the adapted image $\\\\tilde{X}_t^i$ can be formalized as\\n\\n$$\\\\tilde{X}_t^i = F^{-1}(\\\\left[\\\\text{OnePad}(P_i) \\\\odot F_A(X_t^i), F_P(X_t^i)\\\\right]),$$\\n\\nwhere $\\\\odot$ is the element-wise multiplication, and OnePad means padding one around $P_i$ to expand it to the size of $H \\\\times W$. Under the control of the small coefficient $\\\\alpha \\\\in (0, 1)$, we can ensure that the prompt $P_i$ remains lightweight and focuses on the low-frequency component.\"}"}
{"id": "CVPR-2024-708", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Initialize\\n\\nIFFT\\n\\nFFT\\n\\nLow-frequency Prompt\\n\\nContinually Changing Target Domains\\n\\nMemory Bank\\n\\nQuery\\n\\n\u2026\\n\\n\u2026\\n\\nSource\\n\\nStat\\n\\nWarm-up\\n\\nStat\\n\\nTransformation\\n\\n\\\\( \\\\sigma_w = \\\\lambda \\\\sigma_s + (1 - \\\\lambda) \\\\sigma_t \\\\)\\n\\n\\\\( \\\\mu_w = \\\\lambda \\\\mu_s + (1 - \\\\lambda) \\\\mu_t \\\\)\\n\\n\\\\( \\\\lambda \\\\rightarrow 1 \\\\) (\\\\( \\\\mu_s, \\\\sigma_s \\\\)) (\\\\( \\\\mu_w, \\\\sigma_w \\\\)) (\\\\( \\\\mu_t, \\\\sigma_t \\\\))\\n\\nFrozen Source Model\\n\\nBack-prop\\n\\nConvolutional Layer\\n\\nBatch Normalization Layer\\n\\n\u2460\\n\\n\u2461\\n\\n\u2462\\n\\n\u2463\\n\\nFine-tuned Prompt\\n\\nEnqueue\\n\\nEnqueue\\n\\nDequeue\\n\\nDequeue\\n\\nLow-frequency Component\\n\\nFinal\\n\\nOutput\\n\\nTest\\n\\nStat\\n\\nSimultaneously, we recognize that the low-rank decomposition is widely employed in the field of data compression [22] and model fine-tuning [18]. Consequently, we also design a low-rank prompt \\\\( P_{LoRa} \\\\) as a variant (denoted as VPTTA-LR), where \\\\( B_i \\\\in \\\\mathbb{R}^{H \\\\times r \\\\times C} (r \\\\ll W) \\\\), \\\\( A_i \\\\in \\\\mathbb{R}^{r \\\\times W \\\\times C} (r \\\\ll H) \\\\), and \\\\( @ \\\\) denotes the matrix multiplication operation. In this way, the adapted image can be formalized as\\n\\n\\\\[ \\\\tilde{X}_{ti} = X_{ti} + P_{LoRa} \\\\]\\n\\n3.3. Prompt Initialization: Memory Bank\\n\\nTo achieve efficient training, it is critical to produce a suitable initialization for \\\\( P_i \\\\). Therefore, we present a memory-bank-based initialization strategy to benefit the current prompt from previous ones. Specifically, the memory bank \\\\( M \\\\) stores \\\\( S \\\\) pairs of keys and values \\\\( \\\\{q_s, v_s\\\\}_s=1^S \\\\) and holds the First In First Out (FIFO) principle. As shown in Figure 2, the keys are the low-frequency components of previous test images, and the values are their corresponding prompts. Note that when using VPTTA-LR, the keys are the low-rank reconstruction matrix of previous test images. To initialize the current prompt \\\\( P_i \\\\), we first extract the low-frequency component of the current test image \\\\( F_{A_{low}}(X_{ti}) \\\\) and calculate the cosine similarity between \\\\( F_{A_{low}}(X_{ti}) \\\\) and each key \\\\( q_s \\\\) as follows:\\n\\n\\\\[ \\\\text{Cos}(F_{A_{low}}(X_{ti}), q_s) = \\\\frac{\\\\langle F_{A_{low}}(X_{ti}), q_s \\\\rangle}{\\\\|F_{A_{low}}(X_{ti})\\\\| \\\\times \\\\|q_s\\\\|} \\\\]\\n\\nwhere \\\\( \\\\langle \\\\cdot, \\\\cdot \\\\rangle \\\\) denotes the dot product operation, and \\\\( \\\\| \\\\cdot \\\\| \\\\) denotes the Euclidean norm. Since the low-frequency component can represent the style texture of an image, we rank the similarity scores and then retrieve the values of the \\\\( K \\\\) most relevant keys from \\\\( M \\\\) to construct a support set \\\\( R_i = \\\\{q_k, v_k\\\\}_{k=1}^K \\\\) for the current test image. Finally, we assign a weight \\\\( w_k \\\\) to \\\\( v_k \\\\) to initialize the current prompt by\\n\\n\\\\[ P_i = \\\\sum_{k=1}^{K} w_k v_k \\\\]\\n\\nwhere the weights \\\\( \\\\{w_k\\\\} \\\\) are calculated based on the similarity scores [45] and \\\\( P_i \\\\).\\n\\n3.4. Prompt Training: Statistics Alignment\\n\\nInspired by [19, 27, 47], the distribution shift largely exists in distinct BN statistics calculated on the test features (i.e., \\\\( \\\\mu_t \\\\) and \\\\( \\\\sigma_t \\\\)) and stored in the source model \\\\( f_{\\\\theta} \\\\) (i.e., \\\\( \\\\mu_s \\\\) and \\\\( \\\\sigma_s \\\\)), where \\\\( \\\\mu \\\\) and \\\\( \\\\sigma \\\\) respectively denote the mean and standard deviation. To train the prompts to align the statistics, we adopt the absolute-distance-based loss \\\\( L_p \\\\) [19] as follows:\\n\\n\\\\[ L_p = \\\\left| \\\\mu_j - \\\\mu_{jt} \\\\right| + \\\\left| \\\\sigma_j - \\\\sigma_{jt} \\\\right| \\\\]\"}"}
{"id": "CVPR-2024-708", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1: VPTTA algorithm.\\n\\nInitialize:\\nFrozen source model $f_{\\\\theta}$, initial prompt $P$ filled with ones, memory bank $M$, hyperparameters $S$, $K$ and $\\\\tau$.\\n\\nInput:\\nFor each time step $i$, current test image $X_{ti}$.\\n\\n1: \u25b7 Initialize the prompt $P_i$:\\n2: if $i < K$ then\\n3: $P_i = P$\\n4: else\\n5: Construct the support set $R_i$ from $M$\\n6: $P_i = \\\\sum_{k=1}^{K} w_k v_k$, $v_k \\\\in R_i$\\n7: end if\\n8: \u25b7 Train the prompt $P_i$:\\n9: Obtain $\\\\tilde{X}_{ti}$ using $P_i$ by Eq. (1)\\n10: Forward $f_{\\\\theta}(\\\\tilde{X}_{ti})$ and calculate the warm-up statistics $\\\\mu_w$ and $\\\\sigma_w$ by Eq. (4) to replace the $\\\\mu_s$ and $\\\\sigma_s$ stored in $f_{\\\\theta}$\\n11: Backward and Update $P_i$ by Eq. (5)\\n12: \u25b7 Inference:\\n13: Obtain $\\\\tilde{X}_{ti}$ using updated $P_i$ by Eq. (1)\\n14: Forward $O_i = f_{\\\\theta}(\\\\tilde{X}_{ti})$\\n15: \u25b7 Update the memory bank $M$:\\n16: $M.enqueue(q_i, v_i)$, $q_i = FA(X_{ti}), v_i = P_i$\\n17: if len($M$) > $S$ then\\n18: $M.dequeue(q_{first}, v_{first})$\\n19: end if\\n\\nOutput:\\nAdapted prediction $O_i$ where $j$ indicates the $j$-th BN layer in $f_{\\\\theta}$.\\n\\nHowever, aligning the source and target domains within a limited number of iterations (one in this study) provides challenges due to the empty memory bank and undiminished distribution shifts at the beginning of the inference phase. Therefore, we design a warm-up mechanism to address this issue by simulating the warm-up statistics (i.e., $\\\\mu_w$ and $\\\\sigma_w$) as follows:\\n\\n$$\\\\mu_w = \\\\lambda \\\\mu_{ti} + (1 - \\\\lambda) \\\\mu_s, \\\\quad \\\\sigma_w = \\\\lambda \\\\sigma_{ti} + (1 - \\\\lambda) \\\\sigma_s,$$\\n\\nwhere $\\\\lambda = \\\\frac{1}{\\\\sqrt{i/\\\\tau} + 1}$, $i$ indicates the index of the current test image and starts from 1, and $\\\\tau$ is a temperature coefficient to control the rate of transition from the warm-up statistics to the source statistics. Now, the Eq. (3) can be rewritten by:\\n\\n$$L_p = \\\\frac{1}{j} \\\\sum_{j=1}^{J} \\\\left| \\\\frac{\\\\mu_{wj} - \\\\mu_{ti}}{\\\\sigma_{wj} - \\\\sigma_{ti}} \\\\right|,$$\\n\\nand we utilize $\\\\mu_w$ and $\\\\sigma_w$ for normalization instead of $\\\\mu_s$ and $\\\\sigma_s$. As the inference progresses, the warm-up statistics will gradually shift towards the source statistics. By simulating the warm-up statistics between the source and target statistics, we can significantly reduce the training difficulty at the beginning of the inference phase, resulting in learning prompts from easy to difficult. The overall process of VPTTA is summarized in Algorithm 1.\\n\\n4. Experiments\\n\\nWe evaluate our proposed VPTTA on two 2D segmentation benchmark tasks: the joint optic disc (OD) and cup (OC) segmentation task, and the polyp segmentation task.\\n\\n4.1. Datasets and Evaluation Metrics\\n\\nThe OD/OC segmentation dataset comprises five public datasets collected from different medical centres, denoted as domain A (RIM-ONE-r3 [11]), B (REFUGE [32]), C (ORIGA [50]), D (REFUGE-Validation/Test [32]), and E (Drishti-GS [38]). There are 159, 400, 650, 800, and 101 images from these datasets. We cropped a region of interest (ROI) centering at OD with size of $800 \\\\times 800$ for each image following [19], and each ROI is further resized to $512 \\\\times 512$ and normalized by min-max normalization. The Dice score metric ($DSC$) is utilized for evaluation in this task.\\n\\nThe polyp segmentation dataset consists of four public datasets collected from different medical centres, denoted as domain A (BKAI-IGH-NeoPolyp [28]), B (CVC-ClinicDB [1]), C (ETIS-LaribPolypDB [36]), and D (Kvasir-Seg [20]), which have 1000, 612, 196, and 1000 images, respectively. We followed [9] to resize each image to $352 \\\\times 352$ and normalize the resized image by the statistics computed on ImageNet. The $DSC$, enhanced-alignment metric ($E_{\\\\max} \\\\phi$) [8], and structural similarity metric ($S_{\\\\alpha}$) [7] are utilized for evaluation in this task.\\n\\n4.2. Implementation Details\\n\\nFor each task, we trained the source model on each single domain (source domain) and tested it on the left domains (target domains) to calculate the mean metrics to evaluate the methods on different scenarios. In the source-training phase, we trained a ResUNet-34 [17] backbone, following [19], as the baseline for OD/OC segmentation task and trained the PraNet [9] with a Res2Net-based [14] backbone for polyp segmentation task. Due to the complexity of PraNet\u2019s decoder, we only calculated the prompt loss $L_p$ for each BN layer in the encoder instead of the whole network. In the test-adaptation phase, We performed one-iteration adaptation for each batch of test data with a batch size of 1 on all experiments of our VPTTA and other competing methods [45]. As our VPTTA involves an iteration to update and an inference step, distinguishing it from other methods like [41] that do not require a dedicated inference step, we applied the same configuration as VPTTA to other competing methods to ensure consistency in the experimental setup. To deploy our VPTTA, we utilized the Adam optimizer with a learning rate of 0.05 and 0.01 for the OD/OC segmentation task and polyp segmentation task, respectively. The hyperparameters $\\\\alpha$ (size of prompt), $S$ (size of memory bank), $K$ (size of support set), and $\\\\tau$ (temperature coefficient in warm-up) are set to 0.01, 40, 16, and 5 for both segmentation tasks.\"}"}
{"id": "CVPR-2024-708", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1. Performance of our VPTTA, 'Source Only' baseline, and six competing methods on the OD/OC segmentation task. The best and second-best results in each column are highlighted in bold and underline, respectively.\\n\\n| Methods              | Domain A       | Domain B       | Domain C       | Domain D       | Domain E       | Average DSC |\\n|----------------------|----------------|----------------|----------------|----------------|----------------|-------------|\\n|                      | DSC            | DSC            | DSC            | DSC            | DSC            |             |\\n| Source Only (ResUNet-34) | 64.53          | 76.06          | 71.18          | 52.67          | 64.87          |             |\\n|                      | **65.86**      |               |               |                |                |             |\\n| TENT-continual (ICLR 2021) [41] | 73.07          | 78.66          | 71.94          | 46.81          | 70.20          |             |\\n|                      |               | **78.81**      |               |                |                |             |\\n| CoTTA (CVPR 2022) [42] | 75.39          | 75.98          | 69.14          | 53.99          | 70.40          |             |\\n|                      |               |               | **75.98**      |                |                |             |\\n| DLTTA (TMI 2022) [45] | 75.11          | 78.85          | 73.89          | 51.64          | 69.71          |             |\\n|                      |               |               |               | **76.84**      |                |             |\\n| DUA (CVPR 2022) [25]  | 72.28          | 76.59          | 70.13          | 56.17          | 71.38          |             |\\n|                      |               |               |               | **76.84**      |                |             |\\n| SAR (ICLR 2023) [31]  | 74.55          | 77.71          | 70.78          | 55.40          | 71.72          |             |\\n|                      |               |               |               | **77.31**      |                |             |\\n| DomainAdaptor (CVPR 2023) [47] | 74.50          | 76.39          | 71.81          | 56.78          | 70.55          |             |\\n|                      |               |               |               | **76.98**      |                |             |\\n| VPTTA (Ours)         | 73.91          | 79.36          | 74.51          | 56.51          | 75.35          | 71.93       |\\n\\n### Table 2. Performance of our VPTTA, 'Source Only' baseline, and six competing methods on the polyp segmentation task. The best and second-best results in each column are highlighted in bold and underline, respectively.\\n\\n| Methods              | Domain A       | Domain B       | Domain C       | Domain D       | Average DSC |\\n|----------------------|----------------|----------------|----------------|----------------|-------------|\\n|                      | DSC            | DSC            | DSC            | DSC            |             |\\n|                      | DSC            | DSC            | DSC            | DSC            |             |\\n| Source Only (PraNet) | 79.10          | 87.97          | 84.66          | 66.33          | 78.51        |\\n|                      |               | **87.97**      |               |                |             |\\n| TENT-continual (ICLR 2021) [41] | 74.86          | 84.58          | 80.52          | 67.51          | 78.05        |\\n|                      |               |               | **84.58**      |                |             |\\n| CoTTA (CVPR 2022) [42] | 76.46          | 85.98          | 81.94          | 66.99          | 81.40        |\\n|                      |               |               | **85.98**      |                |             |\\n| DLTTA (TMI 2022) [45] | 76.27          | 85.85          | 83.89          | 68.99          | 81.84        |\\n|                      |               |               | **85.85**      |                |             |\\n| DUA (CVPR 2022) [25]  | 78.93          | 87.37          | 83.94          | 70.99          | **84.62**    |\\n|                      |               |               |               |                |             |\\n| SAR (ICLR 2023) [31]  | 76.48          | 85.71          | 81.78          | 55.40          | 79.05        |\\n|                      |               |               |               | **85.71**      |             |\\n| DomainAdaptor (CVPR 2023) [47] | 77.48          | 86.31          | 82.42          | 67.78          | 82.06        |\\n|                      |               |               |               | **86.31**      |             |\\n| VPTTA (Ours)         | **81.00**      | **88.91**      | **84.91**      | **76.87**      | **87.31**    |\\n\\n### Table 3. Results of ablation study on the OD/OC segmentation task. The best and second-best results in each column are highlighted in bold and underline, respectively.\\n\\n| Methods              | Domain A       | Domain B       | Domain C       | Domain D       | Domain E       | Average DSC |\\n|----------------------|----------------|----------------|----------------|----------------|----------------|-------------|\\n|                      | DSC            | DSC            | DSC            | DSC            | DSC            |             |\\n| Low-frequency        | 64.53          | 76.06          | 71.18          | 52.67          | 64.87          |             |\\n| Prompt Memory        | 65.99          | 76.58          | 71.68          | 53.17          | 65.37          |             |\\n| Memory Bank Warm-up  | 66.70          | 79.36          | 72.82          | 55.13          | 67.72          |             |\\n\\n#### 4.3. Experimental Results\\n\\nWe compare our VPTTA with the 'Source Only' baseline (training on the source domain and testing without adaptation), and six competing methods, including a pseudo-label-based method (CoTTA [42]), two entropy-based methods (TENT-continual [41] and SAR [31]), a method dynamically adjusting the learning rate (DLTTA [45]), a method combining entropy-loss and BN statistics fusion (DomainAdaptor [47]), and a BN statistics-based method (DUA [25]) on two segmentation tasks. Specifically, we re-implemented all the competing methods using the same baseline as VPTTA and combined DLTTA with the entropy loss proposed in TENT [45]. The result of each domain is calculated by using the current domain as the source domain and applying the pre-trained model on the left domains.\\n\\n**Comparison on the OD/OC segmentation task.** The results of the proposed VPTTA, the 'Source Only' baseline, and six competing methods on the OD/OC segmentation task are detailed in Table 1. Obviously, all competing methods outperform the 'Source Only' baseline, indicating the effectiveness of adaptation towards the distribution shifts. It can be found that our VPTTA achieves the best overall performance across all domains, underscoring its superior applicability and robustness. Meanwhile, our VPTTA also shows competitive performance in domains where most methods exhibit satisfactory performance (such as D) and also performs well in domains where other methods exhibit suboptimal results (such as C).\\n\\n**Comparison on the polyp segmentation task.** We conducted the comparison experiment on the polyp segmentation task under the same experimental setup, as shown in Table 2. In contrast to the results of the OD/OC segmentation task, the methods based on model updating (i.e., TENT-continual, CoTTA, DLTTA, SAR, and DomainAdaptor) exhibit significantly worse performance in this task and are even inferior to the 'Source Only' baseline. It can be attributed to the fact that the polyps are relatively hidden, and the models tend to segment almost nothing rather than poorly segment under the impact of distribution shifts, resulting in confident and low entropy predictions but wrong\"}"}
{"id": "CVPR-2024-708", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"Figure 3. Visualization of the original images, estimated prompts, and adapted images on the OD/OC segmentation task. We normalize the prompts to [0, 1] for better visualization. The DSC of applying the frozen source model on the original and adapted images is displayed below each image. We also show an example of each source domain on the left side of this diagram. \u2018Ori\u2019: Abbreviation of \u2018Original\u2019.\\n\\n| Source Domain | Target Domains | DSC of Applying Frozen Source Model on Original Images | DSC of Applying Frozen Source Model on Adapted Images |\\n|---------------|----------------|------------------------------------------------------|------------------------------------------------------|\\n| Domain A      | Domain B       | 67.47                                                | 84.39                                                |\\n|               | Domain C       | 84.39                                                | 79.70                                                |\\n|               | Domain D       | 79.70                                                | 87.21                                                |\\n|               | Domain E       | 87.21                                                | 11190                                                |\\n\\nComparison in a long-term continual test-time adaptation. To evaluate our VPTTA in a long-term continual test-time adaptation [42], we conducted the experiments for the OD/OC segmentation task multiple rounds and reported the results in Table 4. Since DUA is a backward-free method and DomainAdaptor resets the pre-trained model before each adaptation, they are unsuitable to be evaluated in this experiment. We compared our VPTTA with TENT, CoTTA, DLTTA, and SAR, all of which involve continuous training of the pre-trained model. The performance across different rounds with the same source domain reflects the extent of catastrophic forgetting in the model, and the average performance across multiple rounds highlights the error accumulation of the model within a series of distinct domains. The results reveal that both optimizing loss functions (CoTTA, DLTTA, SAR) and resetting the model (CoTTA, SAR) are effective for mitigating error accumulation and catastrophic forgetting, while TENT with vanilla entropy-minimization loss suffers from severe performance degradation. Additionally, after multiple rounds of testing, our VPTTA maintains superior performance with minimal performance degradation, emphasizing the effectiveness of...\"}"}
{"id": "CVPR-2024-708", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"To evaluate the contributions of the low-frequency prompt over updating the pre-trained model. The red numbers indicate the performance gain relative to the 'Source Only' baseline. The performance degradation is calculated between the overall average DSC and the average DSC of round 1. 'Ave': Abbreviation of 'Average'.\\n\\nTable 4. Performance of our VPTTA and four competing methods on the OD/OC segmentation task in a long-term continual test-time adaptation. The red numbers indicate the performance gain relative to the 'Source Only' baseline. The performance degradation is calculated between the overall average DSC and the average DSC of round 1.\\n\\nTable 5. Performance of our VPTTA and VPTTA-LR on two segmentation tasks with multiple domains verified the superiority of our methods over other methods and the effectiveness of each component.\\n\\nIn this paper, we proposed the VPTTA, a prompt-based method to alleviate the potential risks (such as catastrophic forgetting) by freezing the pre-trained model. Specially, we presented the low-frequency prompt which is less likely to be affected by the noise in the data. We also introduced a warm-up mechanism to simulate the initialization of the model.\\n\\nTo compare the effectiveness of VPTTA and VPTTA-LR, we repeated the experiments on two segmentation tasks and showed results in Table 5. It can be seen that VPTTA significantly outperforms VPTTA-LR, attributed to the fewer learnable parameters and superior representation of style texture in the frequency domain. Benefiting from the low-frequency prompt initialized, VPTTA-LR achieved over a hundred times fewer parameters than VPTTA-LR.\\n\\nIn conclusion, our experiments on the OD/OC segmentation task verified the effectiveness of our VPTTA, especially in the long-term continual test-time adaptation tasks with multiple domains.\"}"}
{"id": "CVPR-2024-708", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Jorge Bernal, F Javier S\u00e1nchez, Gloria Fern\u00e1ndez-Esparrach, Debora Gil, Cristina Rodr\u00edguez, and Fernando Vilari\u00f1o. Wm-dova maps for accurate polyp highlighting in colonoscopy: Validation vs. saliency maps from physicians. Comput. Med. Imaging and Graph. 43:99\u2013111, 2015.\\n\\n[2] Paola Cascante-Bonilla, Fuwen Tan, Yanjun Qi, and Vicente Ordonez. Curriculum labeling: Revisiting pseudo-labeling for semi-supervised learning. In AAAI, pages 6912\u20136920, 2021.\\n\\n[3] Oscar Chang, Lampros Flokas, and Hod Lipson. Principled weight initialization for hypernetworks. In Int. Conf. Learn. Represent., 2019.\\n\\n[4] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 295\u2013305, 2022.\\n\\n[5] Lin Chen, Huaian Chen, Zhixiang Wei, Xin Jin, Xiao Tan, Yi Jin, and Enhong Chen. Reusing the task-specific classifier as a discriminator: Discriminator-free adversarial domain adaptation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 7181\u20137190, 2022.\\n\\n[6] Mario D\u00f6bler, Robert A Marsden, and Bin Yang. Robust mean teacher for continual and gradual test-time adaptation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 7704\u20137714, 2023.\\n\\n[7] Deng-Ping Fan, Ming-Ming Cheng, Yun Liu, Tao Li, and Ali Borji. Structure-measure: A new way to evaluate foreground maps. In Int. Conf. Comput. Vis., pages 4548\u20134557, 2017.\\n\\n[8] Deng-Ping Fan, Cheng Gong, Yang Cao, Bo Ren, Ming-Ming Cheng, and Ali Borji. Enhanced-alignment measure for binary foreground map evaluation. In IJCAI, pages 698\u2013704, 2018.\\n\\n[9] Deng-Ping Fan, Ge-Peng Ji, Tao Zhou, Geng Chen, Huazhu Fu, Jianbing Shen, and Ling Shao. Pranet: Parallel reverse attention network for polyp segmentation. In Int. Conf. Med. Image Comput. Comput.-Assist. Intervent., pages 263\u2013273. Springer, 2020.\\n\\n[10] Matteo Frigo and Steven G Johnson. FFTW: An adaptive software architecture for the fft. In ICASSP, pages 1381\u20131384. IEEE, 1998.\\n\\n[11] Francisco Fumero, Silvia Alay\u00f3n, Jos\u00e9 L Sanchez, Jose Sigut, and M Gonzalez-Hernandez. RIM-ONE: An open retinal image database for optic nerve evaluation. In Int. Symp. Comput.-based Med. Syst., pages 1\u20136. IEEE, 2011.\\n\\n[12] Yulu Gan, Yan Bai, Yihang Lou, Xianzheng Ma, Renrui Zhang, Nian Shi, and Lin Luo. Decorate the newcomers: Visual domain prompt for continual test time adaptation. In AAAI, pages 7595\u20137603, 2023.\\n\\n[13] Yossi Gandelsman, Yu Sun, Xinlei Chen, and Alexei Efros. Test-time training with masked autoencoders. Adv. Neural Inform. Process. Syst., 35:29374\u201329385, 2022.\\n\\n[14] Shang-Hua Gao, Ming-Ming Cheng, Kai Zhao, Xin-Yu Zhang, Ming-Hsuan Yang, and Philip Torr. Res2net: A new multi-scale backbone architecture. IEEE Trans. Pattern Anal. Mach. Intell., 43(2):652\u2013662, 2019.\\n\\n[15] Yunhe Gao, Xingjian Shi, Yi Zhu, Hao Wang, Zhiqiang Tang, Xiong Zhou, Mu Li, and Dimitris N Metaxas. Visual prompt tuning for test-time domain adaptation. arXiv preprint arXiv:2210.04831, 2022.\\n\\n[16] Mohsen Ghafoorian, Alireza Mehrtash, Tina Kapur, Nico Karssemeijer, Elena Marchiori, Mehran Pesteie, Charles RG Guttmann, Frank-Erik de Leeuw, Clare M Tempany, Bram Van Ginneken, et al. Transfer learning for domain adaptation in mri: Application in brain lesion segmentation. In Int. Conf. Med. Image Comput. Comput.-Assist. Intervent., pages 516\u2013524. Springer, 2017.\\n\\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conf. Comput. Vis. Pattern Recog., pages 770\u2013778, 2016.\\n\\n[18] Edward J Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In Int. Conf. Learn. Represent., 2022.\\n\\n[19] Shishuai Hu, Zehui Liao, and Yong Xia. Prosfda: Prompt learning based source-free domain adaptation for medical image segmentation. arXiv preprint arXiv:2211.11514, 2022.\\n\\n[20] Debesh Jha, Pia H Smedsrud, Michael A Riegler, P\u00e5l Halvorsen, Thomas de Lange, Dag Johansen, and H\u00e5vard D Johansen. Kvasir-seg: A segmented polyp dataset. In Int. Conf. Multimedia Modeling, pages 451\u2013462. Springer, 2020.\\n\\n[21] Christiaan Lamers, Ren\u00e9 Vidal, Nabil Belbachir, Niki van Stein, Thomas Baeck, and Paris Giampouras. Clustering-based domain-incremental learning. In Int. Conf. Comput. Vis., pages 3384\u20133392, 2023.\\n\\n[22] Joonseok Lee, Seungyeon Kim, Guy Lebanon, and Yoram Singer. Local low-rank matrix approximation. In Int. Conf. Mach. Learn., pages 82\u201390. PMLR, 2013.\\n\\n[23] Jian Liang, Ran He, Zhenan Sun, and Tieniu Tan. Exploring uncertainty in pseudo-label guided unsupervised domain adaptation. Pattern Recognition, 96:106996, 2019.\\n\\n[24] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. ACM Comput. Surveys, 55(9):1\u201335, 2023.\\n\\n[25] M Jehanzeb Mirza, Jakub Micorek, Horst Possegger, and Horst Bischof. The norm must go on: Dynamic unsupervised domain adaptation by normalization. In IEEE Conf. Comput. Vis. Pattern Recog., pages 14765\u201314775, 2022.\\n\\n[26] Chaithanya Kumar Mummadi, Robin Hutmacher, Kilian Rambach, Evgeny Levinkov, Thomas Brox, and Jan Hendrik Metzen. Test-time adaptation to distribution shift by confidence maximization and input transformation. arXiv preprint arXiv:2106.14999, 2021.\\n\\n[27] Zachary Nado, Shreyas Padhy, D Sculley, Alexander D\u2019Amour, Balaji Lakshminarayanan, and Jasper Snoek. Evaluating prediction-time batch normalization for robustness under covariate shift. arXiv preprint arXiv:2006.10963, 2020.\"}"}
{"id": "CVPR-2024-708", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Phan Ngoc Lan, Nguyen Sy An, Dao Viet Hang, Dao Van Long, Tran Quang Trung, Nguyen Thi Thuy, and Dinh Viet Sang. Neounet: Towards accurate colon polyp segmentation and neoplasm detection. In Adv. Vis. Comput. Int. Symp., pages 15\u201328. Springer, 2021.\\n\\nA Tuan Nguyen, Thanh Nguyen-Tang, Ser-Nam Lim, and Philip HS Torr. Tipi: Test time adaptation with transformation invariance. In IEEE Conf. Comput. Vis. Pattern Recog., pages 24162\u201324171, 2023.\\n\\nShuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Yaofo Chen, Shijian Zheng, Peilin Zhao, and Mingkui Tan. Efficient test-time model adaptation without forgetting. In Int. Conf. Mach. Learn., pages 16888\u201316905. PMLR, 2022.\\n\\nShuaicheng Niu, Jiaxiang Wu, Yifan Zhang, Zhiquan Wen, Yaofo Chen, Peilin Zhao, and Mingkui Tan. Towards stable test-time adaptation in dynamic wild world. In Int. Conf. Learn. Represent., 2023.\\n\\nJos\u00e9 Ignacio Orlando, Huazhu Fu, Jo\u00e3o Barbosa Breda, Karel van Keer, Deepti R Bathula, Andr\u00e9s Diaz-Pinto, Ruogu Fang, Pheng-Ann Heng, Jeyoung Kim, JoonHo Lee, et al. REFUGE Challenge: A unified framework for evaluating automated methods for glaucoma assessment from fundus photographs. Med. Image Anal., 59:101570, 2020.\\n\\nXingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In Eur. Conf. Comput. Vis., pages 464\u2013479, 2018.\\n\\nSwami Sankaranarayanan, Yogesh Balaji, Arpit Jain, Ser Nam Lim, and Rama Chellappa. Learning from synthetic data: Addressing domain shift for semantic segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 3752\u20133761, 2018.\\n\\nSteffen Schneider, Evgenia Rusak, Luisa Eck, Oliver Bringmann, Wieland Brendel, and Matthias Bethge. Improving robustness against common corruptions by covariate shift adaptation. Adv. Neural Inform. Process. Syst., 33:11539\u201311551, 2020.\\n\\nJuan Silva, Aymeric Histace, Olivier Romain, Xavier Dray, and Bertrand Granado. Toward embedded detection of polyps in wce images for early diagnosis of colorectal cancer. Int. J. Comput.-Assist. Rad. Sur., 9(2):283\u2013293, 2014.\\n\\nSamarth Sinha, Peter Gehler, Francesco Locatello, and Bernt Schiele. Test: Test-time self-training under distribution shift. In IEEE Wint. Conf. Appli. Comput. Vis., pages 2759\u20132769, 2023.\\n\\nJayanthi Sivaswamy, SR Krishnadas, Gopal Datt Joshi, Madhulika Jain, and A Ujjwaft Syed Tabish. Drishti-GS: Retinal image dataset for optic nerve head (onh) segmentation. In IEEE Int. Symp. on Bio. Imaging, pages 53\u201356. IEEE, 2014.\\n\\nJunha Song, Jungsoo Lee, In So Kweon, and Sungha Choi. Ecotta: Memory-efficient continual test-time adaptation via self-distilled regularization. In IEEE Conf. Comput. Vis. Pattern Recog., pages 11920\u201311929, 2023.\\n\\nYu Sun, Xiaolong Wang, Zhuang Liu, John Miller, Alexei Efros, and Moritz Hardt. Test-time training with self-supervision for generalization under distribution shifts. In Int. Conf. Mach. Learn., pages 9229\u20139248. PMLR, 2020.\\n\\nDequan Wang, Evan Shelhamer, Shaoteng Liu, Bruno Olshausen, and Trevor Darrell. Tent: Fully test-time adaptation by entropy minimization. In Int. Conf. Learn. Represent., 2021.\\n\\nQin Wang, Olga Fink, Luc Van Gool, and Dengxin Dai. Continual test-time domain adaptation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 7201\u20137211, 2022.\\n\\nShujun Wang, Lequan Yu, Kang Li, Xin Yang, Chi-Wing Fu, and Pheng-Ann Heng. Boundary and entropy-driven adversarial learning for fundus image segmentation. In Int. Conf. Med. Image Comput. Comput.-Assist. Intervent., pages 102\u2013110. Springer, 2019.\\n\\nWei Wang, Zhun Zhong, Weijie Wang, Xi Chen, Charles Ling, Boyu Wang, and Nicu Sebe. Dynamically instance-guided adaptation: A backward-free approach for test-time domain adaptive semantic segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 24090\u201324099, 2023.\\n\\nHongzheng Yang, Cheng Chen, Meirui Jiang, Quande Liu, Jianfeng Cao, Pheng Ann Heng, and Qi Dou. Dltta: Dynamic learning rate for test-time adaptation on cross-domain medical images. IEEE Trans. Med. Imaging, 41(12):3575\u20133586, 2022.\\n\\nYanchao Yang and Stefano Soatto. FDA: Fourier domain adaptation for semantic segmentation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 4085\u20134095, 2020.\\n\\nJian Zhang, Lei Qi, Yinghuan Shi, and Yang Gao. Domainadaptor: A novel approach to test-time adaptation. In IEEE Conf. Comput. Vis. Pattern Recog., pages 18971\u201318981, 2023.\\n\\nMarvin Zhang, Sergey Levine, and Chelsea Finn. Memo: Test time robustness via adaptation and augmentation. Adv. Neural Inform. Process. Syst., 35:38629\u201338642, 2022.\\n\\nYizhe Zhang, Tao Zhou, Yuhui Tao, Shuo Wang, Ye Wu, Benyuan Liu, Pengfei Gu, Qiang Chen, and Danny Z Chen. TestFit: A plug-and-play one-pass test time method for medical image segmentation. Medical Image Analysis, 92:103069, 2024.\\n\\nZhuo Zhang, Feng Shou Yin, Jiang Liu, Wing Kee Wong, Ngan Meng Tan, Beng Hai Lee, Jun Cheng, and Tien Yin Wong. ORIGA-light: An online retinal fundus image database for glaucoma analysis and research. In Int. Conf. IEEE Eng. in Med. and Bio., pages 3065\u20133068. IEEE, 2010.\\n\\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. In IEEE Conf. Comput. Vis. Pattern Recog., pages 16816\u201316825, 2022.\\n\\nChen Zhu, Renkun Ni, Zheng Xu, Kezhi Kong, W Ronny Huang, and Tom Goldstein. Gradinit: Learning to initialize neural networks for stable and efficient training. Adv. Neural Inform. Process. Syst., 34:16410\u201316422, 2021.\\n\\nYukun Zuo, Hantao Yao, and Changsheng Xu. Attention-based multi-source domain adaptation. IEEE Trans. Image Process., 30:3793\u20133803, 2021.\"}"}
