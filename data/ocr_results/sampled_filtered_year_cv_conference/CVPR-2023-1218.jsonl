{"id": "CVPR-2023-1218", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Quantitative comparison on the LOL-real dataset.\\n\\n| Method     | PSNR | SSIM  |\\n|------------|------|-------|\\n| SID        | 13.24| 0.442 |\\n| DeepUPE    | 13.27| 0.452 |\\n| KIND       | 14.74| 0.641 |\\n| DeepLPF    | 14.10| 0.480 |\\n| FIDE       | 16.85| 0.678 |\\n| LPNet      | 17.80| 0.792 |\\n| MIR-Net    | 20.02| 0.820 |\\n| RF         | 14.05| 0.458 |\\n| 3DLUT      | 17.59| 0.721 |\\n| UNIE       | 20.85| 0.724 |\\n| LCDR       | 18.57| 0.641 |\\n| LLFlow     | 19.36| 0.705 |\\n| **Ours**   | 24.62| 0.867 |\\n\\nTable 2. Quantitative comparison on the LOL-synthetic dataset.\\n\\n| Method     | PSNR | SSIM  |\\n|------------|------|-------|\\n| SID        | 18.19| 0.745 |\\n| A3DLUT     | 20.29| 0.831 |\\n| Band       | 18.23| 0.617 |\\n| EG         | 18.37| 0.723 |\\n| Retinex    | 20.06| 0.815 |\\n| Sparse     | 19.23| 0.736 |\\n| DSN        | 20.51| 0.831 |\\n| RCTNet     | 20.37| 0.834 |\\n| UTVNet     | 20.28| 0.752 |\\n| SCI        | 21.16| 0.840 |\\n| URetinex   | 21.48| 0.849 |\\n| **SNR**    | 24.62| 0.867 |\\n\\nTable 3. Quantitative comparison on the SID dataset (sRGB domain).\\n\\n| Method     | PSNR | SSIM  |\\n|------------|------|-------|\\n| SID        | 16.97| 0.591 |\\n| A3DLUT     | 17.01| 0.604 |\\n| Band       | 18.02| 0.583 |\\n| EG         | 18.07| 0.600 |\\n| Retinex    | 18.34| 0.578 |\\n| Sparse     | 18.07| 0.600 |\\n| DSN        | 20.08| 0.598 |\\n| RCTNet     | 20.84| 0.605 |\\n| UTVNet     | 16.44| 0.596 |\\n| SCI        | 20.11| 0.592 |\\n| URetinex   | 20.67| 0.602 |\\n| **SNR**    | 22.53| 0.587 |\\n\\n3.4. Loss Function\\n\\nOur framework is trained end-to-end, and the loss function can be divided into three parts.\\n\\n**Loss for appearance modeling.**\\n\\nThe loss function for the appearance modeling part is the reconstruction error between $I_a$ and the ground truth $\\\\bar{I}$. The loss is computed at both the pixel level and perceptual level, as $L_a = \\\\|I_a - \\\\bar{I}\\\\| + \\\\|\\\\Phi(I_a) - \\\\Phi(\\\\bar{I})\\\\|$, \\\\(10\\\\), where $\\\\Phi()$ extracts features from the VGG network \\\\[39\\\\].\\n\\n**Loss for structure modeling.**\\n\\nOne approach for supervising $S$ is employing the structure of the normal-light data as the ground truth and utilizing the regression loss. This strategy is adopted by existing edge prediction methods \\\\[33,40\\\\]. For the input of $I$, we assume the ground truth is the edge map extracted from $\\\\bar{I}$ (using the Canny edge detector \\\\[2\\\\]), as $\\\\bar{I}_s = C(\\\\bar{I})$, \\\\(11\\\\), where $C$ is the Canny detector. Moreover, the invisibility of content and the influence of noise in dark images would aggravate the ill-posed degree of estimating structures. Thus, it is difficult to detect structural details from the dark input picture with only the regression loss. We find the GAN loss is effective in dealing with the drawback of the regression loss. The GAN loss is conducted by setting a discriminator $D$, as $E_I \\\\log(1 + \\\\exp(-D(S(I))))$, $L_g = E_I \\\\log(1 + \\\\exp(-D(S(I))))$, $L_d = E_I \\\\log(1 + \\\\exp(D(I_s))) + E_I \\\\log(1 + \\\\exp(+D(I_s)))$, \\\\(12\\\\), where $L_g$ and $L_d$ are the loss functions for $S$ and $D$, respectively, and $E$ is the mean operation.\\n\\n**Loss for SGEM.**\\n\\nSGEM is to refine the appearance modeling, and the loss is also the reconstruction error, as $L_m = \\\\|b_I - \\\\bar{I}\\\\| + \\\\|\\\\Phi(b_I) - \\\\Phi(\\\\bar{I})\\\\|$, \\\\(13\\\\).\\n\\n**Overall loss function.**\\n\\nThe overall framework is trained in an end-to-end manner and the overall loss function is $L = \\\\lambda_1 L_a + \\\\lambda_2 L_s + \\\\lambda_3 L_g + \\\\lambda_4 L_m$, \\\\(14\\\\), where $\\\\lambda_1$, $\\\\lambda_2$, $\\\\lambda_3$, $\\\\lambda_4$ are the loss weights.\\n\\n4. Experiments\\n\\n4.1. Implementation Details\\n\\nWe implement our framework in PyTorch \\\\[32\\\\]. Due to the limited computation resource, our framework is trained on 4 GPUs with RTX3090. For the loss minimization, we adopt the Adam \\\\[18\\\\] optimizer with momentum set to 0.9.\\n\\n4.2. Datasets\\n\\nExisting low-light enhancement datasets have different properties. SID \\\\[3\\\\] is a challenging dataset in both sRGB and RAW domains, and many dark areas need to be enhanced with structural guidance. For SID, each input sample is a pair of short- and long-exposure images, and low-light images have heavy noise since they were captured in extremely dark environments. We use the subset captured by the Sony camera of SID for experiments. Moreover, we also adopt LOL \\\\[62\\\\] dataset, which is divided into LOL-real and LOL-synthetic. Low-light images in LOL-real were collected from the real world; LOL-synthetic was created by synthesis.\\n\\n4.3. Comparison on sRGB Domain\\n\\nWe compare our method with a rich collection of SOTA methods for low-light enhancement, including SID \\\\[3\\\\],\"}"}
{"id": "CVPR-2023-1218", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Our Structure Map\\n\\nFigure 4. Visual comparisons on LOL-real (top) and LOL-synthetic (bottom). \u201cOurs\u201d have less noise and clearer visibility.\\n\\nDeepUPE [44], KIND [69], DeepLPF [30], FIDE [55], LPNet [22], MIR-Net [67], RF [19], 3DLUT [68], A3DLUT [46], Band [61], EG [13], Retinex [24], Sparse [62], UNIE [14], LCDR [43], LLFlow [49], DSN [70], RCTNet [17], UTVNet [71], SCI [28], URetinex [54], and SNR [56].\\n\\nQuantitative analysis.\\nWe adopt PSNR and SSIM [51] for evaluation. A higher SSIM means more high-frequency details and structures in results. Tables 1 and 2 show the comparisons on LOL-real and LOL-synthetic. Our method surpasses all the baselines, and the improvement of SSIM is very evident thanks to the successful structure modeling and guidance. These numbers are obtained either from the respective papers or by running the respective public code.\\n\\nTable 3 shows the comparison on SID. It can be seen that although previous SOTA approaches, e.g., SNR [56], can achieve high PSNR, their corresponding SSIM values are not satisfying. Our framework again has the best performance over others and has obvious superiority to others by large margins on SSIM.\\n\\nQualitative analysis.\\nWe present visual samples in Fig. 4 to compare our method with the baseline that achieves the best performance on LOL-real and LOL-synthetic. Our result shows better visual quality with higher contrast, more accurate and clear details, more natural color consistency and brightness. Fig. 5 also shows visual comparisons on SID. While the input images in SID have evident noise and weak illumination, our method can produce more realistic results with fewer artifacts than others.\\n\\n4.4. Comparison on RAW Domain\\nWe compare our method with existing low-light image enhancement methods that are designed for RAW inputs. The baselines include CAN [5], DeepUPE [44], SID [3], EEMEFN [74], LLPackNet [20], FIDE [55], DID [29], SGN [8], DCE [9], RED [21], and ABF [6]. Compared\\n\\n| Methods        | PSNR | SSIM |\\n|----------------|------|------|\\n| DeepUPE [44]   | 29.13| 0.792|\\n| SID [3]        | 28.88| 0.787|\\n| EEMEFN [74]    | 29.60| 0.795|\\n| DCE [9]        | 26.53| 0.730|\\n| LLPackNet [20] | 27.83| 0.750|\\n| FIDE [55]      | 29.56| 0.799|\\n| DID [29]       | 28.41| 0.780|\\n| SGN [8]        | 28.91| 0.789|\\n| RED [21]       | 28.66| 0.790|\\n| ABF [6]        | 29.65| 0.797|\\n| SNR [56]       | 29.75| 0.812|\\n| Ours           | 30.17| 0.834|\\n\\nTable 4. Quantitative comparison on SID's RAW domain.\\n\\nWith the enhancement on the sRGB domain, the RAW inputs have more information, leading to the enhancement with higher PSNR and SSIM.\\n\\nThe quantitative results are shown in Table 4. Our method still obtains the SOTA performance on the RAW domain in terms of both PSNR and SSIM. Our method results in the best SSIM that characterize the richness and sharpness of the images, since our approach formulates the effective structure modeling explicitly. The qualitative results in Fig. 5 also support the superiority of our framework in the RAW domain. As shown in the figure, these results also manifest that our method effectively enhances the image lightness and reveals details, while suppressing artifacts.\\n\\n4.5. Ablation Study\\nWe consider the following ablation settings by removing different components from our framework individually.\\n\\n1. \u201cOurs w/o A\u201d: remove the module of A, only the input image and the structure map are set as the input of E.\\n2. \u201cOurs w/o S\u201d: delete the structural modeling module S, and the framework is changed to the structure of two concatenated networks for appearance modeling.\\n3. \u201cOurs w/o F\u201d: replace the SAFE with the traditional encoder for the StyleGAN [59].\\n4. \u201cOurs w/o G\u201d: remove structure-guided feature synthesis in E, and set the output of S as the input of E.\\n\\nTable 5. Ablation study.\"}"}
{"id": "CVPR-2023-1218", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Visual comparisons on SID's sRGB (top) and RAW (bottom) domains. \u201cOurs\u201d have less noise and clearer visual details.\\n\\n5. \u201cOurs w/o S.G.\u201d: use the SOTA encoder-decoder-based edge prediction network [33] to implement S.\\n\\n6. \u201cOurs w/o GAN\u201d: train S without GAN loss.\\n\\nWe performed ablation studies on all three datasets on the sRGB domain. Table 5 summarizes the results. Compared with all ablation settings, our full setting yields higher PSNR and SSIM. Comparing \u201cOurs w/o A\u201d with \u201cOurs\u201d shows the necessity of conducting both the appearance and structure modeling in our framework. And the comparison between \u201cOurs w/o S\u201d and \u201cOurs\u201d demonstrates the effectiveness of the structure guidance to enhance appearance modeling. The results also show effects of \u201cSAFE\u201d (\u201cOurs w/o F\u201d vs. \u201cOurs\u201d), \u201cstructure-guided feature synthesis\u201d (\u201cOurs w/o G\u201d vs. \u201cOurs\u201d), and the GAN loss in structure modeling (\u201cOurs w/o GAN\u201d vs. \u201cOurs\u201d). Moreover, comparing \u201cOurs w/o S.G.\u201d with \u201cOurs\u201d proves the superiority of our S over existing encoder-decoder-based edge detection networks on low-light images.\\n\\nFurthermore, to demonstrate the robustness of our framework, we conduct the evaluation setting where extra noise (the Gaussian distribution with mean as 0, variance ranges from 30 to 50) is added in the input image. The results are displayed in Table 5 as \u201cOurs with noise\u201d, which is close to the situation without noise, showing the robustness of our framework towards perturbations.\\n\\n4.6. Evaluation for Structural Modeling\\n\\nIn this section, we conduct ablation studies for the structure modeling. Since we set the edge map of the normal-light image as the ground truth, we use the cross-entropy (CE) and the $L_2$ distance between the prediction and the ground truth as the metric. We compare the ablation settings that also employ the structure modeling, i.e., \u201cOurs w/o GAN\u201d, \u201cOurs w/o S.G.\u201d, and \u201cOurs w/o F\u201d. The results are reported in Table 6. Compared with all ablation settings, our full setting achieves the most accurate structure modeling from the low-light images. By comparing \u201cOurs w/o F\u201d and \u201cOurs w/o S.G.\u201d with \u201cOurs\u201d, we can demonstrate the superiority of our $S$ in the structure modeling to other alternatives. By comparing \u201cOurs\u201d with \u201cOurs w/o GAN\u201d, we can show that our GAN loss in structure modeling is beneficial for improving the structure modeling performance.\\n\\nTable 5. Results of the ablation study in Sec. 4.5.\\n\\n| Methods         | LOL-real PSNR | LOL-real SSIM | LOL-synthetic PSNR | LOL-synthetic SSIM | SID PSNR | SID SSIM |\\n|-----------------|---------------|---------------|--------------------|--------------------|----------|----------|\\n| Ours w/o A      | 20.17         | 0.801         | 23.59              | 0.879              | 22.59    | 0.639    |\\n| Ours w/o S      | 18.14         | 0.773         | 21.20              | 0.881              | 20.47    | 0.623    |\\n| Ours w/o F      | 20.21         | 0.812         | 23.05              | 0.888              | 22.35    | 0.635    |\\n| Ours w/o G      | 19.39         | 0.784         | 21.71              | 0.868              | 21.15    | 0.629    |\\n| Ours w/o S.G.   | 20.73         | 0.820         | 23.30              | 0.898              | 22.50    | 0.632    |\\n| Ours w/o GAN    | 21.28         | 0.812         | 23.17              | 0.883              | 22.14    | 0.642    |\\n| Ours with noise | 24.15         | 0.832         | 24.07              | 0.880              | 22.86    | 0.648    |\\n| Ours            | 24.62         | 0.867         | 25.62              | 0.905              | 23.18    | 0.664    |\\n\\nTable 6. Results of the ablation study in Sec. 4.6.\\n\\n| Methods         | CE LOL-real | $L_2$ LOL-real | CE LOL-synthetic | $L_2$ LOL-synthetic | CE SID | $L_2$ SID |\\n|-----------------|-------------|----------------|------------------|------------------|--------|----------|\\n| Ours w/o GAN    | 0.2581      | 0.3650         | 0.2144           | 0.3936           | 0.5335 | 0.5034   |\\n| Ours w/o S.G.   | 0.2923      | 0.3805         | 0.2133           | 0.3833           | 0.5035 | 0.5405   |\\n| Ours w/o F      | 0.3070      | 0.3553         | 0.2795           | 0.3675           | 0.5351 | 0.4905   |\\n| Ours            | 0.2130      | 0.3042         | 0.2072           | 0.3032           | 0.4352 | 0.4541   |\"}"}
{"id": "CVPR-2023-1218", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.7. User Study\\n\\nWe conduct a large-scale user study with 100 participants for human subjective evaluation, and the five strongest baselines are chosen by mean PSNR on SID and LOL. All methods are trained on SID, and we evaluate their performances on the real-captured low-light photos (a total of 50 images) from an iPhone 8. The capturing environments are various, including indoor and outdoor scenes. Following the settings in [56], the evaluation is completed via user ratings on the six questions shown in Fig. 7, where scores range from 1 (worst) to 5 (best). Fig. 7 exhibits the rating distributions of different methods. We can see that our framework receives more high and fewer low ratings, showing the superior perception of our results.\\n\\n4.8. Structure Modeling with More Data\\n\\nAll the above results (ours and baselines) are obtained from the networks trained without extra data. The collection of paired low-light and normal-light data, i.e., $I$ and $\\\\bar{I}$, is difficult and expensive, while the paired data to supervise the structure modeling $S$ is trivial since the ground truth $\\\\bar{I}_s$ can be obtained with the edge detection algorithm from $\\\\bar{I}$. Therefore, we can easily obtain extra data pair for $S$ from another dataset containing normal-light data. Following [60], we adopt A V A dataset [31] to acquire the data pair to finetune $S$. The result can be improved again by synthesizing better structure for very dark areas, as shown in Table 7 (Ours with E.D.).\\n\\n4.9. Ours with Other Structural Representations\\n\\nIn this work, we adopt the edge for the structure modeling since the edge is a general structural representation for various scenes. Our framework is also applicable to other structural representations, e.g., the segmentation and depth maps. We adopt the DPT model [35], which is trained on ADE20K [72] and MIX 6 [35], to extract the normal-light data's segmentation maps and depth maps. And they are employed as the ground truth to train $S$. As shown in Table 7, all these results (\u201cOurs with Seg.\u201d and \u201cOurs with Dep.\u201d) are better than the baselines in Tables 1, 2, 3, demonstrating the effectiveness of our structure modeling and guidance strategy.\\n\\n5. Conclusion\\n\\nIn this paper, we propose a new framework to conduct structure modeling and employ the restored structure maps to enhance the appearance modeling results. Different from existing methods, the structure modeling in our framework is undertaken via a modified generative model with structural features and is trained with the GAN loss. We further design a novel structure-guided enhancement module for appearance enhancement with structure-guided feature synthesis layers. Extensive experiments on sRGB and RAW domains demonstrate the effectiveness of our framework.\\n\\nLimitation.\\n\\nThe structure generation with our model in extremely dark areas (with almost no information) could have artifacts when only trained with existing low-light datasets. This issue can be alleviated by training with large-scale data as indicated in Sec. 4.8, requiring more computation resources and a long training time.\\n\\nAcknowledgments.\\n\\nThis work is partially supported by Shenzhen Science and Technology Program KQTD20210811090149095.\"}"}
{"id": "CVPR-2023-1218", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Low-Light Image Enhancement via Structure Modeling and Guidance\\n\\nXiaogang Xu, Ruixing Wang, Jiangbo Lu\\n\\n1. Zhejiang Lab\\n2. Honor Device Co., Ltd.\\n3. SmartMore Corporation\\n\\nxgxu@zhejianglab.com, ruixingw@hustunique.com, jiangbo@smartmore.com\\n\\nAbstract\\n\\nThis paper proposes a new framework for low-light image enhancement by simultaneously conducting the appearance as well as structure modeling. It employs the structural feature to guide the appearance enhancement, leading to sharp and realistic results. The structure modeling in our framework is implemented as the edge detection in low-light images. It is achieved with a modified generative model via designing a structure-aware feature extractor and generator. The detected edge maps can accurately emphasize the essential structural information, and the edge prediction is robust towards the noises in dark areas. Moreover, to improve the appearance modeling, which is implemented with a simple U-Net, a novel structure-guided enhancement module is proposed with structure-guided feature synthesis layers. The appearance modeling, edge detector, and enhancement module can be trained end-to-end. The experiments are conducted on representative datasets (sRGB and RAW domains), showing that our model consistently achieves SOTA performance on all datasets with the same architecture. The code is available at https://github.com/xiaogang00/SMG-LLIE.\\n\\n1. Introduction\\n\\nThe low-light enhancement aims to recover normal-light and noise-free images from dark and noisy pictures, which is a long-standing and significant computer vision topic. It has broad application fields, including low-light imaging [11, 27, 50], and also benefits many downstream vision tasks, e.g., nighttime detection [28, 48, 64]. Some methods have been proposed to tackle the low-light enhancement problem. They design networks that learn to manipulate color, tone, and contrast [7, 10, 45, 63], and some recent works also account for noise in images [24, 55]. Most of these works optimize the appearance distance between the output and the ground truth. However, they ignore the explicit modeling of structural details in dark areas and thus resulting in blurry outcomes and low SSIM [51] values, as shown in Fig. 2. Some works [34, 74] have noticed the effect of employing structural information, e.g., edge, to promote the enhancement. Edge guides the enhancement by distinguishing between different parts in the dark regions. Moreover, adding sensible edge priors into dark regions reduces the ill-posed degree in optimizing the appearance reconstruction. These frameworks [34, 74] perform the structure modeling with encoder-decoder-based networks and a regression loss. However, the corresponding structure modeling results are not satisfying due to the uncertainty in the dark areas caused by severely poor visibility and noise. Furthermore, the strategy of using the extracted structural information needs to be improved from the existing straightforward concatenation approach [34, 74].\\n\\nIn this paper, we propose to utilize a generative model $S$ trained with a GAN loss to perform the structure modeling with the form of edges. Then, we design a new mechanism $E$ to facilitate the initial low-light appearance enhancement (the module is denoted as $A$) with structure-guided feature synthesis. With effective structure modeling and guidance, our framework can output sharp and realistic results with satisfactory reconstruction quality as shown in Fig. 2. Compared with previous structure modeling networks,\"}"}
{"id": "CVPR-2023-1218", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. A challenging low-light frame (a), from SID-sRGB [3], enhanced by a SOTA method (c) and our method (e). Our method can synthesize the structure map (d) from the input image, leading to clearer details, more distinct contrast, and more vivid color. Although (c) has high PSNR as 28.17, its SSIM is low as 0.75. Ours achieves high scores for both dB and SSIM, as 28.60dB and 0.80.\\n\\nThe proposed generative model $S$ has two significant modifications. First, we notice the impact of providing structure-aware descriptors into both the encoder and decoder of $S$, disentangling the appearance representation and underlying the structural information. Thus, we design a Structure-Aware Feature Extractor (SAFE) as the encoder part, which extracts structure-aware features from the dark image and its gradients via spatially-varying operations (achieved with adaptive long-range and short-range computations). The extracted structure-aware tensors are then fed into the decoder part to generate the desired structure maps. Moreover, different from current approaches, which employ the structure maps of normal-light images to conduct the regression learning, we find the nice property of using a GAN loss. The GAN loss can reduce the artifacts in the generated structure maps that are caused by the noise and invisibility, highlighting the essential structure required for enhancement. The backbone of $S$ is implemented as a modified StyleGAN.\\n\\nTo boost the appearance by leveraging the obtained structure maps, we design a Structure-Guided Enhancement Module (SGEM) as $E$. The main target of SGEM is to learn the residual, which can improve the initial appearance modeling results. In SGEM, spatially-adaptive kernels and normalization parameters are generated according to the structure maps. Then, the features in each layer of the SGEM's decoder will be processed with spatially-adaptive convolutions and normalization. Although the overall architecture of SGEM takes the form of a simple U-Net [38], it can effectively enhance the original appearance.\\n\\n$S$, $A$, and $E$ can be trained end-to-end simultaneously. Extensive experiments are conducted on representative benchmarks. Experimental results show that our framework achieves SOTA performance on both PSNR and SSIM metrics with the same architecture on all datasets, as shown in Fig. 1. In summary, our work's contribution is four-fold.\\n\\n1. We propose a new framework for low-light enhancement by conducting structure modeling and guidance simultaneously to boost the appearance enhancement.\\n2. We design a novel structure modeling method, where structure-aware features are formulated and trained with a GAN loss.\\n3. A novel structure-guided enhancement approach is proposed for appearance improvement guided by the restored structure maps.\\n4. Extensive experiments are conducted on different datasets in both sRGB and RAW domains, showing the effectiveness and generalization of our framework.\\n\\n2. Related Work\\n\\nLow-light enhancement with learning. The current low-light image enhancement methods with deep learning mainly optimize the appearance reconstruction error between the output and the ground truth [1, 9, 13, 17, 24, 26, 46, 55, 57, 58, 61, 62, 67, 68, 70, 71]. However, the enhanced appearance tends to be blurry in the dark regions even with SOTA approaches [56]. To this, some works utilize the structural information [16, 74] to enhance appearance. To obtain the structure maps, some of them use offline-computed edges/gradients [11, 16, 36, 41] or offline-trained networks [23] which are not adaptively optimized with the low-light data and result in artifacts inevitably. Although several methods [34, 74] have been proposed to train an edge detector and an image enhancement network simultaneously with both regression losses, their improvements are still limited. Moreover, existing strategies mainly concatenate the extracted structure maps with the images to facilitate the enhancement [16, 34, 74], which, however, cannot set the guidance in all layers of the enhancement network. Different from current works, we propose a new generative model to robustly extract the edge map from a dark input image with the GAN loss, providing the critical edge information for enhancement in every feature layer.\\n\\nGenerative model for restoration. The recent development on the generative model enables a wide range of networks, e.g., StyleGAN [15], to achieve significant restoration effects in some tasks, e.g., face restoration [12, 59, 73]. However, their performances are highly dependent on the pre-trained model within several categories. In this paper, we demonstrate that generative models with the GAN loss can be utilized in a novel way to synthesize structural information for the restoration task of low-light image enhancement. Such an approach does not rely on the pre-trained model, and such an idea can actually be extended to various restoration tasks, opening up a new research direction.\"}"}
{"id": "CVPR-2023-1218", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. The overview of our framework with explicit appearance modeling $A$, structure modeling $S$, and SGEM $E$. The supervision for $I_a$ and $I_b$ is the normal-light image $\\\\bar{I}$, and for $I_s$ is the edge $\\\\bar{I}_s$ extracted from $\\\\bar{I}$. The overall framework can be trained end-to-end.\\n\\n3. Method\\nThe overview of our framework can be viewed in Fig. 3. The proposed framework conducts explicit modeling for both appearance $A$ as well as structure $S$, and they will be described in Sec. 3.1 and Sec. 3.2, respectively. The structure maps are employed to guide the enhancement of appearance modeling with the module of $E$, which will be introduced in Sec. 3.3. The overall framework can be trained end-to-end as elaborated in Sec. 3.4.\\n\\n3.1. Appearance Modeling\\nIn our framework, given an input low-light image $I$, we adopt a common U-Net [3] as the network for appearance modeling, as shown in Fig. 3 (a), where $I_a = A(I)$. The experimental results show that our approach combined with the structural modeling can achieve SOTA performance, with such an ordinary network for appearance prediction.\\n\\n3.2. Structure Modeling\\nIn this paper, we adopt edge information for structure modeling, which is a general representation applicable to various scenes. We find that the effective generative model, e.g., the StyleGAN [15], can be modified to be a strong structure estimator $S$ for low-light images as shown in Fig. 3 (b). Equipped with both the structure regression loss and the GAN loss, $S$ can accurately predict the effective structure maps that are beneficial for appearance enhancement. In this paper, we use the backbone of StyleGAN to formulate $S$. Given the input image $I$, we can obtain its structure map $S$ as $I_s = S(I) = G(F(I))$, where $F$ is the encoder part, as \u201cStructure-Aware Feature Extractor (SAFE)\u201d that contains $N$ layers and two mapping functions, and $G$ is decoder part, as \u201cStructure-Aware Generator (SAG)\u201d. $F$ and $G$ enable our framework to perform better structure modeling than previous generative models and edge detectors.\\n\\n3.2.1 Structure-Aware Feature Extractor (SAFE)\\nAlthough the traditional encoder-based generative model can be employed as the structure extractor, its performance can be improved by feeding structural features into both the encoder and the generator. Previous encoder-based generative models extract information from only the content feature, which is suitable for the content generation (e.g., sRGB images) while insufficient for structure modeling. Moreover, the conventional feature extractor of the generative model mainly utilizes the short-range operation [37, 42, 47], e.g., CNN. However, as indicated in [56], formulating useful representations in dark areas with low Signal-to-Noise Ratio (SNR) needs long-range operations. Thus, we design the SAFE in $S$ as shown in Fig. 3 (b), whose main target is to extract the reliable structural features that can be utilized next in the SAG. Different from conventional generative models\u2019 encoders, SAFE obtains the required information from both content and gradient maps which are helpful for edge modeling. Spatially-varying operations are formulated in SAFE for feature extraction.\\n\\nThe formulation of gradient maps. As shown in Fig. 3 (b), the input image $I$ is extracted by a multi-layer encoder, $F_1, ..., F_N$, with down-sampling in each layer. Suppose\"}"}
{"id": "CVPR-2023-1218", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where \\\\( M \\\\) is the number of different directions that can be adaptively merged with the Gradient Map of the input feature. Moreover, these spatially-varying extracted features for different axes, as shown in Fig. 3 (c), can effectively extract structural features. For this purpose, we design the spatially-varying operations in the encoder to efficiently extract structural features. For example, let \\\\( F \\\\) denote the feature extraction process, then the first order gradient is computed as \\\\( \\\\nabla f \\\\). The feature extraction process can be written as \\\\( f = g_f(l) = \\\\nabla F \\\\), where \\\\( g_f \\\\) is the Grad-F module.\\n\\nAfter obtaining the structure-aware features \\\\( f \\\\), the next step is to get the corresponding Long-Range Encoder (LRE) and Short-Range Encoder (SRE) modules. The transformer blocks are proven to have extraordinary ability in the long-range modeling [4, 25, 52, 53, 65, 66], and the CNN blocks are capable of extracting short-range characteristics. Thus, LRE and SRE are implemented as the transformer and CNN blocks, respectively. To build an efficient transformer model, we adopt the transformer block with the window-based attention mechanism [25] and local-enhanced forward module to merge the outputs from long-range and short-range operations, whose architecture is the multilayer perceptron. We adopt another Long-Short-Range Fusion (LSR-F) module to merge the outputs from long-range and short-range feature extraction for structure.\"}"}
{"id": "CVPR-2023-1218", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Jianrui Cai, Shuhang Gu, and Lei Zhang. Learning a deep single image contrast enhancer from multi-exposure images. IEEE TIP, 2018.\\n\\n[2] John Canny. A computational approach to edge detection. IEEE TPAMI, 1986.\\n\\n[3] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. Learning to see in the dark. In CVPR, 2018.\\n\\n[4] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yip-ing Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu, and Wen Gao. Pre-trained image processing transformer. In CVPR, 2021.\\n\\n[5] Qifeng Chen, Jia Xu, and Vladlen Koltun. Fast image processing with fully-convolutional networks. In ICCV, 2017.\\n\\n[6] Xingbo Dong, Wanyan Xu, Zhihui Miao, Lan Ma, Chao Zhang, Jiewen Yang, Zhe Jin, Andrew Beng Jin Teoh, and Jiajun Shen. Abandoning the Bayer-Filter to see in the dark. In CVPR, 2022.\\n\\n[7] Xueyang Fu, Delu Zeng, Yue Huang, Xiao-Ping Zhang, and Xinghao Ding. A weighted variational model for simultaneous reflectance and illumination estimation. In CVPR, 2016.\\n\\n[8] Shuhang Gu, Yawei Li, Luc Van Gool, and Radu Timofte. Self-guided network for fast image denoising. In ICCV, 2019.\\n\\n[9] Chunle Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and Runmin Cong. Zero-reference deep curve estimation for low-light image enhancement. In CVPR, 2020.\\n\\n[10] Xiaojie Guo, Yu Li, and Haibin Ling. LIME: Low-light image enhancement via illumination map estimation. IEEE TIP, 2016.\\n\\n[11] Shijie Hao, Xu Han, Yanrong Guo, Xin Xu, and Meng Wang. Low-light image enhancement with semi-decoupled decomposition. IEEE TMM, 2020.\\n\\n[12] Jingwen He, Wu Shi, Kai Chen, Lean Fu, and Chao Dong. GCFSR: a generative and controllable face super resolution method without facial and GAN priors. In CVPR, 2022.\\n\\n[13] Yifan Jiang, Xinyu Gong, Ding Liu, Yu Cheng, Chen Fang, Xiaohui Shen, Jianchao Yang, Pan Zhou, and Zhangyang Wang. EnlightenGAN: Deep light enhancement without paired supervision. IEEE TIP, 2021.\\n\\n[14] Yeying Jin, Wenhan Yang, and Robby T Tan. Unsupervised night image enhancement: When layer decomposition meets light-effects suppression. In ECCV, 2022.\\n\\n[15] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In CVPR, 2020.\\n\\n[16] Eungyeup Kim, Sanghyeon Lee, Jeonghoon Park, Somi Choi, Choonghyun Seo, and Jaegul Choo. Deep edge-aware interactive colorization against color-bleeding effects. In ICCV, 2021.\\n\\n[17] Hanul Kim, Su-Min Choi, Chang-Su Kim, and Yeong Jun Koh. Representative color transform for image enhancement. In ICCV, 2021.\\n\\n[18] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv:1412.6980, 2014.\\n\\n[19] Satoshi Kosugi and Toshihiko Yamasaki. Unpaired image enhancement featuring reinforcement-learning-controlled image editing software. In AAAI, 2020.\\n\\n[20] Mohit Lamba, Atul Balaji, and Kaushik Mitra. Towards fast and light-weight restoration of dark images. In BMVC, 2020.\\n\\n[21] Mohit Lamba and Kaushik Mitra. Restoring extremely dark images in real time. In CVPR, 2021.\\n\\n[22] Jiaqian Li, Juncheng Li, Faming Fang, Fang Li, and Guixu Zhang. Luminance-aware pyramid network for low-light image enhancement. IEEE TMM, 2020.\\n\\n[23] Dong Liang, Ling Li, Mingqiang Wei, Shuo Yang, Liyan Zhang, Wenhan Yang, Yun Du, and Huiyu Zhou. Semantically contrastive learning for low-light image enhancement. In AAAI, 2022.\\n\\n[24] Risheng Liu, Long Ma, Jiaao Zhang, Xin Fan, and Zhongxuan Luo. Retinex-inspired unrolling with cooperative prior architecture search for low-light image enhancement. In CVPR, 2021.\\n\\n[25] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.\\n\\n[26] Kin Gwn Lore, Adedotun Akintayo, and Soumik Sarkar. LL-Net: A deep autoencoder approach to natural low-light image enhancement. PR, 2017.\\n\\n[27] Yucheng Lu and Seung-Won Jung. Progressive joint low-light enhancement and noise removal for raw images. IEEE TIP, 2022.\\n\\n[28] Long Ma, Tengyu Ma, Risheng Liu, Xin Fan, and Zhongxuan Luo. Toward fast, flexible, and robust low-light image enhancement. In CVPR, 2022.\\n\\n[29] Paras Maharjan, Li Li, Zhu Li, Ning Xu, Chongyang Ma, and Yue Li. Improving extreme low-light image denoising via residual learning. In 2019 IEEE International Conference on Multimedia and Expo (ICME), 2019.\\n\\n[30] Sean Moran, Pierre Marza, Steven McDonagh, Sarah Parisot, and Gregory Slabaugh. DeepLPF: Deep local parametric filters for image enhancement. In CVPR, 2020.\\n\\n[31] Naila Murray, Luca Marchesotti, and Florent Perronnin. A V A: A large-scale database for aesthetic visual analysis. In CVPR, 2012.\\n\\n[32] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. In NeurIPS, 2019.\\n\\n[33] Mengyang Pu, Yaping Huang, Yuming Liu, Qingji Guan, and Haibin Ling. EDTER: Edge detection with transformer. In CVPR, 2022.\\n\\n[34] Deepanshu Rana, Kanishk Jayant Lal, and Anil Singh Parihar. Edge guided low-light image enhancement. In 2021 5th International Conference on Intelligent Computing and Control Systems (ICICCS), 2021.\"}"}
{"id": "CVPR-2023-1218", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ren\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In ICCV, 2021.\\n\\nWenqi Ren, Sifei Liu, Lin Ma, Qianqian Xu, Xiangyu Xu, Xiaochun Cao, Junping Du, and Ming-Hsuan Yang. Low-light image enhancement via a deep hybrid network. IEEE TIP, 2019.\\n\\nElad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding in style: a StyleGAN encoder for image-to-image translation. In CVPR, 2021.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, 2015.\\n\\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. In ICLR, 2014.\\n\\nZhuo Su, Wenzhe Liu, Zitong Yu, Dewen Hu, Qing Liao, Qi Tian, Matti Pietik\u00e4inen, and Li Liu. Pixel difference networks for efficient edge detection. In ICCV, 2021.\\n\\nMasayuki Tanaka, Takashi Shibata, and Masatoshi Okutomi. Gradient-based low-light image enhancement. In 2019 IEEE International Conference on Consumer Electronics (ICCE), 2019.\\n\\nOmer Tov, Yuval Alaluf, Yotam Nitzan, Or Patashnik, and Daniel Cohen-Or. Designing an encoder for StyleGAN image manipulation. ACM TOG, 2021.\\n\\nHaoyuan Wang, Ke Xu, and Rynson WH Lau. Local color distributions prior for image enhancement. In ECCV, 2022.\\n\\nRuixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen, Wei-Shi Zheng, and Jiaya Jia. Underexposed photo enhancement using deep illumination estimation. In CVPR, 2019.\\n\\nShuhang Wang, Jin Zheng, Hai-Miao Hu, and Bo Li. Naturalness preserved enhancement algorithm for non-uniform illumination images. IEEE TIP, 2013.\\n\\nTao Wang, Yong Li, Jingyang Peng, Yipeng Ma, Xian Wang, Fenglong Song, and Youliang Yan. Real-time image enhancer via learnable spatial-aware 3D lookup tables. In ICCV, 2021.\\n\\nTengfei Wang, Yong Zhang, Yanbo Fan, Jue Wang, and Qifeng Chen. High-fidelity GAN inversion for image attribute editing. In CVPR, 2022.\\n\\nWenjing Wang, Wenhan Yang, and Jiaying Liu. Hla-face: Joint high-low adaptation for low light face detection. In CVPR, 2021.\\n\\nYufei Wang, Renjie Wan, Wenhan Yang, Haoliang Li, Lap-Pui Chau, and Alex Kot. Low-light image enhancement with normalizing flow. In AAAI, 2022.\\n\\nYun-Fei Wang, He-Ming Liu, and Zhao-Wang Fu. Low-light image enhancement via the absorption light scattering model. IEEE TIP, 2019.\\n\\nZhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE TIP, 2004.\\n\\nZhendong Wang, Xiaodong Cun, Jianmin Bao, and Jianzhuang Liu. Uformer: A general U-Shaped transformer for image restoration. In CVPR, 2022.\\n\\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. CvT: Introducing convolutions to vision transformers. In ICCV, 2021.\\n\\nWenhui Wu, Jian Weng, Pingping Zhang, Xu Wang, Wenhan Yang, and Jianmin Jiang. URetinex-Net: Retinex-Based deep unfolding network for low-light image enhancement. In CVPR, 2022.\\n\\nKe Xu, Xin Yang, Baocai Yin, and Rynson WH. Lau. Learning to restore low-light images via decomposition-and-enhancement. In CVPR, 2020.\\n\\nXiaogang Xu, Ruixing Wang, Chi-Wing Fu, and Jiaya Jia. SNR-Aware low-light image enhancement. In CVPR, 2022.\\n\\nJianzhou Yan, Stephen Lin, Bing Kang Sing, and Xiaoou Tang. A learning-to-rank approach for image color enhancement. In CVPR, 2014.\\n\\nZhicheng Yan, Hao Zhang, Baoyuan Wang, Sylvain Paris, and Yizhou Yu. Automatic photo adjustment using deep neural networks. ACM TOG, 2016.\\n\\nTao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. GAN prior embedded network for blind face restoration in the wild. In CVPR, 2021.\\n\\nWenhan Yang, Shiqi Wang, Yuming Fang, Yue Wang, and Jiaying Liu. From fidelity to perceptual quality: A semi-supervised approach for low-light image enhancement. In CVPR, 2020.\\n\\nWenhan Yang, Shiqi Wang, Yuming Fang, Yue Wang, and Jiaying Liu. Band representation-based semi-supervised low-light image enhancement: Bridging the gap between signal fidelity and perceptual quality. IEEE TIP, 2021.\\n\\nWenhan Yang, Wenjing Wang, Haofeng Huang, Shiqi Wang, and Jiaying Liu. Sparse gradient regularized deep Retinex network for robust low-light image enhancement. IEEE TIP, 2021.\\n\\nZhenqiang Ying, Ge Li, Yurui Ren, Ronggang Wang, and Wenmin Wang. A new low-light image enhancement algorithm using camera response model. In ICCV, 2017.\\n\\nJun Yu, Xinlong Hao, and Peng He. Single-stage face detection under extremely low-light conditions. In ICCV, 2021.\\n\\nKun Yuan, Shaopeng Guo, Ziwei Liu, Aojun Zhou, Feng-wei Yu, and Wei Wu. Incorporating convolution designs into visual transformers. In ICCV, 2021.\\n\\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zihang Jiang, Francis EH. Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-Token ViT: Training vision transformers from scratch on ImageNet. In ICCV, 2021.\\n\\nSyed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, Ming-Hsuan Yang, and Ling Shao. Learning enriched features for real image restoration and enhancement. In ECCV, 2020.\\n\\nHui Zeng, Jianrui Cai, Lida Li, Zisheng Cao, and Lei Zhang. Learning image-adaptive 3D lookup tables for high performance photo enhancement in real-time. IEEE TPAMI, 2020.\"}"}
{"id": "CVPR-2023-1218", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yonghua Zhang, Jiawan Zhang, and Xiaojie Guo. Kindling the darkness: A practical low-light image enhancer. In ACM MM, 2019.\\n\\nLin Zhao, Shao-Ping Lu, Tao Chen, Zhenglu Yang, and Ariel Shamir. Deep symmetric network for underexposed image enhancement with recurrent attentional learning. In ICCV, 2021.\\n\\nChuanjun Zheng, Daming Shi, and Wentian Shi. Adaptive unfolding total variation network for low-light image enhancement. In ICCV, 2021.\\n\\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene parsing through ade20k dataset. In CVPR, 2017.\\n\\nFeida Zhu, Junwei Zhu, Wenqing Chu, Xinyi Zhang, Xiaozhong Ji, Chengjie Wang, and Ying Tai. Blind face restoration via integrating face shape and generative priors. In CVPR, 2022.\\n\\nMinfeng Zhu, Pingbo Pan, Wei Chen, and Yi Yang. EEMEFN: Low-light image enhancement via edge-enhanced multi-exposure fusion network. In AAAI, 2020.\"}"}
