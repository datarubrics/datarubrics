{"id": "CVPR-2023-530", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Z. Cai and N. Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In CVPR, 2018.\\n\\n[2] N. Carion, F. Massa, G. Synnaeve, N. Usunier, A. Kirillov, and S. Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\\n\\n[3] X. Dai, Y. Chen, J. Yang, P. Zhang, L. Yuan, and L. Zhang. Dynamic detr: End-to-end object detection with dynamic attention. In ICCV, 2021.\\n\\n[4] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2018.\\n\\n[5] A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2020.\\n\\n[6] M. Everingham, L. Van Gool, C. K. Williams, J. Winn, and A. Zisserman. The pascal visual object classes (voc) challenge. IJCV, 2010.\\n\\n[7] A. Geiger, P. Lenz, C. Stiller, and R. Urtasun. Vision meets robotics: The kitti dataset. The International Journal of Robotics Research, 2013.\\n\\n[8] G. Georgakis, M. A. Reza, A. Mousavian, P.-H. Le, and J. Ko\u0161eck\u00e1. Multiview rgb-d dataset for object instance detection. In 3DV, 2016.\\n\\n[9] R. Girshick. Fast r-cnn. In CVPR, 2015.\\n\\n[10] R. Girshick, J. Donahue, T. Darrell, and J. Malik. Rich feature hierarchies for accurate object detection and semantic segmentation. In CVPR, 2014.\\n\\n[11] X. Gu, T.-Y. Lin, W. Kuo, and Y. Cui. Open-vocabulary object detection via vision and language knowledge distillation. In ICLR, 2021.\\n\\n[12] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\n[13] N. Inoue, R. Furuta, T. Yamasaki, and K. Aizawa. Cross-domain weakly-supervised object detection through progressive domain adaptation. In CVPR, 2018.\\n\\n[14] R. Krishna, Y. Zhu, O. Groth, J. Johnson, K. Hata, J. Kravitz, S. Chen, Y. Kalantidis, L.-J. Li, D. A. Shamma, et al. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 2017.\\n\\n[15] J. Lambert, Z. Liu, O. Sener, J. Hays, and V. Koltun. Mseg: A composite dataset for multi-domain semantic segmentation. In CVPR, 2020.\\n\\n[16] S. Lan, Z. Ren, Y. Wu, L. S. Davis, and G. Hua. Saccadenet: A fast and accurate object detector. In CVPR, 2020.\\n\\n[17] S. Lan, X. Yang, Z. Yu, Z. Wu, J. M. Alvarez, and A. Anandkumar. Vision transformers are good mask auto-labelers. arXiv preprint arXiv:2301.03992, 2023.\\n\\n[18] H. Li, Z. Wu, C. Zhu, C. Xiong, R. Socher, and L. S. Davis. Learning from noisy anchors for one-stage object detection. In CVPR, 2020.\\n\\n[19] L. H. Li, P. Zhang, H. Zhang, J. Yang, C. Li, Y. Zhong, L. Wang, L. Yuan, L. Zhang, J.-N. Hwang, et al. Grounded language-image pre-training. In CVPR, 2022.\\n\\n[20] T.-Y. Lin, P. Goyal, R. Girshick, K. He, and P. Doll\u00e1r. Focal loss for dense object detection. In ICCV, 2017.\\n\\n[21] T.-Y. Lin, M. Maire, S. Belongie, J. Hays, P. Perona, D. Ramanan, P. Doll\u00e1r, and C. L. Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\\n\\n[22] Z. Liu, Y. Lin, Y. Cao, H. Hu, Y. Wei, Z. Zhang, S. Lin, and B. Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.\\n\\n[23] I. Loshchilov and F. Hutter. Decoupled weight decay regularization. In ICLR, 2018.\\n\\n[24] D. Meng, X. Chen, Z. Fan, G. Zeng, H. Li, Y. Yuan, L. Sun, and J. Wang. Conditional detr for fast training convergence. In ICCV, 2021.\\n\\n[25] L. Meng, H. Li, B.-C. Chen, S. Lan, Z. Wu, Y.-G. Jiang, and S.-N. Lim. Adavit: Adaptive vision transformers for efficient image recognition. In CVPR, 2022.\\n\\n[26] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. In ICLR, 2013.\\n\\n[27] A. Mogelmose, M. M. Trivedi, and T. B. Moeslund. Vision-based traffic sign detection and analysis for intelligent driver assistance systems: Perspectives and survey. TPAMI, 2012.\\n\\n[28] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In EMNLP, 2014.\\n\\n[29] M. E. Peters, M. Neumann, M. Iyyer, M. Gardner, C. Clark, K. Lee, and L. Zettlemoyer. Deep contextualized word representations. In NAACL, 2018.\\n\\n[30] H. Qiu, Y. Ma, Z. Li, S. Liu, and J. Sun. Borderdet: Border feature for dense object detection. In ECCV, 2020.\\n\\n[31] A. Radford, J. W. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021.\\n\\n[32] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, et al. Language models are unsupervised multi-task learners. OpenAI blog, 2019.\\n\\n[33] J. Redmon, S. Divvala, R. Girshick, and A. Farhadi. You only look once: Unified, real-time object detection. In CVPR, 2016.\\n\\n[34] S. Ren, K. He, R. Girshick, and J. Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. PAMI, 2016.\\n\\n[35] S. Shao, Z. Li, T. Zhang, C. Peng, G. Yu, X. Zhang, J. Li, and J. Sun. Objects365: A large-scale, high-quality dataset for object detection. In ICCV, 2019.\\n\\n[36] B. Singh, H. Li, A. Sharma, and L. S. Davis. R-fcn-3000 at 30fps: Decoupling detection and classification. In Proceedings of the IEEE conference on computer vision and pattern recognition, 2018.\"}"}
{"id": "CVPR-2023-530", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"P. Sun, R. Zhang, Y. Jiang, T. Kong, C. Xu, W. Zhan, M. Tomizuka, L. Li, Z. Yuan, C. Wang, et al. Sparse r-cnn: End-to-end object detection with learnable proposals. In CVPR, 2021.\\n\\nZ. Tian, C. Shen, H. Chen, and T. He. Fcos: Fully convolutional one-stage object detection. In CVPR, 2019.\\n\\nR. Wang, D. Chen, Z. Wu, Y. Chen, X. Dai, M. Liu, Y.-G. Jiang, L. Zhou, and L. Yuan. Bevt: Bert pretraining of video transformers. In CVPR, 2022.\\n\\nX. Wang, Z. Cai, D. Gao, and N. Vasconcelos. Towards universal object detection by domain attention. In CVPR, 2019.\\n\\nY. Wang, X. Zhang, T. Yang, and J. Sun. Anchor detr: Query design for transformer-based detector. In AAAI, 2022.\\n\\nG.-S. Xia, X. Bai, J. Ding, Z. Zhu, S. Belongie, J. Luo, M. Datcu, M. Pelillo, and L. Zhang. Dota: A large-scale dataset for object detection in aerial images. In CVPR, 2018.\\n\\nS. Xie, R. Girshick, P. Doll\u00e1r, Z. Tu, and K. He. Aggregated residual transformations for deep neural networks. In CVPR, 2017.\\n\\nK. Yan, X. Wang, L. Lu, and R. M. Summers. Deeplesion: automated mining of large-scale lesion annotations and universal lesion detection with deep learning. Journal of medical imaging, 2018.\\n\\nH. Yang, H.-Y. Wu, and H. Chen. Detecting 11k classes: Large scale object detection without fine-grained bounding boxes. In ICCV, 2019.\\n\\nS. Yang, P. Luo, C.-C. Loy, and X. Tang. Wider face: A face detection benchmark. In CVPR, 2016.\\n\\nP. Zhang, X. Li, X. Hu, J. Yang, L. Zhang, L. Wang, Y. Choi, and J. Gao. Vinvl: Revisiting visual representations in vision-language models. In CVPR, 2021.\\n\\nX. Zhao, S. Schulter, G. Sharma, Y.-H. Tsai, M. Chandraker, and Y. Wu. Object detection with a unified label space from multiple datasets. In ECCV, 2020.\\n\\nY. Zhong, J. Yang, P. Zhang, C. Li, N. Codella, L. H. Li, L. Zhou, X. Dai, L. Yuan, Y. Li, et al. Regionclip: Region-based language-image pretraining. In CVPR, 2022.\\n\\nX. Zhou, V. Koltun, and P. Kr\u00e4henb\u00fchl. Simple multi-dataset detection. In CVPR, 2022.\\n\\nX. Zhu, W. Su, L. Lu, B. Li, X. Wang, and J. Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR, 2020.\"}"}
{"id": "CVPR-2023-530", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Detection Hub: Unifying Object Detection Datasets via Query Adaptation on Language Embedding\\n\\nLingchen Meng, Xiyang Dai, Yinpeng Chen, Pengchuan Zhang, Dongdong Chen, Mengchen Liu, Jianfeng Wang, Zuxuan Wu, Lu Yuan, Yu-Gang Jiang\\n\\n1 Shanghai Key Lab of Intell. Info. Processing, School of CS, Fudan University\\n2 Shanghai Collaborative Innovation Center of Intelligent Visual Computing\\n3 Microsoft\\n\\nAbstract\\n\\nCombining multiple datasets enables performance boost on many computer vision tasks. But similar trend has not been witnessed in object detection when combining multiple datasets due to two inconsistencies among detection datasets: taxonomy difference and domain gap. In this paper, we address these challenges by a new design (named Detection Hub) that is dataset-aware and category-aligned. It not only mitigates the dataset inconsistency but also provides coherent guidance for the detector to learn across multiple datasets. In particular, the dataset-aware design is achieved by learning a dataset embedding that is used to adapt object queries as well as convolutional kernels in detection heads. The categories across datasets are semantically aligned into a unified space by replacing one-hot category representations with word embedding and leveraging the semantic coherence of language embedding. Detection Hub fulfills the benefits of large data on object detection. Experiments demonstrate that joint training on multiple datasets achieves significant performance gains over training on each dataset alone. Detection Hub further achieves SoTA performance on UODB benchmark with wide variety of datasets.\\n\\n1. Introduction\\n\\nRecent computer vision development has demonstrated significant benefits of leveraging large-scale data for computer vision tasks, such as image retrieval [31], image recognition [47] and video recognition [39]. However, limited effort has been explored for the object detection task due to the lack of a unified large-scale data. A naive attempt is to combine all annotated data from different sources. However, due to the diversity of objects and the cost of annotating bounding boxes in images, traditional object detection datasets are collected in a domain-specific way, where a limited number of interested categories are regarded as foregrounds and the rest of objects as backgrounds. This results in significant domain gaps between different datasets: the taxonomy are different and the annotations are inconsistent; Even under a unified language embedding, the distribution of categories among datasets are significant different and requires special handling.\\n\\n(a) There are significant domain gaps between different datasets: the taxonomy are different and the annotations are inconsistent; (b) Even under a unified language embedding, the distribution of categories among datasets are significant different and requires special handling.\"}"}
{"id": "CVPR-2023-530", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"in a non-trivial domain shift between different datasets and limits detectors to be trained and tested on a single dataset in order to achieve the best performance on such dataset. In this paper, we attempt to answer the question: \u201cHow can we unify multiple object detection datasets training with a single general object detector?\u201d Towards this goal, we observe two challenges: taxonomy difference and annotation inconsistency, both of which introduce the domain gap, shown in Figure 1 (a). More specifically, for the taxonomy difference issue, the semantic names of similar concepts in different datasets may be very different. And for the annotation inconsistency issue, given similar images, foreground objects in one dataset may be labeled as background in another dataset. The existence of these two challenges may be the key underlying reason why most current detectors only focus on a specific training set, rather than deriving a universal object detector for multiple datasets.\\n\\nCompared to traditional methods that regard semantic categories as class indices, converting them into language embedding can naturally unify the taxonomy and eases the challenge in recent works [19, 31]. However, it won\u2019t solve the problem, as shown in Figure 1 (b), the distribution of categories among datasets are significant different, which contributes most to the domain gap.\\n\\nInspired by recent success of visual-language models [19, 31], we propose a simple yet effective method \u201cDetection Hub\u201d that can enjoy the synergy of multiple datasets. It builds upon current end-to-end detectors which use learnable object queries to produce final detection results [37]. To overcome the aforementioned problems, we have two key ideas accordingly: 1) map the semantic category names of different datasets into a category-aligned embedding, and 2) more importantly, use the embedding to dynamically adapt object queries so that each dataset has its own specific query set. We further change the one-hot based classification branch into vision-language alignment, which can well align categories of different datasets. Accordingly, we adopt a region-to-word alignment loss instead of classical cross entropy loss, which makes our \u201cDetection Hub\u201d not limited by a fixed category vocabulary.\\n\\nOur method leverages the linguistic property of pre-trained language encoders (such that categories with similar semantic meanings across datasets will be automatically embedded together without using an expert-designed label mappers). In addition, categories specific to different datasets will be preserved by dedicated embedding. Moreover, as each dataset has its own adapted object query set to generate detection results, the detector can learn how to adapt its behavior to each dataset based on its specific query set. Therefore, the potential competition or disturbance caused by the annotation inconsistency among different datasets can be avoided.\\n\\nTo demonstrate the effectiveness of our method, we train our \u201cDetection Hub\u201d on three standard object detection datasets jointly: COCO [21], Object365 [35] and Visual-Genome [14]. These large-scale datasets have different properties of taxonomy, vocabulary size and annotation quality. Detecton Hub achieves 45.3, 23.2 and 5.7 AP on each dataset, with significant performance gain of +2.3, +1.0, +0.9 compared with each independently model.\\n\\nTo further verify the effectiveness on datasets with larger variance, we conduct experiments on UODB [40], a combination of 11 extremely varied datasets. Detection Hub achieves an average score of 71 and outperforms the previous SoTA UniversalDA [40] by a large margin of 6.8 point.\\n\\n2. Related Work\\nObject Detection. Object detection [9,10,18,20,34,36,38] has been studied for a long time and become a predominant direction. Beginning with the initial success of R-CNN [10], a two-stage paradigm is formulated by combining a region proposal detector and a region-wise CNN classifier. Fast R-CNN [9] formalizes the idea of region-wise feature extraction using ROI pooling to extract fixed-size region features and then predicts their classes and boxes with fully connected layers. Faster R-CNN [34] further introduces a Region Proposal Network (RPN) to generate region proposals and merges the two-stage into a single network by sharing their convolutional features. Since then, this two-stage architecture has become a leading object detection framework. Further works continue to improve this framework by introducing more stages [1, 16]. Recently, Vision Transformers [5] and end-to-end object detection methods have attracted such attention. DETR [2] first turns the object detection problem into a query-based set prediction problem and formulates object detection as an end-to-end framework. Many follow-ups [17, 24, 25, 51] further developed these methods into other tasks like acceleration and pseudo labeling. Beside, multiple recent methods were proposed to improve the problems in different directions. Deformable DETR [51] first introduces multi-scale deformable attention to replace transformers in DETR and reduces the key sampling points around the reference point to significantly accelerate the training speed. Dynamic DETR [3] further models dynamic attention among multiple dimensions with both a convolution-based encoder and a decoder to further reduce learning difficulty and improves the performance. Sparse R-CNN [37] proposes a sparse set of learnable proposal boxes and a dynamic head to perform end-to-end detection upon the R-CNN framework. Our proposed method is also a query-based object detector. Unlike those DETR query designs, our linguistic adapted query is irrelevant to position and plays a role like a semantic-guided filter.\\n\\nMultiple Datasets Training. There are a few early attempts on training with multiple datasets to recognize a wide variety of objects. Recent works have demonstrated that object detectors trained with multiple datasets can achieve better performance than detectors trained with a single dataset. This is because training with multiple datasets can help the detector learn more general object features and improve its robustness to variations in data distribution. The key idea is to train the detector with a large number of images from different datasets, which allows the detector to learn features that are relevant across a wider range of scenes and objects. This can be achieved by using a multi-task learning approach, where the detector is trained to recognize objects in different datasets simultaneously. Another approach is to use data augmentation techniques to artificially increase the diversity of the training set, such as data mirroring, flipping, and re-scaling. To further verify the effectiveness on datasets with larger variance, we conduct experiments on UODB [40], a combination of 11 extremely varied datasets. Detection Hub achieves an average score of 71 and outperforms the previous SoTA UniversalDA [40] by a large margin of 6.8 point.\"}"}
{"id": "CVPR-2023-530", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"YOLO9000 first proposed a method to jointly train on object detection and image classification, which expanded the detection vocabulary via classification labels. Later, ImageNet and OpenImages with only a small fraction of fully annotated classes. Attempted to build a universal object detection system that is capable of working on various image domains, from diverse datasets by introducing a series of adaptation layers based on the principles of squeeze and excitation. Further proposed to improve loss functions that integrated partial but correct annotations with complementary but noisy pseudo labels among different datasets. Most recently, proposed to train a shared detector with dataset-specific heads and then learned to project the outputs into a unified label space. Thanks to the advantages of leveraging language embedding, our method does not need to learn a joint label space to merge different datasets. This naturally addresses the scaling up of categories and joint of different datasets.\\n\\nLanguage Embedding.\\n\\nGenerative and efficient language representation is an attractive topic in the past few decades. As an integral role in the modern NLP system, word embedding can encode rich semantic in a structured way. Most widely used word embeddings methods, Word2Vec, GloVe, leverage the co-occurrence statistics in a large corpus. As a result, words with similar meanings are close in the embedding space, which can encode rich semantic information. Furthermore, many follow-ups extract context-sensitive features rather than static features. They perform sequence modeling with RNNs or Transformers on the top of traditional static word embeddings. Most recently, learning visual representation with language embedding from language supervision is a promising trend due to rich semantic information and flexible transferable ability of natural language. As a milestone, CLIP designs an image-text alignment pre-train task and performs contrastive learning on a large amount of image-text pairs. Motivated by CLIP, many following works try to improve the training strategy. GLIP reformulate the object detection as phrase grounding, which makes detection benefit from large grounding datasets. RegionCLIP and ViLD leverage the semantics space of CLIP for open-vocabulary object detection. Unlike previous methods attempting to learn a language embedding specifically, we freeze a pre-trained language embedding and design an adaption mechanism to dynamically adapt queries on categories based on the different distributions of datasets.\\n\\n3. Our Method\\n\\n3.1. Revisiting End-to-end Object Detection\\n\\nEnd-to-end detectors utilize object queries to encode the content and position statistics over the training dataset and drive the detector to predict desired objects localization without non-differential components (such as pre-defined anchors and non-maximum suppression). Given a set of learnable queries $v \\\\in \\\\mathbb{R}^{N \\\\times d}$, an end-to-end detector utilizes a transformer $T$ to generate $N$ corresponding predictions. Then the optimization process can be abstracted as solving a matching problem $L_{\\\\text{match}}$ among images in the training dataset $D$:\\n\\n$$\\\\sigma = \\\\arg\\\\min_{\\\\sigma} \\\\sum_{i} L_{\\\\text{match}}(y_i, T(\\\\sigma N, i))$$\\n\\nIdeally, these $N$ learnable queries can cover an arbitrary number of boxes in each image. However, considering $Q$ was optimized on the whole dataset, it will cause each object query to be in charge of multiple boxes and categories, meanwhile competing with each other to overfit to the training data. This limitation will be further amplified when learning across multiple datasets. In this paper, we build upon Sparse-RCNN as our default instantiation due to its intuitive explanation and training efficiency. Although achieving great performance on object detection, Sparse-RCNN has limitations on multi-dataset training: the learnable queries and features are dataset-dependant, which suffer from inconsistencies when the object statistics of datasets vary. Moreover, the classification branch is dataset-specific and hard to be jointly learned across different datasets. To enjoy the synergy and avoid such limitations, we propose category-aligned embedding and use it to adapt the object queries for each dataset to effectively formulate dataset-aware queries. In this way, our \\\"Detection Hub\\\" can learn how to adapt its behavior for each dataset based on its own query set.\\n\\n3.2. Category-aligned Embedding\\n\\nGiven two datasets $D_1, D_2$ with $n_1, n_2$ categories respectively, we desire to have an embedding function $E$ that can map them together in a common space. Traditional one-hot embedding cannot serve this purpose as it treats category solely as an index. Later works tried to manually or automatically learn a mapping function across datasets with limited success.\\n\\nIt is natural to think about using category names directly, e.g., use language models to generate unified embedding. In our method, we further leverage the semantic properties of the category set, through using a pretrained language model to encode the language embedding. Specifically, we concatenate the category names from a dataset to together and then use a language model to obtain the language embedding $E$ of the dataset:\\n\\n$$E = \\\\text{Embed}(\\\\text{Tokenizer}(\\\\text{CategorySet}))$$\"}"}
{"id": "CVPR-2023-530", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Consider that there are $N_d$ datasets $D_{multi} = \\\\{D_1, D_2, ..., D_{N_d}\\\\}$, and thus we can obtain a set of dataset-specific language embedding $E_{multi} = \\\\{E_1, E_2, ..., E_{N_d}\\\\}$ accordingly. $E_{multi}$ can be considered as an automatic \u201clabel mapper\u201d by its very nature: all semantic category names of different datasets are mapped into a unified language space, where categories of similar meaning will be close while categories specific to each dataset will be represented by dedicated embeddings.\\n\\nWith this great property, we further convert the original index-based classification branch (dataset-specific) used in SparseRCNN into vision-language alignment based classification branch (shared). Accordingly, we adopt a region-word alignment loss rather than the traditional cross-entropy loss for optimization. In particular, we first use a BERT model to take dataset embedding $E$, and estimate the inner relationships among different categories in each of the dataset-specific language embedding and obtain an enhanced embedding $E'$. Then we project $E'$ and object features $x_{box}^*$ into a same vision-language space via two MLP branches $FC_E$ and $FC_V$ respectively. Following [19, 31], the alignment scores $S$ can be calculated by the dot product between $E'$ and $x_{obj}^*$ to reflects the similarities between visual objects and semantic categories:\\n\\n$$E' = \\\\text{BERT}(E)$$\\n\\n$$S = \\\\text{Sigmoid}(FC_E(E') \\\\cdot FC_V(x_{obj}^*))$$\\n\\nSince we treat the classification as vision-language alignment, the classification target is converted into sub-word grounding [19] instead of one-hot label [34, 37]. For each prediction and ground-truth pair, we mark the sub-word belonging to the target category as a positive pair, i.e., $\\\\hat{T}_{i,j} = 1$, otherwise $\\\\hat{T}_{i,j} = 0$. Finally, the alignment scores can be optimized with the vision-language alignment loss same as the binary cross-entropy loss in traditional detector:\\n\\n$$L_{align}(S, \\\\hat{T}) = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\sum_{j=1}^{L} S_{i,j} \\\\cdot \\\\hat{T}_{i,j} + (1 - S_{i,j}) \\\\cdot (1 - \\\\hat{T}_{i,j})$$\\n\\n$$\\\\hat{T}_{i,j} = \\\\begin{cases} 1, & \\\\text{if token } i,j \\\\text{ belongs to } T_i \\\\\\\\ 0, & \\\\text{otherwise} \\\\end{cases}$$\\n\\nwhere $N$ is the number of queries, and $L$ is the length of the dataset embedding $E$. \\n\\n### 3.3. Dataset-aware Query\\n\\nAs described above, to fully unleash the power of large amount of data in different datasets, our method proposes to use the dataset-specific language embedding to adapt the object queries so that the model can learn to adapt its behavior for each dataset. In detail, given a dataset $D$ and its language embedding $E$, we can achieve the query adaptation by simply performing a cross-attention between the learnable queries $Q$ and $E$. Then the following multi-head-self-attention is used to enhance the adapted queries.\\n\\n$$Q_D = \\\\text{Cross-Attn}(Q, E)$$\\n\\n$$Q_D^* = \\\\text{MHSA}(Q)$$\"}"}
{"id": "CVPR-2023-530", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $Q_D$ is the adapted query for dataset $D$, and $Q_D^*$ is the enhanced query. Conceptually, through cross-attention, we encode the dataset-specific language embedding into the adapted queries $Q_D$, which is used as object query as Fig 2 and makes our detector dataset-aware.\\n\\nFollowing [37], enhanced queries $Q_D^*$ and object features $x_{obj}$ are interacted using dynamic convolutions. In contrast to [37], we use enhanced queries to generate kernels for dynamic convolutions with a few linear layers in the dynamic instance interactive head.\\n\\n$$x_{obj}^* = \\\\text{DyConv}(x_{obj}, Q_D^*) = \\\\text{Conv}(K_2(Q_D^*), \\\\text{Conv}(K_1(Q_D^*), x_{obj}))$$\\n\\n(8)\\n\\nwhere $x_{obj}^*$ is the interacted object feature between enhanced adapted query and object feature; $K_i(Q_D^*) \\\\in \\\\mathbb{R}^{k \\\\times k \\\\times c_{in} \\\\times c_{out}}$ is the dynamic kernel generated by the enhanced adapted query $Q_D^*$.\\n\\nTo further address the challenge of position variance, besides the head, we also use a lightweight query-based RPN with one layer of convolution and the adapted queries to generate the dataset-specific convolution kernels for the RPN dynamically. Then we use the proposals of the highest $N$ scores generated by the query-based RPN as initial proposals boxes for the dynamic instance interactive head.\\n\\nThe query-based RPN and decoder in our detector are both optimized in an end-to-end manner similar to [3, 37, 51].\\n\\n4. Experiment\\n\\n4.1. Implementation Detail\\n\\nWe mainly evaluate our model on three popular object detection benchmarks: COCO [21], Object365 [35] and Visual-Genome [14] following the common practice. COCO dataset contains 118,000 images collected from web images on 80 common classes. Object365 dataset contains around 740,000 images with 365 classes. Each image is densely annotated by human labelers to ensure quality. Visual-Genome dataset contains around 108,077 images with about 1600 classes. The images are very diverse and often contain complex categories with a long tail distribution. Since boxes are generated by algorithms, they are more noise compared to COCO and Object365. Due to different category numbers, image diversity, and annotation density, these three datasets provide a good test bed on the performance of joint training.\\n\\nWe implement our method in PyTorch and train our models using V100 GPUs. We use AdamW [23] optimizer and step down the learning rate by a rate of 0.1 at 78% and 93% of epochs. For ablation studies, we use standard ImageNet-1k pre-trained ResNet-50 [12] as the backbone with $5 \\\\times 10^{-5}$ learning rate and $1 \\\\times 10^{-4}$ weight decay and train it with standard $1 \\\\times$ schedule. We demonstrate the effectiveness of each component and also compare it with other methods under this standard setup. For multi-dataset joint training, our detector can treat any datasets in a unified way. We only need to sample a batch from all datasets, and sample their corresponding categories to build the dataset specific language embedding. We apply dataset re-sampling to make sure that each dataset was trained at only $1 \\\\times$ schedule. We also apply category re-balancing to handle the long-tailed distributions of Object365. Besides, we also evaluate our method on a variety of backbones to demonstrate robustness. For experiments comparing with state-of-the-art methods, we train our method with Swin Large [22] backbone at the $2 \\\\times$ schedule with multi-scale training. We set the learning rate and weight decay the same as ResNet-50. During evaluation, we evaluate our best model with multi-scale testing to compare with state-of-the-art methods reported without using test time augmentation. There is no other augmentation or optimization tricks used during training.\\n\\n4.2. Effectiveness of Multi-dataset Training\\n\\nFor the baseline, we build a Sparse R-CNN model with a ResNet-50 and train on three single-dataset separately. As for baseline for multi-dataset joint training, we follow [40], which simply concatenates all datasets and maps their taxonomy together. Then we can train it with a bigger label mapping layer with a common loss. For our proposed method, we evaluate both joint and separate training ways. As shown in Table 1, under standard $1 \\\\times$ schedule, the jointly trained baseline model is observed a clear performance drop on COCO and VG compared to models trained separately. It demonstrated that simply mapping different labels together doesn't work well due to the domain gaps and taxonomy differences between the datasets. On the contrary, our query-adapted joint training obtains higher performance on each dataset, especially offering a significant gain at 2.3, 1.0, and 0.9 mAP gain on COCO, Object365, and Visual-Genome, respectively. The experiment well demonstrated that our single detector can leverage benefits from multiple datasets training simultaneously.\\n\\nWe also train our method with multiple backbones, such as ResNet-50, ResNext-101, Swin-Tiny, Swin-Large to verify its robustness. As shown in Table 2, it is clear to see that our method obtains significant performance gains on all datasets when jointly trained compared to separate trained. Furthermore, we also evaluate our method's multi-dataset training stability under different training lengths. As shown in Fig 3, the performance gain preserves even when prolonging the training beyond full convergence. These experiments well demonstrate the effectiveness and robustness of our method under multiple datasets joint training.\"}"}
{"id": "CVPR-2023-530", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 1.\\nCompared with baseline methods under single and multiple dataset training.\\n\\n| Method                        | AP 1x | AP 2x | AP 3x |\\n|-------------------------------|-------|-------|-------|\\n| R-50 \\\\[12\\\\]                  | 43.0  | 22.2  | 4.8   |\\n| Swin-T \\\\[22\\\\]                | 46.3  | 25.1  | 5.6   |\\n| RX-101-DCN \\\\[43\\\\]            | 49.2  | 28.3  | 6.1   |\\n| Swin-L \\\\[22\\\\]                | 52.5  | 35.1  | 8.0   |\\n\\n| Method                        | AP 1x | AP 2x | AP 3x |\\n|-------------------------------|-------|-------|-------|\\n| Ours                          | 43.0  | 22.2  | 4.8   |\\n| + Simple merge \\\\[15\\\\]         | 40.6  | 16.1  | 4.9   |\\n| + Cat-aligned Embed           | 40.1  | 16.1  | 4.4   |\\n| + D-Q-adapt Decoder           | 41.1  | 23.3  | 5.2   |\\n| + D-Q-adapt RPN               | 43.0  | 22.2  | 4.8   |\\n\\n## Table 2.\\nComparison with different backbones under separate and joint training.\\n\\n| Method                        | AP 1x | AP 2x | AP 3x |\\n|-------------------------------|-------|-------|-------|\\n| Instance Sampling             | 15.5  | 2.3   | 0.1   |\\n| Global Sampling               | 34.5  | 19.2  | 4.8   |\\n| Dataset-aware Sampling        | 43.0  | 22.2  | 4.8   |\\n\\n## Table 3.\\nEffectiveness of query adaption under separate and joint training.\\n\\n| Query num | Separate Training | Joint Training |\\n|-----------|-------------------|----------------|\\n| 100       | 41.5 20.2 3.8     | 42.3 20.3 5.0  |\\n| 300       | 43.0 22.2 4.8     | 45.3 23.2 5.7  |\\n\\n## Table 4.\\nAblation study on the number of adapted query under separate and joint training.\\n\\nAll models are trained with the R-50 backbone.\\n\\n## Table 5.\\nGeneralization capability of cross dataset evaluation.\\n\\n| Method                        | COCO | COCO O365 | VG | O365 + VG |\\n|-------------------------------|------|-----------|----|----------|\\n| R-50 \\\\[12\\\\]                  | 34.6 | 45.3      | 10.2| 15.7     |\\n| Swin-T \\\\[22\\\\]                | 22.6 | 32.0      | 12.9| 17.1     |\\n| RX-101-DCN \\\\[43\\\\]            | 36.9 | 49.4      | 16.0| 17.7     |\\n\\n## Table 6.\\nBaseline evolution under separate and joint training.\\n\\n| Method                        | Separate Joint |\\n|-------------------------------|----------------|\\n| Sparse-RCNN \\\\[37\\\\] + Merge \\\\[15\\\\] | 40.6 40.4 17.4 4.3 |\\n| + Cat-aligned Embed           | 40.1 0.5 43.1 2.7 |\\n| + D-Q-adapt Decoder           | 41.1 1.0 44.0 0.9 |\\n| + D-Q-adapt RPN               | 43.0 1.9 45.6 1.6 |\\n\\nWe then evaluate the effectiveness of our proposed query adaption method. To compare, we design two baselines: \u201cInstance Sampling\u201d means only considering categories belonging to each training image. This limits the usage of category embedding to image level, where categories from different images are not interacted with each other; \u201cGlobal Sampling\u201d means we consider the categories from all datasets. This allows all categories in different datasets to interact with each other. As shown in Table 3, query adaption is the key to effective multi-dataset training. With \u201cGlobal Sampling\u201d, separate and joint training drops significantly due to the negative interfering between different datasets. Finally, our dataset-aware query can support \u201cDataset-aware Sampling\u201d, which samples category names according to the source dataset of each training image specifically to improve performance.\\n\\nIn addition, we also conduct an ablation on utilizing different numbers of adapted queries. As shown in Table 4, it is obvious that more adapted queries further enlarge the performance gains from joint training compared to separate training. To further verify the generalization capability of our adapted queries, we conduct the cross dataset evaluation by evaluating on five datasets out of the training set.\"}"}
{"id": "CVPR-2023-530", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Method | Backbone | AP | AP | AP | AP | AP | AP\\n--- | --- | --- | --- | --- | --- | --- | ---\\nDeformable-DETR [30] | R-50 | 37.2 | 55.5 | 40.5 | 21.1 | 40.7 | 50.5\\nDynamic-DETR [3] | R-50 | 42.9 | 61.0 | 46.3 | 24.6 | 44.9 | 54.4\\nSparse-RCNN [37] | R-50 | 39.5 | 57.7 | 42.8 | 21.8 | 42.3 | 54.4\\nOurs (separate training) | R-50 | 43.0 | 60.8 | 47.1 | 27.5 | 45.4 | 55.6\\nOurs (joint training) | R-50 | 45.3 | 63.5 | 49.7 | 30.1 | 48.3 | 57.3\\n\\nTable 7. Comparison with SoTA query-based models under standard 1\u00d7 setup using the ResNet-50 backbone on COCO val.\\n\\nTable 8. Ablation on UODB, where Avg means the average score of 11 datasets. Single means the baseline trained on each single dataset; we report the UniDA [40] of best Avg score in the original paper. Ours means our method jointly trained on UODB. All models take ResNet-50 as backbone.\\n\\n4.4. Effectiveness of Detector\\nWe also evaluate our query adapted detector separately to compare it with popular detectors. To conduct a fair comparison and ensure reproduction, we first train our method with a ResNet-50 on COCO data only using a standard 1\u00d7 schedule without augmentation and multi-dataset training. We compare our result with state-of-the-art query-based methods reported in a similar setting, such as [3, 37, 51]. Shown in Table 7, our method achieves state-of-the-art performance at 43.0 AP. In addition, our method really shines when trained under multi-dataset joint training and outperforms previous methods by clear margins.\\n\\n4.5. Effectiveness of Each Component\\nTo further clarify the baseline evolution from Sparse-RCNN to our model, we study the effectiveness of each component under separate and joint training. The two main differences can be summarized as follows: 1) Category-aligned Embedding replaces the traditional classifier through visual-word alignment and 2) Dataset-aware Query is incorporated for both the decoder and the extra RPN. As shown in Tab 6 Such a combination offers a 2.4 AP improvement on COCO under separate training and best boost the detector under joint training.\\n\\n4.6. Effectiveness on Extreme Varied Datasets\\nWe further evaluate on UODB [40], a combination of COCO [21], KITTI [7], WiderFace [46], VOC [6], LISA [27], DOTA [42], Watercolor [13], Clipart [13], Comic [13], Kitchen [8], and DeepLesion [44]. These datasets cover wide variations in category, camera view, image style, etc. Following UODB [40], we train our model with ResNet-50 as backbone and report the AP for each dataset separately. As shown in Table 8, our Detection Hub can outperform UDA [40] by a large margin on avg score. It provides a convincing results, highlighting the effectiveness on the combinations of extremely varied datasets.\\n\\n4.7. Comparison with SoTA\\nFinally, we compare our method with state-of-the-art object detectors using a large backbone. We report our best performance with Swin Large jointly trained on COCO, Object365, and Visual-Genome datasets. Unlike previous methods that require separate pre-training and fine-tuning steps, our method directly reports the performance after joint training without domain-specific fine-tuning. As shown in Table 9, we compare with recent methods that leverage multiple datasets. Compared to previous best results, our method archives new state-of-the-art results on both Object365 and Visual-Genome. On COCO, our method is competitive with recent work HTC++ [22]. However, HTC++ is first pre-trained on Object365 and COCO, then finetuned on COCO for a better result. Besides, unlike HTC++ combining the segmentation task and detection task, our method achieves this performance without the help of instance segmentation information. Meanwhile, our method demonstrates significant improvements on both COCO and Object365 over concurrent work [50]. In conclusion, thanks to our effective query adaption under multi-dataset joint training, our method can take advantage of different data to further advance the state-of-the-art results.\"}"}
{"id": "CVPR-2023-530", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method        | Training Data                  | Data Size | AP  | AP  | AP 50 | HTC++ [22] | VinVL [47] | UniDet [50] | Ours  |\\n|---------------|--------------------------------|-----------|-----|-----|-------|------------|------------|-------------|-------|\\n|               | COCO                          | 0.7M      | 57.7| \u2013   | \u2013     | \u2013          | \u2013          | 52.9        | 57.0  |\\n|               | O365                          |           |     |     |       |            |            | 33.7        | 37.2  |\\n|               | VG                            |           |     |     |       |            |            | 8.9         | 14.4  |\\n|               | COCO+O365                     | 2.6M      |     |     |       |            |            |             |       |\\n|               | COCO+O365+VG                  |           |     |     |       |            |            |             |       |\\n|               | COCO+O365+OID+Mapillary       | 2.5M      |     |     |       |            |            |             |       |\\n\\nTable 9. Compared to SoTA on all three datasets. \"-\" indicates the numbers are not available for us.\\n\\nFigure 4. Visualization of different predictions results from our method. The first odd rows show the predicted category names. The even rows show which dataset contributes to our final prediction.\\n\\n4.8. Visualization\\n\\nFinally, we visualize the predictions in Fig 4, where the odd rows show the predicted category names and the even show which dataset contributes to the prediction. Our predictions effectively combine the taxonomies from multiple datasets and yield better predictions. This well justifies our motivation for proposing a universal object detector.\\n\\n5. Conclusion\\n\\nUnifying multiple object detection datasets with a single object detector is a non-trivial problem. We propose \u201cDetection Hub\u201d to address the inherent taxonomy differences and annotation inconsistency challenges. It can enjoy the synergy of multiple datasets and achieve substantial improvements over the independently trained detector baseline. Meanwhile, we also find that the performance on the dataset with a large vocabulary may be constrained by the maximum length of language embedding. In the future, we will try to expand our work to open-world object detection by combining more datasets from different domains to cover a wide variety of concepts.\\n\\nAcknowledgement\\n\\nThis project was supported by National Key R&D Program of China (No. 2021ZD0112805).\"}"}
