{"id": "CVPR-2022-151", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Automatic Synthesis of Diverse Weak Supervision Sources for Behavior Analysis\\n\\nAlbert Tseng\\nNuro\\nJennifer J. Sun\\nCaltech\\nYisong Yue\\nCaltech, Argo AI\\n\\nAbstract\\n\\nObtaining annotations for large training sets is expensive, especially in settings where domain knowledge is required, such as behavior analysis. Weak supervision has been studied to reduce annotation costs by using weak labels from task-specific labeling functions (LFs) to augment ground truth labels. However, domain experts still need to hand-craft different LFs for different tasks, limiting scalability. To reduce expert effort, we present AutoSWAP: a framework for automatically synthesizing data-efficient task-level LFs. The key to our approach is to efficiently represent expert knowledge in a reusable domain-specific language and more general domain-level LFs, with which we use state-of-the-art program synthesis techniques and a small labeled dataset to generate task-level LFs. Additionally, we propose a novel structural diversity cost that allows for efficient synthesis of diverse sets of LFs, further improving AutoSWAP\u2019s performance. We evaluate AutoSWAP in three behavior analysis domains and demonstrate that AutoSWAP outperforms existing approaches using only a fraction of the data. Our results suggest that AutoSWAP is an effective way to automatically generate LFs that can significantly reduce expert effort for behavior analysis.\\n\\n1. Introduction\\n\\nIn recent years, machine learning has enabled the study of large-scale datasets in many behavior analysis domains, such as neuroscience [24, 27], sports analytics [30, 37], and motion forecasting [7]. However, obtaining labeled data to train models can be difficult and costly, especially when domain expertise is required for annotation, such as for many behavior analysis tasks [24]. One way to reduce annotation cost is through weak supervision, which uses noisy, task-level heuristic \u201clabeling functions\u201d (LFs) to weakly label data. LFs for a specific task (task-level LFs) are supplied by domain experts, and are applied to obtain a set of weak labels. Weakly labeled data can then be used in downstream settings, such as active learning [4] and self-training [17].\\n\\nWhile weak supervision has worked well in a wide range of settings [4, 10, 23], it has not been well-explored for behavior analysis tasks. For one, the requirement that LFs must provide labels and not, for example, features prevents more general domain knowledge from being used [22] (e.g. the behavioral features in [14, 24]). Furthermore, new LFs must be hand-crafted by domain experts for new tasks (such as new behaviors to study), limiting the scalability of manual weak supervision [33]. To address these challenges, we study efficient domain knowledge representations and develop automated weak supervision methods towards reducing annotation bottlenecks in behavior analysis settings.\\n\\nOur Approach.\\n\\nWe propose AutoSWAP (Automatic Synthesized Weak Supervision), a data-efficient framework for automatically generating task-level LFs using a novel diverse program synthesis formulation. As depicted in Figure 1, experts provide a domain-specific language (DSL) and domain-level LFs (LFs specific to a domain of tasks) for a given domain, such as mouse behaviors or vehicle motion planning. For each task to be studied in that domain, experts provide a small labeled dataset to specify the task, and AutoSWAP returns a set of structurally diverse task-level LFs that can be used in weakly supervised frameworks. The domain-level LFs (Figure 2) provide fine-grained domain knowledge while the task-level LFs provide low-data efficiency.\\n\\nFigure 1. We present AutoSWAP, a framework for automatically synthesizing diverse sets of task-level labeling functions (LFs) with a small labeled dataset and domain knowledge encoded in domain-level LFs and a DSL. AutoSWAP significantly reduces labeling effort by automating LF generation.\"}"}
{"id": "CVPR-2022-151", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"grained, label-space agnostic \\\"atomic instructions,\\\" while the DSL contains abstract structural domain knowledge for composing the more general domain-level LFs into task-level LFs (Figure 3). The novel diversity cost enables AutoSWAP to generate structurally diverse LFs, which we and others empirically show outperform structurally homogeneous LFs in downstream tasks [33].\\n\\nTo the best of our knowledge, we are the first to demonstrate the effectiveness of program synthesis for automated LF generation. Existing works for generating LFs include iteratively selecting LFs by repeatedly querying experts for feedback [5] and training exponentially many simple heuristics models [33], which have limitations in scalability and tractability. In contrast, our approach represents domain knowledge in a DSL and domain-level LFs, which can then be used to automatically synthesize LFs for arbitrary tasks in a domain with our diverse program synthesizer.\\n\\nWe evaluate our approach in three behavior analysis domains with both sequential and nonsequential data: mouse [27], fly [14], and basketball player [36] behaviors. In these domains, data collection is expensive and new tasks frequently emerge, highlighting the importance of scalability. The datasets we use are based on agent trajectories, which provide low-dimensional inputs for easily creating domain-level LFs. We show that with existing expert defined domain-level LFs from [14, 24] and a simple DSL, AutoSWAP is capable of synthesizing high quality LFs with very little labeled data. These LFs outperform LFs from existing automatic weak supervision methods [33] and offer a data efficient approach to reducing domain expert effort.\\n\\nTo summarize, our contributions are:\\n\u2022 We propose AutoSWAP, which combines program synthesis with weak supervision to scalably and efficiently generate labeling functions.\\n\u2022 We propose a novel program-structural diversity cost that enables AutoSWAP to directly synthesize diverse sets of labeling functions, which we empirically show are more data efficient than purely optimal sets.\\n\u2022 We evaluate AutoSWAP in multiple behavior analysis domains and downstream tasks, and show that AutoSWAP is capable of significantly improving data efficiency and reducing expert cost.\\n\\nOur implementation of AutoSWAP can be found at https://github.com/autoswap/autoswap_cvpr_2022.\\n\\n2. Related Work\\n\\nBehavior Analysis\\nIn many domains, such as behavioral neuroscience [19, 24], sports analytics [36, 37], and traffic modeling [9], agent pose and location trajectory data is used for behavior analysis. This data is usually extracted from recorded videos using detectors and pose estimators [14, 24]; for example, we use trajectories from [24], [14], and StatsPerform for our mouse, fly, and basketball datasets, respectively.\\n\\nTo accurately analyze this data for complex behaviors, frame-level behavior labels from domain experts are usually needed. However, annotating large datasets is time-consuming and monotonous [1], motivating methods for label-efficient modeling. For example, self-supervised learning [28] and unsupervised behavior discovery methods [3, 6, 19] aim to learn efficient behavior representations and discover new behaviors, respectively. Our work is complementary to these methods in that this is not a comparison between weak supervision and self-supervision. Rather, we evaluate the merits of our synthesized LFs in the context of weak supervision for learning expert-defined behaviors.\\n\\nWeak Supervision\\nWeak supervision with LFs was introduced in the context of data programming [23]. Since then, LFs have been applied in a variety of settings, including for active learning [4, 20] and self-training [17] tasks. Our work is complementary to these works in that we automatically learn LFs that can be used as inputs to existing weakly supervised frameworks. We note that we are not the first to propose learning LFs from a small amount of training data. For example, IWS iteratively proposes rules and queries domain experts in a large-scale feedback loop [5]. More similar to our work, SNUBA [33] trains heuristics models, but does so without domain knowledge and has run-time exponential in the number of features. To the best of our knowledge, we are the first to apply program synthesis to this problem, and our framework outperforms existing model-based methods for learning LFs.\\n\\nProgram Synthesis\\nTraditionally, programming by example has been used to synthesize programs from a DSL that respect hard constraints on input/output examples [15, 26]. In recent years, a growing number of works have studied synthesizing programs with soft constraints, such as minimizing a loss function [13, 21, 25, 31]. This relaxed form of program synthesis has been applied to a number of different domains including web information extraction [8], image structure analysis [12], and learning interpretable agent policies [34]. Of these works, algorithms that learn differentiable programs, such as [25], have shown great promise in being able to efficiently and simultaneously optimize program architectures and parameters. Here, we use concepts from differentiable program synthesis algorithms to synthesize diverse sets of LFs.\\n\\n3. Methods\\nWe introduce AutoSWAP, a framework for automatically generating diverse sets of task-level LFs. In our framework, domain experts provide a set of domain-level LFs and...\"}"}
{"id": "CVPR-2022-151", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"```python\\ndef is_attacking(fly, tgt):\\n    f2t_angle = atan((tgt.y - fly.y) / (tgt.x - fly.x))\\n    rel_angle = abs(fly.abs_angle - f2t_angle)\\n    return fly.speed > 2 and rel_angle < 0.1\\n```\\n\\n```python\\ndef wing_ratio(fly, tgt):\\n    return quantize(fly.wing_x / fly.wing_y, 4)\\n```\\n\\n```python\\ndef relative_speed(fly, tgt):\\n    return abs(fly.speed) / abs(tgt.speed)\\n```\\n\\nFigure 2. Domain experts provide domain-level labeling functions, such as the ones above for the fly domain. Some domain-level LFs ($\\\\lambda_1$, $\\\\lambda_2$) label for specific tasks (and would be considered task-level LFs on their own), while others ($\\\\lambda_3$) return features.\\n\\n### 3.1. Background\\n\\n**Domain-level Labeling Functions.** In weak supervision, users provide a set of task-level hand-crafted heuristics called labeling functions (LFs). LFs can be noisy and abstain from labeling, but LFs must output in downstream task's label space $Y$. We relax this requirement in AutoSWAP by allowing domain experts to provide domain-level LFs (Figure 2). These LFs do not have to output in $Y$, which reduces LF creation overhead and allows for more expressive LFs. This also allows us to reuse LFs across multiple tasks within the same domain, aiding scalability.\\n\\n**Domain Specific Languages.** Domain specific languages (DSLs) define the allowable submodules and structures in synthesized programs, and are a key component of program synthesis algorithms. Many recent works have adopted purely functional DSLs [25], where DSL items are functions that output to the input space of other DSL items or the final output space. In AutoSWAP, domain experts provide a purely functional DSL with program structures that may be useful in generated LFs. We show empirically that even using a very simple DSL in AutoSWAP can result in significant reductions in expert effort.\\n\\n### 3.2. AutoSWAP\\n\\n**Synthesizing Diverse Sets of Programs.** Diverse sets of LFs have been shown to improve data efficiency relative to purely optimal sets in downstream applications of weak supervision [33]. This is partly due to diverse sets having improved label coverage (fewer data points where all LFs abstain) [33], and from having more learning signals for the downstream model [29]. The program synthesizer in Section 3.1 can be run repeatedly to obtain a set of purely optimal LFs, but there is no guarantee that the set will be diverse. Here, we introduce a structural diversity cost and admissible heuristic that allows for direct synthesis of diverse sets of programs using informed search algorithms. We empirically show that using the diversity cost improves performance, corroborating [33]'s observations.\\n\\nConsider a complete program $P$, which is a composition of variables in $D$. By construction of $G$, we can convert $P$ to a tree $T_P$ where each node is a variable in $P$ and a node's children are its input variables (Figure 3). Then, given a set of complete programs $P$ and a complete program $P'$, we define the structural cost $C_{P,P'}$ of $P$ relative to $P'$ as:\\n\\n$$C_{P,P'} = q \\\\frac{1}{\\\\|P\\\\|} \\\\sum_{P' \\\\in P} ZSS(T_P, T_{P'})$$\\n\\nwhere $q: \\\\mathbb{R} \\\\rightarrow \\\\mathbb{R}$ is a user defined monotonically increasing function and $ZSS$ is the Zhang-Shasha tree edit distance.\"}"}
{"id": "CVPR-2022-151", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. A complete program and its tree representation. Each \u2018? \u2019 represents one child node function. The depicted program is an actual AutoSW AP LF for the \u201clunge vs. no behavior\u201d in the Fly domain. The program can be interpreted as \u201cIf the linear speed between the flies is small, classify the angular domain-level LFs of the flies. Otherwise, classify the product of transformations of the linear speed and positional domain-level LFs.\u201d\\n\\nNote the parameters (red) are not included in the structural diversity cost. (TED) [38]. Essentially, programs with a higher average TED to the elements of \\\\( P \\\\) incur a lower diversity cost.\\n\\nSince this structural cost is not defined for incomplete programs or neural completions, \\\\( C_P, P \\\\) cannot be used in informed search algorithms. However, the following admissible heuristic \\\\( H_P, P \\\\) for incomplete programs \\\\( P_I \\\\) allows us to create a set of diverse programs by iteratively synthesizing programs and adding them to \\\\( P \\\\).\\n\\nLemma 3.1. Let \\\\( P_I \\\\) be an incomplete program and \\\\( T_P I \\\\) be the tree of its known variables. \\\\( T_P I \\\\) is guaranteed to exist by construction of \\\\( G \\\\). Define \\\\( H_P, P \\\\) as:\\n\\n\\\\[\\nU_P I, P' = m - \\\\|P_I\\\\| + ZSS(T_P I, T_P'),\\n\\\\]\\n\\n\\\\[\\nH_P, P = q\\\\|P\\\\|X_P' \\\\in P U_P I, P',\\n\\\\]\\n\\nwhere \\\\( \\\\|P_I\\\\| \\\\) is the number of known variables in \\\\( P_I \\\\). \\\\( H_P, P \\\\) is an admissible heuristic for the CTG from \\\\( P_I \\\\) in \\\\( G \\\\).\\n\\nProof. Consider \\\\( U_P I, P' \\\\).\\n\\n\\\\[\\nm - \\\\|P_I\\\\|\\n\\\\]\\n\\nis an upper bound on the TED between \\\\( T_P I \\\\) and the tree of any complete descendant \\\\( P^* \\\\) of \\\\( P_I \\\\) in \\\\( G \\\\). From the triangle inequality,\\n\\n\\\\[\\nU_P I, P' = m - \\\\|P_I\\\\| + ZSS(T_P I, T_P') \\\\geq ZSS(T_P I, T_P^*) + ZSS(T_P I, T_P')\\n\\\\]\\n\\n\\\\[\\n\\\\geq ZSS(T_P^*, T_P').\\n\\\\]\\n\\nThen, as TEDs are nonnegative, \\\\( m \\\\geq \\\\|P_I\\\\| \\\\), and \\\\( q \\\\) is nondecreasing,\\n\\n\\\\[\\nH_P, P \\\\leq C_P^*, P.\\n\\\\]\\n\\nThus, \\\\( H \\\\) a admissible heuristic for the structural CTG from \\\\( P_I \\\\).\\n\\nAutoSW AP Framework. AutoSW AP uses program synthesis to automate significant parts of the weak supervision pipeline and reduce domain expert effort. Domain experts provide a set of domain-level LFs \\\\( \\\\Lambda_m = \\\\{\\\\lambda_i: X \\\\rightarrow Y\\\\} \\\\), a purely functional DSL \\\\( D \\\\), and a small labeled dataset \\\\((X, Y) \\\\in (X, Y)\\\\) to specify tasks within the domain. In order to use \\\\( \\\\Lambda_m \\\\) when synthesizing programs with \\\\( D \\\\), all \\\\( \\\\lambda_i \\\\) must be added to \\\\( D \\\\). This can be done either by implementing each \\\\( \\\\lambda_i \\\\) with operations from \\\\( D \\\\), or precomputing and\\n\\nAlgorithm 1: AutoSW AP.\\n\\nInput: \\\\( \\\\Lambda_m, D \\\\) labeled dataset \\\\( D_L \\\\), # LFs \\\\( n \\\\)\\n\\nOutput: task-level LFs \\\\( \\\\Lambda_D \\\\leftarrow Combine \\\\Lambda_M \\\\) and \\\\( D \\\\)\\n\\n\\\\( P \\\\leftarrow \\\\emptyset \\\\)\\n\\nwhile \\\\( \\\\|P\\\\| \\\\leq n \\\\) do\\n\\nSynthesize \\\\( P \\\\) with \\\\( D \\\\), \\\\( D_L \\\\), \\\\( P \\\\)\\n\\n\\\\( P \\\\leftarrow P \\\\cup \\\\{P\\\\} \\\\)\\n\\nend\\n\\n\\\\( \\\\Lambda \\\\leftarrow P \\\\), return \\\\( \\\\Lambda \\\\).\\n\\nAlgorithm 2: AutoSW AP for Active Learning.\\n\\nInput: \\\\( \\\\Lambda_m, D \\\\) \\\\( n \\\\) unlabeled \\\\( X_U \\\\), \\\\( A \\\\).\\n\\nSort \\\\( A \\\\) in increasing order.\\n\\nRandomly select \\\\( A_1 \\\\) points \\\\( X_L \\\\) from \\\\( X_U \\\\).\\n\\n\\\\( X_U \\\\leftarrow X_U \\\\setminus X_L \\\\)\\n\\n\\\\( Y_L \\\\leftarrow Obtain \\\\) labels for \\\\( X_L \\\\).\\n\\nfor \\\\( i = 1, \\\\ldots, \\\\|A\\\\| - 1 \\\\) do\\n\\n\\\\( \\\\Lambda_i \\\\leftarrow \\\\text{AutoSW AP}(\\\\Lambda_m \\\\), \\\\( D \\\\), \\\\( (X_L, Y_L) \\\\), \\\\( n \\\\)).\\n\\n\\\\( X' \\\\leftarrow X_L \\\\Lambda_i(X_L) \\\\)\\n\\nTrain downstream classifier \\\\( C_i \\\\) with \\\\( (X'_L, Y_L) \\\\).\\n\\nSelect \\\\( A_{i+1} \\\\) points \\\\( X'_L \\\\) using max entropy uncertainty sampling.\\n\\n\\\\( X_U \\\\leftarrow X_U \\\\setminus X'_L \\\\)\\n\\n\\\\( X_L \\\\leftarrow X_L \\\\cup X'_L \\\\)\\n\\n\\\\( Y_L \\\\leftarrow Y_L \\\\cup \\\\{ \\\\text{Obtain labels for } X'_L \\\\} \\\\)\\n\\nend\\n\\nselecting \\\\( \\\\Lambda_m \\\\) \\\\((X)\\\\) as input features in \\\\( D \\\\); we do the latter in our experiments. With \\\\( D \\\\), AutoSW AP runs the diverse program synthesis algorithm \\\\( n \\\\) times to generate a set \\\\( \\\\Lambda \\\\) of \\\\( n \\\\) LFs. \\\\( \\\\Lambda \\\\) can then be used in downstream tasks, such as in weak supervision label models to generate weak labels. See Algorithm 1 for a detailed description of AutoSW AP.\\n\\n3.3. Downstream Tasks\\n\\nWe describe two downstream tasks in which weak labels can be used. These examples, which our experiments are based on, are just a subset of the many weakly supervised learning frameworks in existence such as ASTRA [17].\\n\\nActive Learning. Active learning is a paradigm where the learning algorithm can selectively query for new data to be labeled. Here, we use labels from task-level LFs as additional features for a downstream classifier. The downstream classifier's predictions are used to select data for labeling. To evaluate generated LFs in active learning settings, we consider the performance of downstream classifiers at multiple data amounts. Given a sorted list \\\\( A \\\\) of data amounts, at each amount we generate new LFs, train a downstream classifier, and select data points for labeling to form the next batch. An exact description of our active learning setup for AutoSW AP can be found in Algorithm 2.\"}"}
{"id": "CVPR-2022-151", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] David J Anderson and Pietro Perona. Toward a science of computational ethology. *Neuron*, 84(1):18\u201331, 2014.\\n\\n[2] Jenna Wiens Armand McQueen and John Guttag. Automatically recognizing on-ball screens. In *MIT Sloan Sports Analytics Conference*, 2014.\\n\\n[3] Gordon J Berman, Daniel M Choi, William Bialek, and Joshua W Shaevitz. Mapping the stereotyped behaviour of freely moving fruit flies. *Journal of The Royal Society Interface*, 11(99):20140672, 2014.\\n\\n[4] Samantha Biegel, Rafah El-Khatib, Luiz Otavio Vilas Boas Oliveira, Max Baak, and Nanne Aben. Active weasul: Improving weak supervision with active learning. *arXiv preprint arXiv:2104.14847*, 2021.\\n\\n[5] Benedikt Boecking, Willie Neiswanger, Eric Xing, and Artur Dubrawski. Interactive weak supervision: Learning useful heuristics for data labeling. In *International Conference on Learning Representations*, 2021.\\n\\n[6] Adam J Calhoun, Jonathan W Pillow, and Mala Murthy. Unsupervised identification of the internal states that shape natural behavior. *Nature neuroscience*, 22(12):2040\u20132049, 2019.\\n\\n[7] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, Dea Wang, Peter Carr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3d tracking and forecasting with rich maps. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 8748\u20138757, 2019.\\n\\n[8] Qiaochu Chen, Aaron Lamoreaux, Xinyu Wang, Greg Durrett, Osbert Bastani, and Isil Dillig. Web question answering with neurosymbolic program synthesis. In *Proceedings of the 42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation*, pages 328\u2013343, 2021.\\n\\n[9] James Colyar and John Halkias. Us highway 101 dataset. *Federal Highway Administration (FHWA), Tech. Rep. FHWA-HRT-07-030*, 2007.\\n\\n[10] Jared A. Dunnmon, Alexander J. Ratner, Khaled Saab, Nishith Khandwala, Matthew Markert, Hersh Sagreiya, Roger Goldman, Christopher Lee-Messer, Matthew P. Lungren, Daniel L. Rubin, and Christopher R\u00e9. Cross-modal data programming enables rapid medical machine learning. *Patterns*, 1(2):100019, 2020.\\n\\n[11] Kevin Ellis, Lucas Morales, Mathias Sabl\u00e9-Meyer, Armando Solar-Lezama, and Josh Tenenbaum. Learning libraries of subroutines for neurally\u2013guided bayesian program induction. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 31. Curran Associates, Inc., 2018.\\n\\n[12] Kevin Ellis, Daniel Ritchie, Armando Solar-Lezama, and Josh Tenenbaum. Learning to infer graphics programs from hand-drawn images. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, *Advances in Neural Information Processing Systems*, volume 31. Curran Associates, Inc., 2018.\\n\\n[13] Kevin Ellis, Armando Solar-Lezama, and Josh Tenenbaum. Unsupervised learning by program synthesis. 2015.\\n\\n[14] Eyrun Eyjolfsdottir, Steve Branson, Xavier P Burgos-Artizzu, Eric D Hoopfer, Jonathan Schor, David J Anderson, and Pietro Perona. Detecting social actions of fruit flies. In *European Conference on Computer Vision*, pages 772\u2013787. Springer, 2014.\\n\\n[15] John K Feser, Swarat Chaudhuri, and Isil Dillig. Synthesizing data structure transformations from input-output examples. *ACM SIGPLAN Notices*, 50(6):229\u2013239, 2015.\\n\\n[16] Larry R. Harris. The heuristic search under conditions of error. *Artif. Intell.*, 5:217\u2013234, 1974.\\n\\n[17] Giannis Karamanolakis, Subhabrata (Subho) Mukherjee, Guoqing Zheng, and Ahmed H. Awadallah. Self-training with weak supervision. In *NAACL 2021*. NAACL 2021, May 2021.\\n\\n[18] David Lewis, Jason Catlett, W. Cohen, and Haym Hirsh. Heterogeneous uncertainty sampling for supervised learning. 1996.\\n\\n[19] Kevin Luxem, Falko Fuhrmann, Johannes K\u00fcrsch, Stefan Remy, and Pavol Bauer. Identifying behavioral structure from deep variational embeddings of animal motion. *bioRxiv*, 2020.\\n\\n[20] Mona Nashaat, Aindrila Ghosh, James Miller, Shaikh Quader, Chad Marston, and Jean-Francois Puget. Hybridization of active learning and data programming for labeling large industrial datasets. In *2018 IEEE International Conference on Big Data (Big Data)*, pages 46\u201355. IEEE, 2018.\\n\\n[21] Emilio Parisotto, Abdel-rahman Mohamed, Rishabh Singh, Lihong Li, Dengyong Zhou, and Pushmeet Kohli. Neuro-symbolic program synthesis. *arXiv preprint arXiv:1611.01855*, 2016.\\n\\n[22] Alexander Ratner, Braden Hancock, Jared Dunnmon, Frederic Sala, Shreyash Pandey, and Christopher R\u00e9. Training complex models with multi-task weak supervision. *Proceedings of the AAAI Conference on Artificial Intelligence*, 33(01):4763\u20134771, Jul. 2019.\\n\\n[23] Alexander J Ratner, Christopher M De Sa, Sen Wu, Daniel Selsam, and Christopher R\u00e9. Data programming: Creating large training sets, quickly. *Advances in neural information processing systems*, 29:3567\u20133575, 2016.\\n\\n[24] Cristina Segalin, Jalani Williams, Tomomi Karigo, May Hui, Moriel Zelikowsky, Jennifer J. Sun, Pietro Perona, David J. Anderson, and Ann Kennedy. The mouse action recognition system (mars): a software pipeline for automated analysis of social behaviors in mice. *bioRxiv* https://doi.org/10.1101/2020.07.26.222299, 2020.\\n\\n[25] Ameesh Shah, Eric Zhan, Jennifer J Sun, Abhinav Verma, Yisong Yue, and Swarat Chaudhuri. Learning differentiable programs with admissible neural heuristics. In *Neural Information Processing Systems*, 2020.\\n\\n[26] Armando Solar-Lezama, Liviu Tancau, Rastislav Bodik, Sanjit Seshia, and Vijay Saraswat. Combinatorial sketching for finite programs. In *Proceedings of the 12th international conference on Architectural support for programming languages and operating systems*, pages 404\u2013415, 2006.\"}"}
{"id": "CVPR-2022-151", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jennifer J. Sun, Tomomi Karigo, Dipam Chakraborty, Sharada Mohanty, Benjamin Wild, Quan Sun, Chen Chen, David Anderson, Pietro Perona, Yisong Yue, and Ann Kennedy. The multi-agent behavior dataset: Mouse dyadic social interactions. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.\\n\\nJennifer J Sun, Ann Kennedy, Eric Zhan, David J Anderson, Yisong Yue, and Pietro Perona. Task programming: Learning data efficient behavior representations. In Conference on Computer Vision and Pattern Recognition, 2021.\\n\\nTao Sun and Zhi-Hua Zhou. Structural diversity for decision tree ensemble learning. Frontiers of Computer Science, 12, 02 2018.\\n\\nKarl Tuyls, Shayegan Omidshafiei, Paul Muller, Zhe Wang, Jerome Connor, Daniel Hennes, Ian Graham, William Spearsman, Tim Waskett, Dafydd Steel, et al. Game plan: What AI can do for football, and what football can do for AI. Journal of Artificial Intelligence Research, 71:41\u201388, 2021.\\n\\nLazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, and Swarat Chaudhuri. Houdini: Lifelong learning as program synthesis. In Advances in neural information processing systems, 2018.\\n\\nLazar Valkov, Dipak Chaudhari, Akash Srivastava, Charles Sutton, and Swarat Chaudhuri. Houdini: Lifelong learning as program synthesis. In S. Bengio, H. Wallach, H. Larochelle, K. Grauman, N. Cesa-Bianchi, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 31. Curran Associates, Inc., 2018.\\n\\nParoma Varma and Christopher R\u00e9. Snuba: Automating weak supervision to label training data. In Proceedings of the VLDB Endowment. International Conference on Very Large Databases, volume 12, page 223. NIH Public Access, 2018.\\n\\nAbhinav Verma, Vijayaraghavan Murali, Rishabh Singh, Pushmeet Kohli, and Swarat Chaudhuri. Programmatically interpretable reinforcement learning. In International Conference on Machine Learning, pages 5045\u20135054. PMLR, 2018.\\n\\nQizhe Xie, Minh-Thang Luong, Eduard Hovy, and Quoc V. Le. Self-training with noisy student improves ImageNet classification, 2020.\\n\\nYisong Yue, Patrick Lucey, Peter Carr, Alina Bialkowski, and Iain Matthews. Learning fine-grained spatial models for dynamic sports play prediction. In 2014 IEEE international conference on data mining, pages 670\u2013679. IEEE, 2014.\\n\\nEric Zhan, Stephan Zheng, Yisong Yue, Long Sha, and Patrick Lucey. Generating multi-agent trajectories using programmatic weak supervision. In International Conference on Learning Representations, 2019.\\n\\nKaizhong Zhang and Dennis Shasha. Simple fast algorithms for the editing distance between trees and related problems. SIAM J. Comput., 18:1245\u20131262, 12 1989.\"}"}
{"id": "CVPR-2022-151", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 3: AutoSW AP for Weak Supervision.\\n\\nInput: \\\\( \\\\Lambda \\\\), \\\\( D \\\\), \\\\( n \\\\), Labeled (\\\\( X_L \\\\), \\\\( Y_L \\\\)), Unlabeled \\\\( X_U \\\\), \\\\( A \\\\).\\n\\n\\\\( \\\\Lambda \\\\leftarrow \\\\text{AutoSW AP}(\\\\Lambda, D, (X_L, Y_L), n) \\\\).\\n\\n\\\\( \\\\Lambda \\\\leftarrow \\\\text{Abstain}(\\\\Lambda) \\\\).\\n\\nSort \\\\( A \\\\) in increasing order.\\n\\nfor \\\\( i = 1, \\\\ldots, \\\\| A \\\\| \\\\) do\\n\\nRandomly select \\\\( A_i \\\\) points \\\\( X_P \\\\) from \\\\( X_U \\\\).\\n\\n\\\\( X'_L \\\\leftarrow X_L \\\\cup X_P \\\\).\\n\\n\\\\( Y'_L \\\\leftarrow Y_L \\\\cup \\\\Lambda(X_P) \\\\).\\n\\nTrain downstream classifier \\\\( C_i \\\\) with \\\\( (X'_L, Y'_L) \\\\).\\n\\nend for\\n\\nEvalually depend on a generative label model weakly label un-\\n\\nlabeled samples. Using no ground truth labels, the genera-\\n\\ntive model produces probabilistic estimates (\\\"weak labels\\\")\\n\\nfor the true labels \\\\( Y_U \\\\) of an unlabeled set \\\\( X_U \\\\) by modeling\\n\\nthe LF outputs \\\\( \\\\Lambda(X_U) \\\\). Weakly labeled data can then be\\n\\nused to augment labeled datasets in downstream tasks.\\n\\nTo evaluate AutoSW AP in weak supervision settings, we\\n\\nstart with a small labeled dataset \\\\( D_L \\\\) and a list of unlabeled\\n\\ndata amounts \\\\( A \\\\). LFs are generated using the small labeled\\n\\ndataset and abstain using the method in [33]. Then, weak\\n\\nlabels are generated from these LFs for all unlabeled data\\n\\nusing the generative model. For each data amount \\\\( A_i \\\\in A \\\\),\\n\\na random set \\\\( D_{PL} \\\\) of \\\\( A_i \\\\) weakly labeled data points is se-\\n\\nlected and the performance of a downstream classifier is\\n\\nmeasured using the training set \\\\( D_L \\\\cup D_{PL} \\\\). An exact de-\\n\\nscription of our weak supervision setup is in Algorithm 3.\\n\\n4. Experiments\\n\\nWe evaluate AutoSW AP in multiple real world be-\\n\\nhavior analysis domains (Section 4.1), and show that our\\n\\nframework outperforms existing LF generation methods in\\n\\nweak supervision and active learning settings (Section 5.1).\\n\\nSince researchers often study multiple behaviors in a do-\\n\\nmain [14, 24], we consider each behavior its own task.\\n\\n4.1. Datasets\\n\\nWe use datasets from behavioral neuroscience (mouse\\n\\nand fly behaviors) as well as sports analytics (basketball\\n\\nplayer trajectories). These datasets include rare behav-\\n\\niors, multi-behavior tasks, and sequential data, making them\\n\\ngood representations of real-world behavior analysis tasks.\\n\\nEach dataset contains a train, validation, and test split; the\\n\\nvalidation split is only used for model checkpoint selection.\\n\\nFly vs. Fly (Fly). The fly dataset [14] contains frame-\\n\\nlevel annotations of videos of interactions between two fruit\\n\\nflies. Our train, validation, and test sets contain 552k, 20k,\\n\\nand 166k frames. We use fly trajectories tracked by Fly-\\n\\nTracker [14] and evaluate on 6 behaviors: lunge, wing\\n\\nthreat, tussle, wing extension, circle, copulation. This is\\n\\na multi-label dataset and we report the mean Average Pre-\\n\\ncision (mAP) over binary classification tasks for each be-\\n\\nhavior. All behaviors except for copulation are rare; lunge,\\n\\nwing threat, and tussle occur in \\\\(< 5\\\\%\\\\) of frames, and wing\\n\\nextension and circle occur in \\\\(< 1\\\\%\\\\) of frames. The domain-\\n\\nlevel LFs for this dataset are based on features from [14].\\n\\nCalMS21 (Mouse). The CalMS21 dataset [27] consists\\n\\nof frame-level pose and behavior annotations from videos\\n\\nof interactions between pairs of mice. We use data from\\n\\nTask 1 (532k train, 20k validation, 119k test) and evaluate\\n\\non a set of 3 behaviors: attack, investigation, and mount.\\n\\nThese behaviors are mutually exclusive and we report the\\n\\nmAP over these classes. We use a subset of the features in\\n\\n[24] as domain-level LFs for this dataset.\\n\\nBasketball. The Basketball dataset, also used in [25,\\n\\n36, 37], contains sequences of basketball player trajectories\\n\\nfrom Stats Perform (18k train, 1k validation, 2.7k test). La-\\n\\nbels for which offense player (5 total) had the ball for the\\n\\nmajority of the sequence were extracted with [2]. We per-\\n\\nform sequential classification in downstream tasks, and re-\\n\\nport the mAP over each offense player vs. the other 4. Our\\n\\ndomain-level LFs include player acceleration, velocity, and\\n\\nposition among others. We exclude information about the\\n\\nball position in the domain-level LFs and data features to\\n\\nfocus on analyzing player behaviors.\\n\\n4.2. Baselines\\n\\nWe compare AutoSW AP to two main baselines: student\\n\\nnetworks from student-teacher training and decision trees\\n\\nfrom SNUBA [33]. We show that AutoSW AP outperforms\\n\\nboth in data efficiency, requiring a fraction of the data to\\n\\nachieve or exceed performance parity. For both baselines,\\n\\ndomain-level LFs are incorporated as input features to eval-\\n\\nuate the effectiveness of AutoSW AP and not the domain-\\n\\nlevel LFs themselves. We do not compare against IWS [5],\\n\\nas IWS is a human-in-the-loop LF generation system. We\\n\\nalso do not compare against ASTRA [17], as ASTRA is a\\n\\nweak supervision framework for using\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n"}
{"id": "CVPR-2022-151", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"on a diversity and performance metric, where $k$ is the feature dimension of $X$. Clearly, this is intractable for large $k$, which is often the case for behavior analysis tasks. Furthermore, SNUBA does not use domain knowledge, instead relying on the complete set of decision trees for data efficiency. In relation to SNUBA, AutoSW AP can be viewed as a scalable alternative to the synthesizer and pruner stages.\\n\\n4.3. Training Setup\\n\\nOur experimental setup consists of two stages: obtaining LFs, and evaluating generated LFs in downstream tasks. Our downstream tasks include active learning, where LFs are used to select data for labeling, and weak supervision, where LFs generate pseudolabels for unlabeled data points.\\n\\n4.3.1 Obtaining labeling functions\\n\\nSynthesized Programs via AutoSW AP. For each domain, we use a simple DSL that includes add, multiply, fold, and differentiable if-then-else (ITE) structures among others. We synthesize programs with our diverse program synthesizer and $A^*$ search. Our cost function is the sum of the $F_1$ cost from [25] and our diversity cost $C_P$. We set $q(x)$ to $x^2$ and $m$ to $\\\\log_2 \\\\| \\\\Lambda_m \\\\|$. Program parameters are trained with weighted cross entropy loss. More information about the exact DSL used is in the Supplementary Materials.\\n\\nStudent Networks. We use neural networks for frame classification tasks and LSTMs for scene classification tasks. To induce diversity in the learned student networks, we take inspiration from [35] and randomly set the size of each layer so the \u201cexpected\u201d student network is of similar capacity as the downstream classifier. All student networks are trained using weighted cross entropy loss.\\n\\nDecision Trees. We fit decision trees using Gini impurity as the split criteria. We limit the depth of decision trees to $\\\\log_2 k$, so the number of nodes is $O(k)$. We select diverse sets of decision trees by pruning a superset of trees based on coverage and performance, similar to how SNUBA does [33]. However, unlike SNUBA, we group our features when generating the superset, as training $2^{k-1}$ decision trees is intractable with our datasets.\\n\\n4.3.2 Downstream Tasks\\n\\nWe use 3 LFs in our main experiments. Experiments with more LFs (5, 7) are in the Supplementary Materials.\\n\\nActive Learning. As previously described, we evaluate the performance of AutoSW AP at multiple data amounts, selecting additional labeled data with active learning at each amount (Algorithm 2). We use max-entropy uncertainty sampling on downstream classifier outputs to select points for labeling [18]. We use $\\\\{1000, 2000, 3500, 5000, 7500, 12500, 25000, 50000\\\\}$ frames for the fly and mouse datasets and $\\\\{500, 1000, 1500, 2000, 3000, 4000, 5000\\\\}$ sequences for the basketball dataset.\\n\\nWeak Supervision. In our weak supervision experiments, we use factor graph model proposed in [22, 23].\\n\\n$$p_{\\\\theta}(Y_U, \\\\Lambda) = Z^{-1} \\\\theta \\\\exp -\\\\|X_U\\\\| \\\\sum_{i=1}^{\\\\theta} \\\\phi_{Acc}^i, j(\\\\Lambda(X_U, Y_U), Y_U)$$.\\n\\n(3)\\n\\nHere, LF accuracies are modeled by factor $\\\\phi_{Acc}^{i, j}(\\\\Lambda, Y_U) = 1\\\\{\\\\Lambda_j(X_U) = Y_U\\\\}$, and the proportion of data the LF labels is modeled by $\\\\phi_{Lab}^{i, j}(\\\\Lambda, Y_U) = 1\\\\{\\\\Lambda_j(X_U) \\\\neq \\\\emptyset\\\\}$. For the labeled dataset, we use 2000 frames for the fly and mouse datasets, and 500 sequences for the basketball dataset. Our unlabeled data amounts are set to $\\\\{1 \\\\times \\\\theta, 2 \\\\times \\\\theta, 3 \\\\times \\\\theta, 4 \\\\times \\\\theta, 5 \\\\times \\\\} \\\\theta$ the number of labeled points.\\n\\n5. Results\\n\\nWe compare the data efficiency of AutoSW AP against the baselines on our behavior analysis datasets. We do not run the decision tree (SNUBA) baseline on the Basketball dataset as it contains only sequential data.\\n\\n5.1. Data Efficiency Results\\n\\nActive Learning. AutoSW AP LFs are far more data efficient than baseline methods across all datasets, indicating that AutoSW AP is effective in reducing label cost in active learning settings (Figure 4). This difference is especially pronounced in the Mouse dataset, where AutoSW AP achieves parity with decision tree LFs with roughly $30 \\\\times$ less data. In the Fly dataset, AutoSW AP is consistently $\\\\sim 4 \\\\times$ more data efficient than the baselines, and no baseline is able to reach performance parity with AutoSW AP by 50000 samples (9.1% of the entire Fly dataset). We observe a similar trend in the Basketball dataset, with AutoSW AP being $\\\\sim 2 \\\\times$ as data efficient. We also observe an improvement in data efficiency even when using random sampling, and note that uncertainty sampling widens the gap between AutoSW AP and the baselines.\\n\\nWhile AutoSW AP LFs themselves do not necessarily perform better than baseline LFs when evaluated on their own (see the Supplementary Materials), they do provide a stronger learning signal for downstream classifiers than the baselines. These data efficiency differences can be attributed in part to the structural domain knowledge encoded in the DSL, as the domain-level LFs themselves perform significantly worse. For example, a AutoSW AP LF classifying \u201clunge vs. no behavior\u201d for the Fly dataset can be seen in Figure 3, and the structure of this program cannot be easily approximated with a decision tree or a neural network.\\n\\nWeak Supervision. Similar to our active learning experiments, we observe that AutoSW AP is more data efficient than the baselines in weak supervision settings (Figure 5). We note that the ground truth labels are not a baseline in...\"}"}
{"id": "CVPR-2022-151", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. AutoSW AP Active Learning Experiments. Each line represents the mean of 5 random seeds for an automatic labeling function method. The shaded region is the standard error of the seeds. As can be seen, AutoSW AP matches or outperforms all baseline methods using only a fraction of the data. Note that all plots are on log-log scales.\\n\\nFigure 5. AutoSW AP Weak Supervision Experiments. Each line represents the mean of 5 random seeds for an automatic labeling function method. The shaded region is the standard error of the seeds. The gray line shows performance when ground truth labels are used as weak labels. Although it may seem odd that AutoSW AP outperforms ground truth labels in the Mouse dataset, weak labels have been observed to outperform ground truth labels in other works [17]. Note that all plots are on log-log scales.\\n\\nIn this setting, as they are essentially an \\\"optimal\\\" case where the weak labels match the ground truth labels. On the Fly dataset, AutoSW AP generally performs better than both baselines, and on the Mouse and Basketball datasets, no baseline is able to match the performance of AutoSW AP LFs at any evaluated amount of annotated data. AutoSW AP is even able to outperform the ground truth labels in the Mouse dataset at some levels of annotated data, which indicates that the learned LFs are especially informative. Finally, we observe that AutoSW AP generally improves with more weakly labeled data points, which is useful as there is no expert annotation cost to using more weakly labeled data points.\\n\\n5.2. Additional Results\\n\\nAutoSW AP Diversity Cost. The diversity cost is an important part of AutoSW AP. As can be seen in Figure 6, synthesizing purely optimal programs w.r.t. Equation 1 results in worse performance than synthesizing diverse sets of programs. This mirrors the observations in [33], where using diverse sets of decision trees improves performance.\\n\\nInterpretability of Labeling Functions. An important part of behavior analysis is being able to interpret learned models. Neural networks and LSTMs are by nature not interpretable. Decision trees offer some degree of interpretability, but are limited to branched if-then-else states.\"}"}
{"id": "CVPR-2022-151", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Diversity Cost Utility Comparison. Synthesizing diverse sets of programs instead of purely $\\\\epsilon$-optimal sets improves AutoSW AP, showing the utility of the structural diversity cost.\\n\\nEffect on Rare Behaviors. Rare behaviors can be difficult to analyze, as even with large datasets very little data exists. Our fly domain results show that AutoSW AP greatly improves data efficiency for rare behaviors, as 5 of the 6 behaviors we study occur in <5% of the frames. We note the copulation task (which is not rare) does not bias our Fly domain data efficiency comparison as all tested methods achieve near-perfect performance on it.\\n\\n6. Discussion and Conclusion\\n\\nWe propose AutoSW AP, a framework that uses program synthesis to automatically synthesize diverse LFs. Our results demonstrate the effectiveness of our framework in both active learning and weak supervision settings and across three behavior analysis settings. We find that with existing domain-level LFs [14, 24] and a simple DSL, AutoSW AP can synthesize highly data efficient task-level LFs with minimal amounts of labeled data, thus reducing annotation requirements for domain experts.\\n\\nAdditionally, we introduce a novel structural diversity cost and admissible heuristic for synthesized programs, which allows AutoSW AP to scalably synthesize diverse LFs with informed search algorithms. This further improves the performance of our framework in behavior analysis settings, all without requiring domain experts to repeatedly handcraft task-level LFs. Overall, AutoSW AP effectively integrates weak supervision with behavior analysis, and greatly reduces domain expert effort through automatically synthesizing task-level LFs from domain-level knowledge.\\n\\nLimitations. While our DSL and LFs are at the domain-level, our method requires task-level information in the form of a small labeled dataset to synthesize LFs. Additionally, the LFs provided by domain experts should be informative of behavior (although we do show that current behavioral features [14,24] studied by domain experts are sufficient for this task). Extensions to automate other aspects of our framework while taking into account domain expert knowledge, such as library learning [11] or integrating perception [32], may further reduce expert effort. However, we note that our current framework already leads to significant reductions in data requirements.\\n\\nSocietal Impact. Automatically generating interpretable LFs to reduce expert effort can help behavior analysis across domains, such as in neuroscience, ethology, sports analytics, and autonomous vehicles, among others. Our framework leverages inductive biases in the DSL to produce interpretable programs; however, since humans create the DSL, interpret programs, and annotate data, users should be aware of potential human-encoded biases in these steps. Additional care is especially needed in human behavior domains, such as with informed consent of participants and responsible handling of data.\\n\\n7. Acknowledgements\\n\\nWe thank Adith Swaminathan at Microsoft Research and Pietro Perona at Caltech for their feedback and discussions regarding this work. We also thank Microsoft Research for compute resources. This work is partially supported by NSF Award #1918839 (YY) and NSERC Award #PGSD3-532647-2019 (JJS).\"}"}
