{"id": "CVPR-2023-1055", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Rameen Abdal, Peihao Zhu, John Femiani, Niloy J Mitra, and Peter Wonka. Clip2stylegan: Unsupervised extraction of stylegan edit directions. arXiv preprint arXiv:2112.05219, 2021.\\n\\n[2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18208\u201318218, 2022.\\n\\n[3] Kyungjune Baek, Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Hyunjung Shim. Rethinking the truly unsupervised image-to-image translation. In ICCV, pages 14154\u201314163, 2021.\\n\\n[4] Hyojin Bahng, Sunghyo Chung, Seungjoo Yoo, and Jaegul Choo. Exploring unlabeled faces for novel attribute discovery. In CVPR, pages 5821\u20135830, 2020.\\n\\n[5] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image and video editing. In European Conference on Computer Vision, pages 707\u2013723. Springer, 2022.\\n\\n[6] Deblina Bhattacharjee, Seungryong Kim, Guillaume Vizier, and Mathieu Salzmann. Dunit: Detection-based unsupervised image-to-image translation. In CVPR, pages 4787\u20134796, 2020.\\n\\n[7] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101\u2014mining discriminative components with random forests. In ECCV, pages 446\u2013461. Springer, 2014.\\n\\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. NeurIPS, 33:1877\u20131901, 2020.\\n\\n[9] Yunjey Choi, Minje Choi, Munyoung Kim, Jung-Woo Ha, Sunghun Kim, and Jaegul Choo. Stargan: Unified generative adversarial networks for multi-domain image-to-image translation. In CVPR, pages 8789\u20138797, 2018.\\n\\n[10] Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In CVPR, 2020.\\n\\n[11] Aviv Gabbay and Yedid Hoshen. Scaling-up disentanglement for image translation. arXiv preprint arXiv:2103.14017, 2021.\\n\\n[12] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.\\n\\n[13] Chunjiang Ge, Rui Huang, Mixue Xie, Zihang Lai, Shiji Song, Shuang Li, and Gao Huang. Domain adaptation via prompt learning. arXiv preprint arXiv:2202.06687, 2022.\\n\\n[14] Abel Gonzalez-Garcia, Joost Van De Weijer, and Yoshua Bengio. Image-to-image translation for cross-domain disentanglement. arXiv preprint arXiv:1805.09730, 2018.\\n\\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.\\n\\n[16] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022.\\n\\n[17] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In NeurIPS, pages 6626\u20136637, 2017.\\n\\n[18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. arXiv preprint arxiv:2006.11239, 2020.\\n\\n[19] Judy Hoffman, Eric Tzeng, Taesung Park, Jun-Yan Zhu, Phillip Isola, Kate Saenko, Alexei Efros, and Trevor Darrell. Cycada: Cycle-consistent adversarial domain adaptation. In ICML, pages 1989\u20131998. PMLR, 2018.\\n\\n[20] Tony Huang, Jack Chu, and Fangyun Wei. Unsupervised prompt learning for vision-language models. arXiv preprint arXiv:2204.03649, 2022.\\n\\n[21] Xun Huang, Ming-Yu Liu, Serge Belongie, and Jan Kautz. Multimodal unsupervised image-to-image translation. In ECCV, pages 172\u2013189, 2018.\\n\\n[22] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In CVPR, pages 1125\u20131134, 2017.\\n\\n[23] Somi Jeong, Youngjung Kim, Eungbean Lee, and Kwanghoon Sohn. Memory-guided unsupervised image-to-image translation. In CVPR, pages 6558\u20136567, 2021.\\n\\n[24] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, pages 4904\u20134916. PMLR, 2021.\\n\\n[25] Chen Ju, Tengda Han, Kunhao Zheng, Ya Zhang, and Weidi Xie. Prompting visual-language models for efficient video understanding. In European Conference on Computer Vision, pages 105\u2013124. Springer, 2022.\\n\\n[26] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, pages 4401\u20134410, 2019.\\n\\n[27] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of StyleGAN. In Proc. CVPR, 2020.\\n\\n[28] Gwanghyun Kim and Jong Chul Ye. Diffusionclip: Text-guided image manipulation using diffusion models. 2021.\\n\\n[29] Kunhee Kim, Sanghun Park, Eunyeong Jeon, Taehun Kim, and Daijin Kim. A style-aware discriminator for controllable image translation. In CVPR, 2022.\\n\\n[30] Soohyun Kim, Jongbeom Baek, Jihye Park, Gyeongnyeon Kim, and Seungryong Kim. Instaformer: Instance-aware image-to-image translation with transformer. In CVPR, pages 18321\u201318331, 2022.\\n\\n[31] Hsin-Ying Lee, Hung-Yu Tseng, Jia-Bin Huang, Maneesh Singh, and Ming-Hsuan Yang. Diverse image-to-image translation via disentangled representations. In ECCV, pages 35\u201351, 2018.\\n\\n[32] Wanhua Li, Xiaoke Huang, Zheng Zhu, Yansong Tang, Xiu Li, Jie Zhou, and Jiwen Lu. Ordinalclip: Learning...\"}"}
{"id": "CVPR-2023-1055", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[33] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021.\\n\\n[34] Ming-Yu Liu, Thomas Breuel, and Jan Kautz. Unsupervised image-to-image translation networks. In NeurIPS, pages 700\u2013708, 2017.\\n\\n[35] Ming-Yu Liu, Xun Huang, Arun Mallya, Tero Karras, Timo Aila, Jaakko Lehtinen, and Jan Kautz. Few-shot unsupervised image-to-image translation. In arxiv, 2019.\\n\\n[36] Pengfei Liu, Weizhe Yuan, Jinlan Fu, Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig. Pre-train, prompt, and predict: A systematic survey of prompting methods in natural language processing. arXiv preprint arXiv:2107.13586, 2021.\\n\\n[37] Steven Liu, Tongzhou Wang, David Bau, Jun-Yan Zhu, and Antonio Torralba. Diverse image generation via self-conditioned gans. In CVPR, pages 14286\u201314295, 2020.\\n\\n[38] Xingchao Liu, Chengyue Gong, Lemeng Wu, Shujian Zhang, Hao Su, and Qiang Liu. Fusedream: Training-free text-to-image generation with improved clip+ gan space optimization. arXiv preprint arXiv:2112.01573, 2021.\\n\\n[39] Xihui Liu, Dong Huk Park, Samaneh Azadi, Gong Zhang, Arman Chopikyan, Yuxiao Hu, Humphrey Shi, Anna Rohrbach, and Trevor Darrell. More control for free! image synthesis with semantic diffusion guidance. arXiv preprint arXiv:2112.05744, 2021.\\n\\n[40] Yahui Liu, Enver Sangineto, Yajing Chen, Linchao Bao, Haoxian Zhang, Nicu Sebe, Bruno Lepri, Wei Wang, and Marco De Nadai. Smoothing the disentangled latent style space for unsupervised image-to-image translation. In CVPR, pages 10785\u201310794, 2021.\\n\\n[41] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In ICCV, pages 3730\u20133738, 2015.\\n\\n[42] Mario Lu \u02c7ci\u00b4c, Michael Tschannen, Marvin Ritter, Xiaohua Zhai, Olivier Bachem, and Sylvain Gelly. High-fidelity image generation with fewer labels. In ICML, pages 4183\u20134192. PMLR, 2019.\\n\\n[43] Qi Mao, Hsin-Ying Lee, Hung-Yu Tseng, Siwei Ma, and Ming-Hsuan Yang. Mode seeking generative adversarial networks for diverse image synthesis. In CVPR, pages 1429\u20131437, 2019.\\n\\n[44] Muhammad Ferjad Naeem, Seong Joon Oh, Youngjung Uh, Yunjey Choi, and Jaejun Yoo. Reliable fidelity and diversity metrics for generative models. In ICML, pages 7176\u20137185. PMLR, 2020.\\n\\n[45] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\\n\\n[46] Taesung Park, Alexei A Efros, Richard Zhang, and Jun-Yan Zhu. Contrastive learning for unpaired image-to-image translation. In ECCV, pages 319\u2013345. Springer, 2020.\\n\\n[47] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. In ICCV, pages 2085\u20132094, 2021.\\n\\n[48] Fabio Petroni, Tim Rockt \u00a8aschel, Patrick Lewis, Anton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. Language models as knowledge bases? arXiv preprint arXiv:1909.01066, 2019.\\n\\n[49] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 8748\u20138763. PMLR, 2021.\\n\\n[50] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. arXiv preprint arXiv:2208.12242, 2022.\\n\\n[51] Kuniaki Saito, Kate Saenko, and Ming-Yu Liu. Coco-funit: Few-shot unsupervised image translation with a content conditioned style encoder. In ECCV, pages 382\u2013398. Springer, 2020.\\n\\n[52] Taylor Shin, Yasaman Razeghi, Robert L Logan IV , Eric Wallace, and Sameer Singh. Autoprompt: Eliciting knowledge from language models with automatically generated prompts. arXiv preprint arXiv:2010.15980, 2020.\\n\\n[53] Manli Shu, Weili Nie, De-An Huang, Zhiding Yu, Tom Goldstein, Anima Anandkumar, and Chaowei Xiao. Test-time prompt tuning for zero-shot generalization in vision-language models. arXiv preprint arXiv:2209.07511, 2022.\\n\\n[54] Ivan Skorokhodov, Grigorii Sotnikov, and Mohamed Elhoseiny. Aligning latent and image spaces to connect the unconnectable. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14144\u201314153, 2021.\\n\\n[55] Yaxing Wang, Salman Khan, Abel Gonzalez-Garcia, Joost van de Weijer, and Fahad Shahbaz Khan. Semi-supervised learning for few-shot image-to-image translation. In CVPR, pages 4453\u20134462, 2020.\\n\\n[56] Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Zhen-tao Tan, Lu Yuan, Weiming Zhang, and Nenghai Yu. Hairclip: Design your hair by text and reference image. arXiv preprint arXiv:2112.05142, 2021.\\n\\n[57] Po-Wei Wu, Yu-Jing Lin, Che-Han Chang, Edward Y Chang, and Shih-Wei Liao. Relgan: Multi-domain image-to-image translation via relative attributes. In ICCV, pages 5914\u20135922, 2019.\\n\\n[58] Dingdong Yang, Seunghoon Hong, Yunseok Jang, Tianchen Zhao, and Honglak Lee. Diversity-sensitive conditional generative adversarial networks. In ICLR, 2019.\\n\\n[59] Yuan Yao, Ao Zhang, Zhengyan Zhang, Zhiyuan Liu, Tat-Seng Chua, and Maosong Sun. Cpt: Colorful prompt tuning for pre-trained vision-language models. arXiv preprint arXiv:2109.11797, 2021.\\n\\n[60] Zili Yi, Hao Zhang, Ping Tan, and Minglun Gong. Dualgan: Unsupervised dual learning for image-to-image translation. In ICCV, pages 2849\u20132857, 2017.\\n\\n[61] Pan Zhang, Bo Zhang, Dong Chen, Lu Yuan, and Fang Wen. Cross-domain correspondence learning for exemplar-based image translation. In CVPR, pages 5143\u20135153, 2020.\"}"}
{"id": "CVPR-2023-1055", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. The spatially-correlative loss for various image translation tasks. In CVPR, pages 16407\u201316417, 2021.\\n\\nChunting Zhou, Junxian He, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Prompt consistency for zero-shot task generalization. arXiv preprint arXiv:2205.00049, 2022.\\n\\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. arXiv preprint arXiv:2109.01134, 2021.\\n\\nKaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Conditional prompt learning for vision-language models. arXiv preprint arXiv:2203.05557, 2022.\\n\\nXingran Zhou, Bo Zhang, Ting Zhang, Pan Zhang, Jianmin Bao, Dong Chen, Zhongfei Zhang, and Fang Wen. Cocosnet v2: Full-resolution correspondence learning for image translation. In CVPR, pages 11465\u201311475, 2021.\\n\\nJun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei A Efros. Unpaired image-to-image translation using cycle-consistent adversarial networks. In ICCV, 2017.\"}"}
{"id": "CVPR-2023-1055", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"LANIT: Language-Driven Image-to-Image Translation for Unlabeled Data\\n\\nJihye Park* 1, Sunwoo Kim* 1, Soohyun Kim* 1, Seokju Cho1, Jaejun Yoo2, Youngjung Uh3, Seungryong Kim\u2020 1\\n\\n1 Korea University, Seoul, Korea\\n2 UNIST, Ulsan, Korea\\n3 Yonsei University, Seoul, Korea\\n\\n1 {ghp1112,sw-kim,shkim1211,seokjucho,seungryongkim}@korea.ac.kr\\n2 jaejun.yoo@unist.ac.kr\\n3 yj.uh@yonsei.ac.kr\\n\\nAbstract\\nExisting techniques for image-to-image translation commonly have suffered from two critical problems: heavy reliance on per-sample domain annotation and/or inability to handle multiple attributes per image. Recent truly-unsupervised methods adopt clustering approaches to easily provide per-sample one-hot domain labels. However, they cannot account for the real-world setting: one sample may have multiple attributes. In addition, the semantics of the clusters are not easily coupled to human understanding.\\n\\nTo overcome these, we present LANguage-driven Image-to-image Translation model, dubbed LANIT. We leverage easy-to-obtain candidate attributes given in texts for a dataset: the similarity between images and attributes indicates per-sample domain labels. This formulation naturally enables multi-hot labels so that users can specify the target domain with a set of attributes in language. To account for the case that the initial prompts are inaccurate, we also present prompt learning. We further present domain regularization loss that enforces translated images to be mapped to the corresponding domain. Experiments on several standard benchmarks demonstrate that LANIT achieves comparable or superior performance to existing models. The code is available at github.com/KU-CVLAB/LANIT.\\n\\n1. Introduction\\nUnpaired image-to-translation frameworks aim to translate an image from one domain to another [21,31,46]. They are typically trained on many images with their respective domain labels. It has been widely studied for a decade in numerous computer vision applications, such as image manipulation [14,62,66], multi-modal translation [3,9,10,21,35], and instance-aware translation [6, 23, 30].\\n\\nConventional methods [9, 10, 21] required at least per-sample domain supervision based on the strict assumption that each image should be described by a one-hot domain label, which is frequently violated; e.g., a face can have multiple attributes. In addition, annotating domain label for each image may become considerably labor-intensive and error-prone, and subjectivity may lead to inconsistency across different annotators.\\n\\nTo mitigate such a heavy reliance on per-sample domain annotation, some recent methods formulate the problem in a few-shot setting [35, 51] or semi-supervised learning setting [55]. More recently, TUNIT [3] and Kim et al. [29] present a framework for jointly learning domain clustering and translation within a unified model, thus enabling a truly-unsupervised learning of image-to-image translation. Although these methods ease the burden of per-sample domain annotation and show relatively competitive performance, they still inherit the limitation of using one-hot domain labels to train the image translation model. In addition, these truly-unsupervised learning methods only learn domain clusters that are dominant in the dataset, which sometimes do not reflect semantic meanings. Such limitations make users confused in understanding the semantic meaning of each learned domain cluster.\\n\\nIn this paper, to overcome the problems in per-sample-level and unsupervised learning methods, we propose using the dataset-level annotation for the first time as exemplified in Fig. 1. In practice, it could be much easier to list candidate textual domain descriptions that describe images in the dataset. Using the candidate attributes given in texts for a dataset, we specify the target domain given a reference image according to its similarity to the attributes in vision-language embedding [49]. Then, a generator translates a source image to the target domain using the target style code of the reference image. As similarities naturally lead to a multi-hot label, we can aggregate the style codes of multiple domains to represent the style code for a joint domain. In addition, to account for the fact that the initial prompts may be inaccurate, we also present prompt learning. We further present domain regularization loss that enforces translated images to be mapped to the corresponding domain. Experiments on several standard benchmarks demonstrate that LANIT achieves comparable or superior performance to existing models. The code is available at github.com/KU-CVLAB/LANIT.\"}"}
{"id": "CVPR-2023-1055", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Levels of supervision. For unpaired image-to-image translation, (a) conventional methods \\\\[10, 21, 35, 55\\\\] require at least per-sample-level domain supervision, which is often hard to collect. To overcome this, (b) unsupervised learning methods \\\\[3, 29\\\\] learn image translation model using a dataset itself without any supervision, but it shows limited performance and lacks the semantic understanding of each cluster, limiting its applicability. Unlike them, (c) we present a novel framework that requires a dataset with possible textual domain descriptions (i.e., dataset-level annotation), which achieves comparable or even better performance than previous methods.\\n\\nFigure 2. Examples of semantic encoding. The existing unsupervised method \\\\[3\\\\] allows users to translate one of several clusters. However, the learned clusters lack semantic meaning and sometimes some attributes do not appear in any clusters. Unlike this, our framework can select and train the domains that have explicit semantic meaning, which is more applicable.\\n\\nSource\\n\u201csmiling\u201d \u201cbang\u201d \u201cmale\u201d \u201cmustache\u201d \u201cblonde\u201d \u201clipstick\u201d\\n\\nCluster 1\\nCluster 2\\nCluster 3\\nCluster 4\\nCluster 5\\n\\nUnsup\\nOurs\\n\\n2. Related Work\\n\\nImage-to-Image Translation. While early methods for image-to-image translation considered a paired setting \\\\[22\\\\], most recent state-of-the-arts focus on an unpaired setting \\\\[3, 11, 19, 40, 46, 57, 62, 66\\\\]. This trend was initiated by CycleGAN \\\\[67\\\\], of which many variants were proposed, formulating uni-modal models \\\\[34, 60, 61\\\\] and multi-modal models \\\\[9, 10, 21, 31\\\\]. Although these methods overcome the inherent limitation of the paired setting, they still require at least per-sample domain annotation, which is often labor intensive or even impossible to achieve in some cases. In addition, this per-sample supervision does not consider that some images may require multi-hot domain labels such as facial attributes, e.g., blond and young.\\n\\nTo mitigate the reliance on per-sample supervision, FU-NIT \\\\[35\\\\] and CoCo-FUNIT \\\\[51\\\\] proposed a few-shot setting, but still required a few annotations. SEMIT \\\\[55\\\\] utilizes a pseudo labeling technique. S3GAN \\\\[42\\\\], Self-conditioned GAN \\\\[37\\\\], and \\\\[4\\\\] adopt clustering methods or a classifier. Recently, TUNIT \\\\[3\\\\] and Kim et al. \\\\[29\\\\] have proposed the truly-unsupervised frameworks with clustering approach or prototype-based learning, which do not need any per-sample domain supervision. However, their performance was limited in that it is challenging to understand the semantic meaning of each pseudo domain, which in turn limits their applicability. Note that the aforementioned methods also inherit the problem of the one-hot domain labeling assumption.\\n\\nVision-Language Model in Image Manipulation. Many large-scale vision-language models \\\\[24, 49\\\\] have been popularly adopted in computer vision problems. Especially, CLIP \\\\[49\\\\] has been employed in image synthesis and manipulation with GANs \\\\[1, 38, 47, 56\\\\]. For instance, StyleCLIP \\\\[47\\\\] and HairCLIP \\\\[56\\\\] manipulate the latent space of StyleGAN \\\\[27\\\\], through the guidance of CLIP, but they heavily rely on pre-trained StyleGAN. CLIP2StyleGAN extends such an approach in an unsupervised manner that eliminates the need for target style descriptions. More recently, some works \\\\[2, 12, 16, 28, 39, 45, 50\\\\] combine a diffusion model \\\\[18\\\\] with CLIP \\\\[49\\\\] for text-driven image manipulation with high fidelity. However, they show limited performance in realistically translating in the real image domain. Furthermore, they rely on pre-trained diffusion models.\"}"}
{"id": "CVPR-2023-1055", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Network configuration. Our model consists of content and style encoders ($E_C, E_S$), vision-language encoders ($E_V, E_L$), and generator ($G$). We extract content and style vectors from content $x$ and style $y$, respectively. By leveraging vision-language features and the proposed adaptive thresholding technique, we measure the pseudo domain label $d_y$ of $y$. We generate $\\\\hat{y}$ with content vector $c_x$ and aggregated style vector $a_y$ through the generator.\\n\\nFusion model [18], and they are only available on text-conditioned manipulation. On the other hand, our LANIT is not dependent on pre-trained StyleGAN [27] and also learns style manifold from given image dataset and a few numbers of prompts that designate explicit semantics to be learned.\\n\\nPrompt Learning. Since the success of pretrained large-scale language models such as GPT-3 [8] in NLP, various methods for optimizing the prompt in a text format have been proposed [33, 36, 48, 52]. Inspired by these works, some works [13, 20, 25, 32, 59, 64, 65] using CLIP [49] also attempted to optimize the input prompt. For instance, CoCoOp [65] showed that using the learned continuous prompt could surpass the manually-designed discrete prompt-based method on zero-shot image classification. However, they require class supervision. Recently, several works [20, 53, 63] propose unsupervised prompt tuning. In this work, we explore combining image translation with prompt learning in an unsupervised way for the first time.\\n\\n3. Methodology\\n\\n3.1. Motivation and Problem Formulation\\n\\nWe leverage a list of dataset-level domain descriptions in the text form and their similarity with the unlabeled images in CLIP [49] embedding space to measure a pseudo domain label of style image. By doing so, we aim to alleviate difficulty in the labeling process and give explicit semantic meaning in the learned domain for the multi-hot label setting.\\n\\nOne naive approach obtaining the pseudo domain label is by calculating the similarity between image and text features with CLIP [49]. However, we observe that this leads to inaccurate labeling and noisy translation since the image-text feature space in CLIP [49] is not perfectly aligned. To mitigate this issue, we introduce two solutions, an adaptive thresholding with the base prompt for selecting confident pseudo labels, and a prompt learning technique with domain regularization loss for boosting confidence in calculating similarity.\\n\\n3.2. Image Translation with Pseudo Domain Label\\n\\nOur model, consisting of content encoder $E_C$, style encoder $E_S$, mapping encoder $E_M$, and generator $G$ similar to StarGAN2 [10], aims to learn an image-to-image translation with content image $x$ and style image $y$ (or between domains $X$ and $Y$) to generate a translated image $\\\\hat{y}$.\\n\\nIn specific, we first extract the content vector and style vector from $x \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 3}$ and $y \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 3}$ with height $H$ and width $W$ such that $c_x = E_C(x) \\\\in \\\\mathbb{R}^{h_c \\\\times w_c \\\\times c}$ with height $h_c$, width $w_c$ and channel $c$, and $s_y = E_{S,n}(y) \\\\in \\\\mathbb{R}^{1 \\\\times s}$ for $n$-th domain (or $\\\\tilde{s}_y = E_M(z)$ from a random latent $z$), respectively. While existing methods, e.g., StarGAN2 [10], select a single vector from $\\\\{s_y^1, \\\\ldots, s_y^N\\\\}$ of $N$ domains according to ground-truth one-hot domain label, which is inserted to the generator, we aggregate such style vectors $\\\\{s_y^1, \\\\ldots, s_y^N\\\\}$ with the pseudo domain label $d_y = [d_y^n]_{n=1}^N \\\\in \\\\mathbb{R}^{N \\\\times 1}$ defined as a binary vector, which will be discussed in the following, such that $a_y = 1^M_y \\\\sum_{n=1}^N s_y^n d_y^n$, (1) where $M_y$ is the number of one values in the pseudo domain label $d_y$. With this aggregated style vector $a_y \\\\in \\\\mathbb{R}^{1 \\\\times s}$ and content vector $c_x$, our networks finally generates the output such that $\\\\hat{y} = G(c_x, a_y)$, containing a style normalization layer. The detailed network architecture is described in the suppl. material.\"}"}
{"id": "CVPR-2023-1055", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3. Language-Driven Domain Labeling\\n\\nIn this section, we explain how to obtain the pseudo multi-hot domain label $d_y$ for an image $y$ using pretrained vision-language model. Unlike TUNIT [3] that clusters the images from the dataset using the visual features only, we use $n$-th domain's textual description as prompt $t_n \\\\in \\\\mathbb{R}^{1 \\\\times L}$ which is defined as $t_n = [p_{domain_n}, p_{l}(l \\\\in \\\\{1, ..., L\\\\})]$, which consists of one of candidate domains $\\\\{p_{domain_n}\\\\}$ and a template $p_l$ that is shared with all the candidate domains, where $p_l$ is a vector for a word with the same dimension as $p_{domain_n}$. For instance, all $p_l$ and $p_{domain_n}$ can be initialized with a template, e.g., \\\"a face with\\\", and given textual domain descriptions, e.g., \\\"black hair\\\", respectively. It is utilized to softly cluster the images by measuring the similarity between visual features and language features. In our framework, all the prompts $T = [t_n]_{n=1}^N \\\\in \\\\mathbb{R}^{N \\\\times (L+1)}$ are initialized by the human-annotated domain descriptions or keywords, namely dataset-level supervision.\\n\\nBased on this definition, we first extract vision and language features $v_y = E_V(y) \\\\in \\\\mathbb{R}^{1 \\\\times k}$ with $k$ channels, and $[u_t]_{n=1}^N = E_L(T) \\\\in \\\\mathbb{R}^{N \\\\times k}$ with $k$ channels from style $y$ and all the prompts $T$, respectively, by using pretrained vision-language model, i.e., CLIP [49]. Note that any vision-language models with vision and language encoders, i.e., ALIGN [24], can be used in our framework. We then measure a similarity $f_y = [f_y, t_n]_{n=1}^N \\\\in \\\\mathbb{R}^{N \\\\times 1}$, computed as:\\n\\n$$f_y, t_n = \\\\bar{v}_y \\\\cdot \\\\bar{u}_t,$$\\n\\nwhere $\\\\bar{v}_y = v_y / \\\\|v_y\\\\|$ and $\\\\bar{u}_t = u_t / \\\\|u_t\\\\|$.\\n\\nBy utilizing the measured similarity $f_y$, we can obtain the multi-hot pseudo domain label $d_y$ with a simple candidate selecting method, e.g., top-$K$ operation or thresholding. However, with top-$K$ operation, we can just select $K$ number of attributes ignoring the other attributes that would exist in the style image. Moreover, a simple thresholding with a predefined hyperparameter would limit the performance. To overcome these, we present an adaptive thresholding technique based on the observation that the more specified text descriptions, e.g., \\\"a face with blonde hair\\\" make higher similarity with corresponding images than the base prompt as a neutral text description, e.g., \\\"a face with\\\", that represent each dataset. We set the base prompt $p$ as the template, meaning that they are updated simultaneously, and thus $u_p = E_L(p) \\\\in \\\\mathbb{R}^{1 \\\\times k}$. We then define the domain label $d_y = [d_{yn}]_{n=1}^N \\\\in \\\\mathbb{R}^{N \\\\times 1}$ as follows:\\n\\n$$d_{yn} = \\\\begin{cases} 1, & \\\\text{if } f_y, t_n > \\\\bar{v}_y \\\\cdot \\\\bar{u}_p, \\\\\\\\ 0, & \\\\text{otherwise} \\\\end{cases}.$$\\n\\n3.4. Prompt Learning\\n\\nSo far, we have discussed the method for language-driven domain labeling and image translation with the pseudo domain label. However, since the pre-defined templates may not be optimal to describe all the images in the dataset, the performance would be noisy and limited. To overcome these, we present a prompt learning technique as shown in Fig. 4. Similar to [20, 64, 65], we set the prompt $t_n$ as learnable except $p_{domain_n}$. In other words, the templates $[p_l]$ are set as the learnable parameters in $t_n$. By allowing the template to be updated, more optimal prompts can be optimized, which in turn boosts the translation performance. Note that the input domain descriptions should be given in our framework, but they can also be obtained by using a predefined dictionary [1, 49] in a manner that the highly relevant texts to the dataset can be selected based on the similarity between an image and candidate texts.\\n\\n3.5. Loss Functions\\n\\nAdversarial Loss. We adopt an adversarial loss to guide the translated images $\\\\hat{y}$ to arrive at the training distribution of the target domain. Following StarGAN2 [10] and TUNIT [3], we also adopt the multi-domain discriminators, but we use the outputs of multi-domain discriminators weighted\"}"}
{"id": "CVPR-2023-1055", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"by a multi-hot domain label $d$:\\n\\n$$L_{adv} = \\\\sum_{n=1}^{N} \\\\left[ \\\\log D_n(y_n) d_y_n + \\\\log(1 - D_n(G(x, a_y))) d_y_n \\\\right],$$\\n\\n(5)\\n\\nwhere $D_n(\\\\cdot)$ denotes the $n$-th discriminator output and $d_y_n$ denotes the $n$-th element of $d_y$. It should be noted that if we set $d_y$ as a one-hot domain label by ground-truth or pseudo-label, this loss function becomes the same adversarial loss in [3, 10].\\n\\n**Domain Regularization Loss.** We further present a domain regularization loss that encourages the networks to generate an image $\\\\hat{y}$ following the domain label of input style image $y$ more precisely. To this end, perhaps one of the simplest functions is to use the dot product between $d_y$ and $f_{\\\\hat{y}}$ such that\\n\\n$$L_{dl} = d_y \\\\cdot f_{\\\\hat{y}}.$$  \\n\\nMinimizing this, however, can induce erroneous solutions, since considering an inactive domain label, i.e., $d_y_n = 0$, may degrade the performance further.\\n\\nTo overcome this, we present a novel loss function. We first make a domain label pair, $d_y$ and $d_{y \\\\text{inv}}(n)$, which has the same labels with $d_y$ except the $n$-th label by replacing the $n$-th domain label $d_y_n$ to $d_{y \\\\text{inv}}(n) = 1 - d_y_n$. We then generate $\\\\hat{y}' = G(\\\\hat{c}_y, a_{\\\\hat{y} \\\\text{inv}})$ where $a_{\\\\hat{y} \\\\text{inv}}$ is obtained using $\\\\hat{y}$ and $d_{y \\\\text{inv}}(n)$. We then encourage not only $f_{\\\\hat{y}}$ to follow $d_y$ at $n$-th label, but also $f_{\\\\hat{y} \\\\text{inv}}$ to follow $d_{y \\\\text{inv}}(n)$ at $n$-th label, such that\\n\\n$$L_{dl} = H(d_y_n, f_{\\\\hat{y}n}) + H(d_{y \\\\text{inv}}(n), f_{\\\\hat{y} \\\\text{inv}}(n)).$$  \\n\\n(6)\\n\\nwhere $H(\\\\cdot, \\\\cdot)$ denotes a binary cross-entropy.\\n\\n**Cycle-Consistency Loss.** To make the translated image similar to its original image, which also regularizes the ill-posed translation problem, we adopt a cycle-consistency loss such that\\n\\n$$L_{cyc} = \\\\mathbb{E}_{x, y} \\\\left[ \\\\| x - G((\\\\hat{y}), a_x) \\\\|_1 \\\\right],$$\\n\\n(7)\\n\\nwhere $c_{\\\\hat{y}}$ denotes the content from $\\\\hat{y}$, and $a_x$ denotes the style vector of input $x$. By encouraging the generator $G$ to reconstruct the input image $x$ with estimated style vector $a_x$, $G$ learns to preserve the original characteristics of $x$.\\n\\n**Style Reconstruction Loss.** While the generator is able to synthesize realistic images with the losses above, the synthesized results are not guaranteed to be style-consistent with $y$. In order to better learn style representation, we compute $l^{-1}$ loss between the style vector from the translated image and style image such that\\n\\n$$L_{sty} = \\\\mathbb{E}_{x, y} \\\\left[ \\\\| s_y - E_S(\\\\hat{y}) \\\\|_1 \\\\right].$$\\n\\n(8)\\n\\n**Style Diversification Loss.** To learn the generator to produce diverse images, we employ a diversity sensitive loss [43, 58] defined as:\\n\\n$$L_{ds} = \\\\mathbb{E}_{x, y} \\\\left[ \\\\| G(x, E_M(z_1)) - G(x, E_M(z_2)) \\\\|_1 \\\\right],$$\\n\\n(9)\\n\\nwhere $z_1$ and $z_2$ are random latent vectors from Gaussian distribution. Note that we only adopt the diversification loss using random vectors, not the style vector $a_y$ from an image. Also, we utilize the domain label $d_y$ from the style image $y$ as a pseudo-label for the latent vector.\\n\\n**Overall Objective.** Full loss functions are as follows:\\n\\n$$L_{total} = \\\\lambda_{adv} L_{adv} + \\\\lambda_{dl} L_{dl} + \\\\lambda_{cyc} L_{cyc} - \\\\lambda_{ds} L_{ds},$$\\n\\n(10)\\n\\nwhere $\\\\lambda_{adv}$, $\\\\lambda_{dl}$, $\\\\lambda_{cyc}$, $\\\\lambda_{sty}$, and $\\\\lambda_{ds}$ are hyper-parameters.\\n\\n### 4. Experiments\\n\\n#### Implementation Details\\n\\nWe adopted StarGAN2 [10] as the baseline architecture for content, style, mapping encoders, generator, and discriminator. Additional implementation details will be explained in the supplementary material.\\n\\n**Setting up Templates and Candidate Domains.** We set the candidate domains by randomly sampling among the domain or attribute names in each dataset. For a fair comparison, we experiment three times with different domains respectively to measure quantitative results. The selected candidate domains are shown in the supplementary materials. We refer to the templates from [5, 12] and slightly modify them to align for each dataset. To improve the accuracy of pseudo labels, we also adopt template augmentation in our baseline model.\\n\\n**Datasets.** We conduct the experiments on four standard datasets, including Animal Faces-10, Food-10, CelebA-HQ, and LHQ [7, 35, 41, 54]. In particular, we consider 10 textual domain descriptions among Animal Faces-149 and Food-101, respectively, selected in TUNIT [3], to define the initial prompt of our framework. For CelebA-HQ, we obtain 40 attributes, and sample 10 attributes for three times to define the initial prompt and report the average results. Since LHQ dataset [54] does not provide any attributes, we sampled various target domain descriptions and selected 10 domains for experiments.\\n\\n**Evaluation Metrics.** In our experiments, we adopt three quantitative evaluation metrics. First, the mean of class-wise Fr\u00e9chet Inception Distance (mFID) [17] is used to evaluate how the translated image reflects the input style. In addition, we use Density and Coverage (D&C) [44], which measure the fidelity and diversity of the translated images, respectively. The lower the mFID value, the better quality of the image. As fidelity and diversity get better, the D&C scores are higher or closer to 1. We also measure Acc., namely classification accuracy, between the domain labels with the highest probability and ground-truth domain labels.\"}"}
{"id": "CVPR-2023-1055", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Quantitative Results.\\n\\nWe first report the quantitative comparison of our model with StarGAN2 [10], Smoothing [40] (supervised), TUNIT [3] and Kim et al. [29] (unsupervised) on CelebA-HQ, AnimalFaces-10 and Food-10 datasets. Note that StarGAN2 and Smoothing are trained using 10 classes of ground-truth per-sample domain labels. Our and truly-unsupervised models use GT labels for calculating quantitative metrics. Tab. 1 shows the quantitative comparison in terms of mFID, D&C metrics. LANIT shows the best mFID and D&C scores, proving that our model outperforms the state-of-the-art methods in terms of image quality, fidelity and diversity. We also observe that LANIT consistently outperforms unsupervised models by a large margin on all three datasets on Acc. metric. Although our LANIT only utilizes unlabeled datasets with dataset-level text supervision, it shows competitive results against StarGAN2 [10] and Smoothing [40], which require ground-truth per-sample domain labels. Note that we do not provide quantitative results on LHQ since it does not provide ground-truth labels.\\n\\nQualitative Results.\\n\\nWe show visual results for Tab. 1 in Fig. 5. We observe that the quality differences between our LANIT and TUNIT [3] or Kim et al. [29] are significant. The images generated by the other methods tend to contain artifacts and show limited performance in reflecting diverse attributes from style images, while our method yields stable performance in terms of style representation and semantic consistency. Especially, we show that our model is highly capable of capturing multiple subtle attributes, such as \u201csmiling\u201d or \u201cstraight hair\u201d, while TUNIT [3] and Kim et al. [29] often fail. Interestingly, our LANIT shows competitive results compared to StarGAN2 [10] and Smoothing [40] which are supervised methods, even with better.\"}"}
{"id": "CVPR-2023-1055", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Quantitative comparison of LANIT with TUNIT [3] by varying the number of domains.\\n\\nTable 3. Quantitative results with pre-defined dictionary.\\n\\n4.2. Ablation Study and Analysis\\n\\nNumber of Domain Descriptions \\\\(N\\\\). In Tab. 2, we validate the effects of the number of candidate domains \\\\(N\\\\) compared to TUNIT [3]. Our results show that a different number of \\\\(N\\\\) brings minor changes to our model than TUNIT [3], which justifies the robustness of our framework. In animal faces-10, LANIT shows the best performance when there exist 10 candidate domains, whereas in CelebA-HQ, there is little difference in the FID score when \\\\(N\\\\) is over 10.\\n\\nPredefined Dictionary. We also evaluate the quantitative results when using a predefined dictionary instead of exploiting specific domain descriptions that we manually provide. For this experiment, we regard the whole class names in each dataset as a dictionary. For sampling candidate domains, we first calculated similarity values on the whole images and target domains, and then only averaged the similarity values assigned to the target domains in the dictionary. Then, we ordered average values in descending order and selected 10 domains. While our LANIT shows a slight drop in both mFID and F1 scores as shown in the Tab. 3 for such a setting in all the datasets, it still records competitive performance compared to the default setting.\\n\\nAdaptive Thresholding with Base Prompt. We study the effectiveness of the adaptive thresholding technique using base prompt compared to the simple top-\\\\(K\\\\) method in Tab. 4. We find that the proposed technique brings a large improvement in performance. This indicates that it enables the model to mitigate the influence of ambiguous or noisy\"}"}
{"id": "CVPR-2023-1055", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4.\\n\\n| Method         | mFID  | \u2193 D&C | \u2191 F1   |\\n|----------------|-------|-------|--------|\\n| A LANIT (Top-1) | 49.65 | 0.56  | 0.32   |\\n| B LANIT (Top-3) | 41.68 | 0.68  | 0.30   |\\n| C Baseline      | 33.79 | 0.70  | 0.26   |\\n| D + DL Loss     | 29.21 | 0.86  | 0.32   |\\n| E + Prompt Learning | 27.96 | 0.91  | 0.34   |\\n\\nTable 5.\\n\\n| Method         | mFID  | \u2193 D&C | \u2191 F1   |\\n|----------------|-------|-------|--------|\\n| End-to-End     | 30.22 | 0.81  | 0.27   |\\n| Stage-wise     | 27.96 | 0.91  | 0.34   |\\n\\nWe evaluate our model with single or multiple activated top-K attributes. We can see that the result of Top-3 significantly surpasses Top-1. This demonstrates that an image can be described more precisely when given with multiple keywords or text descriptions, while most prior works including TUNIT, Kim et al. and StarGAN2 assume to assign a single label for each image. In addition, we evaluate the qualitative results in Fig. 8 corresponding to the Tab. 4. It shows that our LANIT captures more diverse attributes rather than Top-1 and Top-3 models.\\n\\nDomain Regularization Loss. As shown in Tab. 4 (D and E), our LANIT improves performance Domain Regularization (DL) loss compared to the baseline on CelebA-HQ. Especially, D&C metrics are significantly increased by adding our proposed domain regularization loss. In Fig. 8, the generated image has the style of reference image more faithfully by using domain regularization loss.\\n\\nPrompt Learning. In order to evaluate the effectiveness of prompt learning, we compare the estimated pseudo-domain labels with and without the proposed prompt learning, where the latter can be regarded as an offline clustering. It shows that with prompt learning, the probability of an image/text prompt pair is more accurately estimated. Since prompt learning helps reduce label noise by refining the prompt, the result without prompt learning shows limited clustering performance, which is verified in Tab. 4 (E).\\n\\nIn addition, we further analyze the effect of when to start prompt learning Tab. 5. The stage-wise learning, where we start prompt learning after convergence of the generative model, shows better performance than end-to-end learning with prompt learning. The reason is that the well-generated image corresponding to a given prompt has a critical role in prompt learning. Note that the base prompt is also updated with prompt learning simultaneously.\\n\\n5. Conclusion\\n\\nIn this paper, we propose LANguage-driven Image-to-image Translation framework for Unlabeled Data (LANIT) that leverages dataset-level supervision. By using textual domain descriptions as dataset-level supervision, our model not only mitigates the heavy dependency on per-sample supervision but also considers multi-hot domain labels for an image. The proposed labeling method allows users to explicitly define the semantic meaning of the target domain, which was not possible in the previous truly-unsupervised I2I framework. In addition, as we observe that a naive adoption of a pre-trained vision-language model has limited controllability and even drops the performance of the model, we present some techniques to overcome such limitations: adaptive thresholding, domain regularization loss, and prompt learning. Our experiments show that our LANIT performs better than truly-unsupervised methods in controllability on the application. Furthermore, the quantitative results of our framework have scored competitive or even better values than the existing image-to-image translation models that require per-sample supervision.\\n\\nAcknowledgements.\\n\\nThis research was supported by the MSIT, Korea (IITP-2023-2020-0-01819, ICT Creative Consilience program, No. 2020-0-00368, A Neural-Symbolic Model for Knowledge Acquisition and Inference Techniques), and National Research Foundation of Korea (NRF2021R1C1C1006897).\"}"}
