{"id": "CVPR-2022-1023", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learning Non-target Knowledge for Few-shot Semantic Segmentation\\n\\nYuanwei Liu\\nNian Liu*\\nQinglong Cao\\nXiwen Yao\\nJunwei Han\\nLing Shao\\n\\n1 Northwestern Polytechnical University\\n2 Inception Institute of Artificial Intelligence\\n3 Terminus Group, China\\n\\n{liuyuanwei9809, liunian228, caoql19980603, yaoxiwen517, junweihan2010} @gmail.com\\nling.shao@ieee.org\\n\\nAbstract\\nExisting studies in few-shot semantic segmentation only focus on mining the target object information, however, of\u00adten are hard to tell ambiguous regions, especially in non\u00adtarget regions, which include background (BG) and Dis\u00adtracting Objects (DOs). To alleviate this problem, we pro\u00adpose a novel framework, namely Non-Target Region Elim\u00adinating (NTRE) network, to explicitly mine and eliminate BG and DO regions in the query. First, a BG Mining Module (BGMM) is proposed to extract the BG region via learning a general BG prototype. To this end, we design a BG loss to supervise the learning of BGMM only using the known target object segmentation ground truth. Then, a BG Eliminating Module and a DO Eliminating Module are proposed to successively filter out the BG and DO in\u00adformation from the query feature, based on which we can obtain a BG and DO-free target object segmentation result. Furthermore, we propose a prototypical contrastive learning algorithm to improve the model ability of distinguish\u00ading the target object from DOs. Extensive experiments on both PASCAL-5i and COCO-20i datasets show that our ap\u00adproach is effective despite its simplicity. Code is available at https://github.com/LIUYUANWEI98/NERTNet\\n\\n1. Introduction\\nDue to the rapid development of fully convolutional net\u00adwork (FCN) architectures, deep learning has made milestone progress in semantic segmentation. Most meth\u00adods adopt the fully-supervised learning scheme and require a mass of annotated data for training. Although they can achieve good performance, their data-hungry nature de\u00admands time and labor-consuming image annotations. To alleviate this problem, few-shot semantic segmentation was proposed to segment unseen object classes in query images with only a few annotated samples, namely supports. Currently, there are many existing researches exploring various deep learning methods for few-shot semantic seg\u00admentation [14, 20, 29, 31, 33, 42]. They usually extract fea\u00adtures from both query and support images first, and then extract the class-specific representation using the support masks. Finally, a matching network is leveraged to segment the target object in the query image using the class repre\u00adsented by the support images.\\n\\nMost typically, prototypical learning methods [6, 31, 33, 42, 44] use masked average pooling (MAP) on the target object regions of the support images to form a single or a few prototypes. Then, prototypes are used to segment the target object in the query image via conducting dense feature matching.\\n\\nFigure 1. Previous methods often show false positive predictions in non-target regions. Pixels in red indicate the target objects, while pixels in green mean false positive predictions.\\n\\nFigure 2. Comparison between existing framework and ours for few-shot segmentation. The main difference is that the former only mines target category information, while we propose to eliminate co-existing pixels belonging to non-target regions, including the background (BG) and distracting objects (DO).\"}"}
{"id": "CVPR-2022-1023", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Although some achievements have been made, these methods all focus on digging out more effective target information from supports as much as possible, and then transferring them to the query image to achieve segmentation (see Figure 2 (a)). However, as illustrated in Figure 1, they often suffer from the false positive prediction in backgrounds (BG) and co-existing objects belonging to other classes, namely, distracting objects (DOs). The main reason is that solely focusing on target objects in the few-shot setting makes their models hard on learning discriminative features and differentiating ambiguous regions.\\n\\nTo alleviate this problem, we rethink the few shot semantic segmentation task from a new perspective, that is, mining and excluding non-target regions, i.e., BG and DO regions, rather than directly segmenting the target object. From this point, in this paper, we propose a novel framework, namely non-target region eliminating (NTRE) network for few-shot semantic segmentation. As shown in Figure 2 (b), we first develop a BG mining module (BGMM) to obtain a BG prototype and segment the BG region. Then, a BG eliminating module (BGEM) is proposed to filter out the BG information from the query feature. Next, the target prototype from the support is utilized in a matching network to activate the target object in the query feature. Subsequently, we adopt a DO eliminating module (DOEM) to mine the DO region first and then filter out the DO information from the query feature. As such, finally, we can obtain an accurate target segmentation result without the distraction from the BG and DO regions.\\n\\nIn the BGMM, obtaining the BG prototype is not straightforward as we obtain the support prototype. Considering that BG regions universally exist in almost every image, such as sky, grass, walls, and etc, we propose to learn a general BG prototype from both query and support images in the training set. Based on this prototype, we can segment the BG regions for all images easily. Since having no ground truth BG segmentation masks to supervise the model learning, we specifically design a BG mining loss based on the known target segmentation masks.\\n\\nFurthermore, considering that it's hard to learn a good prototype feature embedding space to differentiate DOs from the target object under the few-shot setting, we propose the prototypical contrastive learning (PCL) method to improve the object-discrimination ability of the network by refining the prototype feature embeddings. Specifically, for a query target prototype, we treat the corresponding support target prototype as the positive sample, while the DO prototypes both in query and support are considered as negative samples. We then propose a PCL loss to enforce the prototype embeddings to be similar within the target prototypes and dissimilar between target and DO prototypes. As such, the PCL could effectively help the network distinguish target objects from DOs.\\n\\nIn summary, our contributions are as follows:\\n\u2022 To the best of our knowledge, this is the first time to mine and eliminate non-target regions, including BG and DOs, for few-shot semantic segmentation, which can effectively decrease false positive predictions.\\n\u2022 We propose the BGMM, BGEM, and DOEM for effectively implementing the mining and eliminating of the BG and DO regions. A novel BG mining loss is also proposed for training the BGMM without using BG ground truth.\\n\u2022 We propose a PCL method to improve the model ability for better distinguishing target objects from DOs.\\n\u2022 Extensive experiments on PASCAL-5 and COCO-20 show that our proposed framework yields a new state-of-the-art performance, especially on the 1-shot setting.\\n\\n2. Related Works\\nSemantic Segmentation. Compared with convolutional neural networks (CNNs) [13], the emergence of the FCN [21] has brought great progress to the semantic segmentation task. Specifically, the fully connected layers in CNNs are replaced by fully convolutional layers to enable pixel-level prediction. Based on the FCN, various network architectures are proposed to tackle the semantic segmentation problem in recent works. For example, [5,8,15,30,40,43,46] propose various attention mechanisms embedded in the FCN architecture. Some other works utilize different feature fusion methods, such as the pyramid pooling module [45], dilated convolution kernels [2], multi-scale feature aggregation [10], dense atrous spatial pyramid pooling (ASPP) [39]. However, these traditional semantic segmentation networks are powerless when dealing with unseen categories. Meanwhile, training such networks are computationally costly and also requires labor-consuming pixel-level annotations on large-scale data.\\n\\nFew-shot Semantic Segmentation. Few-shot semantic segmentation aims to segment unseen object classes in query images with only a few annotated samples. There are two mainstream frameworks to segment the target objects in query images. One is the pixel-level matching framework, which is firstly proposed by [26]. This framework usually generates the target object prototype from the support features first, and then segments the query using dense feature matching. Another is the pixel-level measurement framework, proposed by [44], which measured the embedding similarity between the query and the supports. Following the first framework, CANet [42] utilized an Iterative Optimization Module (IOM) to refine the prediction progressively after concatenating the support prototype and the query features. PFENet [31] proposed a prior mask by calculating the cosine similarity between the support and query features.\"}"}
{"id": "CVPR-2022-1023", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ASGNet [14] proposed a superpixel-guided clustering method to obtain multi-part prototypes from the support and used an allocation strategy to reconstruct the support feature map instead of using prototype expanding. Following the second framework, PANet [33] embedded different object classes into different prototypes with a pre-trained encoder. Then, the query image was labeled based on the distance between the representations of the query image and the prototypes. Yang et al. [38] proposed a novel joint-training framework via introducing additional base category prototypes to mine latent novel classes during training. Most of these previous methods focus on directly segmenting the target object. Differently, in this paper, we are the first to propose leveraging complementary non-target knowledge and eliminating distracting regions for few-shot segmentation.\\n\\nContrastive Learning\\nMost previous computer vision researches focus on designing artificially preferred network architectures to tackle various computer vision tasks. The emergence of contrastive learning [11] brings our focus back to mining better deep feature representations via contrasting positive and negative samples. SimCLR [3] proposed a simple self-supervised contrastive learning paradigm by using different data augmentation methods to form positive and negative samples for each image instance. MoCo [4, 11] proposed to store negative samples using a dynamically updated queue, in which only the stored feature vectors from recent batches are used for training. As such, MoCo solved the inconsistency problem of the sampled features due to the optimization to the encoder. Very recently, Wang et al. [34] introduced contrastive learning in supervised semantic segmentation and proposed a pixel-wise contrastive algorithm. They treated the pixel embeddings of the same class and different classes as positive and negative samples, respectively. Different from them, in our work, we propose the PCL scheme to improve the objects-discrimination ability of the extracted prototypes, which could effectively help the network distinguish target objects from DOs.\\n\\n3. Proposed Method\\n3.1. Problem Definition\\nFew-shot semantic segmentation aims to train a model on base classes, and segment unseen objects in query images with a few annotated support samples without retraining. Typically, all the datasets are divided into two subsets. One is the training set \\\\( D_{\\\\text{base}} \\\\) with the base classes \\\\( C_{\\\\text{base}} \\\\). The other is the testing set \\\\( D_{\\\\text{novel}} \\\\) with the novel classes \\\\( C_{\\\\text{novel}} \\\\). These two sets of classes are disjoint, i.e., \\\\( C_{\\\\text{base}} \\\\cap C_{\\\\text{novel}} = \\\\emptyset \\\\). Specifically, the training set \\\\( D_{\\\\text{base}} \\\\) is partitioned into several episodes after randomly sampling \\\\( K + 1 \\\\) image-mask pairs that contain objects from a specific class in \\\\( C_{\\\\text{base}} \\\\). The testing set is composed of similar episodes, except that the data are sampled from the \\\\( C_{\\\\text{novel}} \\\\). For one episode, \\\\( K \\\\) image-mask pairs are treated as the support set \\\\( S = \\\\{ (I_s^i, M_s^i) \\\\}_{i=1}^K \\\\) to segment the objects of the target class in the remaining one sample, which is termed the query set \\\\( Q = \\\\{ (I_q^i, M_q^i) \\\\} \\\\). Here, \\\\( I \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 3} \\\\) indicates the RGB image and \\\\( M \\\\in \\\\mathbb{R}^{H \\\\times W} \\\\) indicates the corresponding mask. \\\\( Q = \\\\{ (I_q^i, M_q^i) \\\\} \\\\) is provided with the ground truth only during training. Following this episode, the network is trained on \\\\( D_{\\\\text{base}} \\\\) and evaluated on \\\\( D_{\\\\text{novel}} \\\\).\\n\\n3.2. Overview\\nAs aforementioned, few-shot semantic segmentation models usually fail in ambiguous non-target regions. Motivated by this observation, we propose to mine and eliminate non-target regions, which include the background (BG) regions and the distracting object (DO) regions.\\n\\nWe first follow previous methods and use a pre-trained backbone to extract the query feature map \\\\( X_s \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C} \\\\) and the support feature map \\\\( X_q \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C} \\\\) from corresponding images, respectively. Then, the BG mining module (BGMM) is proposed to mine the BG region via learning a general BG prototype, which is randomly initialized and subsequently learned on the training set. We also propose a novel BG loss without using accurate BG segmentation ground truth. Next, a BG eliminating module (BGEM) is utilized to filter out the BG information from the query feature. Subsequently, we follow prototypical learning to activate the target region in the query feature using the support target prototype and feature matching (FM). An initial target object segmentation mask can be further obtained. After that, a DO eliminating module (DOEM) is proposed to filter out DO information from the query feature. The DO region can be first mined by combining the BG segmentation map and the initial target prediction. Then, the DO prototype can be obtained from the query feature and used for DO elimination. The prototypical contrastive learning (PCL) is also adopted for better discriminating the target object from DOs. Finally, a segmentation network is used to achieve the BG and DO-free prediction.\\n\\n3.3. Background Mining and Eliminating\\n3.3.1 Background Mining Module\\nBackground regions, in which no obvious objects appear, commonly exist in most images. Based on this commonality, we propose to use a BG prototype to encode general BG knowledge. It can be represented as \\\\( P_{\\\\text{BG}} \\\\in \\\\mathbb{R}^{1 \\\\times 1 \\\\times D} \\\\), where \\\\( D \\\\) is the channel dimension. Inspired by some saliency detection methods [17, 18], it is feasible to learn \\\\( P_{\\\\text{BG}} \\\\) on a large number of images and then use it to detect BG regions on any natural image. Hence, in this paper, we first randomly initialize \\\\( P_{\\\\text{BG}} \\\\)....\"}"}
{"id": "CVPR-2022-1023", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Overall architecture of the proposed method for few-shot semantic segmentation. Our network is composed of four parts. After extracting features from both the support and query images via a pre-trained backbone, our Background Mining Module (BGMM) is performed to obtain a BG prototype and segment the BG regions. Meanwhile, Background Eliminating Module (BGEM) is performed to eliminate the BG regions. The third part is to obtain the activated query feature and further an initial target prediction via Feature Matching (FM). The last part is to eliminate the distracting objects by our proposed Distracting Objects Eliminating Module (DOEM).\\n\\n\\\\[ y_{q}^{BG} = F_{3 \\\\times 3}(X_{q} \\\\oplus \\\\hat{P}_{BG}), \\\\]\\n\\\\[ y_{s}^{BG} = F_{3 \\\\times 3}(X_{s} \\\\oplus \\\\hat{P}_{BG}), \\\\]\\n\\nwhere \\\\( \\\\oplus \\\\) denotes the concatenation operation along the channel dimension and \\\\( F_{3 \\\\times 3} \\\\) is composed of two \\\\( 3 \\\\times 3 \\\\) convolutional layers and shares the same weights in both (1) and (2).\\n\\n**Background Mining Loss.** In the few-shot semantic segmentation task, we only have the ground truth of target object masks, i.e., \\\\( M_{q} \\\\) and \\\\( M_{s} \\\\), and have no BG region ground truth. In order to force \\\\( P_{BG} \\\\) effectively predict the BG region as we expected, we design a BG mining loss to optimize this learning process as below:\\n\\n\\\\[ L_{BG} = -\\\\frac{1}{N} \\\\sum_{i} \\\\log(1 - y_{q/s}^{BG}(i)) M_{q/s}(i) - \\\\alpha \\\\frac{1}{Z} \\\\sum_{j} \\\\log(y_{q/s}^{BG}(j)), \\\\]\\n\\nwhere \\\\( i \\\\) and \\\\( j \\\\) are the indexes of the spatial locations. \\\\( M_{q/s} \\\\) is the ground truth of target objects belonging to query or support. \\\\( N \\\\) denotes the total number of the target object pixels and \\\\( Z \\\\) is equal to \\\\( H \\\\times W \\\\). \\\\( \\\\alpha \\\\) is a hyperparameter to weight the second term. The core idea of this loss is that the BG prediction should belong to the reverse region of the target object, i.e., predicting zero in \\\\( y_{q/s}^{BG} \\\\) for the pixels that belong to the target object. However, solely using this constraint may lead to a trivial solution that predicts all zeros for \\\\( y_{q/s}^{BG} \\\\). To alleviate this problem, we add the second term as a regularization to force the module must predict valid BG regions for every image.\\n\\n### 3.3.2 Background Eliminating Module\\n\\nWe further use the expanded BG prototype \\\\( \\\\hat{P}_{BG} \\\\) to filter out the BG information from the query feature map via prototypical learning. Specifically, we first concatenate \\\\( \\\\hat{P}_{BG} \\\\) with \\\\( X_{q} \\\\) and then use a convolutional layer \\\\( F_{1 \\\\times 1}(\\\\cdot) \\\\) to exclude the BG information in the query. The whole process can be denoted as:\\n\\n\\\\[ X_{q}^{BG} = F_{1 \\\\times 1}(X_{q} \\\\oplus \\\\hat{P}_{BG}), \\\\]\\n\\nwhere \\\\( X_{q}^{BG} \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C} \\\\) denotes the BG-filtered query feature and \\\\( F_{1 \\\\times 1}(\\\\cdot) \\\\) denotes a \\\\( 1 \\\\times 1 \\\\) convolutional layer.\\n\\n### 3.4. Support Feature Matching\\n\\nFollowing previous methods, we further use dense feature matching to activate the target object region on the \\\\( X_{q}^{BG} \\\\). Concretely, masked average pooling (MAP) is first used on the support feature map \\\\( X_{s} \\\\) to get the support prototype \\\\( P_{s} \\\\in \\\\mathbb{R}^{1 \\\\times 1 \\\\times C} \\\\). Then, it is expanded to \\\\( \\\\hat{P}_{s} \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C} \\\\) and concatenated with the BG-filtered query feature.\"}"}
{"id": "CVPR-2022-1023", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We also follow [31] and introduce a prior confidence map \\\\( C_p \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 1} \\\\) via computing the maximum similarity score at pixel-level. After that, we obtain the activated query feature \\\\( X_{q_{act}} \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C} \\\\) and further achieve the initial target object prediction \\\\( y_{q_{ini}} \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 1} \\\\):\\n\\n\\\\[\\nX_{q_{act}} = F_1 \\\\times 1( X_{q_{BG}} \\\\oplus \\\\hat{P}_s \\\\oplus C_p ), \\\\quad (5)\\n\\\\]\\n\\n\\\\[\\ny_{q_{ini}} = F_3 \\\\times 3( X_{q_{act}} ), \\\\quad (6)\\n\\\\]\\n\\nwhere \\\\( F_1 \\\\times 1( \\\\cdot ) \\\\) is the same as in (4) and \\\\( F_3 \\\\times 3( \\\\cdot ) \\\\) is the same as in (1).\\n\\n### 3.5. Distracting Objects Eliminating\\n\\n#### 3.5.1 Distracting Object Eliminating Module\\n\\nAlthough we have eliminated BG information in \\\\( X_{q_{act}} \\\\), it may still suffer from the distraction of DOs. To this end, we design the DOEM to further filter out DO information from \\\\( X_{q_{act}} \\\\) for more accurate target object prediction. To be specific, we mine the potential DO region in the query based on the known BG region in \\\\( y_{q_{BG}} \\\\) and the target object region in \\\\( y_{q_{ini}} \\\\). Intuitively, the DO region is complementary to the union of the BG region and target region. Hence, we have:\\n\\n\\\\[\\nY_{q_{DO}} = 1 - (Y_{q_{BG}} \\\\cup Y_{q_{ini}}), \\\\quad (7)\\n\\\\]\\n\\nwhere \\\\( Y_{q_{DO}} \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 1} \\\\) denotes the DO mask. \\\\( Y_{q_{BG}} \\\\) and \\\\( Y_{q_{ini}} \\\\) are the binary maps corresponding to \\\\( y_{q_{BG}} \\\\) and \\\\( y_{q_{ini}} \\\\), respectively.\\n\\nNext, we utilize \\\\( Y_{q_{DO}} \\\\) to obtain the DO prototype \\\\( P_{q_{DO}} \\\\in \\\\mathbb{R}^{1 \\\\times 1 \\\\times C} \\\\) via performing MAP on the query feature map:\\n\\n\\\\[\\nP_{q_{DO}} = P_{X_{q_{act}}} \\\\otimes Y_{q_{DO}} \\\\sum_{Y_{q_{DO}}}, \\\\quad (8)\\n\\\\]\\n\\nwhere \\\\( \\\\otimes \\\\) denotes the element-wise multiplication and the summation sums over all spatial locations.\\n\\nAfter that, we expand the \\\\( P_{q_{DO}} \\\\) into \\\\( \\\\hat{P}_{q_{DO}} \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C} \\\\) and combine it with the activated query feature map \\\\( X_{q_{ini}} \\\\) to eliminate the DO information. Finally, the combined feature is passed into a segmentation network, for which we use the Feature Enrichment Module (FEM) in [31], to obtain the BG and DO-free prediction:\\n\\n\\\\[\\ny_{q} = \\\\text{Seg}(X_{q_{ini}} \\\\oplus \\\\hat{P}_{q_{DO}}), \\\\quad (9)\\n\\\\]\\n\\nwhere \\\\( y_{q} \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 1} \\\\) is the final target object segmentation result of our whole model.\\n\\n#### 3.5.2 Prototypical Contrastive Learning\\n\\nThe DOEM only cares about the DO mask \\\\( Y_{q_{DO}} \\\\) in the DO eliminating process. However, a good DO eliminating model requires not only accurate DO masks, but also good prototype feature embeddings that can differentiate the target objects from DOs easily. Inspired by recent research on contrastive learning, we propose the prototypical contrastive learning (PCL) method to refine the feature embeddings of different prototypes. With the help of PCL, we want to make the prototype features between the target objects and the DOs more discriminative, and the prototypes between the target objects of the query and the support more similar.\\n\\nTo this end, we need to obtain the target prototypes and DO prototypes for both the query and the support first. For the target prototypes, we have obtained it for the support, i.e., \\\\( P_s \\\\), from Section 3.4. For the query image, we first binarize the final target prediction \\\\( y_{q} \\\\) as the target mask and then adopt MAP on the query feature to generate the target prototype of the query \\\\( P_q \\\\in \\\\mathbb{R}^{1 \\\\times 1 \\\\times C} \\\\). As for the DO prototypes, we have computed it for the query, i.e., \\\\( P_{q_{DO}} \\\\), in (8). For the support image, we adopt the same workflow to compute the DO prototype of the support \\\\( P_s_{DO} \\\\), i.e., using the target mask \\\\( M_s \\\\) and the BG mask \\\\( Y_{s_{BG}} \\\\) to generate the DO mask, and then conducting MAP on the support feature.\\n\\n**Prototypical Contrastive Learning Loss.** According to the paradigm of contrastive learning, we propose a PCL loss to optimize the above prototype feature embeddings. For the query prototype \\\\( P_q \\\\), we treat the corresponding support prototype \\\\( P_s \\\\) as the positive sample, while the DO prototypes in both query and support as negative samples. Considering that a large number of negative samples is indispensable for contrastive learning, we build a DO prototype bank \\\\( B \\\\) to store the embeddings of 2000 DO prototypes in the latest batches during training. Note that they can be sampled across different episodes of the same class. At last, inspired by InfoNCE [23], we propose our PCL loss:\\n\\n\\\\[\\nL_{PCL} = -\\\\log e^{\\\\cos(P_q, P_s)} \\\\sum_{P_q \\\\in P_{q_{DO}}} e^{\\\\cos(P_q, P_{q_{DO}})} + e^{\\\\cos(P_q, P_{s_{DO}})}, \\\\quad (10)\\n\\\\]\\n\\nwhere \\\\( \\\\cos(\\\\cdot, \\\\cdot) \\\\) denotes the cosine similarity.\\n\\n### 3.6. Total Training Loss\\n\\nWe use two binary cross-entropy losses to supervise the training of the initial target prediction \\\\( y_{q_{ini}} \\\\) and the final prediction \\\\( y_{q} \\\\), composing the target segmentation loss \\\\( L_T \\\\). Finally, our total training loss includes \\\\( L_T \\\\), the BG loss \\\\( L_{BG} \\\\) in (3), and the PCL loss \\\\( L_{PCL} \\\\) in (10):\\n\\n\\\\[\\nL = \\\\beta L_T + \\\\lambda L_{BG} + \\\\gamma L_{PCL}, \\\\quad (11)\\n\\\\]\\n\\n\\\\[\\nL_T = \\\\text{BCE}(y_{q_{ini}}, M_q) + \\\\text{BCE}(y_q, M_q), \\\\quad (12)\\n\\\\]\\n\\nwhere \\\\( \\\\text{BCE} \\\\) denotes the binary cross-entropy loss and \\\\( \\\\beta, \\\\lambda, \\\\gamma \\\\) are adjustable loss weights.\"}"}
{"id": "CVPR-2022-1023", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Qualitative results of our proposed NTRENet and PFENet. From left to right: support images, query images, prediction of PFENet, prediction of NTRENet.\\n\\nFrom SDS [9]. The total 20 categories are partitioned into 4 folds as in [33] for cross validation and each fold contains 5 categories. COCO-20i is a larger datasets based on the MSCOCO [16] dataset. Similar to PASCAL-5i, the total 80 categories are also partitioned into 4 folds for cross validation, where each fold includes 20 categories. For both the datasets, we test on 1 fold and train on the remaining 3 folds.\\n\\nEvaluation Metrics. Following previous methods [19,20,26,27], we adopt the class mean intersection over union (mIoU) as a primary evaluation metric for ablation studies and comparisons. In addition, we report the results of foreground-background IoU (FB-IoU), which only cares about the performance on target and non-target regions instead of differentiating categories, for a more comprehensive comparison. The precision, whose formulation follows $\\\\frac{TP}{TP + FP}$, also be leveraged to report our modules performance on decreasing false positives.\\n\\n4.2. Implementation Details\\n\\nFollowing previous works, we respectively use ResNet-50, ResNet-101 [12], and VGG-16 [28] as the backbones to construct our network for fair comparisons. These backbones are all pre-trained on the ImageNet classification task and their weights are fixed during training. Our network is implemented using PyTorch [24] and all the experiments are conducted on one NVIDIA RTX 3090 GPU. We use random scaling, horizontal flipping, and random rotation within [-10,10] degrees as data augmentation to increase the training data. Finally, we randomly crop images and masks with the size of $473 \\\\times 473$ to train our model. During training, we use SGD as our optimizer, where the initial learning rate, batch size, weight decay, and momentum are set as 0.03, 32, 0.0001, and 0.9, respectively. We train our model for 200 epochs on PASCAL-5i and 50 epochs on COCO-20i, respectively. The learning rate is decayed using the polynomial annealing policy with the power set to 0.9. During the evaluation, we follow [38] to randomly sample 1000 support-query pairs on PASCAL-5i and 4000 pairs on COCO-20i, respectively.\\n\\n4.3. Comparison with State-of-the-art Methods\\n\\nPASCAL-5i. Table 1 shows the performance comparison on PASCAL-5i between our method and several representative models. We can see that, on all the three backbones (i.e., VGG-16, Resnet-50, and Resnet-101), our method outperforms all previous models by a large margin in terms of both mIoU and FB-IoU. Specifically, under the 1-shot setting, the averaged mIoU scores of our method are 59.0, 64.2, and 63.7 on the VGG-16, Resnet-50, and Resnet-101 backbones, respectively, surpassing state-of-the-art results by 1.2%, 3.4%, and 1.8%, respectively. Meanwhile, in terms of FB-IoU, our method outperforms the previous best results by 1.1%, 5.0%, and 3.3% on the three backbones, respectively. Under the 5-shot setting, our method only obtains the best results on the VGG-16 backbone in terms of mIoU, but outperforms previous state-of-the-art FB-IoU results by 2.6%, 5.7%, and 1% on all three backbones, respectively.\\n\\nCOCO-20i. Although COCO-20i is a more challenging dataset with a large number of images with realistic scenes, we still obtain superior performance, which is shown in Table 2. Here we follow previous works and only use the Resnet-50 and Resnet-101 backbones. Table 2 shows that our method respectively yields the averaged mIoU scores of 39.3 and 39.1 on the two backbones under the 1-shot setting, outperforming previous best results by a large margin of 4.8% and 5.7%, respectively. In the 5-shot setting, the FB-IoU results also verify the superiority of our method, despite the challenging scenarios.\\n\\nLimitations. We find that the averaged mIoU results of our model do not achieve obvious advantages in the 5-shot setting, compared with previous state-of-the-arts. We argue that this is reasonable since our method mainly focuses on eliminating non-target regions instead of segmenting the target object. As such, although the number of support samples increases from one to five, it does not introduce additional non-target information to our method. However, we hope our work could provide a novel perspective on the opposite side of traditional methods for future works.\\n\\nQualitative Comparison. We show the qualitative comparison of the predicted segmentation masks generated by our method and a typical traditional model that focuses on segmenting the target object, i.e., PFENet [31], in Figure 4. We can see that PFENet could not segment the target objects accurately due to the distraction of non-target regions.\"}"}
{"id": "CVPR-2022-1023", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Class mIoU and FB-IoU results of four folds on PASCAL-$i$.\\n\\n| Backbone | Methods     | 1-Shot | 5-Shot | Mean FB-IoU | 1-Shot | 5-Shot | Mean FB-IoU |\\n|----------|-------------|--------|--------|-------------|--------|--------|-------------|\\n|          |             | Fold-0 | Fold-1 | Fold-2      | Fold-3 |        |             |\\n| VGG-16   | OSLSM [26]  | 33.6   | 55.3   | 40.9        | 33.5   | 40.8   | 61.3        |\\n|          | co-FCN [25] | 36.7   | 50.6   | 44.9        | 32.4   | 41.1   | 60.1        |\\n|          | RPMM [37]   | 47.1   | 65.8   | 50.6        | 48.5   | 53.0   | -           |\\n|          | PFENet [31] | 56.9   | 68.2   | 54.4        | 52.4   | 58.0   | 72.3        |\\n|          | MMNet [35]  | 57.1   | 67.2   | 56.6        | 52.3   | 58.3   | -           |\\n|          | NTRENet     | 57.7   | 67.6   | 57.1        | 53.7   | 59.0   | 73.1        |\\n| ResNet-50| CANet [42]  | 52.5   | 65.9   | 51.3        | 51.9   | 55.4   | 66.2        |\\n|          | RPMM [37]   | 55.2   | 66.9   | 52.6        | 50.7   | 56.3   | -           |\\n|          | PFENet [31] | 61.7   | 69.5   | 55.4        | 56.3   | 60.8   | 73.3        |\\n|          | SCL [41]    | 63.0   | 70.0   | 56.5        | 57.7   | 61.8   | 71.9        |\\n|          | ASGNet [14] | 58.8   | 67.9   | 56.8        | 53.7   | 59.3   | 71.7        |\\n|          | ReRPI [1]   | 59.8   | 68.3   | 62.1        | 48.5   | 59.7   | -           |\\n|          | SAGNN [36]  | 64.7   | 69.6   | 57.0        | 57.2   | 62.1   | 73.2        |\\n|          | MLC [38]    | 59.2   | 71.2   | 65.6        | 52.5   | 62.6   | -           |\\n|          | NTRENet     | 65.4   | 72.3   | 59.4        | 59.8   | 64.2   | 77.0        |\\n\\nTable 2. Class mIoU and FB-IoU results of four folds on COCO-$i$.\\n\\n| Backbone | Methods     | 1-Shot | 5-Shot | Mean FB-IoU | 1-Shot | 5-Shot | Mean FB-IoU |\\n|----------|-------------|--------|--------|-------------|--------|--------|-------------|\\n|          |             | Fold-0 | Fold-1 | Fold-2      | Fold-3 |        |             |\\n| ResNet-50| PPNet [20]  | 28.1   | 30.8   | 29.5        | 27.7   | 29.0   | -           |\\n|          | RPMM [37]   | 29.5   | 36.8   | 28.9        | 27.0   | 30.6   | -           |\\n|          | ASGNet [14] | -      | -      | -           | -      | 34.6   | 60.4        |\\n|          | MMNet [35]  | 34.9   | 41.0   | 37.2        | 37.0   | 37.5   | -           |\\n|          | MLC [38]    | 46.8   | 35.3   | 26.2        | 27.1   | 33.9   | -           |\\n|          | NTRENet     | 36.8   | 42.6   | 39.9        | 37.9   | 39.3   | 68.5        |\\n| ResNet-101| DAN [32]    | -      | -      | -           | -      | 24.4   | 62.3        |\\n|          | SCL [41]    | 36.4   | 38.6   | 37.5        | 35.4   | 37.0   | -           |\\n|          | PFENet [31] | 34.3   | 33.0   | 32.3        | 30.1   | 32.4   | 58.6        |\\n|          | MLC [38]    | 50.2   | 37.8   | 27.1        | 30.4   | 36.4   | -           |\\n|          | SAGNN [36]  | 36.1   | 41.0   | 38.2        | 33.5   | 37.2   | 60.9        |\\n|          | NTRENet     | 38.3   | 40.4   | 39.5        | 38.1   | 39.1   | 67.5        |\\n\\nHowever, our proposed NTRENet can obtain much more accurate results with much fewer false positive predictions in BG and DO regions, thus clearly demonstrating the effectiveness of our proposed method.\\n\\n4.4. Ablation Study\\n\\nEffectiveness of Different Modules.\\n\\nWe conduct extensive ablation studies on PASCAL-$i$ in the 1-shot setting to validate the effectiveness of our proposed key modules, i.e., BGEM, DOEM, and PCL. We remove these three modules from our NTRENet as the baseline model, which only uses the support prototype to directly segment the target object as in [31]. As Table 3 shows, eliminating the BG regions using BGEM achieves as large as 4% performance improvement compared to the baseline model. Meanwhile, using DOEM to mine and eliminate DO regions obtains another 2% performance gain. Finally, using the PCL scheme to boost the model capability of discriminating different objects leads to further 1% performance improvement. These results clearly demonstrate the effectiveness of our proposed BGEM, DOEM, and PCL. In addition, we use precision to verify the performance of our method on decreasing false positives. The results show that our full method achieves 3% precision improvement compared to the baseline model and all of the proposed BGEM, DOEM, and PCL can progressively improve the precision, i.e., decrease false positives.\\n\\nChoice of negative samples in PCL.\\n\\nThe whole non-target region includes both BG and DO regions. Hence, we choose other alternative negative samples, i.e., prototypes generated from the whole non-target region and the BG region, respectively, to apply in PCL. We conduct comparative studies to further validate the effectiveness of our method.\"}"}
{"id": "CVPR-2022-1023", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Visualization of different ablative results. From left to right: Support images, the results of baseline, BG prediction, the results of only using BGEM, DO prediction, the results of using BGEM+DOEM, the results of using BGEM+DOEM+PCL (i.e., full model), Ground truth.\\n\\nTable 4. Comparison of the choice of negative samples in PCL. mIoU results are reported on the PASCAL-5i dataset under the 1-shot setting.\\n\\n| Negative samples     | split0 | split1 | split2 | split3 | mean  |\\n|----------------------|--------|--------|--------|--------|-------|\\n| Non-target Prototypes| 63.7   | 69.0   | 57.8   | 56.8   | 61.8  |\\n| BG Prototypes        | 64.4   | 68.7   | 58.4   | 58.6   | 62.6  |\\n| DO Prototypes        | 65.4   | 72.3   | 59.4   | 59.8   | 64.2  |\\n\\nIn Table 4, the results show that DO prototypes work more effectively than others since DO regions are more confusing and thus play a role of hard negative samples.\\n\\nQualitative Comparison. We further show some qualitative results in Figure 5 to prove the effectiveness of our proposed BGEM, DOEM, and PCL in an intuitionistic way. Column 2 shows predictions from baseline. In column 3, we show the BG prediction masks obtained from BGMM. We find that our BGMM can effectively mine the universal BG regions in query. Column 5 reveals the predicted masks of DO regions in DOEM. Columns 4 and 6 show that using BGEM and DOEM can effectively help eliminate the BG and DO regions compared with the baseline results. Finally, column 7 indicates that using PCL can further discriminate the target objects from DOs in detail.\\n\\nInfluence of the Channel Dimension of the BG Prototype. The channel dimension of the BG prototype is crucial since it determines how much general BG information it can encode. We conduct ablation experiments to explore its optimal value and the results are shown in Figure 6. It shows that using 512 channels achieves the best performance for fold 2 and 3, while using 640 channels outperforms other values for fold 0, 1, and the mean result. Hence, we use 640 channels in the BG prototype in our network setting.\\n\\n5. Conclusion\\n\\nWe address the few-shot semantic segmentation from a new perspective and propose a novel NTRE framework to pay attention to BG and DO regions. We propose the BGMM, BGEM, and DOEM for effectively implementing the mining and eliminating to the BG and DOs. Particularly, the BG mining loss is proposed to supervise the learning of the BGMM and a BG prototype without using BG ground truth. Besides, PCL is proposed to improve the model ability for better distinguishing target objects from DOs. Extensive experiments on two benchmark datasets demonstrate the performance superiority of our method over the previous methods.\\n\\nAcknowledgments: This work was supported in part by the Key-Area Research and Development Program of Guangdong Province under Grant 2019B010110001, and the National Natural Science Foundation of China under Grants 62071388, 62136007, U20B2065 and 62036005, the Key R&D Program of Shaanxi Province under Grant 2021ZDLGY01-08, and the National Key R&D Program of China under Grant 2020AAA0105701.\"}"}
{"id": "CVPR-2022-1023", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Malik Boudiaf, Hoel Kervadec, Ziko Imtiaz Masud, Pablo Piantanida, Ismail Ben Ayed, and Jose Dolz. Few-shot segmentation without meta-learning: A good transductive inference is all you need? In CVPR, pages 13979\u201313988, 2021.\\n\\n[2] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, pages 801\u2013818, 2018.\\n\\n[3] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. pages 1597\u20131607. PMLR, 2020.\\n\\n[4] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.\\n\\n[5] Sungha Choi, Joanne T Kim, and Jaegul Choo. Cars can\u2019t fly up in the sky: Improving urban-scene segmentation via height-driven attention networks. In CVPR, pages 9373\u20139383, 2020.\\n\\n[6] Nanqing Dong and Eric P Xing. Few-shot semantic segmentation with prototype learning. In BMVC, volume 3, 2018.\\n\\n[7] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual objects (voc) challenge. International journal of computer vision, 88(2):303\u2013338, 2010.\\n\\n[8] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In CVPR, pages 3146\u20133154, 2019.\\n\\n[9] Bharath Hariharan, Pablo Arbelaez, Lubomir Bourdev, Subhransu Maji, and Jitendra Malik. Semantic contours from inverse detectors. In ICCV, pages 991\u2013998. IEEE, 2011.\\n\\n[10] Junjun He, Zhongying Deng, Lei Zhou, Yali Wang, and Yu Qiao. Adaptive pyramid context network for semantic segmentation. In CVPR, pages 7519\u20137528, 2019.\\n\\n[11] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In CVPR, pages 9729\u20139738, 2020.\\n\\n[12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.\\n\\n[13] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25:1097\u20131105, 2012.\\n\\n[14] Gen Li, Varun Jampani, Laura Sevilla-Lara, Deqing Sun, Jonghyun Kim, and Joongkyu Kim. Adaptive prototype learning and allocation for few-shot segmentation. In CVPR, pages 8334\u20138343, 2021.\\n\\n[15] Xia Li, Zhisheng Zhong, Jianlong Wu, Yibo Yang, Zhouchen Lin, and Hong Liu. Expectation-maximization attention networks for semantic segmentation. In ICCV, pages 9167\u20139176, 2019.\\n\\n[16] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740\u2013755. Springer, 2014.\\n\\n[17] Nian Liu, Junwei Han, and Ming-Hsuan Yang. Picanet: Learning pixel-wise contextual attention for saliency detection. In CVPR, pages 3089\u20133098, 2018.\\n\\n[18] Nian Liu, Ni Zhang, Kaiyuan Wan, Ling Shao, and Junwei Han. Visual saliency transformer. In ICCV, pages 4722\u20134732, 2021.\\n\\n[19] Weide Liu, Chi Zhang, Guosheng Lin, and Fayao Liu. Crnet: Cross-reference networks for few-shot segmentation. In CVPR, pages 4165\u20134173, 2020.\\n\\n[20] Yongfei Liu, Xiangyi Zhang, Songyang Zhang, and Xuming He. Part-aware prototype network for few-shot semantic segmentation. In ECCV, pages 142\u2013158. Springer, 2020.\\n\\n[21] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, pages 3431\u20133440, 2015.\\n\\n[22] Khoi Nguyen and Sinisa Todorovic. Feature weighting and boosting for few-shot segmentation. In ICCV, pages 622\u2013631, 2019.\\n\\n[23] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018.\\n\\n[24] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32:8026\u20138037, 2019.\\n\\n[25] Kate Rakelly, Evan Shelhamer, Trevor Darrell, Alyosha Efros, and Sergey Levine. Conditional networks for few-shot semantic segmentation. 2018.\\n\\n[26] Amirreza Shaban, Shray Bansal, Zhen Liu, Irfan Essa, and Byron Boots. One-shot learning for semantic segmentation. arXiv preprint arXiv:1709.03410, 2017.\\n\\n[27] Mennatullah Siam, Boris N Oreshkin, and Martin Jagersand. Amp: Adaptive masked proxies for few-shot segmentation. In ICCV, pages 5249\u20135258, 2019.\\n\\n[28] Karen Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\\n\\n[29] Jake Snell, Kevin Swersky, and Richard S Zemel. Prototypical networks for few-shot learning. arXiv preprint arXiv:1703.05175, 2017.\\n\\n[30] Andrew Tao, Karan Sapra, and Bryan Catanzaro. Hierarchical multi-scale attention for semantic segmentation. arXiv preprint arXiv:2005.10821, 2020.\\n\\n[31] Zhuotao Tian, Hengshuang Zhao, Michelle Shu, Zhicheng Yang, Ruiyu Li, and Jiaya Jia. Prior guided feature enrichment network for few-shot segmentation. IEEE TPAMI (01):1\u20131, 2020.\\n\\n[32] Haochen Wang, Xudong Zhang, Yutao Hu, Yandan Yang, Xianbin Cao, and Xiantong Zhen. Few-shot semantic segmentation with democratic attention networks. In ECCV, pages 730\u2013746. Springer, 2020.\"}"}
{"id": "CVPR-2022-1023", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kaixin Wang, Jun Hao Liew, Yingtian Zou, Daquan Zhou, and Jiashi Feng. Panet: Few-shot image semantic segmentation with prototype alignment. In ICCV, pages 9197\u20139206, 2019.\\n\\nWenguan Wang, Tianfei Zhou, Fisher Yu, Jifeng Dai, Endre Konukoglu, and Luc Van Gool. Exploring cross-image pixel contrast for semantic segmentation. arXiv preprint arXiv:2101.11939, 2021.\\n\\nZhonghua Wu, Xiangxi Shi, Guosheng Lin, and Jianfei Cai. Learning meta-class memory for few-shot semantic segmentation. In ICCV, pages 517\u2013526, 2021.\\n\\nGuo-Sen Xie, Jie Liu, Huan Xiong, and Ling Shao. Scale-aware graph neural network for few-shot semantic segmentation. In CVPR, pages 5475\u20135484, 2021.\\n\\nBoyu Yang, Chang Liu, Bohao Li, Jianbin Jiao, and Qixiang Ye. Prototype mixture models for few-shot semantic segmentation. In ECCV, pages 763\u2013778. Springer, 2020.\\n\\nLihe Yang, Wei Zhuo, Lei Qi, Yinghuan Shi, and Yang Gao. Mining latent classes for few-shot segmentation. arXiv preprint arXiv:2103.15402, 2021.\\n\\nMaoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan Yang. Denseaspp for semantic segmentation in street scenes. In CVPR, pages 3684\u20133692, 2018.\\n\\nYuhui Yuan, Lang Huang, Jianyuan Guo, Chao Zhang, Xilin Chen, and Jingdong Wang. Ocnet: Object context network for scene parsing. arXiv preprint arXiv:1809.00916, 2018.\\n\\nBingfeng Zhang, Jimin Xiao, and Terry Qin. Self-guided and cross-guided learning for few-shot segmentation. In CVPR, pages 8312\u20138321, 2021.\\n\\nChi Zhang, Guosheng Lin, Fayao Liu, Rui Yao, and Chunhua Shen. Canet: Class-agnostic segmentation networks with iterative refinement and attentive few-shot learning. In CVPR, pages 5217\u20135226, 2019.\\n\\nFan Zhang, Yanqin Chen, Zhihang Li, Zhibin Hong, Jingtuo Liu, Feifei Ma, Junyu Han, and Errui Ding. Acfnet: Attentional class feature network for semantic segmentation. In ICCV, pages 6798\u20136807, 2019.\\n\\nXiaolin Zhang, Yunchao Wei, Yi Yang, and Thomas S Huang. Sg-one: Similarity guidance network for one-shot semantic segmentation. IEEE transactions on cybernetics, 50(9):3855\u20133865, 2020.\\n\\nHengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, pages 2881\u20132890, 2017.\\n\\nZhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, and Xiaang Bai. Asymmetric non-local neural networks for semantic segmentation. In ICCV, pages 593\u2013602, 2019.\"}"}
