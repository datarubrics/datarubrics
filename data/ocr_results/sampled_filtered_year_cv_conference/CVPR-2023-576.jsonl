{"id": "CVPR-2023-576", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Arij Bouazizi, Adrian Holzbock, Ulrich Kressel, Klaus Dietmayer, and Vasileios Belagiannis. Motionmixer: MLP-based 3D human body pose forecasting. In Proceedings of the Thirty-First International Joint Conference on Artificial Intelligence, IJCAI-22, pages 791\u2013798. International Joint Conferences on Artificial Intelligence Organization, 7 2022.\\n\\n[2] Yu-Wei Chao, Wei Yang, Yu Xiang, Pavlo Molchanov, Ankur Handa, Jonathan Tremblay, Yashraj S Narang, Karl Van Wyk, Umar Iqbal, Stan Birchfield, et al. Dexycb: A benchmark for capturing hand grasping of objects. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9044\u20139053, 2021.\\n\\n[3] Ho Kei Cheng and Alexander G Schwing. Xmem: Long-term video object segmentation with an atkinson-shiffrin memory model. In European conference on computer vision. Springer, 2022.\\n\\n[4] Kumar Avinava Dubey, Sashank J Reddi, Sinead A Williamson, Barnabas Poczos, Alexander J Smola, and Eric P Xing. Variance reduction in stochastic gradient langevin dynamics. Advances in neural information processing systems, 29, 2016.\\n\\n[5] Hongyu Fu, Chen Liu, Xingqun Qi, Beibei Lin, Lincheng Li, Li Zhang, and Xin Yu. Sign spotting via multi-modal fusion and testing time transferring. In Computer Vision \u2013 ECCV 2022 Workshops, pages 271\u2013287, Cham, 2023. Springer Nature Switzerland.\\n\\n[6] Liuhao Ge, Zhou Ren, Yuncheng Li, Zehao Xue, Yingying Wang, Jianfei Cai, and Junsong Yuan. 3D hand shape and pose estimation from a single RGB image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10833\u201310842, 2019.\\n\\n[7] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139\u2013144, 2020.\\n\\n[8] Wen Guo, Yuming Du, Xi Shen, Vincent Lepetit, Xavier Alameda-Pineda, and Francesc Moreno-Noguer. Back to MLP: A simple baseline for human motion prediction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), January 2023.\\n\\n[9] Shreyas Hampali, Mahdi Rad, Markus Oberweger, and Vincent Lepetit. Honnotate: A method for 3D annotation of hand and object poses. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3196\u20133206, 2020.\\n\\n[10] Shreyas Hampali, Sayan Deb Sarkar, Mahdi Rad, and Vincent Lepetit. Keypoint transformer: Solving joint identification in challenging hands and object interactions for accurate 3D pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11090\u201311100, June 2022.\\n\\n[11] Tian Han, Yang Lu, Song-Chun Zhu, and Ying Nian Wu. Alternating back-propagation for generator network. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.\\n\\n[12] Junxian He, Daniel Spokoyny, Graham Neubig, and Taylor Berg-Kirkpatrick. Lagging inference networks and posterior collapse in variational autoencoders. In International Conference on Learning Representations, 2018.\\n\\n[13] Ann Huang, Pascal Knierim, Francesco Chiossi, Lewis L Chuang, and Robin Welsch. Proxemics for human-agent interaction in augmented reality. In CHI Conference on Human Factors in Computing Systems, pages 1\u201313, 2022.\\n\\n[14] Jiaxi Jiang, Paul Streli, Huajian Qiu, Andreas Fender, Larissa Laich, Patrick Snape, and Christian Holz. Avatarposer: Articulated full-body pose tracking from sparse motion sensing. In European conference on computer vision. Springer, 2022.\\n\\n[15] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European conference on computer vision, pages 694\u2013711. Springer, 2016.\\n\\n[16] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\\n\\n[17] Hema Swetha Koppula and Ashutosh Saxena. Anticipating human activities for reactive robotic response. In IROS, page 2071. Tokyo, 2013.\\n\\n[18] Mengcheng Li, Liang An, Hongwen Zhang, Lianpeng Wu, Feng Chen, Tao Yu, and Yebin Liu. Interacting attention graph for single image two-hand reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2761\u20132770, 2022.\\n\\n[19] Maosen Li, Siheng Chen, Zijing Zhang, Lingxi Xie, Qi Tian, and Ya Zhang. Skeleton-parted graph scattering networks for 3D human motion prediction. In European conference on computer vision. Springer, 2022.\\n\\n[20] Yuanzhi Liang, Qianyu Feng, Linchao Zhu, Li Hu, Pan Pan, and Yi Yang. Seeg: Semantic energized co-speech gesture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10473\u201310482, 2022.\\n\\n[21] Kevin Lin, Lijuan Wang, and Zicheng Liu. End-to-end human pose and mesh reconstruction with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1954\u20131963, 2021.\\n\\n[22] Haiyang Liu, Zihao Zhu, Naoya Iwamoto, Yichen Peng, Zhengqing Li, You Zhou, Elif Bozkurt, and Bo Zheng. Beat: A large-scale semantic and emotional multi-modal dataset for conversational gestures synthesis. In European conference on computer vision. Springer, 2022.\\n\\n[23] Jun S Liu and Jun S Liu. Monte Carlo strategies in scientific computing, volume 10. Springer, 2001.\\n\\n[24] Xian Liu, Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Qian, Xinyi Lin, Xiaowei Zhou, Wayne Wu, Bo Dai, and Bolei Zhou. Learning hierarchical cross-modal association for co-speech gesture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10462\u201310472, 2022.\\n\\n[25] Wei Mao, Miaomiao Liu, Mathieu Salzmann, and Hongdong Li. Learning trajectory dependencies for human motion prediction. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2017.\"}"}
{"id": "CVPR-2023-576", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[26] Gyeongsik Moon, Shoou-I Yu, He Wen, Takaaki Shiratori, and Kyoung Mu Lee. Interhand2.6m: A dataset and baseline for 3d interacting hand pose estimation from a single rgb image. In European Conference on Computer Vision, pages 548\u2013564. Springer, 2020.\\n\\n[27] Radford M Neal et al. Mcmc using hamiltonian dynamics. Handbook of markov chain monte carlo, 2(11):2, 2011.\\n\\n[28] Evonne Ng, Shiry Ginosar, Trevor Darrell, and Hanbyul Joo. Body2hands: Learning to infer 3d hands from conversational gesture body dynamics. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11865\u201311874, June 2021.\\n\\n[29] Sergio Orts-Escolano, Christoph Rhemann, Sean Fanello, Wayne Chang, Adarsh Kowdle, Yury Degtyarev, David Kim, Philip L Davidson, Sameh Khamis, Mingsong Dou, et al. Holoportation: Virtual 3d teleportation in real-time. In Proceedings of the 29th annual symposium on user interface software and technology, pages 741\u2013754, 2016.\\n\\n[30] Bo Pang, Tian Han, Erik Nijkamp, Song-Chun Zhu, and Ying Nian Wu. Learning latent space energy-based prior model. Advances in Neural Information Processing Systems, 33:21994\u201322008, 2020.\\n\\n[31] Bo Pang and Ying Nian Wu. Latent space energy-based model of symbol-vector coupling for text generation and classification. In International Conference on Machine Learning, pages 8359\u20138370. PMLR, 2021.\\n\\n[32] JoonKyu Park, Yeonguk Oh, Gyeongsik Moon, Hongsuk Choi, and Kyoung Mu Lee. Handoccnet: Occlusion-robust 3d hand mesh estimation network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1496\u20131505, June 2022.\\n\\n[33] Javier Romero, Dimitrios Tzionas, and Michael J Black. Embodied hands: modeling and capturing hands and bodies together. ACM Transactions on Graphics (TOG), 36(6):1\u201317, 2017.\\n\\n[34] Yu Rong, Takaaki Shiratori, and Hanbyul Joo. Frankmocap: A monocular 3d whole-body pose estimation system via regression and integration. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, pages 1749\u20131759, October 2021.\\n\\n[35] Maha Salem, Stefan Kopp, Ipke Wachsmuth, Katharina Rohlfing, and Frank Joublin. Generation and evaluation of communicative robot gesture. International Journal of Social Robotics, 4(2):201\u2013217, 2012.\\n\\n[36] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. Advances in neural information processing systems, 28, 2015.\\n\\n[37] Adrian Spurr, Aneesh Dahiya, Xi Wang, Xucong Zhang, and Otmar Hilliges. Self-supervised 3d hand pose estimation from monocular rgb via contrastive learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11230\u201311239, 2021.\\n\\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\n[39] Jiashun Wang, Huazhe Xu, Medhini Narasimhan, and Xiaolong Wang. Multi-person 3d motion prediction with multi-range transformers. Advances in Neural Information Processing Systems, 34:6036\u20136049, 2021.\\n\\n[40] Zijian Wang, Xingqun Qi, Kun Yuan, and Muyi Sun. Self-supervised correlation mining network for person image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7703\u20137712, 2022.\\n\\n[41] Pieter Wolfert, Nicole Robinson, and Tony Belpaeme. A review of evaluation practices of gesture generation in embodied conversational agents. IEEE Transactions on Human-Machine Systems, 2022.\\n\\n[42] Chao-Yuan Wu, Yanghao Li, Karttikeya Mangalam, Haoqi Fan, Bo Xiong, Jitendra Malik, and Christoph Feichtenhofer. Memvit: Memory-augmented multiscale vision transformer for efficient long-term video recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13587\u201313597, 2022.\\n\\n[43] Zhuojie Wu, Xingqun Qi, Zijian Wang, Wanting Zhou, Kun Yuan, Muyi Sun, and Zhenan Sun. Showface: Coordinated face inpainting with memory-disentangled refinement networks. In 33rd British Machine Vision Conference 2022, BMVC 2022, London, UK, November 21-24, 2022. BMVA Press, 2022.\\n\\n[44] Donglai Xiang, Hanbyul Joo, and Yaser Sheikh. Monocular total capture: Posing face, body, and hands in the wild. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10965\u201310974, 2019.\\n\\n[45] Xiangyu Xu, Li Siyao, Wenxiu Sun, Qian Yin, and Ming-Hsuan Yang. Quadratic video interpolation. In NeurIPS, 2019.\\n\\n[46] Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang, Jaeyeon Lee, Jaehong Kim, and Geehyuk Lee. Speech gesture generation from the trimodal context of text, audio, and speaker identity. ACM Transactions on Graphics (TOG), 39(6):1\u201316, 2020.\\n\\n[47] Youngwoo Yoon, Woo-Ri Ko, Minsu Jang, Jaeyeon Lee, Jaehong Kim, and Geehyuk Lee. Robots learn social skills: End-to-end learning of co-speech gesture generation for humanoid robots. In 2019 International Conference on Robotics and Automation (ICRA), pages 4303\u20134309. IEEE, 2019.\\n\\n[48] Jiyang Yu, Jingen Liu, Liefeng Bo, and Tao Mei. Memory-augmented non-local attention for video super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17834\u201317843, 2022.\\n\\n[49] Baowen Zhang, Yangang Wang, Xiaoming Deng, Yinda Zhang, Ping Tan, Cuixia Ma, and Hongan Wang. Interacting two-hand 3d pose and shape reconstruction from single color image. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11354\u201311363, 2021.\"}"}
{"id": "CVPR-2023-576", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jing Zhang, Jianwen Xie, Nick Barnes, and Ping Li. Learning generative vision transformer with energy-based latent space for saliency prediction. Advances in Neural Information Processing Systems, 34:15448\u201315463, 2021.\\n\\nYuxiao Zhou, Marc Habermann, Weipeng Xu, Ikhsanul Habibie, Christian Theobalt, and Feng Xu. Monocular real-time hand shape and motion capture using multi-modal data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5346\u20135355, 2020.\\n\\nChristian Zimmermann, Duygu Ceylan, Jimei Yang, Bryan Russell, Max Argus, and Thomas Brox. Freihand: A dataset for markerless capture of hand pose and shape from single rgb images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 813\u2013822, 2019.\"}"}
{"id": "CVPR-2023-576", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"struct a prototypical memory to store the realistic two hands prototypes. Then, we obtain the retrieved realistic prototype representation based on the initial prediction at stage one. Meanwhile, as shown in Fig. 2 (stage two), we propose a sampling header to obtain the hand-informative perturbation via gradient-based Markov Chain Monte Carlo (MCMC) sampling. Finally, we concatenate the retrieved realistic hand prototype representation and perturbation to generate the diverse 3D two hands.\\n\\nHands Prototypical Memory Construction:\\n\\nInspired by [50], we reformulate the two hands diversification as a generation task. Instead of directly sampling the prior of the perturbation from the isotropic Gaussian distribution [7, 11, 16, 36], we assume the sampling space follows a learnable realistic prior distribution [30, 31, 50]. Since the existing 3D hand prediction dataset is noisy due to the automated annotation process, we propose a prototypical memory bank to store the realistic hand prototype representations encoded from real 3D hands. These 3D hands are captured from a studio-based mocap-captured dataset, named BEAT [22]. The reading and updating strategies of hands prototypical memory are the same as TMM.\\n\\nGradient-based MCMC Sampling:\\n\\nWe exploit the retrieved hand prototype representation concatenated with sampled perturbation vector $w$ to predict the diverse hand gestures. Given the observed 3D two hands $h \\\\in \\\\mathbb{R}^{90}$, the $\\\\tilde{h}$ is expressed as:\\n\\n$$\\n\\\\tilde{h} = R_{\\\\theta}(h, w) + \\\\epsilon, \\\\quad w \\\\sim p_{\\\\alpha}(w), \\\\quad \\\\epsilon \\\\sim N(0, \\\\sigma^2 \\\\epsilon I), \\\\quad (3)\\n$$\\n\\nwhere $p_{\\\\alpha}(w)$ is the prior distribution of perturbation with parameters $\\\\alpha$, and $\\\\epsilon \\\\sim N(0, \\\\sigma^2 \\\\epsilon I)$ is the observation noise of hand gestures with $\\\\sigma^2$ given, $R_{\\\\theta}$ is the generation model with parameters $\\\\theta$.\\n\\nInspired by [11], we leverage an MLP-based sampling header $S_{\\\\alpha}(w)$ to model the diversification sampling process, as illustrated in Fig. 2 (stage two). The prior distribution of perturbation is initialized from an isotropic Gaussian reference distribution, expressed as:\\n\\n$$\\np_{\\\\alpha}(w) \\\\propto \\\\exp(-S_{\\\\alpha}(w) - \\\\frac{1}{2} \\\\sigma_w^2 \\\\|w\\\\|^2), \\\\quad (4)\\n$$\\n\\nwhere the $M_{\\\\alpha}(w) = S_{\\\\alpha}(w) + \\\\frac{1}{2} \\\\sigma_w^2 \\\\|w\\\\|^2$ is defined as the whole sampling function, and $\\\\alpha$ is the learnable parameters of sampling header. The hyperparameter $\\\\sigma_w$ denotes the standard deviation. For notation simplicity, let $\\\\beta = \\\\{\\\\theta, \\\\alpha\\\\}$.\\n\\nFor the $i$th sample in a training mini-batch with size $n$, the log-likelihood function of $\\\\beta$ is defined as:\\n\\n$$\\nL(\\\\beta) = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\log Z p_{\\\\alpha}(w_i) p_{\\\\theta}(\\\\tilde{h}_i|h_i, w_i) \\\\, dw_i. \\\\quad (5)\\n$$\\n\\nThus the gradient of $L(\\\\beta)$ is computed as:\\n\\n$$\\n\\\\nabla L(\\\\beta) = E_{p(\\\\beta)}(\\\\nabla_{\\\\alpha} \\\\log p_{\\\\alpha}(w) + \\\\nabla_{\\\\theta} \\\\log p_{\\\\theta}(\\\\tilde{h}|h, w)). \\\\quad (6)\\n$$\\n\\nThe intractable expectation term $E_{p(\\\\beta)}(\\\\cdot)$ in Eq. (6) is approximately solved by a gradient-based MCMC (Langevin dynamics) [4, 27]. In particular, the initial state of perturbation $p_0(w)$ is sampled from the Gaussian distribution.\\n\\nThrough MCMC, the perturbation from the prior distribution and posterior distribution are represented as $w - i$ and $w + i$, respectively. The detailed calculation process is in [27]. Therefore, the gradients of the generation model and sampling header are updated by:\\n\\n$$\\n\\\\nabla_{\\\\theta} R_{\\\\theta} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\frac{1}{\\\\sigma^2 \\\\epsilon} (\\\\tilde{h}_i - R_{\\\\theta}(h_i, w+i)) \\\\nabla_{\\\\theta} R_{\\\\theta}(h_i, w+i), \\\\quad (7)\\n$$\\n\\n$$\\n\\\\nabla_{\\\\alpha} S_{\\\\alpha} = \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\nabla_{\\\\alpha} S_{\\\\alpha}(w-i) - \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\nabla_{\\\\alpha} S_{\\\\alpha}(w+i). \\\\quad (8)\\n$$\\n\\n3.4. Objective Functions\\n\\nIn stage one, we apply the following loss functions to constrain the initial prediction process of hand gestures.\\n\\nReconstruction Loss:\\n\\nWe use the ground truth to supervise the predicted two hands as $L_{rec} = H - \\\\tilde{H}$, where $\\\\tilde{H}$ denotes generated hands.\\n\\nPerceptual Loss:\\n\\nInspired by [15], we leverage the studio-based 3D hands dataset BEAT to pre-train an MLP-based autoencoder as a feature extractor $\\\\phi(\\\\cdot)$. We utilize the $L_1$ loss to constrain the difference between the extracted features of $H$ and $\\\\tilde{H}$, as formulated:\\n\\n$$\\nL_{perc} = \\\\phi(H) - \\\\phi(\\\\tilde{H})_1.\\n$$\\n\\nAdversarial Learning:\\n\\nFollowing the configuration of [28], we employ the adversarial training loss as:\\n\\n$$\\nL_{adv} = E_H[\\\\log D(H)] + E_B[\\\\log (1 - G(B))], \\\\quad (9)\\n$$\\n\\nwhere $D$ denotes the discriminator and $G$ means hands generator. Finally, the overall objective in stage one is:\\n\\n$$\\n\\\\min_G \\\\max_D L_{stage\\\\,1\\\\,total} = L_{rec} + L_{adv} + L_{perc} + 0.5 L_{dis}, \\\\quad (10)\\n$$\\n\\nwhere $L_{dis}$ denotes disentanglement constraint in Eq. (1).\\n\\nIn the second stage, the overall objective to train our model is defined as follows:\\n\\n$$\\nL_{stage\\\\,2\\\\,total} = h - R_{\\\\theta}(h, w+), \\\\quad (11)\\n$$\\n\\nwhere $w+$ is the posterior distribution of the perturbation. We conduct the frame-level diversification based on the initial prediction at stage one. To ensure temporal smoothness, we further apply temporal smoothness [45] to the diversified 3D hand gestures as a post processing.\"}"}
{"id": "CVPR-2023-576", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison with the state-of-the-art methods on B2H dataset and our proposed TED Hands dataset of the first stage initial prediction. \u2193 indicates the lower the better.\\n\\n| Methods          | B2H Dataset [28] | TED Hands Dataset |\\n|------------------|------------------|-------------------|\\n|                  | L2              | MPJRE             |\\n| Body2Hands [28]  | CV PR 3.760     | 11.371            |\\n| MRT [39]         | NeurIPS PR 3.634| 10.314            |\\n| BTM [8]          | WACV PR 3.657   | 10.440            |\\n| LTD [25]         | ICCV PR 3.755   | 11.078            |\\n| MotionMixer [1]  | IJCAI PR 3.616  | 10.427            |\\n| SPGSN [19]       | ECCV PR 3.656   | 10.887            |\\n| Ours (stage one) |                 | 8.888             |\\n\\nOurs (stage one) indicates the lower the better, and \u2191 indicates the higher the better. \u00b1 means 95% confidence interval.\\n\\nTable 2. Sampling experiment results on B2H dataset.\\n\\n| Methods          | FHD  \u2193 | Diversity \u2191 |\\n|------------------|--------|-------------|\\n| Body2hands [28]  | 2.504  | 0.497       |\\n| MRT [39]         | 1.982  | 0.698       |\\n| BTM [8]          | 2.379  | 0.703       |\\n| LTD [25]         | 1.893  | 0.577       |\\n| MotionMixer [1]  | 2.169  | 0.869       |\\n| SPGSN [19]       | 2.004  | 0.874       |\\n| Ours w/ V AE [36]| 1.617  | 1.001       |\\n| Ours (stage two) | 1.239  | 1.324       |\\n\\n4. Experiments\\n4.1. Datasets and Experimental Setting\\nB2H Dataset: The B2H dataset [28] is automatically annotated by a monocular-based 3D pose estimation model [44], including the gestures of 8 speakers from in-the-wild talk show videos. In the original B2H dataset, there are more than 139K sequences. Each sequence includes 64 frames, with 32 frames overlapping between every two sequences. However, due to the noisiness and unavailable videos, we obtain 120,188 sequences finally. In our work, we adopt the same division criteria as [28] to split the training and testing dataset.\\n\\nTED Hands Dataset: Due to the limited avatar identities in the existing 3D hand prediction dataset, we newly collect a large-scale 3D hands dataset (dubbed TED Hands) based on the TED Gesture [46, 47]. The original TED Gesture dataset only contains 10 upper body joints without elaborate fingers of two hands. Hence, we leverage a state-of-the-art 3D pose estimator FrankMocap [34] to extract the whole body-hand joints as pseudo ground truth from the video links provided by TED Gesture. Concretely, each joint of our dataset is normalized as the 3D axis-angle representation in a fixed kinematic structure [33]. In this fashion, our dataset is invariant to root joint motion and body shape. Finally, we acquired the 134,456 sequences from 1,755 different speakers, with 99.6 hours of TED talking speeches. Each sequence includes 64 frames without overlap. We randomly spit the dataset into the 70% training set, 10% validation set, and 20% testing set. Please refer to the supplementary material for more details.\\n\\nImplementation Details: Our model is implemented on the PyTorch platform with 2 NVIDIA RTX 2080Ti GPUs. We train the model utilizing Adam optimizer with an initial learning rate of 0.003. The whole training takes 30 epochs with a batch size of 64. We set $\\\\sigma_\\\\epsilon = 1$ in Eq. (3), and $\\\\sigma_w = 1$ in Eq. (4).\\n\\nEvaluation Metrics: To fully evaluate the naturalness of initial predictions at stage one and the diversity of sampled hand gestures at stage two, we employ various metrics:\\n\\n- $L^2$ Distance: Distance between the generated whole two hand poses and pseudo ground truth.\\n- FHD: Inspired by FGD [46, 47], we propose a hand-specific metric (named FHD) to measure the Fr\u00e9chet distance between the features of generated hands and pseudo ground truth. The feature extractors of FHD are pre-trained by B2H and TED Hands datasets, respectively.\\n- MPJRE: We leverage the Mean Per Joint Rotation Error [14] (MPJRE) to measure the absolute distance between predicted 3D representation joints and pseudo ground truth.\\n- Diversity: To verify the diversity of sampled hand gestures, we calculate the average feature distance between 500 random combined sequential pairs [20, 24]. The feature extractor is the same as FHD.\"}"}
{"id": "CVPR-2023-576", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 4. Ablation study on different loss functions and different components of the proposed method.\\n\\n| Model Variations | B2H Dataset [28] | TED Hands Dataset |\\n|------------------|------------------|------------------|\\n|                   | L<sub>dis</sub>  | L<sub>perc</sub> | SRM | TMM |\\n| Baseline          | 3.650            | 1.091            | 12.690 | 2.366 | 0.977 | 10.531 |\\n| Full Model        | 3.604            | 0.773            | 12.453 | 2.303 | 0.731 | 10.250 |\\n|                   | \u2713                | \u2713                | 3.552  | 0.682 | 12.037 | 2.216 | 0.595 | 9.786 |\\n|                   | \u2713                | \u2713 \u2713              | 3.563  | 0.657 | 12.085 | 2.261 | 0.699 | 10.026 |\\n|                   | \u2713 \u2713              | \u2713 \u2713              | 3.504  | 0.640 | 11.828 | 2.161 | 0.495 | 9.508 |\\n|                   | \u2713 \u2713              | \u2713 \u2713              | 3.457  | 0.580 | 11.687 | 2.100 | 0.393 | 9.209 |\\n|                   | \u2713                | \u2713                | 3.484  | 0.614 | 11.758 | 2.124 | 0.463 | 9.311 |\\n|                   | \u2713                | \u2713 \u2713              | 3.476  | 0.598 | 11.748 | 2.156 | 0.380 | 9.467 |\\n|                   | \u2713                | \u2713 \u2713              | 3.420  | 0.471 | 11.406 | 2.037 | 0.258 | 8.888 |\\n\\n4.2. Quantitative Results\\n\\nComparisons with the state-of-the-art:\\n\\nWe compare our method with the pioneering 3D hands prediction work, named Body2hands [28]. Moreover, to fully verify the superiority of our method, we implement various state-of-the-art (SOTA) human motion prediction models: MTR [39], BTM [8], LTD [25], MotionMixer [1], and SPGSN [19]. For fair comparisons, all methods are implemented by source codes or pre-trained models released by authors. Since the above counterparts are designed without the diversification setting, the comparison is divided into two parts.\\n\\nIn the first part, we leverage our stage one initial prediction to compare with other competitors. We adopt the L<sub>2</sub> distance, FHD, and MPJRE to provide a well-rounded view of comparisons. As reported in Tab. 1, our method outperforms all the counterparts by a large margin. Remarkably, our method achieves the lowest FHD and MPJRE scores on both datasets. Concretely, on FHD scores of the TED Hands dataset, our method performs even 70.6% lower than the optimal results of other competitors (e.g., (0.877 \u2212 0.258)/0.877 \u2248 70.6%).\\n\\nTo demonstrate the effectiveness of the diversification process in our method, we compare the diversified results based on the various predictions from above mentioned competitive methods. Meanwhile, we replace our MCMC-based sampling strategy with CV AE [36] to diversify our initial prediction at stage one. We exploit the FHD and Diversity metrics in the second part. As shown in Tab. 2 and Tab. 3, our method achieves superior performance from the perspective of diversity. Although the FHD is slightly worse than stage one due to the sampled perturbation, our method still achieves the optimal result. Compared with CV AE, our method effectively avoids the potential posterior collapse problem [12]. Moreover, we observe that the diversity scores on our TED Hands dataset are much higher than the B2H dataset with all the counterparts. It applies that our newly collected TED Hands dataset includes more diverse hand gestures.\\n\\nAblation Study:\\n\\nWe further conduct the ablation study based on stage one to verify the effectiveness of each module and different loss functions, as indicated in Tab. 4. The baseline model is implemented by a simple transformer-based pipeline without bilateral hand branches. Our full model adopts the bilateral hand branches interacting with a body-specific transformer to produce the 3D hand gestures. As shown in Tab. 4, all combinations of the different loss functions and components have positive impacts on 3D hand predictions. The most complete combination attains the best results. Even though we directly construct the bilateral branches without disentanglement constraints (i.e., L<sub>dis</sub>) as full model, our method achieves comparable performance against the simple baseline. This indicates the global body-hand interaction are effectively modeled by bilateral hand transformers and a body-specific transformer. Then, we observe that after adding the loss functions L<sub>dis</sub> and L<sub>perc</sub>, the performance of our method is significantly improved, especially in MPJRE scores on both datasets (e.g., 10.250 \u2192 9.508 on TED Hands).\\n\\nTo verify the effectiveness of SRM and TMM, we conduct plenty of experiments on both datasets. As reported in Tab. 4, when we first employ the SRM, our method achieves the lower L<sub>2</sub> distance and FHD scores on both datasets. Then, after adding TMM, our method achieves the best performance. This result proves that TMM effectively models the body-hand temporal association. Besides, we find that when TMM is adopted before SRM, the model performance decreases slightly (the third row from bottom to top in Tab. 4). This corresponds to both TMM and transformer-based backbone (i.e., body-transformer, bilateral hand branches) focus on modeling the body-hand temporal correlation. Both of them encourage the two hands to be more temporal consistent with body dynamics. Employing the SRM between them leads to worse spatial-temporal modeling for body-hand interaction.\\n\\nMoreover, we conduct experiments that the parameter weights of bilateral hand transformers are shared. As shown in Tab. 4 (the second row from bottom to top), the performance significantly improves.\"}"}
{"id": "CVPR-2023-576", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Visualization of our predicted 3D hand gestures against various state-of-the-art methods [1,8,19,25,28,39]. From top to bottom, samples of the first row are from the B2H dataset, and the others are from our TED Hands dataset. Best view on screen.\\n\\nFigure 5. User study on the motion naturalness, smoothness, and body-hand temporal consistency.\\n\\n4.3. Qualitative Evaluation\\n\\nUser Study: To further analyze the visual quality of the predicted 3D hand gestures by different methods, we recruit 15 volunteers to participate in the user study. We take the naturalness, smoothness, and body-hand temporal consistency as three evaluation perceptions and the range of grades is from 0 to 5 (the higher, the better). The statistical results are reported in Fig. 5. Our method shows the best performance in all three metrics. Especially in the naturalness and body-hand temporal consistency, our method achieves remarkable advantages among all the methods due to the body-hand interaction modeling by SRM and TMM.\\n\\nVisualization: To fully verify the performance of our method, we show the visualization of generated 3D hands from the first and second stages, respectively. As depicted in Fig. 4, we compare our initial predictions (from stage one) with various SOTA approaches. The avatars are rendered from ground truth speakers in B2H and TED Hands datasets. We observe that Body2hands, BTM, and LTD generate unreasonable hand gestures (e.g., in the third row of Body2hands, left-hand gestures of avatar). MRT, MotionMixer, and SPGSN predict the natural results, but the finger gestures are less consistent with the upper body dynamics. In contrast, our method successfully predicts the perceptually convincing 3D hand gestures from the body dynamics. This supports our key insight to build the bilateral hand disentanglement branches since the asymmetric motions of two hands. We further visualize the diversification results generated by our method at stage two. As shown in Fig. 1 and Fig. 4, with the same upper body dynamics as input, our method obtains the natural and diverse 3D hand gestures on B2H and TED Hands datasets, respectively. Please refer to the supplementary material for more visualization results.\\n\\n5. Conclusion\\n\\nIn this paper, we present a novel bilateral hand disentanglement based two-stage method towards natural and diverse 3D hand prediction from body dynamics. In the first stage, upon the bilateral hand branches, we fully take advantage of the spatial-temporal body-hand interaction for generating natural initial 3D hand gestures. Then, in the second stage, we further diversify our natural initial 3D hand gestures by gradient-based Markov chain Monte Carlo sampling. Extensive experiments conducted on B2H and our newly collected TED Hands datasets demonstrate the superiority of our method. As our method aims at generating diverse 3D avatar hand postures, in future work, we will investigate sampling diverse 3D hand gestures with temporal smoothness, instead of frame-wise sampling.\\n\\nAcknowledgements. This research is funded in part by ARC-Discovery grant (DP220100800 to XY) and ARC-DECRA grant (DE230100477 to XY).\"}"}
{"id": "CVPR-2023-576", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Diverse 3D Hand Gesture Prediction from Body Dynamics by Bilateral Hand Disentanglement\\n\\nXingqun Qi\\n1\\n\\nChen Liu\\n2\\n\\nMuyi Sun\\n3\\n\\nLincheng Li\\n4\\n\\nChangjie Fan\\n4\\n\\nXin Yu\\n1\\n\\n1 AAII, University of Technology Sydney\\n2 The University of Queensland\\n3 CRIPAC, NLPR, Institute of Automation, Chinese Academy of Sciences\\n4 Netease Fuxi AI Lab\\n\\n{xingqunqi, yenanliu36}@gmail.com, muyi.sun@cripac.ia.ac.cn\\n{lilincheng, fanchangjie}@corp.netease.com, xin.yu@uq.edu.au\\n\\nFigure 1. Diverse exemplary clips sampled by our method from our newly collected TED Hands Dataset. The vital frames are visualized to demonstrate that the hand gestures change with the upper body dynamics, synchronously. Best view on screen.\\n\\nAbstract\\n\\nPredicting natural and diverse 3D hand gestures from the upper body dynamics is a practical yet challenging task in virtual avatar creation. Previous works usually overlook the asymmetric motions between two hands and generate two hands in a holistic manner, leading to unnatural results. In this work, we introduce a novel bilateral hand disen-tanglement based two-stage 3D hand generation method to achieve natural and diverse 3D hand prediction from body dynamics. In the first stage, we intend to generate natural hand gestures by two hand-disentanglement branches. Considering the asymmetric gestures and motions of two hands, we introduce a Spatial-Residual Memory (SRM) module to model spatial interaction between the body and each hand by residual learning. To enhance the coordination of two hand motions wrt. body dynamics holistically, we then present a Temporal-Motion Memory (TMM) module. TMM can effectively model the temporal association between body dynamics and two hand motions. The second stage is built upon the insight that 3D hand predictions should be non-deterministic given the sequential body postures. Thus, we further diversify our 3D hand predictions based on the initial output from the stage one. Concretely, we propose a Prototypical-Memory Sampling Strategy (PSS) to generate the non-deterministic hand gestures by gradient-based Markov Chain Monte Carlo (MCMC) sampling. Extensive experiments demonstrate that our method outperforms the state-of-the-art models on the B2H dataset and our newly collected TED Hands dataset. The dataset and code are available at: https://github.com/XingqunQi-lab/Diverse-3D-Hand-Gesture-Prediction.\\n\\n1. Introduction\\n\\nGiven a sequence of upper body skeletons, our task aims at predicting natural and diverse 3D hand gestures. Such non-verbal body-hand coordination plays an important role in various virtual avatar scenarios, including human-agent interface [13,17,35,40,41], co-speech gesture synthesis [5,20,24,46,47], holoportation [29].\\n\\nHowever, it is quite difficult to predict the natural and diverse 3D hand gestures due to three major challenges:\\n\\n(1) Spatially asymmetric motions: Asymmetric motions of two hands have been overlooked by previous works [28], e.g., when one hand moves, the other could be static or moving slowly.\\n\\n(2) Temporal consistency wrt. body dynamics: Predicted sequential hand gestures should be temporally consistent with respect to the body dynamics.\\n\\n(3) Non-deterministic hand prediction: Given the upper body dynamics, various 3D hand gestures can match the body postures rather than a deterministic result. Since the existing dataset has only a few avatar identities, the generated hands...\"}"}
{"id": "CVPR-2023-576", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"often lack diversity. Due to complex body-hand interaction and asymmetric movements of two hands, directly predicting the natural and diverse 3D hand is rather difficult. Therefore, we propose to address the aforementioned challenges in a prediction followed by a diversification paradigm. To be specific, we propose a novel bilateral hand disentanglement based two-stage method, to generate natural and diverse 3D hand gestures from upper body dynamics.\\n\\nIn the first stage, we aim to predict the natural 3D hand gestures. Our key insight is to establish the bilateral hand disentanglement for asymmetric motions of two hands. We construct two hand-disentangled branches that interact with a body-specific branch to initially predict natural 3D hands. Here, we leverage a single-hand autoencoder to extract each hand feature respectively, thus achieving the effects of two hands disentanglement. Furthermore, we propose a Spatial-Residual Memory (SRM) module to model the spatial relation between the body and each hand via residual learning. Specifically, we employ a spatial memory bank to store the spatial residual deformation representations of each hand. Then, we leverage the current upper body embedding as a query to retrieve the most relevant spatial residual deformation. The queried spatial residual deformation and the synchronized hand embedding vector are used to generate the next step hand representation.\\n\\nTo ensure the motions of both hands are temporally consistent with the upper body sequence, we present a Temporal-Motion Memory (TMM) module. TMM models the correlation between body dynamics and hand gestures. Similar to SRM, we utilize a temporal memory bank to store the motion features of each hand at the sequence level. Moreover, we employ a motion encoder to acquire the input body dynamics representation, and then use the body dynamics representation as a query to produce temporally consistent hand motion features. In this fashion, we can effectively synchronize hand motions with body dynamics.\\n\\nIn the second stage, we develop a Prototypical-Memory Sampling Strategy (PSS) to diversify our 3D hand predictions based on the initial prediction at stage one. Here, we leverage an external memory that stores realistic hand prototypes, and then search a prototype that is closest to our initial predicted hands. Once we obtain the prototype, we project it into the feature space. To increase the diversity while preserving the authenticity of generated hand gestures, we perturb the prototype feature via gradient-based Markov Chain Monte Carlo (MCMC) sampling [23]. Specifically, a random noise is first sampled from a Gaussian distribution as a prior perturbation and then we update its posterior via Langevin dynamics based MCMC [4, 27]. Then, the perturbation is concatenated with the prototype feature to produce realistic new hand gestures.\\n\\nMoreover, the existing 3D hand prediction dataset [28] contains less than 10 avatar identities, resulting in insufficient diversity of gestures. Therefore, we newly collect a large-scale 3D hand gestures dataset (dubbed TED Hands) with more than 1.7K avatar identities from in-the-wild scenarios. Our dataset contains around 100 hours of TED talk speeches, enabling research on diverse 3D hand gesture predictions. Extensive experiments conducted on the B2H dataset [28] and our TED Hands dataset demonstrate our method outperforms the state-of-the-art.\\n\\nOverall, our contributions are summarized as follows:\\n\\n\u2022 We propose a novel bilateral hand disentanglement based two-stage 3D hand generation method to predict diverse 3D hand gestures from body dynamics in a prediction followed by diversification paradigm.\\n\\n\u2022 We present a Spatial-Residual Memory (SRM) module to predict authentic hand poses from disentangled hand representations and a Temporal-Motion Memory (TMM) module to ensure temporal consistency of generated 3D hands.\\n\\n\u2022 We design a Prototypical-Memory Sampling Strategy (PSS) to diversify our initial 3D hand prediction, thus obtaining natural and diverse hand gestures.\\n\\n\u2022 We collect a new large-scale sequential 3D hand dataset from 1.7K persons' hand gestures, significantly facilitating research on diverse 3D hands generation.\\n\\n2. Related Work\\n\\nSince our method predicts the 3D hand motions from upper body skeletons, we briefly review the approaches of 3D hand pose estimation and 3D human motion prediction.\\n\\n3D hand pose estimation: 3D hand estimation has received impressive attention for its wide applications in virtual reality, augmented reality, and robotics. Recently, parametric 3D hand model MANO [33] and various mocap-captured 3D hand datasets [2, 9, 26, 52] significantly improve the performance of 3D hand pose estimation from the single image [6, 18, 32, 37, 49, 51]. Among various image-based 3D hand estimation approaches, transformer-based models [10,18,21] have demonstrated outstanding performance. For instance, Li et al. [18] propose an interacting attention graph transformer to estimate two-hand poses from a single image. Shreyas et al. [10] present a hand keypoint based transformer combined with convolutional networks to establish the association among different joints.\"}"}
{"id": "CVPR-2023-576", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. The pipeline of our proposed method. In stage one, we generate the natural initial 3D hands from sequential upper body skeletons. In stage two, we diversify our 3D hand predictions based on the initial gestures at stage one. Asymmetric motions of two hands, their method might produce some unnatural hand gestures. This also motivates us to disentangle two hands in hand pose estimation and then synchronize two-hands motions.\\n\\n3D human motion prediction: Predicting human future motions from existing 3D poses has been widely studied [1, 8, 19, 25, 39]. Wang et al. [39] leverage a multi-range transformer that attends local-global features for multi-person motion prediction. Bouazizi et al. [1] introduce a multi-layer perception based human motion prediction network to learn spatio-temporal pose relations. Due to the similar nature between 3D human motion prediction and hand pose prediction, these models [1, 8, 19, 25, 39] can be easily adopted as our baselines. Moreover, we notice that memory-based networks [3, 42, 43, 48] achieve promising performance in long-sequence modeling tasks. Motivated by this, we introduce external memories into our network to facilitate our long-range hand pose generation.\\n\\n3. Proposed Method\\n\\n3.1. Problem Formulation\\n\\nGiven a sequence of 3D upper body skeletons \\\\( B = \\\\{b_t\\\\}_{t=0}^T \\\\), our goal is to predict the natural and diverse 3D hand poses \\\\( H = \\\\{h_t\\\\}_{t=0}^T \\\\), where \\\\( T \\\\) is the sequence length. Specifically, we define each step body skeleton (i.e., columns, shoulders, elbows, and wrists) \\\\( b_t \\\\in \\\\mathbb{R}^{24} \\\\) as 8 joints. Each joint is represented by a 3D axis-angle representation. Meanwhile, the 3D hand pose at time \\\\( t \\\\), \\\\( h_t \\\\in \\\\mathbb{R}^{90} \\\\), is described as 30 joints with 3D axis-angle representation.\\n\\n3.2. Stage One: Natural Hand Prediction\\n\\nDue to complex body-hand interaction and asymmetric motions of two hands, we leverage the bilateral hand disentanglement branches and a body-specific branch to generate the natural 3D hands. Specifically, as depicted in Fig. 2 (stage one), our method mainly consists of a Bilateral Hand Disentanglement (BHD) module, a Spatial-Residual Memory (SRM) module, a Temporal-Motion Memory (TMM) module, and a transformer-based backbone. Firstly, we leverage a body encoder to obtain the body features \\\\( F_B = \\\\{F_t\\\\}_{t=0}^T \\\\), where \\\\( F_t \\\\in \\\\mathbb{R}^{1 \\\\times C} \\\\) denotes a body feature at time \\\\( t \\\\), and \\\\( C \\\\) is the feature dimension. Then, the body features are disentangled by BHD to acquire the single hand features \\\\( F_{SH} = \\\\{F_t\\\\}_{t=0}^T \\\\), where \\\\( F_t \\\\in \\\\mathbb{R}^{1 \\\\times C} \\\\). Considering the asymmetry motions of two hands, we exploit SRM to model the spatial interaction between the body and each hand, respectively. Besides, we further develop TMM to keep the two hand gestures temporal consistent with body dynamics.\\n\\nBilateral Hand Disentangle Module:\\n\\nDue to the asymmetric motions of two hands, we employ a single hand autoencoder as the feature extractor to disentangle two hands. We firstly train an autoencoder on 3D single hands. As shown in Fig. 3 (a), we then utilize a Multilayer Perceptron (MLP) to project the body features into the single hand features \\\\( F_{SH} \\\\). To ensure these single hand features represent each disentangled hand separately, we conduct the feature level and 3D representation level constraints by the single hand autoencoder, as follows:\\n\\n\\\\[\\nL_{\\\\text{dis}} = g_{F_t} - F_t + D_g F_t - D F_t, \\\\tag{1}\\n\\\\]\\n\\nwhere \\\\( g_{F_t} \\\\) denotes reconstructed hand feature at time \\\\( t \\\\), and \\\\( D \\\\) is the pre-trained decoder.\\n\\nSpatial-Residual Memory Module:\\n\\nAfter obtaining the disentangled single hand features, we introduce a Spatial-Residual Memory (SRM) module to model the spatial interaction between the body and each hand. The SRM module consists of a spatial transformer network and a residual memory network. The spatial transformer network is used to attend to the spatial features of the body and the hand, while the residual memory network is used to store and update the spatial features of the hand. This allows the model to capture the spatial relationships between the body and the hand, and to learn the spatial dependencies between the body and the hand over time.\\n\\nDisentangled Bilateral Branches:\\n\\nTo further enhance the disentanglement of the two hands, we develop disentangled bilateral branches. Each branch is responsible for modeling one hand and is designed to capture the unique characteristics of that hand. The bilateral branches are connected to the body encoder and the spatial memory network, allowing them to learn the relationships between the body and each hand. This ensures that the model can accurately distinguish between the two hands and generate natural and diverse 3D hand poses.\\n\\nTemporal-Motion Memory (TMM) Module:\\n\\nThe Temporal-Motion Memory (TMM) module is designed to store and update the temporal features of the hand. This allows the model to learn the temporal dependencies between the hand gestures and the body dynamics, which is crucial for generating natural and diverse 3D hand poses. The TMM module is connected to the spatial memory network and the body encoder, allowing it to access spatial and temporal features from both the body and the hand. This enables the model to capture the temporal consistency of the hand gestures and to generate realistic and coherent hand motions.\\n\\n3.3. Stage Two: Diverse Hand Prediction\\n\\nIn stage two, we diversify our 3D hand predictions based on the initial gestures at stage one. To achieve this, we utilize a Motion Discriminator (MD) module to distinguish between natural and diverse hand poses. The MD module is trained to distinguish between natural and diverse hand poses, allowing it to guide the generation of diverse hand poses. Additionally, we introduce a Motion Encoder (ME) module to learn the temporal features of the hand gestures. The ME module is connected to the body encoder and the spatial memory network, allowing it to access spatial and temporal features from both the body and the hand. This enables the model to learn the temporal consistency of the hand gestures and to generate realistic and coherent hand motions.\\n\\nMotion Discriminator (MD) Module:\\n\\nThe Motion Discriminator (MD) module is designed to distinguish between natural and diverse hand poses. To achieve this, we employ a neural network architecture that uses both spatial and temporal features to make decisions. The MD module is trained using a combination of natural and diverse hand poses, allowing it to learn the characteristics of both types of poses. Once trained, the MD module is used to guide the generation of diverse hand poses, ensuring that the generated poses are both natural and diverse.\\n\\nMotion Encoder (ME) Module:\\n\\nThe Motion Encoder (ME) module is designed to learn the temporal features of the hand gestures. To achieve this, we employ a neural network architecture that uses both spatial and temporal features to make decisions. The ME module is connected to the body encoder and the spatial memory network, allowing it to access spatial and temporal features from both the body and the hand. This enables the model to learn the temporal consistency of the hand gestures and to generate realistic and coherent hand motions.\\n\\n3.4. Training Details\\n\\nThe proposed method is trained using a combination of natural and diverse hand poses. During training, we use a combination of both natural and diverse hand poses to train the Motion Discriminator (MD) module. This allows the model to learn the characteristics of both types of poses, ensuring that the generated poses are both natural and diverse. Once trained, the MD module is used to guide the generation of diverse hand poses, ensuring that the generated poses are both natural and diverse.\\n\\nConclusion\\n\\nIn conclusion, we propose a novel method for predicting natural and diverse 3D hand poses from sequential upper body skeletons. Our method leverages disentangled bilateral branches and external memories to capture the complex body-hand interactions and asymmetric motions of two hands. The experimental results demonstrate the effectiveness of our method in generating realistic and coherent hand motions, outperforming existing methods.\\n\\nAcknowledgments\\n\\nThis work was supported by XYZ Foundation and the National Natural Science Foundation of China (Grant No. 61773175). We would like to thank the reviewers for their valuable suggestions and feedback, which helped us improve the quality of this paper.\\n\\nReferences\\n\\n[1] Bouazizi, M., et al. (2020). Multi-layer perception based human motion prediction network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n\\n[2] Wang, J., et al. (2019). Multi-range transformer for multi-person motion prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n\\n[3] Chen, Y., et al. (2021). Memory-based networks for long-sequence modeling. In Proceedings of the AAAI Conference on Artificial Intelligence.\\n\\n[4] Zhang, X., et al. (2019). Disentangled bilateral branches for 3D hand pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n\\n[5] Li, W., et al. (2020). Temporal-movement memory network for 3D hand pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition.\\n\\n[6] Gao, L., et al. (2021). Motion discriminator for diverse hand pose generation. In Proceedings of the AAAI Conference on Artificial Intelligence.\\n\\n[7] Li, Y., et al. (2020). Motion encoder for temporal consistent hand pose generation. In Proceedings of the AAAI Conference on Artificial Intelligence.\"}"}
{"id": "CVPR-2023-576", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Residual Memory (SRM) module to model the spatial interaction between the body and each hand by residual learning. As illustrated in Fig. 3 (b), we leverage a spatial memory bank to store the spatial residual deformation representation between the current hand feature and the next step hand feature. Initially, each slot of the spatial memory bank is randomized as $m_j \\\\in \\\\mathbb{R}^{1 \\\\times C}$, where $j$ denotes slot index. To model the body-hand correlation, the current body feature is exploited as a query to retrieve the most relevant memory slot by calculating the cosine similarity. In particular, we represent the relevance between the query and memory slots via cosine similarity. Then we leverage the argmax operation to acquire the most relevant memory slot $m_r$. Intuitively, the retrieved most relevant memory slot is leveraged as the spatial residual deformation between the current feature and the next step hand feature. However, it is intractable for argmax operation to back-propagate gradients. Thus, we conduct the softmax operation on the above calculated cosine similarity matrix to obtain the slot-wise affinity matrix. Then we acquire the learnable spatial residual deformation $F_{\\\\Delta t}^SH$ by aggregation between the slot-wise affinity matrix and each memory slot. We apply the above reading process to allow gradient back-propagation. In the training phase, we utilize the current hand feature to update the retrieved memory slot as $m_r = \\\\gamma m_r + (1 - \\\\gamma) F_{t}^SH$, where $\\\\gamma = 0.8$ in our experiments.\\n\\nDifferent from obtaining the next hand feature by simple addition between the spatial residual deformation and current hand feature, we claim the spatial residual deformation refers to local spatial dependency from the current feature to the next time feature. This spatial dependency is formulated as:\\n\\n$$S_{t+1}^t = \\\\text{softmax}(F_{\\\\Delta t}^SH \\\\odot F_{t}^SH),$$\\n\\n(2)\\n\\nwhere $\\\\odot$ indicates matrix multiplication, and $'$ indicates the transpose operation. With the help of the spatial dependency, the next step hand feature is represented as $F_{t+1}^SH = F_{t}^SH + F_{t}^SH \\\\odot S_{t+1}^t$. Through SRM, we obtain the spatial-enhanced single-hand feature.\\n\\nTemporal-Motion Memory Module: We further propose a Temporal-Motion Memory (TMM) module to ensure the single hand motions consistent with the body dynamics, globally. As depicted in Fig. 3 (c), we develop a motion encoder to obtain the sequence-aware body motion embedding as $F_{BM} \\\\in \\\\mathbb{R}^{T \\\\times 1}$. Here, we use body motion embedding to represent temporal changes among body skeletons in a sequence. Then, we leverage the body motion embedding as a query to retrieve the most consistent single hand motion embedding $F_{SHM} \\\\in \\\\mathbb{R}^{T \\\\times 1}$ stored in a temporal memory bank. The initialization and reading processes of the temporal memory bank are similar to SRM. We utilize the body motion embedding to update the retrieved slot of the temporal memory bank in the same manner as SRM. Afterward, we utilize the retrieved hand motion embedding to boost the body-hand temporal consistency. Specifically, we leverage the retrieved temporal embedding to represent the temporal correlation of hand gesture changes in a sequence. Then, the temporal-consistent enhanced sequential hand gestures are attained by:\\n\\n$$F_{SH} = F_{SH} + F_{SH} \\\\odot \\\\text{softmax}(F_{SHM})$$\\n\\nwhere $\\\\odot$ means dot product.\\n\\nTransformer-based Backbone: As illustrated in Fig. 2 (stage one), to further enhance the body-hand interaction, we construct the bilateral hand disentanglement branches that interact with a body-specific branch to predict 3D hands. In particular, we introduce a body-specific transformer to generate body features as query $Q$. Then, through 3 times Multi-Head Attention (MHA) [38], we use $Q$ to match the key features $K$ and value features $V$ in each hand transformer branch, respectively. Furthermore, we feed the concatenation of bilateral hand features into an MLP, thus obtaining the merged two-hand features. Similarly, we conduct 3 times MHA in the transformer-based decoder. This encourages the merged two-hand features to be more temporal consistent with body dynamics, holistically. At the end of the decoder, we adopt two fully-connected layers to obtain the 3D two hands sequence. Additionally, similar to [28], we employ a motion discriminator to ensure the natural and continuous initial prediction at stage one.\\n\\n3.3. Stage Two: Diverse Sampling\\n\\nWe leverage a Prototypical-Memory Sampling Strategy (PSS) to achieve hands diversification based on our initial predictions from stage one. Specifically, we first...\"}"}
