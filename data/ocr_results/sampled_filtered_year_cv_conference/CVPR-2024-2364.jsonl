{"id": "CVPR-2024-2364", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nIt is common to observe performance degradation when transferring models trained on some (source) datasets to target testing data due to a domain gap between them. Existing methods for bridging this gap, such as domain adaptation (DA), may require the source data on which the model was trained (often not available), while others, i.e., source-free DA, require many passes through the testing data. We propose an online test-time adaptation method for depth completion, the task of inferring a dense depth map from a single image and associated sparse depth map, that closes the performance gap in a single pass. We first present a study on how the domain shift in each data modality affects model performance. Based on our observations that the sparse depth modality exhibits a much smaller covariate shift than the image, we design an embedding module trained in the source domain that preserves a mapping from features encoding only sparse depth to those encoding image and sparse depth. During test time, sparse depth features are projected using this map as a proxy for source domain features and are used as guidance to train a set of auxiliary parameters (i.e., adaptation layer) to align image and sparse depth features from the target test domain to that of the source domain. We evaluate our method on indoor and outdoor scenarios and show that it improves over baselines by an average of 21.1%. Code available at github.com/seobbro/TTA-depth-completion.\\n\\n1. Introduction\\n\\nReconstructing the 3-dimensional (3D) structure of an environment can support a number of spatial tasks, from robotic navigation and manipulation to augmented and virtual reality. Most systems addressing these tasks are built for sensor platforms equipped with range (i.e., lidar or radar) or optics (i.e., camera or sensors). While range sensors can measure the 3D coordinates of the surrounding space, they often yield point clouds that are sparse. Likewise, these coordinates can also be estimated from images by means of Structure-from-Motion (SfM) or Visual Inertial Odometry (VIO). For the goal of dense mapping, depth completion is the task of recovering the dense depth of a 3D scene as observed from a sparse point cloud, which is often post-processed into a sparse depth map by projecting the points onto the image plane, and guided by a synchronized calibrated image.\\n\\nTraining a depth completion model can be done in a supervised (using ground truth) or unsupervised (using SfM) manner. The former dominates in performance, but requires expensive annotations that are often unavailable; the latter uses unannotated images, but they must satisfy SfM assumptions between frames, i.e., motion, covisibility, etc. Like most learning-based methods, models trained under both paradigms typically experience a performance drop when tested on a new dataset due to a covariate shift, i.e., domain gap. As we can only assume that a single pair of image and sparse depth map is available in the target domain for the depth completion, models belonging to either learning paradigms cannot easily be trained or adapted to the new domain even when given the testing data. We focus on test-time adaptation (TTA) for depth completion, where one is given access to the test data in a stream, i.e., one batch at a time, without being able to revisit previously-seen examples. The goal is to learn causally and to quickly adapt a set of pre-existing weights trained on a source domain to a target test domain, so one can reduce the performance gap.\\n\\nWe begin with some motivating observations on the effects of the domain gap: (i) Errors in target domain tend to be higher when feed both the image and sparse depth as input rather than sparse depth only, as shown in Fig. 1. This implies that the depth modality exhibits a smaller covariate shift between the source and target domains than the image modality, to the extent that forgoing the image altogether often yields superior results than using either both sparse depth and image or the image alone. (ii) Yet, when operating in the source domain, we observe the opposite effect \u2013 forgoing the image is detrimental to performance. Naturally, this begs the question: How should one leverage data modalities that are less sensitive to the domain shift (e.g., sparse depth) to support alignment between source and target domains for modalities that are more sensitive (e.g., RGB image)?\\n\\nTo answer this question, we investigate a test-time adaptation approach that learns an embedding for guiding\"}"}
{"id": "CVPR-2024-2364", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"model parameter update by exploiting the data modality (sparse depth) that is less sensitive to the domain shift. The embedding module maps the latent features encoding sparse depth to the latent features encoding both image and sparse depth. The mapping is trained in the source domain and frozen when deployed to the target domain for adaptation. During test time, sparse depth is first fed through the encoder and mapped, through the embedding module, to yield a proxy for image and sparse depth embeddings from the source domain \u2013 we refer to the embedded sparse depth features as proxy embeddings. Note: As the mapping is learned in the source domain, the proxy embeddings will also follow the distribution of source image and sparse depth embeddings. Next, both image and sparse depth from the target test domain are fed as input to the encoder. By maximizing the similarity between test-time input embeddings and the proxy embeddings, we align the target distribution to that of the source to reduce the domain gap. In other words, our method exploits a proxy modality for guiding test-time adaptation and we call the approach, ProxyTTA. When used in conjunction with typical loss functions to penalize discrepancies between predictions and input sparse depth, and abrupt depth transitions, i.e., Total Variation, the embeddings serve as regularization to guide the model parameter update and prevent excessive drift from those trained on the source data. Following test-time adaptation conventions, we assume limited computational resources, and that inputs arrive in a stream of small batches and must be processed within a time budget without access to the past data. To ensure fast model updates under these constraints, we deploy auxiliary parameters, or an adaptation layer, to be updated while freezing the rest of the network \u2013 thus achieving low-cost adaptation. We demonstrate our method in both indoor (VIO) and outdoor (lidar) settings across six datasets, where we not only target typical adaptation scenarios where the shift exists between real and synthetic data domains with similar scenes, i.e. from KITTI [39] to Virtual KITTI [11], but also between different scene layouts, i.e., from VOID [45] to NYUv2 [26] and SceneNet [25]. Our proxy embeddings consistently improve over baselines by an average of 21.09% across all methods and datasets. To the best of our knowledge, we are the first to introduce test-time adaptation for depth completion.\\n\\n2. Related work\\n\\nTest Time Adaptation (TTA) aims to adapt a given model, pretrained on source data, to test data without access to the source training data. Related fields along this vein include unsupervised domain adaptation [12, 28], which utilizes source domain data (in practice, this may not be available) for adaptation, and source-free domain adaptation [19], which does not assume access to source data, but allows access to test data on multiple passes. In contrast, we focus on test-time adaptation where we do not have access to source data and must adapt to test data in a single pass. Previous studies have proposed strategies to select the source model's component to be preserved, such as the class prototypes extracted from the source data [5, 22, 33], the subset of source model parameters [16, 42], and the discriminative feature from the self-supervised learning (SSL) [5]. For instance, [42] proposes TENT, a simple but effective batch-norm layer adaptation with entropy minimization for fully test-time adaptation. TTT [38] performs classification-layer adaptation by updating the last linear layer of the source model; [22] extends this with TTT++ and utilizes joint task-specific and model-specific information based on self-supervised learning. [16] presents T3A, an optimization-free classifier adjustment module. [5] uses shift-agnostic weight regularization (SWR) to prevent an effect from the erroneous signal in test time, jointly with the nearest source prototype classifier and a self-supervised proxy task. [2] proposes a contrastive learning with an online pseudo-label refinement while [33] proposes pseudo-label refinement and momentum update for 3D point cloud segmentation. [43] proposes continual test-time adaptation based on stochastic restoration and weight-averaged pseudo-labels. [35] uses efficient residual modules to realign the pretrained weights. The above methods largely focus on single-image-based tasks, i.e., classification [2, 5, 22] and semantic segmentation [33], and rely on entropy constraints from [42]. Unlike prior work on classification [22, 38] and segmentation [33], depth completion is a regression problem; hence, existing methods using entropy-based objectives [42], which operate on logits, are not applicable in this task. Instead, we propose to minimize sparse depth reconstruction and local smoothness objectives \u2013 similar to that of some existing unsupervised methods [44, 45] \u2013 and to maximize cosine similarity between the proxy embeddings and the test time image and sparse depth embeddings.\\n\\nDepth Completion aims to output dense depth from a single image and synchronized point cloud, i.e., from lidar or tracked by VIO, projected onto a sparse depth map, by multimodal fusion [8, 45, 50\u201352]. Unsupervised depth completion approaches rely on Structure-from-Motion (SfM) for training and require access to an auxiliary training dataset containing stereo image pairs [34] or monocular videos [24, 45\u201348] with synchronized sparse depth maps. Typically, they minimize a linear combination of photometric reprojection consistency, sparse depth reconstruction error, and local smoothness [24, 45\u201347]. These methods can support online training, but are limited by the need for stereo or monocular videos with sufficient parallax and co-visibility. In contrast, our approach does not rely on SfM and can be used in more general scenarios. Supervised depth completion trains the model by minimizing a loss with respect to ground truth. [3, 15] focus on network operations and designs to effectively deal with...\"}"}
{"id": "CVPR-2024-2364", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Model sensitivity to input modalities. While utilizing both sparse depth and image as input, the best performance is achieved in the source domain (VOID). Yet, forgoing the image in the test domain (NYUv2) often yields lower error than using both as input.\\n\\nSparse inputs. [17, 24, 53] propose early and late fusion of image and depth encoder features while [14] uses separate networks for each. [20] proposes a multi-scale cascaded hourglass network to enhance the depth encoder with image features. [4] proposes convolutional spatial propagation network; [27] extends it to non-local spatial propagation to refine an initial depth map based on confidence and learnable affinity; [21] further extends it to dynamic spatial propagation. [9, 10, 30, 31] learn uncertainty of the depth estimates. [41] utilizes confidence maps to combine depth predictions while [29, 49, 55] use the surface normals to guide depth prediction. [18] incorporates cost volume for depth prediction. [32] devises transformer architecture with cross-modal attention, and [54] proposes a hybrid convolution-transformer architecture for depth completion.\\n\\nWhile both unsupervised and supervised methods have demonstrated strong performance on benchmarks, they often fail to generalize to test datasets with large domain discrepancies. Moreover, obtaining ground truth is unrealistic for real-time applications, and accumulating sufficient parallax incurs large latencies \u2013 presenting significant challenges for online adaptation. Unlike past works, we do not assume access to ground truth nor data outside of the input.\\n\\nUnsupervised Domain Adaptation (UDA) addresses the discrepancy between labeled source data and unlabeled target data [6, 12, 28, 37]. The only existing UDA depth completion method [23] models the domain gap as the noise in sparse points and the appearance in images. Unlike most UDA approaches that require source data during adaptation, we are only given the inputs necessary for inference in a stream without the ability to revisit past data, and must update the model online under a limited computational budget.\\n\\nOur Contributions. We present (i) a study on how the domain shift in each data modality (e.g., image and sparse depth) affects model performance when transferring it from source to target test domain. This study motivates (ii) our approach to learn an embedding of sparse depth features (which are less sensitive to the domain shift) that serves as proxy to source features for guiding test-time adaptation. (iii) To the best of our knowledge, we are the first to propose test-time adaptation for the depth completion task, and (iv) will release code, models, and dataset benchmarking setup to make development accessible for the research community.\\n\\n3. Method Formulation\\n\\nFor ease of use, we assume access to a (source) pretrained depth completion model $f_\\\\theta$ that infers a dense depth map $\\\\hat{d}$ from a calibrated RGB image $I \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 3}$ and its associated sparse point cloud projected onto the image plane as a sparse depth map $z \\\\in \\\\mathbb{R}^{H \\\\times W +}$, i.e., $f_\\\\theta(I, z) \\\\rightarrow \\\\hat{d} \\\\in \\\\mathbb{R}^{H \\\\times W +}$.\\n\\nFor simplicity, we assume that the model was trained to minimize a supervised loss between prediction and ground truth $d \\\\in \\\\mathbb{R}^{H \\\\times W +}$ on a source dataset $D_s = \\\\{I_s(n), z_s(n), d_s(n)\\\\}_{n=1}^{N_s}$, where $N_s$ indicates the number of data samples. Following conventions in TTA, we assume access to the source domain dataset prior to deployment.\\n\\nDuring test-time adaptation, we follow the protocol of [22, 38], where we only have access to the target domain data $D_t = \\\\{I_t(n), z_t(n)\\\\}_{n=1}^{N_t}$ and utilize an online procedure to adapt to unseen $N_t$ target data samples. Note that we make no assumptions about supervision during test-time; hence, while we present results on supervised methods for controlled experiments, we see our method being applicable towards unsupervised methods as well.\\n\\nOur method, ProxyTTA, is split into three stages (Fig. 3):\"}"}
{"id": "CVPR-2024-2364", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Model sensitivity to input modalities.\\n\\nDepth completion networks have a high reliance on sparse depth modality. Performing inference in a novel domain without the RGB image, i.e., using just sparse depth as input, can improve over using both data modalities.\\n\\n(a) During an initial stage, we augment the network encoder with an adaptation layer and train it using source domain data. (b) In the preparation stage, we learn a mapping from sparse depth features to image and sparse depth (proxy) embeddings. (c) During test time, we do not need the source dataset; we freeze the mapping and use its proxy embeddings for updating the adaptation layer parameters in test domain.\\n\\n3.1. Sensitivity Study on Data Modalities\\n\\nTo motivate our approach, we begin with a sensitivity study of depth completion networks to input modalities, e.g., image, sparse depth, and the effect of domain shift on them. To this end, we alter the inputs by zeroing out either $I$ or $z$ to yield $(I, z)$, $(I_0, z)$, and $(I, z_0)$, where $I_0$ and $z_0$ indicate the zero matrices with identical size to $I$ and $z$, respectively. We evaluate the pretrained models using $(I, z)$, $(I_0, z)$, and $(I, z_0)$ to highlight their dependence on each input modality and to gauge their sensitivity when one modality gives no useful information at all. Fig. 1 and Fig. 2 show qualitative (error maps) and quantitative results (bar graphs), respectively, of pretrained depth completion models when fed the different inputs on the source dataset $D_s$ and the target dataset $D_t$.\\n\\nIn the source domain, inference using both image and sparse depth as inputs, i.e., $\\\\hat{d}_s(I, z)$, shows the best performance. Surprisingly, the inference using sparse depth alone (i.e., with null-image) $\\\\hat{d}_s(I_0, z)$ is comparable to $\\\\hat{d}_s(I, z)$. This shows the first intuition behind our approach: (i) Even though depth inputs are sparse, they are sufficient to support the reconstruction of the scene. Additionally, inference with image alone (i.e., null-depth) $\\\\hat{d}_s(I, z_0)$ is worse than $\\\\hat{d}_s(I_0, z)$ and $\\\\hat{d}_s(I, z)$, which suggests that a depth completion network relies heavily on sparse depth modality for inference, and the image for guiding recovery of finer details.\\n\\nIn the target test domain, expectedly, performance degrades for inference using both image and sparse depth due to a covariate shift. Remarkably, we observe that predictions from sparse depth alone $\\\\hat{d}_t(I_0, z)$ remain consistent in performance to those using both inputs $\\\\hat{d}_t(I, z)$. Moreover, we observe that in most cases $\\\\hat{d}_t(I_0, z)$, in fact, outperforms $\\\\hat{d}_t(I, z)$ across several methods and datasets, i.e., inference without image information in the test domain is better than with it. Conversely, the performance gap between inference with both inputs, $\\\\hat{d}_t(I, z)$, and just the image, $\\\\hat{d}_t(I, z_0)$, becomes more evident under the domain shift. This observation illustrates another intuition: (ii) The domain shift largely affects the image modality, and less so depth.\\n\\nThe two intuitions above motivate our approach. As object shapes tend to persist across domains, and the measured sparse points being a coarse representation of them, we aim to leverage sparse depth modality to bridge the domain gap. To this end, we exploit the observation that depth completion networks are able to recover (coarse) 3D scenes from sparse points alone and that the image serves to propagate and refine depth for regions lacking points. This is done by learning to map features encoding sparse depth inputs to features encoding both modalities in the source domain and, during test-time, recover the source domain features compatible with target domain sparse depth to guide model adaptation. Specifically, as observed, the covariate shift is largely photometric, so we propose to adapt the RGB image encoder branch by introducing a adaptation layer: a single convolutional layer designed to align target domain RGB embeddings to those of the source domain. As the rest of the network is frozen, adapting just the adaptation layer allows for low-cost model updates.\\n\\nIntuition for integrating adaptation layer.\\n\\nGuided by our observations, the adaptation layer should be (i) placed in the image encoder branch prior to the fusion of image and depth features, and (ii) located within later layers to modulate higher level representations (i.e., object shapes, as opposed to low-level edges). (iii) Connected as a skip connection to decoder to more directly affect the output.\"}"}
{"id": "CVPR-2024-2364", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2. Preparation Stage - Source Domain\\n\\nInitialize adaptation layer from source domain.\\n\\nUpdating the entire network is largely infeasible in test-time adaptation scenarios. For the sake of speed and efficiency, we implement an adaptation layer $m_{\\\\phi}$, i.e., a convolutional layer, within the encoder of a pretrained network. Note that the entire network will be frozen during all stages of our method with the exception of the adaptation layer and proxy mapping, where both will be initialized during preparation stage in the source domain; the proxy mapping will then be frozen and used to adapt $m_{\\\\phi}$ in the target domain. To ease the adaptation process, we initialize the $m_{\\\\phi}$ by minimizing a supervised loss over the source dataset (Fig. 3-(a)). We denote the pretrained encoder integrated with $m_{\\\\phi}$ as $e_{\\\\phi}$.\\n\\nLearning proxy mapping from source domain.\\n\\nAs observed in Fig. 1, the best results in the source domain are achieved by feeding in both the image and sparse depth modalities for inference. However, the image is susceptible to domain shift which degrades performance when the model is transferred to an unseen test domain. Conversely, sparse depth is more resilient to the domain shift than RGB images, i.e., the shape of a car (or another object) remains similar regardless of (synthetic or real) domain. Our method aims to leverage the sparse depth modality, which is less sensitive to the domain shift, in the downstream adaptation process. To this end, we employ a soft mapping [13] from just the encoded sparse depth features to sparse depth and image features to learn the photometric information that is captured from the same scene as the sparse point cloud. This strategy allows us to learn the mapping that projects the sparse depth features to \\\"proxy\\\" embeddings close to those that also encode the image. In other words, it fills in what is missing in the image encoder branch by predicting the residual latent image encoding that is compatible with the input sparse depth, i.e., 3D scene. As this is trained in the source domain, the mapping naturally yields proxy embeddings that encode the source domain image (and sparse depth), which can be later used to guide the adaptation layer $m_{\\\\phi}$ to transform test domain RGB features close to those of the source domain. This mapping by MLPs can be denoted as $g_{\\\\psi}(\\\\cdot)$, $g_{\\\\psi}'(\\\\cdot)$; to learn them, we get two embeddings $ppp_s$ and $qqq_s$,\\n\\n$$ppp_s = h_{\\\\omega}(g_{\\\\psi}(StopGrad(e_{\\\\phi}(I_0, z_s)))),$$\\n\\n$$qqq_s = StopGrad(g_{\\\\psi}'(e_{\\\\phi}(I_s, z_s))).$$ (1)\\n\\nwhere $e_{\\\\phi}$ denotes the encoder augmented with the adaptation layer trained on source dataset, $I_s, z_s$, the image and sparse depth from source domain, and $I_0$ the null-image. The embedding modules $g_{\\\\psi}$ and $h_{\\\\omega}$ are updated to maximize the similarity between $ppp_s$ and $qqq_s$. To learn them, we minimize:\\n\\n$$\\\\ell_{\\\\text{prepare}} = 1 - \\\\frac{ppp_s \\\\parallel ppp_s \\\\parallel \\\\cdot qqq_s \\\\parallel qqq_s \\\\parallel}{(ppp_s \\\\parallel qqq_s \\\\parallel)}.$$ (2)\\n\\nwhere $\\\\parallel \\\\cdot \\\\parallel$ is $L_2$-norm, and $(aaa \\\\cdot bbb)$ indicates the dot product of the vectors $aaa$ and $bbb$. To this end, we first train the MLP heads $g_{\\\\psi}, h_{\\\\omega}$ by minimizing Eqn. 2. Note that the MLP head $g_{\\\\psi}'$ is updated with EMA update following BYOL [13] to avoid collapse:\\n\\n$$g_{\\\\psi}' \\\\leftarrow \\\\tau \\\\cdot g_{\\\\psi}' + (1 - \\\\tau) \\\\cdot g_{\\\\psi}.$$ (3)\\n\\nOnce the mapping is learned, we can freeze the embedding module and deploy it for test-time adaptation where we update the adaptation layer weights $\\\\phi$ to maximize the similarity between the embeddings of a test domain image and sparse depth, and its proxy from the source domain. Naturally, due to the domain shift, the embeddings will yield\"}"}
{"id": "CVPR-2024-2364", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"low similarity scores; hence, maximizing the scores through our proxy embedding implicitly aligns the target RGB distribution to that of the source distribution, i.e., minimizing the cosine similarity between the source and target distributions.\\n\\n3.3. Deploying Proxy Mapping to Target Domain\\n\\nAdaptation stage aims to update the adaptation layer parameters by minimizing a test-time loss function over the target test domain data $I_t, z_t \\\\in D_t$. To do so, we deploy the learned proxy mapping module (MLP heads $g^*\\\\psi(\\\\cdot), g^*\\\\psi'(\\\\cdot), h^*\\\\omega(\\\\cdot)$) along with the adaptation layer $m\\\\phi$ integrated into the frozen encoder as $e\\\\phi$.\\n\\nAdaptation loss. For adaptation, our loss is composed of a linear combination of three loss terms:\\n\\n$$L_{adapt} = w_z\\\\ell_z + w_{sm}\\\\ell_{sm} + w_{proxy}\\\\ell_{proxy}, \\\\quad (3)$$\\n\\nwhere $\\\\ell_z, \\\\ell_{sm}$ denote sparse depth consistency loss and local smoothness loss, respectively, $\\\\ell_{proxy}$ is proxy mapping consistency loss, and $w$ indicates a weight of each loss term.\\n\\nSparse Depth Consistency. Sparse point clouds capture a coarse structure of the 3D scene. To obtain metric scale predictions consistent with the scene structure, we minimize $L_1$ error between the sparse depth $z_t$ and the prediction $\\\\hat{d}_t$:\\n\\n$$\\\\ell_z = \\\\frac{1}{|\\\\Omega(z_t)|} \\\\sum_{x \\\\in \\\\Omega(z_t)} |\\\\hat{d}_t(x) - z_t(x)|, \\\\quad (4)$$\\n\\nwhere $x \\\\in \\\\Omega(z_t)$ are the pixel locations where sparse points were projected onto the image plane.\\n\\nLocal Smoothness. Based on the assumption of local smoothness and connectivity in a 3D scene, we impose the same in the predicted depth map $\\\\hat{d}_t$. Specifically, we apply an $L_1$ penalty to its gradients in both the x- and y-directions (i.e., $\\\\partial_X$ and $\\\\partial_Y$). We balance the weight of each term with $\\\\lambda_X$ and $\\\\lambda_Y$, to allow discontinuities over object boundaries based on the image gradients, where $\\\\lambda_X(x) = e^{-|\\\\partial_X I_t(x)|}$, $\\\\lambda_Y(x) = e^{-|\\\\partial_Y I_t(x)|}$, and $\\\\Omega$ denotes the image domain.\\n\\n$$\\\\ell_{sm} = \\\\frac{1}{|\\\\Omega|} \\\\sum_{x \\\\in \\\\Omega} \\\\lambda_X(x)|\\\\partial_X \\\\hat{d}_t(x)| + \\\\lambda_Y(x)|\\\\partial_Y \\\\hat{d}_t(x)|. \\\\quad (5)$$\\n\\nProxy Consistency. In order to regularize the adaptation with the learned mapping from the previous stage, we freeze the weight parameters of MLP heads $g^*\\\\psi(\\\\cdot), h^*\\\\omega(\\\\cdot)$, and update the parameters of the adaptation layer $m\\\\phi$. First, we obtain the features $ppp_t$ and $qqq_t$ using the null-image $I_0$ in one and the given target test domain image $I_t$ in the other:\\n\\n$$ppp_t = \\\\text{StopGrad}(h^*\\\\omega(g^*\\\\psi(e\\\\phi(I_0, z_t))))), \\\\quad qqq_t = g^*\\\\psi'(e\\\\phi(I_t, z_t)). \\\\quad (6)$$\\n\\nWe maximize the cosine similarity between the feature $qqq_t$ and $ppp_t$ via a proxy loss $\\\\ell_{proxy}$ to update adaptation layer $m\\\\phi$:\\n\\n$$\\\\ell_{proxy} = 1 - \\\\frac{ppp_t \\\\parallel ppp_t \\\\parallel \\\\cdot qqq_t \\\\parallel qqq_t \\\\parallel}{}, \\\\quad (7)$$\\n\\n4. Experiments\\n\\nWe demonstrate the effectiveness of our approach on a mix of both real and synthetic datasets including indoor SLAM/VIO scenarios (VOID [45], NYUv2 [26], SceneNet [25], and ScanNet [7]) and outdoor driving scenarios using lidar sensor (KITTI [39], Virtual KITTI (VKITTI) [11], nuScenes [1], and Waymo Open Dataset [36]). We chose three representative architectures of current depth completion methods to test our method: MSG-CHN [20] (CNN-based), NLSPN [27] (SPN-based) and CostDCNet [18] (cost volume-based). All reported results are averaged over 5 independent trials. We describe implementation details, hyper-parameters used, hardware requirements, evaluation metrics as well as additional experimental results in the Supp. Mat.\\n\\nMain Result. We use pretrained models (MSG-CHN, NLSPN, and CostDCNet) from the two source datasets, VOID for indoor, and KITTI for outdoor. For indoor, we adapt models pretrained on VOID to NYUv2, SceneNet, and ScanNet; for outdoors, we adapt from KITTI to VKITTI (with fog), nuScenes, and Waymo. BN Adapt denotes updating the batch statistics (i.e., running mean and variance). BN Adapt, $\\\\ell_z, \\\\ell_{sm}$ is a variation of TENT [42] which minimizes Eqn. 4, 5 instead of entropy by updating learnable scale factors. CoTTA denotes replacing proxy loss with $L_1$ consistency loss w.r.t. the pretrained prediction [43]. ProxyTTA-fast denotes our method without batch norm update, which improves adaptation runtime by 25.32%.\\n\\nOur method consistently improves over baselines and variants of BN Adapt (Table 1). Specifically, we improve over BN Adapt, $\\\\ell_z, \\\\ell_{sm}$ by 11.60% on average across all methods for indoor, 19.73% on outdoors, and 15.67% overall to achieve state-of-the-art performance. Qualitatively, Fig. 4 and Fig. 5 show that our method performs better in boundary regions and homogeneous regions, thus exhibiting less oversmoothing on curtains in Fig. 4-(a) and car in Fig. 5-(b), and undersmoothing on blackboard in Fig. 4-(d) and road in Fig. 5-(a), respectively, during adaptation. This trend is due to the proxy loss and the adaptation layer, which allows us to adapt with minimum weight adjustments while preserving high-level features (object shapes) learned from the source domain by mapping the target RGB modality to that of the source domain. Notably, ProxyTTA-fast still improves over BN Adapt even though we only adapt our adaptation layer, which demonstrates the effectiveness of our design choice as well as our proposed proxy embeddings. We visualize image and sparse depth features from the source and target domains along with proxy embeddings in the target domain using t-SNE [40] in Fig. 1 of Supp. Mat.; we observe that proxy embeddings are close to source domain features.\\n\\nComparison to BN adaptation\\n\\nTo assess the impact of our adaptation layer, we compare to batch norm\\n\\n1 MSG-CHN lacks Batch Norm (BN) layer so we cannot use BN adapt.\\n\\n20524\\n\\n20524\"}"}
{"id": "CVPR-2024-2364", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method       | MAE     | RMSE    | MAE     | RMSE    |\\n|--------------|---------|---------|---------|---------|\\n| KITTI        |         |         |         |         |\\n| \u2192 VKITTI-FOG |         |         |         |         |\\n| KITTI        |         |         |         |         |\\n| \u2192 nuScenes   |         |         |         |         |\\n| KITTI        |         |         |         |         |\\n| \u2192 Waymo      |         |         |         |         |\\n| MSG-CHN      |         |         |         |         |\\n| Pretrained   | 2842.88 | 6557.38 | 3331.82 | 6449.09 |\\n| CoTTA        | 730.6   | \u00b111.67  | 3330.23 | \u00b144.83  |\\n| ProxyTTA-fast (Ours) | 728.24 | \u00b13.73   | 3087.36 | \u00b115.92  |\\n| NLSPN        |         |         |         |         |\\n| Pretrained   | 1309.99 | 7423.48 | 2656.60 | 6146.59 |\\n| BN Adapt     | 1140.21 | \u00b135.89  | 4592.86 | \u00b1198.21 |\\n| CoTTA        | 767.93  | \u00b15.47   | 3799.88 | \u00b117.29  |\\n| ProxyTTA-fast (Ours) | 732.61 | \u00b129.57  | 3002.19 | \u00b152.29  |\\n| CostDCNet    |         |         |         |         |\\n| Pretrained   | 1042.98 | 6301.60 | 3064.72 | 6630.64 |\\n| BN Adapt     | 1476.57 | \u00b11.38   | 5428.20 | \u00b18.15   |\\n| CoTTA        | 147.69  | \u00b15.30   | 376.87  | \u00b121.25  |\\n| ProxyTTA-fast (Ours) | 131.93 | \u00b12.58   | 269.02  | \u00b15.61   |\\n\\n| Table 1. Qualitative results. For indoors, we adapt from VOID to NYUv2, SceneNet, and ScanNet; for outdoors, from KITTI to VKITTI with fog, nuScenes, and Waymo. Bold denotes best and Italics second-best. ProxyTTA-fast denotes our method without updating BatchNorm. (BN) adaptation from TENT [42]. In BN adaptation, we only update the batch norm layer\u2019s scale and shift factor based on the loss function. On average, BN Adapt with $\\\\ell_z, \\\\ell_{sm}$ improves the pretrained model by 32.77%; whereas, updating just our adaptation layer (ProxyTTA-fast) improves it more by 34.07% (Table 1). The improvement of ProxyTTA-fast over BN adapt demonstrates the efficacy of updating adaptation layer, which directly adjusts the high-level features from the RGB branch guided by proxy loss, where BN adapt re-aligns the learned source features from both RGB and range sensors by updating feature statistics. Nonetheless, the best results are achieved when we include batch norm update, which improves the pretrained model by 44.53%, but at the cost $\\\\approx 33.2\\\\%$ of total extra time. The improvement of ProxyTTA over BN adapt implies that the large domain discrepancy may not be addressed by adapting only BN parameters (i.e., scaling and shifting); ProxyTTA explicitly adjusts RGB features by updating the adaptation layer with proxy embeddings as guidance. We also compared our approach to CoTTA [43], which adapts the whole model parameters using the prediction from the teacher model updated by exponential moving average of pretrained weight and the model prediction. We combined additional loss $\\\\ell_z, \\\\ell_{sm}$ on top of CoTTA loss, since we observed that the models cannot be adapted with CoTTA alone. Specifically, our method without proxy shows a 25.26% average improvement on the CoTTA method. CoTTA updates the whole parameters including RGB and sparse depth branch, which causes a drift from the learned model parameters. On the other side, our method only updates additional layer at RGB branch, based on the study from the most domain discrepancy comes from RGB modality as studied in Sec. 3.1, and this prevents the model from a drift from learned domain. Also, CoTTA assumes the test-time augmentation can mitigate the domain shift. However, the results shows test-time augmentation on RGB image, causing a small distributional shift, may not solve a large domain discrepancy. Also, our method with batch normalization layer update shows 26.52% average improvement, while using 25.05% less adaptation time. Note: CoTTA costs not only additional memory for teacher model but also inference time to get the teacher model prediction, even if CoTTA does not require any preparation process. Overall, our method shows 21.09% average improvement over BN adapt and CoTTA methods. |\"}"}
{"id": "CVPR-2024-2364", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Qualitative results on NYUv2. For indoors scenarios, ProxyTTA performs better in boundary regions displaying the discontinuity in depth (e.g., curtains, (a)), as well as homogeneous regions (e.g., blackboard, (d)). Boxes highlight detailed comparisons.\\n\\nFigure 5. Qualitative results on NuScenes. For outdoor adaptation scenarios, ProxyTTA improves over BN Adapt and CoTTA, notably in both depth-discontinuous regions (e.g., car in (b)) and homogeneous regions (e.g., road in (a) and (b)). Boxes highlight detailed comparisons.\\n\\n5. Discussion\\nWe have proposed a method for test-time adaptation for depth completion that leverages the strength of complementary multi-sensor setup in the presence of domain shift. By studying model sensitivity to each input modality as well as the data under domain shift, we designed a way to exploit the modality (sparse depth) that is less sensitive to guide adaptation. We do so through a proxy embedding that learns the photometric information from the source domain that is compatible with the sparse depth depicting a 3D scene. Our proxy embedding works well as a regularizer for scenarios where there exists covariate shifts in photometry (i.e., KITTI to VKITTI) as well as scene layouts (i.e., VOID to NYUv2 and SceneNet). While one may surmise that the application of the embeddings are specific to scene distributions, we show otherwise. VOID (classrooms, laboratories, and gardens), NYUv2 (households and shopping centers), and SceneNet (randomly arranged synthetic rooms) all differ in layouts. The proxy embedding captures latent photometric features of the object shapes populating them; the same proxy embedding can be transferred across domains even when scene differ, but share objects within them. This leads to possible limitations in the scenarios where the source dataset is sampled from scenes that do not share any objects with the target test dataset; in this case, the proxy embeddings should give little to no gain and one must rely on generic regularizers like local smoothness. Additionally, while we follow the conventions in TTA and assume access to the source dataset prior to deployment, in reality, many models are trained on private datasets, so adapting \u201coff-the-shelf\u201d models remains a challenge. In such cases, one must incorporate our preparation pipeline into their model training and release the adaptation layer and proxy embedding module together with network weights. Nonetheless, this is the first test-time adaptation work in depth completion; in addition to our findings, we release models, dataset, adaptation, and evaluation code, and hope to further motivate interest in TTA for multi-modal tasks like depth completion.\\n\\nAcknowledgements.\\nThis work was supported by NSF-2112562.\"}"}
{"id": "CVPR-2024-2364", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giacarlo Baldan, and Oscar Beijbom. nuscenes: A multi-modal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11621\u201311631, 2020.\\n\\n[2] Dian Chen, Dequan Wang, Trevor Darrell, and Sayna Ebrahimi. Contrastive test-time adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 295\u2013305, 2022.\\n\\n[3] Yun Chen, Bin Yang, Ming Liang, and Raquel Urtasun. Learning joint 2d-3d representations for depth completion. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10023\u201310032, 2019.\\n\\n[4] Xinjing Cheng, Peng Wang, Chenye Guan, and Ruigang Yang. Cspn++: Learning context and resource aware convolutional spatial propagation networks for depth completion. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 10615\u201310622, 2020.\\n\\n[5] Sungha Choi, Seunghan Yang, Seokeon Choi, and Sungrack Yun. Improving test-time adaptation via shift-agnostic weight regularization and nearest source prototypes. In ECCV, pages 440\u2013458. Springer, 2022.\\n\\n[6] Safa Cicek and Stefano Soatto. Unsupervised domain adaptation via regularized conditional alignment. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1416\u20131425, 2019.\\n\\n[7] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5828\u20135839, 2017.\\n\\n[8] Yiming Dou, Fengyu Yang, Yi Liu, Antonio Loquercio, and Andrew Owens. Tactile-augmented radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024.\\n\\n[9] Abdelrahman Eldesokey, Michael Felsberg, and Fahad Shahbaz Khan. Propagating confidences through cnns for sparse data regression. arXiv preprint arXiv:1805.11913, 2018.\\n\\n[10] Abdelrahman Eldesokey, Michael Felsberg, Karl Holmquist, and Michael Persson. Uncertainty-aware cnns for depth completion: Uncertainty from beginning to end. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12014\u201312023, 2020.\\n\\n[11] Adrien Gaidon, Qiao Wang, Yohann Cabon, and Eleonora Vig. Virtual worlds as proxy for multi-object tracking analysis. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4340\u20134349, 2016.\\n\\n[12] Yaroslav Ganin and Victor Lempitsky. Unsupervised domain adaptation by backpropagation. In International conference on machine learning, pages 1180\u20131189. PMLR, 2015.\\n\\n[13] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your own latent-a new approach to self-supervised learning. Advances in neural information processing systems, 33:21271\u201321284, 2020.\\n\\n[14] Mu Hu, Shuling Wang, Bin Li, Shiyu Ning, Li Fan, and Xiaojin Gong. Penet: Towards precise and efficient image guided depth completion. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 13656\u201313662. IEEE, 2021.\\n\\n[15] Zixuan Huang, Junming Fan, Shenggan Cheng, Shuai Yi, Xiaogang Wang, and Hongsheng Li. Hms-net: Hierarchical multi-scale sparsity-invariant network for sparse depth completion. IEEE Transactions on Image Processing, 29:3429\u20133441, 2019.\\n\\n[16] Yusuke Iwasawa and Yutaka Matsuo. Test-time classifier adjustment module for model-agnostic domain generalization. 34:2427\u20132440, 2021.\\n\\n[17] Maximilian Jaritz, Raoul De Charette, Emilie Wirbel, Xavier Perrotton, and Fawzi Nashashibi. Sparse and dense data with cnns: Depth completion and semantic segmentation. In 2018 International Conference on 3D Vision (3DV), pages 52\u201360. IEEE, 2018.\\n\\n[18] Jaewon Kam, Jungeon Kim, Soongjin Kim, Jaesik Park, and Seungyong Lee. Costdcnet: Cost volume based depth completion for a single rgb-d image. In European Conference on Computer Vision, pages 257\u2013274. Springer, 2022.\\n\\n[19] Youngeun Kim, Donghyeon Cho, Kyeongtak Han, Priyadarshini Panda, and Sungeun Hong. Domain adaptation without source data. IEEE Transactions on Artificial Intelligence, 2(6):508\u2013518, 2021.\\n\\n[20] Ang Li, Zejian Yuan, Yonggen Ling, Wanchao Chi, Chong Zhang, et al. A multi-scale guided cascade hourglass network for depth completion. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 32\u201340, 2020.\\n\\n[21] Yuankai Lin, Tao Cheng, Qi Zhong, Wending Zhou, and Hua Yang. Dynamic spatial propagation network for depth completion. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1638\u20131646, 2022.\\n\\n[22] Yuejiang Liu, Parth Kothari, Bastien Van Delft, Baptiste Bellot-Gurlet, Taylor Mordan, and Alexandre Alahi. TTT++: When does self-supervised test-time training fail or thrive? 34:21808\u201321820, 2021.\\n\\n[23] Adrian Lopez-Rodriguez, Benjamin Busam, and Krystian Mikolajczyk. Project to adapt: Domain adaptation for depth completion from noisy and sparse sensor data. In Proceedings of the Asian Conference on Computer Vision, 2020.\\n\\n[24] Fangchang Ma, Guilherme Venturelli Cavalheiro, and Sertac Karaman. Self-supervised sparse-to-dense: Self-supervised depth completion from lidar and monocular camera. In 2019 International Conference on Robotics and Automation (ICRA), pages 3288\u20133295. IEEE, 2019.\\n\\n[25] John McCormac, Ankur Handa, Stefan Leutenegger, and Andrew J Davison. Scenenet rgb-d: 5m photorealistic images of synthetic indoor trajectories with ground truth. arXiv preprint arXiv:1612.05079, 2016.\\n\\n[26] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012.\"}"}
{"id": "CVPR-2024-2364", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2024-2364", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yanchao Yang, Alex Wong, and Stefano Soatto. Dense depth posterior (ddp) from single image and sparse range. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3353\u20133362, 2019.\\n\\nZhang Youmin, Guo Xianda, Poggi Matteo, Zhu Zheng, Huang Guan, and Mattoccia Stefano. Completionformer: Depth completion with convolutions and vision transformers. arXiv preprint arXiv:2304.13030, 2023.\\n\\nYinda Zhang and Thomas Funkhouser. Deep depth completion of a single rgb-d image. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 175\u2013185, 2018.\"}"}
