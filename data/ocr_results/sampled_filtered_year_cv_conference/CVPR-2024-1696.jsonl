{"id": "CVPR-2024-1696", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Novel View Synthesis with View-Dependent Effects from a Single Image\\n\\nJuan Luis Gonzalez Bello\\njuanluisgb@kaist.ac.kr\\n\\nMunchurl Kim\\nmkimee@kaist.ac.kr (corresponding author)\\n\\nSchool of Electrical Engineering\\nKorea Advanced Institute of Science and Technology (KAIST)\\n\\nFigure 1. Comparison of single-view NVS. See Supplemental and https://shorturl.at/ltJT7 for videos.\\n\\nAbstract\\n\\nIn this paper, we address single image-based novel view synthesis (NVS) by firstly integrating view-dependent effects (VDE) into the process. Our approach leverages camera motion priors to model VDE, treating negative disparity as the representation of these effects in the scene. By identifying that specularities align with camera motion, we infuse VDEs into input images by aggregating pixel colors along the negative depth region of epipolar lines. Additionally, we introduce a \u2018relaxed volumetric rendering\u2019 approximation, enhancing efficiency by computing densities in a single pass for NVS from single images. Notably, our method learns single-image NVS from image sequences alone, making it a fully self-supervised learning approach that requires no depth or camera pose annotations. We present extensive experimental results and show that our proposed method can learn NVS with VDEs, outperforming the SOTA single-view NVS methods on the RealEstate10k and MannequinChallenge datasets. Visit our project site\\n\\n1. Introduction\\n\\nNovel view synthesis (NVS) is a fundamental computer vision task that aims to generate new views of the input scene from arbitrarily different camera positions. Recent advances in NVS have shown that view-dependent effects (VDE), which increase the realism and perceived quality of the novel views, can also be learned and incorporated into the rendering pipelines. Particularly, NeRF [32] has demonstrated that radiance fields can be effectively learned by a multi-layer perceptron (MLP) that allows rendering geometry and VDEs from multi-view captures.\\n\\nNeRF relies purely on multi-view consistency and cannot exploit prior knowledge, such as textures and depth cues common across natural scenes. This limits NeRF when only a few or only one view is available. On the other hand, to leverage the 3D prior knowledge in multi-view datasets, PixelNeRF [46] proposed to train an MLP that takes as inputs the spatial locations, the viewing directions, and the pixel-aligned deep features to generate the colors and densities in the radiance fields. However, PixelNeRF is still limited in learning the ill-posed task of directly mapping input image pixels to VDEs. Other works, such as single-image MPIs [40], MINE [26], BehindTheScenes [42], and SceneNeRF [2] have also proposed single-view-based NVS, but poorly model VDEs.\\n\\nVDEs depend on the material's reflectance, which is a function of the material properties and the light's angle of incidence. Learning such material properties and light sources from a single image is a very ill-posed problem.\"}"}
{"id": "CVPR-2024-1696", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"While MLP [32] and spherical-harmonic-based [11, 43, 45] techniques to encode VDEs are effective when learning from multiple input images, they are still constrained when learning from single images. Instead, for the first time, to tackle the estimation of VDEs for single-view-based NVS, we propose to rely on the contents of the single images and estimated (during training) or user-defined (during test time) camera motions to estimate photo-metrically realistic view-dependent effects. Our main contributions are:\\n\\n\u2022 We first propose a single view NVS method with VDEs, called NVSVDE-Net. By recognizing camera motion priors and negative disparities govern view-dependent appearance, we model VDEs as the negative disparities in the scene induced by the target camera motion.\\n\\n\u2022 The NVSVDE-Net is the first to be trained in a completely self-supervised manner in the sense that neither depths nor pose annotations are required. While other methods rely on given depths and/or camera poses, the NVSVDE-Net learns only from image sequences. During test, only one single image input is required for the NVSVDE-Net to render novel views with VDEs.\\n\\n\u2022 A novel \u2018relaxed volumetric rendering\u2019 approximation method is proposed, allowing fast and efficient rendering of novel views from a single image. The NVSVDE-Net\u2019s rendering pipeline well approximates volumetric rendering by a single pass of a convolutional or transformer-based backbone and re-calibration and sampler MLP blocks. Additionally, to better learn from image sequences only, we introduce a new coarse-to-fine camera pose estimation network as a secondary contribution.\\n\\n2. Related Works\\n\\nUnlike the classical techniques for single view NVS which require user input or rely on hard-coded domain-specific assumptions [21, 22], deep-learning-based approaches can leverage image representations that are common across scenes to render novel views automatically. We distinguish between two kinds of deep-learning-based NVS methods from single images: one kind requires an additional pre-computed depth map [18, 25, 33] and the other kind only uses a single image during test time [2, 26, 37, 39, 40, 42, 44, 46]. Our method lies in the second category, with the remarkable exception of not requiring ground truth (GT) depths and GT camera poses during training. We focus on static forward-facing scenes, not object-centric 360-degree rendering [29].\\n\\n2.1. MPI-Based Single View NVS\\n\\nThe multi-plane image representation (MPI) [50] maps single or multiple images into a camera-centric layered 3D representation. Each input image pixel $p = (u, v)$ is mapped to $D$-number of colors and opacities. New views are rendered by sampling the MPI colors and opacities from novel camera poses and by aggregating them via volumetric rendering [9]. In [40], Tucker and Snavely proposed single-view NVS with MPIs. Estimating densities and colors in MPIs is challenging and not well-posed to model view-dependent effects, as the MPI\u2019s colors are not a function of the viewing directions. Furthermore, Sampling distances in MPIs depend on intersections with multi-image planes, potentially hindering high-quality rendering. On the other hand, MINE [26] borrows from [32] and [50] by adopting an MPI that is trained and queried via NeRF-like strategies, where the MPI representation is decoded one depth plane at a time. However, MINE still cannot model VDEs, and its sequential decoding increases computational complexity.\\n\\nIn contrast, our method explicitly models VDEs and approximates volumetric rendering by relaxing alpha compositing and obtaining refined sample distances at the target camera view for fine-grained rendering. Furthermore, we do not rely on ORB-SLAM2 predicted camera poses and sparse 3D point clouds for supervision.\\n\\n2.2. NeRF-based Single View NVS\\n\\nIn Neural Radiance Fields (NeRF) [32], Mildenhall et al. introduced a method that effectively maps 3D coordinates and viewing directions to color and density values using an MLP for volumetric rendering. However, NeRF has limitations: Besides overfitting to a single scene, it needs several reference views with accurate camera poses and long processing times. Nevertheless, recent advances have dramatically reduced neural rendering train and test times [4, 24]. PixelNeRF [46] is a cross-scene generalizable variant of NeRF, sacrificing rendering quality for scene generalization. It maps not only 3D ray points but also projected deep features into color and opacity values in radiance fields. Even when viewing directions are incorporated into PixelNeRF\u2019s MLP, the lack of a view-dependent inductive bias prevents PixeNeRF from generating VDEs.\\n\\nWimbauer et al. extended PixelNeRF in BehindScenes [42] by sampling colors from epipolar lines and processing densities with an MLP head for each point in the target ray. BehindScenes is designed to focus on 3D geometry but still lacks a mechanism for modeling VDEs as colors are projected between the minimum and maximum distance bounds in the target view. SceneRF [2] is also built on PixelNeRF but incorporates a probabilistic ray sampling strategy and a Spherical U-Net, along with depth penalties [14, 15]. Their sampling strategy uses Gaussian mixtures and requires multiple forward passes for rendering. Like NeRF [32], the MLPs in PixelNeRF [46], BehindScenes [42], and SceneRF [2] independently process each sample, amounting to a considerable computational complexity. In contrast, our approach with fine-grained ray sampling and relaxed volumetric rendering allows for all opacity estimations in a single pass, yielding efficiency gains.\"}"}
{"id": "CVPR-2024-1696", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2.3. Generative Single-View NVS\\n\\nNVS from a single image has also benefited from new advances in generative models. In [35], Ren et al. used an autoregressive transformer with a VQ-GAN [10] decoder to model long-term future frames. Similarly, Rombach et al. proposed GeoGPT [36] where they showed that autoregressive transformers can implicitly learn 3D relations between source and target images. In [39], Tseng et al. proposed a diffusion model [20] with epipolar attention blocks at its bottleneck that learn to infuse source view information along the epipolar lines into the target synthetic view during the diffusion process. In VQ3D [37], Sargent et al. proposed to learn 3D-aware representations and generation from ImageNet [7] using transformer-based autoencoders to map images and latent vectors into a tri-plane representation for neural rendering [3]. Pre-trained DPT [34] depths and various adversarial losses are used to train VQ3D. VQ3D is limited to render low-resolution views (256\u00d7256) with constrained viewpoints more related to object-centric rendering.\\n\\nEven when generative models aid NVS from a single image in generating novel views with very large baselines, they suffer from severe inconsistencies due to the stochastic nature of generative models. In addition, they suffer from large computational requirements and inference times that still make them impractical for single-view NVS.\\n\\n2.4. View Dependent Effects\\n\\nView-dependent effects (VDE) in NVS from multiple images have been achieved by incorporating viewing directions into the scene representations. In the case of NeRFs [1, 32], viewing directions are fed into the late stages of the MLPs, while in other works [11, 11, 43] view-dependent effects are modeled by spherical harmonics which map viewing directions to intensity changes. Even though these methods show SOTA view-dependent effects, they have the severe limitation of not generalizing cross-scenes and requiring several reference frames for inference. Moreover, fitting new scenes is a time-consuming optimization process that can take several hours.\\n\\nIn contrast, we introduce a new rendering pipeline, the NVSVDE-Net, which learns to approximate volumetric rendering for estimating novel views and view-dependent effects (VDE) from videos without requiring camera pose or depth labels. Trained in a self-supervised manner from image sequences, our NVSVDE-Net can render novel views with VDEs during inference, even from unseen single-image inputs. This marks the first instance of showcasing VDEs estimated from single images, leveraging local context and camera motion priors.\\n\\n3. Proposed Method\\n\\nVolumetric rendering [9] synthesizes novel camera views by traversing rays \\\\( r \\\\) that originate in the target view camera into a 3D volume of colors \\\\( c \\\\) and densities \\\\( \\\\sigma \\\\). The continuous volumetric rendering equation (VRE) is\\n\\n\\\\[\\nC(r) = \\\\int_{t_n}^{t_f} T(t) \\\\sigma(t) c(t) \\\\, dt, (1)\\n\\\\]\\n\\nwhere the accumulated transmittance \\\\( T(t) \\\\) indicates the probability of \\\\( r \\\\) traveling between the near distance bound \\\\( t_n \\\\) and ray distance \\\\( t \\\\) without hitting a particle. On the other hand, the density \\\\( \\\\sigma(t) \\\\) is understood as the probability of \\\\( r \\\\) hitting a particle exactly at \\\\( t \\\\). \\\\( t_f \\\\) is the far distance bound.\\n\\nIn NeRF [32], a large number of samples along the ray is considered to discretely approximate the VRE by Monte-Carlo sampling while the processing blocks, such as an MLP or convolutional layers, compute \\\\( \\\\sigma_i \\\\) and \\\\( c_i \\\\) for each \\\\( i \\\\)th sample along the ray. For this reason, the methods inspired in NeRF [2, 26, 42, 46] tend to yield very slow rendering times as each sample requires an independent forward pass.\\n\\n3.1. Relaxed Volumetric Rendering\\n\\nEstimating the \\\\( c_i \\\\) and \\\\( \\\\sigma_i \\\\) for volumetric rendering is not a trivial task. The complexity of volumetric rendering increases when a few or a single observation is available, as it becomes a one(pixel color)-to-many(densities and colors) mapping. For this reason, recent works such as [42] ease the network burden by only predicting density values (by several passes of an MLP head) and using colors directly projected from the input image \\\\( I \\\\).\\n\\nTo better incorporate a 3D inductive bias into our models, we propose to relax further the VRE Eq. (1) by directly...\"}"}
{"id": "CVPR-2024-1696", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"modeling the \u2018ray point weights\u2019 also known as $T$low\u2019 the camera motion. This operation is equivalent to generating a target VDE by re-sampling the pixels that \u2018follow\u2019 the camera view. Additionally, a depth estimate $\\\\hat{\\\\nu}$ describes the scene\u2019s geometry seen from the input or source camera. The reflected ray (or surface projection) \u2018moves\u2019 upwards in the target relative to their reflective surfaces. While the reflective view-dependent effects (VDE), such as the reflection of objects, can also be depicted in Fig. 2-(a), seem to \u2018follow\u2019 the camera motions (blue ray). In our relaxed volumetric rendering, VDE \u2018motion\u2019 cannot be larger than the corresponding camera view.\\n\\n3.2. Synthesis of View-Dependent Effects\\n\\nRendering allows us to efficiently and uniformly sample the multi-image planes. This allows NVS to be treated as a classification problem instead of estimated colors $c_i$ using projected colors from a VDE-infused input image $I$. For each ray in our relaxed volumetric rendering, VDE samples from $I$ generate the final color estimate $I_c$.\\n\\n3.3. NVSVDE-Net\\n\\nWith our relaxed volumetric rendering and VDE synthesis, we can generate novel views with VDEs. Finally, $H$ aids in generating VDEs via the respective depth (geometry) and VDE logits, $D_c$\u2013$D_{\\\\nu}$ from the VDE logits $P(\\\\nu|\\\\nu_i)$. Note that the identity $K_{\\\\nu}$ is a very small number. As shown in Fig. 3, the target VDE (green ray) intersects the reflection at the source camera in the negative depth region (blue sample). The same effect can be achieved by keeping a positive VDE depth and inverting the relative camera motion between the source and target views. The same effect can be achieved by keeping a positive VDE depth and inverting the relative camera motion between the source and target views.\"}"}
{"id": "CVPR-2024-1696", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. NVSVDE-Net architecture. NVSVDE-Net models VDEs as negative scene disparities under the target camera motion \\\\( R_c \\\\) and \\\\( t_c \\\\).\\n\\nNovel views are estimated in two stages, firstly with coarse fixed ray samples \\\\( t_i \\\\), then with refined adaptive sampling distances \\\\( t^*_k(p) \\\\) (Eq. 2). NVSVDE-Net predicts \\\\( D_L \\\\) and \\\\( V_L \\\\) from geometry and VDE pixel-aligned features in a single forward pass by\\n\\n\\\\[\\nD_L(p) = F_D(W_D(p), \\\\gamma(p, R_c, t_c)),\\n\\\\]\\n\\\\[\\nV_L(p) = F_V(W_V(p), \\\\gamma(p, R_c, t_c)).\\n\\\\]\\n\\nwhere \\\\( F_D \\\\) and \\\\( F_V \\\\) are MLP heads (Linear-ELU-Linear) that re-calibrate the geometry and VDE pixel-aligned features, \\\\( W_D \\\\) and \\\\( W_V \\\\) respectively, for the target camera view \\\\( c \\\\).\\n\\nNote that such calibration is needed as the samples at distances \\\\( t_i \\\\) (in Eq. 2) and \\\\( 1/v_i \\\\) (in Eq. 5) are measured from the target camera view \\\\( I_c \\\\), not the input source view \\\\( I \\\\).\\n\\nPixel positional information \\\\( p \\\\) and relative camera extrinsics \\\\( R_c \\\\) and \\\\( t_c \\\\) are also fed into \\\\( F_D \\\\) and \\\\( F_V \\\\) via the positional encoding \\\\( \\\\gamma(\u00b7) \\\\). We explored both a learnable \\\\( \\\\gamma^\\\\theta \\\\) (fast and compact but can potentially overfit) and a sine-cosine \\\\( \\\\gamma \\\\) positional encoding (more general but slower due to requiring more channels) where there was little difference.\\n\\n\\\\( W_D \\\\) and \\\\( W_V \\\\) enable cross-scene generalization and are simultaneously estimated from a CNN-based (or transformer-based) encoder-decoder backbone \\\\( F_W \\\\) as shown in Fig. 4.\\n\\n\\\\( F_W \\\\) is also fed with the single image input \\\\( I \\\\) and the relative pixel locations \\\\( (U, V) \\\\) to improve learning from random-resized and -cropped patches [16] as\\n\\n\\\\[\\n(W_D, W_V) = F_W(I, (U, V)).\\n\\\\]\\n\\nOnce \\\\( D_L \\\\) and \\\\( V_L \\\\) are computed, they can be used in Eqs. (2) and (5) to yield the VDE-infused synthetic view \\\\( I''_c \\\\), as shown in the center of Fig. 4. However, the quality of \\\\( I''_c \\\\) is closely tied to the number of samples \\\\( N \\\\) in our relaxed volumetric rendering approximation. Naively increasing \\\\( N \\\\) can incur additional computational complexity. Instead, we incorporate a sampler block \\\\( F_S \\\\) to estimate fine-grained ray samples from projected depth probabilities and colors.\\n\\n### 3.3.1 The Sampler Block\\n\\nThe sampler block \\\\( F_S \\\\) in our NVSVDE-Net (top left of Fig. 4) takes as input the projected probability logits \\\\( D_P \\\\) and projected VDE-infused colors from \\\\( I_\\\\nu_c \\\\). These are then mapped by a fully-connected network (Linear-ELU-Linear-ELU-Linear) into \\\\( N \\\\times \\\\) refined per-pixel sampling distances \\\\( t^*_k(p) \\\\) and soft-maxed weights \\\\( w^*_k(p) \\\\) as given by\\n\\n\\\\[\\nw^*_k(p), t^*_k(p) = F_S(D_P(p), \\\\{I_\\\\nu_c(g(p, t_i, R_c|t_c, K))\\\\}_{i=0}^{N-1}).\\n\\\\]\\n\\n\\\\( w^*_k \\\\) and \\\\( t^*_k \\\\) respectively replace \\\\( D_P \\\\) and \\\\( t_i \\\\) in Eq. (2) to yield the final synthetic image \\\\( I_c' \\\\) by a fine-grained relaxed volumetric rendering as\\n\\n\\\\[\\nI_c'(p) = P_{k=1}^{N} w^*_k(p) I_\\\\nu_c(g(p, t^*_k(p), R_c|t_c, K)).\\n\\\\]\\n\\nNote the architecture in Fig. 4 only requires the backbone to be run once per reference image. Once \\\\( W_D \\\\) and \\\\( W_V \\\\) are estimated, the novel views are generated by running the computationally inexpensive re-calibration (\\\\( F_D, F_V \\\\)) and the sampler (\\\\( F_S \\\\)) blocks according to the relaxed VRE in Eq. (11). Contrary to previous works [2, 26, 42, 46] that also incorporate MLP heads for late-stage rendering, we only need to run them once instead of running them for each point in the target rays. Furthermore, previous single-view-based NVS methods [2, 26, 40, 42, 46] require either depths or pose GTs (or both), while our method learns from image sequences only in an entirely self-supervised manner with the aid from an improved camera pose estimation network.\\n\\n### 3.4. Improved Camera Pose Estimation\\n\\nPreviously, camera-pose estimation networks [15, 49] have shown reasonable performance for the monocular depth estimation. Our PoseNet refines initial coarse extrinsics by predicting a residual between rotation-aligned views.\"}"}
{"id": "CVPR-2024-1696", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"estimation task in driving datasets [6, 13]. However, they can only handle relatively simple camera motions because the cameras mounted on cars are primarily exposed to Z-axis motion and small horizontal rotations. In contrast, indoor datasets [50] and hand-held captured scenes [27] contain arbitrary translations with considerable rotations on all axes.\\n\\nWe observe that based on perceptual features, it is much simpler to understand 3D translation between two images if they are first rotation-aligned. We incorporate this observation into our improved PoseNet as depicted in Fig. 5. In our PoseNet, from a pair of input images \\\\((I, I_c)\\\\), coarse camera extrinsics \\\\((R_0, t_0)\\\\) are estimated by aggregating their deep features (estimated by a shared encoder network) with a convolutional and global average pooling (Conv+GAP) layer. This coarse stage resembles the PoseNets in [15, 49].\\n\\n\\\\(R_0\\\\) is then used to rotation-align \\\\(I_c\\\\) to \\\\(I\\\\). We then extract deep features from the rotation-aligned pair that contain much more relevant visual features, such as closer vanishing points and disparities. By an additional Conv + GAP layer, residual translation and rotation values, \\\\(\\\\Delta R\\\\) and \\\\(\\\\Delta t\\\\), are computed and added to \\\\(R_0\\\\) and \\\\(t_0\\\\) to yield the final relative extrinsic parameters \\\\(R_c\\\\) and \\\\(t_c\\\\).\\n\\n3.5. Loss Functions\\n\\nFrom a training input image \\\\(I\\\\), synthetic images are estimated for both the previous and next views \\\\((I_{\u22121}, I_{+1})\\\\) in the video sequence. We utilize a synthesis loss, \\\\(l_{\\\\text{syn}}\\\\), between each synthetic view, both coarse and fine \\\\((I_{\u2032\u2032\u22121}, I_{\u2032\u2032+1}, I_{\u2032\u22121}, I_{\u2032+1})\\\\), and their corresponding GT images \\\\((I_{\u22121}, I_{+1})\\\\). Additionally, we incorporate disparity smoothness \\\\(l_{\\\\text{sm}}\\\\) that aids in regularizing \\\\(\\\\hat{D}\\\\). The total loss function is given by\\n\\n\\\\[\\nl_{\\\\text{total}} = l_{\\\\text{syn}}(I_{\u2032\u2032c}) + l_{\\\\text{syn}}(I_{\u2032c}) + \\\\alpha_{\\\\text{sm}} l_{\\\\text{sm}}.\\n\\\\]\\n\\nThe synthesis loss, \\\\(l_{\\\\text{syn}}\\\\), is a combination of L1 and perceptual (VGG) loss [23] to enforce similar colors and structures between synthetic and GT views. The VGG loss penalizes deformations, textures, and lack of sharpness as it compares estimated and GT views in the deep feature space of a pre-trained image classification network. The L2 norm of the perceptual error of the first three max-pool VGG layers (denoted by \\\\(\\\\phi_{l}\\\\)) was utilized.\\n\\n\\\\[\\nl_{\\\\text{syn}} = ||I_{oc} \u2212 I_c||_1 + \\\\alpha_p P_{3\\\\{l=1}\\\\}||\\\\phi_{l}(I_{oc}) \u2212 \\\\phi_{l}(I_c)||_2^2,\\n\\\\]\\n\\nwhere \\\\(\\\\alpha_p = 0.01\\\\) balances the contributions of the L1 and VGG terms. \\\\(I_{oc} = (1 \u2212 O_c)\u2299I_c + O_c\u2299I_{\u2032c}\\\\) is the synthetic view with occluded contents replaced by those in \\\\(I_c\\\\). \\\\(\u2299\\\\) is the Hadamart product. We compute the occlusion mask \\\\(O_c\\\\) following [17, 31] as\\n\\n\\\\[\\nO_c(p) = \\\\frac{1}{N\u22121} \\\\sum_{i=0}^{N\u22121} \\\\sigma(D_{\\\\text{L}}(g(p, t_i, R_c|t_c, K))\\\\).\\n\\\\]\\n\\nThe edge-aware smoothness loss \\\\(l_{\\\\text{sm}}\\\\) with a weight of \\\\(\\\\alpha_{\\\\text{sm}} = 0.05\\\\) regularizes \\\\(\\\\hat{D}\\\\) to be smooth in homogenous image regions. See Supplemental for more details.\\n\\n4. Experiments and Results\\n\\nExtensive experimental results show our method can generate, for the first time, NVS with VDEs from single image inputs on real datasets that contain complex camera motions and scenes with no depth or pose annotations. Please see Supplemental for additional results and videos.\\n\\nWe train our NVSVDE-Net with the Adam optimizer \\\\((\\\\beta_1: 0.9, \\\\beta_2: 0.999)\\\\) with a batch size of 6 for 50 epochs and 3k iterations per epoch. The initial learning rate is set to \\\\(10^{-4}\\\\) and is halved at 50, 75, and 90% of the training for stability and convergence. We set \\\\(N = 32, N_\\\\nu = 32, N^* = 16\\\\) for our relaxed volumetric rendering. We train all models on the RealEstate10k [50] and Mannequin-Challenge [27] datasets with random spatial data augmentations such as random resize and crops, random horizontal flip, and value-based data augmentations such as random gamma, brightness, and color shifts. Training patches are of \\\\(240 \\\\times 426\\\\) obtained from randomly resized images between 30% and 85% of their original resolution. Finally, 8-bit RGB images are normalized to the \\\\([-0.5, 0.5]\\\\) range.\\n\\nWe adopt the same ResNet34 [19] IMAGENet [7] pre-trained backbone in all models (in ours and [2, 42, 46]), otherwise specified. In particular, we set \\\\(N = 64, 48, \\\\text{and } 32\\\\) ray samples for PixelNeRF [46], BehindScenes [42], and SceneRF [2], respectively. To match NVSVDE-Net's \\\\(N^*\\\\), we use 4 Gaussians with four samples in SceneRF. For a fair comparison, we train [2, 26, 40, 42, 46] under the same self-supervised conditions as our NVSVDE-Net. We observed [2, 42, 46] are unstable in the early epochs when learning self-supervised, so they are trained from a fully-trained PoseNet from our best NVSVDE-Net.\\n\\n4.1. Datasets\\n\\nRealEstate10k (RE10) [50] is a dataset consisting of 10M frames from approximately 80k video clips from around 10k YouTube videos of indoor and outdoor human-made environments. After downloading, we account for 400k and 180k for training and testing, respectively. We randomly select the 4, 8, 12, or 16-th previous or next frame for training. We use every 1k sample from the test set for testing.\\n\\nThe MannequinChallenge (MC) [27] dataset is a large, diverse, and challenging dataset that mainly contains humans in both indoor and outdoor scenarios. It contains 170K frames from 2k YouTube videos where people try to stay stationary while a recording camera moves around the scene. After downloading, we account for 60k samples for training and 9k samples for testing. We randomly select up to the 6-th previous or next frame for training. We use every 20-th frame from the test set for testing.\\n\\nIn both datasets, images have a full resolution of 720\u00d71280. We train with 1/3 resolution patches and test with 1/2 resolution inputs. We render two views from an input test view, the previous \\\\(k\\\\)-th frame and the next \\\\(k\\\\)-th frame for evaluation. \\\\(k\\\\) is set to 8 and 1 for RE10k and 10418.\"}"}
{"id": "CVPR-2024-1696", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. NVS results.\\n\\n| Methods            | VDE MAE | PSNR   | PSNR $_{lf}$ | SSIM  | LPIPS  |\\n|--------------------|---------|--------|--------------|-------|--------|\\n| RealEstate10k (RE10k) Dataset [50] |\\n| PixelNerf [46] No | 0.0417  | 22.8455| 28.0945     | 0.7818| 0.3256 |\\n| BehindScenes [42] No | 0.0466  | 22.9949| 28.5941     | 0.8068| 0.2762 |\\n| MINE [26] No       | 0.0415  | 23.1657| 27.8785     | 0.8041| 0.2976 |\\n| Single-view MPI [40] No | 0.0374  | 23.6260| 28.9447     | 0.8112| 0.2925 |\\n| SceneRF [2] No     | 0.0373  | 23.6087| 28.9636     | 0.8130| 0.2709 |\\n| NVSVDE-Net (Ours) Yes | 0.0319  | 24.3131| 30.2529     | 0.8397| 0.2325 |\\n\\nTable 2. NVSVDE-Net ablation studies.\\n\\n| Methods            | VDE MAE | PSNR   | PSNR $_{lf}$ | SSIM  | LPIPS  |\\n|--------------------|---------|--------|--------------|-------|--------|\\n| MannequinChallenge (MC) Dataset [27] |\\n| PixelNerf [46] No | 0.0511  | 21.3047| 25.2781     | 0.7580| 0.3455 |\\n| BehindScenes [42] No | 0.0463  | 21.4307| 25.9280     | 0.7831| 0.3101 |\\n| SceneRF [2] No     | 0.0467  | 21.5992| 25.8119     | 0.7796| 0.3080 |\\n| NVSVDE-Net (Ours) Yes | 0.0405  | 22.4274| 27.0263     | 0.8130| 0.2733 |\\n\\n4.2. Results on RealEstate10k (RE10k)\\nThe scenes in RE10k contain a considerable amount of perceptually significant view-dependent effects. As shown in Fig. 6, our NVSVDE-Net learns single-view-based realistic NVS and VDEs whose activations map the most reflective regions in the corresponding input images well.\\n\\nTable 1 shows the results on RE10K [50]. Our NVSVDE-Net significantly outperforms SceneRF [2] and MPI [40] by a large margin of 0.7dB in PSNR and 1.3dB in PSNR $_{lf}$ and previous methods [26, 42, 46] by more than 1.1dB in PSNR and 1.6dB in PSNR $_{lf}$. We qualitatively compare PixelNeRF, BehindScenes, and NVSVDE-Net in Fig. 7, which shows that our model yields more detailed synthetic views with fewer artifacts. Fig. 7 also shows that methods [42, 46] that do not model VDEs struggle to yield plausible geometries for the reflective regions.\\n\\nFig. 8 demonstrates cases where MPI [40] and NVSVDE-Net perform similarly (top row) and instances where MPI [40] fails due to reflective surfaces, as indicated by higher error maps (bottom 2 rows). This shows that modeling VDEs by incorrect geometries is insufficient for higher-quality rendering. Our method exhibits more realistic highlights and sharper novel views due to VDE synthesis and fine-grained relaxed VR.\\n\\nMeasuring VDEs is difficult due to their sparse nature. We observed that most VDE that could be modeled from single images consist of low-frequency information, such as glossy reflections. Based on that assumption, we propose the PSNR $_{lf}$ metric in Table 1. PSNR $_{lf}$ takes the PSNR between an estimated image and its GT image, both filtered by a 21\u00d721-sized Gaussian kernel before comparison. Other metrics that focus on strong image structures, such as SSIM and LPIPS, showed transparency to the changes in VDEs, as VDEs rarely change the high-frequency information or high-level structures in the natural scenes.\"}"}
{"id": "CVPR-2024-1696", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.2.1 Ablation Studies\\n\\nTable 2 shows ablation studies for our NVSVDEnet. We study the effects of our proposed VDE synthesis in (w/o VDE), which yields 0.15dB lower PSNR than our full model. Even though the PSNR difference is not very large, a clear advantage in predicted geometries can be observed in Fig. 10, where our model w/o VDEs predicts holes for the reflective regions. We also ablate the effects of our VDEs by disabling the VDE estimation in our full model, denoted by (VDE disabled), which yields 0.15dB lower PSNR.\\n\\nNext, we ablate design choices in our proposed relaxed volumetric rendering. (w/o FD & FV) in Table 2 shows the negative effects of not adjusting the depth and VDE logs. (N = 48, N\u2217 = 0) shows that the coarse synthetic output, even with more samples, is still 0.3dB in PSNR behind our full model. On the other hand, (N = 32, N\u2217 = 32) shows that only a few adaptive N\u2217 = 16 in our full model is enough for optimal results. (I\u2032\u2032c) is the coarse synthetic output of our full model. Interestingly, (I\u2032\u2032c) is 0.2dB lower than I\u2032c, but 0.14dB higher in PSNR than (N = 48, N\u2217 = 0), showing that the joint supervision of I\u2032c benefits it. The fine-grained I\u2032c removes most double-edge artifacts due to discrete ray distance discretization as shown in Fig. 10. In our relaxed volumetric rendering, we also explore estimating density values \u03c3 instead of T(t)\u03c3(t).\\n\\nHowever, estimating \u03c3 was not only computationally more expensive due to the computation of the accumulated transmittance for each ray sample but also yielded slightly worse quality metrics. A periodic encoding \u03b3 was also explored, but we observed slightly worse quality metrics.\\n\\nWe also provide metrics for our NVSVDE-Net with different backbones, such as ResNet18 (R18) and Swin transformer [30] (Swin-t) transformer backbone. Even when NVSVDE-Net (swint) quantitative metrics are not better our ResNet34 full model, its estimated depth and VDE maps are perceptually more consistent (see Supplemental for qualitative comparison).\\n\\nFinally, we also ablate the effect of our improved PoseNet, with a synthesis quality impact of \u223c0.5dB and \u223c0.9dB higher PSNR than the widely adopted [49]'s PoseNet, on RE10k and MC respectively. Test poses are also estimated, reflecting the quality of our PoseNet.\\n\\n4.3. Results on MannequinChallenge (MC)\\n\\nThe bottom part of Table 1 shows quantitative results for single-image-based NVS on the MC [27] validation set. The NVSVDE-Net outperforms the existing methods at least by 0.8dB in PSNR and \u223c1.2dB in PSNR. Fig. 9 depicts qualitative comparisons between our NVSVDE-Net and SceneRF [2]. Again, our method yields sharper and more consistent single-view NVS outputs.\\n\\n4.4. Runtime Analysis\\n\\nOur NVSVDE-Net takes 42ms for a 640 \u00d7 360 image on an Nvidia A6000 GPU on vanilla PyTorch. 12ms are used for the backbone operation and 30ms for projections, sampler, and rendering. Since the backbone runs only once for each input image, it only takes 30ms to render novel views of the same scene. Even though our method is near real-time with 33FPS, further speed-ups (up to 3 \u00d7) are possible with optimized Python packages, such as TensorRT. In comparison, SceneRF, PixelNeRF, and BehindScenes in Table 1, which all share the same 25M-parameter backbone, take 160, 112, and 99ms for rendering a 640 \u00d7 360 image, respectively.\\n\\n5. Conclusions\\n\\nWe firstly presented a novel method, the NVSVDE-Net, that can learn to perform NVS and VDE estimation on single images in a self-supervised manner from monocular image sequences. We showed that our method generalizes well on unseen test images and that it can generate plausible VDEs and depth maps from a single image. In addition, our NVSVDE-Net incorporates a relaxed approximation to volumetric rendering, which we further improve by incorporating a sampler module for fine-grained ray sampling and rendering. Our NVSVDE-Net yields more realistic NVS images with VDEs in comparison to the recent SOTA methods such as PixelNeRF, BHindScenes, and SceneRF on the RE10k and MC datasets.\\n\\nAcknowledgement\\n\\nThis work was supported by IITP grant funded by the Korea government (MSIT) (No. RS2022-00144444, Deep Learning Based Visual Representational Learning and Rendering of Static and Dynamic Scenes).\"}"}
{"id": "CVPR-2024-1696", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-nerf: A multiscale representation for anti-aliasing neural radiance fields, 2021.\\n\\n[2] Anh-Quan Cao and Raoul de Charette. Scenerf: Self-supervised monocular 3d scene reconstruction with radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9387\u20139398, 2023.\\n\\n[3] Eric R Chan, Connor Z Lin, Matthew A Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16123\u201316133, 2022.\\n\\n[4] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. Mobilenerf: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. In The Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[5] Djork-Ar\u00e8ne Clevert, Thomas Unterthiner, and Sepp Hochreiter. Fast and accurate deep network learning by exponential linear units (elus). arXiv preprint arXiv:1511.07289, 2015.\\n\\n[6] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 3213\u20133223, 2016.\\n\\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248\u2013255. Ieee, 2009.\\n\\n[8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n\\n[9] Robert A Drebin, Loren Carpenter, and Pat Hanrahan. Volume rendering. ACM Siggraph Computer Graphics, 22(4):65\u201374, 1988.\\n\\n[10] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12873\u201312883, 2021.\\n\\n[11] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 5491\u20135500. IEEE, 2022.\\n\\n[12] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep ordinal regression network for monocular depth estimation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2002\u20132011, 2018.\\n\\n[13] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In 2012 IEEE Conference on Computer Vision and Pattern Recognition, pages 3354\u20133361. IEEE, 2012.\\n\\n[14] Cl\u00e9ment Godard, Oisin Mac Aodha, and Gabriel J Brostow. Unsupervised monocular depth estimation with left-right consistency. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 270\u2013279, 2017.\\n\\n[15] Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. Digging into self-supervised monocular depth estimation. In Proceedings of the IEEE International Conference on Computer Vision, pages 3828\u20133838, 2019.\\n\\n[16] Juan Luis Gonzalez and Munchurl Kim. Plade-net: Towards pixel-level accuracy for self-supervised single-view depth estimation with neural positional encoding and distilled matting loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6851\u20136860, 2021.\\n\\n[17] Juan Luis Gonzalez-Bello and Munchurl Kim. Forget about the lidar: Self-supervised depth estimators with med probability volumes. In Advances in Neural Information Processing Systems, pages 12626\u201312637. Curran Associates, Inc., 2020.\\n\\n[18] Yuxuan Han, Ruicheng Wang, and Jiaolong Yang. Single-view view synthesis in the wild with learned adaptive multiplane images. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1\u20138, 2022.\\n\\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\n[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.\\n\\n[21] Derek Hoiem, Alexei A. Efros, and Martial Hebert. Automatic photo pop-up. In ACM SIGGRAPH 2005 Papers, pages 577\u2013584, New York, NY, USA, 2005. ACM.\\n\\n[22] Youichi Horry, Ken Anjyo, and Kiyoshi Arai. Tour into the picture: Using spidery mesh interface to make animation from a single image\u201d. pages 225\u2013232, 1997.\\n\\n[23] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European conference on computer vision, pages 694\u2013711. Springer, 2016.\\n\\n[24] Bernhard Kerbl, Georgios Kopanas, Thomas Leimk\u00fchler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), 2023.\\n\\n[25] Johannes Kopf, Kevin Matzen, Suhib Alsisan, Ocean Quigley, Francis Ge, Yangming Chong, Josh Patterson, Jan-Michael Frahm, Shu Wu, Matthew Yu, et al. One shot 3d photography. ACM Transactions on Graphics (TOG), 39(4):76\u20131, 2020.\"}"}
{"id": "CVPR-2024-1696", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, and Gim Hee Lee. MINE: towards continuous depth MPI with nerf for novel view synthesis. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 12558\u201312568. IEEE, 2021.\\n\\nZhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, and William T. Freeman. Learning the depths of moving people by watching frozen people. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nZhengqi Li, Qianqian Wang, Forrester Cole, Richard Tucker, and Noah Snavely. Dynibar: Neural dynamic image-based rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4273\u20134284, 2023.\\n\\nKai-En Lin, Yen-Chen Lin, Wei-Sheng Lai, Tsung-Yi Lin, Yi-Chang Shih, and Ravi Ramamoorthi. Vision transformer for nerf-based view synthesis from a single input image. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 806\u2013815, 2023.\\n\\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.\\n\\nBen Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: practical view synthesis with prescriptive sampling guidelines. ACM Trans. Graph., 38(4):29:1\u201329:14, 2019.\\n\\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Computer Vision - ECCV 2020 - 16th European Conference, Glasgow, UK, August 23-28, 2020, Proceedings, Part I, pages 405\u2013421. Springer, 2020.\\n\\nSimon Niklaus, Long Mai, Jimei Yang, and Feng Liu. 3d ken burns effect from a single image. ACM Transactions on Graphics (ToG), 38(6):1\u201315, 2019.\\n\\nRen \u00b4e Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12179\u201312188, 2021.\\n\\nXuanchi Ren and Xiaolong Wang. Look outside the room: Synthesizing a consistent long-term 3d scene video from a single image. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3563\u20133573, 2022.\\n\\nRobin Rombach, Patrick Esser, and Bj \u00a8orn Ommer. Geometry-free view synthesis: Transformers and no 3d priors. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14356\u201314366, 2021.\\n\\nKyle Sargent, Jing Yu Koh, Han Zhang, Huiwen Chang, Charles Herrmann, Pratul Srinivasan, Jiajun Wu, and Deqing Sun. VQ3D: Learning a 3D-aware generative model on ImageNet. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2023.\\n\\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\\n\\nHung-Yu Tseng, Qinbo Li, Changil Kim, Suhib Alsisan, Jia-Bin Huang, and Johannes Kopf. Consistent view synthesis with pose-guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16773\u201316783, 2023.\\n\\nRichard Tucker and Noah Snavely. Single-view view synthesis with multiplane images. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\nZhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Trans. Image Process., 13(4):600\u2013612, 2004.\\n\\nF. Wimbauer, N. Yang, C. Rupprecht, and D. Cremers. Behind the scenes: Density fields for single view reconstruction. In 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 9076\u20139086, Los Alamitos, CA, USA, 2023. IEEE Computer Society.\\n\\nSuttisak Wizadwongsa, Pakkapon Phongthawee, Jiraphon Yenphraphai, and Supasorn Suwajanakorn. Nex: Real-time view synthesis with neural basis expansion. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 8534\u20138543. Computer Vision Foundation / IEEE, 2021.\\n\\nJunyuan Xie, Ross Girshick, and Ali Farhadi. Deep3d: Fully automatic 2d-to-3d video conversion with deep convolutional neural networks. In European Conference on Computer Vision, pages 842\u2013857. Springer, 2016.\\n\\nAlex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. Plenoctrees for real-time rendering of neural radiance fields. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 5732\u20135741. IEEE, 2021.\\n\\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelnerf: Neural radiance fields from one or few images. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 4578\u20134587. Computer Vision Foundation / IEEE, 2021.\\n\\nRichard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In 2018 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2018, Salt Lake City, UT, USA, June 18-22, 2018, pages 586\u2013595. Computer Vision Foundation / IEEE Computer Society, 2018.\\n\\nKaichen Zhou, Lanqing Hong, Changhao Chen, Hang Xu, Chaoqiang Ye, Qingyong Hu, and Zhenguo Li. Devnet: Self-supervised monocular depth learning via density volume construction. In Computer Vision \u2013 ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXIX, page 125\u2013142, Berlin, Heidelberg, 2022. Springer-Verlag.\"}"}
{"id": "CVPR-2024-1696", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
