{"id": "CVPR-2022-1766", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nExisting knowledge distillation works for semantic segmentation mainly focus on transferring high-level contextual knowledge from teacher to student. However, low-level texture knowledge is also of vital importance for characterizing the local structural pattern and global statistical property, such as boundary, smoothness, regularity and color contrast, which may not be well addressed by high-level deep features. In this paper, we are intended to take full advantage of both structural and statistical texture knowledge and propose a novel Structural and Statistical Texture Knowledge Distillation (SSTKD) framework for Semantic Segmentation. Specifically, for structural texture knowledge, we introduce a Contourlet Decomposition Module (CDM) that decomposes low-level features with iterative laplacian pyramid and directional filter bank to mine the structural texture knowledge. For statistical knowledge, we propose a Denoised Texture Intensity Equalization Module (DTIEM) to adaptively extract and enhance statistical texture knowledge through heuristics iterative quantization and denoised operation. Finally, each knowledge learning is supervised by an individual loss function, forcing the student network to mimic the teacher better from a broader perspective. Experiments show that the proposed method achieves state-of-the-art performance on Cityscapes, Pascal VOC 2012 and ADE20K datasets.\\n\\n1. Introduction\\n\\nSemantic segmentation, which aims to assign each pixel a unique category label for the input image, is a crucial and challenging task in computer vision. Recently, deep fully convolution network [32] based methods have achieved remarkable results in semantic segmentation, and extensive methods have been investigated to improve the segmentation accuracy by introducing sophisticated models [6, 8, 23, 27, 44, 46, 47, 50, 52]. However, these methods are usually based on a large model, which contains tremendous parameters. Since semantic segmentation has shown great potential in many applications like autonomous driving, video surveillance, robot sensing and so on, how to keep efficient inference speed and high accuracy with high-performance networks is a critical issue.\\n\\nFigure 1. The overview of the structural and statistical texture knowledge distillation of an example image. Two kinds of the texture knowledge are extracted from the low-level feature of a CNN backbone. The original structural and statistical texture are fuzzy and in low-contrast. After distillation, the contour is clearer and the intensity contrast is more equalized, showing that two kinds of the texture are both enhanced.\"}"}
{"id": "CVPR-2022-1766", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"resolutions is a critical problem.\\n\\nThe focus of this paper is knowledge distillation, which is introduced by Hinton et al. [22] based on a teacher-student framework, and has received increasing attention in the semantic segmentation community [20, 30, 31, 37, 42]. Previous works mainly focus on the high-level contextual knowledge [30, 37] or the final response knowledge [20, 31, 42], which are appropriate to capture the global context and long-range relation dependencies among pixels, but will also result in coarse and inaccurate segmentation results, since they are usually extracted with a large receptive field and miss many low-level texture details. In this paper, we concentrate on exploiting the texture knowledge from the teacher to enrich the low-level information of the student. According to the digital image processing [18], texture is a region descriptor which can provide measures for both local structural property and global statistical property of an image. The structural property can also be viewed as spectral domain analysis and often refers to some local patterns, such as boundary, smoothness, and coarseness. While the statistical property pays more attention to the global distribution analysis, such as histogram of intensity.\\n\\nBased on the above analysis, we propose a novel Structural and Statistical Texture Knowledge Distillation (SSTKD) framework to effectively distillate two kinds of the texture knowledge from the teacher model to the student model, as shown in Figure 1. More comprehensively, we introduce a Contourlet Decomposition Module (CDM) that decomposes low-level features to mine the structural texture knowledge with iterative laplacian pyramid and directional filter bank. The contourlet decomposition is a kind of multiscale geometric analysis tool and can enable the neural network the ability of geometric transformations, thus is naturally suitable for describing the structural properties. Moreover, we propose a Denoised Texture Intensity Equalization module (DTIEM) to adaptively extract and enhance the statistical knowledge, cooperated with an Anchor-Based Adaptive Importance Sampler. The DTIEM can effectively describe the statistical texture intensity in a statistical manner in deep neural networks, as well as suppress the noise produced by the amplification effect in near-constant regions during the texture equalization. Overall, our contribution is threefold:\\n\\n\u2022 To our knowledge, it is the first work to introduce both the structural and statistical texture to knowledge distillation for semantic segmentation. We propose a novel Structural and Statistical Texture Knowledge Distillation (SSTKD) framework to effectively extract and enhance the unified texture knowledge and apply them to teacher-student distillation.\\n\\n\u2022 More comprehensively, we introduce the Contourlet Decomposition Module (CDM) and propose the Denoised Texture Intensity Equalization Module (DTIEM) to describe the structural and statistical texture, respectively. Moreover, DTIEM utilizes an adaptive importance sampler and a denoised operation for efficient and accurate characterization.\\n\\n\u2022 Experimental results show that the proposed framework achieves the state-of-the-art performance on three popular benchmark datasets in spite of the choice of student backbones.\\n\\n2. Related Work\\n\\nSemantic Segmentation.\\n\\nExisting works for semantic segmentation mainly focused on contextual information and elaborate network [16, 21, 26, 33]. Many works [6\u20138, 23, 28, 50] try to take advantage of rich contextual information in deep features, while the improvement of network design has also significantly driven the performance, such as graph modules [23, 24, 39] and attention techniques [7, 17, 48]. How to find a better trade-off between accuracy and efficiency have been discussed for a long time. Real-time semantic segmentation algorithms aim to produce high-quality prediction under limited calculation [2, 34, 41, 45, 49]. BiSeNet [45] introduced spatial path and semantic path to reduce computational cost. However, though their efficiency, there is a large gap in the performance to be considered.\\n\\nKnowledge Distillation.\\n\\nIn a typical knowledge distillation framework, the logits are used as the knowledge from the teacher model [4, 22]. Many works followed this paradigm in other visual applications, such as object detection and human pose estimation. In semantic segmentation, existing methods also utilized this idea to produce fundamental results. They transferred the output maps to the student model, distilling the class probabilities for each pixel separately. Based on this, they further proposed to extract the different knowledge from the task-specific view. He et al. [20] proposed an affinity distill module to transfer the long-range dependencies among widely separated spatial regions from a teacher model to a student model. SKD [30] proposed structured knowledge distillation to transfer pairwise relations and holistic knowledge with the help of adversarial learning. IFVD [40] forced the student model to mimic the intra-class feature variation of the teacher model. CWD [37] minimized the Kullback\u2013Leibler divergence between the channel-wise probability map of the teacher and student networks. Different from them, we first introduce the texture knowledge to semantic segmentation, showing an effective framework for this task.\\n\\nTexture in Semantic Segmentation.\\n\\nIn the perspective of digital image processing [18], texture is a kind of descriptor that provides measures of properties such as smoothness, coarseness, regularity, and so on. Image texture is not only about the local structural patterns, but also global statistical properties.\"}"}
{"id": "CVPR-2022-1766", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"output map\\nOutput map\\n\\nFigure 2. An overview of our proposed framework. PSPNet [50] is used as the model architecture for both teacher and student network, which consists of the backbone network, pyramid pooling module (PPM) and the final output map. Apart from the response knowledge, we further propose to extract the texture knowledge from low-level features. The corresponding parts of two kinds of the texture knowledge are depicted in the light red [1, 3, 29] and light green below the network pipeline, respectively.\\n\\nZhu et al. [52] firstly introduced the statistical texture to semantic segmentation, and proposed a Quantization and Count Operator (QCO) to extract the low-level statistical texture feature, which is then aggregated with high-level contextual feature to obtain a more precise segmentation map. However, the QCO performs as a global manner and sparse quantization, as well as neglects the noise produced by the amplification effect. Moreover, structural texture information is not well addressed in their work. In this paper, we focus on the unified texture information including both structural and statistical, and propose to improve the statistical characterization with anchor-based importance sampling, heuristics iterative quantization and denoised operation, as well as propose the CDM to re-emphasize the structural texture.\\n\\n3. Method\\nIn this section, we introduce the proposed Structural and Statistical Texture Knowledge Distillation (SSTKD) framework in detail. Firstly, we introduce the overall structure in section 3.1. Subsequently, we introduce the Structural Texture Knowledge Distillation and Statistical Texture Distillation in section 3.2 and 3.3, respectively. Finally, we provide the optimization process in section 3.4.\\n\\n3.1. Overview\\nThe overall framework of the proposed method is illustrated in Figure 2. The upper network is the teacher network while the lower one is the student network. Following the previous works [30, 37, 40], the PSPNet [50] architecture is used for both the teacher and student, and ResNet-101 and ResNet-18 [19] are used as their backbone respectively, which can be also changed to any other backbone networks. Firstly, we adopt the same basic idea in knowledge distillation to align the response-based knowledge between the teacher and student as previous works [30, 37, 40]. Specifically, we use the KL divergence to supervise the pixel-wise probability distribution and adversarial learning to strengthen the output segmentation maps. Furthermore, we propose to extract two kinds of the texture knowledge...\"}"}
{"id": "CVPR-2022-1766", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. LP decomposition [11, 12, 29]. The low-pass subbands \\\\( a \\\\) is generated from the input \\\\( x \\\\) with a low-pass analysis filters \\\\( H \\\\) and a sampling matrix \\\\( S \\\\). The high-pass subbands \\\\( b \\\\) are then computed as the difference between \\\\( x \\\\) and the prediction of \\\\( a \\\\), with a sampling matrix \\\\( S \\\\) followed by a low-pass synthesis filters \\\\( G \\\\).\\n\\nfrom the first two layers of ResNet backbone for both the teacher and student model, as the texture information is more reflected on low-level features. For structural texture knowledge, we introduce a Contourlet Decomposition Module (CDM) which exploits structural information in the spectral space. For statistical texture knowledge, we introduce a Denoised Texture Intensity Equalization Module (DTIEM) to adaptively extract the statistical texture intensity histogram with an adaptive importance sampler, then enhance it with a denoised operation and graph reasoning. Finally we optimize two kinds of the knowledge between the teacher and student model with two individual Mean Squared (L2) loss.\\n\\n3.2. Structural Texture Knowledge Distillation\\n\\nTraditional filters have inherent advantages for texture representation with different scales and directions in the spectral domain, and here we consider to utilizing the contourlet decomposition, a kind of multiscale geometric analysis tool which has substantial advantages in locality and directionality [11\u201314, 29, 36], and can enhance the ability of geometric transformations in CNN. Based on the advantages, we introduce a Contourlet Decomposition Module (CDM) [11, 29] to mine the texture knowledge in the spectral space. The light red part in Figure 2 shows the details of CDM. Specifically, it adopts a Laplacian Pyramid (LP) [5] and a directional filter bank (DFB) [1,3,9] iteratively on the low-pass image. The LP is aimed to obtain multiscale decomposition. As shown in Figure 3, given the input feature \\\\( x \\\\), a low-pass analysis filters \\\\( H \\\\) and a sampling matrix \\\\( S \\\\) are used to generate downsampled low-pass subbands, then the high-pass subbands are obtained by the difference between the original \\\\( x \\\\) and the intermediate result, which is computed by a sampling matrix \\\\( S \\\\) and a low-pass synthesis filters \\\\( G \\\\).[5, 29]. Next, DFB is utilized to reconstruct the original signal with a minimum sample representation, which is generated by \\\\( m \\\\)-level binary tree decomposition in the two dimensional frequency domain, resulting in \\\\( 2^m \\\\) directional subbands [3]. For example, the frequency domain is divided into \\\\( 2^3 = 8 \\\\) directional subbands when \\\\( m = 3 \\\\), and the subbands 0-3 and 4-7 correspond to the vertical and horizontal details, respectively. Finally, the output of the contourlet decomposition in level \\\\( n \\\\) can be described by the following equations:\\n\\n\\\\[\\nF_{l,n+1}, F_{h,n+1} = LP(F_{l,n}) \\\\downarrow_p F_{bds,n+1} = DFB(F_{h,n+1})\\n\\\\]\\n\\n\\\\( n \\\\in [1, m] \\\\) (1)\\n\\nwhere the symbol \\\\( \\\\downarrow_p \\\\) is downsampling operator, \\\\( p \\\\) denotes the interlaced downsampling factor, \\\\( l \\\\) and \\\\( h \\\\) represent the low-pass and high-pass components respectively, \\\\( bds \\\\) denotes the bandpass directional subbands.\\n\\nFor a richer expression, we stack multiple contourlet decomposition layers iteratively in the CDM. In this way, abundant bandpass directional features are obtained via the contourlet decomposition from the low-level features, which are used as the structural texture knowledge \\\\( F_{str} \\\\) for distillation. We apply the CDM to the teacher and student networks respectively and use the conventional Mean Squared (L2) loss to formulate the texture distillation loss:\\n\\n\\\\[\\nL_{str}(S) = \\\\frac{1}{(W \\\\times H)X} \\\\sum_{i \\\\in \\\\mathbb{R}} F_{str}^T_i - F_{str}^S_i)^2\\n\\\\]\\n\\nwhere \\\\( F_{str}^T_i \\\\) and \\\\( F_{str}^S_i \\\\) denote the \\\\( i \\\\)th pixel in texture features produced from the teacher network \\\\( T \\\\) and the student network \\\\( S \\\\), and \\\\( i \\\\in \\\\mathbb{R} = W \\\\times H \\\\) represents the feature size.\\n\\n3.3. Statistical Texture Distillation\\n\\nThe statistical texture is usually of a wide variety and a continuous distribution in spectral domain, which is difficult to be extracted and optimized in deep neural networks. Previous work [52] firstly proposed a Quantization and Count Operator (QCO) to describe the statistical texture, which quantized the whole input feature into multiple uniform levels, then counted the number of features belonging to each level to get the quantization encoding matrix, followed by a graph module to perform quantization levels redistribution and texture enhancement. However, there are three limitations in the ordinary QCO. Firstly, it quantizes the input feature in a global manner, resulting in very sparse and discrete quantization levels distribution, which can not well balance the trade-off between accurate texture and computation burden. Secondly, amounts of pixels will never be quantized to any level as it generates the initial quantization levels by a uniform distribution, meanwhile using several narrow-peak functions to quantize the input feature. Thirdly, it may come across quantization noise produced by the over-amplify effect in near-constant areas during quantization process.\\n\\nWe propose to describe the statistical knowledge based on the ordinary QCO. Moreover, we propose three improvements in response to the three limitations as shown in the light green part in Figure 2. Firstly, for the sake of accurate characterization and efficient computation, we exploit an\"}"}
{"id": "CVPR-2022-1766", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Anchor-Based Adaptive Importance Sampler to only select the important regions for extraction. Secondly, we design a heuristics iterative method for a more equalized quantization level initialization. Thirdly, we utilize a denoised operation to suppress the over-amplify effect. Based on above analysis, we propose the Denoised Texture Intensity Equalization Module (DTIEM) to extract and enhance the statistical texture adaptively. In the following sections, we illustrate the above modules in detail.\\n\\nAnchor-Based Adaptive Importance Sampler. Extracting the statistical texture of the whole input feature is straightforward, but it lacks the attention on discriminative regions and only works well when the feature intensities of pixels are near-uniformly distributed. However, real-world scenes are usually offended with chaotic conditions and the pixel intensities tend to be heavily imbalanced distributed. Moreover, the global operation on a whole image is not able to describe local intensity contrast accurately and always means a large computation burden. Thus we consider using an importance sampling method to mine the hard-to-classify areas, where the feature intensity distribution is discrete and varying, while the statistical texture is rich and diverse. Following the typical paradigm [25], we design an Anchor-Based Adaptive Importance Sampler. It is designed to bias selection towards most uncertain regions, while retaining some degree of uniform coverage, by the following steps [25].\\n\\n(i) Over Generation: Aiming at sample $M$ points, to guarantee the variety and recall, we over-generate candidate points by randomly selecting the $kM$ ($k > 1$) points with a uniform distribution.\\n\\n(ii) Importance sampling: In the $kM$ points, we are intended to choose out the most uncertain $\\\\beta M$ ($\\\\beta \\\\in [0, 1]$) ones with an anchor-based adaptive importance sampling strategy.\\n\\n(iii) Coverage: to balance the distribution, we select the remaining $(1 - \\\\beta) M$ ones from the rest of points with a uniform distribution.\\n\\nAs found that the most essential step lies in importance sampling, we formulate this process as follows. For each sample $s_i \\\\in kM$, we set several anchors with various scales and aspect ratios at the location. In this way, we generate $\\\\xi$ region proposals $R_i$ for each $s_i$. For each $r_{ij} \\\\in R_i$ ($i \\\\in [1, kM], j \\\\in [1, \\\\xi]$), we calculate its sample probability by:\\n\\n$$\\\\text{prob}_{ij} = \\\\text{std}(r_{ij})$$\\n\\nwhere $\\\\text{std}(\\\\cdot)$ means the variance function. It shows that the region with larger variance will be more likely sampled, as its intensity distribution is diverse and may have more rich statistical textures that need to be enhanced. Noted that the student utilizes the same importance sampling results as teacher to select region proposals.\\n\\nTexture Extraction and Intensity Equalization. QCO is inspired of histogram quantification [18] and describes texture in a statistical manner. Given the input feature $A \\\\in \\\\mathbb{R}^{C \\\\times H \\\\times W}$, it firstly calculates the self-similarity matrix $S$, and quantizes it into $N$ uniform levels $L$. Next it counts the value for each quantization level and gets the quantization encoding matrix $E \\\\in \\\\mathbb{R}^{N \\\\times HW}$. Finally it utilizes a simple fully-connected graph to perform quantization levels redistribution and texture enhancement. The quantization can be formulated as:\\n\\n$$L_n = \\\\max(S) - \\\\min(S)\\\\cdot n \\\\in [1, N]$$\\n\\n$$E_{ni} = \\\\frac{1}{N} \\\\cdot \\\\begin{cases} 1 - |L_n - S_i|, & \\\\text{if } 0 < L_n - S_i < 0.5N \\\\\\\\ N, & \\\\text{else} \\\\end{cases}$$\\n\\nwhere $i \\\\in [1, HW]$. Based on the above discussion, we propose the other two improvements here.\\n\\nFor the quantization level, a heuristics iterative method is adopted instead of a uniform quantization. Specifically, we use a simple $t$-step uniform sampling to formulate an attention sampled results. Firstly, we over-quantize the input into $2^N$ levels to guarantee that most points can be quantized to one of the levels, obtaining the quantization encoding matrix with Eq. 4. Based on the count values of all quantization levels, we set an intensity ratio threshold $\\\\delta$ to divide the quantization levels into two groups $G_\\\\leq\\\\delta$ and $G_\\\\geq\\\\delta$, which are then re-quantized into $\\\\alpha N$ ($\\\\alpha \\\\in [0, 1]$) and $(1 - \\\\alpha) N$ levels to obtain final $N$ levels. The above process can be performed iteratively. In this way, the imbalanced distribution problem among quantization levels can be weaken. Moreover, for each quantization function in Eq. 4, we widen its peak adaptively by replacing the $N$ with its current group levels number, as each intermediate group level number is much smaller than $N$, thus the cover range of each quantization function can be broadened.\\n\\nIntensity-Limited Denoised Strategy. For the noise over-amplity problem, inspired by the Contrast-Limited Adaptive Histogram Equalization [35], we propose an intensity-limited denoised strategy to constrain the intensity peak and redistribute the extra peak to all quantization levels dynamically. More comprehensively, for each selected region and $N$ quantization levels, we get the initial quantization encoding matrix $E$ by Eq. 4. Due to the near-constance sub-area, the count value of some levels may show in a very high peak [35], we perform an intensity-clip operation to limit these peaks with a given ratio threshold $\\\\theta$, and then redistribute the sum of extra peak values $E_{\\\\text{extra}}$ equally among all the quantization levels. Finally, the denoised quantization encoding matrix $DE$ can be calculated:\\n\\n$$E_{\\\\text{extra}} = N \\\\cdot \\\\max(E_n - \\\\theta \\\\cdot \\\\max(E), 0)$$\\n\\n$$DE_n = \\\\theta \\\\cdot \\\\max(E) + E_{\\\\text{extra}} \\\\cdot \\\\frac{N}{\\\\theta \\\\cdot \\\\max(E) + E_{\\\\text{extra}}} \\\\text{ if } E_n > \\\\theta \\\\cdot \\\\max(E)$$\\n\\n$$E_n + E_{\\\\text{extra}} \\\\frac{N}{\\\\theta \\\\cdot \\\\max(E) + E_{\\\\text{extra}}} \\\\text{ else}$$\"}"}
{"id": "CVPR-2022-1766", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where $n \\\\in [1, N]$, max$(\\\\cdot)$ is the maximum function. Then DE is also enhanced with the graph reasoning as the previous work [52] to get the statistical texture knowledge $F_{sta}$.\\n\\nSimilarly as the structural texture knowledge distillation, we apply the DTIEM to both teacher and student respectively and use the L2 loss to formulate the distillation loss:\\n\\n$$L_{sta}(S_i) = \\\\frac{1}{W \\\\times H} \\\\sum_{X_i \\\\in \\\\mathbb{R}} (F_{sta}; T_i - F_{sta}; S_i)^2$$\\n\\nwhere $F_{sta}; T_i$ and $F_{sta}; S_i$ denote the statistical texture knowledge of the teacher and student respectively.\\n\\n3.4. Optimization\\n\\nFollowing the common practice and previous knowledge distillation works for semantic segmentation [30, 40], we also add the fundamental response-based distillation loss $L_{re}$ and adversarial loss $L_{adv}$ for stable gradient descent optimization:\\n\\n$$L_{re} = \\\\frac{1}{W_{re} \\\\times H_{re}} \\\\sum_{X_i \\\\in \\\\mathbb{R}} KL(P_{re}; T_i || P_{re}; S_i)$$\\n\\nwhere $P_{re}; T_i$ and $P_{re}; S_i$ denote the class probabilities of $i$-th pixel produced by the teacher and student model respectively, and $i \\\\in \\\\mathbb{R} = W_{re} \\\\times H_{re}$ represents the output size.\\n\\nThe adversarial training is aimed to formulate the holistic distillation problem [30, 43] and denoted as follows:\\n\\n$$L_{adv} = \\\\mathbb{E}_{S \\\\sim p(S)} \\\\left[ D(S|I) \\\\right]$$\\n\\nwhere $\\\\mathbb{E}(\\\\cdot)$ is the expectation operator, and $D(\\\\cdot)$ is the discriminator.\\n\\n$I$ and $S$ are the input image and corresponding segmentation map respectively.\\n\\nTherefore, for the overall optimization, the whole objective function consists of a conventional cross-entropy loss $L_{seg}$ for semantic segmentation and the abovementioned distillation loss:\\n\\n$$L = L_{seg} + \\\\lambda_1 L_{str} + \\\\lambda_2 L_{sta} + \\\\lambda_3 L_{re} - \\\\lambda_4 L_{adv}$$\\n\\nwhere $\\\\lambda_1, \\\\lambda_2, \\\\lambda_3, \\\\lambda_4$ are set to 0.9, 1.15, 5, 0.01, respectively.\\n\\n4. Experiments\\n\\n4.1. Datasets and Evaluation Metrics\\n\\nTo verify the effectiveness of the proposed method, we conduct experiments on the following large-scale datasets.\\n\\nCityscapes.\\n\\nThe Cityscapes dataset [10] has 5,000 images captured from 50 different cities, and contains 19 semantic classes. Each image has $2048 \\\\times 1024$ pixels, which have high quality pixel-level labels of 19 semantic classes. There are 2,979/500/1,525 images for training, validation and testing.\\n\\nADE20K.\\n\\nThe ADE20K dataset has 20K/2K/3K images for training, validation, and testing, and contains 150 classes of diverse scenes.\\n\\nPascal VOC 2012.\\n\\nThe Pascal VOC 2012 dataset [15] is a segmentation benchmark of 10,582/1,449/1,456 images for training, validation and testing, which involves 20 foreground object classes and one background class.\\n\\nEvaluation Metrics.\\n\\nIn all experiments, we adopt the mean Intersection-over-Union (mIoU) to study the distillation effectiveness. The model size is represented by the number of network parameters, and the Complexity is evaluated by the sum of floating point operations (FLOPs) in one forward propagation on a fixed input size.\\n\\n4.2. Implementation Details\\n\\nFollowing [30, 37, 40], we adopt PSPNet [50] with ResNet101 [19] backbone as the teacher network, and use PSPNet with different compact backbones as the student networks, including Resnet18 [19] and EfficientNet-B1 [38], which also validates the effectiveness when the teacher model and the student model are of different architectural types. In this paper, we adopt an offline distillation method, first train the teacher model and then keep the parameters frozen during the distillation progress. In the training process of the student network, random scaling (from 0.5 to 2.1) and random horizontal flipping (with the probability of 0.5) are applied as the data augmentation. We implement two-level contourlet decomposition iteratively in the CDM, where $m$ is set to 4 and 3 respectively. We set $N = 50$, $\\\\alpha = 0.3$, $\\\\theta = 0.0$. Stochastic Gradient Descent with momentum is deployed as the optimizer, where the momentum is 0.9 and weight decay rate is 1e-5. The base learning rate is 0.015 and multiplied by $(1 - \\\\frac{\\\\text{iter}}{\\\\text{iter}_{\\\\text{max}}})^{0.9}$.\\n\\nWe train the model for 80000 iterations with the batch size of 16.\\n\\n4.3. Ablation Study\\n\\nIn all the ablation studies, we use Cityscapes validation dataset, and ResNet-18 pretrained from ImageNet as the backbone of the student network.\\n\\nEfficacy of Two Kinds of the Texture Knowledge.\\n\\nTable 1 shows the effectiveness of two kinds of the texture knowledge.\\n\\n| Method | mIoU(%) |\\n|--------|---------|\\n| T: PSPNet-R101 | 78.56 |\\n| S: PSPNet-R18 | 69.10 |\\n| +Response Knowledge | 72.47 |\\n| +Response+Structural Texture Knowledge | 74.10 |\\n| +Response+Statistical Texture Knowledge | 74.69 |\\n| +Response+Structural+Statistical Texture Knowledge | 75.15 |\\n\\nTable 1. Efficacy of two kinds of the texture knowledge.\"}"}
{"id": "CVPR-2022-1766", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. The impact of layer number of contourlet decomposition.\\n\\n| Method   | Params (M) | FLOPs (G) |\\n|----------|------------|-----------|\\n| PSPNet   | 70.43      | 574.9     |\\n| CDM      | 1.24       | 10.90     |\\n| DTIEM    | 2.80       | 23.73     |\\n\\nTable 3. The FLOPs and Parameters of the proposed texture modules.\\n\\nComponents in DTIEM\\n\\n| Components                        | mIoU(%) |\\n|-----------------------------------|---------|\\n| baseline                          | 72.47   |\\n| +Global                           | 73.57   |\\n| +Adap. Samp.                      | 74.06   |\\n| +Adap. Samp. + Heuristics Init.   | 74.25   |\\n| +Adap. Samp. + Heuristics Init. + Denoised | 74.69   |\\n\\nTable 4. Ablation Study of Statistical Knowledge. \u201cGlobal\u201d means global operation without sampling, \u201cAdap. Samp.\u201d means anchor-based adaptive importance sampling, \u201cHeuristics Init.\u201d means quantization levels heuristics initialization.\\n\\nedge. The student network without distillation achieves the result of 69.10%, and the response knowledge improves it to 72.47%. Then we further add two kinds of the texture knowledge successively to validate the effect of each one. Concretely, it shows that the structural texture knowledge brings the improvement to 5.0%, and the statistical texture knowledge boosts the improvement to 5.59%. Finally, when we add both the texture knowledge, the performance is promoted to 75.15% with a large increase of 6.05%. The gap between student and teacher is finally reduced, providing a closer result to the teacher network.\\n\\nAnalysis of Structural Texture Knowledge.\\nWe conduct experiments to verify the effectiveness of the components in the structural texture module, and show the impact of level number of contourlet decomposition in the CDM. As shown in Table 2, \u201cbaseline\u201d means the results of student network with response knowledge, with the level number gradually increasing, mIoU is gradually increasing and stays still around 74.1%, which indicates that the texture knowledge is almost saturated when the level number gets to 2.\\n\\nAnalysis of Statistical Texture Knowledge.\\nIn Table 4, we show the effectness of different components in the statistical texture module, \u201cbaseline\u201d means the results of student network with response knowledge. Firstly, we perform a global feature intensity equalization on the whole image and get a limited improvement. Then we add the adaptive sampling, quantization level heuristics initialization and the denoised strategy successively, and the performance is gradually improved in this case.\\n\\nComplexity of Texture Extraction Modules.\\nWe show that the proposed texture modules are lightweight in Table 3, which are estimated with the fixed input size. It shows that the CDM and DTIEM only bring very little extra cost compared to the PSPNet.\\n\\nVisualization.\\nFigure 4 shows the visualization of the low-level feature from stage 1 of the backbone. KD means knowledge distillation. (a) is the original image. (b) is from the student network without texture knowledge distillation, (c) shows the changes after applying it in our method. Line 1 and 3 show the structural texture, while line 2 and 4 show the statistical texture.\\n\\nFigure 5. Visual improvements on Cityscapes dataset: (a) original images, (b) w/o distillation, (c) Our distillation method, (d) ground truth. Our method improves the student network w/o distillation to produce more accurate and detailed results, which are circled by dotted lines.\"}"}
{"id": "CVPR-2022-1766", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method              | Cityscapes | Pascal VOC | ADE20K |\\n|---------------------|------------|-----------|--------|\\n| ENet [34]           | 58.3       | -         | -      |\\n| ICNet [49]          | 69.5       | -         | -      |\\n| FCN [32]            | 62.7       | -         | -      |\\n| RefineNet [27]      | 73.6       | -         | -      |\\n| OCNet [47]          | 80.1       | -         | -      |\\n| STLNet [52]         | 82.3       | -         | -      |\\n| T: PSPNet-R101 [50] | 78.56      | 69.6      | 39.91  |\\n| S: PSPNet-R18       | 69.10      | 65.42     | 24.65  |\\n| + SKDS [30]         | 72.70      | 67.60     | 24.65  |\\n| + IFVD [40]         | 74.08      | -         | -      |\\n| + CWD [31]          | 74.87      | -         | -      |\\n| + SSTKD             | 75.15      | -         | -      |\\n| S: Deeplab-R18      | 73.37      | 69.61     | 39.91  |\\n| + SKDS [30]         | 73.87      | 69.61     | 39.91  |\\n| + IFVD [40]         | 74.09      | 69.61     | 39.91  |\\n| + CWD [31]          | 75.91      | 69.61     | 39.91  |\\n| + SSTKD             | 76.13      | 69.61     | 39.91  |\\n| S: EfficientNet-B1  | 60.40      | 69.61     | 39.91  |\\n| + SKDS [30]         | 63.13      | 69.61     | 39.91  |\\n| + IFVD [40]         | 66.50      | 69.61     | 39.91  |\\n| + CWD [31]          | -          | -         | -      |\\n| + SSTKD             | 68.26      | 69.61     | 39.91  |\\n\\nTable 5. Quantitative results on Cityscapes. \u201cR18\u201d (\u201cR101\u201d) means ResNet-18 (ResNet-101).\\n\\nTable 6. Quantitative results on Pascal VOC 2012 and ADE20K. \u201cR18\u201d (\u201cR101\u201d) means ResNet-18 (ResNet-101).\"}"}
{"id": "CVPR-2022-1766", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Vincent Andrearczyk and Paul F Whelan. Using filter banks in convolutional neural networks for texture classification. Pattern Recognition Letters, 84:63\u201369, 2016.\\n\\n[2] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. Segnet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE transactions on pattern analysis and machine intelligence, 39(12):2481\u20132495, 2017.\\n\\n[3] Roberto H Bamberger and Mark JT Smith. A filter bank for the directional decomposition of images: Theory and design. IEEE transactions on signal processing, 40(4):882\u2013893, 1992.\\n\\n[4] Cristian Bucilu, Rich Caruana, and Alexandru Niculescu-Mizil. Model compression. In Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, pages 535\u2013541, 2006.\\n\\n[5] Peter Burt and Edward Adelson. The laplacian pyramid as a compact image code. IEEE Transactions on communications, 31(4):532\u2013540, 1983.\\n\\n[6] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834\u2013848, 2017.\\n\\n[7] Liang-Chieh Chen, Yi Yang, Jiang Wang, Wei Xu, and Alan L Yuille. Attention to scale: Scale-aware semantic image segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3640\u20133649, 2016.\\n\\n[8] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European conference on computer vision (ECCV), pages 801\u2013818, 2018.\\n\\n[9] Mircea Cimpoi, Subhransu Maji, and Andrea Vedaldi. Deep filter banks for texture recognition and segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3828\u20133836, 2015.\\n\\n[10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3213\u20133223, 2016.\\n\\n[11] Minh N Do and Martin Vetterli. Contourlets: a directional multiresolution image representation. In Proceedings. International Conference on Image Processing, volume 1, pages I\u2013I. IEEE, 2002.\\n\\n[12] Minh N Do and Martin Vetterli. Contourlets, beyond wavelets. New York: Academic, 2003.\\n\\n[13] Minh N Do and Martin Vetterli. The contourlet transform: an efficient directional multiresolution image representation. IEEE Transactions on image processing, 14(12):2091\u20132106, 2005.\\n\\n[14] David Leigh Donoho and Ana Georgina Flesia. Can recent innovations in harmonic analysis explain key findings in natural image statistics? Network: computation in neural systems, 12(3):371\u2013393, 2001.\\n\\n[15] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (voc) challenge. International journal of computer vision, 88(2):303\u2013338, 2010.\\n\\n[16] Clement Farabet, Camille Couprie, Laurent Najman, and Yann LeCun. Learning hierarchical features for scene labeling. IEEE transactions on pattern analysis and machine intelligence, 35(8):1915\u20131929, 2012.\\n\\n[17] Jun Fu, Jing Liu, Haijie Tian, Yong Li, Yongjun Bao, Zhiwei Fang, and Hanqing Lu. Dual attention network for scene segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3146\u20133154, 2019.\\n\\n[18] Rafael C Gonzalez, Richard E Woods, et al. Digital image processing, 2002.\\n\\n[19] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\n[20] Tong He, Chunhua Shen, Zhi Tian, Dong Gong, Changming Sun, and Youliang Yan. Knowledge adaptation for efficient semantic segmentation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 578\u2013587, 2019.\\n\\n[21] Xuming He, Richard S Zemel, and Miguel \u00b4A Carreira-Perpi\u02dcn\u00b4an. Multiscale conditional random fields for image labeling. In Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004., volume 2, pages II\u2013II. IEEE, 2004.\\n\\n[22] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531, 2015.\\n\\n[23] Hanzhe Hu, Deyi Ji, Weihao Gan, Shuai Bai, Wei Wu, and Junjie Yan. Class-wise dynamic graph convolution for semantic segmentation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XVII 16, pages 1\u201317. Springer, 2020.\\n\\n[24] Deyi Ji, Haoran Wang, Hanzhe Hu, Weihao Gan, Wei Wu, and Junjie Yan. Context-aware graph convolution network for target re-identification. arXiv preprint arXiv:2012.04298, 2020.\\n\\n[25] Alexander Kirillov, Yuxin Wu, Kaiming He, and Ross Girshick. Pointrend: Image segmentation as rendering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9799\u20139808, 2020.\\n\\n[26] L'ubor Ladick `y, Chris Russell, Pushmeet Kohli, and Philip HS Torr. Associative hierarchical crfs for object class image segmentation. In 2009 IEEE 12th International Conference on Computer Vision, pages 739\u2013746. IEEE, 2009.\\n\\n[27] Guosheng Lin, Anton Milan, Chunhua Shen, and Ian Reid. Refinenet: Multi-path refinement networks for high-resolution semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1925\u20131934, 2017.\\n\\n[28] Guosheng Lin, Chunhua Shen, Anton Van Den Hengel, and Ian Reid. Exploring context with deep structured models for 16884 16884\"}"}
{"id": "CVPR-2022-1766", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[29] Mengkun Liu, Licheng Jiao, Xu Liu, Lingling Li, Fang Liu, and Shuyuan Yang. C-cnn: Contourlet convolutional neural networks. *IEEE Transactions on Neural Networks and Learning Systems*, 32(6):2636\u20132649, 2020.\\n\\n[30] Yifan Liu, Ke Chen, Chris Liu, Zengchang Qin, Zhenbo Luo, and Jingdong Wang. Structured knowledge distillation for semantic segmentation. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 2604\u20132613, 2019.\\n\\n[31] Yifan Liu, Changyong Shu, Jingdong Wang, and Chunhua Shen. Structured knowledge distillation for dense prediction. *IEEE transactions on pattern analysis and machine intelligence*, 2020.\\n\\n[32] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 3431\u20133440, 2015.\\n\\n[33] Mohammadreza Mostajabi, Payman Yadollahpour, and Gregory Shakhnarovich. Feedforward semantic segmentation with zoom-out features. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 3376\u20133385, 2015.\\n\\n[34] Adam Paszke, Abhishek Chaurasia, Sangpil Kim, and Eugenio Culurciello. Enet: A deep neural network architecture for real-time semantic segmentation. *arXiv preprint arXiv:1606.02147*, 2016.\\n\\n[35] Stephen M Pizer, E Philip Amburn, John D Austin, Robert Cromartie, Ari Geselowitz, Trey Greer, Bart ter Haar Romeny, John B Zimmerman, and Karel Zuiderveld. Adaptive histogram equalization and its variations. *Computer vision, graphics, and image processing*, 39(3):355\u2013368, 1987.\\n\\n[36] Yuheng Sha, Lin Cong, Qiang Sun, and Licheng Jiao. Unsupervised image segmentation using contourlet domain hidden markov trees model. In *International Conference Image Analysis and Recognition*, pages 32\u201339. Springer, 2005.\\n\\n[37] Changyong Shu, Yifan Liu, Jianfei Gao, Zheng Yan, and Chunhua Shen. Channel-wise knowledge distillation for dense prediction. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 5311\u20135320, 2021.\\n\\n[38] Mingxing Tan and Quoc V Le. Efficientnet: Rethinking model scaling for convolutional neural networks. *arXiv preprint arXiv:1905.11946*, 2019.\\n\\n[39] Petar Veli \u02c7ckovi\u00b4c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua Bengio. Graph attention networks. *arXiv preprint arXiv:1710.10903*, 2017.\\n\\n[40] Yukang Wang, Wei Zhou, Tao Jiang, Xiang Bai, and Yongchao Xu. Intra-class feature variation distillation for semantic segmentation. In *Proceedings of the European conference on computer vision (ECCV)*, 2020.\\n\\n[41] Shinji Watanabe, Takaaki Hori, Shigeki Karita, Tomoki Hayashi, Jiro Nishitoba, Yuya Unno, Nelson Enrique Yalta Soplin, Jahn Heymann, Matthew Wiesner, Nanxin Chen, et al. Espnet: End-to-end speech processing toolkit. *arXiv preprint arXiv:1804.00015*, 2018.\\n\\n[42] Jiafeng Xie, Bing Shuai, Jian-Fang Hu, Jingyang Lin, and Wei-Shi Zheng. Improving fast segmentation with teacher-student learning. *arXiv preprint arXiv:1810.08476*, 2018.\\n\\n[43] Zheng Xu, Yen-Chang Hsu, and Jiawei Huang. Training shallow and thin networks for acceleration via knowledge distillation with conditional adversarial networks. *International Conference on Learning Representations Workshop*, 2018.\\n\\n[44] Maoke Yang, Kun Yu, Chi Zhang, Zhiwei Li, and Kuiyuan Yang. Denseaspp for semantic segmentation in street scenes. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 3684\u20133692, 2018.\\n\\n[45] Changqian Yu, Jingbo Wang, Chao Peng, Changxin Gao, Gang Yu, and Nong Sang. Bisenet: Bilateral segmentation network for real-time semantic segmentation. In *Proceedings of the European conference on computer vision (ECCV)*, pages 325\u2013341, 2018.\\n\\n[46] Yuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation. In *Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part VI*. 16 pages 173\u2013190. Springer, 2020.\\n\\n[47] Yuhui Yuan and Jingdong Wang. Ocnet: Object context network for scene parsing. *arXiv preprint arXiv:1809.00916*, 2018.\\n\\n[48] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. *arXiv preprint arXiv:1612.03928*, 2016.\\n\\n[49] Hengshuang Zhao, Xiaojuan Qi, Xiaoyong Shen, Jianping Shi, and Jiaya Jia. Icnet for real-time semantic segmentation on high-resolution images. In *Proceedings of the European Conference on Computer Vision (ECCV)*, pages 405\u2013420, 2018.\\n\\n[50] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 2881\u20132890, 2017.\\n\\n[51] Hengshuang Zhao, Yi Zhang, Shu Liu, Jianping Shi, Chen Change Loy, Dahua Lin, and Jiaya Jia. Psanet: Pointwise spatial attention network for scene parsing. In *Proceedings of the European Conference on Computer Vision (ECCV)*, September 2018.\\n\\n[52] Lanyun Zhu, Deyi Ji, Shiping Zhu, Weihao Gan, Wei Wu, and Junjie Yan. Learning statistical texture for semantic segmentation. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, pages 12537\u201312546, June 2021.\"}"}
