{"id": "CVPR-2024-152", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"An Empirical Study of Scaling Law for Scene Text Recognition\\n\\nMiao Rang\u2217\\nZhenni Bi\u2217\\nChuanjian Liu\\nYunhe Wang\\nKai Han\u2020\\nHuawei Noah\u2019s Ark Lab\\n{rangmiao1, bizhenni, liuchuanjian, yunhe.wang, kai.han}@huawei.com\\n\\nAbstract\\nThe laws of model size, data volume, computation and model performance have been extensively studied in the field of Natural Language Processing (NLP). However, the scaling laws in Scene Text Recognition (STR) have not yet been investigated. To address this, we conducted comprehensive studies that involved examining the correlations between performance and the scale of models, data volume and computation in the field of text recognition. Conclusively, the study demonstrates smooth power laws between performance and model size, as well as training data volume, when other influencing factors are held constant. Additionally, we have constructed a large-scale dataset called REBU-Syn, which comprises 6 M real samples and 18 M synthetic samples. Based on the disclosed scaling law and new dataset, we successfully trained a scene text recognition model, achieving a new state-of-the-art on 6 common test benchmarks with top-1 average accuracy of 97.42%. The models and dataset are publicly available at large-ocr-model.github.io.\\n\\n1. Introduction\\nOptical Character Recognition (OCR) technologies are pivotal in extracting textual content from images, including scanned documents and photographs. However, this paper narrows its focus to the text recognition phase, specifically to Scene Text Recognition (STR). STR stands out in the OCR field due to its complexity and the unique challenges it presents, such as variable illumination, occlusion, distortion, and viewing angles. These factors pose significant challenges in text identification compared to recognizing text in scanned documents. As a rapidly evolving field of research, STR offers substantial opportunities for technological advancements and innovation. Given this context, our study aims to specifically explore the scaling laws that are applicable to STR. By delving into the study of scaling laws, our primary objective is to gain a deeper understanding of how adjustments in system parameters impact the performance of STR. This investigation holds the potential to unveil new avenues for improvement in this challenging domain.\\n\\nWith the introduction of large-scale models in deep learning, an increasing number of academics are focusing on the potential and growth trends of these models, hoping that they will contribute to the development and design of future models. In the field of NLP [4, 8], numerous experiments have been carried out to investigate scaling model laws [14, 18, 24, 72]. The results show that the larger the volume of data fed into the neural network, the better its performance. Therefore, large language models trained on vast amounts of data have dominated the field of NLP. However, in the STR domain, research predominantly focuses on enhancing model performance using fixed\"}"}
{"id": "CVPR-2024-152", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"data volumes and model sizes \\\\cite{3, 31, 53}. Studies specifically addressing the scaling laws in large STR models are noticeably sparse, which casts uncertainty on the potential impact of large-scale models and substantial data volumes in STR. Transformer-based models have achieved state-of-the-art performance in various text recognition tasks and challenges \\\\cite{31, 47, 52, 58}.\\n\\nIn this paper, we explore the scaling laws of STR by transformer-based models. Our focus is on unraveling the relationships between model size, data volume and computation with the model performance. Our experimental framework encompasses a wide range of models, with parameter counts ranging from 50 million to 1 billion, and data volumes that vary from 1 million to 1 billion training samples. Additionally, we extend our exploration to computational durations ranging from 100 to 1000 hours. This comprehensive analysis allows us to draw insightful conclusions about the scaling law in text recognition. Furthermore, we introduce a novel dataset called REB-U-Syn, which combines real-world and synthetic data. This dataset has been meticulously compiled from existing public datasets, providing a valuable resource for further research in this field.\\n\\nThroughout this research, we develop an advanced method for large-scale training. This method involves a comprehensive examination of various strategies, such as optimizing training hyperparameters, analyzing data distributions and utilizing pre-training techniques. Our objective is to create a model characterized by exceptional precision and accuracy. The culmination of these efforts is the training of CLIP4STR-L using REB-U-Syn. This approach results in achieving a groundbreaking state-of-the-art performance of $97.42\\\\%$ on the test benchmark (see Fig 1). The following is a compilation of additional scaling laws for STR observations:\\n\\n- The scaling law holds in the field of STR. There exist smooth power laws between the size of the model, the volume of data, computation and performance.\\n- Large-scale models make better use of samples than small-scale models which means that large models achieve lower error rate with fixed amount of data.\\n- The proportion of training data from different sources is crucial for model training.\\n- Models pre-trained on STR-related data are more effective in STR tasks than models pretrained on general images like ImageNet.\\n\\n2. Related Work\\n\\nModel Scale\\nRecent research has extensively explored the scaling laws for Transformer language models, particularly in the field of NLP \\\\cite{24, 33}. These studies have established a set of universal principles for modeling scale. However, research specifically addressing STR remains scarce. Transformer-based methods, known for their higher tolerance to increased model depth and width, have been applied in various fields \\\\cite{10, 13, 15, 30, 46}. This study leverages these methods, with a specific focus on their application in STR, to provide guidance on making effective adjustments in model size.\\n\\nData Scale\\nIn the domain of image recognition, the scale of data plays a critical role. The performance of various models is significantly influenced by the size of the datasets used \\\\cite{5, 48, 50}. While different model types require varying data volumes, some previous methods \\\\cite{2} explored the impact of STR recognition tasks on different data scales, but their main focus was on CNN-based or attention-based approaches \\\\cite{7, 9, 29, 34, 53, 55}, and they focus solely on reducing the data scale. Furthermore, the availability of public datasets has facilitated extensive research and experimentation in this field \\\\cite{5, 48, 50}. This paper aims to build upon these foundations by conducting a comprehensive investigation into the effects of data scale, both at the lower and upper limits, as well as the distribution of data in STR tasks. Additionally, this study offers new insights into the alignment of real and synthetic data during the training of optimal models, filling a gap in current research.\\n\\nScaling Laws\\nThe rapid advancement of Large Language Models (LLMs) like ChatGPT \\\\cite{41} and GPT-4 \\\\cite{42} has sparked research into universal scaling laws \\\\cite{24, 30} in deep learning. These studies explore the relationship between model size, data volume, computation and performance, providing training principles for large models in NLP. \\\\cite{20} describes laws for autoregressive generative modeling. Similar scaling theories have also emerged in the field of computer vision \\\\cite{72}, as demonstrated by the training of ViT-G with 2B parameters \\\\cite{72}. Furthermore, recent work has been done on the scaling law of CLIP \\\\cite{45} has revealed task- and dataset-dependent scaling behaviors \\\\cite{32}. Building upon these foundational insights, this study represents a unique exploration of scaling laws within the context of STR. Specifically, it explores the allocation of parameters and the internal structure of the transformer model, with the aim of optimizing performance for text recognition. This investigation makes a unique contribution to the expanding body of research on scaling laws, particularly in the underexplored domain of STR.\"}"}
{"id": "CVPR-2024-152", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Currently, we have amalgamated all publicly available datasets to construct the REBU-Syn dataset. This paper also includes a thorough analysis of the data proportions obtained from various sources. Finally, we will provide a detailed overview of the training parameter settings used in our study.\\n\\n3.1. Model Scaling\\n\\nTrOCR [31] is a text recognition model that utilizes pure Transformer architecture. It integrates pre-trained Computer Vision (CV) and NLP models. And it is the first work that jointly leverages image Transformer and text Transformer for text recognition tasks. The scaling laws of Transformer language models [24] and Vision Transformers [72] have been studied, the scaling laws for models in the text recognition have not yet been explored. Based on this, we scaled the TrOCR model sizes and attempted to analyze the accuracy change curves of models with varying sizes.\\n\\nIn TrOCR, the encoder and decoder parts utilize pre-trained image Transformer and text Transformer models, respectively. These pre-trained models utilize large-scale unlabeled data for image understanding and language modeling. As a result, TrOCR does not require additional language models for post-processing, and the model outperforms current state-of-the-art models in tasks related to printing and handwriting recognition. In order to continue benefiting from pre-training for related tasks, we select the most suitable combination of encoder and decoder in TrOCR for scaling.\\n\\nFor TrOCR-S, we use DeiT SMALL [59] to initialize the encoder and MiniLM [64] to initialize the decoder. TrOCR-B uses BEIT BASE [4] to initialize the encoder and RoBERTa LARGE [36] to initialize the decoder. TrOCR-L and TrOCR-H utilize BEIT LARGE to initialize the encoder and RoBERTa LARGE to initialize the decoder. The model's parameters range from 43.09 million to 1 billion, and the details of the parameters are shown in Table 1.\\n\\n| Model     | Encoder | FLOPs (G) | Params (M) |\\n|-----------|---------|-----------|------------|\\n|           | layers  | hidden sizes | heads     |            |\\n| TROCR-S   | 12      | 384       | 6         | 13.31      |\\n| TROCR-B   | 12      | 768       | 12        | 62.01      |\\n| TROCR-L   | 24      | 1024      | 16        | 191.00     |\\n| TROCR-H   | 48      | 1200      | 16        | 497.91     |\\n\\nTable 1. Architecture specifications of TrOCR variants.\\n\\nPARSeq [6] follows an encoder-decoder architecture. PARSeq is also based on the Transformer framework with excellent accuracy, which perfectly fits the scope of our research. The encoder part utilizes the Vision Transformer (ViT) model to extract image features, while the decoder follows the same architecture as the pre-LayerNorm [63]. Transformer decoder in this study utilizes twice the number of attention heads, where nhead = dmodel/32. In contrast to the standard ViT, the encoder removes the [class] token and inputs all the output tokens into the decoder.\\n\\nPARSeq has two models in the original paper, PARSeq-Ti and PARSeq-S. In order to investigate the law of large models in the field of text recognition, the scaling law of the ViT model was demonstrated. [72]. Based on this, we scaled PARSeq to 4 different sizes. On the basis of the original paper PARSeq-S, the model was expanded to 3 sizes: PARSeq-B, PARSeq-L, and PARSeq-H. The scale of the model was also expanded from 22 million to 0.6 billion. The configurations with different scale PARSeq models can be seen in Table 2.\\n\\n| Model     | Encoder | FLOPs (G) | Params (M) |\\n|-----------|---------|-----------|------------|\\n|           | layers  | hidden sizes | heads     |            |\\n| PARSeq-S  | 12      | 384       | 6         | 2.76       |\\n| PARSeq-B  | 12      | 768       | 12        | 17.20      |\\n| PARSeq-L  | 24      | 1024      | 16        | 49.90      |\\n| PARSeq-H  | 32      | 1280      | 16        | 98.10      |\\n\\nTable 2. Architecture specifications of PARSeq variants.\\n\\n3.2. Dataset\\n\\nTraining Dataset\\n\\nThe training datasets for text recognition are typically categorized into synthetic and real data. Historically, scene text recognition models primarily relied on synthetic data due to the scarcity of real-world data. However, the recent increase in the availability of real data has shifted this trend. It has been observed that models trained on real data tend to be more sample-efficient compared to those trained on synthetic data. In light of this, we meticulously collected both synthetic and real data, employing various strategies to construct the REBU-Syn dataset. This dataset comprises approximately 6M real data samples and 18M public synthetic data samples, as detailed in Table 3. The ratio of synthetic to real data in REBU-Syn is 3:1. Furthermore, we utilized synthesis technology to generate an additional 60M data samples, similar to MJST, termed MJST +.\\n\\nReal Dataset\\n\\nWe gather real images from 4 widely-accessible datasets to assemble the REBU. The R component consists of commonly used real data [6], including COCO-Text (COCO) [60], RCTW17 [54], UberText (Uber) [67], ArT [12], LSVT [57], MLT19 [40], ReCTS [35], TextOCR [56] and OpenVINO [27]. A detailed analysis of these datasets is presented in [6]. U, another segment of REBU, includes 4 million labeled images across 14 datasets, collectively referred to as Union14M-L [6]. B represents the training data from benchmark sources, encompassing datasets such as IIIT 5k-word (IIIT5k) [38], Street View Text (SVT) [62], ICDAR13 [25] and ICDAR15 [26]. Furthermore, E is composed of images from two commonly used real datasets in text detection tasks, namely Total Text [11] and CTW1500 [71]. This inclusion significantly expands the range of real data in our study.\"}"}
{"id": "CVPR-2024-152", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Public Synthetic Dataset\\n\\nMJSynth (MJ) [22] and SynthText (ST) [19] are two widely-utilized synthetic datasets in the field of scene text recognition, containing 8.9M million and 5.5M million data samples respectively. Additionally, we incorporated two other composite datasets into our study. Curved SyntheText (CST) and SyntheAdd (SA) [21]. CST is specifically designed for text detection tasks, primarily comprising curved text data. SA, generated with the SynthText engine, is aimed at synthesizing less common characters, including punctuation marks.\\n\\nGenerate Synthetic Dataset\\n\\nTo closely align with the MJ and ST datasets, we created MJST using two data generation tools: TextRecognitionDataGenerator 1 and SynthText 2. The TextRecognitionDataGenerator is adept at producing data that mimics complex scenes, encompassing effects such as blurring, tilting and distortion. SynthText, on the other hand, specializes in synthesizing data akin to ST, resulting in samples that blend more seamlessly with natural scenes.\\n\\nTo augment the diversity of the generated corpus, we sourced 700,000 corpus entries from the most extensively utilized English corpus website globally 3. For the background selection in our synthesized images, we employed natural scene pictures provided by SynthText as the backdrop. Utilizing these two synthesis methods, we successfully synthesized a total of 60M data samples. Code for data synthesis is available 4.\\n\\n| Source Dataset | Instances |\\n|----------------|-----------|\\n| Public Real    | 3.3M      |\\n| Public Real Extra Real Data (E) | 15k |\\n| Public Real BenchMark (B) | 7.5K |\\n| Public Real Union14M (U) | 3.1M |\\n| Public Synthetic MJ | 5.5M |\\n| Public Synthetic ST | 8.9M |\\n| Public Synthetic CST | 1.8M |\\n| Public Synthetic SA | 1.2M |\\n\\nTable 3. Statistics of REBU-Syn datasets, including Public Real and Public Synthetic. Generate Synthetic can be used additionally.\\n\\nTest Dataset\\n\\nTo assess the performance of our model, we utilized 6 publicly available real scene text datasets: IIIT5k-Words (IIIT5k) [38], Street View Text (SVT) [62], ICDAR 2013 (IC13) [25], ICDAR 2015 (IC15) [26], SVT-Perspective (SVTP) [43] and CUTE80 (CUTE) [49]. Both the IC13 and IC15 test sets have various subdivisions. We follow the division proposed by Yu et al [70], using a version of the IC15 test set containing 1,811 images, and the IC13 test set comprising 857 images.\\n\\nHowever, to address challenges posed by differing annotation formats and the presence of duplicate, non-Latin, and damaged samples, we employed the following data fusion strategy:\\n\\n- **Polygonal Text**\\n  - We sourced synthesized data from datasets used in text detection tasks with polygonal annotation boxes, such as Curved SyntheText, SyntheAdd and STR Benchmark. To adapt these polygonal texts for use, we improved upon the method proposed in [6]. Our approach involves identifying the minimum bounding box of the polygon and applying a perspective transformation, avoiding direct clipping using maximum and minimum coordinates. This method retains challenging samples, as suggested in [6], while minimizing background interference, thus enabling the recognizer to focus on pertinent areas.\\n\\n- **Remove invalid chars and samples**\\n  - Focusing on Latin characters, which have extensive data availability, we retained samples composed only of letters and symbols. Samples not in our predefined dictionary were discarded.\\n\\n- **Remove duplicate data**\\n  - As we integrated multiple datasets, some of which overlapped, we meticulously removed any duplicate entries.\\n\\n3.3. Experiment Settings\\n\\nWe took use of the publicly available implementations of TrOCR and PARSeq as baseline models. To achieve optimal performance, we tailored the number of training epochs and adjusted the learning rates. The specific implementation details are as follows:\\n\\n**Hyper-Parameters**\\n\\nFor our experiments, we use V100 GPUs equipped with 32GB of memory to train all models. The learning rates are set differently for various models. Specifically, TrOCR-S is trained with a batch size of 1024 and a learning rate of $4 \\\\times 10^{-4}$. TrOCR-B employs a batch size of 256 with a learning rate of $1 \\\\times 10^{-4}$, and TrOCR-L operates with a batch size of 128 and a learning rate of $4 \\\\times 10^{-5}$.\\n\\nWe use BPE [51] of Fairseq and SentencePiece [28] for tokenizing text lines into word pieces. For PARSeq models, a consistent learning rate of $7 \\\\times 10^{-4}$ is used, with the batch size adjusted to be as close to 1024 as possible.\\n\\n**Evaluation Metrics**\\n\\nWord accuracy was the primary metric for evaluating the datasets of scene text. In this work, we standardized the final output string to match the commonly used 36-character set (lowercase alphanumeric) to ensure a fair comparison across different models and datasets.\\n\\n4. Results and Analysis\\n\\n4.1. Smooth Power Laws\\n\\nModel performance is primarily influenced by three variables: the number of model parameters $N$, the volume of the training data $D$, and the computation of the model $C$.\\n\\nIn this section, we explore the power laws among these variables.\"}"}
{"id": "CVPR-2024-152", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"influential factors with model performance. To effectively characterize the scaling of models, we have conducted training with a variety of models, including TrOCR and PARSeq.\\n\\n### Table 4. Word accuracy with different TrOCR model sizes.\\n\\n| Model  | Avg IC13 | IIIT5k | SVT  | CUTE80 | Avg IC15 | SVTP |\\n|--------|----------|--------|------|--------|----------|------|\\n| TrOCR-S| 90.65    | 85.60  | 85.94| 74.31  | 81.93    | 90.65|\\n| TrOCR-B| 96.14    | 92.00  | 91.56| 80.56  | 88.56    | 96.14|\\n| TrOCR-L| 96.50    | 92.90  | 92.81| 84.38  | 89.84    | 96.50|\\n| TrOCR-H| 97.31    | 93.57  | 94.22| 87.50  | 90.94    | 97.31|\\n\\n4.1.1 The power law of model when data is fixed.\\n\\n- **Scaling TrOCR Models**\\n  \\n  We trained 4 different scales (ranging in size from 43.09M to 1B) of TrOCR models. In order to maintain fairness and consistency with the experimental setting in the original TrOCR paper, we use MJ and ST to train TrOCR models with different model sizes. The experimental results on 6 common test benchmarks are shown in Table 4. As shown in Fig 2a, our analysis reveals a linear relationship on the log-log plot between the parameter count $N$ and modeling performance. This relationship can be described by a power-law equation ($E = aC^b$). Employing Algorithm 1 in the appendix, we utilized the first three models (TrOCR-S, TrOCR-B and TrOCR-L) to obtain the power function equation $E(N)$. The last model (TrOCR-H) accurately aligns with the fitted straight line, demonstrating the effectiveness of the power law. The power law of the TrOCR model is as follows:\\n\\n$$E(N) = 1.97 \\\\times 10^4 / N^{0.223}$$\\n\\n- **Scaling PARSeq Models**\\n  \\n  To further validate the power law in relation to model parameters, we trained PARSeq models across 4 different scales with sizes ranging from 22M to 0.6B parameters, using the REBU-Syn dataset. The results of these experiments on 6 common test benchmarks are detailed in Table 5. As shown in Fig 3, the PARSeq demonstrates a similar trend to that observed with TrOCR, reinforcing the existence of the power law on model size. The power law of the PARSeq model is as follows:\\n\\n$$E(N) = 6.316 \\\\times 10^{-74} / N^{0.018}$$\\n\\n4.1.2 The power-law of data when model size is fixed.\\n\\n- **Scaling Data Volume on TrOCR**\\n  \\n  In order to explore the impact of data volume on model performance. We use MJ+ST and MJST+ for training TrOCR-B. We randomly sampled data at various scales, with sizes ranging from 0.75M to 75M. The experimental results of TrOCR-B based on data of different scales are compiled in Table 6. We used various levels of data volume (solid blue line) to fit the power function (Eq. 3) as shown by the solid grey line in Fig. 2b. The remaining portion of the data size (represented by the dashed blue line) still closely follows the power function, providing further evidence that the data volume adheres to the power function.\\n\\n$$E(D) = 1.84 \\\\times 10^5 / D^{0.327}$$\\n\\n- **Scaling Data Volume on PARSeq**\\n  \\n  Based on the power law of data volume, we utilize REBU-Syn in PARSeq-S training. By gradually expanding the data samples, the accuracy of PARSeq-S has been significantly improved in the Table 7.\\n\\n### Table 7. PARSeq-S average accuracy in different percent of training data.\\n\\n| Data Volume | Avg IC13 | IIIT5k | SVT  | CUTE80 | Avg IC15 | SVTP |\\n|-------------|----------|--------|------|--------|----------|------|\\n| R3.30M      | 95.57    | 97.32  | 97.87| 97.37  | 97.22    | 90.34|\\n| R+B+E3.32M  | 95.63    | 97.43  | 97.97| 97.84  | 98.96    | 90.28|\\n| R+B+E+U6.42M| 96.12    | 99.53  | 97.93| 97.53  | 98.96    | 91.39|\\n| R+B+E+U+MJST20.82M | 96.45 | 98.48  | 98.40| 97.84  | 98.61    | 91.44|\\n| R+B+E+U+Syn23.82M | 96.85    | 98.93  | 98.63| 98.61  | 99.31    | 92.32|\\n\\n4.1.3 The power law of computation\\n\\nWith the power laws of model size and data volume separately, we infer that the error rate and the compute budget can also be fit with the power law. We perform the study on TrOCR model. The outcome is depicted as the gray line on the plot on the right-hand side of Fig. 2c. It can be fitted with the power formula as in Eq. 4.\\n\\n$$E(C) = 4.45 \\\\times 10^4 / C^{0.3271}$$\\n\\n4.2. Other Observations\\n\\nLarge-scale models make better use of samples. As we continue to enlarge the model size, the model accuracy improves consistently. The phenomenon can be observed...\"}"}
{"id": "CVPR-2024-152", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2. Improvement in TrOCR model performance with increasing model sizes, data volumes, and training computation. Model performance is measured by calculating the average word error rate on 6 common test benchmarks\\n\\n(a) Model sizes (Params)\\n\\n(b) Data volumes (M)\\n\\n(c) Computation (training hours)\\n\\nFigure 3. The average word error rate on 6 common test benchmarks was calculated using the PARSeq model size. The solid line represents the fitted power law $E(\\\\cdot)$, and the points on the dotted line correspond to the power law equation.\\n\\nTable 4 and 5. To improve training results, we can modify recognition models built on the Transformer architecture by utilizing the scaling laws of the vision Transformer. As shown in Figure 4 with respect to the total number of images \\\"seen\\\" during PARSeq training stage of different sizes, it is clear that larger models utilize samples more effectively than their smaller models. When PARSeq models of different sizes are trained with an equal number of samples, smaller models exhibit a higher error rate compared to larger models. Furthermore, we observed that larger models tend to require fewer epochs to converge. For instance, PARSeq-S reached optimal accuracy in 32 epochs, whereas PARSeq-B needed only 14 epochs, and PARSeq-L just 5 epochs. These findings suggest that with adequate training resources, it is more beneficial to train a larger model for fewer steps. This mirrors similar findings in language modelling [24] and machine translation [17]. However, when training time is a limiting factor, opting for a smaller model may be more practical.\\n\\nThe proportion of training data from various sources is crucial for model training. The REBU-Syn comprises both real and synthetic data. According to prior studies [6, 73], real data typically outperforms synthetic data in training efficiency, though synthetic data still plays a valuable role. Due to the high costs associated with obtaining and labeling real data, which often do not meet the volume requirements for model training, reliance on synthetic data is necessary. However, the effectiveness of synthetic data raises a question: Does more synthetic data always equate to better performance? Our findings suggest that an optimal ratio between real and synthetic data is crucial for enhanced model performance.\\n\\nTo achieve this objective, we conducted an experiment to investigate the proportional relationship between data obtained from different sources.\"}"}
{"id": "CVPR-2024-152", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tained from various sources and determine the most efficient utilization of synthetic data. Synthetic data can be primarily categorized into two types: MJ+ST and Syn (CST+SA). MJ+ST, characterized by its large volume but homogeneous nature (consisting mostly of straight and clear samples), contrasts with SynText, which has a smaller volume (only one-fifth of MJ+ST) and primarily consists of curved texts.\\n\\nTo evaluate the impact of different synthetic data sources on model accuracy, we trained PARSeq using a combination of real data and these synthetic datasets. The results, as presented in Table 8, are revealing. The accuracy achieved using real data combined with MJ+ST is 96.24%, only marginally higher by 0.05% compared to using real data with Syn. Given that the volume of MJ+ST is five times that of Syn, this implies that complex synthetic data is more sample-efficient. By simultaneously utilizing synthetic data from both MJ+ST and SynText along with real data, we observed a substantial enhancement in the accuracy of PARSeq, elevating it to state-of-the-art levels. This combination of diverse synthetic data styles, when integrated with real data, expands the range of the training data distribution. Such comprehensive coverage effectively enhances the overall quality and performance of the model.\\n\\nTable 8. PARSeq-S average accuracy of integrating diverse synthetic and real data types.\\n\\n| Real DataSet     | Syn DataSet   | Data Ratio | Word Acc |\\n|------------------|---------------|------------|----------|\\n| R+E+B+U          | Syn           | 1:0.5      | 96.19%   |\\n| R+E+B+U          | MJ+ST         | 1:2.5      | 96.24%   |\\n| R+E+B+U+Syn      | MJ+ST         | 1:3        | 96.85%   |\\n\\nAdditionally, we investigated the effect of different synthetic-to-real data ratios on the accuracy of PARSeq-S. We maintained a constant amount of real data and progressively increased the volume of synthetic data. The ratio of synthetic to real data varied from 0.5 to 5 times. These varying proportions were achieved through random sampling. To augment the total volume of synthetic data, we randomly selected 18M samples from MJST and combined them with the synthetic data in REBU-Syn, culminating in a total of 36M synthetic data samples.\\n\\nTable 9. PARSeq-S average accuracy on 6 common test benchmarks with varying ratios of synthetic and real data.\\n\\n| Data Ratio    | Word Acc |\\n|---------------|----------|\\n| Real:Syn=1:0.5| 96.32%   |\\n| Real:Syn=1:1  | 96.50%   |\\n| Real:Syn=1:2  | 96.59%   |\\n| Real:Syn=1:3  | 96.85%   |\\n| Real:Syn=1:4  | 96.76%   |\\n| Real:Syn=1:5  | 95.70%   |\\n\\nWhile synthetic data proves effective, it requires careful balancing with real data. As shown in Table 9, a gradual increase in synthetic data led to some improvement in accuracy. Notably, the highest accuracy of 96.85% was achieved with a synthetic-to-real data ratio of 1:3. Beyond this ratio, the accuracy began to decline, likely due to the data distribution becoming overly skewed towards synthetic data, which can adversely affect model performance. Therefore, we recommend a real-to-synthetic data ratio of 1:3. This balance offers the most significant improvement in accuracy without incurring excessive training costs.\\n\\nTable 10. Average accuracy achieved by using visual task pre-training and STR task pre-training on 6 common test benchmarks.\\n\\n| Pre-training Data | Training Data | Backbone | Word Acc |\\n|-------------------|---------------|----------|----------|\\n| Scratch           | R+E+B+U      | ViT-S    | 96.12%   |\\n| Scratch           | R+E+B+U+Syn  | ViT-S    | 96.85%   |\\n| Scratch           | R+E+B+U      | ViT-L    | 97.03%   |\\n| Scratch           | R+E+B+U+Syn  | ViT-L    | 96.74%   |\\n| ImageNet-21k      | R+E+B+U+Syn  | ViT-L    | 97.03%   |\\n\\nThe utility of pretrained models in low-level vision tasks is well-known, but their applicability in STR tasks warrants investigation. To address this, we experimented with various pretrained models, some trained on ImageNet and others specifically for text recognition tasks. In the last two rows of Table 10, we maintained consistent training schedules, learning rates and epochs as used for PARSeq. Intriguingly, the ImageNet-21k pre-trained models underperformed compared to those trained from scratch, a trend observed in both PARSeq and CLIP4STR models. This suggests that pretraining on non-STR-specific tasks might not be beneficial, and can even be detrimental to STR performance. The STR task necessitates a connection between visual and textual elements, akin to the CLIP experiment's original purpose, whereas purely visual tasks focus more on high-level semantics and lack the textual nuances critical for STR.\\n\\nAdditionally, when we trained PARSeq-S using the REBU-Syn dataset, it achieved a higher accuracy of 96.85% compared to training solely on the real data REBU. Further fine-tuning the 96.85% model with REBU led to an increased accuracy of 97.01%, indicating an improvement. This demonstrates the efficacy of task-related pretrained models in STR tasks. To attain higher accuracy, a recommended approach is training on all data first and then fine-tune on real data.\\n\\n4.3. Comparison with SOTA Methods\\n\\nRecently, the remarkable performance of CLIP4STR across multiple benchmarks prompted us to conduct further experiments, guided by our scaling law. Initially, we focused on data composition, employing a 3:1 ratio of synthetic to real data for training the model, in conjunction with fine-tuning using a pre-trained model for related tasks. Our reproducible results led to a notable improvement in CLIP4STR-B, enhancing its accuracy from 96.54% to 97.25%, an increase of 0.71%.\"}"}
{"id": "CVPR-2024-152", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 11. Word accuracy on 6 common test benchmarks, * indicates training with REBU-Syn, Avg is the weighted average result on 6 common test benchmarks.\\n\\nTo further delve into the impact of larger models, we replicated this experiment on CLIP4STR-L. This model achieved a new state-of-the-art, recording a top-1 average accuracy of 97.42% on 6 common test benchmarks, as detailed in Table 11. These findings underscore the significant role of large models in advancing the field of STR.\\n\\n5. Discussion and Conclusion\\n\\nIn this paper, we demonstrate the existence of smooth power laws in the field of STR. Specifically, we show that increasing the model size, data volume, and computational resources leads to predictable improvements in model performance. Additionally, we identified several key principles for effective model training in STR: 1) Large-scale models utilize samples more efficiently. 2) The proportion of training data from various sources is crucial for model training; 3) Task-related pre-trained models enhance effectiveness. Beyond identifying these guiding principles, we compiled a large-scale dataset to improve the performance of the STR model. Leveraging these rules, we successfully trained a model that achieved a new state-of-the-art average accuracy of 97.42% on the test benchmark.\\n\\nWe conduct extensive experiments on both model scaling and data scaling, successfully demonstrating the existence of scaling laws in STR. Furthermore, we observe that data scaling is a particularly advantageous approach as it enhances model accuracy without incurring additional costs during training or inference. However, challenges persist in the realm of model scaling. While large-scale models exhibit superior performance with substantial data, their training is considerably more costly. Adjusting each parameter can be extremely expensive, with each training iteration potentially costing millions of dollars. To optimize the performance of these models, careful selection of the best hyperparameters during training is essential. We hope that our work can attract more researchers' attention to reduce the training cost of large-scale models.\\n\\nOur experiments are based on large-scale natural scene text data sets. In the future, we will consider exploring the scaling law in more challenging text recognition data sets such as handwriting and historical texts.\"}"}
{"id": "CVPR-2024-152", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Rowel Atienza. Vision transformer for fast and efficient scene text recognition. 2021.\\n\\n[2] Jeonghun Baek, Geewook Kim, Junyeop Lee, Sungrae Park, Dongyoon Han, Sangdoo Yun, Seong Joon Oh, and Hwal-suk Lee. What is wrong with scene text recognition model comparisons? dataset and model analysis. In Proceedings of the IEEE/CVF international conference on computer vision, pages 4715\u20134723, 2019.\\n\\n[3] Jeonghun Baek, Yusuke Matsui, and Kiyoharu Aizawa. What if we only use real datasets for scene text recognition? toward scene text recognition with fewer labels. 2021.\\n\\n[4] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv preprint arXiv:2106.08254, 2021.\\n\\n[5] Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: A large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019.\\n\\n[6] Darwin Bautista and Rowel Atienza. Scene text recognition with permuted autoregressive sequence models. 2022.\\n\\n[7] Fedor Borisyuk, Albert Gordo, and Viswanath Sivakumar. Rosetta: Large scale system for text detection and recognition in images. In Proceedings of the 24th ACM SIGKDD international conference on knowledge discovery & data mining, pages 71\u201379, 2018.\\n\\n[8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:1877\u20131901, 2020.\\n\\n[9] Zhanzhan Cheng, Fan Bai, Yunlu Xu, Gang Zheng, Shiliang Pu, and Shuigeng Zhou. Focusing attention: Towards accurate text recognition in natural images. In Proceedings of the IEEE international conference on computer vision, pages 5076\u20135084, 2017.\\n\\n[10] Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019.\\n\\n[11] Chee Kheng Ch'ng and Chee Seng Chan. Total-text: A comprehensive dataset for scene text detection and recognition. In 2017 14th IAPR international conference on document analysis and recognition (ICDAR), pages 935\u2013942. IEEE, 2017.\\n\\n[12] Chee-Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao Zhang, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, Chee Seng Chan, and Lianwen Jin. Icdar2019 robust reading challenge on arbitrary-shaped text (rrc-art). 2019.\\n\\n[13] Mostafa Dehghani, Stephan Gouws, Oriol Vinyals, Jakob Uszkoreit, and \u0141ukasz Kaiser. Universal transformers. arXiv preprint arXiv:1807.03819, 2018.\\n\\n[14] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Scaling vision transformers to 22 billion parameters. arXiv preprint arXiv:2302.05442, 2023.\\n\\n[15] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018.\\n\\n[16] Shancheng Fang, Hongtao Xie, Yuxin Wang, Zhendong Mao, and Yongdong Zhang. Read like humans: Autonomous, bidirectional and iterative language modeling for scene text recognition. 2021.\\n\\n[17] Patrick Fernandes, Behrooz Ghorbani, Xavier Garcia, Markus Freitag, and Orhan Firat. Scaling laws for multilingual neural machine translation. 2023.\\n\\n[18] Behrooz Ghorbani, Orhan Firat, Markus Freitag, Ankur Bapna, Maxim Krikun, Xavier Garcia, Ciprian Chelba, and Colin Cherry. Scaling laws for neural machine translation. arXiv preprint arXiv:2109.07740, 2021.\\n\\n[19] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman. Synthetic data for text localisation in natural images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2315\u20132324, 2016.\\n\\n[20] Tom Henighan, Jared Kaplan, Mor Katz, Mark Chen, Christopher Hesse, Jacob Jackson, Heewoo Jun, Tom Brown, Prafulla Dhariwal, Scott Gray, et al. Scaling laws for autoregressive generative modeling. arXiv preprint arXiv:2010.14701, 2020.\\n\\n[21] Mingxin Huang, Yuliang Liu, Zhenghao Peng, Chongyu Liu, Dahua Lin, Shenggao Zhu, Nicholas Yuan, Kai Ding, and Lianwen Jin. Swintextspotter: Scene text spotting via better synergy between text detection and text recognition. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4593\u20134603, 2022.\\n\\n[22] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Synthetic data and artificial neural networks for natural scene text recognition. arXiv preprint arXiv:1406.2227, 2014.\\n\\n[23] Qing Jiang, Jiapeng Wang, Dezhi Peng, Chongyu Liu, and Lianwen Jin. Revisiting scene text recognition: A data perspective. 2023.\\n\\n[24] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020.\\n\\n[25] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras. Icdar 2013 robust reading competition. pages 1484\u20131493, 2013.\\n\\n[26] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al. Icdar 2015 competition on robust reading. pages 1156\u20131160, 2015.\\n\\n[27] Ilya Krylov, Sergei Nosov, and Vladislav Sovrasov. Open images v5 text annotation and yet another mask text spotter. 15627\"}"}
{"id": "CVPR-2024-152", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Taku Kudo and John Richardson. SentencePiece: A simple and language independent subword tokenizer and detokenizer for neural text processing. 2018.\\n\\nChen-Yu Lee and Simon Osindero. Recursive recurrent nets with attention modeling for OCR in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2231\u20132239, 2016.\\n\\nDmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. GShard: Scaling giant models with conditional computation and automatic sharding. 2020.\\n\\nMinghao Li, Tengchao Lv, Jingye Chen, Lei Cui, Yijuan Lu, Dinei Florencio, Cha Zhang, Zhoujun Li, and Furu Wei. TroCR: Transformer-based optical character recognition with pre-trained models. arXiv preprint arXiv:2109.10282, 2021.\\n\\nXianhang Li, Zeyu Wang, and Cihang Xie. An inverse scaling law for CLIP training. arXiv preprint arXiv:2305.07017, 2023.\\n\\nPeter J Liu, Mohammad Saleh, Etienne Pot, Ben Goodrich, Ryan Sepassi, Lukasz Kaiser, and Noam Shazeer. Generating wikipedia by summarizing long sequences. arXiv preprint arXiv:1801.10198, 2018.\\n\\nWei Liu, Chaofeng Chen, and Kwan-Yee Wong. CharNet: A character-aware neural network for distorted scene text recognition. In Proceedings of the AAAI conference on artificial intelligence, 2018.\\n\\nXi Liu, Rui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao, Mingkun Yang, Xiang Bai, Baoguang Shi, Dimosthenis Karatzas, Shijian Lu, and C. V. Jawahar. ICDAR 2019 robust reading challenge on reading Chinese text on signboard. 2019.\\n\\nYinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. RoBERTa: A robustly optimized BERT pretraining approach. arXiv preprint arXiv:1907.11692, 2019.\\n\\nPengyuan Lyu, Chengquan Zhang, Shanshan Liu, Meina Qiao, Yangliu Xu, Liang Wu, Kun Yao, Junyu Han, Er-rui Ding, and Jingdong Wang. MaskOCR: Text recognition with masked encoder-decoder pretraining. arXiv preprint arXiv:2206.00311, 2022.\\n\\nAnand Mishra, Karteek Alahari, and CV Jawahar. Scene text recognition using higher order language priors. BMVC-British machine vision conference, 2012.\\n\\nByeonghu Na, Yoonsik Kim, and Sungrae Park. Multi-modal text recognition networks: Interactive enhancements between visual and semantic features. 2022.\\n\\nNibal Nayef, Yash Patel, Michal Busta, Pinaki Nath Choudhury, Dimosthenis Karatzas, Wafa Khlif, Jiri Matas, Umapada Pal, Jean-Christophe Burie, Cheng Lin Liu, and Jean-Marc Ogier. ICDAR2019 robust reading challenge on multilingual scene text detection and recognition \u2013 RRC-MLT-2019. 2019.\\n\\nOpenAI. ChatGPT. https://openai.com/blog/chatgpt/, 2023.\\n\\nOpenAI. GPT-4 technical report. 2023.\\n\\nTrung Quy Phan, Palaiahnakote Shivakumara, Shangxuan Tian, and Chew Lim Tan. Recognizing text with perspective distortion in natural scenes. pages 569\u2013576, 2013.\\n\\nZhi Qiao, Yu Zhou, Dongbao Yang, Yucan Zhou, and Weiping Wang. Seed: Semantics enhanced encoder-decoder framework for scene text recognition. 2020.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. The Journal of Machine Learning Research, 21(1):5485\u20135551, 2020.\\n\\nZobeir Raisi, Mohamed A Naiel, Georges Younes, Steven Wardell, and John S Zelek. Transformer-based text detection in the wild. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3162\u20133171, 2021.\\n\\nBenjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do ImageNet classifiers generalize to ImageNet? In International conference on machine learning, pages 5389\u20135400. PMLR, 2019.\\n\\nAnhar Risnumawan, Palaiahnakote Shivakumara, Chee Seng Chan, and Chew Lim Tan. A robust arbitrary text detection system for natural scene images. Expert Systems with Applications, 41(18):8027\u20138048, 2014.\\n\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. ImageNet large scale visual recognition challenge. International journal of computer vision, 115:211\u2013252, 2015.\\n\\nRico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units. 2016.\\n\\nFenfen Sheng, Zhineng Chen, and Bo Xu. NrTR: A no-recurrence sequence-to-sequence model for scene text recognition. In 2019 International conference on document analysis and recognition (ICDAR), pages 781\u2013786. IEEE, 2019.\\n\\nBaoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition. IEEE transactions on pattern analysis and machine intelligence, 39(11):2298\u20132304, 2016.\\n\\nBaoguang Shi, Cong Yao, Minghui Liao, Mingkun Yang, Pei Xu, Linyan Cui, Serge Belongie, Shijian Lu, and Xiang Bai. ICDAR2017 competition on reading Chinese text in the wild (RCTW-17). 2018.\\n\\nKaren Simonyan and Andrew Zisserman. Very deep convolutional networks for large-scale image recognition. arXiv preprint arXiv:1409.1556, 2014.\"}"}
{"id": "CVPR-2024-152", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Amanpreet Singh, Guan Pang, Mandy Toh, Jing Huang, Wojciech Galuba, and Tal Hassner. Textocr: Towards large-scale end-to-end reasoning for arbitrary-shaped scene text. 2021.\\n\\nYipeng Sun, Jiaming Liu, Wei Liu, Junyu Han, Errui Ding, and Jingtuo Liu. Chinese street view text: Large-scale Chinese text reading with partially supervised learning. 2020.\\n\\nYew Lee Tan, Adams Wai-Kin Kong, and Jung-Jae Kim. Pure transformer with integrated experts for scene text recognition. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXVIII, pages 481\u2013497. Springer, 2022.\\n\\nHugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. pages 10347\u201310357, 2021.\\n\\nAndreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. 2016.\\n\\nZhaoyi Wan, Minghang He, Haoran Chen, Xiang Bai, and Cong Yao. Textscanner: Reading characters in order for robust scene text recognition. CoRR, abs/1912.12422, 2019.\\n\\nKai Wang, Boris Babenko, and Serge Belongie. End-to-end scene text recognition. pages 1457\u20131464, 2011.\\n\\nQiang Wang, Bei Li, Tong Xiao, Jingbo Zhu, Changliang Li, Derek F Wong, and Lidia S Chao. Learning deep transformer models for machine translation. arXiv preprint arXiv:1906.01787, 2019.\\n\\nWenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. Minilm: Deep self-attention distillation for task-agnostic compression of pre-trained transformers. Advances in Neural Information Processing Systems, 33:5776\u20135788, 2020.\\n\\nYuxin Wang, Hongtao Xie, Shancheng Fang, Jing Wang, Shenggao Zhu, and Yongdong Zhang. From two to one: A new scene text recognizer with visual language modeling network. 2021.\\n\\nYuxin Wang, Hongtao Xie, Shancheng Fang, Mengting Xing, Jing Wang, Shenggao Zhu, and Yongdong Zhang. Petr: Rethinking the capability of transformer-based language model in scene text recognition. IEEE Transactions on Image Processing, 31:5585\u20135598, 2022.\\n\\nI. Zharkov P. Zhang K. Seifert Y. Zhang, L. Gueguen and B. Kadlec. Uber-text: A large-scale dataset for optical character recognition from street-level imagery. Scene Understanding Workshop-CVPR, 2017.\\n\\nMingkun Yang, Minghui Liao, Pu Lu, Jing Wang, Shenggao Zhu, Hualin Luo, Qi Tian, and Xiang Bai. Reading and writing: Discriminative and generative modeling for self-supervised text recognition. 2023.\\n\\nDeli Yu, Xuan Li, Chengquan Zhang, Junyu Han, Jingtuo Liu, and Errui Ding. Towards accurate scene text recognition with semantic reasoning networks. CoRR, abs/2003.12294, 2020.\\n\\nLiu Yuliang, Jin Lianwen, Zhang Shuaitao, and Zhang Sheng. Detecting curve text in the wild: New dataset and new solution. arXiv preprint arXiv:1712.02170, 2017.\\n\\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucias Beyer. Scaling vision transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12104\u201312113, 2022.\\n\\nShuai Zhao, Xiaohan Wang, Linchao Zhu, Ruijie Quan, and Yi Yang. Clip4str: A simple baseline for scene text recognition with pre-trained vision-language model. 2023.\"}"}
