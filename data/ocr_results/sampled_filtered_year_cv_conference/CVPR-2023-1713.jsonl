{"id": "CVPR-2023-1713", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Harsh Agrawal, Peter Anderson, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi Parikh, and Stefan Lee. nocaps: novel object captioning at scale. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 8947\u20138956. IEEE, 2019.\\n\\n[2] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Menisch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. Flamingo: a visual language model for few-shot learning. CoRR, abs/2204.14198, 2022.\\n\\n[3] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: visual question answering. In 2015 IEEE International Conference on Computer Vision, ICCV 2015, Santiago, Chile, December 7-13, 2015, pages 2425\u20132433. IEEE Computer Society, 2015.\\n\\n[4] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\\n\\n[5] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12M: Pushing web-scale image-text pre-training to recognize long-tail visual concepts. In CVPR, 2021.\\n\\n[6] Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedanta, Saurabh Gupta, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO captions: Data collection and evaluation server. ArXiv preprint, abs/1504.00325, 2015.\\n\\n[7] Xi Chen, Xiao Wang, Soravit Changpinyo, A. J. Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. Pali: A jointly-scaled multilingual language-image model. CoRR, abs/2209.06794, 2022.\\n\\n[8] Felix Chern, Blake Hechtman, Andy Davis, Ruiqi Guo, David Majnemer, and Sanjiv Kumar. TPU-KNN: K nearest neighbor search at peak flop/s. CoRR, abs/2206.14286, 2022.\\n\\n[9] Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways. CoRR, abs/2204.02311, 2022.\\n\\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\\n\\n[11] Fran\u00e7ois Gard\u00e8res, Maryam Ziaeefard, Baptiste Abeloos, and Freddy Lecue. ConceptBert: Concept-aware representation for visual question answering. In Findings of the Association for Computational Linguistics: EMNLP 2020, pages 489\u2013498, Online, 2020. Association for Computational Linguistics.\\n\\n[12] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in visual question answering. In 2017 IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2017, Honolulu, HI, USA, July 21-26, 2017, pages 6325\u20136334. IEEE Computer Society, 2017.\\n\\n[13] Liangke Gui, Borui Wang, Qiuyuan Huang, Alexander Hauptmann, Yonatan Bisk, and Jianfeng Gao. KAT: A knowledge augmented transformer for vision-and-language. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 956\u2013968, Seattle, United States, 2022. Association for Computational Linguistics.\\n\\n[14] Kelvin Guu, Kenton Lee, Zora Tung, Panupong Pasupat, and Ming-Wei Chang. REALM: retrieval-augmented language model pre-training. ArXiv preprint, abs/2002.08909, 2020.\\n\\n[15] Ziniu Hu, Yichong Xu, Wenhao Yu, Shuohang Wang, Ziyi Yang, Chenguang Zhu, Kai-Wei Chang, and Yizhou Sun. Empowering language models with knowledge graph reasoning for open-domain question answering. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, 2022. Association for Computational Linguistics.\"}"}
{"id": "CVPR-2023-1713", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[16] Ziniu Hu, Zhe Zhao, Xinyang Yi, Tiansheng Yao, Lichan Hong, Yizhou Sun, and Ed H. Chi. Improving multi-task generalization via regularizing spurious correlation. CoRR, abs/2205.09797, 2022.\\n\\n[17] Gautier Izacard and Edouard Grave. Distilling knowledge from reader to retriever for question answering. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021.\\n\\n[18] Gautier Izacard, Patrick Lewis, Maria Lomeli, Lucas Hosseini, Fabio Petroni, Timo Schick, Jane Dwivedi-Yu, Armand Joulin, Sebastian Riedel, and Edouard Grave. Few-shot learning with retrieval augmented language models. CoRR, abs/2208.03299, 2022.\\n\\n[19] Andrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Jo\u00e3o Carreira. Perceiver: General perception with iterative attention. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 4651\u20134664. PMLR, 2021.\\n\\n[20] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In Proceedings of the 38th International Conference on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, volume 139 of Proceedings of Machine Learning Research, pages 4904\u20134916. PMLR, 2021.\\n\\n[21] Amita Kamath, Christopher Clark, Tanmay Gupta, Eric Kolve, Derek Hoiem, and Aniruddha Kembhavi. Webly supervised concept expansion for general purpose vision models. In Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXXVI, volume 13696 of Lecture Notes in Computer Science, pages 662\u2013681. Springer, 2022.\\n\\n[22] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wen-tau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 6769\u20136781, Online, 2020. Association for Computational Linguistics.\\n\\n[23] Patrick S. H. Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K\u00fcttler, Mike Lewis, Wen-tau Yih, Tim Rockt\u00e4schel, Sebastian Riedel, and Douwe Kiela. Retrieval-augmented generation for knowledge-intensive NLP tasks. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020.\\n\\n[24] Yuanze Lin, Yujia Xie, Dongdong Chen, Yichong Xu, Chenguang Zhu, and Lu Yuan. REVIVE: regional visual representation matters in knowledge-based visual question answering. CoRR, abs/2206.01201, 2022.\\n\\n[25] Alexander Long, Wei Yin, Thalaiyasingam Ajanthan, Vu Nguyen, Pulak Purkait, Ravi Garg, Alan Blair, Chunhua Shen, and Anton van den Hengel. Retrieval augmented classification for long-tail visual recognition. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 6949\u20136959. IEEE, 2022.\\n\\n[26] Jiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 13\u201323, 2019.\\n\\n[27] Man Luo, Yankai Zeng, Pratyay Banerjee, and Chitta Baral. Weakly-supervised visual-retriever-reader for knowledge-based question answering. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6417\u20136431, Online and Punta Cana, Dominican Republic, 2021. Association for Computational Linguistics.\\n\\n[28] Kenneth Marino, Xinlei Chen, Devi Parikh, Abhinav Gupta, and Marcus Rohrbach. KRISP: integrating implicit and symbolic knowledge for open-domain knowledge-based VQA. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 14111\u201314121. Computer Vision Foundation / IEEE, 2021.\\n\\n[29] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: A visual question answering benchmark requiring external knowledge. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 3195\u20133204. Computer Vision Foundation / IEEE, 2019.\\n\\n[30] Ron Mokady, Amir Hertz, and Amit H. Bermano. CLIP-cap: CLIP prefix for image captioning. ArXiv preprint, abs/2111.09734, 2021.\\n\\n[31] Medhini Narasimhan, Svetlana Lazebnik, and Alexander G. Schwing. Out of the box: Reasoning with graph convolution nets for factual visual question answering. In Advances in Neural Information Processing Systems 31: Annual Conference on Neural Information Processing Systems 2018, NeurIPS 2018, December 3-8, 2018, Montr\u00e9al, Canada, pages 2659\u20132670, 2018.\\n\\n[32] Barlas Oguz, Xilun Chen, Vladimir Karpukhin, Stan Pesterliev, Dmytro Okhonko, Michael Sejr Schlichtkrull, Sonal Gupta, Yashar Mehdad, and Scott Yih. Unified open-domain question answering with structured and unstructured knowledge. ArXiv preprint, abs/2012.14610, 2020.\\n\\n[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1\u2013140:67, 2020.\\n\\n[34] Devendra Singh Sachan, Siva Reddy, William L. Hamilton, Chris Dyer, and Dani Yogatama. End-to-end training of multi-document reader and retriever for open-domain question answering. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 25968\u201325981, 2021.\"}"}
{"id": "CVPR-2023-1713", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-OKVQA: A benchmark for visual question answering using world knowledge. CoRR, abs/2206.01718, 2022.\\n\\nAnshumali Shrivastava and Ping Li. Asymmetric LSH (ALSH) for sublinear time maximum inner product search (MIPS). In Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems 2014, December 8-13 2014, Montreal, Quebec, Canada, pages 2321\u20132329, 2014.\\n\\nKrishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. WIT: wikipedia-based image text dataset for multimodal multilingual machine learning. In SIGIR '21: The 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, Virtual Event, Canada, July 11-15, 2021, pages 2443\u20132449. ACM, 2021.\\n\\nHao Tan and Mohit Bansal. LXMERT: Learning cross-modality encoder representations from transformers. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 5100\u20135111, Hong Kong, China, 2019. Association for Computational Linguistics.\\n\\nRamakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. CIDEr: Consensus-based image description evaluation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2015, Boston, MA, USA, June 7-12, 2015, pages 4566\u20134575. IEEE Computer Society, 2015.\\n\\nDenny Vrandecic and Markus Kr\u00f6tzsch. Wikidata: a free collaborative knowledgebase. Commun. ACM, 57(10):78\u201385, 2014.\\n\\nPeng Wang, Qi Wu, Chunhua Shen, Anthony R. Dick, and Anton van den Hengel. Explicit knowledge-based reasoning for visual question answering. In Proceedings of the Twenty-Sixth International Joint Conference on Artificial Intelligence, IJCAI 2017, Melbourne, Australia, August 19-25, 2017, pages 1290\u20131296. ijcai.org, 2017.\\n\\nPeng Wang, Qi Wu, Chunhua Shen, Anthony R. Dick, and Anton van den Hengel. FVQA: fact-based visual question answering. IEEE Trans. Pattern Anal. Mach. Intell., 40(10):2413\u20132427, 2018.\\n\\nWenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a foreign language: Beit pretraining for all vision and vision-language tasks. CoRR, abs/2208.10442, 2022.\\n\\nYanan Wang, Michihiro Yasunaga, Hongyu Ren, Shinya Wada, and Jure Leskovec. VQA-GNN: reasoning with multi-modal semantic graph for visual question answering. CoRR, abs/2205.11501, 2022.\\n\\nZirui Wang, Jiahui Yu, Adams Wei Yu, Zihang Dai, Yulia Tsvetkov, and Yuan Cao. Simvlm: Simple visual language model pretraining with weak supervision. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022.\\n\\nJialin Wu, Jiasen Lu, Ashish Sabharwal, and Roozbeh Mottaghi. Multi-modal answer validation for knowledge-based VQA. In Thirty-Sixth AAAI Conference on Artificial Intelligence, AAAI 2022, Thirty-Fourth Conference on Innovative Applications of Artificial Intelligence, IAAI 2022, The Twelveth Symposium on Educational Advances in Artificial Intelligence, EAAI 2022 Virtual Event, February 22 - March 1, 2022, pages 2712\u20132721. AAAI Press, 2022.\\n\\nWei Yang, Yuqing Xie, Aileen Lin, Xingyu Li, Luchen Tan, Kun Xiong, Ming Li, and Jimmy Lin. End-to-end open-domain question answering with BERTserini. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics (Demonstrations), pages 72\u201377, Minneapolis, Minnesota, 2019. Association for Computational Linguistics.\\n\\nZhengyuan Yang, Zhe Gan, Jianfeng Wang, Xiaowei Hu, Yumao Lu, Zicheng Liu, and Lijuan Wang. An empirical study of GPT-3 for few-shot knowledge-based VQA. ArXiv preprint, abs/2109.05014, 2021.\\n\\nJiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mojtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive captioners are image-text foundation models. CoRR, abs/2205.01917, 2022.\\n\\nXiaohua Zhai, Alexander Kolesnikov, Neil Houlsby, and Lucas Beyer. Scaling vision transformers. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1204\u20131213. IEEE, 2022.\\n\\nXiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 18102\u201318112. IEEE, 2022.\\n\\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 5579\u20135588. Computer Vision Foundation / IEEE, 2021.\"}"}
{"id": "CVPR-2023-1713", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Statistics of the knowledge sources used.\\n\\n| Knowledge Source | Corpus Size | Type of Text | Avg. Text Length |\\n|------------------|-------------|--------------|-----------------|\\n| WIT [37]         | 5,233,186   | Wikipedia Passage | 258             |\\n| CC12M [5]        | 10,009,901  | Alt-Text Caption | 37              |\\n| VQA-V2 [12]      | 123,287     | Question Answer | 111             |\\n| WikiData [40]    | 4,947,397   | Linearized Triplets | 326             |\\n\\nTable 2. Model configuration of different REVEL variants.\\n\\n| Model Name | T5 Variant | Image Encoder | # params. | GFLOPs |\\n|------------|------------|---------------|-----------|--------|\\n| REV EA     | L-Base     | T5-Base ViT-B/16 | 0.4B | 120    |\\n| REVEL      | L-Large    | T5-Large ViT-L/16 | 1.4B | 528    |\\n| REVEL      | L           | T5-Large ViT-g/14 | 2.1B | 795    |\\n\\nCross-knowledge attention. The detailed procedure is illustrated in Figure 3. We firstly compute a latent soft attention mask over \\\\(X\\\\) as \\\\(\\\\text{Mask}_{\\\\text{att}} = [1, p(z_1|x), \\\\ldots, p(z_K|x)]\\\\).\\n\\nFinally, we pass the fused representation \\\\(f(X, \\\\text{Mask}_{\\\\text{att}})\\\\) into a T5 decoder module \\\\(g(\u00b7)\\\\) to generate the textual output.\\n\\n4. Generative Pre-Training\\n\\nThe existing VQA datasets are not large enough for training a complex multi-component model like ours from scratch. Therefore, we pre-train our model on a massive image-text corpus. In Sec. 4.1 we go over the details of our pre-training data and objective. Then in Sec. 4.2 we introduce the various sources of knowledge used in our experiments. Finally, in Sec. 4.3 we describe the pre-training implementation details.\\n\\n4.1. Pre-Training Objective\\n\\nWe pre-train our model on the Web-Image-Text dataset [51], a large-scale corpus containing 3 billion image-alt-text caption pairs collected from the public Web. Since the dataset is noisy, we add a filter to remove data points whose captions are shorter than 50 characters. This yields roughly 1.3 billion image caption pairs for pre-training.\\n\\nWe denote the pre-training Web-Image-Text dataset [51] as \\\\(D\\\\). We use the text generation objective used in Wang et al. [45]) to pre-train our model on \\\\(D\\\\). Given an image-text example \\\\(x = (img, txt)\\\\) from \\\\(D\\\\), we randomly sample a prefix length \\\\(T_p\\\\). We feed \\\\(x_{<T_p}\\\\) that contains the text prefix and image to the model as input and our objective is to generate \\\\(x_{\\\\geq T_p}\\\\) containing the rest of the text as output. The training goal is to condition on \\\\(x_{<T_p}\\\\) and autoregressively generate the remaining text sequence \\\\(x_{\\\\geq T_p}\\\\):\\n\\n\\\\[\\nL_{\\\\text{PrefixLM}} = -\\\\mathbb{E}_{x \\\\sim D} \\\\log p(x_{\\\\geq T_p} | x_{<T_p})\\n\\\\]\\n\\n\\\\[\\n= -\\\\mathbb{E}_{x \\\\sim D} \\\\sum_{i \\\\geq T_p} \\\\log p(x_i | x_{<i})\\n\\\\]\\n\\nWarm Starting the Model\\n\\nIn order to pre-train all components of our model end-to-end, we need to warm start the retriever at a good state. Otherwise, if starting with random weights, the retriever would often return irrelevant memory items that would never generate useful training signals. To avoid this cold-start problem, we propose to construct an initial retrieval dataset with pseudo ground-truth knowledge to give the pre-training a reasonable head start. We create a modified version of the Wikipedia-Image-Text (WIT) [37] dataset for this purpose. Each image-caption pair in WIT also comes with a corresponding Wikipedia passage (words surrounding the text). We put together the surrounding passage with the query image and use it as the pseudo ground-truth knowledge that corresponds to the input query. As the passage provides rich information about the image and caption, it definitely is useful for initializing the model. To avoid the model from relying on low-level image features for retrieval, we apply random data augmentation to the input query image. Given this modified dataset that contains pseudo retrieval ground-truth, we train the query and memory key embeddings by optimizing the following contrastive loss:\\n\\n\\\\[\\nL_{\\\\text{contra}} = -\\\\log \\\\text{Softmax}(\\\\text{Emb}_{\\\\text{Query}}(x) \\\\cdot \\\\text{Emb}_{\\\\text{Key}}(\\\\hat{z}))\\n\\\\]\\n\\nwhere \\\\(\\\\hat{z}\\\\) represents the pseudo ground-truth knowledge entry corresponding to the input query \\\\(x\\\\).\\n\\n4.2. Knowledge Sources\\n\\nWe use the following four sources of knowledge in our experiments:\\n\\n- **Wikipedia-Image-Text (WIT) [37]** consists of the images in Wikipedia, as well as their alt-text captions and contextualized text passages.\\n- **Conceptual (CC12M) [5]** contains web images paired with alt-text captions. It includes many long-tail entities.\\n- **VQA-v2 [12]** is a visual question answering dataset. We merge all question-answer pairs per image into a single passage.\\n- **WikiData [40]** is a structural knowledge graph encoding relations between Wikipedia entities. We linearize all relational triplets per entity into a textual passage following the procedure of [32]. We have listed the statistical details of these knowledge sources in Table 1.\\n\\n4.3. Implementation Details\\n\\nIncorporating all the components introduced above, REVEL can be directly pre-trained over large-scale image caption datasets after proper initialization. As our model architecture is based on T5 and ViT, we use pre-trained ViT checkpoints from [50] and pre-trained T5 checkpoints from [33] to initialize the encoder parameters. The query head, key head and attentive fusion layers are initialized from upper T5, while the base text encoder is initialized from lower T5. The combination of these modules can be found in Table 2 for three model variants, R\\\\text{E}V\\\\text{E}L-Base, R\\\\text{E}V\\\\text{E}L-Large and R\\\\text{E}V\\\\text{E}L, of which the largest R\\\\text{E}V\\\\text{E}L model has around 2 billion parameters.\"}"}
{"id": "CVPR-2023-1713", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"## Experimental Results\\n\\nWe evaluate our proposed method on knowledge-based VQA in Sec. 5.1 and image captioning in Sec. 5.2. We then conduct ablation studies in Sec. 5.3 to analyze the impact of each model component on overall performance.\\n\\n### Table 3.\\n\\n| VQA Model Name | Knowledge Sources | Accuracy (%) |\\n|----------------|-------------------|--------------|\\n| MUTAN+AN [29]  | Wikipedia + ConceptNet | 27.8          |\\n| ConceptBERT [11] | Wikipedia       | 33.7          |\\n| KRISP [28]     | Wikipedia + ConceptNet | 38.4          |\\n| Visual Retriever-Reader [27] | Google Search     | 39.2          |\\n| MAVEx [46]     | Wikipedia+ConceptNet+Google Images | 39.4          |\\n| KAT-Explicit [13] | Wikidata       | 44.3          |\\n| PICa-Base [48] | Frozen GPT-3  | 43.3          |\\n| PICa-Full [48] | Frozen GPT-3  | 48.0          |\\n| KAT [13] (Single) | Wikidata + Frozen GPT-3 | 53.1          |\\n| KAT [13] (Ensemble) | Wikidata + Frozen GPT-3 | 54.4          |\\n| ReVIVE [24] (Single) | Wikidata + Frozen GPT-3 | 56.6          |\\n| ReVIVE [24] (Ensemble) | Wikidata + Frozen GPT-3 | 58.0          |\\n| REV-EAE-L-Base | WIT + CC12M + Wikidata + VQA-2 | 55.2          |\\n| REV-EAE-L-Large | WIT + CC12M + Wikidata + VQA-2 | 58.0          |\\n| REV-EAE-L | WIT + CC12M + Wikidata + VQA-2 | 59.1          |\\n\\n**Distributed Online Retrieval.** Finding the top-k most-relevant knowledge entries is a standard Maximum Inner Product Search (MIPS) problem. There are approximate search algorithms [8, 36] that scale sub-linearly with the size of the knowledge corpus $|C|$. We use TPU-KNN [8] to conduct distributed MIPS search, by splitting and storing the memory embeddings across all training devices. The query is synced to each device, which retrieves approximate top-K results from its own memory. Then these results are combined to compute the global top-K retrieved items.\\n\\n**Pre-Training Pipeline.** We first train the multimodal retriever on our modified version of the Wikipedia Image Text (WIT) dataset via L*contra*. We use the Adafactor optimizer without momentum ($\\\\beta_1 = 0$, $\\\\beta_2 = 0.999$), with weight decay of 0.0012, and with a peak learning rate of $6 \\\\times 10^4$, to train for 10 epochs. We use this checkpoint to warm-start our generative pre-training. We set the number of retrieved knowledge entries as $K = 10$ during pre-training, and use adafactor with a peak learning rate of $1 \\\\times 10^{-3}$ and inverse squared root learning rate scheduler with 10,000 linear warm-up steps. We use $L_{PrefixLM}$ as the main objective, adding $L_{contra}$, $L_{decor}$ and $L_{align}$ weighted by 0.01. We use a batch size of 4096 across 256 CloudTPUv4 chips and train for about 5 days.\\n\\n### Table 4.\\n\\n| VQA Model Name | Knowledge Sources | Accuracy (%) |\\n|----------------|-------------------|--------------|\\n| ViLBERT [26]  |                  | 30.6          |\\n| LXMERT [38]   |                  | 30.7          |\\n| ClipCap [30]  |                  | 30.9          |\\n| KRISP [28]    |                  | 33.7          |\\n| GPV-2 [21]    |                  | 48.6          |\\n| REV-EAE-L-Base |                  | 50.4          |\\n| REV-EAE-L-Large |                | 51.5          |\\n| REV-EAE-L     |                  | 52.2          |\\n\\n## 5.1. Evaluating on Knowledge-Based VQA\\n\\nOne of the most knowledge intensive visual-language tasks is knowledge-based visual question answering (VQA), exemplified by the OK-VQA [29] and A-OKVQA [35] benchmarks. To finetune our pre-trained model on these VQA tasks, we use the same generative objective where the model takes in an image question pair as input and generates the text answer as output. There are a few differences between the fine-tuning and the pre-training stages: 1) we set the number of retrieved knowledge entries to $K = 50$, so the model is able to retrieve sufficient supporting evidence; 2) we freeze the whole base V-L encoder to stabilize training; and 3) we use a batch size of 128, with the Adafactor optimizer, a peak learning rate of 1e-4. We use the soft VQA accuracy metric [3] to evaluate the model\u2019s generated answer.\\n\\nOur results on OKVQA and A-OKVQA datasets are shown in Table 3 and Table 4 respectively. For OKVQA, earlier attempts that incorporate a fixed knowledge retriever report results that are below 45%. Recently a series of works utilize large language models (e.g., GPT-3) as implicit knowledge sources, which achieve much better performance.\"}"}
{"id": "CVPR-2023-1713", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. Image Captioning results on MSCOCO (Karpathy-test split) and NoCaps (val set). Evaluated using the CIDEr metric.\\n\\n| Model         | # params. | CIDEr | Val set | Val set |\\n|---------------|-----------|-------|---------|---------|\\n| Flamingo [2]  |           |       |         |         |\\n| VinVL [52]    | 138.1 -   | 80B   |         |         |\\n| SimVLM [45]   | 140.9 105.1 | 0.4B  |         |         |\\n| CoCa [49]     | 143.3 112.2 | 1.5B  |         |         |\\n| REV-EL-Base   | 141.1 115.8 | 0.4B  |         |         |\\n| REV-EL-Large  | 144.5 121.3 | 1.4B  |         |         |\\n| REV-EL       | 145.4 123.0 | 2.1B  |         |         |\\n\\nTable 5.\\n\\nWith the trade-off of a huge computational cost, REV-EL achieves higher performance than those methods without relying on such large language models. Compared with the previous state-of-the-art, KAT and Revive, which also utilizes T5-Large as a generator, REV-EL achieves accuracy of 59.1%, which is +6.0% higher than the single KAT [13] model and +2.5% higher than Revive [24].\\n\\nOn A-OKVQA, REV-EL achieves 52.2% accuracy, which is +3.6% higher than the previous best, GPV-2 [21].\\n\\nWe also show two examples of these datasets in Figure 4. All these results show that, with proper end-to-end retrieval training and a diverse set of knowledge sources, REV-EL can learn to retrieve meaningful knowledge entries, and achieve promising results without relying on a large language model.\\n\\n5.2. Evaluating on Image Captioning\\n\\nWe also evaluate REV-EL on image captioning benchmarks: MSCOCO Captions [6] and NoCaps [1]. We follow the evaluation protocol used in [49]. We directly fine-tune our generator model on the MSCOCO training split via cross-entropy generative objective. We measure our performance on the MSCOCO test split and NoCaps val set with the CIDEr metric [39]. The results of these two datasets are shown in Table 5. Note that REV-EL achieves better results than strong recent baselines such as SimVLM [45] and CoCa [49] on both benchmarks. Notably, REV-EL-Large with 1.4B parameters outperforms the 2.1B-parameter CoCa model and is significantly better than 80B-parameter Flamingo model [2].\\n\\n5.3. Analyzing Effects of Key Model Components\\n\\nIn the following we study which design choices contribute most to the model\u2019s performance. We focus on three research questions: (1) Does utilizing multiple knowledge sources enhance performance? (2) Does the proposed attentive fusion surpass existing end-to-end retrieval training methods?\\n\\nAs shown in the last column of Table 3, REV-EL stores external knowledge as value embeddings on disk, occupying 993GB of space. The key embeddings consume 10GB space and are kept in TPU memory for fast lookup. On the other hand, KAT and REVIVE need to load the entire 350GB GPT-3 model in the GPU/TPU memory. Furthermore, storing WikiData on disk consumes 500GB of disk memory.\\n\\nFigure 4.\\n\\nVQA Examples. REV-EL is able to use knowledge from different sources to correctly answer the question. We show more examples in Figure 1-3 of Supplementary Material, indicating that our model can retrieve and use items from diverse knowledge sources to correctly solve different input query.\\n\\n(3) Can we add knowledge by only updating the memory without modifying model parameters?\\n\\nAnalyzing multiple knowledge sources. A major distinction of REV-EL compared to previous retrieval-augmented approaches is its capacity to utilize a diverse set of knowledge sources during inference. To assess the relative importance of each data source and the efficacy of retrieving from various corpora, we conduct two ablation studies: 1) Only-One-Left: employing a single knowledge source to evaluate the outcomes; and 2) Leave-One-Out: excluding one knowledge source from the complete set C.\\n\\nThese ablation studies are executed using the REV-EL Base, evaluated on the OKVQA validation set under the aforementioned conditions. As shown in Figure 5, among the four knowledge sources utilized in this paper, WIT is the most informative, with the highest accuracy when used in isolation (53.1). The remaining three corpora, CC12M, VQA-v2, and WikiData, do not offer the same level of informativeness as WIT when utilized independently. However, excluding any of these corpora from the complete dataset results in performance decreases of 1.3%, 0.6%, and 1.1%, respectively. This observation implies that these knowledge sources effectively complement one another, contributing valuable information to enhance performance. To further substantiate this hypothesis, we perform an additional experiment involving pairs of knowledge sources, as illustrated in Figure 6. Notably, even when paired with an informative knowledge source such as WIT, incorporating an extra corpus consistently leads to performance improvements.\\n\\nAnalyzing different retrieval training methods. Another core component of REV-EL is the attentive fusion layer, which supports efficient joint training of the retriever and generator. We investigate its performance compared to two existing retrieval training method categories: 1) a frozen retriever based on ALIGN [20] representations; 2) end-to-end retrieval training methods including Attention Distill [17], EMDR [47], and Perplexity Distill [18].\"}"}
{"id": "CVPR-2023-1713", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Figure 5. OKVQA Accuracy of R\\\\textsubscript{E}V\\\\textsubscript{E}A\\\\textsubscript{L} using 1) Only-One-Left: only use a single knowledge source; 2) Leave-One-Out: use all without this knowledge source.\\n\\nFigure 6. OKVQA Accuracy of R\\\\textsubscript{E}V\\\\textsubscript{E}A\\\\textsubscript{L} using all Pair of Knowledge Sources. Results show that combining multiple sources could consistently improve performance.\\n\\n| Retrieval Method     | Acc@10  | Acc@100 | GFLOPs |\\n|----------------------|---------|---------|--------|\\n| ALIGN [20] (fixed)   | 0.638   | 0.793   | 44.7   |\\n| Attention Distill [17]| 0.674   | 0.835   | 45.9   | 119    |\\n| EMDR\\\\textsuperscript{2} [47]| 0.691   | 0.869   | 46.5   | 561    |\\n| Perplexity Distill [18]| 0.704   | 0.886   | 46.7   | 561    |\\n| Ours (Attentive Fusion)| 0.726   | 0.894   | 47.3   | 120    |\\n\\nTable 6. Analysis of Retrieval Training Method: We train R\\\\textsubscript{E}V\\\\textsubscript{E}A\\\\textsubscript{L}-Base (frozen generator, only train randomly initialized retriever) to retrieve from the WIT dataset (only text passage without image), and show the retrieval accuracy at the first 10 or 100 results, as well as fine-tuned OKVQA accuracy.\\n\\nWe use the pre-trained R\\\\textsubscript{E}V\\\\textsubscript{E}A\\\\textsubscript{L}-Base model, fix the generator and randomly initialize the retriever (query head and key head). We utilize our modified version of WIT dataset with pseudo ground-truth retrieved labels as the evaluation corpus. We evaluate retrieval performance by checking whether the correct passage appears in top-10/100 results. For the ALIGN model, we directly evaluate the retrieval results from the pre-trained checkpoint, while for other models, we perform retrieval-augmented training on the WIT dataset. To prevent the model from relying on image similarity for accurate results, we only use text passages as knowledge entries and discard images. Subsequently, we finetune the model on OKVQA and report its accuracy. The results are presented in Table 6. We observe that directly using pre-trained encoder does not perform well, even with a strong model like ALIGN. Moreover, among the various end-to-end retrieval training approaches, our attentive fusion method attains better accuracy in both retrieval and OKVQA tasks.\\n\\nImportantly, our technique exhibits a computational cost (quantified by GFLOPs) comparable to that of attention distillation, yet significantly lower than EMDR\\\\textsuperscript{2} and Perplexity distillation. This indicates that our proposed method is more efficient and effective for pre-training retrieval-augmented visual-language models.\\n\\nAnalyzing Knowledge Modification. One advantage of utilizing knowledge memory is that we could easily add or update knowledge entries without re-training model's parameters. To validate this, we conducted ablation studies in which we removed a specific percentage of knowledge entries from the corpora and assessed the performance of the R\\\\textsubscript{E}V\\\\textsubscript{E}A\\\\textsubscript{L}-Base model on the OKVQA dataset. Subsequently, we add the removed knowledge back into the corpora, allowing the trained model to make predictions using the complete set of corpora. This approach ensured that the removed knowledge was not seen by the model during fine-tuning, enabling us to test its ability to accurately retrieve and utilize that knowledge for problem-solving.\\n\\nThe results are illustrated in Figure 7, with the blue curves representing the inference outcomes without the removed knowledge and the orange curve depicting the results after adding the removed knowledge back. A notable performance improvement was observed upon reintroducing the knowledge (orange curve) compared to the outcomes with the removed knowledge (blue curve). Specifically, for the model fine-tuned with only 10% of the knowledge, the reintroduction of the removed knowledge resulted in an accuracy of 51.8 (+6.7 higher than when removed). This finding demonstrates that the R\\\\textsubscript{E}V\\\\textsubscript{E}A\\\\textsubscript{L} model can swiftly adapt to new knowledge by merely updating the memory, obviating the need for re-training model parameters.\\n\\n6. Conclusion\\nThis paper presents an end-to-end Retrieval-augmented Visual Language model (R\\\\textsubscript{E}V\\\\textsubscript{E}A\\\\textsubscript{L}), which contains a knowledge retriever that learns to utilize a diverse set of knowledge sources with different modality. The retriever is trained jointly with the generator to return multiple knowledge entries. We pre-train R\\\\textsubscript{E}V\\\\textsubscript{E}A\\\\textsubscript{L} on a massive image-text corpus with four diverse knowledge corpora, and achieves state-of-the-art results on knowledge-intensive visual question answering and image caption tasks. In the future we'd explore the ability of this model to be used for attribution, and applying it to broader class of multimodal tasks.\"}"}
{"id": "CVPR-2023-1713", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"REVIEW: Retrieval-Augmented Visual-Language Pre-Training with Multi-Source Multimodal Knowledge Memory\\n\\nZiniu Hu1*, Ahmet Iscen2, Chen Sun2, Zirui Wang2, Kai-Wei Chang1, Yizhou Sun1, Cordelia Schmid2, David A. Ross2, Alireza Fathi2\\n\\n1University of California, Los Angeles, 2Google Research\\n\\nFigure 1. We augment a visual-language model with the ability to retrieve multiple knowledge entries from a diverse set of knowledge sources, which helps generation. Both retriever and generator are trained jointly, end-to-end, by optimizing a language modeling objective.\\n\\nAbstract\\n\\nIn this paper, we propose an end-to-end Retrieval-Augmented Visual Language Model (REVEL) that learns to encode world knowledge into a large-scale memory, and to retrieve from it to answer knowledge-intensive queries. REVEL consists of four key components: the memory, the encoder, the retriever and the generator. The large-scale memory encodes various sources of multimodal world knowledge (e.g., image-text pairs, question answering pairs, knowledge graph triplets, etc.) via a unified encoder. The retriever finds the most relevant knowledge entries in the memory, and the generator fuses the retrieved knowledge with the input query to produce the output. A key novelty in our approach is that the memory, encoder, retriever and generator are all pre-trained end-to-end on a massive amount of data. Furthermore, our approach can use a diverse set of multimodal knowledge sources, which is shown to result in significant gains. We show that REVEL achieves state-of-the-art results on visual question answering and image captioning. The project page of this work is reveal.github.io.\\n\\n1. Introduction\\n\\nRecent large-scale models such as T5 [33], GPT-3 [4], PaLM [9], CoCa [49], Flamingo [2], BEIT-3 [43] and PaLI [7] have demonstrated the ability to store substantial amounts of world knowledge, when scaled to tens of billions of parameters and trained on vast text and image corpora. These models achieve state-of-the-art results in downstream tasks such as image captioning, visual question answering and open vocabulary recognition. Yet, these models have a number of drawbacks: (i) they require massive scale, of parameters, data and computation, and (ii) they need to be re-trained every time the world knowledge is updated.\\n\\nTo address these issues, we adopt a different approach. Instead of statically compiling world knowledge into model weights, we transform the knowledge into a key-value memory through neural representation learning. Our model learns to utilize the memory for answering knowledge-intensive queries. By decoupling the knowledge memorization from reasoning, we enable our model to leverage various external sources of knowledge (e.g., Wikipedia passages and images [37], the WikiData knowledge graph [40], Web image-text pairs [5] and visual question answering data [12]). This enables the model parameters to focus on understanding the query and conducting reasoning, rather than being dedicated to memorization.\\n\\nRetrieval-augmented models have attracted a fair amount of attention in the fields of NLP [14, 18] and computer vision [13, 25]. Typically, these models often use a pre-existing single-modality backbone to encode and retrieve information from the knowledge corpus. Such approaches do not leverage all available modalities in the query and knowledge.\\n\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\"}"}
{"id": "CVPR-2023-1713", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"edge corpora, and hence they might not find the information that is most helpful for generating the model output. A key novelty in our approach is that we encode and store various sources of multimodal world knowledge into a unified memory, which the retriever can access via multimodal query encodings, to find the most relevant information from across complementary sources. Our multimodal memory and retriever are pre-trained end-to-end together with the rest of the model, on a massive amount of data and using diverse knowledge sources.\\n\\nA key challenge of pre-training the multimodal retriever end-to-end is the lack of direct supervision. There is no ground-truth indicating which knowledge entries are most helpful for answering knowledge-intensive queries. Some of the existing works in NLP [14, 23, 34] propose to acquire training signal by assessing the usefulness of each retrieved knowledge entry independently for helping language modelling. This approach is inefficient, as it involves estimating hundreds of retrieved knowledge entries independently, and also inaccurate as it discards the dependency between different knowledge entries in the retrieval set. In contrast, we propose to get this training signal while simultaneously considering multiple retrieved knowledge entries, by introducing an attentive fusion layer that injects retrieval score into the attention calculation procedure. This enables the retrieval module to be differentiable and jointly pre-trained with the rest of the model.\\n\\nIn summary, our key contributions are as follows:\\n\u2022 We are the first to propose an end-to-end pre-training paradigm that learns to index into a large-scale memory to solve knowledge-intensive visual-language tasks.\\n\u2022 Our method can construct a large-scale memory by encoding various sources of multimodal world knowledge, including Wikipedia passage, web images with alt-text captions, and knowledge graph triplets.\\n\u2022 R EVEL achieves state-of-the-art performance on several knowledge-intensive visual question answering and image captioning datasets. Notably on the OKVQA benchmark, R EVEL achieves a new state-of-the-art, 59.1% accuracy, while using order of magnitude fewer parameters than previous works.\\n\\n2. Related Work and Background\\nKnowledge-based Visual Question Answering.\\nTo evaluate a model's ability to comprehend multimodal world knowledge not easily inferred from input data, several knowledge-based Visual Question Answering (VQA) datasets have been introduced. KB-VQA [41] and FVQA [42] design questions that can be answered by retrieving relevant triplets from domain-specific structured knowledge graphs. OK-VQA [29] improves these datasets by necessitating the use of external knowledge, which goes beyond what can be directly observed in the input images. More recently, A-OKVQA [35] offers further improvements to OK-VQA by exclusively selecting questions that demand both external knowledge and commonsense reasoning about the image scenes. To tackle knowledge-based VQA tasks, many approaches have been proposed to incorporate external knowledge into visual-language models. One line of research uses explicit knowledge from structured knowledge graphs [11,15,31,44] or unstructured text corpora [27,28,46]. The key component for these works is the knowledge retriever. Some works [11, 28, 31, 46] utilize off-the-shelf vision detection models to generate image tags for knowledge retrieval, while others train the retrieval model via distant supervision [27] or auxiliary tasks (e.g. entity linking) [13]. Another research direction aims to incorporate implicit knowledge from pre-trained Large Language Models, such as GPT-3 [4] or PaLM [9]. These approaches utilize off-the-shelf image caption models to convert images into text, feed them into a language model, and use the generated text output as augmented knowledge [13, 24, 48]. Our work follows the first direction, augmenting a vision-language model with an explicit knowledge retriever. The main distinction is that we propose an end-to-end training framework to jointly learn the answer generator and retriever, rather than using a fixed or predefined knowledge retrieval.\\n\\nEnd-to-End Training of Retrieval-Augmented Models.\\nGiven the advantage of knowledge retrieval, a key question is how to get learning signal to train the retrieval model. For tasks with annotated retrieval ground-truth, retrieval training can be conducted via standard contrastive learning [22]. However, most tasks do not provide clear indications of which knowledge entries are relevant for generating answers. To this end, a series of studies have investigated retrieval training using supervision derived from downstream tasks. REALM [14] trains a single-document retriever by concatenating each retrieved result with the query, to calculate the final loss independently. A similar approach has been used by EMDR [34] for multi-document retrieval training. FID-KD [17] proposes to use the aggregated attention score calculated by the generator as a distillation signal to train the retriever. Atlas [18] further introduces a perplexity distillation loss and a leave-one-out variant. Our R EVEL proposes to inject the retrieval scores directly into an attentive fusion module, enabling to train the retriever to directly optimize downstream tasks as well as pre-training objectives.\\n\\n3. Method\\nWe propose a Retrieval-Augmented Visual Language Model (R EVEL), which learns to use knowledge from different sources for solving knowledge-intensive tasks. For both pre-training and fine-tuning, our goal is to learn the distribution $P(y|x)$ to generate a textual output $y$ conditioned on a multimodal input query $x$. R EVEL contains...\"}"}
{"id": "CVPR-2023-1713", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. The overall workflow of REVEL consists of four main steps: (a) encode a multimodal input into a sequence of token embeddings and a summarized query embedding; (b) encode each knowledge entry from different corpus into unified key and value embedding pairs, where key is used to index the memory and value contains full information of the knowledge; (c) retrieve top-K most similar knowledge items from different knowledge sources, and return the pre-computed in-memory value embeddings and re-encoded value; and (d) fuse the top-K knowledge items via attentive knowledge fusion layer by injecting the retrieval score as a prior during attention calculation. This facilitates REVEL's key novelty: the memory, encoder, retriever and the generator can be jointly trained in an end-to-end manner.\\n\\nGiven an input query $x$, we first retrieve $K$ possibly helpful entries $M = \\\\{m_1, \\\\ldots, m_K\\\\}$ from the memory corpora $\\\\tilde{M}$. Each $m_i$ is a memory entry containing the encoded single key embedding and a sequence of value embeddings (we will describe how to encode knowledge items into memory entries in Sec. 3.2). With it, the retriever can use embedding similarity to find relevant memory entries. We model this retrieval process as sampling from distribution $p(M|x)$. Then, we condition on both the retrieved set $M$ and the original input query $x$ to generate the output $y$, modeled as $p(y|x, M)$. To obtain the overall likelihood of generating $y$, we treat $M$ as a latent variable from the entire memory $\\\\tilde{M}$ and marginalize over it yielding:\\n\\n$$p(y|x) = \\\\sum_{M \\\\subset \\\\tilde{M}} p(M|x) \\\\cdot p(y|x, M).$$\\n\\nHowever, this marginal probability involves an intractable summation over all size-$K$ subsets of the memory corpora $\\\\tilde{M}$. We approximate this instead by using the top-$K$ entries in memory with the highest probability under $p(M|x)$. This is reasonable if most of the unrelated memory entries do not contribute to the generation. Note that we use an online memory that is updated as the knowledge encoder is trained end-to-end with the rest of the model.\\n\\nFigure 2 illustrates the overall workflow of REVEL, and we describe each component in this section. In particular, in Sec. 3.1 we describe how the query is encoded. In Sec. 3.2 we go over how the multimodal knowledge memory is constructed and updated during pre-training. Next, we describe how we retrieve the memory entries that are most relevant to the input query in Sec. 3.3. Finally, in Sec. 3.4 we describe the generator that fuses the query and retrieved knowledge and decodes them into the generated text.\\n\\n3.1. Query Encoding\\n\\nFigure 2 (a) depicts how the input image-text query is encoded. We use a base visual-language encoder $b(\\\\cdot)$ to turn the query input and each knowledge item (with potentially different modalities e.g. text-only, image-only or image-text pairs) into a sequence of embeddings (tokens). We adopt a Vision Transformer (ViT) [10] to encode the images and we use a lower-layer T5 encoder [33] to encode the texts. We add a projection layer on top of the ViT model to map the image tokens into the same space as the text tokens. We then concatenate the two modalities together. We use an upper-layer T5 module as both the query Head $\\\\phi_{\\\\text{Query}}(\\\\cdot)$ and the key Head $\\\\phi_{\\\\text{Key}}(\\\\cdot)$ to compute the query embedding and memory keys. We take the output of the first $[\\\\text{CLS}]$ tokens followed by a linear projection and L2-normalization to summarize the input into a $d$-dimensional embedding.\\n\\n3.2. Memory\\n\\nFigure 2 (b) shows how memory is constructed and updated by encoding knowledge items. Our approach differs from previous works primarily by leveraging a diverse set of multimodal knowledge corpora (WikiData knowledge graph, Wikimedia passages and images, Web image-text pairs). Throughout the paper, we denote each corpus as $C_j = \\\\{z_{j1}, \\\\ldots, z_{jN}\\\\}$, in which each $z_{ji} \\\\in C_j$ is a knowledge item that could be an image-text pair, text only, image only, or a knowledge graph triplet. We denote the unified knowledge corpus as $\\\\tilde{C} = C_1 \\\\cup C_2 \\\\cdots \\\\cup C_S$ that combines $|\\\\tilde{C}| = S$ different knowledge corpora. We encode the external knowledge corpora into a unified memory $\\\\tilde{M} = [M_1, \\\\ldots, M_{|\\\\tilde{C}|}]$.\\n\\nEach knowledge item $z_i$ is encoded into a key/value pair $m_i = (\\\\text{Emb}_{\\\\text{Key}}(z_i), \\\\text{Emb}_{\\\\text{Value}}(z_i))$ in memory. Each key $1$ We denote the last $l$ layers of a T5 encoder as 'upper-layer', and the remaining ones including the token embedding layer as 'lower-layer'.}"}
{"id": "CVPR-2023-1713", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Emb Key (z) = \\\\( \\\\phi \\\\) Key b (z) \\\\( \\\\in \\\\mathbb{R}^d \\\\) is a \\\\( d \\\\)-dimensional embedding vector encoded via Key Head. Each value is a sequence of token embeddings representing the full information of knowledge item \\\\( z \\\\). We follow a similar procedure as in [14] to precompute key/value embeddings of knowledge items from different sources and index them in a unified knowledge memory. We continuously re-compute the memory key/value embeddings as the model parameters get updated during the pre-training phase. We update the memory \\\\( \\\\tilde{M} \\\\) asynchronously at every 1000 training steps.\\n\\n**Scaling Memory by Compression**\\n\\nA naive solution for encoding the memory value is to keep the whole sequence of tokens for each knowledge item. Then, the generator could fuse the input query and the top-K retrieved memory values by concatenating all their tokens together and feeding them into a Transformer Encoder-Decoder pipeline [23]. This approach has two issues: (1) storing hundreds of millions of knowledge items in memory is impractical given that each memory value would consist of hundreds of tokens; (2) transformer encoder has quadratic complexity with respect to the total number of tokens times \\\\( K \\\\) for self-attention.\\n\\nTherefore, we propose to use the Perceiver architecture [19] as the Value Head to encode and compress knowledge items. The Perceiver model uses a transformer decoder \\\\( \\\\psi (\\\\cdot) \\\\) with learnable \\\\( c \\\\)-length latent embeddings to compress the full token sequence into an arbitrary length \\\\( c \\\\), such that \\\\( \\\\text{Emb Value}(z) = \\\\psi (b(z)) \\\\in \\\\mathbb{R}^{c \\\\times d} \\\\). (In our experiments we use \\\\( c = 32 \\\\)). This lets us retrieve top-K memory entries for \\\\( K \\\\) as large as a hundred. To make the compressed embeddings generated by Perceiver more expressive, we add two additional regularizations. The first one is a disentangled regularization [16] that forces every two output tokens to be linearly de-correlated\\n\\n\\\\[\\nL_{\\\\text{decor}} = \\\\frac{1}{K} \\\\sum_{i,j=1}^{K} \\\\text{Covariance}(\\\\psi(b(z_i)), \\\\psi(b(z_j)))^2\\n\\\\]\\n\\nand the second one is an alignment regularization that minimizes the distance of L2-Norm between the query and compressed knowledge embedding:\\n\\n\\\\[\\nL_{\\\\text{align}} = 1 - \\\\frac{1}{\\\\sum_{z} \\\\| \\\\psi(b(z)) \\\\|^2_2 \\\\sum_{x} \\\\| b(x) \\\\|^2_2}\\n\\\\]\\n\\n**3.3. Retriever**\\n\\nFigure 2 (c) shows \\\\( \\\\text{RE} \\\\text{VE} \\\\text{AL} \\\\)'s retrieval procedure. Given the input query \\\\( x \\\\), the retriever's task is to find top-K memory entries \\\\( M \\\\) with the highest probability \\\\( p(M|x) \\\\) which we approximate as\\n\\n\\\\[\\np(M|x) = \\\\frac{1}{|M|} \\\\sum_{m \\\\in M} p(m|x)\\n\\\\]\\n\\nby retrieving each entry independently. Note that we retrieve from a large-scale unified memory \\\\( \\\\tilde{M} = [M_1, \\\\ldots, \\\\tilde{C}| \\\\tilde{M}] \\\\) that is constructed from a diverse set of knowledge sources. To help the query to better choose the most appropriate knowledge sources, we learn a gating function that models the probability of retrieving from each memory corpus. With the corpus gating, for \\\\( m_ji \\\\in M_j \\\\) we re-weight \\\\( p(m_ji|x) \\\\) by\\n\\n\\\\[\\np(m_ji|x) = p(M_j|x) \\\\cdot p(m_ji|x; M_j)\\n\\\\]\\n\\nwhere\\n\\n\\\\[\\nGate_{M_j}(x) = \\\\text{Softmax}(W \\\\cdot \\\\text{Emb Query}(x) + b)[j]\\n\\\\]\\n\\nis a softmax gating that assigns a score to each memory corpus \\\\( M_j \\\\), with \\\\( W \\\\) and \\\\( b \\\\) as function parameters.\\n\\n\\\\[\\n\\\\text{Rel}(x, m_ji) \\\\text{ models relevance score between query } x \\\\text{ and each memory entry via embedding dot product, such that }\\n\\\\[\\n\\\\text{Rel}(x, m_ji) = \\\\text{Emb Query}(x)^T \\\\cdot \\\\text{Emb Key}(z_ji).\\n\\\\]\\n\\nwhere \\\\( z_i \\\\) is the knowledge item corresponding to the memory entry \\\\( m_i \\\\) and \\\\( \\\\tau \\\\) is the temperature parameter.\\n\\nAfter identifying the top-K memory entries, the retriever passes the pre-computed in-memory key and value embeddings to the generator. In the meantime, to support end-to-end training of the encoders, we also re-encode a small portion (i.e., 10\\\\% \\\\( \\\\% \\\\)) of the retrieved knowledge items \\\\( z_i \\\\) from scratch. In this way, the memory encoders could be updated with moderate computational cost. We concatenate the re-encoded knowledge with in-memory ones to construct the final top-K retrieved key/value embeddings.\\n\\n**3.4. Generator**\\n\\nFigure 2 (d) shows how the query and the retrieved knowledge items are fused to generate the output answer. All \\\\( K \\\\) retrieved memory values are concatenated with the query embedding, which is feasible due to the Perceiver module utilized as the value head \\\\( \\\\psi (\\\\cdot) \\\\), compressing each knowledge item into a short sequence. We denote the concatenated query embedding and memory values as\\n\\n\\\\[\\nX = [b(x), \\\\psi(b(z_1)), \\\\ldots, \\\\psi(b(z_K))] \\\\in \\\\mathbb{R}^{(I+c\\\\cdot K) \\\\times d},\\n\\\\]\\n\\nwhere \\\\( I \\\\) is the number of tokens of the input query \\\\( x \\\\) and \\\\( c \\\\) is the number of compressed tokens. To guide the generator towards attending to the most important items in \\\\( X \\\\) and facilitate backpropagation of gradients to the retriever, we propose an attentive fusion module \\\\( f(\\\\cdot) \\\\) capable of incorporating the retriever score as a prior for calculating\"}"}
