{"id": "CVPR-2023-780", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Jia-Wang Bian, Huangying Zhan, Naiyan Wang, Tat-Jun Chin, Chunhua Shen, and Ian Reid. Unsupervised depth learning in challenging indoor video: Weak rectification to rescue. arXiv preprint arXiv:2006.02708, 2020.\\n\\n[2] Adriano Cardace, Luca De Luigi, Pierluigi Zama Ramirez, Samuele Salti, and Luigi Di Stefano. Plugging self-supervised monocular depth into unsupervised domain adaptation for semantic segmentation. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 1129\u20131139, 2022.\\n\\n[3] Marcela Carvalho, Bertrand Le Saux, Pauline Trouv\u00e9-Peloux, Andr\u00e9s Almansa, and Fr\u00e9d\u00e9ric Champagnat. Deep depth from defocus: how can defocus blur improve 3D estimation using dense neural networks? 3DRW ECCV Workshop, 2018.\\n\\n[4] Paolo Favaro, Andrea Mennucci, and Stefano Soatto. Observing shape from defocused images. International Journal of Computer Vision, 52(1):25\u201343, 2003.\\n\\n[5] Ravi Garg, Vijay Kumar Bg, Gustavo Carneiro, and Ian Reid. Unsupervised cnn for single view depth estimation: Geometry to the rescue. In European conference on computer vision, pages 740\u2013756. Springer, 2016.\\n\\n[6] Cl\u00e9ment Godard, Oisin Mac Aodha, and Gabriel J Brostow. Unsupervised monocular depth estimation with left-right consistency. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 270\u2013279, 2017.\\n\\n[7] Cl\u00e9ment Godard, Oisin Mac Aodha, Michael Firman, and Gabriel J Brostow. Digging into self-supervised monocular depth estimation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3828\u20133838, 2019.\\n\\n[8] Shir Gur and Lior Wolf. Single image depth estimation trained via depth from defocus cues. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7675\u20137684, 2019.\\n\\n[9] Lukas Hoyer, Dengxin Dai, Yuhua Chen, Adrian Koring, Suman Saha, and Luc Van Gool. Three ways to improve semantic segmentation with self-supervised depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11130\u201311140, 2021.\\n\\n[10] Pan Ji, Runze Li, Bir Bhanu, and Yi Xu. Monoindoor: Towards good practice of self-supervised monocular depth estimation for indoor environments. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12787\u201312796, 2021.\\n\\n[11] Huaizu Jiang, Gustav Larsson, Michael Maire Greg Shakhnarovich, and Erik Learned-Miller. Self-supervised relative depth learning for urban scene understanding. In Proceedings of the european conference on computer vision (eccv), pages 19\u201335, 2018.\\n\\n[12] Diederik P. Kingma and Jimmy Ba. Adam: A method for stochastic optimization. CoRR, abs/1412.6980, 2015.\\n\\n[13] Rudolf Kingslake. A history of the photographic lens. Academic press, 1989.\\n\\n[14] Yawen Lu, Garrett Milliron, John Slagter, and Guoyu Lu. Self-supervised single-image depth estimation from focus and defocus clues. IEEE Robotics and Automation Letters, 6(4):6281\u20136288, 2021.\\n\\n[15] Maxim Maximov, Kevin Galim, and Laura Leal-Taix\u00e9. Focus on defocus: Bridging the synthetic to real domain gap for depth estimation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068\u20131077, 2020.\\n\\n[16] Michael Moeller, Martin Benning, Carola Sch\u00f6nlieb, and Daniel Cremers. Variational depth from focus reconstruction. IEEE Transactions on Image Processing, 24(12):5369\u20135378, 2015.\\n\\n[17] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support inference from rgbd images. In ECCV, 2012.\\n\\n[18] Shree K Nayar and Yasuo Nakagawa. Shape from focus. IEEE Transactions on Pattern analysis and machine intelligence, 16(8):824\u2013831, 1994.\\n\\n[19] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas K\u00f6pf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. Curran Associates Inc., Red Hook, NY , USA, 2019.\\n\\n[20] Zhongzheng Ren and Yong Jae Lee. Cross-domain self-supervised multi-task feature learning using synthetic imagery. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 762\u2013771, 2018.\\n\\n[21] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015.\\n\\n[22] Supasorn Suwajanakorn, Carlos Hernandez, and Steven M. Seitz. Depth from focus with your mobile phone. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3497\u20133506, 2015.\\n\\n[23] Huixuan Tang, Scott Cohen, Brian Price, Stephen Schiller, and Kiriakos N. Kutulakos. Depth from defocus in the wild. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4773\u20134781, 2017.\\n\\n[24] Ning-Hsu Wang, Ren Wang, Yu-Lun Liu, Yu-Hao Huang, Yu-Lin Chang, Chia-Ping Chen, and Kevin Jou. Bridging unsupervised and supervised depth from focus via all-in-focus supervision. In International Conference on Computer Vision, 2021.\\n\\n[25] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600\u2013612, 2004.\\n\\n[26] Masahiro Watanabe, Shree K Nayar, and Minori N Noguchi. Real-time computation of depth from defocus. In Three-Dimensional and Unconventional Imaging for Industrial Inspection and Metrology, volume 2599, pages 14\u201325. SPIE, 1996.\"}"}
{"id": "CVPR-2023-780", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fengting Yang, Xiaolei Huang, and Zihan Zhou. Deep depth from focus with differential focus volume. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12642\u201312651, 2022.\\n\\nZhichao Yin and Jianping Shi. Geonet: Unsupervised learning of dense depth, optical flow and camera pose. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1983\u20131992, 2018.\\n\\nTinghui Zhou, Matthew Brown, Noah Snavely, and David G Lowe. Unsupervised learning of depth and ego-motion from video. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1851\u20131858, 2017.\"}"}
{"id": "CVPR-2023-780", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where the accurate depth map and AIF image is the only solution to this function. Consequently, from the aspect of model architecture, the DAIF-Net is supposed to take a focal stack with an arbitrary number of images and output the underlying AIF image and depth map. We modify the encoder of the U-Net [21], and design our DAIF-Net, as shown in Figure 4. In order to support a flexible number of input branches, every image will be passed through the same encoder and bottleneck. In order to learn the depth and AIF image from a focal stack simultaneously, we observe that sharpness is the connection between them. Intuitively, a sharper point indicates a closer distance between the depth and the focus distance. Meanwhile, sharper points better preserve the corresponding colors of the AIF images. Therefore, training a model that is sensitive to the sharpest points in the focal stacks is beneficial to predicting both AIF images and depth maps. To measure the sharpness, our designed model exploits the difference between image pixel-wise local feature maps, i.e., the absolute sharpness, and the stack-wise global feature maps, i.e., the sharpest regions. To compare the local feature maps and the global feature maps, our model adopts layer-wise global pooling from DefocusNet [15]. Specifically, the model computes the output of the convolution block for every input image to produce the local feature maps. Then the maximum values are selected from the local feature maps across all branches to generate the global feature maps. The global feature maps are then concatenated to the local feature maps and the combined features are passed into the next convolution block. This way, global and local feature maps are visible to every branch. By comparison, the sharper regions will be highlighted by the multi-input encoder. The decoder then produces the AIF image and the depth jointly from the sharpness clue. Note that the inputs to the model have 4 channels: the RGB color channels and the focus distance channel, where we expand the focus distance to the size of the image, and concatenate it to the color channels. The focus distances can be extracted from the EXIF properties, which come together with the image and can be acquired with minimal cost. The focus distance is an essential input in our model. It increases the absolute accuracy of depth estimation, while not affecting the self-supervised characteristics of the framework.\\n\\n3.3. Self-supervised Training\\nWhile our DAIF-Net is trained by estimating an accurate depth map and AIF image that can perfectly reconstruct the input focal stack using the optical model, it is hard to achieve this goal solely using the reconstruction loss in practice. Noticing that the CoC can have the same value in front of and behind the focus point, and the defocus is ambiguous in texture-less regions, we delicately design auxiliary losses and regularization to support our self-supervised training. First, we briefly introduce our reconstruction loss.\\n\\nReconstruction Loss. We adopt the reconstruction loss from [8] and have:\\n\\n$$L_{\\\\text{recon}} = \\\\frac{1}{N^2} \\\\sum_{x} \\\\alpha \\\\left(1 - \\\\text{SSIM}(\\\\bar{J}, J^2)\\\\right) + (1 - \\\\alpha) \\\\left\\\\|\\\\bar{J} - J\\\\right\\\\|_1,$$\\n\\n(8)\\n\\nwhere \\\\(J\\\\) is the input focal stack and \\\\(\\\\bar{J}\\\\) is the reconstructed focal stack, \\\\(\\\\text{SSIM}\\\\) is the Structural Similarity measure [25], and \\\\(\\\\alpha\\\\) is used to balance the scale between the \\\\(\\\\text{SSIM}\\\\) and the \\\\(L_1\\\\) loss which we set to be 0.85.\\n\\nCoarse AIF Image. We notice that the defocus map is a natural clue for multi-focus fusion. Given a stack of defocus maps, we acquire the index of the minimum defocus value for each pixel position and fuse these sharpest pixels to form the coarse AIF image. If the defocus maps are accurate, this is the sharpest image we can produce without extra calculation. Meanwhile, since the defocus map is produced from the depth map, a clearer coarse AIF image indicates a more precise map, and thus a more accurate depth map. Therefore, we regularize the blurriness to encourage the images to be clear, so that the quality of depth map can be improved.\\n\\nBlurriness Regularization. To evaluate the sharpness of AIF image, we apply a Laplacian filter for the edge map and calculate its variance. By taking the negative log sum of the normalized variance, the blurry estimation loss [14] encourages the image to be as clear as possible:\\n\\n$$L_{\\\\text{blur}} = -\\\\frac{1}{N^2} \\\\sum_{c=1}^{C} \\\\beta \\\\log \\\\left(\\\\sum_{i} \\\\sum_{j} \\\\left(\\\\nabla^2 \\\\hat{I}(i,j)\\\\right)^2\\\\right),$$\\n\\n(9)\\n\\nwhere \\\\(I\\\\) is the image, \\\\(M\\\\) is the number of pixels, \\\\(\\\\mu\\\\) is the mean pixels value, \\\\(\\\\nabla^2\\\\) is the Laplacian operator and \\\\(\\\\beta\\\\) is a scale factor which we set to be 0.01. We apply the blur estimation loss on the predicted and the coarse AIF images to improve their sharpness.\\n\\nSmoothness Regularization. To prevent the estimated depth from drastic disparity in homogeneous regions and increase the consistency between the estimated depth and AIF image, we apply the smoothness prior to regularize the predicted depth map. The edge-aware smoothness regularization is:\\n\\n$$L_{\\\\text{smooth}} = \\\\frac{1}{N^2} \\\\sum_{x} \\\\left|\\\\frac{\\\\partial}{\\\\partial x} \\\\hat{D}\\\\right| e^{-\\\\beta \\\\frac{\\\\partial}{\\\\partial x} \\\\hat{I}} + \\\\left|\\\\frac{\\\\partial}{\\\\partial y} \\\\hat{D}\\\\right| e^{-\\\\beta \\\\frac{\\\\partial}{\\\\partial y} \\\\hat{I}},$$\\n\\n(10)\\n\\nwhere \\\\(\\\\hat{I}\\\\) is the predicted AIF image, \\\\(\\\\hat{D}\\\\) is the predicted depth map, and \\\\(\\\\beta\\\\) is the scale factor for the edge sensitivity which we set to be 2.5. The overall loss then becomes:\\n\\n$$L = L_{\\\\text{recon}} + L_{\\\\text{blur}}(\\\\bar{I}) + L_{\\\\text{blur}}(\\\\hat{I}) + \\\\lambda L_{\\\\text{smooth}},$$\\n\\n(11)\\n\\nwhere \\\\(\\\\hat{I}\\\\) is the coarse AIF image, and \\\\(\\\\lambda\\\\) is used to control the importance of the the smooth regularization, which we set to be 0.5.\"}"}
{"id": "CVPR-2023-780", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Experiments\\n\\nIn this section, we show qualitative and quantitative results of our proposed framework on a synthetic dataset [15], a real dataset with synthetic defocus blur [8, 14] and a real focal stack dataset [22].\\n\\n4.1. Configurations\\n\\nThe framework is implemented in Pytorch [19]. We train the model using the Adam optimizer [12] and the cosine annealing scheduler with an initial learning rate of $5 \\\\times 10^{-4}$. The batch is set to 32 and the focal stack size is 5 if not explicitly specified. We train our model for 2000 epochs on a single NVIDIA V100 GPU for each experiment.\\n\\n4.2. Datasets\\n\\nSynthetic Dataset\\n\\nDefocusNet Dataset [15] is a synthetic dataset rendered using Blender. The dataset consists of random objects distributed in front of a wall. A virtual optical camera takes five images of the scene with varying focus distances and forms a focal stack. The original dataset is widely used in supervised methods [15, 24, 27]. However, the focus distances of the focal stacks are overly concentrated, causing indistinguishable defocus blur. Therefore, to perform an experiment in a similar setting, we regenerate the dataset with a set of more distributed focus distances using the code provided by the dataset author. The new dataset consists of 1000 focal stacks, with $f$-number $N_2$, focal length $f = 2.9$ mm, and focal distance $F$ at 0.3 m, 0.45 m, 0.75 m, 1.2 m and 1.8 m. The maximum depth is 3 m.\\n\\nReal Image with Synthetic Defocus Blur.\\n\\nTo acquire sufficient realistic defocus images for model training and evaluation, we render the focal stack datasets from RGB-D datasets using the thin-lens equation and PSF convolution layers as in Sec. 3.1. The RGB-D datasets we use is NYUv2 dataset [17]. NYUv2 dataset with synthetic defocus blur [8, 14] is also commonly used in DFD tasks. The dataset consists of 1449 pairs of aligned RGB images and depth maps. We train our model on the 795 training pairs and evaluate on the 654 testing pairs. To render the focal stacks, we choose focal length $f = 50$ mm, $f$-number $N = 8$ and focus distance $F$ at 1 m, 1.5 m, 2.5 m, 4 m and 6 m. The maximum depth is 10 m.\\n\\nReal Focal Stack Dataset.\\n\\nMobile Depth [22] is a real-world DFD dataset captured by a Samsung Galaxy S3 mobile phone. It consists of 11 aligned focal stacks with 14 to 33 images per stack. Since neither depth ground-truth nor camera parameters are provided, we only perform qualitative evaluation and comparison on this dataset with no further finetuning.\\n\\n4.3. Evaluation\\n\\nDefocusNet Dataset.\\n\\nWe split the 1000 focal stacks into 500 training focal stacks and 500 testing focal stacks. For a fair comparison, we trained our method, along with the open-source state-of-the-art DFD methods [15, 27], on our new training set. We present the qualitative results of the testing set in the supplementary material. The quantitative evaluation results are shown in Table 1. It is expected that our method does not perform as well as the supervised methods. This is because the DefocusNet dataset is texture-less, and our self-supervised framework is less sensitive to backgrounds, where defocus change is less obvious. Such issues do not exist for supervised methods because they always have access to the depth ground-truth. Meanwhile, we observe that our method is on par with the supervised methods when counting the results only for depths less than 0.5 m, which indicates that our self-supervised method has higher accuracy in closer ranges.\\n\\nNYUv2 Dataset.\\n\\nWe present the results of our framework trained on the NYUv2 dataset with the synthetic focal stacks. In addition to Figure 1, more qualitative results are provided in the supplementary material. We evaluate our models on the sparse testing focal stacks and compare our results to other DFD methods. Table 2 shows that in scenes with complex textures, our method is on par with the state-of-the-art on the majority of the metrics. Note that [8, 14] rely on AIF image for training or inference; DFF-FV/DFV [27] are the state-of-the-art supervised DFD works on synthetic datasets. The result of AiFDepthNet [24] is not presented because they have not released their training code. In contrast, our framework is completely self-supervised, with no AIF images or depth ground-truth required.\\n\\nMobile Depth Dataset.\\n\\nTo evaluate our model on real focal stacks, qualitative experiments are performed on the Mobile Depth dataset [22]. We compare our model with MobileDFF [16], AiFDepthNet [24], DDF-DFV/FV [27] and the finetuned AiFDepthNet using AIF ground-truth. We also compare our generated AIF image with the AIF image produced by MobileDFF. Figure 5 shows parts of the results. Among these, MobileDFF is an optimization-based method and takes the full focal stack ranging from 14 to 30 images per stack. The resting deep methods generate depth using 5 uniformly sampled images from focal stacks. Since the dataset has no focus distance provided, we evenly assign the focus distances and normalize resulting depth map. From the figure, we can find that DFD models are generally sensitive to closer objects and edges. This is because the defocus change is more drastic at a close distance as demonstrated in Figure 7, and image defocus is more obvious at the edge than in the regions without textures. Such characteristic is more apparent in our framework since our framework estimates depth completely depends on defocus information. While most of the deep methods give reasonable depth estimation, we claim that our framework is more advantageous since it is fully self-supervised.\"}"}
{"id": "CVPR-2023-780", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Methods\\n\\n| Input       | Moeller et al. [16] | Suwajanakorn, Hernandez, and Seitz [22] | Gur and Wolf [8] | Defocus-Net [14] | Focus-Net [14] | DFF-FV [27] | DFF-DFV [27] |\\n|-------------|---------------------|-----------------------------------------|------------------|------------------|----------------|-------------|--------------|\\n| Focal stack | 0.670 0.778 0.912   | 0.688 0.802 0.917                      | 0.720 0.887 0.951| 0.732 0.887 0.951| 0.748 0.892 0.949| 0.956 0.979 0.988| 0.967 0.980 0.990 |\\n| Self-sup w/ AIF |              |                                         |                  |                  |                |             |              |\\n| In-focus    | 0.912 0.967 0.983  | 0.911 0.933 0.938                      | 0.977 0.996 0.999| 0.976 0.996 0.999| 0.977 0.996 0.999| 0.977 0.996 0.999| 0.977 0.996 0.999 |\\n\\nTable 1. Evaluation results on DefocusNet test set. Regular means all results are considered; <0.5m only counts results for depth less than 0.5 meters. The results indicate that our model is on par with the supervised state-of-the-arts in a closer range.\\n\\n| Input       | Gur and Wolf [8] | Defocus-Net [14] | Focus-Net [14] | DFF-FV [27] | DFF-DFV [27] |\\n|-------------|------------------|------------------|----------------|-------------|--------------|\\n| Focal stack | 0.720 0.887 0.951| 0.732 0.887 0.951| 0.748 0.892 0.949| 0.956 0.979 0.988| 0.967 0.980 0.990 |\\n| Self-sup w/ AIF |              |                  |                |             |              |\\n| In-focus    | 0.951 0.935 0.926| 0.951 0.935 0.926| 0.951 0.935 0.926| 0.951 0.935 0.926| 0.951 0.935 0.926 |\\n\\nTable 2. Evaluation results on NYUv2 test set. Note that the results of [8,14,16,22] are recorded from [14]. Self-sup w/ AIF means that the method is self-supervised but utilizes AIF ground-truth. Results show that our method is on par with the state-of-the-art on NYUv2 dataset for the depth-from-defocus task.\\n\\n| Setting                | \u03b4\u2081\u2191 \u03b4\u2082\u2191 \u03b4\u2083\u2191 | RMSE\u2193 | AbsRel\u2193 |\\n|------------------------|------------|-------|---------|\\n| w/o cAIF blur          | 0.934 0.973 0.986 | 0.462 | 0.254   |\\n| w/o smooth             | 0.912 0.976 0.984 | 0.477 | 0.278   |\\n| Ours Original          | 0.950 0.979 0.987 | 0.325 | 0.170   |\\n| Ours New Param.        | 0.957 0.979 0.986 | 0.348 | 0.178   |\\n| Ours Noisy             | 0.943 0.980 0.986 | 0.340 | 0.189   |\\n\\nTable 3. Ablation studies for the regularization, the camera parameters and data generation process.\\n\\n5. Discussion\\n\\nIn this section, we provide the ablation study on regularization, camera parameter selection and data generation process. We also discuss some of the limitations of our framework. Further discussion on the selection of hyperparameters and extension to the framework are provided in the supplementary material.\\n\\n5.1. Ablation Study\\n\\nRegularization Selection.\\n\\nIn our work, we propose that by regularizing the blurriness of the coarse AIF and the smoothness of the estimated depth, we can improve the model performance. As shown in Table 3, we perform ablation studies using NYUv2 Dataset. From the table, we can observe that the regularization improves the accuracy (\u03b4\u2081) of our model by 1.6% and 3.8%, respectively. Our model also celebrates a drop in error rates (RMSE, AbsRel) with complete regularization. Therefore, the experiments prove the effectiveness of our regularization.\\n\\nCamera Parameters.\\n\\nWe perform an experiment to show that knowledge of the camera parameters is not necessary. Instead of using the camera parameter used in rendering the NYUv2 focal stack dataset, we train our model on another set of camera parameters: f-number \\\\( N = 1.2 \\\\), focal distance \\\\( f = 17 \\\\) mm. The result is shown in the second last row of Table 3. We can see that even being trained with different sets of camera parameters, our framework still provides promising results. Note that although we can train our model on different camera parameters, the camera parameters need to be selected so that it generates distinguishable defocus blurs as discussed in Sec. 3.1.1.\\n\\nData Generation Process.\\n\\nTo illustrate that the high performance of our method on the NYUv2 dataset is not because of the matching between the generated focal stack and the reconstructed defocus images, we slightly modify the data generation process. To be specific, we add Gaussian noise to the generated images. Further details are provided in the supplementary material.\"}"}
{"id": "CVPR-2023-780", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Qualitative depth estimation and AIF prediction results on the Mobile Depth dataset. The warmer color indicates a larger depth.\\n\\nNote that AiFDepthNet* is the finetuned model using AIF information.\\n\\nWe added noise to every rendered defocus image and train our model on the noisy NYUv2 Focal Stack. The result is shown in the last row of Table 3. We can see that the performance of our model does not vary even noise is added, which proves that our method is robust against noise and works regardless of the defocus generation process.\\n\\n5.2. Limitations\\n\\nOur DAIF-Net predicts the depth and AIF image by solving Eq. (7). Theoretically, a focal stack with two images is enough to solve the equation. However, in practice, more images with distinct defocus blurs are needed. Intuitively, a larger focal stack helps the model to better find the sharpest point instead of the sharper point. In addition, two images may have close defocus values at a certain depth as shown in the right of Fig. 7. Such indistinguishable defocus blurs make focal stack reconstruction and model optimization challenging. Therefore, we need more than two images in the focal stack so that distinguishable defocus values are available at every depth. To sum up, our framework is more suitable for closed scenes and textured scenes, where the defocus blurs are easier to be observed.\\n\\n6. Conclusion\\n\\nIn this paper, we first propose a real yet challenging setting for DFD tasks, where only sparse focal stacks are provided for training and testing. Then, to train a DFD model in such scenarios, we design a novel self-supervised framework for this task. In this framework, the DAIF-Net predicts depth and AIF image simultaneously from a focal stack. The model is then supervised by reconstructing the input focal stack using the optical model. Our framework relieves the need for the AIF or depth ground-truth, which makes its application to the real world more feasible. Experiments on various datasets also demonstrate the superior performance of our models, which set a strong baseline for the completely self-supervised DFD tasks. In the future, we plan to integrate the knowledge of the CoC curve into the DAIF-Net. It is also necessary to establish a real focal stack dataset with focus distance and depth ground-truth included so that future DFD works can have a quantitative benchmark on more realistic data.\\n\\nAcknowledgements. This research is supported in part by Shanghai AI Laboratory, the Natural Science Basic Research Program of Shaanxi under Grant 2021JQ-204, and the National Natural Science Foundation of China under Grant 62106183 and 62106182.\"}"}
{"id": "CVPR-2023-780", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nDepth-from-defocus (DFD), modeling the relationship between depth and defocus pattern in images, has demonstrated promising performance in depth estimation. Recently, several self-supervised works try to overcome the difficulties in acquiring accurate depth ground-truth. However, they depend on the all-in-focus (AIF) images, which cannot be captured in real-world scenarios. Such limitation discourages the applications of DFD methods. To tackle this issue, we propose a completely self-supervised framework that estimates depth purely from a sparse focal stack. We show that our framework circumvents the needs for the depth and AIF image ground-truth, and receives superior predictions, thus closing the gap between the theoretical success of DFD works and their applications in the real world. In particular, we propose (i) a more realistic setting for DFD tasks, where no depth or AIF image ground-truth is available; (ii) a novel self-supervision framework that provides reliable predictions of depth and AIF image under the challenging setting. The proposed framework uses a neural model to predict the depth and AIF image, and utilizes an optical model to validate and refine the prediction. We verify our framework on three benchmark datasets with rendered focal stacks and real focal stacks. Qualitative and quantitative evaluations show that our method provides a strong baseline for self-supervised DFD tasks. The source code is publicly available at https://github.com/Ehzoahis/DEReD.\\n\\n1. Introduction\\n\\nDepth estimation is a fundamental task in computer vision. Predicting depth from RGB images shows great potential in applications like autonomous cars or robots. It has also been used as a powerful pretext training task in unsupervised downstream tasks including visual feature extraction [11, 20], semantic segmentation [2, 9], etc. However, collecting sufficiently diverse datasets with per-pixel depth ground-truth for supervised learning is challenging. To overcome this limitation, self-supervised depth estimation works exploit the consistency of 3D geometries within stereo pairs [5, 6] or monocular videos [7, 10], and have shown impressive results. Nevertheless collecting synchronized multi-view images or videos is resource-consuming.\\n\\nAnother clue for depth estimation is defocus. Defocus blur is a measurable property of images that is associated with the geometry of camera lens and the changes in depth. Previous works [4, 22] prove that depth can be recovered from a set of images with different focus distances, i.e., a focal stack, by observing their blurry amounts. With the help of deep learning methods, supervised DFD works [15, 27]...\"}"}
{"id": "CVPR-2023-780", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"estimate depth in a data-driven way. Recent DFD methods \\\\cite{8, 14, 24} claim they are freed from being supervised by the depth ground-truth. However, these works utilize the all-in-focus (AIF) image in model training, which is more of a theoretical concept than images captured by camera lens \\\\cite{13} in the real world. Existing works generally treat images taken with small apertures as the AIF image. Such approximations inevitably contain regional blurriness and suffer from underexposure in short range. Thus, current DFD methods have drawbacks in practical applications.\\n\\nIn order to close the gap between theories and real-world applications, we design a more realistic setting for the DFD tasks: only focal stacks are provided in model training, while the availability of depth and AIF images ground-truth is deprived. This is a challenging task because we no longer have the direct/indirect optimization goal for the model. To tackle this challenge, we propose a self-supervised DFD framework, which constrains the predicted depth map and AIF image to be accurate by precisely reconstructing focal stacks. Specifically, our framework consists of a neural model, $\\\\text{DepthAIF-Net}$ ($\\\\text{DAIF-Net}$), which predicts the AIF image and the depth map from the focal stack, and a physical-realistic optical model that reconstructs the input from the predicted AIF image and depth map. Since all images in a focal stack are sharing the same depth and AIF image, and the physics model can deterministically map them to focal stacks, accurate depth maps and AIF images are a necessary and sufficient condition for precisely reconstructing the input. Therefore, by assuring the consistency between the input and the reconstructed focal stack, the prediction of depth map and AIF image can be immediately improved. To the best of our knowledge, this is the first self-supervised DFD work that relieves the need for AIF images and depth ground-truth. Such improvements over previous supervised and indirectly-supervised works make our method more applicable in real scenarios.\\n\\nExtensive experiments are conducted on both synthetic and real datasets. Results show that the proposed framework is on par with the state-of-the-art supervised/indirectly-supervised depth-from-focus/defocus methods and has convincing visual quality, as shown in Figure 1. Additionally, we apply our model to the data in the wild, and demonstrate the ability of our framework to be applied in real scenarios. Finally, we extend the training paradigm of our framework so that our model has the ability to be transferred between focal stacks with an arbitrary number of images.\\n\\nOur contribution is three-fold:\\n\\n- We design a more realistic and challenging scenario for the Depth-from-Defocus tasks, where only focal stacks are available in model training and evaluation.\\n- We propose the first completely self-supervised framework for DFD tasks. The framework predicts depth and AIF images simultaneously from a focal stacks and is supervised by reconstructing the input.\\n- Our framework performs favorably against the supervised state-of-the-art methods, providing a strong baseline for future self-supervised DFD tasks.\\n\\n2. Related Works\\n\\n2.1. Self-supervised Monocular Depth Estimation\\n\\nMonocular depth estimation works aim to regress a dense depth map from a single RGB image. Methods for self-supervised monocular depth estimation utilize scene geometry as a constraint in training. Specifically, \\\\cite{5} introduces the differentiable inverse warping and proposes an auto-encoder-based model trained from stereo pairs; the follow-up work \\\\cite{6} adds left-right consistency to further exploit geometry clues. To train models from video sequences, ego-motion \\\\cite{29} and optical flow \\\\cite{28} of the scenes are also added to the frameworks. Following these works, recent works \\\\cite{1, 10} have shown impressive results on indoor datasets. Although self-supervised monocular depth estimation works are capable of producing depth maps without training with depth ground-truth, they require a large number of stereo pairs and/or monocular video sequences. Additionally, because the absolute scales are ambiguous in monocular depth estimation, such models are facing challenges in transferring to unseen datasets.\\n\\n2.2. Depth from Focus/Defocus\\n\\nDepth estimation from focus/defocus works reconstructs the depth map by observing the blurriness of images caused by the lens effect. Analytical depth-from-focus (DFF) and depth-from-defocus (DFD) approaches \\\\cite{4, 18, 26} calculate depth by measuring the sharpness of each pixel. Both approaches result in low-quality depth maps. Optimization-based DFF methods \\\\cite{16, 22} usually take dense focal stacks as input, which produce high-quality depth maps but are time-consuming. Recent deep methods are trying to estimate depth from sparse focal stacks or a single defocus image. \\\\cite{3} shows that images with defocus blur aid the model to learn depth better. \\\\cite{15} transfers supervised models trained on synthetic datasets to real world datasets by learning to estimate the defocus map as an intermediate clue. \\\\cite{24} further makes use of the implicit mutual information of depth and AIF image to finetune the model. \\\\cite{14} trains a DefocusNet and a FocusNet simultaneously to perform DFD and DFF tasks and the models are supervised by their consistency. \\\\cite{27} add focus volume and differential focus volume to their supervised model to improve estimation quality. Note all the aforementioned models either require ground-truth or AIF image in addition to the focal stack, while neither of them is trivial to acquire in the wild. Moti...\"}"}
{"id": "CVPR-2023-780", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this section, we introduce our framework for depth estimation from sparse focal stacks. The framework is visualized in Figure 2. In the training stage, the DAIF-Net estimates AIF images and depth maps from the focal stacks. The optical model then reconstructs the inputs from the predicted depth and AIF image by simulating the physical process of defocus generation. Losses and regularization are applied to encourage the detailed reconstruction, and consequently improve the quality of the estimated depth and AIF image. In the inference stage, we estimate the depth map and AIF image directly from the trained DAIF-Net without the rendering process.\\n\\nIn the following, we first elaborate on the optical model behind the generation of defocus blur (Sec. 3.1). Then, we introduce our DAIF-Net and the intuition behind it (Sec. 3.2). Finally, we describe our supervision strategy that supports the self-supervised learning framework (Sec. 3.3).\\n\\n3.1. Optical Model of Defocus Blur Generation\\n\\n3.1.1 Defocus Map Generation\\n\\nDefocus blur is a well-studied phenomenon that naturally exists in optical imaging systems. To quantitatively measure the defocus blur in an image, we introduce the defocus map. Given an optical system, the defocus map can be calculated from the depth map once we establish the relationship between depth and defocus. As illustrated in the left of Figure 7, when a point light source is out-of-focus, the light rays will converge either in front of or behind the image plane, and form a blurry circle on the image plane. The circle of confusion ($CoC$) measures the diameter of such a blurry circle. If the point light source is in focus, it will form an infinitely small point on the image plane, making it the sharpest projection with the minimum $CoC$. Therefore, $CoC$ describes the level of blurriness, in another word, the amount of defocus. Deriving from the thin-lens equation, the relationship between $CoC$, scene depth and camera parameters are well established:\\n\\n$$CoC = \\\\frac{A}{d_o - F} \\\\cdot \\\\frac{d_o - F}{f},$$\\n\\nwhere $A$ is the aperture diameter, $d_o$ is the object distance, or depth, $F$ is the focus distance, $f$ is the focal length. In practice, the f-number, $N = \\\\frac{f}{A}$, is commonly used to describe the aperture size. Since all of the parameters are in units of meters, we also need to convert $CoC$ into units of pixels. Finally, we need the radius, $\\\\sigma$, of the circle of confusion in focal stack rendering. The resulting equation becomes:\\n\\n$$\\\\sigma = CoC^2 \\\\cdot p = \\\\frac{1}{2} \\\\cdot p \\\\cdot \\\\frac{d_o - F}{d_o \\\\cdot (F - f)} \\\\cdot \\\\frac{f}{N} \\\\cdot (F - f),$$\\n\\nwhere $p$ is the CMOS pixel size. With Eq. (2) and a set of camera parameters, we can easily convert a depth map into defocus map. By varying the focus distance $F$, we can produce the defocus map for a focal stack. The right of Figure 7 shows how defocus responds to the changing of depth at the different focus distances. Note that with the larger focus distance, it is generally harder to distinguish the amount of defocus, especially when two focus distances are close. Since training our framework heavily relies on reconstructing the focal stack, indistinguishable defocus defects the effectiveness for our method. Therefore, we tend to avoid this issue by properly selecting the camera parameters, i.e., the f-number $N$, the focal length $f$ and the focus distance $F$.\\n\\n3.1.2 Defocus Image Rendering\\n\\nGiven the defocus map and the AIF image, we can explicitly model the generation process of the defocus image. Taking advantage of the deterministic relationship, our predicted depth and AIF image can be supervised by reconstructing the input focal stack. To render a defocus image, we convolve the AIF image with the point spread function (PSF). PSF describes the pattern of a point light source transmitting to the image plane through the camera lens. In practice, we calculate the defocus blur using a simplified disc-shaped PSF, i.e., a Gaussian kernel, following previous works [8, 23]. We also consider the radius of the pixel-level circle of confusion $CoC$ as the standard deviation for the Gaussian kernel. Given the defocus map $\\\\Sigma$ calculated in Sec. 3.1.1, the PSF center at $x, y$ with $\\\\sigma = \\\\Sigma_{x,y}$ can be written as:\\n\\n$$F_{x,y}(u, v) = \\\\frac{1}{2\\\\pi \\\\Sigma_{x,y}^2} \\\\exp\\\\left(-\\\\frac{u^2 + v^2}{2\\\\Sigma_{x,y}^2}\\\\right),$$\\n\\nwhere $u, v$ is the location offset to the kernel center. Let $I$ be the AIF image and $J$ be the rendered defocus image, we produce $J$ with defocus map $\\\\Sigma$ by convolving $F$ with $I$:\\n\\n$$J = I \\\\otimes F.$$\\n\\nNote $F$ is a pixel-specific Gaussian kernel, therefore, unlike the traditional convolution, we need to change the kernel as the convolution window moves. Here, we adopt the PSF convolution layer [8]. The PSF convolution layer is implemented in CUDA, thus supporting backpropagation and can be incorporated into our training pipeline. By providing the AIF image $I$ and the defocus map $\\\\Sigma$, the PSF convolution layer will calculate the Gaussian kernel with the standard deviation clue from $\\\\Sigma$ on the air and convolve with $I$. In\"}"}
{"id": "CVPR-2023-780", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. An overview of the proposed framework. The framework consists of a neural model, DAIF-Net, and an optical model. The optical model is composed of the thin-lens module and the PSF convolution layer. The DAIF-Net estimates the depth map and AIF image, and the optical model reconstructs the input focal stack to supervise the prediction.\\n\\nFigure 3. Left: Illustration of the thin-lens equation. See text for symbol definitions. Right: The response curve of the CoC radius, $\\\\sigma$, at different scene depth, with different focus distance.\\n\\nIn practice, the window size for convolution is $7 \\\\times 7$ and pixels with defocus value $\\\\sigma < 1$ will be treated as a clear pixel since its CoC radius is less than one pixel. By generating multiple defocus maps and rendering the corresponding defocus images, we can construct the focal stack with varying focus distances.\\n\\nDiscussion on the Optical Model. We choose the physics-realistic optical model to reconstruct the focal stack over a neural model mainly for two reasons. First, the optical model is a physically explainable process, therefore, an implicit constraint on the physical meanings will be applied to the inputs. In another word, the optical model encourages the predicted depth and AIF images to have their corresponding physical properties so that the focal stack can be properly reconstructed. However, using a neural model does not have such constraints. Second, it is hard to optimize the DAIF-Net model and a neural model for reconstruction jointly. In contrast, using an optical model requires no training. Meanwhile, the physics model is naturally more robust and has a provable performance.\\n\\n3.2. DAIF-Net\\n\\nThe optical model depicts the forward process of generating defocus image from the AIF image and the depth. The DAIF-Net architecture takes a focal stack of arbitrary size and estimates the depth map and AIF image. The parameters of encoders and bottlenecks are shared across all branches. We adopt the global pooling [15] and fuse the branches by selecting the maxima of their features.\\n\\nThe DAIF-Net calculates the following equation explicitly:\\n\\n$$ J_F = G_F(I, D) \\\\quad (5) $$\\n\\nwhere $J_F$ is the defocus image with focus distance $F$, $G$ is the optical model, $I$ and $D$ are the AIF image and the depth map respectively.\\n\\nIn contrast, predicting depth and AIF image from the focal stack, the DAIF-Net is implicitly learning the reverse expression of Eq. (5). To be specific, we are learning:\\n\\n$$ \\\\{I, D\\\\} = D(J_{0F}, J_{1F}, \\\\ldots, J_{kF}) \\\\quad (6) $$\\n\\nwhere $D$ is the DAIF-Net. Solving this equation using a single defocus image is an ill-posed problem because we are calculating two variables from one input. Taking advantage that the defocus images in a focal stack are sharing the same depth and AIF image. Thus, we can train our model on a focal stack to make this task solvable, and our DAIF-Net has the following expression:\\n\\n$$ \\\\{I, D\\\\} = D(J_{0F}, J_{1F}, \\\\ldots, J_{kF}) \\\\quad (7) $$\"}"}
