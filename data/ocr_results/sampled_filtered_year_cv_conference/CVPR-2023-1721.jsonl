{"id": "CVPR-2023-1721", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Visual Question Answering (VQA) models have been shown to suffer from language biases, where the model learns a correlation between the question and the answer, ignoring the image. While early works attempted to use question-only models or data augmentations to reduce this bias, we propose an adaptive margin loss approach having two components. The first component considers the frequency of answers within a question type in the training data, which addresses the concern of the class-imbalance causing the language biases. However, it does not take into account the answering difficulty of the samples, which impacts their learning. We address this through the second component, where instance-specific margins are learnt, allowing the model to distinguish between samples of varying complexity. We introduce a bias-injecting component to our model, and compute the instance-specific margins from the confidence of this component. We combine these with the estimated margins to consider both answer-frequency and task-complexity in the training loss. We show that, while the margin loss is effective for out-of-distribution (ood) data, the bias-injecting component is essential for generalising to in-distribution (id) data. Our proposed approach, Robust Margin Loss for Visual Question Answering (RMLVQA) improves upon the existing state-of-the-art results when compared to augmentation-free methods on benchmark VQA datasets suffering from language biases, while maintaining competitive performance on id data, making our method the most robust one among all comparable methods.\\n\\n1. Introduction\\nVisual question answering (VQA) lies at the intersection of computer vision and natural language processing. It is the task of answering a question based on a given image. VQA networks need to combine knowledge from both visual scene and the question to predict the answer. These systems have numerous applications such as aiding the visually impaired in understanding their surroundings, image retrieval systems in e-commerce, and robotics.\\n\\nWith the success of deep learning, research in VQA has made great strides in recent years [3, 5, 16]. However, studies have shown that deep networks may learn correlations between the question and answer alone, ignoring the image modality [3, 21, 36]. They fail to do multimodal reasoning, specifically, if there is a class imbalance in the answer distributions of the training and test sets. For example, if most of the questions starting with \\\"What color..?\\\" are paired with the answer \\\"red\\\" in the training data, the model memorizes this trend to answer \\\"red\\\" for all color based questions in the test set, irrespective of the image.\\n\\nOne solution to this problem is to perform data augmentations in various ways [1, 4, 11, 15, 27, 42, 43, 46]. These methods outperform most other debiasing techniques in the literature. However, augmentation strategies are dependent on the dataset in question and the type of biases observed, which makes the process manual and tedious. Another extensively explored solution is to learn the bias in the dataset separately using a question-only branch [9, 12, 18] and explicitly removing the learnt bias from the base model.\\n\\nMargin losses have been widely used for a number of tasks. Cao et al. [10] address the problem of long-tailed recognition through an adaptive margin loss, ensuring higher cosine margin penalty for the tail classes and lower penalty for other classes. A similar adaptive cosine margin penalty has been applied to mitigate the language bias problem in VQA by a previous work [17]. Margin losses have been widely used in deep face recognition as well, where it is desired that the features of two images of the same person should be as similar as possible, whereas those of two different people should be far apart. Margin losses\"}"}
{"id": "CVPR-2023-1721", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are used in this regard for facial feature discrimination to maximize the decision margin. Rather than the Euclidean space, both these margin losses project their feature spaces onto a fixed radius hypersphere. While the well known CosFace loss [41] uses cosine margin penalties like Cao et al. [10], ArcFace [14], uses angular margins to maximize the decision margins among different classes in this feature space. Angles correspond to geodesic distance on the projected hypersphere which stabilizes the training process and is shown to improve the discriminative power of face recognition models as compared to cosine margin based methods.\\n\\nWe believe that the language bias problem of VQA can be addressed by learning discriminative features for the biased and unbiased samples in the dataset. To this end, we implement an adaptive angular margin loss, inspired by ArcFace, as margins allow models to distinguish between different kinds of samples. However, the key question here would be how to set the adaptive margins to cater to the specific problem of language biases. Traditional models over-trust the questions over the images due to a class imbalance in the training and test set answer distributions. Hence, one way to set the margins would be to ensure that the training samples with frequent answers are given a smaller penalty due to the abundance of those answers in the dataset, whereas those with rare answers are given a higher margin value. We refer to the resultant margin values as the frequency-based margins. However, one factor that is ignored in this aspect is the answering difficulty of each sample. We argue that more margin should be given to hard samples compared to the easier ones even if the corresponding answers are frequent. In this regard, we propose the learning of instance-specific margins during training, so as to allow the model to distinguish between hard and easy samples alongside frequent and rare samples. To the best of our knowledge, this is one of the first attempts in learning margins automatically and parallely during training. We combine these learnable margins with the frequency based margins used in prior works [17], so as to allow both frequency and complexity of data samples to control the training dynamics. To compute these learnable margins, we introduce a bias injecting module to our model that is trained using Cross-Entropy (CE) loss. We show that the CE loss additionally clusters the training samples based on the dataset bias itself, thereby aiding the margin loss further. We further introduce a supervised contrastive loss [23] to pull the features of samples having the same answer together, while pushing others apart.\\n\\nAdaptive margin losses, with margins calculated by using frequency of answers in the dataset, perform well in theood setting. However, we show that they cause a drop in the in-domain performance, which raises questions on their robustness. As pointed by [40], it is crucial for VQA models to perform well on both in-domain andood data since the test set distribution is unknown apriori. We mitigate this issue by ensembling the outputs of the bias injecting component and the proposed learnable margin-loss trained classification head during inference. This makes the model robust to the difference in answer distributions of the training and test sets. Our overall method is called RMLVQA - Robust Margin Loss for Visual Question Answering with language biases. The key contributions of this work can be summarized as follows:\\n\\n\u2022 We propose to mitigate the well known problem of language bias in VQA models by introducing an instance-specific adaptive margin loss, to allow the use of different margins for the learning of samples with varying complexities, in addition to the use of frequency-based margins. To achieve this, we introduce a bias-injecting component and allow the margins to be computed based on prediction probabilities of this branch. We show that this clusters samples in the feature space based on the bias present in the dataset.\\n\\n\u2022 We propose to overcome the id-ood trade-off in margin-based losses, by ensembling the outputs of the bias-injecting component and the main model.\\n\\n\u2022 We further introduce a supervised contrastive loss that pulls features of training samples having the same ground truth answers together, while pushing apart others. This aids the margin loss further.\\n\\n\u2022 Through extensive experiments and ablations, we show how the proposed approach achieves state-of-the-art results when compared to augmentation-free methods on theood VQA-CP v2 dataset, while maintaining competitive performance on the id VQA v2 dataset. This makes our model the most robust one among all non-augmentation based methods.\\n\\nThe code, hyperparameter analyses, and results on multiple datasets are shared as supplementary material.\\n\\n2. Related Work\\n\\nVQA and the language bias problem.\\n\\nVQA [5, 16] is the task of answering natural language questions given an image. VQA v1 & v2 [5, 16] are widely used datasets that are used to benchmark different algorithms. However, prior works have shown that these datasets have biases which can be amplified in the trained models [2, 3, 19, 36, 37]. Specifically, in case of language bias, the model learns a correlation between the question and the answer directly. For example, if the model always predicts \\\"tennis\\\" for the sport-related questions, it can achieve around 40% accuracy on the VQA v1 dataset, as the images of the VQA datasets are from MS-COCO [28], which has many \\\"tennis\\\" related images. To effectively quantify the extent of bias in the trained models, a new dataset VQA-CP v2 [3] was created from the\"}"}
{"id": "CVPR-2023-1721", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VQA v2 dataset. In this work, we aim to solve the language bias problem of VQA, where we evaluate our models on theood benchmark dataset VQA-CP v2 and theid VQA v2.\\n\\nBias mitigation techniques. The language bias problem reduces the generalizability of the VQA models, resulting in a drop in performance when the training set answer distribution is different from that of the test set. There are three popular categories of solutions in the literature tackling this problem, which we discuss below.\\n\\nVisual Grounding based methods [34, 45] used human annotations as supervision to attention maps [13, 33], whereas more recent works use Grad-CAM for the same [36, 43].\\n\\nEnsemble based methods use a separate question-only model to capture the bias, and further remove the bias explicitly from the base VQA model [9, 12, 35]. Apart from these techniques, Niu et al. [31] introduce a counterfactual inference framework to mitigate the biases. KV and Mit-tal [25] use a visually grounded question encoder such that the visual information is embedded in the question representation itself. Niu et al. [32] improve bothood generalizability andid performance through introspective knowledge distillation.\\n\\nAugmentation based methods remove bias by augmenting the dataset to make it more balanced. Some works generate counterfactual data by masking or transforming the critical objects and words, and further generating appropriate ground truth answers [11,15,27]. Other works introduce negative samples by shuffling the inputs (images and questions) in the minibatch to reduce bias without requiring any annotation [40,42,46]. Wen et al. [42] use two kinds of negative samples for balancing the dataset, and construct both question-only and vision-only models to remove the biases.\\n\\nMargin-based methods. Max-margin classifiers are used in popular machine learning algorithms like SVMs [38]. Margin losses are commonly used in many problems such as deep face recognition [7, 14, 29, 41], long tailed or class imbalanced learning [10, 22, 30] and few-shot learning [26]. Guo et al. [17] apply an adaptive cosine margin loss to discriminate the frequent and rare answers for a given question type and implement this by transforming the multimodal feature space to a fixed radius hypersphere. The adaptive margins are estimated from the training data based on the frequency of occurrence of an answer in questions of a given type, ensuring that rare answers are given more margin penalty, whereas frequent classes are given less penalty.\\n\\n3. Proposed approach\\n3.1. Preliminaries\\nWe consider the problem of classification based Visual Question Answering (VQA). It is the task of answering questions based on an image. Given a datasetD havingn samples, with imagev \u2208 V, questionq \u2208 Q and answer(class label)a \u2208 A, the goal is to train a model to optimize a mapping functionf: V \u00d7 Q \u2192 R|A| to generate predictions for the given question and image. The VQA model consists of four parts: (1) image feature extractor ev, (2) question feature extractor eq, (3) the VQA modelmf which fuses the image and question features to generate the joint multimodal featuresx, (4) classifierc having weightsW, that generates the logitsf. Most of the classical VQA models [4, 6, 8, 24, 44] follow this paradigm and formulate the problem as follows:\\n\\n\\\\[f(v, q) = c(m_f(e_v(v), e_q(q)))\\\\]\\n\\nStandard Cross-Entropy loss. VQA models can be trained by optimizing the Cross-Entropy (CE) loss shown below:\\n\\n\\\\[L_s = \\\\sum_{i=1}^{|A|} -a_i \\\\log \\\\frac{\\\\exp(f_i)}{\\\\sum_{j=1}^{|A|} \\\\exp(f_j)}\\\\]\\n\\nTo solve the problem of language bias in VQA, the model should learn discriminative features for different answers for a given question type so that it can distinguish frequently occurring answers from rarely occurring answers, as well as those with varying complexity. However, standard CE loss only favours the samples with frequently occurring answers.\\n\\nNormalized CE loss. Before defining the margin loss, we define a reformulation of the CE loss as a cosine loss [14, 41], by\\n\\n\\\\[\\\\hat{W_i} = W_i / \\\\|W_i\\\\|\\\\] and \\\\[\\\\hat{x} = s \\\\hat{x} / \\\\|x\\\\|\\\\], where s is a scaling parameter. Let \\\\(\\\\theta_i\\\\) be the angle between \\\\(x\\\\) and \\\\(W_i\\\\). Therefore the logit for each \\\\(a_i\\\\) is transformed as (keeping the bias term as 0 for simplicity):\\n\\n\\\\[f_i = \\\\hat{W}_i^\\\\top \\\\hat{x} = \\\\|\\\\hat{W}_i\\\\|\\\\|\\\\hat{x}\\\\| \\\\cos \\\\theta_i = s \\\\cos \\\\theta_i\\\\]\\n\\nThe joint features \\\\(\\\\hat{x}\\\\) are thus distributed on a hypersphere with a radius \\\\(s\\\\). This makes the normalized CE loss as:\\n\\n\\\\[L_{ns} = \\\\sum_{i=1}^{|A|} -a_i \\\\log \\\\frac{s \\\\cos \\\\theta_i}{\\\\sum_{j=1}^{|A|} \\\\exp(s \\\\cos \\\\theta_j)}\\\\]\\n\\n3.2. RMLVQA\\nIn this subsection, we define our method, which is a Robust Margin Loss based approach for solving the problem of language biases in VQA.\\n\\nAdaptive Angular Margin Loss. We introduce an adaptive angular margin loss that ensures decision margin maximization through angular margins in an adaptive way. The normalized CE loss transforms the feature space to an angular space of radius \\\\(s\\\\). The motivation behind an adaptive\"}"}
{"id": "CVPR-2023-1721", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"What kind of ball are they chasing?\\n\\nFigure 1. Overview of our model. The orange dashed region shows the proposed bias-injecting module, trained using the CE loss. The blue dashed region shows how the instance-based margins are generated and then combined with the randomized frequency-based margins to compute the final margins. The final prediction is obtained by combining the two logit heads.\\n\\nOne aspect of the adaptive margins is to ensure that the instances with frequent answers are allowed a smaller margin than those with the rare ones. We call these margins $m_{freq}[i]$. Similar to [17], these margins are calculated based on the question type as shown below:\\n\\n$$\\\\bar{m}_{k}\\\\left[i\\\\right] = \\\\frac{n_{k}^{i}}{P_{\\\\left|A\\\\right|}j=1n_{k}^{j}+\\\\epsilon}$$  \\\\hspace{1cm} (5)\\n\\n$$m_{k}\\\\left[i\\\\right] = 1 - \\\\bar{m}_{k}\\\\left[i\\\\right]$$  \\\\hspace{1cm} (6)\\n\\nwhere $\\\\bar{m}_{k}\\\\left[i\\\\right]$ measures the probability of occurrence of answer $a_{i}$ in the training data for a given question type $qt_{k}$. $n_{k}^{i}$ is the frequency of answer $a_{i}$ in the training set calculated for the given $qt_{k}$, and $\\\\epsilon$ is a hyperparameter for avoiding computational overflow. $m_{k}\\\\left[i\\\\right]$ is the adaptive margin for answer $a_{i} \\\\in A$ corresponding to $qt_{k}$.\\n\\nThe adaptive angular margin loss adds a margin penalty to the angle between the features $x$ and the classifier weights $W_{i}$ for the $i$th class as shown below. Since the margin is placed on the angle, it maps exactly to the \\\"geodesic\\\" distance on the hypersphere [14]. While Deng et al. [14] set a constant value for the margin, i.e. 0.5, our margins are adaptive in nature.\\n\\n$$L_{k}\\\\text{Angular} = \\\\frac{1}{\\\\left|A\\\\right|} \\\\sum_{i=1}^{\\\\left|A\\\\right|} -a_{i}\\\\log \\\\exp(s\\\\cos(\\\\theta_{i}+m_{freq}[i])) \\\\prod_{j=1}^{\\\\left|A\\\\right|} \\\\exp(s\\\\cos(\\\\theta_{j}+m_{freq}[j]))$$\\n\\nFor notational simplicity, in the coming sections, we omit $k$ from the margin and loss computations.\\n\\nRandomization of the estimated margins. Boutros et al. [7] suggest that in typical margin losses, setting constant margins can limit the generalizability and discriminative power of a model. We note that $m_{freq}[i]$ is constant for each $a_{i} \\\\in A$ for a given question type $qt_{k}$ over all samples in the training set. When $qt_{k}$ is \\\"how many\\\", the margin for the answer \\\"2\\\" is same for all the training instances belonging to this question type. We believe that setting the same high margin value for a specific rare answer in a given question type and similarly lower value for a frequently appearing answer can lead to overcorrection of the language bias, by forcing the model to focus on the rare answers more than the frequent ones. Therefore, under a given question type $qt_{k}$, for every answer $a_{i}$, we use a randomized version of $\\\\bar{m}_{freq}[i]$, called $\\\\bar{m}_{ran}[i]$ in the following manner:\\n\\n$$\\\\bar{m}_{ran}[i] = \\\\mathcal{N}(\\\\bar{m}_{freq}[i], \\\\sigma)$$  \\\\hspace{1cm} (7)\\n\\nwhere $\\\\mathcal{N}$ is the Gaussian distribution, and $\\\\sigma$ is the standard deviation, which is a hyperparameter. This impedes the model from overcorrecting with respect to the rare answers, thus increasing its generalizability. Finally, we obtain the randomized margin $m_{ran}[i] = 1 - \\\\bar{m}_{ran}[i]$ for each $a_{i} \\\\in A$ in each $qt_{k}$.\\n\\nInstance-based margins. The frequency-based margins estimated from the training data are constant for a given question type. So, for two different questions starting with \\\"how many\\\", the margins are same for each answer, irrespective of the model's difficulty in answering these instances. Ideally if one of the samples is hard to answer, more margin should be given to its ground truth class compared to the easier one. Randomizing $m_{freq}$ as described above does not take the complexity of each sample into account. To this end, our model learns instance-level margins during training. We augment an auxiliary classifier.\"}"}
{"id": "CVPR-2023-1721", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Ehsan Abbasnejad, Damien Teney, Amin Parvaneh, Javen Shi, and Anton van den Hengel. Counterfactual vision and language learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10044\u201310054, 2020.\\n\\n[2] Aishwarya Agrawal, Dhruv Batra, and Devi Parikh. Analyzing the behavior of visual question answering models. arXiv preprint arXiv:1606.07356, 2016.\\n\\n[3] Aishwarya Agrawal, Dhruv Batra, Devi Parikh, and Anirudh Kembhavi. Don't just assume; look and answer: Overcoming priors for visual question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4971\u20134980, 2018.\\n\\n[4] Peter Anderson, Xiaodong He, Chris Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6077\u20136086, 2018.\\n\\n[5] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 2425\u20132433, 2015.\\n\\n[6] Hedi Ben-Younes, Remi Cadene, Nicolas Thome, and Matthieu Cord. Block: Bilinear superdiagonal fusion for visual question answering and visual relationship detection. In Proceedings of the AAAI conference on artificial intelligence, volume 33, pages 8102\u20138109, 2019.\\n\\n[7] Fadi Boutros, Naser Damer, Florian Kirchbuchner, and Arjan Kuijper. Elasticface: Elastic margin loss for deep face recognition. arXiv preprint arXiv:2109.09416, 2021.\\n\\n[8] Remi Cadene, Hedi Ben-Younes, Matthieu Cord, and Nicolas Thome. Murel: Multimodal relational reasoning for visual question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1989\u20131998, 2019.\\n\\n[9] Remi Cadene, Corentin Dancette, Matthieu Cord, Devi Parikh, et al. Rubi: Reducing unimodal biases for visual question answering. Advances in neural information processing systems, 32, 2019.\\n\\n[10] Kaidi Cao, Colin Wei, Adrien Gaidon, Nikos Arechiga, and Tengyu Ma. Learning imbalanced datasets with label-distribution-aware margin loss. Advances in neural information processing systems, 32, 2019.\\n\\n[11] Long Chen, Xin Yan, Jun Xiao, Hanwang Zhang, Shiliang Pu, and Yueting Zhuang. Counterfactual samples synthesizing for robust visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10800\u201310809, 2020.\\n\\n[12] Christopher Clark, Mark Yatskar, and Luke Zettlemoyer. Don't take the easy way out: ensemble based methods for avoiding known dataset biases. arXiv preprint arXiv:1909.03683, 2019.\\n\\n[13] Abhishek Das, Harsh Agrawal, Larry Zitnick, Devi Parikh, and Dhruv Batra. Human attention in visual question answering: Do humans and deep networks look at the same regions? Computer Vision and Image Understanding, 163:90\u2013100, 2017.\\n\\n[14] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4690\u20134699, 2019.\\n\\n[15] Tejas Gokhale, Pratyay Banerjee, Chitta Baral, and Yezhou Yang. Mutant: A training paradigm for out-of-distribution generalization in visual question answering. arXiv preprint arXiv:2009.08566, 2020.\\n\\n[16] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa matter: Elevating the role of image understanding in visual question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6904\u20136913, 2017.\\n\\n[17] Yangyang Guo, Liqiang Nie, Zhiyong Cheng, Feng Ji, Ji Zhang, and Alberto Del Bimbo. Adavqa: Overcoming language priors with adapted margin cosine loss. arXiv preprint arXiv:2105.01993, 2021.\\n\\n[18] Xinzhe Han, Shuhui Wang, Chi Su, Qingming Huang, and Qi Tian. Greedy gradient ensemble for robust visual question answering. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1584\u20131593, 2021.\\n\\n[19] Allan Jabri, Armand Joulin, and Laurens van der Maaten. Revisiting visual question answering baselines. In European conference on computer vision, pages 727\u2013739. Springer, 2016.\\n\\n[20] Chenchen Jing, Yuwei Wu, Xiaoxun Zhang, Yunde Jia, and Qi Wu. Overcoming language priors in vqa via decomposed linguistic representations. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11181\u201311188, 2020.\\n\\n[21] Kushal Kafle and Christopher Kanan. An analysis of visual question answering algorithms. In Proceedings of the IEEE International Conference on Computer Vision, pages 1965\u20131973, 2017.\\n\\n[22] Salman Khan, Munawar Hayat, Syed Waqas Zamir, Jianbing Shen, and Ling Shao. Striking the right balance with uncertainty. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 103\u2013112, 2019.\\n\\n[23] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. Advances in Neural Information Processing Systems, 33:18661\u201318673, 2020.\\n\\n[24] Jin-Hwa Kim, Jaehyun Jun, and Byoung-Tak Zhang. Bilinear attention networks. Advances in Neural Information Processing Systems, 31, 2018.\\n\\n[25] Gouthaman KV and Anurag Mittal. Reducing language biases in visual question answering with visually-grounded question encoder. In European Conference on Computer Vision, pages 18\u201334. Springer, 2020.\"}"}
{"id": "CVPR-2023-1721", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Aoxue Li, Weiran Huang, Xu Lan, Jiashi Feng, Zhenguo Li, and Liwei Wang. Boosting few-shot learning with adaptive margin loss. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\\n\\nZujie Liang, Weitao Jiang, Haifeng Hu, and Jiaying Zhu. Learning to contrast the counterfactual samples for robust visual question answering. In Proceedings of the 2020 conference on empirical methods in natural language processing (EMNLP), pages 3285\u20133292, 2020.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.\\n\\nWeiyang Liu, Yandong Wen, Zhiding Yu, Ming Li, Bhiksha Raj, and Le Song. Sphereface: Deep hypersphere embedding for face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 212\u2013220, 2017.\\n\\nZiwei Liu, Zhongqi Miao, Xiaohang Zhan, Jiayun Wang, Boqing Gong, and Stella X Yu. Large-scale long-tailed recognition in an open world. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2537\u20132546, 2019.\\n\\nYulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. Counterfactual vqa: A cause-effect look at language bias. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12700\u201312710, 2021.\\n\\nYulei Niu and Hanwang Zhang. Introspective distillation for robust question answering. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nDong Huk Park, Lisa Anne Hendricks, Zeynep Akata, Anna Rohrbach, Bernt Schiele, Trevor Darrell, and Marcus Rohrbach. Multimodal explanations: Justifying decisions and pointing to the evidence. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8779\u20138788, 2018.\\n\\nTingting Qiao, Jianfeng Dong, and Duanqing Xu. Exploring human-like attention supervision in visual question answering. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018.\\n\\nSainandan Ramakrishnan, Aishwarya Agrawal, and Stefan Lee. Overcoming language priors in visual question answering with adversarial regularization. Advances in Neural Information Processing Systems, 31, 2018.\\n\\nRamprasaath R Selvaraju, Stefan Lee, Yilin Shen, Hongxia Jin, Shalini Ghosh, Larry Heck, Dhruv Batra, and Devi Parikh. Taking a hint: Leveraging explanations to make vision and language models more grounded. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2591\u20132600, 2019.\\n\\nMeet Shah, Xinlei Chen, Marcus Rohrbach, and Devi Parikh. Cycle-consistency for robust visual question answering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6649\u20136658, 2019.\\n\\nJohan AK Suykens and Joos Vandewalle. Least squares support vector machine classifiers. Neural processing letters, 9(3):293\u2013300, 1999.\\n\\nDamien Teney, Ehsan Abbasnejad, and Anton van den Hengel. Unshuffling data for improved generalization. arXiv preprint arXiv:2002.11894, 2020.\\n\\nDamien Teney, Ehsan Abbasnejad, Kushal Kafle, Robik Shrestha, Christopher Kanan, and Anton Van Den Hengel. On the value of out-of-distribution testing: An example of goodhart\u2019s law. Advances in Neural Information Processing Systems, 33:407\u2013417, 2020.\\n\\nHao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5265\u20135274, 2018.\\n\\nZhiquan Wen, Guanghui Xu, Mingkui Tan, Qingyao Wu, and Qi Wu. Debiased visual question answering from feature and sample perspectives. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nJialin Wu and Raymond Mooney. Self-critical reasoning for robust visual question answering. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nZichao Yang, Xiaodong He, Jianfeng Gao, Li Deng, and Alex Smola. Stacked attention networks for image question answering. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 21\u201329, 2016.\\n\\nYundong Zhang, Juan Carlos Niebles, and Alvaro Soto. Interpretable visual question answering by visual grounding from attention supervision mining. In 2019 ieee winter conference on applications of computer vision (wacv), pages 349\u2013357. IEEE, 2019.\\n\\nXi Zhu, Zhendong Mao, Chunxiao Liu, Peng Zhang, Bin Wang, and Yongdong Zhang. Overcoming language priors with self-supervised learning for visual question answering. arXiv preprint arXiv:2012.11528, 2020.\"}"}
{"id": "CVPR-2023-1721", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tion head to the feature (the orange dashed box in Fig. 1) which is architecturally same as the original model classifier. We call this the bias-injecting component. Following Eq. 1, the formulation of this component becomes:\\n\\n\\\\[ f_b(v, q) = c_b(x) \\\\quad (8) \\\\]\\n\\nwhere, \\\\( f_b \\\\) refers to the logits generated by this component and \\\\( c_b \\\\) refers to the classification head. It is to be noted that while \\\\( c_b \\\\neq c \\\\), both are trained to predict the answers given the questions and the images. Finally, we denote \\\\( \\\\bar{m}_{\\\\text{ins}} = \\\\text{softmax}(f_b/\\\\tau) \\\\) as the confidence, (i.e. prediction probability) of the bias-injecting component in answering a question. Here \\\\( \\\\tau \\\\) stands for temperature which is a hyperparameter to the network. We define \\\\( m_{\\\\text{ins}} = 1 - \\\\bar{m}_{\\\\text{ins}} \\\\) as the instance-level learnable margin, obtained from the model during training. These learned margins are similar with the frequency-based margins when samples are far away from the decision boundary. However, for training instances that are close, \\\\( m_{\\\\text{ins}} \\\\) increases the margins if the model confidence is low for the ground truth class, and decreases the same in case the model has high confidence in predicting the ground truth answer. We combine this instance-level confidence for the ground truth class with the previously calculated \\\\( m_{\\\\text{ran}} \\\\) for each sample \\\\((v, q)\\\\) to obtain the final margins \\\\( m_{\\\\text{comb}} \\\\), as shown in the blue dashed box in Fig. 1.\\n\\n\\\\[ m_{\\\\text{comb}}[\\\\text{gt}] = \\\\beta m_{\\\\text{ran}}[\\\\text{gt}] + (1 - \\\\beta) m_{\\\\text{ins}}[\\\\text{gt}] \\\\quad (9) \\\\]\\n\\n\\\\( \\\\text{gt} \\\\) refers to the index of the ground truth class of the sample, and \\\\( \\\\beta \\\\) is a hyperparameter used for combining the two margins. For all indices \\\\( l \\\\neq \\\\text{gt} \\\\), \\\\( m_{\\\\text{comb}}[l] = m_{\\\\text{ran}}[l] \\\\). In the first few training epochs, we do not compute \\\\( m_{\\\\text{ins}} \\\\) as the model is in the initial stage of its learning, i.e. we set \\\\( \\\\beta = 1 \\\\). As training progresses, \\\\( \\\\beta \\\\) is decreased in a step-wise manner to increase the contributions from the learnt margins.\\n\\nThe bias-injecting component. The bias-injecting component is a classifier appended to the features \\\\( x \\\\) trained using the standard CE loss. As the CE loss is known to favour the frequently appearing answers in the training data, it automatically learns the bias in the dataset, and by backpropagating this bias into the network, it clusters samples in the feature space based on the source of the bias. This aids the margin loss as it can now separate the (frequent, rare), and the (easy, difficult) samples inside the individual clusters. Moreover, the advantage of using it to generate the learnable margins is that the bias captured is now reflected directly in \\\\( \\\\bar{m}_{\\\\text{ins}} \\\\), which ensures that it is close to \\\\( \\\\bar{m}_{\\\\text{freq}} \\\\), but still provides information on the answering difficulty of a samples lying close to the decision boundary. Another advantage of this component is that by learning the data bias, this module is capable of generalizing to any in-distribution data, whereas the predictions from the primary classifier \\\\( c_f \\\\) favour theood data.\\n\\nInference. While margin losses are effective on anood test data, they result in degraded performance on id test set. To mitigate this problem, during inference time, we take advantage of both the bias-injecting component \\\\( f_b \\\\) and the primary logit head \\\\( f \\\\), as the former is useful in making predictions for an id test data, while the latter is useful in anood setting. This makes our model robust to different data distributions. We denote by \\\\( \\\\hat{p}_{\\\\text{comb}} \\\\) the combined predicted answer probabilities. This is obtained by combining the predictions from \\\\( c \\\\) and \\\\( c_b \\\\) as defined below:\\n\\n\\\\[ \\\\hat{p}_{\\\\text{comb}} = \\\\alpha \\\\hat{p}_c + (1 - \\\\alpha) \\\\hat{p}_b \\\\quad (10) \\\\]\\n\\nwhere \\\\( \\\\alpha \\\\) is a hyperparameter controlling the weight of each logit head during inference, \\\\( \\\\hat{p}_c = \\\\text{softmax}(f) \\\\) are the predictions from classifier \\\\( c \\\\), \\\\( \\\\hat{p}_b = \\\\text{softmax}(f_b/\\\\tau) \\\\) are the predictions from the bias-injecting component \\\\( c_b \\\\). The answer predicted is therefore \\\\( \\\\hat{a} = \\\\text{argmax}(\\\\hat{p}_{\\\\text{comb}}) \\\\). The temperature \\\\( \\\\tau \\\\) is same as that defined previously for the instance-based margins.\\n\\nSupervised Contrastive (SupCon) Loss. We further separate samples in the feature space based on the SupCon loss. In addition to creating discriminative features for (frequent, rare) and (easy, difficult) samples, the SupCon loss leads to a degree of discrimination among samples with different ground truth answers in the feature space. We consider a mini-batch of size \\\\( B \\\\) of multimodal features denoted as \\\\( \\\\{x_1, x_2, \\\\ldots, x_B\\\\} \\\\) and corresponding answers as \\\\( \\\\{a_1, a_2, \\\\ldots, a_B\\\\} \\\\). If we consider the current sample with index \\\\( j \\\\), the set of positive examples from the mini-batch is denoted by \\\\( P_j: \\\\{i \\\\in B \\\\text{ s.t. } a_i = a_j\\\\} \\\\). Similarly, the set of negative examples is denoted by \\\\( N_j: \\\\{i \\\\in B \\\\text{ s.t. } a_i \\\\neq a_j\\\\} \\\\). The SupCon loss [23] is defined as:\\n\\n\\\\[ L_{\\\\text{sup-\\\\text{con}}} = \\\\sum_{j \\\\in B} -\\\\frac{1}{|P_j|} \\\\sum_{p \\\\in P_j} \\\\log \\\\exp\\\\left(\\\\frac{x_j^T x_p}{\\\\tau}\\\\right) \\\\sum_{n \\\\in N_j} \\\\exp\\\\left(\\\\frac{x_j^T x_n}{\\\\tau}\\\\right), \\\\quad (11) \\\\]\\n\\nWe set the temperature \\\\( \\\\tau = 1 \\\\) for all experiments as we do not find any significant improvements by changing its value.\\n\\nTotal Loss: The final loss function for each sample is:\\n\\n\\\\[ L = L_{\\\\text{Angular}}(m_{\\\\text{comb}}) + L_s + L_{\\\\text{sup-\\\\text{con}}} \\\\quad (12) \\\\]\\n\\nwhere \\\\( L_{\\\\text{Angular}} \\\\) is the angular margin loss, \\\\( L_s \\\\) is the CE loss training the bias injecting component, \\\\( L_{\\\\text{sup-\\\\text{con}}} \\\\) is the SupCon loss. Our final model can be seen in Fig. 1. We name our overall method RMLVQA.\\n\\n4. Experiments\\n\\nDataset details. We evaluate our model on the VQA-CP v2 dataset (Visual Question Answering Under Changing Priors) [3], which is anood benchmark, based on the standard evaluation metric shown by Antol et al. [5]. It was created by reorganizing the training and validation sets of the VQA v2 [16] dataset, ensuring that the distribution of answers and images is consistent over time.\"}"}
{"id": "CVPR-2023-1721", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of answers for each question type in the training set and the test set are different. The training set of VQA-CP v2 contains approximately 121k images and 438k questions, the test set contains approximately 98k images and 220k questions. Following previous works, we also evaluate our model on the VQA v2 validation set.\\n\\n4.1. Results\\n\\nQuantitative analysis.\\n\\nIn Table 1, we show a comparative analysis of the performance of RMLVQA on both VQA-CP v2 test and VQA v2 validation sets. We report the overall accuracies along with those on \\\"yes/no\\\", \\\"number\\\" and \\\"other\\\" type questions. We observe that our method gains around 6.39% in the overall accuracy for VQA-CP in comparison with AdaVQA [17], which is the cosine margin loss trained model. We observe that AdaVQA, while performing well on VQA-CP v2, has considerably low scores for the in-domain VQA v2. With RMLVQA, the validation accuracy of VQA v2 is 59.99%, which is 13.01% higher than AdaVQA. This shows that our method is the first one trained by adaptive margins that is robust to both id andood data.\\n\\nWith a score of 60.41% for VQA-CP v2, the model improves upon the current state-of-the-art augmentation-free models. In addition to reporting the performance of the various models in literature on the two datasets, we show the relative difference in accuracies between VQA-CP v2 test data and VQA v2 validation data. RMLVQA outperforms all other non-augmentation based models in terms of robustness, with the lowest difference in the two accuracies (see Table 1), indicating that our method is the most robust compared to all other methods that do not use any augmentation, and perform equally well on both VQA-CP v2 and VQA v2. For all experiments, Updn [4] is our base network.\\n\\nQualitative analysis.\\n\\nWe further demonstrate the effectiveness of our method in Fig. 2 which provides qualitative results on the VQA-CP v2 test set for RMLVQA and the base network UpDn. In the first row, we show examples for question type \\\"what color\\\". The base network UpDn [4] focuses on the incorrect region in the image, given by attention map scores, and outputs \\\"blue\\\", the most frequent answer in the training set for the given question type. The best scoring region from the attention maps of our model is localized at the correct region in the image, and it correctly outputs \\\"red\\\", which is not a frequent answer in \\\"what color\\\". In the second example, belonging to question type \\\"what is the person\\\", we see that while the baseline outputs \\\"phone\\\", which is a frequent class under that question type, RMLVQA predicts the correct answer \\\"hat\\\", which is a rare class, thus showing the effectiveness of our method. While the baseline outputs incorrect answers, it still focuses on the correct region in the image. This crucial observation indicates that the bias in the model weights do not allow this visual information to propagate forward. Thus it still picks from the frequent answers for the given question type.\\n\\nChoice of hyperparameter $\\\\alpha$ for inference time combination of prediction probabilities.\\n\\nChoice of $\\\\alpha$ is crucial as it decides how much weight to put on the prediction probabilities of the bias-injecting component $c_b$ and the primary classifier $c$. As VQA-CP v2 does not have a validation set, we choose $\\\\alpha = 0.5$ for fair evaluation. However, we report the accuracies of VQA-CP v2 test and VQA v2 validation sets for different values of $\\\\alpha$ in Table 2. We observe that $\\\\alpha = 0.5$ leads to the most robust performance of our method on both id andood data, where the relative gap of accuracies is just 0.42%. While we already choose the optimal value of $\\\\alpha$ for reporting model performance, our experiments demonstrate that one can choose an appropriate value to control the tradeoff between id andood performances.\\n\\n4.2. Ablation studies\\n\\nIn this subsection, we discuss the role of each component of RMLVQA. We evaluate them on the VQA-CP v2 test and VQA v2 validation data and show the results in Table 3.\\n\\nEffectiveness of angular margins.\\n\\nWe call the base adaptive angular margin loss trained model RMLVQA-Base, which only considers the frequency based margins. Compared to AdaVQA, it leads to a rise of 3.22% in the VQA-CP test accuracy and an increase of 2.74% in the accuracy of the VQA-v2 validation set. This shows the effectiveness of the angular margin loss over the cosine margins both in id andood data, validating the choice of the angular margin loss for addressing the language bias problem of VQA.\\n\\nEffect of randomization of the margins.\\n\\nRandomizing the margins of each answer leads to an improvement in the accuracy values of VQA-CP v2 test, as shown in Table 3. Further, we also notice that it improves the in-domain accuracy of VQA v2 by a large margin (7.12%), thus showing its effectiveness in generalizability.\\n\\nRole of the learnable margins.\\n\\nAs explained earlier, $m_{ins}$ represents the confidence of the bias-injecting component in answering a question. We show the positive effect of adding these margins to $m_{ran}$ in Table 3 for VQA-CP v2, with a slight loss in VQA v2. In Fig. 4 we show the Spearman's rank correlation coefficient $\\\\rho$ between $m_{freq}$ and $m_{ins}$ for the question type 'how many' for our method (for the angular and cosine margin losses defined in Sec 3 and AdaVQA [17] respectively), averaged over the relevant samples in each training epoch. We note the following: a) $\\\\rho$ is high throughout, indicating that the order of margin values in $m_{ins}$ for a certain sample remains similar to that of the margin values in $m_{freq}$ for the question type to which the sample belongs. b) $\\\\rho$ decreases over epochs, indicating the excess information being carried by $m_{ins}$, aiding in the discrimination of samples whose answer frequencies are similar, but have different complexities.\\n\\nRole of the bias-injecting component.\\n\\nIn Table 3, we show that the bias-injecting component increases the model's overall accuracy by 1.47% for VQA-CP v2 (3.65% for VQA v2).\"}"}
{"id": "CVPR-2023-1721", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Accuracy comparisons with other methods on the VQA-CP v2 and VQA v2 datasets. The methods have been grouped into different categories (separated by lines in the table): base approaches, methods using human annotations, those that modify the language encoder, models weakening the language bias in different ways, our approach RMLVQA trained with UpDn as backbone, and finally the augmentation based methods. The best performance in each column is highlighted in bold. We highlight the overall accuracy of RMLVQA on VQA-CP with an underline as it achieves state-of-the-art on augmentation-free methods. * denotes numbers shown in [17]. We report the average accuracy of our model over 5 random seeds (along with standard deviations in the superscript). Columns ending with \"-CP\u201d denote accuracies for VQA-CP v2, others denote accuracies for VQA v2. \u2020 indicates our implementation.\\n\\n| Model   | Y/N-CP | Num-CP | Others-CP | Overall-CP | Diff |\\n|---------|--------|--------|-----------|------------|------|\\n| SAN     | 38.16  | 35.16  | 11.74     | 21.77      | 24.96|\\n| GVQA    | 57.99  | 13.68  | 22.14     | 31.30      | 34.65|\\n| UpDn    | 42.27  | 11.93  | 46.05     | 39.74      | 42.45|\\n| S-MRL   | 42.85  | 12.81  | 43.22     | 38.46      | 41.96|\\n| AttAlign| 43.02  | 11.89  | 45.00     | 41.37      | 44.13|\\n| HINT    | 67.27  | 10.61  | 45.88     | 46.73      | 48.65|\\n| SCR     | 70.41  | 10.42  | 47.29     | 48.47      | 50.68|\\n| VGQE    | 66.35  | 27.08  | -         | 64.04      | -    |\\n| DLR     | 70.99  | 18.72  | 45.57     | 48.54      | 50.73|\\n| AdvReg  | 65.49  | 15.48  | 35.48     | 37.96      | 40.58|\\n| RUBi    | 68.65  | 20.28  | 43.18     | 47.17      | 49.58|\\n| LMH     | 70.29  |        | 44.10     | 44.86      | 47.62|\\n| CF-VQA  | 90.61  | 21.50  | 45.05     | 55.13      | 57.31|\\n| IntroD  | 90.79  | 17.92  | 46.92     | 54.84      | 57.06|\\n| GGE-DQ  | 87.04  | 27.75  | 49.59     | 57.32      | 59.51|\\n| AdaVQA  | 70.83  | 49.00  | 46.29     | 46.29      | 49.00|\\n| RMLVQA  | 89.98  | \u00b10.46  | 45.96     | 48.74      | 51.52|\\n\\nFigure 2. Qualitative analysis of RMLVQA. We show two images from two question types in the rows. Column (a) represents the ground truth answers, column (b) represents the answers predicted by the baseline UpDn model. Column (c) represents the answer predicted by our method. The bounding boxes show the highest scored region in the attention map of the individual models.\"}"}
{"id": "CVPR-2023-1721", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. TSNE visualization of the feature space with respect to different question types for the model components, evaluated incrementally: a) RMLVQA-Base, b) Gaussian randomization, c) Bias injection.\\n\\nFigure 4. Spearman's rank correlation coefficient between \\\\( m_{freq} \\\\) and \\\\( m_{ins} \\\\) over epochs for question type \u201chow many\u201d on VQA-CP v2 test set.\\n\\nTable 2. Role of \\\\( \\\\alpha \\\\) in the inference stage of RMLVQA.\\n\\n| \\\\( \\\\alpha \\\\) | VQA-CP v2 Test | VQA-v2 Val | Gap |\\n| --- | --- | --- | --- |\\n| 1.0 | 60.54 | 58.16 | 2.38 |\\n| 0.8 | 60.50 | 58.83 | 1.67 |\\n| 0.6 | 60.33 | 59.57 | 0.76 |\\n| 0.4 | 60.41 | 59.99 | 0.42 |\\n| 0.2 | 60.87 | 60.46 | 0.59 |\\n| 0.0 | 60.48 | 61.26 | 22.06 |\\n\\nTable 3. Ablations. In this table we show the ablations of RMLVQA, evaluated on VQA-CP v2 test & VQA v2 val.\\n\\n| Model | Y/N | Num Others | Overall VQA v2 Val |\\n| --- | --- | --- | --- |\\n| RMLVQA-Base | Y | N | 49.62 |\\n| +Randomization | Y | N | 47.83 |\\n| +Bias Injection | Y | N | 55.59 |\\n| +Learnable Margin | Y | N | 74.87 |\\n| +SupCon Loss | Y | N | 99.59 |\\n| -Backprop | Y | N | 59.67 |\\n\\nThe effectiveness of this component can be understood from Fig. 3, where we show the TSNE visualizations of how answers from different question types are placed in the feature space for the components of RMLVQA. Fig. 3(a) shows that while for some question types like \u2018how many\u2019, \u2018are\u2019, answers are well separated, some others like \u2018do\u2019, \u2018are they\u2019, \u2018is there\u2019 are close, out of which some are entangled. Although the randomization increases flexibility of the adaptive margins, it does not disentangle these question types, as seen in Fig. 3b. Finally, in Fig. 3(c), we see that the bias-injecting component aids in separating the answers according to their question types. This is because the bias sources of VQA-CP is its question types, and hence this component clusters samples in the training set based on the same. This is crucial, as with more entanglement, the model tends to answer questions of one question type with the answers of another type. In the last row of Table 3, we also observe the effect of removing the backpropagation of the \\\\( CE \\\\) loss (used to train the bias-injecting component) into the main network for VQA-CP v2. The classifier weights in the bias-injecting component learn the language bias in the dataset and backpropagate the same into the network, thus helping the model separate the answers across different question types in the feature space. We utilize this knowledge in the inference stage, where we combine the two logit heads, thus helping the model to generalize to both id andood test data, as is evident from Tables 1 and 2.\\n\\nRole of the SupCon Loss. The addition of the SupCon loss is shown to improve the performance of our model empirically for both VQA-CP and VQA v2 as shown in Table 3.\\n\\n5. Conclusion\\n\\nAlthough margin-based losses are useful in mitigating language biases in VQA, they may not distinguish between answers of different task-complexity levels in the feature space. To distinguish between samples having similar answer frequency, but different complexity, we learn instance-based margins for each sample from the model during training. We introduce a bias-injecting component in our model and utilize its confidence in answer prediction as the instance-level margins. Being trained by the \\\\( CE \\\\) loss, this component further clusters the training samples based on the source of the data bias, guiding the margin loss to distinguish frequent and rare answers inside each cluster. We ensemble the outputs of the bias-injecting component with those of the main model during inference to achieve state-of-the-art results among existing augmentation-free methods on theood VQA-CP v2 dataset, while being the most robust with respect to both id andood test sets.\\n\\n6. Acknowledgments\\n\\nThis work was supported by a research grant (CRG/2021/005925) from SERB, DST, Govt. of India. Abhipsa Basu is supported by the PMRF fellowship.\"}"}
