{"id": "CVPR-2022-601", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Escaping Data Scarcity for High-Resolution Heterogeneous Face Hallucination\\n\\nYiqun Mei*, Pengfei Guo*, V ishal M. Patel\\nJohns Hopkins University\\n\\nAbstract\\n\\nIn Heterogeneous Face Recognition (HFR), the objective is to match faces across two different domains such as visible and thermal. Large domain discrepancy makes HFR a difficult problem. Recent methods attempting to fill the gap via synthesis have achieved promising results, but their performance is still limited by the scarcity of paired training data. In practice, large-scale heterogeneous face data are often inaccessible due to the high cost of acquisition and annotation process as well as privacy regulations. In this paper, we propose a new face hallucination paradigm for HFR, which not only enables data-efficient synthesis but also allows to scale up model training without breaking any privacy policy. Unlike existing methods that learn face synthesis entirely from scratch, our approach is particularly designed to take advantage of rich and diverse facial priors from visible domain for more faithful hallucination. On the other hand, large-scale training is enabled by introducing a new federated learning scheme to allow institution-wise collaborations while avoiding explicit data sharing. Extensive experiments demonstrate the advantages of our approach in tackling HFR under current data limitations. In a unified framework, our method yields the state-of-the-art hallucination results on multiple HFR datasets.\\n\\n1. Introduction\\n\\nDeep convolutional neural networks have led to unprecedented success on visual face recognition [1, 2, 3, 4], where state-of-the-art methods achieve more than 99% accuracy on multiple benchmarks. These near-perfect performances come from both well-elaborated architectures and exhaustive training on massive datasets. Nevertheless, in many real-world scenarios with low-visibility, such as low-light and night-time, it is often infeasible to obtain clear visible (VIS) images. Under these circumstances, sensors deployed for other imaging spectra, e.g. Thermal (TH), can capture more discriminative information and serve as a more reliable solution. This raises a great need of heterogeneous face recognition (HFR) [5, 6, 7], an important task in computer vision and biometrics, that matches images from TH modality to its VIS counterpart. The HFR problem has numerous applications in surveillance, monitoring and security.\\n\\nUnfortunately, due to the large domain discrepancy, naively deploying a state-of-the-art face recognition algorithm trained on VIS images often leads to poor performance on a TH dataset [8]. Over the past decade, tremendous efforts have been spent to address the HFR problem by either learning domain-invariant features [9, 10, 11, 12] or finding a common subspace [13, 14, 15, 16]. Owing to the rapid progress in Generative Adversarial Networks (GANs) [17, 18], most recent methods [19, 20, 21, 22, 23, 24] reformulate HFR as a face synthesis/translation problem. The resulting \\\"recognition via hallucination\\\" scheme embraces a huge benefit that any off-the-shelf recognizer can be directly applied on the synthesised images.\\n\\nWhile these synthesis-based approaches fill the gap to some extent, the produced VIS images are still unsatisfactory, often accompanied with distorted and incorrect facial structures (shown in Figure 4), which significantly degrades the recognition accuracy. We found that the bottleneck is likely due to the limited size of the dataset which fails to offer sufficient information to guide image synthesis. Unlike visible images that are easy to obtain and widely available over the Internet [25, 26], collecting and annotating a large-scale high-quality TH dataset is difficult. Challenges stem from many aspects. First, the acquisition process is both time-consuming and costly, which often requires laborious setup and non-trivial calibrations [27, 28]. Second, the diversity of collected data can be limited. Due to the physical constraints, it is typically infeasible for a single institution to collect a comprehensive dataset that covers a diverse set of identities with various attributes, such as race, gender and age. Most existing datasets [1, 2, 3, 4] are confined to a small number of subjects, leading to biased results and over-fitting. Third, face data is privacy sensitive. Since it contains subjects' personal identification information, one has to deal with privacy concerns when collecting and storing them, making it...\"}"}
{"id": "CVPR-2022-601", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"difficult to share the data with other institutions. Besides poor synthesis quality, most existing methods can only process images at a resolution no more than $128 \\\\times 128$. This not only leads to visually unappealing results, but also reduces their applicability in many downstream tasks that depend on high-resolution inputs, such as face parsing \\\\[38, 39\\\\], editing \\\\[74\\\\] and reenactment \\\\[53\\\\].\\n\\nIn this paper, we present a unified hallucination framework for HFR, that is capable of synthesizing high-resolution visible faces ($512 \\\\times 512$) from low-resolution heterogeneous data (i.e. smaller than $128 \\\\times 128$), with superior realness and higher fidelity. Our approach consists of two separate strategies. The first one comes with a new generation paradigm inspired by the recent success in GAN inversion \\\\[68\\\\]. The core idea is to leverage rich and diverse facial priors from the visible domain to eschew the need of learning generation from scratch. This is accomplished by embedding a pretrained GAN (e.g. StyleGAN \\\\[25, 26\\\\]) as a facial decoder which hallucinates visible faces conditioned on the latent representations of a U-shaped encoder. The encoder is carefully designed with a novel Multi-scale Contexts Aggregation (MSCA) mechanism which merges scale-wise information to enhance representation. MSCA offers better fine-grained generation control and is pivotal for preserving identity information. The proposed method, called \\\\textit{Visual Prior enhanced GAN} (VPGAN), can break the underlying data limitation by producing faces with state-of-the-art accuracy and photo-realism.\\n\\nDeep models are data hungry and VPGAN may be further improved with large-scale training. However, in practice, HFR data tends to be separately collected and dispersed among different institutions. Due to privacy concerns, one cannot simply share the data for centralized training. To this end, our second strategy introduces Federated Learning (FL) \\\\[29\\\\] to further improve HFR, which enables collaborative model training while avoiding explicit data exchanging. Specifically, we allow each institution to perform local training on their private HFR data and deploy a centralized server to periodically communicate with each institution, aggregating local models and updating a global model. The whole process does not involve any data transfer but benefits deep models a lot by integrating information from a significantly broader range of data. To make our approach more suitable for real-world HFR, we have to tackle the heterogeneous data distributions across institutions. This challenge, which may be caused by differences in sensor types or acquisition protocols, can lead to locally skewed updates, resulting in slow convergence and sub-optimal performance \\\\[31, 35\\\\]. To tackle this issue, we build our FL algorithm based on a new Model Proximity Regularization (MPR), which corrects local gradient updates by constraining the discrepancy between the latent representations from the global and local models. As a result, our approach achieves superior robustness towards non-ideal data distribution, implying its applicability for solving real-world HFR problems.\\n\\nIn our unified framework, we integrate VPGAN as the basic component and use it in the proposed FL scheme. We term this new framework VPFL. In the experiment section, we demonstrate that our approach generates VIS faces at high-resolution with superior realness and accuracy.\\n\\nWithin the infrared spectrum, various modalities have been explored for thermal-to-visible (TH-VIS) face recognition. These include Near Infrared (NIR), Short-Wave Infrared (SWIR), Mid-Wave Infrared (MWIR) and Long-Wave Infrared (LWIR). The use of a particular thermal modality depends on the application. For instance, in long-range surveillance applications, SWIR or MWIR modalities are often used. Unlike NIR images, which are close to the VIS spectrum images, LWIR images are often acquired in low-resolution with many facial details missing on the captured imagery. This is well-reflected by the performance drop of the existing recognition methods on such datasets \\\\[23, 51\\\\]. Also, greater than 99% accuracy has been reported on many NIR face datasets \\\\[8, 12\\\\]. However, the performance of various HFR methods on LWIR data is significantly low \\\\[23, 51\\\\].\\n\\nIn summary, the main contributions of our paper are:\\n\\n\u2022 We propose a new data-efficient generation scheme for HFR. Thanks to the powerful visual priors, it manages to alleviate fundamental data challenges, resulting in superior hallucination results.\\n\\n\u2022 We introduce a unified framework to make large-scale training possible in real-world scenarios. VPFL makes multi-institutional collaborations possible without raising any privacy concerns.\\n\\n\u2022 Extensive experiments show that our method can produce faces with state-of-the-art photo-realism and fidelity, which in turn significantly boosts the recognition accuracy. These merits show its great potential to serve as a universal solution towards real-world applications.\\n\\n2. Related Work\\n\\nHeterogeneous Face Recognition. Compared to the methods relying on feature/subspace learning, recognition via synthesis \\\\[8, 11\u201313, 41, 75, 76\\\\] has received significant attention, because off-the-shelf face recognition algorithms can be applied to the synthesized images. Our discussion will focus on these types of methods. Early deep learning-based approaches directly learn a CNN for cross-spectral mapping. For example, Lezama et al. \\\\[30\\\\] train a CNN for NIR-VIS and improve results with low-rank assumption. Riggan et al. \\\\[56\\\\] enhance the discriminative quality of the synthesized images by modeling both global and local regions. Recent methods leverage GANs to improve the hallucination qualities. Zhang et al. \\\\[76\\\\] propose GAN-VFS that jointly learns semantic rich features and facial reconstruction, which...\"}"}
{"id": "CVPR-2022-601", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. The proposed VPGAN. It adopts an encoder-decoder structure. The UNet encoder extracts style codes as well as multi-scale representations and then transmits them to the decoder for generation control. MSCA enhances the encoder by merging multi-scale information, which proves to be crucial for accurate hallucination. A pre-trained StyleGAN serves as the facial decoder and generates the desired visible face. A is the linear transformation of the style codes and B plays a similar role as that of noise injection, which results in more photo-realistic and accurate generation. Further improvements are based on cycle-consistency, more advanced loss, and attention mechanism.\\n\\nOur method is related to GAN inversion which relies on pre-trained GAN priors for better image manipulation and restoration. Early approaches explicitly \u201cinvert GANs\u201d, which iteratively find the closest latent code of a targeting image. For example, PULSE for photo upsampling gradually searches the correct latent code of a StyleGAN by optimizing a downsampling loss. More recent methods use a DNN encoder and learn to predict the latent code in one forward pass. Our work is inspired by these approaches but exploits its ability in transferring visual priors for data-efficient heterogeneous face hallucination.\\n\\nFederated Learning is a decentralized machine learning framework which leverages data from multiple institutions or users to collaboratively train a global model without directly sharing their local data. Addressing heterogeneous data distribution across devices or institutions in the real-world deployment of FL applications draws emerging attention. Several FL methods targeting on this issue are built upon FedAvg, FedProx, and Agnostic Federated Learning. FedDyn is proposed to address the issue that there is an inconsistency between minima of the local model loss and those of the global loss by introducing a dynamic regularizer in each client. While those works conduct rigorous theoretical analysis, their performance is not validated on practical applications. Recently, Aggarwal et al. proposed face recognition methods based on the FL framework. However, it is worth noting that the multi-institutional collaborative approach based on FL for face hallucination has not been well studied in the literature.\\n\\n3. Proposed Method\\n\\nIn this section, we first formulate the TH-VIS face hallucination problem and then describe our method in detail. Given a TH image $I_{TH}$, our goal is to reconstruct a VIS face $I_{VIS}$ by learning a mapping function $I_{VIS} = F(I_{TH})$.\\n\\nThe synthesized face $I_{VIS}$ should be both visually realistic and accurate, and thus can be used for face matching. As discussed earlier, due to various reasons, $I_{TH}$ is often captured in low-resolution. Unlike existing methods (which only synthesize faces at $128 \\\\times 128$), our work performs joint translation and upsampling. To the best of our knowledge, this is the first work that can hallucinate high-resolution faces ($512 \\\\times 512$) in HFR.\\n\\n3.1. VPGAN\\n\\nConventional methods learn synthesis entirely from paired datasets. Due to data limitations, they can hardly output clear and high-quality images. Our method instead only learns to control generation by making use of diverse visual priors encapsulated in a pre-trained GAN. We leverage off-the-shelf StyleGAN pretrained on FFHQ, which contains 70,000 high-resolution faces. As shown in Figure 1, VPGAN adopts an encoder-decoder architecture. Only the encoder is required to train with the HFR dataset for guiding the hallucination. To output a visible face, we first extract global style codes $w = MLP(UNet_E(I_{TH}))$, where $UNet_E$ and MLP are the encoder part of the UNet and fully connected layers, respectively.\"}"}
{"id": "CVPR-2022-601", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"set of multi-scale features $F$ from every stage of the decoder for fine-grained generation control, i.e., $F_i = UNet(D_i(I_{TH}))$.\\n\\nA visible face can then be produced via $I_{VIS} = S(w, \\\\{F_1, ..., F_n\\\\})$, where $S$ is the StyleGAN decoder. Thanks to diverse visual priors such as face geometry, color and texture, VPGAN is able to alleviate the need of large datasets and yields more faithful results.\\n\\nImproved UNet Encoder. U-shaped structure has shown great capability in obtaining semantic-rich representations. However, we found that a naive UNet is incapable of generating accurate local structures, which are crucial for face-matching. This is because features in the decoder are upsampled from coarser scales, and thus lack sufficient fine-grained information. Errors from early stages are also transmitted to the subsequent layers, leading to incorrect synthesis. To this end, we improve the conventional UNet with a new Multi-Scale Contexts Aggregation (MSCA) mechanism, which provides a comprehensive encoding of the input image at multiple scales. This ensures the resulting features at all levels contain both coarse and fine information and thus leads to better generation control. As shown in Figure 1, MSCA computes an output by adaptively merging multi-scale features from the UNet encoder (after first upscaling and down-scaling to match the spatial dimensions). Formally, the output features at $i$-th level can be expressed as follows:\\n\\n$$\\\\text{out}_i = \\\\text{MSCA}(E_1^\\\\uparrow, ..., E_i^\\\\downarrow, ..., E_n^\\\\downarrow)$$\\n\\nwhere $\\\\uparrow$ and $\\\\downarrow$ denote the upscaling and downscaling operation, respectively. We will demonstrate that this simple design is crucial for accurate face matching in Section 4.2.1.\\n\\nEmbedded Visual Prior. Our key design is to utilize visual priors embedded in a pre-trained GAN. The StyleGAN decoder stores diverse facial knowledge and acts similar to a memory bank or dictionary, where the extracted style codes (from encoder) query desired faces. The style codes $w$ can be incorporated either in a similar way to [26], by directly applying the modulation and demodulation operations on the convolution kernel of each style block, or through AdaIN as used in [25]. Either operation is easy to implement and results in good generation quality. The multi-scale features from the UNet act in a similar way as the noise injection in the original StyleGAN. But rather than achieving localized variation, here we want to control detailed facial components to be consistent with the input image. We achieve this via a modulation operation (B in Figure 1) similar to [48, 64]. Specifically, we compute pixel-wise scale and shift parameters $\\\\gamma_i$ and $\\\\beta_i$ from the $i$-th multi-scale feature $F_i$ via a simple $1 \\\\times 1$ convolution layer. And then we use it to calibrate the output feature from the $i$-th StyleGAN layer:\\n\\n$$S_{+i} = \\\\gamma_i \\\\odot S_{-i} + \\\\beta_i$$\\n\\nwhere $\\\\odot$ denotes Hadamard product and $S_{-i}$ is the pre-modulated feature. After calibration, the resulting $S_{+i}$ is passed to the next stage for subsequent generation.\\n\\nTraining Objectives. To ensure both realness and fidelity, our training objective consists of four terms: (1) reconstruction loss $L_r$, (2) adversarial loss $L_{adv}$, (3) perceptual loss $L_p$, and (4) Identity Loss $L_{id}$. The overall loss can be expressed as follows:\\n\\n$$L_{gen} = L_r + \\\\lambda_a L_{adv} + \\\\lambda_b L_p + \\\\lambda_c L_{id}$$\\n\\nwhere $\\\\lambda_a$, $\\\\lambda_b$ and $\\\\lambda_c$ are corresponding balancing parameters. We define the reconstruction loss as the standard $L_1$ distance between the synthesized image $I_{VIS}$ and ground-truth image $I_{GT}$ to ensure content consistency. The adversarial loss is directly inherited from StyleGAN for more sharp generation. To improve visual quality while preserving identity, we further adopt the perceptual loss and the identity loss. Both can be expressed as feature-wise distance of a given CNN (e.g., a pre-trained VGG):\\n\\n$$L_p, L_{id} = 1_{H_iW_iC_i} \\\\| V_i(I_{GT}) - V_i(I_{VIS}) \\\\|_1,$$\\n\\nwhere $V$ is the corresponding CNN. $H_i, W_i, C_i$ are the height, width and channel number of the $i$-th feature map in $V$. Here, we use an ImageNet pre-trained VGG for $L_p$ and a simple ArcFace for $L_{id}$.\"}"}
{"id": "CVPR-2022-601", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] The buaa-visnir face database instructions. 2012.\\n\\n[2] Durmus Alp Emre Acar, Yue Zhao, Ramon Matas, Matthew Mattina, Paul Whatmough, and Venkatesh Saligrama. Federated learning based on dynamic regularization. In International Conference on Learning Representations, 2020.\\n\\n[3] Divyansh Aggarwal, Jiayu Zhou, and Anil K Jain. Fedface: Collaborative learning of face recognition model. arXiv preprint arXiv:2104.03008, 2021.\\n\\n[4] Kelvin CK Chan, Xintao Wang, Xiangyu Xu, Jinwei Gu, and Chen Change Loy. Glean: Generative latent bank for large-factor image super-resolution. In IEEE Conference on Computer Vision and Pattern Recognition, pages 14245\u201314254, 2021.\\n\\n[5] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 4690\u20134699, 2019.\\n\\n[6] Xing Di, Benjamin S Riggan, Shuowen Hu, Nathaniel J Short, and Vishal M Patel. Polarimetric thermal to visible face verification via self-attention guided synthesis. In IEEE International Conference on Biometrics, pages 1\u20138, 2019.\\n\\n[7] Xing Di, He Zhang, and Vishal M Patel. Polarimetric thermal to visible face verification via attribute preserved synthesis. In IEEE International Conference on Biometrics Theory, Applications and Systems, pages 1\u201310, 2018.\\n\\n[8] Boyan Duan, Chaoyou Fu, Yi Li, Xingguang Song, and Ran He. Cross-spectral face hallucination via disentangling independent factors. In IEEE Conference on Computer Vision and Pattern Recognition, pages 7930\u20137938, 2020.\\n\\n[9] Virginia Espinosa-Du \u00b4ro, Marcos Faundez-Zanuy, and Ji \u02c7r\u00b4\u0131 Mekyska. A new face database simultaneously acquired in visible, near-infrared and thermal spectrums. Cognitive Computation, 5(1):119\u2013135, 2013.\\n\\n[10] Chaoyou Fu, Yibo Hu, Xiang Wu, Hailin Shi, Tao Mei, and Ran He. Cm-nas: Cross-modality neural architecture search for visible-infrared person re-identification. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11823\u201311832, 2021.\\n\\n[11] Chaoyou Fu, Yibo Hu, Xiang Wu, Guoli Wang, Qian Zhang, and Ran He. High-fidelity face manipulation with extreme poses and expressions. IEEE Transactions on Information Forensics and Security, 16:2218\u20132231, 2021.\\n\\n[12] Chaoyou Fu, Xiang Wu, Yibo Hu, Huaibo Huang, and Ran He. Dual variational generation for low shot heterogeneous face recognition. Advances in Neural Information Processing Systems, 32:2674\u20132683, 2019.\\n\\n[13] Chaoyou Fu, Xiang Wu, Yibo Hu, Huaibo Huang, and Ran He. Dvg-face: Dual variational generation for heterogeneous face recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\\n\\n[14] Hamed Kiani Galoogahi and Terence Sim. Face sketch recognition by local radon binary pattern: Lrbp. In IEEE International Conference on Image Processing, pages 1837\u20131840, 2012.\\n\\n[15] Dihong Gong, Zhifeng Li, Weilin Huang, Xuelong Li, and Dacheng Tao. Heterogeneous face recognition: A common encoding feature discriminant approach. IEEE Transactions on Image Processing, 26(5):2079\u20132089, 2017.\\n\\n[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014.\\n\\n[17] Jinjin Gu, Yujun Shen, and Bolei Zhou. Image processing using multi-code gan prior. In IEEE Conference on Computer Vision and Pattern Recognition, pages 3012\u20133021, 2020.\\n\\n[18] Pengfei Guo, Puyang Wang, Jinyuan Zhou, Shanshan Jiang, and Vishal M Patel. Multi-institutional collaborations for improving deep learning-based magnetic resonance image reconstruction using federated learning. In IEEE Conference on Computer Vision and Pattern Recognition, pages 2423\u20132432, 2021.\\n\\n[19] Pengfei Guo, Dong Yang, Ali Hatamizadeh, An Xu, Ziyue Xu, Wenqi Li, Can Zhao, Daguang Xu, Stephanie Harmon, Evrim Turkbey, et al. Auto-fedrl: Federated hyperparameter optimization for multi-institutional medical image segmentation. arXiv preprint arXiv:2203.06338, 2022.\\n\\n[20] Yandong Guo, Lei Zhang, Xu Xiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In European Conference on Computer Vision, pages 87\u2013102. Springer, 2016.\\n\\n[21] Ran He, Xiang Wu, Zhenan Sun, and Tieniu Tan. Wasserstein cnn: Learning invariant features for nir-vis face recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(7):1761\u20131773, 2018.\\n\\n[22] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In Proceedings of the IEEE international conference on computer vision, pages 1501\u20131510, 2017.\\n\\n[23] Rakhil Immidisetti, Shuowen Hu, and Vishal M. Patel. Simultaneous face hallucination and translation for thermal to visible face verification using axial-gan. In IEEE International Joint Conference on Biometrics, pages 1\u20138, 2021.\\n\\n[24] Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei A Efros. Image-to-image translation with conditional adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, pages 1125\u20131134, 2017.\\n\\n[25] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In IEEE Conference on Computer Vision and Pattern Recognition, pages 4401\u20134410, 2019.\\n\\n[26] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In IEEE Conference on Computer Vision and Pattern Recognition, pages 8110\u20138119, 2020.\\n\\n[27] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In International Conference on Learning Representations, 2015.\\n\\n[28] Brendan F Klare and Anil K Jain. Heterogeneous face recognition using kernel prototype similarities. IEEE Transactions\"}"}
{"id": "CVPR-2022-601", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[29] Jakub Kone\u010dn\u00fd, H Brendan McMahan, Felix X Yu, Peter Richt\u00e1rik, Ananda Theertha Suresh, and Dave Bacon. Federated learning: Strategies for improving communication efficiency. arXiv preprint arXiv:1610.05492, 2016.\\n\\n[30] Jos\u00e9 Lezama, Qiang Qiu, and Guillermo Sapiro. Not afraid of the dark: Nir-vis face recognition via cross-spectral hallucination and low-rank embedding. In IEEE Conference on Computer Vision and Pattern Recognition, pages 6628\u20136637, 2017.\\n\\n[31] Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He. Federated learning on non-iid data silos: An experimental study. arXiv preprint arXiv:2102.02079, 2021.\\n\\n[32] Stan Z Li, RuFeng Chu, ShengCai Liao, and Lun Zhang. Illumination invariant face recognition using near-infrared images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(4):627\u2013639, 2007.\\n\\n[33] Stan Z Li, Zhen Lei, and Meng Ao. The hfb face database for heterogeneous face biometrics research. In IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1\u20138, 2009.\\n\\n[34] Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar Sanjabi, Ameet Talwalkar, and Virginia Smith. Federated optimization in heterogeneous networks. In Proceedings of Machine Learning and Systems, volume 2, pages 429\u2013450, 2020.\\n\\n[35] Xiang Li, Kaixuan Huang, Wenhao Yang, Shusen Wang, and Zhihua Zhang. On the convergence of fedavg on non-iid data. In International Conference on Learning Representations, 2019.\\n\\n[36] Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael Kamp, and Qi Dou. FedBN: Federated learning on non-IID features via local batch normalization. In International Conference on Learning Representations, 2021.\\n\\n[37] Shengcai Liao, Dong Yi, Zhen Lei, Rui Qin, and Stan Z Li. Heterogeneous face recognition from local structures of normalized appearance. In IEEE International Conference on Biometrics, pages 209\u2013218. Springer, 2009.\\n\\n[38] Jinpeng Lin, Hao Yang, Dong Chen, Ming Zeng, Fang Wen, and Lu Yuan. Face parsing with roi tanh-warping. In IEEE Conference on Computer Vision and Pattern Recognition, pages 5654\u20135663, 2019.\\n\\n[39] Sifei Liu, Jianping Shi, Ji Liang, and Ming-Hsuan Yang. Face parsing via recurrent propagation. arXiv preprint arXiv:1708.01936, 2017.\\n\\n[40] Sifei Liu, Dong Yi, Zhen Lei, and Stan Z Li. Heterogeneous face image matching using multi-scale features. In IEEE International Conference on Biometrics, pages 79\u201384, 2012.\\n\\n[41] Khawla Mallat, Naser Damer, Fadi Boutros, Arjan Kuijper, and Jean-Luc Dugelay. Cross-spectrum thermal to visible face recognition based on cascaded image synthesis. In IEEE International Conference on Biometrics, pages 1\u20138, 2019.\\n\\n[42] Khawla Mallat and Jean-Luc Dugelay. A benchmark database of visible and thermal paired face images across multiple variations. In International Conference of the Biometrics Special Interest Group (BIOSIG), pages 1\u20135, 2018.\\n\\n[43] Brendan McMahan, Eider Moore, Daniel Ramage, Seth Hampson, and Blaise Aguera y Arcas. Communication-efficient learning of deep networks from decentralized data. In Artificial Intelligence and Statistics, pages 1273\u20131282. PMLR, 2017.\\n\\n[44] Sachit Menon, Alexandru Damian, Shijia Hu, Nikhil Ravi, and Cynthia Rudin. Pulse: Self-supervised photo upsampling via latent space exploration of generative models. In IEEE Conference on Computer Vision and Pattern Recognition, pages 2437\u20132445, 2020.\\n\\n[45] Anish Mittal, Rajiv Soundararajan, and Alan C Bovik. Making a \u201ccompletely blind\u201d image quality analyzer. IEEE Signal processing letters, 20(3):209\u2013212, 2012.\\n\\n[46] Mehryar Mohri, Gary Sivek, and Ananda Theertha Suresh. Agnostic federated learning. In International Conference on Machine Learning, pages 4615\u20134625. PMLR, 2019.\\n\\n[47] Karen Panetta, Qianwen Wan, Sos Agaian, Srijith Rajeev, Shreyas Kamath, Rahul Rajendran, Shishir Paramathma Rao, Aleksandra Kaszowska, Holly A Taylor, Arash Samani, et al. A comprehensive database for benchmarking imaging systems. IEEE Transactions on Pattern Analysis and Machine Intelligence, 42(3):509\u2013520, 2018.\\n\\n[48] Taesung Park, Ming-Yu Liu, Ting-Chun Wang, and Jun-Yan Zhu. Semantic image synthesis with spatially-adaptive normalization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2337\u20132346, 2019.\\n\\n[49] Chunlei Peng, Nannan Wang, Jie Li, and Xinbo Gao. Soft semantic representation for cross-domain face recognition. IEEE Transactions on Information Forensics and Security, 16:346\u2013360, 2020.\\n\\n[50] Xingchao Peng, Zijun Huang, Yizhe Zhu, and Kate Saenko. Federated adversarial domain adaptation. arXiv preprint arXiv:1911.02054, 2019.\\n\\n[51] N. Peri, J. Gleason, C. D. Castillo, T. Bourlai, V. M. Patel, and R. Chellappa. A synthesis-based approach for thermal-to-visible face verification. In IEEE International Conference on Automatic Face and Gesture Recognition, 2021.\\n\\n[52] Domenick Poster, Matthew Thielke, Robert Nguyen, Srinivasan Rajaraman, Xing Di, Cedric Nimpa Fondje, Vishal M Patel, Nathaniel J Short, Benjamin S Riggan, Nasser M Nasrabadi, et al. A large-scale, time-synchronized visible and thermal face dataset. In IEEE Winter Conference on Applications of Computer Vision, pages 1559\u20131568, 2021.\\n\\n[53] Albert Pumarola, Antonio Agudo, Aleix M. Martinez, Alberto Sanfeliu, and Francesc Moreno-Noguer. Ganimation: Anatomically-aware facial animation from a single image. In European Conference on Computer Vision. Springer, 2018.\\n\\n[54] Sashank Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Kone\u010dn\u00fd, Sanjiv Kumar, and H Brendan McMahan. Adaptive federated optimization. arXiv preprint arXiv:2003.00295, 2020.\\n\\n[55] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding in style: a stylegan encoder for image-to-image translation. In IEEE Conference on Computer Vision and Pattern Recognition, pages 2287\u20132296, 2021.\"}"}
{"id": "CVPR-2022-601", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Benjamin S Riggan, Nathaniel J Short, and Shuowen Hu. Thermal to visible synthesis of face images using multiple regions. In IEEE Winter Conference on Applications of Computer Vision, pages 30\u201338, 2018.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-Net: Convolutional networks for biomedical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 234\u2013241. Springer, 2015.\\n\\nFlorian Schroff, Dmitry Kalenichenko, and James Philbin. Facenet: A unified embedding for face recognition and clustering. In IEEE Conference on Computer Vision and Pattern Recognition, pages 815\u2013823, 2015.\\n\\nMing Shao, Dmitry Kit, and Yun Fu. Generalized transfer subspace learning through low-rank constraint. International Journal of Computer Vision, 109(1-2):74\u201393, 2014.\\n\\nLingxiao Song, Man Zhang, Xiang Wu, and Ran He. Adversarial discriminative heterogeneous face recognition. In the AAAI Conference on Artificial Intelligence, volume 32, 2018.\\n\\nHao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 5265\u20135274, 2018.\\n\\nXintao Wang, Yu Li, Honglun Zhang, and Ying Shan. Towards real-world blind face restoration with generative facial prior. In IEEE Conference on Computer Vision and Pattern Recognition, pages 9168\u20139178, 2021.\\n\\nXiaogang Wang and Xiaoou Tang. Face photo-sketch synthesis and recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 31(11):1955\u20131967, 2008.\\n\\nXintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Recovering realistic texture in image super-resolution by deep spatial feature transform. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 606\u2013615, 2018.\\n\\nZhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600\u2013612, 2004.\\n\\nYandong Wen, Kaipeng Zhang, Zhifeng Li, and Yu Qiao. A discriminative feature learning approach for deep face recognition. In European Conference on Computer Vision, pages 499\u2013515. Springer, 2016.\\n\\nXiang Wu, Ran He, Zhenan Sun, and Tieniu Tan. A light CNN for deep face representation with noisy labels. IEEE Transactions on Information Forensics and Security, 13(11):2884\u20132896, 2018.\\n\\nWeihao Xia, Yulun Zhang, Yujiu Yang, Jing-Hao Xue, Bolei Zhou, and Ming-Hsuan Yang. Gan inversion: A survey. arXiv preprint arXiv:2101.05278, 2021.\\n\\nAn Xu, Wenqi Li, Pengfei Guo, Dong Yang, Holger Roth, Ali Hatamizadeh, Can Zhao, Daguang Xu, Heng Huang, and Ziyue Xu. Closing the generalization gap of cross-silo federated medical image segmentation. arXiv preprint arXiv:2203.10144, 2022.\\n\\nLingbo Yang, Shanshe Wang, Siwei Ma, Wen Gao, Chang Liu, Pan Wang, and Peiran Ren. Hifacegan: Face renovation via collaborative suppression and replenishment. In ACM International Conference on Multimedia, pages 1551\u20131560, 2020.\\n\\nTao Yang, Peiran Ren, Xuansong Xie, and Lei Zhang. Gan prior embedded network for blind face restoration in the wild. In IEEE Conference on Computer Vision and Pattern Recognition, pages 672\u2013681, 2021.\\n\\nDong Yi, Rong Liu, RuFeng Chu, Zhen Lei, and Stan Z Li. Face matching between near infrared and visible light images. In IEEE International Conference on Biometrics, pages 523\u2013530. Springer, 2007.\\n\\nJunchi Yu, Jie Cao, Yi Li, Xiaofei Jia, and Ran He. Pose-preserving cross spectral face hallucination. In International Joint Conference on Artificial Intelligence, pages 1018\u20131024, 2019.\\n\\nYu Zeng, Zhe Lin, and Vishaal M Patel. Sketchedit: Mask-free local image manipulation with partial sketches. arXiv preprint arXiv:2111.15078, 2021.\\n\\nHe Zhang, Vishaal M Patel, Benjamin S Riggan, and Shuowen Hu. Generative adversarial network-based synthesis of visible faces from polarimetric thermal faces. In IEEE International Joint Conference on Biometrics, pages 100\u2013107, 2017.\\n\\nHe Zhang, Benjamin S Riggan, Shuowen Hu, Nathaniel J Short, and Vishal M Patel. Synthesis of high-quality visible faces from polarimetric thermal faces using generative adversarial networks. International Journal of Computer Vision, 127(6):845\u2013862, 2019.\\n\\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In IEEE Conference on Computer Vision and Pattern Recognition, pages 586\u2013595, 2018.\"}"}
{"id": "CVPR-2022-601", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. An overview of the proposed FL framework. Through $q$ rounds of communication between data centers and the server, the collaboratively trained global model parameterized by $\\\\Theta_q$ can be obtained in a data privacy-preserving manner. The zoomed-in subplot shows the proposed Model Proximity Regularization (MPR) in a local institution $k$.\\n\\nAlgorithm 1: VPFL with MPR\\n\\nInput: $D_1^s, D_2^s, \\\\ldots, D_K^s$, $K$ dispersed datasets; $P$, local update steps; $Q$, communication rounds; $\\\\gamma$, learning rate; $\\\\Theta_1, \\\\ldots, \\\\Theta_K$, local models; $\\\\Theta$, global model.\\n\\n\u25b7 parameters initialization\\nfor $q = 0$ to $Q$\\ndo\\n  for $k = 0$ to $K$ in parallel\\ndo\\n    \u25b7 deploy weights $\\\\Theta_q$ to local model\\n    for $p = 0$ to $P$\\ndo\\n      VPGAN Face Hallucination: \u25b7 compute loss $L_{gen,k}$ using Eq. 5\\n      Model Proximity Regularization: \u25b7 compute the proximal term with respect to $\\\\Theta_{p,q}$ and $\\\\Theta_q$\\n    \u25b7 compute the final local objective using Eq. 9 and update $\\\\Theta_{p,q}$\\n  \u25b7 upload weights to the central server\\n  \u25b7 update the global model using Eq. 8\\nend\\ndelimiter\\nreturn $\\\\Theta_Q$\\n\\nTo maximize data utilization and learn a more generic model, we propose a vanilla FL framework based on FedAvg \\\\[43\\\\]. Rather than directly sharing private datasets, we leverage a centralized server to indirectly harvest information from all available institutions. This is achieved by periodically aggregating local models and broadcasting the updated results to all participants. A global update in the central server is calculated as follows:\\n\\n$$\\\\Theta_q = \\\\frac{1}{K} \\\\sum_{k=1}^{K} \\\\Theta^q_k,$$ \\\\hspace{1cm} (8)\\n\\nwhere $q$ represents the $q$-th communication round. The final trained global model $\\\\Theta_Q$ is obtained after $Q$ rounds of client-server communications.\\n\\nModel Proximity Regularization. While our vanilla FL algorithm manages to scale-up training, in real-world applications, the non-i.i.d. data distribution among institutions will still inevitably hurt the performance \\\\[34\\\\]. Due to the dissimilar local objectives $L_{gen,k}$ resulting from heterogeneous data distribution, local updates without proper constraints will cause the resulting model to skew toward the optima of its local objective, leading to inconsistency with the global one. Previous works \\\\[18, 50\\\\] circumvent this issue via FL adversarial training between source and target domains. However, such methods require directly sharing the latent features between participants, which compromises the privacy-preserving principle.\\n\\nInspired by \\\\[2, 34, 46\\\\], here we introduce a new Model Proximity Regularization (MPR) to correct local updates which can be easily combined with VPGAN. As shown in Figure 2, rather than solely minimizing the local objective $L_{gen,k}$, MPR introduces an extra proximal term for each local solver to force the proximity of latent codes from the current local model and the initial global model. The final local objective $L_{gen,k}$ is adjusted to\\n\\n$$L_k = L_{gen,k} + \\\\lambda d (\\\\|w - w_q\\\\|_2^2 + n \\\\sum_{i=1}^{n} \\\\|F_i - F^q_i\\\\|_2^2),$$ \\\\hspace{1cm} (9)\\n\\nwhere $\\\\lambda_d$ is a balancing parameter. In our unified framework VPFL, we integrate VPGAN as the base model and use it in the FL framework. VPFL thus can jointly enjoy benefits of strong visual priors and large-scale training for more realistic and faithful hallucination. The detailed training process can be found in Algorithm 1.\\n\\n4. Experiments\\n\\nIn our experiments, we focus on synthesizing $512 \\\\times 512$ VIS faces from $128 \\\\times 128$ TH images, with an emphasis on both recognition accuracy and image quality. More analysis, discussion and additional results for resolutions less than $128 \\\\times 128$ can be found in the supplementary material.\\n\\nDataset and Evaluation Metrics. So far, there are no standardized protocols in this field. Existing methods report results trained and tested on custom datasets/splits. This paper selects two common datasets (VIS-TH \\\\[42\\\\] and ARL-VTF \\\\[52\\\\]) where high-resolution VIS images are available. To study the effect of data restriction, we intentionally choose one dataset to be more challenging than the other. We create VIS-TH image pairs by cropping $512 \\\\times 512$ and $128 \\\\times 128$ face regions respectively. VIS-TH is a challenging dataset containing data from 50 subjects. Images from each subject contain 21 faces varying significantly in pose, expression and light conditions. VIS-TH images are captured via a dual-sensor camera in LWIR.\"}"}
{"id": "CVPR-2022-601", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. The schematics of the clients partition and different training strategies.\\n\\nWe construct the training set by randomly selecting data from 40 subjects. The remaining data from 10 subjects are used as the testing set. ARL-VTF provides subjects' data in LWIR modality with annotations for alignment. We create a dataset by randomly selecting a subset of 160 subjects with variations only in expressions as the training set, 20 subjects' data as the validation set, and 40 subjects' data as the testing set. The resulting data split contains 3,200 training pairs, 400 validation pairs, and 985 testing pairs. The color adjustment is applied to mitigate overexposure of the VIS images.\\n\\nEvaluation Metrics. This paper extends the existing verification protocol with image quality measurements. For verification, we follow and report Rank-1 accuracy, Verification Rate (VR) @ False Accept Rate (FAR)= 1% and VR@FAR= 0.1%. One VIS image of each subject is added to the gallery set and the probe set contains all TH images. To measure image quality, we report perceptual metrics LPIPS, NIQE, identity metric Deg (cosine distance between LightCNN features), and pixel-wise PSNR and SSIM.\\n\\n4.1. Implementation and Training Details For VPGAN, we adopt off-the-shelf StyleGAN2 as our facial decoder. The UNet encoder contains 5 downsample stages and 7 upsample stages for joint face translation and upsampling. Feature at the lowest level has a spatial size of $4 \\\\times 4$. The network is trained using the Adam optimizer with the following hyperparameters: initial learning rate of $2e^{-3}$ for the first 140K iterations then reduced to $1e^{-3}$; 150K maximum iterations; batch size of 4; $\\\\lambda_a = 1$; $\\\\lambda_b = 10$; $\\\\lambda_c = 100$; $\\\\lambda_d = 10e^{-4}$ if applicable. We implement the proposed model using PyTorch on Nvidia RTX8000 GPUs.\\n\\nFor VPFL, we adopt the same hyperparameters as that in VPGAN except 80K maximum iterations. The periodical communication between clients and the server is set to 200 iterations. Two training sets are further split into 4 subsets by sampling from a Dirichlet distribution ($\\\\alpha=0.3$) to simulate heterogeneous data distribution in the FL scheme, resulting in 8 independent clients as shown in Figure 3. Detailed dataset statistics in each client is provided in the supplementary material. For experiments in the FL setting, we not only compare different FL algorithms but also privacy-preserving alternative strategies. Models of Local Only are trained only with data from a single client and evaluated on two testing datasets. We denote the method that obtains independently trained models from all local clients and fuses their outputs as Fused, which does not violate privacy regulations. In addition, we can obtain a model trained by all available data, which is denoted by Centralized. Since it is prohibited in FL, we treat it as an upper bound. Figure 3 provides the schematics of different training and evaluation strategies in the FL setting.\\n\\nTable 1. Image quality results on the VIS-TH dataset. Red and blue indicates the best and the second best performance.\\n\\n| Methods          | LPIPS \u2193 | NIQE \u2193 | Deg. \u2191 | PSNR \u2191 | SSIM \u2191 |\\n|------------------|---------|--------|--------|--------|--------|\\n| TH               | 0.7147  | 10.666 | 36.13  | 6.41   | 0.3619 |\\n| Pixel2Pixel      | 0.3837  | 6.642  | 43.97  | 16.64  | 0.6818 |\\n| HiFaceGAN        | 0.3769  | 5.973  | 51.03  | 15.75  | 0.6794 |\\n| GANVFS           | 0.4012  | 6.314  | 43.95  | 16.69  | 0.6569 |\\n| SAGAN            | 0.2786  | 5.899  | 62.35  | 18.15  | 0.7179 |\\n| AxialGAN         | 0.2688  | 5.761  | 62.66  | 19.02  | 0.7190 |\\n| VPGAN (ours)     | 0.2253  | 5.508  | 68.36  | 18.96  | 0.7456 |\\n\\n4.2. Evaluations for VPGAN Results on the VIS-TH dataset. To demonstrate the effectiveness of our VPGAN, we first report results on the challenging VIS-TH dataset and compare it with 5 representative methods: Pixel2Pixel, HiFaceGAN, GANVFS, SAGAN and AxialGAN. Pixel2Pixel is a well-known image-to-image translation method. HiFaceGAN is the state-of-the-art approach for face restoration. For TH-VIS hallucination, we select three leading methods GANVFS, SAGAN, and AxialGAN. Note: only AxialGAN has made their code publicly available. For GANVFS and SAGAN, implementations are acquired from authors. Visual results are shown in Figure 4. As can be seen from this figure, previous approaches fail to generate clear visible faces. Specifically, Pixel2Pixel, HiFaceGAN, GANVFS show strong artifacts and distortions in the generated faces. SAGAN and AxialGAN improve hallucination by adapting self-attention mechanism, but the produced images are still very blurry. In contrast, VPGAN outperforms previous methods by a large margin and it synthesizes the most faithful and accurate faces. Results for quantitative quality assessment are reported in Table 1. Our approach achieves the best performance in almost all metrics. VPGAN also obtains the highest Deg. value, indicating its superior ability in preserving identity. All of these results demonstrate the huge benefits of using visual priors for HFR.\\n\\nWe report verification results in Table 2. Given the superior generation quality, there is no surprise that VP-GAN achieves the best performance on all metrics.\"}"}
{"id": "CVPR-2022-601", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Visual comparison on the TH-VIS and ARL-VTF datasets. Low-resolution TH inputs are attached at the bottom right corner of the GT images with the real scale ratio (128:512) preserved. Our VPGAN can synthesize high-quality faces even with challenging expressions and large poses. Best viewed by zooming to 400% in the screen.\\n\\nTable 2. Verification results on the VIS-TH dataset.\\n\\n| Method      | Rank-1 VR@FAR=1% | VR@FAR=0.1% |\\n|-------------|------------------|-------------|\\n| LightCNN    | 30.48            | 8.57        | 2.86        |\\n| Pixel2Pixel | 15.24            | 2.21        | 0.07        |\\n| HiFaceGAN   | 44.76            | 10.95       | 2.86        |\\n| GANVFS      | 18.11            | 7.29        | 1.90        |\\n| SAGAN       | 63.33            | 23.81       | 17.62       |\\n| AxialGAN    | 66.67            | 24.76       | 13.81       |\\n| VPGAN (ours)| 76.67            | 45.71       | 20.00       |\\n\\nTable 3. Image quality results on the ARL-VTF dataset.\\n\\n| Methods  | LPIPS \u2193 | NIQE \u2193 | Deg. \u2191 | PSNR \u2191 | SSIM \u2191 |\\n|----------|---------|--------|--------|--------|--------|\\n| TH        | 0.6721  | 10.176 | 42.34  | 5.63   | 0.2940 |\\n| Pixel2Pixel | 0.2038  | 6.298  | 70.67  | 19.46  | 0.7759 |\\n| HiFaceGAN  | 0.2166  | 7.274  | 70.11  | 19.67  | 0.7954 |\\n| GANVFS    | 0.2433  | 6.679  | 67.26  | 19.76  | 0.7511 |\\n| SAGAN     | 0.1925  | 6.155  | 71.12  | 20.11  | 0.7772 |\\n| AxialGAN  | 0.1998  | 6.223  | 69.75  | 20.17  | 0.7770 |\\n| VPGAN (ours) | 0.1713  | 6.059  | 72.00  | 20.29  | 0.7883 |\\n\\nTable 4. Verification results on the ARL-VTF dataset.\\n\\n| Method      | Rank-1 VR@FAR=1% | VR@FAR=0.1% |\\n|-------------|------------------|-------------|\\n| LightCNN    | 11.07            | 9.24        | 4.57        |\\n| Pixel2Pixel | 70.96            | 56.35       | 33.60       |\\n| HiFaceGAN   | 70.15            | 56.65       | 32.18       |\\n| GANVFS      | 70.76            | 45.99       | 22.03       |\\n| SAGAN       | 71.16            | 54.11       | 38.07       |\\n| AxialGAN    | 71.57            | 57.16       | 37.36       |\\n| VPGAN (ours)| 74.16            | 59.96       | 41.27       |\\n\\nically, our method significantly improves the baseline LightCNN by 46%, and previous state-of-the-art Axial-GAN by 10% in Rank-1 accuracy. In contrast, low-quality hallucinations, e.g., from Pixel2Pixel and GANVFS, can also impair the performance.\\n\\nResults on the ARL-VTF dataset. To study the effect of data restriction, we further report results on the ARL-VTF dataset. This dataset contains \u00d74 more subjects (160) than VIS-TH with only slight variations in expressions. Note that it is non-trivial for a single institution to achieve diversity at this scale. As shown in Figure 4, data simplicity allows previous methods to yield better visual results as expected. While most of them are able to produce a face outline, they struggle to create detailed facial components. In contrast, our approach can produce realistic and faithful facial details. Its superior hallucination ability is further verified by the quantitative results in Table 3. Face verification results are shown in Table 4. While previous methods can achieve reasonable performance, VPGAN still reaches the best performance given the more clear and accurate face details.\\n\\n4.2.1 Ablation Study\\n\\nEffects of Visual Priors. The key design of VPGAN is to leverage rich and diverse visual priors for better hallucination. This is achieved by incorporating a series of visual priors into the network architecture. The visual priors are extracted from a database of high-quality images, which are then used to guide the generation process. The results show that including visual priors significantly improves the performance of our method. The network is able to generate faces with more realistic and detailed features, which is evident in the visual comparison. Moreover, the quantitative results further confirm the effectiveness of using visual priors. The rank-1 accuracy of VPGAN is higher than the baseline methods, indicating that the visual priors indeed play a crucial role in improving the hallucination quality.\"}"}
{"id": "CVPR-2022-601", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 5. Ablation study on the VIS-TH dataset.\\n\\n| Rank-1 VR@FAR=1% | Deg. | LPIPS | PSNR | SSIM | Deg. |\\n|------------------|------|-------|------|------|------|\\n| w/o VP           | 52.85| 19.04 | 0.2948| 18.23|\\n| w/o MSCA         | 71.90| 31.43 | 0.2460| 18.74|\\n| VPGAN            | 76.19| 38.10 | 0.2381| 18.85|\\n| GT Local Fused   |      |       |      |      |\\n| FedProx VPFL w/o MPR |    |      |      |      |\\n\\nFigure 5. Visual comparison under the FL setting. VPFL is able to recover the most accurate face components.\\n\\n4.3. Evaluations for VPFL\\n\\nHere we show the benefits of collaborative training for HFR by revealing performance and generalizability gaps. Table 6 presents the verification and image quality assessment results of different privacy-preserving strategies on three sub-tables VIS-TH, ARL-VTF, and Global Test Avg.\\n\\n**Global Test Avg.** refers to the average performance on the two datasets and reflects generalizability.\\n\\nThe first 8 rows of each sub-table report the results of locally trained models (Local Only). We treat them as baselines. Due to the data heterogeneity, all locally trained models exhibit low generalizability on the data from another distribution. For example, C1-C4 achieve low performance on ARL-VTF and vice versa. This can also be verified by their poor results on the Global Test Avg. In addition, the naive Fused strategy cannot consistently improve the performance. In contrast, apparent improvements can be observed by introducing the FL scheme, which demonstrates the necessity of collaborative training. When compared with the strong FL baseline FedProx [34], our VPFL achieves the significantly better overall performance and is closest to the Centralized upper bound. These results indicate that VPFL can generalize well and is more robust towards heterogeneous data distribution.\\n\\n### Table 6. Verification and image quality comparisons with different methods under the FL setting. VR1% denotes VR@FAR=1%.\\n\\n| Methods | Rank-1 \u2191 | VR1% \u2191 | VR0.1% \u2191 | Deg. \u2191 | PSNR \u2191 | SSIM \u2191 | LPIPS \u2193 | NIQE \u2193 |\\n|---------|----------|--------|----------|--------|--------|--------|---------|--------|\\n| Local Only C1 | 59.05    | 30.95  | 17.62    | 60.69  | 18.05  | 0.723  | 0.262   | 5.903  |\\n| Local Only C2 | 32.86    | 1.90   | 0.48     | 51.52  | 16.56  | 0.699  | 0.315   | 5.809  |\\n| Local Only C3 | 50.95    | 20.00  | 7.14     | 55.49  | 17.65  | 0.718  | 0.283   | 6.403  |\\n| Local Only C4 | 46.19    | 17.62  | 10.00    | 56.82  | 17.01  | 0.700  | 0.294   | 5.748  |\\n| Local Only C5 | 47.62    | 17.14  | 8.10     | 51.64  | 14.94  | 0.664  | 0.383   | 5.911  |\\n| Local Only C6 | 42.86    | 11.43  | 6.19     | 53.17  | 14.75  | 0.664  | 0.385   | 6.171  |\\n| Local Only C7 | 40.95    | 15.24  | 5.24     | 51.30  | 15.01  | 0.668  | 0.385   | 6.420  |\\n| Local Only C8 | 40.95    | 15.71  | 5.71     | 52.20  | 14.80  | 0.665  | 0.384   | 6.426  |\\n| Fused      | 37.62    | 16.19  | 7.14     | 55.83  | 17.36  | 0.732  | 0.328   | 6.934  |\\n| FedProx [34] | 66.19   | 30.95  | 20.00    | 61.99  | 17.86  | 0.718  | 0.262   | 5.565  |\\n| VPFL w/o MPR | 70.95   | 30.95  | 22.38    | 61.16  | 18.19  | 0.719  | 0.254   | 5.579  |\\n| VPFL       | 73.81   | 35.71  | 25.71    | 65.81  | 18.81  | 0.728  | 0.245   | 5.651  |\\n| Centralized | 76.67   | 39.05  | 24.76    | 66.63  | 18.71  | 0.743  | 0.232   | 5.729  |\\n\\nThe last three rows of each sub-table show the advantages come from the newly designed MPR. These quantitative results are also aligned with the visual comparison in Figure 5. One can see that VPFL yields the most accurate and faithful hallucination results.\\n\\n5. Conclusion\\n\\nIn this paper, we proposed a unified framework VPFL for heterogeneous face hallucination. VPFL consists of a novel VPGAN and a new Federated Learning (FL) scheme. VPGAN introduces powerful visual priors to avoid learning hallucination from scratch, resulting in more accurate generation under the current data limitations. With the consideration of practical privacy issues, the proposed FL scheme allows institution-wise collaborations without sharing data, making large-scale training possible. Extensive experiments demonstrate that VPFL can significantly boost HFR by synthesizing accurate and realistic visible faces at a resolution unseen in the literature. Discussion of limitations can be found in the supplementary material.\\n\\nAcknowledgments\\n\\nThis work was supported by NSF CARRER award 2045489.\"}"}
