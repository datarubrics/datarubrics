{"id": "CVPR-2023-992", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Improving Visual Representation Learning through Perceptual Understanding\\n\\nSamyakh Tukra Frederick Hoffman Ken Chatfield\\n\\nTractable AI\\n\\nAbstract\\nWe present an extension to masked autoencoders (MAE) which improves on the representations learnt by the model by explicitly encouraging the learning of higher scene-level features. We do this by: (i) the introduction of a perceptual similarity term between generated and real images (ii) incorporating several techniques from the adversarial training literature including multi-scale training and adaptive discriminator augmentation. The combination of these results in not only better pixel reconstruction but also representations which appear to capture better higher-level details within images. More consequentially, we show how our method, Perceptual MAE, leads to better performance when used for downstream tasks outperforming previous methods. We achieve 78.1% top-1 accuracy linear probing on ImageNet-1K and up to 88.1% when fine-tuning, with similar results for other downstream tasks, all without use of additional pre-trained models or data.\\n\\n1. Introduction\\nSelf-supervision provides a powerful framework for training deep neural networks without relying on explicit supervision or labels where learning proceeds by predicting one part of the input data from another. Approaches based on denoising autoencoders [45], where the input is masked and the missing parts reconstructed, have shown to be effective for pre-training in NLP with BERT [8], and more recently similar techniques have been applied for learning visual representations from images [1,4,17,30]. Such methods effectively use image reconstruction as a pretext task on the basis that by learning to predict missing patches useful representations can be learnt for downstream tasks.\\n\\nOne challenge when applying such techniques to images is that, unlike language where words contain some level of semantic meaning by design, the pixels in images are natural signals containing high-frequency variations. Therefore, image-based denoising autoencoders have been adapted to avoid learning trivial solutions to reconstruction based on local textures or patterns. BEiT [1] uses an intermediary codebook of patches such that pixels are not reconstructed directly, whilst MAE [17] masks a high proportion of the image to force the model to learn how to reconstruct whole scenes with limited context.\\n\\nIn this paper, we build upon MAE and ask how we can move beyond the implicit conditioning of high masking ratios to explicitly incorporate the learning of higher-order 'semantic' features into the learning objective. To do this, we focus on introducing scene-level information by adding a perceptual loss term [22]. This works by constraining feature map similarity with a second pre-trained network, a technique which has been shown empirically in the generative modelling literature to improve perceptual reconstruction quality [54]. In addition, this also provides a mechanism to incorporate relevant scene-level cues contained in the second network (which could be e.g. a strong ImageNet classifier or a pre-trained language-vision embedding).\\n\\nOne of the benefits of MAE is that it can rapidly learn strong representations using only self-supervision from the images in the target pre-training set. To maintain this property, we introduce a second idea: tying the features not with a separate network, but with an adversarial discriminator trained in parallel to distinguish between real and generated images. Both ideas combined result in not only a lower reconstruction error, but also learnt representations which better capture details of the scene layout and object boundaries (see Figure 3) without either explicit supervision or the use of hand-engineered inductive biases.\\n\\nFinally, we build on these results and show that techniques from the generative modelling literature such as multi-scale gradients [24] and adaptive discriminator augmentation [25] can lead to further improvements in the learnt representation, and this also translates into a further boost in performance across downstream tasks. We hypothesise that the issues that these methods were designed to overcome, such as mode collapse during adversarial training and incomplete learning of the underlying data distribution, are related to overfitting on low-level image features.\\n\\nOur contributions can be summarized as follows: (i) we introduce a simple and self-contained technique to improve the representations learnt by masked image modelling based...\"}"}
{"id": "CVPR-2023-992", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"on perceptual loss and adversarial learning, (ii) we perform a rigorous evaluation of this method and variants, and set new state-of-the-art for masked image modelling without additional data for classification on ImageNet-1K, object detection on MS COCO and semantic segmentation on ADE20K, (iii) we demonstrate this approach can further draw on powerful pre-trained models when available, resulting in a further boost in performance and (iv) we show our approach leads qualitatively to more \u2018object-centric\u2019 representations and stronger performance with frozen features (in the linear probe setting) compared to MAE.\\n\\n2. Related Work\\n\\n2.1. Masked Image Modelling\\n\\nSelf-supervised learning has led to learning systems that do not depend on data labelling, where the raw data itself provides the supervisory signal for training. This results in models with feature representations that are generalisable to many tasks. Self-supervised learning has shown considerable success in Natural Language Processing (NLP) [2, 33, 44], where random parts of the input text are masked and the model is tasked with predicting the invisible content. This has become the de facto method of pre-training NLP models. Compared to this direct-prediction approach, the first performant approaches to self-supervised visual representation learning instead used predefined discriminative tasks such as estimating distortions of the input image [13, 15], patch re-ordering [11, 37], re-coloring a grayscale image input [53], and contrastive learning [5, 42].\\n\\nRecently, inspired by NLP and facilitated by the advent of Transformer models (Vision Transformers [30] in particular) masked image modelling (MIM) returns to the idea of direct-prediction, randomly masking pixels of an input image before predicting the invisible content. Early work included iGPT [4] which downsized images and then directly predicted unknown pixel values in an autoregressive manner. Recent methods have moved towards predicting full resolution patches in an autoencoder configuration [1, 17, 51]. BEiT [1] relies on an additional generative model (dVAE [38]) pre-trained on a large corpus of images (250M) with the pretext task to predict for masked matches the closest visual token from a pre-trained codebook. MAE [17] takes a simpler approach, demonstrating that direct pixel prediction of masked regions using Mean Squared Error is also effective when a very large proportion of the image is masked out.\\n\\nThese recent methods produce strong performance when fine-tuning over downstream tasks, but generally discriminative self-supervision has continued to be more performant in a linear probe setting [17] suggesting focusing on the learning of features of the right level of abstraction remains a challenge. We seek to address this in our work.\\n\\n2.2. Perceptual Similarity\\n\\nThe aim of perceptual similarity [54] is to mimic human visual perception. Humans are capable of understanding images on an abstract level, relying on high level concepts and semantic cues that define the underlying relationships between different entities in the frame. Perceptual similarity aims to mimic this human-like judgement by defining metrics which encode perceptual distance, with this being higher for image representations that similarly better capture visual semantic concepts.\\n\\nStructural Similarity Index (SSIM), an early form of perceptual loss, attempts to capture properties of an image that when varied are perceived by humans as substantially different. Images are compared on three key features: (i) luminance (i.e. pixel intensity), (ii) contrast and (iii) structure [48]. Further work extended this to compute similarity at multiple scales [49]. In parallel other similar metrics have been proposed such as: Peak Signal to Noise ratio (PSNR) [21], Feature Similarity Index (FSIM) [52] and HDR-VDP-2 [34].\\n\\nAn alternative approach to capturing perceptual similarity is to not compare differences between pixels but instead compute the differences between the intermediary features learnt by a neural network and those extracted from a second fixed network pre-trained in a supervised manner on a large dataset, on the basis that these capture the higher-level semantically meaningful features required for accurate classification. This is the approach taken by [22] who pre-train a VGG network on ImageNet and then use this for learning. Such a feature matching based approach has been successfully applied to many tasks since in computer vision [41, 46, 47].\\n\\nIn our work, we draw on the feature matching based approach to perceptual similarity but remove the requirement for a pre-trained network, instead learning perceptual similarity dynamically. In parallel to this work, PeCo [12] also experiments with perceptual loss to prepare a perceptually aware codebook for masked image modelling. However, in our case we apply this directly during pre-training which is much more effective, as it enables the encoder to learn directly higher-level cues from the second network.\\n\\n2.3. Generative modelling\\n\\nIf perceptual similarity tries to capture the semantic structure of images, generative models aim to capture the underlying distribution of the image data. An example is Generative Adversarial Networks (GANs) [23\u201325, 27]. The samples created by a generator model are evaluated by a separately trained discriminator model which is tasked with determining real images from generated images. This is trained in parallel with the generator using an adversarial loss function. Since GANs learn the underlying data distribution implicitly via a discriminator the original for...\"}"}
{"id": "CVPR-2023-992", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The StyleGAN family of papers \\\\cite{25\u201327} introduced several improvements to the learning of the discriminator further designed to improve the stability of generated images. Perceptual path length regularisation \\\\cite{54} enforces that small changes in the input latent code (in our work: the input to the decoder) lead to changes of a similar magnitude in the feature maps of the discriminator, thus ensuring good normalisation of the input codes. Adaptive Discriminator Augmentation (ADA) \\\\cite{25} enables the use of heavy augmentation when training the discriminator, avoiding overfitting even with smaller volumes of training data, whilst ensuring these augmentations do not affect the output of the generator. Both encourage the underlying feature space to be more stable to small changes in low-level image statistics. In this work, we explore if these additions, along with multi-scale learning described above additionally incorporated into the masked autoencoder architecture, can therefore help to learn richer, high-level representations when using adversarial training for masked image modelling.\\n\\nAn alternative to GANs for generative modelling is provided by explicit generative models which aim to capture the underlying data distribution directly. These methods avoid some of the issues such as mode collapse suffered by implicit modelling but, given the need to model the full distribution, generally are more sensitive to the volume of data used for training. Examples include Variational Autoencoders (VAE) \\\\cite{20, 29}, Flow-based models \\\\cite{10, 28} and Diffusion models \\\\cite{9, 36}. VQ-VAE \\\\cite{43} builds on VAE by learning a discrete latent space rather than a continuous one by the creation of a codebook. Recently this was applied for masked image modelling in BEiT \\\\cite{1}, where the rich latent representations learnt by a VQ-VAE model pre-trained on large data \\\\cite{38} are used as a prediction target. In this work, we also explore using such a large pre-trained VQ-VAE model when combined with perceptual similarity loss.\\n\\n3. Methodology\\n\\nThe learning framework used for this work is based on MAE \\\\cite{17}. In Section 3.1 we describe how the MAE loss is extended with a perceptual loss term. In Section 3.2 we then describe variants of adversarial loss which is also added to the objective. In Section 3.3 we describe modifications to the MAE architecture to maximize learning in the encoder stage when using multi-scale gradients.\\n\\n3.1. MAE with Perceptual Loss\\n\\nThe pixel reconstruction loss from the original MAE formulation is extended to include a perceptual loss term:\\n\\n\\\\[\\nL_{G} = ||G(I_m) - I||_1 + L_{G_{\\\\text{perceptual}}}\\n\\\\]\\n\\nWhere \\\\(G\\\\) is the MAE model, \\\\(I\\\\) is the original image and \\\\(I_m\\\\) is the original image randomly masked. We follow the convention in the generative modelling literature, and use L1 loss rather than L2 loss for the reconstruction term.\\n\\nMS-SSIM:\\n\\nOur baseline perceptual loss is based on structural similarity index, specifically the multi-scale variant (MS-SSIM) \\\\cite{49}. The multi-scale component aids in reducing artefacts formed around the edges of the output reconstructed image \\\\(I'\\\\). The perceptual loss term is thus:\\n\\n\\\\[\\nL_{G_{\\\\text{ssim}}} = \\\\frac{1}{N} \\\\sum_{i,j} \\\\alpha (1 - \\\\text{SSIM}(G(I_m)_{ij}, I_{ij}))\\n\\\\]\\n\\nWhere \\\\(i, j\\\\) are the pixel indexes and \\\\(N\\\\) is the total number of scales, set to 4. We use a \\\\(3 \\\\times 3\\\\) block filter for each scale. \\\\(\\\\alpha\\\\) is a weighting constant, with the L1 error weighted by the inverse \\\\((1 - \\\\alpha)\\\\).\\n\\nFeature matching:\\n\\nBased on the feature and style reconstruction losses of \\\\cite{22} our second perceptual loss relies on a separate loss network with the decoder network encouraged to have similar feature representations as each corresponding layer of the loss network \\\\(\\\\phi\\\\). In the original formulation \\\\(\\\\phi\\\\) is a fixed VGG network pre-trained on ImageNet as described in Section 2.2. To avoid this dependency on an external pre-trained network, we instead introduce an additional discriminator network \\\\(D\\\\) which will act as our loss network \\\\(\\\\phi\\\\). This is trained in an adversarial setup to distinguish between the reconstructed image from the decoder \\\\(G\\\\) and the original image prior to masking. The intuition is that the features learnt through this task also contain higher-order perceptual cues which can be used to guide training of the decoder.\\n\\nThe perceptual loss comprises two parts. The first transfers high-level semantics (individual features are similar), with a second style term added which learns overall image statistics (correlations between features across the image are similar) giving:\\n\\n\\\\[\\nL_{G_{\\\\text{feat}}} = \\\\sum_{j} \\\\delta_f \\\\frac{1}{N_j} \\\\sum_{X}||\\\\phi_j(G(I_m)) - \\\\phi_j(I)||_1 + \\\\sum_{j} \\\\delta_s \\\\frac{1}{N_j} \\\\sum_{X}||\\\\Psi(\\\\phi_j(G(I_m))) - \\\\Psi(\\\\phi_j(I))||_1\\n\\\\]\\n\\nWhere \\\\(j\\\\) is the index of the layer, \\\\(N_j\\\\) denotes the number of elements in each layer, \\\\(\\\\Psi\\\\) is the Gram matrix function \\\\cite{14} and \\\\(\\\\delta_f\\\\) and \\\\(\\\\delta_s\\\\) are constant weighting factors.\"}"}
{"id": "CVPR-2023-992", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Additionally, the adversarial loss is added to $L^G$. Any adversarial loss function can be used, but for our base-line experiments we use LS-GAN [35] which has shown to achieve more stable optimisation over the original min-max classification loss. This gives us the generator-discriminator loss pair:\\n\\n$$L^{D_{adv}} = \\\\frac{1}{2} [(D(I) - 1)^2] + \\\\left[(D(G(I_{im})) - 1)^2\\\\right]$$\\n\\n$$L^{G_{adv}} = \\\\frac{1}{2} [(D(G(I_{im})) - 1)^2]$$\\n\\nThe full loss function for the decoder then becomes:\\n\\n$$L^G = ||G(I_{im}) - I||_1 + L^{G_{feat}} + L^{G_{adv}}$$\\n\\nBeyond the feature matching acting as a learnt perceptual loss, adding this term also has the further advantage of stabilising adversarial training.\\n\\ndV AE perceptual:\\nTo provide a perceptual learning baseline also using the stronger supervision from a pre-trained network for comparison, we experiment with feature matching loss with the discrete variational autoencoder (dV AE) from [38]. For this, the same feature matching loss in Equation 3 above is used with the pre-trained dV AE encoder model component acting as the loss network $\\\\phi$. This then is the perceptual loss term, giving the full loss function for the decoder:\\n\\n$$L^G = ||G(I_{im}) - I||_1 + L^{G_{feat}}$$\\n\\nThe dV AE was trained for image tokenization on the DALL-E dataset, comprising 250 million images. Therefore, the rich features encoded in its weights provide strong higher-order perceptual cues for decoder training.\\n\\n3.2. Adversarial Training Variants\\nFor feature matching based perceptual learning, any adversarial loss function can be used. In addition to the LS-GAN loss used in our baseline model, we experimented with two further variants, introduced below. Both were formulated to address issues with the original GAN formulation such as training instability and mode collapse [23]. We hypothesize that the richer distributions learnt by these methods will provide stronger cues for perceptual learning.\\n\\nMSG-GAN:\\nTo stabilise the training of the generator, MSG-GAN [23] allows for the flow of gradients from the discriminator to the generator at multiple scales. This is done by adding skip connections from intermediate layers of the generator to intermediate layers of the discriminator. The loss function for training $D$ and $G$ remains unchanged.\\n\\nStyleGANv2-ADA:\\nWe take all modifications made to the discriminator in the StyleGANv2-ADA [25] paper. Building on MSG-GAN, perceptual path regularisation between the decoder input and discriminator feature maps is added. Adaptive discriminator augmentation is also applied to all samples during training. The loss function for training $D$ and $G$ remains unchanged.\\n\\n3.3. Model Architecture\\nOne issue with the multi-scale GAN formulation used for both MSG-GAN and StyleGANv2-ADA methods is that the multi-scale learning occurs via skip connections between the discriminator $D$ and decoder $G$. This means that only the decoder benefits from the multi-scale gradient penalty during training, and this is removed post pre-training, leaving only the encoder when adapting to down-stream tasks.\\n\\nTo distribute the learning more evenly between encoder and decoder, similar to U-Net [39] we additionally introduce skip connections between intermediate encoder and decoder layers, as shown in Figure 1. In this modified architecture, which we term MSG-MAE, multi-scale signal is shared also with the encoder. In further detail: mask tokens are added to the encoder feature maps of dimension $d$ along with positional embeddings. Via the skip connections, this is concatenated with the decoder feature map of the matching scale, creating a sequence of length $2^d$. This combined feature is passed through a single learnt linear layer to return the dimension to $d$. The output is then processed by the decoder transformer layer, with the result forward propagated to the next layer and simultaneously converted to an RGB image at the required scale for the multi-scale loss.\\n\\n4. Implementation Details\\nPre-training:\\nWe use the ViT-B and ViT-L architectures from the MAE paper [17], with ViT-B and ViT-L trained for 300 and 1600 epochs respectively over the ImageNet-1K (IN1K) [7] training set. In each case, the input patch size is fixed to 16x16 and we mask 75% of input patches during training. For both ViT-B and ViT-L, the decoder architecture remains consistent. The model dimensions, hyperparameters and data augmentation strategies follow those of the original MAE paper [17] and we train with a batch size of 16. The Adam optimizer is used with a weighted decay, where the learning rate is 0.00015, weight decay is 0.05 (cosine strategy), 40 warm-up epochs are used and the momentum parameters $\\\\beta_1$ and $\\\\beta_2$ are 0.9 and 0.95.\\n\\nIn our experiments, the weighting factors in $L^G_{feat}$ (where applied) is given a weighting factor $\\\\delta_f$ of 0.05. The $L^G_{ssim}$ weighting factor $\\\\alpha$ is set to 0.85. In both cases, the result is to focus learning on the perceptual term, with the smaller $\\\\delta_s$ value still resulting in a large weighting once accounting for the larger relative magnitude of the $L^G_{feat}$ term. The parameter choice is based on other works in the literature [32, 41, 46]. $\\\\delta_s$ is given a fixed value of 40. In order to give time for the discriminator to learn new features with which to compute perceptual similarity, a training schedule is used where the learning rate decreases from 0.00015 to 0.000015 over the last 1000 epochs.\"}"}
{"id": "CVPR-2023-992", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. In ICLR, April 2022.\\n\\n[2] Tom Brown, Benjamin Mann, Nick Ryder, and et. al. Sub-biah. Language models are few-shot learners. volume 33, pages 1877\u20131901. Curran Associates, Inc., 2020.\\n\\n[3] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021.\\n\\n[4] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pre-training from pixels. volume 119 of Proceedings of Machine Learning Research, pages 1691\u20131703. PMLR, 13\u201318 Jul 2020.\\n\\n[5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. volume 119 of Proceedings of Machine Learning Research, pages 1597\u20131607. PMLR, 13\u201318 Jul 2020.\\n\\n[6] Xinlei Chen*, Saining Xie*, and Kaiming He. An empirical study of training self-supervised vision transformers. arXiv preprint arXiv:2104.02057, 2021.\\n\\n[7] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In CVPR, pages 248\u2013255, 2009.\\n\\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina N. Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. 2018.\\n\\n[9] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. volume 34, pages 8780\u20138794. Curran Associates, Inc., 2021.\\n\\n[10] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP. In ICLR. OpenReview.net, 2017.\\n\\n[11] Carl Doersch, Abhinav Gupta, and Alexei A. Efros. Unsupervised visual representation learning by context prediction. In ICCV, pages 1422\u20131430, 2015.\\n\\n[12] Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, and Nenghai Yu. PeCo: Perceptual codebook for bert pre-training of vision transformers. arXiv preprint arXiv:2111.12710, 2021.\\n\\n[13] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. volume 27. Curran Associates, Inc., 2014.\\n\\n[14] Leon A. Gatys, Alexander S. Ecker, and Matthias Bethge. Image style transfer using convolutional neural networks. In CVPR, pages 2414\u20132423, 2016.\\n\\n[15] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. ArXiv, abs/1803.07728, 2018.\\n\\n[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. volume 27. Curran Associates, Inc., 2014.\\n\\n[17] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. CVPR, 2022.\\n\\n[18] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In ICCV, pages 2980\u20132988, 2017.\\n\\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. page 6629\u20136640, Red Hook, NY , USA, 2017. Curran Associates Inc.\\n\\n[20] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with a constrained variational framework. In ICLR, 2017.\\n\\n[21] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In ICPR, pages 2366\u20132369, 2010.\\n\\n[22] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In ECCV, 2016.\\n\\n[23] Animesh Karnewar, Oliver Wang, and Raghu Sesha Iyengar. Msg-gan: Multi-scale gradient gan for stable image synthesis. In CVPR, 2020.\\n\\n[24] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. Progressive growing of gans for improved quality, stability, and variation. In ICLR, 2018.\\n\\n[25] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. 2020.\\n\\n[26] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In CVPR, pages 4396\u20134405, 2019.\\n\\n[27] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In CVPR, pages 8107\u20138116, 2020.\\n\\n[28] Durk P Kingma and Prafulla Dhariwal. Glow: Generative flow with invertible 1x1 convolutions. volume 31. Curran Associates, Inc., 2018.\\n\\n[29] Diederik P. Kingma and Max Welling. Auto-Encoding Variational Bayes. In ICLR, 2014.\\n\\n[30] Alexander Kolesnikov, Alexey Dosovitskiy, Dirk Weisbensen, Georg Heigold, Jakob Uszkoreit, Lucas Beyer, Matthias Minderer, Mostafa Dehghani, Neil Houlsby, Sylvain Gelly, Thomas Unterthiner, and Xiaohua Zhai. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\\n\\n[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, Z\u00fcrich, 2014. Oral.\\n\\n[32] Guilin Liu, Fitsum A. Reda, Kevin J. Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irregular holes using partial convolutions. In ECCV, 2018.\"}"}
{"id": "CVPR-2023-992", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized bert pretraining approach, 2019. cite arxiv:1907.11692.\\n\\nRafa\u0142 Mantiuk, Kil Joong Kim, Allan G. Rempel, and Wolfgang Heidrich. HDR-VDP-2: A calibrated visual metric for visibility and quality predictions in all luminance conditions. ACM Trans. Graph., 30(4), jul 2011.\\n\\nXudong Mao, Qing Li, Haoran Xie, Raymond Y. K. Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. pages 2813\u20132821, 2017.\\n\\nAlexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. volume 162 of Proceedings of Machine Learning Research, pages 16784\u201316804. PMLR, 17\u201323 Jul 2022.\\n\\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In Bastian Leibe, Jiri Matas, Nicu Sebe, and Max Welling, editors, ECCV, pages 69\u201384, Cham, 2016. Springer International Publishing.\\n\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. volume 139 of Proceedings of Machine Learning Research, pages 8821\u20138831. PMLR, 18\u201324 Jul 2021.\\n\\nO. Ronneberger, P. Fischer, and T. Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, volume 9351 of LNCS, pages 234\u2013241. Springer, 2015. (available on arXiv:1505.04597 [cs.CV]).\\n\\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, Xi Chen, and Xi Chen. Improved techniques for training gans. volume 29. Curran Associates, Inc., 2016.\\n\\nSamyakh Tukra, Hani J. Marcus, and Stamatia Giannarou. See-through vision with unsupervised scene occlusion reconstruction. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):3779\u20133790, 2022.\\n\\nA \u00a8aron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. ArXiv, abs/1807.03748, 2018.\\n\\nAaron van den Oord, Oriol Vinyals, and koray kavukcuoglu. Neural discrete representation learning. volume 30. Curran Associates, Inc., 2017.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. volume 30, 2017.\\n\\nP. Vincent, H. Larochelle, Y. Bengio, and P.-A. Manzagol. Extracting and composing robust features with denoising autoencoders. 2008.\\n\\nTing-Chun Wang, Ming-Yu Liu, Jun-Yan Zhu, Andrew Tao, Jan Kautz, and Bryan Catanzaro. High-resolution image synthesis and semantic manipulation with conditional gans. In CVPR, 2018.\\n\\nXintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In ECCV, September 2018.\\n\\nZhou Wang, Alan C. Bovik, Hamid R. Sheikh, and Eero P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600\u2013612, 2004.\\n\\nZ. Wang, E.P. Simoncelli, and A.C. Bovik. Multiscale structural similarity for image quality assessment. In The Thirty-Seventh Asilomar Conference on Signals, Systems & Computers, volume 2, pages 1398\u20131402 Vol.2, 2003.\\n\\nTete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV. Springer, 2018.\\n\\nZhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In CVPR, 2022.\\n\\nLin Zhang, Lei Zhang, Xuanqin Mou, and David Zhang. FSIM: A feature similarity index for image quality assessment. IEEE Transactions on Image Processing, 20(8):2378\u20132386, 2011.\\n\\nRichard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In ECCV, 2016.\\n\\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018.\\n\\nBolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. IJCV, 127(3):302\u2013321, 2019.\"}"}
{"id": "CVPR-2023-992", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. The Multi-Scale Gradient MAE (MSG-MAE) architecture. Dotted lines denote skip connections and solid forward propagation.\\n\\nwhereby the perceptual loss term $L_G$ is applied only on even numbered epochs is used. This avoids the generator distribution collapsing to that of the discriminator and ensures well-balanced learning. All experiments were conducted on a GPU cluster consisting of 8xV100 Nvidia GPUs.\\n\\nFine-tuning:\\n\\nFor fine-tuning, we take the pre-trained MAE encoder model and replace the decoder architecture with a task-specific head (initialised with random weights), similar to [17]. For image classification we replace the decoder with the original ViT model classification head [30]. For object detection and segmentation on MS-COCO [31] we use a Mask-RCNN [18] decoder model and for ADE-20K semantic segmentation [55] we adopt the UperNet model [50] as our decoder. When transferring to downstream tasks the intermediate feature maps of MSG-MAE are not used for the classification tasks. However, they are used for detection and segmentation, given the Mask-RCNN and UperNet architectures operate at multiple scales. All fine-tuned models are trained with an Adam optimizer with weighted decay. The learning rate is 0.001, weight decay is 0.05 (cosine strategy), warm-up epochs 5, and momentum parameters $\\\\beta_1$ and $\\\\beta_2$ are 0.9 and 0.95.\\n\\n5. Experiments\\n\\nIn this section, we evaluate our models following self-supervised pre-training on the ImageNet-1K (IN1K) [7] training set. We first explore the main properties of the learnt representations in Section 5.1 in terms of (i) the fidelity of reconstructed output, (ii) the qualitative attention maps from the pre-trained model and (iii) linear probe results for downstream classification. Following this, in Section 5.2 we show the downstream performance of our models for transfer learning comparing this to previous work: fine-tuning on ImageNet-1K for classification, COCO for object detection and ADE20K for segmentation.\\n\\n5.1. Main properties\\n\\nImage reconstruction.\\n\\nIn Table 1 we evaluate the reconstruction quality of the decoder stage of our models.\"}"}
{"id": "CVPR-2023-992", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Figure 2. Differences in reconstruction quality of the model variants on samples from the ImageNet-1K validation set. The key areas of focus are highlighted in green. Columns (A-B) show the ground truth image and masking, (C-G) the reconstructed image for each method.\\n\\nTable 2. Classification performance on ImageNet1K (IN1K). All models are pre-trained via self-supervision followed by either training of a linear probe or fine-tuning. Loss functions with '-P' include a perceptual loss term. The best result is highlighted in bold.\\n\\n| Method     | Loss Function | Pre-training Data | ViT-B | ViT-B | ViT-L |\\n|------------|---------------|-------------------|-------|-------|-------|\\n| iGPT [4]   | Cross Entropy  | IN1K              | \u2013     | \u2013     | 66.5  |\\n| DINO [3]   | Cross Entropy  | IN1K              | 78.2  |       | \u2013     |\\n| MoCo v3 [6]| InfoNCE [42]  | IN1K              | 76.7  | 83.2  | 84.1  |\\n| BEiT [1]   | Negative Log Likelihood | IN1K + DALL-E | 56.7  | 83.2  | 85.2  |\\n| MAE [17]   | MSE            | IN1K              | 67.8  | 83.6  |       |\\n|            | a              |                   |       |       | 85.9  |\\n| MAE        | MS-SSIM + L1   | IN1K              | 71.2  | 84.1  | 86.3  |\\n| MAE        | LS-GAN-P       | IN1K              | 72.5  | 84.5  | 86.5  |\\n| MSG-MAE    | MSG-GAN-P      | IN1K              | 75.6  | 85.3  |       |\\n|            | b              |                   |       |       | 87.2  |\\n| MSG-MAE    | StyleGANv2-ADA-P | IN1K         | 78.1  | 86.2  | 88.1  |\\n| MAE        | dV AE-P        | IN1K + DALL-E     | 79.8  | 86.9  | 88.6  |\\n| MAE        | LS-GAN         | IN1K              | \u2013     | 83.3  | 85.3  |\\n| MSG-MAE    | MSG-GAN        | IN1K              | \u2013     | 84.7  |       |\\n|            | c              |                   |       |       | 86.5  |\\n| MSG-MAE    | MSG-GAN-P      | IN1K              | \u2013     | 83.2  |       |\\n|            | d              |                   |       |       | 85.6  |\\n| MAE        | StyleGANv2-ADA-P | IN1K         | \u2013     | 84.5  | 86.2  |\\n\\nOver the IN1K validation set using the following quantitative measures: L1 error, Peak Signal to Noise Ratio (PSNR), Structural Similarity Index (SSIM) [49], Inception Score (IS) [40] and Fr\u00e9chet inception distance (FID) [19].\\n\\nThese experiments use the ViT-B variant of the encoder, pre-trained using each of our perceptual losses. For each of our methods, we observe a gradual increase in the fidelity of the reconstructed patches on the pixel-level measures (L1, PSNR, SSIM). However, what is particularly striking is the consistent boost of +10% for each method in FID score (with a similar pattern observed for IS). These methods compute a higher-level notion of perceptual similarity by comparing intermediary feature maps from a network pre-trained using a supervised objective for real and generated images, and suggests that through the introduction of a perceptual loss term the decoder learns a more generalisable notion of perceptual similarity. Examples of reconstructed patches are shown in Figure 2.\\n\\nSelf-attention maps. To evaluate qualitatively whether the features learnt by our approach properly capture the high-level semantics of the image over low-level details we visualise the attention maps from the final layer of our network, shown in Figure 3. Compared to the original MAE formulation, the combination of perceptual and ad...\"}"}
{"id": "CVPR-2023-992", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Object detection and semantic segmentation performance on MS COCO and ADE20K. All models were pre-trained using the ImageNet-1K training set (without labels). Loss functions with \u2018-P\u2019 include a perceptual loss term. Best results are highlighted in bold.\\n\\n| Method          | Loss Function                  | Pre-training Data | mAP Box | mAP Mask | mIoU |\\n|-----------------|--------------------------------|-------------------|---------|----------|------|\\n| DINO [3]        | Cross Entropy                  | IN1K              | 44.1    |          |      |\\n| MoCo v3 [6]     | InfoNCE [42]                   | IN1K              | 47.9    | 42.7     | 47.3 |\\n| BEiT [1]        | Negative Log Likelihood        | IN1K + DALL-E     | 49.8    | 44.4     | 47.1 |\\n| MAE [17]        | MSE                            | IN1K              | 50.3    | 44.9     | 48.1 |\\n| MAE MS-SSIM + L1| IN1K                           | 50.8              | 45.1    |          | 48.8 |\\n| MAE LS-GAN-P    | IN1K                           | 51.4              | 45.4    |          | 49.2 |\\n| MSG-MAE MSG-GAN-P| IN1K                        | 52.3              | 45.8    |          | 49.7 |\\n|               | MSG-GAN-P                      | IN1K              | 53.5    | 46.1     | 50.4 |\\n| MAE dV AE-P     | IN1K + DALL-E                  | 53.9              | 46.4    |          | 50.9 |\\n| MAE StyleGANv2-ADA-P | IN1K                | 50.9              | 45.9    |          | 49.3 |\\n| MAE StyleGANv2-ADA-P | IN1K                | 51.8              | 45.5    |          | 49.1 |\\n\\nVersarial loss (LS-GAN) leads to sharper focus on the object in the frame despite no supervision being used during training. The addition of multi-scale gradients (MSG-GAN) and adaptive discriminator augmentation and perceptual path length regularisation (StyleGANv2-ADA) brings further improvement.\\n\\nIn particular, with our best method, we achieve similar qualitative results to DINO [3], a self-supervised method which takes a contrastive approach to learning and requires careful balancing of the loss and sampling of image crops within batches compared to the simpler reconstruction-based approach used by our method.\\n\\nLinear probing. To evaluate quantitatively the extent to which the features learnt by our approach capture useful semantic information, a common approach is to freeze the backbone of the pre-trained encoder model and train a simple linear classifier on top. We report the results for this over the IN1K validation set in Table 2, comparing to the MAE, BEiT and contrastive learning approach MoCo v3.\\n\\nOur baseline model variant trained via MS-SSIM achieves 71.2% accuracy, 3% higher than the original MAE trained via MSE [17]. StyleGANv2-ADA-P attains 78.1%, a boost of 10% compared to the original MAE. This significant increase suggests that our perceptual loss term leads to much more informative features being learnt without fine-tuning with labels being necessary. For comparison, we also include results when using perceptual loss computed instead against a pre-trained network. When adding this stronger supervision, the accuracy further improves to 79.8%, although this introduces a dependency on an external network and training data magnitudes larger than IN1K.\\n\\nTable 4. Computational cost. The relative time to train per epoch. All figures computed with the ViT-B architecture using 8xV100s.\\n\\n|                      | Rel. time / epoch | # Parameters |\\n|----------------------|-------------------|--------------|\\n| MAE LS-GAN-P         | 0.23              | 113M         |\\n| StyleGANv2-ADA-P    | 0.54              | 113M (+5%)   |\\n\\nComputational properties. All experiments were run on 8xV100s, and took between 1-3 weeks to train to convergence for ViT-B. The relative training times and parameter counts are shown in Table 4. We also tried training MAE longer (e.g. 900 epochs instead of 300, to match the total time for StyleGANv2-ADA-P) and did not observe significant performance improvement.\\n\\n5.2. Downstream Learning Results\\n\\nImage classification. We fine-tune our models on IN1K, with the results shown in Table 2. Using ViT-B, we see a consistent boost in performance across the board by adding a perceptual loss term, obtaining an accuracy of 86.2% with our best method (StyleGANv2-ADA-P) and outperforming MAE and BEiT by 2.6% and 3% respectively.\\n\\nMoving to ViT-L architecture, we obtain 88.1% accuracy, outperforming all previous methods training only on IN1K data and in particular resulting in comparable accuracy to that reported by MAE of 87.8% using the much larger ViT-H 448 architecture (632M parameters vs 86M parameters, with input image of size 448 rather than 224).\\n\\nIf we use a pre-trained network with dV AE-P, we obtain a further boost of our best accuracy to 88.6%.\"}"}
{"id": "CVPR-2023-992", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Attention maps of models pre-trained on ImageNet-1K without labels. We visualise the self-attention of the \\\\([\\\\text{CLS}]\\\\) token of the last layer. Sample images were selected randomly from the ImageNet-1K validation set. Column (A) is the original input image, (B-F) the outputs from our different losses and (G) is the output from DINO for comparison.\\n\\nCOCO with the results shown in Table 3. Our best method achieves \\\\(53.5\\\\) AP\\\\(_{\\\\text{box}}\\\\) using a ViT-B architecture, which outperforms the previous best reported result of MAE trained with ViT-B by \\\\(3.2\\\\). Similarly, for semantic segmentation, we fine-tune an UperNet head on ADE20K with the results shown also in Table 3. Again using a ViT-B architecture, here we obtain up to \\\\(50.4\\\\) mIOU, \\\\(1.2\\\\) higher than for MAE.\\n\\nImpact of perceptual loss term. When training a base-line LS-GAN without a perceptual loss term, we are unable to train a model that performs better than the baseline MSE loss [17] and observe clear stability issues during training. However, with an MSG-GAN loss training is much more stable. Referring to Table 2, for ViT-B with adversarial and reconstruction loss only we obtain a \\\\(1.1\\\\)% boost (a vs. c) over the baseline MSE loss for image classification. However, this remains less than the \\\\(1.7\\\\)% boost (a vs. b) with perceptual component added. An even larger gap is observed for ViT-L (\\\\(0.6\\\\)% vs. \\\\(1.3\\\\)% boost). This suggests the perceptual loss term plays an important role not only for training stability but also is a large driver of performance.\\n\\nImpact of multi-scale MAE. Training with a multi-scale loss without updating the MAE architecture as described in Section 3.3 results in a drop of performance of over \\\\(2\\\\)% for image classification as seen in Table 2 (b vs. d), with a similar drop also observed for object detection and semantic segmentation in Table 3 (b vs. d).\\n\\n6. Conclusion\\nWe explored a method for incorporating the learning of higher-level features from images explicitly into the learning objective of a masked autoencoder. By introducing a perceptual loss term and adversarial training, we showed how the representations learnt by MAE [17] could be improved, boosting transfer performance for downstream tasks such as image classification, object detection and semantic segmentation. In particular, this performance boost is observed not only when fine-tuning, but also in the linear probe setting where contrastive methods have historically done better. This suggests that by combining the rich super-vision of the pixel reconstruction task with a more focused higher-level learning signal we can greatly improve the data efficiency of the masked autoencoder approach.\\n\\nThis work also helps to start to address one of the key differences between the use of masked modelling for images and text: that images and image patches do not have inherent semantic meaning. Many questions remain about how to learn cues of the right level of abstraction directly from image data.\"}"}
