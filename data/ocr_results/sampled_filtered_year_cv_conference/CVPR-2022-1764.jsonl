{"id": "CVPR-2022-1764", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stochastic Variance Reduced Ensemble Adversarial Attack for Boosting the Adversarial Transferability\\n\\nYifeng Xiong1, Jiadong Lin1, Min Zhang1, John E. Hopcroft2, Kun He1\u2020\\n\\n1Department of Computer Science, Huazhong University of Science and Technology, Wuhan, China\\n2Department of Computer Science, Cornell University, Ithaca, NY, USA\\n\\n{xiongyf,jdlin,mzhang}@hust.edu.cn, jeh@cs.cornell.edu, brooklet60@hust.edu.cn\\n\\nAbstract\\n\\nThe black-box adversarial attack has attracted impressive attention for its practical use in the field of deep learning security. Meanwhile, it is very challenging as there is no access to the network architecture or internal weights of the target model. Based on the hypothesis that if an example remains adversarial for multiple models, then it is more likely to transfer the attack capability to other models, the ensemble-based adversarial attack methods are efficient and widely used for black-box attacks. However, ways of ensemble attack are rather less investigated, and existing ensemble attacks simply fuse the outputs of all the models evenly. In this work, we treat the iterative ensemble attack as a stochastic gradient descent optimization process, in which the variance of the gradients on different models may lead to poor local optima. To this end, we propose a novel attack method called the stochastic variance reduced ensemble (SVRE) attack, which could reduce the gradient variance of the ensemble models and take full advantage of the ensemble attack. Empirical results on the standard ImageNet dataset demonstrate that the proposed method could boost the adversarial transferability and outperforms existing ensemble attacks significantly. Code is available at https://github.com/JHL-HUST/SVRE.\\n\\n1. Introduction\\n\\nDeep neural networks (DNNs) have shown impressive performance on various computer vision tasks. However, recent researches have shown that DNNs are strikingly vulnerable to adversarial examples crafted by adding human-imperceptible perturbations [7,23,28]. Moreover, adversarial examples are known to be transferable that the examples crafted for one model can also mislead other black-box models [17, 20, 22]. Generating adversarial examples, i.e., adversarial attack, has drawn enormous attention since it can help evaluate the robustness of different models [2, 29] and improve their robustness by adversarial training [7,19]. Various adversarial attack methods have been proposed, including the optimization-based methods such as box-constrained L-BFGS [28] and Carlini & Wagner's method [2], the gradient-based methods such as Fast Gradient Sign Method [7] and its iterative variants [13,19], etc. In general, these adversarial attack methods can achieve high attack success rates in the white-box setting [2], where the attacker can access the complete information of the target model, including the model architecture and gradient information. However, these methods often exhibit low attack success rates in the black-box setting [3], where the attacker cannot access the information of the target model. In this case, the attacker either utilizes the transferability of adversarial examples to fool the black-box model, or attacks directly based on a small amount of queries on the output of the black-box model.\\n\\nIn recent years, a number of methods have been proposed to enhance the transferability of adversarial examples so as to improve the attack success rates in the black-box setting, including the gradient optimization attacks [3, 16, 31], input transformation attacks [4, 16, 35], and model ensemble attacks [3, 17]. Among these methods, the model ensemble attacks are efficient and have been broadly adopted in boosting the black-box attack performance [5, 16, 35]. However, as compared to the other two categories that have been explored in depth, the category of model ensemble attack is rather less investigated.\\n\\nIn this work, we observe that the existing model ensemble attack methods simply fuse the outputs of all models directly but ignore the variance of the gradients on different models, which may limit the potential capability of the model ensemble attacks. Due to the inherent difference of the model architectures, the optimization paths of the models may differ widely, indicating that there exists considerable difference on the variance of the gradient directions among the possible models. Such variance may cause the...\"}"}
{"id": "CVPR-2022-1764", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"optimization direction of the ensemble attack to be less accurate. As a result, the attack capability of the transferred adversarial examples is rather limited.\\n\\nTo address this issue, we propose a novel method called the stochastic variance reduced ensemble (SVRE) attack to enhance the adversarial transferability of ensemble attacks. Our method is inspired by the stochastic variance reduced gradient (SVRG) method [12] designed for stochastic optimization, which has an outer loop that maintains an average gradient on a batch of data and an inner loop that randomly draws an instance from the batch and calculates an unbiased estimate of gradient based on the variance reduction. In our method, we regard the ensemble models as the batch of data for the outer loop and randomly draw a model at each iteration of the inner loop. Taking the benign image as the initial adversarial example, the outer loop calculates the average gradient on the batch of models, and copies the current example to the inner loop, then the inner loop conducts multiple iterations of inner adversarial example updates. At each inner iteration, SVRE calculates the current gradient on a randomly picked model, tuned by the gradient bias of the outer adversarial example on this randomly picked model and on the ensemble model. At the end of the inner loop, the outer adversarial example is updated using the tuned gradient of the newest inner adversarial example. In this way, SVRE can obtain a more accurate gradient update at the outer loop to escape from poor local optima such that the crafted adversarial example would not \\\"overfit\\\" the ensemble model. Hence, the crafted adversarial example is expected to have higher transferability to other unknown models. To our knowledge, this is the first work to investigate the limitation of existing ensemble attack through the lens of gradient variance on multiple models. Extensive experiments on the ImageNet dataset demonstrate that SVRE consistently outperforms the vanilla ensemble model attack in the black-box setting.\\n\\n2. Related Works\\n\\nLet $x$ and $y$ be a benign image and the corresponding true label, respectively. Let $J(x, y)$ be the loss function of the classifier and $B_\\\\epsilon(x) = \\\\{x': \\\\|x-x'\\\\|_p \\\\leq \\\\epsilon\\\\}$ be the $L_p$-norm ball centered at $x$ with radius $\\\\epsilon$. The goal of non-targeted adversarial attacks is to search for an adversarial example $x_{adv} \\\\in B_\\\\epsilon(x)$ that maximizes the loss $J(x_{adv}, y)$.\\n\\nTo align with previous works, we focus on $L_\\\\infty$-norm non-targeted adversarial attacks.  \\n\\n2.1. Adversarial Attacks\\n\\nExisting adversarial attack methods can be categorized into three groups, namely gradient optimization attacks [3, 7, 13, 16, 31], input transformation attacks [3, 16, 32, 35], and model ensemble attacks [3, 17].\\n\\nGradient optimization attacks. The most typical adversarial attack based on gradient is the Fast Gradient Sign Method (FGSM) [7], which uses the gradient direction of the loss function with respect to the input image to generate a fixed amount of perturbation. Kurakin et al. [13] propose the Basic Iterative Method (BIM) to run multiple iterations of FGSM with a small perturbation. Madry et al. [19] propose a noisy version of BIM, named the Projected Gradient Descent (PGD). Although PGD exhibits good attack performance in the white-box setting [1], it overfits the target model easily and yields weak transferability in the black-box setting. In order to improve the transferability of adversarial attacks, Dong et al. [3] propose to boost the adversarial attack with momentum. More recently, Lin et al. [16] introduce Nesterov accelerated gradient method into the gradient-based attack to look ahead effectively to avoid overfitting. Wang et al. [31] reduce the variance of the gradient at each iteration to stabilize the update direction.\\n\\nInput transformation attacks. Another line of attacks focuses on adopting various input transformations to further improve the transferability of adversarial examples. Xie et al. [35] propose the Diverse Input Method (DIM), which utilizes random resizing and padding to create diverse input patterns to generate adversarial examples. Dong et al. [4] propose the Translation-Invariant Method (TIM), which optimizes the perturbation over a set of translated images. Lin et al. [16] discover the scale-invariant property of deep learning models and propose the Scale-Invariant Method (SIM), which optimizes the adversarial perturbations over the scale copies of the input images. Wang et al. [32] propose the Admix, that calculates the gradient on the input image admixed with a small portion of each add-in image to craft more transferable adversaries.\\n\\nModel ensemble attacks. Liu et al. [17] find that attacking multiple models simultaneously can also improve the attack transferability. They fuse the predictions of multiple models to get the loss of ensemble predictions and adopt existing adversarial attacks (e.g., FGSM and PGD) to generate adversarial examples using the loss. Dong et al. [3] propose two variants of the model ensemble attack, namely fusing the logits and fusing the losses, respectively. Compared with various explorations on gradient optimization or input transformation, the model ensemble attacks are far less investigated, and existing methods only simply fuse the output predictions, logits, or losses.\\n\\n2.2. Adversarial Defenses\\n\\nAs the counterpart of adversarial attacks, various defense methods have also been proposed, including adversarial training based defenses [6, 19, 24, 25, 28, 30, 34, 37] and input transformation based defenses [8, 10, 11, 15, 18, 21, 33, 36].\\n\\nAdversarial training based defenses. Adversarial training is considered one of the most efficacious defense methods.\"}"}
{"id": "CVPR-2022-1764", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"approaches which augments the training data by generating adversarial examples during the training process. Tram`er et al. [30] propose ensemble adversarial training, which augments the training data with perturbations transferred from other models. Madry et al. [19] propose PGD-Adversarial Training (PGD-AT), which augments the training data with adversarial examples crafted by PGD attack. Xie et al. [34] develop new network architectures that increase adversarial robustness by performing feature denoising. Adversarial training, while promising, is computationally expensive and hard to scale to large-scale datasets [14].\\n\\nInput transformation based defenses. This line of defenses aims to diminish the adversarial perturbations from the input data. Guo et al. [8] and Xie et al. [33] conduct transformations on images to remove the adversarial perturbations. Liao et al. [15] use high-level representation guided denoiser (HGD) to purify the adversarial images. Xu et al. [36] propose two feature squeezing methods, i.e. bit reduction (Bit-R) and spatial smoothing to detect adversarial examples. Liu et al. [18] propose the feature distillation (FD), which adopts a JPEG-based defensive compression framework to diminish the adversarial perturbations. Jia et al. [11] utilizes an end-to-end image compression model named ComDefend to defend against adversarial examples. Jia et al. [10] leverage the randomized smoothing (RS) to train a certifiably robust ImageNet classifier. Naseer et al. [21] develop a neural representation purifier (NRP) model, which learns to purify the adversarially perturbed images through automatically derived supervision.\\n\\n3. Methodology\\n\\nWe focus on addressing the adversarial transferability through the lens of reducing the gradient variance of the ensemble models used for crafting the adversarial example. Since our method is based on the model ensemble attack, we first introduce the existing ensemble attack methods, then present our motivation and elaborate the proposed SVRE in detail.\\n\\n3.1. Ensemble Attack Methods\\n\\nThe ensemble attack [3, 17] is an effective strategy to enhance the adversarial transferability. Its basic idea is to generate the adversarial examples using multiple models.\\n\\nEnsemble on predictions. Liu et al. [17] first propose to achieve an ensemble attack by averaging the predictions (predicted probability) of the models. For an ensemble of K models, the loss function of the ensemble model is:\\n\\n$$ J(x, y) = -\\\\frac{1}{y} \\\\cdot \\\\log(P_{k=1}^{K} w_k p_k(x)), $$\\n\\nwhere \\\\( y \\\\) is the one-hot encoding of the ground-truth label \\\\( x \\\\), \\\\( p_k \\\\) is the prediction of the \\\\( k \\\\)-th model, and \\\\( w_k \\\\geq 0 \\\\) is the ensemble weight constrained by \\\\( \\\\sum_{k=1}^{K} w_k = 1 \\\\).\\n\\nEnsemble on logits. Dong et al. [3] propose to fuse the logits (output before softmax) of models. For the ensemble of \\\\( K \\\\) models, the loss function ensembled on logits is:\\n\\n$$ J(x, y) = -\\\\frac{1}{y} \\\\cdot \\\\log(\\\\text{softmax}(P_{k=1}^{K} w_k l_k(x))), $$\\n\\nwhere \\\\( l_k \\\\) is the logits of the \\\\( k \\\\)-th model.\\n\\nEnsemble on losses. Dong et al. [3] also introduce an alternative ensemble attack by averaging the loss of \\\\( K \\\\) models as follows:\\n\\n$$ J(x, y) = \\\\sum_{k=1}^{K} w_k J_k(x, y), $$\\n\\nwhere \\\\( J_k \\\\) is the loss of the \\\\( k \\\\)-th model. For the weight parameters, the three methods simply choose the average weight in experiments, i.e. \\\\( w_k = \\\\frac{1}{K} \\\\).\\n\\n3.2. Rethinking the Ensemble Attack\\n\\nThe ensemble attack method has been broadly adopted in enhancing the performance of black-box attacks [3,5,16,17,31,35]. However, to our knowledge, researchers only utilize the existing ensemble attack strategy as a plug-and-play module to enhance their own attack methods, but did not delve into the ensemble attack method itself.\\n\\nIntuitively, existing ensemble attack methods [3, 17] are helpful in improving the adversarial transferability because attacking an ensemble model can help to find better local maxima and makes it easier to generalize to other black-box models. However, merely averaging the outputs (logits, predictions or loss) of the models to build an ensemble model for the adversarial attack may limit the attack performance, as the individual optimization path of different model may vary diversely, but the variance is not considered, leading to an overfit to the ensemble model.\\n\\nAs demonstrated in Figure 1, the cosine similarity between the update direction of a sampled image on different models is extremely low, indicating there exists a considerable gap in the optimization direction among these models (See model details in Section 4.1). We argue that simply...\"}"}
{"id": "CVPR-2022-1764", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fusing the predictions/logits/losses of the models but ignoring the variance of the gradients on different models would lead to a suboptimal result, and limit the performance of ensemble attacks.\\n\\n3.3. Stochastic Variance Reduced Ensemble Attack\\n\\nIn previous works, Lin et al. and Wang et al. [16, 31] analogize the process of the adversarial example generation to the process of neural network training, of which the white-box model is analogized to the training data and the black-box model is analogized to the test data. Hence, the iterative optimization on crafting the adversarial example using the input image can be regarded as the parameter update of neural networks, and the transferability of the adversarial example is analogized to the generalization of models.\\n\\nIn this work, we treat the iterative ensemble attack as a stochastic gradient descent optimization process, in which at each iteration, the attacker always chooses the batch of the ensemble models for update. During the course of the adversarial example generation, the gradient variance on different models may lead to poor local optima. Hence, we aim to reduce the gradient variance so as to stabilize the gradient update direction, making the induced gradient be generalized better to other possible models.\\n\\nInspired by the stochastic variance reduced gradient (SVRG) method [12] designed for stochastic optimization, we propose a stochastic variance reduced ensemble attack method to address the gradient variance of the models so as to take full advantage of the ensemble attack. The basic idea of SVRG is to reduce the inherent variance of Stochastic Gradient Descent (SGD) using predictive variance reduction, while we aim to reduce the inherent gradient variance of multiple models. The integration of SVRE with MI-FGSM [3], SVRE-MI-FGSM, is summarized in Algorithm 1.\\n\\nDenote the traditional model ensemble attack method as Ens. The main difference of our method to Ens is that SVRE has an inner update loop, where SVRE obtains a variance reduced stochastic gradient by $M$ updates. Specifically, we first obtain the gradient of the multiple models, $g_{ens}$, by one pass over the models and maintain this value during $M$ inner iterations. Then, we randomly pick a model from the ensemble models, obtain the stochastic inner gradient after variance reduction $\\\\tilde{g}_m$, and update the inner adversarial example using the accumulate gradients of $\\\\tilde{g}_m$. In the end, we update the outer gradient using the accumulate gradient of the last inner loop. As $\\\\tilde{g}_m$ is the unbiased estimate of the gradient of $g_{ens}$, $(\\\\nabla_x J_k(x_{adv}, y) - g_{ens})$ helps to reduce the gradient on different models.\\n\\nIn a nutshell, the existing Ens method directly uses the average gradient of the ensemble models $g_{ens}$ to update the adversarial example, while SVRE uses the stochastic variance reduced gradient $\\\\tilde{g}$ to update the adversarial example.\\n\\n**Algorithm 1** The SVRE-MI-FGSM attack algorithm\\n\\n**Input:** A benign example $x$ and its label $y$, a set of $K$ surrogate models and the corresponding losses $\\\\{J_1, \\\\ldots, J_K\\\\}$, an ensemble loss $J$ chosen from $\\\\{\\\\text{Eq. (1)}, \\\\text{Eq. (2)}, \\\\text{Eq. (3)}\\\\}$\\n\\n**Input:** The perturbation bound $\\\\epsilon$, number of iterations $T$, internal update frequency $M$, internal step size $\\\\beta$, decay factor $\\\\mu_1$, internal decay factor $\\\\mu_2$\\n\\n**Output:** An adversarial example $x_{adv}$ that fulfills $\\\\|x_{adv} - x\\\\|_\\\\infty \\\\leq \\\\epsilon$\\n\\n1: $\\\\alpha = \\\\epsilon / T$;\\n2: $G_0 = 0$\\n3: Initialize $x_{adv} = x$\\n4: for $t = 0$ to $T - 1$ do\\n5: # Calculate the gradient of the ensemble model\\n6: Get the loss of the ensemble model $J(x_{adv}, y)$;\\n7: Calculate the gradient of the ensemble model $g_{ens} = \\\\frac{1}{K} \\\\nabla_x J(x_{adv}, y)$;\\n8: # Stochastic variance reduction via $M$ updates\\n9: Initialize $\\\\tilde{x}_0 = x_{adv}$;\\n10: $\\\\tilde{G}_0 = 0$\\n11: for $m = 0$ to $M - 1$ do\\n12: Randomly pick a model index $k \\\\in \\\\{1, \\\\ldots, K\\\\}$\\n13: Get the corresponding loss $J_k \\\\in \\\\{J_1, \\\\ldots, J_K\\\\}$\\n14: $\\\\tilde{g}_m = \\\\nabla_x J_k(\\\\tilde{x}_m, y) - (\\\\nabla_x J_k(x_{adv}, y) - g_{ens})$\\n15: # Update the inner gradient by momentum\\n16: $\\\\tilde{G}_{m+1} = \\\\mu_2 \\\\cdot \\\\tilde{G}_m + \\\\tilde{g}_m$\\n17: # Update the inner adversarial example\\n18: Update $\\\\tilde{x}_{m+1} = \\\\text{Clip}_{\\\\epsilon x} \\\\{\\\\tilde{x}_m + \\\\beta \\\\cdot \\\\text{sign}(\\\\tilde{G}_{m+1})\\\\}$\\n19: end for\\n20: # Update the outer gradient by momentum\\n21: $G_{t+1} = \\\\mu_1 \\\\cdot G_t + \\\\tilde{G}_M$\\n22: # Update the outer adversarial example\\n23: Update $x_{adv_{t+1}} = \\\\text{Clip}_{\\\\epsilon x} \\\\{x_{adv_t} + \\\\alpha \\\\cdot \\\\text{sign}(G_{t+1})\\\\}$\\n24: end for\\n25: return $x_{adv} = x_{adv_T}$\"}"}
{"id": "CVPR-2022-1764", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Experiments\\n\\nThis section first introduces the experimental setup, then reports the attack success rate on normally trained models and defense models, showing that SVRE outperforms Ens significantly for black-box attacks. We further demonstrate that SVRE increases the average loss on black-box models by a large margin. In the end, we perform ablation studies to manifest the effectiveness of the key parameters in SVRE.\\n\\n4.1. Experimental Setup\\n\\nDataset.\\nWe conduct experiments on an ImageNet-compatible dataset which is comprised of 1,000 images and is widely used in recent FGSM-based attacks [4, 5].\\n\\nNetworks.\\nWe consider four normally trained networks, i.e., Inception-v3 (Inc-v3) [27], Inception-v4 (Inc-v4), Resnet-v2-152 (Res-152) [26], and Inception-Resnet-v2 (IncRes-v2) [9]. For adversarially trained models, we consider Inc-v3 ens3, Inc-v3 ens4 and IncRes-v2 ens [30]. Besides, we consider nine defense models which are shown to be robust against black-box attacks, including the top-3 defense methods in the NIPS competition: HGD [15], R&P [33], NIPS-r3 [35] and six recently proposed defense methods: Bit-R [36], JPEG [8], FD [18], ComDefend [11], RS [10] and NRP [21].\\n\\nBaselines.\\nWe compare the proposed SVRE with Ens based on the advanced gradient-based attacks, including I-FGSM [7], MI-FGSM [3], TIM [4], TI-DIM [4], and SI-TI-DIM [16]. For Ens, we adopt the ensemble method that fuses the logits of difference models [3], which is confirmed better than the ensemble on predictions or losses. In addition, we run the SVRE attack for 5 times with different random seeds and average the results to reduce the impact of randomness.\\n\\nHyper-parameters.\\nTo align with the previous works [3, 4, 16, 35], we set the maximum perturbation \\\\( \\\\epsilon = \\\\frac{16}{255} \\\\). For I-FGSM, the number of iterations is 10, and the step size is \\\\( \\\\alpha = 1.6 \\\\). For MI-FGSM, we set the decay factor \\\\( \\\\mu = 1.0 \\\\). For TIM, we adopt the Gaussian kernel with size \\\\( 7 \\\\times 7 \\\\). For TI-DIM, the transformation probability \\\\( p \\\\) is set to 0.5. For SI-TI-DIM, we set the number of copies \\\\( m \\\\) to 5. For SVRE, we set the internal update frequency \\\\( M \\\\) to four times the number of ensemble models and the internal step size \\\\( \\\\beta \\\\) is set the same as \\\\( \\\\alpha \\\\), the internal decay factor \\\\( \\\\mu_2 \\\\) is set to 1.0.\\n\\n4.2. Attack Normally Trained Models\\n\\nWe first compare the performance of our method on the normally trained models, including Inc-v3, Inc-v4, Res-101, and IncRes-v2. Specifically, we keep one model as the hold-out black-box model and generate adversarial examples on an ensemble of the other three models by Ens and SVRE integrated with various base methods.\\n\\nTable 1. The attack success rates (%) of adversarial examples against the hold-out model. We study four normal models: Inc-v3, Inc-v4, IncRes-v2 and Res-101. For each model, the adversarial examples are crafted on an ensemble of the other three.\\n\\n| Attack Method | Base Model | SVRE | Ens |\\n|---------------|------------|------|-----|\\n| I-FGSM        | Inc-v3     | 89.24 | 66.70 |\\n| MI-FGSM       | Inc-v3     | 96.84 | 83.64 |\\n| TIM           | Inc-v3     | 96.10 | 88.70 |\\n| TI-DIM        | Inc-v3     | 97.78 | 94.10 |\\n| SI-TI-DIM     | Inc-v3     | 98.80 | 97.60 |\\n\\nTable 1 shows the attack performance on the hold out model. SVRE outperforms Ens across all the test models. The average improvement of SVRE over Ens on the base attack of I-FGSM is significant at 16.19%. Even on the advanced attack methods, MI-FGSM, TIM, DIM and SI-TI-DIM, the average improvements of SVRE over Ens are still considerable, which are 9.46%, 5.35%, 2.86% and 1.27%, respectively. The results demonstrate that SVRE can effectively improve the transferability of adversarial examples on normally trained models.\\n\\n4.3. Attack Advanced Defense Models\\n\\nTo further validate the efficacy of our method in practice, we continue to evaluate SVRE on models with various advanced defenses. Specifically, we craft the adversarial examples on the ensemble of four normally trained models, i.e., Inc-v3, Inc-v4, Res-15 and IncRes-v2, and test the transferability of the crafted adversaries on defense models.\\n\\nWe first evaluate the transferability of the adversaries on three adversarially trained models, Inc-v3 ens3, Inc-v3 ens4 and IncRes-v2 ens. The results are shown in Table 2. We see that SVRE outperforms Ens on the black-box attack of each adversarially trained models by a large margin. Among the base methods that the ensemble attacks integrate with, SVRE exhibits the highest improvement on TIM, as SVRE-TIM yields a 17.30% higher average attack success rate than Ens-TIM. Besides, SVRE also performs well in the white-box setting, and can slightly improve the white-box attack performance in most cases.\\n\\nIn addition to the adversarially trained models, we also evaluate the crafted examples on nine models with advanced defense mechanisms. The results are shown in Table 3. SVRE outperforms Ens by a clear margin across all the comparisons. The strongest version of our method, SVRE...\"}"}
{"id": "CVPR-2022-1764", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Attack Type | Method | Inc-v3 ens3 | Inc-v4 ens4 | IncRes-v2 ens | Average |\\n|------------|--------|------------|------------|---------------|---------|\\n| I-FGSM Ens | 100.00 | 100.00     | 99.60      | 99.80         | 27.10   |\\n| SVRE       | 99.80  | 99.60      | 99.38      | 99.58         | 40.08   |\\n| MI-FGSM Ens| 99.90  | 99.90      | 99.70      | 99.50         | 50.50   |\\n| SVRE       | 99.96  | 99.96      | 99.86      | 99.82         | 64.54   |\\n| TIM Ens    | 99.80  | 99.40      | 99.00      | 98.70         | 73.50   |\\n| SVRE       | 99.84  | 99.90      | 99.80      | 99.70         | 87.88   |\\n| TI-DIM Ens | 99.50  | 99.40      | 99.00      | 98.70         | 87.40   |\\n| SVRE       | 99.86  | 99.80      | 99.68      | 99.34         | 89.06   |\\n| SI-TI-DIM Ens | 99.70 | 99.40      | 99.30      | 99.40         | 95.60   |\\n| SVRE       | 99.98  | 99.96      | 99.90      | 99.80         | 98.56   |\\n\\n**Table 3.** The black-box attack success rates (%) against nine models with advanced defense mechanism.\\n\\n| Attack Type | Method | HGD | R&P | NIPS-r3 | Bit-R | JPEG | FD | ComDefend | RS | NRP | Average |\\n|------------|--------|-----|-----|---------|-------|------|----|-----------|----|-----|---------|\\n| I-FGSM Ens | 27.00  | 15.20 | 18.90 | 26.00 | 41.80 | 37.10 | 56.00 | 25.20 | 17.30 | 29.39   |\\n| SVRE       | 45.48  | 25.02 | 34.10 | 30.96 | 62.06 | 50.42 | 66.98 | 26.98 | 21.60 | 40.40   |\\n| MI-FGSM Ens | 41.30 | 33.00 | 44.60 | 39.70 | 75.90 | 62.80 | 77.50 | 36.90 | 27.30 | 48.78   |\\n| SVRE       | 44.06 | 40.72 | 59.54 | 43.42 | 89.06 | 73.28 | 86.60 | 39.12 | 28.46 | 56.03   |\\n| TIM Ens    | 72.50 | 60.50 | 67.20 | 49.30 | 82.60 | 74.80 | 85.10 | 47.80 | 37.60 | 64.16   |\\n| SVRE       | 87.10 | 80.16 | 83.84 | 62.26 | 91.96 | 83.96 | 92.22 | 62.46 | 52.24 | 77.36   |\\n| TI-DIM Ens | 87.40 | 81.20 | 85.70 | 63.00 | 91.70 | 84.30 | 91.90 | 57.90 | 49.80 | 76.99   |\\n| SVRE       | 94.86 | 91.92 | 93.22 | 72.88 | 96.48 | 90.76 | 95.98 | 73.60 | 52.24 | 86.12   |\\n| SI-TI-DIM Ens | 95.70 | 93.20 | 94.10 | 82.70 | 96.70 | 93.30 | 97.90 | 78.00 | 76.80 | 89.82   |\\n| SVRE       | 97.70 | 96.12 | 97.48 | 86.64 | 98.54 | 95.60 | 99.06 | 85.72 | 85.44 | 93.59   |\\n\\nThe black-box attack success rates (%) against three adversarially trained models. The adversarial examples are generated on the ensemble models, i.e., Inc-v3, Inc-v4, IncRes-v2 and Res-101.\\n\\nThe integrated with SI-TI-DIM can achieve an average attack success rate of 93.59% on these defense models in the black-box setting, which raises a new security issue for the robustness of deep learning models.\\n\\n4.4. Comparison on Loss\\n\\nThe above experiments have demonstrated that SVRE significantly improves the attack success rate of adversarial attacks. To provide intuitive evidence that SVRE can effectively boost the transferability of adversarial examples, we average the loss over the adversarial images generated in Section 4.3 on four white-box models and three black-box models respectively, and depict the improvement curve for the average loss in Figure 2. The loss can indirectly reflect the adversarial efficacy. A higher loss indicates a stronger adversarial intensity, and a higher loss on the black-box model indicates a stronger transferability.\\n\\nWe can see in Figure 2 (b) that SVRE increases the average loss over Ens on black-box models remarkably. In terms of the white-box setting in Figure 2 (a), SVRE and Ens are comparative, indicating that the improvement of SVRE in transferability is not based on the premise of sacrificing the performance of white-box attacks.\\n\\n4.5. Ablation Study on Hyper-parameters\\n\\nIn this subsection, we conduct a series of ablation experiments to study the impact of the parameters in SVRE. Here we attack the ensemble of Inc-v3, Inc-v4, Res-152 and IncRes-v2 and test the transferability of the adversaries on the adversarially trained models Inc-v3 ens3, Inc-v3 ens4 and IncRes-v2 ens, as the setting in Section 4.2.\\n\\nOn the internal update frequency. We first analyze the effectiveness of the internal update frequency on the attack success rate of SVRE. We integrate I-FGSM, MI-FGSM and SI-MI-DIM attacks with SVRE, respectively, and range the internal update frequency from 0 to 32 with a granularity of 4. Note that if M = 0, SVRE trivially degenerates to the normal ensemble method of Ens. Since the attack success rate in the white-box setting is close to 100%, we only show the results for black-box attacks, as\"}"}
{"id": "CVPR-2022-1764", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. The average loss on seven models against Ens and SVRE integrated with five attacks, respectively.\\n\\nFigure 3. The attack success rate (%) of SVRE integrated with I-FGSM, MI-FGSM and SI-TI-DIM. It degenerates to the integration with Ens when $M = 0$.\\n\\nFigure 4. The attack success rate (%) of I-FGSM, MI-FGSM and SI-TI-DIM after integrated with SVRE on different internal step size $\\\\beta$. \\n\\n(a) SVRE-I-FGSM\\n\\n(b) SVRE-MI-FGSM\\n\\n(c) SVRE-SI-TI-DIM\"}"}
{"id": "CVPR-2022-1764", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. The attack success rate (%) of SVRE-MI-FGSM and Ens-MI-FGSM for different total number of gradient calculations.\\n\\nshown in Figure 3. A first glance shows that our SVRE has achieved an impressive improvement over Ens ($M = 0$). As the number of iterations increases, the attack success rate increases and reaches the peak at about $M = 16$. We also observe from the convex curve that either too many iterations or too few iterations may cause the adversarial examples to overfit the current model and harm the attack transferability.\\n\\nOn the internal step size $\\\\beta$. The internal step size $\\\\beta$ plays a crucial role in improving the attack success rate, as it determines the extent of the data point update in each inner loop. Similarly, we perform SVRE integrated with I-FGSM, MI-FGSM and SI-MI-DIM respectively, fix $\\\\alpha = 1.6$ and let $\\\\beta$ ranges from 0.1 doubled to 25.6. As shown in Figure 4, the performance of SVRE varies with the step size, and the best step size varies for different methods. For a fair comparison, we did not deliberately set different best parameters for each method but choose $\\\\beta = 1.6$. For practical applications, the best step size can be adopted for a specific attack to obtain a higher performance.\\n\\nOn the number of iterations $T$. For the same number of iterations, SVRE has more gradient calculations due to its inner loop. To show that the gains of SVRE is not simply from increasing the number of gradient calculations, we perform additional analysis on the total number of iterations. Taking the internal update frequency $M = 16$ and the number of ensemble models $K = 4$ as an example, each iteration requires 4 queries of the model in Ens, while for SVRE, the inner loop requires $16 \\\\times 2 = 32$ additional queries. The overall number of queries for SVRE is 9 times that of Ens. Then, what if we increase the number of iterations for other methods? One can observe from Figure 5 that the attack success rate of Ens-MI-FGSM against black-box models gradually decays with the increment on the total number of gradient calculations, and there is a big gap even when the total number reaches 360. This experiment demonstrates that simply increasing the number of iterations on Ens could not gain the high attack performance of SVRE.\\n\\n5. Conclusion\\n\\nIn this work, we propose a novel method called the stochastic variance reduced ensemble (SVRE) attack to enhance the transferability of the crafted adversarial examples. Different from the existing model ensemble attacks that simply fuse the outputs of multiple models evenly, the proposed SVRE takes the gradient variance of different models into account and reduces the variance to stabilize the gradient update on ensemble attacks. In this way, SVRE can craft adversarial examples with higher transferability for other possible models. Extensive experiments demonstrate that SVRE outperforms the vanilla model ensemble attack in the black-box setting significantly, at the same time SVRE keeps roughly the same performance in the white-box setting.\\n\\nCompared with broad investigations on the gradient optimization or input transformation attacks, the ensemble attacks are less explored in previous studies. Our work could shed light on the great potential of boosting the adversarial transferability through a better design on the ensemble methods. In future work, we wish our work could inspire more in-depth works in this direction for ensemble attacks.\\n\\nAcknowledgements\\n\\nThis work is supported by National Natural Science Foundation of China (62076105) and Hubei International Cooperation Foundation of China (2021EHB011).\"}"}
{"id": "CVPR-2022-1764", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Anish Athalye, Nicholas Carlini, and David A. Wagner. Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples. In Proceedings of the 35th International Conference on Machine Learning, ICML, volume 80, pages 274\u2013283, 2018.\\n\\n[2] Nicholas Carlini and David Wagner. Towards evaluating the robustness of neural networks. In 2017 IEEE Symposium on Security and Privacy (SP), pages 39\u201357, 2017.\\n\\n[3] Yinpeng Dong, Fangzhou Liao, Tianyu Pang, Hang Su, Jun Zhu, Xiaolin Hu, and Jianguo Li. Boosting adversarial attacks with momentum. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 9185\u20139193, 2018.\\n\\n[4] Yinpeng Dong, Tianyu Pang, Hang Su, and Jun Zhu. Evading defenses to transferable adversarial examples by translation-invariant attacks. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 4312\u20134321, 2019.\\n\\n[5] Lianli Gao, Qilong Zhang, Jingkuan Song, Xianglong Liu, and Heng Tao Shen. Patch-wise attack for fooling deep neural network. In European Conference on Computer Vision, ECCV, pages 307\u2013322, 2020.\\n\\n[6] Chengyue Gong, Tongzheng Ren, Mao Ye, and Qiang Liu. Maxup: Lightweight adversarial training with data augmentation improves neural network training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2474\u20132483, June 2021.\\n\\n[7] Ian J. Goodfellow, Jonathon Shlens, and Christian Szegedy. Explaining and harnessing adversarial examples. In 3rd International Conference on Learning Representations, ICLR, 2015.\\n\\n[8] Chuan Guo, Mayank Rana, Moustapha Ciss\u00e9, and Laurens van der Maaten. Countering adversarial images using input transformations. In 6th International Conference on Learning Representations, ICLR, 2018.\\n\\n[9] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 770\u2013778, 2016.\\n\\n[10] Jinyuan Jia, Xiaoyu Cao, Binghui Wang, and Neil Zhenqiang Gong. Certified robustness for top-k predictions against adversarial perturbations via randomized smoothing. In 8th International Conference on Learning Representations, ICLR, 2020.\\n\\n[11] Xiaojun Jia, Xingxing Wei, Xiaochun Cao, and Hassan Foroosh. ComDefend: An efficient image compression model to defend adversarial examples. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 6084\u20136092, 2019.\\n\\n[12] Rie Johnson and Tong Zhang. Accelerating stochastic gradient descent using predictive variance reduction. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013, pages 315\u2013323, 2013.\\n\\n[13] Alexey Kurakin, Ian J Goodfellow, and Samy Bengio. Adversarial examples in the physical world. In 5nd International Conference on Learning Representations, ICLR (Workshop), 2017.\\n\\n[14] Alexey Kurakin, Ian J. Goodfellow, and Samy Bengio. Adversarial machine learning at scale. In 5th International Conference on Learning Representations, ICLR, 2017.\\n\\n[15] Fangzhou Liao, Ming Liang, Yinpeng Dong, Tianyu Pang, Xiaolin Hu, and Jun Zhu. Defense against adversarial attacks using high-level representation guided denoiser. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 1778\u20131787, 2018.\\n\\n[16] Jiadong Lin, Chuanbiao Song, Kun He, Liwei Wang, and John E. Hopcroft. Nesterov accelerated gradient and scale invariance for adversarial attacks. In 8th International Conference on Learning Representations, ICLR, 2020.\\n\\n[17] Yanpei Liu, Xinyun Chen, Chang Liu, and Dawn Song. Delving into transferable adversarial examples and black-box attacks. In 5th International Conference on Learning Representations, ICLR, 2017.\\n\\n[18] Zihao Liu, Qi Liu, Tao Liu, Nuo Xu, Xue Lin, Yanzhi Wang, and Wujie Wen. Feature distillation: Dnn-oriented JPEG compression against adversarial examples. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 860\u2013868, 2019.\\n\\n[19] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt, Dimitris Tsipras, and Adrian Vladu. Towards deep learning models resistant to adversarial attacks. In 6th International Conference on Learning Representations, ICLR, 2018.\\n\\n[20] Seyed-Mohsen Moosavi-Dezfooli, Alhussein Fawzi, Omar Fawzi, and Pascal Frossard. Universal adversarial perturbations. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 86\u201394, 2017.\\n\\n[21] Muzammal Naseer, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Fatih Porikli. A self-supervised approach for adversarial robustness. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 259\u2013268, 2020.\\n\\n[22] Nicolas Papernot, Patrick McDaniel, Ian Goodfellow, Somesh Jha, Z Berkay Celik, and Ananthram Swami. Practical black-box attacks against machine learning. In Proceedings of the 2017 ACM on Asia conference on computer and communications security, pages 506\u2013519, 2017.\\n\\n[23] Nicolas Papernot, Patrick McDaniel, Somesh Jha, Matt Fredrikson, Z Berkay Celik, and Ananthram Swami. The limitations of deep learning in adversarial settings. In 2016 IEEE European Symposium on Security and Privacy (EuroS&P), pages 372\u2013387, 2016.\\n\\n[24] Chuanbiao Song, Kun He, Jiadong Lin, Liwei Wang, and John E. Hopcroft. Robust local features for improving the generalization of adversarial training. In 8th International Conference on Learning Representations, ICLR, 2020.\\n\\n[25] Chuanbiao Song, Kun He, Liwei Wang, and John E. Hopcroft. Improving the generalization of adversarial training with domain adaptation. In 7th International Conference on Learning Representations, ICLR, 2019.\\n\\n[26] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and Alexander A. Alemi. Inception-v4, Inception-Resnet and .\"14991\"}"}
{"id": "CVPR-2022-1764", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the impact of Residual Connections on learning. In Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence, pages 4278\u20134284, 2017.\\n\\n[27] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 2818\u20132826, 2016.\\n\\n[28] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan Bruna, Dumitru Erhan, Ian J. Goodfellow, and Rob Fergus. Intriguing properties of neural networks. In 2nd International Conference on Learning Representations, ICLR, 2014.\\n\\n[29] Florian Tramer, Nicholas Carlini, Wieland Brendel, and Aleksander Madry. On adaptive attacks to adversarial example defenses. Advances in Neural Information Processing Systems, NIPS, 33, 2020.\\n\\n[30] Florian Tramer, Alexey Kurakin, Nicolas Papernot, Ian J. Goodfellow, Dan Boneh, and Patrick D. McDaniel. Ensemble adversarial training: Attacks and defenses. In 6th International Conference on Learning Representations, ICLR, 2018.\\n\\n[31] Xiaosen Wang and Kun He. Enhancing the transferability of adversarial attacks through variance tuning. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 1924\u20131933. Computer Vision Foundation / IEEE, 2021.\\n\\n[32] Xiaosen Wang, Xuanran He, Jingdong Wang, and Kun He. Admix: Enhancing the transferability of adversarial attacks. CoRR, abs/2102.00436, 2021.\\n\\n[33] Cihang Xie, Jianyu Wang, Zhishuai Zhang, Zhou Ren, and Alan L. Yuille. Mitigating adversarial effects through randomization. In 6th International Conference on Learning Representations, ICLR, 2018.\\n\\n[34] Cihang Xie, Yuxin Wu, Laurens van der Maaten, Alan L. Yuille, and Kaiming He. Feature denoising for improving adversarial robustness. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 501\u2013509, 2019.\\n\\n[35] Cihang Xie, Zhishuai Zhang, Yuyin Zhou, Song Bai, Jianyu Wang, Zhou Ren, and Alan L. Yuille. Improving transferability of adversarial examples with input diversity. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR, pages 2730\u20132739, 2019.\\n\\n[36] Weilin Xu, David Evans, and Yanjun Qi. Feature squeezing: Detecting adversarial examples in deep neural networks. In The Network and Distributed System Security, NDSS, 2018.\\n\\n[37] Runtian Zhai, Tianle Cai, Di He, Chen Dan, Kun He, John E. Hopcroft, and Liwei Wang. Adversarially robust generalization just requires more unlabeled data. CoRR, abs/1906.00555, 2019.\"}"}
