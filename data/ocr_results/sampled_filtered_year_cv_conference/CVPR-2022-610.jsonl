{"id": "CVPR-2022-610", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Event-aided Direct Sparse Odometry\\n\\nJavier Hidalgo-Carri\u00f3n\\nGuillermo Gallego\\nDavide Scaramuzza\\n\\n1 Dept. of Informatics, Univ. of Zurich and Dept. of Neuroinformatics, Univ. of Zurich and ETH Zurich.\\n2 Technische Universit\u00e4t Berlin, Einstein Center Digital Future and SCIoI Excellence Cluster, Germany.\\n\\nFigure 1. Camera trajectory and estimated 3D map (left). The top-right inset shows the sliding window map (grayscale points) with the current keyframe map (pseudo-colored blue-red points, according to event polarity). The bottom-right insets show the color image (frame) with the real events (left) and the image obtained by the event generative model (right).\\n\\nAbstract\\n\\nWe introduce EDS, a direct monocular visual odometry using events and frames. Our algorithm leverages the event generation model to track the camera motion in the blind time between frames. The method formulates a direct probabilistic approach of observed brightness increments. Per-pixel brightness increments are predicted using a sparse number of selected 3D points and are compared to the events via the brightness increment error to estimate camera motion. The method recovers a semi-dense 3D map using photometric bundle adjustment. EDS is the first method to perform 6-DOF VO using events and frames with a direct approach. By design it overcomes the problem of changing appearance in indirect methods. Our results outperform all previous event-based odometry solutions. We also show that, for a target error performance, EDS can work at lower frame rates than state-of-the-art frame-based VO solutions. This opens the door to low-power motion-tracking applications where frames are sparingly triggered \\\"on demand\\\" and our method tracks the motion in between. We release code and datasets to the public.\\n\\nMultimedia Material\\n\\nCode and dataset can be found at: https://rpg.ifi.uzh.ch/eds\\n\\n1. Introduction\\n\\nVisual Odometry (VO) is a paramount tool in computer vision, robotics and any application that requires spatial reasoning [1\u20133]. In recent years, considerable progress has been made on this topic [1, 4\u20136]. However, VO systems are limited by the capabilities of their physical devices (sensors, processors, and power). Some of these limitations (e.g., motion blur, dynamic range) can been tackled with novel and/or more robust sensors, such as event cameras.\\n\\nEvent cameras [7\u20139] are bio-inspired sensors that work radically different from traditional cameras. Instead of capturing brightness images at a fixed rate, they measure asynchronous, per-pixel brightness changes, called \\\"events\\\". This principle of operation endows event cameras with outstanding properties, such as low latency, high temporal resolution (in the order of \u00b5s) and low power (milliwatts instead of watts). The large potential of event cameras to tackle VO and related problems in challenging scenarios has been investigated in [10\u201321]. We refer to a recent comprehensive survey paper for further details [22].\\n\\nEvent-based VO is a challenging problem that has been addressed step-by-step in scenarios with increasing complexity. Two fundamental challenges arise when working with event cameras: noise (caused by timestamp jitter, pixel 5781\\n\\nSee an illustrative animation: https://youtu.be/LauQ6LWTkxM?t=30\"}"}
{"id": "CVPR-2022-610", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"manufacturing mismatch or non-linear circuity effects) and data association \\\\[22, 23\\\\], i.e., establishing correspondences between events to identify which events are triggered by the same scene point. This is so because each event carries little information and the temporal edge patterns conveyed by the events depend on motion. These two issues make event-based keypoints used by indirect methods difficult to detect and track with sufficient stability; for this reason, grayscale frames \\\\[14\\\\] or motion compensation and inertial sensor fusion \\\\[18\\\\] have been used to mitigate their effect.\\n\\nEvent-based methods might be categorized according to whether they exploit the Event Generation Model (EGM) \\\\[13,24\\\\] or not \\\\[14,17,25,26\\\\]. The EGM states how events are created when a predefined contrast threshold is reached \\\\[7, 22\\\\]. It is a photometric relationship between brightness \\\"changes\\\" (i.e., events) and \\\"absolute\\\" brightness. Experiments on event-based feature tracking \\\\[23\\\\] have shown that methods exploiting the EGM achieve higher accuracy than those that do not. However such a comparison is yet to be performed for 6-DOF camera tracking (i.e., ego-motion estimation). Current solutions \\\\[13, 17\\\\] are prone to lose tracking, either because the convergence of the estimated 3D map is slow \\\\[13\\\\] or because the edge-patterns change quickly from one packet of events to the next one \\\\[17\\\\]. That is, there is no long-term appearance, like the grayscale frames in \\\\[23\\\\], to latch onto and improve tracking robustness, which we intend to do.\\n\\nWe propose to tackle the event-based VO problem by understanding and overcoming the shortcomings of previous methods. To the best of our knowledge, this work is the first monocular method to perform 6-DOF VO using events and frames with a direct approach. Our contribution lies in the front-end, fusing the information from events and frames tightly using the EGM (as opposed to previous works that loosely coupled them \\\\[18\\\\]). Estimated poses, points, and selected frames are fed into a sliding-window photometric bundle adjustment (PBA) back-end, which minimizes brightness errors. This is the first time that PBA is used in the context of event-camera VO. Internally, we adopt a key-frame-based approach, recovering inverse depth at pixels with high gradient as in DSO \\\\[5\\\\]. However, our method allows tracking the camera motion in the blind time between frames using only events. This opens the door for frames to be triggered sparingly, i.e., \\\"on demand\\\", thus potentially saving energy in the system, which is a desirable feature in AR/VR applications and in platforms with a limited power budget. Our contributions are summarized as follows:\\n\\n- The first formulation of a monocular 6-DOF visual odometry combining events and grayscale frames in a direct approach with photometric bundle adjustment.\\n- Camera motion tracking using a sparse set of pixels, minimizing the normalized brightness change of events.\\n- A compelling evaluation on publicly available datasets outperforming previous solutions, and a sensitivity study to gain insights about our method.\\n- A new dataset with high quality events, color frames, and IMU data to foster research in monocular VO.\\n\\n\\\\[\\\\text{Table 1. Comparison of event-based monocular 6-DOF VO/SLAM methods} \\\\]\\n\\n| Method            | Events | Frames | D/I | EGM   | Remarks                                  |\\n|-------------------|--------|--------|-----|-------|-----------------------------------------|\\n| Kim et al. \\\\[13\\\\] | \u2713      | \u2717      | D   | \u2713     | Three parallel EKFs                     |\\n| Rebecq et al. \\\\[17\\\\] | \u2713      | \u2717      | D   | \u2717     | Parallel tracking and mapping            |\\n| Kueng et al. \\\\[14\\\\] | \u2713      | \u2713      | I   | \u2717     | Tracking event features for VO           |\\n| Rosinol et al. \\\\[18\\\\] | \u2713      | \u2713      | I   | \u2717     | Loosely coupled front-end                |\\n| This work         | \u2713      | \u2713      | D   | \u2713     | Tightly coupled front-end                |\\n\\n\\\\[\\\\text{2. Related Work} \\\\]\\n\\nEvent-based VO methods might be direct or indirect depending on whether they process raw pixel information directly or indirectly via some intermediate representation. Indirect methods \\\\[14, 15\\\\], like frame-based approaches, extract keypoints \\\\[27, 28\\\\] from the input (event) data in the front-end before passing them to the back-end. Direct methods \\\\[13, 17\\\\] attempt to directly process all events available. Since events correspond to per-pixel brightness changes, they naturally convey the information about the motion of the scene edges (assuming constant illumination). Early event-based VO works \\\\[11, 24, 29\\\\] tackled simple camera motions, such as 3-DOF (planar or rotational), and hence did not account for depth. The most general case of a freely moving camera (6-DOF) has been tackled only recently \\\\[13, 14, 17\\\\]. In terms of scene texture, high contrast and/or structured scenes have been addressed before focusing on more difficult, natural 3D scenes. Event-based VO is still in its infancy, with the majority of works addressing only camera tracking \\\\[12, 16, 20, 30\u201332\\\\] because of its simplicity.\"}"}
{"id": "CVPR-2022-610", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"event-based 3-DOF VO [21,29,36,37]. All these ideas sug-\\n\\nCombining events and frames... Complementary sources of visual informa-\\n\\nCombining them has proven useful to improve... such as... video frame interpolation [42]. We also seek to get the best of both... sensors for VO-related tasks, as implied by [18, 32].\\n\\nIn contrast to [18], which treats events and frames as un-\\n\\nIn short, as Table 1 shows, there is a gap in the character-\\n\\n3. Direct Odometry with Events and Frames\\n\\nThis section describes our method, which is summarized...\\n\\n3.1. Event Generation Model (EGM)\\n\\nHow an Event Camera Works\\n\\nEach pixel of an event camera produces an event\\n\\ne_{k} = (u_{k}, t_{k}, p_{k})\\n\\nwhenever it detects that the logarithmic brightness\\n\\nL at that pixel,\\n\\nu_{k} = (x_{k}, y_{k})^{\\\\top},\\n\\nchanges by a specified amount\\n\\nC (called contrast sensitivity) [7]:\\n\\n\\\\Delta L(u_{k}, t_{k}) = L(u_{k}, t_{k}) - L(u_{k}, t_{k} - \\\\Delta t_{k}) = p_{k} C, \\\\quad (1)\\n\\nwhere the polarity\\n\\np_{k} \\\\in \\\\{+1, -1\\\\}\\n\\nis the sign of the bright-\\n\\nness change, and\\n\\n\\\\Delta t_{k}\\n\\nis the time elapsed since the last event\\n\\nat the same pixel. Event timestamps\\n\\nt_{k}\\n\\nhave \\\\mu s resolution.\\n\\nA single pixel has its own sampling rate (which depends on\\n\\nthe visual input) and produces events proportionally to the\\n\\namount of scene motion. An event camera does not out-\\n\\nput images at a constant rate, but rather a stream of asyn-\\n\\nchronous events in space-time.\\n\\nLinearized Event Generation Model. Collecting\\n\\npixelwise the polarities of a set of events\\n\\nE = \\\\{e_{k}\\\\}_{k=1}^{N_{e}}\\n\\nin the time interval\\n\\nT = \\\\{t_{k}\\\\}_{k=1}^{N_{e}}\\n\\nproduces a brightness incre-\\n\\nment image:\\n\\n\\\\Delta L(u) = \\\\sum_{t_{k} \\\\in T} p_{k} C \\\\delta(u - u_{k}), \\\\quad (2)\\n\\nwhere the Kronecker\\n\\n\\\\delta\\n\\nselects the appropriate pixel. If the\\n\\nnumber of events\\n\\nN_{e}\\n\\nspans a small delta time\\n\\n\\\\Delta t = t_{N_{e}} - t_{1},\\n\\nthe increment (1) can be approximated using Taylor's ex-\\n\\npansion. Further substituting the brightness constancy as-\\n\\nsumption gives that\\n\\n\\\\Delta L\\n\\nis caused by brightness gradients\\n\\n\\\\nabla L\\n\\nmoving with velocity\\n\\nv\\n\\non the image plane [23, 43]:\\n\\n\\\\Delta L(u) \\\\approx -\\\\nabla L(u) \\\\cdot \\\\Delta u = -\\\\nabla L(u) \\\\cdot v(u) \\\\Delta t. \\\\quad (3)\\n\\n3.2. Front-End\\n\\nIn the VO front-end (Fig. 2) we use (2) to create pseudo-\\n\\nmeasurements from the events (top branch of Fig. 2), and\\n\\nuse the right hand side of (3) to predict those from the frame\\n\\n\\\\hat{L}\\n\\nand the current state of the VO system (middle branch\\n\\nof Fig. 2). Our goal is, roughly speaking, to estimate the\\n\\nVO state that best predicts the measurements (Fig. 3). This\\n\\nstrategy is described in the upcoming subsections.\\n\\nEvent Weighting. As pointed out in [32], there is a\\n\\ntrade-off in the selection of\\n\\nN_{e}\\nto build (2): a small\\n\\nN_{e}\\ndoes not yield sufficient SNR or evidence of existing edge\\n\\nmotion, whereas a large\\n\\nN_{e}\\nproduces accumulation blur and\\n\\nbreaks the assumption about events being triggered by the\\n\\ncamera at a single location. To tackle this issue, we select\\n\\nN_{e} large to have enough SNR and we modify (2) to accu-\\nmulate weighted polarities\\n\\nw_{k} p_{k} \\\\leftarrow p_{k}.\\n\\nWe use Gaussian\\n\\nweights\\n\\nw_{k}\\nin time index\\n\\nk\\n\\n(top branch in Fig. 2). The\\n\\nweights emphasize the central part of the window of events\\n\\nE, hence producing thinner edges (less accumulation blur)\\n\\nthan in the unweighted case\\n\\n(w_{k} = 1).\\n\\nEvents Prediction using Frames. The middle branch\\n\\nof Fig. 2 computes the right hand side of (3) and selects only\\n\\npixels at scene contours for event prediction. A keyframe in\\n\\nthe middle branch of Fig. 2 comprises a brightness frame\\n\\nand a (semi-dense) inverse depth map (Sec. 3.2.2).\\n\\nThe spatial gradient (3) of the logarithmic normalized in-\\ntensity of the keyframe\\n\\n\\\\hat{L}\\nis computed using the Sobel op-\\nderator. The image-point velocity\\n\\nv\\n\\nin (3) is purely geomet-\\n\\nric, given in terms of the camera pose\\n\\nT , the camera's lin-\\near and angular velocities\\n\\n\\\\dot{T} = (V^{\\\\top}, \\\\omega^{\\\\top})^{\\\\top},\\n\\nand the depth\\n\\nd_{u}.\\n\\n= Z(u)\\n\\nof the 3D point with respect to the camera [45]:\\n\\nv(u) = J(u, d_{u}) \\\\dot{T}, \\\\quad (4)\\n\\nwhere\\n\\nJ(u, d_{u})\\nis the\\n\\n2 \\\\times 6\\n\\nfeature sensitivity matrix\\n\\nJ(u, d_{u}) = \\n\\n- \\\\frac{1}{d_{u}} d_{u} 0\\n\\nu_{x} \\\\frac{d_{u}}{u_{y}} \\\\frac{1 + u_{x}^{2}}{u_{y}} - u_{x} u_{y}\\n\\n- \\\\frac{1}{d_{u}} u_{y} \\\\frac{d_{u}}{d_{u} 1 + u_{y}^{2} - u_{x} u_{y} - u_{x} u_{y}} \\\\frac{1 + u_{x}^{2} + u_{y}^{2}}{u_{x} u_{y}}. \\\\quad (5)\"}"}
{"id": "CVPR-2022-610", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Block diagram of the proposed event-based direct odometry approach. Events and frames acquired by a camera, such as the DA VIS346 [44], are fed to the front-end, where they are fused using the event generation model (EGM). The front-end selects sparse points on scene edges (i.e., events) with respect to the keyframes. As the camera moves, it generates events, and the camera pose is estimated with respect to the last keyframe. Poses and keyframes are passed to the back-end, which performs non-linear refinement of the poses and depth estimates via photometric bundle adjustment. These are later fed back to the front-end to sustain the good performance of the VO system. Events are colored in blue/red according to polarity $p_k$, indicating positive/negative brightness increments.\\n\\nInserting (4) in (3) gives the predicted brightness change $\\\\Delta \\\\hat{L}(u) \\\\approx -\\\\nabla \\\\hat{L}(u) \\\\cdot J(u, du) \\\\cdot \\\\dot{T} \\\\Delta t$.\\n\\nThe pose $T$ and velocity $\\\\dot{T}$ are global quantities shared by all image pixels $u$. The keyframe brightness $\\\\hat{L}$ and the delta time $\\\\Delta t$ (given by the event timestamps) are known. As noted in the block diagram of Fig. 2, the depth $du$ is an input to the front-end, given by the back-end. Initialization of the depth estimates is described in Sec. 3.4.\\n\\nCandidate Points Selection. Only the most informative pixels of the keyframes are used. This focuses computational resources while maintaining accuracy. Specifically, we select pixels with a sufficiently strong gradient (i.e., contours). To have a good distribution of the selected pixels over the image plane, we follow a tiling approach, dividing the image into rectangular tiles (e.g., $11 \\\\times 11$ tiles) and selecting a percentage of the pixels with the largest brightness gradient on each tile (typically 10-15% of the image pixels). Note that at a keyframe the pixels with strongest gradients overlap with the pixels where events are triggered (since events are due to moving edges). As the camera moves, these two sets of pixels begin to depart; however, it is through the estimation of the camera motion (Sec. 3.2.1) and the scene depth (Sec. 3.2.2) that we keep them aligned, thus maintaining a correspondence between them.\\n\\n3.2.1 Camera Tracking\\n\\nCamera tracking (illustrated on the top right of the front-end block in Fig. 2) is performed with respect to the last keyframe. We split the event stream into packets (i.e., temporal windows), and create event frames (2) using the Gaussian weighting mentioned above. We cast the camera tracking problem as a joint optimization over the camera motion parameters (6-DOF pose and its velocity):\\n\\n$$(\\\\delta T^*, \\\\dot{T}^*) = \\\\arg \\\\min_{\\\\delta T, \\\\dot{T}} \\\\Delta \\\\hat{L} \\\\| \\\\Delta \\\\hat{L} \\\\|_2 - \\\\| \\\\Delta L \\\\|_2 \\\\cdot \\\\gamma.$$  \\n\\n(7)\\n\\nThe error is the Huber norm $\\\\gamma$ of the difference between normalized brightness increments ($\\\\Delta L$ from the events and $\\\\Delta \\\\hat{L}$ from the keyframe). Norms are computed over a sparse set: the above-mentioned selected pixels in the keyframe. Hence, the increments due to the events are transferred to the image plane of the keyframe: (i) first, we find the location $u_e$ on the event image plane corresponding to a selected keyframe pixel $u_f$:\\n\\n$$u_e = \\\\pi(T_{e,f} \\\\pi^{-1} u_f, du_f),$$  \\n\\n(8)\\n\\nwhere $\\\\pi^{-1}: R^2 \\\\rightarrow R^3$ backprojects a keyframe pixel, $T_{e,f} \\\\in SE(3)$ changes coordinates to the current event camera; and (ii) we compute $\\\\Delta L(u_e)$ by cubic interpolation of the brightness increment (2). The normalized values of $\\\\Delta L(u_e)$ and $\\\\Delta \\\\hat{L}(u_f)$ are compared in (7).\\n\\nThe pose $T \\\\in SE(3)$ is parametrized using a 3-vector and a quaternion. The velocity $\\\\dot{T}$ is parametrized using a 6-vector. To minimize (7), we use the Ceres solver [46], which performs local Lie group parametrization of $T$. \\n\\n5784\"}"}
{"id": "CVPR-2022-610", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Compared to the camera tracker [32], which projects a global photometric 3D map onto the current event camera location, we compute the error (7): (i) on the keyframe (local depth maps), rather than on the event frame (because we lack dense optical flow from a global 3D map to use in the EGM), (ii) only in a sparse group of pixels (as opposed to the entire image plane) conforming a local semi-dense map.\\n\\nThe output of the front-end is the current keyframe (brightness image and inverse depth map) and the estimated camera motion (relative to the keyframe) of each event packet. These are passed to the back-end for further non-linear refinement (Sec. 3.3).\\n\\nKeyframe Selection. A keyframe is created when one of two conditions is met: (i) the number of selected points decreases by 20-30% (because they fall out of the field of view (FOV)). (ii) the relative rotation of the event camera with respect to the keyframe exceeds a given threshold.\\n\\n3.2.2 Mapping (Depth Estimation) As a new keyframe is created, the inverse depth estimates from past keyframes are used to populate those of the new keyframe. Likewise, the set of selected pixels is transferred to the new keyframe (akin to (8)). The inverse depth values at the remaining pixels are initialized using nearest neighbors with a k-d tree. This is simple and effective. In our VO system, inverse depth estimates are refined in the back-end.\\n\\n3.3. Back-End (Non-linear Refinement) The back-end (bottom branch in Fig. 2) performs non-linear refinement of the camera poses and the 3D structure via photometric bundle adjustment (PBA). It minimizes the objective function:\\n\\n$$\\\\sum_{i \\\\in F} \\\\sum_{u \\\\in P_i} \\\\sum_{j \\\\in F} \\\\Pi(\\\\tau(u)) F_{\\\\hat{L}_i}(u) - F_{\\\\hat{L}_j}(u') \\\\gamma(\\\\Omega),$$\\n\\nwhere $i \\\\in F$ runs over all keyframes, $u$ runs over all selected pixels $P_i$ in keyframe $i$, $j$ runs over all keyframes in which point $u$ is visible, and $u'$ is the corresponding point to $u$ on the $j$-th keyframe. We confer robustness by using the Huber norm $\\\\gamma$, which deemphasizes bad correspondences, and by discarding outliers (based on obnoxiously large errors). Errors are measured around each image point using an 8-pixel patch and assuming the same depth estimate for all pixels in the patch (residual pattern #8 in [5]).\\n\\n$N_k$ keyframes are kept in a sliding window estimator (we use 7, as in [5], since it is a good accuracy-efficiency compromise). On average, around 2000\u20138000 points are used in the back-end. We have implemented a PBA using Ceres [46] with automatic differentiation. We have also combined our front-end with DSO's PBA [5], since our design is modular.\\n\\n3.4. Bootstrapping To initialize the system, we may try three methods on the frames: (i) classical multi-view geometry, (ii) learning-based monocular depth prediction or (ii) DSO's coarse initializer. The first approach uses the 8-point algorithm [47] for the first frames until the sliding window is full (we skip frames to increase parallax). Once initialization is successful, the frames become keyframes, we select points with large gradient, and perform one PBA step to refine the map. If the above fails (i.e.: planar scenes), we use single image depth prediction via MiDAS [48]. Ultimately, DSO's coarse initializer works the best to initialize. The magnitude of the predicted depth is arbitrary, but this is known in monocular VO since scale is unobservable (a Gauge freedom [49]).\\n\\n4. Experiments We now evaluate the designed monocular VO method. First, we present the datasets and metrics used for the evaluation (Sec. 4.1). Second, we introduce the baseline methods (Sec. 4.2). Third, we evaluate the performance of the method, comparing with the state of the art (Sec. 4.3). Finally, we analyze the sensitivity of the results to various perturbations and limitations in Sec. 4.4.\\n\\n4.1. Datasets and Evaluation Metric We test on sequences from the standard dataset [50] to assess the performance of the proposed VO method. Data provided by [50] was collected with a hand-held stereo DA VIS240C in an indoor environment, and ground truth poses were given by a motion capture system with sub-millimeter accuracy. See the supplementary material for collected sequences with our custom-made beamsplitter. To assess the performance of the full VO method, we report ego-motion estimation results using standard metrics: absolute trajectory error (ATE) and rotation RMSE [51]. We use the toolbox from [52] to evaluate the poses given by different visual odometry solutions. Monocular methods are tested using data from the left camera only. The tracking and mapping parts of the VO methods are connected, so depth estimation errors are subsumed in the accuracy of the estimated camera trajectory, in a tightly coupled manner.\\n\\n4.2. Baseline Methods We compare with other methods in Tab. 1, with stereo event-based methods (for reference) and with event and frame-based methods [5, 33]. The event-based monocular methods used for comparison are EVO and USLAM.\\n\\n\u2022 EVO [17] is a semi-dense VO approach that builds maps using event-based space sweeping [53] and tracks the camera motion by edge-map alignment, creating binary images of 1k-2k events and aligning them to the projected...\"}"}
{"id": "CVPR-2022-610", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Event frames (2) (left) and EGM frames (6) (right) for the monitor sequence. Here, positive brightness changes are displayed in white, and negative ones in black. Gray color means there is no brightness change. This figure contains animations that can be viewed in Acrobat Reader.\\n\\npoint cloud map. EVO does not exploit the EGM (3) since it does not use frames nor recovers image brightness.\\n\\n\u2022 USLAM [18] is an indirect monocular method that combines events, frames and IMU measurements. Its front-end converts events into frames by motion compensation using the IMU's gyroscope and the median scene depth. Then FAST corners [54] are extracted and tracked separately on the event frames and the grayscale frames, and are passed to a state-of-the-art geometric feature-based back-end [55]. USLAM is the only method that we consider with an IMU. The IMU is tightly used in the front-end for event frame creation, and so removing it is not possible without breaking the (robustness of the) method.\\n\\n\u2022 ORB-SLAM [33] and DSO [5] are the state-of-the-art indirect and direct frame-based monocular visual odometry methods, respectively. We also compare with these methods, using different types of frames (from the DA VIS or reconstructed using E2VID [56]).\\n\\n\u2022 EDS performs event-to-image alignment in the front-end using 20 events with an overlap of 50%, generating a new event frame and tracking the camera motion every new 10 events. A visualization is shown in Fig. 3. This gives an effective (and adaptive) event-frame rate of \u224860 FPS depending on camera motion while the standard frames are given at a fixed rate of 50 ms (i.e., 20 FPS).\\n\\n4.3. Ego-motion Estimation Results\\n\\nTables 2\u20133 and Fig. 4 report quantitative and qualitative results of the comparison of our method with the state of the art on sequences from the dataset [50]. Fig. 4 shows sample estimated depth maps and corresponding point clouds (back-projected depth maps). We kept the original depth map visualization of the baseline methods (EVO and DSO).\\n\\nComparison to event-based baselines. Qualitatively, Fig. 4 shows that EDS correctly estimates depth at most contour pixels, forming semi-dense structures in the colored depth maps and point clouds. The recovered 3D maps have a higher level of detail than those given by EVO. USLAM produced too sparse maps to convey any visual insight.\\n\\nQuantitatively (Tab. 2), EDS outperforms all other monocular baseline methods, even without using inertial measurements (which are known to improve robustness and increase accuracy in VO [57]). Our approach also outperforms the state-of-the-art event-only stereo method ESVO [26], despite the fact that our method is monocular and hence does not exploit the spatial parallax of stereo setups. The tight fusion in the front-end (between frames and events) and the PBA in the back-end compensate for the lack of stereo baseline in the event data.\\n\\nComparison to frame-based baselines. Qualitatively (Fig. 4), DSO produces dilated depth maps for visualization. Hence, they look more complete, but they have also more outliers than those produced by EDS.\\n\\nQuantitatively (Tab. 3), in terms of translation errors our method is consistently better than monocular ORB-SLAM, and only slightly worse than stereo ORB-SLAM (\\\"F+F\\\") with bundle adjustment enabled [33], which is on average the best performing method. We obtain similar results as DSO, being more accurate (ATE 1.5 cm) on the desk sequence due to a faster camera motion. We also run DSO on reconstructed frames at 60 FPS using E2VID [56], on the same 20 kevents as EDS. The results show a lower ATE in 5786.\"}"}
{"id": "CVPR-2022-610", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Qualitative comparison on four test sequences from [50]. The first three rows depict pseudo-colored inverse depth maps for each method. EVO's color code is yellow-near blue-far, while DSO and EDS colors are red-near blue-far. The depth range is 1\u20137 m in all sequences. The 3D point cloud reconstructed by EDS is shown in the last row, with grayscale values from the keyframe.\\n\\nthe desk sequence (low texture scene) for DSO\u2020. However it fails in the sequences with high texture (bin and boxes), where E2VID has difficulties to correctly reconstruct the frames due to the limitations of the trained model. Our findings agree with those of EKLT (see [23, Tab. 7] for further details). In terms of rotation errors, EDS is in line with the baselines. Events help estimating camera rotation, especially during sudden motions. This is the reason why DSO\u2020 and EDS produce the most accurate results in the desk sequence. The large rotational error for DSO in desk is due to an insufficient frame rate for the given camera speed.\\n\\nLow Frame Rate Experiments. Increasing motion speed makes frames further apart, hence it is effectively equivalent to reducing frame rate (except for motion blur), and event-based odometry excels at high-speed motion [16], where usually frame-based approaches fail. Therefore, next, we progressively reduce the frame rate and compare with the state-of-the-art monocular methods from Tab. 3.\\n\\nWe run DSO in two modes: (i) DSO with tracking recovery enabled using 27 different small rotations (see Sec. 3.1 in [5]) and (ii) DSO with tracking recovery disabled, indicated as DSO\u2217. We also compare with ORB-SLAM (full SLAM system, for completeness) and ORB-SLAM\u2217 (a more fair baseline with loop closure disabled and the same number of nodes in the covisibility graph as our sliding window, i.e., 7 keyframes). A comprehensive visualization of the results is shown in Fig. 5, where the average ATE for all sequences is depicted at the different frame rates (including the original frame rate from Tab. 3). The plot shows that EDS is almost agnostic to a decrease of the frame rate, while the errors significantly grow for DSO and DSO\u2217 as the frame rate decreases. DSO with tracking recovery enabled succeeds until 10 FPS, but is not able to recover the camera pose at lower frame rates. Compared to DSO\u2217, EDS does not require a tracking-recovery strategy since it is capable of continuously tracking at any frame rate using events, only limited by the computational cost. The ORB-SLAM variants produce very competitive results and degrade more gracefully than DSO for low frame rates. Still, our method also performs better than ORB-SLAM\u2217. Therefore, the EDS tracker is more robust than frame-based state-of-the-art odometry in this scenario.\\n\\n4.4. Sensitivity Study and Limitations\\n\\nDepth inaccuracies and noise in C are the main subject of study. The brightness change in the EGM (6) highly de...\"}"}
{"id": "CVPR-2022-610", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Average Absolute Trajectory Error (RMS) for ORB-SLAM [33], ORB-SLAM\\\\(^*\\\\) (w/o loop closure), DSO [5], DSO\\\\(^*\\\\) (w/o recovery-tracking) and EDS (ours) on dataset [50].\\n\\nErrors are computed for EDS every time an event-frame is tracked (equal at any frame rate), whereas for the baseline frame-based methods they are computed only when a frame is received (i.e., according to the frame rate).\\n\\nThis makes camera tracking more sensitive to accurate point triangulation than direct image alignment methods. Events are also susceptible to noise in \\\\(C\\\\) due to manufacturing, which causes a mismatch in the optical flow constraint (3). We use the simulator in [58] to generate synthetic sequences with building-like scenes (e.g., atrium [32]) and to control the scene and event camera parameters, in order to understand the strengths and limitations of EDS.\\n\\nIn one study, we perturb the ground truth 3D map with zero-mean Gaussian noise of standard deviation \\\\(\\\\sigma_d = 1 - 50\\\\%\\\\) of the median scene depth, which is \\\\(9.7\\\\) m in the atrium sequence [32]. In another study, the contrast sensitivity is set to a mean value of \\\\(\\\\mu_C = 0.5\\\\) and is perturbed with Gaussian noise of standard deviation \\\\(\\\\sigma_C \\\\in [0.05, 0.25]\\\\). Figures 6 and 7 show the depth and contrast errors resulting from these two sensitivity studies, respectively. As we observe in the box plots, the EDS tracker gracefully degrades as depth noise increases, while it has an abrupt degradation in terms of contrast noise, not being able to track the camera motion when the contrast noise has \\\\(\\\\sigma_C > 0.15\\\\). One limitation of this approach might be that events are HDR while frames are not necessarily, which makes the EGM estimation challenging (see Fig. 3). We refer the reader to the supplementary material for more details and experiments.\\n\\n5. Conclusion\\n\\nWe have presented the first monocular direct 6-DOF VO method using events and frames. EDS has several innovations with respect to prior event-based methods, such as the tight coupling of the events and frames in the front-end and the photometric bundle adjustment in the back-end. We strove to compare with multiple event- and frame-based baselines; the results showed that EDS outperforms all event-based methods, and that it is in line with DSO at 10-20 FPS. However, EDS outperforms DSO and ORB-SLAM without loop closure in low frame rate scenarios, tracking with events accurately in between frames. The sensitivity study showed that EDS is robust to depth noise as well as contrast sensitivity event noise. We release the code and dataset to the public, and hope that this research will spark new ideas on low-power and robust VO by combining the best of events and frames.\\n\\nAcknowledgement\\n\\nThis work was supported by Huawei Zurich Research Center, the National Centre of Competence in Research (NCCR) Robotics through the Swiss National Science Foundation under grant agreement No. 51NF40 185543, and the European Research Council (ERC) under grant agreement No. 864042 (AGILEFLIGHT).\"}"}
{"id": "CVPR-2022-610", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Cesar Cadena, Luca Carlone, Henry Carrillo, Yasir Latif, Davide Scaramuzza, Jos\u00e9 Neira, Ian D. Reid, and John J. Leonard, \\\"Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age,\\\" IEEE Trans. Robot., vol. 32, no. 6, pp. 1309\u20131332, 2016.\\n\\n[2] Andrew J. Davison, \\\"FutureMapping: The computational structure of spatial AI systems,\\\" arXiv e-prints, Mar. 2018.\\n\\n[3] Antonio Loquercio, Elia Kaufmann, Ren\u00e9 Ranftl, Matthias M\u00fcller, Vladlen Koltun, and Davide Scaramuzza, \\\"Learning high-speed flight in the wild,\\\" Science Robotics, Oct. 2021.\\n\\n[4] Ra\u00fal Mur-Artal, Jos\u00e9 M. M. Montiel, and Juan D. Tard\u00f3s, \\\"ORB-SLAM: a versatile and accurate monocular SLAM system,\\\" IEEE Trans. Robot., vol. 31, no. 5, pp. 1147\u20131163, 2015.\\n\\n[5] Jakob Engel, Vladlen Koltun, and Daniel Cremers, \\\"Direct Sparse Odometry,\\\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, pp. 611\u2013625, Mar. 2018.\\n\\n[6] Christian Forster, Zichao Zhang, Michael Gassner, Manuel Werlberger, and Davide Scaramuzza, \\\"SVO: Semidirect visual odometry for monocular and multicamera systems,\\\" IEEE Trans. Robot., vol. 33, no. 2, pp. 249\u2013265, 2017.\\n\\n[7] Patrick Lichtsteiner, Christoph Posch, and Tobi Delbruck, \\\"A 128 \u00d7 128 120 dB 15 \u00b5s latency asynchronous temporal contrast vision sensor,\\\" IEEE J. Solid-State Circuits, vol. 43, no. 2, pp. 566\u2013576, 2008.\\n\\n[8] Christoph Posch, Daniel Matolin, and Rainer Wohlgenannt, \\\"A QVGA 143dB dynamic range asynchronous address-event PWM dynamic image sensor with lossless pixel-level video compression,\\\" in IEEE Intl. Solid-State Circuits Conf. (ISSCC), pp. 400\u2013401, 2010.\\n\\n[9] Christoph Posch, Teresa Serrano-Gotarredona, Bernabe Linares-Barranco, and Tobi Delbruck, \\\"Retinomorphic event-based vision sensors: Bioinspired cameras with spiking output,\\\" Proc. IEEE, vol. 102, pp. 1470\u20131484, Oct. 2014.\\n\\n[10] Andrea Censi and Davide Scaramuzza, \\\"Low-latency event-based visual odometry,\\\" in IEEE Int. Conf. Robot. Autom. (ICRA), pp. 703\u2013710, 2014.\\n\\n[11] Hanme Kim, Ankur Handa, Ryad Benosman, Sio-Hoi Ieng, and Andrew J. Davison, \\\"Simultaneous mosaicing and tracking with an event camera,\\\" in British Mach. Vis. Conf. (BMVC), 2014.\\n\\n[12] Elias Mueggler, Basil Huber, and Davide Scaramuzza, \\\"Event-based, 6-DOF pose tracking for high-speed maneuvers,\\\" in IEEE/RSJ Int. Conf. Intell. Robot. Syst. (IROS), pp. 2761\u20132768, 2014.\\n\\n[13] Hanme Kim, Stefan Leutenegger, and Andrew J. Davison, \\\"Real-time 3D reconstruction and 6-DoF tracking with an event camera,\\\" in Eur. Conf. Comput. Vis. (ECCV), pp. 349\u2013364, 2016.\\n\\n[14] Beat Kueng, Elias Mueggler, Guillermo Gallego, and Davide Scaramuzza, \\\"Low-latency visual odometry using event-based feature tracks,\\\" in IEEE/RSJ Int. Conf. Intell. Robot. Syst. (IROS), pp. 16\u201323, 2016.\\n\\n[15] Alex Zihao Zhu, Nikolay Atanasov, and Kostas Daniilidis, \\\"Event-based visual inertial odometry,\\\" in IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 5816\u20135824, 2017.\\n\\n[16] Guillermo Gallego, Jon E. A. Lund, Elias Mueggler, Henri Rebecq, Tobi Delbruck, and Davide Scaramuzza, \\\"Event-based, 6-DOF camera tracking from photometric depth maps,\\\" IEEE Trans. Pattern Anal. Mach. Intell., vol. 40, pp. 2402\u20132412, Oct. 2018.\\n\\n[17] Henri Rebecq, Timo Horstsch\u00e4fer, Guillermo Gallego, and Davide Scaramuzza, \\\"EVO: A geometric approach to event-based 6-DOF parallel tracking and mapping in real-time,\\\" IEEE Robot. Autom. Lett., vol. 2, no. 2, pp. 593\u2013600, 2017.\\n\\n[18] Antoni Rosinol Vidal, Henri Rebecq, Timo Horstsch\u00e4fer, and Davide Scaramuzza, \\\"Ultimate SLAM? combining events, images, and IMU for robust visual SLAM in HDR and high speed scenarios,\\\" IEEE Robot. Autom. Lett., vol. 3, pp. 994\u20131001, Apr. 2018.\\n\\n[19] Guillermo Gallego, Henri Rebecq, and Davide Scaramuzza, \\\"A unifying contrast maximization framework for event cameras, with applications to motion, depth, and optical flow estimation,\\\" in IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), pp. 3867\u20133876, 2018.\\n\\n[20] William Oswaldo Chamorro Hernandez, Juan Andrade-Cetto, and Joan Sol\u00e0 Ortega, \\\"High-speed event camera tracking,\\\" in British Mach. Vis. Conf. (BMVC), 2020.\\n\\n[21] Haram Kim and H. Jin Kim, \\\"Real-time rotational motion estimation with contrast maximization over globally aligned events,\\\" IEEE Robot. Autom. Lett., vol. 6, no. 3, pp. 6016\u20136023, 2021.\\n\\n[22] Guillermo Gallego, Tobi Delbruck, Garrick Orchard, Chiara Bartolozzi, Brian Taba, Andrea Censi, Stefan Leutenegger, Andrew Davison, J\u00f6rg Conradt, Kostas Daniilidis, and Davide Scaramuzza, \\\"Event-based vision: A survey,\\\" IEEE Trans. Pattern Anal. Mach. Intell., 2020.\\n\\n[23] Daniel Gehrig, Henri Rebecq, Guillermo Gallego, and Davide Scaramuzza, \\\"EKLT: Asynchronous photometric feature tracking using events and frames,\\\" Int. J. Comput. Vis., 2019.\\n\\n[24] Matthew Cook, Luca Gugelmann, Florian Jug, Christoph Krautz, and Angelika Steger, \\\"Interacting maps for fast visual interpretation,\\\" in Int. Joint Conf. Neural Netw. (IJCNN), pp. 770\u2013776, 2011.\\n\\n[25] Alex Zihao Zhu, Liangzhe Yuan, Kenneth Chaney, and Kostas Daniilidis, \\\"Unsupervised event-based learning of optical flow, depth, and egomotion,\\\" in IEEE Conf. Comput. Vis. Pattern Recog. (CVPR), 2019.\\n\\n[26] Yi Zhou, Guillermo Gallego, and Shaojie Shen, \\\"Event-based stereo visual odometry,\\\" IEEE Trans. Robot., vol. 37, no. 5, pp. 1433\u20131450, 2021.\\n\\n[27] Elias Mueggler, Chiara Bartolozzi, and Davide Scaramuzza, \\\"Fast event-based corner detection,\\\" in British Mach. Vis. Conf. (BMVC), 2017.\\n\\n[28] Ignacio Alzugaray and Margarita Chli, \\\"Asynchronous corner detection and tracking for event cameras in real time,\\\" IEEE Robot. Autom. Lett., vol. 3, pp. 3177\u20133184, Oct. 2018.\"}"}
{"id": "CVPR-2022-610", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"David Weikersdorfer, Raoul Hoffmann, and J\u00f6rg Conradt, \\\"Simultaneous localization and mapping for event-based vision systems,\\\" in *Int. Conf. Comput. Vis. Syst. (ICVS)*, pp. 133\u2013142, 2013.\\n\\nDavid Weikersdorfer and J\u00f6rg Conradt, \\\"Event-based particle filtering for robot self-localization,\\\" in *IEEE Int. Conf. Robot. Biomimetics (ROBIO)*, pp. 866\u2013870, 2012.\\n\\nElias Mueggler, Guillermo Gallego, and Davide Scaramuzza, \\\"Continuous-time trajectory estimation for event-based vision sensors,\\\" in *Robotics: Science and Systems (RSS)*, 2015.\\n\\nSamuel Bryner, Guillermo Gallego, Henri Rebecq, and Davide Scaramuzza, \\\"Event-based, direct camera tracking from a photometric 3D map using nonlinear optimization,\\\" in *IEEE Int. Conf. Robot. Autom. (ICRA)*, 2019.\\n\\nRa\u00fal Mur-Artal and Juan D. Tard\u00f3s, \\\"ORB-SLAM2: An open-source SLAM system for monocular, stereo, and RGB-D cameras,\\\" *IEEE Trans. Robot.*, vol. 33, pp. 1255\u20131262, Oct. 2017.\\n\\nValentina Vasco, Arren Glover, and Chiara Bartolozzi, \\\"Fast event-based Harris corner detection exploiting the advantages of event-driven cameras,\\\" in *IEEE/RSJ Int. Conf. Intell. Robot. Syst. (IROS)*, 2016.\\n\\nPhilippe Chiberre, Etienne Perot, Amos Sironi, and Vincent Lepetit, \\\"Detecting stable keypoints from events through image gradient prediction,\\\" in *IEEE Conf. Comput. Vis. Pattern Recog. Workshops (CVPRW)*, 2021.\\n\\nGuillermo Gallego and Davide Scaramuzza, \\\"Accurate angular velocity estimation with an event camera,\\\" *IEEE Robot. Autom. Lett.*, vol. 2, no. 2, pp. 632\u2013639, 2017.\\n\\nChristian Reinbacher, Gottfried Munda, and Thomas Pock, \\\"Real-time panoramic tracking for event cameras,\\\" in *IEEE Int. Conf. Comput. Photography (ICCP)*, pp. 1\u20139, 2017.\\n\\nCedric Scheerlinck, Nick Barnes, and Robert Mahony, \\\"Continuous-time intensity estimation using event cameras,\\\" in *Asian Conf. Comput. Vis. (ACCV)*, 2018.\\n\\nZhenjiang Ni, Aude Bolopion, Joel Agnus, Ryad Benosman, and St\u00e9phane R\u00e9gnier, \\\"Asynchronous event-based visual shape tracking for stable haptic feedback in microrobotics,\\\" *IEEE Trans. Robot.*, vol. 28, no. 5, pp. 1081\u20131089, 2012.\\n\\nDaniel Gehrig, Michelle Ruegg, Mathias Gehrig, Javier Hidalgo-Carrio, and Davide Scaramuzza, \\\"Combining events and frames using recurrent asynchronous multimodal networks for monocular depth prediction,\\\" *IEEE Robot. Autom. Lett.*, 2021.\\n\\nLiyuan Pan, Richard I. Hartley, Miao-miao Liu, Xin Yu, and Yuchao Dai, \\\"High frame rate video reconstruction based on an event camera,\\\" *IEEE Trans. Pattern Anal. Mach. Intell.*, 2020.\\n\\nStepan Tulyakov, Daniel Gehrig, Stamatios Georgoulis, Julius Erbach, Mathias Gehrig, Yuanyou Li, and Davide Scaramuzza, \\\"TimeLens: Event-based video frame interpolation,\\\" in *IEEE Conf. Comput. Vis. Pattern Recog. (CVPR)*, 2021.\\n\\nGuillermo Gallego, Christian Forster, Elias Mueggler, and Davide Scaramuzza, \\\"Event-based camera pose tracking using a generative event model.\\\" arXiv:1510.01972, 2015.\\n\\nGemma Taverni, Diederik Paul Moeys, Chenghan Li, Celso Cavaco, Vasyl Motsnyi, David San Segundo Bello, and Tobi Delbruck, \\\"Front and back illuminated Dynamic and Active Pixel Vision Sensors comparison,\\\" *IEEE Trans. Circuits Syst. II*, vol. 65, no. 5, pp. 677\u2013681, 2018.\\n\\nPeter Corke, *Robotics, Vision and Control: Fundamental Algorithms in MATLAB*. Springer Tracts in Advanced Robotics, Springer, 2017.\\n\\nA. Agarwal, K. Mierle, and Others, \\\"Ceres solver.\\\" http://ceres-solver.org.\\n\\nRichard Hartley and Andrew Zisserman, *Multiple View Geometry in Computer Vision*. Cambridge University Press, 2003.\\n\\nRen\u00e9 Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun, \\\"Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer,\\\" *IEEE Trans. Pattern Anal. Mach. Intell.*, 2020.\\n\\nZichao Zhang, Guillermo Gallego, and Davide Scaramuzza, \\\"On the comparison of gauge freedom handling in optimization-based visual-inertial state estimation,\\\" *IEEE Robot. Autom. Lett.*, vol. 3, pp. 2710\u20132717, July 2018.\\n\\nYi Zhou, Guillermo Gallego, Henri Rebecq, Laurent Kneip, Hongdong Li, and Davide Scaramuzza, \\\"Semi-dense 3D reconstruction with a stereo event camera,\\\" in *Eur. Conf. Comput. Vis. (ECCV)*, pp. 242\u2013258, 2018.\\n\\nJ. Sturm, N. Engelhard, F. Endres, W. Burgard, and D. Cremers, \\\"A benchmark for the evaluation of RGB-D SLAM systems,\\\" in *IEEE/RSJ Int. Conf. Intell. Robot. Syst. (IROS)*, Oct. 2012.\\n\\nZichao Zhang and Davide Scaramuzza, \\\"A tutorial on quantitative trajectory evaluation for visual(-inertial) odometry,\\\" in *IEEE/RSJ Int. Conf. Intell. Robot. Syst. (IROS)*, 2018.\\n\\nHenri Rebecq, Guillermo Gallego, Elias Mueggler, and Davide Scaramuzza, \\\"EMVS: Event-based multi-view stereo\u20143D reconstruction with an event camera in real-time,\\\" *Int. J. Comput. Vis.*, vol. 126, pp. 1394\u20131414, Dec. 2018.\\n\\nEdward Rosten and Tom Drummond, \\\"Machine learning for high-speed corner detection,\\\" in *Eur. Conf. Comput. Vis. (ECCV)*, pp. 430\u2013443, 2006.\\n\\nS. Leutenegger, S. Lynen, M. Bosse, R. Siegwart, and P. Furgale, \\\"Keyframe-based visual-inertial SLAM using nonlinear optimization,\\\" *Int. J. Robot. Research*, 2015.\\n\\nHenri Rebecq, Ren\u00e9 Ranftl, Vladlen Koltun, and Davide Scaramuzza, \\\"High speed and high dynamic range video with an event camera,\\\" *IEEE Trans. Pattern Anal. Mach. Intell.*, 2019.\\n\\nChristian Forster, Luca Carlone, Frank Dellaert, and Davide Scaramuzza, \\\"On-manifold preintegration for real-time visual-inertial odometry,\\\" *IEEE Trans. Robot.*, vol. 33, no. 1, pp. 1\u201321, 2017.\\n\\nHenri Rebecq, Daniel Gehrig, and Davide Scaramuzza, \\\"ESIM: an open event camera simulator,\\\" in *Conf. on Robotics Learning (CoRL)*, 2018.\\n\\nEdward Rosten and Tom Drummond, \\\"Machine learning for high-speed corner detection,\\\" in *Eur. Conf. Comput. Vis. (ECCV)*, pp. 430\u2013443, 2006.\"}"}
