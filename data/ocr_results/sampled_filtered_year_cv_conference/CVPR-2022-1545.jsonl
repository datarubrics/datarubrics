{"id": "CVPR-2022-1545", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Rethinking Bayesian Deep Learning Methods for Semi-Supervised Volumetric Medical Image Segmentation\\n\\nJianfeng Wang\\nDepartment of Computer Science\\nUniversity of Oxford\\njianfeng.wang@cs.ox.ac.uk\\n\\nThomas Lukasiewicz\\nDepartment of Computer Science\\nUniversity of Oxford\\nthomas.lukasiewicz@cs.ox.ac.uk\\n\\nAbstract\\nRecently, several Bayesian deep learning methods have been proposed for semi-supervised medical image segmentation. Although they have achieved promising results on medical benchmarks, some problems are still existing. Firstly, their overall architectures belong to the discriminative models, and hence, in the early stage of training, they only use labeled data for training, which might make them overfit to the labeled data. Secondly, in fact, they are only partially based on Bayesian deep learning, as their overall architectures are not designed under the Bayesian framework. However, unifying the overall architecture under the Bayesian perspective can make the architecture have a rigorous theoretical basis, so that each part of the architecture can have a clear probabilistic interpretation. Therefore, to solve the problems, we propose a new generative Bayesian deep learning (GBDL) architecture. GBDL belongs to the generative models, whose target is to estimate the joint distribution of input medical volumes and their corresponding labels. Estimating the joint distribution implicitly involves the distribution of data, so both labeled and unlabeled data can be utilized in the early stage of training, which alleviates the potential overfitting problem. Besides, GBDL is completely designed under the Bayesian framework, and thus we give its full Bayesian formulation, which lays a theoretical probabilistic foundation for our architecture. Extensive experiments show that our GBDL outperforms previous state-of-the-art methods in terms of four commonly used evaluation indicators on three public medical datasets.\\n\\n1. Introduction\\nDeep neural networks are a powerful tool for visual learning, and they have outperformed previous methods that are based on hand-crafted features in many classical computer vision tasks. Due to the power of deep neural networks, researchers also try to use them to segment pathology regions from medical images. Training segmentation networks usually relies on large labeled medical image datasets, but they are scarce and difficult to obtain, since not everyone is qualified to annotate medical data, and there is only a limited number of medical experts. Therefore, semi-supervised segmentation has become an important research direction in the area of medical computer vision.\\n\\nSeveral methods [1, 2, 5, 6, 9\u201311, 14, 15, 17\u201319, 21, 22] have been proposed in recent years. Among these works, Bayesian deep-learning-based methods [14, 15, 17, 18, 21] are closely related to this work, as they are based on Monte Carlo (MC) dropout [4], which is an approximation of Bayesian neural networks (BNNs). These methods are mainly built with the teacher-student architecture, which can be regarded as the discriminative model. Specifically, they initially build the model \\\\( P(Y|X) \\\\) only from labeled data, and then apply the model to generate pseudo labels that are refined or rectified based on the epistemic uncertainty [7] provided by MC dropout for unlabeled ones. Then, the pseudo labels are further combined with unlabeled and labeled data to further train the overall architecture.\\n\\nThe works [14, 15, 17, 18, 21] mainly have two issues. Firstly, their models are built only with labeled data in the early stage of training and may overfit to them, since the quantity of labeled data is limited. Consequently, the new pseudo labels generated by the models might be inaccurate, which adversely impacts the subsequent training process. Secondly, their models are only partially based on Bayesian deep learning and are not designed under the Bayesian framework. Hence, they are unable to give a Bayesian formulation with regard to their models, lacking a solid theoretical basis. As a result, some modules in their models are empirically designed, and the functions of those modules remain unclear.\\n\\n1 Note that we only consider previous methods that build their architectures with 3D CNNs, since our architecture is also based on 3D CNNs. In most cases, using 3D CNNs to process medical volumetric data performs better than using 2D CNNs, as the relationships among neighboring slices are preserved in 3D CNNs. Thus, comparing our method with 2D-CNN-based methods is unfair to them.\"}"}
{"id": "CVPR-2022-1545", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this work, we aim to fix these two problems by proposing a new generative model, named generative Bayesian deep learning (GBDL) architecture, as well as its corresponding full Bayesian formulation. Unlike teacher-student-based architectures, GBDL estimates the joint probability distribution $P(X,Y)$ via both labeled and unlabeled data, based on which more reliable pseudo-labels can be generated, since the generative model is more faithful about the overall data distribution $P(X)$ and the intrinsic relationship between inputs and their corresponding labels (modeled by $P(X,Y)$), alleviating the overfitting problem. Moreover, GBDL is entirely constructed under the Bayesian framework, and its related full Bayesian formulation lays a solid theoretical foundation, so that every part of GBDL has its corresponding probabilistic formulation and interpretation.\\n\\nThe main contributions of this paper are as follows:\\n\\n\u2022 We propose a new generative Bayesian deep learning (GBDL) architecture. GBDL aims to capture the joint distribution of inputs and labels, which moderates the potential overfitting problem of the teacher-student architecture in previous works [14, 15, 17, 18, 21].\\n\\n\u2022 Compared with previous methods [14, 15, 17, 18, 21], GBDL is completely designed under the Bayesian framework, and its full Bayesian formulation is also given here. Thus, it has a theoretical probabilistic foundation, and the choice of each module in GBDL and the construction of the loss functions are more mathematically grounded.\\n\\n\u2022 In extensive experiments, GBDL outperforms previous methods with a healthy margin in terms of four evaluation indicators on three public medical datasets, illustrating that GBDL is a superior architecture for semi-supervised volumetric medical image segmentation.\\n\\nThe rest of this paper is organized as follows. In Section 2, we review related methods. Section 3 presents our GBDL architecture and its Bayesian formulation, followed by the experimental settings and results in Section 4. In Section 5, we give a summary and an outlook on future research. The source code is available at https://github.com/Jianf-Wang/GBDL.\\n\\n2. Related Work\\n\\nIn this section, we briefly review related Bayesian and other deep learning methods for semi-supervised volumetric medical image segmentation in the past few years.\\n\\n2.1. Bayesian Deep Learning Methods\\n\\nPrevious Bayesian deep learning methods simply use MC dropout as a tool to detect when and where deep models might make false predictions [14, 15, 17, 18, 21]. Specifically, an uncertainty-aware self-ensembling model [21] was proposed based on the teacher-student model and MC dropout. In particular, a teacher model and a student model were built, where the latter learnt from the former by minimizing the segmentation loss on the labeled data and the consistency loss on all the input data. MC dropout was leveraged to filter out the unreliable predictions and to preserve the reliable ones given by the teacher model, and the refined predictions can help the student model learn from unlabeled data. Based on this uncertainty-aware teacher-student model, a double-uncertainty weighted method [18] was proposed, which takes both the prediction and the feature uncertainty into consideration for refining the predictions of the teacher model during training. Sedai et al. [14] used MC dropout for training the teacher model as well, but they designed a novel loss function to guide the student model by adaptively weighting regions with unreliable soft labels to improve the final segmentation performance, rather than simply removing the unreliable predictions. Shi et al. [15] improved the way of uncertainty estimation by designing two additional models, which are called object conservation model and object-radical model, respectively. Based on these two models, certain region masks and uncertain region masks can be obtained to improve pseudo labels and to prevent the possible error propagation during training. Since the multi-task learning can boost the performance of segmentation, and it has not been considered in previous works, Wang et al. [17] combined it into their architecture, and imposed the uncertainty estimation on all tasks to get a tripled-uncertainty that is further used to guide the training of their student model.\\n\\n2.2. Other Deep Learning Methods\\n\\nConcerning other deep learning methods proposed recently, they are mainly classified as three categories: new training and learning strategies [1, 22], shape-aware or structure-aware based methods [5, 6, 9], and consistency regularization based methods [2, 10, 11, 19]. The new training and learning strategies aim to gradually improve the quality of pseudo labels during training. For example, Bai et al. [1] proposed an iterative training strategy, in which the network is first trained on labeled data, and then it predicts pseudo labels for unlabeled data, which are further combined with the labeled data to train the network again. During the iterative training process, a conditional random field (CRF) [8] was used to refine the pseudo labels. Zeng et al. [22] proposed a reciprocal learning strategy for their teacher-student architecture. The strategy contains a feedback mechanism for the teacher network via observing how pseudo labels would affect the student, which is omitted in previous teacher-student based models.\\n\\nThe shape-aware or structure-aware based methods encourage the network to explore complex geometric information in medical images. For instance, Hang et al. [5] proposed a local and global structure-aware entropy-regularized mean teacher (LG-ER-MT) architecture for semi-supervised learning.\"}"}
{"id": "CVPR-2022-1545", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"It extracts local spatial structural information by calculating inter-voxel similarities within small volumes and global geometric structure information by utilizing weighted self-information. Li et al. [9] introduced a shape-aware semi-supervised segmentation strategy that integrates a more flexible geometric representation into the network for boosting the performance. Huang et al. [6] designed a 3D Graph Shape-aware Self-ensembling Network (3D Graph-S\u00b2Net), which is composed by a multi-task learning network and a graph-based module. The former performs the semantic segmentation task and predicts the signed distance map that encodes rich features of object shape and surface, while the latter explores co-occurrence relations and diffuse information between these two tasks.\\n\\nThe consistency-regularization-based methods try to add different consistency constraints for unlabeled data. In particular, Bortsova et al. [2] proposed a model that is implemented by a Siamese architecture with two identical branches to receive differently transformed versions of the same images. Its target is to learn the consistency under different transformations of the same input. Luo et al. [10] designed a novel dual-task-consistency model, whose basic idea is to encourage consistent predictions of the same input under different tasks. Xie et al. [19] proposed a pairwise relation-based semi-supervised (PRS\u00b2) model for gland segmentation on histology tissue images. In this model, a supervised segmentation network (S-Net) and an unsupervised pairwise relation network (PR-Net) are built. The PR-Net learns both semantic consistency and image representations from each pair of images in the feature space for improving the segmentation performance of S-Net. Luo et al. [11] proposed a pyramid (i.e., multi-scale) architecture that encourages the predictions of an unlabeled input at multiple scales to be consistent, which serves as a regularization for unlabeled data.\\n\\n3. Methodology\\n\\nIn this section, we start from giving the Bayesian formulation of our GBDL architecture. Thereafter, the architecture and related loss functions are introduced in detail.\\n\\n3.1. Bayesian Formulation\\n\\nLearning Procedure. The objective of semi-supervised volumetric medical image segmentation is to learn a segmentation network, whose weights are denoted by $W$, with partially labeled data, and the Bayesian treatment of this target is to learn the posterior distribution, namely, $P(W | X, Y_L)$. Now, we use $X$ to denote the input volumes, which contains labeled input volumes ($X_L$) and unlabeled input volumes ($X_U$), i.e., $X = \\\\{X_L, X_U\\\\}$, and we use $Y = \\\\{Y_L, Y_U\\\\}$ to denote the ground-truth labels with respect to $X$. Each label in $Y$ is a voxel-wise segmentation label map that has the same shape as its corresponding input volume. Note that $Y_U$ represents the ground-truth labels of $X_U$, which are not observed in the training data. The whole learning procedure of $P(W | X, Y_L)$ can be written as:\\n\\n$$P(W | X, Y_L) = \\\\int \\\\int \\\\int P(W | X, Y) P(X, Y | Z) P(Z | X, Y_L) \\\\, dZ \\\\, dX \\\\, dY,$$\\n\\n(1)\\n\\nwhere $Z$ is the latent representation that governs the joint distribution of $X$ and $Y$, denoted by $P(X, Y | Z)$. To estimate the joint distribution, one should learn $Z$ from $X$ and $Y_L$. Concerning that Eq. (1) is intractable, we take the MC approximation of it as follows:\\n\\n$$P(W | X, Y_L) \\\\approx 1 \\\\frac{1}{MN} \\\\sum_{i=0}^{M-1} \\\\sum_{j=0}^{N-1} P(W | X(i,j), Y(i,j)).$$\\n\\n(2)\\n\\nIn this approximation, $M$ latent representations $Z$ are drawn from $P(Z | X, Y_L)$. Then, $N$ pairs of input volumes and labels are obtained from $P(X, Y | Z)$, which are further used to get the posterior distribution. Thus, to generate $X$ and $Y$, one should obtain the joint distribution $P(X, Y)$, i.e., estimating the distribution of $Z$. Overall, the learning procedure contains two steps:\\n\\n\u2022 Learning the distribution of $Z$ that governs the joint distribution $P(X, Y)$.\\n\\n\u2022 Learning the posterior distribution $P(W | X, Y)$ based on $X$ and $Y$ sampled from $P(X, Y)$.\\n\\nInference Procedure. The inference procedure can be formulated as follows:\\n\\n$$P(Y_{\\\\text{pred}} | X_{\\\\text{test}}, X, Y_L) = \\\\int P(Y_{\\\\text{pred}} | X_{\\\\text{test}}, W) P(W | X, Y_L) \\\\, dW,$$\\n\\n(3)\\n\\nwhere $X_{\\\\text{test}}$ and $Y_{\\\\text{pred}}$ denote the test input and the predicted result, respectively. The posterior distribution $P(W | X, Y_L)$ is learnt based on $X$ and $Y$ sampled from $P(X, Y)$, and thus the posterior can also be replaced by $P(W | X, Y)$. Since Eq. (3) is intractable as well, its MC approximation can be written as:\\n\\n$$P(Y_{\\\\text{pred}} | X_{\\\\text{test}}, X, Y_L) \\\\approx 1 \\\\frac{1}{T} \\\\sum_{i=0}^{T-1} P(Y_{\\\\text{pred}} | X_{\\\\text{test}}, W_i),$$\\n\\n(4)\\n\\nwhere $T$ models are drawn from $P(W | X, Y)$, which is usually implemented via MC dropout [4] with $T$ times feedforward passes. For each model ($W_i$) sampled from $P(W | X, Y)$, a prediction result can be obtained, and the final prediction of $X_{\\\\text{test}}$ can be calculated by averaging the $T$ results. In addition, we get the epistemic uncertainty by calculating the entropy of these predictions.\"}"}
{"id": "CVPR-2022-1545", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3D-UNet Encoder\\nInput Volume\\nMean vector of each slice\\nCovariance matrix of each slice\\nLatent representation of each slice\\n3D-UNet Decoder\\nReconstruct the input volume, i.e., $P(X | Z)$\\n3D-UNet Decoder\\n3D-UNet Encoder\\nGenerate labels based on $Z$ and $X$, i.e., $P(Y_L | X_L, Z)$\\nRegular 3D-Unet with MC dropout, i.e., $P(W | X, Y)$\\n\\nInput volumes and labels are drawn from $P(X, Y | Z)$ to train the regular 3D-Unet with MC dropout.\\n\\nFor every slice, sample from $(0, I)$. MSE $L_{MSE}$, $L_{CE} + L_{Dice}$, $L_{KL}[ Q(Z|X) \\\\| P(Z) ]$\\n\\nFigure 1. GBDL for semi-supervised volumetric medical image segmentation, including a latent representation learning (LRL) architecture (in the green dotted box) and a regular 3D-UNet with MC dropout (in the red dotted box). Only the regular 3D-UNet with MC dropout is used during testing. For simplicity, the shortcut connections between the paired 3D-UNet encoder and decoder are omitted.\\n\\n3.2. GBDL and Loss Functions\\nUnder the guidance of the given Bayesian formulation, we start to introduce the details of GBDL and loss functions.\\n\\nConcerning the two steps mentioned in the learning procedure above, GBDL has a latent representation learning (LRL) architecture and a regular 3D-UNet with MC dropout, which are shown in the green dotted box and the red dotted box in Figure 1, respectively. LRL is designed to learn the distribution of $Z$ and to capture the joint distribution $P(X,Y)$, while the regular 3D-UNet with MC dropout is the Bayesian deep model used for parameterizing the posterior distribution $P(W | X, Y)$.\\n\\nAs for LRL, we first assume a variational probability distribution $Q(Z)$ to represent the distribution of the latent representation $Z$. In this work, we follow the conditional variational autoencoder (cVAE) [16] to use the input to modulate $Z$, and thus, $Q(Z)$ can be rewritten as $Q(Z | X)$. Note that this work focuses on using 3D CNNs to process volumetric medical data, and therefore, each input volume contains several slices, $Q(Z | X)$ is actually the joint distribution of all these slices, i.e., $Q(Z | X) = q(z_0, z_1, z_2, ..., z_{n-1} | x_0, x_1, x_2, ..., x_{n-1})$, where $n$ is the number of slices in an input volume, $x_i$ and $z_i$ are a slice and its latent representation, respectively. To obtain the latent representation of each slice $z_i$, a 3D-UNet encoder is used in LRL. As the downsampling layers of the 3D-UNet encoder do not perform on the depth, and the 3D convolutional layers used in the encoder are of the kernel size $3$, the padding size $1$, and the stride $1$ (shown in the supplementary material), each $z_i$ is actually conditioned on $n_{rf}$ slices, where $n_{rf}$ is the total receptive field along the depth dimension, which is determined by the number of 3D convolutional layers in the encoder. For ease of calculation, we assume that the distribution of the latent representation of each slice follows a multivariate Gaussian distribution and that the latent representations of slices are independent from each other. It is worth noting that such independence assumption on the latent representations of slices is reasonable, as in this way, the latent representation of each slice just rests on $n_{rf}$ slices, which is consistent with the fact that each slice is closely relevant to its neighboring slices and distant slices may have no contribution to the slice. Therefore, based on the independence assumption, $Q(Z | X)$ can be decomposed into $\\\\prod_{i=0}^{n-1} q(z_i | x(i_{n_{rf}}))$, where $x(i_{n_{rf}})$ denotes the input slices that contribute to the $z_i$. After obtaining the latent representation $Z$, we can write the evidence lower bound (ELBO) as follows (with proof in the supplementary material):\\n\\n$$\\\\log P(X,Y) \\\\geq E_{Q} [ \\\\log P(Y | X,Z) + \\\\log P(X | Z) ] - E_{Q} [ \\\\log \\\\left( \\\\frac{Q(Z | X)}{P(Z)} \\\\right) ] \\\\quad (5)$$\\n\\nwhere $E_{Q}$ denotes the expectation over $Q(Z | X)$. Thus, the learning objective is to maximize the ELBO (Eq. (5)), which is achieved by maximizing $E_{Q} [ \\\\log P(Y | X,Z) + \\\\log P(X | Z) ]$ and minimizing $E_{Q} [ \\\\log \\\\left( \\\\frac{Q(Z | X)}{P(Z)} \\\\right) ]$.\\n\\nFirstly, to maximize $P(X | Z)$, another 3D-UNet decoder is used to take $z_i$ as input and the mean square error ($L_{MSE}$) is used as the loss function, which aims to reconstruct each slice of the input volume. Secondly, to maximize the probability $P(Y | X,Z)$, another branch can be built, which contains a 3D-UNet encoder and a 3D-UNet decoder as well. Since only parts of data have labels, $P(Y | X,Z)$ can also be rewritten as $P(Y_L | X_L, Z)$. In this branch, the 3D-UNet encoder receives $X_L$ to extract features that are further combined with their corresponding $Z$ to be input to the 3D-UNet decoder. Then, $Y_L$ is leveraged to calculate the dice loss.\"}"}
{"id": "CVPR-2022-1545", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"L Dice and cross-entropy loss $L_{CE}$ with $X$ to maximize the probability $P(Y|X, Z)$. Thirdly, minimizing $E_Q[\\\\log(Q(Z|X))/P(Z)]$ is to use $Q(Z|X)$ to approximate the prior distribution of $Z$, and in most cases, assuming that $Q(Z|X)$ and $P(Z)$ follow the same probability distribution can make the computation convenient. The following theorem (with proof in the supplementary material) is useful for constructing the distribution of $Q(Z|X)$.\\n\\n**Theorem 1.** The product of any number $n \\\\geq 1$ of multivariate Gaussian probability density functions with precision $\\\\Lambda_i$ and mean $\\\\mu_i$ results in an unnormalized Gaussian curve with precision $\\\\Lambda_* = \\\\sum_{i=0}^{n-1} \\\\Lambda_i$ and mean $\\\\mu_* = \\\\frac{\\\\Lambda_*^{-1} \\\\left( \\\\sum_{i=0}^{n-1} \\\\Lambda_i \\\\mu_i \\\\right)}{\\\\Lambda_*}$.\\n\\nAlthough an unnormalized Gaussian curve (denoted $f(x)$) is obtained by multiplying several multivariate Gaussian PDFs, a Gaussian distribution can still be obtained by simply normalizing $f(x)$ with $\\\\int f(x) \\\\, dx$ that does not contain $x$. Therefore, based on Theorem 1, we can suppose that $Q(Z|X)$ follows a multivariate Gaussian distribution. In addition, we also assume that the prior distribution $P(Z)$ follows a multivariate standard normal distribution. To minimize $E_Q[\\\\log(Q(Z|X))/P(Z)]$, we propose a new loss function as follows (with proof in the supplementary material).\\n\\n**Corollary 1.** Minimizing $E_Q[\\\\log(Q(Z|X))/P(Z)]$ is equivalent to minimizing $L_{KL}[Q(Z|X)||P(Z)] = \\\\frac{1}{2} \\\\cdot \\\\left( -\\\\log \\\\det(\\\\sum_{i=0}^{n-1} \\\\Lambda_i) + \\\\text{tr}(\\\\sum_{i=0}^{n-1} \\\\Lambda_i^{-1} \\\\cdot (\\\\sum_{i=0}^{n-1} \\\\Lambda_i \\\\mu_i)^T \\\\cdot (\\\\sum_{i=0}^{n-1} \\\\Lambda_i^{-1} \\\\cdot (\\\\sum_{i=0}^{n-1} \\\\Lambda_i \\\\mu_i)) - 2 \\\\cdot (\\\\sum_{i=0}^{n-1} \\\\Lambda_i \\\\mu_i) - D \\\\right)$, \\\\hspace{1cm} (6)\\n\\nwhere $n$ denotes the number of slices, and $D$ denotes the number of dimensions of mean vectors.\\n\\nTo summarize, maximizing ELBO (Eq. (5)) is equivalent to optimizing $L_{RL}$ and minimizing the loss function:\\n\\n$$L_{ELBO} = \\\\lambda_1 L_{CE} + \\\\lambda_2 L_{Dice} + \\\\lambda_3 L_{MSE} + \\\\lambda_4 L_{KL}[Q(Z|X)||P(Z)],$$ \\\\hspace{1cm} (7)\\n\\nwhere $\\\\lambda_1, \\\\lambda_2, \\\\lambda_3, \\\\lambda_4$ are coefficients. Significantly, cVAE [16] has some similar properties when compared with our LRL architecture, and it is important to emphasize the differences between cVAE-based models and our LRL, which mainly contain three aspects:\\n\\n- **Different ELBO:** cVAE is mainly used for generation tasks, in which the data distribution $P(X)$ is considered, and labels $Y$ are not involved into the ELBO of cVAE. In contrast, our LRL concerns the joint distribution of $X$ and $Y$, leading to a different ELBO (Eq. (5)).\\n\\n- **Independent Components Assumption:** cVAE assumes that the components in the latent representation of an input image are independent from each other. As a result, the encoder of cVAE only gives a mean vector and a variance vector for a latent representation. However, our LRL does not follow this assumption for each slice, and the encoder gives a mean vector and a covariance matrix for the latent representation of each slice.\\n\\n- **Joint Variational Distribution:** cVAE is mainly used for 2D inputs. In contrast, our LRL is specifically designed for 3D-CNN-based architectures for processing volumetric inputs. Therefore, since each input volume contains several slices, the variational distribution becomes a joint distribution over their latent representations, resulting in a different formulation of Kullback\u2013Leibler (KL) divergence between the joint variational distribution and the prior distribution (Eq. (6)).\\n\\nOnce LRL is trained well, the joint distribution of $X$ and $Y$ will be captured, and the second step is to learn the posterior distribution $P(W|X,Y)$. Therefore, input volumes and corresponding labels are generated from the fixed LRL to train the regular 3D-UNet with MC dropout with two loss functions that are widely used in medical image segmentation, namely, the dice loss $L_{Dice}$ and the cross-entropy loss $L_{CE}$. In this paper, we use $L_{Seg}$ to denote their summation:\\n\\n$$L_{Seg} = \\\\beta_1 L_{CE} + \\\\beta_2 L_{Dice},$$ \\\\hspace{1cm} (8)\\n\\nwhere $\\\\beta_1$ and $\\\\beta_2$ are the coefficient of the two loss terms in $L_{Seg}$. In the real implementation, the new generated pseudo labels will be combined with the unlabeled data and labeled data to train the regular 3D-UNet with MC dropout. According to Eq. (4), only the the regular 3D-UNet with MC dropout is used in the test phase. In addition, as we do a full Bayesian inference during testing, it is convenient to obtain the voxel-wise epistemic uncertainty for each predicted result by calculating the entropy (\\\\[-\\\\sum_{c=0}^{C-1} p_c \\\\log_2 p_c\\\\]) voxel-by-voxel, where $C$ is the number of classes.\\n\\n### 4. Experiments\\n\\nWe now report on experiments of the proposed GBDL on three public medical benchmarks. To save space, implementation details are shown in the supplementary material.\\n\\n#### 4.1. Datasets\\n\\nThe KiTS19 dataset is a kidney tumor segmentation dataset, which has 210 labeled 3D computed tomography (CT) scans for training and validation. We followed the settings in previous works [3, 18] to use 160 CT scans for training and validation. \\n\\n3In practice, the encoder of cVAE gives log-variance vectors instead of variance vectors.\\n\\n4Although the independent components assumption is widely used, it could be too strong and may degrade the performance.\\n\\n5[https://kits19.grand-challenge.org/](https://kits19.grand-challenge.org/)\"}"}
{"id": "CVPR-2022-1545", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Atrial Segmentation Challenge\\n\\nAtrial Segmentation Challenge (KiTS19) dataset has been released by the [Medical Decathlon](http://medicaldecathlon.com/index.html). We comprehensively evaluated various models and hyperparameter configurations to determine the best approach for atrial segmentation. The dataset consists of 131 training and 70 testing points, with each point representing a scan.\\n\\n### Scans used\\n\\n- 160 CT scans were used for training.\\n- 16 CT scans were used for testing.\\n\\n### Evaluation Metrics\\n\\nFour evaluation metrics are used to assess the performance of different models:\\n\\n- **Dice Score:** Measures the similarity between two sets, ranging from 0 to 1, where 1 indicates perfect overlap.\\n- **Average Surface Distance (ASD):** Measures the average distance between the boundaries of two object regions.\\n- **Jaccard Score:** Measures the overlap between two object regions, ranging from 0 to 1, where 1 indicates perfect overlap.\\n- **95HD:** Measures the closest point distance between two object regions.\\n\\n### Ablation Studies\\n\\nAblation studies were conducted on the KiTS19 and Atrial Segmentation Challenge datasets. The performance of different models is compared across various hyperparameter configurations.\\n\\n| Model Variant | Scans Used | Dice Score | ASD | Jaccard Score | 95HD |\\n|---------------|------------|------------|-----|--------------|------|\\n| w/o L         | 160        | 0.821 \u00b1 0.008 | 6.85 \u00b1 0.024 | 0.792 \u00b1 0.019 | 0.742 \u00b1 0.022 |\\n| w/o MSE       | 160        | 0.821 \u00b1 0.014 | 8.01 \u00b1 0.025 | 0.810 \u00b1 0.014 | 0.784 \u00b1 0.022 |\\n| w/ IC-LRL     | 160        | 0.814 \u00b1 0.011 | 7.19 \u00b1 0.021 | 0.814 \u00b1 0.011 | 0.782 \u00b1 0.020 |\\n| w/ GBDL       | 160        | 0.840 \u00b1 0.016 | 9.32 \u00b1 0.021 | 0.823 \u00b1 0.016 | 0.796 \u00b1 0.021 |\\n| w/ 3D-UNet    | 160        | 0.888 \u00b1 0.004 | 8.99 \u00b1 0.017 | 0.888 \u00b1 0.004 | 0.861 \u00b1 0.020 |\\n| w/ MC dropout | 160        | 0.821 \u00b1 0.014 | 8.32 \u00b1 0.021 | 0.814 \u00b1 0.014 | 0.810 \u00b1 0.020 |\\n| w/ LRL        | 160        | 0.840 \u00b1 0.016 | 9.32 \u00b1 0.021 | 0.823 \u00b1 0.016 | 0.796 \u00b1 0.021 |\\n\\nThe models were then compared against a baseline model, which utilized a 3D-UNet with MC dropout and was trained with a limited number of hyperparameters. The results for this baseline model are as follows:\\n\\n| Baseline Model | Scans Used | Dice Score | ASD | Jaccard Score | 95HD |\\n|---------------|------------|------------|-----|--------------|------|\\n| 3D-UNet       | 160        | 0.801 \u00b1 0.019 | 11.08 \u00b1 0.024 | 0.742 \u00b1 0.021 | 0.784 \u00b1 0.022 |\\n\\n### Conclusion\\n\\nThe GBDL (LRL) model achieved the highest Dice Score (0.888) and lowest Jaccard Score (0.578) when compared to the other models. The 3D-UNet model with MC dropout also performed well, achieving a Dice Score of 0.832 and a Jaccard Score of 0.745. Further research is needed to determine the optimal hyperparameter configurations and to improve the performance of atrial segmentation models.\"}"}
{"id": "CVPR-2022-1545", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. For a fair comparison, the $\\\\mu_v$ and $\\\\Lambda_v$ are of the same number of dimensions as the $\\\\mu_i$ and $\\\\Lambda_i$ in the original LRL. The results show that the volume-based LRL performs worse than the original LRL that is presented in Section 3.2, demonstrating that the original LRL can better model the joint distribution. Based on this observation, we hypothesize that keeping a difference among slices is important, since slices and their corresponding segmentation masks are different from each other. The original LRL adds perturbation to each slice, which is beneficial to keeping such a difference. However, the volume-based LRL only adds random noise to the latent representation of the whole volume, which goes against keeping such a difference. Third, considering that the independent components assumption is widely used in cVAE, we also evaluated the LRL variant with this assumption, and we denote this variant as \u201cIC-LRL\u201d. In this case, $L_{KL}[Q(Z|X)||P(Z)]$ degenerates to $1/2 \\\\cdot \\\\sum D [\u2212\\\\log \\\\sum_{n\u22121 \\\\leq i \\\\leq 0} 1/\\\\sigma_i^2 + 1/\\\\sigma_i^2 + (1/\\\\sum_{n\u22121 \\\\leq i \\\\leq 0} 1/\\\\sigma_i^2) \\\\mu_i \\\\sigma_i^2]^2 \u2212 1$, where $\\\\mu_i$ and $\\\\sigma_i$ are the mean vector and the variance vector of the Gaussian distribution for each slice in the input volume, respectively. The results shown in Table 1 indicate that the IC-LRL performs worse than our original LRL, verifying that the independent components assumption harms the performance of LRL.\\n\\nFurthermore, we analyzed the two important parts of the proposed LRL, i.e., the reconstruction loss $L_{MSE}$ and the term $L_{KL}[Q(Z|X)||P(Z)]$. We removed these two loss terms, respectively, to study their impact on GBDL. As Table 2 shows, when $L_{MSE}$ or $L_{KL}[Q(Z|X)||P(Z)]$ is removed, a performance decline can be observed, demonstrating the importance of these two terms and the correctness of our Bayesian formulation.\\n\\nBesides, as the two 3D-UNet encoders in LRL are of the same architecture (see the supplementary material), and both aim at extracting features from input volumes, we evaluated a case where the two encoders share their weights. Nevertheless, sharing the weights degrades the performance, as shown in Table 2. Similarly, although some layers of the two 3D-UNet decoders in the LRL also have the same architecture, sharing their weights degrades the performance.\"}"}
{"id": "CVPR-2022-1545", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"configuration, sharing their weights would clearly lead to a lower performance, as the respective objectives of them are different, leading to different gradient descent directions.\\n\\nFinally, when compared with the baseline model, the proposed GBDL can improve the performance with a huge margin with respect to the four assessment metrics for both datasets. So, the proposed method is a good solution for semi-supervised volumetric medical imaging segmentation.\\n\\n4.4. Comparison with State-of-the-Art Methods\\n\\nThe proposed GBDL is compared with previous state-of-the-art methods, and since previous works provide no standard deviation, we also only report the mean values of the four evaluation metrics in Tables 3, 4, and 5. Since most previous methods are based on VNet, for fair comparison, our regular 3D-UNet with MC dropout is designed to have similar numbers of parameters. Furthermore, we also re-implement previous Bayesian deep learning methods based on the regular 3D-UNet with MC dropout, and according to the results in Tables 3 and 4, their performance are similar to their VNet counterparts, meaning that the comparison between our method and previous methods is relatively fair.\\n\\nBy Tables 3, 4, and 5, the performance of GBDL is higher than previous state-of-the-art results on the three public medical datasets. When the number of labeled data decreases, the performance gap between our GBDL and previous state-of-the-art methods becomes larger, which indicates that our method is more robust when fewer labeled data were used for training. More importantly, GBDL performs better than all previous Bayesian deep-learning-based methods [15, 17, 18, 21], which verifies that GBDL is better than the teacher-student architecture used in them and that the generative learning paradigm is more suitable for pseudo-label generation and solving the semi-supervised segmentation task than the discriminative counterpart.\\n\\nFurthermore, we compare the patch accuracy vs. patch uncertainty (PAvPU) metric [13] for GBDL and previous Bayesian-based methods to evaluate their uncertainty estimation. By Table 6, GBDL can output more reliable and well-calibrated uncertainty estimates than other methods, so that clinicians can do a post-processing and refine the results in practice based on the uncertainty given by GBDL.\\n\\nFinally, since full Bayesian inference is time-consuming, we show the relationship among the number of feedforward passes $T$, the time consumption, and the performance in Figure 2. By Figure 2(a), the time costs rise with increasing $T$, but by Figure 2(b) and (c), the Dice score and the PAvPU metric are not greatly impacted by $T$. Thus, $T = 5$ can be chosen for saving time to get the uncertainty in practice, with only a minor performance degradation.\\n\\n5. Summary and Outlook\\n\\nIn this paper, we rethink the issues in previous Bayesian deep-learning-based methods for semi-supervised volumetric medical image segmentation, and have designed a new generative Bayesian deep learning (GBDL) architecture to solve them. The proposed method outperforms previous state-of-the-art methods on three public medical benchmarks, showing its effectiveness for handling the data with only a very limited number of annotations.\\n\\nHowever, in GBDL, there is still a limitation with regard to the time consumption for full Bayesian inference and uncertainty estimation. By Figure 2(a), several numbers of feedforward passes will lead to a high cost in terms of time. Since Bayesian deep learning is important for some safety-critical areas, such as medical diagnosis and autonomous driving, future research can try to find better methods to improve the inference speed for GBDL. Moreover, since GBDL is a new proposed architecture, future research can also consider to combine other previous deep-learning-based methods (such as different training and learning strategies) with GBDL to further improve the performance.\\n\\nAcknowledgments.\\n\\nThis work was partially supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1, by the AXA Research Fund, by the EPSRC grant EP/R013667/1, and by the EU TAILOR grant. We also acknowledge the use of the EPSRC-funded Tier 2 facility JADE (EP/P020275/1) and GPU computing support by Scan Computers International Ltd.\"}"}
{"id": "CVPR-2022-1545", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Wenjia Bai, Ozan Oktay, Matthew Sinclair, Hideaki Suzuki, Martin Rajchl, Giacomo Tarroni, Ben Glocker, Andrew King, Paul M Matthews, and Daniel Rueckert. Semi-supervised learning for network-based cardiac MR image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 253\u2013260. Springer, 2017.\\n\\n[2] Gerda Bortsova, Florian Dubost, Laurens Hogeweg, Ioannis Katramados, and Marleen de Bruijne. Semi-supervised medical image segmentation via learning consistency under transformations. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 810\u2013818. Springer, 2019.\\n\\n[3] Kang Fang and Wu-Jun Li. DMNet: Difference minimization network for semi-supervised segmentation in medical images. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 532\u2013541. Springer, 2020.\\n\\n[4] Yarin Gal and Zoubin Ghahramani. Dropout as a bayesian approximation: Representing model uncertainty in deep learning. In International Conference on Machine Learning, pages 1050\u20131059. PMLR, 2016.\\n\\n[5] Wenlong Hang, Wei Feng, Shuang Liang, Lequan Yu, Qiong Wang, Kup-Sze Choi, and Jing Qin. Local and global structure-aware entropy regularized mean teacher model for 3D left atrium segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 562\u2013571. Springer, 2020.\\n\\n[6] Huimin Huang, Nan Zhou, Lanfen Lin, Hongjie Hu, Yutaro Iwamoto, Xian-Hua Han, Yen-Wei Chen, and Ruofeng Tong. 3D Graph-S2Net: Shape-aware self-ensembling network for semi-supervised segmentation with bilateral graph convolution. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 416\u2013427. Springer, 2021.\\n\\n[7] Alex Kendall and Yarin Gal. What uncertainties do we need in bayesian deep learning for computer vision? In Advances in Neural Information Processing Systems, 2017.\\n\\n[8] Philipp Kr\u00a8ahenb\u00a8uhl and Vladlen Koltun. Efficient inference in fully connected CRFs with gaussian edge potentials. Volume 24, pages 109\u2013117, 2011.\\n\\n[9] Shuailin Li, Chuyu Zhang, and Xuming He. Shape-aware semi-supervised 3D semantic segmentation for medical images. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 552\u2013561. Springer, 2020.\\n\\n[10] Xiangde Luo, Jieneng Chen, Tao Song, and Guotai Wang. Semi-supervised medical image segmentation through dual-task consistency. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 8801\u20138809, 2021.\\n\\n[11] Xiangde Luo, Wenjun Liao, Jieneng Chen, Tao Song, Yinan Chen, Shichuan Zhang, Nianyong Chen, Guotai Wang, and Shaoting Zhang. Efficient semi-supervised gross target volume of nasopharyngeal carcinoma segmentation via uncertainty rectified pyramid consistency. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 318\u2013329. Springer, 2021.\\n\\n[12] Fausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-Net: Fully convolutional neural networks for volumetric medical image segmentation. In International Conference on 3D Vision (3DV), pages 565\u2013571. IEEE, 2016.\\n\\n[13] Jishnu Mukhoti and Yarin Gal. Evaluating bayesian deep learning methods for semantic segmentation. arXiv:1811.12709, 2018.\\n\\n[14] Suman Sedai, Bhavna Antony, Ravneet Rai, Katie Jones, Hiroshi Ishikawa, Joel Schuman, Wollstein Gadi, and Rahil Garnavi. Uncertainty guided semi-supervised segmentation of retinal layers in oct images. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 282\u2013290. Springer, 2019.\\n\\n[15] Yinghuan Shi, Jian Zhang, Tong Ling, Jiwen Lu, Yefeng Zheng, Qian Yu, Lei Qi, and Yang Gao. Inconsistency-aware uncertainty estimation for semi-supervised medical image segmentation. IEEE Transactions on Medical Imaging, 2021.\\n\\n[16] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. Volume 28, pages 3483\u20133491, 2015.\\n\\n[17] Kaiping Wang, Bo Zhan, Chen Zu, Xi Wu, Jiliu Zhou, Luping Zhou, and Yan Wang. Tripled-uncertainty guided mean teacher model for semi-supervised medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 450\u2013460. Springer, 2021.\\n\\n[18] Yixin Wang, Yao Zhang, Jiang Tian, Cheng Zhong, Zhongchao Shi, Yang Zhang, and Zhiqiang He. Double-uncertainty weighted method for semi-supervised learning. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 542\u2013551. Springer, 2020.\\n\\n[19] Yutong Xie, Jianpeng Zhang, Zhibin Liao, Johan Verjans, Chunhua Shen, and Yong Xia. Pairwise relation learning for semi-supervised gland segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 417\u2013427. Springer, 2020.\\n\\n[20] Zhaohan Xiong, Qing Xia, Zhiqiang Hu, Ning Huang, Cheng Bian, Yefeng Zheng, Sulaiman Vesal, Nishant Ravikumar, Andreas Maier, Xin Yang, et al. A global benchmark of algorithms for segmenting the left atrium from late gadolinium-enhanced cardiac magnetic resonance imaging. Medical Image Analysis, 67:101832, 2020.\\n\\n[21] Lequan Yu, Shujun Wang, Xiaomeng Li, Chi-Wing Fu, and Pheng-Ann Heng. Uncertainty-aware self-ensembling model for semi-supervised 3D left atrium segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 605\u2013613. Springer, 2019.\\n\\n[22] Xiangyun Zeng, Rian Huang, Yuming Zhong, Dong Sun, Chu Han, Di Lin, Dong Ni, and Yi Wang. Reciprocal learning for semi-supervised segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 352\u2013361. Springer, 2021.\"}"}
