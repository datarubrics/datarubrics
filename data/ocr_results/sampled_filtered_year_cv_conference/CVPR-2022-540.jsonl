{"id": "CVPR-2022-540", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Luca Bertinetto, Joao F Henriques, Philip Torr, and Andrea Vedaldi. Meta-learning with differentiable closed-form solvers. In International Conference on Learning Representations, 2018.\\n\\n[2] Xin Dong, Shangyu Chen, and Sinno Pan. Learning to prune deep neural networks via layer-wise optimal brain surgeon. In Advances in Neural Information Processing Systems, pages 4857\u20134867, 2017.\\n\\n[3] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International Conference on Machine Learning, pages 1126\u20131135. PMLR, 2017.\\n\\n[4] Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural networks. arXiv preprint arXiv:1803.03635, 2018.\\n\\n[5] Yunhui Guo, Noel C Codella, Leonid Karlinsky, James V Codella, John R Smith, Kate Saenko, Tajana Rosing, and Rogerio Feris. A broader study of cross-domain few-shot learning. In European Conference on Computer Vision, pages 124\u2013141. Springer, 2020.\\n\\n[6] Song Han, Huizi Mao, and William J Dally. Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding. arXiv preprint arXiv:1510.00149, 2015.\\n\\n[7] Song Han, Jeff Pool, John Tran, and William Dally. Learning both weights and connections for efficient neural network. In Advances in neural information processing systems, pages 1135\u20131143, 2015.\\n\\n[8] Yihui He, X. Zhang, and Jian Sun. Channel pruning for accelerating very deep neural networks. 2017 IEEE International Conference on Computer Vision (ICCV), pages 1398\u20131406, 2017.\\n\\n[9] Timothy M Hospedales, Antreas Antoniou, Paul Micaelli, and Amos J Storkey. Meta-learning in neural networks: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\\n\\n[10] Gregory R. Koch. Siamese neural networks for one-shot image recognition. 2015.\\n\\n[11] Brenden Lake, Ruslan Salakhutdinov, Jason Gross, and Joshua Tenenbaum. One shot learning of simple visual concepts. In Proceedings of the annual meeting of the cognitive science society, volume 33, 2011.\\n\\n[12] Yann LeCun, John S Denker, and Sara A Solla. Optimal brain damage. In Advances in neural information processing systems, pages 598\u2013605, 1990.\\n\\n[13] Kwonjoon Lee, Subhransu Maji, Avinash Ravichandran, and Stefano Soatto. Meta-learning with differentiable convex optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10657\u201310665, 2019.\\n\\n[14] Carl Lemaire, Andrew Achkar, and Pierre-Marc Jodoin. Structured pruning of neural networks with budget-aware regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9108\u20139116, 2019.\\n\\n[15] Hao Li, Asim Kadav, Igor Durdanovic, H. Samet, and H. Graf. Pruning filters for efficient convnets. ArXiv, abs/1608.08710, 2017.\\n\\n[16] Zhuang Liu, Jianguo Li, Zhiqiang Shen, Gao Huang, Shoumeng Yan, and Changshui Zhang. Learning efficient convolutional networks through network slimming. In Proceedings of the IEEE International Conference on Computer Vision, 2017.\\n\\n[17] Zechun Liu, Baoyuan Wu, Wenhan Luo, Xin Yang, Wei Liu, and Kwang-Ting Cheng. Bi-real net: Enhancing the performance of 1-bit cnns with improved representational capability and advanced training algorithm. In Proceedings of the European conference on computer vision (ECCV), pages 722\u2013737, 2018.\\n\\n[18] Jian-Hao Luo, Jianxin Wu, and Weiyao Lin. Thinet: A filter level pruning method for deep neural network compression. 2017 IEEE International Conference on Computer Vision (ICCV), pages 5068\u20135076, 2017.\\n\\n[19] Tsendsuren Munkhdalai and Hong Yu. Meta networks. In International Conference on Machine Learning, pages 2554\u20132563. PMLR, 2017.\\n\\n[20] Alex Nichol, Joshua Achiam, and John Schulman. On first-order meta-learning algorithms. arXiv preprint arXiv:1803.02999, 2018.\\n\\n[21] Haotong Qin, Ruihao Gong, Xianglong Liu, Mingzhu Shen, Ziran Wei, Fengwei Yu, and Jingkuan Song. Forward and backward information retention for accurate binary neural networks. In IEEE CVPR, 2020.\\n\\n[22] Aravind Rajeswaran, Chelsea Finn, Sham M Kakade, and Sergey Levine. Meta-learning with implicit gradients. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, pages 113\u2013124, 2019.\\n\\n[23] Adam Santoro, Sergey Bartunov, Matthew Botvinick, Daan Wierstra, and Timothy Lillicrap. Meta-learning with memory-augmented neural networks. In Proceedings of the 33rd International Conference on International Conference on Machine Learning - Volume 48, ICML'16, page 1842\u20131850. JMLR.org, 2016.\\n\\n[24] Jake Snell, Kevin Swersky, and Richard S. Zemel. Prototypical networks for few-shot learning. CoRR, abs/1703.05175, 2017.\\n\\n[25] Hongduan Tian, Bo Liu, Xiao-Tong Yuan, and Qingshan Liu. Meta-learning with network pruning. In European Conference on Computer Vision, pages 675\u2013700. Springer, 2020.\\n\\n[26] Rishabh Tiwari, Udbhav Bamba, Arnav Chavan, and Deepak Gupta. Chipnet: Budget-aware pruning with heaviside continuous approximations. In International Conference on Learning Representations, 2021.\\n\\n[27] Oriol Vinyals, Charles Blundell, Timothy Lillicrap, koray kavukcuoglu, and Daan Wierstra. Matching networks for one shot learning. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.\"}"}
{"id": "CVPR-2022-540", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Oriol Vinyals, Charles Blundell, Timothy Lillicrap, Daan Wierstra, et al. Matching networks for one shot learning. Advances in neural information processing systems, 29:3630\u20133638, 2016.\\n\\nLilian Weng. Meta-learning: Learning to learn fast. lilianweng.github.io/lil-log, 2018.\\n\\nMitchell Wortsman, Kiana Ehsani, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Learning to learn how to learn: Self-adaptive visual navigation using meta-learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6750\u20136759, 2019.\\n\\nShipeng Yan, Songyang Zhang, and Xuming He. A dual attention network with semantic embedding for few-shot learning. In AAAI, 2019.\\n\\nMichael Zhu and Suyog Gupta. To prune, or not to prune: exploring the efficacy of pruning for model compression. arXiv preprint arXiv:1710.01878, 2017.\\n\\nLuisa Zintgraf, Kyriacos Shiarli, Vitaly Kurin, Katja Hofmann, and Shimon Whiteson. Fast context adaptation via meta-learning. In International Conference on Machine Learning, pages 7693\u20137702. PMLR, 2019.\"}"}
{"id": "CVPR-2022-540", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Dynamic Kernel Selection for Improved Generalization and Memory Efficiency in Meta-learning\\n\\nArnav Chavan\u2217\u2020\u2021, Rishabh Tiwari\u2217\u2020\u2021, Udbhav Bamba\u2020\u2021 and Deepak K. Gupta\u2020\\n\\nTransmute AI Lab (Texmin Hub), Indian Institute of Technology, ISM Dhanbad\\n{arnavchavan04,akchitra99,ubamba98,guptadeepak2806}@gmail.com\\n\\nAbstract\\n\\nGradient based meta-learning methods are prone to overfit on the meta-training set, and this behaviour is more prominent with large and complex networks. Moreover, large networks restrict the application of meta-learning models on low-power edge devices. While choosing smaller networks avoid these issues to a certain extent, it affects the overall generalization leading to reduced performance. Clearly, there is an approximately optimal choice of network architecture that is best suited for every meta-learning problem, however, identifying it beforehand is not straightforward. In this paper, we present META DOCK, a task-specific dynamic kernel selection strategy for designing compressed CNN models that generalize well on unseen tasks in meta-learning. Our method is based on the hypothesis that for a given set of similar tasks, not all kernels of the network are needed by each individual task. Rather, each task uses only a fraction of the kernels, and the selection of the kernels per task can be learnt dynamically as a part of the inner update steps. META DOCK compresses the meta-model as well as the task-specific inner models, thus providing significant reduction in model size for each task, and through constraining the number of active kernels for every task, it implicitly mitigates the issue of meta-overfitting. We show that for the same inference budget, pruned versions of large CNN models obtained using our approach consistently outperform the conventional choices of CNN models. META DOCK couples well with popular meta-learning approaches such as iMAML [22]. The efficacy of our method is validated on CIFAR-fs [1] and mini-ImageNet [28] datasets, and we have observed that our approach can provide improvements in model accuracy of up to 2% on standard meta-learning benchmark, while reducing the model size by more than 75%. Our code is available at https://github.com/transmuteAI/MetaDOCK.\\n\\n* indicates equal contribution. Arnav Chavan is the corresponding author.\\n\\n1. Introduction\\n\\nAn important characteristic desired by any artificial intelligence (AI) system is the ability to solve tasks under several different conditions, and that it can adapt quickly in unseen environments with the help of limited data. While the field of deep learning has progressed significantly with the development of several efficient algorithms, it is well known that the standard learning methods tend to perform well when training is done with the millions of data points and break down when the statistics of the inference data differs from that of the training set. Meta-learning tries to solve this problem by learning-to-learn the model weights over different episodes from a family of tasks with limited data. This learning process helps the models to generalize well and adapt quickly over unseen tasks with the help of few examples. Such setting has many practical advantages in vision and reinforcement learning problems like few-shot image classification [3, 11, 22, 28], navigation [30], domain adaptation [5] to name a few.\\n\\nThe key idea behind these kind of meta-learning approaches is to learn generalized weights which can be modified easily for a newer task. Gradient based approaches tend to face the problem of overfitting to a greater extent than other methods. The two kind of overfitting observed in them are - 1) Inner-task overfitting, which refers to task-specific overfitting of the meta-model, an extensive research has been done on this problem [13, 33] as it commonly seen in all the deep learning approaches and several methods have been proposed to counter this, for example, inner-regularization, dropout, weight decay, learning rate scheduling, etc. 2) Inter-task overfitting or meta-overfitting, that is overfitting of meta-model on seen tasks and failure to generalize well on unseen tasks, the study on this problem is limited, few of the common approaches are adding implicit regularizations [22], choosing larger CNN networks to increase learning capacity [13], using initialization techniques to improve generalisation [25].\\n\\nIn this paper, we propose to improve the generalization of gradient based meta-learning models as well as...\"}"}
{"id": "CVPR-2022-540", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the associated memory efficiency through a pruning-in-\\nlearning strategy. We present Meta-Learning with D\\ndynamic Optimization of CNN Kernels (META DOCK), a gradient-\\nbased dynamic learning scheme to identify the optimal set\\nof kernels for each meta-learning task. META DOCK is\\nbased on the hypothesis that each task needs only a small\\nsubset of kernels from the complete set of kernels that exist\\nin the traditional meta-learning models, and using exces-\\nsive kernels could potentially cause meta-overfitting. This\\nreduction of the number of kernels per task breaks down\\nfurther into two parts: reducing the kernels in the final\\nmeta-model and learning to further optimize the choice at\\nthe task-level during the inner update steps. META DOCK\\nuses the gradient information accumulated during the inner\\nupdate steps to activate/deactivate the kernels in the meta-\\nmodel.\\n\\nThe major contributions of META DOCK are as fol-\\nlows:\\n\u2022 We demonstrate that meta-learning models can be\\nmade to generalize better on unseen tasks through ef-\\nficient pruning of the excessive and undesired parts\\nof the network at the meta-level as well as for each task.\\n\u2022 Our META DOCK strategy dynamically identifies the\\nright set of kernels that are to be retained for each\\ntask, and discards the rest. This helps to avoid overfit-\\nting and improves the reliability of the meta-learning\\nmethods.\\n\u2022 Through pruning the meta-model as well as the task-\\nspecific models, META DOCK reduces the size of\\nthe model significantly. The resultant smaller meta-\\nmodels are better suited for deployment on low-power\\ndevices and improve the inference efficiency of the\\nmodel.\\n\u2022 We demonstrate through successful integration of\\nMETA DOCK with popular meta-learning approach:\\niMAML wherein META DOCK improves the perfor-\\nmance on unseen tasks on benchmark datasets.\\n\\n2. Related Work\\nMost standard machine learning models cannot easily\\nadapt to unseen tasks, and meta-learning attempts to cir-\\ncumvent this issue [9]. Common approaches to solve meta-\\nlearning problems [29] are metric-based learning, model-\\nbased methods and optimization-based methods. In metric-\\nbased learning, the core idea is similar to nearest neighbors\\nalgorithms where an embedding function is used to encode\\nthe input signal to smaller dimensional feature vector which\\nare further used to distinguish one class from another. These\\nmethods makes an assumption that the sample embeddings\\nof same classes should be closer to each other and those\\nSiamese Neural Network [10], matching networks [27], prototypical net-\\nworks [24] are few of the prominent works in this domain.\\nModel-Based [19, 23, 31] methods design models for fast\\nlearning, this is either archived by model's internal design or\\nwith the help of another meta-model. Optimization-Based\\nmethods use a modified back-propagation to handle learn-\\ning by few samples across tasks. Few of the common ap-\\nproaches are MAML [3], iMAML [22] and Reptile [20].\\n\\nThe method proposed in this paper is related to reducing\\nthe inter-task overfitting, also referred to as meta-overfitting\\nin the optimization-based meta-models, with the help of\\ntask-specific kernel selection. We focus our study on the\\noptimization-based methods as these methods are inher-\\nrently designed for smaller models as compared to metric-\\nbased and model-based meta learning methods. We achieve\\nthis by developing a pruning strategy that finds different\\nsubset of kernel for each different task. Network prun-\\ning is one the the prominent ways to remove redundant\\nweights in deep neural network. Some of the common\\nways to prune networks are as follows. 1) Unstructured\\npruning [2, 4, 6, 7, 12, 32], these methods reduce the stor-\\nage requirements, however, no acceleration is currently\\npossible through recent hardware. 2) Structured pruning\\n[8,15,16,18,26], these methods prune the entire channels or\\nlayers of the neural network maintaining the structure of the\\nneural network. The kernel selection pruning used by our\\nmethod is closely related to structured pruning approaches\\nas it maintains the regular structure and selects the kernel to\\napply for each output channel in a convolutional layer. An-\\nother approach that uses pruning to improve generalization\\nin meta-learning is proposed in [25]. They use pruning as\\na method of model weight initialization and show general-\\nization to certain extent but they fail to induce any sparsity\\nor compression and are very compute heavy as they require\\n3\\\\times the training with respect to normal meta-learning with-\\nout any memory/compute gain in inference.\"}"}
{"id": "CVPR-2022-540", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Relative contribution of 32 different $3 \\\\times 3$ kernels in mapping 10 different output channels for (a) no budget constraint, and (b) a budget constraint of 50% on the total number of kernels that are used per mapping.\\n\\nConventionally, meta-learning is posed as a bi-level optimization problem involving updates of the optimization parameters of the model at two different levels: meta update (outer update) and adaptation (inner update). At the meta-update level, parameter set $\\\\theta \\\\in \\\\Theta$ is optimized and this together with task-specific training set are used to obtain $\\\\phi_i$ for any given task $T_i$. The goal of meta-learning is then to optimize the meta-parameters $\\\\theta$ such that for the optimal solution, its adaptation, denoted by $A_d(\\\\cdot)$, for a certain task $T_i$ using $D_{tr}i$, minimizes the test loss of the respective task, $L(\\\\phi_i, D_{test}i)$. Mathematically, it can be stated as\\n\\n$$\\\\theta^*_{ML} := \\\\arg\\\\min_{\\\\theta \\\\in \\\\Theta} F(\\\\theta),$$\\n\\nwhere,\\n\\n$$F(\\\\theta) = \\\\frac{1}{M} \\\\sum_{i=1}^{M} L(A_d(\\\\theta, D_{tr}i), D_{test}i).$$\\n\\nDuring inference, dataset $D_{tr}j$ corresponding to a new task $T_j \\\\sim P(T)$ is used to update $\\\\theta_{ML}$ to obtain the task-specific parameters $\\\\phi_j = A_d(\\\\theta_{ML}, D_{tr}j)$.\\n\\n3.2. Kernel selection in meta-learning\\n\\nAs has been described above, the traditional approach in meta-learning is to learn a meta-model that can be adapted to different tasks. Although the tasks are assumed to be sampled from a common distribution $P(T)$, they vary to some extent. Thus, it is evident that for a single meta-model to fit across all the tasks, it needs to possess sufficient bandwidth of model weights. However, it is of interest to explore whether all the model weights are indeed needed for each of these tasks, and if not, can we adapt this usage in a task-specific manner.\\n\\nFigure 1 shows an example of kernel usage for a few mappings between the kernels and the output channels. For the sake of visual clarity, we show here only first 32 kernels of the total 64 from the last layer of our model. For each output channel, we show the relative contribution of the various kernels. From Figure 1a, it is seen that the contributions of the different input kernels vary across different output channels with some kernels being more important than the others for an output channel. Figure 1b shows the same 32 kernels as above, but for a model pruned with our method to a budget constraint of 50% on the total fraction of the kernels to be used. The resulting sparsity is clearly visible in Figure 1b where we see that for each output channel, a significant set of kernels are no more relevant. We have observed and later report in this paper that even at such sparse configurations, the overall performance of the model is at par with the model trained without any budget constraint. As anticipated, different output channels use different set of input kernels, however, the overall selection of kernels is still sparse. Clearly, there is a scope of using a reduced number of kernels per output channel, and we achieve it in this paper through smart kernel selection, also referred as kernel pruning.\\n\\nKernel selection, when combined with meta-learning, facilitates the identification of optimal set of kernels specific to each task. This leads to compressed meta-models as well as task-specific models that are less vulnerable to overfitting and provide improved memory efficiency. Figure 2 shows an example of kernel selection used for task-specific pruning in meta-learning. We see here the binary correlation map for multiple task pairs, and it is observed that the choice of input kernels for constructing the output channels varies across the tasks. Clearly, not all kernels of a model are relevant for each task, and the unused kernels can be eliminated through our dynamic kernel selection strategy.\"}"}
{"id": "CVPR-2022-540", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We present here META DOCK, a dynamic kernel selection strategy for meta-learning that treats task-specific kernel selection as an integral part of the model optimization process. To start with, META DOCK modifies the optimization problem stated in Eq. 1 as follows.\\n\\n$$\\\\theta^*, z^* := \\\\arg\\\\min_{\\\\theta \\\\in \\\\Theta, z \\\\in Z} F(\\\\theta, z),$$\\n\\n$$F(\\\\theta, z) = \\\\sum_{i=1}^{M} L_A(d(z, \\\\theta; D_{train}), D_{test})$$\\n\\nNote here that $z$ denotes a set of masking parameters. Based on this formulation, it is evident that the pruning masks are learnt at two different levels: meta-level and adaptation phase (inner-level). The inner step facilitates to identify kernels that are to be used for a certain task and the rest are ignored.\\n\\nFor the inner-level learning process, META DOCK employs the adaptation function $A_d(\\\\cdot)$. Inspired from the iMAML algorithm, we construct $A_d(\\\\cdot)$ to be the following regularized problem:\\n\\n$$A_d^*(\\\\theta, z; D_{train}) = \\\\arg\\\\min_{\\\\phi \\\\in \\\\Theta, \\\\zeta \\\\in Z} L(B(\\\\zeta) \\\\odot K(\\\\phi); D_{train}) + \\\\lambda_1 \\\\| \\\\theta - \\\\phi \\\\|_2^2 + \\\\lambda_2 \\\\| \\\\zeta - \\\\zeta \\\\|_2^2 + \\\\lambda_3 \\\\| V(\\\\zeta) - V_0 \\\\|_2^2 + \\\\lambda_4 \\\\| \\\\zeta \\\\|_1$$\\n\\nThe task-specific model parameters and the pruning masks are denoted by $\\\\phi$ and $\\\\zeta$ in the equation above. Further, $K(\\\\phi)$ denotes the set of kernels constructed from the model parameters $\\\\phi$ and $\\\\odot$ denotes the elementwise multiplication operation. Each mask gets multiplied with one kernel in the network such that the total count of masks is equal to the number of kernels of the model. When masking the kernels, the masks $\\\\zeta$ are projected to 0 or 1 using the operator $B(\\\\cdot)$, and this function is defined as\\n\\n$$B(\\\\zeta_j) = \\\\begin{cases} 1, & \\\\text{if } \\\\zeta_i > 0 \\\\\\\\ 0, & \\\\text{otherwise} \\\\end{cases}$$\\n\\nAs seen in Eq. 3, there are four regularization terms in the objective function weighted by terms $\\\\lambda_1, \\\\lambda_2, \\\\lambda_3$ and $\\\\lambda_4$.\\n\\nSimilar to the conventional iMAML strategy, the first term encourages the task-specific model parameters $\\\\phi_i$ to stay close to $\\\\theta$. This regularization helps to obtain a strong prior on $\\\\theta$ such that the learnt meta-model requires only a very few update steps and very limited new training samples to adapt to a new task. Similarly, the second regularization term in Eq. 3 ensures that the task-specific dynamic kernel selection builds up on the kernels identified to be active at the meta-model level. This regularization imposes a strong prior on $z$ and helps to dynamically select the task-specific kernels with only a few update steps. The third regularization terms accounts for the budget constraint to be imposed on the optimization problem. The total budget $V(\\\\zeta)$ for any task is defined as\\n\\n$$V(\\\\zeta) = \\\\text{Sum total of active kernels},$$\\n\\n$$\\\\sum_{j=1}^{M} B(\\\\zeta_j) \\\\cdot \\\\text{Total number of kernels}.$$\\n\\nFor $M$ tasks, we have $M$ such inequality constraints, and rather than incorporating them directly, we add a combined regularization term for the same. The implementation of budget constraint is generic in our implementation, and can be used to impose budget on model volume, channels and FLOPs, among others, as in [14, 26]. The last regularization term induces $\\\\ell_1$-sparsity on $z$. For the masks associated with most dynamic kernels, we have observed that this penalty term helps to keep the value of $z$ close to 0, thereby facilitating easy switching between active and inactive across different tasks.\\n\\nThe optimization problem posed in Eq. 3 is solved through iteratively alternating between the following two update steps:\\n\\n$$\\\\theta \\\\leftarrow \\\\theta - \\\\eta_1 d\\\\theta F(\\\\theta, z; D_{train}),$$\\n\\n$$z \\\\leftarrow z - \\\\eta_2 d\\\\zeta F(\\\\theta, z; D_{train}),$$\\n\\nwhere $d\\\\theta(\\\\cdot)$ and $d\\\\zeta(\\\\cdot)$ denote the first-order gradients with respect to $\\\\theta$ and $z$, respectively. Based on Eq. 3, the update step for the masks can be further expanded as\\n\\n$$z \\\\leftarrow z - \\\\eta_2 \\\\sum_{i=1}^{M} dA_d^*(\\\\theta, z) \\\\cdot \\\\nabla \\\\zeta L(A_d^*(\\\\zeta, \\\\theta)).$$\\n\\nA similar expression can be stated for $\\\\theta$. This two-step update strategy allows to achieve stability of the intertwined optimization problem as well as helps convergence with the gradient descent method.\\n\\nFigure 3 shows the schematic representation of the workflow of META DOCK and additional details on the pipeline of the approach are provided in Algorithm 1. META DOCK starts with initializing the meta-model with weights from a pretrained model, thus $\\\\theta = \\\\theta_0$. With every kernel of the initial model, we then associate a pruning mask, and the values of the mask are sampled from a uniform distribution (to keep all the masks active initially). The goal of META DOCK is then to simultaneously optimize the model weights as well as the associated masks. At every meta-step, a set of tasks are sampled from the pre-defined distribution of tasks, and inner updates are performed independently for each task to obtain $\\\\phi_i$ from $\\\\theta$. Based on the average of the error gradient across the whole batch of tasks, an update to the meta-model is performed. This process is repeated until\"}"}
{"id": "CVPR-2022-540", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the desired level of convergence is achieved. Details related to each function outlined in Algorithm 1 can be found in the supplementary material.\\n\\nFor better convergence, META DOCK employs a two-step model update strategy as stated in Eqs. 6 and 7. In the first step, model is updated with the gradients of the error with respect to the model parameters while keeping the values of \\\\( z \\\\) thresholded to 0 or 1 using \\\\( B(\\\\cdot) \\\\). Note that due to \\\\( B(\\\\cdot) \\\\), gradients cannot be computed for \\\\( z \\\\). Thus, during the second step, model is updated using pseudo-gradients, i.e., the backward propagation ignores the projection function and the gradient is approximated with a sigmoid function. This approach is very commonly employed in the pruning and binarization literature (e.g. [17, 21]), and we have found it to work well in the META DOCK framework as weights are directly optimized over discrete masks eliminating the need of separate retraining stage completely.\\n\\n4. Experiment\\n\\nWe discuss here various experiments conducted in this paper to demonstrate the efficacy of META DOCK. First we conduct an experiment to analyze how our two-step pruning strategy performs compared to the classical continuous pruning for meta-learning. In another set of experiments, we perform a comparative study between task-specific pruning as well as global pruning. It is of interest to analyze whether the few inner update steps are sufficient to perform task-specific pruning, and if yes, we study whether the resultant models perform well or not. We propose a metric to measure the extent of overfit in meta-learning and conduct experiments to analyze whether the models obtained from META DOCK generalize better than their unpruned counterparts.\\n\\nWe study the performance of META DOCK on standard 4-conv models trained on mini-ImageNet [28] and CIFAR-fs [1] datasets for several different choices of pruning budget. For the meta-validation tasks, we sample 600 tasks randomly from the meta-validation split. In contrast to the existing methods, we take all possible combinations of classes from meta-test split to generate a large number of meta-test tasks. For \\\\( C \\\\) classes in the complete meta-test set, the total number of meta-test tasks we generate for N-way setting is \\\\( C^{C_N} \\\\). This translates to 15504 test tasks for both CIFAR-fs and mini-ImageNet (\\\\( C = 20 \\\\), \\\\( N = 5 \\\\)).\\n\\nWe evaluate on meta-validation tasks after every 1000 meta-training steps and save the model based on the best average meta-validation accuracy. Finally, we evaluate this saved model on all generated meta-test tasks. This strategy is much more robust than reporting meta-validation accuracy alone which often overfits due to repeated evaluation.\"}"}
{"id": "CVPR-2022-540", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Performance scores on CIFAR-fs dataset, 5-way 5-shot setting for continuous pruning and discrete two-step (M \\\\( \\\\text{ETA} \\\\) DOCK) pruning at different budgets.\\n\\n| Budget Method | Val Acc (%) | Test Acc (%) |\\n|---------------|-------------|--------------|\\n| 25.0 Continuous | 67.49 \u00b1 1.00 | 67.78 \u00b1 0.19 |\\n| 25.2 M \\\\( \\\\text{ETA} \\\\) DOCK | 69.65 \u00b1 0.94 | 69.30 \u00b1 0.19 |\\n| 12.5 Continuous | 64.40 \u00b1 1.01 | 64.87 \u00b1 0.20 |\\n| 13.0 M \\\\( \\\\text{ETA} \\\\) DOCK | 70.69 \u00b1 0.97 | 70.15 \u00b1 0.19 |\\n\\nTable 2. Performance of CIFAR-fs, 5-way 5-shot setting for global and task-specific pruning at different budgets.\\n\\n| Budget Method | Val Acc (%) | Test Acc (%) |\\n|---------------|-------------|--------------|\\n| 13.9 Global | 69.95 \u00b1 0.98 | 69.98 \u00b1 0.19 |\\n| 13.0 Task-Specific | 70.69 \u00b1 0.97 | 70.15 \u00b1 0.19 |\\n| 7.8 Global | 69.96 \u00b1 0.98 | 69.92 \u00b1 0.19 |\\n| 7.2 Task-Specific | 70.49 \u00b1 0.99 | 70.21 \u00b1 0.19 |\\n\\n4.1. Optimal pruning strategy for meta-learning\\n\\nContinuous vs. two-step model update scheme. Commonly, most pruning methods employ continuous pruning scheme. This involves learning soft masks as part of the model training process. Rather than using discrete values of 0 and 1 for the masks, continuous schemes optimize them for \\\\([0, 1]\\\\), and add additional penalty on the objective function to push final masks to 0 (inactive kernel) and 1 (active kernel). As described earlier, M \\\\( \\\\text{ETA} \\\\) DOCK employs a two-step discrete pruning scheme and in this experiment, we compare its performance with the classical continuous pruning method.\\n\\nFor the continuous scheme, we follow the approach similar to \\\\[16\\\\], and introduce kernel masks to induce sparsity in the pre-trained meta model. The target objective is to rank the kernels according to their influence on the final performance with the help of induced mask values. This model is jointly trained with regular meta-learning loss functions combined with \\\\( \\\\ell_1 \\\\)-norm on masks with the same training hyperparameters as used in pre-training stage. Once pruning is completed, depending upon target budget, kernels with low mask values are eliminated and compressed meta-model structure is extracted. Finally, this compressed meta-model is retrained with similar strategy used for pre-training so that the weights can adjust themselves from continuous masks in the pruning stage to binary/non-existing masks in the final structure. An important point to note is that models pruned with this strategy has a reduced size but it is task-independent, i.e., the same meta-model is used in all tasks. In contrast to our two step discrete pruning strategy where weight optimization on binary masks occur in a single stage, continuous pruning requires an additional finetuning stage for the compressed meta-model to adapt model weights from continuous masks in pruning stage to discrete/non-existing masks in final model thus increasing total pruning time.\\n\\nThe results for continuous scheme as well as our approach on CIFAR-fs, 5-way, 5-shot are shown in Table 1. It can be clearly seen that our pruning method outperforms the continuous pruning baseline at both 25% and 12.5% budgets. At extreme budget (12.5%), our approach improves the performance significantly over the baseline. This clearly indicates that the two-step strategy with discrete projection employed in M \\\\( \\\\text{ETA} \\\\) DOCK is more stable and a better approach for pruning in meta-learning.\\n\\nGlobal pruning vs. task-specific pruning. We further analyze if our task-specific pruning adds value to the overall performance and efficiency of the model. For this, we compare our approach with global pruning, where only pruning is performed at the meta-model level. Task specific pruning allows each task to adapt the model weights as well as mask values, thereby changing the architectures accordingly. However, global pruning freezes the compressed meta-model and only allows weights to adapt task-wise.\\n\\nThe results for global and task-specific pruning on CIFAR-FS, 5-way 5-shot are shown in Table 2. Task-specific pruning outperforms global pruning at both 12.5% and 6.25% budgets while achieving a greater degree of compression at the same time. This asserts the fact that based on the difficulty of the task, model should be able to compress or expand itself in a given range. This further motivates us to adopt task-specific pruning in all our further experiments.\\n\\n4.2. Learning task-specific compressed meta-models\\n\\nWe discuss here the results of task-specific pruning obtained using M \\\\( \\\\text{ETA} \\\\) DOCK on standard 4-Conv models with 64 and 128 channels for CIFAR-fs 5-way 1-shot and 5-way 5-shot settings. Related results are presented in Tables 3, 4, 5 and 6. Here, meta-budget indicates the fraction of kernels retained in the meta-model, and task-budget refers to the fraction remaining in the task-specific models. In general, we see that the performance of the pruned meta-models is...\"}"}
{"id": "CVPR-2022-540", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 3. Performance of 4-Conv model with 128 channels on CIFAR-fs with 5-way 5-Shot setting at different budgets.\\n\\n| Budget (%) | Meta Task | Validation Test | Test Set |\\n|------------|-----------|-----------------|----------|\\n| iMAML [22] | 68.97 \u00b1 0.94 | 68.08 \u00b1 0.19 | 15.5 |\\n| 69.45 \u00b1 0.96 | 68.62 \u00b1 0.19 | 18.2 |\\n| 69.65 \u00b1 0.94 | 69.30 \u00b1 0.19 | 14.1 |\\n\\n### Table 4. Performance of 4-Conv model with 128 channels on CIFAR-fs with 5-way 1-Shot setting at different budgets.\\n\\n| Budget (%) | Meta Task | Validation Test | Test Set |\\n|------------|-----------|-----------------|----------|\\n| iMAML [22] | 56.50 \u00b1 1.98 | 55.23 \u00b1 0.38 | 35.4 |\\n| 58.27 \u00b1 1.91 | 55.46 \u00b1 0.38 | 30.7 |\\n| 58.40 \u00b1 1.89 | 56.04 \u00b1 0.38 | 24.4 |\\n\\n### Table 5. Performance of 4-Conv model with 64 channels on CIFAR-fs with 5-way 5-Shot setting at different budgets.\\n\\n| Budget (%) | Meta Task | Validation Test | Test Set |\\n|------------|-----------|-----------------|----------|\\n| iMAML [22] | 67.90 \u00b1 0.98 | 67.25 \u00b1 0.20 | 6.0 |\\n| 69.51 \u00b1 0.94 | 69.17 \u00b1 0.19 | 8.6 |\\n| 69.91 \u00b1 0.96 | 69.75 \u00b1 0.19 | 4.8 |\\n\\n### Table 6. Performance of 4-Conv model with 64 channels on CIFAR-fs with 5-way 1-Shot setting at different budgets.\\n\\n| Budget (%) | Meta Task | Validation Test | Test Set |\\n|------------|-----------|-----------------|----------|\\n| iMAML [22] | 55.97 \u00b1 1.95 | 54.10 \u00b1 0.38 | 24.7 |\\n| 56.30 \u00b1 2.02 | 54.55 \u00b1 0.38 | 22.3 |\\n| 57.33 \u00b1 1.91 | 55.28 \u00b1 0.38 | 14.7 |\\n\\nHigher than their unpruned counterparts and this gain is observed to be the highest for 5-way, 5-shot setting with 128 channels (Table 3). For this case, pruning greatly improves over the validation and test sets and the best performance is achieved at \u223c7% budget increasing the performance over the base model by \u223c2%. Similarly in 5-way, 1-shot setting with 128 channels (Table 4) pruning at 25% budget improves the base performance by \u223c1%.\\n\\nTo analyze the generalizability of META-DOCK across other datasets, we also report results for mini-ImageNet in Tables 7, 8, 9 and 10. We observe that for this dataset as well, the pruned models consistently outperform the base-line model, except for very low budgets of less than 10%, where minor drops in performance are observed. Similar to CIFAR-fs, performance gains of up to 2% in accuracy are observed for this dataset as well. Clearly, with META-DOCK, it is possible to learn compressed models that perform better than the baseline models in meta-learning.\\n\\nAn interesting observation across both the datasets is that performing task-specific pruning of a larger model is more beneficial than just using an unpruned model of the same size. Intuitively, this seems right since the pruned model will have a more optimized distribution of kernels compared to the unpruned one. For example, we observe for both the datasets, that the 4-conv model when trained with 128 channels and pruned for 25% budget, attains higher accuracy than the pretrained 4-conv model comprising 64 channels - both these models have the same size. The former attains an average meta-test accuracy score of 70.15% on CIFAR-fs 5-way 1-shot (Table 3), while the latter scores 67.25%, thus increasing the performance by an absolute margin of 2.9%. Similarly, the corresponding increase in accuracy for mini-ImageNet dataset is \u223c2.2% (Table 7 and 9). Similar trends can also be observed for other model pairs as well across datasets and different model settings. This suggests that pruning a large network to get task-specific models is better than using a pretrained model of similar size.\"}"}
{"id": "CVPR-2022-540", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7. Performance of 4-Conv model with 128 channels on mini-ImageNet with 5-way 5-Shot setting at different budgets.\\n\\n| Budget (%) | Accuracy (%) | \u2191 | MO | \u2193 |\\n|------------|--------------|---|----|---|\\n|            | iMAML [22]   |   |     |   |\\n| 61.55 \u00b1 0.91 | 63.39 \u00b1 0.18 | 17.6 | 57.4 | 50.3 |\\n| 63.19 \u00b1 0.92 | 63.54 \u00b1 0.18 | 18.8 | 32.2 | 25.3 |\\n| 63.65 \u00b1 0.92 | 64.34 \u00b1 0.18 | 13.1 | 15.9 | 12.8 |\\n| 63.62 \u00b1 0.94 | 64.05 \u00b1 0.18 | 11.4 | 8.2 | 6.8 |\\n\\nTable 8. Performance of 4-Conv model with 128 channels on mini-ImageNet with 5-way 1-Shot setting at different budgets.\\n\\n| Budget (%) | Accuracy (%) | \u2191 | MO | \u2193 |\\n|------------|--------------|---|----|---|\\n|            | iMAML [22]   |   |     |   |\\n| 46.90 \u00b1 1.77 | 45.55 \u00b1 0.36 | 21.3 | 57.8 | 50.5 |\\n| 47.07 \u00b1 1.84 | 45.72 \u00b1 0.36 | 23.4 | 28.6 | 26.0 |\\n| 47.17 \u00b1 1.89 | 45.90 \u00b1 0.36 | 20.2 | 12.9 | 12.8 |\\n| 47.10 \u00b1 1.93 | 45.97 \u00b1 0.36 | 17.3 | 7.1 | 7.0 |\\n\\nTable 9. Performance of 4-Conv model with 64 channels on mini-ImageNet with 5-way 5-Shot setting at different budgets.\\n\\n| Budget (%) | Accuracy (%) | \u2191 | MO | \u2193 |\\n|------------|--------------|---|----|---|\\n|            | iMAML [22]   |   |     |   |\\n| 60.58 \u00b1 0.94 | 62.13 \u00b1 0.18 | 12.4 | 56.2 | 50.2 |\\n| 63.30 \u00b1 0.93 | 63.5 \u00b1 0.18  | 13.2 | 30.0 | 25.2 |\\n| 63.29 \u00b1 0.93 | 63.4 \u00b1 0.19  | 11.2 | 15.1 | 12.8 |\\n| 62.90 \u00b1 0.96 | 62.72 \u00b1 0.18 | 9.6  | 8.2  | 6.4 |\\n\\nTable 10. Performance of 4-Conv model with 64 channels on mini-ImageNet with 5-way 1-Shot setting at different budgets.\\n\\n| Budget (%) | Accuracy (%) | \u2191 | MO | \u2193 |\\n|------------|--------------|---|----|---|\\n|            | iMAML [22]   |   |     |   |\\n| 45.20 \u00b1 1.88 | 44.58 \u00b1 0.36 | 23.1 | 60.2 | 50.4 |\\n| 46.20 \u00b1 1.92 | 44.88 \u00b1 0.36 | 21.3 | 26.6 | 25.5 |\\n| 45.72 \u00b1 1.93 | 44.92 \u00b1 0.36 | 20.5 | 13.0 | 12.7 |\\n| 45.27 \u00b1 1.91 | 44.87 \u00b1 0.36 | 14.8 | 6.8  | 6.6 |\\n\\nWe further study the compute and storage efficiency of our pruned models by analyzing the obtained accuracy scores versus the number of parameters and FLOPs at different pruning budgets. Figure 4 shows parameters vs. accuracy plot for CIFAR-fs and mini-ImageNet datasets on 5-way, 5-shot setting with 4-Conv-128 model. We compare M ETA DOCK with iMAML and it can be seen that M ETA DOCK consistently outperforms iMAML while substantially reducing the number of parameters. After a threshold parameter count, performance starts to drop for both CIFAR-fs and mini-ImageNet but the extent of compression achieved before that point is already significant.\\n\\nSimilarly, Figure 5 shows FLOPs vs. accuracy plot for CIFAR-fs and mini-ImageNet dataset on 5-way, 5-shot setting with 4-Conv-128 model. For mini-ImageNet, more than $3 \\\\times$ reduction in FLOPs is achieved while improving the accuracy by 1%. Similarly, for CIFAR-fs, more than $5 \\\\times$ reduction in FLOPs is achieved while improving the accuracy by more than 2%. As seen in parameters vs. accuracy plot, performance here as well tends to drop beyond a certain FLOPs' threshold.\\n\\nPruning improves generalization. To measure the extent of generalization in meta-learning, we introduce a metric, Measure of Meta-Overfitting (MO), to compare generalization of different models on unseen tasks. We define it as:\\n\\n\\\\[\\nMO = \\\\frac{Acc_{\\\\text{train}} - Acc_{\\\\text{test}}}{Acc_{\\\\text{test}}} \\\\times 100\\n\\\\]\\n\\nwhere Acc_{\\\\text{train}} refers to the average accuracy on the test set of 600 randomly sampled tasks that are seen by the model during meta-training stage and Acc_{\\\\text{test}} refers to the average accuracy on the test set of 600 randomly sampled unseen tasks. A low value of this metric indicates that Acc_{\\\\text{train}} is close to Acc_{\\\\text{test}} and/or Acc_{\\\\text{test}} is fairly high which implies better generalization. Note that MO alone cannot be used to judge a model's performance but it gives a fair measure of meta-overfitting.\\n\\nTo analyze the measure of overfitting for M ETA DOCK, we look at the MO scores reported in the tables above. It is observed that the task-specific pruned models show lower MO score and this score reduces with lower budget constraints. These results clearly demonstrate that M ETA DOCK helps to improve generalization in meta-learning.\\n\\n5. Conclusion and Future Work\\n\\nIn this paper, we have presented M ETA DOCK, a dynamic kernel pruning strategy to improve generalization and build memory efficient models in meta-learning. M ETA DOCK achieves model compression at two levels - global/meta as well as task-specific. With its efficient adaptation capability, M ETA DOCK can create compressed models that generalize better on unseen tasks. Through several numerical experiments, we have demonstrated the efficacy of M ETA DOCK. For our benchmark experiments, we demonstrated that M ETA DOCK builds pruned models whose accuracy scores are up to 2% higher than the unpruned networks, while reducing the model size by more than 75%. Further, we showed that for the same inference budget, pruned variants of larger models obtained from M ETA DOCK outperform the unpruned smaller networks substantially. With these results, we hope to have addressed an important research question of how metalearning models can generalize better on unseen tasks while achieving better memory efficiency. As part of our future work, we hope to integrate our method with other meta-learning strategies not limited to optimization-based method.\"}"}
