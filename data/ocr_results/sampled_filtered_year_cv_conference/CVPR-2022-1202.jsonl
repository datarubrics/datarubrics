{"id": "CVPR-2022-1202", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nHumans can perceive multiple expressions, each one with varying intensity, in the picture of a face. We propose a methodology for collecting and modeling multidimensional modulated expression annotations from human annotators. Our data reveals that the perception of some expressions can be quite different across observers; thus, our model is designed to represent ambiguity alongside intensity. An empirical exploration of how many dimensions are necessary to capture the perception of facial expression suggests six principal expression dimensions are sufficient. Using our method, we collected multidimensional modulated expression annotations for 1,000 images culled from the popular ExpW in-the-wild dataset. As a proof of principle of our improved measurement technique, we used these annotations to benchmark four public domain algorithms for automated facial expression prediction.\\n\\n1. Introduction\\n\\nHumans communicate using their body. Automating the perception of bodily and vocal expressions is necessary towards building machines that can interact gracefully with humans [8, 59]. Facial expression is an important channel of the communication [9, 16], and perception of facial expressions is important for social interaction [16, 29]. Computer vision researchers have long been interested in measuring human facial expressions from images and video [4, 12, 41, 47, 56] with the aim of replicating it in machines [4, 41].\\n\\nAutomated facial analysis is rooted in machine learning, thus model training and benchmarking rely on large well-annotated datasets. This raises three questions which are not well addressed in the facial expression perception literature: First, how should images be annotated, i.e. what is a good representation of human perception of facial expressions? Second, can we measure the perception reproducibly? Third, what is the best framework for evaluating the automated algorithms? We present an answer to the first question above, and the second and third will be addressed in future work.\\n\\nFigure 1. Face expression perception is multidimensional, nuanced and subjective. Perceived expression is measured by asking 9 crowdsourced annotators to report perceived intensity for each of six dimensions (15 and 21 dimensions in other experiments as described in Sec. 4.1). Annotation histograms and Beta distribution fits (Sec. 3.2) are shown. One dimension, 'happy', captures the expression of the first face. The second requires two: 'happy' and 'surprised'. The third requires more dimensions and is more ambiguous.\\n\\nFigure 2. Expression predictions compared to ground truth. The outputs of 4 expression prediction algorithms (colored symbols) are compared to the ground truth obtained from our annotations for one image. Gray bands: confidence intervals of our ground truth, \u00b5 (See Sec. 3.2 and 4.3).\"}"}
{"id": "CVPR-2022-1202", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Evidence for intensity and compound expressions found in the arts. (Top) Expressions vary in intensity. (Bottom) Any pair of primary expressions may be combined to obtain a valid composite expression (Table 1, Sec. 2.1). (Adapted with permission from Scott McCloud\u2019s *Making Comics* [42], pages 84-85).\\n\\nand, yet, efficiently? Third, what is the right metric to compare an algorithm\u2019s output with human ground truth?\\n\\nTo answer the first question we rely on insight from experimental psychology [15, 25, 31, 45], which suggests that facial expressions are multidimensional and are perceived with different degrees of intensity (see Fig. 1). This contrasts with the common practice in computer vision where expressions are often annotated as a binary and/or a one-hot code (Sec. 2). To answer the second question, we focus on crowdsourcing \u2013 we annotated 1,000 ethnically diverse in-the-wild faces from a public dataset [60], where each was rated by 9 annotators for each of the 6 primary Ekman expressions [17], the corresponding 15 compound expressions [42, 49] and the combined set of 21 expressions. On this dataset we explore the consistency of annotations, as well as the number of dimensions that need to be annotated. Lastly, we propose a metric that compares human annotations, including their ambiguity, with algorithm prediction, and use our annotated dataset to benchmark four recent algorithms (Sec. 4.3).\\n\\nOur main contributions are:\\n\\n1. An efficient method for collecting and modeling reproducible, multi-dimensional, modulated facial expression annotations crowdsourced from humans. The novel modeling technique transforms annotations into probability distributions to express measures of expression intensity and ambiguity.\\n\\n2. A benchmark for facial expression prediction algorithms consisting of annotations on 1,000 face images from a public in-the-wild dataset, and a metric for algorithmic accuracy of expression prediction.\\n\\n2. Related work\\n\\n2.1. Expression and perception\\n\\nThe mechanics and repertoire of facial expression are fairly well understood. Ekman [15] postulated six primary dimensions (happy, angry, surprised, sad, fearful, disgusted), independent of culture and experience, and described the muscle actions that produce such primary expressions [17]. This point of view has, by and large, stood the test of time [31]. Artists have empirically observed that some expressions involve simultaneously more than one of the six primary dimensions [42]. Recent research has explored that intuition and characterized the corresponding facial actions [13].\\n\\nThe perception of facial expressions is also well studied, alongside the perception of other socially relevant attributes such as gender, age, and trustworthiness. Human annotators can make fast judgments [54], which may be used in important decisions [50]. Often, there is good agreement amongst annotators on their perception, although some annotators are believed to be more perceptive and reliable [25]. Whether and when the annotators\u2019 judgments correspond to meaningful and useful information is still debated [30]. Notably, Barrett et al. [2] questioned whether a person\u2019s internal emotional state may be inferred from facial expression, an assumption central to Ekman\u2019s earlier views on emotion and facial expression. Our method and data will help provide empirical evidence for the ongoing Barrett-Ekman debate.\\n\\n2.2. Automating prediction of expression perception\\n\\nComputer vision algorithms may be used to measure facial expressions and predict the voluntary report that a regular person would make of their perception of the expression upon looking at a face image. We use the shorthand \u201cprediction of expression\u201d and \u201cexpression classification\u201d, and the meaning should be clear by context.\\n\\nDetecting and analyzing human faces was recognized as an important task from the inception of computer vision [28]. Since the early 1990s, computer vision researchers have been interested in automating the perception of facial expressions [4, 12, 41, 47, 56] and deep learning is...\"}"}
{"id": "CVPR-2022-1202", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We trained three regressors to predict compound expressions from those of primary ones. The first regressor (full model) utilizes all 6 primary expressions as features to predict each compound expression. The second (compound hyp.) uses 2 primary expressions to predict each compound expression, where the two expressions are taken from the compound expressions hypothesis (Table 1). The third model (best 2) uses the two primary expressions that best fit each compound expression. The \u00b5 of the fitted beta distributions are also plotted (see Sec. 3.2). The gray confidence intervals are computed from the beta distribution and represent 66% confidence. Goodness-of-fit measurements comparing the different regressors are discussed in Sec. 4.4.\\n\\nThe prevalent approach of modern algorithms [1, 19, 34, 52, 55]. We make no direct contribution to algorithms. Rather, we focus on defining a minimal and sufficient representation for expression perception, methods for dataset annotation and methods for benchmarking algorithms.\\n\\n2.3. Data annotation of perceived expression\\n\\nDatasets for training and testing automatic models are annotated by human observers who classify the expressions they perceive. There are two popular approaches. The first measures facial actions [4, 17], i.e. the contraction of specific muscles, a demanding task even for experts. Once the facial actions are measured, one may infer the underlying expression. The second, which we adopt, aims at making use of the perception of regular people and thus focuses directly at naming facial expressions. The perception of many face attributes is empirically consistent across human annotators [24, 31], which justifies considering it as an intrinsic property of the image. However, a number of questions are left open in the literature.\\n\\nFirst, crowd-sourcing, e.g. on Amazon Mechanical Turk (AMT), has been adapted in the domain, but the consistency of reports in such setting, has never been studied to the best of our knowledge. Goodfellow [21] discussed observations on human performance for expression classification but not for crowdsourced settings and not through formal experiments. Recent work [35] compared annotations from different annotators with the goal of consolidating scores, yet did not quantify the reliability of the annotation mechanism. Our work directly analyzes the reproducibility of crowd-sourced annotations, and provides practical recommendations on how many expression dimensions and how many annotators are needed.\\n\\nSecond, it is not known whether all images are equally consistently interpreted, and whether some are ambiguous.\\n\\nTable 2. Synopsis of face image collections annotated for expression. (*) Primary and compound expressions. (\u2021) EmotioNet annotated facial action units to generate expression categories. (+) ExpW faces are sourced from both movies and in-the-wild images. (\u2020) RAF-ML asked each annotator to select one expression out of all categories, the rating is binary per category. (\u2020\u2020) RAF-ML obtained modulated labels by combining binary annotations from a large number of annotators. (Sec. 2.3)\\n\\nWe address this question and find that, indeed, some face images are ambiguous, which motivates representing the ground truth as an interval, or a probability distribution, rather than a single label or value (Sec. 4.2).\"}"}
{"id": "CVPR-2022-1202", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Third, while it is well established in the psychology literature that facial expressions differ in intensity and multiple primary expressions (or compound expressions) can coexist [13, 42, 49], none of the existing datasets that one may use for testing facial expression algorithms support both multi-class encoding and modulated intensity of each expression class (see Table 2). We propose here a method for collecting annotations of multidimensional modulated expressions for each face image.\\n\\nThe method used to generate the RAF-ML dataset [35] produced multi-dimensional modulated annotations and is most similar to ours; however, they asked annotators to pick only one expression per face, while we allow annotators to pick an intensity rating for each expression per face. Furthermore, their method required 40 annotators per face while in our case we find that 5-6 annotators are needed (Sec 4.2), thus, our method is less expensive and more likely to capture subtle expression variations. Blank et al. [5] also introduced a novel method to estimate multi-dimensional intensity ratings from discrete one-hot annotations using co-occurrence matrices. However, it has not yet been shown that such approaches are an equivalent or sufficient representation of how a person would perceive multiple expressions in a face.\\n\\nFourth, while it has been suggested that compound expressions may be thought of as combinations of the six primary expressions [13, 42] (see Table 1), it has not yet been verified whether the perception of crowdsourced annotators satisfies this hypothesis, and thus whether one may simply annotate the six primary expressions, or whether both primary and compound (15 pairwise compounded expressions) need to be annotated (for a total of 21 expressions). This question is explored in Sec. 4.4.\\n\\n2.4. Benchmarking perceived expression\\n\\nMost previous methods for benchmarking facial expression algorithms are based on comparing the output of the algorithm to a one-hot encoding [1, 6, 19, 52, 55]. Recently, RAF-ML [35] proposed the direct usage of several common metrics for evaluating multi-dimensional measurements onto expression prediction. Along this direction, we further propose a new metric where not only modulated algorithmic predictions are compared with modulated ground truth annotations via a distance metric, but also the ground truth ambiguity is represented as a probability distribution and the algorithm's predictions are matched to such distribution using a cross-entropy metric.\\n\\n3. Methods\\n\\n3.1. Multidimensional modulated annotation\\n\\nPeople may perceive one or more primary expressions at varying intensities when viewing a face. To adequately capture this phenomenon, we designed three graphic interfaces where the annotator may report how much or how little they perceive each facial expression on a 5-point scale (Fig. 5). Throughout our annotation collection, we adapted three sets of multi-dimensional expression options in our interface: (1) 6 primary expressions ('happy','angry','surprised','sad','fearful','disgusted'), (2) 15 compound expressions ('cruel', 'outraged', 'dreadful', 'contemptuous', 'betrayed', 'amazed', 'desperate', 'miserable', 'hopeful', 'spooked', 'disbelieving', 'disappointed', 'horrified', 'devastated', and 'remorseful'), and (3) the set of 21 primary and compound expressions.\\n\\nWe took an online crowdsourcing approach to collect the human annotations because it is easily accessible, scalable, and cost-efficient. The custom interface was developed using Amazon SageMaker Ground Truth, which allows one to easily crowdsource the task. The experimental details on curating the dataset are described in Sec. 4.1. To measure the reliability and efficiency of the online crowdsourcing approach, we then analyze the annotator efficiency and rating consistency in Sec. 4.2. A benchmark analysis on four public-domain algorithms is presented in Sec. 4.3, and an exploration of modeling compound expressions is introduced in Sec. 4.4.\\n\\n3.2. Modeling annotations with Beta distributions\\n\\nAfter multidimensional modulated annotations have been collected, they can be used to model the variability in human perception of a given expression. From the annotation interface (Fig. 5), we map the modulated intensity choices \u201cNot at all\u201d, \u201cSomewhat\u201d, ..., \u201cExtremely\u201d to numerical values $0.1, 0.3, 0.5, 0.7, 0.9$. We denote the modulated annotator intensity rating with $r(i,d,l) \\\\in \\\\{0.1, 0.3, 0.5, 0.7, 0.9\\\\}$, where $i \\\\in [1, 2, ..., N]$ indicates the image index ($N = 1,000$), $d \\\\in [1, 2, ..., D]$ is the expression dimension ($D = 6, 15$ or 21), and $l \\\\in [1, 2, ..., L]$ indicates the human label annotator index (typically $L = 9$). In order to model expression perception intensity (magnitude) and spread (uncertainty) of the annotators' perceptions, we fit a Beta distribution [26] (Sec. 4.1) for each set of annotators' ratings $r(i,d) = \\\\{r(i,d,1), r(i,d,2), ..., r(i,d,L)\\\\}$.\"}"}
{"id": "CVPR-2022-1202", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for each expression dimension $d$ of each face image $i$. We fit the ratings to the beta distribution $g(r | \\\\alpha(i,d), \\\\beta(i,d))$ using maximum likelihood estimation (MLE) \\\\cite{46} obtaining parameters $\\\\alpha(i,d)$ and $\\\\beta(i,d)$. A small uniform regularization noise sampled from $[-0.1, 0.1]$ is added to each rating before computing the MLE. Omitting the subscripts for simplicity, the two parameters $\\\\alpha$ and $\\\\beta$ control the shape of the curve, i.e. the position of the mode (or anti-mode) and the dispersion (or variance). The mean, $\\\\mu$, of the distribution is defined as $\\\\mu = \\\\frac{\\\\alpha}{\\\\alpha + \\\\beta}$. Confidence intervals visualize the range of 68.3% (median $\\\\pm \\\\sigma$) of the distribution.\\n\\nThe ratings we collect from annotators are discrete values while the Beta distribution is continuous. Therefore, we approximate the definite integral of $g(r | \\\\alpha, \\\\beta)$ following the principle of the trapezoidal rule \\\\cite{57} with the resolution of the partition being the same as the five-level rating. This approximation yields a discrete version of the Beta function which we denote as $\\\\bar{g}(r | \\\\alpha, \\\\beta)$ and use for calculation.\\n\\n3.3. Annotation ambiguity and cross entropy\\n\\nAnnotators, based on their perception, may disagree on the intensity of each facial expression for the same face. We adapt a formulation of entropy to measure such intensity ambiguity amongst annotators. For each face image $i$ per expression dimension $d$, we take a set of $L$ ratings $r(i,d) = \\\\{r(i,d,1), r(i,d,2), ..., r(i,d,L)\\\\}$, and calculate the entropy $S$ as\\n\\n$$S(r(i,d)) = \\\\sum_{r=0.1}^{0.9} f(r, r(i,d)) \\\\ast \\\\log(f(r, r(i,d))) \\\\quad (1)$$\\n\\nwhere $f(r, r(i,d))$ indicates the frequency of rating value $r \\\\in \\\\{0.1, 0.3, 0.5, 0.7, 0.9\\\\}$ in the set of $L$ intensity ratings in $r(i,d)$. We apply the entropy to analyze the extent to which there is a consensus amongst the annotators and find the prevalent perception (Sec 4.2).\\n\\nWe introduce a cross entropy measure for comparison between a Beta distribution fitted from a set of ratings and another single intensity rating. If the latter comes from a human annotator, then we have $r \\\\in \\\\{0.1, 0.3, 0.5, 0.7, 0.9\\\\}$; if the latter comes from a model's prediction, we quantize it into the discrete values for fair cross-entropy calculation. To this point, we define the cross entropy between a discrete rating $r$ and a Beta distribution given by $\\\\alpha$ and $\\\\beta$ as:\\n\\n$$H_\\\\beta(r | \\\\alpha, \\\\beta) = -\\\\log(\\\\bar{g}(r | \\\\alpha, \\\\beta)) \\\\quad (2)$$\\n\\nwhich we use in analysis and benchmarking (Sec. 4).\\n\\n3.4. Algorithm benchmarking metrics\\n\\nMultidimensional modulated annotations can be used to benchmark current expression perception algorithms. The model predicted facial expression intensity for image $i$ along expression dimension $d$ can be denoted as $\\\\hat{r}(i,d) \\\\in (0, 1)$. We calculate the cross-entropy $H_d$ for an expression dimension $d$ across all images using:\\n\\n$$H_d = \\\\frac{1}{N} \\\\sum_{i=1}^{N} H_\\\\beta(\\\\hat{r}(i,d) | \\\\alpha, \\\\beta), \\\\quad (3)$$\\n\\nwhere $H_\\\\beta(\\\\hat{r} | \\\\alpha, \\\\beta)$ is defined in Eq. 2. We also introduce $M_d$, a measure of absolute distance between the Beta distribution and the rating, for each expression dimension $d$ as:\\n\\n$$M_d = \\\\frac{1}{N} \\\\sum_{i=1}^{N} |\\\\mu(i,d) - \\\\hat{r}(i,d)|, \\\\quad (4)$$\\n\\nwhere $\\\\mu(i,d)$ is the distribution mean for image $i$ along dimension $d$.\\n\\n4. Experiments and analysis\\n\\nThe following subsections introduce the experimental design used to validate multidimensional modulated annotations (Sec. 4.1), assess annotator behavior during the task (Sec. 4.2), demonstrate their potential for use to benchmark expression perception algorithms (Sec. 4.3) and test the compound expression hypothesis (Sec. 4.4).\"}"}
{"id": "CVPR-2022-1202", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We designed a custom Amazon SageMaker GroundTruth graphic interface (Fig. 5) to collect annotator data in three experiments. First, participants were recruited to rate face images by the 6 primary expressions and paid at a rate of $0.072 per completed image. In the second experiment, participants rated images by the 15 compound expressions and were paid $0.24 per image. In the final experiment, participants rated images by the set of 21 expressions and were paid $0.24 per image. The median time taken to annotate 6, 15, and 21 expressions was 25, 56, and 62 seconds respectively, yielding an average pay rate of $13.23 per hour. Each image was annotated by 9 unique AMT participants per experiment, resulting in approximately 27,000 annotations and 358.3 hours of work.\\n\\nBeta distribution fitting.\\n\\nAnnotator intensity ratings were used to estimate shape parameters, $\\\\alpha$ and $\\\\beta$, of a Beta distribution per expression per face (Sec. 3.2).\"}"}
{"id": "CVPR-2022-1202", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. How many annotators are needed for reproducible measurements?\\n\\nBeta distributions are fit to the data using all possible combinations of c = 2, 3, ..., L-1 annotator ratings (Sec. 4.2). Cross-entropy, $H_\\\\beta$ (Eq. 2, Sec. 3.3), was computed from the left-out annotations. The mean cross-entropy values across all images are shown by expression and the number of annotations used to fit each distribution. Cross-entropy saturates around 5-6 ratings, indicating six annotators are sufficient to collect reproducible ratings.\\n\\nOur annotation approach lets us examine not only the prevalence of expressions, but also the variability amongst annotations. We find that some expressions yield near-perfect annotator agreement while others receive intensity annotations across the entire scale, indicating that the expression is ambiguous. We modeled this by calculating the entropy $S$ defined in Eq. 1, for each intensity distribution along each expression dimension. Fig. 9 illustrates three sample images with relatively low, medium, and high entropy scores. The histogram of entropy scores across all images annotated for the primary expressions is shown in Fig. 8. Annotators agreed most in their ratings of 'disgusted' (mean=0.82, std=0.37) and agreed least on 'surprised' (mean=0.99, std=0.28).\\n\\nSince perceived expressions can be subjective and ambiguous, it can be hypothesized that multiple annotators are needed to generate reproducible expression distributions for a face image. To explore this, we conducted an analysis to determine how many annotators are needed to fit expression distributions that effectively model human perception. We calculated the cross-entropy, $H_\\\\beta$ using Eq. 2, between the beta fit and the left out annotator intensity ratings when using all combinations of c = 2, 3, ..., L-1 ratings where L=9. Table 3 shows the mean $H_\\\\beta$ values across all combinations of annotator ratings per expression dimension for each image. Entropy values generally decrease as more ratings are used to fit the beta, converging around 5 or 6 ratings for most expressions. The 'happy' and 'angry' dimensions have comparatively low entropy values whereas 'disgusted' has comparatively large entropy values, even when using 8 intensity ratings to fit the beta. Some expression dimensions, where more perception variance exists, may need more annotators when crowdsourcing to generate a stable distribution.\\n\\n4.3. Benchmarking algorithms\\n\\nMultidimensional modulated annotator ratings can help us better model the human perception of expression and assess the performance of automated systems. Many modern expression perception algorithms output confidence scores for the 6 primary expressions. Cross entropy ($H_d$) and absolute distance ($M_d$) described in Sec. 3.4 can be used to directly compare algorithmic scores with human perception. We applied our benchmark dataset and method to the evaluation of 4 state-of-the-art expression detection algorithms, including two commercial algorithms ('C1', 'C2') and 2 open source academic algorithms (RMN [48], DAN [53]). Each algorithm takes a face image as input and produces an expression vector as output that included at least the 6 primary expressions of interest. We directly compare the algorithmic output with the mean, $\\\\mu$, of the beta distribution. We evaluate the performance of the algorithms using $H_d$ and $M_d$ between $\\\\mu$ and the algorithm's predicted value. Our cross-entropy metric specifically accounts for the spread of annotator ratings across the 5 bins.\\n\\nWe also generate annotator ground truth for an additional expression dimension ($d = D + 1$), 'neutral'. With the rationale that 'neutral' could be interpreted as the opposite of any of the non-neutral facial expressions, we formulate a \\\"neutral\\\" raw ratings, per annotator $l$, per image $i$, denoted as $v_{D+1} = 4 - \\\\max(v_1, v_2, ..., v_D)$, then compute the normalized intensity rating $r_{D+1}$. Benchmark results can be found in Fig. 10, where DAN shows the best performance at predicting intensity for primary expressions in terms of absolute distance and cross-entropy metric, followed by C1. We show in Fig. 2 a sample image and the primary expression predictions of the 4 selected algorithms. To compare our metrics with traditional metrics used to benchmark expression detection algorithms, we calculated classification accuracy and F-score using the original one-hot ExpW labels on the 1,000 subset. Results for each algorithm are shown in Table 4 where DAN appear to be the best across half the expressions and metrics. Thus, our evaluation method gives a sharper endorsement to the DAN algorithm.\\n\\n4.4. Testing the compound expression hypothesis\\n\\nAre six primary dimensions sufficient to completely characterize a facial expression [17] (Sec. 2)? More strenuously: are many/most/all expressions well represented as the compounds of one-two primary expressions [13, 42]?\\n\\nThanks to our 1,000 annotated face images, where we collected perceptions limiting annotators to, respectively, the 6 primary, the corresponding 15 two-way compound, and the 21 = 6 + 15 expressions dimensions, we are now in a position to start exploring these questions. To this end, we constructed three algorithms to infer compound expressions from primary ones. The first is a linear model taking into account all primary expressions. The second is a set of 15 linear models, each one of which combines the two normative compound expressions (Table 1). The third is a set of 15 linear models where we selected the two most informative primary expressions for each compound expression. See Fig. 4 for an example.\"}"}
{"id": "CVPR-2022-1202", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10. Expression prediction benchmark.\\n\\nFour state-of-the-art automated expression prediction algorithms were benchmarked using two metrics: cross entropy ($H$) and absolute distance ($M$) as defined in Sec. 3.4. The box plots show the median (horizontal line), 66% confidence intervals (colored boxes) and 95% confidence intervals (whiskers) (see Sec. 4.3).\\n\\nTable 4. Comparing traditional benchmark metrics. We compare the results of 4 algorithms on original ExpW ground truth [60] using standard multi-class classification metrics: accuracy and F-score (Sec. 4.3). The highest score per metric per expression dimension is highlighted. Both Accuracy and F-score are to be maximized.\\n\\nWe conducted a 10-fold cross-validation experiment where we fit the models using 90% of images and predicted the \u00b5 (see Sec 3.3). Each time, we measured the Mean Absolute Error (MAE) between the prediction and the actual value (we express MAE as percent of the [0, 1] dynamic range). We found MAE=6.2% for the full model, and MAE=7.5% and 7.8% respectively for the two models utilizing only two dimensions. These findings suggest that compound expressions may be predicted accurately using the six primary dimensions, with a small amount of information lost in the process. Further analysis is needed to better characterize this small effect.\\n\\n5. Conclusion and discussion\\n\\nWe proposed a novel method to collect and model multi-dimensional modulated facial expression annotations. The method improves upon previous work because it does not rely on expert annotators, multiple expressions are simultaneously annotated (up to 21) with their perceived level of intensity, and expression ambiguity can be measured. Using our method, we annotated a diverse set of 1,000 in-the-wild face images. We were then able to benchmark two commercial and two academic expression prediction algorithms using two different metrics and found the DAN [53] algorithm to be the best overall. Our metrics refine our understanding of algorithm performance by considering the underlying distribution of human expression perception. Additional findings emerge from our study. First, 5-6 crowdsourced annotators are sufficient for achieving reproducible measurements. Second, while the expression of most in-the-wild face images is well characterized by one of Ekman's six primary dimensions, some faces require two dimensions to be characterized. Third, annotator perceptions are best measured using a modulated, rather than binary, scale. Fourth, annotators will agree on the perception of most faces; however, for a number of expressions perception is ambiguous, with annotations spreading over many intensities. Any method that benchmarks expression prediction algorithms has to take such ambiguities into account. Fifth, while we find that reducing expression annotation to six primary dimensions is quite effective, we observe that a small amount of information is lost in the process \u2013 understanding how this happens will require further investigation.\\n\\nFuture directions of research in this area include further exploring algorithmic techniques that allow models to learn from both clear and ambiguous facial expressions. Another interesting question is exploring individual annotator behavior, e.g. understanding whether annotators of different age, gender, and ethnicity may perceive facial expressions differently in a systematic way and whether some annotators may have richer perception than others.\\n\\nEthics discussion. Measuring human perception of facial expression from images helps build better benchmarks for automating the perception of facial expression in machines. This, in turn, will enable engineers to build machines that can better interact with human users, thus enabling machines to address a wider range of human needs. Alongside the obvious benefits, this technology presents risks, including change in the patterns of human social interaction, control of privacy, the potential for racial bias, and potentially unforeseen economic impact due to rapid technological change [23, 27].\\n\\nAcknowledgements: We are grateful to Aleix Martinez, Umit Keles, Ralph Adolphs, Stefano Soatto, Ayanna Howard, and Wei Li for guidance in reading the relevant cross-disciplinary literature and providing insightful comments on the manuscript.\"}"}
{"id": "CVPR-2022-1202", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] D. Acharya, Z. Huang, D. Pani Paudel, and L. Van Gool. Covariance pooling for facial expression recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 367\u2013374, 2018.\\n\\n[2] L. F. Barrett, R. Adolphs, S. Marsella, A. M. Martinez, and S. D. Pollak. Emotional expressions reconsidered: Challenges to inferring emotion from human facial movements. Psychological science in the public interest, 20(1):1\u201368, 2019.\\n\\n[3] E. Barsoum, C. Zhang, C. C. Ferrer, and Z. Zhang. Training deep networks for facial expression recognition with crowdsourced label distribution. In Proceedings of the 18th ACM International Conference on Multimodal Interaction, pages 279\u2013283, 2016.\\n\\n[4] M. S. Bartlett, J. C. Hager, P. Ekman, and T. J. Sejnowski. Measuring facial expressions by computer image analysis. Psychophysiology, 36(2):253\u2013263, 1999.\\n\\n[5] C. Blank, S. Zaman, A. Wesley, P. Tsiamyrtzis, D. R. Da Cunha Silva, R. Gutierrez-Osuna, G. Mark, and I. Pavlidis. Emotional Footprints of Email Interruptions, page 1\u201312. Association for Computing Machinery, New York, NY, USA, 2020.\\n\\n[6] D. Bryant and A. Howard. A comparative analysis of emotion-detecting AI systems with respect to algorithm performance and dataset diversity. In Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society, AIES '19, page 377\u2013382, New York, NY, USA, 2019. Association for Computing Machinery.\\n\\n[7] M. I. Conley, D. V. Dellarco, E. Rubien-Thomas, A. O. Cohen, A. Cervera, N. Tottenham, and B. Casey. The racially diverse affective expression (radiate) face stimulus set. Psychiatry research, 270:1059\u20131067, 2018.\\n\\n[8] R. Cowie, E. Douglas-Cowie, N. Tsapatsoulis, G. Votsis, S. Kollias, W. Fellenz, and J. G. Taylor. Emotion recognition in human-computer interaction. IEEE Signal processing magazine, 18(1):32\u201380, 2001.\\n\\n[9] C. Darwin. The expression of the emotions in man and animals. University of Chicago press, 2015.\\n\\n[10] A. Dawel, L. Wright, J. Irons, R. Dumbleton, R. Palermo, R. O'Kearney, and E. McKone. Perceived emotion genuineness: normative ratings for popular facial expression stimuli and the development of perceived-as-genuine and perceived-as-fake sets. Behavior research methods, 49(4):1539\u20131562, 2017.\\n\\n[11] S. Deng, Y. Xiong, M. Wang, W. Xia, and S. Soatto. Harnessing unrecognizable faces for improving face recognition. arXiv preprint arXiv:2106.04112, 2021.\\n\\n[12] G. Donato, M. S. Bartlett, J. C. Hager, P. Ekman, and T. J. Sejnowski. Classifying facial actions. IEEE Transactions on pattern analysis and machine intelligence, 21(10):974\u2013989, 1999.\\n\\n[13] S. Du, Y. Tao, and A. M. Martinez. Compound facial expressions of emotion. Proceedings of the National Academy of Sciences, 111(15):E1454\u2013E1462, 2014.\\n\\n[14] H. L. Egger, D. S. Pine, E. Nelson, E. Leibenluft, M. Ernst, K. E. Towbin, and A. Angold. The nimh child emotional faces picture set (nimh-chefs): a new set of children's facial emotion stimuli. International journal of methods in psychiatric research, 20(3):145\u2013156, 2011.\\n\\n[15] P. Ekman. An argument for basic emotions. Cognition & emotion, 6(3-4):169\u2013200, 1992.\\n\\n[16] P. Ekman. Facial expression and emotion. American psychologist, 48(4):384, 1993.\\n\\n[17] P. Ekman and E. L. Rosenberg. What the face reveals: Basic and applied studies of spontaneous expression using the Facial Action Coding System (FACS). Oxford University Press, USA, 1997.\\n\\n[18] C. Fabian Benitez-Quiroz, R. Srinivasan, and A. M. Martinez. Emotionet: An accurate, real-time algorithm for the automatic annotation of a million facial expressions in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5562\u20135570, 2016.\\n\\n[19] A. H. Farzaneh and X. Qi. Facial expression recognition in the wild via deep attentive center loss. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 2402\u20132411, 2021.\\n\\n[20] E. Goeleven, R. De Raedt, L. Leyman, and B. Verschuere. The karolinska directed emotional faces: a validation study. Cognition and emotion, 22(6):1094\u20131118, 2008.\\n\\n[21] I. J. Goodfellow, D. Erhan, P. L. Carrier, A. Courville, M. Mirza, B. Hamner, W. Cukierski, Y. Tang, D.-H. Lee, et al. Challenges in representation learning: A report on three machine learning contests. In International conference on neural information processing, pages 117\u2013124. Springer, 2013.\\n\\n[22] J. Guo, Z. Lei, J. Wan, E. Avots, N. Hajarolasvadi, B. Knyazev, A. Kuharenko, J. C. S. J. Junior, X. Bar\u00f3, H. Demirel, et al. Dominant and complementary emotion recognition from still images of faces. IEEE Access, 6:26391\u201326403, 2018.\\n\\n[23] Y. N. Harari. 21 Lessons for the 21st Century. Random House, 2018.\\n\\n[24] N. Hirschberg, L. E. Jones, and M. Haggerty. What's in a face: Individual differences in face perception. Journal of Research in Personality, 12(4):488\u2013499, 1978.\\n\\n[25] H. Hoffmann, H. Kessler, T. Eppel, S. Rukavina, and H. C. Traue. Expression intensity, gender and facial emotion recognition: Women recognize only subtle facial emotions better than men. Acta psychologica, 135(3):278\u2013283, 2010.\\n\\n[26] N. L. Johnson, S. Kotz, and N. Balakrishnan. Continuous univariate distributions, volume 2, volume 289. John wiley & sons, 1995.\\n\\n[27] S. Jonze. Her (film). Annapurna Pictures, 2013.\\n\\n[28] T. Kanade et al. Computer recognition of human faces, volume 47. Birkh\u00e4user Basel, 1977.\\n\\n[29] U. Keles, C. Lin, and R. Adolphs. A cautionary note on predicting social judgments from faces with deep neural networks, Jan 2021.\\n\\n[30] U. Keles, C. Lin, and R. Adolphs. A cautionary note on predicting social judgments from faces with deep neural networks. Affective Science, pages 1\u201317, 2021.\"}"}
{"id": "CVPR-2022-1202", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"D. Keltner, D. Sauter, J. Tracy, and A. Cowen. Emotional expression: Advances in basic emotion theory. *Journal of nonverbal behavior*, pages 1\u201328, 2019.\\n\\nO. Langner, R. Dotsch, G. Bijlstra, D. H. Wigboldus, S. T. Hawk, and A. Van Knippenberg. Presentation and validation of the radboud faces database. *Cognition and emotion*, 24(8):1377\u20131388, 2010.\\n\\nJ. S. Lerner and D. Keltner. Fear, anger, and risk. *Journal of personality and social psychology*, 81(1):146, 2001.\\n\\nH. Li, N. Wang, X. Ding, X. Yang, and X. Gao. Adaptively learning facial expression representation via cf labels and distillation. *IEEE Transactions on Image Processing*, 30:2016\u20132028, 2021.\\n\\nS. Li and W. Deng. Blended emotion in-the-wild: Multi-label facial expression recognition using crowdsourced annotations and deep locality feature learning. *International Journal of Computer Vision*, 127(6):884\u2013906, 2019.\\n\\nS. Li, W. Deng, and J. Du. Reliable crowdsourcing and deep locality-preserving learning for expression recognition in the wild. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 2852\u20132861, 2017.\\n\\nV. LoBue and C. Thrasher. The child affective facial expressions (cafe) set: Validity and reliability from untrained adults. *Frontiers in psychology*, 5:1532, 2015.\\n\\nJ. Lu, X. Xie, and R. Zhang. Focusing on appraisals: How and why anger and fear influence driving risk perception. *Journal of safety research*, 45:65\u201373, 2013.\\n\\nM. Lyons, S. Akamatsu, M. Kamachi, and J. Gyoba. Coding facial expressions with gabor wavelets. In *Proceedings Third IEEE international conference on automatic face and gesture recognition*, pages 200\u2013205. IEEE, 1998.\\n\\nD. S. Ma, J. Correll, and B. Wittenbrink. The chicago face database: A free stimulus set of faces and norming data. *Behavior research methods*, 47(4):1122\u20131135, 2015.\\n\\nK. Mase. Recognition of facial expression from optical flow. *IEICE TRANSACTIONS on Information and Systems*, 74(10):3474\u20133483, 1991.\\n\\nS. McCloud. *Making comics: Storytelling secrets of comics, manga and graphic novels*. Harper New York, 2006.\\n\\nA. S. Meuwissen, J. E. Anderson, and P. D. Zelazo. The creation and validation of the developmental emotional faces stimulus set. *Behavior research methods*, 49(3):960\u2013966, 2017.\\n\\nA. Mollahosseini, B. Hasani, and M. H. Mahoor. Affectnet: A database for facial expression, valence, and arousal computing in the wild. *IEEE Transactions on Affective Computing*, 10(1):18\u201331, 2017.\\n\\nB. Montagne, R. P. Kessels, E. H. De Haan, and D. I. Perrett. The emotion recognition task: A paradigm to measure the perception of facial emotional expressions at different intensities. *Perceptual and motor skills*, 104(2):589\u2013598, 2007.\\n\\nI. J. Myung. Tutorial on maximum likelihood estimation. *Journal of mathematical Psychology*, 47(1):90\u2013100, 2003.\\n\\nC. Padgett and G. W. Cottrell. Representing face images for emotion classification. In *Advances in neural information processing systems*, pages 894\u2013900, 1997.\\n\\nL. Pham, T. H. Vu, and T. A. Tran. Facial expression recognition using residual masking network. In *2020 25th International Conference on Pattern Recognition (ICPR)*, pages 4513\u20134519. IEEE, 2021.\\n\\nR. Plutchik. Emotions: A general psychoevolutionary theory. In *Approaches to emotion*, 1984:197\u2013219, 1984.\\n\\nA. Todorov, A. N. Mandisodza, A. Goren, and C. C. Hall. Inferences of competence from faces predict election outcomes. *Science*, 308(5728):1623\u20131626, 2005.\\n\\nN. Tottenham, J. W. Tanaka, A. C. Leon, T. McCarry, M. Nurse, T. A. Hare, D. J. Marcus, A. Westerlund, B. Casey, and C. Nelson. The nimstim set of facial expressions: Judgments from untrained research participants. *Psychiatry research*, 168(3):242\u2013249, 2009.\\n\\nK. Wang, X. Peng, J. Yang, S. Lu, and Y. Qiao. Suppressing uncertainties for large-scale facial expression recognition. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 6897\u20136906, 2020.\\n\\nZ. Wen, W. Lin, T. Wang, and G. Xu. Distract your attention: Multi-head cross attention network for facial expression recognition. *arXiv preprint arXiv:2109.07270*, 2021.\\n\\nJ. Willis and A. Todorov. First impressions: Making up your mind after a 100-ms exposure to a face. *Psychological science*, 17(7):592\u2013598, 2006.\\n\\nF. Xue, Q. Wang, and G. Guo. Transfer: Learning relation-aware facial expression representations with transformers. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 3601\u20133610, 2021.\\n\\nY. Yacoob and L. S. Davis. Recognizing human facial expressions from long image sequences using optical flow. *IEEE Transactions on pattern analysis and machine intelligence*, 18(6):636\u2013642, 1996.\\n\\nS.-T. Yeh et al. Using trapezoidal rule for the area under a curve calculation. In *Proceedings of the 27th Annual SAS User Group International (SUGI'02)*, pages 1\u20135, 2002.\\n\\nL. Yin, X. Wei, Y. Sun, J. Wang, and M. J. Rosato. A 3d facial expression database for facial behavior research. In *7th international conference on automatic face and gesture recognition (FGR06)*, pages 211\u2013216. IEEE, 2006.\\n\\nZ. Zeng, M. Pantic, G. I. Roisman, and T. S. Huang. A survey of affect recognition methods: Audio, visual, and spontaneous expressions. *IEEE transactions on pattern analysis and machine intelligence*, 31(1):39\u201358, 2008.\\n\\nZ. Zhang, P. Luo, C.-C. Loy, and X. Tang. Learning social relation traits from face images. In *Proceedings of the IEEE International Conference on Computer Vision*, pages 3631\u20133639, 2015.\"}"}
