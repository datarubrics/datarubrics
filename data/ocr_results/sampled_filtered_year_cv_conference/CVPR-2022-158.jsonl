{"id": "CVPR-2022-158", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"BARC: Learning to Regress 3D Dog Shape from Images by Exploiting Breed Information\\n\\nNadine R\u00fcegg 1,2, Silvia Zuffi 3, Konrad Schindler 1 and Michael J. Black 2\\n\\n1 ETH Z\u00fcrich, Switzerland\\n2 Max Planck Institute for Intelligent Systems, T\u00fcbingen, Germany\\n3 IMA TI-CNR, Milan, Italy\\n\\nFigure 1. Monocular 3D shape and pose regression of 3D dogs from 2D images.\\n\\nSince 3D training data is limited, BARC uses breed information at training time via triplet and classification losses to learn how to regress realistic 3D shapes at test time.\\n\\nAbstract\\n\\nOur goal is to recover the 3D shape and pose of dogs from a single image. This is a challenging task because dogs exhibit a wide range of shapes and appearances, and are highly articulated. Recent work has proposed to directly regress the SMAL animal model, with additional limb scale parameters, from images. Our method, called BARC (Breed-Augmented Regression using Classification), goes beyond prior work in several important ways. First, we modify the SMAL shape space to be more appropriate for representing dog shape. But, even with a better shape model, the problem of regressing dog shape from an image is still challenging because we lack paired images with 3D ground truth. To compensate for the lack of paired data, we formulate novel losses that exploit information about dog breeds. In particular, we exploit the fact that dogs of the same breed have similar body shapes. We formulate a novel breed similarity loss consisting of two parts: One term encourages the shape of dogs from the same breed to be more similar than dogs of different breeds. The second one, a breed classification loss, helps to produce recognizable breed-specific shapes. Through ablation studies, we find that our breed losses significantly improve shape accuracy over a baseline without them. We also compare BARC qualitatively to WLDO with a perceptual study and find that our approach produces dogs that are significantly more realistic. This work shows that a-priori information about genetic similarity can help to compensate for the lack of 3D training data. This concept may be applicable to other animal species or groups of species.\\n\\nOur code is publicly available for research purposes at https://barc.is.tue.mpg.de/.\\n\\n1. Introduction\\n\\nLearning to infer 3D models of articulated and non-rigid objects from 2D images is challenging. For the case of humans, recent methods leverage detailed parametric models of human body shape and pose, like SMPL \\\\[10\\\\]. Such models have been learned from thousands of high-resolution 3D scans of people in varied poses. This approach cannot be...\"}"}
{"id": "CVPR-2022-158", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"replicated for most animal species because they are diffi-\\ncult, or even impossible, to scan in a controlled environ-\\nment. Moreover, paired training data of animals with known\\n3D shape is even rarer. To make progress, we must leverage\\nside information that can be easily obtained, yet constrains\\nthe task of 3D shape and pose estimation.\\n\\nThe 3D reconstruction of animal shape and pose has\\nmany real-life applications, ranging from biology and\\nbiomechanics to conservation. Specifically, the non-\\ninvasive capture of 3D body shape supports morphology\\nand health-from-shape analysis. Markerless motion capture\\nallows 3D motion analysis for animals that it is not possi-\\nbile to capture in a lab setting. Here we focus on dogs as a\\nrich, representative, test case. Dogs exhibit a wide range of\\nshapes, are non-rigid, and have complex articulation. Con-\\nsequently, dogs are challenging and representative of many\\nother animals.\\n\\nHere, our goal is to learn to estimate a dog's 3D shape\\nand pose from a monocular, uncontrolled image. Given the\\nlack of 3D training data, we train a regression network with\\n2D supervision, in the form of keypoints and silhouettes.\\nWith only such 2D information, the problem is, however,\\nheavily under-constrained: many 3D shapes can explain the\\n2D image evidence equally well. To make the task well-\\nposed, we need additional, prior information. Here we ex-\\nploring a novel source of a-priori knowledge: a dog's shape\\nis determined, in part, by its breed. Even a trained amateur\\nrecognizes the breed by looking at a dog's shape (and\\nappearance).\\n\\nDogs are a particularly interesting case to explore the\\nrole of breeds because of their large variety. Dogs have been\\ndomesticated and bred for a long time, for diverse purposes\\nsuch as companionship, hunting, or herding, but also racing,\\npulling sleds, finding truffles, etc. Consequently, breeders\\nhave selected for a range of traits including body shape (as\\nwell as temperament, appearance, etc.) which has led to a\\nlarge number of breeds with very different characteristics.\\nA recent analysis of the dog genome illustrates the relation-\\nship between different breeds that exist today.\\n\\nBreeds are grouped into clades, often with high shape similarity\\nwithin a clade. Figure 2 shows a cladogram of 161 domes-\\ntic dog breeds.\\n\\nHere, we explore the use of genetic side information, in\\nthe form of breed labels, to train a regressor that infers 3D\\ndog shape from 2D images. Specifically, we train a novel\\nneural network called BARC, for \\\"Breed-Augmented Re-\\ngression using Classification.\\\" We follow the approach of\\nregressing a parametric 3D shape model directly from im-\\nages, which is common in human pose and shape es-\\ntimation. Here, we use the SMAL animal model to\\ndefine the kinematic chain and mesh template. We extend\\nSMAL in several ways to be a better foundation for learning\\nabout dog shape, this includes adding limb scale factors and\\nextension its shape space with additional 3D dog shapes.\\n\\nTo solve the problem of estimating dog shape from im-\\nages, we make several contributions. (1) We propose a\\nnovel neural network architecture to regress 3D dog shape\\nand 3D pose from images. (2) To make training feas-\\nible from 2D silhouettes and keypoints, we exploit the fact\\nthat 2D images of the same breed should produce similar\\n3D shapes, while different breeds (mostly) have different\\nshapes. With this assumption, we impose classification and\\ntriplet losses on the training images, which come with breed\\nlabels. (3) As a result, we learn a breed-aware latent shape\\nspace, in which we can identify breed clusters and relation-\\nships in agreement with the cladogram in Fig. 2. (4) Op-\\ntionally, we show how to exploit 3D models, if available for\\nsome breeds.\\n\\nAlthough we use one of the largest dog datasets in the\\nliterature, the large number of dog breeds (in our case 120)\\nmeans there are only a few images per breed. One can inter-\\npret our method as learning a common shape manifold for\\nall dogs (as not enough examples are available per breed),\\nwhile using the breed labels to locally regularize it. To our\\nknowledge, this is the first method that exploits breed infor-\\nmation to regress the 3D shape of animals from images.\\n\\nWe train the network on the Stanford Extra (StanExt) training set, which we extend with eye, withers and throat\\nkeypoints, and test our approach on the StanExt test set.\\nWe evaluate on this dataset of 120 different dog breeds and\\nshow that we learn a latent shape space for dogs in which\\nmore closely related dogs are in closer proximity (Fig.\\n3).\"}"}
{"id": "CVPR-2022-158", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Through ablation studies, we evaluate the impact of different types of breed information and find that each loss leads to a significant improvement in shape accuracy. We evaluate accuracy with standard 2D measures like PCK and IOU, but these do not necessarily reflect 3D accuracy. Consequently, we create a dataset of 3D dogs to evaluate shape of corresponding breeds. This allows a quantitative evaluation, and we significantly outperform the prior art (WLDO). Finally, to evaluate shape estimates for in-the-wild images, we use a perceptual study to compare methods. We find that our final model is more realistic than ablated versions or WLDO.\\n\\n2. Related Work\\n\\nWhile many approaches focus on 3D reconstruction of humans from images, there is comparably little work on animal 3D pose and shape estimation. Animal reconstruction from images has been approached in two main ways: model-free and model-based.\\n\\nModel-free 3D Reconstruction. These methods do not exploit an existing 3D shape model. Ntouskos et al. create 3D animal shapes by assembling 3D primitives obtained by fitting manually segmented parts in multiple images of different animals from the same class. Vicente and Agapito deform a template extracted from a reference image to fit a new image using keypoints and the silhouette, without addressing articulation. Kanazawa et al. learn to regress 3D bird shape, given keypoints and silhouettes; birds exhibit rather limited articulation. Recent work obviates the need for 2D keypoints.\\n\\nModel-based 3D Reconstruction. In one of the first 3D animal reconstruction methods from images, Cashman and Fitzgibbon deform a 3D dolphin template, learning a low-dimensional deformation model from hand-clicked keypoints and manual segmentation. They also apply their method to a pigeon and a polar bear. A limitation of this approach is that articulation is not explicitly modeled. In contrast, Zuffi et al. introduce SMAL, a deformable 3D articulated quadruped animal model. Similar to the widely adopted human body model, SMPL, SMAL represents 3D articulated shapes with a low-dimensional linear shape space. Due to the lack of real 3D animal scans, SMAL is learned from scanned toy figurines of different quadruped species. Since dogs are not well represented by SMAL, Biggs et al. extend the SMAL model by adding scale parameters for limb lengths. In an articulated 3D model of birds is defined in terms of limb scale variations and used to learn shape from images; it is unclear whether this method easily extends to more complex animals.\\n\\nThe early work using SMAL uses an optimization-based approach to fit the model to image evidence and to refine the animal shapes. In other methods, Biggs et al. show how to extract accurate animal shape and pose from videos, while Kearney et al. estimate dog shape and pose from RGBD-images. More relevant to BARC are learning-based methods that regress animal pose and shape directly. Biggs et al. estimate dog pose and shape from single images by regressing pose and shape parameters of their model to training images of the StanExt dataset. Their initial pose prior is improved using expectation maximization with respect to fits of their model to the images. Zuffi et al. regress a zebra SMAL model from images by exploiting a texture map and learn a shape space for the Grevy's zebra. They train on synthetic data. In contrast to these methods, Sanakoyeu et al. neither predict 3D directly from the image nor rely on sparsely annotated keypoints. Rather they show how to transfer DensePose from humans to a non-human primate. This approach does not recover 3D shape or pose.\\n\\nSupervision without 3D Ground Truth. All 3D approaches rely on certain 2D features such as keypoints, segmentation masks or DensePose annotations as a supervision signal. Sometimes those 2D signals are used as an intermediate representation before the model is lifted to 3D. Mu et al. exploit synthetic 3D data to predict 2D keypoints and a coarse body part segmentation map. They introduce a new dataset for animal 2D keypoint prediction and show how to transfer knowledge between domains, particularly from seen quadruped species to unseen ones. Still other work encourages similarity between objects of similar shape, with small intra-class variability. They neither exploit breed information nor use contrastive learning to construct a structured latent space.\\n\\n3. Approach\\n\\nThe present work explores how known breed information at training time can be leveraged to learn to regress a high-quality 3D model of dogs. To that end, we combine a parametric dog model with a neural network that maps images to model instances. In the following, we describe the model we use, the network architecture in which it is embedded, and the loss functions used to train the architecture, including the novel breed losses.\\n\\n3.1. Dog Model\\n\\nFor the parametric representation of a dog's shape and pose, we employ a variant of SMAL. We start from 41 scanned animal toy figurines of several different species (already used as part of the original SMAL model) as well as 3D Unity canine models in the animal equivalent of the canonical T-pose; i.e., standing with straight legs and tail pointing backwards. We purchased the same pack of Unity models as was exploited by to initialize their mixture.\"}"}
{"id": "CVPR-2022-158", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learned latent space.\\n\\nFigure 3. t-SNE visualization of the 64-dimensional latent shape variable for dogs in the test set. Large markers indicate average values within each of the clades in Fig. 2. Left: Latent space of the network trained without breed similarity loss. Note that the clade means are all near the population mean, indicating poor clustering. Center and right: With breed similarity loss. For each clade, colors with different saturation indicate different breeds within the clade.\\n\\nTo that end, we fit a mesh with the same topology as SMAL (and WLDO) to the new dogs, add these to the original SMAL training set and recompute the mean shape and the PCA shape space. The resulting model differs from the original SMAL in three respects: (1) different input data; (2) reweighting of the inputs such that 50% of the total weight is assigned to dogs; and (3) rescaling of the meshes such that the torso always has length 1. We further adapt an idea from WLDO and extend the model with scaling parameters \\\\( \\\\kappa \\\\) (where the actual scale is \\\\( \\\\exp(\\\\kappa) \\\\)) for the limbs, plus an additional scale for the head length. The scaling is applied to the bone lengths, and propagated to the surface mesh via their corresponding linear blend skinning (LBS) weights. For compactness, we collect the PCA shape coefficients \\\\( \\\\beta_{pca} \\\\) and limb scales \\\\( \\\\kappa \\\\) into a shape vector \\\\( \\\\beta \\\\).\\n\\n3.2. Architecture\\n\\nSimilar to [15, 28], we use separate shape and pose branches. Figure 4 shows the overall architecture of BARC, consisting of a joint stacked hourglass encoder, a shape branch, a pose branch, and a 3D prediction and reprojection module.\\n\\nStacked Hourglass:\\n\\nFirst, the input image is encoded and 2D keypoint heatmaps, as well as a segmentation map, are predicted with a pre-trained stacked hourglass network. 2D keypoint locations are extracted from the heatmaps with \\\"numerical coordinate regression\\\" (NCR, [13]). The segmentation map is encoded with a scheme similar to \\\"basis point sets\\\" (BPS, [16]) for 3D point cloud encoding. To our knowledge, we are the first to apply BSP in 2D. Compared to the full segmentation map, this encoding is lightweight, easy to compute for silhouettes, and has a similar format as the NCR keypoints. We find that, despite the reduction to a small number of sample points, the silhouette encoding still improves the 3D prediction over 2D keypoints alone.\\n\\nShape-branch:\\n\\nThe input image and the predicted segmentation map are concatenated and fed to a ResNet34 that predicts a latent encoding \\\\( z \\\\) of the dog's shape. \\\\( z \\\\) is decoded into both a breed (class) score and a vector of body shape coefficients \\\\( \\\\beta \\\\). We have experimented with different sub-networks between \\\\( z \\\\) and \\\\( \\\\beta \\\\) and find that the breed similarity loss is most effective when the connection is as direct as possible, with only single, fully-connected layers between \\\\( z \\\\) and each of the shape vectors \\\\( \\\\kappa \\\\) and \\\\( \\\\beta_{pca} \\\\). These shape coefficients are applied to the 3D dog template to obtain a shape, whose bone lengths are passed on to the pose branch.\\n\\nPose-branch:\\n\\nThe predicted 2D keypoints, the BPS encoding of the silhouette and the bone lengths from the shape network form the input to estimate the dog's 3D pose, its translation w.r.t. the camera coordinate system and the camera's focal length. The pose is represented as a 6D rotation [29] for each joint, including a root rotation. Instead of predicting all rotations directly, we predict root rotation and a latent pose representation \\\\( y \\\\). Following recent work on human pose estimation, we implement an invertible neural network (INN) that maps each latent variable \\\\( y \\\\) to a pose. This INN is used in the context of a normalizing flow pose prior trained on the RGBD-Dog dataset [8]. Similar to [27], we build this network consisting of Real-NVP blocks, but because of the smaller size of the RGBD-Dog dataset, compared to AMASS [11], our network is much smaller than in previous work on human pose estimation. The aim of the INN is to map the distribution of 3D dog poses to a simple and tractable density function, i.e. a spherical multivariate Gaussian distribution. To train the pose prior, we exploit the RGBD-Dog dataset [8], which contains walking, trotting and jumping sequences, but no sitting or lying poses. Note that the INN is pretrained to serve as a pose prior and kept fixed during final network training.\\n\\n3D Prediction and Reprojection Module:\\n\\nAs a last step, BARC poses the model according to the predicted shape, pose and translation, and reprojects the keypoints and silhouette to image space, using the predicted focal length. To minimize the silhouette and keypoint reprojection errors, we employ the Pytorch3D differentiable renderer [17].\"}"}
{"id": "CVPR-2022-158", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Benjamin Biggs, Ollie Boyne, James Charles, Andrew Fitzgibbon, and Roberto Cipolla. Who left the dogs out: 3D animal reconstruction with expectation maximization in the loop. In ECCV, pages 195\u2013211, 2020.\\n\\n[2] Benjamin Biggs, Thomas Roddick, Andrew Fitzgibbon, and Roberto Cipolla. Creatures great and SMAL: Recovering the shape and motion of animals from video. In ACCV, pages 3\u201319, 2018.\\n\\n[3] Jinkun Cao, Hongyang Tang, Hao-Shu Fang, Xiaoyong Shen, Cewu Lu, and Yu-Wing Tai. Cross-domain adaptation for animal pose estimation. In ICCV, pages 9498\u20139507, 2019.\\n\\n[4] Thomas J. Cashman and Andrew W. Fitzgibbon. What shape are dolphins? building 3D morphable models from 2D images. pages 232\u2013244, 2013.\\n\\n[5] Parker H. G., Dreger D. L., Rimbault M., Davis B. W., Mullen A. B., Carpintero-Ramirez G., and Ostrander E. A. Genomic analyses reveal the influence of geographic origin, migration, and hybridization on modern dog breed development. Cell Reports, 4(19):697\u2013708, 2017.\\n\\n[6] Shubham Goel, Angjoo Kanazawa, and Jitendra Malik. Shape and viewpoints without keypoints. In ECCV, pages 88\u2013104, 2020.\\n\\n[7] Angjoo Kanazawa, Shubham Tulsiani, Alexei A Efros, and Jitendra Malik. Learning category-specific mesh reconstruction from image collections. In ECCV, pages 371\u2013386, 2018.\\n\\n[8] Sin\u00e9ad Kearney, Wenbin Li, Martin Parsons, Kwang In Kim, and Darren Cosker. RGBD-dog: Predicting canine pose from RGBD sensors. In CVPR, pages 8336\u20138345, 2020.\\n\\n[9] Aditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image categorization. In CVPR Workshops, 2011.\\n\\n[10] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: A skinned multi-person linear model. ACM TOG \u2013 SIGGRAPH Asia, 34(6):248:1\u2013248:16, 2015.\\n\\n[11] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. Amass: Archive of motion capture as surface shapes. In ICCV, pages 5442\u20135451, 2019.\\n\\n[12] Jiteng Mu, Weichao Qiu, Gregory D Hager, and Alan L Yuille. Learning from synthetic animals. In CVPR, pages 12386\u201312395, 2020.\\n\\n[13] Aiden Nibali, Zhen He, Stuart Morgan, and Luke Prendergast. Numerical coordinate regression with convolutional neural networks. arXiv preprint arXiv:1801.07372, 2018.\\n\\n[14] Valsamis Ntouskos, Marta Sanzari, Bruno Cafaro, Federico Nardi, Fabrizio Natola, Fiora Pirri, and Manuel Ruiz. Component-wise modeling of articulated objects. In ICCV, pages 2327\u20132335, 2015.\\n\\n[15] Georgios Pavlakos, Luyang Zhu, Xiaowei Zhou, and Kostas Daniilidis. Learning to estimate 3d human pose and shape from a single color image. In CVPR, pages 459\u2013468, 2018.\\n\\n[16] Sergey Prokudin, Christoph Lassner, and Javier Romero. Efficient learning on point clouds with basis point sets. In ICCV, pages 4332\u20134341, 2019.\\n\\n[17] Nikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. Accelerating 3d deep learning with PyTorch3D. arXiv:2007.08501, 2020.\\n\\n[18] Artsiom Sanakoyeu, Vasil Khalidov, Maureen S McCarthy, Andrea Vedaldi, and Natalia Neverova. Transferring dense pose to proximal animal classes. In CVPR, pages 5233\u20135242, 2020.\\n\\n[19] Soubhik Sanyal, Timo Bolkart, Haiwen Feng, and Michael J. Black. Learning to regress 3d face shape and expression from an image without 3d supervision. In CVPR, pages 7763\u20137772, 2019.\\n\\n[20] Florian Schroff, Dmitry Kalenichenko, and James Philbin. FaceNet: A unified embedding for face recognition and clustering. In CVPR, pages 815\u2013823, 2015.\\n\\n[21] Yaniv Taigman, Ming Yang, Marc'Aurelio Ranzato, and Lior Wolf. DeepFace: Closing the gap to human-level performance in face verification. In CVPR, pages 1701\u20131708, 2014.\\n\\n[22] Shubham Tulsiani, Nilesh Kulkarni, and Abhinav Gupta. Implicit mesh reconstruction from unannotated image collections. arXiv preprint arXiv:2007.08504, 2020.\\n\\n[23] Laurens van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. Journal of Machine Learning Research, 9:2579\u20132605, 2008.\\n\\n[24] S Vicente and L Agapito. Balloon shapes: Reconstructing and deforming objects with volume from images. In 3DV, pages 223\u2013230, 2013.\\n\\n[25] YuFu Wang, Nikos Kolotouros, Kostas Daniilidis, and Marc Badger. Birds of a feather: Capturing avian shape models from images. In CVPR, pages 14739\u201314749, 2021.\\n\\n[26] Shangzhe Wu, Tomas Jakab, Christian Rupprecht, and Andrea Vedaldi. Dove: Learning deformable 3d objects by watching videos. arXiv preprint arXiv:2107.10844, 2021.\\n\\n[27] Andrei Zanfir, Eduard Gabriel Bazavan, Hongyi Xu, William T Freeman, Rahul Sukthankar, and Cristian Sminchisescu. Weakly supervised 3d human pose and shape reconstruction with normalizing flows. In ECCV, pages 465\u2013481, 2020.\\n\\n[28] Hongwen Zhang, Jie Cao, Guo Lu, Wanli Ouyang, and Zhenan Sun. Learning 3d human shape and pose from dense body parts. In TPAMI, 2020. early access.\\n\\n[29] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. In CVPR, pages 5745\u20135753, 2019.\\n\\n[30] Silvia Zuffi, Angjoo Kanazawa, Tanya Berger-Wolf, and Michael J Black. Three-D safari: Learning to estimate zebras pose, shape, and texture from images \u201cin the wild\u201d. In ICCV, pages 5359\u20135368, 2019.\\n\\n[31] Silvia Zuffi, Angjoo Kanazawa, and Michael J Black. Lions and tigers and bears: Capturing non-rigid, 3d, articulated shape from images. In CVPR, pages 3955\u20133963, 2018.\\n\\n[32] Silvia Zuffi, Angjoo Kanazawa, David W Jacobs, and Michael J Black. 3D menagerie: Modeling the 3D shape and pose of animals. In CVPR, pages 6365\u20136373, 2017.\"}"}
{"id": "CVPR-2022-158", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3. Training Procedure\\n\\nThe complexity of articulated, deformable 3D model fitting requires a number of different loss functions, as well as careful pretraining.\\n\\nStacked Hourglass Pretraining:\\n\\nThe stacked hourglass is pretrained to predict keypoints and the segmentation map. The StanExt dog dataset provides labels for both. The keypoint loss consists of two parts, a mean squared error (MSE) between the predicted and true heatmaps, and an L2-distance between the predicted and true keypoint coordinates. For the silhouette, we use the cross-entropy between ground truth and predicted masks. As usual for stacked hourglasses, we calculate the losses after every stage.\\n\\nPose-branch Pretraining:\\n\\nWe use the same dataset (RGBD-Dog) that is used to train the pose prior to also pretrain the pose branch. We sample poses and random shapes and project them to a 256 \u00d7 256 image with a random translation and focal length. The projected keypoints and silhouette serve as input to the network. MSE losses are used to penalize deviations between the predicted values and the ground truth. In addition, we use an MSE error between the predicted pose latent representation $y$ and its ground truth.\\n\\nMain Training:\\n\\nThe stacked hourglass is kept fixed, while all other network parameters are jointly optimized. We point out that we do not have access to 3D ground truth, and, based on 2D keypoints, the true shape and pose is ambiguous. To regularize the solution, we therefore combine reprojection losses with suitable priors. These loss terms are described below.\\n\\n3.4. Standard Losses\\n\\nKeypoint Reprojection Loss $L_{kp}$ is the weighted mean squared error between predicted $k_{pred}$ and ground truth 2D keypoint locations $k_{gt}$:\\n\\n$$L_{kp} = \\\\frac{\\\\sum_{n=1}^{N_{kp}} w_n d(k_{pred}, k_{gt})^2}{\\\\sum_{n=1}^{N_{kp}} w_n},$$\\n\\nwhere $d(k_{pred}, k_{gt})$ is the 2D Euclidean distance between the predicted and ground truth location of the $n$-th keypoint.\\n\\nThe weights, $w_n$, balance the influence of keypoints; see Sup. Mat.\\n\\nSilhouette Reprojection Loss $L_{sil}$ is the squared pixel error between the rendered $s_{pred}$ and ground truth silhouette $s_{gt}$:\\n\\n$$L_{sil} = \\\\sum_{x=1}^{256} \\\\sum_{y=1}^{256} (s_{pred} - s_{gt})^2,$$\\n\\nThis is used only for images where the mean keypoint reprojection error $L_{kp,m}$ is below a threshold $T$.\\n\\nShape Prior:\\n\\nThis is a weighted sum of two parts, $L_{sh} = w_\\\\beta L_{sh,\\\\beta} + w_\\\\kappa L_{sh,\\\\kappa}$. The first penalises deviations from a multivariate Gaussian with mean $\\\\mu_{pca}$ and covariance $\\\\Sigma_{pca}$:\\n\\n$$L_{sh,\\\\beta} = (\\\\beta_{pca} - \\\\mu_{pca})^\\\\top \\\\Sigma_{pca}^{-1} (\\\\beta_{pca} - \\\\mu_{pca}).$$\\n\\nAdditionally, we penalise deviations from scale 1 with an element-wise squared loss on the scale factors $\\\\kappa$,\\n\\n$$L_{sh,\\\\kappa} = \\\\sum_{i=1}^{7} \\\\kappa_i^2.$$\\n\\nThe shape prior loss is assigned a low weight and serves only to stabilise the shape against missing evidence.\\n\\nPose Prior:\\n\\n$L_p$ penalises 3D poses that have low likelihood. Again, it consists of two terms, a normalizing flow pose prior as well as a regularization regarding lateral leg movements. The normalizing flow pose prior penalizes the negative log-likelihood of a given pose sample. Since the\"}"}
{"id": "CVPR-2022-158", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"learned latent representation $y$ follows a multivariate normal distribution, the pose prior reduces to:\\n$$L_{\\\\text{pnf}} \\\\propto y^\\\\top y.$$ (5)\\n\\nThe normalizing flow prior is trained on the RGBD-Dog dataset which has a limited set of poses compared to the natural poses in the StanExt dataset. Consequently, with only this prior, the network can infer 3D poses where the legs move unnaturally sideways. Thus, we add a second term $L_{\\\\text{pside}}$ that penalizes sideways poses of the three joints in each leg. The final pose prior is:\\n$$L_{\\\\text{p}} = w_{\\\\text{nf}} L_{\\\\text{pnf}} + w_{\\\\text{side}} L_{\\\\text{pside}},$$ (6)\\n\\nwith weights $w_{\\\\text{nf}}$ and $w_{\\\\text{side}}$, the latter set to a low value.\\n\\n**Camera Prior** $L_{\\\\text{cam}}$: Since focal length $f_{\\\\text{pred}}$ is heavily correlated with depth (object-to-camera distance), we find it useful to penalise the squared deviation from a reasonably predefined target focal length $f_{\\\\text{target}}$:\\n$$L_{\\\\text{cam}} = (f_{\\\\text{pred}} - f_{\\\\text{target}})^2.$$ (7)\\n\\n### 3.5. Novel Breed Losses\\n\\nThe losses described so far do not depend on the breed. To exploit breed labels for the training images, we introduce an additional breed triplet loss, as well as an auxiliary breed classification loss. We summarize those two losses as breed similarity loss. Given the dog meshes used during 3D model learning (Sec. 3.1) we moreover define a specific shape prior for those particular breeds.\\n\\n**Breed Triplet Loss** $L_{\\\\text{Btriplet}}$: Dogs of the same breed usually are somewhat similar in shape. However, this does not imply that there is no intra-class variation, nor that different breeds necessarily have dissimilar shape. Hence, we implement this with a triplet loss. We have experimented with different metric learning losses, but found that they all exhibit similar behaviour. Triplet losses are commonly used in person re-identification (ReID) methods, where the goal is to learn features that are discriminative for person identity \\\\cite{20, 21}. RingNet used a similar idea to learn 3D head shape from images without 3D supervision \\\\cite{19}.\\n\\nApplying the loss directly to the shape $\\\\beta$ does not work well. Shape changes along different principal directions may have different scales, moreover shape changes due to limb scaling are not orthogonal to the PCA coefficients $\\\\beta_{\\\\text{pca}}$. We find it better to apply the triplet loss to the latent encodings $z$. Given a batch with an anchor sample $z_a$, a positive sample $z_p$ of the same breed and a negative sample $z_n$ from a different breed, we calculate the triplet loss,\\n$$L_{\\\\text{Btriplet}} = \\\\sum_{i=1}^{N_{\\\\text{triplets}}} \\\\max(0, d(z_{a,i}, z_{p,i}) - d(z_{n,i}, z_{a,i}) + m),$$ (8)\\n\\nwhere $m$ is the margin and $d$ denotes the distance between the two samples.\\n\\n**Breed Classification Loss** $L_{\\\\text{Bcs}}$: We further bias the estimation towards recognisable, breed-specific shapes with an auxiliary breed classification task, supervised with a standard cross-entropy loss on the breed labels:\\n$$L_{\\\\text{Bcs}} = -\\\\sum_{c=1}^{N_{\\\\text{classes}}} y_{o,c} \\\\log(p_{o,c}),$$ (9)\\n\\nwhere $p_{o,c}$ is the predicted probability that observation $o$ is of class $c$ and $y$ is a binary indicator if label $c$ is the correct class for observation $o$. The full similarity loss reads:\\n$$L_{\\\\text{Bsim}} = w_{\\\\text{triplet}} L_{\\\\text{Btriplet}} + w_{\\\\text{cs}} L_{\\\\text{Bcs}},$$ (10)\\n\\nwhere $w_{\\\\text{triplet}}$ and $w_{\\\\text{cs}}$ are weights.\\n\\n**3D Model Loss** $L_{\\\\text{B3D}}$: We have access to a small number of 3D dogs (Unity models) and a few 3D scans of toy figurines. These models encompass 11 of the 120 breeds in StanExt. For these breeds, we optionally enforce similarity between the prediction and the available 3D ground truth shape, via a component-wise loss on the shape coefficients $\\\\beta$:\\n$$L_{\\\\text{B3D}} = (\\\\beta_{\\\\text{pred}} - \\\\beta_{\\\\text{breed}}^\\\\text{pca})^2 + (\\\\kappa_{\\\\text{pred}} - \\\\kappa_{\\\\text{breed}})^2.$$ (11)\\n\\n### 4. Experiments\\n\\nWe evaluate our approach on the Stanford Extra Dog dataset (StanExt) \\\\cite{1}. StanExt provides labels for 20 keypoints, silhouette annotations and dog breed labels. We extend the 20 keypoints in the training set with withers, throat and eyes. These predictions are obtained by training a separate stacked hourglass on the Animal Pose dataset \\\\cite{3}.\\n\\n#### 4.1. Evaluation Methods\\n\\n**2D Reprojection Error**: In the absence of 3D ground truth, it is common to evaluate 3D shape and pose predictions in terms of reprojection errors in image space. We provide results for intersection over union (IoU) on the silhouette as well as percentage of correct keypoints (PCK).\\n\\n**Perceptual Shape Evaluation**: Many implausible 3D shapes have low 2D reprojection errors, but for in-the-wild images we do not have access to ground-truth 3D shapes that would allow a meaningful comparison. Instead, we run a study to evaluate relative perceptual correctness where humans visually assess the 3D shapes regressed from in-the-wild images. Using Amazon Mechanical Turk (AMT), qualified workers judge which of two rendered 3D body shapes better corresponds to a query dog image. To focus the workers on shape, we render the dogs in the T-pose. For an example and details of the task see Sup. Mat.\\n\\n**Breed Prototype Consistency**: Quantitative evaluation of 3D error for uncontrolled images is challenging. To address this, we exploit the fact that dogs of the same breed\"}"}
{"id": "CVPR-2022-158", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison to SOTA. Numbers for 3D-M, CGAS, and WLDO reproduced from.\\n\\n|     | 3D-M | CGAS | WLDO | Ours |\\n|-----|------|------|------|------|\\n| Avg |  69.9 |  63.5 |  74.2 |  75.7 |\\n| Legs|  69.7 |  28.6 |  78.8 |  83.7 |\\n| Tail|  68.3 |  30.7 |  76.4 |  83.9 |\\n| Ears|  68.0 |  34.5 |  63.9 |  64.1 |\\n| Face|  57.8 |  25.9 |  78.1 |  82.8 |\\n\\n4.2. Comparison to Baselines\\nIn terms of 2D error metrics (IoU and PCK) BARC outperforms prior art, i.e., WLDO, CGAS, and 3D-M. Table 1 summarizes the results. In the perceptual comparison, BARC is also judged to represent the depicted dog better than its closest competitor WLDO, in an overwhelming 90.6% of all cases. See last line of Table 2. The marked gap in visual realism is evident in Fig. 5. More BARC results, for different breeds, are displayed in Fig. 6.\\n\\n4.3. Ablation Study\\nOur key contribution is the addition of breed losses to improve 3D shape regression. To ablate the impact of individual loss terms, 2D errors are not meaningful, so we again report results in terms of relative perceptual correctness (Table 2) and in terms of consistency with the prototype breed shape (Table 3). We compare three versions of our method: (i) our network, trained without any breed losses; (ii) the same network with the breed similarity losses \\\\(L_{\\\\text{sim}}\\\\), i.e., classification and triplet loss; (iii) with all breed losses, including the 3D model loss \\\\(L_{\\\\text{3D}}\\\\). The results, on both metrics, show consistent improvements with the addition of each breed loss. In terms of perceptual agreement, the two parts of our loss have similar impact. The triplet and classification losses bring a clear improvement, even though they do not explicitly constrain 3D shape. Breed-specific 3D shape information can further improve the prediction, but may be difficult to collect at large scale. Note that adding 3D CG models as additional supervision leads to a small improvement (on average) across all breeds, even though they are only available for 11 out of 120 breeds. All differences in votes are highly significant (\\\\(\\\\chi^2\\\\)-test, \\\\(p < 0.0001\\\\)).\"}"}
{"id": "CVPR-2022-158", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. BARC results. Each row shows the input image with the projected 3D shape. Below that is a rendering of the posed 3D shape.\\n\\nTable 3. 3D Shape Evaluation. Average over 20 breeds.\\n\\nTable 4. Ablation study. Breed prototype consistency error for different setting.\\n\\nTo make the influence of the breed information more tangible, we also visualize the effect of the breed similarity loss. Figure 3 shows a t-SNE visualization of the latent feature spaces learned by (left) a network without $L_{Bsim}$ and (middle, right) an identical network trained with $L_{Bsim}$. The breed similarity pulls dogs of the same breed closer together in the latent space $z$, which is closely linked to the body shape parameters $\\\\beta$. Different saturation levels of the same color indicate breeds within the clade. Even though the notion of clades is not imposed or made explicit anywhere in our network, breeds of the same clade tend to cluster. This suggests that not only within breeds, but also above breed level, shape knowledge can be transferred.\\n\\n5. Conclusion\\n\\nWe present a method to reconstruct 3D pose and shape of dogs from images. Monocular 3D reconstruction of articulated objects is an unconstrained problem that requires strong priors on 3D shape and pose. We overcome the limitation of current 3D shape models of animals by training for model-based shape prediction with a novel breed-aware loss. We obtain state-of-the-art estimates of 3D dog shape and pose from images while also producing consistent, breed-specific 3D shape reconstructions. Our results outperform previous work metrically and perceptually. Combining visual appearance and genetic information through breed labels, we obtain a latent space that expresses relations between different breeds in accordance with recent studies on the dog breed genome. We believe this is the first work that combines breed information for learning to reconstruct 3D animal shape, and we hope it will be the basis of further investigation for other species.\\n\\nLimitations and Ethics.\\n\\nBARC is limited by its shape space and is not able to go outside it. Given the high-quality regression results, future work should explore learning an improved shape space from images by exploiting breed constraints. We focused mainly on shape, but pose and motion are also important, and learning models of these from image data may be possible using our methods. Our research uses public image sources of dogs, and no animal experiments were conducted. While we focus on dogs, our method should be applicable to other animals and may eventually find positive uses in conservation, animal science, veterinary medicine, etc.\\n\\nAknowledgements.\\n\\nThis research was supported by the Max Planck ETH Center for Learning Systems. A conflict of interest disclosure for Michael J. Black can be found here https://files.is.tue.mpg.de/black/CoI_CVPR_2022.txt.\"}"}
