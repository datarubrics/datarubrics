{"id": "CVPR-2022-638", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"EyePAD++: A Distillation-based approach for joint Eye Authentication and Presentation Attack Detection using Periocular Images\\n\\nPrithviraj Dhar1, Amit Kumar2, Kirsten Kaplan2, Khushi Gupta2, Rakesh Ranjan2, Rama Chellappa1\\n\\n1Johns Hopkins University, 2Reality Labs, Meta\\n\\n{pdhar1,rchella4}@jhu.edu, {akumar14,kkaplan,khushigupta,rakeshr}@fb.com\\n\\nAbstract\\nA practical eye authentication (EA) system targeted for edge devices needs to perform authentication and be robust to presentation attacks, all while remaining compute and latency efficient. However, existing eye-based frameworks a) perform authentication and Presentation Attack Detection (PAD) independently and b) involve significant pre-processing steps to extract the iris region. Here, we introduce a joint framework for EA and PAD using periocular images. While a deep Multitask Learning (MTL) network can perform both the tasks, MTL suffers from the forgetting effect since the training datasets for EA and PAD are disjoint. To overcome this, we propose Eye Authentication with PAD (EyePAD), a distillation-based method that trains a single network for EA and PAD while reducing the effect of forgetting. To further improve the EA performance, we introduce a novel approach called EyePAD++ that includes training an MTL network on both EA and PAD data, while distilling the 'versatility' of the EyePAD network through an additional distillation step. Our proposed methods outperform the SOTA in PAD and obtain near-SOTA performance in eye-to-eye verification, without any pre-processing. We also demonstrate the efficacy of EyePAD and EyePAD++ in user-to-user verification with PAD across network backbones and image quality.\\n\\n1. Introduction\\nEye Authentication (EA) using irises has been widely used for biometric authentication. With the current advancements in head-mounted technology, eye-based authentication is likely to become an essential part of authenticating users against their wearable devices. While highly accurate, EA systems are also vulnerable to 'Presentation Attacks' [14, 47]. These attacks seek to fool the authentication system by presenting artificial eye images, such as printed iris images of an individual [5, 16], or cosmetic contacts [21]. While researchers have proposed methods to train networks that achieve SOTA performance in either Iris segmentation\\nNormalization and enhancement\\nIris segmentation/detection\\n\\nFigure 1. EA pipelines segment out the iris region from the periocular image and normalize the iris image before feeding it to the network. Segmentation/detection is also used in most PAD pipelines. We train a single network for both EA and PAD without any pre-processing steps, using the entire periocular image. EA or PAD, a practical eye-based biometric system that can be used on edge devices must be able to perform both of these tasks accurately and simultaneously, with low latency and in an energy efficient manner. Therefore, in this work we propose strategies to develop a single deep network for both EA and PAD using periocular images.\\n\\nOne of the key steps in EA is pre-processing. Most EA frameworks [45, 53] use an auxiliary segmentation network to extract the iris region from the periocular image. The iris is then unwrapped into a rectangular image and is fed to the eye authentication system. This geometric normalization step was first proposed in [7]. Pre-processing is also an important step in iris PAD pipelines that requires another segmentation network [14, 47], or a third party iris detection software [38, 41]. Such pre-processing steps potentially make the EA and PAD pipelines computationally expensive, making it impractical to embed these biometric systems on edge devices with limited computational resources. We investigate and propose techniques to perform EA and PAD using the entire periocular image without any active pre-processing. In doing so, we adhere to our goal of using a single network in a truer sense.\"}"}
{"id": "CVPR-2022-638", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For a given subject, irises of left and right eyes demonstrate different textural patterns. In most of the existing works in EA [45, 53], the CNN models are trained on the left irises for classification. During evaluation, a right iris is verified against the right irises of the same or other subject. We refer to this evaluation method as 'eye-to-eye verification'. However, a more practical protocol would be to perform user-to-user verification, i.e. consider both left and right eyes of a given test subject (query user) and verify it against one or more pairs of left-right irises of same or different user (i.e. gallery user). To this end, we propose a new evaluation protocol to match the left-right pair of a query user with that of a gallery user.\\n\\nWe consider the problem of EA and PAD as a disjoint multitask learning problem because the authentication task presumes real images, which is why the current datasets for EA do not include PAD labels. A possible single-network solution is to train a deep multitask network for both tasks, alternatingly training the EA and PAD branches with their respective dataset each iteration (as done in [37]). However, several works [18, 22, 27] have shown that Multitask Learning (MTL) frameworks for disjoint tasks demonstrate forgetting effect (see Sec. 3). Hence, we propose two novel knowledge distillation-based techniques called Eye-PAD and EyePAD++ to incrementally learn EA and PAD tasks, while minimizing the forgetting effect. In summary, we make the following contributions in this work:\\n\\n1. We propose a user-to-user verification protocol that can be used to authenticate one query user against one or many samples of a gallery user. This is more practical than the existing protocol for eye-to-eye verification.\\n\\n2. To the best of our knowledge, we are the first to explore the problem of EA and PAD using a single network. We introduce a new metric called Overall False Rejection Rate (OFRR) to evaluate the performance of the entire system (EA and PAD), using only authentication data.\\n\\n3. We propose a distillation-based method called Eye-Authorization with Presentation Attack Detection (Eye-PAD) for jointly performing EA and PAD. To further improve the verification performance, we propose EyePAD++. EyePAD++ inherits the versatility of the Eye-PAD network through distillation and combines it with the specificity of multitask learning. EyePAD++ consistently outperforms the existing baselines for MTL, in terms of OFRR. We show the efficacy of Eye-PAD and EyePAD++ across different network backbones (Densenet121 [20], MobilenetV3 [19] and HR-net64 [44]), and image quality degradation (blur and noise). Additionally, we apply our methods to jointly perform eye-to-eye verification and PAD, following the commonly used train-test protocols. Although the current SOTA approaches use pre-processing, our proposed method outperforms the existing SOTA in PAD task, and obtain comparable user-to-user verification performance without any pre-processing.\\n\\n### Table 1. Pre-processing steps in recent EA/PAD frameworks\\n\\n| Method               | Pre-processing Steps |\\n|----------------------|----------------------|\\n| IrisCode [29]        | Segmentation, geometric normalization |\\n| Ordinal [42]         | Segmentation, geometric normalization |\\n| UniNet [53]          | Segmentation, geometric normalization |\\n| DRFnet [45]          | Segmentation, geometric normalization |\\n| [35]                 | Segmentation, geometric normalization |\\n| [14]                 | Segmentation with UIST [38] |\\n| D-net-PAD [41]       | Detection with VeriEye [3], A-PBS [11] |\\n| EyePAD (ours)        | None |\\n| EyePAD++ (ours)      | None |\"}"}
{"id": "CVPR-2022-638", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Authentication dataset\\n\\nDisjoint Multitask Learning and Knowledge Distillation:\\nDisjoint multitask learning (MTL) is the process of training a network to perform multiple tasks using data samples that have labels for either of the tasks, but not for all the tasks. Training a single network for EA and PAD is a disjoint MTL task because EA datasets do not include PAD labels. One solution is to follow the existing disjoint multitask learning strategies [24, 28, 37] and update each branch of the network alternately. However, it is well known [22, 27] that alternating training suffers from the forgetting effect [27] and degrades performance in multitask learning. Knowledge Distillation (KD) [15] has been commonly used to reduce forgetting in continual learning [9, 27, 39, 40, 52]. Inspired by this, [22, 26] employ feature-level KD for multitasking. In [22], KD is used to distill the information from the network from a previous iteration \\\\(i-1\\\\) that was updated for task A (teacher), while training it to perform task B in the current iteration \\\\(i\\\\) (student). However, in this scenario, the teacher network is not fully trained in the initial few iterations and thus the distillation step may not help preserve task A information. Similar to [22], we propose strategies employing feature-level KD for disjoint multitasking (EA and PAD). But, unlike [22], we ensure that the teacher network in our proposed methods is fully trained in one or more tasks.\\n\\n3. Proposed approach\\nOur objective is to build a single network that is proficient in performing two disjoint tasks: EA and PAD. We intend to build this framework for edge devices with limited on-device compute. Thus, we exclude any pre-processing step for detecting or segmenting the iris region and use the entire periocular image as input. Mutitask Learning (MTL) is a possible approach in this scenario. Most of the MTL methods for disjoint tasks [37] alternately feed the data from different tasks. However, as shown in [22], MTL demonstrats the forgetting effect. Consider an MTL network with shared backbone and different heads designed to perform two tasks A and B. Suppose that the training batches for task A and B are fed to this MTL network alternately. Here, the weights of the shared backbone modified by the gradients corresponding to the loss for task A in iteration \\\\(i\\\\), may be rewritten in the next iteration \\\\((i+1)\\\\) by the gradients corresponding to the loss for task B. This may lead to forgetting of task A. Therefore, instead of MTL, we propose to use knowledge distillation to learn both tasks through a single network. Here, we intend to first train a teacher network \\\\(M_t\\\\) for EA, following which we train a student network \\\\(M_s\\\\) for PAD, while distilling the authentication information from \\\\(M_t\\\\) to \\\\(M_s\\\\) to minimize the forgetting effect.\\n\\n3.1. Eye Authentication with Presentation Attack Detection (EyePAD) and EyePAD++\\nWe now explain the steps in our proposed methods: EyePAD and EyePAD++ (Fig. 2b):\\nStep 1: We train the teacher network \\\\(M_t\\\\) using periocular images from the EA dataset to perform EA. Similar to [45], we use triplet loss to train \\\\(M_t\\\\). We first extract features \\\\(f_i\\\\) for all the images using the penultimate layer of \\\\(M_t\\\\). To select the \\\\(n\\\\)th triplet in a given batch, we randomly select an anchor feature \\\\(f(a)\\\\) belonging to category \\\\(C\\\\). After that, we select the hardest positive feature \\\\(f(pos)\\\\) and the hardest negative feature \\\\(f(neg)\\\\) as follows:\\n\\\\[\\nf(pos) = \\\\arg\\\\max_{i \\\\in \\\\mathcal{C}, i \\\\neq a} \\\\left( \\\\|f_i - f(a)\\\\|_2 \\\\right), \\\\quad f(neg) = \\\\arg\\\\min_{i \\\\notin \\\\mathcal{C}} \\\\left( \\\\|f_i - f(a)\\\\|_2 \\\\right)\\n\\\\]\\nThen we compute the triplet loss \\\\(L_{id}\\\\) for the entire batch (of size \\\\(N\\\\)) as:\\n\\\\[\\nL_{id} = \\\\frac{1}{N} \\\\sum_{n=1}^{N} \\\\max \\\\left( \\\\|f(pos) - f(a)\\\\|_2 - \\\\|f(neg) - f(a)\\\\|_2 + \\\\alpha, 0 \\\\right)\\n\\\\]\\nwhere \\\\(\\\\alpha\\\\) denotes the distance margin.\\nStep 2 (Feature-level knowledge distillation - EyePAD): We initialize a student network \\\\(M_s\\\\) using \\\\(M_t\\\\), and train it for PAD. Let \\\\(I\\\\) be an image from the PAD dataset. \\\\(I\\\\) is fed to \\\\(M_t\\\\). The student network \\\\(M_s\\\\) is trained to perform PAD while distilling the 'versatility' of \\\\(M_s\\\\) from \\\\(M_t\\\\).\"}"}
{"id": "CVPR-2022-638", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"both $M_t$ and $M_s$, to obtain features $f_t$ and $f_s$, extracted using the penultimate layer of the corresponding networks. To constrain $M_s$ to process an eye image like $M_t$, we employ feature-level KD and minimize the cosine distance between $f_t$ and $f_s$ using the proposed distillation loss $L_{\\\\text{dis}}$.\\n\\n$$L_{\\\\text{dis}}(f_s, f_t) = 1 - \\\\frac{f_s \\\\cdot f_t}{\\\\|f_s\\\\| \\\\|f_t\\\\|}.$$  \\n\\n(2)\\n\\nOur application of feature-level KD is inspired by [40]. Note that we do not apply KD on the output scores as done in [27], since eye-based matching protocols like [45] use the features from the penultimate layer (and not the output score vector). Additionally, we would like $M_s$ to classify a given image as live (also referred to as 'real' or 'bona-fide') or spoof using $L_{\\\\text{pad}}$, which is a standard cross-entropy classification loss. Combining these constraints, we train $M_s$ using the multitask classification loss $L_{\\\\text{multi}}$ as\\n\\n$$L_{\\\\text{multi}} = L_{\\\\text{pad}} + \\\\lambda_1 L_{\\\\text{dis}},$$  \\n\\n(3)\\n\\nwhere $\\\\lambda_1$ is used to weight $L_{\\\\text{dis}}$. In this step, the teacher $M_t$ remains frozen. To evaluate the verification performance, we use features extracted from the penultimate layer of the trained $M_s$ for the test EA data and perform user-to-user verification. We feed the test PAD data to $M_s$ and evaluate its performance in live/spoof classification. We find that the student network $M_s$ obtained from EyePAD is a versatile network that is effective for both EA and PAD. However, compared to $M_t$ (that was only trained for EA), $M_s$ obtains slightly lower verification performance. We hypothesize that $M_s$ demonstrates this drop in performance because it was never trained for EA. Therefore, we introduce an additional step to train an MTL network (initialized with $M_s$) while distilling the versatility of the EyePAD student to this network (Fig. 2b).\\n\\nStep 3 (EyePAD++): We initialize a new student network $M^*_s$ using $M_s$. $M^*_s$ is trained for both EA and PAD in an MTL fashion. Following the commonly used strategies for disjoint multitasking [37], the batches from EA and PAD data are alternated after every iteration. To reduce forgetting, we additionally constrain $M^*_s$ to mimic $M_s$, which acts as its teacher, using the same knowledge distillation used in step 2. We feed the training image to both $M_s$ and $M^*_s$ and obtain features $f_s$ and $f^*_s$ respectively. $M_s$ remains frozen in this step. We use them to compute $L_{\\\\text{dis}}$ as:\\n\\n$$L_{\\\\text{dis}}(f_s, f^*_s) = 1 - \\\\frac{f_s \\\\cdot f^*_s}{\\\\|f_s\\\\| \\\\|f^*_s\\\\|}.$$  \\n\\n(4)\\n\\nWhen authentication data is fed to $M^*_s$ (say, during iteration $i$), we compute $L_{\\\\text{id}}$ as follows:\\n\\n$$L_{\\\\text{id}} = L_{\\\\text{id}} + \\\\lambda_2 L_{\\\\text{dis}},$$  \\n\\n(5)\\n\\nHere, $L_{\\\\text{id}}$ is the triplet loss from Eq. 1. When PAD data is fed to $M^*_s$ (during iteration $i+1$), we compute $L_{\\\\text{pad}}$ as:\\n\\n$$L_{\\\\text{pad}} = L_{\\\\text{pad}} + \\\\lambda_2 L_{\\\\text{dis}}.$$  \\n\\n(6)\\n\\nFigure 3. We perform train and test our networks on (a) the original (clean) datasets, (b) their blurred and (c) their noisy versions.\\n\\nwhere $L_{\\\\text{pad}}$ is a standard classification loss used in step 2 of EyePAD. Thus, $L_{\\\\text{id}}$ and $L_{\\\\text{pad}}$ are alternately used to optimize $M^*_s$. For inference, we use the trained $M^*_s$ for user-to-user verification and PAD. Hyperparameter details for EyePAD and EyePAD++ are provided in the supplementary material.\\n\\n4. Experiments\\n\\n4.1. Baseline methods\\n\\nSingle task networks: To estimate the standard user-to-user verification (EA) and PAD performance, we test the 'EA only' and 'PAD only' networks. The 'EA only' network is the teacher network $M_t$ used in Step 1 of EyePAD. Multitask Learning (MTL): We train a multitask network for EA and PAD by alternately feeding EA and PAD batches (see step 3 of EyePAD++) and alternately optimizing using $L_{\\\\text{pad}}$ (Sec. 3.1) and $L_{\\\\text{id}}$ (Eq. 1).\\n\\nMulti-teacher Multitasking (MTMT) [26]: MTMT [26] is a recently proposed multitask framework that combines MTL with multi-teacher knowledge distillation. MTMT has been shown to outperform MTL and other SOTA multitask methods such as GradNorm [4]. Here, single task networks are first trained in specific tasks. An MTL network $M$ is then trained for multiple tasks, while information from the single task networks is distilled into $M$. A key difference between MTMT and EyePAD++ is that MTMT enforces distillation from multiple task-specific teachers whereas EyePAD++ includes distillation from a single teacher that is proficient in performing multiple tasks. We implement MTMT as one of our baselines for joint EA and PAD (Fig. 2a). Firstly, we train two single task models: $M_{\\\\text{auth}}$ for EA and $M_{\\\\text{pad}}$ for PAD, and then distill information from them while training a student MTL network $M$. We use the same feature-level distillation used in EyePAD (Step 2) and EyePAD++. A given image is fed to $M_{\\\\text{auth}}$, $M$, and $M_{\\\\text{pad}}$, generating features from the penultimate layers $f_{\\\\text{auth}}$, $f_M$ and $f_{\\\\text{pad}}$, respectively. $L_{\\\\text{dis}}$ then constrains $f_M$ to be closer to $f_{\\\\text{auth}}$ and $f_{\\\\text{pad}}$. $M_{\\\\text{auth}}$ and $M_{\\\\text{pad}}$ remain frozen in this step. We alternately feed the training batches for EA and PAD. So, when EA data is forwarded to $M_{\\\\text{auth}}$, $M_{\\\\text{pad}}$, $M$, we optimize $M$ using $L_{\\\\text{id}}_{\\\\text{mtmt}}$.\\n\\n$$L_{\\\\text{id}}_{\\\\text{mtmt}} = L_{\\\\text{id}} + \\\\lambda_{\\\\text{auth}} L_{\\\\text{dis}}(f_{\\\\text{auth}}, f_M) + \\\\lambda_{\\\\text{pad}} L_{\\\\text{dis}}(f_{\\\\text{pad}}, f_M)$$  \\n\\n(7)\\n\\nHere $L_{\\\\text{id}}$ is the triplet loss defined in Eq. 1. $\\\\lambda_{\\\\text{auth}}$, $\\\\lambda_{\\\\text{pad}}$ denote the distillation weights from teacher $M_{\\\\text{auth}}$ and $M_{\\\\text{pad}}$.\"}"}
{"id": "CVPR-2022-638", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Kevin W Bowyer and Patrick J Flynn. The ND-IRIS-0405 iris image dataset. arXiv preprint arXiv:1606.04853, 2016.\\n\\n[2] Cunjian Chen and Arun Ross. A multi-task convolutional neural network for joint iris detection and presentation attack detection. In 2018 IEEE Winter Applications of Computer Vision Workshops (WACVW), pages 44\u201351. IEEE, 2018.\\n\\n[3] Cunjian Chen and Arun Ross. An explainable attention-guided iris presentation attack detector. In WACV (Workshops), pages 97\u2013106, 2021.\\n\\n[4] Zhao Chen, Vijay Badrinarayanan, Chen-Yu Lee, and Andrew Rabinovich. Gradnorm: Gradient normalization for adaptive loss balancing in deep multitask networks. In International Conference on Machine Learning, pages 794\u2013803. PMLR, 2018.\\n\\n[5] Adam Czajka. Database of iris printouts and its application: Development of liveness detection method for iris recognition. In 2013 18th International Conference on Methods & Models in Automation & Robotics (MMAR), pages 28\u201333. IEEE, 2013.\\n\\n[6] John Daugman. How iris recognition works. In The essential guide to image processing, pages 715\u2013739. Elsevier, 2009.\\n\\n[7] John G Daugman. High confidence visual recognition of persons by a test of statistical independence. IEEE transactions on pattern analysis and machine intelligence, 15(11):1148\u20131161, 1993.\\n\\n[8] P Dhar, C Castillo, and R Chellappa. On measuring the iconicity of a face. In 2019 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 2137\u20132145. IEEE, 2019.\\n\\n[9] Prithviraj Dhar, Rajat Vikram Singh, Kuan-Chuan Peng, Ziyan Wu, and Rama Chellappa. Learning without memorizing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5138\u20135146, 2019.\\n\\n[10] Meiling Fang, Naser Damer, Fadi Boutros, Florian Kirchbuchner, and Arjan Kuijper. Deep learning multi-layer fusion for an accurate iris presentation attack detection. In 2020 IEEE 23rd International Conference on Information Fusion (FUSION), pages 1\u20138. IEEE, 2020.\\n\\n[11] Meiling Fang, Naser Damer, Fadi Boutros, Florian Kirchbuchner, and Arjan Kuijper. Iris presentation attack detection by attention-based and deep pixel-wise binary supervision network. In 2021 IEEE International Joint Conference on Biometrics (IJCB), pages 1\u20138. IEEE, 2021.\\n\\n[12] Abhishek Gangwar and Akanksha Joshi. Deepirisnet: Deep iris representation with applications in iris recognition and cross-sensor iris recognition. In 2016 IEEE international conference on image processing (ICIP), pages 2301\u20132305. IEEE, 2016.\\n\\n[13] Fei He, Ye Han, Han Wang, Jinchao Ji, Yuanning Liu, and Zhiqiang Ma. Deep learning architecture for iris recognition based on optimal gabor filters and deep belief network. Journal of Electronic Imaging, 26(2):023005, 2017.\\n\\n[14] Lingxiao He, Haiqing Li, Fei Liu, Nianfeng Liu, Zhenan Sun, and Zhaofeng He. Multi-patch convolution neural network for iris liveness detection. In 2016 IEEE 8th International Conference on Biometrics Theory, Applications and Systems (BTAS), pages 1\u20137. IEEE, 2016.\\n\\n[15] Geoffrey Hinton, Oriol Vinyals, and Jeffrey Dean. Distilling the knowledge in a neural network. In NIPS Deep Learning and Representation Learning Workshop, 2015.\\n\\n[16] Steven Hoffman, Renu Sharma, and Arun Ross. Convolutional neural networks for iris presentation attack detection: Toward cross-dataset and cross-sensor generalization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 1620\u20131628, 2018.\\n\\n[17] Steven Hoffman, Renu Sharma, and Arun Ross. Iris+ ocular: Generalized iris presentation attack detection using multiple convolutional neural networks. In 2019 International Conference on Biometrics (ICB), pages 1\u20138. IEEE, 2019.\\n\\n[18] Yan Hong, Li Niu, Jianfu Zhang, and Liqing Zhang. Beyond without forgetting: Multi-task learning for classification with disjoint datasets. In 2020 IEEE International Conference on Multimedia and Expo (ICME), pages 1\u20136. IEEE, 2020.\\n\\n[19] Andrew Howard, Mark Sandler, Grace Chu, Liang-Chieh Chen, Bo Chen, Mingxing Tan, Weijun Wang, Yukun Zhu, Ruoming Pang, Vijay Vasudevan, et al. Searching for mobilenetv3. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1314\u20131324, 2019.\\n\\n[20] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 4700\u20134708, 2017.\\n\\n[21] Ken Hughes and Kevin W Bowyer. Detection of contact-lens-based iris biometric spoofs using stereo imaging. In 2013 46th Hawaii International Conference on System Sciences, pages 1763\u20131772. IEEE, 2013.\\n\\n[22] Dong-Jin Kim, Jinsoo Choi, Tae-Hyun Oh, Youngjin Yoon, and In So Kweon. Disjoint multi-task learning between heterogeneous human-centric tasks. In 2018 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1699\u20131708. IEEE, 2018.\\n\\n[23] Gabriela Y Kimura, Diego R Lucio, Alceu S Britto Jr, and David Menotti. CNN hyperparameter tuning applied to iris liveness detection. arXiv preprint arXiv:2003.00833, 2020.\\n\\n[24] Iasonas Kokkinos. Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 6129\u20136138, 2017.\\n\\n[25] Andrey Kuehlkamp, Allan Pinto, Anderson Rocha, Kevin W Bowyer, and Adam Czajka. Ensemble of multi-view learning classifiers for cross-domain iris presentation attack detection. IEEE Transactions on Information Forensics and Security, 14(6):1419\u20131431, 2018.\\n\\n[26] Wei-Hong Li and Hakan Bilen. Knowledge distillation for multi-task learning. In European Conference on Computer Vision, pages 163\u2013176. Springer, 2020.\"}"}
{"id": "CVPR-2022-638", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[28] An-An Liu, Yu-Ting Su, Wei-Zhi Nie, and Mohan Kankanhalli. Hierarchical clustering multi-task learning for joint human action grouping and recognition. IEEE transactions on pattern analysis and machine intelligence, 39(1):102\u2013114, 2016.\\n\\n[29] Libor Masek et al. Recognition of human iris patterns for biometric identification. PhD thesis, Citeseer, 2003.\\n\\n[30] B Maze, J Adams, J A Duncan, N Kalka, T Miller, C Otto, A K Jain, W T Niggel, J Anderson, J Cheney, et al. IARPA janus benchmark-c: Face dataset and protocol. In 2018 International Conference on Biometrics (ICB), pages 158\u2013165. IEEE, 2018.\\n\\n[31] David Menotti, Giovani Chiachia, Allan Pinto, William Robinson Schwartz, Helio Pedrini, Alexandre Xavier Falcao, and Anderson Rocha. Deep representations for iris, face, and fingerprint spoofing detection. IEEE Transactions on Information Forensics and Security, 10(4):864\u2013879, 2015.\\n\\n[32] Kien Nguyen, Clinton Fookes, Arun Ross, and Sridha Sridharan. Iris recognition with off-the-shelf cnn features: A deep learning perspective. IEEE Access, 6:18848\u201318855, 2017.\\n\\n[33] Federico Pala and Bir Bhanu. Iris liveness detection by relative distance comparisons. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 162\u2013169, 2017.\\n\\n[34] P Jonathon Phillips, W Todd Scruggs, Alice J O\u2019Toole, Patrick J Flynn, Kevin W Bowyer, Cathy L Schott, and Matthew Sharpe. Frvt 2006 and ice 2006 large-scale experimental results. IEEE transactions on pattern analysis and machine intelligence, 32(5):831\u2013846, 2009.\\n\\n[35] Ramachandra Raghavendra, Kiran B Raja, and Christoph Busch. Contlensnet: Robust iris contact lens detection using deep convolutional neural networks. In 2017 IEEE Winter Conference on Applications of Computer Vision (WACV), pages 1160\u20131167. IEEE, 2017.\\n\\n[36] R Ranjan, A Bansal, J Zheng, H Xu, J Gleason, B Lu, A Nandan, J-C Chen, C D Castillo, and R Chellappa. A fast and accurate system for face detection, identification, and verification. IEEE Transactions on Biometrics, Behavior, and Identity Science, 1(2):82\u201396, 2019.\\n\\n[37] R Ranjan, S Sankaranarayanan, C D Castillo, and R Chellappa. An all-in-one convolutional neural network for face analysis. In 2017 12th IEEE International Conference on Automatic Face & Gesture Recognition (FG 2017), pages 17\u201324. IEEE, 2017.\\n\\n[38] Christian Rathgeb, Andreas Uhl, Peter Wild, and Heinz Hofbauer. Design decisions for an iris recognition sdk. In Handbook of iris recognition, pages 359\u2013396. Springer, 2016.\\n\\n[39] Sylvestre-Alvise Rebuffi, Alexander Kolesnikov, Georg Sperl, and Christoph H Lampert. iCaRL: Incremental classifier and representation learning. In Proc. CVPR, 2017.\\n\\n[40] Adriana Romero, Nicolas Ballas, Samira Ebrahimi Kahou, Antoine Chassang, Carlo Gatta, and Yoshua Bengio. Fitnets: Hints for thin deep nets. ICLR 2015, 2015.\\n\\n[41] Renu Sharma and Arun Ross. D-netpad: An explainable and interpretable iris presentation attack detector. In 2020 IEEE International Joint Conference on Biometrics (IJCB), pages 1\u201310. IEEE, 2020.\\n\\n[42] Zhenan Sun and Tieniu Tan. Ordinal measures for iris recognition. IEEE Transactions on pattern analysis and machine intelligence, 31(12):2211\u20132226, 2008.\\n\\n[43] Xingqiang Tang, Jiangtao Xie, and Peihua Li. Deep convolutional features for iris recognition. In Chinese conference on biometric recognition, pages 391\u2013400. Springer, 2017.\\n\\n[44] Jingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. IEEE transactions on pattern analysis and machine intelligence, 2020.\\n\\n[45] Kuo Wang and Ajay Kumar. Toward more accurate iris recognition using dilated residual features. IEEE Transactions on Information Forensics and Security, 14(12):3233\u20133245, 2019.\\n\\n[46] Daksha Yadav, Naman Kohli, Akshay Agarwal, Mayank Vatsa, Richa Singh, and Afzel Noore. Fusion of handcrafted and deep learning features for large-scale multiple iris presentation attack detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 572\u2013579, 2018.\\n\\n[47] Daksha Yadav, Naman Kohli, Mayank Vatsa, Richa Singh, and Afzel Noore. Detecting textured contact lens in uncontrolled environment using densepad. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 0\u20130, 2019.\\n\\n[48] David Yambay, Benedict Becker, Naman Kohli, Daksha Yadav, Adam Czajka, Kevin W Bowyer, Stephanie Schuckers, Richa Singh, Mayank Vatsa, Afzel Noore, et al. Livdet iris 2017\u2014iris liveness detection competition 2017. In 2017 IEEE International Joint Conference on Biometrics (IJCB), pages 733\u2013741. IEEE, 2017.\\n\\n[49] David Yambay, James S. Doyle, Kevin W. Bowyer, Adam Czajka, and Stephanie Schuckers. Livdet-iris 2013 - iris liveness detection competition 2013. In IEEE International Joint Conference on Biometrics, pages 1\u20138, 2014.\\n\\n[50] David Yambay, Brian Walczak, Stephanie Schuckers, and Adam Czajka. Livdet-iris 2015 - iris liveness detection competition 2015. In 2017 IEEE International Conference on Identity, Security and Behavior Analysis (ISBA), pages 1\u20136, 2017.\\n\\n[51] Kai Yang, Zihao Xu, and Jingjing Fei. Dualsanet: Dual spatial attention network for iris recognition. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 889\u2013897, 2021.\\n\\n[52] Sergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. In ICLR, 2017.\\n\\n[53] Zijing Zhao and Ajay Kumar. Towards more accurate iris recognition using deeply learned spatially corresponding features. In Proceedings of the IEEE international conference on computer vision, pages 3809\u20133818, 2017.\"}"}
{"id": "CVPR-2022-638", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The datasets we use in this work are academic datasets with high quality images. We build the PAD test dataset by combining the official test splits of CU-LivDet and ND-LivDet from the 2013, 2015, and 2017 challenges. To enable other researchers to replicate our experiments, we provide the train and test splits in the supplementary material.\\n\\nWe further degrade the image quality. Therefore, in addition to the original datasets, we also perform experiments by degrading the datasets (separately): (i) Blur: We add Gaussian blur with a random kernel size between 1 and 5 to the test images. (ii) Noise: We add additive white Gaussian noise with a standard deviation $\\\\sigma$ to the training images, and add blur with kernel size of 5 to the test images.\\n\\nAlso, environmental conditions like lighting may further degrade the image quality. Therefore, in addition to the original datasets, we also perform experiments by degrading the datasets (separately): (i) Blur: We add Gaussian blur with a random kernel size between 1 and 5 to the test images. (ii) Noise: We add additive white Gaussian noise with a standard deviation $\\\\sigma$ to the training images, and add blur with kernel size of 5 to the test images.\\n\\nUsing the original datasets, we also perform experiments by degrading the datasets (separately): (i) Blur: We add Gaussian blur with a random kernel size between 1 and 5 to the test images. (ii) Noise: We add additive white Gaussian noise with a standard deviation $\\\\sigma$ to the training images, and add blur with kernel size of 5 to the test images.\\n\\n### Datasets and network architectures used\\n\\nWe use the ND-Iris-0405 [1, 34] dataset, used widely for eye authentication using irises. The dataset consists of 356 users that are divided into two subsets: (1) 206 users from 2013, and (2) 150 users from 2017.\\n\\nFor PAD training data, we combine the official test splits of CU-LivDet and ND-LivDet from the 2013, 2015, and 2017 challenges. CU-LivDet consists of three categories: Live, patterned lens, and printed images. ND-LivDet consists of two categories: Live and patterned lens. We summarize the datasets used in our work in Table 2.\\n\\n| Dataset                  | User-to-user verification (1 Query, 1 Gallery) | User-to-user verification (1 Query, 1 Gallery) |\\n|--------------------------|-----------------------------------------------|-----------------------------------------------|\\n| ND-Iris-0405 (2013)      | 1306 gallery (2013)                           | 2925 query (2013)                             |\\n| ND-Iris-0405 (2015)      | 4231 gallery (2015)                           | 7532 query (2015)                             |\\n| ND-Iris-0405 (2017)      | 7949 gallery (2017)                           | 14600 query (2017)                            |\\n| CU-LivDet (2013)         | 5222 gallery (2013)                           | 10444 query (2013)                            |\\n| CU-LivDet (2015)         | 10444 gallery (2015)                          | 20888 query (2015)                            |\\n| CU-LivDet (2017)         | 20888 gallery (2017)                          | 41776 query (2017)                            |\\n\\n### Network architectures used\\n\\nWe provide the hyperparameter information for the baselines using the Densenet121 backbone [20]. This is motivated by the fact that we use the HRnet64 [44] and MobilenetV3 [19] networks used in the experiments using the HRnet64 [44] and MobilenetV3 [19].\\n\\n### User to user verification protocol\\n\\nFor a given user $u$ in user set $U$, we select 10 left and 10 right eye images to build the gallery set $G_u$. Similarly, we select 5 left and 5 right eye images to build the query set $Q_u$. For each query image, we compute the feature $f_q$ and then compute their respective features $f_q(L,A)$ and $f_q(R,B)$. To enable other researchers to replicate our experiments, we provide the detailed user-to-user verification protocol in Protocol 1. To match query user A to gallery user B, we compute the features for the left and right query images and average them to compute a single feature $f_q(L,A)$. We then feed the left and right query images to a model that optimizes similarity for user A and user B, respectively. In the same way, we compute the features for the left and right query images and average them to compute a single feature $f_q(R,B)$. We then feed the left and right query images to a model that optimizes similarity for user A and user B, respectively. To enable other researchers to replicate our experiments, we provide the detailed user-to-user verification protocol in Protocol 1.\\n\\n### Protocol 1\\n\\n1. Initialize $L_A = \\\\emptyset$, $R_A = \\\\emptyset$, $L_B = \\\\emptyset$, $R_B = \\\\emptyset$.\\n2. Randomly select $K$ left query images from $Q_u$ for user A.\\n3. Randomly select $K$ left query images from $Q_v$ for user B.\\n4. Randomly select $K$ right query images from $Q_u$ for user A.\\n5. Randomly select $K$ right query images from $Q_v$ for user B.\\n6. Left query feature $f_q(L,A)$ computed from the left query images.\\n7. Right query feature $f_q(R,B)$ computed from the right query images.\\n8. Left query dictionary $S_q(L,A)$ computed from the left query images.\\n9. Right query dictionary $S_q(R,B)$ computed from the right query images.\\n10. Left query $L_A$ computed from the left query images.\\n11. Right query $R_A$ computed from the right query images.\\n12. Left and right gallery features $S_q(L_A, R_A)$ computed from the left and right gallery images.\\n13. Similarity $S_q(L_A, R_B)$ computed from the left and right gallery images.\\n14. Left gallery $L_B$ computed from the left gallery images.\\n15. Right gallery $R_B$ computed from the right gallery images.\\n16. Left gallery $L_B$ computed from the left gallery images.\\n17. Right gallery $R_B$ computed from the right gallery images.\\n18. Left query $L_A$ computed from the left query images.\\n19. Right query $R_A$ computed from the right query images.\\n20. Left query $L_B$ computed from the left query images.\\n21. Right query $R_B$ computed from the right query images.\\n22. Left gallery $L_A$ computed from the left gallery images.\\n23. Right gallery $R_A$ computed from the right gallery images.\\n24. Left gallery $L_B$ computed from the left gallery images.\\n25. Right gallery $R_B$ computed from the right gallery images.\\n26. Left query $L_A$ computed from the left query images.\\n27. Right query $R_A$ computed from the right query images.\\n28. Left query $L_B$ computed from the left query images.\\n29. Right query $R_B$ computed from the right query images.\\n30. Left gallery $L_A$ computed from the left gallery images.\\n31. Right gallery $R_A$ computed from the right gallery images.\\n32. Left gallery $L_B$ computed from the left gallery images.\\n33. Right gallery $R_B$ computed from the right gallery images.\\n34. Left query $L_A$ computed from the left query images.\\n35. Right query $R_A$ computed from the right query images.\\n36. Left query $L_B$ computed from the left query images.\\n37. Right query $R_B$ computed from the right query images.\\n38. Left gallery $L_A$ computed from the left gallery images.\\n39. Right gallery $R_A$ computed from the right gallery images.\\n40. Left gallery $L_B$ computed from the left gallery images.\\n41. Right gallery $R_B$ computed from the right gallery images.\\n42. Left query $L_A$ computed from the left query images.\\n43. Right query $R_A$ computed from the right query images.\\n44. Left query $L_B$ computed from the left query images.\\n45. Right query $R_B$ computed from the right query images.\\n46. Left gallery $L_A$ computed from the left gallery images.\\n47. Right gallery $R_A$ computed from the right gallery images.\\n48. Left gallery $L_B$ computed from the left gallery images.\\n49. Right gallery $R_B$ computed from the right gallery images.\\n50. Left query $L_A$ computed from the left query images.\\n51. Right query $R_A$ computed from the right query images.\\n52. Left query $L_B$ computed from the left query images.\\n53. Right query $R_B$ computed from the right query images.\\n54. Left gallery $L_A$ computed from the left gallery images.\\n55. Right gallery $R_A$ computed from the right gallery images.\\n56. Left gallery $L_B$ computed from the left gallery images.\\n57. Right gallery $R_B$ computed from the right gallery images.\\n58. Left query $L_A$ computed from the left query images.\\n59. Right query $R_A$ computed from the right query images.\\n60. Left query $L_B$ computed from the left query images.\\n61. Right query $R_B$ computed from the right query images.\\n62. Left gallery $L_A$ computed from the left gallery images.\\n63. Right gallery $R_A$ computed from the right gallery images.\\n64. Left gallery $L_B$ computed from the left gallery images.\\n65. Right gallery $R_B$ computed from the right gallery images.\\n66. Left query $L_A$ computed from the left query images.\\n67. Right query $R_A$ computed from the right query images.\\n68. Left query $L_B$ computed from the left query images.\\n69. Right query $R_B$ computed from the right query images.\\n70. Left gallery $L_A$ computed from the left gallery images.\\n71. Right gallery $R_A$ computed from the right gallery images.\\n72. Left gallery $L_B$ computed from the left gallery images.\\n73. Right gallery $R_B$ computed from the right gallery images.\\n74. Left query $L_A$ computed from the left query images.\\n75. Right query $R_A$ computed from the right query images.\\n76. Left query $L_B$ computed from the left query images.\\n77. Right query $R_B$ computed from the right query images.\\n78. Left gallery $L_A$ computed from the left gallery images.\\n79. Right gallery $R_A$ computed from the right gallery images.\\n80. Left gallery $L_B$ computed from the left gallery images.\\n81. Right gallery $R_B$ computed from the right gallery images.\\n82. Left query $L_A$ computed from the left query images.\\n83. Right query $R_A$ computed from the right query images.\\n84. Left query $L_B$ computed from the left query images.\\n85. Right query $R_B$ computed from the right query images.\\n86. Left gallery $L_A$ computed from the left gallery images.\\n87. Right gallery $R_A$ computed from the right gallery images.\\n88. Left gallery $L_B$ computed from the left gallery images.\\n89. Right gallery $R_B$ computed from the right gallery images.\\n90. Left query $L_A$ computed from the left query images.\\n91. Right query $R_A$ computed from the right query images.\\n92. Left query $L_B$ computed from the left query images.\\n93. Right query $R_B$ computed from the right query images.\\n94. Left gallery $L_A$ computed from the left gallery images.\\n95. Right gallery $R_A$ computed from the right gallery images.\\n96. Left gallery $L_B$ computed from the left gallery images.\\n97. Right gallery $R_B$ computed from the right gallery images.\\n98. Left query $L_A$ computed from the left query images.\\n99. Right query $R_A$ computed from the right query images.\\n100. Left query $L_B$ computed from the left query images.\\n101. Right query $R_B$ computed from the right query images.\\n102. Left gallery $L_A$ computed from the left gallery images.\\n103. Right gallery $R_A$ computed from the right gallery images.\\n104. Left gallery $L_B$ computed from the left gallery images.\\n105. Right gallery $R_B$ computed from the right gallery images.\\n106. Left query $L_A$ computed from the left query images.\\n107. Right query $R_A$ computed from the right query images.\\n108. Left query $L_B$ computed from the left query images.\\n109. Right query $R_B$ computed from the right query images.\\n110. Left gallery $L_A$ computed from the left gallery images.\\n111. Right gallery $R_A$ computed from the right gallery images.\\n112. Left gallery $L_B$ computed from the left gallery images.\\n113. Right gallery $R_B$ computed from the right gallery images.\\n114. Left query $L_A$ computed from the left query images.\\n115. Right query $R_A$ computed from the right query images.\\n116. Left query $L_B$ computed from the left query images.\\n117. Right query $R_B$ computed from the right query images.\\n118. Left gallery $L_A$ computed from the left gallery images.\\n119. Right gallery $R_A$ computed from the right gallery images.\\n120. Left gallery $L_B$ computed from the left gallery images.\\n121. Right gallery $R_B$ computed from the right gallery images.\\n122. Left query $L_A$ computed from the left query images.\\n123. Right query $R_A$ computed from the right query images.\\n124. Left query $L_B$ computed from the left query images.\\n125. Right query $R_B$ computed from the right query images.\\n126. Left gallery $L_A$ computed from the left gallery images.\\n127. Right gallery $R_A$ computed from the right gallery images.\\n128. Left gallery $L_B$ computed from the left gallery images.\\n129. Right gallery $R_B$ computed from the right gallery images.\\n130. Left query $L_A$ computed from the left query images.\\n131. Right query $R_A$ computed from the right query images.\\n132. Left query $L_B$ computed from the left query images.\\n133. Right query $R_B$ computed from the right query images.\\n134. Left gallery $L_A$ computed from the left gallery images.\\n135. Right gallery $R_A$ computed from the right gallery images.\\n136. Left gallery $L_B$ computed from the left gallery images.\\n137. Right gallery $R_B$ computed from the right gallery images.\\n138. Left query $L_A$ computed from the left query images.\\n139. Right query $R_A$ computed from the right query images.\\n140. Left query $L_B$ computed from the left query images.\\n141. Right query $R_B$ computed from the right query images.\\n142. Left gallery $L_A$ computed from the left gallery images.\\n143. Right gallery $R_A$ computed from the right gallery images.\\n144. Left gallery $L_B$ computed from the left gallery images.\\n145. Right gallery $R_B$ computed from the right gallery images.\\n146. Left query $L_A$ computed from the left query images.\\n147. Right query $R_A$ computed from the right query images.\\n148. Left query $L_B$ computed from the left query images.\\n149. Right query $R_B$ computed from the right query images.\\n150. Left gallery $L_A$ computed from the left gallery images.\\n151. Right gallery $R_A$ computed from the right gallery images.\\n152. Left gallery $L_B$ computed from the left gallery images.\\n153. Right gallery $R_B$ computed from the right gallery images.\\n154. Left query $L_A$ computed from the left query images.\\n155. Right query $R_A$ computed from the right query images.\\n156. Left query $L_B$ computed from the left query images.\\n157. Right query $R_B$ computed from the right query images.\\n158. Left gallery $L_A$ computed from the left gallery images.\\n159. Right gallery $R_A$ computed from the right gallery images.\\n160. Left gallery $L_B$ computed from the left gallery images.\\n161. Right gallery $R_B$ computed from the right gallery"}
{"id": "CVPR-2022-638", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"average feature vectors (left) CNN\\naverage feature vectors (right) CNN\\n\\nFigure 4. User-to-user verification: Verifying query user A ($q_A$) against gallery user B ($g_B$) with $K$ pairs for gallery user B.\\n\\nProtocol 2\\nComputing OFRR\\n\\n1: Required: Model $M$, Query dataset $Q$, Gallery dataset $G$\\n\\n2: Required: Similarity dictionary $S$ from Protocol 1\\n\\n3: Required: Query dictionaries $Q_L$, $Q_R$ from Protocol 1\\n\\n4: Required: Similarity threshold $t_{auth}$ from Protocol 1\\n\\n5: Required: PAD threshold $t_{pad}$ for SAR=5%\\n\\n6: Initialize: Spoof rejects $X_{spoof} = 0$, EA rejects $X_{auth} = 0$\\n\\n7: for Query user $q_A$, gallery user $g_A$ in $Q,G$ do\\n\\n8: $q_L(A) \\\\leftarrow Q_L[q_A]$, $q_R(A) \\\\leftarrow Q_R[q_A]$\\n\\n9: Left query PAD logit $o_L(q_A) = M(q_L(A))$\\n\\n10: Right query PAD logit $o_R(q_A) = M(q_R(A))$\\n\\n11: if $o_L(q_A) > t_{pad}$ or $o_R(q_A) > t_{pad}$ then\\n\\n12: $X_{spoof} = X_{spoof} + 1$ // falsely rejected as spoof\\n\\n13: else\\n\\n14: if $S[q_A, g_A] < t_{auth}$ then\\n\\n15: $X_{auth} = X_{auth} + 1$ // falsely rejected as non-match\\n\\n16: end if\\n\\n17: end if\\n\\n18: end for\\n\\n19: Overall false rejection rate OFRR = $(X_{spoof} + X_{auth}) / |Q|$\\n\\nBased on the similarity threshold, a match/non-match is predicted (Fig. 4).\\n\\n4.4. Metrics for EA and PAD\\n\\nPerforming the similarity computation (Eq. 9) for every possible pairs from $(Q,G)$ and varying the similarity threshold for deciding match/non-match, we compute the ROC curve and report the True Acceptance Rates (TARs) at FAR=$10^{-4}$, $10^{-3}$, $10^{-2}$. In the biometrics literature [8, 30, 36], it is common to use several gallery samples in authentication. But, for authentication on edge devices, the number of gallery samples that can be used depends on the storage capacity of the edge device. Therefore, for evaluating the EA performance of a given model, we use one query left-right pair and $K = 1/2/5$ gallery left-right pair(s) for verification.\\n\\nPAD performance is evaluated with four commonly used metrics: (i) True Detection Rate (TDR) at a False Detection Rate of 0.002, (ii) Attack Presentation Classifier Error Rate (APCER), that is the fraction of spoof samples misclassified as Live, (iii) Bonafide Presentation Classifier Error Rate (BPCER), that is the fraction of live samples misclassified as spoof, (iv) Half Total Error Rate (HTER), the average of APCER and BPCER. Following the protocol in [48], we use a threshold of 0.5 for computing APCER and BPCER.\\n\\nWhile these metrics gauge either the EA or PAD performance, they cannot jointly evaluate PAD and EA. Hence, we define a new metric in the next subsection.\\n\\n4.4.1 Overall False Reject Rate (OFRR)\\n\\nWe evaluate EA performance on the test EA data (ND-Iris-0405) and the PAD performance on the test subset of CU-LivDet datasets from the 2013, 2015 and 2017 challenges. An ideal metric must measure PAD and EA performance simultaneously on a single dataset. Such a metric must measure:\\n\\n- How often does the model reject true users from accessing the system?\\n- A true user in the EA dataset can be falsely rejected as: (1) 'Spoof' by the PAD pipeline, or (2) 'Non-match' by the user-to-user verification pipeline. In this regard, we introduce a new metric called Overall False Rejection Rate (OFRR) for true query users in the EA test subset. The steps for computing the OFRR of true users are summarized in Protocol 2. To determine OFRR, we must first set thresholds for the rates at which PAD misclassifies spoof as live (i.e. Spoof Acceptance Rate or SAR) and EA falsely accepts non-match pairs (FAR). The PAD threshold $t_{pad}$ is computed as the point where SAR=5% when PAD test data is fed to the model. Similarly, when EA test data is fed to the model, the similarity threshold $t_{auth}$ is computed as the point where the user-to-user verification results in FAR=$10^{-3}$.\\n\\nAfter computing $t_{pad}$ and $t_{auth}$, we feed the EA test data to the model again. We then compute the number of query users falsely rejected as spoof ($X_{spoof}$) using $t_{pad}$. Here, we reject a query user if at least one of the associated eye images is classified as spoof (Line 12 in Protocol 2). For those query users classified as live, we verify them for EA against matching gallery users, using our user-to-user verification protocol (Fig 4). We compute the number of query users that are falsely rejected as non-match $X_{auth}$ using $t_{auth}$. Finally, we compute the Overall False Reject Rate (OFRR) as $OFRR = X_{spoof} + X_{auth} / |Q|$ (10). $|Q| = total number of query users in the EA test dataset, which, in our case, is 150 (Sec. 4.2). Ideally, an authentication system for EA and PAD must have a lower OFRR.\"}"}
{"id": "CVPR-2022-638", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method      | OFRR (\u2193) | TDR (\u2191) | HTER (\u2193) |\\n|------------|----------|---------|----------|\\n| EA only    | -0.886 0.958 0.996 | -     | -        |\\n| PAD only   | -        | -       | -        |\\n| OFRR       | 10\u207b\u2074 10\u207b\u00b3 10\u207b\u00b2 |        |          |\\n| TDR        |          |         | APCER    |\\n|            |          |         | BPCER    |\\n|            |          |         | HTER     |\\n|            |          |         |          |\\n| MTL        | 0.100 0.693 0.905 0.988 | 0.074 0.803 0.943 0.986 | 0.052 |\\n|            |          |         |          |\\n| MTL        | 0.087 0.919 0.963 0.992 | 0.068 0.872 0.950 0.989 | 0.052 |\\n|            |          |         |          |\\n| EyePAD     | 0.079 0.843 0.926 0.993 | 0.046 0.887 0.961 0.997 | 0.947 |\\n|            |          |         |          |\\n| EyePAD++   | 0.072 0.901 0.952 0.990 | 0.055 0.906 0.966 0.997 | 0.941 |\\n|            |          |         |          |\\n| EA only    | -0.832 0.916 0.979 | -     | -        |\\n| PAD only   | -        | -       | -        |\\n| OFRR       | 10\u207b\u2074 10\u207b\u00b3 10\u207b\u00b2 |        |          |\\n| TDR        |          |         | APCER    |\\n|            |          |         | BPCER    |\\n|            |          |         | HTER     |\\n|            |          |         |          |\\n| MTL        | 0.244 0.745 0.871 0.974 | 0.209 0.801 0.910 0.989 | 0.199 |\\n|            |          |         |          |\\n| MTL        | 0.236 0.753 0.894 0.974 | 0.213 0.841 0.921 0.986 | 0.189 |\\n|            |          |         |          |\\n| EyePAD     | 0.231 0.757 0.876 0.979 | 0.204 0.796 0.933 0.974 | 0.196 |\\n|            |          |         |          |\\n| EyePAD++   | 0.201 0.830 0.916 0.988 | 0.188 0.854 0.947 0.986 | 0.192 |\\n|            |          |         |          |\\n| EA only    | -0.760 0.901 0.980 | -     | -        |\\n| PAD only   | -        | -       | -        |\\n| OFRR       | 10\u207b\u2074 10\u207b\u00b3 10\u207b\u00b2 |        |          |\\n| TDR        |          |         | APCER    |\\n|            |          |         | BPCER    |\\n|            |          |         | HTER     |\\n|            |          |         |          |\\n| MTL        | 0.184 0.768 0.891 0.981 | 0.170 0.819 0.927 0.993 | 0.152 |\\n|            |          |         |          |\\n| MTL        | 0.168 0.777 0.891 0.979 | 0.144 0.851 0.927 0.990 | 0.105 |\\n|            |          |         |          |\\n| EyePAD     | 0.162 0.718 0.852 0.976 | 0.128 0.757 0.891 0.984 | 0.094 |\\n|            |          |         |          |\\n| EyePAD++   | 0.144 0.777 0.882 0.979 | 0.111 0.797 0.919 0.983 | 0.082 |\\n\\nTable 3. EA and PAD with Densenet121 trained and evaluated on (top) original, (middle) blurred, (bottom) Noisy data: For user-to-user verification, we report TAR@FAR=10\u207b\u2074 10\u207b\u00b3 10\u207b\u00b2. For PAD, we report TDR@FDR=0.002 and APCER, BPCER, HTER. OFRR jointly measures EA and PAD performance on ND-Iris-0405. EyePAD++ obtains the lowest OFRR. Bold: Best, Underlined: Second best.\\n\\n4.5. Results\\nEA and PAD with Densenet backbone: We perform the EA and PAD experiments using the Densenet121 backbone for the original datasets and degraded datasets (Table 3). EyePAD++ obtains the lowest OFRR in most cases.\"}"}
{"id": "CVPR-2022-638", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of the problem settings. Moreover, EyePAD++ obtains higher user-to-user verification performance than existing multitasking baselines at most FARs. This demonstrates the advantage of the additional distillation step combined with MTL. The PAD performance demonstrated by EyePAD++ is also comparable to that of other multitasking baselines.\\n\\nEA and PAD with HRnet64 and MobilenetV3 backbone:\\nTo demonstrate the generalizability of our proposed methods, we repeat the same experiments with the HRnet64 backbone (Table 4). However, training a Densenet or HRnet64 model is computationally expensive. So, we also perform the same experiment with the MobilenetV3 backbone, that is much more computationally efficient than Densenet (Table 5). More detailed results with 1 or 2 gallery pairs are provided in the supplementary material.\\n\\nOnce again we find that EyePAD++ obtains lower OFRR than MTL and MTMT. The superiority of EyePAD++ with MobilenetV3 indicates that EyePAD++ can be used for performing EA and PAD on compute engines with low capacity that are available on edge devices.\\n\\nEye-to-eye verification with PAD:\\nTo compare our proposed methods with current SOTA in PAD and EA, we perform eye-to-eye verification with PAD. Here, for EA, we follow [45, 53] and use the first 25 left eye images of every user in the ND-Iris-0405 dataset [1] for training. We use the first 10 right eye images of the users for testing. For evaluating EA, we use the same eye-to-eye verification in [45, 53]. For PAD, we follow [11, 41] and only use the official train and test split of the CU-LivDet-2017 dataset. We perform this experiment with Densenet121. While training and testing our methods and baselines (Sec. 4.1), we exclude pre-processing. From Table 6, we infer that EyePAD and EyePAD++ achieve better PAD performance (i.e. TDR @ FDR=0.002) than the current SOTA PAD algorithms, without any pre-processing. Moreover, EyePAD++ achieves higher EA performance (TAR at FAR=\\\\(10^{-3}\\\\)) than the comparable baselines (i.e. EA only network, MTL and MTMT). The EA performance for EyePAD and EyePAD++ is comparable to but slightly lower than that of the SOTA [45, 53], with a difference of less than 4%. We believe that this difference is due to excluding pre-processing steps for limiting computational cost.\\n\\nEyePAD++ v/s MTMT [26]:\\nBoth EyePAD++ and MTMT combine MTL with feature-level KD. However, the student MTL network in MTMT does not inherit the 'versatility' through distillation since its teachers are single-task models that are not versatile. On the other hand, EyePAD++ uses distillation from a single versatile teacher (\\\\(M_s\\\\)), that is proficient in both the tasks. As a result, the student network \\\\(M_s^*\\\\) in EyePAD++ inherits the versatility of its teacher network.\"}"}
