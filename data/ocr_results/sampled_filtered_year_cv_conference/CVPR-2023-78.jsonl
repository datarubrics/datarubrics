{"id": "CVPR-2023-78", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. ShapeFormer: A shape feature is encoded into shape token through self-attention, avoiding information loss caused by common strategies, e.g., pooling.\\n\\n$L$ is element-wise addition. where $f_0$ is the shape token and $\\\\text{PE}(\\\\cdot)$ is the positional encoding function\u2014explained in detail shortly. The input $z$ is operated on by learnable weight matrices $W_q$, $W_k$ and $W_v$ to obtain query $q$, key $k$ and value $v$ as\\n\\n$q = zW_q$, $k = zW_k$, $v = zW_v$.\\n\\n(5)\\nThe attention module performs the following computation.\\n\\n$y' = X_{\\\\text{softmax}} qk^\\\\top \\\\sqrt{d}$\\n\\n(6)\\n\\n$y = \\\\text{Concat} y'(0), y'(1), \\\\ldots, y'(m-1)$\\n\\n(7)\\n\\nwhere $y'$ is the output of single attention head and $m$ is the number of the attention heads. Subsequently, it performs\\n\\n$o = A(F(A(y)))$\\n\\n(8)\\n\\nwhere $A(\\\\cdot)$ denotes add and normalization operations and $F(\\\\cdot)$ denotes a FFN with two linear layers with ReLU activation. The calculations expressed as Eq. (6)-(8) comprise one layer of ShapeFormer. The output of the $l$th layer is\\n\\n$h = f_0(l)j, f_1(l)j, f_2(l)j, \\\\ldots, f_n(l)j$\\n\\n(9)\\n\\nThe shape token $f_0(l)j$ is fed to an MLP to get the shape feature $p(O)$:\\n\\n$p(O) = \\\\text{MLP} f_0(l)j$.\\n\\nObject-Scene Positional Encoding. Learned positional encoding can benefit transformer-based modules [19, 27, 32, 40]. In ShapeFormer, we must pay more attention to the relative positional relationships between the shape key points and the object centers. These relations allow us to encode shape information at object level. Therefore, we propose Object-Scene Positional Encoding, which not only encodes the absolute position of the point cloud, but also enables object-level positional encoding. Specifically, our positional encoding consists of two components\\n\\n$\\\\text{PE} = \\\\text{PE}_s + \\\\text{PE}_o$\\n\\n(10)\\n\\nwhere $\\\\text{PE}_s$ is Scene-level positional encoding and $\\\\text{PE}_o$ is Object-level encoding. We compute these as\\n\\n$\\\\text{PE}_s = \\\\text{MLP}_s c_j; s_1j, s_2j, \\\\ldots, s_nj$\\n\\n(11)\\n\\n$\\\\text{PE}_o = \\\\text{MLP}_0; s_0j - sc_j, s_1j - sc_j, \\\\ldots, s_nj - sc_j$\\n\\n(12)\\n\\nwhere $sc_j$ denotes the candidate coordinates, $snj$ represents the shape key point coordinates, $0$ is a zero vector with the same dimension as $snj$.\\n\\n3.4. Semantics Guided Module\\n\\nThe background points in a scene affect local feature extraction [5, 11, 49], which inevitably affects the shape encoding adversely through the shape key points. Although foreground points are selected for voting during training, we cannot supervise them during testing. Therefore, we further improve our AShapeFormer with a Semantics Guided Module (SGM) to alleviate the influence of background points in shape features. As illustrated in Fig. 5, we feed the seed point features $f_i$ to MLP layers to predict the point cloud semantic segmentation score, which is proportional to the probability that the point cloud belongs to the foreground points [5]. If the point cloud lies within the ground truth range of the object bounding box, we consider the point cloud as the foreground point, otherwise it belongs to the background. We compute the foreground confidence $p_i \\\\in [0,1]$ as\\n\\n$p_i = \\\\sigma(\\\\text{MLP}_s(f_i))$\\n\\n(13)\\n\\nwhere MLP$_s$ represents multiple MLP layers and $\\\\sigma(\\\\cdot)$ is the sigmoid activation. Binary cross-entropy loss function is used for the semantic segmentation. The segmentation score is used as a weight to adjust the contribution of different seed points on the shape feature as\\n\\n$e_f = p_i \\\\times f_i$\\n\\n(14)\\n\\nwhere $\\\\times$ denotes element-wise multiplication. The re-weighted features $e_f$ are fed to the ShapeFormer module for...\"}"}
{"id": "CVPR-2023-78", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"shape encoding. We use the same strategy to re-weight the vote features for better vote aggregation. The SGM module helps AShapeFormer reduce or even eliminate the influence of background points on object shape perception, and better use the features of foreground points to encode more accurate shape information.\\n\\nThrough the backbone, due to the guidance of semantic information, we can sample more seed points without worrying about the background points. Our experiments (\u00a7 4.3) ascertain that more foreground seed points and vote points are obtained through SGM.\\n\\n3.5. Network Loss\\n\\nWe train the entire network end-to-end using the loss function of the newly proposed AShapeFormer, defined as\\n\\n\\\\[ L = \\\\lambda_1 L_{\\\\text{sgm}} + \\\\lambda_2 L_{\\\\text{vote}} + \\\\lambda_3 L_{\\\\text{box}} + \\\\lambda_4 L_{\\\\text{cls}} + \\\\lambda_5 L_{\\\\text{obj}}, \\\\]\\n\\n(15)\\n\\nwhere \\\\( \\\\lambda \\\\)'s are the balancing factors.\\n\\n\\\\( L_{\\\\text{sgm}} \\\\) is used to supervise the foreground/background seed points prediction in SGM, which we define as follows\\n\\n\\\\[ L_{\\\\text{sgm}} = -\\\\frac{1}{M} \\\\sum_{i=1}^{M} [b_p \\\\ln (p) + (1-b_p) \\\\ln (1-p)], \\\\]\\n\\n(16)\\n\\nwhere \\\\( p \\\\) and \\\\( b_p \\\\) denote the predicted segmentation score and the ground-truth score (1 for foreground and 0 for background).\\n\\nM is the total number of input points.\\n\\nThe loss terms \\\\( L_{\\\\text{vote}}, L_{\\\\text{obj}}, L_{\\\\text{cls}} \\\\) and \\\\( L_{\\\\text{box}} \\\\) in Eq. (15) indicate the per-point vote regression loss, the objectness loss, the object classification loss, and the bounding box loss, respectively. These loss terms are inspired by the label assignment strategy of VoteNet [29]. We provide details of these losses along with balancing factor settings in the supplementary material.\\n\\n4. Experiments\\n\\n4.1. Setup and Implementation Details\\n\\nDue to its plug-n-play nature, the proposed AShapeFormer can be assembled to several backbones. In our experiments, we show its assembly with VoteNet [29], imVoteNet [28], RBGNet [42] and GroupFree3D [23].\\n\\nTo generate foreground/background labels for the sample points, we regard all the points within the labeled 3D bounding boxes as foreground points, and the points outside the boxes as the background points. We optimize the networks using the Adam algorithm, which is trained on an RTX 3090 GPU with batch size of 8. We set the initial learning rate to 0.004 when training on the SUNRGBD dataset and 0.008 when training on the Scannet dataset, and decay it by 0.1 in the steps of [120, 140, 180]. We train the network from scratch for a total of 200 epochs. We also implement our method on the 3D object detection toolbox MMdetection3D [8]. Our implementation is consistent with the MMdetection3D framework, and uses the Adamw [24] algorithm. More training details are provided in the supplementary material.\\n\\nWe evaluate the performance of the proposed AShapeFormer on two popular datasets of indoor scenes, namely; ScanNet dataset [9] and SUN RGB-D dataset [38].\\n\\n4.2. Comparisons with State-of-the-art\\n\\nWe compare our method with the existing state-of-the-art on ScanNet V2 and SUN RGB-D dataset, considering methods such as VoteNet [29] imVoteNet [28], BRNet [7], GroupFree3D [23], TokenFusion [44] etc.\\n\\nQuantitative comparison.\\n\\nThe results on SUN RGB-D are summarized in Table 1. As a plug-n-play module, our enhancements outperform the baselines by remarkable performance gains. For instance, we achieve an absolute gain of more than 3.5% and 1.7% for VoteNet [29] and imVoteNet [28], respectively. Note that, considering the plug-n-play nature of our contribution, mAP@0.25 is a very challenging metric because 3D object detectors have already achieved remarkable performance on this metric. It is hard to achieve a high gain under this metric. In particular, our AShapeFormer applied to imVoteNet* [28] achieves 65.8% on map@0.25, which outperforms all the existing methods. This ascertains that our method is effective even for highly optimized techniques. Table 2 summarizes the results on ScanNetV2. Taking VoteNet [29] as the baseline, our method achieves remarkable 4.5% and 8.1% improvements at mAP@0.25 and mAP@0.5, respectively. Applying AShapeFormer to the more recent Transformer method GroupFree3D [23] and RBGNet [42] also has a significant improvement.\\n\\nQualitative comparison.\\n\\nIn Fig. 6 and Fig. 7, we visualize the representative 3D object detection results from our method and the baseline methods. These results demonstrate that applying our method to the baseline detector achieves more reliable detection results with more accurate bounding boxes and orientations. As compared to the baselines, our method can discover more missing objects. For example, in Fig. 6 upper left corner, VoteNet misses the challenging object, which is discovered by enhancing it with AShapeFormer. Our method also eliminates false positives, e.g., the results in the second row of Fig. 6 show that there are three chairs around each table. VoteNet detects 5 chairs, whereas our enhancement results are consistent with the ground truth. Figure. 7 shows the visualization results of imVoteNet+AShapeformer on the SUN RGBD dataset. The second and third column both show the ability of our method to eliminate false positives as result of using more foreground point information in the process. In the first column, our method also detects two desks that are not labeled in the scene. This implies that the indicator AP score, might actually underestimate the performance of AShapeFormer.\\n\\nWe only provide representative examples in Fig. 6 and 7.\"}"}
{"id": "CVPR-2023-78", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"| Model                | mAP@0.25 | mAP@0.5 |  |  |  |  |  |  |  |  |\\n|----------------------|----------|---------|---|---|---|---|---|---|---|---|\\n| VoteNet [29]         | 57.7     | 83.0    | 47.3 | 64.0 | 75.3 | 90.1 | 22.0 | 29.8 | 62.2 | 28.8 |\\n| VoteNet \u2217 [29]       | 59.7     | 84.8    | 49.6 | 67.8 | 77.6 | 87.4 | 24.3 | 29.3 | 61.9 | 32.1 |\\n| BRNet [7]            | 61.1     | 86.9    | 51.8 | 66.4 | 77.4 | 91.3 | 29.6 | 35.9 | 65.9 | 29.7 |\\n| Groupfree3D [23]     | 63.0     | 87.8    | 53.8 | 70.0 | 79.4 | 91.1 | 32.6 | 36.0 | 66.7 | 32.5 |\\n| imVoteNet [28]       | 63.4     | 87.6    | 51.1 | 70.7 | 76.7 | 90.5 | 28.7 | 41.4 | 69.9 | 41.3 |\\n| imVoteNet \u2217 [28]     | 64.5     | 88.5    | 51.6 | 73.2 | 79.2 | 90.2 | 30.9 | 38.0 | 67.3 | 46.4 |\\n| RBGNet [42]          | 64.1     | 88.4    | 54.5 | 71.0 |     |     |     |     |     |     |\\n| FCAF3D [34]          | 64.2     | 88.3    | 53.0 | 69.7 | 81.1 | 91.3 | 34.0 | 40.1 | 71.9 | 33.0 |\\n| TokenFusion [44]     | 64.9     | -       | -   | -   | -   | -   | -   | -   | -   | -   |\\n| DisARM [11]          | 65.3     | 87.5    | 52.7 | 74.1 | 80.7 | 91.6 | 33.3 | 39.8 | 69.5 | 43.7 |\\n| Ours (VoteNet)       | 61.2 (+3.5) | 86.9 | 51.5 | 67.8 | 78.8 | 91.2 | 29.0 | 33.6 | 65.0 | 31.3 |\\n| Ours (VoteNet \u2217)     | 62.2 (+2.5) | 86.9 | 51.3 | 69.3 | 78.9 | 90.2 | 28.2 | 34.6 | 65.9 | 35.6 |\\n| Ours (imVoteNet)     | 65.1 (+1.7) | 89.2 | 53.7 | 72.9 | 78.3 | 90.8 | 30.2 | 43.2 | 70.0 | 46.5 |\\n| Ours (imVoteNet \u2217)   | 65.8 (+1.3) | 87.6 | 55.2 | 72.8 | 80.9 | 92.5 | 31.2 | 45.8 | 67.7 | 43.7 |\\n\\nTable 1. 3D object detection results on SUN RGB-D validation set with mAP@0.25.\\n\\n* denotes that the model is implemented on MMDetection3D. Ours (M) denotes that M is enhanced with our AShapeFormer. Our enhancement enables considerable performance gain despite the highly competitive performance of existing methods on mAP@0.25. Best results in each column are green highlighted.\\n\\nFigure 6. Representative qualitative results on ScanNet V2 dataset [9]. As compared to the baseline, i.e., VoteNet [29], AShapeFormer enhancement not only enables detection of more challenging objects, but also reduces false positive detections. Best viewed on screen.\\n\\n| Model                  | mAP@0.25 | mAP@0.5 |\\n|------------------------|----------|---------|\\n| HGNet [6]               | 61.3     | 34.4    |\\n| VoteNet [29]           | 58.6     | 33.5    |\\n| VoteNet \u2217 [29]         | 63.8     | 44.2    |\\n| MLCVNet [46]           | 64.5     | 41.4    |\\n| 3DETR [26]             | 65.0     | 47.0    |\\n| GroupFree3D [23]       | 69.1     | 52.8    |\\n| RBGNet [42]            | 70.6     | 55.2    |\\n| Ours (VoteNet)         | 63.1 (+4.5) | 41.6 (+8.1) |\\n| Ours (VoteNet \u2217)       | 66.6 (+2.8) | 47.8 (+3.6) |\\n| Ours (GroupFree3D)     | 70.4 (+1.3) | 53.4 (+0.6) |\\n| Ours (RBGNet)          | 71.1 (+0.5) | 56.6 (+1.4) |\\n\\nTable 2. 3D object detection results on ScanNet V2 validation set. A consistent improvement is achieved by enhancing existing methods with the AShapeFormer module.\\n\\n4.3. Ablation Study and Discussion\\n\\nWe conduct an extensive ablation study to analyze the efficacy of different sub-modules of our method. Table 3 compares the detection results of the naive method (\u00a7 3.2), ShapeFormer (\u00a7 3.3) and SGM (\u00a7 3.4) combined with the vanilla VoteNet on the SUN RGB-D dataset when the IOU is 0.25. It can be seen that when the naive shape encoding is used (without Shapeformer and SGM) there is only incremental performance improvement because the background points are mixed in the encoding, and the pooling layer loses shape information. The SGM utilizes guidance of semantic information to not only sample more foreground points, but also further suppress the contribution of the background points in shape encoding. With the help of SGM, we can better encode the shape feature of the object. Hence, as compared to the original VoteNet, absolute performance gain is 1.8%. The ShapeFormer is a standalone enhancement that does not require Naive baseline. It gives better\"}"}
{"id": "CVPR-2023-78", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7. Qualitative results on SUN RGB-D dataset [38]. Best viewed on screen. Our method often correctly detects those objects for which ground truth annotation is not provided. This implies that mAP values of our method are under-estimated.\\n\\nFigure 8. Semantic Guided Sampling and Voting. Best viewed on screen. See supplementary for more results.\\n\\nTable 3. Contribution of sub-modules of AShapeFormer (SUN RGB-D dataset). Naive is the naive approach from \u00a7 3.2. SF is Shape Former (\u00a7 3.3), SGM is Semantic Guided Module (\u00a7 3.3).\\n\\n| Method      | mean size | cls loss | mean vote loss |\\n|-------------|-----------|----------|----------------|\\n| VoteNet [29]| 0.52      | 0.04     |                |\\n| AShapeFormer| 0.38      | 0.03     |                |\\n\\nTable 4. Detection size error comparison on ScanNet V2. SGM module [29]. It can be seen that the seed points and vote points contain a large number of outliers. More background points affect the quality of shape encoding adversely. Scattered vote points containing a large number of outliers cannot provide high-quality candidate points for the detection head. The second row shows that with the help of SGM, we find more foreground points, and the vote points are also more compact and closer to the center, which is beneficial to our shape encoding and vote aggregation.\\n\\nThe candidate points of vanilla VoteNet [29] are aggregated from voting points. The vote points only contain the positioning information of the object, which is sub-optimal. Our method makes full use of the shape key points distributed on the surface of the object, which can encode the shape of the object, so it can more accurately predict the size and direction of the boxs. Table 4 summarizes a clearly favorable comparison of the prediction size errors of AShapeFormer and VoteNet [29] on the ScanNet dataset.\\n\\n5. Conclusion\\n\\nWe propose a plug-n-play module to improve the performance of indoor 3D object detection by actively encoding shape information of the object. We first sample the shape key points of the object and re-weight their features by guiding them with semantic information. Then, to avoid the loss of fine-grained information, we utilize multi-head attention to encode object shape features. Finally, object-level shape features are fused with the candidate features and fed to the detection head. Results show that our model achieves state-of-the-art performance when assembled with existing methods. In the future, we will explore to incorporate RGB images and point cloud completion methods to encode more complete shape information in our technique.\\n\\nAcknowledgement.\\nThis work was supported by the NSFC (U2013203, 61973106, U1913202); the Natural Science Fund of Hunan Province (2021JJ10024, 2022JJ40100); the Project of Talent Innovation and Sharing Alliance of Quanzhou City under Grant 2021C062L; the Key Research and Development Project of Science and the Technology Plan of Hunan Province under Grant 2022GK2014.\"}"}
{"id": "CVPR-2023-78", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"AShapeFormer: Semantics-Guided Object-Level Active Shape Encoding for 3D Object Detection via Transformers\\n\\nZechuan Li, Hongshan Yu, Zhengeng Yang, Tongjia Chen, Naveed Akhtar\\n\\nAbstract\\n\\n3D object detection techniques commonly follow a pipeline that aggregates predicted object central point features to compute candidate points. However, these candidate points contain only positional information, largely ignoring the object-level shape information. This eventually leads to sub-optimal 3D object detection. In this work, we propose AShapeFormer, a semantics-guided object-level shape encoding module for 3D object detection. This is a plug-n-play module that leverages multi-head attention to encode object shape information. We also propose shape tokens and object-scene positional encoding to ensure that the shape information is fully exploited. Moreover, we introduce a semantic guidance sub-module to sample more foreground points and suppress the influence of background points for a better object shape perception. We demonstrate a straightforward enhancement of multiple existing methods with our AShapeFormer. Through extensive experiments on the popular SUN RGB-D and ScanNetV2 dataset, we show that our enhanced models are able to outperform the baselines by a considerable absolute margin of up to 8.1%. Code will be available at https://github.com/ZechuanLi/AShapeFormer\\n\\n1. Introduction\\n\\nAs an important scene understanding task, 3D object detection [13, 20, 47] aims to detect 3D bounding boxes and semantic categories in 3D point cloud scenes. It plays an important role in many downstream tasks, such as augmented reality [2, 3], mobile robots [18, 41, 52], and autonomous navigation [1, 36, 37, 39]. Object detection has made significant progress in the 2D domain [15, 22, 33]. However, owing to the sparse and irregular nature of the point cloud data, 2D detection techniques are generally not readily applicable to the 3D object detection task.\\n\\nInspired by their 2D counterparts, early attempts in 3D object detection, e.g., [16, 39], mapped irregular point clouds to regular 3D voxels, thereafter using 3DCNNs for feature extraction and object detection. However, voxelization inevitably loses fine-grained information of the point clouds, which adversely affects the detection performance. With the advances that allow direct processing of the point clouds with deep neural models, e.g., [30, 31], recent methods aim at directly predicting the 3D bounding boxes from the original unordered point clouds. Among these techniques, VoteNet [29] and its variants [7, 12, 28, 45, 46, 50] have achieved remarkable performance.\\n\\nThese point-wise methods follow a common underlying pipeline which includes first aggregating certain predicted point features into candidate points. The candidate points are later used to estimate the 3D bounding box information, e.g., center, size, and orientation, along with the associated semantic labels. As illustrated in Fig. 1, despite their excellent performance, these methods still face a few major...\"}"}
{"id": "CVPR-2023-78", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"challenges. (1) The final prediction relies strongly on the quality of the candidate points. However, these points fail to encode important object-level features such as contours and the shape of the 3D objects. (2) The methods must regress over the candidate points, and these points are often influenced by the background points. This propagates the error to cause offsets in the eventual predictions. Current attempts to mitigate these issues rely on generating new features \\\\[7, 45\\\\] or sampling more points \\\\[42\\\\]. However, these are resource intensive solutions, which must still rely on the vote point regression quality.\\n\\nTo address the problems, we introduce a novel plug-n-play neural module, named AShapeFormer. It can be easily assembled with many existing 3D object detection methods to provide a considerable performance boost. Our key driving insight is that by utilizing implicit object-level shape features, a detector can be made aware of the object shape distribution. Specifically, our module utilizes multi-head attention to encode the object shape information. We aggregate the object shape features using a self-attention mechanism. Inspired by ViT \\\\[10\\\\] and BERT \\\\[19\\\\], we introduce a shape token as the output of the final shape feature to avoid information loss caused by simplistic operations, e.g., pooling. Additionally, we devise a semantic guidance mechanism to sample more foreground points and assign different weights to their features, which improves the shape feature generation. Semantic segmentation scores are also utilized during the aggregation of vote points to reduce the influence of irrelevant vote points and obtain better candidates.\\n\\nWe provide successful demonstration of boosting both point-based \\\\[28, 29\\\\] and Transformer \\\\[23\\\\] baselines with our method, achieving strong performance gains. Our experimental results (\u00a7 4.1) show that AShapeFormer boosts the multi-class mean average precision (mAP) up to 3.5% on the challenging SUN RGB-D dataset \\\\[38\\\\] and 8.1% on the ScanNet V2 dataset \\\\[9\\\\]. Highlights of our contributions include the following.\\n\\n- We propose a plug-and-play active shape encoding module named AShapeFormer, which can be combined with many existing 3D object detection networks to achieve a considerable performance boost.\\n- To the best of our knowledge, our method is the first to combine multi-head attention and semantic guidance to encode strong object shape features for robust classification and accurate bounding box regression.\\n- We demonstrate a considerable mAP boost on SUN RGB-D (mAP@0.25) and ScanNet V2 datasets by enhancing the state-of-the-art methods with our module.\\n\\n2. Related Work\\n\\nDue to its key importance in several downstream tasks, 3D object detection has recently attracted significant interest of the research community. According to the different backbones, indoor 3D object detection methods are mainly divided into the following categories.\\n\\n1. Voxel-based methods: Projecting the 3D point cloud to a regular 3D voxel representation can resolve the problems caused by the sparse and irregular nature of the point clouds. VoxNet \\\\[25\\\\] first used 3D convolutional network layers \\\\[21\\\\] for feature extraction and detection of point clouds exploiting voxelization. 3D-SIS \\\\[16\\\\] maps 2D image data to voxels, and realizes the fusion of multi-modal data to achieve better object detection and instance segmentation performance at the cost of more complex training process. GSDN \\\\[14\\\\] employs sparse convolution \\\\[48\\\\] to improve the efficiency of 3D convolution. Its encoder-decoder structure is built from sparse 3D convolution blocks. FCAF3D \\\\[34\\\\] adopts the basic architecture of GSDN to improve it as an anchor-free method. This is claimed to improve the efficiency and performance of the original proposal. Although 3D convolution can effectively process point cloud voxels, the process of voxelization inevitably damages the fine-grained information \\\\[39\\\\] in the point clouds. Moreover, the operation of filling zeros \\\\[51\\\\] in voxelization also introduces noise, which is detrimental to the detection accuracy.\\n\\n2. Voting-based methods: With the emergence of methods to directly process 3D point clouds under neural modelling, e.g., PointNet \\\\[30\\\\] and PointNet++ \\\\[31\\\\]; it has become viable to directly detect 3D objects in the original point clouds. Numerous point-based detection methods have recently appeared in the literature. Among them, VoteNet \\\\[29\\\\] has attracted considerable attention for the indoor 3D object detection. It redefines the traditional Hough voting process as an object center point regression problem through MLPs, and generates object proposals by sampling from multiple voting points within the same cluster. MLCVNet \\\\[46\\\\] introduces a context module based on the VoteNet to learn the contextual information of the scene, which helps in semantic understanding during the detection. H3DNet \\\\[50\\\\] selects the optimal solution from multiple voting results, and uses multiple geometric primitives to provide more and more accurate constraints to the bounding box. VeNet \\\\[45\\\\] uses customized modules for the VoteNet before, during and after voting, to gain an accuracy advantage. BRNet \\\\[7\\\\] proposes a representative point generation module to trace back virtually generated representative points from the voting, so that the network can take some advantage from the object shape.\\n\\n3. Transformer-based methods: Transformers \\\\[40\\\\] have achieved excellent performance in the field of NLP \\\\[19, 32\\\\] and 2D images \\\\[4, 10, 17, 27, 43\\\\]. They have also gradually started to surface in the techniques for point cloud processing. 3DETR \\\\[26\\\\] first proposed an end-to-end transformer model for 3D object detection, which almost maintains the vanilla Transformer architecture. Groupfree3D \\\\[23\\\\] uses a Transformer block to replace the group operation after...\"}"}
{"id": "CVPR-2023-78", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Schematics of AShapeFormer in action: Given an input point cloud of \\\\( N \\\\) points with XYZ coordinates, and optional corresponding RGB image \\\\( I \\\\), we sample seed points using a backbone. The sampled seed points are processed by a voting module. The seed and vote points are used by our AShapeFormer module. Within AShapeFormer, the Semantics Guided Module (SGM) pays more attention to the foreground points, while Shape Former additionally incorporates object-level shape features with multi-head attention, also employing positional encoding (PE). The Channel Attention Module (CAM) fuses the shape information enriched features with candidate features to provide candidates to a detection head that predicts the bounding boxes.\\n\\nObject shape awareness: Besides the three categories of methods discussed above, the dimension of object shape awareness in the existing literature also relates to our central idea. We can find instances where approaches implicitly or explicitly aim to perceive the object shape to achieve performance gains. For example, RGBNet [42] generates a certain number of rays centered on the voting point and uses the category of the points on the rays to perceive the surface geometry. However, a large number of manual parameters and feature extraction branches increase the complexity of the network and require a very long training time. BRNet [7] generatively backtracks the points to capture local structural features in the original point cloud. It essentially converts VoteNet into a two-stage detection method. HGNet [6] perceives the local shape information by modeling the relative position between the point clouds. However, object-level shape information is not used in the method, which still hinders accurate 3D bounding box estimation.\\n\\n3. Proposed Approach\\n\\nThis section describes the technical details of our approach, Active Shape Encoding via Transformer (AShapeFormer). An overview of the approach is first provided in \u00a7 3.1. In \u00a7 3.2 to \u00a7 3.5, we elaborate on the components and the learning objective of our technique. To discuss the method, we adopt a VoteNet [29] based backbone to extract features and sampling seed points for the AShapeFormer, and also use its detection head to output the 3D bounding boxes. However, it is emphasized that our central module is easily assembled with other detectors, e.g., [23, 28], due to the plug-n-play nature of AShapeFormer.\\n\\n3.1. Overview\\n\\nAs illustrated in Fig. 2, the input to our method is a 3D point cloud \\\\( P \\\\). It can optionally also contain the RGB image \\\\( I \\\\) of the scene. First, a PointNet++ [31] backbone is used to sample seed points. If the input data includes images, we optionally add a 2D backbone. The backbone extracts input features while sampling the seed points, and then obtains vote points through a voting step. The seed and vote points are fed to the AShapeFormer module. There are three main sub-modules in AShapeFormer, namely Semantics Guided Module (SGM), Channel Attention Module (CAM), and ShapeFormer. To better encode the object-level shape information, SGM gives more attention to the foreground points when sampling the seed points, and adaptively weights different features. ShapeFormer encodes the object-level shape features through a multi-head attention mechanism. CAM adaptively fuses shape features and the candidate features. Finally, the features enriched with object-level shape information are fed to the detection head to provide the 3D bounding boxes.\\n\\n3.2. Object-Level Shape Encoding\\n\\nIn this section, we introduce a naive shape encoding method and propose Channel Attention Module (CAM) to fuse the object-level shape feature and candidate feature. Indoor 3D object detection generally follows a pipeline that aggregates some predicted object centre features into candidate point features. Let us denote a set of seed points as \\\\( \\\\{ s_i \\\\}_{M_i=1}^M \\\\), where \\\\( s_i = [x_i; f_i] \\\\), with \\\\( x_i \\\\in \\\\mathbb{R}^3 \\\\) and \\\\( f_i \\\\in \\\\mathbb{R}^3 \\\\), and a set of vote points as \\\\( \\\\{ v_i \\\\}_{M_i=1}^M \\\\), where \\\\( v_i = [y_i; g_i] \\\\).\"}"}
{"id": "CVPR-2023-78", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Object-Level Shape Encoding without SGM and ShapeFormer. Notations: M&M denotes MLP followed by Max-pooling. FC is fully connected layer. \\\\( R^3 \\\\) \\\\( C \\\\). Here, \\\\( x_i \\\\) and \\\\( y_i \\\\) are the coordinates of the seed and vote points, respectively; and the vote point \\\\( y_i \\\\) is the center point of the object where \\\\( x_i \\\\) location is predicted by the voting module. The \\\\( f_i \\\\) and \\\\( g_i \\\\) denote the features of the seed and vote points, respectively. In the pipeline, \\\\( K \\\\) candidate points \\\\( \\\\{v_j\\\\}^{K'}_{j=1} \\\\) are obtained by sampling from \\\\( M \\\\) vote points \\\\( \\\\{v_i\\\\}^{M'}_{i=1} \\\\). Local grouping based on ball query \\\\([31]\\\\) centered on \\\\( \\\\{v_j\\\\}^{K'}_{j=1} \\\\) is done to obtain a set of candidate clusters \\\\( \\\\{C_1, C_2, \\\\ldots, C_K\\\\} \\\\). As shown in Fig. 3, each cluster \\\\( C_i = v_{1j}, v_{2j}, \\\\ldots, v_{nj} \\\\) gets a single vector representation as \\\\( p(C) = \\\\text{MaxPool}(\\\\text{MLP}(C)) \\\\), (1) where \\\\( p(C) \\\\) is the candidate feature. A few existing methods \\\\([28, 29]\\\\) directly feed \\\\( p(C) \\\\) to the detection head to predict the 3D bounding box. However, these center point features do not particularly help the network to perceive object-level information, which is sub-optimal. To sidestep the issue, BRNet \\\\([7]\\\\) and RBGNet \\\\([42]\\\\) aim at enabling object shape perception by generating pseudo-representative points. This improves the detection performance, ascertaining the importance of shape information in the task. However, these operations not only require a large number of hyper-parameters, but also ignore an important fact that as compared to the pseudo-representative point sets, real seed point clusters can describe the shape information of objects more accurately. This is a key insight used in our method. As illustrated in Fig. 3, we encode the object-level shape feature using the seed points located on the same object. The vote points belonging to the same object are much more compact than the seed points. Hence, using indices of the candidate local grouping is beneficial for grouping seed points into cluster \\\\( O = s_1j, s_2j, \\\\ldots, s_nj \\\\), which belong to the same object. To get a single vector representation of the object shape \\\\( p(O) \\\\), a naive approach can be similar to the candidate point local grouping, i.e., using MLP and Max-pooling \\\\( p(O) = \\\\text{MaxPool}(\\\\text{MLP}(O)) \\\\). We consider the above naive option at this stage to emphasize on the plug-n-play nature of our module as well as to keep the flow of discussion. We eventually use a sophisticated \u2018ShapeFormer\u2019 sub-module \u00a7 3.3 to account for the object shape. In any case, candidate features \\\\( p(C) \\\\) and object-level shape features \\\\( p(O) \\\\) are distributed in different feature spaces. Hence, in order to fuse these features, we propose the Channel Attention Module (CAM). As shown in Fig. 3, CAM consists of fully connected layers with sigmoid activations. It adaptively learns weights for the features in different spaces such that they seamlessly fuse in the later processing. The candidate and shape features are combined through CAM as follows \\\\( p(S) = \\\\text{Concat}(\\\\text{CAM}(p(C)), \\\\text{CAM}(p(O))) \\\\), (3) where \\\\( p(S) \\\\) is the new candidate feature, which is enriched with object-level shape information. The \\\\( p(S) \\\\) is eventually fed to a detection head to generate the 3D bounding boxes. We note that our experimental results (\u00a7 4.3) also find the naive object-level shape encoding method proposed in this section reasonably effective. However, the explicit contribution of shape information goes beyond that. In our experience, the naive method still suffers from two main problems. (1) It experiences loss of fine-grained information in the interaction of shape key points. (2) It is marred by interference of the background points. These problems are resolved with the use of the proposed ShapeFormer and the Semantic Guided Module (SGM), discussed below.\\n\\n3.3. ShapeFormer\\n\\nMulti-head attention \\\\([4, 19, 40]\\\\) is known for its ability to model contextual information. Its permutation invariant properties are also ideal for point cloud processing. However, its high computational requirement hinders its use for point clouds. We circumvent this issue by dealing with object specific points in our approach.\\n\\nWe introduce a shape encoding module based on a multi-head attention, named ShapeFormer - see Fig. 4. As compared to the naive shape encoding method discussed in the preceding section, ShapeFormer allows stronger shape encoding. In a naive construction, one gets a single vector representation of the object shape with an operation like max pooling, inevitably losing fine-grained information \\\\([35]\\\\). Addressing that, we prepend a learnable embedding shape token to the sequence of point features, whose state at the output of the ShapeFormer serves as the object-level shape feature. Specifically, given the seed points cluster on the same object (shape key points) \\\\( O = s_1j, s_2j, \\\\ldots, s_nj \\\\), its corresponding features are \\\\( F = f_1j, f_2j, \\\\ldots, f_nj \\\\). The input \\\\( z(0) \\\\) of ShapeFormer is \\\\( z(0) = f_0j; F + \\\\text{PE}(O) \\\\), (4)\"}"}
{"id": "CVPR-2023-78", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Eduardo Arnold, Omar Y Al-Jarrah, Mehrdad Dianati, Saber Fallah, David Oxtoby, and Alex Mouzakitis. A survey on 3D object detection methods for autonomous driving applications. *IEEE Transactions on Intelligent Transportation Systems*, 20(10):3782\u20133795, 2019.\\n\\n[2] Ronald T Azuma. A survey of augmented reality. *Presence: Teleoperators & Virtual Environments*, 6(4):355\u2013385, 1997.\\n\\n[3] Mark Billinghurst, Adrian Clark, Gun Lee, et al. A survey of augmented reality. *Foundations and Trends\u00ae in Human\u2013Computer Interaction*, 8(2-3):73\u2013272, 2015.\\n\\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In *European conference on computer vision*, pages 213\u2013229. Springer, 2020.\\n\\n[5] Chen Chen, Zhe Chen, Jing Zhang, and Dacheng Tao. Sasa: Semantics-augmented set abstraction for point-based 3D object detection. In *AAAI Conference on Artificial Intelligence*, volume 1, 2022.\\n\\n[6] Jintai Chen, Biwen Lei, Qingyu Song, Haochao Ying, Danny Z Chen, and Jian Wu. A hierarchical graph network for 3D object detection on point clouds. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 392\u2013401, 2020.\\n\\n[7] Bowen Cheng, Lu Sheng, Shaoshuai Shi, Ming Yang, and Dong Xu. Back-tracing representative points for voting-based 3D object detection in point clouds. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 8963\u20138972, 2021.\\n\\n[8] MMDetection3D Contributors. MMDetection3D: OpenMMLab next-generation platform for general 3D object detection. [https://github.com/open-mmlab/mmdetection3d](https://github.com/open-mmlab/mmdetection3d), 2020.\\n\\n[9] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3D reconstructions of indoor scenes. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 5828\u20135839, 2017.\\n\\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In *International Conference on Learning Representations*, 2021.\\n\\n[11] Yao Duan, Chenyang Zhu, Yuqing Lan, Renjiao Yi, Xinwang Liu, and Kai Xu. Disarm: Displacement aware relation module for 3D detection. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 16980\u201316989, 2022.\\n\\n[12] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias Nie\u00dfner. 3D-MPA: Multi-proposal aggregation for 3D semantic instance segmentation. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 9031\u20139040, 2020.\\n\\n[13] Yulan Guo, Hanyun Wang, Qingyong Hu, Hao Liu, Li Liu, and Mohammed Bennamoun. Deep learning for 3D point clouds: A survey. *IEEE transactions on pattern analysis and machine intelligence*, 43(12):4338\u20134364, 2020.\\n\\n[14] JunYoung Gwak, Christopher Choy, and Silvio Savarese. Generative sparse detection networks for 3D single-shot object detection. In *European conference on computer vision*, pages 297\u2013313. Springer, 2020.\\n\\n[15] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask R-CNN. In *Proceedings of the IEEE international conference on computer vision*, pages 2961\u20132969, 2017.\\n\\n[16] Ji Hou, Angela Dai, and Matthias Nie\u00dfner. 3D-SIS: 3D semantic instance segmentation of RGB-D scans. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 4421\u20134430, 2019.\\n\\n[17] Han Hu, Jiayuan Gu, Zheng Zhang, Jifeng Dai, and Yichen Wei. Relation networks for object detection. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 3588\u20133597, 2018.\\n\\n[18] Ahmed Hussein, Mohamed Medhat Gaber, Eyad Elyan, and Chrisina Jayne. Imitation learning: A survey of learning methods. *ACM Computing Surveys (CSUR)*, 50(2):1\u201335, 2017.\\n\\n[19] Jacob Devlin Ming-Wei Chang Kenton and Lee Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In *Proceedings of NAACL-HLT*, pages 4171\u20134186, 2019.\\n\\n[20] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In *Proceedings of the IEEE/CVF conference on computer vision and pattern recognition*, pages 12697\u201312705, 2019.\\n\\n[21] Bo Li. 3D fully convolutional network for vehicle detection in point cloud. In *2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)*, pages 1513\u20131518. IEEE, 2017.\\n\\n[22] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In *Proceedings of the IEEE conference on computer vision and pattern recognition*, pages 2117\u20132125, 2017.\\n\\n[23] Ze Liu, Zheng Zhang, Yue Cao, Han Hu, and Xin Tong. Group-free 3D object detection via transformers. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 2949\u20132958, 2021.\\n\\n[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In *International Conference on Learning Representations*, 2018.\\n\\n[25] Daniel Maturana and Sebastian Scherer. VoxNet: A 3D convolutional neural network for real-time object recognition. In *2015 IEEE/RSJ international conference on intelligent robots and systems (IROS)*, pages 922\u2013928. IEEE, 2015.\\n\\n[26] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-to-end transformer model for 3D object detection. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 2906\u20132917, 2021.\"}"}
{"id": "CVPR-2023-78", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
