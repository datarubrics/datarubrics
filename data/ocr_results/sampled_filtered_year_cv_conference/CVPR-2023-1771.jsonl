{"id": "CVPR-2023-1771", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Francesca Babiloni, Ioannis Marras, Filippos Kokkinos, Jiankang Deng, Grigorios G Chrysos, and Stefanos Zafeiriou. Poly-nl: Linear complexity non-local layers with polynomials. In International Conference on Computer Vision (ICCV), 2021.\\n\\n[2] Olivier Chapelle, Jason Weston, L\u00e9on Bottou, and Vladimir Vapnik. Vicinal risk minimization. In Advances in neural information processing systems (NeurIPS), pages 416\u2013422, 2001.\\n\\n[3] Moulik Choraria, Leello Tadesse Dadi, Grigorios G Chrysos, Julien Mairal, and Volkan Cevher. The spectral bias of polynomial neural networks. In International Conference on Learning Representations (ICLR), 2022.\\n\\n[4] Grigorios G Chrysos, Markos Georgopoulos, Jiankang Deng, Jean Kossaifi, Yannis Panagakis, and Anima Anandkumar. Augmenting deep classifiers with polynomial neural networks. In European Conference on Computer Vision (ECCV), 2022.\\n\\n[5] Grigorios G Chrysos, Stylianos Moschoglou, Giorgos Bouritsas, Yannis Panagakis, Jiankang Deng, and Stefanos Zafeiriou. \u03c0-nets: Deep polynomial neural networks. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\n[6] Dan Ciregan, Ueli Meier, and J\u00fcrgen Schmidhuber. Multi-column deep neural networks for image classification. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 3642\u20133649. IEEE, 2012.\\n\\n[7] Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215\u2013223. JMLR Workshop and Conference Proceedings, 2011.\\n\\n[8] Michael Cogswell, Faruk Ahmed, Ross Girshick, Larry Zitnick, and Dhruv Batra. Reducing overfitting in deep networks by decorrelating representations. In International Conference on Learning Representations (ICLR), 2016.\\n\\n[9] MMClassification Contributors. Openmmlab's image classification toolbox and benchmark. https://github.com/open-mmlab/mmclassification, 2020.\\n\\n[10] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 248\u2013255, 2009.\\n\\n[11] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.\\n\\n[12] Abhimanyu Dubey, Filip Radenovic, and Dhruv Mahajan. Scalable interpretability via polynomials. In Advances in neural information processing systems (NeurIPS), 2022.\\n\\n[13] Feng-Lei Fan, Mengzhou Li, Fei Wang, Rongjie Lai, and Ge Wang. Expressivity and trainability of quadratic networks. arXiv preprint arXiv:2110.06081, 2021.\\n\\n[14] Golnaz Ghiasi, Tsung-Yi Lin, and Quoc V Le. Dropblock: A regularization method for convolutional networks. arXiv preprint arXiv:1810.12890, 2018.\\n\\n[15] Xavier Glorot and Yoshua Bengio. Understanding the difficulty of training deep feedforward neural networks. In International Conference on Artificial Intelligence and Statistics (AISTATS), pages 249\u2013256, 2010.\\n\\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In International Conference on Computer Vision (ICCV), pages 1026\u20131034, 2015.\\n\\n[17] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770\u2013778, 2016.\\n\\n[18] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 7132\u20137141, 2018.\\n\\n[19] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 4700\u20134708, 2017.\\n\\n[20] Lei Huang, Yi Zhou, Fan Zhu, Li Liu, and Ling Shao. Iterative normalization: Beyond standardization towards efficient whitening. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 4874\u20134883, 2019.\\n\\n[21] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning, pages 448\u2013456. PMLR, 2015.\\n\\n[22] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\\n\\n[23] Alex Krizhevsky, Vinod Nair, and Geoffrey Hinton. Cifar-100 (canadian institute for advanced research).\\n\\n[24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In Advances in neural information processing systems (NeurIPS), pages 1097\u20131105, 2012.\\n\\n[25] Ya Le and Xuan Yang. Tiny imagenet visual recognition challenge. CS 231N, 7:7, 2015.\\n\\n[26] Xiang Li, Wenhai Wang, Xiaolin Hu, and Jian Yang. Selective kernel networks. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 510\u2013519, 2019.\\n\\n[27] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Conference on Computer Vision and Pattern Recognition (CVPR), pages 2117\u20132125, 2017.\\n\\n[28] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In International Conference on Computer Vision (ICCV), pages 2980\u20132988, 2017.\\n\\n[29] Julien Mairal. End-to-end kernel learning with supervised convolutional kernel networks. Advances in neural information processing systems (NeurIPS), 29, 2016.\\n\\n[30] Julien Mairal, Piotr Koniusz, Zaid Harchaoui, and Cordelia Schmid. Convolutional kernel networks. Advances in neural information processing systems (NeurIPS), 27, 2014.\"}"}
{"id": "CVPR-2023-1771", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2023-1771", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Regularization of polynomial networks for image recognition\\n\\nGrigorios G Chrysos\\nBohan Wang\\nJiankang Deng\\nVolkan Cevher\\n\\n1 LIONS, EPFL, Lausanne, Switzerland\\n2 Huawei UKRD\\n\\n[name.surname]@epfl.ch, jiankangdeng@gmail.com\\n\\nAbstract\\n\\nDeep Neural Networks (DNNs) have obtained impressive performance across tasks, however they still remain as black boxes, e.g., hard to theoretically analyze. At the same time, Polynomial Networks (PNs) have emerged as an alternative method with a promising performance and improved interpretability but have yet to reach the performance of the powerful DNN baselines. In this work, we aim to close this performance gap. We introduce a class of PNs, which are able to reach the performance of ResNet across a range of six benchmarks. We demonstrate that strong regularization is critical and conduct an extensive study of the exact regularization schemes required to match performance. To further motivate the regularization schemes, we introduce D-PolyNets that achieve a higher-degree of expansion than previously proposed polynomial networks. D-PolyNets are more parameter-efficient while achieving a similar performance as other polynomial networks. We expect that our new models can lead to an understanding of the role of elementwise activation functions (which are no longer required for training PNs). The source code is available at https://github.com/grigorisg9gr/regularized_polynomials.\\n\\n1. Introduction\\n\\nDeep neural networks (DNNs) are dominating the research agenda in computer vision since the previous decade owing to their stellar performance in image recognition [17, 24] and object detection [27, 28]. The design of tailored normalization schemes [21], data augmentation [11] and specific architectural blocks [19, 42] have further fostered this trend. However, our theoretical understanding of DNNs pales in comparison. There is little progress in making DNNs interpretable, or a principled understanding of the training dynamics or the role of the network depth.\\n\\nSo far, a handful of works have attempted to mitigate that lack of understanding by designing principled architectures. Combining neural networks with the research on kernel methods has emerged for designing principled architectures with guarantees. In [30], the kernel feature map of the training data is used for achieving invariance to certain transformations. Recently, high-performing kernels were used for defining a principled architecture [40]. Using fixed components such as wavelets has been considered for replacing the learnable convolutions [33]. Another approach approximates the target function with a polynomial expansion. Polynomial Nets (PNs) rely on capturing higher-order correlations of the input data for expressing the output without the use of elementwise activation functions [37]. Despite the progress in the principled design of networks, the aforementioned works have yet to achieve a performance comparable to standard baselines, such as the performance of the seminal residual neural networks (ResNet) [17].\\n\\nIn this work, we aim to close the gap between well-established neural network architectures and principled architectures, enabling polynomial networks to reach the performance of the powerful neural networks across a range of tasks.\"}"}
{"id": "CVPR-2023-1771", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"architectures by focusing on the PNs. In particular, we concentrate on the recent parametrization of $\\\\Pi$-Nets [5] that has outperformed the aforementioned principled methods. We validate our hypothesis that the performance of PNs can be significantly improved through strong regularization schemes. To this end, we introduce a class of polynomial networks, called $R$-PolyNets. In our study, we explore which regularization schemes can improve the performance of PNs. For instance, we find that initializations proposed for neural networks [15, 36] are not optimal for PNs. Overall, our exploration enables $R$-PolyNets to achieve performance on par with the (unregularized) ResNet, which is the de facto neural network baseline.\\n\\nTo further motivate our regularization schemes, we design a new class of polynomial expansions achieving a higher total degree of expansion than previous PNs. In $R$-PolyNets, the final degree of expansion is obtained by a sequential concatenation of a series of lower-degree polynomial expansions. That is, $R$-PolyNets concatenate $N$ polynomials of second-degree to obtain a $2N$ polynomial expansion. Instead, we use outputs from previous polynomials in the current expansion, increasing the previous total degree. Our goals are twofold: a) transfer representations from earlier polynomials, b) increase the total degree of polynomial expansion. The proposed regularization schemes are critical for training these dense polynomials, named $D$-PolyNets. We showcase that $D$-PolyNets are more expressive than previously proposed polynomial expansions. Overall, our contributions can be summarized as follows:\\n\\n\u2022 We introduce a class of regularized polynomial networks, called $R$-PolyNets, in sec. 3.\\n\u2022 We propose densely connected polynomials, called $D$-PolyNets. $D$-PolyNets use multiple terms from a previous polynomial as input to the current polynomial resulting in a higher-degree of expansion than previous PNs (sec. 4).\\n\u2022 Our thorough validation in both image and audio recognition illustrates the critical components for achieving performance equivalent to vanilla DNNs.\\n\\n2. Related work\\n\\n2.1. Polynomial networks\\nPolynomial networks (PNs) capture higher-order interactions between the input elements using high-degree polynomial expansions. PNs have demonstrated a promising performance in standard benchmarks in image generation [5] and image recognition [4]. Beyond the empirical progress, various (theoretical) properties of PNs have been recently explored [34, 53]. In particular, PNs augment the expressivity of DNNs [13], while they offer benefits in the extrapolation or learning high-frequency functions [3, 47]. More importantly, the recent work of [12] highlights how PNs can learn powerful interpretable models.\\n\\nWhen PNs are combined with element-wise activation functions, they can achieve state-of-the-art performance as demonstrated in [1, 5, 18, 26, 44, 49, 50]. However, many of the beneficial properties, such as the interpretability are not applicable for these models. Therefore, the hybrid models combining polynomial expansions with activation functions are not the focus of our work, since they share similar drawbacks to DNNs.\\n\\n2.2. Regularization of neural networks\\nDeep neural networks (DNNs) can be prone to overfitting and regularization methods are widely used to mitigate this issue. Hence, we summarize below three categories of regularization techniques: a) data augmentation, b) intermediate learned features and c) auxiliary loss terms.\\n\\nData augmentation: Data augmentation techniques are often used in image recognition pipelines [11, 51, 52]. Mixup [52] uses a linear interpolation between two training samples to improve generalization based on empirical vicinal risk minimization [2]. Cutout [11] removes contiguous regions from the input images, generating augmented training dataset with partially occluded versions of existing samples. This enables the network to focus on non-dominant part of the training sample. CutMix [51] combines the previous two augmentation methods by replacing a patch of the image with a patch from another training image.\\n\\nFeature normalization: Apart from the data augmentation, feature normalization enables deeper neural networks to be trained. Dropout [41] is a prominent example of such feature normalization techniques. Dropout randomly drops units (and the corresponding connections) during training to avoid co-adaptation of units. Despite its initial success, dropout has not been widely used with convolutional neural nets. Instead, dropblock [14] randomly drops a contiguous region of a feature map. Additional regularization schemes, such as average or max pooling, rely on the idea of aggregating features from a local region of the feature map to avoid the sensitivity to small spatial distortions [6].\\n\\nAuxiliary loss terms: In addition to the classification losses, additional loss terms, such as Tikhonov regularization, can result in more stable learning and can avoid overfitting [39] [Chapter 13]. Weight decay forces sparsity on the weights by penalizing their norm. [8] proposes to decorrelate the different units by encouraging the covariance matrix of the features to be close to the identity matrix. For classification problems, label smoothing can prevent the networks from predicting the training examples over-confidently [31].\\n\\nDespite the progress in regularization schemes and the various theoretical connections, no consensus has been reached over a single regularization scheme that performs better.\"}"}
{"id": "CVPR-2023-1771", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Nomenclature on the symbols used in this work\\n\\n| Symbol | Dimension(s) | Definition |\\n|--------|--------------|------------|\\n| n, N   | N           | Polynomial term degree, total approximation degree. |\\n| r      | N           | Rank of the decomposition. |\\n| z      | R           | Input to polynomial expansion. |\\n| B, \u03b8   | R \u00d7 r, R_o  | Parameters in the decomposition. |\\n| H[n], J[n], K[n] | R_d \u00d7 r, R_k \u00d7 r, R_\u03c9 \u00d7 r | Parameters in the hierarchical decomposition. |\\n| \u03a6, \u03a8   | R \u00d7 r, R_r  | Regularization matrices. |\\n\\n\u2217 \u2212 Hadamard (element-wise) product.\\n\\nWell in all cases, so often a combination of regularization schemes are used in modern image recognition pipelines.\\n\\n3. Regularizing polynomial networks\\n\\nIn this section, we introduce R-PolyNets in sec. 3.1, while we refer to the training techniques used in sec. 3.2.\\n\\nNotation: Vectors (or matrices) are indicated with lowercase boldface letters e.g., \\\\( \\\\mathbf{x} \\\\) (or \\\\( \\\\mathbf{X} \\\\)). Tensors are identified by calligraphic letters, e.g., \\\\( \\\\mathcal{X} \\\\). The main symbols along with their dimensions are summarized in Table 1.\\n\\n3.1. Proposed model\\n\\nOne critical component for learning PNs is their regularization [45]. To this end, we introduce a class of PNs, called R-PolyNets. R-PolyNets include two regularization matrices \\\\( \\\\Phi, \\\\Psi \\\\) that can result in different normalization schemes as we indicate below.\\n\\nWe express an \\\\( N \\\\)th degree polynomial expansion of the input vector \\\\( z \\\\) with a simple recursive equation as follows:\\n\\n\\\\[\\ny_n = \\\\Phi H^T[n] z \\\\ast \\\\Psi J^T[n] y_n - 1 + K^T[n] k[n]_o + y_{n-1},\\n\\\\]\\n\\n(1)\\n\\nwhere \\\\( H[n], J[n], K[n] \\\\) are trainable parameters. Eq. (1) can be recursively applied for \\\\( n = 2, \\\\ldots, N \\\\) for an \\\\( N \\\\)th degree polynomial expansion with \\\\( y_1 := z \\\\) and \\\\( y := B + \\\\theta \\\\). The output \\\\( y \\\\) captures high-order correlations between the input elements of \\\\( z \\\\). Eq. (1) enables us to build a polynomial expansion of arbitrary degree; we demonstrate in Fig. 2 how this can be implemented for 3rd degree expansion. Each term in Eq. (1) has a specific purpose: a) \\\\( H[n] \\\\) performs a linear transformation of the input, b) \\\\( J[n] \\\\) performs a linear transformation of the output of the previous layer, as performed in regular neural networks. The resulting two representations are then multiplied element-wise, and a skip connection is added.\\n\\nOur method allows for a variety of normalization schemes through the matrices \\\\( \\\\Phi \\\\) and \\\\( \\\\Psi \\\\). For example, we can use the matrix \\\\( I - h \\\\rightarrow 1 \\\\) (where \\\\( I \\\\) is the identity matrix, \\\\( h \\\\) is the dimensionality of the vector being multiplied, and \\\\( - \\\\rightarrow 1 \\\\) is a matrix of ones) to subtract the mean from each element, effectively creating a zero-mean vector. This extends the previously proposed \\\\( \\\\Pi \\\\)-Nets and can recover it as a special case when \\\\( \\\\Phi \\\\) and \\\\( \\\\Psi \\\\) are both identity transformations.\\n\\nHowever, we emphasize that normalization is necessary in our experimentation, making our method a significant extension of the previously proposed PNs in achieving performance on par with DNNs. We develop the details of the normalization schemes used in practice in the next section.\\n\\nWe provide an additional model for our new method in case a different underlying tensor decomposition is selected, which is discussed in sec.B. It is worth noting that, in practice, convolutions are often used instead of the full matrices in Eq. (1). This aligns with the implementation choices of our prior works [4, 5].\\n\\n3.2. Training Configuration\\n\\nWe propose a number of techniques that enforce stronger regularization of the PNs. The regularization schemes are divided into three categories: initialization (sec. 3.2.1), normalization (sec. 3.2.2) and auxiliary regularization (sec. 3.2.3). The precise details of each training configuration are offered in the supplementary, e.g., in Table 9.\\n\\n3.2.1 Initialization scheme\\n\\nThe initialization of the weights is a critical topic in deep (convolutional) neural networks [16]. The proposed initialization schemes are developed either for fully-connected neural networks [15] or for convolutional neural networks [16]. However, PNs do not belong in the aforementioned classes of neural networks, thus we need a new initialization approach.\"}"}
{"id": "CVPR-2023-1771", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In our preliminary experiments, we noticed that the high-degree polynomial expansions are sensitive to the initialization scheme, and values closer to zero work better. We propose a simple initialization scheme below and defer the theoretical analysis on the initialization schemes of PNs to a future work.\\n\\n**Technique 1**\\n\\nLet $H[n]$, $J[n]$, $K[n] \\\\sim N(0, \\\\sigma^2 I)$ with $\\\\sigma = r D M_n$, (2) for $n = 1, \\\\ldots, N$ with $M_n$ the total number of polynomial parameters of $n$th order. In other words, we initialize polynomial parameters with zero-mean gaussian distribution. In practice, we choose $D = 16$.\\n\\n**3.2.2 Normalization scheme**\\n\\nNormalization is a core component for training of DNNs, while we expect normalization to have a significant role in enabling training PNs. Despite the popularity of batch normalization (BN) [21], BN normalizes the features across a batch of samples, which might not be ideal for high-degree expansions. Instead, instance normalization (IN) [43] computes the mean and variance for each sample and each channel to normalize the features. However, a combination of both can be beneficial for our goal.\\n\\n**Technique 2**\\n\\nWe adopt the normalization scheme IBN, which combines instance normalization (IN) and batch normalization (BN) for PNs [35]. For each block in the first three layers, we apply IN for $0.8 \\\\cdot C$ (number of channels produced by the convolution) channels, and BN for the other channels, after the first convolution. In the final layer, we only implement batch normalization to preserve discrimination between individual representations in the latent space. Note that the parameters $\\\\Phi$, $\\\\Psi$ of Eq. (1) are implemented using these normalization schemes.\\n\\n**3.2.3 Auxiliary regularization schemes**\\n\\nThree auxiliary regularization schemes are used: one auxiliary loss, one feature augmentation, and one feature regularization. Following the convention of $\\\\Pi$-Nets, the network consists of product of polynomials of the form of Eq. (1). That is, the output of each polynomial is used as the input to the next polynomial, which results in a higher degree polynomial expansion, increasing the significance of regularization. We utilize Label smoothing [31], DropBlock [14] and max pooling. Max pooling is applied after each polynomial expansion except the final one.\\n\\n**Technique 3**\\n\\nWe adopt Label smoothing [31], DropBlock [14] and max pooling in the proposed framework. Label smoothing is applied on the labels and Dropblock on the feature maps. We add max pooling layer after each individual polynomial expansion (of the form of Eq. (1)) in $R$-PolyNets.\\n\\n**4. Dense connections across polynomials**\\n\\nTo showcase the representative power of the regularized polynomial expansion, we propose a new type of PNs, called $D$-PolyNets. In the polynomial networks proposed above (or in the literature), a sequence of polynomial expansions is used with the output of $i$th polynomial being used as the input of $(i+1)$th polynomial. Adding $N$ such second-degree polynomials results in an overall polynomial expansion of degree $2N$.\\n\\nIn $D$-PolyNets, we enable additional connections across polynomials which results in a higher degree of expansion. To achieve that we enable outputs from the $i$th polynomial being used as a) input to the next polynomial, b) as a term in the Hadamard products of a next polynomial. Let us assume that each polynomial includes a single recursive step with potentially multiple terms. In Eq. (1), taking a single recursive step (i.e., $n = 2$) includes a Hadamard product between a filtered version of the input $z$ and a filtered version of the previous recursive term $y_1$. On the contrary, in $D$-PolyNets, a single recursive term includes both of the aforementioned terms along with the outputs from previous polynomials. The schematic in Fig. 3 depicts $D$-PolyNets assuming each polynomial includes a single recursive step. This can be trivially extended to any number of recursive steps, while each polynomial can also rely on a different tensor decomposition.\"}"}
{"id": "CVPR-2023-1771", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the recursive formulation of $D$-PolyNets based on (1) is the following expressions:\\n\\n$$y[i]_n = y[i]_{n-1} + \\\\Phi H^T[n] z^* \\\\Psi J^T[n] y[n-1] + K^T[n] k[n] \\\\ast i - 1 \\\\tau = 1 y[\\\\tau]_n.$$ (3)\\n\\nThe total degree of expansion in Eq. (3) is higher than the corresponding one in $R$-PolyNets or $\\\\Pi$-Nets. This makes the requirement for strong regularization imperative to avoid exploding gradients.\\n\\nEquation (1) enables us to build a regularized polynomial expansion of arbitrary degree, $y[n]$. Apart from the training techniques in sec 3.2, we propose below specific techniques to enforce a strong regularization of $D$-PolyNets.\\n\\nTraining Configuration of $D$-PolyNets:\\n\\nOur preliminary experiments indicate that iterative normalization [20] is beneficial in this case. Additionally, we include a learnable parameter $\\\\rho_\\\\tau$ which regularizes the contribution of each previous polynomial in the current Hadamard product.\\n\\n5. Experiments\\n\\nIn this section, we evaluate the proposed models across a range of six in image recognition and one standard dataset in audio recognition. We describe below the datasets, and the setup, then we conduct an ablation study to evaluate the contribution of different techniques in sec. 5.1. Subsequently, we conduct the main body of the experiments in various widely-used datasets in sec. 5.2 and sec. 5.3. We extend beyond the standard image classification tasks, with audio classification and fine-grained classification in sec. 5.4 and sec. 5.5 respectively. Details on the datasets along with additional experiments (including an experiment with deeper networks and the runtime comparison in FLOPs) are developed in sec. D. The results in the supplementary verify the empirical findings below, while the proposed $R$-PolyNets has a similar number of FLOPs as the $\\\\Pi$-Nets, e.g., in Table 21.\\n\\nTraining details: Each model is trained for 120 epochs with batch size 128. The SGD optimizer is used with initial learning rate of 0.1. The learning rate is multiplied with a factor of 0.1 in epochs 40, 60, 80, 100. For Tiny ImageNet, the learning rates of $R$-PolyNets and $D$-PolyNets are multiplied by 0.92 every epoch. For data augmentation we adopt random cropping with 4-pixel padding and horizontal flipping. Unless mentioned otherwise, each experiment is conducted 5 times and the average and the standard deviations are also reported.\\n\\nCompared methods: The family of $\\\\Pi$-Nets is the main baseline we use in our comparisons. Namely, the $\\\\Pi$-Nets use the structure of ResNet18, where each block from the original ResNet is converted into a second-degree polynomial expansion. As a reminder, $\\\\Pi$-Nets do not use element-wise activation functions, and result in a high-degree polynomial expansion. The recent PDC [4] is also added as a baseline with many of its properties shared with $\\\\Pi$-Nets.\\n\\nWe also report the accuracy of two further methods: (a) the popular ResNet18, which is the de facto baseline in image recognition, (b) hybrid $\\\\Pi$-Nets, i.e., polynomial expansions with element-wise activation functions. The last two methods are added as a reference (thus added with grayscale color). Outperforming the vanilla ResNet18 or the hybrid $\\\\Pi$-Nets is not our goal in this work. We aim at demonstrating for the first time that polynomial expansions can be on par with feed-forward neural networks.\\n\\n5.1. Ablation study\\n\\nBelow, we conduct three ablation experiments. In the first experiment, we showcase how the different components proposed in sec. 3 can decrease the error in image recognition. We choose Cifar-100 for this experiment. To facilitate the presentation, we gradually insert different regularization components on $\\\\Pi$-Nets to advocate for stronger regularization techniques and highlight their benefit in the final accuracy.\\n\\nFig. 4 summarizes the results of the experiment. Notice that the initialization plus the normalization scheme already makes a significant impact on the accuracy. Then, max pooling, the feature augmentation and regularized loss contribute to reduce overfitting and to achieve the final performance. In the next experiments, we consider the last row of Fig. 4 as the model that is used for the main comparisons.\"}"}
{"id": "CVPR-2023-1771", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Accuracy of R-PolyNets (IBN + max pooling + Drop-block + Label smoothing) and D-PolyNets (IBN + max pooling + Dropblock + Label smoothing) with different initialization schemes on Cifar-100. Note that D-PolyNets contains 7M parameters (down from the 11M of R-PolyNets) and one block less than R-PolyNets.\\n\\n| Model Initialization | Accuracy (%) |\\n|----------------------|--------------|\\n| Xavier               | 0.765 \u00b1 0.002|\\n| Orthogonal           | 0.765 \u00b1 0.001|\\n| Kaiming normal       | 0.767 \u00b1 0.003|\\n| Kaiming uniform      | 0.767 \u00b1 0.004|\\n| zero-mean            | 0.769 \u00b1 0.002|\\n\\nIn the second experiment, we utilize well-established initialization schemes, i.e., Xavier initialization [15], orthogonal matrix initialization [36], Kaiming initialization [16], and evaluate their performances on the proposed R-PolyNets. The results in Table 2 indicate that previously proposed initializations cannot perform as well in the R-PolyNets. This is not contradictory to the studies of those initialization schemes, since they were derived for the neural network structure, which differs substantially from the structure of PNs.\\n\\nIn the third experiment, we vary the degree of polynomials applied in R-PolyNets. The results in Fig. 5 indicate that an increasing degree of polynomial expansion can increase the accuracy. Given the correspondence between the standard ResNet18 with 8 residual blocks and the 2^8 degree polynomial, we use this 2^8 degree in the rest of the experiments unless explicitly stated otherwise.\\n\\n5.2. Image classification on smaller datasets\\n\\nWe conduct our main experimentation in the following four datasets: Cifar-10, Cifar-100, STL-10, Tiny ImageNet. Table 3 exhibits the accuracy of each compared method across all datasets. The results indicate that the R-PolyNets consistently outperform the baseline \u03a0-Nets by a large margin. In STL-10 the accuracy increases from 56.3% to 82.8%, which is a 47.1% relative increase in the performance. In Fig. 6 the test accuracy per epoch is depicted; notice that the proposed method has a consistently higher accuracy over the \u03a0-Nets throughout the training. This consistent improvement demonstrates the efficacy of the proposed method. In Table 3, ResNet18 and hybrid \u03a0-Nets are added. R-PolyNets achieve a higher performance than ResNet18 and hybrid \u03a0-Nets on the three benchmarks of Cifar-10, Cifar-100 and STL-10, while the three methods perform on par on Tiny ImageNet. These observations verify our proposal that regularized polynomials can achieve a similar performance with the standard baselines of neural networks or hybrid \u03a0-Nets.\\n\\nDiscussion on R-PolyNets: A reasonable question would be whether the studied training techniques are unique in enabling the training of R-PolyNets. Our preliminary experiments with alternative methods indicate that different combinations of training techniques can indeed perform well. However, the proposed techniques are the only ones we found that perform well in a range of datasets. To verify this, we conducted two experiments to assess alternative training techniques of R-PolyNets.\\n\\nIn the first experiment, we increase the weight decay to train R-PolyNets on Cifar-100. We notice that max pooling and Label smoothing can prevent R-PolyNets from overfitting the train samples. However, the alternative regularization schemes may also help R-PolyNets achieve the similar final performance. To verify this, we conduct another experiment of training R-PolyNets with different regularization techniques. The result is presented in Table 6.\\n\\nAs the results in Table 5 and Table 6 illustrate, different combinations of regularization techniques can indeed improve the test performance of R-PolyNets. Notice that the CutMix and Stochastic depth can also help PN without element-wise activation functions perform on par with established DNNs. However, we find their behavior is dataset-dependent, so we do not include them in our final scheme.\\n\\nDiscussion on D-PolyNets: The results in Table 3 demonstrate that D-PolyNets can match the test performance of R-PolyNets, which both outperform the baseline \u03a0-Nets. However, D-PolyNets have 41.7% fewer parameters. The representation power of D-PolyNets is improved, because the total degree of polynomial expansion output\\n\\n16128\\n\\n16128\"}"}
{"id": "CVPR-2023-1771", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Test error on (a) Cifar-10, (b) Cifar-100 and (c) STL-10. The highlighted region depicts the variance in the accuracy. Interestingly, the proposed $R$-PolyNets and $D$-PolyNets outperform the $\\\\Pi$-Nets from the first few epochs, while the absolute difference in the error is not decreasing as the training progresses. Notice that the proposed training techniques enable $R$-PolyNets and $D$-PolyNets to be on par with the ResNet18 baseline in STL-10 and even outperform the baselines in Cifar-100.\\n\\nTable 3. Accuracy on Cifar-10, Cifar-100, STL-10 and Tiny ImageNet. The symbol '# par' abbreviates the number of parameters. $D$-PolyNets containing 7M parameters. Note that $R$-PolyNets and $D$-PolyNets without activation functions can outperform $\\\\Pi$-Nets without activation functions significantly on Cifar-10, Cifar-100, STL-10 and Tiny ImageNet. Moreover, $R$-PolyNets and $D$-PolyNets can match the performances of baseline models (e.g. $\\\\Pi$-Nets with activation functions and ResNet18) on Cifar-10, Cifar-100 and STL-10.\\n\\n| Dataset     | Model       | # par (M) | Accuracy (\u00b1)     |\\n|-------------|-------------|-----------|------------------|\\n| Cifar-10    | ResNet18    | 11.2       | 0.944 \u00b1 0.001    |\\n|             | Hybrid $\\\\Pi$-Nets | 6.0   | 0.944 \u00b1 0.002    |\\n|             | PDC         | 5.4       | 0.909 \u00b1 0.002    |\\n|             | $\\\\Pi$-Nets  | 11.9       | 0.907 \u00b1 0.003    |\\n|             | $R$-PolyNets | 11.9       | 0.945 \u00b1 0.000    |\\n|             | $D$-PolyNets | 7.1        | 0.947 \u00b1 0.002    |\\n| Cifar-100   | ResNet18    | 11.2       | 0.760 \u00b1 0.003    |\\n|             | Hybrid $\\\\Pi$-Nets | 6.1   | 0.765 \u00b1 0.004    |\\n|             | PDC         | 5.5       | 0.689 \u00b1 0.002    |\\n|             | $\\\\Pi$-Nets  | 11.9       | 0.677 \u00b1 0.006    |\\n|             | $R$-PolyNets | 11.9       | 0.769 \u00b1 0.002    |\\n|             | $D$-PolyNets | 7.2        | 0.767 \u00b1 0.003    |\\n| STL-10      | ResNet18    | 11.2       | 0.741 \u00b1 0.016    |\\n|             | Hybrid $\\\\Pi$-Nets | 6.0   | 0.775 \u00b1 0.006    |\\n|             | PDC         | 5.4       | 0.681 \u00b1 0.006    |\\n|             | $\\\\Pi$-Nets  | 11.9       | 0.563 \u00b1 0.008    |\\n|             | $R$-PolyNets | 11.9       | 0.828 \u00b1 0.003    |\\n|             | $D$-PolyNets | 7.1        | 0.834 \u00b1 0.006    |\\n| Tiny ImageNet | ResNet18    | 11.3       | 0.615 \u00b1 0.002    |\\n|             | Hybrid $\\\\Pi$-Nets | 6.1   | 0.611 \u00b1 0.004    |\\n|             | PDC         | 5.5       | 0.452 \u00b1 0.002    |\\n|             | $\\\\Pi$-Nets  | 12.0       | 0.502 \u00b1 0.007    |\\n|             | $R$-PolyNets | 12.0       | 0.615 \u00b1 0.004    |\\n|             | $D$-PolyNets | 7.2        | 0.618 \u00b1 0.001    |\\n\\n5.3. ImageNet Classification\\nWe conduct a large-scale classification experiment on ImageNet [10]. We employ the mmclassification toolkit [9] to train networks on the training set and report the 224\u00d7224 single-crop top-1 and the top-5 errors on the validation set. Our pre-processing and augmentation strategy follows the settings of the baseline (i.e., ResNet18). All models are trained for 100 epochs on 8 GPUs with 32 images per GPU (effective batch size of 256) with synchronous SGD of momentum 0.9. The learning rate is initialized to 0.1, and decays by a factor of 10 at the 30th, 60th, and 90th epochs. The results in Table 4 validate our findings on the rest of the datasets and confirm that $R$-PolyNets are able to reach the performance of standard neural network baselines. $D$-PolyNets perform on par with $R$-PolyNets, while having a slightly reduced number of parameters. The reduction in parameters is smaller than the respective numbers with smaller scale datasets (e.g., Table 3), which might indicate that further regularization is required for $D$-PolyNets when scaled to more complex datasets.\\n\\n5.4. Audio classification\\nWe perform an experiment on the Speech Commands dataset to evaluate $R$-PolyNets and $D$-PolyNets on a distribution that differs from that of natural images. The accuracy for each model is reported in Table 7. Noticeably, hybrid $\\\\Pi$-Nets, $R$-PolyNets, $D$-PolyNets and ResNet18 can achieve the accuracy over 0.975, which showcases the representational power of $R$-PolyNets and $D$-PolyNets on audio classification. By comparison, $\\\\Pi$-Nets and PDC has accuracy below 0.975.\\n\\n5.5. Fine-grained classification\\nWe conduct one last experiment on fine-grained classification to validate further the regularization scheme of $R$-PolyNets and $D$-PolyNets.\"}"}
{"id": "CVPR-2023-1771", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We compare our models with state-of-the-art deep convolutional neural networks, \\\\( \\\\Phi \\\\)-Nets and hybrid \\\\( \\\\Phi \\\\)-Nets. We report the top-1 and top-5 accuracy on the validation set of ImageNet as well as the number of parameters. Our models are highlighted in gray. The symbol '# par' abbreviates the number of parameters.\\n\\n| Model Image Size | # par (M) | Top-1 Acc. (%) | Top-5 Acc. (%) |\\n|------------------|-----------|----------------|----------------|\\n| ImageNet-1K trained models | | | |\\n| ResNet18 224 | 2 | 69.69 | 89.078 |\\n| ResNet18 without activations 224 | 2 | 69.536 | 89.986 |\\n| Hybrid \\\\( \\\\Phi \\\\)-Nets 224 | 2 | 70.740 | 89.548 |\\n| \\\\( \\\\Phi \\\\)-Nets 224 | 2 | 70.380 | 89.280 |\\n| \\\\( R \\\\)-PolyNets 224 | 2 | 70.380 | 89.228 |\\n| \\\\( D \\\\)-PolyNets 224 | 2 | 70.380 | 89.228 |\\n\\nTable 5. The impact of weight decay changes on \\\\( R \\\\)-PolyNets (IBN + max pooling + Dropblock + Label smoothing) trained on Cifar-100.\\n\\n| Weight decay | Accuracy |\\n|--------------|----------|\\n| \\\\( 5 \\\\times 10^{-4} \\\\) | 0.766 \u00b1 0.002 |\\n| \\\\( 6 \\\\times 10^{-4} \\\\) | 0.768 \u00b1 0.004 |\\n| \\\\( 7 \\\\times 10^{-4} \\\\) | 0.768 \u00b1 0.002 |\\n\\nTable 6. Accuracy on Cifar-100. Note that Data aumentation (i.e. Cutmix) can achieve the best test performance. Overall, these alternative training techniques obtain a state-of-the-art result with respect to the baseline model (ResNet18).\\n\\n| Models | Accuracy |\\n|--------|----------|\\n| \\\\( R \\\\)-PolyNets (IBN + maxpooling + Dropblock + Label smoothing) | 0.766 \u00b1 0.002 |\\n| \\\\( R \\\\)-PolyNets (IBN + maxpooling + Dropblock + CutMix) | 0.771 \u00b1 0.002 |\\n| \\\\( R \\\\)-PolyNets (IBN + maxpooling + Stochastic depth + Label smoothing) | 0.769 \u00b1 0.002 |\\n\\nTable 7. Accuracy on Speech Command. Note the Hybrid \\\\( \\\\Phi \\\\)-Nets, \\\\( R \\\\)-PolyNets, \\\\( D \\\\)-PolyNets and ResNet18 can achieve the same test performance.\\n\\n| Dataset Model | # par (M) | Accuracy |\\n|---------------|-----------|----------|\\n| Speech command | | | |\\n| ResNet18 11.2 | | 0.977 |\\n| Hybrid \\\\( \\\\Phi \\\\)-Nets 6.0 | | 0.977 |\\n| PDC 5.5 | | 0.972 |\\n| \\\\( \\\\Phi \\\\)-Nets 11.9 | | 0.972 |\\n| \\\\( R \\\\)-PolyNets 11.9 | | 0.977 |\\n| \\\\( D \\\\)-PolyNets 7.2 | | 0.977 |\\n\\nPolyNets. We select the Oxford 102 Flowers dataset [32], which contains 102 flower categories with 10 training images and 10 validation images annotated per class. The accuracy for each model is exhibited in Table 8, where \\\\( R \\\\)-PolyNets performs favorably to the vanilla ResNet18.\\n\\n| Dataset Model | # par (M) | Accuracy |\\n|---------------|-----------|----------|\\n| Oxford Flower | | | |\\n| ResNet18 11.2 | | 0.877 |\\n| Hybrid \\\\( \\\\Phi \\\\)-Nets 6.1 | | 0.889 |\\n| PDC 5.5 | | 0.885 |\\n| \\\\( \\\\Phi \\\\)-Nets 11.9 | | 0.826 |\\n| \\\\( R \\\\)-PolyNets 11.9 | | 0.949 |\\n| \\\\( D \\\\)-PolyNets 7.2 | | 0.941 |\\n\\n6. Conclusion\\nIn this work, we focus on Polynomial Nets (PNs) for image recognition. We propose a new parametrization of PNs that enables them to avoid reported overfitting issues using custom initialization and normalization schemes. We show the case how the proposed model, called \\\\( R \\\\)-PolyNets, extends previously proposed PN models. Our thorough evaluation with six datasets exhibits a significant improvement over previously proposed PN models and establish \\\\( R \\\\)-PolyNets as an alternative to existing DNNs. Furthermore, we introduce \\\\( D \\\\)-PolyNets that leverage dense connections across sequential polynomials to capture higher-order correlations. Experimentally, \\\\( D \\\\)-PolyNets verify their expressivity over alternative PNs. We believe that our work can encourage further research in alternative models for image recognition.\\n\\nLimitations: A deeper theoretical understanding of PNs is needed, particularly regarding the link between the degree, the regularization requirements and the generalization error. Concretely, each block of polynomials we are using is composed of lower-degree polynomial expansions. We hypothesize that the high-degree obtained from the sequential polynomial blocks might be sufficient for image recognition tasks, but might not suffice for harder tasks. In addition, the theoretical study of the initialization or the regularization requirements on PNs remains elusive.\\n\\nAcknowledgements: We are thankful to the reviewers for their feedback and constructive comments. This work was supported by Hasler Foundation Program: Hasler Responsible AI (project number 21043). This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement n\u00b0 725594 - time-data).\"}"}
