{"id": "CVPR-2023-1861", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"SPiN-NeRF: Multiview Segmentation and Perceptual Inpainting with Neural Radiance Fields\\n\\nAshkan Mirzaei\\nTristan Aumentado-Armstrong\\nKonstantinos G. Derpanis\\nJonathan Kelly\\nMarcus A. Brubaker\\nIgor Gilitschenski\\nAlex Levinshtein\\n\\n1 Samsung AI Centre Toronto\\n2 University of Toronto\\n3 York University\\n4 Vector Institute for AI\\n\\n{a.mirzaei, tristan.aumentado-armstrong, j.kelly, igor.gilitschenski}@partner.samsung.com, {kosta, mab}@eecs.yorku.ca, alex.levinshtein@samsung.com\\n\\nFigure 1. An example of the inputs and outputs of our 3D inpainting framework. In addition to the images captured from the scene and their corresponding camera parameters, users are tasked with providing a few points in a single image to indicate which object they wish to remove from the scene (upper-left inset). These sparse annotations are then automatically transferred to all other views, and utilized for multiview mask construction (upper-right inset). The resulting 3D-consistent mask is used in a perceptual optimization problem that results in 3D scene inpainting (lower row), with rendered depth from the optimized NeRF shown for each image as an inset.\\n\\nAbstract\\n\\nNeural Radiance Fields (NeRFs) have emerged as a popular approach for novel view synthesis. While NeRFs are quickly being adapted for a wider set of applications, intuitively editing NeRF scenes is still an open challenge. One important editing task is the removal of unwanted objects from a 3D scene, such that the replaced region is visually plausible and consistent with its context. We refer to this task as 3D inpainting. In 3D, solutions must be both consistent across multiple views and geometrically valid. In this paper, we propose a novel 3D inpainting method that addresses these challenges. Given a small set of posed images and sparse annotations in a single input image, our framework first rapidly obtains a 3D segmentation mask for a target object. Using the mask, a perceptual optimization-based approach is then introduced that leverages learned 2D image inpainters, distilling their information into 3D space, while ensuring view consistency. We also address the lack of a diverse benchmark for evaluating 3D scene inpainting methods by introducing a dataset comprised of challenging real-world scenes. In particular, our dataset contains views of the same scene with and without a target object, enabling more principled benchmarking of the 3D inpainting task. We first demonstrate the superiority of our approach on multiview segmentation, comparing to NeRF-based methods and 2D segmentation approaches. We then evaluate on the task of 3D inpainting, establishing state-of-the-art performance against other NeRF manipulation algorithms, as well as a strong 2D image inpainter baseline.\\n\\n1. Introduction\\n\\nNeural rendering methods, especially Neural Radiance Fields (NeRFs), have recently emerged as a new modality for representing and reconstructing scenes, achieving impressive results for novel view synthesis. Substantial research effort continues to focus on formulating more efficient NeRFs (e.g., [35, 50]), to make NeRFs more accessible in use-cases with more limited computational resources.\"}"}
{"id": "CVPR-2023-1861", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tional resources. As NeRFs become more widely accessible, the need for editing and manipulating the scenes represented by NeRFs will continue to grow. One notable editing application is removing objects and inpainting the 3D scene, analogous to the well-studied 2D image inpainting task [23]. However, several obstacles impede progress on this task, not only for the 3D inpainting process itself, but also in obtaining the input segmentation masks. First, NeRF scenes are implicitly encoded within the neural mapping weights, resulting in an entangled and uninterpretable representation that is non-trivial to manipulate (compared to, say, the explicit discretized form of 2D image arrays or meshes in 3D). Moreover, any attempt to inpaint a 3D scene must not only generate a perceptually realistic appearance in a single given view, but also preserve fundamental 3D properties, such as appearance consistency across views and geometric plausibility. Finally, to obtain masks for the target object, it is more intuitive for most end users to interact with 2D images, rather than 3D interfaces; however, requiring annotations of multiple images (and maintaining view-consistent segments) is burdensome to users. An appealing alternative is to expect only a minimal set of annotations for a single view. This motivates a method capable of obtaining a view-consistent 3D segmentation mask of the object (for use in inpainting) from single-view sparse annotations.\\n\\nIn this paper, we address these challenges with an integrated method that takes in multiview images of a scene, extracts a 3D mask with minimal user input, and fits a NeRF to the masked images, such that the target object is replaced with plausible 3D appearance and geometry. Existing interactive 2D segmentation methods do not consider the 3D aspects of the problem (e.g., [42]), while current NeRF-based approaches are unable to use sparse annotations [76] to perform well, or do not attain sufficient accuracy [44]. Similarly, while some current NeRF manipulation algorithms allow object removal, they do not attempt to provide perceptually realistic inpaintings of newly unveiled parts of space (e.g., [64]). To our knowledge, this is the first approach that handles both interactive multiview segmentation and full 3D inpainting in a single framework.\\n\\nOur technique leverages off-the-shelf, 3D-unaware models for segmentation and inpainting, and transfers their outputs to 3D space in a view-consistent manner. Building on the (2D) interactive segmentation [8, 15, 33] literature, our framework starts from a small number of user-defined image points on a target object (and a few negative samples outside it). From these, our algorithm initializes masks with a video-based model [4], and lifts them into a coherent 3D segmentation via fitting a semantic NeRF [36, 76, 77]. Then, after applying a pretrained 2D inpainter [48] to the multiview image set, a customized NeRF fitting process is used to reconstruct the 3D inpainted scene, utilizing perceptual losses [72] to account for inconsistencies in the 2D inpainted images, as well as inpainted depth images to regularize the geometry of the masked region. Overall, we provide a complete method, from object selection to novel view synthesis of the inpainted scenes, in a unified framework with minimal burden on the user, illustrated in Figure 1.\\n\\nWe demonstrate the effectiveness of our approach through extensive qualitative and quantitative evaluations. In addition, we address the lack of a benchmark for comparing scene inpainting methods, and introduce a new dataset where the \u201cground-truth inpaintings\u201d (i.e., real images of the scene without the object) are available as well.\\n\\nIn summary, our contributions are as follows: (i) a complete process for 3D scene manipulation, starting from object selection with minimal user interaction and ending with a 3D inpainted NeRF scene; (ii) to perform such selection, an extension of 2D segmentation models to the multiview case, capable of recovering 3D-consistent masks from sparse annotations; (iii) to ensure view-consistency and perceptual plausibility, a novel optimization-based formulation of 3D inpainting in NeRFs, which leverages 2D inpainters; and (iv) a new dataset for 3D object removal evaluation that includes corresponding object-free ground-truth.\\n\\n2. Related Work\\n\\nImage Inpainting. Inpainting in 2D has received significant research attention [23]. While early techniques relied on patches [9, 16], recent neural methods optimize both perceptual realism and reconstruction (e.g., [22, 27, 48]). Various lines of research continue to be explored for improving visual fidelity, including adversarial training (e.g., [40, 73]), architectural advances (e.g., [27, 30, 62, 63]), pluralistic outputs (e.g., [73, 75]), multiscale processing (e.g., [21, 58, 69]), and perceptual metrics (e.g., [22, 48, 72]). Our work leverages LaMa [48], which applies frequency-domain transforms [7] inspired by transformers [11, 12]. Yet, none of these lift the problem into 3D; thus, inpainting multiple captures of a scene in a consistent manner remains an underexplored task. While there are some existing 3D-aware image inpainting algorithms, they either only partially operate in 3D [65], rely on reference images [74], or consider more limited scenarios [24]. In contrast, our method operates directly in 3D, via the multiview-based NeRF model.\\n\\nNeRF Manipulation. Representing scenes via volumetric rendering has recently become an important research direction [50]. Based on differentiable volume rendering [18, 52] and positional encoding [13, 49, 53], NeRFs [35] have demonstrated remarkable performance in novel-view synthesis. Recent works have studied potential improvements in NeRF's training or rendering speed [5, 6, 17, 38, 47, 67], reconstruction quality [1, 2, 10, 29], and data requirements [28, 34, 56, 59, 68]. However, manipulating NeRF scenes remains a challenge. Clip-NeRF [55], Object-NeRF [64], LaTeRF [36], and others [25, 32, 70] introduce approaches...\"}"}
{"id": "CVPR-2023-1861", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to alter and complete objects represented by NeRFs; however, their performance is limited to simple objects, rather than scenes with significant clutter and texture, or they focus on tasks other than general inpainting (e.g., recoloring or deforming). Most closely related to our method is NeRF-In \\\\cite{31}, a concurrent unpublished work, which inpaints NeRF scenes with geometry and radiance priors from 2D image inpainters, but does not address inconsistency; instead, its use of a simple pixelwise loss relegates it to simply reducing the number of views used for fitting, which reduces final view synthesis quality. Similarly using a pixelwise loss, the concurrent Remove-NeRF model \\\\cite{60} reduces inconsistencies by excluding views based on an uncertainty mechanism. In contrast, our approach is able to inpaint NeRF representations of challenging real-world scenes by incorporating 2D information in a view-consistent manner, via a perceptual training regime. This avoids over-constraints on the inpainting, which would normally lead to blurriness.\\n\\n3. Background: Neural Radiance Fields\\n\\nNeRFs \\\\cite{35} encode a 3D scene as a function, $f(x, d) \\\\rightarrow (c, \\\\rho)$, that maps a 3D coordinate, $x$, and a view direction, $d$, to a color, $c$, and density, $\\\\rho$. The function $f$ can be modelled in various ways (e.g., \\\\cite{35, 47}). For a ray, $r$, the expected color is estimated by volumetric rendering via quadrature; the ray is divided into $N$ sections between $t_n$ and $t_f$ (the near and far bounds), with $t_i$ sampled from the $i$-th section, to render the estimated color, $b_C(r)$:\\n\\n$$b_C(r) = \\\\sum_{i=1}^{N} T_i(1 - exp(-\\\\rho_i))c_i,$$\\n\\nwhere $T_i = exp(-\\\\rho_1 - \\\\rho_2 - \\\\cdots - \\\\rho_i)$ is the transmittance, $t_i + 1$ is the distance between two adjacent points, and $c_i$ and $\\\\rho_i$ are the color and density at $t_i$, respectively. For the rays passing through pixels of the training views, the ground-truth color $C_{GT}(r)$ is available, and the model is optimized using the reconstruction loss:\\n\\n$$L_{rec} = \\\\sum_{r \\\\in R} ||b_C(r) - C_{GT}(r)||^2,$$\\n\\nwhere $R$ is a ray batch sampled from the training views.\\n\\n4. Method\\n\\nGiven a set of RGB images, $I = \\\\{I_i\\\\}_{n=1}^{N}$, with corresponding 3D poses, $G = \\\\{G_i\\\\}_{n=1}^{N}$, and camera intrinsic matrix, $K$, our model expects one additional \u201csource\u201d view with sparse user annotations (i.e., a few points identifying the unwanted object). From these inputs, we produce a NeRF model of the scene, capable of synthesizing an inpainted image from any novel view. We begin by obtaining an initial 3D mask from the single-view annotated source ($\\\\S$ 4.1.1), followed by fitting a semantic NeRF, to improve the consistency and quality of the mask ($\\\\S$ 4.1.2). Finally, in $\\\\S$ 4.2, we describe our view-consistent inpainting method, which takes the views and recovered masks as inputs. Our approach leverages the outputs of 2D inpainters \\\\cite{48} as appearance and geometry priors to supervise the fitting of a new NeRF. Figure 1 illustrates our entire approach, including the inputs and outputs. Additional details are in our supplementary material.\\n\\n4.1. Multiview Segmentation\\n\\n4.1.1 Mask Initialization\\n\\nWe first describe how we initialize a rough 3D mask from single-view annotations. Denote the annotated source view as $I_1$. The sparse information about the object and the source view are given to an interactive segmentation model \\\\cite{15} to estimate the initial source object mask, $c_M_1$. The training views are then treated as a video sequence and, along with $c_M_1$, given to a video instance segmentation model \\\\cite{4, 57}, to compute $V(\\\\{I_i\\\\}_{n=1}^{N}, c_M_1) = \\\\{c_M_i\\\\}_{n=1}^{N}$, where $c_M_i$ is the initial guess for the object mask for $I_i$. The initial masks, $\\\\{c_M_i\\\\}_{n=1}^{N}$, are typically inaccurate around the boundaries, since the training views are not actually adjacent video frames, and video segmentation models are usually 3D-unaware. Hence, we use a semantic NeRF model \\\\cite{36, 76, 77} to resolve the inconsistencies and improve the masks ($\\\\S$ 4.1.2), thus obtaining the masks for each input view, $\\\\{M_i\\\\}_{n=1}^{N}$, to use for inpainting ($\\\\S$ 4.2).\\n\\n4.1.2 NeRF-based Segmentation\\n\\nOur multiview segmentation module takes the input RGB images, $\\\\{I_i\\\\}_{n=1}^{N}$, the corresponding camera intrinsic and extrinsic parameters, and the initial masks, $\\\\{c_M_i\\\\}_{n=1}^{N}$, and trains a semantic NeRF \\\\cite{76}. Figure 2 depicts the network used in the semantic NeRF; for a point, $x$, and a view direction, $d$, in addition to a density, $(x)$, and color, $c(x, d)$, it returns a pre-sigmoid \u201cobjectness\u201d logit, $s(x)$. The objectness probability is then acquired as $p(x) = Sigmoid(s(x))$. \\n\\n20671 20671\"}"}
{"id": "CVPR-2023-1861", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"We use Instant-NGP as our NeRF architecture due to its fast convergence. The expected objectness logit, $b_S(r)$, associated with a ray, $r$, is obtained by rendering the logits of the points on $r$ instead of their colors, with respect to the densities, in Eq.\\n\\n$$b_S(r) = \\\\sum_{i=1}^{N} T_i (1 - \\\\exp(-i s_i)), \\\\quad (3)$$\\n\\nwhere for simplicity, $s_i(r(t_i))$ is denoted by $s_i$. The objectness probability of a ray, $b_P(r) = \\\\text{Sigmoid}(b_S(r))$, is then supervised using the classification loss:\\n\\n$$L_{clf} = \\\\frac{1}{|R|} \\\\sum_{r \\\\in R} \\\\text{BCE}(b_P(r), \\\\text{masked}), \\\\quad (4)$$\\n\\nwhere $1$ is the indicator function, BCE stands for the binary cross entropy loss, and $R_{masked}$ is the set of rays passing through pixels that are masked in $\\\\{c_Mi\\\\}_{i=1}^{n_i}$. During the calculation of the classification loss, $L_{clf}$, the weights of the colors in the rendering equation (Eq.\\n\\n$$L_{mv} = L_{rec} + \\\\lambda_{clf} L_{clf}, \\\\quad (5)$$\\n\\nwhere the classification weight, $\\\\lambda_{clf}$, is a hyper-parameter.\\n\\nAfter optimization, 3D-consistent masks, $\\\\{M_i\\\\}_{i=1}^{n_i}$, are obtained by thresholding the objectness probabilities and masking the pixels with probabilities greater than 0.5. Finally, we use two stages for optimization to further improve the masks; after obtaining the initial 3D mask, the masks are rendered from the training views, and are used to supervise a secondary multiview segmentation model as initial guesses (instead of the video segmentation outputs).\\n\\n4.2. Multiview Inpainting\\n\\nFigure 3 shows an overview of our view-consistent inpainting method. As the paucity of data precludes directly training a 3D inpainter, our method leverages existing 2D inpainters to obtain depth and appearance priors, which then supervise the fitting of a NeRF to the completed scene. This inpainted NeRF is trained using the following loss:\\n\\n$$L_{inp} = L_{0}^{rec} + \\\\lambda_{LPIPS} L_{LPIPS} + \\\\lambda_{depth} L_{depth}, \\\\quad (6)$$\\n\\nwhere $L_{0}^{rec}$ is the reconstruction loss for the unmasked pixels, and $L_{LPIPS}$ and $L_{depth}$ define the perceptual and depth losses (see below), with weights $\\\\lambda_{LPIPS}$ and $\\\\lambda_{depth}$.\\n\\nIBRNet images in Fig. 3,5,6,7 by Wang et al. available in IBRNet under an Apache License 2.0.\"}"}
{"id": "CVPR-2023-1861", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"perceptual loss to only fit the colors of the scene. For this purpose, we use a NeRF optimized on images that include the unwanted object, and render the depth maps, \\\\( D_i \\\\) for \\\\( i = 1 \\\\), corresponding to the training views. Depth maps are calculated by substituting the distance to the camera instead of the color of points into Eq. 1:\\n\\n\\\\[\\nD(r) = \\\\prod_{i=1}^{N} T_i (1 - \\\\exp(-r_i))\\n\\\\]\\n\\nThe rendered depths are then given to an inpainter to obtain inpainted depth maps, \\\\( eD_i \\\\) for \\\\( i = 1 \\\\), where \\\\( eD_i \\\\) is obtained as:\\n\\n\\\\[\\neD_i = \\\\text{INP}(D_i, M_i)\\n\\\\]\\n\\nWe found that using LaMa for depth inpainting, as in the RGB case, gave sufficiently high-quality results. Note that this is all calculated as a preprocessing step, and with a NeRF optimized on the original scene. This NeRF can be the same model used for multi-view segmentation. If using another source for obtaining masks, such as human annotated masks, a new NeRF is fitted to the scene. These depth maps are then used to supervise the inpainted NeRF's geometry, via the \\\\( L^2 \\\\) distance of its rendered depths, \\\\( bD_i(r) \\\\), to the inpainted depths, \\\\( eD_i(r) \\\\):\\n\\n\\\\[\\nL_{depth} = \\\\frac{1}{|R|} \\\\sum_{r \\\\in R} (bD_i(r) - eD_i(r))^2\\n\\\\]\\n\\n4.2.3 Patch-based Optimization\\n\\nCalculating the perceptual loss, Eq. 7, requires full input views to be rendered during the optimization. Since rendering each pixel necessitates multiple forward passes through the MLP, for high-resolution images, this is an expensive process, resulting in issues such as (i) the batch size, \\\\( |B| \\\\), has to be small to fit the rendered images and their corresponding computation graphs in memory, and (ii) slow optimization, even with batch sizes as small as \\\\( |B| = 1 \\\\).\\n\\nA straightforward solution is to render a downsized image and compare it to the downsized version of the inpainted images; however, this leads to a loss of information if the downsizing factor is large. Following image-based works (e.g., SinGAN and DPNN), and 3D works (e.g., ARF), we perform the computations on a patch-basis; instead of rendering complete views, we render batches of smaller patches, and compare them with their counterparts in the inpainted images based on the perceptual loss. Only patches inside the bounding box of the object mask are used. For fitting the unmasked areas, recall that \\\\( L_0 \\\\) (Eq. 6) simply alters \\\\( L_{rec} \\\\) (Eq. 2) to sample rays only from unmasked pixels. By separating the perceptual and reconstruction losses, we prevent inconsistency within the mask, while avoiding unnecessary changes to the rest of the scene.\\n\\n4.2.4 Mask Refinement\\n\\nHere, we consider further leveraging the multiview data to guide the image inpainter. In particular, parts of the training images that are currently being generated by the 2D image inpainter might be visible in other views; in such cases, there is no need to hallucinate those details, since they can be retrieved from the other views. To prevent such unnecessary inpaintings, we propose a mask refinement approach: for each source image, depth, and mask tuple, \\\\( (I_s, D_s, M_s) \\\\), we substitute pixels in \\\\( I_s \\\\) and \\\\( D_s \\\\) that are visible from at least one other view, to shrink the source mask, \\\\( M_s \\\\). After this refinement step, only parts of \\\\( I_s \\\\) and \\\\( D_s \\\\) that are occluded by the undesired object in all of the training views will remain masked. As a result, the image inpainter has to fill in a smaller area, resulting in improved inpaintings. Please see our supplementary material for details.\\n\\n5. Experiments\\n\\nDataset. To evaluate multiview segmentation (MVSeg), we adopt real-world scenes from LLFF, NeRF-360, NeRF-Supervision, and Shiny. For multiview (MV) inpainting, in addition to providing qualitative results on scenes from IBRNet, we address the need for a standard benchmark including ground-truth captures of scenes without the unwanted object as test views, and introduce a dataset containing 10 real-world forward-facing scenes with human annotated object masks. For each scene, we provide 60 training images with the object and 40 test images without the object. This dataset is further suitable for evaluating tasks such as real-time 3D inpainting, unsupervised 3D segmentation, and video inpainting. Figure 4 shows sample views from two scenes of our dataset.\\n\\nMetrics. To evaluate our segmentation model, we use the accuracy of the predictions (pixel-wise) and the intersection over union (IoU) metric. For MV inpainting, we follow the image-to-image literature and report the average learned perceptual image patch similarity (LPIPS), and the average Fr\u00e9chet inception distance (FID) between the generated and real images.\"}"}
{"id": "CVPR-2023-1861", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Quantitative evaluation of multiview segmentation models, for the task of transferring the source mask to other views.\\n\\n| Method | Acc. | IoU |\\n|--------|------|-----|\\n| Proj. + Grab Cut | 91.08 | 46.61 |\\n| Proj. + EdgeFlow | 96.84 | 81.63 |\\n| Semantic NeRF | 94.63 | 75.13 |\\n| Proj. + EdgeFlow + Semantic NeRF | 97.26 | 83.95 |\\n| Feature Field Distillation | 97.37 | 83.07 |\\n| Video Segmentation | 98.43 | 88.34 |\\n| Ours | 98.85 | 90.96 |\\n| Ours (two-stage) | 98.91 | 91.66 |\\n\\nMultiview Segmentation Baselines. For MVSeg, one category of baselines is projection-based approaches: the source mask is projected into the other views using the scene geometry from a NeRF. This gives us an incomplete mask in the other views. Then, a variety of interactive segmentation approaches are applied to the incomplete masks, propagating them to obtain complete object masks: Proj. + Grab Cut and Proj. + EdgeFlow. In addition, we consider Proj. + EdgeFlow + Semantic NeRF, where an additional Semantic NeRF is fitted to make the outputs 3D consistent. Another baseline is a representative of the concurrent works on distilling 2D pixel-level features to 3D scenes and post-processing them for obtaining segmentation masks. As a baseline for video-segmentation, we compare to Dino since it does not rely on temporally close neighboring frames.\\n\\nMultiview Inpainting Baselines. Masked NeRF only uses the unmasked pixels to optimize a NeRF. Object NeRF filters the unwanted points in 3D without explicitly inpainting the missing regions. No code is available for NeRF-In, so we use our own implementation of their model, with slight modifications (we use LaMa as the inpainter). In addition, we compare our results to LaMa as a representative of state-of-the-art 2D inpainters. To enable fair comparison between the NeRF-based 3D models (which, in addition to inpainting, have to synthesize novel views), we compare to LaMa by (i) fitting a NeRF on the views with the object, (ii) rendering the test views from the fitted NeRF, and (iii) passing these rendered images to LaMa. Finally, for reference and as an ideal \u201cgold standard\u201d 3D inpainting baseline, we fit a NeRF on the ground-truth test images, use the optimized NeRF to render the test-views, and then compare the rendered results to the ground-truth. We provide these results for completeness, as since the test views that do not contain the object should not be available to the model during the inference.\\n\\nTable 2. Quantitative evaluation of our inpainting method, using human-annotated object masks.\\n\\n| Method | LPIPS | FID |\\n|--------|-------|-----|\\n| Ideal | 0.4079 | 100.25 |\\n| LaMa (2D) | 0.5369 | 174.61 |\\n| Object NeRF | 0.6829 | 271.80 |\\n| Masked NeRF | 0.6030 | 294.69 |\\n| NeRF-In | 0.5699 | 238.33 |\\n| NeRF-In (Single) | 0.4884 | 183.23 |\\n| Ours (no geo. inpainting) | 0.4952 | 200.34 |\\n| Ours | 0.4654 | 156.64 |\\n| Ours (Refined RGB/Depth) | 0.4664 | 163.79 |\\n| Ours (Refined RGB) | 0.4529 | 147.31 |\\n\\nAn upper-bound on the best possible results one can expect to obtain when using the same NeRF architecture.\"}"}
{"id": "CVPR-2023-1861", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. A qualitative comparison of our multiview segmentation model against Neural Volumetric Object Selection (NVOS) [44] (reproduced from original paper), video segmentation [4], and the human-annotated masks (GT). Our two-stage approach refers to running our multiview segmentation model twice, with the output of the first run as the initialization of the second (see \u00a7 4.1.2). Our method is less noisy than NVOS, which also misses some pieces of the target (e.g., the lowest flower in the bottom row), but captures more details than the video segmentation alone (e.g., the blurred edges of the flowers). Our two-stage approach helps fill in some missing aspects of the one-stage output. Please see our supplement for additional qualitative examples.\\n\\nTable 3. Quantitative evaluation of our inpainting method, using the outputs of our multiview segmentation model.\\n\\n| Method               | LPIPS  | FID    |\\n|----------------------|--------|--------|\\n| LaMa (2D)            | 0.5439 | 169.92 |\\n| Object NeRF          | 0.6679 | 286.55 |\\n| Masked NeRF          | 0.6340 | 332.70 |\\n| NeRF-In              | 0.5858 | 240.27 |\\n| NeRF-In (Single)     | 0.5054 | 176.27 |\\n| Ours                 | 0.4662 | 140.56 |\\n\\nThe perceptual loss function, to outperform the 2D inpainter. Table 2 further shows that removing geometry guidance impairs the inpainted scene quality. We display qualitative results of our MV inpainting method in Figure 6, showing that it can reconstruct a view-consistent scene with detailed textures, including coherent view-dependent radiance for both shiny and non-shiny surfaces. In addition, in Figure 7, we provide a visual comparison to the unpublished concurrent work NeRF-In [31], which has the second-lowest error. We observe that a NeRF-In model fitted to all of the inpainted views results in blurry outputs. Alternatively, using a single inpainted view for supervising the masked region leads to artifacts in further views, due to the lack of supervision for the view-dependent radiance, as well as the poor extrapolation capabilities of the network. In contrast, our perceptual approach relaxes the exact reconstruction constraints in the masked region, thus preventing blurriness despite using all of the images, while avoiding artifacts caused by single-view supervision. Table 2 shows that refinement provides a small but significant boost in inpainting quality, due to smaller masks requiring less hallucination from the inpainter. Yet, empirically, refinement only subtly trims the masks (reducing Table 4. Evaluation of our 3D inpainting method across input view number (upper half) and mask dilation level (lower half).\\n\\n| Input Views | 10  | 20  | 40  | 60  |\\n|-------------|-----|-----|-----|-----|\\n| LPIPS       | 0.4726 | 0.4713 | 0.4667 | 0.4654 |\\n| FID         | 171.19 | 172.63 | 158.02 | 156.64 |\\n\\n| Dilation    | 45  | 25  | 5   | 0   |\\n|-------------|-----|-----|-----|-----|\\n| LPIPS       | 0.6102 | 0.5369 | 0.4654 | 0.4904 |\\n| FID         | 283.01 | 230.03 | 156.64 | 164.66 |\\n\\nmean masked area by 4.74% on our dataset), as the camera has limited movement during data collection, to ensure similarity between the training and testing views. Further, due to noisy NeRF geometries projecting incorrect values, refining depths lowers performance, whereas refining colours alone achieves our best results. So far, our experiments examine the performance of our MVSeg and MV inpainting independently; however, one can combine them to remove objects from NeRF scenes with minimal user interaction. Table 3 shows that using the output masks of our MVSeg model, instead of using the human-annotated object masks, results in a subtle decrease in the inpainting quality. However, our model with MVSegs still outperforms other methods, even when they are fitted on human-annotated segmentations.\\n\\n5.2. Variations and Ablation Studies\\n\\nNumber of Input Views. Limiting the number of input views is a standard approach employed in the literature to modulate the reconstruction quality of NeRFs [54]. Table 4 shows that the performance of our inpainter degrades with fewer inputs. Thus, we argue that as better-quality NeRFs are introduced, our approach, which is agnostic to the underlying NeRF model, can readily benefit.\"}"}
{"id": "CVPR-2023-1861", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Visualizations of our view-consistent inpainting results. Upper rows per inset show NeRF renderings of the original scene from novel views, with the first image also displaying the associated mask. Lower rows show the corresponding inpainted view. Notice that the synthesized views maintain consistency with each other; however, view-dependent effects still remain (e.g., the lighting on the uncovered part of the piano). Please see our supplement for additional scenes, and our project website with videos for better visualization.\\n\\nFigure 7. Qualitative comparisons with other baselines. Columns: novel views of the scene, synthesized by a NeRF (on the unmasked images), NeRF-In, NeRF-In (with a single masked training image), and our approach. NeRF-In is significantly more blurry, while NeRF-In (single) tends to have difficulty with details closer to the edge of the mask boundary (zoom into boxes for examples).\\n\\nImportance of Accurate Masks. Here, we examine the impact of accurate masks on inpainting, via variable dilations of the object masks with a $5 \\\\times 5$ kernel. Larger masks lead to relying more on the view-inconsistent outputs and hallucinations of the 2D inpainters, while smaller masks may permit parts of the unwanted object's edges to remain and confuse the 2D inpainter. A subtle dilation is also useful for reducing the effect of shadows. This balance between over-masking and under-masking is demonstrated in Table 4, with five dilation iterations found to be optimal and therefore used for all other experiments.\\n\\n6. Conclusion\\n\\nIn this paper, we presented a novel approach to inpaint NeRF scenes, which enforces viewpoint consistency based on image and geometric priors, given a single-view object mask. In addition, we provided a multiview segmentation method that simplifies the annotation process by using a set of sparse pixel-level clicks on (and around) the undesired object and translating them into a 3D mask that can be rendered from novel views. We provided experiments to show the effectiveness of our segmentation and inpainting methods. The main limitation of our work is the assumption of semantically consistent image priors, potentially only differing in terms of textures. Finally, we introduce a dataset that not only addresses the lack of challenging benchmarks for multiview inpainting, but which we believe can assist future advances in this new line of research.\\n\\nAcknowledgments. This work was conducted at Samsung AI Centre Toronto and it was funded by Mitacs and Samsung Research, Samsung Electronics Co., Ltd.\"}"}
{"id": "CVPR-2023-1861", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Jonathan T. Barron, Ben Mildenhall, Matthew Tancik, Peter Hedman, Ricardo Martin-Brualla, and Pratul P. Srinivasan. Mip-NeRF: A multiscale representation for anti-aliasing neural radiance fields. *ICCV*, 2021.\\n\\n[2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded anti-aliased neural radiance fields. *CVPR*, 2022.\\n\\n[3] Yash Bhalgat. Hashnerf-pytorch. https://github.com/yashbhalgat/HashNeRF-pytorch/, 2022.\\n\\n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In *ICCV*, 2021.\\n\\n[5] Anpei Chen, Zexiang Xu, Andreas Geiger, Jingyi Yu, and Hao Su. Tensorf: Tensorial radiance fields. In *ECCV*, 2022.\\n\\n[6] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. MobileNeRF: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. In *arXiv*, 2022.\\n\\n[7] Lu Chi, Borui Jiang, and Yadong Mu. Fast Fourier convolution. In *NeurIPS*, 2020.\\n\\n[8] PaddlePaddle Contributors. PaddleSeg, end-to-end image segmentation kit based on PaddlePaddle. https://github.com/PaddlePaddle/PaddleSeg, 2019.\\n\\n[9] Antonio Criminisi, Patrick P\u00e9rez, and Kentaro Toyama. Object removal by exemplar-based inpainting. In *CVPR*, 2003.\\n\\n[10] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised NeRF: Fewer views and faster training for free. In *CVPR*, June 2022.\\n\\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In *ICLR*, 2021.\\n\\n[12] Patrick Esser, Robin Rombach, and Bj\u00f6rn Ommer. Taming transformers for high-resolution image synthesis. In *CVPR*, 2021.\\n\\n[13] Jonas Gehring, Michael Auli, David Grangier, Denis Yarats, and Yann N. Dauphin. Convolutional sequence to sequence learning. In *ICML*, 2017.\\n\\n[14] Niv Granot, Ben Feinstein, Assaf Shocher, Shai Bagon, and Michal Irani. Drop the GAN: In defense of patches nearest neighbors as single image generative models. In *CVPR*, 2022.\\n\\n[15] Yuying Hao, Yi Liu, Zewu Wu, Lin Han, Yizhou Chen, Guowei Chen, Lutao Chu, Shiyu Tang, Zhiliang Yu, Zeyu Chen, and Baohua Lai. Edgeflow: Achieving practical interactive segmentation with edge-guided flow. In *ICCV Workshops*, 2021.\\n\\n[16] James Hays and Alexei A Efros. Scene completion using millions of photographs. In *SIGGRAPH*, 2007.\\n\\n[17] Peter Hedman, Pratul P. Srinivasan, Ben Mildenhall, Jonathan T. Barron, and Paul Debevec. Baking neural radiance fields for real-time view synthesis. In *ICCV*, 2021.\\n\\n[18] Philipp Henzler, Niloy J. Mitra, and Tobias Ritschel. Escaping Plato\u2019s cave: 3D shape from adversarial rendering. In *ICCV*, 2019.\\n\\n[19] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. GANs trained by a two time-scale update rule converge to a local Nash equilibrium. *NeurIPS*, 2017.\\n\\n[20] Tao Hu, Shu Liu, Yilun Chen, Tiancheng Shen, and Jiaya Jia. EfficientNeRF: Efficient neural radiance fields. In *CVPR*, 2022.\\n\\n[21] Satoshi Iizuka, Edgar Simo-Serra, and Hiroshi Ishikawa. Globally and locally consistent image completion. *ToG*, 2017.\\n\\n[22] Jitesh Jain, Yuqian Zhou, Ning Yu, and Humphrey Shi. Keys to better image inpainting: Structure and texture go hand in hand. In *WACV*, 2023.\\n\\n[23] Jireh Jam, Connah Kendrick, Kevin Walker, Vincent Drouard, Jison Gee-Sern Hsu, and Moi Hoon Yap. A comprehensive review of past and present image inpainting methods. *CVIU*, 2021:103147, 2021.\\n\\n[24] Varun Jampani, Huiwen Chang, Kyle Sargent, Abhishek Kar, Richard Tucker, Michael Krainin, Dominik Kaeser, William T Freeman, David Salesin, Brian Curless, and Ce Liu. Slide: Single image 3D photography with soft layering and depth-aware inpainting. In *ICCV*, 2021.\\n\\n[25] Kacper Kania, Kwang Moo Yi, Marek Kowalski, Tomasz Trzci\u0144ski, and Andrea Tagliasacchi. CoNeRF: Controllable neural radiance fields. In *CVPR*, 2022.\\n\\n[26] Sosuke Kobayashi, Eiichi Matsumoto, and Vincent Sitzmann. Decomposing NeRF for editing via feature field distillation. In *NeurIPS*, 2022.\\n\\n[27] Wenbo Li, Zhe Lin, Kun Zhou, Lu Qi, Yi Wang, and Jiaya Jia. MAT: Mask-aware transformer for large hole image inpainting. In *CVPR*, 2022.\\n\\n[28] Chen-Hsuan Lin, Wei-Chiu Ma, Antonio Torralba, and Simon Lucey. BARF: Bundle-adjusting neural radiance fields. In *ICCV*, 2021.\\n\\n[29] David B Lindell, Dave Van Veen, Jeong Joon Park, and Gordon Wetzstein. Bacon: Band-limited coordinate networks for multiscale scene representation. In *CVPR*, 2022.\\n\\n[30] Hongyu Liu, Bin Jiang, Yi Xiao, and Chao Yang. Coherent semantic attention for image inpainting. In *ICCV*, 2019.\\n\\n[31] Hao-Kang Liu, I-Chao Shen, and Bing-Yu Chen. NeRF-In: Free-form NeRF inpainting with RGB-D priors. In *arXiv*, 2022.\\n\\n[32] Steven Liu, Xiuming Zhang, Zhoutong Zhang, Richard Zhang, Jun-Yan Zhu, and Bryan Russell. Editing conditional radiance fields. In *ICCV*, 2021.\\n\\n[33] Yi Liu, Lutao Chu, Guowei Chen, Zewu Wu, Zeyu Chen, Baohua Lai, and Yuying Hao. PaddleSeg: A high-efficient development toolkit for image segmentation. In *arXiv*, 2021.\"}"}
{"id": "CVPR-2023-1861", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ben Mildenhall, Pratul P. Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. *ToG*, 2019.\\n\\nBen Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In *ECCV*, 2020.\\n\\nAshkan Mirzaei, Yash Kant, Jonathan Kelly, and Igor Gilitschenski. LaTeRF: Label and text driven object radiance fields. In *ECCV*, 2022.\\n\\nThomas M\u00fcller. Tiny CUDA neural network framework, 2021. https://github.com/nvlabs/tiny-cuda-nn.\\n\\nThomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. *TOG*, 2022.\\n\\nAdam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zachary DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: An imperative style, high-performance deep learning library. In *NeurIPS*, 2019.\\n\\nDeepak Pathak, Philipp Kr\u00e4henb\u00fchl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context encoders: Feature learning by inpainting. In *CVPR*, 2016.\\n\\nBen Poole, Ajay Jain, Jonathan T. Barron, and Ben Mildenhall. Dreamfusion: Text-to-3D using 2D diffusion. In *arXiv*, 2022.\\n\\nHiba Ramadan, Chaymae Lachqar, and Hamid Tairi. A survey of recent interactive image segmentation methods. *CVM*, 2020.\\n\\nChaolin Rao, Huangjie Yu, Haochuan Wan, Jindong Zhou, Yueyang Zheng, Yu Ma, Anpei Chen, Minye Wu, Binzhe Yuan, Pingqiang Zhou, Xin Lou, and Jingyi Yu. Icarus: A specialized architecture for neural radiance fields rendering. In *arXiv*, 2022.\\n\\nZhongzheng Ren, Aseem Agarwala, Bryan Russell, Alexander G. Schwing, and Oliver Wang. Neural volumetric object selection. In *CVPR*, 2022.\\n\\nCarsten Rother, Vladimir Kolmogorov, and Andrew Blake. Interactive foreground extraction using iterated graph cuts. *ACM Transactions on Graphics*, 2012.\\n\\nTamar Rott Shaham, Tali Dekel, and Tomer Michaeli. SInGAN: Learning a generative model from a single natural image. In *ICCV*, 2019.\\n\\nSara Fridovich-Keil and Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In *CVPR*, 2022.\\n\\nRoman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with Fourier convolutions. In *WACV*, 2022.\"}"}
{"id": "CVPR-2023-1861", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zhaoyi Yan, Xiaoming Li, Mu Li, Wangmeng Zuo, and Shiguang Shan. Shift-net: Image inpainting via deep feature rearrangement. In ECCV, 2018.\\n\\nBangbang Yang, Yinda Zhang, Yinghao Xu, Yijin Li, Han Zhou, Hujun Bao, Guofeng Zhang, and Zhaopeng Cui. Learning object-compositional neural radiance field for editable scene rendering. In ICCV, 2021.\\n\\nShunyu Yao, Tzu Ming Hsu, Jun-Yan Zhu, Jiajun Wu, Antonio Torralba, Bill Freeman, and Josh Tenenbaum. 3D-aware scene manipulation via inverse graphics. NeurIPS, 2018.\\n\\nLin Yen-Chen, Pete Florence, Jonathan T. Barron, Tsung-Yi Lin, Alberto Rodriguez, and Phillip Isola. NeRF-Supervision: Learning dense object descriptors from neural radiance fields. In ICRA, 2022.\\n\\nAlex Yu, Ruilong Li, Matthew Tancik, Hao Li, Ren Ng, and Angjoo Kanazawa. PlenOctrees for real-time rendering of neural radiance fields. In ICCV, 2021.\\n\\nAlex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields from one or few images. In CVPR, 2021.\\n\\nJiahui Yu, Zhe Lin, Jimei Yang, Xiaohui Shen, Xin Lu, and Thomas S Huang. Generative image inpainting with contextual attention. CVPR, 2018.\\n\\nYu-Jie Yuan, Yang-Tian Sun, Yu-Kun Lai, Yuewen Ma, Rongfei Jia, and Lin Gao. NeRF-editing: geometry editing of neural radiance fields. In CVPR, 2022.\\n\\nKai Zhang, Nick Kolkin, Sai Bi, Fujun Luan, Zexiang Xu, Eli Shechtman, and Noah Snavely. ARF: Artistic radiance fields. In ECCV, 2022.\\n\\nRichard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In CVPR, 2018.\\n\\nShengyu Zhao, Jonathan Cui, Yilun Sheng, Yue Dong, Xiao Liang, Eric I Chang, and Yan Xu. Large scale image completion via co-modulated generative adversarial networks. In ICLR, 2021.\\n\\nYunhan Zhao, Connelly Barnes, Yuqian Zhou, Eli Shechtman, Sohrab Amirghodsi, and Charless Fowlkes. Geofill: Reference-based image inpainting of scenes with complex geometry. In arXiv, 2022.\\n\\nChuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. Pluralistic image completion. In CVPR, 2019.\\n\\nShuaifeng Zhi, Tristan Laidlow, Stefan Leutenegger, and Andrew Davison. In-place scene labelling and understanding with implicit scene representation. In ICCV, 2021.\\n\\nShuaifeng Zhi, Edgar Sucar, Andre Mouton, Iain Haughton, Tristan Laidlow, and Andrew J. Davison. iLabel: Interactive neural scene labelling. In arXiv, 2021.\"}"}
