{"id": "CVPR-2024-2411", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nWhile Transformers have rapidly gained popularity in various computer vision applications, post-hoc explanations of their internal mechanisms remain largely unexplored. Vision Transformers extract visual information by representing image regions as transformed tokens and integrating them via attention weights. However, existing post-hoc explanation methods merely consider these attention weights, neglecting crucial information from the transformed tokens, which fails to accurately illustrate the rationales behind the models' predictions. To incorporate the influence of token transformation into interpretation, we propose TokenTM, a novel post-hoc explanation method that utilizes our introduced measurement of token transformation effects. Specifically, we quantify token transformation effects by measuring changes in token lengths and correlations in their directions pre- and post-transformation. Moreover, we develop initialization and aggregation rules to integrate both attention weights and token transformation effects across all layers, capturing holistic token contributions throughout the model. Experimental results on segmentation and perturbation tests demonstrate the superiority of our proposed TokenTM compared to state-of-the-art Vision Transformer explanation methods.\\n\\n1. Introduction\\n\\nThe application of Transformer models has resulted in superior performance in computer vision [11, 17, 24, 30, 46, 47], paving the way for new breakthroughs in various tasks. However, the inherent complexity of Vision Transformers often renders them black-box models, making understanding the rationales behind their predictions a challenge. Such a lack of transparency undermines the trustworthiness of decision-making processes [2, 16, 26, 38, 49]. Therefore, it is required to interpret Transformer networks. In the visual domain, post-hoc explanation sheds light on the rationale behind a model's predictions using a heatmap over the input pixel space. This approach is both effective and efficient in providing interpretations that are easily understood by humans [13, 32, 45, 52].\\n\\nThere exist two types of post-hoc explanation methods, general traditional explanations [37, 45] and Transformer-specific attention-based explanations [1, 12]. Although traditional explanations excel in visualizing important pixels for predictions of MLPs and CNNs, their effectiveness markedly decreases when applied to Vision Transformers, due to the fundamental differences in model architectures. Therefore, attention-based explanations design new paradigms specific to Transformers, where attention weights play a dominant role [1, 12, 26, 35]. Integrating attention information, these explanation methods offer a promising approach toward Transformer interpretability.\\n\\nRecent advancements in attention-based explanation techniques have outperformed traditional methods on Vision Transformers. However, the inherent complexity of these models, particularly due to their key components: Multi-Head Self-Attention (MHSA) and Feed Forward Network (FFN) [17], continues to pose unique challenges in terms of explainability. As elucidated in Section 3, we represent the outputs of both MHSA and FFN as weighted sums of transformed tokens, with each token scaled by an attention weight. Existing attention-based methods simply consider the scaling weights indicated by attention maps as the corresponding tokens' contributions, overlooking the impacts of token transformations. As shown in Figure 1, attention weights alone misrepresent the contributions from foreground objects or background regions, while transformation information offers a necessary counterbalance. For instance, even if certain background regions are scaled by high attention weights, their actual contribution can be diminished if they are transformed into smaller or divergent tokens. Conversely, a foreground object, despite receiving minimal attention weights, can play a pivotal role due to significant transformation within the model. Given the in-\"}"}
{"id": "CVPR-2024-2411", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Visualization of attention and transformation weights, and the result of our TokenTM that integrates both of them. Circle sizes signify weight magnitudes or token lengths, and arrows indicate directions. Transformation weights are derived by our proposed measurement, which evaluates the transformation effects by gauging changes in length and direction. Both weights are visualized by heatmaps. Solely using attention weights often fails to localize foreground objects and inaccurately highlights noisy backgrounds as rationales. In contrast, leveraging additional information from transformation, our method produces object-centric post-hoc interpretations.\\n\\nIn this paper, we propose TokenTM, a novel post-hoc explanation method for Vision Transformers using token transformation measurement. We first revisit Vision Transformer layers from a generic perspective, which conceives both MHSA and FFN\u2019s outputs as weighted linear combinations of original and transformed tokens. Intuitively, tokens with increased magnitude and aligned orientations will significantly influence the linear combination outcomes. Therefore, to quantify the effects of transformations, we introduce a measurement that focuses on two fundamental token attributes: length and direction. Using the proposed measure, we integrate transformation effects, quantified by transformation weights, with the attention information, ensuring a faithful assessment of token contributions. Moreover, Vision Transformers consist of sequentially stacked layers, each performing vital token transformations and globally mixing different tokens, a process termed contextualization [47]. Recognizing the accumulative nature of these mechanisms, a single-layer analysis remains insufficient [1, 10, 26]. To holistically evaluate all layers, our solution encompasses an aggregation framework. Employing rigorous initialization and update rules, our framework yields a comprehensive contribution map, which reveals token contributions over the entire model. Experiments on segmentation and perturbation tests show that our TokenTM outperforms state-of-the-art methods.\\n\\nIn summary, our contributions are as follows:\\n\\n(i) We explore post-hoc interpretation for Vision Transformer and identify a primary issue: the lack of comprehensive consideration for token transformations and attention weights, which can result in misleading rationales of the model\u2019s prediction.\\n\\n(ii) We introduce TokenTM, a novel post-hoc explanation method employing token transformation measurement. This measurement regards changes in length and directional correlation, which faithfully assesses the impact of transformations on token contributions.\\n\\n(iii) Our approach establishes an aggregation framework that integrates both attention and token transformation effects across multiple layers, thereby capturing the cumulative nature of Vision Transformers.\\n\\n(iv) Using the proposed measurement and framework, TokenTM demonstrates superior performance in comparison to existing state-of-the-art methods.\\n\\n2. Related Work\\n\\nGeneral Traditional Explanations. Traditional post-hoc explanation methods mainly fall into two groups: gradient-based and attribution-based. Examples of gradient-based methods are Gradient*Input [40], SmoothGrad [41], Deconvolutional Network [50], Full Grad [43], Integrated Gradients [45], and Grad-CAM [37]. These methods produce saliency maps using the gradient. On the other hand, attribution-based methods propagate classification scores backward to the input [5, 20, 22, 25, 31, 39], based on Deep Taylor Decomposition [33]. There are other approaches beyond these two types, such as saliency-based [15, 32, 51], Shapley additive explanation (SHAP) [31], and perturbation-based methods [19]. Although initially designed for MLPs and CNNs, some traditional methods have been adapted for Transformers in recent works [3, 13]. However, without attention weights, these methods still yield suboptimal performance on Vision Transformers.\\n\\nTransformer-specific Attention-based Explanations. A growing line of work in interpretability develops new paradigms specifically for Transformers. Attention maps are widely used in this direction, as they are intrinsic distributions of scaling weights over tokens. Representative methods include Raw Attention [49], which regards attention as an explanation, Rollout [1], which linearly accumulates attention maps, GAE [12], a framework for variants of Transformers, ATTCAT [35], which formulates Attention Class Activation Tokens to estimate their importance, and IIA [7] and DIX [8], which propose path integration using attention. However, these approaches overlook the relative effects of transformations and fail to faithfully aggregate attention and transformation effects.\"}"}
{"id": "CVPR-2024-2411", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Aggregate token contributions across all modules, hindering reliable post-hoc interpretations for Vision Transformers.\\n\\n3. Analysis\\n\\nVision Transformer [17] sequentially stacks \\\\( L \\\\) layers, each containing an MHSA or an FFN. We first reinterpret these layers generically and then analyze the problem in Vision Transformer explanations.\\n\\n3.1. Revisiting Transformer Layers\\n\\nFrom a general viewpoint, MHSA and FFN both process weighted linear combinations of original and transformed tokens. Starting with a reinterpretation of MHSA, we illustrate this shared principle, then we show how FFN emerges as a particular case within the unified formulation.\\n\\nIn MHSA, every token of the input sequence attends to all others by projecting the embeddings to a query, key, and value. Formally, let \\\\( E \\\\in \\\\mathbb{R}^{n \\\\times d} \\\\) be the embeddings matrix (a sequence of tokens), where \\\\( n \\\\) is the number of tokens, and \\\\( d \\\\) is the dimensionality of embedding space. The projections can be expressed as:\\n\\n\\\\[\\nQ = EW^Q,\\nK = EW^K,\\nV = EW^V,\\n\\\\]\\n\\n(1)\\n\\nwhere \\\\( W^Q \\\\in \\\\mathbb{R}^{d \\\\times d_Q} \\\\), \\\\( W^K \\\\in \\\\mathbb{R}^{d \\\\times d_K} \\\\), and \\\\( W^V \\\\in \\\\mathbb{R}^{d \\\\times d_V} \\\\) are parameter matrices. Subsequently, the attention map \\\\( A \\\\) is computed by:\\n\\n\\\\[\\nA = \\\\text{Softmax}(QK^T) \\\\in \\\\mathbb{R}^{n \\\\times n}.\\n\\\\]\\n\\n(2)\\n\\nThen, contextualization is performed on value \\\\( V \\\\) using attention map \\\\( A \\\\), and another linear transformation further projects the resulting embeddings back to the space \\\\( \\\\mathbb{R}^d \\\\):\\n\\n\\\\[\\n(AV)W^H \\\\in \\\\mathbb{R}^{n \\\\times d},\\n\\\\]\\n\\n(3)\\n\\nwhere \\\\( W^H \\\\in \\\\mathbb{R}^{d_V \\\\times d_H} \\\\). To obtain the final output, MHSA integrates the results from multiple heads and incorporates them into original embeddings \\\\( E \\\\) from the previous layer by skip-connection [23]. The output of MHSA is given by:\\n\\n\\\\[\\n\\\\text{MHSA}(E) = E + n_H \\\\sum_{h=1}^{n_H} (AV)W^H\\n\\\\]\\n\\n(4)\\n\\nwhere \\\\( n_H \\\\) is the number of heads. We omit the subscript \\\\( h \\\\) for simplicity. Given the associative property of matrix multiplication, we can regard the combination of value projection and multi-head integration as a simple yet equivalent token transformation featured by\\n\\n\\\\[\\n(fW) = W^VW^H:\\n\\\\]\\n\\n(5)\\n\\nWe then reformulate the MHSA using the definition of transformed embeddings \\\\( e_E = fW \\\\):\\n\\n\\\\[\\n\\\\text{MHSA}(E) = E + n_H \\\\sum_{h=1}^{n_H} Ae_E(fW)\\n\\\\]\\n\\n(6)\\n\\nFrom a vector-level view, each token of the MHSA's output is a weighted sum of original and transformed tokens. With this insight, the FFN embodies a basic form of 'contextualization', where the number of heads \\\\( n_H = 1 \\\\) and the attention map \\\\( A = I \\\\) (identity matrix), as contrasted with Eq. (6). Utilizing the concept of transformed embeddings \\\\( e_E \\\\), the FFN can be expressed as:\\n\\n\\\\[\\n\\\\text{FFN}(E) = E + Ie_E,\\n\\\\]\\n\\n(7)\\n\\nwhere \\\\( e_E \\\\) consists of tokens that have been transformed within the FFN.\\n\\n3.2. Problem in Explaining Vision Transformers\\n\\nAs depicted in Eq. (6) and Eq. (7), Vision Transformer layers are generically expressed as weighted sums of original and transformed tokens, where each token is multiplied by a scaling weight. Existing attention-based explanations typically consider these scaling weights as the corresponding tokens' contributions, which overlooks the influences imparted by the tokens themselves. As illustrated in Figure 1, relying solely on attention weights can misrepresent the contributions from various image patches. This issue necessitates a comprehensive method to faithfully interpret the inner mechanism of Transformer layers and capture the true rationales behind the predictions.\\n\\n4. The Proposed Method\\n\\nIn this section, we propose TokenTM. We first introduce the background of attention-based explanations for Vision Transformers. Then, we develop a token transformation measurement, which evaluates the effects of transformed tokens. Finally, we design an aggregation framework to accumulate both attention and transformation information across all layers.\\n\\n4.1. Attention-based Explanations\\n\\nAttention-based explanation methods [1, 12, 13] measure the contribution of each token using the attention information, as expressed by:\\n\\n\\\\[\\nC = O + E_h(\\\\nabla \\\\mathcal{A}_p(c)) + \\\\odot T,\\n\\\\]\\n\\n(8)\\n\\nHere, \\\\( \\\\odot \\\\) is the Hadamard product. \\\\( C \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) denotes the contribution map, where \\\\( C_{ij} \\\\) represents the influence of the \\\\( j \\\\)-th input token on the \\\\( i \\\\)-th output token. Matrices \\\\( O \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) and \\\\( T \\\\in \\\\mathbb{R}^{n \\\\times n} \\\\) reflect the contributions.\"}"}
{"id": "CVPR-2024-2411", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Measurement of Relative Length\\n\\nMeasurement of Directional Correlation\\n\\nToken Transformation Measurement\\n\\nOriginal token\\n\\nTransformed token\\n\\nTransformation weight\\n\\nFigure 2. Illustration of our token transformation measurement. We depict original and transformed tokens with circles and arrows. Circle sizes reflect lengths, and arrows denote directions. The effects of token transformation are reflected by the changes in length and direction.\\n\\nOur method considers both properties to evaluate these effects, resulting in the corresponding transformation weights.\\n\\nfrom the original and transformed tokens, respectively. Previous explanation methods quantify $O$ and $T$ simply using scaling weights. Specifically, $I$ is an identity matrix representing the self-contributions of original tokens, and $A$ is the attention weights for scaling transformed tokens.\\n\\nMoreover, $\\\\nabla A_p(c) = \\\\frac{\\\\partial p(c)}{\\\\partial A}$ is the partial derivative of attention map w.r.t. the predicted probability for class $c$. $E_{\\\\text{h}}$ is the mean over multiple heads. Note that it averages across heads using the gradient $\\\\nabla A_p(c)$ to become class-specific, and it removes the negative contributions before averaging to avoid distortion [6, 12, 13, 48]. Previous methods' limitation lies in their assumption of equal influences from the skip connection and the attention weights, neglecting the difference between original and transformed tokens.\\n\\n4.2. Token Transformation Measurement\\n\\nTo account for the contributions from transformed tokens, we introduce a measurement that gauges the influence of token transformations and subsequently derives transformation weights. Using these weights, we recalibrate the matrices $O$ and $T$ to encapsulate both attention and transformation insights. Since MHSA and FFN are depicted in a unified form (see Section 3), we can first derive our measurement based on MHSA, and then adapt it for FFN from a generic perspective.\\n\\nDrawing from foundational principles of token representations, we emphasize two core attributes: length and direction [27, 44]. The intuition is that tokens with greater lengths and consistent spatial orientations predominantly determine the results of linear combinations. Thus, our measurement will involve two components, corresponding to these two attributes. The first component is a length function $L(x): R^d \\\\rightarrow R^+$, which measures the length of a token, whether the original or transformed. Mathematically, we instantiate $L$ using $\\\\|x\\\\|_2$. Considering both the length measurement $L$ and the attention weights, we reintroduce $O$ and $T$ as:\\n\\n$$O = I \\\\cdot \\\\text{diag}(L(E_1), L(E_2), \\\\ldots, L(E_n)) \\\\quad (9)$$\\n\\n$$T = A \\\\cdot \\\\text{diag}(L(e_{E_1}), L(e_{E_2}), \\\\ldots, L(e_{E_n})) \\\\quad (10)$$\\n\\nNote that $C_{ij}$ indicates the contribution of the $j$-th input token. Thus we apply the function $L$ column-wise. This approach accounts for both the attention weights and the tokens themselves. To evaluate the changes in contributions after transformations, we now regard the original tokens as reference units and analyze the relative effects of transformations. This is done by normalizing $O$ and $T$ column-wise w.r.t. the lengths of the original tokens. As a result, we obtain:\\n\\n$$O = I, \\\\quad T = A \\\\cdot \\\\text{diag}(L(e_{E_1}), L(e_{E_2}), \\\\ldots, L(e_{E_n})) \\\\quad (11)$$\\n\\nIn this formulation, $O$ uses the identity matrix to represent the contributions from original tokens as basic reference units. Meanwhile, $T$ discerns the relative influences of transformed tokens compared to the original ones using the ratio of their lengths. As detailed in Section 4.3, this approach allows us to initialize a contribution map based on the lengths of the model's input tokens, and then iteratively update the map across layers. The update employs the ratios of token effects between consecutive layers, tracing the evolution of transformations within the model. This framework ensures that our analysis remains grounded to the initial input to the model, yet dynamically adapts to every token transformation and contextualization encountered during the inference process.\\n\\nOur second component focuses on directions. Beyond adjusting lengths, token transformations also influence directions [18]. A transformed token that stays directionally closer in representation space is expected to have a stronger correlation with its original counterpart. To quantify this directional correlation, we introduce a function $C(x, e_x): R^d \\\\times R^d \\\\rightarrow R$. Mathematically, we employ Cosine similarity, which measures the angle between a pair of tokens, i.e., $C(x, e_x) = \\\\cos \\\\langle x, e_x \\\\rangle$.\\n\\nNext, we will use the function $C$ to complement the length factors in matrix $T$. Simply multiplying $C$ as a coefficient may introduce negative\"}"}
{"id": "CVPR-2024-2411", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Samira Abnar and Willem Zuidema. Quantifying attention flow in transformers. In ACL, 2020.\\n\\n[2] Chirag Agarwal, Satyapriya Krishna, Eshika Saxena, Martin Pawelczyk, Nari Johnson, Isha Puri, Marinka Zitnik, and Himabindu Lakkaraju. Openxai: Towards a transparent evaluation of model explanations. In NeurIPS, 2022.\\n\\n[3] Ameen Ali, Thomas Schnake, Oliver Eberle, Gr\u00e9goire Montavon, Klaus-Robert M\u00fcller, and Lior Wolf. Xai for transformers: Better explanations through conservative propagation. In ICML, 2022.\\n\\n[4] Pepa Atanasova, Jakob Grue Simonsen, Christina Lioma, and Isabelle Augenstein. A diagnostic study of explainability techniques for text classification. In EMNLP, 2020.\\n\\n[5] Sebastian Bach, Alexander Binder, Gr\u00e9goire Montavon, Frederick Klauschen, Klaus-Robert M\u00fcller, and Wojciech Samek. On pixel-wise explanations for non-linear classifier decisions by layer-wise relevance propagation. In PloS one, 10(7):e0130140, 2015.\\n\\n[6] Oren Barkan, Edan Hauon, Avi Caciularu, Ori Katz, Itzik Malkiel, Omri Armstrong, and Noam Koenigstein. Gradsam: Explaining transformers via gradient self-attention maps. In CIKM, 2021.\\n\\n[7] Oren Barkan, Yuval Asher, Amit Eshel, Noam Koenigstein, et al. Visual explanations via iterated integrated attributions. In ICCV, 2023.\\n\\n[8] Oren Barkan, Yehonatan Elisha, Jonathan Weill, Yuval Asher, Amit Eshel, and Noam Koenigstein. Deep integrated explanations. In CIKM, 2023.\\n\\n[9] Alexander Binder, Gr\u00e9goire Montavon, Sebastian Lapuschkin, Klaus-Robert M\u00fcller, and Wojciech Samek. Layer-wise relevance propagation for neural networks with local renormalization layers. In ICANN, 2016.\\n\\n[10] Gino Brunner, Yang Liu, Damian Pascual, Oliver Richter, Massimiliano Ciaramita, and Roger Wattenhofer. On identifiability in transformers. In ICLR, 2020.\\n\\n[11] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\\n\\n[12] Hila Chefer, Shir Gur, and Lior Wolf. Generic attention-model explainability for interpreting bi-modal and encoder-decoder transformers. In ICCV, 2021.\\n\\n[13] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In CVPR, 2021.\\n\\n[14] Hanjie Chen, Guangtao Zheng, and Yangfeng Ji. Generating hierarchical explanations on text classification via feature interaction detection. In ACL, 2020.\\n\\n[15] Piotr Dabkowski and Yarin Gal. Real time image saliency for black box classifiers. In NeurIPS, 2017.\\n\\n[16] Jay DeYoung, Sarthak Jain, Nazneen Fatema Rajani, Eric Lehman, Caiming Xiong, Richard Socher, and Byron C Wallace. Eraser: A benchmark to evaluate rationalized nlp models. In ACL, 2020.\\n\\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\\n\\n[18] Kawin Ethayarajh. How contextual are contextualized word representations? comparing the geometry of bert, elmo, and gpt-2 embeddings. In EMNLP-IJCNLP, 2019.\\n\\n[19] Ruth C Fong and Andrea Vedaldi. Interpretable explanations of black boxes by meaningful perturbation. In ICCV, 2017.\\n\\n[20] Jindong Gu, Yinchong Yang, and Volker Tresp. Understanding individual decisions of cnns via contrastive backpropagation. In ACCV, 2018.\\n\\n[21] Matthieu Guillaumin, Daniel Kuttel, and Vittorio Ferrari. Imagenet auto-annotation with segmentation propagation. IJCV, 110:328\u2013348, 2014.\\n\\n[22] Shir Gur, Ameen Ali, and Lior Wolf. Visualization of supervised and self-supervised neural networks via attribution guided factorization. In AAAI, 2021.\\n\\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\n[24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In CVPR, 2022.\\n\\n[25] Brian Kenji Iwana, Ryohei Kuroki, and Seiichi Uchida. Explaining convolutional neural networks using softmax gradient layer-wise relevance propagation. In ICCV Workshop, 2019.\\n\\n[26] Sarthak Jain and Byron C Wallace. Attention is not explanation. In NAACL, 2019.\\n\\n[27] Goro Kobayashi, Tatsuki Kuribayashi, Sho Yokoi, and Koutaro Inui. Attention is not only a weight: Analyzing transformers with vector norms. In EMNLP, 2020.\\n\\n[28] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009.\\n\\n[29] Yang Liu, Qingchao Chen, and Samuel Albanie. Adaptive cross-modal prototypes for cross-domain visual-language retrieval. In CVPR, 2021.\\n\\n[30] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In CVPR, 2022.\\n\\n[31] Scott M Lundberg and Su-In Lee. A unified approach to interpreting model predictions. In NeurIPS, 2017.\\n\\n[32] Aravindh Mahendran and Andrea Vedaldi. Visualizing deep convolutional neural networks using natural pre-images. IJCV, 120:233\u2013255, 2016.\\n\\n[33] Gr\u00e9goire Montavon, Sebastian Lapuschkin, Alexander Binder, Wojciech Samek, and Klaus-Robert M\u00fcller. Explaining nonlinear classification decisions with deep taylor decomposition. PR, 65:211\u2013222, 2017.\\n\\n[34] Dong Nguyen. Comparing automatic and human evaluation of local explanations for text classification. In NAACL, 2018.\\n\\n[35] Yao Qiang, Deng Pan, Chengyin Li, Xin Li, Rhongho Jang, and Dongxiao Zhu. Attcat: Explaining transformers via attention visualization. 2023.\"}"}
{"id": "CVPR-2024-2411", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tentive class activation tokens. In NeurIPS, 2022.\\n\\n[36] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. IJCV, 115:211\u2013252, 2015.\\n\\n[37] Ramprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In ICCV, 2017.\\n\\n[38] Sofia Serrano and Noah A Smith. Is attention interpretable? In ACL, 2019.\\n\\n[39] Avanti Shrikumar, Peyton Greenside, and Anshul Kundaje. Learning important features through propagating activation differences. In ICML, 2017.\\n\\n[40] Avanti Shrikumar, Peyton Greenside, Anna Shcherbina, and Anshul Kundaje. Not just a black box: Learning important features through propagating activation differences. In ICML, 2017.\\n\\n[41] Daniel Smilkov, Nikhil Thorat, Been Kim, Fernanda Vi\u00e9gas, and Martin Wattenberg. Smoothgrad: removing noise by adding noise. In ICML Workshop, 2017.\\n\\n[42] Jost Tobias Springenberg, Alexey Dosovitskiy, Thomas Brox, and Martin Riedmiller. Striving for simplicity: The all convolutional net. In ICLRW, 2015.\\n\\n[43] Suraj Srinivas and Fran\u00e7ois Fleuret. Full-gradient representation for neural network visualization. In NeurIPS, 2019.\\n\\n[44] Gilbert Strang. Linear algebra and its applications. Belmont, CA: Thomson, Brooks/Cole, 2006.\\n\\n[45] Mukund Sundararajan, Ankur Taly, and Qiqi Yan. Axiomatic attribution for deep networks. In ICML, 2017.\\n\\n[46] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In ICML, 2021.\\n\\n[47] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\\n\\n[48] Elena V oita, David Talbot, Fedor Moiseev, Rico Sennrich, and Ivan Titov. Analyzing multi-head self-attention: Specialized heads do the heavy lifting, the rest can be pruned. In ACL, 2019.\\n\\n[49] Sarah Wiegreffe and Yuval Pinter. Attention is not explanation. In EMNLP, 2019.\\n\\n[50] Matthew D Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In ECCV, 2014.\\n\\n[51] Bolei Zhou, Aditya Khosla, Agata Lapedriza, Aude Oliva, and Antonio Torralba. Learning deep features for discriminative localization. In CVPR, 2016.\\n\\n[52] Bolei Zhou, David Bau, Aude Oliva, and Antonio Torralba. Interpreting deep visual representations via network dissection. IEEE TPAMI, 41(9):2131\u20132145, 2018.\"}"}
{"id": "CVPR-2024-2411", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\\n\\nFigure 3. Illustration of our aggregation framework and the explanation pipeline. The overall contribution map is initialized by input token lengths and is updated using our utilization trace token evolution across layers. In \\\\( C \\\\), each \\\\( i \\\\)-th row represents the influences of input tokens \\\\( E_0 \\\\) on the output of the \\\\( l \\\\)-th layer \\\\( E_l \\\\). For \\\\( C_n \\\\), the row w.r.t. \\\\([\\\\text{CLS}]\\\\) token is extracted and reshaped to produce the final explanation map.\\n\\nValues to the contribution map, which will distort the signs of contributions through aggregation [6, 12, 13, 37, 42]. Inspired by [29], we propose the Normalized Exponential Cosine Correlation (NECC). This measurement is normalized to emphasize the relative magnitudes of each correlation instead of the polarity, thereby serving as an effective positive weighting factor. For a sequence of original tokens \\\\( S_E = (E_1, E_2, \\\\ldots, E_n) \\\\) and their transformed counterparts \\\\( S_{eE} = (eE_1, eE_2, \\\\ldots, eE_n) \\\\), the weighting factor for the \\\\( i \\\\)-th pair is given by:\\n\\n\\\\[\\n\\\\text{NECC}(i) = \\\\exp(\\\\frac{C(E_i, eE_i)}{\\\\sum_{k=1}^{n} \\\\exp(\\\\frac{C(E_k, eE_k)})}).\\n\\\\]\\n\\n(12)\\n\\nTreating the original tokens as reference units, NECC is proportional to the extent to which each transformed token correlates with its corresponding original token. Finally, employing two components of our transformation measurement, we define the transformation weights \\\\( W \\\\):\\n\\n\\\\[\\nW = \\\\text{diag}(L(eE))L(E)\\\\text{NECC}(1), \\\\ldots, L(eE_n)L(E_n)\\\\text{NECC}(n)!\\n\\\\]\\n\\n(13)\\n\\nand the update map \\\\( U \\\\) for MHSA, incorporating both attention and transformation information:\\n\\n\\\\[\\nU = O + P\\\\nabla A p(c) + \\\\odot T,\\n\\\\]\\n\\n(14)\\n\\nwith\\n\\n\\\\[\\nO = I, \\\\quad T = A \\\\cdot W.\\n\\\\]\\n\\n(15)\\n\\nHere, \\\\( U_{ij} \\\\) denotes the influence of the \\\\( j \\\\)-th input token on the \\\\( i \\\\)-th output token of the MHSA layer. This formulation balances both length and direction, reflecting how much of the original information is retained or altered in the transformed tokens, to faithfully evaluate their contributions.\\n\\nWe now adapt our established approach to the FFN layer. Recall that FFN also involves a weighted sum of original and transformed tokens (see Eq. (7)). For 'contextualization' in FFN, there is only a single-head and the weighted combination is performed locally. This simpler form can be easily reflected by discarding the gradient-weighted multi-head integration from Eq. (14) and changing the attention map \\\\( A \\\\) in Eq. (15) to an identity matrix. We formulate the update map for the FFN layer:\\n\\n\\\\[\\nU = O + T,\\n\\\\]\\n\\n(16)\\n\\nwith\\n\\n\\\\[\\nO = I, \\\\quad T = I \\\\cdot W.\\n\\\\]\\n\\n(17)\\n\\nHere, \\\\( U_{ij} \\\\) denotes the influence of the \\\\( j \\\\)-th input token on the \\\\( i \\\\)-th output token of the FFN layer. As illustrated by Figure 3, update maps capture the relative effects and will serve as refinements to the overall contribution map as it aggregates across multiple layers.\\n\\n4.3. Aggregation Framework\\n\\nIn Vision Transformers, the influences of token transformations and contextualizations do not merely reside within isolated layers but accumulate across them. For a comprehensive contribution map that reflects the roles of initial tokens corresponding to input image regions, it is necessary to assess the relationships between input tokens and their evolved counterparts in deeper layers. To this end, we introduce an aggregation framework that cohesively measures the combined influences of contextualization and transformation across the entire model.\"}"}
{"id": "CVPR-2024-2411", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Grad-CAM\\nTrans. Attr.\\nRaw Att.\\nRollout\\nGAE\\nOurs\\n\\nFigure 4. Visualizations of post-hoc explanation heatmaps. Our method captures more object-centric regions as the rationales.\\n\\nstate using initial tokens' lengths, as depicted in Figure 3. Specifically, we initialize the map as a diagonal matrix $C_0$:\\n\\n$$C_0 = \\\\text{diag}(L(E_0^1), L(E_0^2), \\\\ldots, L(E_0^n)),$$  \\\\hspace{1cm} (18)\\n\\nwhere $E_0$ denotes the initial embeddings that are fed to the first Transformer layer.\\n\\n4.3.2 Update Rules\\n\\nBased on the DAG representation of information flow [1], layer-wise aggregation is mathematically equivalent to matrix multiplication of intermediate maps. This approach allows for tracing token contributions over the entire model. After initialization, we iteratively update the contribution map $C_{l-1}$ using $U_l$, which incorporates the effects of transformed tokens:\\n\\n$$C_l \\\\leftarrow U_l \\\\cdot C_{l-1} \\\\text{ for } l = 1, 2, \\\\ldots, n.$$  \\\\hspace{1cm} (19)\\n\\nIn this formula, $C_{l-1}$ represents the map that has traced the token contributions up to the $(l-1)$-th layer but has yet to include the effects from the $l$-th layer, and $U_l$ indicates the update map from the $l$-th layer. Using this recursive formula, the map aggregates information about both token transformation and contextualization over the entire model. After applying the updates for all the layers, the final contribution map can be expressed as:\\n\\n$$C_n = U_n \\\\cdot U_{n-1} \\\\cdot \\\\cdots \\\\cdot U_1 \\\\cdot C_0,$$  \\\\hspace{1cm} (20)\\n\\nThis framework provides a cumulative understanding of how initial input tokens contribute to the final output, thereby faithfully localizing the most important pixels for Vision Transformers' predictions.\\n\\n4.4. Utilizing Contribution Map for Interpretation\\n\\nVision Transformers often use designated tokens to perform specific tasks. For example, ViT [17] performs image classification based on a $[\\\\text{CLS}]$ token. This special token becomes significant in the final prediction as it integrates information from all the inputs. To interpret the model's prediction, one can analyze the contribution of each initial input token to the final $[\\\\text{CLS}]$ token [1, 13, 26, 35]. In our proposed TokenTM, this is embodied in the overall contribution map $C_n$. Specifically, the row in $C_n$ that corresponds to the $[\\\\text{CLS}]$ token represents the influences of original tokens. As illustrated in Figure 3, we extract this row and reshape it to the image's spatial dimensions, forming a contribution heatmap. This heatmap highlights regions that are highly influential in determining the prediction, serving as a post-hoc explanation of the model.\\n\\n5. Experiments\\n\\n5.1. Baseline Methods\\n\\nWe consider baseline methods that are widely used and applicable from three types. (i) Gradient-based: Grad-CAM [37], (ii) Attribution-based: LRP [9], Conservative LRP [3], and Transformer Attribution [13]), and (iii) Attention-based: Raw Attention [26], Rollout [1], ATTCAT [35], and GAE [12]).\\n\\n5.2. Evaluated Properties\\n\\nLocalization Ability. This property measures how well an explanation can localize the foreground object recognized by the model. Intuitively, a reliable explanation should be...\"}"}
{"id": "CVPR-2024-2411", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"object-centric, i.e., accurately highlighting the object that\\nthe model uses to make the decision. Following previous\\nworks [6, 12, 13, 37], we perform segmentation on the\\nImageNet-Segmentation dataset [21], using DeiT [46], a\\nprevalent Vision Transformer model. We regard the expla-\\nnation heatmaps as preliminary semantic signals. Then we\\nuse the average value of each heatmap as a threshold to pro-\\nduce binary segmentation maps. Based on the ground-truth\\nmaps, we evaluate the performance on three metrics: Pixel-\\nwise Accuracy, mean Intersection over Union (mIoU), and\\nmean Average Precision (mAP).\\n\\nImpact on Accuracy. This aspect focuses on how well an\\nexplanation captures the correlations between pixels and the\\nmodel\u2019s accuracy. We assess this property by perturbation\\ntests on CIFAR-10, CIFAR-100 [28], and ImageNet [36].\\nThe pre-trained DeiT model is used to perform both posi-\\ntive and negative perturbation tests w.r.t. the predicted and\\nthe ground-truth class. Specifically, these tests include two\\nstages. First, we produce explanation maps for the class\\nwe want to visualize. Second, we gradually remove pixels\\nand evaluate the resulting mean top-1 accuracy. In positive\\ntests, pixels are removed from the most important to the\\nleast, while in negative tests, the removal order is reversed.\\nA faithful explanation should assign higher scores to pixels\\nthat contribute more. Thus, a severe drop in the model\u2019s\\naccuracy is expected in positive tests, while we expect the\\naccuracy to be maintained in negative tests. For quantitative\\nevaluation, we compute the Area Under the Curve (AUC)\\n[w.r.t. the accuracy curve of different perturbation levels.]\\n\\nImpact on Probability. This further measures how well the\\nexplanations capture important pixels for the model\u2019s pre-\\ndicted probabilities. Similarly, it is evaluated by perturba-\\ntion tests. We report results on Area Over the Perturbation\\nCurve (AOPC) and Log-odds score (LOdds) [14, 34, 39].\\nThese metrics quantify the average change of output proba-\\nbilities w.r.t. the predicted label. For comprehensive evalu-\\nations, we report results on ViT-B and ViT-L [17].\\n\\n5.3. Experimental Results\\nQualitative Evaluation. As shown in Figure 4, interpreta-\\ntion heatmaps of TokenTM are more precise and exhaustive.\\n\\nQuantitative Evaluation. (i) Localization Ability. Ta-\\nble 1 reports the segmentation results on the ImageNet-\\nSegmentation dataset. Our proposed TokenTM significantly\\noutperforms all the baselines on pixel accuracy, mIoU, and\\nmAP, demonstrating its stronger localization ability.\\n(ii) Impact on Accuracy. Tables 2, 3, and 4 show the results of\\nthe perturbation tests for both classes. For positive tests, a\\nlower AUC indicates superior performance, while for nega-\\ntive tests, a higher AUC is desirable. The results underscore\\nthe superiority of our method.\\n(iii) Impact on Probability.\\n\\nTable 1. Results of Localization Ability on ImageNet-Seg. [21].\\n\\n| Method     | mAP   | mIoU  | Grad-CAM |\\n|------------|-------|-------|----------|\\n| Ours       | 81.79 | 86.45 | 64.22    |\\n| Cons. LRP  | 64.06 | 68.01 | 36.23    |\\n| Trans. Attr. | 79.17 | 85.81 | 61.02    |\\n| Raw Att.   | 59.07 | 76.51 | 36.36    |\\n| Rollout    | 59.01 | 73.76 | 39.43    |\\n| ATTCAT     | 43.88 | 48.38 | 27.90    |\\n| GAE        | 79.05 | 85.71 | 61.56    |\\n| Grad-CAM   | 67.37 | 79.20 | 46.13    |\\n| LRP        | 50.96 | 55.87 | 32.82    |\\n\\nTable 2. Results of Impact on Accuracy on CIFAR-10 [28].\\n\\n| Method     | mAP   | mIoU  | Grad-CAM |\\n|------------|-------|-------|----------|\\n| Ours       | 74.90 | 37.14 | 75.05    |\\n| Cons. LRP  | 44.92 | 49.15 | 45.76    |\\n| Trans. Attr. | 67.65 | 43.64 | 65.82    |\\n| Raw Att.   | 58.46 | 47.17 | 58.46    |\\n| Rollout    | 64.88 | 42.89 | 64.88    |\\n| ATTCAT     | 41.78 | 42.66 | 41.91    |\\n| GAE        | 71.67 | 37.77 | 71.81    |\\n\\nTable 3. Results of Impact on Accuracy on CIFAR-100 [28].\\n\\n| Method     | mAP   | mIoU  | Grad-CAM |\\n|------------|-------|-------|----------|\\n| Ours       | 54.78 | 16.07 | 55.30    |\\n| Cons. LRP  | 27.11 | 27.99 | 26.84    |\\n| Trans. Attr. | 44.65 | 24.99 | 44.19    |\\n| Raw Att.   | 37.23 | 28.62 | 37.23    |\\n| Rollout    | 41.34 | 24.96 | 41.34    |\\n| ATTCAT     | 23.52 | 22.15 | 23.90    |\\n| GAE        | 52.14 | 17.52 | 52.65    |\\n\\nTable 4. Results of Impact on Accuracy on ImageNet [36].\\n\\n| Method     | mAP   | mIoU  | Grad-CAM |\\n|------------|-------|-------|----------|\\n| Ours       | 58.12 | 15.75 | 59.36    |\\n| Cons. LRP  | 33.24 | 34.35 | 34.33    |\\n| Trans. Attr. | 57.48 | 19.00 | 58.26    |\\n| Raw Att.   | 48.70 | 25.44 | 48.70    |\\n| Rollout    | 43.23 | 32.34 | 43.23    |\\n| ATTCAT     | 32.34 | 31.71 | 33.59    |\\n| GAE        | 57.53 | 15.88 | 58.85    |\"}"}
{"id": "CVPR-2024-2411", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5 reports the results of perturbation tests assessing the model's output probability. Our TokenTM achieves the best performance on Vision Transformers variants.\\n\\n5.4. Ablation Study\\n\\nAblation Study on Proposed Components.\\n\\nWe conduct ablation studies on the effect of our proposed transformation measurement (L and NECC) and aggregation framework (AF). We perform experiments on ImageNet [36], based on ViT-B [17]. Four variants are considered in our ablative experiment:\\n\\n(i) Baseline, which merely applies Eq. (8) to the input tokens without applying our proposed components,\\n(ii) Baseline + AF, which initializes the contribution map as an identity matrix and aggregates across multiple layers using $U_l$, where our proposed relative lengths and NECC measurements are discarded,\\n(iii) Baseline + AF + L, which additionally use relative length measurement in $U_l$ while still excluding the NECC terms, and\\n(iv) Baseline + AF + L + NECC, which is our TokenTM. As shown in Table 6, each proposed component improves the performance on both segmentation and perturbation tests, validating their effectiveness in the Vision Transformer explanation.\\n\\nAblation Study on Aggregation Depth.\\n\\nTo better understand the cumulative nature of Vision Transformers using TokenTM, we study the impact of varying the aggregation depth within our method. Specifically, we increase the number of aggregated layers from the initial few to the entire network depth. Table 7 reports the ablative results on ImageNet [36], using ViT-B [17], where 'n layers' denotes a variant of our TokenTM that (only) aggregates the first n layers of the model. The results reveal a consistent enhancement in performance with increased layer aggregation depth, which suggests that a deeper aggregation of token transformation and contextualization effects is pivotal for capturing the true rationale of the model's inference. Furthermore, Figure 5 illustrates the reasoning procedure of the Vision Transformer. As the aggregation encompasses more layers, the heatmaps progressively refine the focused areas and become more concentrated on the object, localizing the rationale behind the model's prediction.\\n\\n6. Conclusions\\n\\nIn this paper, we explored the challenge of explaining Vision Transformers. Upon reinterpretation, we expressed Transformer layers using a generic form in terms of weighted linear combinations of original and transformed tokens. This new perspective revealed a principal issue in existing post-hoc methods: they solely rely on attention weights and ignore significant information from token transformations, which leads to misunderstandings of rationales behind the models' decisions. To tackle this issue, we proposed TokenTM, an explanation method comprising a token transformation measurement and an aggregation framework. Quantifying the contributions of input tokens throughout the entire model, our method generates more faithful post-hoc interpretations for Vision Transformers.\\n\\nAcknowledgments: This research is supported by NSF IIS-2309073 and ECCS-212352101. This article solely reflects the opinions and conclusions of its authors and not the funding agents.\"}"}
