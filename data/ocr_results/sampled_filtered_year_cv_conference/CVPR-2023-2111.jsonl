{"id": "CVPR-2023-2111", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Toward Stable, Interpretable, and Lightweight Hyperspectral Super-resolution\\n\\nWen-jin Guo, Weiying Xie, Kai Jiang, Yunsong Li, Jie Lei, Leyuan Fang\\n\\n1 State Key Laboratory of Integrated Services Networks, Xidian University\\n2 College of Electrical and Information Engineering, Hunan University\\n\\nguowenjin@stu.xidian.edu.cn wyxie@xidian.edu.cn xdjiangkai@foxmail.com\\njielei, ysli@mail.xidian.edu.cn fangleyuan@gmail.com\\n\\nAbstract\\n\\nFor real applications, existing HSI-SR methods are not only limited to unstable performance under unknown scenarios but also suffer from high computation consumption. In this paper, we develop a new coordination optimization framework for stable, interpretable, and lightweight HSI-SR. Specifically, we create a positive cycle between fusion and degradation estimation under a new probabilistic framework. The estimated degradation is applied to fusion as guidance for a degradation-aware HSI-SR. Under the framework, we establish an explicit degradation estimation method to tackle the indeterminacy and unstable performance caused by the black-box simulation in previous methods. Considering the interpretability in fusion, we integrate spectral mixing prior into the fusion process, which can be easily realized by a tiny autoencoder, leading to a dramatic release of the computation burden. Based on the spectral mixing prior, we then develop a partial fine-tune strategy to reduce the computation cost further. Comprehensive experiments demonstrate the superiority of our method against the state-of-the-arts under synthetic and real datasets. For instance, we achieve a $2.3$ dB promotion on PSNR with $120 \\\\times$ model size reduction and $4300 \\\\times$ FLOPs reduction under the CAVE dataset. Code is available in https://github.com/WenjinGuo/DAEM.\\n\\n1. Introduction\\n\\nDifferent from traditional optical images with a few channels, hyperspectral images (HSIs) with tens to hundreds of bands hold discriminative information about materials, leading to a great advantage in a wide range of applications, e.g., the monitoring and management of ecosystems, biodiversity, and disasters [1\u20138]. However, the physical limitation in imaging causes a trade-off between spatial and spectral resolution. HSIs are suffered from low spatial resolution in real applications. Therefore, hyperspectral super-resolution (HSI-SR), which aims to promote the spatial resolution of HSIs, has become a significant task, and always performs as a necessary pre-processing of HSI applications, i.e., detection and classification [9\u201316].\\n\\nDue to the common optical platforms which are equipped with both HSI sensor and multispectral sensor (such as satellites, airborne platforms), HSIs and multispectral images (MSIs) imaged in the same scene are easy to access. Fusion-based HSI-SR aims to estimate the desired high resolution HSI (HR-HSI) from the corresponding low resolution HSI (LR-HSI) and high resolution MSI (HR-MSI). Naturally, HSI-SR can be modeled as a maximum a posterior (MAP) estimation:\\n\\n$$p(Z|X, Y) \\\\propto p(X, Y|Z, \\\\theta) p(Z|\\\\phi),$$\\n\\nwhere $Z \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times B}$ is the target HR-HSI with $H$, $W$ and $B$ as its height, width and number of bands, respectively. $X \\\\in \\\\mathbb{R}^{h \\\\times w \\\\times B}$ is the observed LR-HSI with $h$ and $w$ as its height and width.\"}"}
{"id": "CVPR-2023-2111", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"its height and width \\\\((h < H, w < W)\\\\). \\n\\n\\\\(Y \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times b}\\\\) is the HR-MSI with \\\\(b\\\\) bands \\\\((b < B)\\\\). \\\\(\\\\theta\\\\) and \\\\(\\\\phi\\\\) are parameters in the likelihood and the prior, respectively. As a pre-processing task, HSI-SR faces two challenges, high fidelity recovery and efficient processing (i.e., low computational burden and fast processing). Referred to Eq. (1), we analyse these two ingredients from three perspectives: the likelihood term, the prior term, and coordination between them.\\n\\n**Likelihood Term**\\n\\nThe likelihood term reflects the degradation process, and the widely accepted degradation model can be formulated as:\\n\\n\\\\[\\nX = (Z \\\\ast C) \\\\downarrow s + N \\\\quad X,\\n\\\\]\\n\\n\\\\[\\nY = Z \\\\times 3R + N \\\\quad Y,\\n\\\\]\\n\\nwhere \\\\(C \\\\in \\\\mathbb{R}^{s \\\\times s}\\\\), \\\\(R \\\\in \\\\mathbb{R}^{B \\\\times b}\\\\) represent the PSF (point spread function) and SRF (spectral response function), respectively. \\\\(\\\\downarrow s\\\\) represents spatial downsampling with \\\\(s\\\\) times. \\\\(\\\\times 3\\\\) is matrix multiple on the third dimension. Early methods demand precise degradation parameters to optimize the likelihood \\\\([17\u201319]\\\\). However, PSF and SRF are various and unknown in real scenarios, hence the high demand for blind HSI-SR. In \\\\([20]\\\\), two convolution blocks are built to simulate the degradation process, which learn the degradation in training sets and apply it to testing samples. Unsupervised blind HSI-SR methods estimate degradation in each image-pair individually \\\\([21\u201323]\\\\). Yao et al. \\\\([22]\\\\) propose a spatial-spectral consistency to further constrain the degradation estimation. Despite better applicability, recent blind HSI-SR methods are limited by the DL-based estimation. On one hand, the implicit modeling of degradation imports a black box in optimization and draws uncertainty, especially in volatile degradation. On the other hand, degradation estimation is an inverse problem with multiple candidates, the common over-fitting in neural networks will lead to inaccurate and unstable performance.\\n\\n**Prior Term**\\n\\nIf only considering the likelihood term, HSI-SR is an ill-posed problem with infinite solutions. The prior term shrinks the solution space as a regularization. Earlier works make assumptions on prior through manual induction of data characteristics, such as low rank \\\\([24, 25]\\\\), and sparsity \\\\([26\u201330]\\\\). Recent supervised deep learning (DL)-based methods replace this process through data characterization by neural networks. These inductive methods will drop a lot on performance when testing samples differ training samples, hence a critical need for general prior assumption.\\n\\nAs an inherent feature in HSIs, spectral mixing prior simulates the common spectral mixing phenomenon in imaging. Unmixing-based methods make a breakthrough in fidelity \\\\([17,18,31,32]\\\\). Moreover, to promote the generalization, coupled autoencoder is proposed to simulate the spectral mixing with deep learning toolkit \\\\([21, 22]\\\\). Despite superior fidelity, recent unsupervised DL-based methods over-focus on network architecture and underestimate the effort of prior knowledge in HSIs, resulted in two drawbacks. On one hand, the over-designed network structures weaken the role of the prior assumption, which harms the interpretability. On the other hand, the complicated network structures pose a heavy burden on power and memory.\\n\\n**Coordination between Likelihood and Prior**\\n\\nThe most recent blind HSI-SR methods generally contain two modules, the fusion module and the degradation estimator \\\\([22,23,33]\\\\). From the viewpoint of MAP problem, the former aims to recover the HR-HSI through the observations under the regularization of the prior. The later minimizes the likelihood term by estimating degradation parameters. The unknown HR-HSI and degradation determine the observations. Naturally, the estimated degradation can be fed to the fusion module as a guidance. However, in recent methods, the degradation estimator and fusion module only interacts in backward stage. The learned degradation parameters are not involved in the updating of fusion module or prior parameters, which brings two weaknesses. Firstly, the nearly independent optimization of likelihood and prior causes more updating steps and slows the convergence. Secondly, optimization with less interaction inducts conflict optimization directions and results in local optimum with unreal recovery.\\n\\nIn this paper, we develop a novel coordination optimization framework for stable, interpretable, and lightweight HSI-SR. Through integrating the Wald protocol in the MAP problem, we construct a postive feedback loop between prior and likelihood. Based on the framework, we explore an explicit degradation estimation to remedy the unstable performance of black-box estimation. As for the prior, we establish a lightweight autoencoder to simulate the spectral mixing prior in HSIs for a interpretable fusion. Our contributions are summarized as follows:\\n\\n1. We explore a coordination optimization framework for HSI-SR with fast convergence and stable performance. Under the framework, the degradation estimation and fusion process promote each other concordantly. To the best of our knowledge, it is the first work to explore the cooperative relationship between prior and likelihood in HSI-SR.\\n2. An explicit estimation method is established in HSI-SR firstly. Through modeling PSF and SRF with anisotropic Gaussian kernel and Gaussian mixture, tiny parameters realize stable and precise estimation.\\n3. We simulate the general spectral mixing prior with only one interpretable autoencoder. Compared with recent complicated models, the network makes a breakthrough in lightweight. Based on the fusion netwrok, we explore a partial optimization strategy in test stage, which only updates the decoder that handles the individual spectral feature of...\"}"}
{"id": "CVPR-2023-2111", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"2. Related Work\\n\\n2.1. Spectral Mixing Prior in HSI-SR\\n\\nRecent unmixing-based methods implement the spectral mixing prior through coupled autoencoder [21, 22, 33]. The observed LR-HSI and HR-MSI are reconstructed by two autoencoders to extract abundances (latent features) and end-members (weights of decoders). Then HR-HSI is generated by the decoder of LR-HSI with the latent feature of HR-MSI as input. There are two limitations. Firstly, the separate unmixing of LR-HSI and HR-MSI leads to a particular sensitivity to the divergence of autoencoders' features. Even several modifications are applied, such as cross-attention [22] and alternative optimization [21]. Secondly, high-dimensional hyperspectral data dramatically increases the computational burden on coupled autoencoder. By contrast, we simulate the unmixing prior with only one autoencoder, leading to more accurate results while reducing the computational complexity significantly.\\n\\n2.2. Degradation Estimation\\n\\nImage enhancement is a typical ill-posed problem with infinite solutions, hence the urgent necessity for precise degradation estimation to constrain the solution space. Estimation of Blur Kernel\\n\\nIn single image super-resolution (SISR), estimation of blur kernel become a hot topic. Wang et al. [34] propose an contrastive learning scheme for degradation representation under relative distances between different blur kernels. Luo et al. [35] introduce probabilistic model and build a single network to mapping the distribution of blur kernels. In [36], an explicit degradation estimation scheme is proposed with modeling blur kernel with anisotropic Gaussian. In HSI-SR, Qu et al. [21] first estimate PSF unsupervisedly with several convolution layers. Yao et al. [22] regularize the estimation with spatial-spectral consistency. In [33], a single convolution layer is applied to simulate the PSF and realize closer result to groundtruth.\\n\\nEstimation of SRF\\n\\nSRF is a specific degradation in HSI-SR. Previous methods model SRF with $1 \\\\times 1$ convolution layers or fully-connect layers [21\u201323, 33, 37].\\n\\n2.3. Wald Protocol\\n\\nThe target HR-HSI with same spatial resolution to HR-MSI is not accessible in real world. To measure the performance of HSI-SR methods, Wald protocol indicates that the performance of recovering LR-HSI from degraded HR-MSI and LR-HSI is consistent to the recovery of unknown HR-HSI, which links the degradation and fusion [38].\\n\\n3. The Proposed Method\\n\\n3.1. Problem Formulation\\n\\nAccording to Eq. (1), HSI-SR is a MAP problem determined by parameters $\\\\theta = \\\\{C, R\\\\}$ and $\\\\phi$. In practice, the degradation parameter $\\\\theta$ and prior knowledge $\\\\phi$ are not completely known. Thus, we re-model blind HSI-SR with an additional inference of parameters:\\n\\n$$\\\\max Z, \\\\theta, \\\\phi \\\\log p(X, Y|Z, \\\\theta) + \\\\log p(Z|X, Y, \\\\phi) + \\\\log p(\\\\phi|X, Y, \\\\theta) + \\\\log p(\\\\theta|X, Y).$$\\n\\n(3)\\n\\nwhere $p(\\\\theta|X, Y)$ represents inferring degradation parameters $\\\\theta$ without the guidance of HR-HSI, leading to unsupervised degradation estimation. The estimated degradation $\\\\theta$ is involved in the inference of fusion module, i.e., $p(\\\\phi|X, Y, \\\\theta)$, which limits the freedom of original MAP model for a coordination optimization.\\n\\nWe will discuss each item in Eq. (3) at the remainder of this section. The solution of Eq. (3) will be introduced in Sec. 4.\\n\\n3.2. Coordination between Likelihood and Prior\\n\\nIn Wald protocol, the fusion module should recover LR-HSI from the degraded LR-HSI and HR-MSI. Based on Wald protocol, we can find:\\n\\n$$X = F((X \\\\ast C) \\\\downarrow s, (Y \\\\ast C) \\\\downarrow s; \\\\phi) + N_X,$$\\n\\n(4)\\n\\nwhere $F(\\\\cdot)$ represents the fusion module. $N_X$ is a Gaussian noise with zero-mean, thus we have:\\n\\n$$p(\\\\phi|X, Y, \\\\theta) = h_{Y}\\\\sum_{i=1}^{w} h_{Y}\\\\sum_{j=1}^{w} N(\\\\hat{X}_{i,j}(\\\\phi)|x_{i,j}, \\\\epsilon_{1}^{2}I),$$\\n\\n(5)\\n\\nwhere $\\\\hat{X} = F((X \\\\ast C) \\\\downarrow s, (Y \\\\ast C) \\\\downarrow s; \\\\phi)$. Evidently, the estimated degradation parameters guide the optimization of the fusion module. Then, the guided fusion result $Z = F(X, Y; \\\\phi)$ will facilitates the update of degradation parameters, i.e., maximum of $p(X, Y|Z, \\\\theta)$. In this way, the coordination optimization framework forms a virtuous circle between degradation estimation and fusion, leading to a robust HSI-SR. Meanwhile, the framework is orthogonal to the degradation estimation module and fusion module. Thus, it can be easily combined with well-designed modules for a further promotion on performance. Next, we will discuss these two modules.\\n\\n3.3. Explicit Degradation Estimation\\n\\nHere, we aim to capture the inherent pattern of degradations explicitly, expecting to a stable and precise estimation. Similar to the widely recognized degradation model\"}"}
{"id": "CVPR-2023-2111", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Stage 1 of M-step: update degradation estimation\\n\\nStage 2 of M-step: update fusion module under Wald protocol\\n\\nFusion module\\n\\n\\\\[ \\\\text{Fusion Module} \\\\]\\n\\n\\\\[ \\\\Phi \\\\]\\n\\n\\\\[ \\\\mathcal{F} \\\\]\\n\\n\\\\[ \\\\mathcal{G} \\\\]\\n\\n\\\\[ \\\\theta \\\\]\\n\\n\\\\[ (|, \\\\mathcal{F}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{G}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{H}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{L}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{M}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{N}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{O}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{P}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{Q}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{R}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{S}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{T}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{U}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{V}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{W}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{X}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{Y}) \\\\]\\n\\n\\\\[ (|, \\\\mathcal{Z}) \\\\]\\n\\n\\\\[ \\\\text{category codes} \\\\]\\n\\n\\\\[ \\\\Phi_{a} \\\\]\\n\\n\\\\[ \\\\Phi_{e} \\\\]\\n\\n\\\\{ \\\\}\\n\\n\\\\[ \\\\theta, \\\\mu, \\\\sigma = \\\\Lambda, \\\\]\\n\\nHR-MSI\\n\\nLR-MSI\\n\\nGenerated\\n\\nLR-HSI\\n\\nGenerated\\n\\nHR-MSIReconstructed\\n\\nLR-HSI\\n\\nEstimated\\n\\nHR-HSI\\n\\nFigure 2. An overview of the proposed coordination optimization HSI-SR framework. Degradation estimation and fusion facilitate a positive feedback loop in the framework. As shown in (c), the estimated PSF is involved in the optimization of the fusion module as a guidance. Explicit modeling of PSF and SRF are built under anisotropic Gaussian kernel and mixture Gaussian for a stable degradation estimation. A lightweight autoencoder is constructed to recover the HR-HSI under the spectral mixing prior for an interpretable fusion. To solve the HSI-SR structure flexibly and efficiently, a Monte Carlo EM algorithm is applied with alternative optimization.\\n\\n\\\\[ \\\\text{[36, 39], we simulate the PSF} \\\\]\\n\\n\\\\[ C_{i,j} = \\\\frac{1}{2\\\\pi} \\\\exp \\\\left( -\\\\frac{1}{2} S^{T} \\\\Lambda S \\\\right), \\\\]\\n\\n\\\\[ S = [i, j]^{T}, i, j \\\\in -\\\\frac{s}{2}, \\\\ldots, \\\\frac{s}{2}, \\\\]\\n\\n\\\\[ (6) \\\\]\\n\\nwhere \\\\( \\\\Lambda \\\\in \\\\mathbb{R}^{2} \\\\) is the covariance of the Gaussian blur kernel, and \\\\( \\\\Lambda \\\\) is a symmetric positive definite matrix. Thus, only 3 parameters can handle the blur kernel.\\n\\nSimilarly, the SRF \\\\( R \\\\) lies in simple mode. We can estimate each column of \\\\( R \\\\) through mixture Gaussian with \\\\( K \\\\) components (only \\\\( 2K \\\\) parameters can realize estimation, which is evidently less than previous methods with \\\\( B \\\\times b \\\\) parameters at least):\\n\\n\\\\[ R_{i,j} = \\\\sum_{k=1}^{K} \\\\frac{1}{\\\\sqrt{2\\\\pi} \\\\sigma_{j,k}} \\\\exp \\\\left( -\\\\frac{(i - \\\\mu_{j,k})^{2}}{2\\\\sigma_{j,k}^{2}} \\\\right), \\\\]\\n\\n\\\\[ (7) \\\\]\\n\\nwhere \\\\( \\\\mu_{j,k} \\\\) and \\\\( \\\\sigma_{j,k} \\\\) are mean and variance of \\\\( k \\\\)-th Gaussian components in \\\\( j \\\\)-th band of HR-MSI. With the estimated degradation parameters, the likelihood \\\\( p(X, Y|Z, \\\\theta) \\\\) can be mathematically expressed as:\\n\\n\\\\[ p(X, Y|Z, \\\\theta) = p(X|Z, \\\\theta) p(Y|Z, \\\\theta), \\\\]\\n\\n\\\\[ p(X|Z, \\\\theta) = H_{Y} \\\\sum_{i=1}^{W} \\\\sum_{j=1}^{N} N(x_{i,j}|(Z \\\\ast C)_{i,j}, \\\\epsilon_{2}), \\\\]\\n\\n\\\\[ p(Y|Z, \\\\theta) = H_{Y} \\\\sum_{i=1}^{W} \\\\sum_{j=1}^{N} N(y_{i,j}|(Z \\\\times R)_{i,j}, \\\\epsilon_{3}), \\\\]\\n\\n\\\\[ (8) \\\\]\\n\\nIn general, the spectral degradation of LR-HSI and the spatial degradation of HR-HSI stay close to each other, which is the so-called spatial-spectral consistency [22]. Therefore, \\\\( p(\\\\theta|X, Y) \\\\) is in direct proportion to the similarity of these two degradations. Naturally, we model the posterior distribution of \\\\( \\\\theta \\\\) as:\\n\\n\\\\[ p(\\\\theta|X, Y) \\\\propto \\\\exp \\\\left( ||X \\\\ast R - (Y \\\\ast C)_{\\\\downarrow s}||^{2} \\\\right). \\\\]\\n\\n\\\\[ (9) \\\\]\\n\\n3.4. Interpretable and Lightweight Fusion\\n\\nTwo ingredients should be considered in fusion. Firstly, the fusion module should reflect the physical generation of HR-HSI as a prior knowledge for a clear interpretability. Secondly, considering the limited computation resource in real applications, the fusion module should be lightweight. For the first point, we build a two stage model to simulate the spectral mixing phenomenon in HSIs imaging:\"}"}
{"id": "CVPR-2023-2111", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Jose M. Bioucas-Dias, Antonio Plaza, Gustavo Camps-Valls, Paul Scheunders, Nasser Nasrabadi, and Jocelyn Chanussot. Hyperspectral remote sensing data analysis and future challenges. IEEE Geoscience and Remote Sensing Magazine, 1(2):6\u201336, 2013.\\n\\n[2] Lujendra Ojha, Mary Beth Wilhelm, Scott L. Murchie, Alfred S. McEwen, James J. Wray, Jennifer Hanley, Marion Mass\u00e9, and Matt Chojnacki. Spectral evidence for hydrated salts in recurring slope lineae on Mars. Nature Geoscience, 8(11):829\u2013832, November 2015.\\n\\n[3] Ying Fu, Antony Lam, Yasuyuki Kobashi, Imari Sato, Takahiro Okabe, and Yoichi Sato. Reflectance and fluorescent spectra recovery based on fluorescent chromaticity variance under varying illumination. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2014.\\n\\n[4] Ying Fu, Antony Lam, Imari Sato, Takahiro Okabe, and Yoichi Sato. Separating reflective and fluorescent components using high frequency illumination in the spectral domain. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), December 2013.\\n\\n[5] Bing Lu, Phuong D. Dao, Jiangui Liu, Yuhong He, and Jiali Shang. Recent advances of hyperspectral imaging technology and applications in agriculture. Remote Sensing, 12(16), 2020.\\n\\n[6] A-K Mahlein, Matheus T Kuska, Jan Behmann, Gerrit Polder, and Achim Walter. Hyperspectral sensors and imaging technologies in phytopathology: state of the art. Annual review of phytopathology, 56:535\u2013558, 2018.\\n\\n[7] Guolan Lu and Baowei Fei. Medical hyperspectral imaging: a review. Journal of biomedical optics, 19(1):010901, 2014.\\n\\n[8] Pedram Ghamisi, Naoto Yokoya, Jun Li, Wenzhi Liao, Sicong Liu, Javier Plaza, Behnood Rasti, and Antonio Plaza. Advances in hyperspectral image and signal processing: A comprehensive overview of the state of the art. IEEE Geoscience and Remote Sensing Magazine, 5(4):37\u201378, 2017.\\n\\n[9] Shutao Li, Weiwei Song, Leyuan Fang, Yushi Chen, Pedram Ghamisi, and J \u00b4on Atli Benediktsson. Deep learning for hyperspectral image classification: An overview. IEEE Transactions on Geoscience and Remote Sensing, 57(9):6690\u20136709, 2019.\\n\\n[10] Xiao Xiang Zhu, Devis Tuia, Lichao Mou, Gui-Song Xia, Liangpei Zhang, Feng Xu, and Friedrich Fraundorfer. Deep learning in remote sensing: A comprehensive review and list of resources. IEEE Geoscience and Remote Sensing Magazine, 5(4):8\u201336, 2017.\\n\\n[11] Hien Van Nguyen, Amit Banerjee, and Rama Chellappa. Tracking via object reflectance using a hyperspectral video camera. In 2010 IEEE Computer Society Conference on Computer Vision and Pattern Recognition-Workshops, pages 44\u201351. IEEE, 2010.\\n\\n[12] Yuliya Tarabalka, Jocelyn Chanussot, and J \u00b4on Atli Benediktsson. Segmentation and classification of hyperspectral images using minimum spanning forest grown from automatically selected markers. IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), 40(5):1267\u20131279, 2009.\\n\\n[13] Qi Wang, Jianzhe Lin, and Yuan Yuan. Salient band selection for hyperspectral image classification via manifold ranking. IEEE transactions on neural networks and learning systems, 27(6):1279\u20131289, 2016.\\n\\n[14] Burak Uzkent, Matthew J Hoffman, and Anthony Vodacek. Real-time vehicle tracking in aerial video using hyperspectral features. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 36\u201344, 2016.\\n\\n[15] Burak Uzkent, Aneesh Rangnekar, and Matthew Hoffman. Aerial vehicle tracking by adaptive fusion of hyperspectral likelihood maps. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 39\u201348, 2017.\\n\\n[16] Yunsong Li, Weiying Xie, and Huaqing Li. Hyperspectral image reconstruction by deep convolutional neural network for classification. Pattern Recognition, 63:371\u2013383, 2017.\\n\\n[17] Naoto Yokoya, Takehisa Yairi, and Akira Iwasaki. Coupled nonnegative matrix factorization unmixing for hyperspectral and multispectral data fusion. IEEE Transactions on Geoscience and Remote Sensing, 50(2):528\u2013537, 2012.\\n\\n[18] Miguel Sim\u00f5es, Jos\u00e9 Bioucas-Dias, Luis B. Almeida, and Jocelyn Chanussot. A convex formulation for hyperspectral image superresolution via subspace-based regularization. IEEE Transactions on Geoscience and Remote Sensing, 53(6):3373\u20133388, 2015.\\n\\n[19] Qi Wei, Nicolas Dobigeon, and Jean-Yves Tourneret. Fast fusion of multi-band images based on solving a Sylvester equation. IEEE Transactions on Image Processing, 24(11):4109\u20134121, 2015.\\n\\n[20] Weisheng Dong, Chen Zhou, Fangfang Wu, Jinjian Wu, Guangming Shi, and Xin Li. Model-guided deep hyperspectral image super-resolution. IEEE Transactions on Image Processing, 30:5754\u20135768, 2021.\\n\\n[21] Ying Qu, Hairong Qi, and Chiman Kwan. Unsupervised sparse dirichlet-net for hyperspectral image super-resolution. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2511\u20132520, 2018.\\n\\n[22] Jing Yao, Danfeng Hong, Jocelyn Chanussot, Deyu Meng, Xiaoxiang Zhu, and Zongben Xu. Cross-attention in coupled unmixing nets for unsupervised hyperspectral super-resolution. In ECCV 2020, pages 208\u2013224, 2020.\\n\\n[23] L. Zhang, J. Nie, W. Wei, Y. Zhang, and L. Shao. Unsupervised adaptation learning for hyperspectral imagery super-resolution. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\"}"}
{"id": "CVPR-2023-2111", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Renwei Dian, Shutao Li, and Leyuan Fang. Learning a low tensor-train rank representation for hyperspectral image super-resolution. IEEE Transactions on Neural Networks and Learning Systems, 30(9):2672\u20132683, 2019.\\n\\nRenwei Dian and Shutao Li. Hyperspectral image super-resolution via subspace-based low tensor multi-rank regularization. IEEE Transactions on Image Processing, 28(10):5135\u20135146, 2019.\\n\\nShutao Li, Renwei Dian, Leyuan Fang, and Jos\u00e9 M. Bioucas-Dias. Fusing hyperspectral and multispectral images via coupled sparse tensor factorization. IEEE Transactions on Image Processing, 27(8):4118\u20134130, 2018.\\n\\nNaveed Akhtar, Faisal Shafait, and Ajmal Mian. Sparse spatio-spectral representation for hyperspectral image super-resolution. In European conference on computer vision, pages 63\u201378. Springer, 2014.\\n\\nBoaz Arad and Ohad Ben-Shahar. Sparse recovery of hyperspectral signal from natural RGB images. In European Conference on Computer Vision, pages 19\u201334. Springer, 2016.\\n\\nRenwei Dian, Leyuan Fang, and Shutao Li. Hyperspectral image super-resolution via non-local sparse tensor factorization. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5344\u20135353, 2017.\\n\\nWeisheng Dong, Fazuo Fu, Guangming Shi, Xun Cao, Jinjian Wu, Guangyu Li, and Xin Li. Hyperspectral image super-resolution via non-negative structured sparse representation. IEEE Transactions on Image Processing, 25(5):2337\u20132352, 2016.\\n\\nNaveed Akhtar, Faisal Shafait, and Ajmal Mian. Bayesian sparse representation for hyperspectral image super-resolution. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3631\u20133640, 2015.\\n\\nWeisheng Dong, Fazuo Fu, Guangming Shi, Xun Cao, Jinjian Wu, Guangyu Li, and Xin Li. Hyperspectral image super-resolution via non-negative structured sparse representation. IEEE Transactions on Image Processing, 25(5):2337\u20132352, May 2016.\\n\\nKe Zheng, Lianru Gao, Wenzhi Liao, Danfeng Hong, Bing Zhang, Ximin Cui, and Jocelyn Chanussot. Coupled convolutional neural network with adaptive response function learning for unsupervised hyperspectral super-resolution. IEEE Transactions on Geoscience and Remote Sensing, 59(3):2487\u20132502, 2021.\\n\\nLongguang Wang, Yingqian Wang, Xiaoyu Dong, Qingyu Xu, Jungang Yang, Wei An, and Yulan Guo. Unsupervised degradation representation learning for blind super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10581\u201310590, 2021.\\n\\nZhengxiong Luo, Yan Huang, Shang Li, Liang Wang, and Tieniu Tan. Learning the degradation distribution for blind image super-resolution. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6063\u20136072, 2022.\\n\\nZongsheng Yue, Qian Zhao, Jianwen Xie, Lei Zhang, Deyu Meng, and Kwan-Yee K. Wong. Blind image super-resolution with elaborate degradation modeling on noise and kernel. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2118\u20132128, 2022.\\n\\nYing Fu, Tao Zhang, Yinqiang Zheng, Debing Zhang, and Hua Huang. Hyperspectral image super-resolution with optimized RGB guidance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11661\u201311670, 2019.\\n\\nLucien Wald. Data fusion: definitions and architectures: fusion of images of different spatial resolutions. Presses des MINES, 2002.\\n\\nQi Xie, Minghao Zhou, Qian Zhao, Zongben Xu, and Deyu Meng. Mhf-net: An interpretable deep network for multispectral and hyperspectral image fusion. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(3):1457\u20131473, 2022.\\n\\nD. P. Kingma and M. Welling. Auto-encoding variational bayes. In ICLR, 2014.\\n\\nD. Iso F. Yasuma, T. Mitsunaga, and S. K. Nayar. Generalized Assorted Pixel Camera: Post-Capture Control of Resolution, Dynamic Range and Spectrum. Technical report, Nov 2008.\\n\\nA. Chakrabarti and T. Zickler. Statistics of Real-World Hyperspectral Images. In Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 193\u2013200, 2011.\\n\\nNaoto Yokoya, Claas Grohnfeldt, and Jocelyn Chanussot. Hyperspectral and multispectral data fusion: A comparative review of the recent literature. IEEE Geoscience and Remote Sensing Magazine, 5(2):29\u201356, 2017.\\n\\nRoberta H. Yuhas, Joseph W. Boardman, and Alexander F. H. Goetz. Determination of semi-arid landscape endmembers and seasonal trends using convex geometry spectral unmixing techniques. In JPL, Summaries of the 4th Annual JPL Airborne Geoscience Workshop. Volume 1: AVIRIS Workshop, 1993.\\n\\nZhou Wang and Alan C. Bovik. A universal image quality index. IEEE signal processing letters, 9(3):81\u201384, 2002.\"}"}
{"id": "CVPR-2023-2111", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"G(\u00b7) aims to map each hyperspectral vector from corresponding category of materials. For the second point, we implement the fusion module with a tiny autoencoder. The encoder stacks five $1 \\\\times 1$ convolution layers, which aims to predict the category of input pixel, while the subsequent decoder recovers the HSIs from the predicted class information on each pixel. Because of the simple mode of spectrums in HSIs, we build the decoder with only two fully-connected layers.\\n\\n4. Optimization\\n\\n4.1. Training\\n\\nFor an efficient solution, we apply Monte Carlo Expectation Maximum (EM) algorithm on Eq. (3), which alternately updates parameters and desired distribution. The overview of the proposed EM algorithm in training stage is presented in Fig. 2.\\n\\nE-step\\n\\nIn this stage, we fix the parameters in Eq. (3) and search the optimal distribution of $Z$. Following the classic EM algorithm, we set it as $p(Z|X, Y, \\\\phi_{\\\\text{old}})$ (where $\\\\phi_{\\\\text{old}}$ is the current fusion parameter). Thus, the corresponding evident lower bound (ELBO, equivalent to the original MAP problem) can be written as:\\n\\n$$L(Z, \\\\phi, \\\\theta; X, Y) \\\\propto Z p(Z|X, Y, \\\\phi_{\\\\text{old}}) \\\\log p(Z, \\\\theta, \\\\phi|X, Y) dZ.$$  \\n\\nThen we simplify the calculation of ELBO through Monte Carlo approach. Through sampling the latent variable $Z$ from $p(Z|X, Y, \\\\phi_{\\\\text{old}})$, the ELBO is simplified to an accumulation:\\n\\n$$L(Z, \\\\theta, \\\\phi; X, Y) \\\\approx \\\\sum_{i=1}^{N} \\\\log p(Z_i, \\\\theta, \\\\phi|X, Y) + \\\\text{const},$$  \\n\\nwhere $Z_i$ is the $i$-th sample from $p(Z|X, Y, \\\\phi_{\\\\text{old}})$ and we set $i=1$ similar to [40].\\n\\nM-step\\n\\nM-step aims to maximize the approximated ELBO in E-step w.r.t. the parameters $\\\\theta, \\\\phi$. We apply alternative optimization strategy to solve this problem. Firstly, we fix $\\\\phi$ and update $\\\\theta$. According to Eq. (3), the object of optimization on $\\\\theta$ is:\\n\\n$$\\\\max_{\\\\theta} \\\\log p(X, Y|Z, \\\\theta) p(\\\\theta|X, Y) = \\\\Rightarrow \\\\min_{\\\\theta} \\\\alpha_1 ||X - G(D(Y; \\\\phi_a); \\\\phi_e)||_2 + \\\\alpha_2 ||Y - Z \\\\times 3R||_2 + \\\\alpha_3 ||X \\\\times 3R - (Y \\\\times C)\\\\downarrow_s||_2.$$  \\n\\nWe use $\\\\alpha_1, \\\\alpha_2, \\\\alpha_3, \\\\beta$ to trade-off the efforts of different terms. Note that we omit $p(Z|X, Y, \\\\phi)$, because the prior knowledge of HR-HSI is contained in the structure of the fusion module.\\n\\nAt the end of optimization, we can estimate the desired HR-HSI through sampling from $p(Z|X, Y, \\\\phi)$, i.e., $Z = G(D(Y; \\\\phi_a); \\\\phi_e)$.\\n\\n4.2. Fast Fine-tuning\\n\\nDifferent from nature images, hyperspectral images vary in different scenes dramatically. Thus, fine-tuning in the processing images is necessary [21\u201323]. Considering the circumstance in real scene, the fine-tune stage should be efficient in time and computational consumption. We develop a fast fine-tune strategy to satisfy above requirements. The overview of the proposed EM algorithm in testing stage is presented in Algorithm 1.\\n\\nWith the learned degradation from the training set, it's unnecessary to update degradation adaption in fine-tuning. We only adapt the fusion module to input images. The two submodules of the fusion module play different roles. As a classifier, the abundance estimator $D(\u00b7)$ handles the relative differences in spectrums and is not necessary to be updated in fine-tune stage. We only optimize the spectral mapping block $G(\u00b7)$:\\n\\n$$\\\\min_{\\\\phi} ||X - G(\\\\tilde{A}; \\\\phi_e)||_2,$$\\n\\nParameters analysis can be found in the Supplemental Materials.\"}"}
{"id": "CVPR-2023-2111", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Visual SR results and the corresponding error images on scene of glass tiles in CA VE dataset under the degradation of PSF, where we display the 22th (620nm) band of the HR-HSI images.\\n\\nFigure 4. Visual SR results and the corresponding error images of imgf27 in Harvard dataset under degradation of PSF, where we display the 15th (480nm) band of the HR-HSI images.\\n\\nwhere \\\\( \\\\tilde{A} = D((Y \\\\ast C) \\\\downarrow s; \\\\phi_a) \\\\) and is fixed in fine-tuning stage.\\n\\n5. Experimental Results\\n\\n5.1. Experiment Setup\\n\\nImplementation Details\\nWe implement our method under Pytorch with a single 3090 GPU. We apply the ADAM optimizer in the training stage with parameters \\\\( \\\\beta_1 = 0.9 \\\\), \\\\( \\\\beta_2 = 0.999 \\\\), and \\\\( \\\\eta = 10^{-8} \\\\). The learning rate in the training and fine-tuning stage is initialized to \\\\( 10^{-3} \\\\) and \\\\( 5 \\\\times 10^{-3} \\\\), respectively. We adopt 100 epochs in the training stage and 250 in the testing stage. The detailed network structure varies in the different datasets with different bands of HSIs, and we will list the clear network structure in the Supplementary Materials.\\n\\nCompared Methods\\nFor comprehensive comparison, we select three numerical methods (including CNMF [17], HySure [18], CSTF [26]), state-of-the-art supervised learning approach, (MHFNet [39]), two unsupervised learning approaches (including CUCaNet [22], and UAL [23]). All of the numerical methods require the PSF for a precise recovery. We provide the groundtruth degradation parameters for these methods in synthetic datasets.\\n\\nBenchmarks\\nWe adopt three synthetic HSI benchmarks and one real HSI benchmark for evaluation, including CA VE [41], Harvard [42], Chikusei [43], and Worldview2 [44].\\n\\nThe CA VE dataset contains 32 HSIs imaging from 32 different indoor scenes. Each HSI has 512 \u00d7 512 pixels and 31 spectral bands measured in the wavelength ranging from 400nm to 700nm. The Harvard dataset consists of 50 HSIs imaged outdoors. Each HSI has 1040 \u00d7 1392 pixels and 31 spectral bands measured in the wavelength ranging from 420nm to 720nm. For the experiment, we select the left top of each image with 1024 \u00d7 1376 pixels. Chikusei is an airborne hyperspectral dataset with 2517 \u00d7 2335 pixels and 128 bands in the spectral range from 363nm to 1018nm. We select the right bottom of Chikusei with 2048 \u00d7 2048 pixels and crop it to 16 non-overlapped images with a size of 512 \u00d7 512 for the experiment. Worldview2 is a real-world dataset with a\\n\\nhttps://github.com/danfenghong/ECCV2020\\nCUCaNet\\nhttps://github.com/JiangtaoNie/UAL\\nhttp://www.cs.columbia.edu/CA VE/databases/\\nhttp://vision.seas.harvard.edu/hyperspec/explore.html\\nhttps://naotoyokoya.com/Download.html\\nhttps://www.l3harrisgeospatial.com/Data-Imagery/Satellite-Imagery/High-Resolution/WorldView-2\\n\\n22277\"}"}
{"id": "CVPR-2023-2111", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Visual SR results and the corresponding error images on Chikusei dataset, where we display the test images with bands 70-100-36 as R-G-B to show.\\n\\nTable 1. Average performance of test methods on three synthetic datasets under six random generated degradations.\\n\\n| Datasets | CA VE | Harvard | Chikusei |\\n|----------|-------|---------|----------|\\n| Methods  | PSNR  | SAM     | ERGAS    | UQI      | PSNR  | SAM     | ERGAS    | UQI      | PSNR  | SAM     | ERGAS    | UQI      |\\n| CNMF     | 29.3  | 17.8    | 2.16     | 0.702    | 25.7  | 13.3    | 1.76     | 0.580    | 24.5  | 17.8    | 1.38     | 0.721    |\\n| HySure   | 28.0  | 24.1    | 1.27     | 0.789    | 35.2  | 7.01    | 0.652    | 0.908    | 21.8  | 10.3    | 2.67     | 0.644    |\\n| CSTF     | 29.5  | 14.8    | 1.03     | 0.817    | 34.7  | 7.16    | 0.640    | 0.923    | 25.7  | 11.8    | 1.56     | 0.848    |\\n| MHFNet   | 37.3  | 9.57    | 0.693    | 0.928    | 41.1  | 3.57    | 0.312    | 0.988    | 33.6  | 7.07    | 0.638    | 0.951    |\\n| CUCaNet  | 34.3  | 13.6    | 0.964    | 0.963    | 37.3  | 6.10    | 1.42     | 0.936    | 27.3  | 5.82    | 0.753    | 0.890    |\\n| UAL      | 37.2  | 9.83    | 0.785    | 0.912    | 40.7  | 7.11    | 0.824    | 0.964    | 28.9  | 7.55    | 1.249    | 0.888    |\\n| Ours     | 39.5  | 6.74    | 0.340    | 0.944    | 42.4  | 3.20    | 0.276    | 0.995    | 33.7  | 3.77    | 0.591    | 0.980    |\\n\\nWe adopt four picture quality indices (PQIs) for quantitative evaluation, including peak signal-to-noise ratio (PSNR), spectral angle mapper (SAM), relative dimensionless global error in synthesis (ERGAS), and universal image quality index (UIQI).\\n\\n5.2. Experiments on Synthetic Benchmarks\\n\\nIn CA VE and Harvard, we conduct the testing sets with the last 16 and 25 images, respectively, and the remaining images are set as training samples for compared supervised methods. We randomly select 8 cropped images in Chikusei as testing samples, and the others are set as training samples. We apply six randomly generated Gaussian blur kernels as PSF to synthesize LR-HSIs for each dataset. The generated blur kernels are visualized in Fig. 6. Following the settings of [39], we apply SRF of Landset8 to generate HR-MSIs of Chikusei, Nikon 700 for HR-MSIs of CA VE and Harvard. There are 6 versions for each dataset corresponding to 6 blur kernels. As shown in Table 1, the proposed method outperforms other comparison methods.\\n\\nFig. 3 depicts the visual performance of different methods in CA VE [41] dataset. As can be observed that our method surpasses other methods in detailed content and global contexts. The same conclusion holds for other datasets, as shown in Fig. 4 and Fig. 5.\\n\\n5.3. Real Scenario Experiments\\n\\nSince the HR-HSI in Worldview2 is unavailable, we apply the Wald protocol to build the training set for supervised methods. Both LR-HSI and HR-MSI are downsampled spatially by a factor of 4, and the original LR-HSI is used as ground truth HR-HSI in the training set. As shown in Fig. 7, visual inspection indicates that our method holds a more clear texture than comparison methods.\\n\\n5.4. Algorithmic Analysis\\n\\nExplicit Degradation Estimation\\n\\nFig. 6 depicts the estimated blur kernels by our method. Obviously, our explicit degradation estimation method accurately captures the degradation characteristics.\\n\\nAs for the SRF, DL-based estimation lose the essential features of SRF. Evidently, the superfluous learning capability causes over-fitting. By contrast, our explicit estimation method achieve a more precise estimation which captures the peak in each band, as shown in Fig. 8.\\n\\nComputational Cost Analysis\\n\\nFurther, we report the FLOPs, model size, training time, and testing time of competing methods in Table 2. Evidently, our method surpasses other methods in lightweight and fast processing.\"}"}
{"id": "CVPR-2023-2111", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7. Visual SR results in WV2 dataset. We show the images with bands 5-3-2 as R-G-B.\\n\\nFigure 8. Groundtruth SRF and the estimated SRFs by DL toolkits and our explicit estimation strategy.\\n\\nTable 2. Computational and time consumption on competing methods.\\n\\n| Methods   | FLOPs | Param | Training Time (min) | Testing Time (s) |\\n|-----------|-------|-------|---------------------|-----------------|\\n| CNMF      | /     | /     | 101.2               |                 |\\n| CSTF      | /     | /     | 123.8               |                 |\\n| HySure    | /     | /     | 153.8               |                 |\\n| MHFNet    | 21226 | 129.4 | 1255                | 23.95           |\\n| CUCaNet   | 7226  | 2.28  |                     | 34.2(min)       |\\n| UAL       | 9275  | 27.05 | 351                 | 29.7            |\\n| Ours      | 1.67  | 0.019 | 86.8                | 20.5            |\\n\\nTable 3. Effort of each component in the proposed method.\\n\\n| Methods  | PSNR | SAM  | Train Time (min) | Test Time (s) |\\n|----------|------|------|------------------|---------------|\\n| w/o coord | 32.56 | 7.717 | 86.8             | 20.5          |\\n| w/o DE   | 32.72 | 7.713 | 83.1             | 18.7          |\\n| Conv DE  | 31.42 | 8.320 | 89.5             | 21.0          |\\n| w/o fine-tune | 27.93 | 13.61 | 86.8             | 0.235         |\\n| w/o pre-train | 30.97 | 11.57 | 0                | 20.5          |\\n| w/o fix enc | 35.79 | 7.198 | 86.8             | 28.2          |\\n| Ours     | 39.48 | 6.825 | 86.8             | 20.5          |\\n\\nAblation Study\\n\\nWe analyze three critical components in our method: (1) coordination optimization between prior and likelihood, (2) the explicit degradation estimation, and (3) fast fine-tune strategy. To verify the effort of the coordination optimization, we break the feedback circle between likelihood and prior through replacing the estimated PSF in the fusion module with fixed PSF (isotropic Gaussian kernel with variance $[0.1, 0.1]$), represented as \u2018w/o coord\u2019. The dropped performance in Table 3 confirms the effort of coordination strategy. For the explicit degradation estimation, we conduct two variances that remove degradation estimation and replace explicit estimation with convolution layers, termed \u2018w/o DE\u2019 and \u2018Conv DE\u2019, respectively. As shown in Table 3, their performance is inferior to the original setting, which verifies the effort of degradation estimation and explicit estimation. As for the fast fine-tune strategy, we build three variances in which product fusion results without fine-tuning, adapt the whole network only on tested samples without pre-train, and optimize the entire network in fine-tune stage, named \u2018w/o fine-tune\u2019, \u2018w/o pre-train\u2019, and \u2018w/o fix enc\u2019, respectively. In Table 3, we can confirm the effort of each component in the corresponding experiment. The setting \u2018w/o fix enc\u2019 shows more time consumption and lower metrics resulted from the abundant learnable parameters and over-fitting. All of the variances are tested in CA VE dataset under the degradation of $\\\\text{PSF}_5$.\\n\\nConclusion\\n\\nIn this paper, we developed a coordination optimization HSI-SR framework which established a virtuous cycle between degradation estimation and fusion. Under the framework, we further explored an explicit degradation estimation strategy with stable performance. Meanwhile, we established a tiny autoencoder under spectral mixing prior for interpretable and lightweight fusion. Six different PSFs were generated to verify the efficiency and stability of our method under unknown scenarios with various degradations. Comprehensive experiments show that our framework outperforms SOTA methods by a large margin among accuracy metrics and computational cost. We leave two research directions as our future work, including implementation on real-world devices and extension to more tasks in image enhancement.\\n\\nAcknowledgement.\\n\\nThis research was supported by the China NSFC projects 62121001, U22B2014, and the Young Elite Scientist Sponsorship Program by the China Association for Science and Technology 2020QNRC001.\"}"}
