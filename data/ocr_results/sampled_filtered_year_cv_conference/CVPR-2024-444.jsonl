{"id": "CVPR-2024-444", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Co-salient object detection (CoSOD) aims to identify the common and salient (usually in the foreground) regions across a given group of images. Although achieving significant progress, state-of-the-art CoSODs could be easily affected by some adversarial perturbations, leading to substantial accuracy reduction. The adversarial perturbations can mislead CoSODs but do not change the high-level semantic information (e.g., concept) of the co-salient objects.\\n\\nIn this paper, we propose a novel robustness enhancement framework by first learning the concept of the co-salient objects based on the input group images and then leveraging this concept to purify adversarial perturbations, which are subsequently fed to CoSODs for robustness enhancement. Specifically, we propose COSALPURE containing two modules, i.e., group-image concept learning and concept-guided diffusion purification. For the first module, we adopt a pre-trained text-to-image diffusion model to learn the concept of co-salient objects within group images where the learned concept is robust to adversarial examples. For the second module, we map the adversarial image to the latent space and then perform diffusion generation by embedding the learned concept into the noise prediction function as an extra condition. Our method can effectively alleviate the influence of the SOTA adversarial attack containing different adversarial patterns, including exposure and noise. The extensive results demonstrate that our method could enhance the robustness of CoSODs significantly. The project is available at https://v1len.github.io/CosalPure.\"}"}
{"id": "CVPR-2024-444", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Introduction\\nCo-salient object detection (CoSOD) plays a pivotal role in visual information analysis, aiming to identify and accentuate common and salient objects across a set of images [33]. This study area, crucial for applications like image segmentation and object recognition, has witnessed considerable advancement with the advent of neural network-based methodologies. These methods excel in discerning shared saliency cues among images, offering a significant leap over traditional saliency detections [20]. However, their robustness is severely tested under adverse conditions, such as adversarial attacks and various image common corruption, including but not limited to motion blur [13].\\n\\nThe susceptibility of CoSOD methods to adversarial perturbations, such as those introduced by Jadena [12], poses a significant challenge. These perturbations, while not altering the high-level semantic information of images, can drastically reduce the accuracy of co-salient object detection. The disparity between the corrupted image's saliency map and the ground truth, as a result of these attacks, highlights a critical vulnerability in current CoSOD approaches.\\n\\nCurrently, there are indeed methods aimed at defending against adversarial attacks, such as DiffPure, which employs a noise addition and denoising strategy to eliminate perturbations. However, when restoring the image, DiffPure does not take into account the identity of the object (i.e., without object-specific information). As a result, the restored images produced by DiffPure may contain artifacts that are artificially generated. These artifacts will affect the detection results of the CoSOD method.\\n\\nTo fill this gap, this work introduces a novel robustness enhancement framework, COSALP. The intuitive idea is to first learn a concept from the group images and then use it to guide the data purification based on text-to-image (T2I) diffusion. The 'concept' means the high-level semantic information of co-salient objects in the group images and falls within the text\u2019s latent space. Specifically, this innovative approach comprises two meticulously designed modules: group-image concept learning and concept-guided diffusion purification. The first module focuses on learning the concept of co-salient objects from group images, demonstrating robustness and resilience to adversarial examples. The second module strategically maps adversarial images into a latent space, following which diffusion generation techniques, steered by the previously learned concept, are employed to purify these images effectively.\\n\\nAs shown in the left panel of Fig. 1, when a group of images containing some adversarial examples is passed into co-salient object detectors, the detection results are poor. We first apply the concept learning module to obtain the shared co-salient semantic information (i.e., the learned concept). The bottom right corner of Fig. 1 shows the visualization results of $c$ via a T2I diffusion model. It is evident that the semantic information in the visualized images aligns with the original group images, demonstrating the effectiveness of the concept learning module. Secondly, we utilize the just-proven effective learned concept $c$ to guide the purification. From the right panel of Fig. 1, we can observe that the purified group images via COSALP exhibit satisfactory performance in the CoSOD task.\\n\\nExtensive experimental results substantiate the effectiveness of COSALP. COSALP stands as a robust, concept-driven solution, paving the way for more reliable and accurate co-salient object detection in an era where image manipulation and corruption are increasingly prevalent.\\n\\n2. Related Work\\nCo-salient object detection. Different from single-image saliency detection [1, 6, 18, 20, 27, 28], the goal of co-saliency detection is to detect common salient objects in a group of images [9, 16, 17, 29, 34, 35], evolving from early feature-based approaches to sophisticated deep learning and semantic-driven methods. Deciphering correspondences among co-salient objects across multiple images is pivotal for co-saliency detection. This challenge can be effectively tackled through optimization-based methods [3, 19], machine learning-based models [5, 31], and deep neural networks [29, 34, 35]. GICD [35] employs a gradient-induced mechanism that pays more attention to discriminative convolutional kernels which helps to locate the co-salient regions. GCAGC [34] presents an adaptive graph convolutional network with attention graph clustering for co-saliency detection.\\n\\nAdversarial attack for co-salient object detection. Jadena [12] is an adversarial attack that jointly tunes the exposure and additive perturbations, which can drastically reduce the accuracy of co-salient object detection.\\n\\nText-to-image diffusion generation model. The popularity of Text-to-Image (T2I) generation [30] is propelled by diffusion models [7, 15, 23], necessitating training on extensive text and image paired datasets like LAION-5B [25]. The adeptly trained model demonstrates proficiency in generating diverse and lifelike images based on user-specific input text prompts, realizing T2I generation. T2I personalization [10, 24] is geared towards steering a diffusion-based T2I model to generate innovative concepts.\\n\\nDiffusion-based image purification methods. DiffPure [22] is a notable approach in the field of image processing, specifically designed to enhance the robustness of images against adversarial attacks. It employs a strategy of introducing controlled noise via the forward stochastic differential equation (SDE) [21] and subsequently denoising the image via the reverse SDE to counteract adversarial perturbations. While effective in reducing these perturbations, it is noteworthy that DiffPure does not explicitly consider object semantics during the image restoration process.\"}"}
{"id": "CVPR-2024-444", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Driven Adaptation (DDA) [11] is a test-time adaptation method that improves model accuracy on shifted target data by updating inputs through a diffusion model [15], effectively avoiding domain-wise re-training.\\n\\n3. Preliminaries and Motivation\\n\\n3.1. Co-salient Object Detection (CoSOD)\\n\\nWe have a group of images \\\\( I = \\\\{ I_i \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 3} \\\\}_{i=1}^N \\\\) that contain \\\\( N \\\\) images, and these images have common salient objects. We denote a CoSOD method as \\\\( \\\\text{COSOD}(\\\\cdot) \\\\), taking \\\\( I \\\\) as input and predicting \\\\( N \\\\) salient maps, \\\\( S = \\\\{ S_i \\\\}_{i=1}^N = \\\\text{COSOD}(I) \\\\), where \\\\( S_i \\\\in \\\\mathbb{R}^{H \\\\times W} \\\\) is a binary map (i.e., the saliency map) indicating the salient region of the \\\\( i \\\\)-th image \\\\( I_i \\\\). We show an example of co-saliency detection results in Fig. 1.\\n\\n3.2. Robust Issues of CoSOD\\n\\nHowever, at times, part of the acquired images may be of low quality (i.e., corrupted by some degradation), which will affect the robustness of CoSOD methods. In particular, [12] proposes the joint adversarial noise and exposure attack that can reduce the detection accuracy of state-of-the-art CoSODs significantly. To be specific, within the entire group, there are \\\\( M \\\\) images that have been added adversarial perturbations, denoted as \\\\( \\\\{ I'_{ij} \\\\}_{j=1}^M \\\\), while the remaining are considered as clean images \\\\( \\\\{ I_k \\\\}_{k=1}^{N-M} \\\\). In this scenario, we can reformulate Eq. (1) as \\\\( S' = \\\\{ S'_i \\\\}_{i=1}^N = \\\\text{COSOD}(\\\\{ I'_{ij} \\\\}_{j=1}^M \\\\cup \\\\{ I_k \\\\}_{k=1}^{N-M}) \\\\).\\n\\nThe difference between \\\\( S \\\\) from Eq. (1) and \\\\( S' \\\\) indicates the robustness of the CoSOD method. Previous research findings indicate that existing CoSOD methods are susceptible to the influence of anomalous data (e.g., adversarial noise and exposure) [12]. Note that the degraded images (e.g., \\\\( \\\\{ I'_{ij} \\\\}_{j=1}^M \\\\)) may not only affect the saliency maps of themselves but also impact the saliency maps of clean images. Therefore, to enhance the robustness of CoSOD methods, defense methods should be developed. However, few works are focusing on this direction. A typical defense method is to purify the input images to remove the effects of degradations. In the following, we study the SOTA purification method, i.e., DiffPure [22], to enhance the robustness and show that enhancing the robustness of CoSOD is a non-trivial task and new technologies should be developed.\\n\\n3.3. DiffPure and Challenges\\n\\nA highly intuitive approach is to perform image reconstruction on input images, hoping to remove degradations. As an existing method for image reconstruction, DiffPure [22] can remove adversarial perturbations by applying forward diffusion followed by a reverse generative process. However, DiffPure is limited in its ability to address adversarial additive perturbations. It presents limited capabilities for handling other degradations like adversarial exposure. Fig. 2 illustrates two cases, with the upper case depicting a koala and the lower case representing a train. The input images for both cases are under the attack method [12]. Processed by DiffPure [22], the purified images perform inferior in the CoSOD task together with their respective group images.\\n\\nWe tend to design a more effective purification method. DiffPure is specifically designed against adversarial attacks for image classification and neglects the specific properties of the CoSOD task:\\n\\n1. Only partial images within the group are attacked, and the clean images contain rich complementary information, which could help enhance the robustness.\\n2. Although the adversarial patterns may affect the semantic features of images, the fact group images contain co-salient objects has not changed. How to utilize such a property should be carefully studied.\\n\\n4. Methodology: COSALPURE\\n\\n4.1. Overview\\n\\nBeyond DiffPure [22], we propose to learn the concept of co-salient objects from the group images and leverage it to guide the purification. Specifically, given group images \\\\( I' = \\\\{ I'_{ij} \\\\}_{j=1}^M \\\\cup \\\\{ I_k \\\\}_{k=1}^{N-M} \\\\) that contains \\\\( M \\\\) degraded images and \\\\( N-M \\\\) clean images, we first learn the concept from \\\\( I' \\\\) via the recent developed textual inversion method. The learned concept is a token and lies in the latent space of texts. We name it as 'concept' since we can use it to generate new images containing the 'concept'. Note that the number of the degraded images (i.e., \\\\( M \\\\)) is unknown during application. We denote the concept of learning as \\\\( c = \\\\text{ConceptLearn}(I') \\\\), and we detail the whole process in Sec. 4.2.\"}"}
{"id": "CVPR-2024-444", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"After obtaining the concept, we aim to leverage it for purification by $\\\\hat{I} = \\\\text{ConceptPure}(c, I)$, $I \\\\in I'$, (4)\\n\\nwhere the image $\\\\hat{I}$ is the purified image of $I$ that may be a clean image or a perturbed image. We detail the concept-guided diffusion purification in Sec. 4.3. For each image in $I'$, we can handle it via Eq. (4) and get a novel group denoted as $\\\\hat{I}$. Then, we feed $\\\\hat{I}$ to CoSOD methods to see whether their robustness is enhanced or not.\\n\\nThe core idea is valid based on a critical assumption: the perturbed images in $I'$ do not affect the concept learning. We detail this in the Sec. 4.2.\\n\\n4.2. Group-Image Concept Learning\\n\\nIn this section, we introduce the detail of group-image concept learning (i.e., Eq (3)), which tends to utilize a group of input images for learning the text-aligned embedding of common objects they have, as shown in Fig. 3 (a). We denote this process as $c = \\\\text{ConceptLearn}\\\\{I'_j\\\\}_{M_j=1}^M \\\\cup \\\\{I_k\\\\}_{N-k=1}^N$ where $c$ represents a token aligned with the texts' latent space and represents the semantic information of common objects.\\n\\nTo this end, we formulate group-image concept learning as the personalizing text-to-image problem [10, 24] to enable text-to-image (T2I) diffusion models to rapidly swift new concept acquisition.\\n\\nText-to-Image Diffusion Model.\\n\\nWe introduce the architecture and procedure of a classical T2I diffusion model [23]. It consists of three core modules: (1) image autoencoder, (2) text encoder, (3) and conditional diffusion model. The image autoencoder module has two submodules: an encoder $E$ and a decoder $D$.\\n\\nIt serves a dual purpose, where the encoder maps an input image $X$ to a low-dimensional latent space with $z = E(X)$, while the decoder transforms the latent representation back into the image space with $D(E(X)) \\\\approx X$.\\n\\nThe text encoder $\\\\Gamma$ firstly processes a text $y$ by tokenizing it and secondly translates it into a latent space text embedding $\\\\Gamma(y)$.\\n\\nThe conditional diffusion model $\\\\epsilon_\\\\theta$ takes the time step $t$, the noisy latent $z_t$ at $t$-th time step and the text embedding $\\\\Gamma(y)$ as input to predict the noise added on $z_t$, denoted as $\\\\epsilon_\\\\theta(z_t, t, \\\\Gamma(y))$.\\n\\nGiven a pre-trained T2I diffusion model and group images $I' = \\\\{I'_j\\\\}_{M_j=1}^M \\\\cup \\\\{I_k\\\\}_{N-k=1}^N$ used for CoSOD task, we aim to learn a concept of the common object within $I'$ by $c = \\\\arg\\\\min_c E_{X \\\\in I', z \\\\in E(X), y, \\\\epsilon \\\\in \\\\mathcal{N}(0, 1), t} (\\\\|\\\\epsilon_\\\\theta(z_t, t, \\\\Upsilon(\\\\Gamma(y), c)) - \\\\epsilon\\\\|^2_2)$, (5)\\n\\nwhere $y$ is a fixed text (i.e., 'a photo of $S^*$') and the function $\\\\Upsilon(\\\\Gamma(y), c)$ is to replace the token of '$S^*$' within $\\\\Gamma(y)$ with $c$. Intuitively, Eq. (5) forces the concept $c$ to represent the co-salient objects within group images and also lies in the text latent space corresponding to the text '$S^*$'. After obtaining $c$, we can embed '$S^*$' into other texts to generate new images via the T2I diffusion model. For example, in Fig. 4 (b), we learn a concept of the co-salient objects (i.e., piano), which corresponds to the text '$S^*$'. Then, we feed a text (e.g., 'a photo of $S^*$') to the T2I model that generates an image containing the object, which means that the learned concept represents the salient objects in $I'$ very well.\"}"}
{"id": "CVPR-2024-444", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Robustness of concept learning.\\n\\nIt is obvious that the above method naturally aligns with our objective since we can exploit it to obtain the common semantic content among the group images for the CoSOD task. The key problem is whether the concept learning would be affected by degradations like adversarial perturbation in the group images. We conduct an empirical study to validate this. Specifically, given a group of clean images (i.e., $I$), we use it to learn a concept via Eq. (5). Meanwhile, we conduct the adversarial CoSOD attack [12] on two images within $I$ and form a new group $I'$. With $I'$, we learn another concept via Eq. (5). Then, we can leverage the two learned concepts to generate images based on the same text prompt. As shown in Fig. 4, we find that the two generated images based on two concepts are similar, demonstrating that the adversarial examples have limited influence on concept learning. This inspires us to leverage the learned concept to purify the adversarial examples.\\n\\n4.3. Concept-guided Diffusion Purification\\n\\nWe propose reconstructing the group images based on the learned concept $c$ to eliminate the potential adversarial patterns as shown in Fig. 3 (b). Considering the advantage of the continuous representation [4, 14] in the smooth image reconstruction and its ability to remove perturbations, we employ a continuous representation module for the initial processing of the input image $X$, denoted as $\\\\tilde{X} = CR(X)$. This module not only somewhat denoises $X$ but also addresses the issue of pre-trained CoSOD models designed for specific resolutions that do not match the input resolution of our employed diffusion model. Subsequently, the encoder of the image autoencoder module maps $\\\\tilde{X}$ to the latent space $z_0 = E(\\\\tilde{X})$. The following procedure is based on the diffusion pipeline and needs two procedures: forward process and reverse process. The forward diffusion process is a fixed Markov chain that iteratively adds a Gaussian noise to the latent $z_0$ over $T$ timesteps, obtaining a sequence of noised images $z_1, z_2, \\\\cdots, z_T$. In each step of the forward progress, the latent at time step $t \\\\in [1, T]$ is updated by $z_t = a_t z_{t-1} + b_t \\\\epsilon_t$, $\\\\epsilon_t \\\\sim N(0, I)$, (6) where $a_t$ and $b_t$ are coefficients and $N(0, I)$ represents the standard Gaussian distribution. By superimposing time steps from $t = 1$ to $T$, Eq. (6) can be simplified to $z_t = \\\\sqrt{\\\\alpha_t} z_0 + \\\\sqrt{1-\\\\alpha_t} \\\\epsilon_t$, $\\\\epsilon_t \\\\sim N(0, I)$, (7) where we have $a^2_t + b^2_t = 1$, and $\\\\alpha_t = a^2_t$, $\\\\alpha_{t-1} = Q_t \\\\tau = 1 \\\\alpha_{\\\\tau}$.\\n\\nAs we set the time step as $T$, the complete forward process can be expressed as $z_T \\\\sim q(z_1: T | z_0) = T \\\\prod_{t=1} z_t | z_{t-1}$. (8)\\n\\nFor the reverse process, it iteratively removes the noise to generate an image in $T$ timesteps. Unlike doing the reverse process directly, our method incorporates the obtained semantic embedding $c$ as additional object information into the pipeline. Then, we can start from $\\\\hat{z}_T$ (alternatively called $\\\\hat{z}_{T-1}$) and progressively predict $\\\\epsilon_{t-1} = \\\\epsilon_\\\\theta(\\\\hat{z}_t, t, c)$, (9) and obtain the latent at time step $t-1$ via $\\\\hat{z}_{t-1} = \\\\sqrt{\\\\bar{\\\\alpha}_{t-1}} (1-\\\\alpha_t) \\\\tilde{z}_0 + \\\\sqrt{\\\\alpha_t} (1-\\\\bar{\\\\alpha}_{t-1}) \\\\hat{z}_t + \\\\sigma_t \\\\xi$, (10) with $\\\\tilde{z}_0 = \\\\hat{z}_t - \\\\sqrt{1-\\\\bar{\\\\alpha}_{t-1}} \\\\epsilon_t - \\\\sqrt{\\\\bar{\\\\alpha}_t} \\\\epsilon_t$, (11) where $\\\\sigma^2_t = (1-\\\\alpha_t)(1-\\\\alpha_{t-1})(1-\\\\alpha_t)$ and $\\\\xi \\\\sim N(0, I)$ according to the sample process of DDPM [15]. We can directly obtain the reconstructed image $\\\\hat{x}$ by using the decoder with formula $\\\\hat{x} = D(\\\\hat{z}_0)$.\\n\\nTo confirm that the concept learned by CO-SAL-PURE is applied accurately in the image reconstruction, we employ DAAM [26] to establish attention maps for the learned concepts on processed images as shown in Fig. 5. In each case, the attention map of the semantic embedding $c$ (i.e., the learned concept) aligns well with the object itself in the image, indicating the effectiveness of CO-SAL-PURE.\\n\\n5. Experiment\\n\\n5.1. Experimental Setup\\n\\nDatasets. We conduct experiments on Cosal2015 [32], iCoseg [2], CoSOD3k [8], and CoCA [36]. These four datasets contain 2,015, 643, 3,316, and 1,295 images of 50, 38, 160, and 80 groups respectively. We apply the SOTA adversarial attack for CoSOD (i.e., Jadena [12]) to the first 50% of images in each group, while the remaining 50% of\"}"}
{"id": "CVPR-2024-444", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Co-saliency detection performance. \\\"Source-Only\\\" means the group of images before processing, including 50% adversarial images and 50% clean images. We highlight the top results of each CoSOD method and each dataset in red.\\n\\n| Method      | Source-Only | DiffPure | DDA          | COSALPURE |\\n|-------------|-------------|----------|--------------|-----------|\\n| GICD        | 0.3493      | 0.4595   | 0.4565       | 0.5602    |\\n| GCAGC       | 0.7306      | 0.7478   | 0.7579       | 0.7898    |\\n| PoolNet     | 0.4038      | 0.5118   | 0.5158       | 0.6177    |\\n| SR\u2191         | 0.1676      | 0.1444   | 0.1469       | 0.1296    |\\n| AP\u2191         | 0.5285      | 0.4987   | 0.5955       | 0.5975    |\\n| F\u03b2\u2191         | 0.7853      | 0.6998   | 0.7928       | 0.7449    |\\n| MAE\u2193        | 0.6302      | 0.5901   | 0.6774       | 0.6521    |\\n\\nTable 2. Co-saliency detection success rates (SR) of entire group of images, only adversarial images and only clean images. This table is to intuitively illustrate the impact of different methods on the adversarial and clean portions of group images.\\n\\n| Method      | Source-Only | DiffPure | DDA          | COSALPURE |\\n|-------------|-------------|----------|--------------|-----------|\\n| GICD        | 0.3493      | 0.4595   | 0.4565       | 0.5602    |\\n| GCAGC       | 0.1053      | 0.3560   | 0.3079       | 0.5416    |\\n| PoolNet     | 0.5884      | 0.5609   | 0.6021       | 0.5785    |\\n| avg\u2191        | 0.5285      | 0.4533   | 0.4924       | 0.5977    |\\n| adv\u2191        | 0.3671      | 0.5636   | 0.5185       | 0.5972    |\\n| clean\u2191      | 0.7642      | 0.7003   | 0.7259       | 0.7239    |\\n\\nWe select the \\\"augment\\\" version of Jadena and follow the settings[12].\\n\\nEvaluation settings. We choose GICD [35] and GCAGC [34] to evaluate our method as they are commonly used state-of-the-art CoSOD methods. Additionally, we take PoolNet [20] into consideration, assessing the performance in salient object detection.\\n\\nBaseline methods. Indeed, there is currently no specific image processing method designed for CoSOD attacks. Hence, we employ two alternative approaches as baselines. DiffPure [22] is a method that utilizes a diffusion model for purifying perturbation-based adversarial images. Diffusion-Driven Adaptation (DDA) [11] builds upon a diffusion-based model by introducing a novel self-ensembling scheme, enhancing the adaptation process by dynamically determining the degree of adaptation. DiffPure and DDA employ the same sampling noise scale as our proposed COSALPURE.\\n\\nMetrics. We employ four metrics to evaluate the co-salient object detection result, including detection success rate (SR), average precision (AP) [33], F-measure score $F_\\\\beta$ with $\\\\beta^2 = 0.3$ [1] and mean absolute error (MAE) [33]. For the detection success rate, we calculate the intersection over union (IOU) between each co-salient object detection result of the reconstructed image and the corresponding ground-truth map. We divide the number of successful results (IOU > 0.5) by the total number of results to calculate SR. In addition, to intuitively illustrate the impact of different methods on CoSOD results, we not only compute SR for the entire group of images but also separately calculate SR for only adversarial images and only clean images.\\n\\nImplementation details. In the group-image concept learning process...\"}"}
{"id": "CVPR-2024-444", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Visualization of co-salient object detection results. We show four visualized cases in this figure, with the source-only/purified image, the ground-truth of the co-saliency map, and the results of GICD, GCAGC, and PoolNet in the columns. Our method, COSALPURE, is highlighted in green.\\n\\nFor the continuous representation [4, 14] module employed in the concept-guided diffusion purification procedure, we constructed a dataset to train it. We select 50,000 samples with a resolution of 224\u00d7224 from ImageNet (1,000 categories, each with 50 samples) and apply noise with the intensity of 16/255 via the PGD attack to these samples to construct the inputs of the continuous representation module. For the ground truth images, we apply clean images with a resolution of 768\u00d7768 corresponding to the input images. We follow the experimental setup of [14] and trained for 10 epochs to obtain the required module. The group-image concept learning procedure and the concept-guided purification procedure utilize the same pre-trained image encoder and conditional diffusion model.\\n\\nFor the concept-guided purification procedure, we set the number of timesteps \\\\(T\\\\) to 250, and the same configuration is applied to baseline methods.\\n\\n5.2. Comparison on Adversarial Attacks\\n\\nWe denote the images before reconstruction (containing 50% adversarial images and 50% clean images) as \u201cSource-Only\u201d. The comparison between our proposed COSALPURE and baselines are shown in Table 1. We consider an image to be successfully detected in the co-salient object detection (CoSOD) task if the IOU of its CoSOD result and the ground-truth map exceed 0.5. Compared to DiffPure [22] and DDA [11], COSALPURE outperforms them in terms of co-salient object detection success rates (SR) across all four datasets. For the other three metrics (i.e., AP [33], \\\\(F_\\\\beta\\\\) [1] and MAE [33]), COSALPURE remains the best at most of the time. We show four visualized cases in Fig. 6. For each case, we present the generated images of COSALPURE and two baselines, DiffPure and DDA. Obviously COSALPURE generates higher-quality images, as it leverages the intrinsic commonality of objects across the group of images. Additionally, we showcase the comparison of the detection results on GICD, GCAGC, and PoolNet. The results from COSALPURE closely approximate the ground-truth map, while the baseline methods struggle to display the correct results.\\n\\nTo intuitively illustrate the impact of different methods on the adversarial and clean portions of group images, we measure the co-salient object detection success rate from three perspectives. In Table 2, \u201cavg\u201d represents the evaluation across the entire group of images, \u201cadv\u201d and \u201cclean\u201d correspond to evaluations on only the 50% images that are under the SOTA attack [12] and on only the 50% images that remain clean. COSALPURE at some times have a lower \u201cclean\u201d SR compared to DDA or source-only. However, DiffPure and DDA are unsatisfactory in \u201cadv\u201d SR, while COSALPURE exhibits a significant lead in \u201cadv\u201d SR, resulting in it consistently performing the best in \u201cavg\u201d SR.\\n\\n5.3. Ablation Study\\n\\nTo validate the effect of the learned concepts on CoSOD results, we conduct ablation studies on Cosal2015 [32] and CoSOD3k [8]. In Table 3, \u201cw/o concept inversion\u201d represents only utilizing the continuous representation module and not applying the subsequent purification process. \u201cw/ None concept\u201d denotes passing a meaningless \u201cNone\u201d as the concept during the purification. \u201cw/ learned concept\u201d denotes the complete pipeline, firstly learning the concept...\"}"}
{"id": "CVPR-2024-444", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Ablation study. \\\"w/o concept inversion\\\" represents only utilize the continuous representation module and not apply the subsequent purification process. \\\"w/ None concept\\\" denotes passing a meaningless \\\"None\\\" as the concept during the purification procedure. \\\"w/ learned concept\\\" denotes the complete pipeline, firstly learning the concept from the entire group of images and secondly passing in the learned concept during the purification procedure.\\n\\n|                  | GICD | GCAGC | PoolNet |\\n|------------------|------|-------|---------|\\n| Source-Only      |      |       |         |\\n| w/o concept inversion | 0.3493 | 0.7306 | 0.4038 |\\n| DiffPure         | 0.3146 | 0.6774 | 0.3763 |\\n| DDA              | 0.3900 | 0.7381 | 0.4425 |\\n| CoSAL Pure       |       |       |         |\\n| w/o concept inversion | 0.5186 | 0.7791 | 0.5809 |\\n| w/ None concept  | 0.5225 | 0.7784 | 0.5886 |\\n| w/ learned concept | 0.5602 | 0.7898 | 0.6177 |\\n\\nFigure 7. Visualization for ablation study.\\n\\nTable 4. Extension to motion blur.\\n\\n|                  | GICD | GCAGC | PoolNet |\\n|------------------|------|-------|---------|\\n| Source-Only      |      |       |         |\\n| DiffPure         | 0.3915 | 0.7408 | 0.4373 |\\n| DDA              | 0.3146 | 0.6774 | 0.3763 |\\n| CoSAL Pure       | 0.4575 | 0.7419 | 0.5241 |\\n\\n5.4. Extension to Common Corruption\\n\\nIn addition to adversarial attacks on CoSOD, we also broaden our experiments to include a common corruption type: motion blur. We select the Cosal2015 dataset and, similar to the adversarial experiments, apply motion blur to the first 50% of images in each group while keeping the remaining 50% of images clean. Here we set the number of timesteps $T$ to 500 and do not employ the continuous representation module. As shown in Table 4, CoSAL Pure performs better than other diffusion-based image processing methods at all four metrics.\\n\\n6. Conclusions\\n\\nThis paper presented CoSAL Pure, an innovative framework enhancing the robustness of co-salient object detection (CoSOD) against adversarial attacks and common image corruptions. Central to our approach are two key innovations: group-image concept learning and concept-guided diffusion purification. Our framework effectively captures and utilizes the high-level semantic concept of co-salient objects from group images, demonstrating notable resilience even in the presence of adversarial examples. Empirical evaluations across datasets like Cosal2015, iCoseg, CoSOD3k, and CoCA showed that CoSAL Pure significantly outperforms existing methods such as DiffPure and DDA in CoSOD tasks. Not only did it achieve higher success rates, but it also excelled in performance metrics like AP, F-measure, and MAE. Additionally, its effectiveness against common image corruptions, like motion blur, underscores its versatility.\\n\\nOur CoSAL Pure represents a substantial advancement in CoSOD, offering robust, concept-driven image purification. It opens avenues for more resilient co-salient object detection, vital in today's landscape of sophisticated image manipulation and corruption. Future work might extend this framework to broader image analysis applications and explore its adaptability to real-world scenarios.\\n\\n7. Acknowledgments\\n\\nGeguang Pu is supported by National Key Research and Development Program (2020AAA0107800), and Shanghai Collaborative Innovation Center of Trusted Industry Internet Software. This work is also supported by the National Research Foundation, Singapore, and DSO National Laboratories under the AI Singapore Programme (AISG Award No: AISG2-GC-2023-008), and Career Development Fund (CDF) of the Agency for Science, Technology and Research (A*STAR) (No.: C233312028).\"}"}
{"id": "CVPR-2024-444", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Radhakrishna Achanta, Sheila Hemami, Francisco Estrada, and Sabine Susstrunk. Frequency-tuned salient region detection. In 2009 IEEE conference on computer vision and pattern recognition, pages 1597\u20131604. IEEE, 2009.\\n\\n[2] Dhruv Batra, Adarsh Kowdle, Devi Parikh, Jiebo Luo, and Tsuhan Chen. icoseg: Interactive co-segmentation with intelligent scribble guidance. In 2010 IEEE computer society conference on computer vision and pattern recognition, pages 3169\u20133176. IEEE, 2010.\\n\\n[3] Xiaochun Cao, Zhiqiang Tao, Bao Zhang, Huazhu Fu, and Wei Feng. Self-adaptively weighted co-saliency detection via rank constraint. IEEE Transactions on Image Processing, 23(9):4175\u20134186, 2014.\\n\\n[4] Yinbo Chen, Sifei Liu, and Xiaolong Wang. Learning continuous image representation with local implicit image function. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 8628\u20138638, 2021.\\n\\n[5] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, and Shi-Min Hu. Salientshape: group saliency in image collections. The visual computer, 30:443\u2013453, 2014.\\n\\n[6] Ming-Ming Cheng, Niloy J Mitra, Xiaolei Huang, Philip HS Torr, and Shi-Min Hu. Global contrast based salient region detection. IEEE transactions on pattern analysis and machine intelligence, 37(3):569\u2013582, 2014.\\n\\n[7] Florinel-Alin Croitoru, Vlad Hondru, Radu Tudor Ionescu, and Mubarak Shah. Diffusion models in vision: A survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.\\n\\n[8] Deng-Ping Fan, Zheng Lin, Ge-Peng Ji, Dingwen Zhang, Huazhu Fu, and Ming-Ming Cheng. Taking a deeper look at co-salient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2919\u20132929, 2020.\\n\\n[9] Deng-Ping Fan, Tengpeng Li, Zheng Lin, Ge-Peng Ji, Dingwen Zhang, Ming-Ming Cheng, Huazhu Fu, and Jianbing Shen. Re-thinking co-salient object detection. IEEE transactions on pattern analysis and machine intelligence, 44(8):4339\u20134354, 2021.\\n\\n[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618, 2022.\\n\\n[11] Jin Gao, Jialing Zhang, Xihui Liu, Trevor Darrell, Evan Shelhamer, and Dequan Wang. Back to the source: Diffusion-driven test-time adaptation. arXiv preprint arXiv:2207.03442, 2022.\\n\\n[12] Ruijun Gao, Qing Guo, Felix Juefei-Xu, Hongkai Yu, Huazhu Fu, Wei Feng, Yang Liu, and Song Wang. Can you spot the chameleon? adversarially camouflaging images from co-salient object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2150\u20132159, 2022.\\n\\n[13] Dan Hendrycks and Thomas Dietterich. Benchmarking neural network robustness to common corruptions and perturbations. Proceedings of the International Conference on Learning Representations, 2019.\\n\\n[14] Chih-Hui Ho and Nuno Vasconcelos. Disco: Adversarial defense with local implicit functions. Advances in Neural Information Processing Systems, 35:23818\u201323837, 2022.\\n\\n[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.\\n\\n[16] Bo Jiang, Xingyue Jiang, Ajian Zhou, Jin Tang, and Bin Luo. A unified multiple graph learning and convolutional network model for co-saliency estimation. In Proceedings of the 27th ACM international conference on multimedia, pages 1375\u20131382, 2019.\\n\\n[17] Bo Li, Zhengxing Sun, Lv Tang, Yunhan Sun, and Jinlong Shi. Detecting robust co-saliency with recurrent co-attention neural network. In IJCAI, page 6, 2019.\\n\\n[18] Guanbin Li and Yizhou Yu. Deep contrast learning for salient object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 478\u2013487, 2016.\\n\\n[19] Hongliang Li, Fanman Meng, Bing Luo, and Shuyuan Zhu. Repairing bad co-segmentation using its quality evaluation and segment propagation. IEEE Transactions on Image Processing, 23(8):3545\u20133559, 2014.\\n\\n[20] Jiang-Jiang Liu, Qibin Hou, Ming-Ming Cheng, Jiashi Feng, and Jianmin Jiang. A simple pooling-based design for real-time salient object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3917\u20133926, 2019.\\n\\n[21] Chenlin Meng, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021.\\n\\n[22] Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Anima Anandkumar. Diffusion models for adversarial purification. arXiv preprint arXiv:2205.07460, 2022.\\n\\n[23] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10684\u201310695, 2022.\\n\\n[24] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500\u201322510, 2023.\\n\\n[25] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.\\n\\n[26] Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, and Ferhan Ture. What the daam: Interpreting stable diffusion models with information theory. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22500\u201322510, 2023.\"}"}
{"id": "CVPR-2024-444", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"sion using cross attention.\\n\\n[27] Linzhao Wang, Lijun Wang, Huchuan Lu, Pingping Zhang, and Xiang Ruan. Saliency detection with recurrent fully convolutional networks. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pages 825\u2013841. Springer, 2016.\\n\\n[28] Wenguan Wang, Qiuxia Lai, Huazhu Fu, Jianbing Shen, Haibin Ling, and Ruigang Yang. Salient object detection in the deep learning era: An in-depth survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(6):3239\u20133259, 2021.\\n\\n[29] Zheng-Jun Zha, Chong Wang, Dong Liu, Hongtao Xie, and Yongdong Zhang. Robust deep co-saliency detection with group semantic and pyramid attention. IEEE transactions on neural networks and learning systems, 31(7):2398\u20132408, 2020.\\n\\n[30] Chenshuang Zhang, Chaoning Zhang, Mengchun Zhang, and In So Kweon. Text-to-image diffusion model in generative ai: A survey. arXiv preprint arXiv:2303.07909, 2023.\\n\\n[31] Dingwen Zhang, Deyu Meng, Chao Li, Lu Jiang, Qian Zhao, and Junwei Han. A self-paced multiple-instance learning framework for co-saliency detection. In Proceedings of the IEEE international conference on computer vision, pages 594\u2013602, 2015.\\n\\n[32] Dingwen Zhang, Junwei Han, Chao Li, Jingdong Wang, and Xuelong Li. Detection of co-salient objects by looking deep and wide. International Journal of Computer Vision, 120:215\u2013232, 2016.\\n\\n[33] Dingwen Zhang, Huazhu Fu, Junwei Han, Ali Borji, and Xuelong Li. A review of co-saliency detection algorithms: Fundamentals, applications, and challenges. ACM Transactions on Intelligent Systems and Technology (TIST), 9(4):1\u201331, 2018.\\n\\n[34] Kaihua Zhang, Tengpeng Li, Shiwen Shen, Bo Liu, Jin Chen, and Qingshan Liu. Adaptive graph convolutional network with attention graph clustering for co-saliency detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9050\u20139059, 2020.\\n\\n[35] Zhao Zhang, Wenda Jin, Jun Xu, and Ming-Ming Cheng. Gradient-induced co-saliency detection. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XII 16, pages 455\u2013472. Springer, 2020.\\n\\n[36] Jia-Xing Zhao, Jiang-Jiang Liu, Deng-Ping Fan, Yang Cao, Jufeng Yang, and Ming-Ming Cheng. Egnet: Edge guidance network for salient object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 8779\u20138788, 2019.\"}"}
