{"id": "CVPR-2024-1588", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Move as You Say, Interact as You Can: Language-guided Human Motion Generation with Scene Affordance\\n\\nZan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, Siyuan Huang\\n\\nindicates corresponding authors\\n\\n1 School of Computer Science & Technology, Beijing Institute of Technology\\n2 National Key Laboratory of General Artificial Intelligence, BIGAI\\n3 Dept. of Automation, Tsinghua University\\n4 CFCS, School of Computer Science, Peking University\\n5 Institute for AI, Peking University\\n6 Yangtze Delta Region Academy of Beijing Institute of Technology, Jiaxing\\n\\nhttps://afford-motion.github.io\\n\\nFigure 1. Language-guided human motion generation in 3D scenes via scene affordance. Employing scene affordance as an intermediate representation enhances motion generation capabilities on benchmarks (a) HumanML3D and (b) HUMANISE, and significantly boosts the model's ability to generalize to (c) unseen scenarios.\\n\\nAbstract\\n\\nDespite significant advancements in text-to-motion synthesis, generating language-guided human motion within 3D environments poses substantial challenges. These challenges stem primarily from (i) the absence of powerful generative models capable of jointly modeling natural language, 3D scenes, and human motion, and (ii) the generative models' intensive data requirements contrasted with the scarcity of comprehensive, high-quality, language-scene-motion datasets.\\n\\nTo tackle these issues, we introduce a novel two-stage framework that employs scene affordance as an intermediate representation, effectively linking 3D scene grounding and conditional motion generation. Our framework comprises an Affordance Diffusion Model (ADM) for predicting explicit affordance map and an Affordance-to-Motion Diffusion Model (AMDM) for generating plausible human motions. By leveraging scene affordance maps, our method overcomes the difficulty in generating human motion under multimodal condition signals, especially when training with limited data lacking extensive language-scene-motion pairs. Our extensive experiments demonstrate that our approach consistently outperforms all baselines on established benchmarks, including HumanML3D and HUMANISE. Additionally, we validate our model's exceptional generalization capabilities on a specially curated evaluation set featuring previously unseen descriptions and scenes.\"}"}
{"id": "CVPR-2024-1588", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"1. Introduction\\n\\nPrior efforts in the field have investigated the integration of diverse modalities, such as textual descriptions [3, 5, 6, 14, 24, 29, 67, 75, 76, 92, 93], audio signals [49, 51, 89], and 3D scenes [4, 32, 37, 80\u201382] for guiding human motion generation. The significant strides in single-modality conditioned motion generation have been complemented by the introduction of Human-Scene Interaction (HSI) through language descriptions by Wang et al. [84], highlighting the demand for controllable motion generation in diverse applications such as animation synthesis [35], film production [78], and synthetic data generation [97, 100]. However, the task of effectively generating semantically driven and scene-aware motions remains daunting due to two principal challenges.\\n\\nThe first challenge entails ensuring that generated motions are descriptive-faithful, physically plausible within the scene, and accurately grounded in specific locations. Though direct application of conditional generative models like conditional Variational Autoencoder (cVAE) [66, 80, 84] and conditional diffusion models [14, 37, 76, 93] has been attempted, the inherent complexity of marrying 3D scene grounding with conditional motion generation presents a significant obstacle. This complexity impedes the model\u2019s ability to generalize across various scenes and descriptions, making it challenging to adapt specific motions (e.g., \u201clie down on the bed\u201d) to analogous actions in new contexts (e.g., \u201clie down on the floor\u201d) within unfamiliar 3D environments.\\n\\nThe second challenge arises from the generative models\u2019 dependency on large volumes of high-quality paired data. Existing HSI datasets [4, 9, 31] lack in both motion quality and diversity, featuring a limited number of scene layouts and, most critically, devoid of HSI descriptions. Although the HUMANISE dataset [84] attempts to address this gap, it is constrained by a narrow scope of action types and the use of fixed-form utterances, limiting the generation of diverse HSIs from varied and free-form language descriptions.\\n\\nIn response to these challenges, we propose to utilize the scene affordance maps as an intermediate representation, as depicted in Fig. 1. This representation is calculated from the distance field between human skeleton joints and the scene\u2019s surface points. The use of the affordance map presents two primary benefits for the generation of language-guided motion in 3D environments. First, it precisely delineates the region grounded in the language description, thereby significantly enhancing the 3D scene grounding essential for motion generation, even in scenarios characterized by limited training data availability. Second, the affordance map, rooted in distance measurements, provides a sophisticated understanding of the geometric interplay between scenes and human motions. This understanding aids in the generation of HSIs and facilitates the model\u2019s ability to generalize across unique scene geometries.\\n\\nExpanding upon this intermediate representation, we propose a novel two-stage model aimed at seamlessly integrating the 3D scene grounding with the language-guided motion generation. The first stage involves the development of an Affordance Diffusion Model (ADM), which employs the Perceiver architecture [38, 39] to predict an affordance map given a specific 3D scene and description. The second stage introduces an Affordance-to-Motion Diffusion Model (AMDM), comprising an affordance encoder and a Transformer backbone, to synthesize human motions by considering both the language descriptions and the affordance maps derived in the first stage.\\n\\nWe conduct extensive evaluations on established benchmarks, including HumanML3D [29] and HUMANISE [84], demonstrating superior performance in text-to-motion generation tasks and highlighting our model\u2019s advanced generalization capabilities on a specially curated evaluation set featuring unseen language descriptions and 3D scenes. These results underscore the utility of our approach in harnessing scene affordances for enriched 3D scene grounding and enhanced conditional motion generation.\\n\\nOur contributions are summarized as follows:\\n\\n\u2022 We introduce a novel two-stage model that incorporates scene affordance as an intermediate representation, bridging the gap between 3D scene grounding and conditional motion generation, and facilitating language-guided human motion synthesis in 3D environments.\\n\\n\u2022 Through extensive quantitative and qualitative evaluations, we demonstrate our method\u2019s superiority over existing motion generation models across the HumanML3D and HUMANISE benchmarks.\\n\\n\u2022 Our model showcases remarkable generalization capabilities, achieving impressive performance in generating human motions for novel language-scene pairs, despite the limited availability of language-scene-motion datasets.\\n\\n2. Related Work\\n\\n2.1. Language, Human Motion, and 3D Scene\\n\\nWe seek to bridge the modalities of language, human motion, and 3D scenes, an area where prior research has often focused on combining just two of these elements. In the realm of 3D Vision-Language (3D-VL), tasks such as 3D object grounding [1, 16, 41, 77, 98, 105], reasoning [7, 21, 57, 88], and captioning [12, 13, 18, 36, 91] have intersected language with 3D scenes. Recent advancements in this area have focused on enhancing open-vocabulary scene understanding by integrating features from foundational models like CLIP [68] into 3D scene analysis [40, 45, 63, 74]. The interaction between language and human motion has been explored through efforts to guide motion generation with semantic cues, including text-to-motion [3, 6, 24, 29, 67] and action-to-motion synthesis [28, 66].\\n\\nExisting HSI works focus on populating static human...\"}"}
{"id": "CVPR-2024-1588", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"figures into 3D scenes [15, 17, 33, 97] and generating temporal human motions within these contextual environments [37, 43, 80\u201382]. A growing body of research [19, 56, 64, 65, 96, 101] has aimed at creating policies for continuous motion synthesis in virtual spaces, treating the challenge as a Reinforcement Learning (RL) task. The pioneering works of Zhao et al. [100] and Wang et al. [84] ventured into the simultaneous modeling of language, 3D scenes, and human motion, integrating semantics (e.g., action labels and descriptive language) into the generation of HSI, requiring interactions to be both physically plausible and semantically consistent. Following this, Xiao et al. [86] leveraged a Large Language Model (LLM) to convert language prompts into sub-task plans, represented as Chain of Contacts (CoC), to facilitate motion planning within 3D scenes.\\n\\nIn our contribution, we present a novel two-stage framework that employs scene affordance [26] as an intermediary to effectively bridge 3D scene grounding with conditioned motion generation. This approach not only enhances multimodal alignment but also improves the generative model's ability to generalize across scenarios, even when trained on the limited paired data available in current datasets [29, 31, 84].\\n\\n2.2. Conditional Human Motion Generation\\n\\nThe past few years have marked significant advancements in the domain of human motion modeling conditioned on diverse signals [102], including past motion [8, 9, 50, 59, 87, 90], audio [49, 51, 89], action labels [28, 66], natural language descriptions [3, 5, 6, 14, 24, 29, 67, 75, 76, 92, 93], objects [11, 25, 52, 73, 95], and 3D scenes [4, 32, 37, 44, 80\u201382]. These approaches, predominantly designed for single-modal conditioning, encounter difficulties in scenarios necessitating the simultaneous consideration of both scene and language cues. For example, methods that seek to align the conditional signal's latent space with that of human motions [2, 6, 67, 75] struggle in this intricate context due to the distinct and complementary nature of 3D scenes and language descriptions in motion generation. The former provides spatial boundaries while the latter offers semantic direction, rendering direct alignment approaches less effective.\\n\\nMoreover, attempts at directly learning the conditional distribution with models such as cVAE [66, 80, 84] and diffusion models [14, 76, 93] often lead to suboptimal outcomes. This is attributed to the complex entanglement of the joint distribution across the three modalities, which complicates the development of an efficient multimodal embedding space, particularly when data is scarce. In response, our research proposes the utilization of scene affordance as an intermediate representation. This strategy aims to simplify the process of generating motion under multiple conditions, thereby enhancing the model's capacity to interpret and generate multimodal HSI more effectively.\\n\\n2.3. Scene Affordance\\n\\nThe concept of \u201caffordance,\u201d initially introduced by Gibson [26], describes the potential actions that the environment offers for interaction. Early investigations into affordances primarily focused on understanding scenes and object affordances through 2D observations [23, 27, 30, 47, 48, 54, 61, 83, 103, 104]. Transitioning to 3D, initial HSI research implicitly incorporated affordances in scene understanding [80\u201382, 84], with more recent work exploring explicit 3D visual affordances [22, 60, 104]. These advancements often represent affordances as contact maps for grasping [42, 46, 53, 55, 85] and scene-conditioned motion synthesis [33, 80, 81, 94, 97]. In our approach, we redefine the affordance map as a generalized distance field between human skeleton joints and surface points of 3D scenes. This model first refines the affordance map using the provided 3D scene and language description. It then utilizes the refined affordance map's grounding and geometric information to improve the subsequent conditional motion generation.\\n\\n3. Preliminaries\\n\\nDiffusion Model\\n\\nDiffusion models [34, 70, 71] are a class of generative models that operate through an iterative denoising process to learn and sample data distributions. They include a forward process and a reverse process. The forward process starts with the real data $X_0$ at step 0, iteratively adds Gaussian noise $\\\\epsilon_t$, and converts $X_0$ to $X_t$ over $t$ steps in a Markovian manner. The one-step forward process can be described as\\n\\n$$q_{p_{X_t|X_{t-1}}} \\\\sim \\\\mathcal{N}(p_{X_t|X_{t-1}}; \\\\beta_t I_{q_{t}}, \\\\beta_t)$$\\n\\nwhere $t\\\\beta_t P_{p_{0,1}}$, $1$ is the pre-defined variance schedule. When $t \\\\to \\\\infty$, $X_t$ is equivalent to an isotropic Gaussian distribution. The entire forward process is given by\\n\\n$$q_{p_{X_1:T|X_0}} \\\\sim \\\\mathcal{N}(p_{X_0}; 0, I_{q_T})$$\\n\\nwhere $T$ is the total number of diffusion steps.\\n\\nIn the reverse process, the diffusion model learns to gradually remove noise for sampling from the Gaussian distribution $X_T$:\\n\\n$$p_{\\\\theta_{X_0:T}} \\\\sim \\\\mathcal{N}(p_{X_T}; \\\\mu_\\\\theta_{p_{X_T}}, \\\\Sigma_\\\\theta_{p_{X_T}}, \\\\mu_{p_{X_T}}, \\\\Sigma_{p_{X_T}})$$\\n\\nwhere $X_T \\\\sim \\\\mathcal{N}(0, I_{q_{T}})$, $\\\\mu_\\\\theta$ and $\\\\Sigma_\\\\theta$ are estimated by the models with learnable parameters $\\\\theta$.\\n\\nProblem Definition\\n\\nWe tackle the task of language-guided human motion generation in 3D scenes. The 3D scene is represented as an RGB point cloud $S_{\\\\mathbb{R}^N \\\\hat{6}}$, while the language description is denoted as $L = w_1, w_2, \\\\ldots, w_M$, comprising $M$ tokenized words. Our objective is to generate motion sequences $X_t f_i$ that are both physically plausible and semantically consistent with the given descriptions, where each sequence consists of $F$ frames. Diverging\"}"}
{"id": "CVPR-2024-1588", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Overview of our method. To generate language-guided human motions in 3D scenes, our framework first predicts the scene affordance map in accordance with the language description using Affordance Diffusion Model (ADM). Next, it generates interactive human motions with Affordance-to-Motion Diffusion Model (AMDM) conditioned on the predicted affordance map.\\n\\nFrom the redundant motion representation used by Guo et al. [29], we parameterize the per-frame human pose using the body joint positions of the SMPL-X body model [62], specifically \\\\( x_i \\\\). For visualization purposes, these poses are converted into body meshes by optimizing the SMPL-X parameters based on joint positions. Please refer to Appendix A for additional details on the optimization process.\\n\\n4. Method\\n\\nWe propose a novel two-stage model for generating plausible human motions conditioned on the 3D scene and language descriptions. Fig. 2 illustrates the model's framework. The first stage introduces an Affordance Diffusion Model (ADM) to generate language-grounded affordance maps. The second stage takes input as the generated affordance map and the language description to synthesize plausible human motions from Gaussian noise via the proposed Affordance-to-Motion Diffusion Model (AMDM).\\n\\n4.1. Affordance Map\\n\\nThe affordance map serves as an intermediate representation that abstracts essential details of a 3D indoor scene to support generalization, accurately ground interaction regions, and preserve vital geometric information. In this work, we derive such an affordance map from the distance field between the points in a 3D scene \\\\( S \\\\) and the human skeleton joints across a motion sequence \\\\( X_t \\\\). We calculate the \\\\( \\\\ell_2 \\\\) distance between each scene point and the skeleton joints per frame, resulting in a per-frame distance field \\\\( d_{p,n,j} \\\\); \\\\( d_{p,n,j} \\\\) measures the distance between the \\\\( n \\\\)-th scene point and the \\\\( j \\\\)-th skeleton joint. Following Mao et al. [59], we transform this distance field into a normalized distance map \\\\( c_{p,n,j} \\\\):\\n\\n\\\\[\\nc_{p,n,j} = \\\\exp \\\\left( \\\\frac{-d_{p,n,j}}{\\\\sigma^2} \\\\right),\\n\\\\]\\n\\nwhere \\\\( \\\\sigma \\\\) is a constant normalizing factor. This operation assigns higher weights to points closer to the joints, thereby aiding in stabilizing the training procedure.\\n\\nTo compute the affordance map \\\\( C \\\\), we employ a max-pooling operation over the temporal dimension of the per-frame distance fields:\\n\\n\\\\[\\nC = \\\\max\\\\text{-pool} \\\\left( c_1, c_2, \\\\ldots, c_F \\\\right).\\n\\\\]\\n\\nThe resulting paired data is denoted as \\\\( p_C, X, S, L \\\\), with \\\\( S \\\\) and \\\\( L \\\\) representing the scene's point cloud and the associated language description, respectively.\"}"}
{"id": "CVPR-2024-1588", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"As depicted in Fig. 2, ADM's architecture is based on the Perceiver \\\\cite{perceiver, perceiver_2}, leveraging an attention mechanism to efficiently extract point-wise features. The Perceiver backbone within ADM consists of three primary components: an Encode block, a Process block, and a Decode block. Initially, the Encode utilizes an attention module to encode the extracted point features along with the noisy affordance map, termed as input features. We denote the concatenation of the language feature and diffusion step embeddings as the latent features. In this configuration, the input features act as the attention module's key and value, with the latent features acting as the query. Next, the Process block refines the latent features through multiple self-attention layers. Finally, the Decode block employs another attention module, allowing the input features to attend to the updated latent features, thereby achieving refined per-point feature refinement. We forward these per-point feature vectors into a linear layer for further processing. Contrary to approaches that predict the added noise $\\\\epsilon_t$, our model directly estimate the input signal \\\\cite{direct_estimation, direct_estimation_2}, allowing the end-to-end training of ADM, denoted as $G_{\\\\theta}$, with a simple objective:\\n\\n$$L_{\\\\text{MSE}} = \\\\mathbb{E}_{C_0, t} \\\\left\\\\{ \\\\left. \\\\left\\\\| C_0 - G_{\\\\theta} p C_t, t, S, L_q \\\\right\\\\|_2^2 \\\\right\\\\} \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right. \\\\right"}
{"id": "CVPR-2024-1588", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for the diversity of motions, and physics-based met-\\n\\nfa. affordance map, and language description) directly into self-\\nnated features from the three modalities (human motion,\\neducer adaptation of AMDM that integrates the concate-\\ncates the AMDM's architecture. Moreover, we examine an\\nbypassing the affordance map generation; this variant repli-\\nmodel variant that directly processes the scene point cloud,\\narchitecture, we implement an\\ncVAE\\n\\nas\\n\\nbased approach by Wang et al. [84], hereafter referred to\\nclude the following baselines: Language2Pose [3], T2M [29],\\nand\\n\\nMDM\\n\\n. For\\nexclusions, we retrain the motion and text feature extractors as\\nthe\\n\\nnovel evaluation set\\n\\nobjects within the scene. We also employ these metrics on\\ning interactions, based on distances from the joints to target\\n\\ndist.\\n\\ni.e\\n\\nthree grounding metrics,\\ngenerated motions. To evaluate\\nquality\\n\\nstudies further evaluate the\\ncontact\\n\\nrics like\\n\\nto determine grounding accuracy,\\net al. [84] and Zhang et al. [97], utilizing metrics of\\nHUMANISE\\n\\nmetrics that are better when closer to \\\"Real\\\" distribution. Our model uses Perceiver in ADM and encoder-based architecture in AMDM.\\n\\n| Baselines       | HumanML3D | HUMANISE |\\n|-----------------|-----------|----------|\\n| Ours            | 0.002     | 0.023    |\\n| cVAE [84]       | 0.007     | 0.008    |\\n| Language2Pose [3]| 0.003     | 0.006    |\\n| T2M [29]        | 0.004     | 0.005    |\\n| MDM [76]        | 0.008     | 0.002    |\\n\\nDue to unique motion represen-\\ntation, the model demonstrates enhanced performance in\\nR-Precision\\n\\nformering all baselines. Specifically, against MDM [76], our\\nmethod demonstrates enhanced performance in\\nFID\\n\\nTab. 1 showcases the quantitative results on HumanML3D,\\nwhile preserving a comparable\\nDiversity\\n\\n\u00d1\\n\\naction score\\n\\n\u00d2\\n\\nquality score\\n\\nMultiModal Dist.\\n\\nMultiModality\\n\\n| Metric                      | HumanML3D | HUMANISE |\\n|-----------------------------|-----------|----------|\\n| Top 1 score                 | 0.38      | 0.34     |\\n| Top 2 score                 | 0.65      | 0.55     |\\n| Top 3 score                 | 0.76      | 0.68     |\"}"}
{"id": "CVPR-2024-1588", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Qualitative results on HUMANISE dataset. The bottom-right figure provides a top-down view. Zoom in for better visualization.\\n\\nAcross goal dist., contact, quality, and action scores, signaling a pronounced advancement in generating human motion that aligns accurately with scene and language instructions.\\n\\nNote that the diminished diversity in the APD metric mainly stems from the enhanced precision in motion generation within our model, which effectively grounds motions in the 3D scene with desired semantics and interactions, as opposed to the motions with potential physical implausibility, incorrect semantics or inadequate grounding observed in the baselines methods.\\n\\nQualitative Results\\nVisualizations in Fig. 3 reveal that the cVAEmodel often fails to accurately ground the target object, indicating limited scene-aware capabilities. Moreover, the one-stage model\u2019s lack of scene geometry awareness can result in human-scene collisions and non-contacts.\\n\\nAffordance Generation Evaluation\\nGrounding distance metrics, detailed in Tab. 3, illustrate that among the three variants, the MLP lags in grounding accuracy when compared to the Perceiver and Point Transformer models. This discrepancy might arise from the MLP\u2019s isolated processing of individual scene points, which limits their information exchange. In contrast, the Perceiver consistently excels, presumably due to its effective integration of point and language features through cross-attention mechanisms.\\n\\n5.5. Results on Novel Evaluation Set\\n\\nResults\\nThe performance on the uniquely curated evaluation set, featuring new scenes and language descriptions from Turkers, is summarized in Tab. 4. Our approach exhibits considerable enhancements in FID while maintaining comparable R-Precision and Multimodal-Dist, suggesting a robust capability to synthesize plausible human motions aligned with the given language instructions. Notably, our method generates a wider variety of human motions, as evidenced by improved scores in metrics MultiModality when compared to baseline methods. These results underscore our approach\u2019s efficacy in producing physically plausible and semantically consistent human motions conditioned on scenes and language instructions, validated through contact, quality, and action scores. Fig. 4 presents qualitative results generated with unseen language descriptions and 3D scenes.\\n\\nFailure Cases\\nFig. 5 depicts typical failure cases encountered by our model. For instance, challenges arise with test scenarios of unseen human-scene interactions, resulting in accurately generated motions in the correct space (e.g., hand washing near a tap) but inaccurate interactions (e.g., failure to align the body appropriately facing the sink). The model also fails with language descriptions of high complexity exceeding its current capabilities.\"}"}
{"id": "CVPR-2024-1588", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Qualitative results on our novel evaluation set. \u201cReal\u201d indicates that we compute these metrics as a reference using the language-motion pairs within the test set of HumanML3D. Of note, our novel evaluation set does not contain ground truth motions.\\n\\n| Model     | R-Precision (Top 3) | FID MultiModal Dist. | Diversity MultiModality | Contact | Non-collision Quality Score | Action Score |\\n|-----------|---------------------|----------------------|-------------------------|---------|-----------------------------|--------------|\\n| Real      | 0.875 \u00b1 0.002       | 0.000 \u00b1 0.000        | 3.342 \u00b1 0.004          | 9.442 \u00b1 0.301 |\\n| one-stage @ Enc | 0.500 \u00b1 0.044     | 1.848 \u00b1 1.634        | 5.954 \u00b1 2.235          | 8.395 \u00b1 4.024 |\\n| Ours @ Enc | 0.478 \u00b1 0.069     | 7.887 \u00b1 1.189        | 6.226 \u00b1 1.261          | 7.935 \u00b1 2.542 |\\n| one-stage @ Dec | 0.403 \u00b1 0.044     | 1.268 \u00b1 1.900        | 6.611 \u00b1 2.227          | 5.031 \u00b1 4.264 |\\n| Ours @ Dec | 0.428 \u00b1 0.023     | 1.027 \u00b1 3.164        | 6.412 \u00b1 2.004          | 5.966 \u00b1 2.993 |\\n\\nFigure 4. Qualitative comparisons on generalization evaluation set. The first row is generated by the one-stage diffusion model and the second row is generated by our model. Our method can generate natural and accurately grounded human motions in unseen 3D scenes.\\n\\n\u201cA man dances on the bed happily.\u201d\\n\u201cA person wanders in the room around the table.\u201d\\n\\nFigure 5. Failure cases. Our model fails while facing entirely unfamiliar HSIs or too complex descriptions.\\n\\n5.6. Ablation Study\\nWe further examine the impact of different ADM architectures on motion generation in the second stage of HUMAN-ISE, utilizing an encoder-based AMDM. As outlined in Tab. 5, both Perceiver and Point Transformer yield superior goal dist. outcomes compared to the MLP, echoing findings from Tab. 3. Furthermore, these architectures enhance the physical realism, as indicated by improved contact scores, with Perceiver models having higher collision rates relative to Point Transformers, echoing observations in Fig. 3.\\n\\nTable 5. Ablation of the architectures of AMDM. The Perceiver architecture slightly outperforms the Point Transformer in the metrics of goal dist. and contact score.\\n\\n| Arch. of ADM | goal dist. | contact | non-collision | G.T. |\\n|--------------|------------|---------|---------------|------|\\n| MLP          | 0.394 \u00b1 0.010 | 73.96 \u00b1 0.434 | 99.84 \u00b1 0.005 |      |\\n| Point Trans. | 0.164 \u00b1 0.010 | 94.39 \u00b1 0.408 | 99.82 \u00b1 0.008 |      |\\n| Perceiver    | 0.156 \u00b1 0.006 | 95.86 \u00b1 0.323 | 99.69 \u00b1 0.007 |      |\\n\\n6. Conclusion\\nWe introduced a novel two-stage model that leverages scene affordance as an intermediate representation to bridge the 3D scene grounding and subsequent conditional motion generation. The quantitative and qualitative results demonstrate promising improvements in HumanML3D and HUMANISE. The model\u2019s adaptability was further validated on a uniquely curated evaluation set featuring unseen scenes and language prompts, showcasing its robustness in novel scenarios.\\n\\nLimitations\\n(i) The reliance on diffusion models contributes to slower inference times, marking a significant drawback for future work. (ii) Although employing affordance maps mitigates the challenges posed by the scarcity of paired data for training in 3D environments, data limitation remains a critical hurdle. Future initiatives should focus on devising strategies to overcome this persistent challenge.\\n\\nAcknowledgments\\nThe authors would like to thank NVIDIA for their generous support of GPUs and hardware. This work is supported in part by the National Science and Technology Major Project (2022ZD0114900), the National Natural Science Foundation of China (NSFC) (62172043), and the Beijing Nova Program.\"}"}
{"id": "CVPR-2024-1588", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas Guibas. Referit3d: Neural listeners for fine-grained 3d object identification in real-world scenes. In European Conference on Computer Vision (ECCV), 2020.\\n\\n[2] Hyemin Ahn, Timothy Ha, Yunho Choi, Hwiyeon Yoo, and Songhwai Oh. Text2action: Generative adversarial synthesis from language to action. In International Conference on Robotics and Automation (ICRA), 2018.\\n\\n[3] Chaitanya Ahuja and Louis-Philippe Morency. Language2pose: Natural language grounded pose forecasting. In International Conference on 3D Vision (3DV), 2019.\\n\\n[4] Joao Pedro Ara\u00b4ujo, Jiaman Li, Karthik Vetrivel, Rishi Agarwal, Jiajun Wu, Deepak Gopinath, Alexander William Clegg, and Karen Liu. Circle: Capture in rich contextual environments. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[5] Nikos Athanasiou, Mathis Petrovich, Michael J. Black, and G\u00fcl Varol. Teach: Temporal action composition for 3d humans. In International Conference on 3D Vision (3DV), 2022.\\n\\n[6] Nikos Athanasiou, Mathis Petrovich, Michael J. Black, and G\u00fcl Varol. SINC: Spatial composition of 3D human motions for simultaneous action generation. In International Conference on Computer Vision (ICCV), 2023.\\n\\n[7] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. Scanqa: 3d question answering for spatial scene understanding. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\n[8] Emad Barsoum, John Kender, and Zicheng Liu. Hp-gan: Probabilistic 3d human motion prediction via gan. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\n[9] Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai, Minh Vo, and Jitendra Malik. Long-term human motion prediction with scene context. In European Conference on Computer Vision (ECCV), 2020.\\n\\n[10] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments. In International Conference on 3D Vision (3DV), 2017.\\n\\n[11] Yu-Wei Chao, Jimei Yang, Weifeng Chen, and Jia Deng. Learning to sit: Synthesizing human-chair interactions via hierarchical control. In AAAI Conference on Artificial Intelligence (AAAI), 2021.\\n\\n[12] Dave Zhenyu Chen, Qirui Wu, Matthias Nie\u00dfner, and Angel X Chang. D3net: a speaker-listener architecture for semi-supervised dense captioning and visual grounding in rgb-d scans. In European Conference on Computer Vision (ECCV), 2022.\\n\\n[13] Sijin Chen, Hongyuan Zhu, Xin Chen, Yinjie Lei, Gang Yu, and Tao Chen. End-to-end 3d dense captioning with vote2cap-detr. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[14] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[15] Yixin Chen, Siyuan Huang, Tao Yuan, Siyuan Qi, Yixin Zhu, and Song-Chun Zhu. Holistic++ scene understanding: Single-view 3d holistic scene parsing and human pose estimation with human-object interaction and physical commonsense. In International Conference on Computer Vision (ICCV), 2019.\\n\\n[16] Yixin Chen, Sai Kumar Dwivedi, Michael J Black, and Dimitrios Tzionas. Detecting human-object contact in images. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[17] Zhenyu Chen, Ali Gholami, Matthias Nie\u00dfner, and Angel X Chang. Scan2cap: Context-aware dense captioning in rgb-d scans. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\n[18] Jieming Cui, Tengyu Liu, Nian Liu, Yaodong Yang, Yixin Zhu, and Siyuan Huang. Anyskill: Learning open-vocabulary physical skill for interactive agents. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\\n\\n[19] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\n[20] Abhishek Das, Samyak Datta, Georgia Gkioxari, Stefan Lee, Devi Parikh, and Dhruv Batra. Embodied question answering. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\n[21] Shengheng Deng, Xun Xu, Chaozheng Wu, Ke Chen, and Kui Jia. 3d affordancenet: A benchmark for visual object affordance understanding. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\n[22] Kuan Fang, Te-Lin Wu, Daniel Yang, Silvio Savarese, and Joseph J Lim. Demo2vec: Reasoning object affordances from online videos. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\n[23] Anindita Ghosh, Noshaba Cheema, Cennet Oguz, Christian Theobalt, and Philipp Slusallek. Synthesis of compositional animations from textual descriptions. In International Conference on Computer Vision (ICCV), 2021.\\n\\n[24] Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, and Philipp Slusallek. Imos: Intent-driven full-body motion synthesis for human-object interactions. In Computer Graphics Forum, 2023.\\n\\n[25] James J Gibson. The theory of affordances. Hilldale, USA, 1(2):67\u201382, 1977.\\n\\n[26] Helmut Grabner, Juergen Gall, and Luc Van Gool. What makes a chair a chair? In Conference on Computer Vision and Pattern Recognition (CVPR), 2011.\"}"}
{"id": "CVPR-2024-1588", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng. Action2motion: Conditioned generation of 3D human motions. In International Conference on Multimedia, 2020.\\n\\nChuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3D human motions from text. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nAbhinav Gupta, Scott Satkin, Alexei A. Efros, and Martial Hebert. From 3D scene geometry to human workspace. In Conference on Computer Vision and Pattern Recognition (CVPR), 2011.\\n\\nMohamed Hassan, Vasileios Choutas, Dimitrios Tzionas, and Michael J. Black. Resolving 3D human pose ambiguities with 3D scene constraints. In International Conference on Computer Vision (ICCV), 2019.\\n\\nMohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael J. Black. Stochastic scene-aware motion prediction. In International Conference on Computer Vision (ICCV), 2021.\\n\\nMohamed Hassan, Partha Ghosh, Joachim Tesch, Dimitrios Tzionas, and Michael J. Black. Populating 3D scenes by learning human-scene interaction. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nJonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nDaniel Holden, Jun Saito, and Taku Komura. A deep learning framework for character motion synthesis and editing. ACM Transactions on Graphics (TOG), 35(4):1\u201311, 2016.\\n\\nJiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3D world. arXiv preprint arXiv:2311.12871, 2023.\\n\\nSiyuan Huang, Zan Wang, Puhao Li, Baoxiong Jia, Tengyu Liu, Yixin Zhu, Wei Liang, and Song-Chun Zhu. Diffusion-based generation, optimization, and planning in 3D scenes. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nAndrew Jaegle, Felix Gimeno, Andy Brock, Oriol Vinyals, Andrew Zisserman, and Joao Carreira. Perceiver: General perception with iterative attention. In International Conference on Machine Learning (ICML), 2021.\\n\\nAndrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, et al. Perceiver io: A general architecture for structured inputs & outputs. In International Conference on Learning Representations (ICLR), 2022.\\n\\nKrishna Murthy Jatavallabhula, Alihusein Kuwajerwala, Qiao Gu, Mohd Omama, Tao Chen, Shuang Li, Ganesh Iyer, Soroush Saryazdi, Nikhil Keetha, Ayush Tewari, et al. Conceptfusion: Open-set multimodal 3D mapping. arXiv preprint arXiv:2302.07241, 2023.\\n\\nBaoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, and Siyuan Huang. Sceneverse: Scaling 3D vision-language learning for grounded scene understanding. arXiv preprint arXiv:2401.09340, 2024.\\n\\nHanwen Jiang, Shaowei Liu, Jiashun Wang, and Xiaolong Wang. Hand-object contact consistency reasoning for human grasps generation. In International Conference on Computer Vision (ICCV), 2021.\\n\\nNan Jiang, Tengyu Liu, Zhexuan Cao, Jieming Cui, Zhiyuan Zhang, Yixin Chen, He Wang, Yixin Zhu, and Siyuan Huang. Full-body articulated human-object interaction. In International Conference on Computer Vision (ICCV), 2023.\\n\\nNan Jiang, Zhiyuan Zhang, Hongjie Li, Xiaoxuan Ma, Zan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, and Siyuan Huang. Scaling up dynamic human-scene interaction model-building. In Conference on Computer Vision and Pattern Recognition (CVPR), 2024.\\n\\nJustin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. In International Conference on Computer Vision (ICCV), 2023.\\n\\nMia Kokic, Danica Kragic, and Jeannette Bohg. Learning task-oriented grasping from human activity datasets. IEEE Robotics and Automation Letters (RA-L), 5(2):3352\u20133359, 2020.\\n\\nHema S Koppula and Ashutosh Saxena. Physically grounded spatio-temporal object affordances. In European Conference on Computer Vision (ECCV), 2014.\\n\\nSumith Kulal, Tim Brooks, Alex Aiken, Jiajun Wu, Jimei Yang, Jingwan Lu, Alexei A Efros, and Krishna Kumar Singh. Putting people in their place: Affordance-aware human insertion into scenes. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\nHsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun Wang, Yu-Ding Lu, Ming-Hsuan Yang, and Jan Kautz. Dancing to music. In Advances in Neural Information Processing Systems (NeurIPS), 2019.\\n\\nChen Li, Zhen Zhang, Wee Sun Lee, and Gim Hee Lee. Convolutional sequence to sequence model for human dynamics. In Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\nJing Li, Di Kang, Wenjie Pei, Xuefei Zhe, Ying Zhang, Zhenyu He, and Linchao Bao. Audio2gestures: Generating diverse gestures from speech audio with conditional variational autoencoders. In International Conference on Computer Vision (ICCV), 2021.\\n\\nJiaman Li, Jiajun Wu, and C. Karen Liu. Object motion guided human motion synthesis. ACM Transactions on Graphics (TOG), 42(6):1\u201311, 2023.\\n\\nPuhao Li, Tengyu Liu, Yuyang Li, Yiran Geng, Yixin Zhu, Yaodong Yang, and Siyuan Huang. Gendexgrasp: Generalizable dexterous grasping. In International Conference on Robotics and Automation (ICRA), 2023.\\n\\nXueting Li, Sifei Liu, Kihwan Kim, Xiaolong Wang, Ming-Hsuan Yang, and Jan Kautz. Putting humans in a scene: Learning affordance in 3D indoor environments. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\"}"}
{"id": "CVPR-2024-1588", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[55] Yuyang Li, Bo Liu, Puhao Li, Yaodong Yang, Yixin Zhu, Tengyu Liu, and Siyuan Huang. Grasp multiple objects with one hand. IEEE Robotics and Automation Letters (RA-L), 9(5):4027\u20134034, 2024.\\n\\n[56] Hung Yu Ling, Fabio Zinno, George Cheng, and Michiel Van De Panne. Character controllers using motion vaes. ACM Transactions on Graphics (TOG), 39(4):40\u20131, 2020.\\n\\n[57] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. Sqa3d: Situated question answering in 3d scenes. In International Conference on Learning Representations (ICLR), 2023.\\n\\n[58] Naureen Mahmood, Nima Ghorbani, Nikolaus F Troje, Gerard Pons-Moll, and Michael J Black. Amass: Archive of motion capture as surface shapes. In International Conference on Computer Vision (ICCV), 2019.\\n\\n[59] Wei Mao, Richard I Hartley, Mathieu Salzmann, et al. Contact-aware human motion forecasting. In Advances in Neural Information Processing Systems (NeurIPS), 2022.\\n\\n[60] Tushar Nagarajan and Kristen Grauman. Learning affordance landscapes for interaction exploration in 3d environments. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\n[61] Tushar Nagarajan, Christoph Feichtenhofer, and Kristen Grauman. Grounded human-object interaction hotspots from video. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\n[62] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael J Black. Expressive body capture: 3d hands, face, and body from a single image. In Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\n[63] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In Conference on Computer Vision and Pattern Recognition (CVPR), 2023.\\n\\n[64] Xue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. Amp: Adversarial motion priors for stylized physics-based character control. ACM Transactions on Graphics (TOG), 40(4):1\u201320, 2021.\\n\\n[65] Xue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler. Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters. ACM Transactions on Graphics (TOG), 41(4):1\u201317, 2022.\\n\\n[66] Mathis Petrovich, Michael J Black, and Gul Varol. Action-conditioned 3d human motion synthesis with transformer vae. In International Conference on Computer Vision (ICCV), 2021.\\n\\n[67] Mathis Petrovich, Michael J Black, and Gul Varol. Temos: Generating diverse human motions from textual descriptions. In European Conference on Computer Vision (ECCV), 2022.\\n\\n[68] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), 2021.\\n\\n[69] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\\n\\n[70] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning (ICML), 2015.\\n\\n[71] Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In Advances in Neural Information Processing Systems (NeurIPS), 2019.\\n\\n[72] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, et al. The replica dataset: A digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019.\\n\\n[73] Omid Taheri, Vasileios Choutas, Michael J Black, and Dimitrios Tzionas. Goal: Generating 4d whole-body motion for hand-object grasping. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\n[74] Ayc A Takmaz, Elisabetta Fedele, Robert W Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. OpenMask3D: Open-Vocabulary 3D Instance Segmentation. In Advances in Neural Information Processing Systems (NeurIPS), 2023.\\n\\n[75] Guy Tevet, Brian Gordon, Amir Hertz, Amit H Bermano, and Daniel Cohen-Or. Motionclip: Exposing human motion generation to clip space. In European Conference on Computer Vision (ECCV), 2022.\\n\\n[76] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In International Conference on Learning Representations (ICLR), 2023.\\n\\n[77] Jesse Thomason, Mohit Shridhar, Yonatan Bisk, Chris Paxton, and Luke Zettlemoyer. Language grounding with 3d objects. In Conference on Robot Learning (CoRL), 2022.\\n\\n[78] Matthew Thorne, David Burke, and Michiel Van De Panne. Motion doodles: an interface for sketching character motion. ACM Transactions on Graphics (TOG), 23(3):424\u2013431, 2004.\\n\\n[79] Mikaela Angelina Uy, Quang-Hieu Pham, Binh-Son Hua, Thanh Nguyen, and Sai-Kit Yeung. Revisiting point cloud classification: A new benchmark dataset and classification model on real-world data. In International Conference on Computer Vision (ICCV), 2019.\\n\\n[80] Jiashun Wang, Huazhe Xu, Jingwei Xu, Sifei Liu, and Xiaolong Wang. Synthesizing long-term 3d human motion and interaction in 3d scenes. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\n[81] Jingbo Wang, Sijie Yan, Bo Dai, and Dahua Lin. Scene-aware generative network for human motion synthesis. In Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\"}"}
{"id": "CVPR-2024-1588", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jingbo Wang, Yu Rong, Jingyuan Liu, Sijie Yan, Dahua Lin, and Bo Dai. Towards diverse and natural scene-aware 3d human motion synthesis. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2022.\\n\\nXiaolong Wang, Rohit Girdhar, and Abhinav Gupta. Binge watching: Scaling affordance learning from sitcoms. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2017.\\n\\nZan Wang, Yixin Chen, Tengyu Liu, Yixin Zhu, Wei Liang, and Siyuan Huang. Humanise: Language-conditioned human motion generation in 3d scenes. In *Advances in Neural Information Processing Systems (NeurIPS)*, 2022.\\n\\nYueh-Hua Wu, Jiashun Wang, and Xiaolong Wang. Learning generalizable dexterous manipulation from human grasp affordance. In *Conference on Robot Learning (CoRL)*, 2023.\\n\\nZeqi Xiao, Tai Wang, Jingbo Wang, Jinkun Cao, Wenwei Zhang, Bo Dai, Dahua Lin, and Jiangmiao Pang. Unified human-scene interaction via prompted chain-of-contacts. In *International Conference on Learning Representations (ICLR)*, 2024.\\n\\nKevin Xie, Tingwu Wang, Umar Iqbal, Yunrong Guo, Sanja Fidler, and Florian Shkurti. Physics-based human motion estimation and synthesis from videos. In *International Conference on Computer Vision (ICCV)*, 2021.\\n\\nShuquan Ye, Dongdong Chen, Songfang Han, and Jing Liao. 3d question answering. In *IEEE Transactions on Visualization and Computer Graph (TVCG)*, pages 1\u201316, 2022.\\n\\nHongwei Yi, Hualin Liang, Yifei Liu, Qiong Cao, Yandong Wen, Timo Bolkart, Dacheng Tao, and Michael J Black. Generating holistic 3d human motion from speech. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2023.\\n\\nYe Yuan and Kris Kitani. Dlow: Diversifying latent flows for diverse human motion prediction. In *European Conference on Computer Vision (ECCV)*, 2020.\\n\\nZhihao Yuan, Xu Yan, Yinghong Liao, Yao Guo, Guanbin Li, Shuguang Cui, and Zhen Li. X-trans2cap: Cross-modal knowledge transfer using transformer for 3d dense captioning. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2022.\\n\\nJianrong Zhang, Yangsong Zhang, Xiaodong Cun, Shaoli Huang, Yong Zhang, Hongwei Zhao, Hongtao Lu, and Xi Shen. T2m-gpt: Generating human motion from textual descriptions with discrete representations. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2023.\\n\\nMingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. In *arXiv preprint arXiv:2208.15001*, 2022.\\n\\nSiwei Zhang, Yan Zhang, Qianli Ma, Michael J Black, and Siyu Tang. Place: Proximity learning of articulation and contact in 3d environments. In *International Conference on 3D Vision (3DV)*, 2020.\\n\\nXiaohan Zhang, Bharat Lal Bhatnagar, Sebastian Starke, Vladimir Guzov, and Gerard Pons-Moll. Couch: Towards controllable human-chair interactions. In *European Conference on Computer Vision (ECCV)*, 2022.\\n\\nYan Zhang and Siyu Tang. The wanderings of odysseus in 3d scenes. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2022.\\n\\nYan Zhang, Mohamed Hassan, Heiko Neumann, Michael J Black, and Siyu Tang. Generating 3d people in scenes without people. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020.\\n\\nYiming Zhang, ZeMing Gong, and Angel X Chang. Multi3drefer: Grounding text description to multiple 3d objects. In *International Conference on Computer Vision (ICCV)*, 2023.\\n\\nHengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In *International Conference on Computer Vision (ICCV)*, 2021.\\n\\nKaifeng Zhao, Shaofei Wang, Yan Zhang, Thabo Beeler, and Siyu Tang. Compositional human-scene interaction synthesis with semantic control. In *European Conference on Computer Vision (ECCV)*, 2022.\\n\\nKaifeng Zhao, Yan Zhang, Shaofei Wang, Thabo Beeler, and Siyu Tang. Synthesizing diverse human motions in 3d indoor scenes. In *International Conference on Computer Vision (ICCV)*, 2023.\\n\\nWentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang. Human motion generation: A survey. In *Transactions on Pattern Analysis and Machine Intelligence (TPAMI)*, pages 1\u201320, 2023.\\n\\nYuke Zhu, Alireza Fathi, and Li Fei-Fei. Reasoning about object affordances in a knowledge base representation. In *European Conference on Computer Vision (ECCV)*, 2014.\\n\\nYixin Zhu, Chenfanfu Jiang, Yibiao Zhao, Demetri Terzopoulos, and Song-Chun Zhu. Inferring forces and learning human utilities from videos. In *Conference on Computer Vision and Pattern Recognition (CVPR)*, 2016.\\n\\nZiyu Zhu, Xiaojian Ma, Yixin Chen, Zhidong Deng, Siyuan Huang, and Qing Li. 3d-vista: Pre-trained transformer for 3d vision and text alignment. In *International Conference on Computer Vision (ICCV)*, 2023.\"}"}
