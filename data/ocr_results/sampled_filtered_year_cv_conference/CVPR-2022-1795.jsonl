{"id": "CVPR-2022-1795", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"features from detection head into the recognition stage, as illustrated in Figure 5. The RC consists of the Transformer encoder \\\\[48\\\\] and four up-sampling structures. The input of RC are the detection features \\\\( f_{\\\\text{det}} \\\\) and three down-sampling features \\\\( \\\\{a_1, a_2, a_3\\\\} \\\\).\\n\\nThe detection features are sent to the Transformer encoder \\\\( \\\\text{TrE}() \\\\), making the information of previous detection stages further fused with \\\\( a_3 \\\\). Then through a stack of up-sampling operation \\\\( \\\\text{Eu}() \\\\) and Sigmoid function \\\\( \\\\phi() \\\\), three masks \\\\( \\\\{M_1, M_2, M_3\\\\} \\\\) for text regions are generated:\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{d}_1 &= \\\\text{TrE}(f_{\\\\text{det}}), \\\\\\\\\\n\\\\text{d}_2 &= (\\\\text{Eu}(\\\\text{d}_1) + a_2), \\\\\\\\\\n\\\\text{d}_3 &= (\\\\text{Eu}(\\\\text{d}_2) + a_1), \\\\\\\\\\nM_i &= \\\\phi(\\\\text{d}_i), i = 1, 2, 3.\\n\\\\end{align*}\\n\\\\]\\n\\nWith the masks \\\\( \\\\{M_1, M_2, M_3\\\\} \\\\) and the input features \\\\( \\\\{a_1, a_2, a_3\\\\} \\\\), we further integrate these features effectively under the following pipeline:\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{r}_1 &= M_1 \\\\cdot a_3, \\\\\\\\\\n\\\\text{r}_2 &= M_2 \\\\cdot (\\\\text{Eu}(\\\\text{r}_1) + a_2), \\\\\\\\\\n\\\\text{r}_3 &= M_3 \\\\cdot (\\\\text{Eu}(\\\\text{r}_2) + a_1),\\n\\\\end{align*}\\n\\\\]\\n\\nwhere \\\\( \\\\{\\\\text{r}_1, \\\\text{r}_2, \\\\text{r}_3\\\\} \\\\) denote the recognition features. The \\\\( \\\\text{r}_3 \\\\) is the fused features in Figure 5, which is finally sent to the recognizer at the highest resolution. As shown in the blue dashed lines in Figure 5, the gradient of the recognition loss \\\\( L_{\\\\text{reg}} \\\\) can be back-propagated to the detection features, enabling RC to implicitly improve the detection head through the recognition supervision.\\n\\nGenerally, to suppress the background, the fused features will be multiplied by a mask \\\\( K \\\\) predicted by detection head (with the supervision of \\\\( L_{\\\\text{mask}} \\\\)). However, the background noise still remains in the feature maps as the detection box is not tight enough. Such issue can be alleviated by the proposed RC since RC uses the detection features to generate tight masks to suppress the background noise, which is supervised by the recognition loss apart from the detection loss. As shown in the upper right corner of Figure 5, \\\\( M_3 \\\\) suppresses more background noise than \\\\( K \\\\), where \\\\( M_3 \\\\) has higher activation in texts region and lower in the background. Therefore the masks \\\\( \\\\{M_1, M_2, M_3\\\\} \\\\) produced by RC, which will be applied to the recognition features \\\\( \\\\{\\\\text{r}_1, \\\\text{r}_2, \\\\text{r}_3\\\\} \\\\), makes recognizer easier to concentrate on the text regions.\\n\\nWith RC, the gradient of recognition loss not only flows back to the backbone network, but also to the proposal features. Optimized by both detection supervision and recognition supervision, the proposal features can better encode the high-level semantic information of the texts. Therefore, the proposed RC can incentivize the coordination between detection and recognition.\\n\\nFigure 5. Detailed structure of Recognition Conversion.\\n\\n3.4. Recognizer\\n\\nAfter applying RC on the feature map, background noise is effectively suppressed and thus the text regions can be bounded more precisely. This enables us to merely use a sequential recognition network to obtain promising recognition results without rectification modules such as TPS \\\\[3\\\\], RoISlide \\\\[8\\\\], Bezier-Align \\\\[28\\\\] or character-level segmentation branch used in MaskTextSpotter \\\\[21\\\\]. To enhance the fine-grained feature extraction and sequence modeling, we adopt a bi-level self-attention mechanism, inspired by \\\\[61\\\\], as the recognition encoder. The two-level self-attention mechanism (TLSAM) contains both fine-grained and coarse-grained self-attention mechanisms for local neighborhood regions and global regions, respectively. Therefore, it can effectively extract fine-grained features while maintaining global modeling capability. As for the decoder, we simply follow MaskTextSpotter by using the Spatial Attention Module (SAM) \\\\[22\\\\]. The recognition loss is as follow:\\n\\n\\\\[\\nL_{\\\\text{reg}} = -\\\\frac{1}{T} \\\\sum_{k=1}^{T} \\\\log p(y_i),\\n\\\\]\\n\\nwhere \\\\( T \\\\) is the max length of the sequence and \\\\( p(y_i) \\\\) is the probability of sequence.\\n\\n4. Experiments\\n\\nWe conduct experiments on various scene text benchmarks, including multi-oriented scene text benchmarks RoIC13 \\\\[22\\\\] and ICDAR 2015 \\\\[18\\\\], multilingual datasets ReCTS \\\\[66\\\\] and Vintext \\\\[36\\\\], and two arbitrarily-shaped scene text benchmarks Total-Text \\\\[6\\\\] and SCUT-CTW 1500 \\\\[29\\\\]. The ablation studies are conducted on Total-Text to verify each component of our proposed method.\"}"}
{"id": "CVPR-2022-1795", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. End-to-end recognition result on ICDAR 2015. \u201cS\u201d, \u201cW\u201d, and \u201cG\u201d represent recognition with \u201cStrong\u201d, \u201cWeak\u201d, and \u201cGeneric\u201d lexicon, respectively.\\n\\n| Method                  | Detection 1-NED | Detection Recall | Detection Precision | Recognition H-mean |\\n|------------------------|-----------------|------------------|---------------------|--------------------|\\n| FOTS [27]              | 82.5            | 78.3             | 80.3                | 50.8               |\\n| MaskTextSpotter [21]   | 88.8            | 89.3             | 89.0                | 67.8               |\\n| AE TextSpotter [54]    | 91.0            | 92.6             | 91.8                | 71.8               |\\n| ABCNet v2 [30]         | 87.5            | 93.6             | 90.4                | 62.7               |\\n| SwinTextSpotter        | 87.1            | 94.1             | 90.4                | 72.5               |\\n\\nTable 2. End-to-end text spotting result and detection result on ReCTS.\\n\\n4.1. Implementation Details\\n\\nWe follow the training strategy in [37]. First, the model is pretrained on the Curved SynthText [28], ICDAR-MLT [34], and the corresponding dataset for 450K iterations. The initialized learning rate is $2.5 \\\\times 10^{-5}$, which reduces to $2.5 \\\\times 10^{-6}$ at 380Kth iteration and $2.5 \\\\times 10^{-7}$ at 420Kth iteration. Then we jointly train the pretrained model for 80K iterations on the Total-Text, ICDAR 2013, and ICDAR-MLT, which decays to a tenth at 60K. Finally, we fine-tune the jointly trained model on the corresponding datasets. We also follow the training strategies in [30] and [36] to train the model on Chinese and Vietnamese.\\n\\nWe extract 4 feature maps with 1/4, 1/8, 1/16, 1/32 resolution of the input image for text detection and recognition. We train our model with image batch size of 8. The following data augmentation strategies are used: (1) random scaling; (2) random rotation; and (3) random crop. Other strategies such as random brightness, contrast, and saturation are also applied during training.\\n\\n4.2. Datasets\\n\\nWe use the following datasets:\\n\\n- Curved SynthText [28] is a synthesized dataset for arbitrarily-shaped scene text. It contains 94,723 images with multi-oriented text and 54,327 images with curved text.\\n- ICDAR 2013 [19] is a scene text dataset proposed in 2013. It contains 229 training images and 233 test images.\\n- ICDAR 2015 [18] is built in 2015. It contains 1,000 training images and 500 test images.\\n- ICDAR 2017 [34] is a multi-lingual text dataset. It contains 7,200 training images, 1,800 validation images. We only select the images containing English texts for training.\\n- ICDAR19 ArT [5] is a dataset for arbitrarily shaped text. It contains 5,603 training images.\\n- ICDAR19 LSVT [46] is a large number of Chinese datasets which contains 30,000 training images.\\n- Total-Text [6] is the benchmark for arbitrarily-shaped scene text. It consists of 1,255 training images and 300 testing images. The word-level polygon boxes are provided as the annotations.\\n- SCUT-CTW1500 [29] is a text-line level arbitrarily-shaped scene text dataset. It consists of 1,000 training images and 500 testing images. Compared to Total-Text, this dataset contains denser and longer text.\\n- ReCTS [66] consists of 20,000 training images and 5,000 testing images. It also provides character-level bounding boxes, which are not used in our method.\\n- VinText [36] is a recently proposed Vietnamese text dataset. It consists of 1,200 training images and 500 testing images.\\n\\n4.3. Comparisons with State-of-the-Art methods\\n\\nExcept in special cases, all values in the table are in percentage. Multi-oriented and Multilingual datasets. We first conduct experiments on ICDAR 2015, showing the superiority of SwinTextSpotter on oriented scene text. Table 1 shows that SwinTextSpotter achieves the best strong lexicon results on ICDAR 2015, without using the character-level annotations which were used by ABCNet v2 and MaskTextSpotter v3. We also conduct experiments on RoIC13 dataset proposed in [22] to verify the rotation robustness of SwinTextSpotter. The end-to-end recognition results are shown in Table 3. Both in Rotation Angle $45^\\\\circ$ and in Rotation Angle $60^\\\\circ$ datasets, SwinTextSpotter can achieve the state-of-the-art in terms of the H-mean metric. Our method significantly outperforms the Mask TextSpotter v3 by 1.5% in terms of H-mean on Rotation Angle $45^\\\\circ$ and 1.3% on Rotation Angle $60^\\\\circ$. In addition to English, we also conduct experiment on Chinese dataset ReCTS and Vietnamese dataset VinText to verify the generality of SwinTextSpotter. As shown in Table 2, for ReCTS, our method surpasses ABCNet v2, which only works on word-level annotation, by 9.8% in 1-NED. SwinTextSpotter has 0.7% higher 1-NED than AE-TextSpotter, the SOTA method requiring additional character-level annotation. For VinText, the end-to-end results are presented in Table 4, \u201cD\u201d means using dictionary for the training of recognizer. SwinTextSpotter can also outperform previous methods on VinText, showing the generalization of our method. It is worth noting that for the above tasks, we do not use dictionary for the training of recognizer as ABCNet+D and Mask TextSpotter v3+D.\"}"}
{"id": "CVPR-2022-1795", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 3. End-to-end recognition result on RoIC13. P, R, H represent precision, recall and Hmean, respectively.\\n\\n| Method                  | H-mean |\\n|-------------------------|--------|\\n| CharNet [60]            | 35.5   |\\n| Mask TextSpotter [21]   | 45.8   |\\n| Mask TextSpotter v3 [22]| 66.8   |\\n| SwinTextSpotter         | 72.5   |\\n\\n### Table 4. End-to-end text spotting result on VinText. ABCNet+D means adding the methods proposed in [36] to ABCNet. The same to Mask Textspotter v3+D.\\n\\n| Method                  | H-mean |\\n|-------------------------|--------|\\n| ABCNet [28]             | 54.2   |\\n| ABCNet+D [36]           | 57.4   |\\n| Mask Textspotter v3 [36]| 53.4   |\\n| Mask Textspotter v3+D   | 68.5   |\\n| SwinTextSpotter         | 71.1   |\\n\\n### Table 5. End-to-end text spotting result and detection result on Total-Text. SwinTextSpotter-Res means using the ResNet50 with FPN as backbone. \u201cNone\u201d represents lexicon-free. \u201cFull\u201d represents that we use all the words appeared in the test set.\\n\\n| Method                  | Detection | End-to-End | H-mean |\\n|-------------------------|-----------|------------|--------|\\n| CharNet [60]            |           |            | 85.6   |\\n| ABCNet [28]             |           |            | -      |\\n| PGNet [53]              |           |            | 86.1   |\\n| Mask TextSpotter [21]   |           |            | 85.2   |\\n| Qin et al. [39]         |           |            | -      |\\n| Mask TextSpotter v3 [22]|           |            | -      |\\n| MANGO [37]              |           |            | -      |\\n| ABCNet v2 [30]          |           |            | 87.0   |\\n| PAN++ [55]              |           |            | -      |\\n| SwinTextSpotter-Res     |           |            | 87.2   |\\n| SwinTextSpotter         |           |            | 88.0   |\\n\\n### Table 6. End-to-end text spotting result and detection result on SCUT-CTW1500. \u201cNone\u201d represents lexicon-free. \u201cFull\u201d represents that we use all the words appeared in the test set.\\n\\n| Method                  | Detection | End-to-End | H-mean |\\n|-------------------------|-----------|------------|--------|\\n| TextDragon [8]          |           |            | 83.6   |\\n| ABCNet [28]             |           |            | 81.4   |\\n| MANGO [37]              |           |            | -      |\\n| ABCNet v2 [30]          |           |            | 84.7   |\\n| SwinTextSpotter         |           |            | 88.0   |\\n\\n4.4. Ablation Studies\\n\\nTo evaluate the effectiveness of the proposed components, we conduct ablation studies on Total-Text. ResNet-50 is used as the baseline backbone. In ablation studies, we only train different variants of SwinTextSpotter on the Curved SynthText, ICDAR-MLT and the corresponding dataset as the first stage described in Section 4.1.\\n\\n**Recognition Conversion**: As shown in Table 7, with RC, the results can be improved by 3.0% and 6.9% for detection and end-to-end scene text spotting, respectively. RC greatly improves the performance of the detector and recognizer. This is mainly because the RC can generate more discriminative features for text regions to boost the performance of text recognition so as to benefit the text detector.\\n\\n**Dilated Swin-Transformer**: We also compare the performance of different backbones. The model with SwinTransformer can achieve 2.9% improvement on end-to-end results over the ResNet-50, but there is no improvement for detection. Incorporating dilated convolution into SwinTransformer can further boost detection by 0.7% and 0.5% in end-to-end results.\\n\\n**Two-level self-attention mechanisms**: In Table 7, we...\"}"}
{"id": "CVPR-2022-1795", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 7. Ablation studies on Total-Text without finetuning. ResNet-50 is used as the baseline backbone. TLSAM stands for the two-level self-attention mechanism.\\n\\n4.5. Limitation and Discussion\\n\\nLong Arbitrarily-Shaped Text. We know that the long arbitrarily-shaped text requires a high resolution feature map to be recognized. When the feature map becomes larger, attention map in the recognizer will also expand. Large attention map may result in mismatch of the recognizer, which leads to the low end-to-end text spotting performance on SCUT-CTW1500. The amount of long arbitrarily-shaped data is limited. Our recognition decoder needs more training data than the 1D-Attention [1] and CTC [10] and so it is not well trained yet. However, the 1-NED result and \u201cFull\u201d result shown in Table 6 narrow the gaps between our method and ABCNet v2, which suggests that the errors mainly occur in individual characters.\\n\\n5. Conclusion\\n\\nIn this paper, we propose SwinTextSpotter. To the best of our knowledge, SwinTextSpotter is the first successful attempt using Transformer-based method and set-prediction scheme for end-to-end scene text spotting. With the core idea to make text recognition as a part of detection, the proposed model tightly couples the detection and recognition instead of only sharing information in the backbone. The proposed Recognition Conversion enables the suppression of background noise in the recognition features by making the detection results differentiable with respect to the recognition loss. Such design greatly simplifies the text spotter framework by removing the rectification module and enables the joint optimization of both the detection and the recognition module toward better spotting performance without character-level annotation. Extensive experiments on public benchmarks demonstrate that SwinTextSpotter can achieve superior performance in end-to-end scene text spotting on arbitrarily-shaped text and multi-lingual text.\\n\\nAcknowledgement\\n\\nThis research is supported in part by NSFC (Grant No.: 61936003) and GD-NSF (no.2017A030312006, No.2021A1515011870).\"}"}
{"id": "CVPR-2022-1795", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align and translate. arXiv preprint arXiv:1409.0473, 2014.\\n\\n[2] Alessandro Bissacco, Mark Cummins, Yuval Netzer, and Hartmut Neven. Photoocr: Reading text in uncontrolled conditions. In Proceedings of the IEEE International Conference on Computer Vision, pages 785\u2013792, 2013.\\n\\n[3] Fred L. Bookstein. Principal warps: Thin-plate splines and the decomposition of deformations. IEEE Transactions on Pattern Analysis and Machine Intelligence, 11(6):567\u2013585, 1989.\\n\\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, pages 213\u2013229. Springer, 2020.\\n\\n[5] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao Zhang, Junyu Han, Errui Ding, et al. Icdar2019 robust reading challenge on arbitrary-shaped text-rrc-art. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1571\u20131576. IEEE, 2019.\\n\\n[6] Chee-Kheng Ch'ng, Chee Seng Chan, and Cheng-Lin Liu. Total-text: toward orientation robustness in scene text detection. International Journal on Document Analysis and Recognition (IJDAR), 23(1):31\u201352, 2020.\\n\\n[7] Shancheng Fang, Hongtao Xie, Yuxin Wang, Zhendong Mao, and Yongdong Zhang. Read like humans: Autonomous, bidirectional and iterative language modeling for scene text recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7098\u20137107, 2021.\\n\\n[8] Wei Feng, Wenhao He, Fei Yin, Xu-Yao Zhang, and Cheng-Lin Liu. Textdragon: An end-to-end framework for arbitrary shaped text spotting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9076\u20139085, 2019.\\n\\n[9] Lluis Gomez and Dimosthenis Karatzas. Textproposals: a text-specific selective search algorithm for word spotting in the wild. Pattern Recognition, 70:60\u201374, 2017.\\n\\n[10] Alex Graves, Santiago Fernandez, Faustino Gomez, and Jurgen Schmidhuber. Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd International Conference on Machine Learning, pages 369\u2013376, 2006.\\n\\n[11] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE International Conference on Computer Vision, pages 2961\u20132969, 2017.\\n\\n[12] Tong He, Zhi Tian, Weilin Huang, Chunhua Shen, Yu Qiao, and Changming Sun. An end-to-end textspotter with explicit alignment and attention. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5020\u20135029, 2018.\\n\\n[13] Jie Hu, Liujuan Cao, Yao Lu, ShengChuan Zhang, Yan Wang, Ke Li, Feiyue Huang, Ling Shao, and Rongrong Ji. Istr: End-to-end instance segmentation with transformers. arXiv preprint arXiv:2105.00637, 2021.\\n\\n[14] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Reading text in the wild with convolutional neural networks. International Journal of Computer Vision, 116(1):1\u201320, 2016.\\n\\n[15] Max Jaderberg, Karen Simonyan, Andrew Zisserman, and Koray Kavukcuoglu. Spatial transformer networks. MIT Press, 2015.\\n\\n[16] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Deep features for text spotting. In European Conference on Computer Vision, pages 512\u2013528. Springer, 2014.\\n\\n[17] Xu Jia, Bert De Brabandere, Tinne Tuytelaars, and Luc V Gool. Dynamic filter networks. Advances in Neural Information Processing Systems, 29:667\u2013675, 2016.\\n\\n[18] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al. Icdar 2015 competition on robust reading. In 2015 13th International Conference on Document Analysis and Recognition (ICDAR), pages 1156\u20131160. IEEE, 2015.\\n\\n[19] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almazan Almazan, and Lluis Pere De Las Heras. Icdar 2013 robust reading competition. In 2013 12th International Conference on Document Analysis and Recognition, pages 1484\u20131493. IEEE, 2013.\\n\\n[20] Hui Li, Peng Wang, and Chunhua Shen. Towards end-to-end text spotting with convolutional recurrent neural networks. In Proceedings of the IEEE International Conference on Computer Vision, pages 5238\u20135246, 2017.\\n\\n[21] M. Liao, P. Lyu, M. He, C. Yao, W. Wu, and X. Bai. Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(2):532\u2013548, 2021.\\n\\n[22] Minghui Liao, Guan Pang, Jing Huang, Tal Hassner, and Xiang Bai. Mask textspotter v3: Segmentation proposal network for robust scene text spotting. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI, pages 706\u2013722. Springer, 2020.\\n\\n[23] Minghui Liao, Baoguang Shi, and Xiang Bai. Textboxes++: A single-shot oriented scene text detector. IEEE Transactions on Image Processing, 27(8):3676\u20133690, 2018.\\n\\n[24] Minghui Liao, Baoguang Shi, Xiang Bai, Xinggang Wang, and Wenyu Liu. Textboxes: A fast text detector with a single deep neural network. In Thirty-first AAAI Conference on Artificial Intelligence, 2017.\\n\\n[25] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2117\u20132125, 2017.\"}"}
{"id": "CVPR-2022-1795", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In Proceedings of the IEEE International Conference on Computer Vision, pages 2980\u20132988, 2017.\\n\\nXuebo Liu, Ding Liang, Shi Yan, Dagui Chen, Yu Qiao, and Junjie Yan. Fots: Fast oriented text spotting with a unified network. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5676\u20135685, 2018.\\n\\nYuliang Liu, Hao Chen, Chunhua Shen, Tong He, Lianwen Jin, and Liangwei Wang. Abcnet: Real-time scene text spotting with adaptive bezier-curve network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9809\u20139818, 2020.\\n\\nYuliang Liu, Lianwen Jin, Shuaitao Zhang, Canjie Luo, and Sheng Zhang. Curved scene text detection via transverse and longitudinal sequence connection. Pattern Recognition, 90:337\u2013345, 2019.\\n\\nYuliang Liu, Chunhua Shen, Lianwen Jin, Tong He, Peng Chen, Chongyu Liu, and Hao Chen. Abcnet v2: Adaptive bezier-curve network for real-time end-to-end text spotting. arXiv preprint arXiv:2105.03620, 2021.\\n\\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In International Conference on Computer Vision (ICCV), 2021.\\n\\nPengyuan Lyu, Minghui Liao, Cong Yao, Wenhao Wu, and Xiang Bai. Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes. In Proceedings of the European Conference on Computer Vision (ECCV), pages 67\u201383, 2018.\\n\\nFausto Milletari, Nassir Navab, and Seyed-Ahmad Ahmadi. V-net: Fully convolutional neural networks for volumetric medical image segmentation. In 2016 Fourth International Conference on 3D Vision (3DV), pages 565\u2013571. IEEE, 2016.\\n\\nNibal Nayef, Fei Yin, Imen Bizid, Hyunsoo Choi, Yuan Feng, Dimosthenis Karatzas, Zhenbo Luo, Umapada Pal, Christophe Rigaud, Joseph Chazalon, et al. Icdar2017 robust reading challenge on multi-lingual scene text detection and script identification-rrc-mlt. In 2017 14th International Conference on Document Analysis and Recognition (ICDAR), volume 1, pages 1454\u20131459. IEEE, 2017.\\n\\nLuk\u00e1\u0161 Neumann and Ji\u0159\u00ed Matas. Real-time lexicon-free scene text localization and recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 38(9):1872\u20131885, 2015.\\n\\nNguyen Nguyen, Thu Nguyen, Vinh Tran, Triet Tran, Thanh Ngo, Thien Nguyen, and Minh Hoai. Dictionary-guided scene text recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\\n\\nLiang Qiao, Ying Chen, Zhanzhan Cheng, Yunlu Xu, Yi Niu, Shiliang Pu, and Fei Wu. Mango: A mask attention guided one-stage scene text spotter. In Proceedings of the Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI), pages 2467\u20132476, 2021.\\n\\nLiang Qiao, Sanli Tang, Zhanzhan Cheng, Yunlu Xu, Yi Niu, Shiliang Pu, and Fei Wu. Text perceptron: Towards end-to-end arbitrary-shaped text spotting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 11899\u201311907, 2020.\\n\\nSiyang Qin, Alessandro Bissacco, Michalis Raptis, Yasuhisa Fujii, and Ying Xiao. Towards unconstrained end-to-end text spotting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4704\u20134714, 2019.\\n\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in Neural Information Processing Systems, 28:91\u201399, 2015.\\n\\nHamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union: A metric and a loss for bounding box regression. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 658\u2013666, 2019.\\n\\nXuejian Rong, Bing Li, J Pablo Munoz, Jizhong Xiao, Aries Arditi, and Yingli Tian. Guided text spotting for assistive blind navigation in unfamiliar indoor environments. In International Symposium on Visual Computing, pages 11\u201322. Springer, 2016.\\n\\nBaoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(11):2298\u20132304, 2016.\\n\\nRussell Stewart, Mykhaylo Andriluka, and Andrew Y Ng. End-to-end people detection in crowded scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2325\u20132333, 2016.\\n\\nPeize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chencfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with learnable proposals. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14454\u201314463, 2021.\\n\\nYipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, et al. Icdar 2019 competition on large-scale street view text with partial labeling-rrc-lsvt. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1557\u20131562. IEEE, 2019.\\n\\nZhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part I, pages 282\u2013298. Springer, 2020.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems, pages 5998\u20136008, 2017.\\n\\nHao Wang, Pu Lu, Hui Zhang, Mingkun Yang, Xiang Bai, Yongchao Xu, Mengchao He, Yongpan Wang, and Wenyu\"}"}
{"id": "CVPR-2022-1795", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Liu. All you need is boundary: Toward arbitrary-shaped text spotting. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 34, pages 12160\u201312167, 2020.\\n\\nHsueh-Cheng Wang, Chelsea Finn, Liam Paull, Michael Kaess, Ruth Rosenholtz, Seth Teller, and John Leonard. Bridging text spotting and slam with junction features. In 2015 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 3701\u20133708. IEEE, 2015.\\n\\nJiapeng Wang, Chongyu Liu, Lianwen Jin, Guozhi Tang, Ji-axin Zhang, Shuaitao Zhang, Qianying Wang, Yaqiang Wu, and Mingxiang Cai. Towards robust visual information extraction in real world: New dataset and novel solution. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35, pages 2738\u20132745, 2021.\\n\\nKai Wang, Boris Babenko, and Serge Belongie. End-to-end scene text recognition. In 2011 International Conference on Computer Vision, pages 1457\u20131464. IEEE, 2011.\\n\\nPengfei Wang, Chengquan Zhang, Fei Qi, Shanshan Liu, Xiaohuiqiang Zhang, Pengyuan Lyu, Junyu Han, Jingtuo Liu, Er-rui Ding, and Guangming Shi. Pgnet: Real-time arbitrarily-shaped text spotting with point gathering network. arXiv preprint arXiv:2104.05458, 2021.\\n\\nWenhai Wang, Xuebo Liu, Xiaozhong Ji, Enze Xie, Ding Liang, ZhiBo Yang, Tong Lu, Chunhua Shen, and Ping Luo. Ae textspotter: Learning visual and linguistic representation for ambiguous text spotting. In European Conference on Computer Vision, pages 457\u2013473. Springer, 2020.\\n\\nWenhai Wang, Enze Xie, Xiang Li, Xuebo Liu, Ding Liang, Yang Zhibo, Tong Lu, and Chunhua Shen. Pan++: Towards efficient and accurate end-to-end spotting of arbitrarily-shaped text. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2021.\\n\\nWenhai Wang, Enze Xie, Xiaoge Song, Yuhang Zang, Wenjia Wang, Tong Lu, Gang Yu, and Chunhua Shen. Efficient and accurate arbitrary-shaped text detection with pixel aggregation network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 8440\u20138449, 2019.\\n\\nYuxin Wang, Hongtao Xie, Shancheng Fang, Jing Wang, Shenggao Zhu, and Yongdong Zhang. From two to one: A new scene text recognizer with visual language modeling network. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 14194\u201314203, 2021.\\n\\nSvante Wold, Kim Esbensen, and Paul Geladi. Principal component analysis. Chemometrics and Intelligent Laboratory Systems, 2(1-3):37\u201352, 1987.\\n\\nHaiping Wu, Bin Xiao, Noel Codella, Mengchen Liu, Xiyang Dai, Lu Yuan, and Lei Zhang. Cvtt: Introducing convolutions to vision transformers. arXiv preprint arXiv:2103.15808, 2021.\\n\\nLinjie Xing, Zhi Tian, Weilin Huang, and Matthew R Scott. Convolutional character networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9126\u20139136, 2019.\\n\\nJianwei Yang, Chunyuan Li, Pengchuan Zhang, Xiyang Dai, Bin Xiao, Lu Yuan, and Jianfeng Gao. Focal self-attention for local-global interactions in vision transformers. arXiv preprint arXiv:2107.00641, 2021.\\n\\nDeli Yu, Xuan Li, Chengquan Zhang, Tao Liu, Junyu Han, Jingtuo Liu, and Errui Ding. Towards accurate scene text recognition with semantic reasoning networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12113\u201312122, 2020.\\n\\nFisher Yu and Vladlen Koltun. Multi-scale context aggregation by dilated convolutions. In ICLR, 2016.\\n\\nChongsheng Zhang, Yuefeng Tao, Kai Du, Weiping Ding, Bin Wang, Ji Liu, and Wei Wang. Character-level street view text spotting based on deep multi-segmentation network for smarter autonomous driving. IEEE Transactions on Artificial Intelligence, pages 1\u20131, 2021.\\n\\nPeng Zhang, Yunlu Xu, Zhanzhan Cheng, Shiliang Pu, Jing Lu, Liang Qiao, Yi Niu, and Fei Wu. Trie: End-to-end text reading and information extraction for document understanding. In Proceedings of the 28th ACM International Conference on Multimedia, pages 1413\u20131422, 2020.\\n\\nRui Zhang, Yongsheng Zhou, Qianyi Jiang, Qi Song, Nan Li, Kai Zhou, Lei Wang, Dong Wang, Minghui Liao, Mingkun Yang, et al. Icdar 2019 robust reading challenge on reading chinese text on signboard. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1577\u20131581. IEEE, 2019.\\n\\nHumen Zhong, Jun Tang, Wenhai Wang, Zhibo Yang, Cong Yao, and Tong Lu. Arts: Eliminating inconsistency between text detection and recognition with auto-rectification text spotter. arXiv preprint arXiv:2110.10405, 2021.\\n\\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\"}"}
{"id": "CVPR-2022-1795", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"SwinTextSpotter: Scene Text Spotting via Better Synergy between Text Detection and Text Recognition\\n\\nMingxin Huang\\nYuliang Liu\\nZhenghao Peng\\nChongyu Liu\\nDahua Lin\\nShenggao Zhu\\nNicholas Yuan\\nKai Ding\\nLianwen Jin\\n\\n1 South China University of Technology\\n2 Chinese University of Hong Kong\\n3 Huawei Cloud AI\\n4 IntSig Information Co., Ltd\\n5 Peng Cheng Laboratory\\n\\neelwjin@scut.edu.cn\\n\\nAbstract\\nEnd-to-end scene text spotting has attracted great attention in recent years due to the success of excavating the intrinsic synergy of the scene text detection and recognition. However, recent state-of-the-art methods usually incorporate detection and recognition simply by sharing the backbone, which does not directly take advantage of the feature interaction between the two tasks. In this paper, we propose a new end-to-end scene text spotting framework termed SwinTextSpotter. Using a transformer encoder with dynamic head as the detector, we unify the two tasks with a novel Recognition Conversion mechanism to explicitly guide text localization through recognition loss. The straightforward design results in a concise framework that requires neither additional rectification module nor character-level annotation for the arbitrarily-shaped text. Qualitative and quantitative experiments on multi-oriented datasets RoIC13 and ICDAR 2015, arbitrarily-shaped datasets Total-Text and CTW1500, and multi-lingual datasets ReCTS (Chinese) and VinText (Vietnamese) demonstrate SwinTextSpotter significantly outperforms existing methods. Code is available at https://github.com/mxin262/SwinTextSpotter.\\n\\n1. Introduction\\nScene text spotting, which aims to detect and recognize the entire word or sentence in natural images, has raised a lot of attention due to its wide range of applications in autonomous driving [64], intelligent navigation [42, 50], and key entities recognition [51, 65], etc. Traditional scene text spotting methods treat detection and recognition as two separate tasks and adopt a pipeline that first localizes and crops the text regions on the input images and then predicts the text sequence by feeding the cropped regions into text recognizer [9, 14, 16, 23, 35]. Such a pipeline may have some limitations, such as (1) error accumulation between these two tasks, e.g., imprecise detection result may heavily hinder the performance of text recognition; (2) separate optimization of the two tasks might not maximize the final performance of text spotting; (3) intensive memory consumption and low inference efficiency.\\n\\nTherefore, many methods [12, 20, 27, 32] attempt to solve text spotting in end-to-end systems, i.e., optimizing detection and recognition jointly in unified architectures. The recognizer can improve the performance of the detector by eliminating the false positive detection results [20, 21]. In turn, even if the detection is not precise, the recognizer can still correctly predict the text sequence by the large receptive field.\\n\\nFigure 1. Effectiveness of Recognition Conversion. The proposed Recognition Conversion explicitly guides the detection, leading to better text spotting performance.\"}"}
{"id": "CVPR-2022-1795", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Another advantage is that an end-to-end system is easier to maintain and transfer to new domains compared to a cascaded pipeline where the model is coupled with the data and thus requires substantial engineering efforts.\\n\\nHowever, there are two limitations in most of the existing end-to-end scene text spotting systems. First, if the detection is simply based on the visual information in the input features, the detector is prone to be distracted by background noise and proposes inconsistent detection, as depicted in Figure 1(a). The interaction between texts in the same image is the crucial factor to eliminate the impact of background noise, since different characters of the same word may contain strong similarities, such as the backgrounds and text styles. Using Transformer can learn rich interactions between text instances. For example, Yu et al. use transformer to make texts interact with each other at semantic level. Fang et al. and Wang et al. further adopt transformer to model the visual relationship between texts. Second, the interactions between detection and recognition is not enough by sharing backbone because neither the recognition loss optimizes the detector nor the recognizer utilizes the detection features. To jointly improve detection and recognition, a character segmentation map is designed by Mask TextSpotter, which simultaneously optimizes the detection and recognition results in the same branch; ABC-Net v2 proposes Adaptive End-to-End Training (AET) strategy using the detection results to extract recognition features instead of only using the ground truths; ARTS improves the performance of the end-to-end text spotting by back-propagating the loss from the recognition branch to the detection branch using a differentiable Spatial Transformer Network (STN). However, these three methods assume the detector proposes text features structurally, e.g. in the reading order. The overall performance of the text spotting is thereafter bounded by the detector.\\n\\nWe propose SwinTextSpotter, an end-to-end trainable Transformer-based framework, stepping toward better synergy between the text detection and recognition. To better distinguish the densely scattered text instances in crowded scenes, we use Transformer and a two-level self-attention mechanism in SwinTextSpotter, stimulating the interactions between the text instances. Addressing the challenge in arbitrarily-shaped scene text spotting, inspired by [13, 45], we regard text detection task as a set-prediction problem and thus adopt a query-based text detector. We further propose Recognition Conversion (RC), which implicitly guides the recognition head through incorporating the detection features. RC can back-propagate recognition information to the detector and suppress the background noise in the features for recognition, leading to the joint optimization of the detector and recognizer. Empowered by the proposed RC, SwinTextSpotter has a concise framework without the character-level annotation and rectification module used in previous works to improve the recognizer. SwinTextSpotter has superior performance in both the detection and the recognition. As illustrated in Figure 1(b), the detector of SwinTextSpotter can accurately localize difficult samples. On the other hand, more accurate detection features can improve the recognizer and result in faster convergence and better performance, as shown in Figure 1(c).\\n\\nWe conduct extensive experiments on six benchmarks, including multi-oriented dataset RoIC13 and ICDAR 2015, arbitrarily-shaped dataset Total-Text and SCUT-CTW1500, and multilingual dataset ReCTS (Chinese) and VinText (Vietnamese). The results demonstrate the superior performance of the SwinTextSpotter: (1) SwinTextSpotter achieves 88.0% F-measure for the detection task on SCUT-CTW1500 and Total-Text, exceeding previous methods by a large margin; (2) SwinTextSpotter significantly outperforms ABCNet v2 by 9.8% in terms of 1-NED for the text spotting task in ReCTs dataset. Additionally, without using character-level annotation on ReCTs, SwinTextSpotter outperforms previous state-of-the-art methods MaskTextSpotter and AE TextSpotter that use such annotation; (3) SwinTextSpotter shows better robustness for the extremely rotated instances on RoIC13 dataset compared to MaskTextSpotter v3.\\n\\nThe main contributions of this work are summarized as follows.\\n\\n\u2022 SwinTextSpotter groundbreakingly shows that Transformer and the set-prediction scheme are effective in end-to-end scene text spotting.\\n\u2022 SwinTextSpotter adopts the Recognition Conversion to exploit the synergy of text detection and recognition.\\n\u2022 SwinTextSpotter is a concise framework that does not require character-level annotation as well as specifically designed rectification module for recognizing arbitrarily-shaped text.\\n\u2022 SwinTextSpotter achieves state-of-the-art performance on multiple public scene text benchmarks.\\n\\n2. Related Work\\n\\nSeparate Scene Text Spotting. In past decades, the emergence of deep learning approaches greatly promote the development of scene text spotting. Wang et al. use a sliding-window-based detector to detect characters and then classify each character. Bissacco et al. combine DNN and HOG features and build a text extraction system by using characters classification. Liao et al. propose the TextBoxes that incorporates the single-shot detector and a text recognizer in two-stage manner. However, the aforementioned methods treat detection and recognition as...\"}"}
{"id": "CVPR-2022-1795", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2. The framework of the proposed SwinTextSpotter. The gray arrows denote the feature extraction from images. The green arrows and orange arrows represent the detection stage and the recognition stage, respectively. The outputs of detection head are refined in $K$ stages. The output detection in the $K$th stage serves as the input to the recognition stage.\\n\\nEnd-to-End Text Spotting. Recently, researchers try to combine detection and recognition into one system. Li et al. [20] unify the detection and recognition into an end-to-end trainable scene text spotting framework. FOTS [27] uses a one stage detector to generate rotated boxes and adopts RoIRotate to sample the oriented text feature into horizontal grid for connecting the detection and recognition. He et al. [12] propose a similar framework using an attention-based recognizer.\\n\\nFor the task of arbitrarily-shaped scene text spotting, Mask TextSpotter series [21, 22, 32] solve the problem without explicit rectification by using character segmentation branch to improve the performance of the recognizer. TextDragon [8] combines the two tasks by RoISlide, a technique that transforms the predicting segments of the text instances into horizontal features. Wang et al. [49] adopt Thin-Plate-Spline [3] transformation to rectify the features. ABCNet [28] and its improved version ABCNet v2 [30] use the BezierAlign to transform the arbitrary-shape texts into regular ones. These methods achieve great progress by using rectification module to unify detection and recognition into end-to-end trainable systems. Qin et al. [39] propose RoI Masking to extract the feature for arbitrarily-shaped text recognition. Similar to [39], PAN++ [55] is based on a faster detector [56]. AE TextSpotter [54] uses the results of recognition to guide detection through language model. Though achieve significantly improvement on the performance of text spotting by sharing backbone, the aforementioned methods neither back-propagate recognition loss to the detector nor use detection features in the recognizer. The detector and the recognizer thus are still relatively independent to each other without joint optimization. Recently, Zhong et al. [67] propose ARTS which passes the gradient of recognition loss to the detector using Spatial Transform Network (STN) [15], demonstrating the power of synergy between the detection and recognition in text spotting.\\n\\n3. Methodology\\n\\nThe overall architecture of SwinTextSpotter is presented in Figure 2, which consists of four components: (1) a backbone based on Swin-Transformer [31]; (2) a query-based text detector; (3) a Recognition Conversion module to bridge the text detector and recognizer; and (4) an attention-based recognizer.\\n\\nAs illustrated in the green arrows of Figure 2, in the first stage of detection, we first randomly initialize trainable parameters to be the boxes $bbox_0$ and proposal features $f_{prop_0}$. To make the proposal features contain global information, we use global average pooling to extract the image features and add them into $f_{prop_0}$. We then extract the RoI features using $bbox_0$. The RoI features and $f_{prop_0}$ are fed into the Transformer encoder with dynamic head. The output of the Transformer encoder is flattened and forms the proposal features $f_{prop_1}$, which will be fed into the detection head to output the detection result. The box $bbox_{k-1}$ and proposal feature $f_{prop_{k-1}}$ will serve as the input to later $k$th stage of detection. The proposal feature $f_{prop_k}$ recurrently updates itself by fusing the RoI features with previous $f_{prop_{k-1}}$, which makes proposal features preserve the information from previous stages. We repeat such refinement for totally $K$ stages, resembling the iterative structure in the query-based detector [4,13,45,68]. Such design allows more robust detection in sizes and aspect ratios [45]. More details of the detector are explained in Section 3.2.\\n\\nSince the recognition stage (orange arrows) requires higher rate of resolution than detection, we use the final detection stage output box $bbox_K$ to obtain the RoI features whose resolution is four times as much as that in the detection stage. In order to keep the resolution of features consistent with the detector when fused with proposal features, we down-sample the RoI features to get three feature maps of descending sizes, denoting by $\\\\{a_1, a_2, a_3\\\\}$. Then we obtain detection features $f_{det}$ by fusing the smallest $a_3$ and the proposal features $f_{prop_{K}}$. The detection features $f_{det}$ in recognition stage contain all previous detection information. Finally the $\\\\{a_1, a_2, a_3\\\\}$ and the detection features $f_{det}$\"}"}
{"id": "CVPR-2022-1795", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"3.1. Dilated Swin-Transformer\\n\\nVanilla convolutions operate locally at fixed size (e.g. $3 \\\\times 3$), which causes low efficacy in connecting remote features. For text spotting, however, modeling the relationships between different texts is critical since scene texts from the same image share strong similarities, such as their backgrounds and text styles. Considering the global modeling capability and computational efficiency, we choose Swin-Transformer [31] with a Feature Pyramid Network (FPN) [25] to build our backbone. Given the blanks existing between words in a line of text, the receptive field should be large enough to help distinguish whether adjacent texts belong to the same text line. To achieve such receptive field, as illustrated in Figure 3, we incorporate two dilated convolution layers [63], one vanilla convolution layer and one residual structure into the original Swin-Transformer, which also introduce the properties of CNN to Transformer [59].\\n\\n3.2. A Query Based Detector\\n\\nWe use a query based detector to detect the text. Based on Sparse R-CNN [45], the query based detector is built on ISTR [13] which treats detection as a set-prediction problem. Our detector uses a set of learnable proposal boxes, alternative to replace massive candidates from the RPN [40], and a set of learnable proposal features, representing high-level semantic vectors of objects. The detector is empirically designed to have six query stages. With the Transformer encoder with dynamic head, latter stages can access the information in former stages stored in the proposal features [17, 45, 47]. Through multiple stages of refinement, the detector can be applied to text at any scale.\\n\\nThe architecture of the detection head in $k$th stage is illustrated in Figure 4. The proposal features in $(k-1)$ stage is represented by $f_{prop}^{k-1} \\\\in \\\\mathbb{R}^{N,d}$. At the stage $k$, the proposal $f_{prop}^{k}$ will serve as the input to next stage. The features $f_{prop}^{k-1}$ produced in previous stage is fed into a self-attention module $\\\\text{MSA}_k$ to model the relationships and generate two sets of convolutional parameters. The detection information in previous stages is therefore embedded into the convolutions. The convolutions conditioned on the previous proposal features is used to encode the RoI features. The RoI features are extracted by $bbox^{k-1}$, the detection result in previous stage, using RoIAlign [11]. The output features of the convolutions is fed into a linear projection layer to produce the $f_{prop}^{k}$ for next stage. The $f_{prop}^{k}$ is subsequently fed the into prediction head to generate $bbox^k$ and $mask^k$. To reduce computation, the 2D mask is transformed into 1D mask vector by the Principal Component Analysis [58] so the $mask^k$ is a one-dimensional vector. When $k = 1$, the $bbox^0$ and $f_{prop}^0$ are randomly initialized parameters, which is the input of the first stage. During training, these parameters are updated via back propagation and learn the inductive bias of the high-level semantic features of text.\\n\\nWe view the text detection task as a set-prediction problem. Formally, we use the bipartite match to match the predictions and ground truths [4, 13, 44, 45]. The matching cost becomes:\\n\\n$$L_{\\\\text{match}} = \\\\lambda_{\\\\text{cls}} \\\\cdot L_{\\\\text{cls}} + \\\\lambda_{L_1} \\\\cdot L_{L_1} + \\\\lambda_{\\\\text{giou}} \\\\cdot L_{\\\\text{giou}} + \\\\lambda_{\\\\text{mask}} \\\\cdot L_{\\\\text{mask}},$$\\n\\n(1)\\n\\nwhere $\\\\lambda$ is the hyper-parameter used to balance the loss. $L_{\\\\text{cls}}$ is the focal loss [26]. The losses for regressing the bounding boxes are $L_1$ loss $L_{L_1}$ and generalized IoU loss $L_{\\\\text{giou}}$ [41]. We compute the mask loss $L_{\\\\text{mask}}$ following [13], which calculates the cosine similarity between the prediction mask and ground truth. The detection loss is similar to the matching cost but we use the $L_2$ loss and dice loss [33] to replace the cosine similarity as in [13].\\n\\n3.3. Recognition Conversion\\n\\nTo better coordinate the detection and recognition, we propose Recognition Conversion (RC) to spatially inject the\"}"}
