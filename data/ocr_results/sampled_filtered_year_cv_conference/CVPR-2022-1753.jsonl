{"id": "CVPR-2022-1753", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nVariational autoencoders (VAEs) have witnessed great success in performing the compression of image datasets. This success, made possible by the bits-back coding framework, has produced competitive compression performance across many benchmarks. However, despite this, VAE architectures are currently limited by a combination of coding practicalities and compression ratios. That is, not only do state-of-the-art methods, such as normalizing flows, often demonstrate out-performance, but the initial bits required in coding makes single and parallel image compression challenging. To remedy this, we introduce Split Hierarchical Variational Compression (SHVC). SHVC introduces two novelties. Firstly, we propose an efficient autoregressive prior, the autoregressive sub-pixel convolution, that allows a generalisation between per-pixel autoregressions and fully factorised probability models. Secondly, we define our coding framework, the autoregressive initial bits, that flexibly supports parallel coding and avoids \u2013 for the first time \u2013 many of the practicalities commonly associated with bits-back coding. In our experiments, we demonstrate SHVC is able to achieve state-of-the-art compression performance across full-resolution lossless image compression tasks, with up to 100x fewer model parameters than competing VAE approaches.\\n\\n1. Introduction\\n\\nThe volume of data, measured in terms of IP traffic, is currently witnessing an exponential year-on-year growth [9]. Fuelled by the demand for high-resolution media content, it is estimated that 80% of this data is in the form of images and video [9]. Data service providers, such as cloud and streaming platforms, have consequently seen costs associated with transmission and storage become prohibitively expensive. For example, an increased demand for streaming services forced major providers to throttle the maximum resolution of video content to 720p during the coronavirus pandemic. As such, these challenges have renewed the need for the development of high-performance data compression codecs.\\n\\nOne solution to this problem has been the development of approaches using likelihood-based generative models capable of discrete density estimation [3,6,13,14,18,22,24,35,36,42,43]. Such methods operate by learning a deep probabilistic model of the data distribution, which, in combination with entropy coders, can be used to compress data. Here, according to Shannon\u2019s source coding theorem [21], the minimal required average codelength is bounded by the expected negative log-likelihood of the data distribution.\\n\\nFrom this family of generative models, there have emerged three dominant modes for data compression: normalizing flows [3,14,42,43], variational autoencoders [18,24,36] and autoregressive models [15, 31, 37]. In fact, each of these approaches can be thought of as a traversal on the Pareto frontier of inference speed and compression performance. With broad generality, autoregressive models can often be the most powerful but the slowest; variational autoencoders are often the weakest but the fastest; and normalizing flows \u2013 depending on the variant \u2013 sit somewhere in between.\\n\\nIn this paper, we consider data compression with VAEs, and focus on extending the efficient frontier; obtaining solutions faster than popular VAEs that achieve state-of-the-art compression ratios. Use of VAEs, however, poses two outstanding challenges. Firstly, we should achieve competitive coding ratios without greatly sacrificing time complexity. For example, best iterates currently require one of two ingredients to improve performance: building either a deep hierarchy of latent variables [8] or use of autoregressive priors [11,29]. The latter idea, especially popular in the codecs of the lossy compression community [25], posits a model that flexibly learns both local (via autoregression) and global (via hierarchical latent representation) data modalities (e.g. low-frequency information). Whilst these approaches, such as MS-PixelCNN [29] and PixelVAE [11], have had some success in achieving more efficient trade-offs, generation of even moderately sized images is still to the order of minutes [22].\\n\\nSecondly, there should exist a practical means by which to efficiently perform single-image compression. Single-image compression then permits parallel coding, which is highly...\"}"}
{"id": "CVPR-2022-1753", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"desirable. However, translating VAE into a lossless codec is currently achieved using the bits-back coding framework (predominantly, bits-back ANS), which requires a large number of initial bits \\\\[12,36,39\\\\] (see Section 3.1). Whilst this is a trivial number of bits on large image datasets (where we can amortize this cost), it renders bits-back an impractical approach for single-image compression. Furthermore, even large datasets are often coded such that images are interlinked. Access to a single image in the middle of a sequence would therefore require all prior images in the bitstream to be additionally decompressed.\\n\\nTo that end, we propose two novelties for use in VAE-based compression designed to address these challenges. The first, our autoregressive sub-pixel convolution, introduces a simple autoregressive factorisation \u2013 not dissimilar from the transformations used in normalizing flows \\\\[3,42,43\\\\] \u2013 designed to present an efficient interpolation between fully-factorised probability distributions and the impractical per-pixel autoregressions. Built from a modified space-to-depth convolution operator, we losslessly downsample data variables before performing a computationally efficient autoregression along the channel dimension. Our autoregressive operator is then advantaged by a number of network evaluations invariant to data dimensions, with each autoregression crucially performed on a downsampled version of the input tensor. More broadly, we view this framework as a generalisation of many popular autoregressive \u201ccontext\u201d models used in data compression \\\\[11,26,31,41\\\\].\\n\\nOur second contribution, autoregressive initial bits, presents a general framework for avoiding the impracticalities of bits-back ANS, allowing for eminently parallelizable coding. This technique, highly compatible with our autoregressive model, partitions the data variable into two splits such that the second partition is conditionally independent of the latent variable(s), given the first. In this way, we illustrate how we can use the entropy coding of the conditionally independent partition to both supply and remove the initial bits necessitated by bits-back ANS. We demonstrate that this approach reduces the bit overhead on a per-image basis by close to 20x.\\n\\nFinally, we combine the above contributions to present our codec, Split Hierarchical Variational Compression (SHVC). SHVC posits a hierarchical VAE of general-form autoregressive priors that permits parallel coding. Using our framework, we outperform all other VAE-based compression approaches with fewer latent variables and a comparable number of neural network evaluations. We further illustrate the effectiveness of our architecture by training a small model which is able to outperform similar VAE approach Bit-Swap \\\\[18\\\\] \u2013 but with 100x fewer model parameters.\"}"}
{"id": "CVPR-2022-1753", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Encode $x_{s+1:2}$ with $p(x_{s+1:2}|x_{1:s})$\\n\\nDecode $z$ with $q(z|x)$\\n\\nEncode $x$ with $p(x|z)$\\n\\nEncode $z$ with $p(z)$\\n\\nInitial Bits\\n\\nEncoder\\n\\nDecode $z$ with $q(z|x)$\\n\\nDecode $x_{s+1:2}$ with $p(x_{s+1:2}|x_{1:s})$\\n\\nRemove Initial Bits\\n\\nDecoder\\n\\nFigure 1. Left: The bb-ANS coding framework as discussed in Section 3.1. Right: The coding framework of ArIB as proposed in Section 4.2.\\n\\nHere we nest a bb-ANS coder inside of a block-based autoregressive structure that removes the need for initial bits.\\n\\n3. Background\\n\\nSuppose access to a dataset of size $n$, $\\\\{x_1, x_2, \\\\ldots, x_n\\\\}$, drawn from some intractable $p_{data}(x)$ that we wish to compress. In order to achieve this, we introduce a discrete probability distribution $p(x)$ that, in combination with entropy coding, requires a codelength of $P_n = \\\\sum_{i=1}^{n} -\\\\log_2 p(x_i)$ bits to represent.\\n\\nIdeally, $p(x)$ should closely resemble $p_{data}(x)$. In such a case, the average codelength in the limit of $n \\\\to \\\\infty$ is given by $E_{p_{data}}[-\\\\log_2 p(x)] \\\\to H(x)$, where $H(x)$ is the entropy of the data. Here, the compression scheme is said to be optimal under Shannon's source coding theorem [32].\\n\\n3.1. Bits-back ANS for V AEs\\n\\nAs discussed in Section 1, variational autoencoders (V AEs) are one popular approach to estimating $p_{data}(x)$, which defines a latent variable model such that $p(x) = \\\\int p(x|z) p(z) dz = \\\\int p(x|z) p(z|x) p(z) dz$, (1) where $p(z)$ is the prior distribution over latent variable $z$. As $p(x)$ is normally intractable, V AEs introduce an approximate posterior $q(z|x)$, which is optimised to maximise a lower bound on the marginal evidence, the Evidence Lower Bound (ELBO) $\\\\log p(x) \\\\geq L = E_{q} - \\\\log q(z|x) + \\\\log p(x|z)p(z)$, (2)\\n\\nwhere low-variance estimates of the expectation in (2) are via Monte-Carlo integration and the reparametrization trick [17].\\n\\nEntropy coding requires explicit probabilities of data symbols. However, in V AEs, the model is factorised into a prior and a likelihood, and therefore does not allow direct coding of the data. To remedy this, several authors have proposed coding variants of bb-ANS [18, 35, 36]. This process is outlined as follows, which, without loss of generality, we describe for a model with a single latent variable. During compression, one decodes $z$ from some auxiliary initial bits with $q(z|x)$; encodes $x$ with $p(x|z)$; and encodes $z$ with $p(z)$ to obtain the complete bitstream. In the decompression stage, one decodes $z$ from the bitstream with $p(z)$; decodes $x$ with $p(x|z)$; encodes $z$ with $q(z|x)$ and thus returns the initial bits (hence bits-back coding). This technique is visualised in Fg. 1 Left.\\n\\nThe first decoding step common in existing bb-ANS codecs requires access to an initial bitstream. This requirement leads to several disadvantages when compared to AC. Firstly, while the initial bits are returned after decompression, the same bits are occupied and not readily readable beforehand. Secondly, although we can amortize the cost of the initial bits across a large dataset by chaining the compressed data, access to any given data point requires decompression of all data points posterior to the one of our target in the original data sequence. As such, compression of a single image carries substantial overhead \u2013 and, by extension, so too do parallel coding implementations.\\n\\n3.2. Deterministic vs Stochastic Posterior Sampling\\n\\nWithin V AEs based lossless compression, the impracticalities associated with bits-back coding are not the only choice. Indeed, approaches that use \u201cdeterministic\u201d posterior sampling [22] may eschew bb-ANS for AC. This approach is almost ubiquitous in the lossless codecs of lossy approaches [2,7,25,26], where eminently parallelizable, low-latency codecs are especially preferable (e.g. streaming media). We note that when using deterministic posterior sampling, the likelihoods associated with the prior in Eq.(2) trivially cancel to zero, such that the objective resolves to maximum likelihood estimation of the\"}"}
{"id": "CVPR-2022-1753", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Average BPD savings against the number of latent variables ($L$ = 1,...,5) used in three VAE lossless compression models. The baseline is the deterministic posterior with $L$ = 1 where the BPD saving is zero. Results are presented as an average for the three approaches trained across CIFAR10, ImageNet32 and ImageNet64.\\n\\nWhilst gaining notable practical coding advantages, in sacrificing a stochastic posterior one also sacrifices the ability to minimize the cost of sending latent variables. To offset this limitation, models will repeatedly downsample (RDS) the number of symbols available to latent representations between each layer. Whilst this limits posterior expressiveness, there is little experimental evidence to support how much this matters in practice. In addition, models with stochastic posteriors require large $L$ to excel (where $L$ is the number of latent variables), which hinders run-time.\\n\\nTo that end, we display the results of a simple experiment in Fig. 2 designed to investigate this difference further. Here we train three VAEs across CIFAR10, ImageNet32 and ImageNet64: two with stochastic posteriors (one with and one without RDS) and a deterministic posterior (with RDS). The architectures in each model are identical (with the exception of downsampling operations) and we quantify compression performance in bits per dimension (BPD). Further experimental details can be found in the Appendix. Here we observe that, even with $L \\\\leq 3$, both stochastic posteriors outperform by $\\\\sim 5\\\\%$. This difference grows as $L$ increases \u2013 but it does not matter if RDS is used (at least, for $L \\\\leq 5$). This result is of important consequence: the best current approach to avoid the impracticalities of bb-ANS (ie. using a deterministic posterior) carries a 5% BPD penalty. For single-image compression with stochastic posteriors, the initial bits required would typically be much larger than this. Likewise, unless extending to a deep hierarchy of latent variables, RDS seems like a compute-efficient choice that does not limit performance.\\n\\n4. Method\\n\\nOur method posits a hierarchical VAE where we parameterize the priors using an autoregressive factorisation. We begin by defining a lossless downsampling convolution operator, before describing its application to density estimation using both weak and strong autoregressive models. We then describe how this autoregressive structure can be leveraged to avoid many of the challenges associated with bb-ANS without sacrificing the performance of stochastic posteriors. Finally, we describe how these contributions can be combined to form our SHVC codec.\\n\\n4.1. Autoregressive Sub-Pixel Convolutions\\n\\nThe space-to-depth and depth-to-space transformations are popular operations across image analysis, from generative modelling [3,14] to super-resolution [33]. They define adjacent operations for efficient up and downsampling transformations by folding spatial dimensions into channel dimensions \u2013 and vice versa. Unlike learned operations, they greatly reduce computational complexity, allowing for greater parallelism by losslessly moving computation (and data) into the channels. Indeed, these operations have become an essential component in papers seeking real-time execution (e.g. [10, 19, 30, 33]).\\n\\nSpecifically, given a tensor of $C$ channels, $H$ height and $W$ width, we define the space-to-depth and depth-to-space transformations, $f$ and $f^{-1}$, such that\\n\\n$$f : \\\\mathbb{R}^{C \\\\times H \\\\times W} \\\\to \\\\mathbb{R}^{C k^2 \\\\times H_k \\\\times W_k}, \\\\quad (3)$$\\n\\n$$f^{-1} : \\\\mathbb{R}^{C \\\\times H \\\\times W} \\\\to \\\\mathbb{R}^{C k^2 \\\\times H_k \\\\times W_k}, \\\\quad (4)$$\\n\\nwhere $k$ is the scale factor. As described in [33], these operations can be efficiently performed using sub-pixel convolutions, which are referred to as pixel unshuffle and pixel shuffle. In particular, their space-to-depth transformation, pixel unshuffle, is performed using a $k$-stride depthwise convolution where the $n$th element of $C_k^2$ filters has one non-zero element such that\\n\\n$$K(n)_{h,w} = \\\\begin{cases} 1 & \\\\text{if} \\\\quad h = \\\\left\\\\lfloor \\\\frac{n}{k} \\\\right\\\\rfloor \\\\mod w, \\\\\\\\ 0 & \\\\text{else}, \\\\end{cases} \\\\quad (5)$$\\n\\nwhere $h,w$ are the indices over spatial dimensions. The result of this operation is visualised in Fig. 3 Centre.\\n\\nDefining a channel-wise autoregression over the resulting tensor would posit a checkerboard autoregressive structure over each of the channels in the original tensor, sequentially. However, as identified in PixelCNN++ [31], sub-pixels in adjacent channels, sharing the same spatial location in the original tensor, have high correlation and therefore do not require complex models to describe the dependency structure. As such, the authors of PixelCNN++ use a linear model predicted by a single network evaluation, conditioned on decoded context, to define the joint distribution across channels. In this way, they obfuscate the need for separate RGB network evaluations. (We note that in our setting, context refers to previously decoded pixels in either the current or previous hierarchical latent variable.) From henceforth what we refer to as a weak autoregression, is then defined similarly to [31] according to\\n\\n$$\\\\text{...}$$\"}"}
{"id": "CVPR-2022-1753", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Jyrki Alakuijala, Ruud van Asseldonk, Sami Boukortt, Martin Bruse, Iulia-Maria Coms, Moritz Firsching, Thomas Fischerbacher, Evgenii Kliuchnikov, Sebastian Gomez, Robert Obryk, et al. Jpeg xl next-generation image compression architecture and coding tools. In *Applications of Digital Image Processing XLII*, volume 11137, page 111370K. International Society for Optics and Photonics, 2019.\\n\\n[2] Johannes Ball\u00e9, David Minnen, Saurabh Singh, Sung Jin Hwang, and Nick Johnston. Variational image compression with a scale hyperprior. In *International Conference on Learning Representations*, 2018.\\n\\n[3] Rianne van den Berg, Alexey A Gritsenko, Mostafa Dehghani, Casper Kaae S\u00f8nderby, and Tim Salimans. IDF++: Analyzing and improving integer discrete flows for lossless compression. In *International Conference on Learning Representations*, 2021.\\n\\n[4] Thomas Boutell and T Lane. Png (portable network graphics) specification version 1.0. Network Working Group, pages 1\u2013102, 1997.\\n\\n[5] Samuel R Bowman, Luke Vilnis, Oriol Vinyals, Andrew M Dai, Rafal Jozefowicz, and Samy Bengio. Generating sentences from a continuous space. 2016.\\n\\n[6] Sheng Cao, Chao-Yuan Wu, and Philipp Kr\u00e4henb\u00fchl. Lossless image compression through super-resolution. *arXiv preprint arXiv:2004.02872*, 2020.\\n\\n[7] Zhengxue Cheng, Heming Sun, Masaru Takeuchi, and Jiro Katto. Learned image compression with discretized gaussian mixture likelihoods and attention modules. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2020.\\n\\n[8] Rewon Child. Very deep VAEs generalize autoregressive models and can outperform them on images. In *International Conference on Learning Representations*, 2021.\\n\\n[9] Cisco. Cisco visual networking index: global mobile data traffic forecast update, 2018\u20132023. *Update*, 2018:2023, 2020.\\n\\n[10] Tiago Cortinhal, George Tzelepis, and Eren Erdal Aksoy. Salsanext: fast, uncertainty-aware semantic segmentation of lidar point clouds for autonomous driving. *arXiv preprint arXiv:2003.03653*, 2020.\\n\\n[11] Ishaan Gulrajani, Kundan Kumar, Faruk Ahmed, Adrien Ali Taiga, Francesco Visin, David V\u00e1zquez, and Aaron Courville. Pixelvae: A latent variable model for natural images. In *International Conference on Learning Representations*, 2017.\\n\\n[12] GE Hinton and Drew van Camp. Keeping neural networks simple by minimising the description length of weights. 1993. In *Proceedings of COLT-93*, pages 5\u201313.\\n\\n[13] Jonathan Ho, Evan Lohn, and Pieter Abbeel. Compression with flows via local bits-back coding. In *Advances in Neural Information Processing Systems*, pages 3879\u20133888, 2019.\\n\\n[14] Emiel Hoogeboom, Jorn Peters, Rianne van den Berg, and Max Welling. Integer discrete flows and lossless compression. In *Advances in Neural Information Processing Systems*, pages 12134\u201312144, 2019.\\n\\n[15] Heewoo Jun, Rewon Child, Mark Chen, John Schulman, Aditya Ramesh, Alec Radford, and Ilya Sutskever. Distribution augmentation for generative modeling. In *International Conference on Machine Learning*, pages 5006\u20135019. PMLR, 2020.\\n\\n[16] Diederik P Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. *Advances in Neural Information Processing Systems*, 2021.\\n\\n[17] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. *International Conference on Learning Representations*, 2014.\\n\\n[18] Friso H Kingma, Pieter Abbeel, and Jonathan Ho. Bit-swap: Recursive bits-back coding for lossless compression with hierarchical latent variables. *International Conference on Machine Learning*, 2019.\\n\\n[19] Haojie Liu, Tong Chen, Qiu Shen, Tao Yue, and Zhan Ma. Deep image compression via end-to-end learning. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops*, 2018.\\n\\n[20] James Lucas, George Tucker, Roger B Grosse, and Mohammad Norouzi. Don't blame the elbo! a linear vae perspective on posterior collapse. *Advances in Neural Information Processing Systems*, 32:9408\u20139418, 2019.\\n\\n[21] David JC MacKay and David JC MacKay. Information theory, inference and learning algorithms. Cambridge university press, 2003.\\n\\n[22] Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Practical full resolution learned lossless image compression. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 10629\u201310638, 2019.\\n\\n[23] Fabian Mentzer, Eirikur Agustsson, Michael Tschannen, Radu Timofte, and Luc Van Gool. Conditional probability models for deep image compression. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)*, 2018.\\n\\n[24] Fabian Mentzer, Luc Van Gool, and Michael Tschannen. Learning better lossless compression using lossy compression. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 6638\u20136647, 2020.\\n\\n[25] David Minnen, Johannes Ball\u00e9, and George D Toderici. Joint autoregressive and hierarchical priors for learned image compression. In *Advances in Neural Information Processing Systems*, 2018.\\n\\n[26] David Minnen and Saurabh Singh. Channel-wise autoregressive entropy models for learned image compression. In *IEEE International Conference on Image Processing*, ICIP.\\n\\n[27] Aaron van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In *Proceedings of The 33rd International Conference on Machine Learning*, 2016.\\n\\n[28] Ali Razavi, Aaron van den Oord, Ben Poole, and Oriol Vinyals. Preventing posterior collapse with delta-V AEs. In *International Conference on Learning Representations*, 2019.\\n\\n[29] Scott Reed, Aaron van den Oord, Nal Kalchbrenner, Sergio G\u00f3mez Colmenarejo, Ziyu Wang, Yutian Chen, Dan Belov, and Nando de Freitas. Parallel multiscale autoregressive density estimation. In *Proceedings of the 34th International Conference on Machine Learning*, 2017.\"}"}
{"id": "CVPR-2022-1753", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"learned flexible-rate video coding.\\n\\n[31] Tim Salimans, Andrej Karpathy, Xi Chen, and Diederik Kingma. Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications. International Conference on Learning Representations, 2017.\\n\\n[32] C. E. Shannon. A mathematical theory of communication. SIGMOBILE Mob. Comput. Commun. Rev., 2001.\\n\\n[33] Wenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1874\u20131883, 2016.\\n\\n[34] Jon Sneyers and Pieter Wuille. Flif: Free lossless image format based on maniac compression. In 2016 IEEE International Conference on Image Processing (ICIP), pages 66\u201370. IEEE, 2016.\\n\\n[35] James Townsend, Tom Bird, and David Barber. Practical lossless compression with latent variables using bits back coding. International Conference on Learning Representations, 2019.\\n\\n[36] James Townsend, Thomas Bird, Julius Kunze, and David Barber. Hilloc: Lossless image compression with hierarchical latent variable models. International Conference on Learning Representations, 2020.\\n\\n[37] Aaron van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. In Advances in Neural Information Processing Systems, pages 4790\u20134798, 2016.\\n\\n[38] Aaron van Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International Conference on Machine Learning, 2016.\\n\\n[39] Chris S Wallace. Classification by minimum-message-length inference. In International Conference on Computing and Information, pages 72\u201381. Springer, 1990.\\n\\n[40] Ian H. Witten, Radford M. Neal, and John G. Cleary. Arithmetic coding for data compression. Commun. ACM, 1987.\\n\\n[41] Honglei Zhang, Francesco Cricri, Hamed R. Tavakoli, Nannan Zou, Emre Aksu, and Miska M. Hannuksela. Lossless image compression using a multi-scale progressive statistical model. In Proceedings of the Asian Conference on Computer Vision (ACCV), 2020.\\n\\n[42] Shifeng Zhang, Ning Kang, Tom Ryder, and Zhenguo Li. iflow: Numerically invertible flows for efficient lossless compression via a uniform coder. Advances in Neural Information Processing Systems, 2021.\\n\\n[43] Shifeng Zhang, Chen Zhang, Ning Kang, and Zhenguo Li. ivpf: Numerical invertible volume preserving flow for efficient lossless compression. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\"}"}
{"id": "CVPR-2022-1753", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Inspired by this, we introduce a new space-to-depth convolution such that the resulting autoregression is alternatively re-ordered into $k^2$ sub-blocks of $C$ channels each. Crucially, the resulting channels in each sub-block share the same spatial index allowing application of the autoregression detailed in (6) and (7).\\n\\nWe note that should $k = H = W$ we return an equivalence to the per-pixel autoregression of PixelCNN++ but perform an autoregression exclusively in the channel dimension. Likewise should $k < H$, we define a block-based context model in raster scan order where, unlike MS-PixelCNN, adjacent blocks are dependent.\\n\\nTo achieve our desired downsampling operation, which we denote by $g(\\\\cdot)$, we expand the depthwise convolutions of (5) into regular three-dimensional kernels where the $n$th of $Ck^2$ $C \\\\times k \\\\times k$ filters has one non-zero element such that\\n\\n$$K(n)_{c,h,w} = \\\\begin{cases} 1 & \\\\text{if } c = n \\\\mod C, h = \\\\lfloor n/Ck \\\\rfloor \\\\mod k, w = \\\\lfloor n/C \\\\rfloor \\\\mod k \\\\\\\\ 0 & \\\\text{else} \\\\end{cases}.$$  \\n\\n(8)\\n\\nWe further visualise this operation in Fig. 3 Right. The resulting density of the downsampled tensor for spatial location $h,w$ is then given by\\n\\n$$p_g(x; k)_{0:Ck^2,h,w|D} = k^2 \\\\prod_{i=0}^{k^2-1} p_g(x; k)_{iC,h,w|D} \\\\times \\\\prod_{j=h,w} \\\\left( g(x; k)_{<j,C,h,w|D} \\\\right),$$\\n\\n(9)\\n\\nwhere $i$ is the index over sub-blocks (i.e. a strong autoregression evaluated using neural networks).\\n\\n**Masked 3D Convolutions**\\n\\nWhilst we are restricted to $k^2$ evaluations per latent variable at inference time, the same does not have to be true during training. One efficient parallel training scheme is to use 3-dimensional convolutions applied to the downsampled tensors by expanding them into a $d \\\\times Ck^2 \\\\times H \\\\times W$ volume [23,27], where $d$ is some auxiliary dimension. Here we can apply zero-masking along the channel dimension of the kernels to enforce the causality condition, along with $k$-stride channel convolutions on the input. Full details are available in the Appendix.\\n\\n**Choice of Distribution**\\n\\nFor our choices of $p$ and $q$, we use a discretized mixture of logistic distributions for $x$ and a discretized univariate logistic distribution for all $z(l)$ [31]. That is, given some mean $\\\\mu$, scale $s$ and uniform discretization bin-width $b$, one can obtain the univariate pmf by integrating the logistic pdf over the discretization bin. For $x$, we typically use a mixture of 5 discrete logistic distributions as defined above.\\n\\n### 4.2. Autoregressive Initial Bits\\n\\nAs discussed in Section 3.1, bb-ANS is able to achieve efficient codelengths, but can lead to several shortcomings. Fortunately, our proposed autoregressive model naturally accommodates the possibility to bypass the auxiliary bits needed in other bb-ANS methods. We achieve this by exploiting the block-based autoregressive structure on the data variable. We outline this process below, which we refer to as autoregressive initial bits (ArIB). Different from the models considered in...\\n\\n* * *\\n\\n...continued\"}"}
{"id": "CVPR-2022-1753", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"where we estimate the respective expectations during training.\\n\\n4.3. Split Hierarchical Variational Compression\\n\\nyielding the same result.\\n\\nIn our experiments, we demonstrate that the performance\\n\\n\\ndemonstrate that it is both orders of magnitude less than initial\\n\\ncosts associated with criteria one are negligible. Crucially, we\\n\\nIn Section 5.3, we found the\\n\\nevaluate the effect of the ArIB constraints on compression\\n\\nIn this Section, we perform a series of experiments to\\n\\nFor purposes of experimentation, we define two versions of\\n\\nFor all of our experiments, we set\\n\\nFor criteria two, we formulate the optimization of (2) as\\n\\nFor this approach to be valid, we require the satisfaction of\\n\\nThere exists some\\n\\n2. The\\n\\nx\\n\\n| 1:\\n\\nx\\n\\n| 2\\n\\np\\n\\nz\\n\\nis the discretized analogue of\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\n2\\n\\n| 1:\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\\n\\n| 2\\n\\np\\n\\nx\\n\\ns\\n\\nx\\n\\nk\\n\\ni\\n\\nz\\n\\n1:\"}"}
{"id": "CVPR-2022-1753", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Compression results in BPD for SHVC and other popular codecs across three low-resolution datasets and three full-resolution datasets. Lower is better. Here \u2021 and \u2217 denote models trained on ImageNet32 and ImageNet64, respectively.\\n\\n5.2. Compression Performance\\n\\nLow-Resolution Images\\n\\nWe begin by testing our method on three toy datasets: CIFAR10, Imagenet32 and Imagenet64. We compare our method against leading approaches from traditional codecs, normal flows and VAEs. (Given the time complexity associated with per-pixel autoregressive factorisations, we follow the broader compression community and eschew them from our comparisons). As an ad hoc test of generalizability, we additionally follow the authors of [14,36,42,43] by training a model on Imagenet32 and testing it across all other datasets. We present our results in terms of bits per dimension (BPD), which we display in Table 1.\\n\\nFull-Resolution Images\\n\\nWe further compare our approach against full-resolution algorithms, i.e., L3C [22]. Here we follow [43] and adopt our Imagenet64 model using a patch-based evaluation protocol in which images are cropped to 64\u00d764.\\n\\nFrom Table 1, we demonstrate reliable out-performance of every other considered codec. Further, when comparing the performance of SHVC to e.g. iFlow across small and full-resolution images we interestingly note that difference in out-performance becomes greater. Indeed, we hypothesise that autoregressive \u201ccontext\u201d becomes an increasingly important inductive bias as the resolution of the image increases \u2013 and likewise the extent of the spatial redundancy.\\n\\nArIB poses non-trivial constraints on the data variable, such that a sufficiently large partition should be: 1. conditionally independent of latent variables; 2. encoded with an entropy larger than that with which the first latent variable is decoded with. Using the objective of Eq.(10), we train models capable of single-image compression: SHVC, SHVC-ArIB and a parameterisation of SHVC with deterministic posteriors using AC (henceforth known as Deterministic SHVC). We display results across CIFAR10, Imagenet32 and Imagenet64 in Fig. 4, and quantity our results as additional bits on a per-image basis against SHVC. In more detail, the overhead in SHVC comes from initial bits, whilst other models incur a performance cost. Here we see that our ArIB adds minimal additional bits \u2013 less than the number in SHVC by a factor of \u223c20. Crucially, our approach also outperforms Deterministic SHVC, which we hope may serve as motivation to adapt the codec to lossy compressors.\\n\\nTable 2. Inference time and BPD across CIFAR10. Inference time measures evaluation of 10,000 test images with a batch size of 100. Inference Speed\\n\\nTo better evaluate our approach in the context of VAEs, we compare SHVC against popular publicly available compression models, HiLLoC [36], Bit-Swap [18] and Integer Discrete Flow (IDF) [14]. We report achieved BPDs in Table 2 and measure inference time in seconds (s) to evaluate the 10,000 CIFAR10 test images with a batch size of 100. Here we observe that our model is faster than HiLLoC for a lower BPD and achieves lower BPD for the same speed as Bit-Swap. We further note that, unlike both considered approaches, we are able to easily achieve parallel coding with minimal overhead. In contrast to SHVC and Bit-Swap, HiLLoC is implemented in TensorFlow. As such, HiLLoC is likely advantaged in terms of run-time environment. We note follow-up paper IDF++ [3] has no publicly available code but uses largely the same architecture as IDF. As such IDF++ inference times can be loosely inferred from IDF speeds.\"}"}
{"id": "CVPR-2022-1753", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Additional bits per single image compressed across SHVC, SHVC-ArIB and Deterministic SHVC. The overhead in SHVC comes from initial bits, whilst other models incur a performance cost.\\n\\n5.3. Ablation Studies\\n\\nSpace-to-depth operation\\n\\nOne alternative in our approach is to replace our downsampling operator of (8) with the usual space-to-depth transformation described in Eq.(5). However, as discussed, this would instead define a weak autoregressive property over spatially adjacent pixels, channel-by-channel. In Fig. 5 we demonstrate the differences resulting from the choice of spatial downsampling transformation by training two models on CIFAR10. Here we observe that our convolutional operator provides non-trivial benefits over the vanilla space-to-depth transformation.\\n\\nChoice of $k$\\n\\nAs discussed, one important hyper-parameter choice is that of $k$. As $k$ grows, the prior becomes more powerful but the time complexity grows. In Fig. 6 we visualise the effect of increasing $k$ on the compression ratio, which is displayed as an average across three models trained on CIFAR10, ImageNet32 and ImageNet64. As discussed, we note that performance of the model peaks at $k=4$, before becoming worse as $k$ increases. This non-intuitive behaviour can be better explained further in Fig. 6, where we see evidence of a posterior collapse common in hierarchical VAEs \u2013 especially those with hierarchical autoregressive priors [5,11,20,28].\"}"}
