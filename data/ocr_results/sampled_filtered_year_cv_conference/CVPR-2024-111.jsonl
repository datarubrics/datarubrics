{"id": "CVPR-2024-111", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Lei Jimmy Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization. arXiv:1607.06450 [stat.ML], 2016.\\n\\n[2] Charles Beattie, Joel Z. Leibo, Denis Teplyashin, Tom Ward, Marcus Wainwright, Heinrich K\u00fcttler, Andrew Lefrancq, Simon Green, V\u00edctor Vald\u00e9s, Amir Sadik, Julian Schrittwieser, Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King, Demis Hassabis, Shane Legg, and Stig Petersen. DeepMind Lab. arXiv:1612.03801 [cs.AI], 2016.\\n\\n[3] Han Cai, Chuang Gan, Ligeng Zhu, and Song Han. TinyTL: Reduce memory, not parameters for efficient on-device learning. In NeurIPS*2020.\\n\\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, pages 213\u2013229, 2020.\\n\\n[5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00e9 J\u00e9gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, pages 9630\u20139640, 2021.\\n\\n[6] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. AdaptFormer: Adapting vision transformers for scalable visual recognition. In NeurIPS*2022.\\n\\n[7] Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proc. IEEE, 105(10):1865\u20131883, 2017.\\n\\n[8] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In CVPR, pages 3606\u20133613, 2014.\\n\\n[9] Mostafa Dehghani, Josip Djolonga, Basil Mustafa, Piotr Padlewski, Jonathan Heek, Justin Gilmer, Andreas Peter Steiner, Mathilde Caron, Robert Geirhos, Ibrahim Alabdalmohsin, Rodolphe Jenatton, Lucas Beyer, Michael Tschannen, Anurag Arnab, Xiao Wang, Carlos Riquelme Ruiz, Matthias Minderer, Joan Puigcerver, Utku Evci, Manoj Kumar, Sjoerd van Steenkiste, Gamaleldin Fathy Elsayed, Aravindh Mahendran, Fisher Yu, Avital Oliver, Fantine Huot, Jasmijn Bastings, Mark Collier, Alexey A. Gritsenko, Vighnesh Birodkar, Cristina Nader Vasconcelos, Yi Tay, Thomas Mensink, Alexander Kolesnikov, Filip Pavetic, Dustin Tran, Thomas Kipf, Mario Lucic, Xiaohua Zhai, Daniel Keysers, Jeremiah J. Harmsen, and Neil Houlsby. Scaling vision transformers to 22 billion parameters. In ICML, pages 7480\u20137512, 2023.\\n\\n[10] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT, pages 4171\u20134186, 2019.\\n\\n[11] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. DeCAF: A deep convolutional activation feature for generic visual recognition. In ICML, pages 647\u2013655, 2014.\\n\\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\\n\\n[13] Emma Dugas, Jorge Jared, and Will Cukierski. Diabetic retinopathy detection. Kaggle, 2015.\\n\\n[14] Li Fei-Fei, Robert Fergus, and Pietro Perona. One-shot learning of object categories. IEEE T. Pattern Anal. Mach. Intell., 28(4):594\u2013611, 2006.\\n\\n[15] Yarin Gal and Zoubin Ghahramani. Dropout as a Bayesian approximation: Representing model uncertainty in deep learning. In ICML, pages 1050\u20131059, 2016.\\n\\n[16] Timnit Gebru, Jonathan Krause, Yilun Wang, Duyun Chen, Jia Deng, and Li Fei-Fei. Fine-grained car detection for visual census estimation. In AAAI, pages 4502\u20134508, 2017.\\n\\n[17] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. Int. J. Robotics Res., 32(11):1231\u20131237, 2013.\\n\\n[18] Yunhui Guo, Yandong Li, Liqiang Wang, and Tajana Rosing. Depthwise convolution is all you need for learning multiple visual domains. In AAAI, pages 8368\u20138375, 2019.\\n\\n[19] Tianxiang Hao, Hui Chen, Yuchen Guo, and Guiguang Ding. Consolidator: Mergable adapter with group connections for visual adaptation. In ICLR, 2023.\\n\\n[20] Haoyu He, Jianfei Cai, Jing Zhang, Dacheng Tao, and Bohan Zhuang. Sensitivity-aware visual parameter-efficient tuning. In ICCV, 2023.\\n\\n[21] Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, and Graham Neubig. Towards a unified view of parameter-efficient transfer learning. In ICLR, 2022.\\n\\n[22] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Delving deep into rectifiers: Surpassing human-level performance on imagenet classification. In ICCV, pages 1026\u20131034, 2015.\\n\\n[23] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, pages 770\u2013778, 2016.\\n\\n[24] Patrick Helber, Benjamin Bischke, Andreas Dengel, and Damian Borth. EuroSAT: A novel dataset and deep learning benchmark for land use and land cover classification. IEEE J. Sel. Top. Appl. Earth Obs. Remote. Sens., 12(7):2217\u20132226, 2019.\\n\\n[25] Dan Hendrycks and Kevin Gimpel. Gaussian error linear units (GELUs). arXiv:1606.08415 [cs.LG], 2023.\\n\\n[26] Grant Van Horn, Steve Branson, Ryan Farrell, Scott Haber, Jessie Barry, Panos Ipeirotis, Pietro Perona, and Serge J. Belongie. Building a bird recognition app and large scale dataset with citizen scientists: The fine print in fine-grained dataset collection. In CVPR, pages 595\u2013604, 2015.\\n\\n[27] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin de Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for NLP. In ICML, pages 2790\u20132799, 2019.\\n\\n[28] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022.\\n\\n[29] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q. Weinberger. Deep networks with stochastic depth. In ECCV, pages 646\u2013661, 2016.\"}"}
{"id": "CVPR-2024-111", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge J. Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In ECCV, pages 709\u2013727, 2022.\\n\\nShibo Jie and Zhi-Hong Deng. FacT: Factor-tuning for lightweight adaptation on vision transformer. In AAAI, pages 1060\u20131068, 2023.\\n\\nJustin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, C. Lawrence Zitnick, and Ross B. Girshick. CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning. In CVPR, pages 1988\u20131997, 2017.\\n\\nAditya Khosla, Nityananda Jayadevaprakash, Bangpeng Yao, and Li Fei-Fei. Novel dataset for fine-grained image categorization. In CVPR Workshop on Fine-grained Visual Classification, 2011.\\n\\nAlex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, Canadian Institute for Advanced Research, 2009.\\n\\nYann LeCun, Fu Jie Huang, and L\u00e9on Bottou. Learning methods for generic object recognition with invariance to pose and lighting. In CVPR, pages 97\u2013104, 2004.\\n\\nBrian Lester, Rami Al-Rfou, and Noah Constant. The power of scale for parameter-efficient prompt tuning. In EMNLP, pages 3045\u20133059, 2021.\\n\\nXiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. In ACL/IJCNLP, pages 4582\u20134597, 2021.\\n\\nDongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: A new baseline for efficient model tuning. In NeurIPS*2022.\\n\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019.\\n\\nArun Mallya, Dillon Davis, and Svetlana Lazebnik. Piggyback: Adapting a single network to multiple tasks by learning to mask weights. In ECCV, pages 72\u201388, 2018.\\n\\nLoic Matthey, Irina Higgins, Demis Hassabis, and Alexander Lerchner. dSprites: Disentanglement testing sprites dataset. https://github.com/deepmind/dsprites-dataset, 2017.\\n\\nPedro Morgado and Nuno Vasconcelos. NetTailor: Tuning the architecture, not just the weights. In CVPR, pages 3044\u20133054, 2019.\\n\\nYuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng. Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on Deep Learning and Unsupervised Feature Learning, 2011.\\n\\nMaria-Elena Nilsback and Andrew Zisserman. Automated flower classification over a large number of classes. In ICVGIP, pages 722\u2013729, 2008.\\n\\nMaxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael G. Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv\u00e9 J\u00e9gou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. DINOv2: Learning robust visual features without supervision. arXiv:2304.07193 [cs.CV], 2023.\\n\\nOmkar M. Parkhi, Andrea Vedaldi, Andrew Zisserman, and C. V. Jawahar. Cats and dogs. In CVPR, pages 3498\u20133505, 2012.\\n\\nJonas Pfeiffer, Aishwarya Kamath, Andreas R\u00fcckl\u00e9, Kyunghyun Cho, and Iryna Gurevych. AdapterFusion: Non-destructive task composition for transfer learning. In EACL, pages 487\u2013503, 2021.\\n\\nAlec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. Technical report, OpenAI, 2018.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In ICML, pages 8748\u20138763, ICML.\\n\\nRen\u00e9 Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In ICCV, pages 12159\u201312168, 2021.\\n\\nSylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. In NIPS*2017, pages 506\u2013516.\\n\\nFrank Rosenblatt. The perceptron: A probabilistic model for information storage and organization in the brain. Psychol. Rev., 65(6):386\u2013408, 1958.\\n\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet large scale visual recognition challenge. Int. J. Comput. Vision, 115(13):211\u2013252, 2015.\\n\\nChristoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia Jitsev. LAION-5B: An open large-scale dataset for training next generation image-text models. In NeurIPS*2022.\\n\\nChristian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jonathon Shlens, and Zbigniew Wojna. Rethinking the Inception architecture for computer vision. In CVPR, pages 2818\u20132826, 2016.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS*2017, pages 5998\u20136008.\\n\\nBastiaan S. Veeling, Jasper Linmans, Jim Winkens, Taco Cohen, and Max Welling. Rotation equivariant CNNs for digital pathology. In MICCAI, pages 210\u2013218, 2018.\\n\\nCatherine Wah, Steve Branson, Peter Welinder, Pietro Perona, and Serge Belongie. The Caltech-UCSD Birds-200-2011 dataset. Technical report, California Institute of Technology, 2011.\\n\\nJianxiong Xiao, James Hays, Krista A. Ehinger, Aude Oliva, and Antonio Torralba. SUN database: Large-scale scene recognition from abbey to zoo. In CVPR, pages 3485\u20133492, 2010.\\n\\nEnze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jos\u00e9 M. \u00c1lvarez, and Ping Luo. SegFormer: Simple and efficient image segmentation with Transformers. arXiv:2105.08408 [cs.CV], 2021.\"}"}
{"id": "CVPR-2024-111", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"efficient design for semantic segmentation with transformers.\\n\\nIn NeurIPS*2021, pages 12077\u201312090.\\n\\n[61] Elad Ben Zaken, Yoav Goldberg, and Shauli Ravfogel. BitFit: Simple parameter-efficient fine-tuning for transformer-based masked language-models. In ACL, 2022.\\n\\n[62] Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, Lucas Beyer, Olivier Bachem, Michael Tschannen, Marcin Michalski, Olivier Bousquet, Sylvain Gelly, and Neil Houlsby. A large-scale study of representation learning with the visual task adaptation benchmark. arXiv:1910.04867 [cs.CV], 2020.\\n\\n[63] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. Neural prompt search. arXiv:2206.04673 [cs.CV], 2022.\"}"}
{"id": "CVPR-2024-111", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adapters Strike Back\\n\\nJan-Martin O. Steitz\\nStefan Roth\\n\\n1 Department of Computer Science, TU Darmstadt\\n2 hessian.AI\\n\\nAbstract\\nAdapters provide an efficient and lightweight mechanism for adapting trained transformer models to a variety of different tasks. However, they have often been found to be outperformed by other adaptation mechanisms including low-rank adaptation. In this paper, we provide an in-depth study of adapters, their internal structure, as well as various implementation choices. We uncover pitfalls for using adapters and suggest a concrete, improved adapter architecture, called Adapter+, that not only outperforms previous adapter implementations but surpasses a number of other, more complex adaptation mechanisms in several challenging settings. Despite this, our suggested adapter is highly robust and, unlike previous work, requires little to no manual intervention when addressing a novel scenario. Adapter+ reaches state-of-the-art average accuracy on the VTAB benchmark, even without a per-task hyperparameter optimization.\\n\\n1. Introduction\\nTransfer learning from an off-the-shelf model, pre-trained on a large dataset like ImageNet [53] to a downstream task by fully fine-tuning the model's parameters is a common paradigm. A typical CNN architecture, like a ResNet [23], has several tens of millions of parameters. However, since the introduction of transformers [56] into the realm of computer vision [4, 5, 12, 49, 50, 60], model sizes have grown exponentially from around a hundred million parameters for a vision transformer (ViT) [12] to more than a billion parameters [9, 45]. This leads to huge storage requirements when fine-tuning on multiple downstream tasks because a complete set of the model's parameters needs to be saved per task. Additionally, large models require correspondingly large datasets [e.g.,, 54] to be trained to their full potential, yet tend to overfit easily if the target dataset in transfer learning is too small. One solution is linear probing [11], where only the linear classifier is trained, but this usually yields inferior results compared to full fine-tuning.\\n\\nAs a consequence, there is a growing interest in parameter-efficient tuning methods. The main idea is to freeze the parameters of the pre-trained model and add a comparatively small amount of parameters to the model, which are then tuned together with the classifier to adapt the model to the downstream task at hand. Representative methods with different underlying concepts include VPT [30], which prepends the sequence of image tokens in the attention with trainable tokens to learn a prompt tuning, LoRA [28], where the attention weights are updated with learnable low-rank decomposition matrices, and Adapters [27], which are small bottleneck modules that are added to every transformer layer of the network. Adapters were first proposed for CNNs by Rebuffi et al. [51] and various formulations [21, 27, 47] exist for the now common ViT architecture.\\n\\nRecent work on parameter-efficient transfer learning [e.g.,, 19, 20, 30, 31, 38, 63] presents adapters as a baseline method for the adaptation to downstream tasks in computer vision. However, we identified various common issues in their implementations, which we find to have a negative influence on the\"}"}
{"id": "CVPR-2024-111", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2. Average accuracy for VTAB subgroups on the test sets.\\n\\nFor methods marked with \u27e2, we report results of our re-evaluation after a complete training schedule with suitable data normalization to ensure a fair comparison. Adapter+ is evaluated with rank $r \\\\in [1..32]$ adaptation performance. For further details, refer to the supplemental material. Additionally, while adapters have been well studied in natural language processing (NLP), there is no study that broadly examines the different adapter configurations for vision transformers. As a result, adapters have seemed to underperform in comparison to recent parameter-efficient adaptation methods, e.g., reported accuracies of adapters on VTAB of 73.9% in [63] and 60.8% in [30].\\n\\nIn this work, we therefore revisit the idea of adapters and investigate how they can perform at their best in connection with ViTs. Our contribution hereby is threefold:\\n\\n1. We show the first in-depth and systematic study on the effects of the adapter position in the transformer and of the adapter\u2019s inner structure with ViTs, as well as evaluate different variants of parameter initialization.\\n\\n2. We further propose a learnable, channel-wise scaling as extension to plain adapters, which proves to be beneficial for computer vision tasks.\\n\\n3. Finally, we present Adapter+, an adapter configuration with an excellent parameter-accuracy trade-off compared to other work, as shown in Fig. 1. Adapter+ reaches a state-of-the-art average accuracy of 77.6% on VTAB [62] without any hyperparameter optimization per task and 3.7 percentage points (pp) over previous adapter baselines. We also reach a state-of-the-art accuracy of 90.7% on FGVC [30] with the lowest number of parameters compared to other methods. Finally, Adapter+ shows the best robustness in terms of accuracy across the VTAB subgroups, see Fig. 2.\\n\\n2. Related work\\n\\nOne possibility to adapt a pre-trained network to a novel task, apart from full fine-tuning, is to only selectively tune some of the parameters, e.g., only training the classifier [11]. Cai et al. [3] proposed to tune only the biases of an otherwise frozen network to adapt it to a downstream task. BitFit [61] then showed the efficacy of this method for NLP transformers.\\n\\n**Modular adaptation.** The concept of adding small, trainable modules with only a few parameters to an otherwise frozen network was first proposed for adapting CNNs by Rebuffi et al. [51] and called adapters. Other approaches replaced all convolutions in the network with depth-wise separable convolutions and only tuned their spatial parts [18], learned binary masks to prune a pre-trained network per target task [40], or created a student network by augmenting the original network with adapter-like modules and skip connections, which then mimicked a teacher network by disabling parts of its pre-trained and added modules [42]. Following the rise of transformers in NLP [10, 48, 56], Houlsby et al. [27] proposed adapter modules in the form of bottlenecks for transformer layers. Pfeiffer et al. [47] conducted an architecture search on NLP tasks to find a more parameter-efficient configuration of adapter modules that only acts on the transformer\u2019s feed-forward network (FFN), thus saving roughly half of the parameters over [27].\\n\\n**Prompt tuning.** Inspired by changing the output of a network for NLP with hand-crafted textual prompts, which modifies the attention over the original input tokens, Lester et al. [36] proposed prompt tuning: A set of learnable tokens is added to the input sequence and trained with backpropagation to prompt a frozen language model to perform downstream tasks. Li and Liang [37] extended on prompt tuning by adding learnable tokens at every transformer layer of the model, which they termed prefix tuning. Jia et al. [30] applied prompt tuning to vision transformers, then called visual prompt tuning (VPT), by preprending the sequence of image patch embeddings with such trainable tokens (VPT-Shallow). They also showed a variant resembling prefix tuning with stronger adaptation capabilities that adds tokens at every layer of the network (VPT-Deep).\\n\\n**Low-rank approaches.** Also focusing on the attention part of the transformer layers, Hu et al. [28] proposed low-rank adaptation (LoRA) where the attention weights are updated with low-rank decomposition matrices. The matrices can be merged with the attention weights for inference. The structure of LoRA is very similar to an adapter, which can be seen as a superset of LoRA acting on the transformer\u2019s FFN. He et al. [21] proposed a formalism to unify LoRA, adapters, and prefix tuning [37]. It allowed them to combine the beneficial aspects of all three methods into a scaled parallel adapter (Scaled PA) for NLP tasks. AdaptFormer [6] then applied the concept of Scaled PA to vision transformers.\"}"}
{"id": "CVPR-2024-111", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Other related work.\\n\\nNewer approaches for vision transformers proposed different techniques to further enhance the parameter-accuracy trade-off in adaptation. NOAH [63] performs an architecture search for a combination of adapters, LoRA, and VPT for each task. SSF [38] scales and shifts the features in the network after every operation, i.e., attention, FFN, layer normalization, with task-specific, trainable modules. Jie and Deng [31] aggregate the weights of a ViT into a single 3D tensor. Task-specific weight updates of this tensor are learned as a matrix decomposed into parameter-efficient factors, hence they termed their method factor-tuning (FacT).\\n\\nSPT [20] measures the importance of the weights of a pretrained network for a downstream task. Based on a desired parameter budget, the most important parameters are chosen for tuning and adapters or LoRA are used for weight matrices that contain enough parameters of importance. Consolidator [19] adapts weights in multiple orderings of channel-wise groups. The updates for all groups are merged for efficient storage and inference.\\n\\nDespite these new developments, we show that the simple concept of adapters exhibits an even better parameter-accuracy trade-off in combination with vision transformers \u2013 if done right and with the addition of a channel-wise scaling.\\n\\n3. Adapters for vision transformers\\n\\n3.1. Vision transformer basics\\n\\nIn this work, we concentrate on the parameter-efficient adaptation of vision transformers (ViT) [12]. The ViT is closely modeled after the transformer model for natural language processing (NLP) proposed by Vaswani et al. [56]. A learned linear projection embeds non-overlapping and flattened patches of the input image into a sequence of \\\\( n \\\\) tokens \\\\( x \\\\in \\\\mathbb{R}^{n \\\\times d} \\\\), where \\\\( d \\\\) is called the hidden dimension of the transformer. A positional encoding is added to the embeddings and the sequence is prepended with a trainable \\\\([CLS]\\\\) token. The sequence length and the dimension of the tokens stay fixed throughout the architecture. The sequence is sent through consecutive transformer layers that each consist of a multi-head self-attention and a feed-forward network (FFN).\\n\\nFor the self-attention, the tokens are projected to queries, keys, and values (\\\\( Q, K, V \\\\)) and the output of each of the \\\\( M \\\\) attention heads is calculated as\\n\\n\\\\[\\n\\\\text{Attention}(x) = \\\\text{Softmax}(Q(x)K(x)^T)\\\\sqrt{d/d'}V(x),\\n\\\\]\\n\\nwith \\\\( d' = d/M \\\\) being the inner dimension of the head. The FFN consists of a multilayer perceptron with two linear layers (with weights \\\\( W_i \\\\) and biases \\\\( b_i \\\\)) and a GELU [25] non-linearity as activation in between:\\n\\n\\\\[\\n\\\\text{FFN}(x) = \\\\text{GELU}(xW_1 + b_1)W_2 + b_2.\\n\\\\]\\n\\nBoth attention and FFN are employed with a preceding layer normalization (LN) [1] and a skip connection and, therefore, transform an input sequence sequentially as\\n\\n\\\\[\\nx \\\\rightarrow \\\\text{Attention} (\\\\text{LN}(x)) + x (3a)\\n\\\\]\\n\\n\\\\[\\nx \\\\rightarrow \\\\text{FFN} (\\\\text{LN}(x)) + x. (3b)\\n\\\\]\\n\\nTo keep the notation concise, we will omit the LNs of attention and FFN in the following; each attention and FFN is assumed to be always preceded by an LN.\\n\\n3.2. Adapters and their inner structure\\n\\nAdapters [27] are small modules that are added to the transformer layers. They allow to tailor a network to a new task or domain, where instead of tuning the parameters of the whole network, only the adapter parameters and the classifier are trained. Adapters take the form of bottlenecks with an inner dimension of \\\\( r \\\\ll d \\\\). We call \\\\( r \\\\) the rank of the adapter. In detail, a down-projection to dimension \\\\( r \\\\) with weights \\\\( W_{\\\\text{down}} \\\\in \\\\mathbb{R}^{d \\\\times r} \\\\) and biases \\\\( b_{\\\\text{down}} \\\\in \\\\mathbb{R}^r \\\\) is followed by a non-linear activation function \\\\( \\\\sigma(\\\\cdot) \\\\), typically a GELU [25] as used throughout the ViT, and an up-projection with weights \\\\( W_{\\\\text{up}} \\\\in \\\\mathbb{R}^{r \\\\times d} \\\\) and biases \\\\( b_{\\\\text{up}} \\\\in \\\\mathbb{R}^d \\\\) back to the hidden dimension \\\\( d \\\\) of the transformer layer. This yields a base adapter module\\n\\n\\\\[\\n\\\\text{Adapter}_{\\\\text{base}}(x) = \\\\sigma(xW_{\\\\text{down}} + b_{\\\\text{down}})W_{\\\\text{up}} + b_{\\\\text{up}}. (4)\\n\\\\]\\n\\nThe base adapter module can be further enhanced with a normalization layer, e.g., a layer normalization (LN) [1]. Additionally, the output of the bottleneck can be scaled by \\\\( s \\\\) as\\n\\n\\\\[\\n\\\\text{Adapter}(x) = s \\\\cdot \\\\text{Adapter}_{\\\\text{base}}(\\\\text{LN}(x)). (5)\\n\\\\]\\n\\nFor layer-wise scaling, the factor \\\\( s \\\\) is taken to be a scalar, i.e., \\\\( s \\\\in \\\\mathbb{R} \\\\), and can be either fixed as a hyperparameter or learned during training. Layer-wise scaling was proposed by He et al. [21] and Hu et al. [28] but deemed not effective compared to a fixed scaling for tasks in NLP. Here, we additionally propose to use a channel-wise, learned scaling where \\\\( s \\\\in \\\\mathbb{R}^d \\\\). We investigate its capabilities in Sec. 4.3. In most cases, the adapter is used with a skip connection, hence the complete feature transformation becomes\\n\\n\\\\[\\nx \\\\rightarrow \\\\text{Adapter}(x) + x. (6)\\n\\\\]\\n\\nThe complete inner structure of an adapter including its skip connection is visualized in Fig. 3a.\\n\\n3.3. Adapter positions\\n\\nAlthough the architecture of bottleneck adapters for transformers is rather simple, there are various ways to plug them into the transformer layer. Previous work has not yet investigated what the optimum position is for the use with a ViT [12]. Here, we evaluate four possible adapter positions, shown in Figs. 3b to 3e. We postulate that it is easier for an adapter to learn to modify features previously transformed...\"}"}
{"id": "CVPR-2024-111", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3. Illustrations of (a) the inner structure of an adapter with feed-forward layers (FF), activation layer (Act), and optional layer normalization (LN) and scaling, (b)\u2013(d) different possible adapter positions to connect the adapter to the FFN section of the transformer layer. Modules with trainable parameters are shown in red and frozen modules in blue.\\n\\nPre-Adapter. The first adapter position we analyze applies the adapter to the output $x$ of the attention section of the transformer layer before it is passed into the FFN, but with the skip connection of the attention already added (Fig. 3b). The feature transformation of the FFN section with the adapter attached, therefore, becomes\\n\\n$$x_{7} \\\\rightarrow \\\\text{FFN} (x) + x + \\\\text{Adapter} (x) + x.$$\\n\\nNote that the two occurrences of Adapter $(x)$ in Eq. (7) refer to the same instantiation. In this configuration, the adapter has the full information from the feature transformation happening in the attention but needs to estimate the transformation that will be happening in the FFN that follows. As a result, especially the last FFN before the linear classifier will be hard to adapt. To the best of our knowledge, this adapter position has not been considered in the literature.\\n\\nPost-Adapter. In this case, the adapter is positioned at the very end of the transformer layer on the output of the FFN with its skip connection added as\\n\\n$$x_{7} \\\\rightarrow \\\\text{Adapter} (x_{7}) + \\\\text{FFN} (x) + x + \\\\text{FFN} (x) + x.$$\\n\\nwhere the FFNs refer to the same instantiation (Fig. 3c). That way, the adapter has access to the feature transformation happening in the FFN and the unmodified features via the skip connection. This position has been proposed by Pfeiffer et al. [47] as the result of an architecture search, but only for adapting transformers for NLP tasks and not for a ViT.\\n\\nParallel-Adapter. Next, we consider a parallel setting as proposed by [21], where the adapter is located parallel to the FFN and both share a skip connection (Fig. 3d):\\n\\n$$x_{7} \\\\rightarrow \\\\text{FFN} (x) + \\\\text{Adapter} (x) + x.$$\\n\\nTherefore, both adapter and FFN work on the output of the attention section of the transformer layer and the adapter needs to learn the necessary residual transformation to the one produced by the frozen FFN.\\n\\nIntermediate-Adapter. Finally, we consider the original adapter position as proposed by Houlsby et al. [27]. The adapter is plugged behind the FFN but before the skip connection of the FFN is added (Fig. 3e). The adapter additionally possesses its own skip connection:\\n\\n$$x_{7} \\\\rightarrow \\\\text{Adapter} (x_{7}) + \\\\text{FFN} (x) + \\\\text{FFN} (x) + x.$$\\n\\nNote that the two occurrences of FFN $(x)$ in Eq. (10) refer to the same instantiation. The adapter sees the transformed features coming from the FFN but cannot access the features added later on by the skip connection of the FFN.\\n\\n3.4. Initialization of adapter parameters\\n\\nSince training a deep learning model is a non-convex optimization problem, the initialization of parameters is important. In this work, we evaluate three different variants of parameter initializations for adapters proposed in the literature. All of them have the goal to initialize the adapters in a way that minimizes the initial influence of the adapters at the start of their training. This is a sensible goal since adapters extend an already pre-trained frozen network.\\n\\nHoulsby initialization. Houlsby et al. [27] propose to draw the weights of the projection matrices from a zero-centered Gaussian distribution with a standard deviation of $\\\\sigma = 0.01$, truncated at $2 \\\\sigma$, and use zero for their biases.\\n\\nBERT initialization. For the BERT model [10], the initialization works similar to [27] but the Gaussian distribution has a standard deviation of $\\\\sigma = 0.02$ and is not truncated. This form of initialization is used by Pfeiffer et al. [47].\\n\\nLoRA initialization. LoRA [28] initializes the weights and biases of the down-projection with a uniform Kaiming He initialization [22]; the weights and biases of the up-projection...\"}"}
{"id": "CVPR-2024-111", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"are initialized to zero. Therefore, the output of the adapter at the beginning of training equals zero and the adapter initially does not contribute.\\n\\n3.5. Data normalization in pre-processing\\n\\nData normalization is common practice during image pre-processing. It is typically done by shifting and scaling of each input pixel $x_{ij}$ for each channel $c$ as\\n\\n$$\\n\\\\hat{x}_{ijc} = \\\\left( x_{ijc} - \\\\mu_c \\\\right) / \\\\sigma_c. \\\\tag{11}\\n$$\\n\\nMost widely used are the mean $\\\\mu = (0.485, 0.456, 0.406)^T$ and standard deviation $\\\\sigma = (0.229, 0.224, 0.225)^T$ of the ImageNet dataset [53], commonly referred to as ImageNet normalization. Another option is using 0.5 for every element of $\\\\mu$ and $\\\\sigma$, which is commonly referred to as Inception normalization because it is used for the Inception family of CNN architectures, starting with Inception-v3 [55]. The ImageNet normalization aims to center the input data around 0 with a standard deviation of 1. The Inception normalization, on the other hand, transforms the input values such they are strictly in range $[-1, 1]$.\\n\\nBecause we try to adapt to a target domain on a very low parameter budget, it is important to use the data normalization the network saw during its pre-training. Otherwise, the parameter-efficient transfer method of choice needs to first compensate for the shift in input data statistics and loses parts of its capacity to adapt to the target domain.\\n\\n4. Experiments\\n\\n4.1. Datasets\\n\\nIn order to carry out a detailed study of the utility of adapters in the context of ViT models, we experiment with two standard benchmarks for task adaptation.\\n\\nVTAB. The Visual Task Adaptation Benchmark (VTAB) [62] consists of 19 tasks, which are further grouped into three categories: Natural, Specialized, and Structured. The Natural group contains natural images captured using standard photographic equipment. The Specialized group is built from datasets of images captured with specialized equipment, from remote sensing and medical domains. Lastly, the Structured group is for evaluating the understanding of the scene structure. Here, the majority of the datasets are compiled from synthetic images with scenes that are easy to assess for humans but have a large domain gap to natural image datasets. Each task of VTAB consists of 800 training and 200 validation images. The test sets have the same number of images as the test sets in the original datasets.\\n\\nFGVC. Following Jia et al. [30], we compile five datasets for fine-grained visual classification (FGVC): CUB-200-2011 [58], NABirds [26], Oxford Flowers [44], Stanford Dogs [33], and Stanford Cars [16]. Because VTAB benchmarks task adaptation in a low-data regime in terms of the number of available training images, we use FGVC to evaluate adaptation methods in settings where training data is abundant. Where validation sets are not available in FGVC, we follow Jia et al. [30] to create the validation splits. For further details regarding the dataset properties of VTAB and FGVC, see supplemental material.\\n\\n4.2. Experimental settings\\n\\nFor all our experiments, we use a ViT-B/16 network [12] that was pre-trained on ImageNet-21k [53]. We follow its pre-training settings, in particular, regarding input data normalization. We train all models with an AdamW [39] optimizer with a learning rate of $10^{-3}$, a weight decay of $10^{-4}$, and a batch size of 64, following [63]. For full fine-tuning, we use a learning rate of $10^{-4}$, which we found leads to better results. We use a cosine learning rate schedule with a linear warm-up over the first 10 epochs and train for 100 epochs in total. We use stochastic depth with linearly increasing drop rates as a function of network depth from 0 to 0.1 for the frozen network and with a drop rate of 0.1 for the adapters during training. Apart from data normalization (cf. Sec. 3.4), we resize input images to $224 \\\\times 224$ px for VTAB and use a randomly resize crop to $224 \\\\times 224$ px and horizontal flipping for FGVC. For the ablations and to determine hyperparameters, we evaluate on the validation splits. We include the validation sets in the training data for producing final results.\\n\\n4.3. Exploring adapter configurations\\n\\nAdapter position. We first evaluate the four possible positions to connect an adapter to the FFN section of the transformer layer, as described in Sec. 3.3. In our ablation, we use Adapter base (cf. Eq. (4)) with rank $r=8$ and use the Houlsby initialization. In this experiment, the adapters neither have a layer normalization nor use scaling.\\n\\nThe results on the VTAB validation set for all four adapter positions are presented in Tab. 1. The Post-Adapter yields the best result with 76.0% average accuracy over all VTAB subgroups. It confirms our hypothesis that the adapter should follow the frozen FFN module because it can then post-hoc modify the features flowing through the network. The parallel configuration comes in second with 75.6% average accuracy, receiving the same input as the FFN but having to\"}"}
{"id": "CVPR-2024-111", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2.\\n\\n| Components                        | Init. | Scaling | Bias | Lin. | Layer | Channel | Accuracy (%), \u00b1 std. dev. |\\n|-----------------------------------|-------|---------|------|------|-------|---------|----------------------------|\\n| Bias Norm Scaling Initialization  | Houlsby | 76.0    | 0.2  | \u2713    | \u2206 base| \u2713       | 75.6 \u00b1 0.4                 |\\n|                                  | Houlsby | 75.5    | 0.3  | \u2713    |       | \u2713       | 75.8 \u00b1 0.5                 |\\n|                                  | BERT   | 75.8    | 0.3  | \u2713    |       |         | 75.8 \u00b1 0.2                 |\\n|                                  | Houlsby | 75.9    | 0.3  | \u2713    |       | \u2713       | 76.2 \u00b1 0.3                 |\\n|                                  | Houlsby | 76.5    | 0.2  | \u2713    | \u2713     |         | 75.8 \u00b1 0.2                 |\\n\\nWhat makes a great adapter? From our systematic exploration of possible adapter configurations, we conclude that adapter modules in the Post-Adapter position with a learnable, channel-wise scaling and Houlsby initialization work best for computer vision tasks. We call our proposed adapter configuration Adapter+. The addition of layer normalization, as suggested by Pfeiffer et al. [47], is not necessary and even leads to detrimental effects in our setting.\\n\\nConfigurations from previous work.\\n\\nDifferent configurations of adapters have been established in previous work. We compare their configurations to our systematic approach with rank $r = 8$ on the VTAB validation sets. Using our own implementations already leads to better results than reported in literature but enables us to compare on equal footing.\\n\\nTable 3.\\n\\n| Configuration          | # Param (M) | Natural Specialized Structured Average |\\n|------------------------|-------------|--------------------------------------|\\n| Houlsby [27], $r = 8$  | 0.39        | 82.9 \u00b1 0.2 85.5 \u00b1 0.3 58.9 \u00b1 0.8 75.8 \u00b1 0.3 |\\n| Houlsby [27], $r = 4$  | 0.24        | 82.9 \u00b1 0.4 84.9 \u00b1 0.3 58.3 \u00b1 0.6 75.4 \u00b1 0.3 |\\n| Pfeiffer [47]          | 0.21        | 82.9 \u00b1 0.3 86.1 \u00b1 0.9 58.4 \u00b1 0.7 75.8 \u00b1 0.4 |\\n| AdaptFormer [6]        | 0.19        | 83.0 \u00b1 0.4 85.0 \u00b1 0.2 57.4 \u00b1 0.5 75.2 \u00b1 0.2 |\\n| Adapter+               | 0.20        | 83.0 \u00b1 0.2 86.8 \u00b1 0.6 59.7 \u00b1 0.4 76.5 \u00b1 0.2 |\\n\\nWherever possible, we re-evaluate the other methods with a suitable data normalization for the pre-trained backbone and after the full training schedule to enable a fair comparison. For LoRA, we use our own implementation because the original work does not cover VTAB. For VPT, we adopt the number of tokens per task from their hyperparameter optimization but find that we do not need to tune learning rate and weight decay per task. Additionally, deviating from the original implementation, we optimize with AdamW [39] instead of SGD [52] and change to an appropriate data normalization. We present the original results from [30] on VTAB together with our re-evaluation. Our improved implementation of VPT increases the average accuracy by 4.4 pp from 72.0% to 76.4%.\\n\\nSSF, FacT, and SPT released code to evaluate on VTAB. For FacT and SPT, we change the data normalization to match the backbone; SSF already uses the correct one. We re-run the provided code and present the results.\"}"}
{"id": "CVPR-2024-111", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 4: Detailed results on the VTAB test sets\\n\\nWe report original results and re-evaluations (\u27f3) in % after a complete training schedule with suitable data normalization. Grayed out numbers are not included in the ranking for best and second best results.\\n\\n| Natural Specialized Structured | # Param (M) | Cifar100 | Caltech101 | DTD | Flower102 | Pets | SVHN | Sun397 | Average | Camelyon | EuroSAT | Resisc45 | Retinopathy | Average | Clevr-Count | Clevr-Dist. | DMLab | KITTI-Dist. | dSpr-Loc. | dSpr-Ori | sNORB-Azi. | sNORB-Ele. | Average |\\n|-------------------------------|-------------|----------|------------|-----|-----------|------|------|--------|---------|---------|--------|---------|------------|---------|-------------|------------|-------|----------|----------|---------|-----------|-----------|---------|----------|\\n| Adapter+                      | r \u2208 [1..8]  | 0.16     | 85.4       | 93.8| 72.7      | 99.1 | 90.6 | 87.6   | 96.8    | 86.7    | 72.9   | 86.2   | 83.4     | 60.9    | 53.8        | 80.8      | 55.2  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..32] | 0.27     | 85.4       | 93.8| 72.7      | 99.1 | 90.7 | 87.6   | 96.8    | 87.8    | 73.9   | 86.5   | 83.4     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.9      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..4]  | 0.11     | 85.4       | 93.8| 72.7      | 99.1 | 90.6 | 87.5   | 96.8    | 87.0    | 73.9   | 86.5   | 83.4     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.4      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..8]  | 0.16     | 85.4       | 93.8| 72.7      | 99.1 | 90.7 | 87.6   | 96.8    | 86.7    | 72.3   | 85.9   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..32] | 0.27     | 85.4       | 93.8| 72.7      | 99.1 | 90.6 | 87.6   | 96.8    | 87.8    | 73.9   | 86.5   | 83.4     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.9      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..4]  | 0.11     | 85.4       | 93.8| 72.7      | 99.1 | 90.6 | 87.5   | 96.9    | 88.2    | 73.9   | 86.5   | 83.4     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.4      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..8]  | 0.16     | 85.4       | 93.8| 72.7      | 99.1 | 90.7 | 87.6   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..32] | 0.27     | 85.4       | 93.8| 72.7      | 99.1 | 90.6 | 87.6   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..4]  | 0.11     | 85.4       | 93.8| 72.7      | 99.1 | 90.6 | 87.5   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..8]  | 0.16     | 85.4       | 93.8| 72.7      | 99.1 | 90.7 | 87.6   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..32] | 0.27     | 85.4       | 93.8| 72.7      | 99.1 | 90.6 | 87.6   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..4]  | 0.11     | 85.4       | 93.8| 72.7      | 99.1 | 90.6 | 87.5   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..8]  | 0.16     | 85.4       | 93.8| 72.7      | 99.1 | 90.7 | 87.6   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..32] | 0.27     | 85.4       | 93.8| 72.7      | 99.1 | 90.6 | 87.6   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..4]  | 0.11     | 85.4       | 93.8| 72.7      | 99.1 | 90.6 | 87.5   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..8]  | 0.16     | 85.4       | 93.8| 72.7      | 99.1 | 90.7 | 87.6   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..32] | 0.27     | 85.4       | 93.8| 72.7      | 99.1 | 90.6 | 87.6   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..4]  | 0.11     | 85.4       | 93.8| 72.7      | 99.1 | 90.6 | 87.5   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..8]  | 0.16     | 85.4       | 93.8| 72.7      | 99.1 | 90.7 | 87.6   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..32] | 0.27     | 85.4       | 93.8| 72.7      | 99.1 | 90.6 | 87.6   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..4]  | 0.11     | 85.4       | 93.8| 72.7      | 99.1 | 90.6 | 87.5   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..8]  | 0.16     | 85.4       | 93.8| 72.7      | 99.1 | 90.7 | 87.6   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..32] | 0.27     | 85.4       | 93.8| 72.7      | 99.1 | 90.6 | 87.6   | 97.0    | 86.7    | 72.3   | 86.2   | 83.2     | 60.9    | 53.8        | 80.8      | 55.3  | 37.9     | 46.9     | 63.3    | 77.7      | 55.6      | 35.7   |\\n| Adapter+                      | r \u2208 [1..4]  | 0.11     |"}
{"id": "CVPR-2024-111", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. Detailed results on the FGVC test sets. We report original results and re-evaluations (\u27f3) in % after a complete training schedule with suitable data normalization. Grayed out numbers are not included in the ranking for best and second best results.\\n\\n| # Param (M) | CUB200 | NABirds | Oxford Flowers | Stanford Dogs | Stanford Cars | Average |\\n|------------|--------|---------|----------------|---------------|--------------|---------|\\n| Full       | 86.0   | 88.0    | 81.5           | 99.2          | 85.6         | 90.6    |\\n| Linear     | 0.18   | 88.9    | 81.8           | 99.5          | 92.6         | 52.8    |\\n| VPT-Deep   | 0.85   | 88.5    | 84.2           | 99.0          | 90.2         | 83.6    |\\n| VPT-Deep (\u27f3) | 0.85 | 90.1 | 83.3 | 99.6 | 90.3 | 85.0 |\\n| SSF        | 0.39   | 89.5    | 85.7           | 99.6          | 89.6         | 89.2    |\\n| SSF (\u27f3)   | 0.39   | 88.9    | 85.0           | 99.6          | 88.9         | 88.9    |\\n| SPT-Adapter | 0.40 | 89.1 | 83.3 | 99.2 | 91.1 | 86.2 |\\n| SPT-LoRA  | 0.52   | 88.6    | 83.4           | 99.5          | 91.4         | 87.3    |\\n| Adapter+ (best epoch) | 0.34 | 90.4 | 85.0 | 99.7 | 92.6 | 89.1 |\\n\\nAdditionally provide numbers for that setting in Tab. 5 for the sake of comparability. Even when training for a complete schedule, Adapter+ shows the best average accuracy with 90.7% over all five datasets in FGVC, 0.4 pp over the second best method under similar evaluation. When early stopping with the test set, Adapter+ reaches 91.4% average accuracy, 0.7 pp over the second best method and 2.4 pp better than full fine-tuning. This demonstrates that Adapter+ also yields state-of-the-art results for task adaptation when training data is abundant while having the best parameter efficiency.\\n\\n4.5. Ablations\\n\\nData normalization. We showcase the effect of using an unsuitable data normalization for the chosen ViT in Tab. 6. The gap between ImageNet and Inception normalization (see Sec. 3.5) is largest for VPT [30], with a 3.4 pp difference in average accuracy, which explains around two-thirds of the gain for our re-evaluation as shown in Fig. 1. We suspect that VPT has less of an ability to scale and shift the data because the learnable tokens only act on the attention mechanism. LoRA [28], FacT [31], and adapters all employ linear layers that can directly scale and shift the features of the frozen backbone and thus compensate better for improper data normalization. It is worth mentioning that our Adapter+ is the most robust to improper normalization out of the methods evaluated, with a gap of only 2.6 pp average accuracy.\\n\\nTraining regularization. We investigate the importance of training regularization methods like stochastic depth [29] and dropout [15] for training adapters on a frozen ViT backbone and evaluate on the VTAB validation sets. We use linearly increasing drop rates as a function of network depth from 0 to 0.1 for the frozen layers of the ViT model, and a drop rate of 0.1 when using dropout or stochastic depth for the adapter modules. The results in Tab. 7 show a clear benefit for using stochastic regularization for the frozen layers as well as the adapters during training. Using dropout in the adapters is only slightly better than no regularization for adapters, with a gain of only 0.1 pp. With an increase in accuracy of 0.7 pp, stochastic depth is the preferred regularization method for adapters. However, our results show that the more important part is the stochastic depth regularization for the frozen modules of the ViT backbone. Disabling it in training leads to a loss of 1.5 pp accuracy compared to a training where stochastic depth is used throughout the model.\\n\\n5. Conclusion\\n\\nApplied at the right position and with an optimal inner structure, the simple concept of adapters produces state-of-the-art results for task adaptation. To understand how adapters can \\\"strike back\\\", we conducted the first systematic and in-depth study on how to best construct adapters and integrate them with vision transformers. This allowed us to determine the optimal connection point for the adapter in the transformer layer. Further, we proposed to use a learnable, channel-wise scaling and showed its benefit for computer vision tasks. Our insights led us to the creation of Adapter+ that yields the highest accuracy and the best parameter-accuracy trade-off on VTAB (77.6%, 0.2M) without any per-task hyperparameter optimization and on FGVC (90.7%, 0.34M), showing its superiority over more complicated methods.\\n\\nAcknowledgements. This work has been funded by the LOEWE initiative (Hesse, Germany) within the emergenCITY center.\"}"}
