{"id": "CVPR-2023-295", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. (a) Masks obtained with Eq. 7, where all the \\\\( L^{-1} \\\\) masks along the \\\\( k \\\\) dimension are the same. (b) Masking the similarity tensor \\\\( W \\\\). (c) SAB alignment.\\n\\n\\\\( L_{SAB} \\\\) is the SAB loss.\\n\\nIt is difficult to find the optimal correspondence between the student and the teacher. The recent TAB works (e.g., [5]) still cannot help the student learn enough knowledge from the teacher when there is a large architecture gap between them, which is verified in our experiments. For our task, the student (MobileViT-v2) is a hybrid architecture, which has convolution layers in the shallow stages and separable self-attention layers in the later blocks. And a separable self-attention layer differs significantly from the traditional self-attention in two ways: 1) it does not learn an explicit attention map; 2) its input and output are still 3D, which are more similar to CNN features. Except for the separable self-attention in the later blocks, the shallow stages with convolution layers undoubtedly have a huge difference from the Transformer structure in CLIP, which has been investigated in previous works [31,41]. Therefore, we design SAB layer-wise alignment for KD of the intermediate layers from the Transformer to the CNN.\\n\\n**SAB Property.** As shown in Fig. 2, the student's layers in our SAB KD can be explained as the bases of the feature space, and each layer of the teacher is a linear combination of the bases. After training, if the student's layers do ideally form the bases, the knowledge of the teacher's layers is learned completely by the student. Our experiment in Section 4.2 shows that the teacher's features in different layers can be well recovered from the features of the student's layers. Next, we describe how to implement SAB KD.\\n\\n**Feature Reshaping and Similarity Computation.**\\n\\nLet the layers of the teacher and the student be\\n\\n\\\\[\\n\\\\text{Layer}_T = [L_{T1}, L_{T2}, \\\\ldots, L_{TM}]\\n\\\\]\\n\\nand\\n\\n\\\\[\\n\\\\text{Layer}_S = [L_{S1}, L_{S2}, \\\\ldots, L_{SN}]\\n\\\\]\\n\\n(usually \\\\( M > N \\\\)), respectively.\\n\\n\\\\( L_{Tk} \\\\in \\\\mathbb{R}^{D \\\\times L} \\\\) is a 2D tensor, where \\\\( D \\\\) is the dimensionality of the token feature and \\\\( L \\\\) is the number of tokens, while \\\\( L_{Sk} \\\\in \\\\mathbb{R}^{C_k \\\\times H_k \\\\times W_k} \\\\) is a 3D tensor, where \\\\( C_k \\\\), \\\\( H_k \\\\) and \\\\( W_k \\\\) are the channel number, height and width of the \\\\( k \\\\)th CNN layer's feature.\\n\\nWe calculate a similarity tensor \\\\( W \\\\) between \\\\( L_{Sk} \\\\) and \\\\( L_{Tk} \\\\), which need to be reshaped first as shown in Figs. 3(a) and (b), respectively. For \\\\( L_{Sk} \\\\), we first apply a learnable linear operator and then do pixel reshuffle on the result, obtaining \\\\( \\\\bar{L}_{Sk}, k = 1, 2, \\\\ldots, N \\\\). Next, \\\\( \\\\bar{L}_{Sk} \\\\) is reshaped to \\\\( \\\\tilde{L}_{Sk} \\\\). As for \\\\( L_{Tk} \\\\), we remove the class token and obtain a \\\\((L-1) \\\\times D\\\\) tensor, denoted as \\\\( \\\\tilde{L}_{Tk}, k = 1, 2, \\\\ldots, M \\\\). Finally, all \\\\( \\\\tilde{L}_{Sk}, k = 1, 2, \\\\ldots, N \\\\), and \\\\( \\\\tilde{L}_{Tk}, k = 1, 2, \\\\ldots, M \\\\), are represented as\\n\\n\\\\[\\n\\\\tilde{\\\\text{Layer}}_S = [\\\\tilde{L}_{S1}, \\\\tilde{L}_{S2}, \\\\ldots, \\\\tilde{L}_{SN}]\\n\\\\]\\n\\nand\\n\\n\\\\[\\n\\\\tilde{\\\\text{Layer}}_T = [\\\\tilde{L}_{T1}, \\\\tilde{L}_{T2}, \\\\ldots, \\\\tilde{L}_{TM}]\\n\\\\]\\n\\nrespectively. After this reshaping, as shown in Fig. 3(c), we are able to measure the similarities between the intermediate layers of the student and the teacher through\\n\\n\\\\[\\nW = \\\\tilde{\\\\text{Layer}}_T \\\\times (\\\\tilde{\\\\text{Layer}}_S)^\\\\top, W \\\\in \\\\mathbb{R}^{(L-1) \\\\times M \\\\times N}.\\n\\\\]\\n\\n**Masking and SAB Alignment.** To speed up the training procedure, we design SAB KD with sequential masks, which follow this formula:\\n\\n\\\\[\\n\\\\text{mask}(i, j) = \\\\begin{cases} \\n0, & \\\\text{if } j > n_0 \\\\\\\\\\n0, & \\\\text{if } j > n_1 \\\\\\\\\\n0, & \\\\text{if } j > n_0 \\\\text{ and } m_0 < i \\\\leq m_1 \\\\\\\\\\n1, & \\\\text{else}\\n\\\\end{cases}\\n\\\\]\\n\\nwhere \\\\( i = 1, 2, \\\\ldots, M \\\\), \\\\( j = 1, 2, \\\\ldots, N \\\\), \\\\( 1 \\\\leq m_0 < m_1 < M \\\\), \\\\( 1 \\\\leq n_0 < n_1 < N \\\\). And the final attention mask \\\\( \\\\text{Mask} \\\\in \\\\mathbb{R}^{(L-1) \\\\times M \\\\times N} \\\\) is composed of \\\\( L^{-1} \\\\) same masks, Fig. 4(a) shows one example with \\\\( m_0 = 1 \\\\), \\\\( n_0 = 2 \\\\), \\\\( m_1 = 3 \\\\), \\\\( n_1 = 3 \\\\).\\n\\n\\\\( \\\\text{mask}(i, j) = 0 \\\\) means to mask all the elements along the \\\\( k \\\\) dimension (Fig. 4(a)) at \\\\((i, j)\\\\) in \\\\( W \\\\), while \\\\( \\\\text{mask}(i, j) = 1 \\\\) means to maintain their similarities, as shown in Fig. 4(b). This masking is based on our experimental finding: The student's lower-level layers\"}"}
{"id": "CVPR-2023-295", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the linear combination of $N$ and the caption-to-video distribution $A$\\n\\nfinally, the SAB KD loss is defined as:\\n\\nLayer in Fig. 4(c), the weighted student layers are calculated as:\\n\\n$\\\\tilde{\\\\sigma}$ denotes the element-wise product. Guided by $= W$\\n\\nof CLIP.\\n\\nbutions of the teacher and local video-caption distributions\\n\\nto-video ($A$, respectively, with\\n\\n$1 = 9.8\\\\% @1$ and 8.2\\\\% @1\\n\\nThe similarity between the $w$ of frames in\\n\\ntively define the local video-to-caption and caption-to-video\\ndistributions to guide the training of the student\\n\\nTo further adapt the student's features to the teacher's multi-\\n\\n3.3.3 Cross-Modality Knowledge distillation\\n\\nGlobal Video-Caption Distribution Alignment\\n\\nLocal Video-Caption Distribution Alignment\\n\\nGlobal Video-Caption Distribution Alignment\\n\\nWe transfer this pre-training knowledge to the student through a local frame-word alignment\\n\\nto image-classification. CLIP fills them with different words\\n\\nimage classification. CLIP fills them with different words\\n\\n\u201cA picture of a cat\u201d and \u201cA picture of a dog\u201d). It can match\\n\\n(e.g., \u201ccat\u201d and \u201cdog\u201d) and results in different captions (e.g.,\\n\\nspecifically, we characterize the cross-modal distributions to guide the training of the stu-\\n\\nin the cross-modality distributions, we employ the teacher's knowledge of\\n\\nthe captions to the corresponding images, showing some\\n\\n\u201cA picture of a cat\u201d and \u201cA picture of a dog\u201d). It can match\\n\\n(image classification. CLIP fills them with different words\\n\\n\u201cA picture of a cat\u201d and \u201cA picture of a dog\u201d). It can match\\n\\n(e.g., \u201ccat\u201d and \u201cdog\u201d) and results in different captions (e.g.,\\n\\nThe similarity between the $w$ of frames in\\n\\n$w$/$v$/$t$ and\\n\\nsimilarities as:\\n\\n$\\\\sigma$/$\\\\tilde{\\\\sigma}$/$\\\\sigma$, where\\n\\nis the softmax function along the first dimension\\n\\n$\\\\sigma$/$\\\\tilde{\\\\sigma}$/$\\\\sigma$, where\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\nis the softmax function along the first dimension\\n\\n"}
{"id": "CVPR-2023-295", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison with state-of-the-art models on MSR-VTT (1k split) for text-to-video retrieval. \\\"PT Datasets\\\": datasets used for pre-training the vision encoder. \\\"HT100M\\\": HowTo100M dataset [28]. \\\"C400M\\\": CLIP-400M dataset [30]. \\\"IN21K\\\": ImageNet21K dataset [9]. \\\"W2M\\\": WebVid-2M dataset [3]. \\\"C3M\\\": CC3M dataset [33]. All the methods in Table are the comparable methods with parameters less than 240M and flops less than 360G. CLIPPING does not use the temporal Transformer and CLIPPING\u2217 compresses the text encoder through TinyBERT [17]. It can be seen from Table 1 that the CLIPPING\u2217 based models also outperform the small model ALL-in-one-S even though they are smaller.\\n\\nTable 2. The Flops and parameters of each module in our method. We adopt 12 frames for each video.\\n\\nFigure 5. Flops and Performances.\\n\\nOther Benchmarks. We also compare our CLIPPING with the state-of-the-art Frozen [3], MDMMT [11], NoiseEst [2] and SupportSet [29] on the LSMDC and MSVD datasets. In Table 3, CLIPPING again obtains the best performance with fewest parameters and Flops, which achieves 88.1%\u201392.9% of the performance of its teacher on the LSMDC and MSVD datasets, showing consistent improvements and generalization across different datasets.\\n\\n4.2. Ablation Study\\n\\nKey Components. We provide detailed ablation study to validate each key component of our proposed method.\\n\\nTable 4. Ablation study of different KD components of CLIPPING on the 1k validation set of MSR-VTT. The first row is the results of the teacher model (Clip4clip). T, S, SAB, CMG and CM_L denote temporal KD, spatial KD, SAB KD, global cross-modality KD and local cross-modality KD, respectively.\\n\\nTable 5. Ablation study of different KD types on MSR-VTT (1k split). All the models are trained for 36 epochs with the same setting (see the supplementary materials for details).\"}"}
{"id": "CVPR-2023-295", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. (a) Feature tensors of the student and the teacher. (b) An example to demonstrate that the teacher's features are the linear combinations of the student features. More examples are provided in the supplementary materials.\\n\\nFigure 7. Visualization of the frame-word alignments. (a) CLIP; (b) \\\\textsc{clipping}$\\\\text{\u2212}$\\\\textsc{tab}; (c) \\\\textsc{clipping}.\\n\\nKD Types. In Table 5, we compare the proposed SAB KD with the previous OL2OL KD and TAB KD. For SAB and TAB, the same layers of the student and the teacher are used for KD, the details of which are given in the supplementary materials. From Table 5, we can see that TAB performs better than OL2OL, and our SAB outperforms TAB. In Section 3.3.2, we use masking to speed up the training. In this study, we also compare \u201cwith masking\u201d and \u201cwithout masking\u201d. The last two rows of Table 5 verify that the masking is beneficial.\\n\\nSAB Property. The student's layers in our SAB KD can be explained as the bases of the feature space, and each layer of the teacher is a linear combination of the bases (Section 3.2). In Fig. 6, we show that this property holds. In SAB KD, the student has 4 layers and the teacher has 12 layers. We randomly pick 3 image examples, and for each example, we select the features of one random token (in the $k$ dimension in Fig. 6(a)). After training, the linear combinations $\\\\tilde{W} \\\\times \\\\tilde{\\\\text{Layer}}_S = \\\\tilde{\\\\text{Layer}}_T w$ show feature patterns very similar to the teacher's features ($\\\\tilde{\\\\text{Layer}}_T$). This property verifies that the teacher's knowledge is fully absorbed by the student.\\n\\nIn Fig. 7, we visualize the frame-word alignments with and without SAB KD. Fig. 7(a) is the result of the CLIP in CLIP4clip. Fig. 7(b) is the result of \\\\textsc{clipping}$\\\\text{\u2212}$\\\\textsc{tab}, which is our model without SAB KD but with the previous TAB KD. It can be seen that the frame-word alignment with SAB KD (Fig. 7(c)) shows clear and correct word-level attentions (e.g., \u201cnaked\u201d, \u201cchild\u201d, \u201cruns\u201d and \u201cthrough\u201d) like CLIP. But with TAB KD, the most important words (e.g., \u201cnaked\u201d and \u201cchild\u201d) cannot be noticed; instead, some insignificant words ([CLS] and \u201ca\u201d) are activated. This study shows that the knowledge of pre-trained CLIP can be more effectively transferred to the student at the fine-tuning stage with our SAB.\\n\\n5. Conclusion\\n\\nIn this paper, we propose a novel and efficient knowledge distillation method that is specially designed for small vision-language models. It includes temporal KD, spatial KD, SAB KD and cross-modality KD. Especially, the SAB KD has the property of the student's layers being the bases of the feature space. After training, the teacher's features are the linear combinations of the bases, indicating that the student has fully absorbed the knowledge of the teacher. CLIPPING significantly outperforms the state-of-the-art and is comparable or even superior to many large pre-training models. In the future, we will apply CLIPPING to other vision-language models for compression.\\n\\nAcknowledgements\\n\\nWe gratefully acknowledge the support of MindSpore [1], CANN (Compute Architecture for Neural Networks) and Ascend AI Processor used for this research.\"}"}
{"id": "CVPR-2023-295", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Elad Amrani, Rami Ben-Ari, Daniel Rotman, and Alex Bronstein. Noise estimation using density estimation for self-supervised multimodal learning. In AAAI, 2021.\\n\\n[2] Max Bain, Arsha Nagrani, G\u00fcl Varol, and Andrew Zisserman. Frozen in Time: A Joint Video and Image Encoder for End-to-End Retrieval. In ICCV, 2021.\\n\\n[3] David Chen and William Dolan. Collecting highly parallel data for paraphrase evaluation. In ACL, 2011.\\n\\n[4] Defang Chen, JianPing Mei, Yuan Zhang, Can Wang, Zhe Wang, Yan Feng, and Chun Chen. Distillation with semantic calibration. In AAAI, 2021.\\n\\n[5] Pengguang Chen, Shu Liu, Hengshuang Zhao, and Jiaya Jia. Distilling Knowledge via Knowledge Review. In CVPR, 2021.\\n\\n[6] Xianing Chen, Qiong Cao, Yujie Zhong, Jing Zhang, Shenghua Gao, and Dacheng Tao. DearKD: Data-efficient early knowledge distillation for vision transformers. In ICLR, 2022.\\n\\n[7] Yinpeng Chen, Xiyang Dai, Dongdong Chen, Mengchen Liu, Xiaoyi Dong, Lu Yuan, and Zicheng Liu. Mobile-Former: Bridging MobileNet and Transformer. In CVPR, 2022.\\n\\n[8] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.\\n\\n[9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021.\\n\\n[10] Maksim Dzabraev, Maksim Kalashnikov, Stepan Komkov, and Aleksandr Petiushko. MDMMT: Multidomain Multimodal Transformer for Video Retrieval. In CVPR, 2021.\\n\\n[11] Tsu-Jui Fu, Linjie Li, Zhe Gan, Kevin Lin, William Yang Wang, Lijuan Wang, and Zicheng Liu. VIOLET: End-to-End Video-Language Transformers with Masked Visual-token Modeling. In arXiv:2111.1268, 2021.\\n\\n[12] Yuying Ge, Yixiao Ge, Xihui Liu, Dian Li, Ying Shan, Xiaohu Qie, and Ping Luo. Bridgeformer: Bridging video-text retrieval with multiple choice questions. In CVPR, 2022.\\n\\n[13] Jianyuan Guo, Kai Han, Han Wu, Yehui Tang, Xinghao Chen, Yunhe Wang, and Chang Xu. CMT: Convolutional Neural Networks Meet Vision Transformers. In CVPR, 2022.\\n\\n[14] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the Knowledge in a Neural Network. In arXiv preprint arXiv:1503.02531, 2015.\\n\\n[15] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021.\\n\\n[16] Xiaoqi Jiao, Yichun Yin, Lifeng Shang, Xin Jiang, Xiao Chen, Linlin Li, Fang Wang, and Qun Liu. TinyBERT: Distilling BERT for Natural Language Understanding. In EMNLP, 2020.\\n\\n[17] Yoon Kim and Alexander M. Rush. Sequence-level knowledge distillation. In EMNLP, 2016.\\n\\n[18] Pavan Kumar, Anasosalu Vasu, James Gabriel, Jeff Zhu, Oncel Tuzel, and Anurag Ranjan. An Improved One millisecond Mobile Backbone. In arXiv preprint arXiv:2206.04040, 2022.\\n\\n[19] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. In arXiv preprint arXiv:2201.12086, 2022.\\n\\n[20] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In NIPS, 2021.\\n\\n[21] Yanyu Li, Geng Yuan, Yang Wen, Eric Hu, Georgios Evangelidis, Sergey Tulyakov, Yanzhi Wang, and Jian Ren. EfficientFormer: Vision Transformers at MobileNet Speed. In arXiv:2206.01191, 2022.\\n\\n[22] Sihao Lin, Hongwei Xie, Bing Wang, Kaicheng Yu, Xiaojun Chang, Xiaodan Liang, and Gang Wang. Knowledge Distillation via the Target-aware Transformer. In CVPR, 2022.\\n\\n[23] Yifan Liu, Chunhua Shen, Changqian Yu, and Jingdong Wang. Efficient Semantic Video Segmentation with Per-Frame Inference. In ECCV, 2020.\\n\\n[24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin Transformer: Hierarchical Vision Transformer using Shifted Windows. In ICCV, 2021.\\n\\n[25] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. Clip4clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021.\\n\\n[26] Sachin Mehta and Mohammad Rastegari. Separable Self-attention for Mobile Vision Transformers. In ICLR, 2022.\\n\\n[27] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac, Makarand Tapaswi, Ivan Laptev, and Josef Sivic. HowTo100M: Learning a Text-Video Embedding by Watching Hundred Million Narrated Video Clips. In ICCV, 2019.\\n\\n[28] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian Metze, Alexander Hauptmann, Jo\u00e3o Henriques, and Andrea Vedaldi. Support-set bottlenecks for video-text representation learning. In ICLR, 2021.\\n\\n[29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, and Jack Clark. Learning transferable visual models from natural language supervision. ICML, 2021.\\n\\n[30] Maithra Raghu, Thomas Unterthiner, Simon Kornblith, Chiyuan Zhang, and Alexey Dosovitskiy. Do vision transformers see like convolutional neural networks? In NIPS, 2021.\\n\\n[31] Anna Rohrbach, Marcus Rohrbach, Niket Tandon, and Bernt Schiele. A dataset for Movie Description. In CVPR, 2015.\"}"}
{"id": "CVPR-2023-295", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018.\\n\\nAlex Jinpeng Wang, Yixiao Ge, Rui Yan, Yuying Ge, Xudong Lin, Guanyu Cai, Jianping Wu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. All in one: Exploring unified video-language pre-training. In arXiv preprint arXiv:2203.07303, 2022.\\n\\nJinpeng Wang, Yixiao Ge, Guanyu Cai, Rui Yan, Xudong Lin, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Object-aware video-language pre-training for retrieval. In CVPR, 2022.\\n\\nZekun Wang, Wenhui Wang, Haichao Zhu, Ming Liu, Bing Qin, and Furu Wei. Distilled dual-encoder model for vision-language understanding. In arXiv preprint arXiv:2112.08723, 2021.\\n\\nHu Xu, Gargi Ghosh, Po-Yao Huang, Dmytro Okhonko, Armen Aghajanyan, Florian Metze, Luke Zettlemoyer, and Christoph Feichtenhofer. Videoclip: Contrastive pre-training for zero-shot video-text understanding. arXiv preprint arXiv:2104.08860, 2021.\\n\\nJun Xu, Tao Mei, Ting Yao, and Yong Rui. MSR-VTT: A Large Video Description Dataset for Bridging Video and Language. 2016.\\n\\nJianwei Yang, Yonatan Bisk, and Jianfeng Gao. Taco: Token-aware cascade contrastive learning for video-text alignment. In ICCV, 2021.\\n\\nWeihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In CVPR, 2022.\\n\\nLi Yuan, Yunpeng Chen, Tao Wang, Weihao Yu, Yujun Shi, Zi-Hang Jiang, Francis E.H. Tay, Jiashi Feng, and Shuicheng Yan. Tokens-to-Token ViT: Training Vision Transformers From Scratch on ImageNet. In ICCV, 2021.\"}"}
{"id": "CVPR-2023-295", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nPre-training a vision-language model and then fine-tuning it on downstream tasks have become a popular paradigm. However, pre-trained vision-language models with the Transformer architecture usually take long inference time. Knowledge distillation has been an efficient technique to transfer the capability of a large model to a small one while maintaining the accuracy, which has achieved remarkable success in natural language processing. However, it faces many problems when applying KD to the multi-modality applications. In this paper, we propose a novel knowledge distillation method, named CLIPPING, where the plentiful knowledge of a large teacher model that has been fine-tuned for video-language tasks with the powerful pre-trained CLIP can be effectively transferred to a small student only at the fine-tuning stage. Especially, a new layer-wise alignment with the student as the base is proposed for knowledge distillation of the intermediate layers in CLIPPING, which enables the student's layers to be the bases of the teacher, and thus allows the student to fully absorb the knowledge of the teacher. CLIPPING with MobileViT-v2 as the vision encoder without any vision-language pre-training achieves 88.1%\u201395.3% of the performance of its teacher on three video-language retrieval benchmarks, with its vision encoder being 19.5x smaller. CLIPPING also significantly outperforms a state-of-the-art small baseline (ALL-in-one-B) on the MSR-VTT dataset, obtaining relatively 7.4% performance gain, with 29% fewer parameters and 86.9% fewer flops. Moreover, CLIPPING is comparable or even superior to many large pre-training models.\\n\\n1 In this paper, CLIPPING means cutting something to make it smaller through distilling.\\n\\n1. Introduction\\n\\nRecently, pre-training a vision-language model and then fine-tuning it on downstream tasks are a popular paradigm [12, 16, 20, 21, 30]. Pre-trained vision-language models (PVLMs) have achieved great success in many multi-modality tasks (e.g., video-text retrieval). However, PVLMs with the Transformer architecture (especially the vision stream) usually consume a huge amount of computation, making them difficult to be deployed on edge devices such as mobile phones. Recent small models [8, 14, 19, 22, 27, 40] show that combining Convolutional Neural Networks (CNNs) and Transformers as a hybrid architecture gets the best of both architectures, but the overall performance of these works is still far from satisfactory when compared to large pre-training models. Apparently, knowledge distillation (KD) [15] is an efficient technique to transfer the capability of a large model to a small one while maintaining the accuracy, which has achieved remarkable success in natural language processing (NLP) [17, 18]. In NLP, the knowledge distillation methods are usually performed at both the pre-training and the fine-tuning stages. However, the collection of the pre-training data for the pre-training knowledge distillation cost huge manpower in multi-modality applications (e.g., the pre-training dataset of CLIP is hard to obtain). Therefore, an efficient knowledge distillation method need to be explored for multi-modality applications. To this end, we propose a novel knowledge distillation method, named CLIPPING, where the plentiful knowledge of a large teacher model Clip4clip [26] that has been fine-tuned for video-language tasks with the powerful pre-trained CLIP [30] can be effectively transferred to a small student only at the fine-tuning stage. The contributions of CLIPPING are summarized below:\\n\\n1) We introduce an efficient approach to distill both the vision knowledge and the cross-modality knowledge from teacher to a small model at the fine-tuning stage. The resulting model shows strong performance on multi-modality.\"}"}
{"id": "CVPR-2023-295", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. (a) and (b) are previous methods of intermediate features' knowledge distillation.\\n\\nLet $L_j, j = 1, 2, \\\\ldots, M$, and $L_S_i, i = 1, 2, \\\\ldots, N$, are the outputs of the $j$th and $i$th layers of the teacher and the student, respectively. OL2OL is the most common One-Layer-to-One-Layer KD, where it selects some layers of the teacher and then distills the layers into the student one by one (e.g., from $L_T j$ to $L_S i$). Teacher-As-Base (TAB) KD uses all the teacher's layers to supervise each layer in the student, where each layer of the student (e.g., $L_S i$) learns the knowledge from all the selected layers of the teacher ($P_{M k=1} w'_{ik} L_T k$, where $w'_{ik}, k = 1, 2, \\\\ldots, M$, are the knowledge selection weights with $P_{M k=1} w'_{ik} = 1$).\\n\\n(c) Our Student-As-Base (SAB) KD enables each of the teacher's layers to pass its knowledge to all the student's layers (e.g., $L_T j \\\\rightarrow P_{N k=1} w_{jk} L_S k$, $P_{N k=1} w_{jk} = 1$). More detailed analysis is provided in Section 3.1.\\n\\n2) We propose a new layer-wise alignment scheme, called Student-As-Base (SAB), for knowledge distillation of the intermediate layers from the Transformer to the CNN in CLIPPING, where the student's layers can be regarded as the bases of the teacher's feature space, forcing the student model to fully absorb the knowledge of the teacher. In our experience, the SAB layer-wise alignment significantly outperforms the previous knowledge distillation methods. We believe it will become a popular paradigm for knowledge distillation of intermediate features.\\n\\n3) We present an effective cross-modal knowledge distillation, which includes knowledge from both the global and local video-caption distributions. We use the video-caption distributions of the teacher to guide the training of the student. Besides, the student can also benefit from the powerful pre-trained CLIP that obtains certain local frame-word attention ability via learning from massive data. Experiments show that this pre-training knowledge can be effectively transferred to the student when jointly trained with our SAB KD only at the fine-tuning stage.\\n\\n4) CLIPPING with MobileViT-v2 \\\\cite{27} as the vision encoder without any vision-language pre-training achieves 91.5\\\\%\u201395.3\\\\% of the performance of its teacher on MSR-VTT video-language retrieval benchmark, with its vision encoder being 19.5x smaller. CLIPPING also significantly outperforms a state-of-the-art small baseline (ALL-in-one-B) on the MSR-VTT dataset, obtaining relatively 7.4\\\\% performance gain, with 29\\\\% fewer parameters and 86.9\\\\% fewer flops. Moreover, CLIPPING is comparable or even superior to many large pre-training models.\\n\\n2. Relative Work\\n\\nVision-Language Retrieval.\\n\\nLearning from web-collected image-text data, large-scale Vision-Language Pre-training (VLP) models such as CLIP \\\\cite{30} have recently demonstrated great success across various downstream tasks. Nowadays, models such as Clip4clip \\\\cite{26} and MD-MMT \\\\cite{11} extended from the pre-trained model CLIP keep appearing for vision-language retrieval. There are also some end-to-end trainable models \\\\cite{3, 37}, which are designed to take advantage of both large-scale image and video captioning datasets. All-in-one \\\\cite{34} is the first work to consider both efficiency and performance for video-language retrieval tasks. It introduces a unified backbone that enables the representation learning of both video-text multimodal and unimodal inputs.\\n\\nKnowledge Distillation.\\n\\nKnowledge distillation (KD) \\\\cite{15} is a simple yet effective technique to improve the performance of a learning model. Earlier works transfer knowledge embedded in the \\\"logits\\\" learned in a large teacher model to a small student model without sacrificing much performance. Recent works \\\\cite{5, 6} use multiple layers of the teacher to supervise each layer in the student, where each layer of the student learns the knowledge from multiple layers of the teacher. \\\\cite{23} proposes a target-aware Transformer and enables the student to mimic each spatial component of the teacher in each distilled layer to boost the student's performance. For multi-modality KD, \\\\cite{36} designs a fusion-encoder model as the teacher and introduces cross-modal attention knowledge to train the dual-encoder student model. The distillation objective is applied at both the pre-training and the fine-tuning stages and helps the dual-encoder model learn interactions of different modalities. TinyBert \\\\cite{17} also introduces a two-stage learning framework that performs Transformer distillation at both the pre-training and the task-specific fine-tuning stages. In this paper, we also focus on KD for transferring the knowledge of a pre-training vision-language model to a small one but only at the fine-tuning stage.\\n\\nTransformers and CNNs.\\n\\nUsually, the pre-training model is a Transformer-based model \\\\cite{10, 25} and the small model is a CNN-based or hybrid architecture \\\\cite{19, 27}. For KD, most existing methods distill knowledge either from a Transformer to another Transformer (T2T) or from a CNN to another CNN (C2C) that computes the loss in the OL2OL or TAB style (see Fig. 1). To the best of our knowledge, KD from a Transformer to a CNN (T2C) has not been explored yet. The most related work \\\\cite{7} distills knowledge from a CNN to a Transformer (C2T) in the OL2OL way. However, previous works (e.g., \\\\cite{31}) have investigated the internal representation structures of ViTs and CNNs, and found striking differences between the two models, such as...\"}"}
{"id": "CVPR-2023-295", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Overview of CLIPPING. There are mainly four knowledge distillation (KD) parts: (1) Temporal KD. (2) Spatial KD. (3) SAB (Student-As-Base) KD. (4) Cross-modality KD. The CLIP's vision encoder is a Transformer. The temporal Transformer is an optional module for CLIPPING.\\n\\nViTs having highly similar representations throughout the model's layers, while CNNs showing obvious distinction of representations between lower and higher layers [31]. Considering such striking differences between ViTs and CNNs, the previous KD (OL2OL and TAB) may not be good distillation ways for our T2C task, which is verified by our experiments.\\n\\n3. Methodology\\n\\nWe propose an efficient approach to distill both the vision knowledge and the cross-modality knowledge from Clip4clip to a small model with MobileViT-v2 as its vision encoder for text-video retrieval task, which can get the best from both sides: powerful multimodal representations of CLIP and efficient mobile vision Transformer of MobileViT-v2. The overall architecture is illustrated in Fig. 2.\\n\\n3.1. Analysis of TAB and SAB\\n\\nWe first show the advantage of our SAB KD over traditional TAB KD. For simplicity, suppose $M = N = 2$.\\n\\nThen, for SAB,\\n\\n$$\\nL_{T1} = w_{11} L_{S1} + w_{12} L_{S2}, \\\\quad w_{11} + w_{12} = 1\\n$$\\n\\n$$\\nL_{T2} = w_{21} L_{S1} + w_{22} L_{S2}, \\\\quad w_{21} + w_{22} = 1\\n$$\\n\\n(1)\\n\\nand for TAB,\\n\\n$$\\nL_{S1} = w'_{11} L_{T1} + w'_{12} L_{T2}, \\\\quad w'_{11} + w'_{12} = 1\\n$$\\n\\n$$\\nL_{S2} = w'_{21} L_{T1} + w'_{22} L_{T2}, \\\\quad w'_{21} + w'_{22} = 1\\n$$\\n\\n(2)\\n\\nObviously, in Eq. 1, the knowledge of the teacher $L_{T1}$ or $L_{T2}$ is the linear combination of $L_{S1}$ and $L_{S2}$. In other words, $L_{S1}$ and $L_{S2}$ are the bases of $L_{T1}$ and $L_{T2}$ of the teacher. In Eq. 2, it can been seen that the teacher's $L_{T1}$ and $L_{T2}$ are as the bases for $L_{S1}$ and $L_{S2}$. Eq. 2 can be converted to:\\n\\n$$\\nL_{T1} = A \\\\cdot L_{S1} + B \\\\cdot L_{S2},\\nL_{T2} = C \\\\cdot L_{S1} + D \\\\cdot L_{S2},\\n$$\\n\\n(3)\\n\\nwhere\\n\\n$$\\nA = w'_{22} w'_{11} w'_{22} - w'_{12} w'_{21},\\nB = -w'_{12} w'_{11} w'_{22} - w'_{12} w'_{21},\\nC = -w'_{21} w'_{11} w'_{22} - w'_{12} w'_{21},\\nD = w'_{11} w'_{11} w'_{22} - w'_{12} w'_{21},\\n$$\\n\\nwhich looks like Eq. 1. It seems that in this case, the student's $L_{S1}$ and $L_{S2}$ could also serve as the bases for $L_{T1}$ and $L_{T2}$. However, (i) $A + B \\\\neq 1$ and $C + D \\\\neq 1$, but it is usually required in the previous KD that all the coefficients in each equation should be summed to be 1 [5,6,23]; (ii) when learning with TAB KD in Eq. 2, the coefficients A, B, C and D are uncontrollable, which may be arbitrary large during learning.\\n\\nFor the reasons above, although it seems that SAB and TAB have the same matrix representation, TAB is unable to get a similar result as SAB and more likely to get to a local minimum during training. Therefore, TAB KD is not equivalent to our SAB KD. When well trained, the student with this SAB KD fully learns the knowledge of the teacher because the teacher's $L_{Ti}$ can be obtained by the linear combination of the student's $L_{Si}$, $i = 1, 2, ..., N$. Our experiment also shows this property (see Section 4.2).\\n\\n3.2. Preliminaries\\n\\nGiven a batch of videos $V$ and captions $T$, Clip4clip learns a similarity function $s(v_i, t_j)$ to calculate the similar...\"}"}
{"id": "CVPR-2023-295", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Reshaping the features of (a) the student $L_S$ and (b) the teacher $L_T$ to the same shape. (c) Similarity Computation.\\n\\nIt obtains the frames' representation and the caption representation in a multi-modal embedding space via CLIP. The frames' representation is denoted as $Z_i = \\\\{z_{i1}, z_{i2}, ..., z_{in}\\\\}$, where $n$ is the number of frames in $v_i$ and $z_{ik}$ ($k = 1, 2, ..., n$) is the class token from the output of the CLIP's vision encoder corresponding to the $k$th input frame. The caption representation is denoted as $W_j = \\\\{w_{j1}, w_{j2}, ..., w_{jm}\\\\}$, where $m$ is the number of words in $t_j$ (including [CLS] and [SEP]) and $w_{jm}$ is used as the representation of $t_j$ in Clip4clip.\\n\\nClip4clip uses a temporal Transformer encoder to obtain the sequential feature, denoted as $Z'_i = \\\\{z'_{i1}, z'_{i2}, ..., z'_{in}\\\\} = \\\\text{TemporalTransformer}(Z_i)$. Then, the mean pooling is used to aggregate the features of all frames to obtain the video representation, $z_i = 1/n \\\\sum_{k=1}^{n} z'_{ik}$.\\n\\nFinally, the similarity functions $s(v_i, t_j)$ and $s(t_j, v_i)$ are defined as:\\n\\n$$s(v_i, t_j) = w_{jm}^\\\\top z_i, s(t_j, v_i) = z_{j}^\\\\top w_{im}.$$  \\n\\nNowadays, models such as Clip4clip based on the pre-trained model CLIP have been applied to many tasks. They pursue better performance with CLIP but have a heavy vision encoder, which makes them difficult to be deployed on edge devices such as mobile phones. Since MobileViT-v2 [27] is a light-weight and mobile-friendly hybrid network, we employ it as the vision encoder of the student model and maintain the original text encoder and temporal Transformer of Clip4clip. Note that the temporal Transformer is an optional module for the CLIPPING model structure, and the text encoder (CLIP bert) can be further compressed through the existing methods (details are shown in the Section 4).\\n\\n3.3. CLIPPING\\n\\nTo distill multimodal knowledge from Clip4clip to the MobileViT-v2-based model, we use four kinds of knowledge transfer: 1) temporal KD, 2) spatial KD, 3) SAB KD, and 4) cross-modality KD.\\n\\n3.3.1 Temporal and Spatial Knowledge Distillation\\n\\nThe temporal KD is motivated by the previous finding that aligning multi-frame dependency from the teacher to the student can enhance the performance of the student [24], which encodes the multi-frame dependency into a latent embedding by using a recurrent unit ConvLSTM. In our architecture, multi-frame dependency can be modeled naturally through the temporal Transformer (Temporal Transformer) in Fig. 2, so we define the temporal KD loss as:\\n\\n$$L_{TKD} = \\\\frac{1}{B} \\\\sum_{i=1}^{B} D_{KL}(Z'_{iS}, Z'_{iT}),$$  \\n\\nwhere $B$ is the batch size, the superscripts $S$ and $T$ denote the student and the teacher, respectively, and $D_{KL}$ is the KL divergence loss function. In addition to imitating the behavior of CLIP for each frame, we use spatial KD to align the spatial embeddings of the student and the teacher by:\\n\\n$$L_{SKD} = \\\\frac{1}{B} \\\\sum_{i=1}^{B} D_{KL}(Z_{Si}, Z_{Ti}).$$\\n\\nThe Temporal Transformer is an optional module for the CLIPPING model structure in our method. In this case, $Z'_{iS}$ is exactly equal to $Z_{Si}$ in $T_{KD}$.\\n\\n3.3.2 SAB Knowledge Distillation\\n\\nRecently, it is discovered that distilling intermediate features is more effective. So in addition to the spatial and temporal KD, we also force the vision encoder of the student to mimic the intermediate layers of the teacher. In the traditional OL2OL KD, some layers of the teacher are selected\"}"}
