{"id": "CVPR-2023-1252", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We present a mask-piloted Transformer which improves masked-attention in Mask2Former for image segmentation. The improvement is based on our observation that Mask2Former suffers from inconsistent mask predictions between consecutive decoder layers, which leads to inconsistent optimization goals and low utilization of decoder queries. To address this problem, we propose a mask-piloted training approach, which additionally feeds noised ground-truth masks in masked-attention and trains the model to reconstruct the original ones. Compared with the predicted masks used in mask-attention, the ground-truth masks serve as a pilot and effectively alleviate the negative impact of inaccurate mask predictions in Mask2Former. Based on this technique, our MP-Former achieves a remarkable performance improvement on all three image segmentation tasks (instance, panoptic, and semantic), yielding $+2.3$ AP and $+1.6$ mIoU on the Cityscapes instance and semantic segmentation tasks with a ResNet-50 backbone. Our method also significantly speeds up the training, outperforming Mask2Former with half of the number of training epochs on ADE20K with both a ResNet-50 and a Swin-L backbones. Moreover, our method only introduces little computation during training and no extra computation during inference. Our code will be released at https://github.com/IDEA-Research/MP-Former.\\n\\n1. Introduction\\n\\nImage segmentation is a fundamental problem in computer vision which includes semantic, instance, and panoptic segmentation. Early works design specialized architectures for different tasks, such as mask prediction models [2, 15] for instance segmentation and per-pixel prediction models [25, 28] for semantic segmentation. Panoptic segmentation [18] is a unified task of instance and semantic segmentation. But universal models for panoptic segmentation such as Panoptic FPN [18] and K-net [34] are usually sub-optimal on instance and semantic segmentation compared with specialized models.\\n\\nRecently, Vision Transformers [29] has achieved tremendous success in many computer vision tasks, such as image classification [11, 24] and object detection [1]. Inspired by DETR's set prediction mechanism, Mask2Former [7] proposes a unified framework and achieves a remarkable performance on all three segmentation tasks. Similar to DETR, it uses learnable queries in Transformer decoders to probe features and adopts bipartite matching to assign predictions to ground truth (GT) masks. Mask2Former also uses predicted masks in a decoder layer as attention masks for the next decoder layer. The attention masks serve as a guidance to help next layer's prediction and greatly ease training.\\n\\nDespite its great success, Mask2Former is still an initial attempt which is mainly based on a vanilla DETR model that has a sub-optimal performance compared to its later variants. For example, in each decoder layer, the predicted masks are built from scratch by dot-producting queries and feature map without performing layer-wise refinement as proposed in deformable DETR [37]. Because each layer builds masks from scratch, the masks predicted by a query in different layers may change dramatically. To show this inconsistent predictions among layers, we build two metrics $mIoU_L$ and $Util_i$ to quantify this problem and analyze its consequences in Section 3. As a result, this problem leads to a low utilization rate of decoder queries, which is especially severe in the first few decoder layers. As the attention masks predicted from an early layer are usually inaccurate, when serving as a guidance for its next layer, they will lead to incoherent predictions among layers.\"}"}
{"id": "CVPR-2023-1252", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"sub-optimal predictions in the next layer. Since detection is a similar task to segmentation, both aiming to locate instances in images, we seek inspirations from detection to improve segmentation. For example, the recently proposed denoising training [19] for detection has shown very effective to stabilize bipartite matching between training epochs. DINO [33] achieves a new SOTA on COCO detection built upon an improved denoising training method. The key idea in denoising training is to feed noised GT boxes in parallel with learnable queries into Transformer decoders and train the model to denoise and recover the GT boxes. Inspired by DN-DETR [19], we propose a mask-piloted (MP) training approach. We divide the decoder queries into two parts, an MP part and a matching part. The matching part is the same as in Mask2Former, feeding learnable queries and adopting bipartite matching to assign predictions to GT instances. In the MP part, we feed class embeddings of GT categories as queries and GT masks as attention masks into a Transformer decoder layer and let the model to reconstruct GT masks and labels. To reinforce the effectiveness of our method, we propose to apply MP training for all decoder layers. Moreover, we also add point noises to GT masks and flipping noises to GT class embeddings to force the model to recover GT masks and labels from inaccurate ones.\\n\\nWe summarize our contributions as follows.\\n\\n1. We propose a mask-piloted training approach to improving masked-attention in Mask2Former, which suffers from inconsistent and inaccurate mask predictions among layers. We also develop techniques including multi-layer mask-piloted training with point noises and label-guided training to further improve the segmentation performance. Our method is computationally efficient, introducing no extra computation during inference and little computation during training.\\n\\n2. Through analyzing the predictions of Mask2Former, we discover a failure mode of Mask2Former\u2014inconsistent predictions between consecutive layers. We build two novel metrics to measure this failure, which are layer-wise query utilization and mean IoU. We also show that our method improves the two metrics by a large margin, which validates its effectiveness on alleviating inconsistent predictions.\\n\\n3. When evaluating on three datasets, including ADE20k, Cityscapes, and MS COCO, our MP-Former outperforms Mask2Former by a large margin. Besides improved performance, our method also speeds up training. Our model exceeds Mask2Former on all three segmentation tasks with only half of the number of training steps on ADE20k. Also, our model trained for 36 epochs is comparable with Mask2Former trained for 50 epochs on instance and panoptic segmentation on MS COCO.\\n\\n2. Related Work\\n\\nGenerally, image segmentation can be divided into three tasks including instance segmentation, semantic segmentation, and panoptic segmentation with respect to different semantics.\\n\\nTraditional Segmentation models\\n\\nTraditionally, researchers develop specialized models and optimization objectives for each task. Instance segmentation is to predict a set of binary masks and their associated categories. Previous methods often predict masks based on bounding boxes produced by detection models. Mask R-CNN [15] builds segmentation upon detection model Faster R-CNN [27] by adding a mask branch in parallel with the detection branch. HTC [2] further proposes to interleave the two branches and add mask information flow to improve segmentation performance. Semantic segmentation focus on category-level semantics without distinguishing instances. Previous models widely formulate it into a per-pixel classification problem. The pioneering work, FCN [25] generates a label for each pixel to solve this problem. Many follow-up works continue with this idea and design more precise pixel-level classification models [3, 4]. Panoptic segmentation [18, 30] is a combination of the two segmentation tasks above to segment both the foreground instances (\u201cthing\u201d) and background semantics (\u201cstuff\u201d).\\n\\nVision Transfomers for segmentation\\n\\nTransformer-based segmentation models emerge with DETR (DEtection TRansformer) [1]. Since that, Transformer [29] has been widely used in many detection [1, 19, 23, 33] and segmentation models [7, 8, 32]. Generally, these models adopt a set prediction objective with bipartite matching to query features of interest. Inspiring progress has been made in many specialized query-based architectures for instance segmentation [13], semantic segmentation [32, 35], panoptic segmentation [21]. Notably, some recent works unify the three segmentation tasks into one model [7, 8, 20, 34]. Among them, Mask2Former [7] proposes masked attention and achieves remarkable performance on all three segmentation tasks. Following DETR, Mask2Former is an end-to-end Transformer for image segmentation. It is composed of a backbone, a pixel decoder, and a Transformer decoder. The pixel decoder is an encoder-only deformable Transformer that takes multi-scale image features as input and output processed multi-scale features. The Transformer decoder takes learnable queries as input to attend to the feature maps output by the pixel decoder. Each decoder layer attends to a feature map of one scale following a coarse-to-fine manner. The predicted masks of one decoder layer will feed to the next layer as the attention masks, which enables the following decoder to focus on more meaningful regions. The design of attention masks eases the training and improves performance.\\n\\nEase Training for Vision transformers\\n\\nVision transformers have an advantage over the convolution-based methods due to their ability to capture long-range dependencies.\"}"}
{"id": "CVPR-2023-1252", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. We visualize layer-wise predictions of Mask2Former and show 4 pairs of failure cases. Each pair is the predictions of the same query in adjacent decoder layers. The red regions are the predicted masks. These cases shows that the predictions of a query may change dramatically between consecutive layers.\\n\\nHowever, it is hard to train because globally searching for an object is a hard task. This phenomenon exists in both detection and segmentation. In detection, DETR suffers from slow convergence requiring 500 training epochs for convergence. Recently, researchers have dived into the meaning of the learnable queries [23, 26, 31, 37]. They either express the queries as reference points or anchor boxes. [19, 33] proposed to add noised ground truth boxes as positional queries for denoising training and they speed up detection greatly. In segmentation, Mask2Former proposed mask attention which makes training easier and speeds up convergence when compared with MaskFormer. We have found some common points for these works that speed up training for vision Transformers. Firstly, they usually give clearer meaning to learnable queries to reduce ambiguity. Secondly, they give local limitations to cross attention and reduce the scope for the model to search objects. Therefore, we believe that giving guidance to cross attention leads to good segmentation performance. Our method is similar to DN-DETR [19] in introducing GT annotations to the Transformer decoder but ours is totally different from it in both the problems we solve and the method we take. Firstly, our method aims to address the inconsistent prediction problem which leads to the low utilization of queries, while DN-DETR aims to solve unstable matching among epochs. Secondly, we use GT masks as the attention masks, while DN-DETR feeds noised GT boxes as decoder queries. Thirdly, noises are optional in our method, but DN-DETR does not work without noises.\\n\\n3. Failure Cases and Solution\\n\\nFailure cases: We view the mask prediction in Mask2Former as a mask refinement process through Transformer decoder layers. By taking predictions from a layer as the attention masks of the next layer, we expected to obtain refined predictions from the next layer. Generally, the predicted masks are assigned to the GT masks by bipartite matching and refined in a coarse-to-fine procedure. Therefore, keeping refinement consistency in different layers is the key to refining the predicted masks consistently. However, when we visualize the predictions of Mask2Former layer by layer, we found a severe inconsistency problem among the predictions from the same query between consecutive decoder layers. As shown in Fig. 1, the predictions of adjacent layers differ significantly. We define a metric named layer-wise mean Intersection-over-Union (IoU) denoted as $\\\\text{mIoU-L}_i$ to quantify this inconsistency. We denote the predicted masks from the $i$-th decoder layer as $\\\\mathbf{M}_i = \\\\mathbf{M}_{i0}, \\\\mathbf{M}_{i1}, \\\\ldots, \\\\mathbf{M}_{iN-1}$, where $N$ is the number of decoder queries. Note that $\\\\mathbf{M}_0$ means the predictions before queries enter the first Transformer decoder layer. We have\\n\\n$$\\n\\\\text{mIoU-L}_i = \\\\frac{1}{N-1} \\\\sum_{n=0}^{N-1} \\\\text{IoU}(\\\\mathbf{M}_{i-1,n}, \\\\mathbf{M}_{i,n})\\n$$\\n\\nAs shown in Table 1, the $\\\\text{mIoU-L}_i$ is quite low, especially for the first few layers, which indicates inconsistent mask prediction results between adjacent layers. Such an inconsistent result means a query may be assigned to different instances (or background for panoptic and semantic segmentation) during layer-by-layer updates. To evaluate the inconsistent matching problem, we define another metric named as layer-wise query utilization denoted as $\\\\text{Util}_i$ for the $i$-th layer, which means the ratio of the matched queries in the $i$-th layer that matches exactly the same instance as in the last layer. We denote the ground truth objects as $\\\\mathbf{T} = \\\\{T_0, T_1, T_2, \\\\ldots, T_{O-1}\\\\}$ where $O$ is the number of ground truth objects. After bipartite matching, we compute an index vector $\\\\mathbf{V}_i = \\\\mathbf{V}_{i0}, \\\\mathbf{V}_{i1}, \\\\ldots, \\\\mathbf{V}_{iN-1}$ to store the matching result of layer $i$ as follows.\\n\\n$$\\n\\\\mathbf{V}_{in} = \\\\begin{cases} \\n\\\\text{o}, & \\\\text{if } \\\\mathbf{M}_{in} \\\\text{ matches } T_{O-1} \\\\\\\\\\n\\\\text{if } \\\\mathbf{M}_{in} \\\\text{ matches nothing} \\\\end{cases}\\n$$\\n\\nWe define the query utilization of layer $i$ as the proportion of GT masks that match with the same query in layer $i$.\"}"}
{"id": "CVPR-2023-1252", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. The mIoU-L\u1d62 and Util\u1d62 of Mask2Former and the proposed methods. The mIoU-L\u1d62 for a query is taken as the IoU of the predicted masks in the (\u1d62\u22121)-th and \ud835\udc22-th decoder layers. The mIoU-L\u1d62 takes the average over all queries. Note that the index of decoder layers starts from 1, so \u201cprediction of the 0-th decoder\u201d means the prediction before entering Transformer decoder. The Util\u1d62 is the ratio of GT masks that match with the same query in \ud835\udc22-th layer and the last layer. \u2217 means that we test Util\u1d62 with bipartite matching to assign predictions in MP part to GT masks. Note that when directly assigning predictions in MP part to corresponding GT masks with hard assignment, Util\u1d62 is always 100%.\\n\\nThe query utilization of layer \ud835\udc22 for the whole data set is averaged over the utilization numbers for all images. Table 1 shows that the query utilizations of the first few layers are too low, which indicate many matched queries in early layers are ended up not being used in the last layer. The low mIoU-L\u1d62 and Util\u1d62 have potential consequences such as inconstant optimization goals and low training efficiency.\\n\\nOur observations based on these two metrics motivate us to develop a more effective training method to make the matching more consistent among layers.\\n\\nSolution: To address the inconsistent prediction problem, inspired by DN-DETR, we proposed a mask-piloted training method. We feed GT masks as attention masks and expect these queries with GT masks can focus on their corresponding GT masks and not be distracted by other instances. As shown in row 2 and 3, Table 1, both mIoU-L\u1d62 and Util\u1d62 are improved by a large margin when adopting mask-piloted training. We also find that only applying mask-piloted (MP) training in the first layer is not enough. Fig 3(a)(b) shows that inconsistent prediction exists even when given GT masks. Therefore, we add a GT mask to all decoder layers to reinforce MP training. As shown in Table 1, multi-layer MP training further improves both mIoU-L\u1d62 and Util\u1d62 especially in latter decoder layers.\\n\\n4. MP-Former\\n\\n4.1. Overview\\n\\nOur method is a training method for improving Mask2Former. Our MP-Former only introduces minor changes in the Mask2Former pipeline during training as shown in the red-line part in Fig. 2. Same as Mask2Former, our model is composed of a backbone, a pixel decoder, and a Transformer decoder. Our improvements are on the Transformer decoder. We divide decoder queries into two parts, a mask-piloted (MP) part and a matching part. The matching part is the same as Mask2Former. The MP part feeds GT masks and GT class embeddings as input and the predictions are directly assigned to the corresponding GT masks without bipartite matching. Our method includes three components which are multi-layer MP training, mask noises, and label-guided training.\\n\\n4.2. Multi-layer MP training\\n\\nIn order to address the inconsistent prediction problem, we propose a multi-layer mask-piloted (MP) training method. In the MP part, we feed an additional set of queries and masks into Transformer where the queries and masks are GT class embeddings and GT masks. On the loss side, we independently assign the outputs of the MP part and the matching part with the GT instances. The predictions of the MP part are directly assigned to their corresponding GT masks and those in the matching part are assigned with bipartite graph matching. The losses for both parts are the same, following the loss design in Mask2Former. We also propose to add GT masks in multiple layers and show that this can further improve the performance with experiments in Table 8. Since we follow Mask2Former to use feature maps of different resolutions in different decoder layers, we interpolate GT masks into different resolutions when applied to different layers.\\n\\n4.3. Noised masks\\n\\nBesides feeding GT masks to alleviate the inconsistency problem, we further propose to use noised GT masks to make mask refinement more robust in each decoder. The intuition is simple, as Mask2former refines the mask from an inaccurate mask predicted from the previous decoder. GT masks without noise may be too easy for the task which prevents further refinement. Therefore, we feed noised masks to the decoder and train the model to reconstruct the original ones, which is similar to DN-DETR [19] for box denoising.\"}"}
{"id": "CVPR-2023-1252", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213\u2013229. Springer, 2020.\\n\\n[2] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4974\u20134983, 2019.\\n\\n[3] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis and machine intelligence, 40(4):834\u2013848, 2017.\\n\\n[4] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.\\n\\n[5] Liang-Chieh Chen, Huiyu Wang, and Siyuan Qiao. Scaling wide residual networks for panoptic segmentation. arXiv preprint arXiv:2011.11675, 2020.\\n\\n[6] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen. Panoptic-deeplab: A simple, strong, and fast baseline for bottom-up panoptic segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12475\u201312485, 2020.\\n\\n[7] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. 2022.\\n\\n[8] Bowen Cheng, Alexander G. Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. 2021.\\n\\n[9] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\\n\\n[10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Scharw\u00e4chter, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset. In CVPR Workshop on The Future of Datasets in Vision, 2015.\\n\\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n\\n[12] Xianzhi Du, Barret Zoph, Wei-Chih Hung, and Tsung-Yi Lin. Simple training strategies and model scaling for object detection. arXiv preprint arXiv:2107.00057, 2021.\\n\\n[13] Yuxin Fang, Shusheng Yang, Xinggang Wang, Yu Li, Chen Fang, Ying Shan, Bin Feng, and Wenyu Liu. Instances as queries. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6910\u20136919, 2021.\\n\\n[14] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2918\u20132928, 2021.\\n\\n[15] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017.\\n\\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013778, 2016.\\n\\n[17] Shihua Huang, Zhichao Lu, Ran Cheng, and Cheng He. Fapn: Feature-aligned pyramid network for dense image prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 864\u2013873, 2021.\\n\\n[18] Alexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Doll\u00e1r. Panoptic segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9404\u20139413, 2019.\\n\\n[19] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel M Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introducing query denoising. arXiv preprint arXiv:2203.01305, 2022.\\n\\n[20] Feng Li, Hao Zhang, Shilong Liu, Lei Zhang, Lionel M Ni, Heung-Yeung Shum, et al. Mask dino: Towards a unified transformer-based framework for object detection and segmentation. arXiv preprint arXiv:2206.02777, 2022.\\n\\n[21] Zhiqi Li, Wenhai Wang, Enze Xie, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, Tong Lu, and Ping Luo. Panoptic segformer. arXiv preprint arXiv:2109.03814, 2021.\\n\\n[22] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.\\n\\n[23] Shilong Liu, Feng Li, Hao Zhang, Xiao Yang, Xianbiao Qi, Hang Su, Jun Zhu, and Lei Zhang. Dab-detr: Dynamic anchor boxes are better queries for detr. arXiv preprint arXiv:2201.12329, 2022.\\n\\n[24] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 10012\u201310022, 2021.\\n\\n[25] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431\u20133440, 2015.\"}"}
{"id": "CVPR-2023-1252", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Additional detr for fast training convergence.\\n\\n[27] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015.\\n\\n[28] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015.\\n\\n[29] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017.\\n\\n[30] Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: End-to-end panoptic segmentation with mask transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5463\u20135474, 2021.\\n\\n[31] Yingming Wang, Xiangyu Zhang, Tong Yang, and Jian Sun. Anchor detr: Query design for transformer-based detector. arXiv preprint arXiv:2109.07107, 2021.\\n\\n[32] Enze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. Segformer: Simple and efficient design for semantic segmentation with transformers. Advances in Neural Information Processing Systems, 34, 2021.\\n\\n[33] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel M Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022.\\n\\n[34] Wenwei Zhang, Jiangmiao Pang, Kai Chen, and Chen Change Loy. K-net: Towards unified image segmentation. Advances in Neural Information Processing Systems, 34, 2021.\\n\\n[35] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6881\u20136890, 2021.\\n\\n[36] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision, 127(3):302\u2013321, 2019.\\n\\n[37] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR 2021: The Ninth International Conference on Learning Representations, 2021.\"}"}
{"id": "CVPR-2023-1252", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. The architecture of our method is the same as Mask2Former (the blue-shaded part), which consists of a backbone, a pixel decoder, and a Transformer decoder. The difference is that we feed extra queries and attention masks which are called the MP part to the Transformer decoder (red-line part in the figure). The MP part contains GT masks as attention masks and GT class embeddings as queries. We feed GT masks into the MP part of all decoder layers. We also add point noises to GT masks and flipping noises to class embeddings which can further improve the performance. Note that this architecture is just for training. In the inference time, the red-line part does not exist, and thus, our pipeline is exactly the same as Mask2Former.\\n\\nFigure 3. (a) The red region is a given GT mask in first decoder layer which covers the man riding the black horse. (b) The predicted mask of the first decoder layer which only covers the head of the man. The predicted mask will be used as the attention mask of the second layer and may mislead the second layer. (c)(d)(e) are demonstrations of point, shift and scale noises. The blue regions are noised masks and the red regions are the GT masks.\\n\\n| Method          | Backbone | PQ AP | Th Pan | mIoU Pan | mIoU S | mIoU M | AP L |\\n|-----------------|----------|-------|--------|----------|--------|--------|------|\\n| MaskFormer [8]  | R50      | 34.7  | -      | -        | -      | -      | -    |\\n| Panoptic-DeepLab [6] | SWideRNet [5] | 37.9  | -      | -        | -      | -      | -    |\\n| Mask2Former [7] | R50      | 39.7  | 26.5   | 46.1     | 26.4   | 10.4   | 28.9 |\\n| MP-Former       | R50      | 40.8  | (+1.1) | 27.1     | 48.3   | (+2.2) | (+1.6)|\\n| MaskFormer [8]  | Swin-L   | -     | -      | -        | -      | -      | 54.1 |\\n| FaPN-MaskFormer [8, 17] | Swin-L   | -     | -      | -        | -      | -      | 55.2 |\\n| Mask2Former [7] | Swin-L   | 48.1  | 34.2   | 54.5     | 34.9   | 16.3   | 40.0 |\\n| MP-Former       | Swin-L   | 49.4  | (+1.3) | 35.2     | 56.0   | 16.3   | 40.7 |\\n\\nTable 2. Image segmentation results on ADE20K val. MP-Former improves the performance on all three segmentation tasks with R50 and Swin-L as backbone. All metrics are evaluated with single-scale inference. Backbones pre-trained on ImageNet-22K are marked with \u2020. - means the results are not reported for the methods.\"}"}
{"id": "CVPR-2023-1252", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Image segmentation results on Cityscapes val. We report results on all three segmentation tasks with R50 or Swin-L as backbone. All metrics are evaluated with single-scale inference. Backbones pre-trained on ImageNet-22K are marked with \u2020.\\n\\n| Method          | Backbone | PQ    | AP    | mIoU   | AP50  | mIoU   |\\n|-----------------|----------|-------|-------|--------|-------|--------|\\n| Panoptic-DeepLab| R50      | 60.3  | 32.1  | 78.7   | -     | -      |\\n| Mask2Former     | R50      | 62.1  | 37.3  | 77.5   | 37.4  | 61.9   |\\n| MP-Former       | R50      | 62.7  | 39.3  | 79.6   | 39.7  | 65.4   |\\n| Mask2Former     | Swin-L   | 66.6  | 43.6  | 82.9   | 43.7  | 71.4   |\\n| MP-Former       | Swin-L   | 67.5  | 45.8  | 83.5   | 44.9  | 72.4   |\\n\\n4.4. Label-guided training\\n\\nWe use the class embeddings of GT categories as queries in the MP part. The reason why we use class embeddings is that queries will dot-product with image features and an intuitive way to distinguish instances is to use their categories. Note that we only use class embeddings in the first layer and do not replace predicted queries with class embeddings in subsequent layers. We also have a classification loss in the MP part. Since the classification task becomes very easy given GT class embeddings, we add flipping noises in class embeddings. We randomly sample GT class embeddings and let them randomly flip to the class embeddings of other categories. For example, \\\\( E_0, E_1, ..., E_K \\\\) denotes the class embeddings of \\\\( K \\\\) categories. An image contains \\\\( N \\\\) instances which belong to Categories \\\\( C_0, C_1, ..., C_N \\\\). So their default class embeddings should be \\\\( E_{C_0}, E_{C_1}, ..., E_{C_N} \\\\). To add flipping noise, we enumerate all \\\\( N \\\\) instances. For \\\\( i \\\\)-th instance, we generate a random number \\\\( r_i \\\\) in \\\\([0, 1]\\\\) to decide whether to flip the class embedding \\\\( E_{C_i} \\\\) or not. If \\\\( r_i < \\\\lambda_l \\\\), we randomly sample another class embedding in \\\\( E_0, E_1, ..., E_K \\\\) to replace \\\\( E_{C_i} \\\\). \\\\( \\\\lambda_l \\\\) denotes the proportion of the sampled class embeddings.\\n\\n5. Experiments\\n\\nWe conduct extensive experiments to compare our method with Mask2Former and other segmentation methods on all three segmentation tasks on three datasets to show the effectiveness of our method. We also conduct several ablation studies to show the effectiveness of each component in our method.\\n\\n5.1. Implementation details\\n\\nModel: Since our method is an improved training method based on Mask2Former, we adopt an identical model setting as Mask2Former except for the mask-piloted (MP) part. In the MP part, we adopt 100 GT masks with point noises. We adopt noise ratios \\\\( \\\\lambda_p = 0.2 \\\\) and \\\\( \\\\lambda_l = 0.2 \\\\) for point noises on masks and flipping noises on class embeddings. We adopt ResNet50 [16] and Swin-L [24] when evaluating our method on ADE20K and Cityscapes. On COCO2017, we use ResNet50 and ResNet101 as the backbone.\\n\\nDataset and metrics: We evaluate MP-Former on three challenging datasets: Cityscapes [9, 10] and ADE20K [36] for all three segmentation tasks and COCO 2017 dataset for panoptic segmentation and instance segmentation. For instance segmentation, we evaluate the mask AP [22] on instances that are denoted as \u201cthing\u201d categories in datasets. For semantic segmentation, we evaluate mean Intersection-over-Union (mIOU) over all categories including both foreground and background. For panoptic segmentation, we evaluate the results with the panoptic quality (PQ) metric [18].\"}"}
{"id": "CVPR-2023-1252", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. Instance segmentation on COCO val2017 with 80 categories. MP-Former outperforms strong baseline Mask2Former without extra computation cost in inference time. For a fair comparison, we only consider single-scale inference and models trained using only COCO train2017 set data. Backbones pre-trained on ImageNet-22K are marked with \u2020.\\n\\n| Method              | Backbone | Query Type | Epochs | PQ mIoU | AP mIoU | #Params | FLOPs | FPS |\\n|---------------------|----------|------------|--------|---------|---------|---------|-------|-----|\\n| DETR [1]            | R50      | 100 queries| 500+25 | 43.4    | 48.2    | 31.1    | -     | -   |\\n| MaskFormer [8]      | R50      | 100 queries| 300    | 46.5    | 51.0    | 39.8    | 33.0  | 57.8|\\n| Mask2Former [7]     | R50      | 100 queries| 50     | 51.0    | 54.5    | 42.7    | 41.9  | 61.5|\\n| MP-Former (ours)    | R50      | 100 queries| 50     | 52.2    | 58.3    | 42.9    | 42.5  | 61.6|\\n| DETR [1]            | R101     | 100 queries| 500+25 | 45.1    | 50.5    | 37.0    | 33.0  | -   |\\n| MaskFormer [8]      | R101     | 100 queries| 300    | 47.6    | 52.5    | 40.3    | 34.1  | 59.3|\\n| Mask2Former [7]     | R101     | 100 queries| 50     | 52.6    | 58.5    | 43.7    | 42.6  | 62.5|\\n| MP-Former (ours)    | R101     | 100 queries| 50     | 52.9    | 58.7    | 44.0    | 43.0  | 61.6|\\n\\nTable 5. Panoptic segmentation on COCO panoptic val2017 with 133 categories. MP-Former outperforms Mask2Former on COCO panoptic segmentation with both R50 and R101 as backbones. Results that are implemented are marked with \u2217.\\n\\n| Method              | Backbone | Query Type | Epochs | PQ | AP | mIoU |\\n|---------------------|----------|------------|--------|----|----|------|\\n| MaskFormer [8]      | S-L\u2020     | 100 queries| 300    | 52.7 | 58.5 | 44.0 |\\n| K-Net [34]          | S-L\u2020     | 100 queries| 36     | 54.6 | 60.2 | 46.0 |\\n| Mask2Former         | S-L\u2020     | 200 queries| 100    | 57.8 | 64.2 | 48.1 |\\n| MP-Former (ours)    | S-L\u2020     | 200 queries| 72     | 58.1 | 64.4 | 48.4 |\\n\\nPerformance on COCO. On COCO val2017, we follow Mask2Former's setting to train for 50 epochs. As shown in Table 4 and Table 5, MP-Former outperforms both Mask2Former and classical methods such as Mask R-CNN [15] on instance segmentation and panoptic segmentation. For instance segmentation, our method achieves +1.1 AP with R50, +0.9 AP with R101 and +0.3 AP with Swin-L [24]. Note that we only train MP-Former for 72 epochs with Swin-L. On panoptic segmentation, our method achieves +0.8 PQ with R50 and +0.3 PQ with R101. Notably, our method is only a pluggable training method that will not affect the model architecture or inference speed. In addition, it only introduces negligible parameters (class embeddings) during training.\\n\\n5.3. Speed up training\\nIn Table 6, we show our MP-Former can significantly speed up training on ADE20k and COCO. We do not report the results on Cityscapes because this dataset is relatively small and only needs 90k steps to converge in Mask2Former, which is not necessary for additional speedup. The results show that our method trained with 36 epochs achieved comparable results with Mask2Former trained with 50 epochs with both R50 and R101 as the backbone on both panoptic and instance segmentation. Moreover, our method trained with half number of training steps (80K steps) achieves comparable results with Mask2Former trained with 160K steps. Note that we use the same batch size as Mask2Former. We also report the clock time. The results show that our method reduces the actual training time by large. For a fair comparison, we use the same number of GPUs to run MP-Former and Mask2Former. Because there may exist differences in speed for different machines, we run our method and Mask2Former on the same machine to test the running time.\\n\\n5.4. Ablation Study\\nWe conduct an ablation study on the COCO instance segmentation task with R50 as backbone. Effectiveness of each component: We show the effectiveness of each version of our method trained for 12 epochs and 50 epochs in Table 7, where we show the result of raw Mask2Former and the result after each version of our method.\"}"}
{"id": "CVPR-2023-1252", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 6. Image segmentation results on ADE20K val and COCO val2017. MP-Former speeds up training by large. On ADE20K, MP-Former trained for 80K steps can achieve comparable result with Mask2Former trained for 160K steps with both R50 and Swin-L as backbone. On COCO, MP-Former trained for 36 epochs can achieve comparable result with Mask2Former trained for 50 epochs with both R50 and R101 as backbone.\\n\\n\u2217means that we adopt number of steps instead of epochs on ADE20k following Mask2Former. We report only single-scale inference results. Backbones pre-trained on ImageNet-22K are marked with \u2020.\\n\\nTable 7. The effectiveness of each component of our method when trained for 12 epochs and 50 epochs. The table shows that point noise does not work when trained for 50 epochs. Especially, our method without noises can improve the results by +0.9 AP. With multi-layer MP training, our method achieves +1.5 AP on COCO instance segmentation with R50 backbone trained for 12 epochs. \u201cep\u201d is short for epochs.\\n\\nLayers for MP training\\n\\n| ep | AP mask |\\n|----|---------|\\n| First layer | 12 | 39.4 |\\n| First three layers | 12 | 39.6 (+0.2) |\\n| All layers | 12 | 40.2 (+0.8) |\\n\\nNoise Type\\n\\n| No noise | 12 | 39.6 |\\n| Point noise | 12 | 39.9 |\\n| Shifting noise | 12 | 39.6 |\\n| Scaling noise | 12 | 39.4 |\\n\\nTable 8. Top 3 lines shows the effectiveness of multi-layer Mask-Piloted (MP) training. The bottom lines of the table shows that shifting and scaling noise do not work for our method. \u201cep\u201d is short for epochs.\\n\\nFeature map\\n\\n| ep | AP mask |\\n|----|---------|\\n| Coarse to fine | 12 | 40.2 |\\n| All large | 12 | 40.0 |\\n\\nAssignment in Mask-Piloted part\\n\\n| Match | 12 | 39.4 |\\n| Hard assignment | 12 | 39.6 |\\n\\nTable 9. Some hyperparameter tuning: 1. coarse-to-fine manner works better for our method than all-large feature maps. 2. Hard assignment lead to better results than matching for MP part queries. \u201cep\u201d is short for epochs.\\n\\nis adopted. Without adding noises, our method achieves an improvement of +0.9 AP. With multi-layer MP-training, our method achieves +1.5 AP. We also find that label noise does not work when trained for 50 epochs. Note that except for the last line of the table, other lines of our method apply MP training on the first 3 layers.\\n\\nApply MP training on multiple layers:\\n\\nIn Table 8, we show the effectiveness of applying GT masks on multiple layers. The results indicate that more layers of GT masks can lead to better performance. By adding noised GT masks in all layers, MP-Former achieves +0.8 AP.\\n\\nAdd noises to GT masks:\\n\\nIn the last four rows of Table 8, we show the effectiveness of adding different types of noises. Note that without any noise, MP-Former achieves 39.6 AP which is corresponding to the 4th row in Table 7. Shifting noise and scaling noise do not work in our method. Point noise achieved +0.3 extra growth.\\n\\nSome hyper-parameter tuning:\\n\\nMask2Former adopts coarse-to-fine feature maps in decoder layers. We try to adopt the largest feature map (1/8 feature map) in all decoder layers. Table 9 shows that a coarse-to-fine manner is better than using the largest feature map in all layers for our MP-Former. We also try to use matching to assign predictions to GT instances in the MP part. The result shows that matching can also improve the result compared with raw Mask2Former but the hard assignment is better.\\n\\n6. Conclusion\\n\\nIn this paper, we have analyzed failure cases of Mask2Former and found a problem of inconsistent predictions between adjacent layers which potentially leads to inconstant optimization goals and low utilization of queries and therefore limits the performance of Mask2Former. In order to address the problem, we developed a new MP-Former, whose key components include multi-layer mask-piloted training with point noises and label-guided training. Our experimental results showed that the proposed training method effectively mitigates the inconsistent prediction problem. When evaluating on three challenging datasets (ADE20K, Citescapes, and MS COCO), our method achieved improvement by a large margin on all three image segmentation tasks (semantic, instance, and panoptic). In addition, our method only introduces little computation during training and no extra computation in inference time. Finally, our method can also significantly speed up training. Especially, when trained with half number of steps, our method exceeds Mask2Former on all three segmentation tasks on ADE20K with both R50 and Swin-L as the backbones.\\n\\nLimitations:\\n\\nWe evaluated MP-Former on Mask2Former, but our method may be useful to some other models such as MaskFormer and HTC. We will take it as our future work.\"}"}
