{"id": "CVPR-2022-428", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Sameer Agarwal, Yasutaka Furukawa, Noah Snavely, Ian Simon, Brian Curless, Steven M Seitz, and Richard Szeliski. Building Rome in a Day. Communications of the ACM, 54(10):105\u2013112, 2011.\\n\\n[2] Relja Arandjelovi\u0107 and Andrew Zisserman. Three Things Everyone Should Know to Improve Object Retrieval. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2911\u20132918, 2012.\\n\\n[3] Vassileios Balntas, Karel Lenc, Andrea Vedaldi, and Krysztian Mikolajczyk. HPatches: A Benchmark and Evaluation of Handcrafted and Learned Local Descriptors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\n[4] Axel Barroso-Laguna, Edgar Riba, Daniel Ponsa, and Krysztian Mikolajczyk. Key.Net: Keypoint Detection by Handcrafted and Learned CNN Filters. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 5836\u20135844, 2019.\\n\\n[5] Herbert Bay, Tinne Tuytelaars, and Luc Van Gool. SURF: Speeded Up Robust Features. In Proceedings of the European Conference on Computer Vision (ECCV), pages 404\u2013417, 2006.\\n\\n[6] Aritra Bhowmik, Stefan Gumhold, Carsten Rother, and Eric Brachmann. Reinforced Feature Points: Optimizing Feature Detection and Description for a High-Level Task. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4948\u20134957, 2020.\\n\\n[7] Sudong Cai, Yulan Guo, Salman Khan, Jiwei Hu, and Gongjian Wen. Ground-to-aerial image geo-localization with a hard exemplar reweighting triplet loss. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 8391\u20138400, 2019.\\n\\n[8] Michael Calonder, Vincent Lepetit, Mustafa Ozuysal, Tomasz Trzcinski, Christoph Strecha, and Pascal Fua. BRIEF: Computing a Local Binary Descriptor Very Fast. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 34(7):1281\u20131298, 2011.\\n\\n[9] Luca Cavalli, Viktor Larsson, Martin Ralf Oswald, Torsten Sattler, and Marc Pollefeys. Handcrafted Outlier Detection Revisited. In Proceedings of the European Conference on Computer Vision (ECCV), pages 770\u2013787, 2020.\\n\\n[10] Hongkai Chen, Zixin Luo, Jiahui Zhang, Lei Zhou, Xuyang Bai, Zeyu Hu, Chiew-Lan Tai, and Long Quan. Learning to Match Features with Seeded Graph Matching Network. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 6301\u20136310, 2021.\\n\\n[11] Peter Hviid Christiansen, Mikkel Fly Kragh, Yury Brodskiy, and Henrik Karstoft. UnsuperPoint: End-to-end Unsupervised Interest Point Detector and Descriptor. arXiv preprint arXiv:1907.04011, 2019.\\n\\n[12] Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperPoint: Self-Supervised Interest Point Detection and Description. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 224\u2013236, 2018.\\n\\n[13] Mihai Dusmanu, Ignacio Rocco, Tomas Pajdla, Marc Pollefeys, Josef Sivic, Akihiko Torii, and Torsten Sattler. D2-Net: A Trainable CNN for Joint Detection and Description of Local Features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\n[14] Patrick Ebel, Anastasiia Mishchuk, Kwang Moo Yi, Pascal Fua, and Eduard Trulls. Beyond Cartesian Representations for Local Descriptors. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 253\u2013262, 2019.\\n\\n[15] Steve R Gunn. Edge Detection Error in the Discrete Laplacian of Gaussian. In Proceedings of the International Conference on Image Processing (ICIP), volume 2, pages 515\u2013519. IEEE, 1998.\\n\\n[16] Yulan Guo, Michael Choi, Kunhong Li, Farid Boussaid, and Mohammed Bennamoun. Soft exemplar highlighting for cross-view image-based geo-localization. IEEE Transactions on Image Processing (TIP), 31:2094\u20132105, 2022.\\n\\n[17] Chris Harris, Mike Stephens, et al. A Combined Corner and Edge Detector. In Alvey vision conference, volume 15, pages 147\u2013151. Citeseer, 1988.\\n\\n[18] Yuhe Jin, Dmytro Mishkin, Anastasiia Mishchuk, Jiri Matas, Pascal Fua, Kwang Moo Yi, and Eduard Trulls. Image Matching Across Wide Baselines: From Paper to Practice. International Journal of Computer Vision (IJCV), 129(2):517\u2013547, 2021.\\n\\n[19] Stefan Leutenegger, Margarita Chli, and Roland Y Siegwart. BRISK: Binary Robust Invariant Scalable Keypoints. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2548\u20132555. IEEE, 2011.\\n\\n[20] Xinghui Li, Kai Han, Shuda Li, and Victor Prisacariu. Dual-Resolution Correspondence Networks. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, 2020.\\n\\n[21] Zhengqi Li and Noah Snavely. MegaDepth: Learning Single-View Depth Prediction From Internet Photos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2041\u20132050, 2018.\\n\\n[22] David G Lowe. Distinctive Image Features from Scale-Invariant Keypoints. International Journal of Computer Vision (IJCV), 60(2):91\u2013110, 2004.\\n\\n[23] Zixin Luo, Tianwei Shen, Lei Zhou, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan. ContextDesc: Local Descriptor Augmentation with Cross-Modality Context. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\n[24] Zixin Luo, Lei Zhou, Xuyang Bai, Hongkai Chen, Jiahui Zhang, Yao Yao, Shiwei Li, Tian Fang, and Long Quan. ASLFeat: Learning Local Features of Accurate Shape and Localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\n[25] Iaroslav Melekhov, Gabriel J Brostow, Juho Kannala, and Daniyar Turmukhambetov. Image Stylization for Robust Features. arXiv preprint arXiv:2008.06959, 2020.\"}"}
{"id": "CVPR-2022-428", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[27] Anastasiia Mishchuk, Dmytro Mishkin, Filip Radenovic, and Jiri Matas. Working Hard to Know Your Neighbor's Margins: Local Descriptor Learning Loss. In Advances in Neural Information Processing Systems (NeurIPS), volume 30, 2017.\\n\\n[28] Dmytro Mishkin, Filip Radenovic, and Jiri Matas. Repeatability Is Not Enough: Learning Affine Regions via Discriminability. In Proceedings of the European Conference on Computer Vision (ECCV), pages 284\u2013300, 2018.\\n\\n[29] Raul Mur-Artal and Juan D Tard\u00f3s. ORB-SLAM2: An Open-Source SLAM System for Monocular, Stereo, and RGB-D Cameras. IEEE Transactions on Robotics (TR), 33(5):1255\u20131262, 2017.\\n\\n[30] Hyeonwoo Noh, Andre Araujo, Jack Sim, Tobias Weyand, and Bohyung Han. Large-Scale Image Retrieval With Attentive Deep Local Features. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 3456\u20133465, 2017.\\n\\n[31] Yuki Ono, Eduard Trulls, Pascal Fua, and Kwang Moo Yi. LF-Net: Learning Local Features from Images. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\\n\\n[32] Udit Singh Parihar, Aniket Gujarathi, Kinal Mehta, Satyajit Tourani, Sourav Garg, Michael Milford, and K Madhava Krishna. RoRD: Rotation-Robust Descriptors and Orthographic Views for Local Feature Matching. In Proceedings of the IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), 2021.\\n\\n[33] R\u00e9mi Pautrat, Viktor Larsson, Martin R Oswald, and Marc Pollefeys. Online Invariance Selection for Local Feature Descriptors. In Proceedings of the European Conference on Computer Vision (ECCV), pages 707\u2013724, 2020.\\n\\n[34] Jerome Revaud, Cesar De Souza, Martin Humenberger, and Philippe Weinzaepfel. R2D2: Reliable and Repeatable Detector and Descriptor. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, pages 12405\u201312415, 2019.\\n\\n[35] Ignacio Rocco, Relja Arandjelovi\u0107, and Josef Sivic. Efficient neighbourhood consensus networks via submanifold sparse convolutions. In Proceedings of the European Conference on Computer Vision (ECCV), pages 605\u2013621. Springer, 2020.\\n\\n[36] Ethan Rublee, Vincent Rabaud, Kurt Konolige, and Gary Bradski. ORB: An Efficient Alternative to SIFT or SURF. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2564\u20132571. IEEE, 2011.\\n\\n[37] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. SuperGlue: Learning Feature Matching With Graph Neural Networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4938\u20134947, 2020.\\n\\n[38] Torsten Sattler, Bastian Leibe, and Leif Kobbelt. Improving Image-Based Localization by Active Correspondence Search. In Proceedings of the European Conference on Computer Vision (ECCV), pages 752\u2013765, 2012.\\n\\n[39] Nikolay Savinov, Akihito Seki, Lubor Ladicky, Torsten Sattler, and Marc Pollefeys. Quad-Networks: Unsupervised Learning to Rank for Interest Point Detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1822\u20131830, 2017.\\n\\n[40] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-Motion Revisited. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4104\u20134113, 2016.\\n\\n[41] Johannes L Schonberger, Hans Hardmeier, Torsten Sattler, and Marc Pollefeys. Comparative Evaluation of Hand-Crafted and Learned Local Features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1482\u20131491, 2017.\\n\\n[42] Jiaming Sun, Zehong Shen, and Yuang Wang. LoFTR: Detector-Free Local Feature Matching With Transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 8922\u20138931, 2021.\\n\\n[43] Ilya Sutskever, James Martens, George Dahl, and Geoffrey Hinton. On the Importance of Initialization And Momentum in Deep Learning. In Proceedings of the International Conference on Machine Learning (ICML), pages 1139\u20131147, 2013.\\n\\n[44] Yurun Tian, Xin Yu, Bin Fan, Fuchao Wu, Huub Heijnen, and Vassileios Balntas. SOSNet: Second Order Similarity Regularization for Local Descriptor Learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11016\u201311025, 2019.\\n\\n[45] Carl Toft, Will Maddern, Akihiko Torii, Lars Hammarstrand, Erik Stenborg, Daniel Safari, Masatoshi Okutomi, Marc Pollefeys, Josef Sivic, Tomas Pajdla, et al. Long-Term Visual Localization Revisited. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 2020.\\n\\n[46] Micha\u0142 J. Tyszkiewicz, Pascal Fua, and Eduard Trulls. DISK: Learning Local Features with Policy Gradient. In Advances in Neural Information Processing Systems (NeurIPS), volume 33, 2020.\\n\\n[47] Qianqian Wang, Xiaowei Zhou, Bharath Hariharan, and Noah Snavely. Learning Feature Descriptors Using Camera Pose Supervision. In Proceedings of the European Conference on Computer Vision (ECCV), volume 12346, pages 757\u2013774, 2020.\\n\\n[48] Xupeng Wang, Ferdous Ahmed Sohel, Mohammed Bennamoun, Yulan Guo, and Hang Lei. Persistence-based interest point detection for 3D deformable surface. In International Conference on Computer Graphics Theory and Applications (GRAPP), pages 58\u201369, 2017.\\n\\n[49] Olivia Wiles, Sebastien Ehrhardt, and Andrew Zisserman. Co-Attention for Conditioned Image Matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15920\u201315929, 2021.\\n\\n[50] Kwang Moo Yi, Eduard Trulls, Vincent Lepetit, and Pascal Fua. LIFT: Learned Invariant Feature Transform. In Proceedings of the European Conference on Computer Vision (ECCV), pages 467\u2013483, 2016.\"}"}
{"id": "CVPR-2022-428", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Zichao Zhang, Torsten Sattler, and Davide Scaramuzza. Reference Pose Generation for Long-term Visual Localization via Learned Features and View Synthesis. International Journal of Computer Vision (IJCV), 129(4):821\u2013844, 2021.\\n\\nYong Zhao, Shibiao Xu, Shuhui Bu, Hongkai Jiang, and Pengcheng Han. GSLAM: A General SLAM Framework. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1110\u20131120, 2019.\\n\\nQunjie Zhou, Torsten Sattler, and Laura Leal-Taixe. Patch2pix: Epipolar-guided pixel-level correspondences. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4669\u20134678, 2021.\"}"}
{"id": "CVPR-2022-428", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Decoupling Makes Weakly Supervised Local Feature Better\\n\\nKunhong Li, Longguang Wang, Li Liu, Qing Ran, Kai Xu, Yulan Guo\\n\\nAbstract\\nWeakly supervised learning can help local feature methods to overcome the obstacle of acquiring a large-scale dataset with densely labeled correspondences. However, since weak supervision cannot distinguish the losses caused by the detection and description steps, directly conducting weakly supervised learning within a joint training describe-then-detect pipeline suffers limited performance. In this paper, we propose a decoupled training describe-then-detect pipeline tailored for weakly supervised local feature learning. Within our pipeline, the detection step is decoupled from the description step and postponed until discriminative and robust descriptors are learned. In addition, we introduce a line-to-window search strategy to explicitly use the camera pose information for better descriptor learning. Extensive experiments show that our method, namely PoSFeat (Camera Pose supervised Feature), outperforms previous fully and weakly supervised methods and achieves state-of-the-art performance on a wide range of downstream task.\\n\\n1. Introduction\\nFinding pixel correspondences is a fundamental problem in computer vision. Sparse local feature [5,12,22,36,48], as one of the mainstream methods to find correspondences, has been widely applied in many areas, such as simultaneous localization and mapping (SLAM) [29, 52], structure from motion (SfM) [1, 40], and visual localization [7, 16, 38, 51]. Traditional sparse local feature methods [5, 22, 36] follow a detect-then-describe pipeline. Specifically, keypoints are first detected and then patches centered at these keypoints are used to generate descriptors. Early methods [15, 17, 19, 22] focus on the detection step and are proposed to distinguish distinctive areas to detect good keypoints. Later works pay more attention to the description step and make attempts to design powerful descriptors using advanced representations [5, 8, 36].\\n\\nMotivated by the success of deep learning, many efforts [23, 27, 31, 44, 50] have been made to replace the detection or description step in the detect-then-describe pipeline with CNNs. Recent works [13,24,34,46] find that keypoints and descriptors are interdependent and propose a joint training describe-then-detect pipeline. Specifically, the description network and detection network are combined into a single CNN and optimized jointly. The joint training describe-then-detect pipeline achieves better performance than the detect-then-describe pipeline, especially under challenging conditions [18,45]. However, these methods are fully supervised and rely on dense ground-truth correspondence labels for training.\\n\\nBecause collecting a large dataset with pixel-level ground-truth correspondences is expensive, self-supervised and weakly supervised learning are investigated for training. Specifically, DeTone et al. [12] used a single image to train a model and achieved state-of-the-art performance. However, weak supervision cannot distinguish the losses caused by the detection and description steps, directly conducting weakly supervised learning within a joint training describe-then-detect pipeline suffers limited performance. In this paper, we propose a decoupled training describe-then-detect pipeline tailored for weakly supervised local feature learning. Within our pipeline, the detection step is decoupled from the description step and postponed until discriminative and robust descriptors are learned. In addition, we introduce a line-to-window search strategy to explicitly use the camera pose information for better descriptor learning. Extensive experiments show that our method, namely PoSFeat (Camera Pose supervised Feature), outperforms previous fully and weakly supervised methods and achieves state-of-the-art performance on a wide range of downstream task.\"}"}
{"id": "CVPR-2022-428", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"age and a virtual homography to generate image pairs to conduct self-supervised learning. However, homography transformation cannot cover complicated geometry transformations in real-world settings, resulting in limited performance. Noh et al. [30] used landmark labels to train the local feature network, which suffers extremely poor performance on viewpoint changes. Owing to the convenience of collecting camera poses, Wang et al. [47] introduced camera poses as weak supervision for descriptor learning. Although weakly supervised learning achieves promising results within the detect-then-describe pipeline, directly applying it to the joint training describe-then-detect pipeline is hard to produce satisfying results [46].\\n\\nWhen a detection network and a description network are jointly optimized within a joint training describe-then-detect pipeline with only weak supervision (e.g., camera pose), the loss produced by these two components cannot be distinguished. Specifically, when only one component is failed (Fig. 2), both the detection network and the description network cannot be correctly updated within a joint training describe-then-detect pipeline. As a result, the description network is hard to produce highly discriminative descriptors, and the detection network may produce false detected keypoints that are out of object boundaries, as shown in Fig. 1.\\n\\nIn this paper, we propose a decoupled training describe-then-detect pipeline tailored for weakly supervised local feature learning. Our main insight is that, with only weak supervision, the detection network relies heavily on a good descriptor for accurate keypoint detection (Fig. 1). Consequently, we decouple the detection network from the description network to postpone it until a discriminative and robust descriptor is learned. Different from the detect-then-describe pipeline that relies on low-level structures for early detection, our keypoints detection depends on the higher-level structures encoded in the descriptors. As a result, better robustness is achieved. In contrast to the joint training describe-then-detect pipeline that simultaneously perform detection and description optimization, the two networks are trained separately and thus the loss function for these two components are decoupled to address the ambiguity. It is demonstrated that our decoupled training describe-then-detect pipeline facilitates local feature methods to achieve much better performance with only weak supervision. Our contributions can be summarized as:\\n\\n1. We introduce a decoupled training describe-then-detect pipeline for weakly supervised local feature learning. This simple yet efficient pipeline significantly improves the performance of weakly supervised local features.\\n\\n2. We propose a line-to-window search strategy to exploit the weak supervision of camera poses for descriptor learning. This strategy can make full use of the geometric information of camera poses to reduce the search space and learn highly discriminative descriptors.\\n\\n3. Our method achieves state-of-the-art performance on three datasets and largely closes the gap between fully and weakly supervised methods.\\n\\n2. Related Works\\n\\n2.1. Fully Supervised Local Feature Methods\\n\\nFully supervised methods conduct local feature learning using pixel-level ground-truth correspondences to provide supervision. Following the detect-then-describe pipeline, early learning-based methods [4,14,23,27,39,44] use CNNs to perform the detection or description steps. Specifically, QuadNet [39] and Key.Net [4] were proposed to use CNNs for keypoint detection. HardNet [27] and SOSNet [44] were developed to leverage CNNs to extract descriptors. Later, LIFT [50] and LFNet [31] were introduced to integrate both detection and description steps into an end-to-end architecture to achieve better performance. Note that, LIFT [50] also introduced a decoupled training to address unstable training issue with full supervision in a detect-then-describe pipeline.\\n\\nRecent works [13, 24, 34, 46] follow a joint training describe-then-detect pipeline in which detection and description are combined into a single CNN and optimized jointly. Specifically, Dusmanu et al. [13] first used a CNN to extract dense features and then selected local maxima of the dense feature map as keypoints. Revaud et al. [34] further took both the repeatability and reliability of the descriptors into consideration for better keypoint detection. Tyszkiewicz et al. [46] used policy gradient to address the discreteness during the selection of sparse keypoints (namely, DISK). Luo et al. [24] adopted deformable convolution to model the geometry information and detected keypoints at multiple scales. By jointly optimizing the detection network and the description network, joint training describe-then-detect pipeline achieves better performance than previous detect-then-describe pipeline.\"}"}
{"id": "CVPR-2022-428", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"2.2. Self-Supervised Local Feature Methods\\n\\nAs a large dataset with densely labeled correspondences is difficult to collect, self-supervised learning has been studied for local feature learning. Specifically, DeTone et al. [12] used a virtual homography to generate an image pair from a single image to conduct self-supervised learning. This method uses a CNN pretrained on synthetic data as a teacher of the detection network. Differently, Christiansen et al. [11] proposed an end-to-end framework to train both the detection network and the description network using virtual homography in a self-supervised manner. Later, Parihar et al. [32] leveraged the homography to enhance the robustness of descriptors to rotation. Nevertheless, simple homography transformations used in these self-supervised methods may not hold in real cases.\\n\\n2.3. Weakly Supervised Local Feature Methods\\n\\nNoh et al. introduced DELF [30], which is trained with an image retrieval task, to achieve local feature extraction. However, the keypoints detected by DELF are sensitive to viewpoint change and thus cannot be applied in real-world settings. For camera poses are easy to collect, Wang et al. [47] used them as weak supervision and introduced an epipolar loss for descriptor learning. This method follows a detect-then-describe pipeline and relies on an off-the-shelf detection method (e.g., SIFT) to detect keypoints. Recently, Tyszkiewicz et al. [46] developed DISK-W to integrate weakly supervised learning in a joint training describe-then-detect pipeline by adopting policy gradient. Nevertheless, when DISK-W is directly trained with a weakly supervised loss (rather than a fully-supervised loss), it suffers a notable performance drop on pixel-wise metrics. As weakly supervised loss cannot distinguish between errors introduced by false keypoints and inaccurate descriptors, this ambiguity hinders the joint training describe-then-detect pipeline to learn good local features.\\n\\n2.4. Learning-based Matcher Methods\\n\\nSince a Brute Force Matcher (also named NN matcher) usually produces low quality raw matches, learning-based matchers are proposed to achieve better matching results. Sarlin et al. [37] proposed SuperGlue to achieve robust matching with a graph neural network (GNN) and an optimal transport algorithm. Chen et al. [10] improved the architecture of GNN to increase the efficiency of descriptor enhancement. Zhou et al. [53] proposed a weakly supervised network to refine raw matches using patch matches as prior. Sun et al. [42] introduced a detector-free matcher to achieve pixel correspondence in a coarse-to-fine manner. Note that, most matcher methods are not the direct competitors of local feature methods. Instead, they can be considered as a post processing step and combined with local features to achieve improved performance.\\n\\n3. Decoupled Training Describe-then-Detect Pipeline\\n\\n3.1. Overview\\n\\nThe decoupled training describe-then-detect pipeline is shown in Fig. 3. We train the description net and detection net individually to suppress the loss ambiguity caused by weak supervision. During training, we first leave out the detection network and optimize the description network to learn good descriptors with a line-to-window strategy. The description network is then frozen to train a detection network for keypoint detection. We follow CAPS [47] to use ResUNet as the description net, which produces a feature map with 1/4 resolution and 128 dimensions as dense descriptors. Additionally, we design a shallow detection net to detect keypoints at the original resolution. For more details about the network architecture, please refer to the supplementary material.\"}"}
{"id": "CVPR-2022-428", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. An illustration of the coarse-to-fine search strategy (a) and our line-to-window search strategy (b). The red line in $F_2$ denotes the epipolar line corresponding to the query point in $F_1$.\\n\\n3.2. Feature Description\\n\\nFollowing the widely used paradigm [47], we impose supervision only on sparse query points sampled from paired images to conduct training of the description network. We first split an image into small grids of size $g \\\\times g$, and randomly sample one point per grid as a query point. Then, we translate relative camera pose into an epipolar constraint and introduce a line-to-window search strategy to reduce search space (Sec. 3.2.1). Moreover, we formulate a loss function by encouraging the predicted matches to obey the epipolar constraint (Sec. 3.2.2).\\n\\n3.2.1 Line-to-Window Search\\n\\nGiven a query point $x_i$ in the query image $I_1$, our goal is to find its correspondence in the reference image $I_2$. Since repetitive structures widely exist in a natural image, the commonly used coarse-to-fine strategy [42, 47] usually selects a mismatched patch such that inferior performance is produced (Fig. 4(a)). Intuitively, the correspondence of the query point $x_i$ is constrained in an epipolar line in the reference image. Therefore, we introduce a line-to-window search strategy to reduce search space for better performance. Our line-to-window search strategy consists of two search steps, as illustrated in Fig. 4(b).\\n\\nSearch along An Epipolar Line.\\n\\nFor a query point $x_i \\\\in I_1$, we first calculate its corresponding epipolar line $L_{x_i}$ in the reference image $I_2$ based on the relative camera pose. Then, we uniformly sample $N_{line}$ points along this epipolar line to formulate the search space $Y_{line} = \\\\{y_{ji}\\\\}$ ($j = 1, ..., N_{line}$). Next, we calculate the matching probability of $x_i$ over $Y_{line}$:\\n\\n$$P(y_{ji}|F_1(x_i), F_2(Y_{line})) = \\\\exp(F_1(x_i)^T F_2(y_{ji})) P_{Y_{line}} \\\\exp(F_1(x_i)^T F_2(y_{ki})),$$\\n\\n(1)\\n\\nwhere $F_1$ and $F_2$ are the feature maps for $I_1$ and $I_2$, respectively. Afterwards, we select $y_i$ with the maximum probability from $Y_{line}$ to determine the coarse location of the correspondence of $x_i$:\\n\\n$$y_i = \\\\arg \\\\max_{y_{ji}} P(y_{ji}|F_1(x_i), F_2(Y_{line})).$$\\n\\n(2)\\n\\nSearch in A Local Window.\\n\\nDue to the discreteness of the candidates in $Y_{line}$, the resultant corresponding point $y_i$ can be far from the groundtruth. To remedy this, a subsequent search is conducted in a local window. First, we calculate the center of the local window:\\n\\n$$y_{center i} = y_i + 0.5 \\\\cdot w_{patch} \\\\cdot u,$$\\n\\n(3)\\n\\nwhere $w_{patch}$ is the window size of a local patch, $u \\\\in \\\\mathbb{R}^2$ is a noise vector drawn from a uniform distribution $U(0, 1)$ to avoid the convergence to trivial solution $F(x) \\\\equiv 0$. Then, a local patch $Y_{patch} \\\\subset I_2$ centered at $y_{center i}$ is cropped from $F_2$ as the search space. Next, we calculate the matching probability of $x_i$ over $Y_{patch}$:\\n\\n$$P(y_{ji}|F_1(x_i), F_2(Y_{patch})) = \\\\exp(F_1(x_i)^T F_2(y_{ji})) P_{Y_{patch}} \\\\exp(F_1(x_i)^T F_2(y_{ki})),$$\\n\\n(4)\\n\\nBecause directly selecting the point with the maximum probability in the local patch is non-differentiable, we calculate the correspondence $\\\\hat{y}_i$ in a differentiable manner:\\n\\n$$\\\\hat{y}_i = E(y_{ji}) = \\\\sum_{y_{ji} \\\\in Y_{patch}} y_{ji} \\\\cdot P(y_{ji}|F_1(x_i), F_2(Y_{patch})).$$\\n\\n(5)\\n\\nCompared to the previous coarse-to-fine search strategy [47], our line-to-window search strategy can make better use of the camera pose information to reduce search space and further improve the discriminativeness of descriptors (as demonstrated in Sec. 4.3).\"}"}
{"id": "CVPR-2022-428", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.2.2 Loss Function\\n\\nWith only weak supervision of camera pose, we calculate the distance of the correspondence \\\\( \\\\hat{y}_i \\\\) to the epipolar line \\\\( L_{x_i} \\\\) as the loss of query point \\\\( x_i \\\\) \\\\[47\\\\]:\\n\\n\\\\[\\nL_{\\\\text{epi}}(\\\\hat{y}_i, x_i) = \\\\text{distance}(\\\\hat{y}_i, L_{x_i})\\n\\\\]\\n\\nThen, we use the weighted sum of the losses over all query points as the final loss:\\n\\n\\\\[\\nL_{\\\\text{desc}} = \\\\sum_i M_i \\\\sigma(x_i) \\\\cdot L_{\\\\text{epi}}(\\\\hat{y}_i, x_i)\\n\\\\]\\n\\nHere, \\\\( M_i \\\\) is a binary mask (which is used to exclude query points whose epipolar lines are not in the reference image) and \\\\( \\\\sigma(x_i) \\\\) is the variance of the probability distribution over \\\\( Y \\\\) patch,\\n\\n\\\\[\\n\\\\sigma(x_i) = \\\\| \\\\hat{y}_i^2 - \\\\mathbb{E}(y_j^2) \\\\|\\n\\\\]\\n\\n3.3. Feature Detection\\n\\nAfter feature description learning, the description network is frozen to produce dense descriptors for keypoint detection, as shown in Fig. 3. Since selecting discrete sparse keypoints is non-differentiable, we adopt the strategy introduced in DISK \\\\[46\\\\], which is based on policy gradient, to achieve network training.\\n\\nFirst, dense descriptors \\\\( F_1 \\\\) and \\\\( F_2 \\\\) are respectively extracted from \\\\( I_1 \\\\) and \\\\( I_2 \\\\), and fed to a detection network to produce keypoint heatmaps. Then, we divide these heatmaps into grids of size \\\\( g_k \\\\times g_k \\\\) and select at most one keypoint from each grid cell. Specifically, we establish a probability distribution \\\\( P_{kp} \\\\) over each grid cell based on the heatmap scores in this cell. Afterwards, \\\\( P_{kp} \\\\) is used to probabilistically select candidate keypoints \\\\( Q_1 = \\\\{x_1, x_2, \\\\ldots\\\\} \\\\) and \\\\( Q_2 = \\\\{y_1, y_2, \\\\ldots\\\\} \\\\) from \\\\( I_1 \\\\) and \\\\( I_2 \\\\), respectively. Next, a matching probability \\\\( P_m \\\\) is calculated based on the feature similarity \\\\( S_{i,j} \\\\) between each pair of candidate keypoints \\\\( (x_i, y_j) \\\\).\\n\\nWith only camera pose supervision, we adopt an epipolar reward similar to Eq. 6 to encourage \\\\( y_j \\\\) to be close to the epipolar line of \\\\( x_i \\\\) (i.e., \\\\( L_{x_i} \\\\)):\\n\\n\\\\[\\nR(x_i, y_j) = \\\\begin{cases} \\n\\\\lambda_p, & \\\\text{if } \\\\text{distance}(y_j, L_{x_i}) \\\\leq \\\\epsilon \\\\\\\\\\n\\\\lambda_n, & \\\\text{if } \\\\text{distance}(y_j, L_{x_i}) > \\\\epsilon,\\n\\\\end{cases}\\n\\\\]\\n\\nThe overall loss function is defined as:\\n\\n\\\\[\\nL_{kp} = -\\\\frac{1}{|Q_1| + |Q_2|} \\\\sum_{x_i, y_j} L_{\\\\text{rew}}(x_i, y_j) + \\\\lambda_{\\\\text{reg}} \\\\sum_{x_i} \\\\log P_{kp}(x_i) + \\\\sum_{y_j} \\\\log P_{kp}(y_j)\\n\\\\]\\n\\nSince our descriptors are well optimized, \\\\( P_m \\\\) can suppress spurious points with low scores. In contrast, in a joint pipeline, descriptors are under-optimized such that spurious points cannot be well distinguished. Please refer to the supplementary material for more details.\"}"}
{"id": "CVPR-2022-428", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 1. MMAscore results achieved by different methods on the HPatches dataset [3]. The MMAscores are calculated from Fig. 5.\\n\\n| Method                        | Overall MMAscore | Illumination MMAscore | Viewpoint MMAscore |\\n|-------------------------------|------------------|-----------------------|--------------------|\\n| Hes. Aff. + Root-SIFT [2]     | 0.584            | 0.544                 | 0.624              |\\n| HAN + HN++ [27] + [28]        | 0.633            | 0.634                 | 0.633              |\\n| SIFT + ContextDesc [22] + [23]| 0.636            | 0.613                 | 0.657              |\\n| D2-Net [13]                   | 0.519            | 0.605                 | 0.440              |\\n| R2D2 [34]                     | 0.695            | 0.727                 | 0.665              |\\n| ASLFeat [24]                  | 0.739            | 0.795                 | 0.687              |\\n| DISK [46]                     | 0.763            |                       |                    |\\n| DELF [30]                     | 0.571            | 0.903                 | 0.262              |\\n| SuperPoint [12]               | 0.658            | 0.715                 | 0.606              |\\n| SIFT + CAPS [47]              | 0.699            | 0.764                 | 0.639              |\\n| DISK-W [46]                   | 0.719            | 0.803                 | 0.649              |\\n| PoSFeat (Ours)                | 0.775            | 0.826                 | 0.728              |\\n\\nAs shown in Fig. 5 and Table 1, the proposed PoSFeat outperforms all previous works, with the highest MMAscore being achieved. Compared to existing weakly supervised methods, our method produces significant performance improvements. Specifically, our method outperforms DISK-W by notable margins under both illumination (0.826 vs. 0.803) and viewpoint (0.728 vs. 0.649) changes, and therefore achieves higher overall MMAscore (0.775 vs. 0.719). We also visualize the matching results in Fig. 6. It can be seen that our PoSFeat produces more reasonable keypoints and less wrong matches. Compared to fully supervised methods, our method still performs favorably with higher MMA scores. This clearly demonstrates the superiority of our method. Note that, because DELF detects keypoints in a low resolution feature map with a fixed grid, it produces the best results under illumination change. However, our method significantly surpasses DELF under viewpoint change (0.728 vs. 0.262) and achieves much better overall performance (0.775 vs. 0.571).\"}"}
{"id": "CVPR-2022-428", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Results achieved by different methods on the Aachen Day-Night dataset [51]. 'LISRD' represents LISRD with SuperPoint keypoints and AdaLAM [9]. Two categories of methods are presented, including feature methods (top) and matchers (bottom).\\n\\nWe compare our method with two families of methods:\\n\\n- **Local feature methods:** D2-Net [13], SuperPoint [12], R2D2 [34], ASLFeat [24], ISRF [25], and LISRD [33].\\n- **Matcher methods:** DualRC-Net [20], SuperGlue [37] + SuperPoint, SparseNCNet [35], LoFTR [42], Patch2Pix [53], and SGMNet [10] + SuperPoint. As mentioned in Sec 2.4, matchers are the cooperators instead of the direct competitors of local features. Therefore, we group them separately.\\n\\nResults. As shown in Table 2, our PoSFeat achieves the state-of-the-art performance among the feature methods. Specifically, on Aachen Day-Night v1, our method achieves the best accuracy in terms of all metrics. Note that, although ASLFeat is a fully supervised method, our PoSFeat still outperforms it on (1m, 5\u25e6).\\n\\nOn Aachen Day-Night v1.1, our method also produces the best performance in all metrics. Note that, although R2D2 [34], ISRF [25], and LISRD [33] are fully-supervised and trained on the Aachen Day-Night dataset, our PoSFeat still achieves better results. We additionally include matcher methods for further comparison. Although these methods take pairs of images as inputs, our PoSFeat achieves comparable or even better performance.\\n\\n4.2.3 3D Reconstruction\\n\\nSettings. We finally evaluate our method on the 3D reconstruction task. We conduct experiments on the ETH local feature benchmark [41]. Four metrics are used for evaluation, including the number of registered images (# Imgs), the number of sparse points (# Pts), track length, and the mean reprojection error (Reproj. Err.).\\n\\nFour families of methods were included for comparison:\\n\\n- **Patch-based method:** Root-SIFT [2, 22].\\n- **Fully supervised dense feature methods:** Reinforced Feature Points [6] (RFP), DISK [46], DISK-W [46], D2-Net [13], and ASLFeat [24].\\n- **Weakly supervised dense feature methods:** SuperPoint [12] and CAPS [47].\\n- **Fully supervised matcher method:** CoAM [49].\\n\\nResults. As shown in Table 3, our method performs favorably against previous methods on the 3D reconstruction task. Specifically, our method produces the lowest reprojection error among all learning-based methods. Moreover, our method achieves the best or second best performance in terms of track length, which demonstrates that our keypoints are robust and thus can be tracked across a large amount of images.\\n\\n4.3. Ablation Study\\n\\nIn this section, we first conduct ablation experiments on the HPatches dataset [3] to demonstrate the effectiveness of our decoupled training describe-then-detect pipeline and line-to-window search strategy. Then, we conduct experiments to study the effectiveness of hyper-parameters in our method, i.e., the number of points sampled from the epipolar line $N_{line}$ and the window size $w_{patch}$. Results and model settings are shown in Fig. 7 and Table 4.\"}"}
{"id": "CVPR-2022-428", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7. Ablation results on HPatches. \u201cL2W\u201d denotes our line-to-window search strategy (illustrated in Fig 4(b)) and \u201cC2F\u201d denotes the coarse-to-fine search strategy [47] (illustrated in Fig 4(a)). \u201clearned\u201d means that the keypoints are generated by a detection network and \u201cSIFT\u201d means that SIFT keypoints (OpenCV default settings) are used. \u201cdecoupled\u201d means the proposed decoupled training pipeline is adopted and \u201cjoint\u201d means the description network and the detection network are jointly optimized.\\n\\nThen, we developed Model 3 based on the detect-then-describe pipeline. Specifically, the description network is combined with SIFT keypoints in Model 3.\\n\\nAs shown in Fig. 7, with only weak supervision, the ambiguity during optimization limits the performance of joint training describe-then-detect approaches (Model 2 and DISK-W). Moreover, Model 2 is even inferior to Model 3 under viewpoint change. Compared to Models 2 and 3, Model 1 with our decoupled training describe-then-detect pipeline produces much higher accuracy. This clearly demonstrates that our decoupled training describe-then-detect pipeline is well suitable to weakly supervised learning to achieve superior performance.\\n\\nWe further test different combinations of keypoints and descriptors (Models 5-8). It can be observed that the improvement mainly comes from the descriptor, and the keypoints are slightly improved on the viewpoint change. Besides, we also illustrate the keypoints produced by our method and DISK-W in Fig. 1. DISK-W generates considerable inaccurate keypoints out of objects (e.g., in the sky). In contrast, our model detects more reasonable keypoints. That is because those mismatched descriptors and erroneous keypoints produced from two different components do not influence each other within our decoupled training describe-then-detect pipeline.\\n\\nLine-to-Window Search Strategy. To validate the effectiveness of our line-to-window search strategy, we developed a network variant (Model 4) by replacing our search strategy with a coarse-to-fine one (as proposed in [47], illustrated in Fig. 4(a)). For fair comparison with Model 3, SIFT keypoints are employed in this network variant. It can be observed that Model 3 outperforms Model 4 by significant margins. That is because our line-to-window search strategy can make full use of the geometry information of camera poses to reduce the search space for accurate localization of correspondences. Consequently, higher accuracy can be achieved.\\n\\nNumber of Sampled Points \\\\(N_{\\\\text{line}}\\\\) and Window Size \\\\(w_{\\\\text{patch}}\\\\). We conduct experiments to study the effects of \\\\(N_{\\\\text{line}}\\\\) and \\\\(w_{\\\\text{patch}}\\\\) during our line-to-window search. More sampled points and a large window size are beneficial to the performance at the expense of higher computational cost. To achieve a trade-off between performance and computational complexity, \\\\(w_{\\\\text{patch}} = 0.100\\\\) and \\\\(N_{\\\\text{line}} = 100\\\\) are used as the default setting.\\n\\n5. Conclusion\\n\\nIn this paper, we introduce a decoupled training describe-then-detect pipeline tailored for weakly supervised local feature learning. Within our pipeline, the detection network is decoupled from the description network and postponed until discriminative and robust descriptors are obtained. In addition, we propose a line-to-window search strategy to explicitly use the camera pose information to reduce search space for better descriptor learning. Extensive experiments show that our method achieves the state-of-the-art performance on three different evaluation frameworks and significantly closes the gap between fully-supervised and weakly supervised methods.\\n\\nAcknowledgement. This work was partially supported by the National Key Research and Development Program of China (No. 2021YFB3100800), the Shenzhen Science and Technology Program (No. RCYX20200714114641140), and National Natural Science Foundation of China (No. U20A20185, 61972435, 62132021).\"}"}
