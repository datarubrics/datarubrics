{"id": "CVPR-2024-389", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Commonsense Prototype for Outdoor Unsupervised 3D Object Detection\\nHai Wu\\nShijia Zhao\\nXun Huang\\nChenglu Wen\\n* Xin Li\\nCheng Wang\\n1 Fujian Key Laboratory of Sensing and Computing for Smart Cities, Xiamen University\\n2 Section of Visual Computing and Interactive Media, Texas A&M University\\n\\nAbstract\\nThe prevalent approaches of unsupervised 3D object detection follow cluster-based pseudo-label generation and iterative self-training processes. However, the challenge arises due to the sparsity of LiDAR scans, which leads to pseudo-labels with erroneous size and position, resulting in subpar detection performance. To tackle this problem, this paper introduces a Commonsense Prototype-based Detector, termed CPD, for unsupervised 3D object detection. CPD first constructs Commonsense Prototype (CProto) characterized by high-quality bounding box and dense points, based on commonsense intuition. Subsequently, CPD refines the low-quality pseudo-labels by leveraging the size prior from CProto. Furthermore, CPD enhances the detection accuracy of sparsely scanned objects by the geometric knowledge from CProto. CPD outperforms state-of-the-art unsupervised 3D detectors on Waymo Open Dataset (WOD), PandaSet, and KITTI datasets by a large margin. Besides, by training CPD on WOD and testing on KITTI, CPD attains 90.85% and 81.01% 3D Average Precision on easy and moderate car classes, respectively. These achievements position CPD in close proximity to fully supervised detectors, highlighting the significance of our method. The code will be available at https://github.com/hailanyi/CPD.\\n\\n1. Introduction\\nAutonomous driving requires reliable detection of 3D objects (e.g. vehicle and cyclist) in urban scenes for safe path planning and navigation. Thanks to the power of neural networks, numerous studies have developed high-performance 3D detectors through fully supervised approaches [4, 15, 30\u201333]. However, these models heavily depend on human annotations from diverse scenes to guarantee their effectiveness across various scenarios. This data labeling process is typically laborious and time-consuming, limiting the wide deployment of detectors in practice [40].\\n\\nSeveral studies have explored approaches to reduce labeling requirements by weakly supervised learning [3, 26, 46], decreasing the label cost by over 80%. Notably, the objects within a 3D scene exhibit distinguishable attributes and can be easily identified through certain commonsense reasoning (see Fig. 1). For example, the objects are usually located on the ground surface with a certain shape; the object sizes are fixed across frames. This insight has prompted us to develop an unsupervised 3D detector that operates without using human annotations.\\n\\nIn recent years, traditional methods leveraged ground removal [9] and clustering technique [42] for unsupervised 3D object detection. However, these methods often struggle to achieve satisfactory performance due to the sparsity and occlusion of objects in 3D scenes. Advanced methods create initial pseudo-labels from point cloud sequences by clustering and bootstrap a good detector by iteratively training a deep network [41]. Nevertheless, the sparse and view-limited nature of LiDAR scanning leads to pseudo-labels with inaccurate sizes and positions, misleading the network convergence and resulting in suboptimal detection performance. A subset of objects, denoted as complete objects $T$, ...\"}"}
{"id": "CVPR-2024-389", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Illustration and statistics of complete and incomplete objects on WOD [25] validation set (large enough to demonstrate the general problem). (a) Pseudo-labels of complete object $T$ are refined by temporal consistency. (b) Pseudo-labels of incomplete object $J$ fail to be refined by temporal consistency. (c) 65% objects lack full scan coverage and generate inaccurate pseudo-labels (Max IoU (Intersection over Union) < 0.5 with GT (Ground Truth)). (d) The vehicle GT of complete object $GT_T$ and incomplete object $GT_J$ have similar size distributions. (e) The pseudo-label of complete object $P_{se_T}$ and incomplete object $P_{se_J}$ have different size distributions. (f)(g) The nearby stationary objects are with high completeness in consecutive frames. Benefit from having at least one complete scan across the entire point cloud sequence, allowing their pseudo-labels to be refined through temporal consistency [41] (see Fig. 2 (a)). However, the majority of objects (e.g. 65% on WOD [25], as shown in Fig.2 (c)), termed incomplete objects $J$, lack full scan coverage (see Fig.2 (b)), and cannot be recovered by temporal consistency.\\n\\nTo tackle this issue, this paper proposes a Commonsense Prototype-based Detector, termed CPD, for unsupervised 3D object detection. CPD is built upon two key insights: (1) The ground truth of intra-class objects keeps a similar size (length, width, and height) distribution between incomplete objects and complete objects (see Fig.2 (d)). (2) The nearby stationary objects are very complete in consecutive frames and can be recognized accurately by commonsense intuition (see Fig.2 (f)(g)). Our idea is to construct a Commonsense Prototype ($C_{Proto}$) set representing accurate geometry and size from complete objects to refine the pseudo-labels of incomplete objects and improve the detection accuracy. To this end, we first design an unsupervised Multi-Frame Clustering ($MFC$) method that yields high-recall initial pseudo-labels. Subsequently, we introduce an unsupervised Completeness and Size Similarity ($CSS$) score that selects high-quality labels to construct the $C_{Proto}$ set. Furthermore, we design a $C_{Proto}$-constrained Box Regularization ($CBR$) method to refine the pseudo-labels by incorporating the size prior from $C_{Proto}$. In addition, we develop $C_{Proto}$-constrained Self-Training ($CST$) that improves the detection accuracy of sparsely scanned objects by the geometry knowledge from $C_{Proto}$.\\n\\nThe effectiveness of our design is verified by experiments on widely used WOD [25], PandaSet [35], and KITTI dataset [6]. Besides, the individual components of our design are also verified by extensive experiments on WOD [25]. The main contributions of this work include:\\n\\n\u2022 We propose a Commonsense Prototype-based Detector (CPD) for unsupervised 3D object detection. CPD outperforms state-of-the-art unsupervised 3D detectors by a large margin.\\n\u2022 We propose Multi-Frame Clustering ($MFC$) and $C_{Proto}$-constrained Box Regularization ($CBR$) for pseudo-label generation and refinement, greatly improving the recall and precision of pseudo-label.\\n\u2022 We propose $C_{Proto}$-constrained Self-Training ($CST$) for unsupervised 3D detection. It improves the recognition and localization accuracy of sparse objects, boosting the detection performance significantly.\\n\\n2. Related Work\\n\\nFully/weakly supervised 3D object detection. Recent fully-supervised 3D detectors build single-stage [8, 10, 27, 39, 48, 49], two-stage [4, 20\u201322, 31\u201333, 37] or multiple stage [2, 30] deep networks for 3D object detection. However, these methods heavily rely on a large amount of precise annotations. Some weakly supervised methods replace the box annotation with low-cost click annotation [17]. Other methods decrease the supervision by only annotating a part of scenes [3, 26, 45, 46] or a part of instances [34]. Unlike all of the above works, we aim to design a 3D detector that does not require human-level annotations.\\n\\nUnsupervised 3D object detection. Previous unsupervised pre-training methods discern latent patterns within the unlabeled data by masked labels [36] or contrastive loss [14, 38]. But these methods require human labels for fine-turning. Traditional methods [1, 19, 24] employ ground removal and clustering for 3D object detection without human labels, but suffer from poor detection performance. Some deep learning-based methods generate pseudo-labels by clustering and use the pseudo-labels to train a 3D detector iteratively. Recent OYSTER [41] improves pseudo-label quality with temporal consistency. However, most pseudo-labels of incomplete objects cannot be recovered by temporal consistency. Our CPD addresses this problem by leveraging the geometry prior from $C_{Proto}$ to refine the pseudo-label and guide the network convergence.\\n\\nPrototype-based methods. The prototype-based methods are widely used in 2D detection [11, 12, 16, 29, 44] when novel classes are incorporated. Inspired by these\"}"}
{"id": "CVPR-2024-389", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"method, Prototypical V oteNet [47] constructs geometric prototypes learned from basic classes for few-shot 3D ob-\\nject detection. GPA-3D [13] and CL3D [18] build geo-\\nmetric prototypes from a source-domain model for domain\\nadaptive 3D detection. However, both the learning from\\nbasic class and training on the source domain require high-\\nquality annotations. Unlike that, we construct CProto using\\ncommonsense knowledge and detect 3D objects in a zero-\\nshot manner without human-level annotations.\\n\\n3. CPD Method\\nThis paper introduces the Commonsense Prototype-based\\nDetector (CPD), a novel approach for unsupervised 3D ob-\\nject detection. As shown in Fig.3, CPD consists of three\\nmain parts: (1) initial label generation; (2) label refinement;\\n(3) self-training. We detail the designs as follows.\\n\\n3.1. Initial Label Generation\\nRecent unsupervised methods [40, 41] detect 3D objects in\\na class-agnostic way. How to classify objects (e.g. vehi-\\ncle and pedestrian) without annotation is still an unsolved\\nchallenge. Our observations indicate that some stationary\\nobjects in consecutive frames, appear more complete (see\\nFig.2 (f)) and can be classified by predefined sizes. This\\nmotivates us to design a Multi-Frame Clustering (MFC)\\nmethod to generate initial labels. MFC involves motion ar-\\ntifact removal, clustering, and post-processing.\\n\\nMotion Artifact Removal (MAR). Directly trans-\\nforming and concatenating $2n+1$ consecutive frames\\n$\\\\{x_{n},...,x_{n}\\\\}$ (i.e., past $n$, future $n$, and the current frame)\\ninto a single point cloud $x_{0}$ introduces motion artifacts from\\nmoving objects, leading to increased label errors as the $n$\\ngrows (see Fig.4(a)). To mitigate this issue, we first trans-\\nform the consecutive frames to global system and calculate\\nthe Persistence Point Score (PPScore) [40] by consecutive\\nframes to identify the points in motion. We keep all the\\npoints from $x_{0}$ and remove moving points from the other\\nframes $x_{n-1},...,x_{1},x_{1},...,x_{n}$. After this removal, we\\nconcatenate the frames to obtain dense points $x_{0}$.\\n\\nClustering and post-processing. In line with recent\\nstudy [41], we apply the ground removal [9], DBSCAN [5]\\non $x_{0}$ to obtain a set of class-agnostic bounding boxes\\n$\\\\hat{b}$. We observe that the objects of the same class typically have similar sizes in 3D\\nspace. Therefore, we pre-define class-specific size thresh-\\nolds (e.g. the length of vehicle is generally larger than\\n0.5m) based on human commonsense to classify\\n$\\\\hat{b}$ into\\ndifferent categories. We then apply class-agnostic track-\\ning to associate the small background objects with fore-\\nground trajectories, and enhance the consistency of objects'\\n sizes by using temporal coherency [41]. This process re-\\nresults in a set of initial pseudo-labels $b = \\\\{b_{j}\\\\}_{j}$, where\\n$b_{j} = [x,y,z,l,w,h,\\\\alpha,\\\\beta,\\\\tau]$ represents position, width,\\nlength, height, azimuth angle, class identity, and tracking\\nidentity, respectively.\\n\\n3.2. CProto-constrained Box Regularization for La-\\nbel Refinement\\nAs noted in Section1, initial labels for incomplete objects\\noften suffer from inaccuracies in sizes and positions. To\"}"}
{"id": "CVPR-2024-389", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Detection distance (m)\\n\\nFigure 4. (a) Length absolute error with different frames. (b) Multi-level occupancy score. (c) Mean size error of initial labels.\\n\\nTo tackle this issue, we introduce the CProto-constrained Box Regularization (CBR) method. The key idea is to construct a high-quality CProto set based on unsupervised scoring from complete objects to refine the pseudo-labels of incomplete objects. Different from OYSTER [41], which can only refine the pseudo-labels of objects having at least one complete scan, our CBR can refine pseudo-labels of all objects, significantly decreasing the overall size and position errors.\\n\\nCompleteness and Size Similarity (CSS) scoring. Existing label scoring methods such as IoU scoring [21] are designed for fully supervised detectors. In contrast, we introduce an unsupervised Completeness and Size Similarity (CSS) method. It aims to approximate the IoU score using commonsense knowledge alone (see Fig. 5).\\n\\nDistance score. CSS first assesses the object completeness based on distance, assuming labels closer to the ego vehicle are likely to be more accurate. For an initial label $b_j$, we normalize the distance to the ego vehicle within the range $[0,1]$ to compute the distance score as\\n\\n$$\\n\\\\psi_1(b_j) = 1 - \\\\frac{N}{\\\\|c_j\\\\|},\\n$$\\n\\nwhere $N$ is the normalization function and $c_j$ is the location of $b_j$. However, this distance-based approach has its limitations. For example, occluded objects near the ego vehicle, which should receive lower scores, are inadvertently assigned high scores due to their proximity. To mitigate this issue, we introduce a Multi-Level Occupancy (MLO) score, further detailed in Fig. 4 (b).\\n\\nMLO score. Considering the diverse sizes of objects, we divide the bounding box of the initial label into multiple grids with different length and width resolutions. The MLO score is then calculated by determining the proportion of grids occupied by cluster points, via\\n\\n$$\\n\\\\psi_2(b_j) = \\\\frac{1}{N_o} \\\\sum_k O_k(r_k^2),\\n$$\\n\\nwhere $N_o$ denotes resolution number, $O_k$ is the number of occupied grids under $k$-th resolution, and $r_k$ is the grid number of $k$-th resolution.\\n\\nSize Similarity (SS) score. While the distance and MLO scores effectively evaluate the localization and size quality, they fall short in assessing classification quality. To bridge this gap, we introduce the SS score. This score utilizes a class-specific template box $a$ (average size of typical objects in Wikipedia) and calculates a truncated KL divergence [7]. Note that, this score is decided by ratio difference, rather than their specific values. Simple commonsense of $l,w,h$ ratios (2:1:1 for Vehicle, 1:1:2 for Pedestrian, 2:1:2 for Cyclist) can also be used here.\\n\\n$$\\n\\\\psi_3(b_j) = 1 - \\\\min(0.05, \\\\frac{\\\\sum \\\\sigma_{q_b \\\\sigma_a \\\\log(q_b \\\\sigma_a / q_a \\\\sigma)})}{0.05},\\n$$\\n\\nwhere $q_a \\\\sigma \\\\in \\\\{l_a, w_a, h_a\\\\}$, $q_b \\\\sigma \\\\in \\\\{l_b, w_b, h_b\\\\}$ refer to the normalized length, width, and height of the template and label.\\n\\nWe linearly combine the three metrics $S(b_j) = \\\\sum_i \\\\omega_i \\\\psi_i(b_j)$ to produce final scoring, where $\\\\omega_i$ is the weighting factor (in this study we adopt a simple average, $\\\\omega_i = 1/3$). For each $b_j \\\\in b$, we compute its CSS score $s_{css_j} = S(b_j)$ and obtain a set of scores $s_j = \\\\{s_{css_j}\\\\}$.\\n\\nCProto set construction. Regular learnable prototype-based methods require annotations [13, 47], which are unavailable in the unsupervised problem. We construct a high-quality CProto set $P = \\\\{P_k\\\\}_k$, representing geometry and size centers based on the unsupervised CSS score. Here, $P_k = \\\\{x_p k, b_p k\\\\}$, where $x_p k$ indicates the inside points, and $b_p k$ refers to the bounding box. Specifically, we first categorize the initial labels $b$ into different groups based on their tracking identity $\\\\tau$. Within each group, we select the high-quality boxes and inside points that meet a high CSS score threshold $\\\\eta$ (determined on validation set, using 0.8 in this study). Then, we transform all points and boxes into a local coordinate system, and obtain $x_p k$ by averaging the high-quality boxes and $b_p k$ by concatenating all the points.\"}"}
{"id": "CVPR-2024-389", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Asma Azim and Olivier Aycard. Detection, classification and tracking of moving objects in a 3d environment. In 2012 IEEE Intelligent Vehicles Symposium, pages 802\u2013807. IEEE, 2012.\\n\\n[2] Qi Cai, Yingwei Pan, Ting Yao, and Tao Mei. 3d cascade rcnn: High quality object detection in point clouds. IEEE Transactions on Image Processing, 31:5706\u20135719, 2022.\\n\\n[3] Zehui Chen, Zhenyu Li, Shuo Wang, Dengpan Fu, and Feng Zhao. Learning from noisy data for semi-supervised 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6929\u20136939, 2023.\\n\\n[4] Jiajun Deng, Shaoshuai Shi, Peiwei Li, Wen gang Zhou, Yanyong Zhang, and Houqiang Li. Voxel r-cnn: Towards high performance voxel-based 3d object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, 2021.\\n\\n[5] Martin Ester, Hans-Peter Kriegel, J\u00f6rg Sander, and Xiaowei Xu. A density-based algorithm for discovering clusters in large spatial databases with noise. In Knowledge Discovery and Data Mining, 1996.\\n\\n[6] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), pages 3354\u20133361, 2012.\\n\\n[7] Goldberger, Gordon, and Greenspan. An efficient image similarity measure based on approximations of kl-divergence between two gaussian mixtures. In Proceedings Ninth IEEE International conference on computer vision, pages 487\u2013493. IEEE, 2003.\\n\\n[8] Chenhang He, Hui Zeng, Jianqiang Huang, Xian-Sheng Hua, and Lei Zhang. Structure aware single-stage 3d object detection from point cloud. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), pages 11873\u201311882, 2020.\\n\\n[9] Michael Himmelsbach, Felix V Hundelshausen, and H-J Wuensche. Fast segmentation of 3d point clouds for ground vehicles. In 2010 IEEE Intelligent Vehicles Symposium, pages 560\u2013565. IEEE, 2010.\\n\\n[10] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), pages 12697\u201312705, 2019.\\n\\n[11] Huifang Li, Yidong Li, Yuanzhouhan Cao, Yu Shan Han, Yi Jin, and Yunchao Wei. Weakly supervised object detection with class prototypical network. IEEE Transactions on Multimedia, 2022.\\n\\n[12] Tianqin Li, Zijie Li, Harold Rockwell, Amir Farimani, and Tai Sing Lee. Prototype memory and attention mechanisms for few shot image generation. In Proceedings of the Eleventh International Conference on Learning Representations, 2022.\\n\\n[13] Ziyu Li, Jingming Guo, Tongtong Cao, Liu Bingbing, and Wankou Yang. Gpa-3d: Geometry-aware prototype alignment for unsupervised domain adaptive 3d object detection from point clouds. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6394\u20136403, 2023.\\n\\n[14] Hanxue Liang, Chenhan Jiang, Dapeng Feng, Xin Chen, Hang Xu, Xiaodan Liang, Wei Zhang, Zhenguo Li, Luc Van Gool, and Sun Yat-sen. Exploring geometry-aware contrast and clustering harmonization for self-supervised 3d object detection. In International Conference on Computer Vision (ICCV), pages 3273\u20133282, 2021.\\n\\n[15] Zhijian Liu, Haotian Tang, Alexander Amini, Xinyu Yang, Huizi Mao, Daniela Rus, and Song Han. Bevfusion: Multi-task multi-sensor fusion with unified bird's-eye view representation. ArXiv, 2022.\\n\\n[16] Xiaonan Lu, Wenhui Diao, Yongqiang Mao, Junxi Li, Peijin Wang, Xian Sun, and Kun Fu. Breaking immutable: information-coupled prototype elaboration for few-shot object detection. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1844\u20131852, 2023.\\n\\n[17] Qinghao Meng, Wenguan Wang, Tianfei Zhou, Jianbing Shen, Yunde Jia, and Luc Van Gool. Towards a weakly supervised framework for 3d point cloud object detection and annotation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(8):4454\u20134468, 2022.\\n\\n[18] Xidong Peng, Xinge Zhu, and Yuexin Ma. Cl3d: Unsupervised domain adaptation for cross-lidar 3d detection. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2047\u20132055, 2023.\\n\\n[19] Gheorghii Postica, Andrea Romanoni, and Matteo Matteucci. Robust moving objects detection in lidar data exploiting visual cues. In 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1093\u20131098. IEEE, 2016.\\n\\n[20] Shaoshuai Shi, Xiaogang Wang, and Hongsheng Li. Pointr-cnn: 3d object proposal generation and detection from point cloud. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), pages 770\u2013779, 2019.\\n\\n[21] Shaoshuai Shi, Chaoxu Guo, Li Jiang, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. Pv-rcnn: Point-voxel feature set abstraction for 3d object detection. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition (CVPR), pages 10526\u201310535, 2020.\\n\\n[22] Shaoshuai Shi, Zhe Wang, Jianping Shi, Xiaogang Wang, and Hongsheng Li. From points to parts: 3d object detection from point cloud with part-aware and part-aggregation network. IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), 43:2647\u20132664, 2021.\\n\\n[23] Jake Snell, Kevin Swersky, and Richard Zemel. Prototypical networks for few-shot learning. Advances in neural information processing systems, 30, 2017.\\n\\n[24] Muhammad Sualeh and Gon-Woo Kim. Dynamic multi-lidar based multiple object detection and tracking. Sensors, 19(6):1474, 2019.\\n\\n[25] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, and al. et. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE conference on\"}"}
{"id": "CVPR-2024-389", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2024-389", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3. CProto-constrained Self-Training (CST)\\n\\nRecent methods \\\\[40, 41\\\\] utilize pseudo-labels for training 3D detectors. However, even after refinement, some pseudo-labels remain inaccurate, diminishing the effectiveness of correct supervision and potentially misleading the training process. To tackle these issues, we propose two designs: (1) **CSS-Weighted Detection Loss**, which assigns different training weights based on label quality to suppress false supervision signals. (2) **Geometry Contrast Loss**, which aligns predictions of sparsely scanned points with the dense CProto, thereby improving feature consistency.\\n\\n### Network Architecture\\n\\nWe adopt a dense-sparse alignment architecture (Fig.3 (c)), consisting of a prototype network \\\\(F_{pro}\\\\) and a detection network \\\\(F_{det}\\\\), constructed from two-stage CenterPoint \\\\[39\\\\]. During training, for each \\\\(b^*_j\\\\), we add its corresponding points \\\\(x_p^k\\\\) from CProto \\\\(P_k\\\\) to the scene to obtain a dense point cloud \\\\(x_{pro}\\\\). We feed \\\\(x_{pro}\\\\) to \\\\(F_{pro}\\\\) to produce relatively good features and detections. We then feed randomly downsampled points \\\\(x_{det}\\\\) as a sparse sample to \\\\(F_{det}\\\\). We align the features and detections from two branches by the detection loss and contrast loss.\\n\\nDuring testing, we feed points without downsampling to the detection network \\\\(F_{det}\\\\) to perform detection.\\n\\n### CSS weight\\n\\nConsidering that the false pseudo-labels may mislead the network convergence, we first calculate a loss weight based on different label qualities. Formally, we convert a CSS score \\\\(s_{css}^i\\\\) of a pseudo-label to\\n\\n\\\\[\\n\\\\omega_i = \\\\begin{cases} \\n0 & s_{css}^i < S_{L} \\\\\\\\\\n\\\\frac{s_{css}^i - S_{L}}{S_{H} - S_{L}} & S_{L} < s_{css}^i < S_{H} \\\\\\\\\\n1 & s_{css}^i > S_{H}\\n\\\\end{cases}\\n\\\\]\\n\\n(4)\\n\\nwhere \\\\(S_H\\\\) and \\\\(S_L\\\\) are high/low-quality thresholds (we empirically set 0.7 and 0.4, respectively).\\n\\n### CSS-weighted detection loss\\n\\nTo decrease the influence of false labels, we formulate the CSS-weighted detection loss to refine \\\\(N_{proposals}\\\\)\\n\\n\\\\[\\nL_{css_{det}} = \\\\frac{1}{N} \\\\sum_{i} \\\\omega_i (L_{pro_i} + L_{det_i})\\n\\\\]\\n\\n(5)\\n\\nwhere \\\\(L_{pro_i}\\\\) and \\\\(L_{det_i}\\\\) are detection losses \\\\[4\\\\] of \\\\(F_{pro}\\\\) and \\\\(F_{det}\\\\), respectively. The losses are calculated by pseudo-labels \\\\(b^*_j\\\\) and network predictions.\\n\\n### Geometry contrast loss\\n\\nWe formulate two contrast losses that minimize the feature and predicted box difference between the prototype and detection network.\\n\\n(1) **Feature contrast loss.** For a foreground RoI \\\\(r_i\\\\) from the detection network, we extract features \\\\(f_p^i\\\\) from the prototype network by voxel set abstract \\\\[4\\\\], and extract features \\\\(f_d^i\\\\) from detection network. We then formulate the contrast loss by cosine distance:\\n\\n\\\\[\\nL_{css_{feat}} = \\\\frac{1}{N_f} \\\\sum_{i} \\\\omega_i f_d^i \\\\cdot f_p^i \\\\|f_d^i\\\\|\\\\|f_p^i\\\\|\\n\\\\]\\n\\n(6)\\n\\nwhere \\\\(N_f\\\\) is the foreground proposal number.\\n\\n(2) **Box contrast loss.** For a box prediction \\\\(d_p^i\\\\) from the prototype network and a box prediction \\\\(d_d^i\\\\) from the detection network. We then formulate the box contrast loss by IoU, location difference, and angle difference:\\n\\n\\\\[\\nL_{css_{box}} = \\\\frac{1}{N_f} \\\\sum_{i} \\\\omega_i [1 - I(d_d^i, d_p^i) + \\\\|c_d^i - c_p^i\\\\| + |\\\\sin(\\\\alpha_d^i - \\\\alpha_p^i)|]\\n\\\\]\\n\\n(7)\\n\\nwhere \\\\(I\\\\) denote IoU function; \\\\(c_d^i, \\\\alpha_d^i\\\\) refers to position and angle of \\\\(d_d^i\\\\); \\\\(c_p^i, \\\\alpha_p^i\\\\) refers to position and angle of \\\\(d_p^i\\\\). We finally summation all losses to training the detector.\\n\\n4. Experiments\\n\\n4.1. Datasets\\n\\n**Waymo Open Dataset (WOD).** We conducted extensive experiments on the WOD \\\\[25\\\\] due to its diverse scenes. The WOD contains 798, 202 and 150 sequences for training, validation and testing, respectively. We adopted similar metrics (3D AP L1 and L2) as fully/weakly supervised methods \\\\[31, 34\\\\]. No annotations were used for training.\\n\\n**PandaSet dataset.** To compare with recent unsupervised methods \\\\[41\\\\], we also conducted experiments on the PandaSet \\\\[35\\\\]. Like \\\\[41\\\\], we split the dataset into 73 training and 30 validation snippets and use class-agnostic BEV AP and recall metrics with 0.3, 0.5, and 0.7 IoU thresholds.\\n\\n**KITTI dataset.** Since the KITTI detection dataset \\\\[6\\\\] did not provide consecutive frames, we only tested our method on the 3769 val split \\\\[4\\\\]. We used similar metrics (Car 3D AP R40 with 0.5 and 0.7 IoU thresholds) as employed in fully/weakly supervised methods \\\\[32, 34\\\\].\"}"}
{"id": "CVPR-2024-389", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method        | Vehicle 3D AP | Pedestrian 3D AP | Cyclist 3D AP |\\n|---------------|---------------|------------------|---------------|\\n| DBSCAN        | 2.32 0.29     | 1.94 0.25        | 0.51 0.00     |\\n| DBSCAN init-train | 17.36 2.65 | 14.87 2.29        | 1.65 0.00     |\\n| MODEST        | 18.51 6.46    | 15.83 5.48       | 11.83 0.17    |\\n| OYSTER        | 30.48 14.66   | 26.21 14.10      | 4.33 0.18     |\\n| Proto-vanilla | 35.22 20.19   | 31.58 18.36      | 17.60 10.34   |\\n| CPD (Ours)    | 57.79         | 37.40            | 50.18         |\\n\\nTable 1. Unsupervised 3D object detection results on WOD validation set. The results of previous methods are reproduced by us.\\n\\n| Method       | 3D AP L1 (IoU 0.7, 0.5) | 3D AP L2 (IoU 0.7, 0.5) |\\n|--------------|-------------------------|-------------------------|\\n| Vehicle      | Pedestrian              | Cyclist                 |\\n| MODEST       | 7.5 0.0                 | 6.5 0.0                 |\\n| OYSTER       | 21.6 0.6                | 18.7 0.5                |\\n| CPD (Ours)   | 37.2                    | 18.6                    |\\n\\nTable 2. Unsupervised 3D detection results on WOD test set.\\n\\n4.2. Implementation Details\\n\\nNetwork details. Both prototype and detection networks adopt the same 3D backbone as CenterPoint and the same RoI refinement network as Voxel-RCNN. For the WOD and KITTI datasets, we use the same detection range and voxel size as CenterPoint. For the Pandaset, we use the same detection range as OYSTER.\\n\\nTraining details. We adopt the widely used global scaling and rotation data augmentation. We trained our network on 8 Tesla V100 GPUs with the ADAM optimizer. We used a learning rate of 0.003 with a one-cycle learning rate strategy. We trained the CPD for 20 epochs.\\n\\n4.3. Comparison with Unsupervised Detectors\\n\\nResults on WOD. The results on the WOD validation set and test set are presented in Table 1 and Table 2. All methods use identical size thresholds to define the object classes and use single traversal. Our method significantly outperforms existing unsupervised methods. Notably, under the 3D AP L2 with IoU thresholds of 0.7, 0.5, and 0.5, our CPD outperforms OYSTER by 18.03%, 13.08%, and 4.55% on Vehicle, Pedestrian, and Cyclist, respectively. These advancements come from our MFC, CBR, and CST designs, which yield superior pseudo-labels and enhanced detection accuracy. CPD also surpasses the Proto-vanilla method, which uses class-specific prototype.\\n\\n| Method     | Labels | Easy Mod. Hard |\\n|------------|--------|----------------|\\n| CenterPoint| 100%   | 97.07 89.23 81.81 |\\n| Sparsely-sup. | 2%    | --- 49.69 31.55 25.91 |\\n| MODEST     | 0      | 47.56 33.43 30.57 |\\n| OYSTER     | 0      | 65.33 54.82 43.59 |\\n| CPD (Ours) | 0      | 90.85 81.01 79.80 |\\n\\nTable 4. Car detection comparison with fully/weakly supervised detectors on KITTI val set. The models are trained on WOD.\\n\\n| Method     | Labels | 3D AP L1 | 3D AP L2 |\\n|------------|--------|----------|----------|\\n| CenterPoint| 100%   | 89.23    | 73.72    |\\n| Sparsely-sup. | 2%    | - 32.15  | - 27.97  |\\n| MODEST     | 0      | 18.51    | 6.46     |\\n| OYSTER     | 0      | 30.48    | 14.66    |\\n| CPD (Ours) | 0      | 57.79    | 37.40    |\\n\\nTable 5. Vehicle detection comparison with fully/weakly supervised detectors on WOD validation set.\\n\\n4.4. Comparison with Fully/Weakly Supervised Detectors\\n\\nResults on KITTI dataset. To further validate our method, we pre-trained our CPD, along with OYSTER and MODEST, on WOD and tested them on the KITTI dataset using Statistical Normalization (SN). The car detection results are in Table 4. We first compared our method with a sparsely supervised method (weakly supervised with 2% labels) that annotates a single instance per frame for training. Our unsupervised CPD outperforms this sparsely supervised method by 23.52% 3D AP @ IoU 0.7 on moderate car class. Additionally, our method attains 90.85% and 81.01% 3D AP for the easy and moderate classes.\"}"}
{"id": "CVPR-2024-389", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4.5. Pseudo-label Comparison\\n\\nTo validate our pseudo-labels, we analyzed their 3D recall and precision on the WOD validation set. As shown in Table 6, our method surpasses the previous best-performing OYSTER with a 9.42% recall and 5.29% precision improvement (under a 0.7 IoU threshold). To understand the sources of this improvement, we examined the IoU between the pseudo-labels and ground truth, and compared the IoU distributions in Fig. 7 (a)(b)(c). We also present the mean absolute error of size, position, and angle between different pseudo-labels in Fig. 7 (d)(e)(f). The IoU distribution of our method is much closer to 1 than other methods, and it also exhibits lower errors in size, position, and angle. These results verify that our MFC and CBR significantly reduce label errors.\\n\\nTable 6. Pseudo-label comparison results on WOD validation set.\\n\\n| Method | 3D Recall | 3D Precision | IoU = 0.5 | IoU = 0.7 |\\n|--------|-----------|--------------|-----------|-----------|\\n| DBSCAN [5] | 22.85 | 16.44 | 6.52 | 29.41 |\\n| MODEST [40] | 17.35 | 12.04 | 4.89 | 32.28 |\\n| OYSTER [41] | 31.10 | 21.01 | 11.12 | 31.22 |\\n| Ours | 45.66 | 39.33 | 20.54 | 34.17 |\\n\\n4.6. Ablation Study\\n\\nComponents analysis of CPD.\\n\\nTo evaluate the individual contributions of our designs, we incrementally added each component and assessed their impact on vehicle detection using the WOD validation set. The results are shown in Table 7. Our MFC method surpasses Single Frame Clustering (SFC) by 2.52% in AP, attributed to the more complete point representation of objects across consecutive frames compared to a single frame. The CBR further enhances performance by 19.27% in AP, as it reduces size and location errors in pseudo-labels. The CST contributes an 8.09% increase in AP, demonstrating the effectiveness of geometric features from CProto in detecting sparse objects.\\n\\nTable 7. CPD component analysis results on WOD validation set.\\n\\n| Components | 3D AP L1 | 3D AP L2 |\\n|------------|----------|----------|\\n| SX C F C B | \u2713        | 17.36    | 2.65     |\\n| SX C F C B | \u2713        | 19.91    | 5.01     |\\n| SX C F C B | \u2713\u2713       | 48.26    | 28.01    |\\n| SX C F C B | \u2713\u2713\u2713      | 57.79    | 37.40    |\\n\\nComponents analysis of CSS Scoring.\\n\\nTo assess the effectiveness of our scoring system, we calculated the BEV AP of initial pseudo-labels with different scores. These...\"}"}
{"id": "CVPR-2024-389", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 9. Visualization comparison of different detection results on WOD validation set.\\n\\nTable 8. CBR component analysis results on WOD validation set.\\n\\nTable 9. CST component analysis results on WOD validation set.\\n\\n4.7. Visualization Comparison\\n\\nTo provide a more intuitive understanding of how our method improves detection performance, we visually compare our results with those of MODEST [40] and OYSTER [41], as shown in Fig. 9. MODEST often misses distant, sparse objects (Fig. 9(1.1)), while OYSTER detects them but inaccurately reports their sizes and positions (Fig. 9(2.1)). In contrast, CPD, using our CProto-based design, not only recognizes these objects but also accurately predicts their sizes and positions (Fig. 9(3.1)). Furthermore, since our CST reduces the influence of false pseudo-labels, the false positives (Fig. 9(3.2)) are also much fewer than the previous methods (Fig. 9(1.2)(2.2)).\\n\\n5. Conclusion\\n\\nThis paper presents the CPD framework, a novel approach for accurate unsupervised 3D object detection. First, we develop an MFC method to generate initial pseudo-labels. Then, a CProto set is constructed using CSS scoring. Next, we introduce a CBR method to refine these pseudo-labels. Lastly, a CST is designed to enhance detection accuracy for sparse objects. Extensive experiments have verified the effectiveness of our design. Notably, for the first time, our unsupervised CPD method surpasses some weakly supervised methods, demonstrating the advancement of our approach.\\n\\nLimitations.\\n\\nOne notable limitation of our work is the significantly lower Average Precision (AP) for minority classes, such as cyclists (Table 1), compared to more prevalent classes like vehicles. This disparity is largely due to the scarce instances of these minority classes within the dataset. Future efforts to collect such objects could be a promising avenue to tackle this issue.\\n\\nAcknowledgements.\\n\\nThis work was supported in part by the National Natural Science Foundation of China (No.62171393), the Fundamental Research Funds for the Central Universities (No.20720220064).\"}"}
