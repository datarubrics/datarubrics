{"id": "CVPR-2024-712", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nWhile recent 3D instance segmentation approaches show promising results based on transformer architectures, they often fail to correctly identify instances with similar appearances. They also ambiguously determine edges, leading to multiple misclassifications of adjacent edge points. In this work, we introduce a novel framework, called EASE, to overcome these challenges and improve the perception of complex 3D instances. We first propose a semantic guidance network to leverage rich semantic knowledge from a language model as intelligent priors, enhancing the functional understanding of real-world instances beyond relying solely on geometrical information. We explicitly instruct the basic instance queries using text embeddings of each instance to learn deep semantic details. Further, we utilize the edge prediction module, encouraging the segmentation network to be edge-aware. We extract voxel-wise edge maps from point features and use them as auxiliary information for learning edge cues. In our extensive experiments on large-scale benchmarks, ScanNetV2, ScanNet200, S3DIS, and STPLS3D, our EASE outperforms existing state-of-the-art models, demonstrating its superior performance.\\n\\n1. Introduction\\n\\nUnderstanding 3D scenes is a fundamental task within 3D computer vision. Given 3D point cloud scenes, identifying and perceiving instances on sparse points, along with assigning semantic class labels, play a crucial role in a comprehensive understanding of the entire spatial environment. In real-world 3D scenarios, significant occlusion and truncation often occur, especially when objects are overlapped or hidden by others. To address these issues, conventional works [3, 7, 8, 16, 18, 40, 43, 44] in 3D instance segmentation (3DIS) primarily focus on accurately generating region proposals (top-down) [16, 43, 44] or effectively grouping points with clustering algorithms (bottom-up) [3, 7, 8, 18, 40]. Inspired by the remarkable sensation of Mask-RCNN [14], the former (proposal-based) methods initially detect instances as bounding boxes and predict masks within each proposed region. However, these strategies are susceptible to the quality of the detection results. On the other hand, the latter (grouping-based) methods use clustering algorithms to group closely related points and aggregate point-wise class labels and instance features. While they show great advancements in 3DIS, these methods require additional manually tuned processing, such as point grouping [18] or voting mechanisms [31], to determine specific geometric properties (e.g., centers, occupancy).\\n\\nRecently, transformer-based 3DIS frameworks [22, 26, 35, 36] have tackled several limitations of traditional approaches by introducing a fully end-to-end pipeline. They directly predict masks from 3D points without resource-\\n\\n*Corresponding author.\\n\\nProject Page: https://kuai-lab.github.io/ease2024\\n\\nFigure 1. Examples of 3DIS for two challenging cases. (a) Semantic failure cases: the baseline [35] confuses seat-shaped objects as chair or toilet (Scene1) and also misclassifies large cuboid blocks as desk (Scene2). And (b) Instance misinterpretation cases: baseline erroneously segments a single object into multiple parts (Scene3) or merges multiple objects into one instance (Scene4).\"}"}
{"id": "CVPR-2024-712", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"exhaustive processing. These methods have achieved high performance and quickly become predominant in the 3DIS domain. However, despite these breakthroughs, they still face challenges in accurately masking everyday 3D instances relying solely on geometrical information. Specifically, they frequently suffer from cases where each instance appears similarly, but their semantic roles differ, as illustrated in Fig. 1 (a). Thus, gaining insight into the semantic properties of instances beyond mere appearance or position is crucial for effectively addressing complex real-world 3D scenarios. From this observation, we consider leveraging semantic knowledge along with the geometry information. Built upon the classic structure of the transformer-based framework [22, 35, 36], which trains queries that encode instance-specific knowledge for direct mask prediction, we introduce a carefully designed semantic guidance network.\\n\\nOne of the most promising ways to utilize semantic information is through visual language models [17, 32]. They learn generous visual-linguistic knowledge from large-scale image-text pairs, which empowers them to achieve outstanding progress in various 2D or 3D vision tasks [9, 11, 23, 29]. In this work, motivated by the success of these works, we explicitly instruct the transformer-based network to learn contextual variations among instances using text embeddings. Specifically, we strengthen the basic instance queries with profound semantic clues of text embeddings representing each instance. Here, the context details can serve as discriminative semantic priors for perceiving complex 3D instances adequately. Through our extensive experiments, we found that our semantic-guided approach effectively overcomes the challenges of existing methods.\\n\\nDifferent from the human vision system, which easily distinguishes boundaries between instances, we empirically observe that most 3DIS models [21, 22, 35, 36, 42] struggle with precisely capturing the boundaries in the 3D space. These methods often misinterpret the spatial range of instances and ambiguously determine their edges, leading to numerous misclassifications of adjacent edge points, as shown in Fig. 1 (b). We consider that these issues arise due to the absence of detailed guidance on inter-object boundaries, resulting in fuzzy edges. To tackle these challenges, we advocate for leveraging the edge prediction module to pilot the whole network to exploit edge-advanced features. We predict edge points across point cloud scenes and operate them as auxiliary information for learning practical edge cues, which encourages accurate recognition of the 3D spatial scope. Note that we supervise this module using dynamically generated point-wise pseudo edge labels.\\n\\nGiven landmark datasets for 3DIS, ScanNetV2 [4], ScanNet200 [33], S3DIS [1], and STPLS3D [2], we validate the effectiveness and robustness of our framework. Our method outperforms the existing state-of-the-art methods. To summarize, our main contributions are listed as follows:\\n\\n\u2022 We propose EASE, a novel 3D instance segmentation framework that utilizes the rich semantic clues from the language model. Specifically, context details of text embeddings serve as smart semantic priors, effectively enhancing the functional comprehension of 3D instances.\\n\\n\u2022 We introduce the edge prediction module guiding the entire network to take advantage of edge-aware features. Also, we utilize the module\u2019s output as auxiliary knowledge for learning edge cues, boosting 3DIS performance.\\n\\n\u2022 We analyze the effectiveness of our proposed method on multiple challenging benchmarks, including ScanNetV2, ScanNet200, S3DIS, and STPLS3D. Extensive experiments on various 3D scenarios validate that our method achieves new State-of-the-Art performance on 3DIS.\\n\\n2. Related Work\\n\\n3D Instance Segmentation. The 3D Instance Segmentation (3DIS) task aims to identify individual instances and assign semantic classes to each point within a 3D point scene. Traditional approaches in 3DIS are primarily categorized into three groups: proposal-based [16, 43, 44], grouping-based [3, 7, 8, 18, 40], and transformer-based methods [22, 35, 36]. The proposal-based approaches initially detect object proposals, such as 3D bounding boxes, and then estimate corresponding per-point semantic classes and instance masks. One of the grouping-based approaches, PointGroup [18], conducts point-wise clustering to group points into instances using dual point sets, which comprise original and shifted coordinates. Recently, transformer-based approaches predict instance queries that represent individual objects and decode them into per-point categories and instance labels directly. Mask3D [35] refines queries using masked cross-attention with hierarchical point features, leading the queries to focus on particular instances. SPFormer [36] updates instance queries via transformer decoder utilizing pre-computed superpoints, which can reduce the whole computational cost of the network. MAFT [22], on the other hand, recognizes representational limitations in the initial mask and replaces the masking process with object-localizing position queries. Based on the core idea of transformer-based methods, our novel approach incorporates a semantic guidance network and an auxiliary edge network to improve instance mask segmentation.\\n\\nSemantic Guided 3D Scene Understanding. Recent research in 3D scene understanding focuses on comprehending large-scale, real-world 3D environments. Conventional studies in this field are broadly categorized into two strategies: closed-set 3D scene understanding, a prevalent approach in diverse tasks [19, 27, 33, 34], and open-vocabulary methods [12, 29, 37], which focus on recognizing unseen categories within datasets. More recently, there has been significant progress in integrating pre-trained visual-language models like CLIP [32] or ALIGN [17] into...\"}"}
{"id": "CVPR-2024-712", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"3. Method\\n\\nIn this section, we introduce a novel 3D Instance Segmentation (3DIS) framework EASE leveraging the rich semantic guidance from the language model. Furthermore, we propose the edge prediction module, which encourages the pipeline to use edge-aware features for effectively perceiving instance boundaries. We provide an overview of the entire pipeline (Sec. 3.1, Fig. 2, and Alg. 1) and then elaborate on the details of our method: (i) Intelligent Semantic Prior (Sec. 3.2) and (ii) Edge-Aware 3DIS Framework (Sec. 3.3).\\n\\n3.1. Overview\\n\\nOur end-to-end 3DIS framework, as illustrated in Fig. 2 and Alg. 1, aims to enhance the comprehension of various instances across 3D point cloud scenes. Based on the classic transformer-based architecture [22, 35], which operates standard transformer building blocks, we directly infer instance masks from 3D point clouds. Our model consists of four main modules: (1) Sparse Convolutional Backbone, (2) Semantic Network, which provide the deep semantic cues of diverse real-world instances, (3) Mask Transformer Decoder with Mask Module and Query Refinement blocks, and (4) Edge Prediction Module, which benefits the whole network to take advantage of edge-aware features.\\n\\nMask Transformer. First, the Sparse Convolutional U-Net Backbone takes a colored point cloud $P$ as input and voxelizes $P$ into $V$ voxels to extract a multi-resolution hierarchical feature maps $F^i$, where $i = 0, 1, 2, 3, 4$. Following [28, 35], we set zero-initialized non-parametric instance queries $Q$, referring to point positions sampled with furthest point sampling (FPS) [30]. Given the $F$ and $Q$, the mask transformer decoder layer iteratively enhances the queries using the Mask Module (MM) and Query Refinement (QR) blocks. In the MM, as shown in Fig. 3 (a), we classify the category $c_i$ for $i = 1, 2, \\\\ldots, N_c$ of each query via linear classification head $f_{\\\\text{class}}$ using following cross-entropy loss:\\n\\n$$L_{\\\\text{cls}} = -\\\\sum_{c, w} c \\\\log f_{\\\\text{class}}(Q)$$\\n\\nwhere $w_c$ denotes one-hot encoded category labels and $D_{\\\\text{rep}}$ represents (input) data distribution. Also, the $Q$ are fed through the query head $f_{\\\\text{query}}$ to project them to the same feature space as $F_0$ for mask prediction. Afterwards, we...\"}"}
{"id": "CVPR-2024-712", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Algorithm 1: Overview of our framework EASE.\\n\\n**Input:** Colored point cloud \\\\( P \\\\).\\n\\n**Result:** Binary 3DIS mask \\\\( \\\\hat{M} \\\\).\\n\\n**Procedure:**\\n\\n1. **Sparse Convolutional Backbone.**\\n   - Extract multi-resolution feature maps \\\\( F_i \\\\) from \\\\( P \\\\).\\n\\n2. **Edge Prediction Module (EPM).**\\n   - Predict edge map \\\\( \\\\hat{E} \\\\) using full-resolution feature \\\\( F_0 \\\\).\\n   - Supervise \\\\( \\\\hat{E} \\\\) with pseudo edge label \\\\( E \\\\).\\n   - Encourage the network to learn edge-aware features.\\n\\n3. **Mask Transformer Decoder.**\\n   - for \\\\( l \\\\) do\\n     - for \\\\( i \\\\) do\\n       - Update queries \\\\( Q \\\\) using \\\\( \\\\text{Mattn} \\\\) and \\\\( F_i \\\\).\\n   - end\\n   - compute the similarity between the projected query features \\\\( Q \\\\) and \\\\( F_0 \\\\) using the dot product operation, then calculate the probability of the instance mask employing the sigmoid function as follows.\\n\\n4. **Semantic Network.**\\n   - Match the set of classes with text labels.\\n   - Encode text into text embeds \\\\( T \\\\) using CLIP.\\n   - Reinforce \\\\( Q \\\\) to contain semantic clues of \\\\( T \\\\).\\n\\n5. **Query Refinement.**\\n   - for \\\\( l \\\\) do\\n     - for \\\\( i \\\\) do\\n       - Update queries \\\\( Q \\\\) iteratively attend to each multi-scale feature \\\\( (F_1, F_2, F_3, F_4) \\\\) as follows:\\n   - end\\n   - end\\n   - compute the similarity between the projected query features \\\\( Q \\\\) and \\\\( F_0 \\\\) using the dot product operation, then calculate the probability of the instance mask employing the sigmoid function as follows.\\n\\n6. **Semantic Fusion.**\\n   - where the threshold value is 0.5 for binary mask. Further, to refine the query representation, we leverage the QR block, including masked cross-attention and self-attention mechanisms. Here, we utilize \\\\( \\\\text{Mattn} \\\\) as the foreground mask for the masked cross-attention layer, where \\\\( Q \\\\) iteratively attend to each multi-scale feature \\\\( (F_1, F_2, F_3, F_4) \\\\) as follows:\\n\\n7. **Semantic Network.**\\n   - for \\\\( l \\\\) do\\n     - for \\\\( i \\\\) do\\n       - Update queries \\\\( Q \\\\) iteratively attend to each multi-scale feature \\\\( (F_1, F_2, F_3, F_4) \\\\) as follows:\\n   - end\\n   - end\\n\\n8. **Semantic Network.**\\n   - for \\\\( l \\\\) do\\n     - for \\\\( i \\\\) do\\n       - Update queries \\\\( Q \\\\) iteratively attend to each multi-scale feature \\\\( (F_1, F_2, F_3, F_4) \\\\) as follows:\\n   - end\\n   - end\\n\\n9. **Semantic Network.**\\n   - for \\\\( l \\\\) do\\n     - for \\\\( i \\\\) do\\n       - Update queries \\\\( Q \\\\) iteratively attend to each multi-scale feature \\\\( (F_1, F_2, F_3, F_4) \\\\) as follows:\\n   - end\\n   - end\\n\\n3.2. Intelligent Semantic Prior\\n\\nIt is important to highlight that precise geometric properties (e.g., locations, distances and orientations) from 3D points significantly enhance 3DIS accuracy. While previous studies [22, 35, 36] benefit from the generous geometric potential of physical information, they are still limited in perceiving intricate real-world 3D instances. Hence, we propose to utilize semantic knowledge of individual instances as smart priors, effectively addressing the challenges.\\n\\nDeep Semantic Knowledge for 3D Instances.\\n\\nRecently, visual language models [17, 32] have highlighted the benefits of their capability to provide general embedded knowledge from large-scale image-text multimodal data. In this work, we explicitly instruct the transformer-based 3DIS network to learn contextual variations among instances by utilizing the power of the prevalent language model CLIP [32]. To achieve this goal, we utilize a carefully designed Semantic Network, as shown in Fig. 3 (b). Specifically, with the refined query \\\\( Q \\\\) from each QR block, we first categorize each query following the linear classification head \\\\( f_{\\\\text{class}} \\\\) in the MM. Given the set of category candidates, we map the category number \\\\( c_i \\\\) for \\\\( i = 1, 2, \\\\ldots, N \\\\) with the corresponding text descriptions (e.g., \\\"Cabinet\\\", \\\"Bookshelf\\\"). Next, these text labels go through the large-scale pre-trained language model, which outputs text embedding vectors \\\\( T \\\\). We then concatenate \\\\( Q \\\\) with \\\\( T \\\\) and strengthen the basic instance queries with deep semantic clues from text embeddings using the Semantic Fusion network as follows:\\n\\n\\\\[\\nQ = \\\\text{Sigmoid}(W_2 \\\\phi(W_1 p(Q)'T))\\n\\\\]\\n\\nwhere \\\\( ' \\\\) indicates channel-wise concatenation, \\\\( W_i \\\\) denotes the learnable parameters of the \\\\( i \\\\)-th linear layer, and \\\\( \\\\phi \\\\) is the non-linear activation function. We apply this process for each iteration of the QR. Ultimately, we consider these explicitly guided queries with contextual variations as discriminative semantic priors for the practical perception of complex 3D instances, enhancing 3DIS accuracy.\\n\\n3.3. Edge-Aware 3DIS Framework\\n\\nIn real-world 3D scenarios, instances are commonly positioned in diverse arrangements without following standard rules or patterns. Especially when instances are closely aligned, the edge information becomes critical for accurate segmentation. To address this challenge, we propose an edge-aware 3DIS framework that integrates edge prediction and instance segmentation to improve the accuracy of 3DIS. The framework consists of two main components: Edge Prediction Module (EPM) and Mask Transformer Decoder (MTD).\\n\\n**Edge Prediction Module (EPM):**\\n\\n- **Input:** Multi-resolution feature maps \\\\( F_i \\\\) from the Sparse Convolutional Backbone.\\n- **Output:** Edge maps \\\\( \\\\hat{E} \\\\).\\n\\n**Procedure:**\\n\\n1. Extract multi-resolution feature maps \\\\( F_i \\\\) from \\\\( P \\\\).\\n2. Predict edge maps \\\\( \\\\hat{E} \\\\) using full-resolution feature \\\\( F_0 \\\\).\\n3. Supervise \\\\( \\\\hat{E} \\\\) with pseudo edge labels \\\\( E \\\\).\\n4. Encourage the network to learn edge-aware features.\\n\\n**Mask Transformer Decoder (MTD):**\\n\\n- **Input:** Edge maps \\\\( \\\\hat{E} \\\\) and multi-resolution features \\\\( F_i \\\\) from the EPM.\\n- **Output:** Binary 3DIS mask \\\\( \\\\hat{M} \\\\).\\n\\n**Procedure:**\\n\\n1. for \\\\( l \\\\) do\\n   - for \\\\( i \\\\) do\\n     - Update queries \\\\( Q \\\\) using \\\\( \\\\text{Mattn} \\\\) and \\\\( F_i \\\\).\\n   - end\\n2. Compute the similarity between the projected query features \\\\( Q \\\\) and \\\\( F_0 \\\\) using the dot product operation, then calculate the probability of the instance mask employing the sigmoid function as follows.\\n\\n**Semantic Network:**\\n\\n- **Input:** Categorized queries \\\\( Q \\\\) and text embeddings \\\\( T \\\\) from CLIP.\\n- **Output:** Final set of refined queries \\\\( Q \\\\).\\n\\n**Procedure:**\\n\\n1. Match the set of classes with text labels.\\n2. Encode text into text embeds \\\\( T \\\\) using CLIP.\\n3. Reinforce \\\\( Q \\\\) to contain semantic clues of \\\\( T \\\\).\\n4. for \\\\( l \\\\) do\\n   - for \\\\( i \\\\) do\\n     - Update queries \\\\( Q \\\\) iteratively attend to each multi-scale feature \\\\( (F_1, F_2, F_3, F_4) \\\\) as follows:\\n   - end\\n   - end\\n5. Compute the similarity between the projected query features \\\\( Q \\\\) and \\\\( F_0 \\\\) using the dot product operation, then calculate the probability of the instance mask employing the sigmoid function as follows.\\n\\n**Query Refinement:**\\n\\n- **Input:** Refined queries \\\\( Q \\\\) from each QR block.\\n- **Output:** Final set of refined queries \\\\( Q \\\\).\\n\\n**Procedure:**\\n\\n1. for \\\\( l \\\\) do\\n   - for \\\\( i \\\\) do\\n     - Update queries \\\\( Q \\\\) iteratively attend to each multi-scale feature \\\\( (F_1, F_2, F_3, F_4) \\\\) as follows:\\n   - end\\n   - end\\n2. Compute the similarity between the projected query features \\\\( Q \\\\) and \\\\( F_0 \\\\) using the dot product operation, then calculate the probability of the instance mask employing the sigmoid function as follows.\\n\\n**Semantic Fusion:**\\n\\n- **Input:** Refined queries \\\\( Q \\\\) and text embeddings \\\\( T \\\\) from CLIP.\\n- **Output:** Final set of refined queries \\\\( Q \\\\).\\n\\n**Procedure:**\\n\\n1. Match the set of classes with text labels.\\n2. Encode text into text embeds \\\\( T \\\\) using CLIP.\\n3. Reinforce \\\\( Q \\\\) to contain semantic clues of \\\\( T \\\\).\\n4. for \\\\( l \\\\) do\\n   - for \\\\( i \\\\) do\\n     - Update queries \\\\( Q \\\\) iteratively attend to each multi-scale feature \\\\( (F_1, F_2, F_3, F_4) \\\\) as follows:\\n   - end\\n   - end\\n5. Compute the similarity between the projected query features \\\\( Q \\\\) and \\\\( F_0 \\\\) using the dot product operation, then calculate the probability of the instance mask employing the sigmoid function as follows.\\n\\n**Query Refinement:**\\n\\n- **Input:** Refined queries \\\\( Q \\\\) from each QR block.\\n- **Output:** Final set of refined queries \\\\( Q \\\\).\\n\\n**Procedure:**\\n\\n1. for \\\\( l \\\\) do\\n   - for \\\\( i \\\\) do\\n     - Update queries \\\\( Q \\\\) iteratively attend to each multi-scale feature \\\\( (F_1, F_2, F_3, F_4) \\\\) as follows:\\n   - end\\n   - end\\n2. Compute the similarity between the projected query features \\\\( Q \\\\) and \\\\( F_0 \\\\) using the dot product operation, then calculate the probability of the instance mask employing the sigmoid function as follows.\"}"}
{"id": "CVPR-2024-712", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pseudo Edge Label Calculation. To optimize the edge prediction module, we first compute point-wise pseudo edge labels for all 3D point scenes. We operate a traditional k-Nearest Neighbors (KNN) classification algorithm based on the KD-tree data structure to explore the k nearest points. Here, we utilize point-wise instance labels from datasets. For all points in a 3D scene, we set each point as a central point and compare their instance labels with those of k neighbor points. As shown in Fig. 4, if the count of distinct label points among its surroundings exceeds a predefined threshold $\\\\tau$, we identify the central point as a boundary point. We finally generate pseudo-binary edge maps $E_{1u}$, where points on the edges are 1 and internal points are 0 for the entire set of points $P$. Then, we voxelize the point-wise edge labels into the voxel-wise edge labels $E_{x0}$ to supervise the edge extraction head. Note that we precompute the pseudo edge labels for every 3D scene in the datasets before training.\\n\\nLearning Edge Cues. With dynamically generated pseudo edge labels $E$, we aim to enrich regular features to recognize the accurate spatial scope of 3D instances with precise boundaries. Specifically, we introduce an Edge Prediction Module, which predicts a voxel-wise dense edge map $\\\\hat{E}_{p0}$ to interactively guide the network in capturing edges of various instances. Given a full-resolution feature map $F_0$ from the backbone network, our edge extraction head, which consists of multiple shared MLPs, estimates the probabilities of edges for each voxel as follows:\\n\\n$$\\\\hat{E}_{p0} = \\\\text{MLP}(F_0) = \\\\text{Sigmoid}(W_2 \\\\cdot \\\\phi(W_1 \\\\cdot F_0))$$\\n\\nwhere $W_i$ denotes the learnable parameters of the $i$-th linear layer, and $\\\\phi$ indicates the non-linear activation function.\\n\\nFollowing [6, 10], as edge voxels constitute a small portion of the entire voxel set, we formulate weighted binary cross entropy loss to train the edge extraction head as follows:\\n\\n$$L_{\\\\text{edge}} = \\\\sum_{i} w \\\\cdot [E_i \\\\log(\\\\hat{E}_i) + \\\\hat{E}_i \\\\log(p)]$$\\n\\nwhere $w$ represent the weight value used to balance the substantial difference between the numbers of edge points and others. Then, we propagate the edge cues of prediction as auxiliary information for the backbone network, improving 3D instance understanding with edge-aware features. Furthermore, we train our edge network jointly in an end-to-end manner with low computational costs.\\n\\nLoss Function\\n\\nWe finally predict instance mask $\\\\hat{M}_f$ using the final set of queries in the MM. For training, we compute the following loss $L_{\\\\text{mask}}$ as a sum of the two losses:\\n\\n$$L_{\\\\text{mask}} = \\\\lambda \\\\cdot \\\\text{BCE}(M_f, \\\\hat{M}_f) + \\\\lambda \\\\cdot \\\\text{dice}(M_f, \\\\hat{M}_f)$$\\n\\nwhere $M_f$ denotes the ground-truth instance mask, and $L_{\\\\text{dice}}$ represents the Dice loss [5]. Ultimately, our model is trained end-to-end by minimizing the following loss $L_{\\\\text{total}}$:\\n\\n$$L_{\\\\text{total}} = \\\\lambda \\\\cdot L_{\\\\text{mask}} + \\\\lambda \\\\cdot L_{\\\\text{cls}} + \\\\lambda \\\\cdot L_{\\\\text{edge}}$$\\n\\nwhere each $\\\\lambda$ is a hyperparameter derived from grid searches to handle the strength of respective loss term.\"}"}
{"id": "CVPR-2024-712", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison of 3D Instance Segmentation performance with state-of-the-art approaches on the ScanNetV2 [4] dataset. We evaluate mean average precision (mAP) with different IoU thresholds over 18 classes on the ScanNetV2 validation / hidden test set. To address these limitations, we present a novel edge-aware framework featuring a semantic guidance network. As reported in Tab. 1, EASE generally outperforms other methods, achieving new state-of-the-art accuracy in terms of mAP and mAP_{50} on both validation (60.2 / 77.2) and hidden test (59.8 / 78.9) sets. These results confirm that our edge cues and smart semantic prior benefits the entire network, leading to high precision in 3D instance segmentation.\\n\\nS3DIS. In Tab. 2, we evaluate 3DIS performance on Area 5 and 6-fold cross-validation of the S3DIS [1] dataset. For Area 5 evaluation, we employ data from Area 5 for validation and the other areas for training. Also, for 6-fold cross-validation, we assess validation scores across six other areas and compute the average. In both evaluations, EASE demonstrates significant performance improvements up to +2.4 / 2.5 (Area 5) and +0.8 / 0.9 (6-fold) for mAP / mAP_{50}.\\n\\nScanNet200. We also demonstrate considerable performance on the ScanNet200 validation set (see Tab. 3). Our method EASE precisely segments 3D instances for various categories compared to current state-of-the-art approaches.\\n\\nSTPLS3D. Remarkably, our EASE network also achieves higher scores on the STPLS3D [2] dataset, which consists of outdoor 3D point cloud scenes. EASE exceeds the second-best results with +1.1 / 1.6 / 1.3 margins in mAP / mAP_{50} / mAP_{25}. These results highlight the effectiveness of our method in understanding diverse real-world 3D instances, both from indoor and outdoor environments.\\n\\nTable 2. Comparison of 3D Instance Segmentation performance with state-of-the-art approaches on the S3DIS [1] Area 5 and 6-fold cross validation set. We evaluate mAP across different IoU thresholds, along with mean precision (mPrec) and mean recall (mRec) at a 50% IoU threshold over 13 classes of the S3DIS.\\n\\nTable 3. Comparison of 3D Instance Segmentation performance with state-of-the-art approaches on the ScanNet200 [33] val set.\\n\\nTable 4. Comparison of 3D Instance Segmentation performance with state-of-the-art approaches on the STPLS3D [2] test dataset.\\n\\n4.3. Ablation Studies\\n\\nEffect of Semantic and Edge Modules. In Tab. 5, we evaluate the variants of our method w/ and w/o the Semantic Network (SN) and Edge Prediction Module (EPM). The addition of two modules improves 3DIS accuracy across all experiments. The SN especially underscores the significance of incorporating rich semantic knowledge for effective segmentation, achieving +1.3 / 0.7 and +1.5 / 1.7 improvements on mAP / mAP_{50} for ScanNetV2 and S3DIS, respectively. EPM further enhances performance via the auxiliary edge information for edge-aware features, resulting in gains of +0.9 / 0.8 and +1.9 / 0.9. Notably, integrating\"}"}
{"id": "CVPR-2024-712", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Method ScanNet Val S3DIS Area 5\\nBaseline Semantic Edge mAP / mAP 50\\n\\n\u2713 - 58.4 / 75.9 56.6 / 68.4\\n\u2713 \u2713 - 59.7 / 76.6 58.1 / 70.1\\n\u2713 \u2713 - 59.3 / 76.7 58.5 / 69.3\\n\u2713 \u2713 \u2713 60.2 / 77.2 59.0 / 71.6\\n\\nTable 5. Ablation study to see the effect of our proposed two main modules: Semantic Network and Edge Prediction Module.\\n\\nMethod ScanNet Val S3DIS Area 5\\nUsage E-wise Sum Concat mAP / mAP 50\\n\\nf cls \u2713 - 58.9 / 76.0 57.8 / 68.5\\nf cls \u2713 - 59.3 / 76.5 58.3 / 70.1\\nS \u00a8 F \u2713 - 59.6 / 76.9 58.6 / 69.6\\nS \u00a8 F \u2713 - 60.2 / 77.2 59.0 / 71.6\\n\\nTable 6. Ablation study of leveraging text embeddings to provide semantic details.\\n\\nf cls represents the classification head in the Mask Module, and S \u00a8 F stands for the Semantic Fusion network.\\n\\nSN and EPM leads to considerable advances (+1.8 / 1.3 and +2.4 / 3.2), confirming the effectiveness of each module.\\n\\nApplications of Text Embeddings.\\nIn Tab. 6, we analyze various applications of text embeddings to determine the most effective strategy for the functional understanding of complex 3D instances. We utilize text embeddings for category classification from f cls of the Mask Module to boost classification accuracy. Also, we strengthen basic queries using the Semantic Fusion (S \u00a8 F) network as described in Sec. 3.2. For experiments with the E-wise Sum, we map the text embeddings to the same feature space as the basic queries through MLPs and sum together. While incorporating semantic cues via element-wise sum enhances 3DIS accuracy, it exhibits limited improvements compared to concatenation. By comparison, applying the semantic knowledge across the entire transformer decoder through the S \u00a8 F outperforms using them only for the classification head.\\n\\nWeight Value of Edge Prediction Loss.\\nTo encourage the network to learn practical edge cues, we utilize an edge prediction network. In particular, we compute the weighted binary cross-entropy loss to effectively train the edge extraction head from sparse edge points. Here, we explore the impact of the coefficient value $w$ of $L_{edge}$ on 3DIS accuracy.\\n\\nFirst, as shown in Fig. 5, we empirically find that $w$ influences the thickness of edges in a 3D scene. In Tab. 7, for the ScanNet [4] consisting of relatively small-scale scenes, a smaller value of $w$ (e.g., 2), resulting in thin edges, leads to higher performance. However, for the vast-scale scenes of $\\\\omega = 10$, $\\\\omega = 20$, Pseudo Edge Label\\n\\nFigure 5. Visualization of edge prediction results (red) for different weight values $w$ in the weighted binary cross-entropy loss $L_{edge}$, along with corresponding pseudo-edge label (green).\\n\\nFeature map $w$ in $L_{edge}$ ScanNet Val S3DIS Area 5\\n\\n| $F_0$ | 59.3 / 76.7 56.9 / 68.6 |\\n| $F_4$ | 59.0 / 76.1 56.7 / 68.5 |\\n| $F_0$ | 58.9 / 76.3 58.4 / 68.9 |\\n| $F_4$ | 58.5 / 76.0 58.2 / 68.6 |\\n| $F_0$ | 58.4 / 75.8 58.5 / 69.3 |\\n| $F_4$ | 58.5 / 75.7 58.2 / 69.0 |\\n\\nTable 7. Ablation study to investigate the impact of varying weight values $w$ in the weighted binary cross-entropy $L_{edge}$, using high ($F_0$) and low ($F_4$) resolution feature maps for edge prediction.\\n\\nthe S3DIS [1], a higher value of $w$ (e.g., 10) leads to better performance with thicker edges. We finally observe the correlation between the scene scale and the weight value $w$; the optimal value for edge learning is proportional to the scale.\\n\\nBesides, methods using high-resolution feature maps (e.g., $F_0$) for edge prediction generally perform better than those using low-resolution features maps (e.g., $F_4$).\\n\\n4.4. Qualitative Analyses\\nVisual Comparison.\\nIn this section, we qualitatively confirm the usefulness of our novel framework EASE. We visualize the predicted semantic and instance masks of the baseline model [35] and ours on the ScanNetV2 [4] validation set, in Fig. 6. We emphasize the key differences using green and yellow boxes. As shown in the semantic results (Sem.), ours accurately classifies instances with similar appearances (e.g., desk and bed, door and cabinet) instead of the baseline, which incorrectly recognizes them. Also, as illustrated in the instance mask results (Inst.) of Scene1 and Scene2, ours segments a single object as one, unlike the baseline, which splits it into multiple parts. Further, in Scene 3 (Inst.), ours correctly distinguishes multiple objects (door and cabinet) in contrast to the baseline model. These qualitative results demonstrate that our method provides more detailed and robust segmentation results than the baseline method.\\n\\nImpact of Utilizing Semantic Priors.\\nTo verify the effectiveness of the Semantic Network, we present t-SNE [38] visualizations of query features for instances with similar 20650 20650\"}"}
{"id": "CVPR-2024-712", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Qualitative comparison of 3D Instance Segmentation performance on the ScanNetV2 [4] validation set. We visualize semantic (Sem.) and instance (Inst.) masks of the baseline model and ours with Ground Truth (GT) masks. The critical differences are emphasized using green and yellow-colored boxes. Note that the color map (top right) represents semantic labels. Best viewed in color.\\n\\nFigure 7. Category probability map (spatial distribution) for queries with high confidence. The x-axis denotes queries, the y-axis represents categories, and the z-axis (right) is probabilities. Appearances, which pose challenges for 3DIS models. As shown in Fig. 8, the baseline (MAFT [22]) model exhibits disorganized clusters, whereas ours shows relatively distinct feature space for challenging instances. Further, in Fig. 7, we visualize the spatial distribution of category probability for instance queries. The baseline tends to confuse per-query categories and predicts multiple categories for each query with low probability, causing semantic misclassifications. However, our model EASE overcomes this problem using semantic priors, providing clearer insight. These qualitative findings confirm that our semantic network effectively prompts the network to learn instance-specific semantic knowledge from intelligent text embedding priors.\\n\\n5. Conclusion\\nCurrent 3D instance segmentation approaches often face challenges in understanding real-world 3D instances relying solely on geometrical information. Specifically, (i) these methods suffer from identifying instances with similar appearances, and (ii) they often misinterpret the spatial extent of instances, resulting in unclear edges. To tackle these challenges, EASE, our novel framework, focuses on context details of text embeddings from the language model as smart priors, enhancing practical semantic understanding. Also, we employ the edge prediction module, guiding the network to reduce misclassifications near edges with edge-aware features. Our extensive experiments verify the effectiveness of EASE, achieving new state-of-the-art scores.\\n\\nAcknowledgments\\nThis work was supported by Culture, Sports and Tourism R&D Program through the Korea Creative Content Agency grant funded by the Ministry of Culture, Sports and Tourism in 2023. (RS-2023-00227409, Development of sketch-based semantic 3D modeling technology for creating user-centric Metaverse content spaces for indoor spaces, 80%), (R2022020068, 4D Content Generation and Copyright Protection with Artificial Intelligence, 10%), Electronics and Telecommunications Research Institute (ETRI) grant funded by the Korean government. (24ZC1200, Research on hyper-realistic interaction technology for five senses and emotional experience, 9%), and the Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (2019-0-00079, Artificial Intelligence Graduate School Program (Korea University), 1%).\"}"}
{"id": "CVPR-2024-712", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Iro Armeni, Ozan Sener, Amir R Zamir, Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 3D semantic parsing of large-scale indoor spaces. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1534\u20131543, 2016. 2, 5, 6, 7\\n\\n[2] Meida Chen, Qingyong Hu, Zifan Yu, Hugues Thomas, Andrew Feng, Yu Hou, Kyle McCullough, Fengbo Ren, and Lucio Soibelman. Stpl3d: A large-scale synthetic and real aerial photogrammetry 3D point cloud dataset. arXiv preprint arXiv:2203.09065, 2022. 2, 5, 6\\n\\n[3] Shaoyu Chen, Jiemin Fang, Qian Zhang, Wenyu Liu, and Xinggang Wang. Hierarchical aggregation for 3D instance segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15467\u201315476, 2021. 1, 2, 6\\n\\n[4] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. Scannet: Richly-annotated 3D reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 5828\u20135839, 2017. 2, 5, 6, 7, 8\\n\\n[5] Ruoxi Deng, Chunhua Shen, Shengjun Liu, Huibing Wang, and Xinru Liu. Learning to predict crisp boundaries. In Proceedings of the European conference on computer vision (ECCV), pages 562\u2013578, 2018. 5\\n\\n[6] Shenglan Du, Nail Ibrahimli, Jantien Stoter, Julian Kooij, and Liangliang Nan. Push-the-boundary: Boundary-aware feature propagation for semantic segmentation of 3D point clouds. In 2022 International Conference on 3D Vision (3DV), pages 1\u201310. IEEE, 2022. 5\\n\\n[7] Cathrin Elich, Francis Engelmann, Theodora Kontogianni, and Bastian Leibe. 3D bird's-eye-view instance segmentation. In German Conference on Pattern Recognition, pages 48\u201361. Springer, 2019. 1, 2\\n\\n[8] Francis Engelmann, Martin Bokeloh, Alireza Fathi, Bastian Leibe, and Matthias Nie\u00dfner. 3D-MPA: Multi-proposal aggregation for 3D semantic instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 9031\u20139040, 2020. 1, 2, 6\\n\\n[9] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level labels. In European Conference on Computer Vision, pages 540\u2013557. Springer, 2022. 2\\n\\n[10] Jingyu Gong, Jiachen Xu, Xin Tan, Jie Zhou, Yanyun Qu, Yuan Xie, and Lizhuang Ma. Boundary-aware geometric encoding for semantic segmentation of point clouds. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 1424\u20131432, 2021. 5\\n\\n[11] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. arXiv preprint arXiv:2104.13921, 2021. 2\\n\\n[12] Huy Ha and Shuran Song. Semantic abstraction: Open-world 3D scene understanding from 2D vision-language models. In 6th Annual Conference on Robot Learning, 2022. 2\\n\\n[13] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3D instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2940\u20132949, 2020. 6\\n\\n[14] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask R-CNN. In Proceedings of the IEEE international conference on computer vision, pages 2961\u20132969, 2017. 1\\n\\n[15] Tong He, Chunhua Shen, and Anton Van Den Hengel. Dyco3D: Robust instance segmentation of 3D point clouds through dynamic convolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 354\u2013363, 2021. 6\\n\\n[16] Ji Hou, Angela Dai, and Matthias Nie\u00dfner. 3D-SIS: 3D semantic instance segmentation of RGB-D scans. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4421\u20134430, 2019. 1, 2, 6\\n\\n[17] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 4904\u20134916. PMLR, 2021. 2, 4\\n\\n[18] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, Chi-Wing Fu, and Jiaya Jia. Pointgroup: Dual-set point grouping for 3D instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and Pattern recognition, pages 4867\u20134876, 2020. 1, 2, 6\\n\\n[19] Abhijit Kundu, Xiaoqi Yin, Alireza Fathi, David Ross, Brian Brewington, Thomas Funkhouser, and Caroline Pantofaru. Virtual multi-view fusion for 3D semantic segmentation. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXIV, pages 518\u2013535. Springer, 2020. 2\\n\\n[20] Jean Lahoud, Bernard Ghanem, Marc Pollefeys, and Martin R Oswald. 3D instance segmentation via multi-task metric learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9256\u20139266, 2019. 6\\n\\n[21] Xin Lai, Jianhui Liu, Li Jiang, Liwei Wang, Hengshuang Zhao, Shu Liu, Xiaojuan Qi, and Jiaya Jia. Stratified transformer for 3D point cloud segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8500\u20138509, 2022. 2\\n\\n[22] Xin Lai, Yuhui Yuan, Ruihang Chu, Yukang Chen, Han Hu, and Jiaya Jia. Mask-attention-free transformer for 3D instance segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3693\u20133703, 2023. 1, 2, 3, 4, 5, 6, 8\\n\\n[23] Feng Liang, Bichen Wu, Xiaoliang Dai, Kunpeng Li, Yinan Zhao, Hang Zhang, Peizhao Zhang, Peter Vajda, and Diana Marculescu. Open-vocabulary semantic segmentation with mask-adapted clip. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7061\u20137070, 2023. 2\\n\\n[24] Zhihao Liang, Zhihao Li, Songcen Xu, Mingkui Tan, and Kui Jia. Instance segmentation in 3D scenes using semantic superpoint tree networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2783\u20132792, 2021. 6\\n\\n[25] Chen Liu and Yasutaka Furukawa. Masc: Multi-scale affinity...\"}"}
{"id": "CVPR-2024-712", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[26] Jiahao Lu, Jiacheng Deng, Chuxin Wang, Jianfeng He, and Tianzhu Zhang. Query refinement transformer for 3d instance segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 18516\u201318526, 2023.\\n\\n[27] John McCormac, Ankur Handa, Andrew Davison, and Stefan Leutenegger. Semanticfusion: Dense 3d semantic mapping with convolutional neural networks. In 2017 IEEE International Conference on Robotics and Automation (ICRA), pages 4628\u20134635. IEEE, 2017.\\n\\n[28] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-to-end transformer model for 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2906\u20132917, 2021.\\n\\n[29] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 815\u2013824, 2023.\\n\\n[30] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas J Guibas. Pointnet++: Deep hierarchical feature learning on point sets in a metric space. Advances in neural information processing systems, 30, 2017.\\n\\n[31] Charles R Qi, Or Litany, Kaiming He, and Leonidas J Guibas. Deep hough voting for 3d object detection in point clouds. In proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9277\u20139286, 2019.\\n\\n[32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\\n\\n[33] David Rozenberszki, Or Litany, and Angela Dai. Language-grounded indoor 3d semantic segmentation in the wild. In European Conference on Computer Vision, pages 125\u2013141. Springer, 2022.\\n\\n[34] Corentin Sautier, Gilles Puy, Spyros Gidaris, Alexandre Boulch, Andrei Bursuc, and Renaud Marlet. Image-to-lidar self-supervised distillation for autonomous driving data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9891\u20139901, 2022.\\n\\n[35] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3d for 3d semantic instance segmentation. arXiv preprint arXiv:2210.03105, 2022.\\n\\n[36] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer for 3d scene instance segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 2393\u20132401, 2023.\\n\\n[37] Ayc \u00b8a Takmaz, Elisabetta Fedele, Robert W Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. Open-mask3d: Open-vocabulary 3d instance segmentation. arXiv preprint arXiv:2306.13631, 2023.\\n\\n[38] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. Journal of machine learning research, 9(11), 2008.\\n\\n[39] Thang Vu, Kookhoi Kim, Tung M Luu, Thanh Nguyen, and Chang D Yoo. Softgroup for 3d instance segmentation on point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2708\u20132717, 2022.\\n\\n[40] Weiyue Wang, Ronald Yu, Qiangui Huang, and Ulrich Neumann. Sgpn: Similarity group proposal network for 3d point cloud instance segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2569\u20132578, 2018.\\n\\n[41] Xinlong Wang, Shu Liu, Xiaoyong Shen, Chunhua Shen, and Jiaya Jia. Associatively segmenting instances and semantics in point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4096\u20134105, 2019.\\n\\n[42] Yizheng Wu, Min Shi, Shuaiyuan Du, Hao Lu, Zhiguo Cao, and Weicai Zhong. 3d instances as 1d kernels. In European Conference on Computer Vision, pages 235\u2013252. Springer, 2022.\\n\\n[43] Bo Yang, Jianan Wang, Ronald Clark, Qingyong Hu, Sen Wang, Andrew Markham, and Niki Trigoni. Learning object bounding boxes for 3d instance segmentation on point clouds. Advances in neural information processing systems, 32, 2019.\\n\\n[44] Li Yi, Wang Zhao, He Wang, Minhyuk Sung, and Leonidas J Guibas. Gspn: Generative shape proposal network for 3d instance segmentation in point cloud. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3947\u20133956, 2019.\\n\\n[45] Min Zhong, Xinghao Chen, Xiaokang Chen, Gang Zeng, and Yunhe Wang. Maskgroup: Hierarchical point grouping and masking for 3d instance segmentation. In 2022 IEEE International Conference on Multimedia and Expo (ICME), pages 1\u20136. IEEE, 2022.\"}"}
