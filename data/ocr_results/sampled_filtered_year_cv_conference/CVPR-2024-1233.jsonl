{"id": "CVPR-2024-1233", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method          | Ego Status | L2 (m) | Collision (%) | CCR (%) | ckpt. source |\\n|-----------------|------------|--------|---------------|---------|--------------|\\n| ST-P3           | \u2713          | 1.59   | 2.64          | 3.73    | 2.65         |\\n|                |            |        |               |         | 0.69         |\\n|                |            |        |               |         | 3.62         |\\n|                |            |        |               |         | 8.39         |\\n|                |            |        |               |         | 4.23         |\\n|                |            |        |               |         | 2.53         |\\n|                |            |        |               |         | 8.17         |\\n|                |            |        |               |         | 14.4         |\\n| Official        | \u2713          | 0.59   | 1.01          | 1.48    | 1.03         |\\n|                |            |        |               |         | 0.16         |\\n|                |            |        |               |         | 0.51         |\\n|                |            |        |               |         | 1.64         |\\n|                |            |        |               |         | 0.77         |\\n|                |            |        |               |         | 0.35         |\\n|                |            |        |               |         | 1.46         |\\n|                |            |        |               |         | 3.99         |\\n|                |            |        |               |         | 1.93         |\\n| UniAD           | \u2713          | 0.35   | 0.63          | 0.99    | 0.66         |\\n|                |            |        |               |         | 0.16         |\\n|                |            |        |               |         | 0.43         |\\n|                |            |        |               |         | 1.27         |\\n|                |            |        |               |         | 0.62         |\\n|                |            |        |               |         | 1.32         |\\n|                |            |        |               |         | 3.63         |\\n|                |            |        |               |         | 1.72         |\\n|                |            |        |               |         | 0.21         |\\n|                | \u2713          | 0.02   | 0.25          | 0.75    | 0.46         |\\n|                |            |        |               |         | 0.02         |\\n|                |            |        |               |         | 0.25         |\\n|                |            |        |               |         | 0.84         |\\n|                |            |        |               |         | 0.37         |\\n|                |            |        |               |         | 1.33         |\\n|                |            |        |               |         | 3.24         |\\n|                | \u2713          | 0.27   | 0.54          | 0.90    | 0.57         |\\n|                |            |        |               |         | 0.04         |\\n|                |            |        |               |         | 0.35         |\\n|                |            |        |               |         | 1.15         |\\n|                |            |        |               |         | 0.54         |\\n|                |            |        |               |         | 0.60         |\\n|                |            |        |               |         | 2.38         |\\n|                |            |        |               |         | 5.18         |\\n|                | \u2713          | 0.17   | 0.34          | 0.60    | 0.37         |\\n|                |            |        |               |         | 0.04         |\\n|                |            |        |               |         | 0.27         |\\n|                |            |        |               |         | 0.85         |\\n|                |            |        |               |         | 0.37         |\\n|                |            |        |               |         | 0.27         |\\n|                |            |        |               |         | 2.52         |\\n|                |            |        |               |         | 6.60         |\\n|                |            |        |               |         | 2.93         |\\n| Official        | \u2713          | 0.27   | 0.54          | 0.90    | 0.57         |\\n|                |            |        |               |         | 0.04         |\\n|                |            |        |               |         | 0.35         |\\n|                |            |        |               |         | 1.80         |\\n|                |            |        |               |         | 0.73         |\\n|                |            |        |               |         | 0.63         |\\n|                |            |        |               |         | 3.38         |\\n|                |            |        |               |         | 7.93         |\\n|                |            |        |               |         | 3.98         |\\n|                | \u2713          | 0.30   | 0.52          | 0.83    | 0.55         |\\n|                |            |        |               |         | 0.10         |\\n|                |            |        |               |         | 0.37         |\\n|                |            |        |               |         | 1.30         |\\n|                |            |        |               |         | 0.59         |\\n|                |            |        |               |         | 1.32         |\\n|                |            |        |               |         | 3.63         |\\n|                |            |        |               |         | 1.72         |\\n|                | \u2713          | 0.28   | 0.42          | 0.68    | 0.46         |\\n|                |            |        |               |         | 0.04         |\\n|                |            |        |               |         | 0.37         |\\n|                |            |        |               |         | 1.07         |\\n|                |            |        |               |         | 0.49         |\\n|                | \u2713          | 0.16   | 0.32          | 0.59    | 0.35         |\\n|                |            |        |               |         | 0.00         |\\n|                |            |        |               |         | 0.27         |\\n|                |            |        |               |         | 0.85         |\\n|                |            |        |               |         | 0.37         |\\n|                |            |        |               |         | 0.27         |\\n|                |            |        |               |         | 2.52         |\\n|                |            |        |               |         | 6.60         |\\n|                |            |        |               |         | 2.93         |\\n|                | \u2713          | 0.29   | 0.73          | 0.34    | 0.35         |\\n|                |            |        |               |         | 0.00         |\\n|                |            |        |               |         | 0.29         |\\n|                |            |        |               |         | 0.73         |\\n|                |            |        |               |         | 0.34         |\\n|                |            |        |               |         | 2.62         |\\n|                |            |        |               |         | 6.51         |\\n|                |            |        |               |         | 3.16         |\\n| Official        | \u2713          | 0.27   | 0.54          | 0.90    | 0.57         |\\n|                |            |        |               |         | 0.04         |\\n|                |            |        |               |         | 0.35         |\\n|                |            |        |               |         | 1.80         |\\n|                |            |        |               |         | 0.73         |\\n|                |            |        |               |         | 0.63         |\\n|                |            |        |               |         | 3.38         |\\n|                |            |        |               |         | 7.93         |\\n|                |            |        |               |         | 3.98         |\\n|                | \u2713          | 0.30   | 0.52          | 0.83    | 0.55         |\\n|                |            |        |               |         | 0.10         |\\n|                |            |        |               |         | 0.37         |\\n|                |            |        |               |         | 1.30         |\\n|                |            |        |               |         | 0.59         |\\n|                |            |        |               |         | 1.32         |\\n|                |            |        |               |         | 3.63         |\\n|                |            |        |               |         | 1.72         |\\n|                | \u2713          | 0.30   | 0.52          | 0.83    | 0.55         |\\n|                |            |        |               |         | 0.10         |\\n|                |            |        |               |         | 0.37         |\\n|                |            |        |               |         | 1.30         |\\n|                |            |        |               |         | 0.59         |\\n|                |            |        |               |         | 1.32         |\\n|                |            |        |               |         | 3.63         |\\n|                |            |        |               |         | 1.72         |\\n|                | \u2713          | 0.30   | 0.52          | 0.83    | 0.55         |\\n|                |            |        |               |         | 0.10         |\\n|                |            |        |               |         | 0.37         |\\n|                |            |        |               |         | 1.30         |\\n|                |            |        |               |         | 0.59         |\\n|                |            |        |               |         | 1.32         |\\n|                |            |        |               |         | 3.63         |\\n|                |            |        |               |         | 1.72         |\\n\\nTable 1. Open-loop planning performance.\\n\\nTable 2. The V AD-base model's robustness to images and ego status.\\n\\nTo ascertain the impact of perceptual information and ego status on the ultimate planning performance, we systematically introduced noise into each component separately. We utilize the official V AD-Base checkpoint that uses ego status in its planner module. *: the results of V AD-Base without ego status in its planner. We can observe that introducing corruption to images markedly affects the perception results, especially in the case of using blank images; nonetheless, this does not markedly disrupt the ultimate planning results. In contrast to the minor impact of image corruption on planning, modifications to the ego vehicle's velocity have a significant effect on the planning results. Experimental results reveal that in an end-to-end model incorporating both ego status and perceptual information, decision-making is disproportionately influenced by ego status, thereby substantially increasing the model's safety risks. \u2020: The collision rate is not precise as the ego car may have departed from the local BEV area. When the input velocity is zero, the model produces almost stationary trajectories, resulting in excellent performance in the CCR. This can be seen as a limitation of the CCR metric.\\n\\nTable 3. While adding map perception task into the BEV-Planner method, we can observe that the model obtains worse L2 distance and collision rate performance, but achieves better CCR. We use a pretrained map perception checkpoint as the initialization of the BEV-Planner (init*) and BEV-Planner+Map model.\"}"}
{"id": "CVPR-2024-1233", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. We exhibit the predicted trajectories of the VAD model (incorporating ego status in its planner) under various image corruptions. All trajectories within a given scene (spanning 20 seconds) are presented in the global coordinate system. Each triangular marker signifies a ground truth trajectory point of the ego vehicle, with different colors representing distinct timesteps. Notably, the model\u2019s predicted trajectory maintains plausibility, even when blank images serve as input. The trajectories within the red boxes, however, are suboptimal, as further elucidated in ??.\\n\\nWhile corruptions were applied to all surround-view images, for the sake of visualization, only the corresponding front-view images at the initial timestep are displayed.\\n\\n| Method          | Ego Status | Collision (m) | Collision-ST (m) | Collision-LR (m) |\\n|-----------------|------------|---------------|------------------|------------------|\\n| BEV-Planner     | \u2717          | 0.10          | 0.37             | 1.30             |\\n| BEV-Planner+Map | \u2717          | 0.12          | 0.37             | 2.19             |\\n\\nTable 4. Collision-ST is the collision rate with going straight driving commands. Collision-LR is the collision rate with turning left/right commands.\\n\\n| Method          | Ego Status | L2 (m) | L2-ST (m) | L2-LR (m) |\\n|-----------------|------------|--------|-----------|-----------|\\n| BEV-Planner     | \u2717          | 0.30   | 0.52      | 0.83      |\\n| BEV-Planner+Map | \u2717          | 0.53   | 0.94      | 1.40      |\\n\\nTable 5. L2-ST is the L2 distance with going straight driving commands. L2-LR is the L2 distance with turning left/right commands.\\n\\nObtain basically similar results in terms of L2 and collision rate. Is it justifiable to assert that our BEV-Planner++ design, characterized by its simplicity and effectiveness, can attain comparable outcomes to other more intricate methodologies, even in the absence of utilizing perception data? In fact, as the performance of the final planning module is predominantly influenced by the ego vehicle status, the design of other components does not significantly affect the planning results. Consequently, we argue that methods utilizing ego status are not directly comparable and conclusions should not be drawn from such comparisons.\\n\\nHow about not using ego status? Given that the ego vehicle status exerts a dominant influence on the planning results, it prompts an important inquiry: Is it feasible and beneficial to exclude ego status in open-loop end-to-end research?\\n\\nNeglected Ego Status in Perception Stage. In fact, existing methods [16, 43] ignore the impact of using ego status on planning in BEV Encoder. More details are in the Appendix.\\n\\nWithout Ego Status, the Simpler, the Better? People might wonder why our BEV-Planner, without using additional perception tasks (including Depth, HD map, Tracking, etc.) and ego status, achieves better results in L2 distance and collision rate than other methods (ID-1 and 4). Since our BEV-Planner performs poorly in terms of CCR, what would happen if we added map perception tasks to our baseline? To address these questions, we designed a \u201cBEV-Planner+Map\u201d model by introducing a map perception task into our pipeline, mainly following the designs of UniAD. As shown in Tab. 3, when map perception is introduced, the model exhibits poorer results in terms of L2 distance and collision rate metrics. The only aspect that aligns with our expectations is that the introduction of map perception significantly reduces the CCR. Through a comparison of BEV-Planner with BEV-Planner (init*), we observe that the use of map-pretrained weights can enhance performance. This finding implies that the decrease in L2 distance and collision rate is not just due to the introduction of map perception but also due to the enhancement of other components.\"}"}
{"id": "CVPR-2024-1233", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. For the VAD-based model that incorporates ego status in its planner, we introduce the noise to the ego velocity with the visual inputs remaining constant. Notably, when the velocity data of the ego vehicle is perturbed, the resulting trajectories exhibit substantial alterations. Setting the vehicle's speed to zero results in a stationary prediction, while a speed of 100 m/s leads to the projection of an implausible trajectory. This indicates a disproportionate dependence of the model's decision-making process on the ego status, even though the perception module continues to provide accurate surrounding information.\\n\\nFigure 5. Introducing ego status in the BEV-Planner++ enables the model to converge very rapidly.\\n\\nand Collision rate observed with the integration of MapFormer in \u201cBEV-Planner+Map\u201d is not due to the pretrained weights. We posit that in most straight-driving scenarios, the addition of lane information may not yield markedly effective information and could indeed introduce some degree of interference. To verify our hypothesis, we evaluated the performance of these methods under varying driving commands. As shown in Tab. 4 and Tab. 5, adding map information significantly increases the L2 distance error and collision rate with going straight commands. In contrast, for turning scenarios, the incorporation of map information effectively reduces the collision rate. Based on the above observations, we can tentatively draw the following conclusions:\\n\\n\u2022 In simple straightforward driving scenarios, the addition of perceptual information does not appear to enhance the model's performance with respect to L2 distance and collision rate. Conversely, the implementation of more intricate multi-task learning paradigms may, in fact, lead to a decrease in the model's overall efficacy.\\n\\n\u2022 In more complex scenarios, such as turns, incorporating perceptual information can be beneficial for planning purposes. However, given the relatively small proportion (13%) of turning scenes in the existing evaluation datasets, the introduction of perceptual information tends to adversely affect the average performance metrics (L2 distance and collision rate) in the final analysis.\\n\\n\u2022 It is imperative to develop a more robust and representative evaluation dataset. The metrics derived from the current evaluation dataset are not entirely persuasive and fail to accurately reflect the true capabilities of the model.\"}"}
{"id": "CVPR-2024-1233", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"New metrics will bring new conclusions. The preceding methodology primarily centered around the L2 distance and collision rate metrics. Our discussion thus far has been largely concentrated on these two metrics. What we want to emphasize is that these two metrics, L2 distance and collision rate, only reflect a partial aspect of a model's planning capabilities. It is not advisable to assess the quality of a model based exclusively on these two metrics. In this paper, we introduce a new metric to evaluate the model's comprehension and adherence to the map: Curb Collision Rate (CCR). As shown in Tab. 1, we can observe that the GoStright strategy frequently intersects with road boundaries, which is in line with our expectations. In terms of this new metric, Ego-MLP performs worse than UniAD and VAD. Our method BEV-Planner, performs the worst on this metric because it does not use any map information. This suggests that relying on past metrics to judge the superiority of different open-loop methods is biased.\\n\\nBased on our proposed new metrics, we also found that the existing collision rate metric can be manipulated with post-processing. More specifically, within UniAD [13], a non-linear optimization module is employed to refine the trajectory predicted by the end-to-end model, ensuring that the anticipated path steers clear of the occupancy grid, thereby aiming to prevent collisions. However, this optimization, while significantly reducing the collision rates with other agents, inadvertently introduces additional safety risks. The absence of adequate constraints in its optimization process, such as the integration of map priors, markedly increases the risk of the optimized trajectory encroaching upon road boundaries, as shown in Tab. 6. In this paper, we report the results of UniAD without its post-processing by default.\\n\\nWhat we wish to underscore is that the primary intention behind proposing CCR is to illuminate the inadequacies within the existing evaluation systems. However, even with the incorporation of CCR, open-loop evaluation systems still encounter numerous challenges. We assert that the evaluation of open-loop autonomous driving systems necessitates a more diverse and stringent evaluation framework. This would enable a more accurate reflection of these systems' capabilities and limitations.\\n\\nWhat the baseline learned in its BEV? As shown in Fig. 5, with the influence of ego status, the models converge rapidly. Considering the challenge of generating valuable BEV features from visual images and comparing the convergence curve of BEV-Planner that does not use ego status, this further demonstrates that ego status information dominates the learning process. Since our baselines are solely supervised by ego trajectory, we are wondering what the model is learning from the images. As shown in Fig. 6, we observed a distinct phenomenon: in BEV-Planner++, the activation range of the feature map predominantly encompasses the immediate vicinity around the ego vehicle, frequently manifesting behind the vehicle itself. This pattern marks a significant deviation from the BEV-Planner's BEV features, which typically concentrate on the area ahead of the vehicle. We speculate that this is due to the introduction of ego status information, which negates the model's need to extract information from BEV features. Hence, the BEV-Planner++ method has almost not learned any effective information.\\n\\n5. Conclusion\\n\\nIn this paper, we present an in-depth analysis of the shortcomings inherent in current open-loop, end-to-end autonomous driving methods. Our objective is to contribute findings that will foster the progressive development of end-to-end autonomous driving.\\n\\nOur conclusions are summarized as follows:\\n\\n- The planning performance of existing open-loop autonomous driving models based on nuScenes is highly affected by ego status (velocity, acceleration, yaw angle). With ego status involved, the model's final predicted trajectories are basically dominated by it, resulting in a diminished use of sensory information.\\n\\n- Existing planning metrics fall short of fully capturing the true performance of models. The evaluation results of the model may vary significantly across different metrics. We advocate for the adoption of more diverse and comprehensive metrics to prevent models from achieving local optimality on specific metrics, which may lead to the neglect of other safety hazards.\\n\\n- Compared to pushing the state-of-the-art performance on the existing nuScenes dataset, we assert that the development of more appropriate datasets and metrics represents a more critical and urgent challenge to tackle.\\n\\nLimitation\\n\\nThere are trade-offs between different planning metrics. Designing an integrated evaluation system for open-loop evaluation presents a significant challenge. Although our baseline method excels in terms of L2 distance and collision rate, its performance is not exceptional in the CCR, primarily because our approach does not utilize any perception annotations, such as HD maps.\\n\\nAcknowledgments\\n\\nThanks to Bencheng Liao and Shaoyu Chen for providing helpful information and model weights of VAD. Tong Lu and Zhiqi Li are supported by the National Natural Science Foundation of China (Grant No. 62372223) and China Mobile Zijin Innovation Insititute (No. NR2310J7M). Zhiqi Li is also supported by the NVIDIA Graduate Fellowship Program.\"}"}
{"id": "CVPR-2024-1233", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Mariusz Bojarski, Davide Del Testa, Daniel Dworakowski, Bernhard Firner, Beat Flepp, Prasoon Goyal, Lawrence D. Jackel, Mathew Monfort, Urs M\u00fcller, Jiakai Zhang, et al. End to end learning for self-driving cars. arXiv preprint arXiv:1604.07316, 2016.\\n\\n[2] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Y. Baldan, Oscar Beijbom. nuscenes: A multi-modal dataset for autonomous driving. In CVPR, 2020.\\n\\n[3] Sergio Casas, Abbas Sadat, and Raquel Urtasun. Mp3: A unified model to map, perceive, predict and plan. In CVPR, 2021.\\n\\n[4] Li Chen, Chonghao Sima, Yang Li, Zehan Zheng, Jiajie Xu, Xiangwei Geng, Hongyang Li, Conghui He, Jianping Shi, Yu Qiao, et al. Persformer: 3d lane detection via perspective transformer and the openlane benchmark. In European Conference on Computer Vision, pages 550\u2013567. Springer, 2022.\\n\\n[5] Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, and Hongyang Li. End-to-end autonomous driving: Challenges and frontiers. arXiv preprint arXiv:2306.16927, 2023.\\n\\n[6] Laurene Claussmann, Marc Revilloud, Dominique Gruyer, and S\u00e9bastien Glaser. A review of motion planning for highway autonomous driving. IEEE Transactions on Intelligent Transportation Systems, 21(5):1826\u20131848, 2019.\\n\\n[7] Dmitri Dolgov, Sebastian Thrun, Michael Montemerlo, and James Diebel. Practical search techniques in path planning for autonomous driving. Ann Arbor, 1001(48105):18\u201380, 2008.\\n\\n[8] Alexey Dosovitskiy, German Ros, Felipe Codevilla, Antonio Lopez, and Vladlen Koltun. Carla: An open urban driving simulator. 2017.\\n\\n[9] Junru Gu, Chenxu Hu, Tianyuan Zhang, Xuanyao Chen, Yilun Wang, Yue Wang, and Hang Zhao. Vip3d: End-to-end visual trajectory prediction via 3d agent queries. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5496\u20135506, 2023.\\n\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\n[11] Peiyun Hu, Aaron Huang, John Dolan, David Held, and Deva Ramanan. Safe local motion planning with self-supervised freespace forecasting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12732\u201312741, 2021.\\n\\n[12] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, and Dacheng Tao. St-p3: End-to-end vision-based autonomous driving via spatial-temporal feature learning. In European Conference on Computer Vision, pages 533\u2013549. Springer, 2022.\\n\\n[13] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17853\u201317862, 2023.\\n\\n[14] Junjie Huang and Guan Huang. BEVDet4D: Exploit temporal cues in multi-camera 3d object detection. arXiv preprint arXiv:2203.17054, 2022.\\n\\n[15] Linyan Huang, Zhiqi Li, Chonghao Sima, Wenhai Wang, Jingdong Wang, Yu Qiao, and Hongyang Li. Leveraging vision-centric multi-modal expertise for 3d object detection. arXiv preprint arXiv:2310.15670, 2023.\\n\\n[16] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. arXiv preprint arXiv:2303.12077, 2023.\\n\\n[17] Tarasha Khurana, Peiyun Hu, Achal Dave, Jason Ziglar, David Held, and Deva Ramanan. Differentiable raycasting for self-supervised occupancy forecasting. In European Conference on Computer Vision, pages 353\u2013369. Springer, 2022.\\n\\n[18] Hongyang Li, Chonghao Sima, Jifeng Dai, Wenhai Wang, Lewei Lu, Huijie Wang, Enze Xie, Zhiqi Li, Hanming Deng, Hao Tian, et al. Delving into the devils of bird's-eye-view perception: A review, evaluation and recipe. arXiv preprint arXiv:2209.05324, 2022.\\n\\n[19] Quanyi Li, Zhenghao Peng, Lan Feng, Qihang Zhang, Zhenghai Xue, and Bolei Zhou. Metadrive: Composing diverse driving scenarios for generalizable reinforcement learning. IEEE transactions on pattern analysis and machine intelligence, 45(3):3461\u20133475, 2022.\\n\\n[20] Qi Li, Yue Wang, Yilun Wang, and Hang Zhao. Hdmapnet: An online hd map construction and evaluation framework. 2022.\\n\\n[21] Yinhao Li, Zheng Ge, Guanyi Yu, Jinrong Yang, Zengran Wang, Yukang Shi, Jianjian Sun, and Zeming Li. BEVDepth: Acquisition of reliable depth for multi-view 3d object detection. arXiv preprint arXiv:2206.10092, 2022.\\n\\n[22] Zhiqi Li, Wenhai Wang, Hongyang Li, Enze Xie, Chonghao Sima, Tong Lu, Qiao Yu, and Jifeng Dai. BEVFormer: Learning bird's-eye-view representation from multi-camera images via spatiotemporal transformers. arXiv preprint arXiv:2203.17270, 2022.\\n\\n[23] Zhiqi Li, Zhiding Yu, David Austin, Mingsheng Fang, Shiyi Lan, Jan Kautz, and Jose M Alvarez. Fb-occ: 3d occupancy prediction based on forward-backward view transformation. arXiv preprint arXiv:2307.01492, 2023.\\n\\n[24] Zhiqi Li, Zhiding Yu, Wenhai Wang, Anima Anandkumar, Tong Lu, and Jose M Alvarez. Fb-bev: Bev representation from forward-backward view transformations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6919\u20136928, 2023.\\n\\n[25] Bencheng Liao, Shaoyu Chen, Xinggang Wang, Tianheng Cheng, Qian Zhang, Wenyu Liu, and Chang Huang. Maptr: Structured modeling and learning for online vectorized hd map construction. arXiv preprint arXiv:2208.14437, 2022.\\n\\n[26] Xuewu Lin, Tianwei Lin, Zixiang Pei, Lichao Huang, and Zhizhong Su. Sparse4d v2: Recurrent temporal fusion with sparse model. arXiv preprint arXiv:2305.14018, 2023.\"}"}
{"id": "CVPR-2024-1233", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Haisong Liu, Yao Teng, Tao Lu, Haiguang Wang, and Limin Wang. Sparsebev: High-performance sparse 3D object detection from multi-camera videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 18580\u201318590, 2023.\\n\\nYingfei Liu, Junjie Yan, Fan Jia, Shuailin Li, Qi Gao, Tiancai Wang, Xiangyu Zhang, and Jian Sun. PETRv2: A unified framework for 3D perception from multi-camera images. arXiv preprint arXiv:2206.01256, 2022.\\n\\nYicheng Liu, Tianyuan Yuan, Yue Wang, Yilun Wang, and Hang Zhao. Vectormapnet: End-to-end vectorized HD map learning. In International Conference on Machine Learning, pages 22352\u201322369. PMLR, 2023.\\n\\nJinhyung Park, Chenfeng Xu, Shijia Yang, Kurt Keutzer, Kris Kitani, Masayoshi Tomizuka, and Wei Zhan. Time will tell: New outlooks and a baseline for temporal multi-view 3D object detection. arXiv preprint arXiv:2210.02443, 2022.\\n\\nJonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3D. In ECCV, 2020.\\n\\nAditya Prakash, Kashyap Chitta, and Andreas Geiger. Multi-modal fusion transformer for end-to-end autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7077\u20137087, 2021.\\n\\n Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan, and Raquel Urtasun. Perceive, predict, and plan: Safe motion planning through interpretable semantic representations. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XXIII, pages 414\u2013430. Springer, 2020.\\n\\nHao Shao, Letian Wang, Ruobing Chen, Hongsheng Li, and Yu Liu. Safety-enhanced autonomous driving using interpretable sensor fusion transformer. In Conference on Robot Learning, pages 726\u2013737. PMLR, 2023.\\n\\nArdi Tampuu, Tambet Matiisen, Maksym Semikin, Dmytro Fishman, and Naveed Muhammad. A survey of end-to-end driving: Architectures and training methods. IEEE Transactions on Neural Networks and Learning Systems, 33(4):1364\u20131384, 2020.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\\n\\nLi Wang, Xinyu Zhang, Baowei Xv, Jinzhao Zhang, Rong Fu, Xiaoyu Wang, Lei Zhu, Haibing Ren, Pingping Lu, Jun Li, et al. Interfusion: Interaction-based 4D radar and lidar fusion for 3D object detection. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 12247\u201312253. IEEE, 2022.\\n\\nRuihao Wang, Jian Qin, Kaiying Li, Yaochen Li, Dong Cao, and Jintao Xu. BEV-Lanedet: An efficient 3D lane detection based on virtual camera via key-points. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1002\u20131011, 2023.\\n\\nShihao Wang, Yingfei Liu, Tiancai Wang, Ying Li, and Xiangyu Zhang. Exploring object-centric temporal modeling for efficient multi-view 3D object detection. arXiv preprint arXiv:2303.11926, 2023.\\n\\nPenghao Wu, Xiaosong Jia, Li Chen, Junchi Yan, Hongyang Li, and Yu Qiao. Trajectory-guided control prediction for end-to-end autonomous driving: A simple yet strong baseline. Advances in Neural Information Processing Systems, 35:6119\u20136132, 2022.\\n\\nEnze Xie, Zhiding Yu, Daquan Zhou, Jonah Philion, Anima Anandkumar, Sanja Fidler, Ping Luo, and Jose M Alvarez. M\u00b2BEV: Multi-camera joint 3D detection and segmentation with unified birds-eye view representation. arXiv preprint arXiv:2204.05088, 2022.\\n\\nChenyu Yang, Yuntao Chen, Hao Tian, Chenxin Tao, Xizhou Zhu, Zhaoxiang Zhang, Gao Huang, Hongyang Li, Yu Qiao, Lewei Lu, et al. BEVFormer v2: Adapting modern image backbones to bird's-eye-view recognition via perspective supervision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17830\u201317839, 2023.\\n\\nTengju Ye, Wei Jing, Chunyong Hu, Shikun Huang, Lingping Gao, Fangzhen Li, Jingke Wang, Ke Guo, Wencong Xiao, Weibo Mao, et al. FusionAD: Multi-modality fusion for prediction and planning tasks of autonomous driving. arXiv preprint arXiv:2308.01006, 2023.\\n\\nJiang-Tian Zhai, Ze Feng, Jinhao Du, Yongqiang Mao, Jiang-Jiang Liu, Zichang Tan, Yifu Zhang, Xiaoqing Ye, and Jingdong Wang. Rethinking the open-loop evaluation of end-to-end autonomous driving in nuScenes. arXiv preprint arXiv:2305.10430, 2023.\"}"}
{"id": "CVPR-2024-1233", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Is Ego Status All You Need for Open-Loop End-to-End Autonomous Driving?\\n\\nZhiqi Li\\\\textsuperscript{1},\\\\textsuperscript{*} Zhiding Yu\\\\textsuperscript{2,\u2020} Shiyi Lan\\\\textsuperscript{2}, Jiahan Li\\\\textsuperscript{1}, Jan Kautz\\\\textsuperscript{2}, Tong Lu\\\\textsuperscript{1}, Jose M. Alvarez\\\\textsuperscript{2}\\n\\n1 National Key Lab for Novel Software Technology, Nanjing University\\n2 NVIDIA\\n\\nAbstract\\nEnd-to-end autonomous driving recently emerged as a promising research direction to target autonomy from a full-stack perspective. Along this line, many of the latest works follow an open-loop evaluation setting on nuScenes to study the planning behavior. In this paper, we delve deeper into the problem by conducting thorough analyses and demystifying more devils in the details. We initially observed that the nuScenes dataset, characterized by relatively simple driving scenarios, leads to an under-utilization of perception information in end-to-end models incorporating ego status, such as the ego vehicle\u2019s velocity. These models tend to rely predominantly on the ego vehicle\u2019s status for future path planning. Beyond the limitations of the dataset, we also note that current metrics do not comprehensively assess the planning quality, leading to potentially biased conclusions drawn from existing benchmarks. To address this issue, we introduce a new metric to evaluate whether the predicted trajectories adhere to the road. We further propose a simple baseline able to achieve competitive results without relying on perception annotations.\\n\\nGiven the current limitations on the benchmark and metrics, we suggest the community reassess relevant prevailing research and be cautious about whether the continued pursuit of state-of-the-art would yield convincing and universal conclusions. Code and models are available at https://github.com/NVlabs/BEV-Planner.\\n\\n1. Introduction\\nEnd-to-end autonomous driving aims to jointly consider perception and planning in a full-stack manner [1, 5, 32, 35]. An underlying motivation is to evaluate autonomous vehicle (AV) perception as a means to an end (planning), instead of overfitting to certain perception metrics. Unlike perception, the planning is generally much more open-ended and hard to quantify [6, 7]. This open-ended nature of planning would ideally favor a closed-loop evaluation setting where other agents could react to the behavior of the ego vehicle, and the raw sensor data could also change accordingly. However, both agent behavior modeling and real-world data simulation within closed-loop simulators [8, 19] remain challenging open problems to date.\\n\\nAs such, closed-loop evaluation inevitably introduces considerable domain gaps to the real world.\\n\\nOpen-loop evaluation, on the other hand, aims to treat human driving as the ground truth and formulate planning as imitation learning [13]. Such formulation allows the readily usage of real-world datasets via simple log-replay, avoiding the domain gaps from simulation. It also offers other advantages, such as the capacity to train and validate models in complex and diverse traffic scenarios, which are often difficult to generate with high fidelity in simulations [5]. For these benefits, a well-established body of research has been developed.\"}"}
{"id": "CVPR-2024-1233", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"search focuses on open-loop end-to-end autonomous driving with real-world dataset [2, 12, 13, 16, 43].\\n\\nCurrent prevailing end-to-end autonomous driving methods [12, 13, 16, 43] commonly use nuScenes [2] for open-loop evaluation of their planning behavior. For instance, UniAD [13] studies the influence of different perception task modules to the final planning behavior. However, AD-MLP [45] recently points out that a simple MLP network can also achieve state-of-the-art planning results, relying solely on the ego status information. This motivates us to ask an important question: Is ego status all you need for open-loop end-to-end autonomous driving?\\n\\nOur answer is yes and no, considering both the pros and cons of using ego status in current benchmarks:\\n\\nYes. Information such as velocity, acceleration and yaw angle in the ego status should apparently benefit the planning task. To verify this, we fix an open issue of AD-MLP and remove the use of history trajectory ground truths (GTs) to prevent potential label leakage. Our reproduced model, Ego-MLP (Fig. 1 a.2), relies solely on the ego status and is on par with state-of-the-art methods in terms of existing L2 distance and collision rate metrics. Another observation is that only existing methods [13, 16, 43], which incorporate ego status information within the planner module, can obtain results on par with Ego-MLP. Although these methods employ additional perception information (tracking, HD map, etc.), they don't demonstrate superiority compared to Ego-MLP. These observations verify the dominating role of ego status in the open-loop evaluation of end-to-end autonomous driving.\\n\\nAnd No. It is also evident that autonomous driving as a safety-critical application should not depend solely on ego status for decision-making. So why does this phenomenon occur where using only ego status can achieve state-of-the-art planning results? To address the question, we present a comprehensive set of analyses covering existing open-loop end-to-end autonomous driving methods. We identify major shortcomings within existing research, including aspects related to datasets, evaluation metrics, and specific model implementations. We itemize and detail these shortcomings in the rest of the section:\\n\\nImbalanced dataset. NuScenes is a commonly used benchmark for open-loop evaluation tasks [11\u201313, 16, 17, 43]. However, our analysis shows that 73.9% of the nuScenes data involve scenarios of driving straightforwardly, as reflected by the distribution of the trajectory in Fig. 2. For these straight-driving scenarios, maintaining the current velocity, direction, or turning rate can be sufficient most of the time. Hence, ego status information can be easily leveraged as a shortcut to fit the planning task, leading to the strong performance of Ego-MLP on nuScenes.\\n\\nExisting metrics are not comprehensive. The remaining 26.1% of nuScenes data involve more challenging driving scenarios for potentially better benchmarks of planning behaviors. However, we argue that the widely used current metrics, such as the L2 distance between prediction and planning GT and the collision rates between ego vehicle and surrounding obstacles, fail to accurately measure the model's planning behavior quality. Through visualizing numerous predicted trajectories generated from various methods, we note that some highly risky trajectories, such as running off the road may not get severely penalized in existing metrics. In response to this issue, we introduce a new metric to calculate the interaction rate between the predicted trajectories and the road boundaries. While focusing on intersection rates with road boundaries, the benchmark will experience a substantial transformation. In terms of this new metric, Ego-MLP tends to predict trajectories that deviate from the road more frequently than UniAD.\\n\\nEgo status bias against driving logic. With ego status being a potential source causing overfitting, we further observe an interesting phenomenon. Our experiment results suggest that, in some cases, completely removing visual input from an existing end-to-end autonomous driving framework does not significantly degrade the planning behavior. This contradicts the basic driving logic in the sense that perception is expected to provide useful information for planning. For instance, blanking all the camera input in VAD [16] leads to complete failure of the perception module but minor degrade in planning, when ego status is present. However, altering the input ego velocity can significantly influence the final predicted trajectory. In conclusion, we conjecture that recent efforts in end-to-end autonomous driving and their state-of-the-art scores on nuScenes are likely to be caused by the over-reliance on ego status, coupled with the dominance of simple driving scenarios. Furthermore, current metrics fall short in comprehensively assessing the quality of model-predicted trajectories.\"}"}
{"id": "CVPR-2024-1233", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"open issues and shortcomings may have under-\\nrepresented the potential complexity of the planning task and\\ncreated a misleading impression that ego status is all you\\nneed for open-loop end-to-end autonomous driving.\\n\\nThe potential interference of ego status in current open-\\nloop end-to-end autonomous driving research raises another\\nquestion: Is it possible to negate this influence by removing\\nego status from the whole model? However, it's important\\nto note that even excluding the impact of ego status,\\nthe reliability of open-loop autonomous driving research based\\non the nuScenes dataset remains in question.\\n\\n2. Related Work\\n\\n2.1. BEV perception\\nIn recent years, BEV-based autonomous driving perception\\nmethods have made great progress. Lift-Splat-Shoot [31]\\nfirstly propose to use latent depth distribution to per-\\nform view transformation. BEVFormer [22] introduces\\ntemporal clues into BEV perception and greatly boosts\\nthe 3D detection performance. A series of subsequent\\nworks [14, 15, 21, 23, 24, 26\u201328, 30, 41, 42] obtain more\\naccurate 3D perception results by obtaining more accurate\\ndepth information or making better use of temporal infor-\\nmation. The incorporation of temporal information typi-\\ncally necessitates the alignment of features across different\\ntimesteps [14, 18, 22, 39]. In the alignment process, the\\nego status is either implicitly encoded within the input fea-\\nture [39] or is explicitly used to translate BEV features [14].\\nMethods [4, 20, 25, 29, 38, 44] explored map perception\\nbased on BEV features.\\n\\n2.2. End-to-end autonomous driving\\nModern autonomous driving systems are usually divided\\ninto three main tasks: perception, prediction, and plan-\\nning. End-to-end autonomous driving that directs learn-\\ning from raw sensor data to planning trajectories or driv-\\ning commands eliminates the need for manual feature ex-\\ntraction, leading to efficient data utilization and adaptabil-\\nity to diverse driving scenarios. There exists a body of re-\\nsearch [34, 37, 40] focused on closed-loop end-to-end driv-\\ning within simulators [8, 19]. However, a domain gap per-\\nsists between the simulator environment and the real world,\\nparticularly concerning sensor data and the motion status of\\nagents. Recently, open-loop end-to-end autonomous driv-\\ning has attracted more attention. End-to-end autonomous\\ndriving methods [3, 9, 13, 16, 33, 43] that involve learning\\nintermediate tasks claim their effectiveness in improving fi-\\nal planning performance. AD-MLP [45] pointed out the\\nissue of imbalanced data distribution in nuScenes and at-\\ntempted to use only ego status as the model input to achieve\\narts performance. However, AD-MLP benefits from utiliz-\\ning the historical trajectory of the ego car as input. Given\\nthat none of the existing methods use the historical trajec-\\ntory information of the ego car, we argue that using the\\nhistorical trajectory in open-loop autonomous driving is a\\nsubject of debate, as the model itself does not generate this\\nhistorical trajectory but rather by an actual human driver.\\n\\n3. Proposed BEV-Planner\\nIn fact, ST-P3 [12], a previous method that often serves as\\na baseline, uses partially incorrect GT data during training\\nand evaluation\\n\\nConsequently, when conducting compar-\\nisons between other methods and ST-P3, the validity of the\\nconclusions drawn must be carefully evaluated. Therefore,\\nin this paper, it is necessary for us to redesign a baseline\\nmethod to compare with existing methods. At the same\\ntime, to better explore the impact of ego status, we also need\\na relatively clear baseline method. Based on these consid-\\nerations, we have designed a very simple baseline in this\\npaper, named BEV-Planner, as shown in the Fig. 1(c). For\\nour pipeline, we first generate the BEV feature and concate-\\nnate it with history BEV features, mainly following the pre-\\nvious method [12, 14, 21]. Please note while concatenating\\nBEV features from different timesteps, we didn't perform\\nfeature alignment. After obtaining the BEV features, we di-\\nrectly perform a cross-attention [36] between the BEV fea-\\ntures and the ego query, which is a learnable embedding.\\nThe final trajectories are predicted based on the refined ego\\nquery through MLPs. The process can be formulated as fol-\\nlows:\\n\\n\\\\[\\n\\\\tau = \\\\text{MLP}(\\\\text{attn}(q = Q, k = B, v = B)),\\n\\\\]\\n\\nwhere \\\\(Q\\\\) is the ego query, \\\\(B\\\\) is the BEV features after tem-\\nporal fusion. \\\\(\\\\tau\\\\) is the final predicted trajectories.\\n\\nTo align with existing methods, we also designed base-\\nline approaches that incorporate ego status into the BEV or\\nplanner modules. The strategy of incorporating ego status\\ninto the BEV aligns with previous approaches [13, 16, 22].\\nThe strategy of incorporating ego status in the planner is di-\\nrectly concatenating the ego query with a vector containing\\nego status.\\n\\nCompared to existing methods, this simple method\\ndidn't require any human-labeled data, including bounding\\nboxes, tracking IDs, HD maps, etc. For this proposed base-\\nlines, we only use one L1 loss for trajectory supervision. We\\nwish to underscore that our proposed baseline method is not\\nintended for real-world deployment, owing to its deficien-\\ncies in providing adequate constraints and interoperability.\\n\\n4. Experiments\\n\\n4.1. Implementation Details\\nOur baseline uses an R50 backbone [10]. The input reso-\\nlution is \\\\(256 \\\\times 704\\\\), smaller than existing methods [13, 16].\\n\\n\\\\[\\\\text{https://github.com/OpenDriveLab/ST-P3/issues/24}\\\\]\"}"}
{"id": "CVPR-2024-1233", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The BEV resolution is 128 \u00d7 128 with a perception range of around 50 meters. For the baseline that uses history BEV features, we directly concatenate the BEV features from the past 4 timesteps to current BEV features along the channel dimension without alignment. A BEV encoder from method [14] is further used to squeeze the channel dimension to 256. We train our model for 12 epochs on 8 V100 GPUs, with a batch size of 32 and a learning rate of 1e-4.\\n\\n4.2. Metrics\\nIn the Appendix, we will introduce the shortcomings of the currently commonly used collision rate and another metric used to evaluate the smoothness of predicted trajectory.\\n\\nCurb Collision Rate (CCR). In this study, to more comprehensively assess the quality of predicted trajectories, we employed a new metric that calculates the collision rate between the predicted trajectories and curbs (road boundaries). Staying on the road is vital for the safety of autonomous driving systems, yet existing evaluation metrics overlook the integration of map priors. Intuitively, safe trajectories should avoid collision with curbs. Our Curb Collision Rate (CCR) typically indicate the possibility of leaving the drivable area, which can pose safety hazards. We recognize that certain annotated road boundaries on nuScenes are indeed traversable, and ground truth trajectories may intersect with these boundaries under specific conditions. However, from a statistical viewpoint, this metric can effectively represent the overall rationality of the model's predicted trajectories. The implementation of the CCR metric is informed by the collision rate. To facilitate this, we rasterize the road boundary using a resolution of 0.1 meters. More details are in the Appendix.\\n\\n4.3. Discussion\\nEgo status plays a key role. While focusing solely on previous metrics L2 distance and collision rate, it is observed that the simple strategy (ID-7), which simply continues straight at the current velocity, achieves surprisingly good results. The Ego-MLP model, which didn't leverage perception clues, is actually on par with UniAD and VAD, which use more complex pipelines. From another perspective, it is observed that existing methods can only match the performance of Ego-MLP when the ego vehicle's status is incorporated into the planner. In contrast, reliance solely on camera inputs leads to results significantly inferior to those achieved by Ego-MLP. Considering these observations, we may tentatively infer an intriguing conclusion: utilizing a combination of sensory information and ego status appears to yield results comparable to those achieved by employing ego status alone. Therefore, in models that integrate both ego vehicle status and perception information, a pertinent question arises: What specific role does the perception information, acquired from camera inputs, play within the final planning module?\\n\\nEgo Status vs. Perceptual Information\\nUndoubtedly, perceptual information constitutes the indispensable foundation of all autonomous driving systems, with ego status additionally offering crucial data such as the vehicle's velocity and acceleration to aid the system's decision-making process. Incorporating both perceptual information and ego status for the ultimate planning should indeed be a judicious strategy within an end-to-end autonomous driving system. However, as shown in Tab. 1, relying solely on ego status can yield planning results that are on par with or even superior to those methods that utilize both ego status and perception modules on previous L2 or collision rate metrics. To ascertain the roles that perceptual information and ego status play in the final planning process, we introduced varying degrees of perturbation to the images and the ego status, as shown in the Tab. 2. We use the official VAD model (which leverages ego status in the planner module) as the base model. It is observable that when disturbances are added to the images, the results of planning marginally decrease and may even exhibit improvement, while perceptual performance significantly deteriorates. Surprisingly, even when blank images are used as input, leading to the complete breakdown of the perception module, the model's planning capabilities remain largely unaffected. The corresponding visualization results are as illustrated in the Fig. 3. In contrast to the model's remarkable robustness to variations in image inputs, it exhibits considerable sensitivity to ego status. Upon altering the velocity of the ego car, we can observe that the planning results of the model are significantly impacted, as shown in Fig. 4. Setting the ego car's speed to 100 m/s results in the model generating wildly impractical planning trajectories. We posit that an autonomous driving system displaying such heightened sensitivity to ego status information harbors considerable safety risks. Furthermore, with planning results being predominantly dictated by ego status, the functions of other modules in the model cannot be reflected. For example, while comparing VAD(ID-6) and BEV-Planner++ (ID-12), we can observe that they...\"}"}
