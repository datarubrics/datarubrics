{"id": "CVPR-2023-1233", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nWe develop a novel method to remove injected backdoors in deep learning models. It works by cloning the benign behaviors of a trojaned model to a new model of the same structure. It trains the clone model from scratch on a very small subset of samples and aims to minimize a cloning loss that denotes the differences between the activations of important neurons across the two models. The set of important neurons varies for each input, depending on their magnitude of activations and their impact on the classification result. We theoretically show our method can better recover benign functions of the backdoor model. Meanwhile, we prove our method can be more effective in removing backdoors compared with fine-tuning. Our experiments show that our technique can effectively remove nine different types of backdoors with minor benign accuracy degradation, outperforming the state-of-the-art backdoor removal techniques that are based on fine-tuning, knowledge distillation, and neuron pruning.\\n\\n1. Introduction\\nBackdoor attack is a prominent threat to applications of Deep Learning models. Misbehaviors, e.g., model misclassification, can be induced by an input stamped with a backdoor trigger, such as a patch with a solid color or a pattern [14, 29]. A large number of various backdoor attacks have been proposed, targeting different modalities and featuring different attack methods [2, 22, 37, 44, 49, 55]. An important defense method is hence to remove backdoors from pre-trained models. For instance, Fine-prune proposed to remove backdoor by fine-tuning a pre-trained model on clean inputs [27]. Distillation [23] removes neurons that are not critical for benign functionalities. Model connectivity repair (MCR) [56] interpolates weight parameters of a poisoned model and its finetuned version. ANP [50] adversarially perturbs neuron weights of a poisoned model and prunes those neurons that are sensitive to perturbations (and hence considered compromised). While these techniques are very effective in their targeted scenarios, they may fall short in some other attacks. For example, if a trojaned model is substantially robust, fine-tuning may not be able to remove the backdoor without lengthy retraining. Distillation may become less effective if a neuron is important for both normal functionalities and backdoor behaviors. And pruning neurons may degrade model benign accuracy. More discussions of related work can be found in Section 2.\\n\\nIn this paper, we propose a novel backdoor removal technique MEDIC as illustrated in Figure 1 (A). It works by cloning the benign functionalities of a pre-trained model to a new sanitized model of the same structure. Given a trojaned model and a small set of clean samples, MEDIC trains the clone model from scratch. The training is not only driven by the cross-entropy loss as in normal model training, but also by forcing the clone model to generate the same internal activation values as the original model at the corresponding internal neurons, i.e., steps 1\u20dd, 2\u20dd, and 4\u20dd in Figure 1 (A). Intuitively, one can consider it essentially derives the weight parameters in the clone model by resolving the activation equivalence constraints. There are a large number of such constraints even with just a small set of clean samples. However, such faithful cloning likely copies the backdoor behaviors as well as it tends to generate the same set of weight values. Therefore, our cloning is further guided by importance (step 3\u20dd). Specifically, for each sample $x$, we only force the important neurons to have the same activation values. A neuron is considered important if (1) it tends to be substantially activated by the sample, when compared to its activation statistics over the entire population (the activation criterion in red in Figure 1 (A)), and (2) the large activation value has substantial impact on the classification result (the impact criterion). The latter can be determined by analyzing the output gradient regarding the neuron activation. By constraining only the important neurons and relaxing the constraints for other neurons, we can minimize the cloning loss and achieve a better model that removes the backdoor. \\n\\n2 MEDIC\\nMEDIC stands for \u201cRemove Model Backdoors via Importance Driven Cloning.\u201d\"}"}
{"id": "CVPR-2023-1233", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The model focuses on cloning the behaviors related to the clean inputs and precludes the backdoor. The set of weight values derived by such cloning are largely different from those in the original model. Intuitively, it is like solving the same set of variables with only a subset of equivalence constraints. The solution tends to differ substantially from that by solving the full set of constraints.\\n\\n**Example.** Figure 2 shows an example by visualizing the internal activations and importance values for a trojaned model on a public benchmark [35]. Image (a) shows a right-turn traffic sign stamped with a polygon trigger in yellow, which induces the classification result of stop sign. Image (c) visualizes the activation values for the trojaned image whereas (d) shows those for its clean version. The differences between the two, visualized in (e), show that the neurons fall into the red box are critical to the backdoor behaviors. A faithful cloning method would copy the behaviors for these neurons (and hence the backdoor). In contrast, our method modulates the cloning process using importance values and hence precludes the backdoor. The last image shows the importance values with the bright points denoting important neurons and the dark ones unimportant. Observe that it prevents copying the behaviors of the compromised neurons for this example as they are unimportant. One may notice that the unimportant compromised neurons for this example may become important for another example. Our method naturally handles this using per-sample importance values.\\n\\nOur technique is different from fine-tuning as it trains from scratch. It is also different from knowledge distillation [25] shown in Figure 1 (B), which aims to copy behaviors across different model structures and constrains logits equivalence (and sometimes internal activation equivalence [53] as well). In contrast, by using the same model structure, we have a one-to-one mapping for individual neurons in the two models such that we can enforce strong constraints on internal equivalence. This allows us to copy behaviors with only a small set of clean samples.\\n\\nWe evaluate our technique on nine different kinds of backdoor attacks. We compare with five latest backdoor removal methods (details in related work). The results show our method can reduce the attack success rate to 8.5% on average with only 2% benign accuracy degradation. It consistently outperforms the other baselines by 25%. It usually takes 15 mins to sanitize a model. We have also conducted an adaptive attack in which the trigger is composed of existing benign features in clean samples. It denotes the most adversary context for MEDIC. Our ablation study shows all the design choices are critical. For example, without using importance values, it can only reduce ASR to 36% on average.\\n\\nIn summary, our main contributions include the following.\\n\\n\u2022 We propose a novel importance driven cloning method to remove backdoors. It only requires a small set of clean samples.\\n\u2022 We theoretically analyze the advantages of the cloning method.\\n\u2022 We empirically show that MEDIC outperforms the state-of-the-art methods.\\n\\n2. Related Work\\n\\nThere are a large body of backdoor attacks [1, 7, 8, 15, 33, 34, 38\u201340, 57, 58]. We focus on a number of representative ones with different natures and used in our experiments. Readers interested in other attacks are referred to a survey [12, 24].\\n\\nBadnet [14] proposes to inject backdoors with a fixed polygon. Clean Label attack [46] first adds adversarial perturbations to target-class samples and makes them misclassified. It then adds a square patch on those inputs without changing their ground-truth label. During inference, the square patch can induce misclassification on non-target class samples. SIG [3] uses a wave-like pattern as trigger. Reflection attack [30] adds the reflection of some external image on the input. Polygon attack [35] injects a polygon-like trigger on the input at specific locations. Filter attack [28] utilizes Instagram filters to transform inputs and labels the transformed inputs to the target class. Warp [32] uses image warping as a trigger pattern. The trigger perturbations hence vary for different inputs. We also consider an adaptive attack which uses benign features from two existing classes as the trigger [26].\\n\\nResearchers have proposed various defense techniques, including backdoor inputs detection that aims to detect and reject input samples with triggers [9, 13, 45, 47]; backdoor scanning that determines whether a given model has backdoor using a small set of clean inputs [20, 28, 42, 48]; backdoor certification that certifies the predictions of individual samples with trigger [19, 31, 51, 52]; model hardening that adversarially trains models using triggers inverted from the subject (clean) model [43]; backdoor removal that erases injected backdoors using a small set of clean samples [23, 27, 50]. Our work falls in the last category. Fineprune [27] iteratively prunes neurons with small activation values on clean samples and then finetunes the pruned model. Note that the pruned model has different structure from the original one. In our experience, having the same structure is critical as it provides the optimal neuron alignment. Model connectivity repair (MCR) [56] interpolates parameters of a poisoned model and its finetuned version. NAD [23] first finetunes a poisoned model and then performs transfer learning, treating the finetuned model as the teacher and the poisoned model as the student. Its effectiveness hinges on the quality of the first finetuning step. ANP [50] adversarially perturbs neuron weights of the poisoned model and prunes neurons sensitive...\"}"}
{"id": "CVPR-2023-1233", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Workflow of MEDIC and comparison with knowledge distillation [17]\\n\\n(a) Illustration of Cloning\\n(b) Benign\\n(c) Backdoor\\n(d) Benign\\n(e) $|c - d|$ (f) $\\\\mathbf{w}$\\n\\nTraffic Sign Neuron's Activation Difference Importance\\n\\nFigure 2. Example, with (a) and (b) the clean and trojaned inputs, (c)-(f) the internal activations of the trojaned input and the clean input, their differences, and the importance values in eq.(4) for the clean input, respectively. We resize and rescale the internal activation for better visualization.\\n\\nTo perturbations. We compare MEDIC with the aforementioned approaches in Section 5 and demonstrate that ours outperforms.\\n\\n3. Method\\n\\nProblem Statement. Given a multi-class backdoor classifier $f^* : X \\\\rightarrow Y$, where $X \\\\subset \\\\mathbb{R}^d$ and $Y \\\\subset \\\\mathbb{R}^C$, $d$ the input dimension and $C$ the number of classes. Samples $(X, Y)$ are drawn from the joint distribution $S = (X, Y)$. The classifier $f^*$ is originally trained on a large dataset $D \\\\sim (X, Y)_n$ and additional backdoor samples $D_a \\\\sim (X_a, Y_a)$. We aim to remove the backdoor behaviors in $f^*$ and retain the benign behaviors. To do so, we leverage a small additional dataset $D_s \\\\sim (X, Y)_q$ where $|D_s| \\\\ll |D|$. It is used to facilitate the cloning process. Function $f_c$ denotes the cloned and sanitized model using our method.\\n\\nOur first goal is to clone the (benign) functionalities of the model instead of its parameters. Specifically, $f_c$ is supposed to produce similar outputs as the original model (with backdoor) on clean inputs. This is stated as the functionality goal as follows.\\n\\n$$\\\\mathbb{E}_{x \\\\sim X} \\\\| f_c(x) - f^*(x) \\\\|_2 \\\\approx 0$$\\n\\nIn addition, $f_c$ shall not have the backdoor behavior on samples from the backdoor distribution $(X_a, Y_a)$, with a high probability $1 - \\\\delta$. It is stated as the sanity goal.\\n\\n$$\\\\mathbb{E}_{(x, y) \\\\sim (X_a, Y_a)} 1_{f_c(x) \\\\neq y} \\\\geq 1 - \\\\delta$$\\n\\nIt is worth noting that similar functionalities do not imply a similar set of parameters. In fact, our importance driven cloning derives a set of parameters different from those in the original model to meet the two stated goals.\\n\\nCloning. To achieve the first goal of copying normal functionalities using only a small set of benign samples, we propose to train a clone model from scratch that has the same structure, by forcing the clone model to have the same activation values for all the corresponding neurons and also the output logits as the original model. Such cloning is different from transfer learning, in which teacher and student models may be of different structures as it is not that meaningful to copy functionalities to a model with the same structure in transfer learning's application scenarios.\\n\\nWhile inputs often have a fixed bound, internal activation values have dynamic ranges. If we directly use activation value differences in the clone training loss, such range variations cause difficulties in convergence. Therefore, we normalize activation values before using them in the loss function. Formally speaking, given $m$ internal neurons of a backdoored model, denoted by functions $f^*_i : \\\\mathbb{R}^d \\\\rightarrow \\\\mathbb{R}$ with $i \\\\in \\\\{1, m\\\\}$ and the corresponding ones in the clone model $f_c_i$, we aim to enforce the corresponding neuron functions to produce the same values. Let $\\\\mu_i$ and $\\\\sigma_i$ be the mean and standard deviation of the random variable $f^*_i(X)$ and $\\\\mathbf{w}_i(x)$ an importance weight. We use $\\\\rightarrow \\\\mathbf{f}(x), \\\\rightarrow \\\\mathbf{w}(x), \\\\mathbf{\\\\sigma}$ and $\\\\mathbf{\\\\mu}$ to represent their concatenated vectors over the $m$ neurons.\\n\\nWe consider each element in a CNN feature map a separate neuron.\"}"}
{"id": "CVPR-2023-1233", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"respectively. We can derive the following loss function.\\n\\n\\\\[ L_{\\\\text{clone}} = \\\\mathbb{E}_{x \\\\sim X} [X_i w_i(x) f_i^*(x) - f_{\\\\text{cl}}(x) \\\\sigma_i^2] \\\\] (1)\\n\\nwhere \\\\( w_i(x) \\\\geq 0 \\\\).\\n\\nIntuitively, we minimize the differences between the outputs of the corresponding neurons. For the moment, one can assume \\\\( w_i(x) = 1/m \\\\), meaning that we enforce such constraints equally on all neurons. We will explain how we change \\\\( w_i(x) \\\\) to preclude backdoor behaviors later in the section.\\n\\nOur results in Figure 3 show that we can faithfully clone the normal functionalities with only 2% samples. We also formally analyze the essence of cloning in Section 4.\\n\\nPruning Backdoor by Importance Driven Cloning. To preclude backdoor while cloning, we leverage the importance value \\\\( w_i(x) \\\\) computed for each sample and each neuron. A neuron is important for a sample, if and only if the neuron is substantially activated (compared to its activation statistics over the entire population) and has a large impact on the final output.\\n\\nTo determine if a neuron is substantially activated, we calculate the absolute difference between its activation and the expected activation as the indicator in Eq.(2). While ideally we would just select the neuron with the largest magnitude, to ensure the loss function is differentiable, we use softmax as an alternative to the max operation. Here \\\\( \\\\tau \\\\) is the temperature used in the softmax function, which can be used to control the strictness of the operation, namely, a high temperature resembles the strict max operation (that has a one-hot output), whereas a lower temperature suggests smoother results.\\n\\n\\\\[ w_{\\\\text{activate}}(x) = \\\\text{SoftMax}(\\\\tau |\\\\rightarrow f(x) - \\\\hat{\\\\mu}|\\\\sigma) \\\\] (2)\\n\\nIt is worth noting that this formula essentially corresponds to the intrinsic normalization in many models (e.g., those with batch normalization [18]), where the statistics are readily accessible. When the subject model does not have any normalization layer, one can collect the statistics from the available samples. Details can be found in Appendix D.1.\\n\\nTo evaluate the impact of activations on classification results, we use the magnitude of gradients as an indicator. However, the gradients are not directly comparable at many layers, especially those that do not have normalization, because the varying magnitude of the activation values at those layers affects the gradients and makes direct comparison inaccurate. To tackle the problem, we introduce variables to denote normalized activations at all layers, called proxy variables, and derive gradients regarding those variables. The resulted gradients are hence comparable. Consider a proxy variable \\\\( h_i^*(x) \\\\), where the neuron \\\\( f_i^*(x) = h_i^*(x) \\\\cdot \\\\sigma_i \\\\). Let \\\\( l \\\\) be the loss function for classification based on \\\\( f_i^*(x) \\\\), we can thus calculate the gradient of proxy variable as follows.\\n\\n\\\\[ g_i^*(x) = \\\\frac{\\\\partial l}{\\\\partial h_i^*(x)} = \\\\frac{\\\\partial l}{\\\\partial f_i^*(x)} \\\\cdot \\\\frac{\\\\partial f_i^*(x)}{\\\\partial h_i^*(x)} = \\\\sigma_i^2 \\\\frac{\\\\partial l}{\\\\partial f_i^*(x)} \\\\]\\n\\nWe then select the neuron with the largest gradient, leveraging the aforementioned soft version of max operation. Let \\\\( \\\\rightarrow g(x) \\\\) be the concatenated vector of \\\\( g_i^*(x) \\\\).\\n\\n\\\\[ w_{\\\\text{impact}}(x) = \\\\text{SoftMax}(\\\\tau \\\\rightarrow g(x)) \\\\] (3)\\n\\nThe next step is to identify the neurons satisfying both Eq.(2) and (3). In a discrete world, we could perform a set intersection. However, our softmax operations do not select the largest weight values. Instead, they produce vectors denoting the importance of all neurons. Hence, we perform a soft version of set intersection, which is differentiable and hence usable during training. Assume \\\\( w_{\\\\text{activate}}(x), w_{\\\\text{impact}}(x) \\\\in [0, 1] \\\\) denote the importance values for a neuron \\\\( i \\\\), computed by the two respective softmax operations in Eq.(2) and (3), with 1 indicating \\\"Important\\\" and 0 \\\"Unimportant\\\". We can derive the neuron\u2019s overall importance as follows.\\n\\n\\\\[ w_i(x) = w_{\\\\text{activate}}(x) \\\\land w_{\\\\text{impact}}(x) = q w_{\\\\text{activate}}(x) \\\\cdot w_{\\\\text{impact}}(x) \\\\] (4)\\n\\nThe operation \\\\( \\\\land \\\\) is essentially a soft version of \\\"boolean conjunction\\\". For example, when \\\\( a = 1 \\\\) and \\\\( b = 1 \\\\), \\\\( a \\\\land b = 1 \\\\).\\n\\nThe square root ensures the output is in the same magnitude as the inputs.\\n\\nGiven the cloning loss, we combine it with the classification loss and train the model. The two losses are balanced through an additional parameter \\\\( \\\\lambda \\\\). Details and an ablation study on \\\\( \\\\lambda \\\\) can be found in Appendix E.3. A step-by-step algorithm can be found in Appendix D.1.\\n\\nDifference from Neuron Pruning based Methods. There are existing works [27, 50] that determine a subset of neurons compromised by poisoning and simply remove those neurons. However, they usually lead to non-trivial benign accuracy degradation (see Section 5). The reason is that a neuron in a compromised model may be important for the classifications of both benign and poisoned inputs. In contrast, our method does not statically decide if a neuron is compromised. If a neuron is important for both benign and poisoned samples, we (only) copy its behaviors for benign inputs.\"}"}
{"id": "CVPR-2023-1233", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Eugene Bagdasaryan and Vitaly Shmatikov. Blind backdoors in deep learning models. arXiv preprint arXiv:2005.03823, 2020.\\n\\n[2] Eugene Bagdasaryan, Andreas Veit, Yiqing Hua, Deborah Estrin, and Vitaly Shmatikov. How to backdoor federated learning. In AISTATS, pages 2938\u20132948, 2020.\\n\\n[3] Mauro Barni, Kassem Kallas, and Benedetta Tondi. A new backdoor attack in cnns by training set corruption without label poisoning. In ICIP, pages 101\u2013105. IEEE, 2019.\\n\\n[4] Peter L. Bartlett, Dylan J. Foster, and Matus Telgarsky. Spectrally-normalized margin bounds for neural networks. In Advances in Neural Information Processing Systems, pages 6240\u20136249, 2017.\\n\\n[5] Peter L. Bartlett and Shahar Mendelson. Rademacher and gaussian complexities: Risk bounds and structural results. J. Mach. Learn. Res., 3:463\u2013482, 2002.\\n\\n[6] Stephen Boyd, Stephen P Boyd, and Lieven Vandenberghe. Convex optimization. Cambridge university press, 2004.\\n\\n[7] Xinyun Chen, Chang Liu, Bo Li, Kimberly Lu, and Dawn Song. Targeted backdoor attacks on deep learning systems using data poisoning. arXiv preprint arXiv:1712.05526, 2017.\\n\\n[8] Siyuan Cheng, Yingqi Liu, Shiqing Ma, and Xiangyu Zhang. Deep feature space trojan attack of neural networks by controlled detoxification. In AAAI, 2021.\\n\\n[9] Edward Chou, Florian Tramer, and Giancarlo Pellegrino. Sentinel: Detecting localized universal attack against deep learning systems. SPW, 2020.\\n\\n[10] Dylan J. Foster and Alexander Rakhlin. $L_\\\\infty$ vector contraction for rademacher complexity. CoRR, abs/1911.06468, 2019.\\n\\n[11] Jannik Fritsch, Tobias Kuehnl, and Andreas Geiger. A new performance measure and evaluation benchmark for road detection algorithms. In International Conference on Intelligent Transportation Systems (ITSC), 2013.\\n\\n[12] Yansong Gao, Bao Gia Doan, Zhi Zhang, Siqi Ma, Anmin Fu, Surya Nepal, and Hyoungshick Kim. Backdoor attacks and countermeasures on deep learning: A comprehensive review. arXiv preprint arXiv:2007.10760, 2020.\\n\\n[13] Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. Strip: A defence against trojan attacks on deep neural networks. In ACSAC, pages 113\u2013125, 2019.\\n\\n[14] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring attacks on deep neural networks. IEEE Access, 7:47230\u201347244, 2019.\\n\\n[15] Tianyu Gu, Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Evaluating backdooring attacks on deep neural networks. IEEE Access, 2019.\\n\\n[16] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. CoRR, abs/1512.03385, 2015.\\n\\n[17] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in a neural network, 2015.\\n\\n[18] Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. CoRR, abs/1502.03167, 2015.\\n\\n[19] Jinyuan Jia, Xiaoyu Cao, and Neil Zhenqiang Gong. Certified robustness of nearest neighbors against data poisoning attacks. In AAAI, 2020.\\n\\n[20] Soheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus patterns: Revealing backdoor attacks in cnns. In CVPR, 2020.\\n\\n[21] Alex Krizhevsky. Learning multiple layers of features from tiny images. Technical report, 2009.\\n\\n[22] Keita Kurita, Paul Michel, and Graham Neubig. Weight poisoning attacks on pre-trained models. In ACL, 2020.\\n\\n[23] Yige Li, Nodens Koren, Lingjuan Lyu, Xixiang Lyu, Bo Li, and Xingjun Ma. Neural attention distillation: Erasing backdoor triggers from deep neural networks. In ICLR, 2021.\\n\\n[24] Yiming Li, Baoyuan Wu, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey. arXiv preprint arXiv:2007.08745, 2020.\\n\\n[25] Zhizhong Li and Derek Hoiem. Learning without forgetting. CoRR, abs/1606.09282, 2016.\\n\\n[26] Junyu Lin, Lei Xu, Yingqi Liu, and Xiangyu Zhang. Composite backdoor attack for deep neural network by mixing existing benign features. In CCS, 2020.\\n\\n[27] Kang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses, pages 273\u2013294. Springer, 2018.\\n\\n[28] Yingqi Liu, Wen-Chuan Lee, Guanhong Tao, Shiqing Ma, Yousra Aafer, and Xiangyu Zhang. Abs: Scanning neural networks for back-doors by artificial brain stimulation. In CCS, pages 1265\u20131282, 2019.\\n\\n[29] Yingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. In NDSS, 2018.\\n\\n[30] Yunfei Liu, Xingjun Ma, James Bailey, and Feng Lu. Reflection backdoor: A natural backdoor attack on deep neural networks. In ECCV, 2020.\\n\\n[31] Michael McCoyd, Won Park, Steven Chen, Neil Shah, Ryan Roggenkemper, Minjune Hwang, Jason Xinyu Liu, and David Wagner. Minority reports defense: Defending against adversarial patches. In International Conference on Applied Cryptography and Network Security, pages 564\u2013582. Springer, 2020.\\n\\n[32] Anh Nguyen and Anh Tran. Wanet \u2013 imperceptible warping-based backdoor attack, 2021.\\n\\n[33] Anh Nguyen and Anh Tran. Wanet\u2013imperceptible warping-based backdoor attack. In ICLR, 2021.\\n\\n[34] Tuan Anh Nguyen and Anh Tran. Input-aware dynamic backdoor attack. NeurIPS, 33, 2020.\\n\\n[35] NIST. TrojAI Leaderboard. https://pages.nist.gov/trojai/, 2019.\\n\\n[36] Patrick Rebeschini. Lecture notes in algorithmic foundations of learning: Covering numbers bounds for rademacher complexity, 2020.\"}"}
{"id": "CVPR-2023-1233", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Shahbaz Rezaei and Xin Liu. A target-agnostic attack on deep models: Exploiting security vulnerabilities of transfer learning. In ICLR, 2020.\\n\\nAniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor attacks. In AAAI, 2020.\\n\\nAhmed Salem, Rui Wen, Michael Backes, Shiqing Ma, and Yang Zhang. Dynamic backdoor attacks against machine learning models. arXiv preprint arXiv:2003.03675, 2020.\\n\\nAli Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In NeurIPS, 2018.\\n\\nTe Juin Lester Tan and Reza Shokri. Bypassing backdoor detection algorithms in deep learning. In IEEE European Symposium on Security and Privacy, EuroS&P 2020, Genoa, Italy, September 7-11, 2020, pages 175\u2013183. IEEE, 2020.\\n\\nDi Tang, XiaoFeng Wang, Haixu Tang, and Kehuan Zhang. Demon in the variant: Statistical analysis of dnns for robust backdoor contamination detection. In 30th USENIX Security Symposium (USENIX Security 21), 2021.\\n\\nGuanhong Tao, Yingqi Liu, Guangyu Shen, Qiuling Xu, Shengwei An, Zhuo Zhang, and Xiangyu Zhang. Model orthonomalization: Class distance hardening in neural networks for better security. In 2022 IEEE Symposium on Security and Privacy (SP). IEEE, 2022.\\n\\nVale Tolpegin, Stacey Truex, Mehmet Emre Gursoy, and Ling Liu. Data poisoning attacks against federated learning systems. In ESORICS, pages 480\u2013501, 2020.\\n\\nBrandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. In NeurIPS, pages 8000\u20138010, 2018.\\n\\nAlexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label backdoor attacks. 2018.\\n\\nAkshaj Kumar Veldanda, Kang Liu, Benjamin Tan, Prashanth Krishnamurthy, Farshad Khorrami, Ramesh Karri, Brendan Dolan-Gavitt, and Siddharth Garg. Nnoculation: broad spectrum and targeted treatment of backdoored dnns. arXiv preprint arXiv:2002.08313.\\n\\nBolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In S&P, pages 707\u2013723, 2019.\\n\\nBolun Wang, Yuanshun Yao, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. With great training comes great vulnerability: Practical attacks against transfer learning. In USENIX Security, pages 1281\u20131297, 2018.\\n\\nDongxian Wu and Yisen Wang. Adversarial neuron pruning purifies backdoored deep models. Advances in Neural Information Processing Systems, 34, 2021.\\n\\nChong Xiang, Arjun Nitin Bhagoji, Vikash Sehwag, and Prateek Mittal. Patchguard: A provably robust defense against adversarial patches via small receptive fields and masking. In 30th USENIX Security Symposium (USENIX Security 21), 2021.\\n\\nChong Xiang, Saeed Mahloujifar, and Prateek Mittal. Patchcleanser: Certifiably robust defense against adversarial patches for any image classifier. arXiv preprint arXiv:2108.09135, 2021.\\n\\nSergey Zagoruyko and Nikos Komodakis. Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer. CoRR, abs/1612.03928, 2016.\\n\\nSergey Zagoruyko and Nikos Komodakis. Wide residual networks. CoRR, abs/1605.07146, 2016.\\n\\nXinyang Zhang, Zheng Zhang, and Ting Wang. Trojaning language models for fun and profit. In European S&P, 2021.\\n\\nPu Zhao, Pin-Yu Chen, Payel Das, Karthikeyan Natesan Ramamurthy, and Xue Lin. Bridging mode connectivity in loss landscapes and adversarial robustness. In International Conference on Learning Representations, 2020.\\n\\nShihao Zhao, Xingjun Ma, Xiang Zheng, James Bailey, Jingjing Chen, and Yu-Gang Jiang. Clean-label backdoor attacks on video recognition models. In CVPR, 2020.\\n\\nChen Zhu, W Ronny Huang, Hengduo Li, Gavin Taylor, Christoph Studer, and Tom Goldstein. Transferable clean-label poisoning attacks on deep neural nets. In ICML, pages 7614\u20137623, 2019.\"}"}
{"id": "CVPR-2023-1233", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"the small set, and to knowledge distillation. That is, cloning tends to have a smaller loss value. Backdoor removal using importance values is merely an application of the property as we essentially leverage importance analysis to select a subset of functionalities only relevant to clean sample classifications for cloning. Figure 3 shows empirical evidence using an example. Observe that knowledge distillation has a larger improvement over training from scratch given more samples. Also observe that training from scratch and knowledge distillation cause non-trivial benign accuracy degradation. In contrast, cloning causes negligible degradation of original backdoor model using even just 2% of data.\\n\\nWe mainly utilize the decomposition, Rademacher complexity [5] and Covering number to derive an upper bound for the testing loss of a classifier with a high probability. We hence show that cloning has a smaller upper bound (of loss) compared to distillation and training from scratch using a small set of samples.\\n\\nBased on the aforementioned analysis, we additionally prove the computation complexity of cloning in Appendix B. We show cloning can be orders of magnitude faster in recovering benign accuracy. Furthermore, in Appendix C, we theoretically show cloning can be more effective than fine-tuning in removing backdoors. This is done by analyzing a general backdoor scenario. We hence prove that cloning can guarantee the backdoor removal for this type of backdoor while fine-tuning can not. And we show the importance weight correctly identify the compromised neurons.\\n\\nIn the following, we first introduce the notations, terms, and a set of lemmas. We then formally show that cloning is better than training from scratch on a small set and distillation.\\n\\nFigure 3. Benign accuracy of models after backdoor removal versus the data used (for a CIFAR-10 model poisoned with CleanLabel backdoor).\\n\\n4.1. Notations and Backgrounds\\n\\nWe consider the original model \\\\( f^* \\\\), model \\\\( f_s \\\\) trained from scratch using \\\\( D_s \\\\) (i.e., the small dataset), clone model \\\\( f_c \\\\) by our method and model \\\\( f_k \\\\) by knowledge distillation [17]. They belong to the function families \\\\( F^* \\\\), \\\\( F_s \\\\), \\\\( F_c \\\\), and \\\\( F_k \\\\), respectively. These families are determined by the specific optimization and learning algorithms. Since each algorithm has its learning bias and will likely generate a different set of parameters.\\n\\nNeural Net. To facilitate the formal analysis, we leverage a simple network. Let \\\\( \\\\psi \\\\) be the ReLU activation function, \\\\( l_\\\\gamma \\\\) the loss function with the Lipschitz constant \\\\( 2\\\\gamma \\\\). Without loss of generality, we assume a two-layer network, where the two layers are abstracted to two respective matrices, \\\\( G \\\\in G \\\\subset \\\\mathbb{R}^{d \\\\times p} \\\\) and \\\\( H \\\\in H \\\\subset \\\\mathbb{R}^{p \\\\times C} \\\\), with \\\\( p \\\\) the number of hidden units. Let \\\\( s \\\\) be the softmax function. The network is represented as \\\\( f(x) = s \\\\circ o(x) = H \\\\psi(Gx) \\\\).\\n\\nIntuitively, \\\\( f(x) \\\\) outputs the probability, \\\\( o(x) \\\\) is the logits value and \\\\( g(x) \\\\) is the hidden layer. It is worth noting that our analysis can naturally extend to complex network since the proof structure stays the same.\\n\\nLoss. As a common practice in multi-class analysis, we use the margin loss [4] as \\\\( l_\\\\gamma(f(x), y) \\\\). Its definition can be found in Appendix eq.(9). A smaller loss value indicates better performance.\\n\\nRademacher Complexity. Let \\\\( F \\\\) be a family of functions , \\\\( D \\\\) a set of samples, and \\\\( \\\\sigma \\\\) uniformly sampled from \\\\( \\\\{-1, +1\\\\}^{\\\\mid D\\\\mid} \\\\). The empirical Rademacher complexity is defined as follows.\\n\\n\\\\[\\n\\\\hat{R}(F|D) = \\\\mathbb{E}_{\\\\sigma}[\\\\sup_{f \\\\in F} \\\\frac{1}{\\\\mid D\\\\mid} \\\\sum_{i \\\\in D} \\\\sigma_i f(x_i)]\\n\\\\]\\n\\nObserve that if \\\\( F \\\\) contains only a fixed function, the value is 0 suggesting no complexity (and hence trivial to learn).\\n\\nCovering Number Bound. It is difficult to directly compute Rademacher complexity in our context. We hence derive its upper-bound using covering number. Let \\\\( \\\\| \\\\cdot \\\\|_{p,D} \\\\) be a pseudo-norm on function family \\\\( F \\\\) with respect to a vector norm \\\\( \\\\| \\\\cdot \\\\|_p \\\\) and data \\\\( D \\\\) (definition in Appendix eq. (20)), we define the covering number \\\\( N(F, \\\\| \\\\cdot \\\\|_{p,D}, \\\\epsilon) \\\\) as the size of minimal-\\\\( \\\\epsilon \\\\)-cover of \\\\( F \\\\) under this pseudo-norm.\\n\\nIntuitively, this number means how many functions are representative such that they can cover all the learning outcomes with the \\\\( \\\\epsilon \\\\) bound. The covering number of a function family is dependent on the learning method. For example, when we include the loss to minimize the difference between \\\\( f_c \\\\in F_c \\\\) and \\\\( f^* \\\\in F^* \\\\) (during cloning), we essentially reduce the covering number of the function difference family \\\\( N(\\\\{f_c - f^*\\\\}, \\\\| \\\\cdot \\\\|_{p,D}, \\\\epsilon) \\\\).\\n\\nLemmas. In the following, we introduce three lemmas that are needed in later analysis. Their proofs can be found in the Appendix. In Lemma 1, we derive the Lipschitz constant of the loss function. In Lemma 2, we bound the Lipschitz of softmax function. In Lemma 3, we bound the Rademacher complexity by a function over the covering number.\\n\\nLemma 1. For any logits \\\\( o_1, o_2 \\\\in \\\\mathbb{R}^c \\\\) and \\\\( y \\\\in Y \\\\), we have\\n\\n\\\\[\\n|l_\\\\gamma(o_1, y) - l_\\\\gamma(o_2, y)| \\\\leq 2\\\\gamma \\\\|o_1 - o_2\\\\|_\\\\infty.\\n\\\\]\"}"}
{"id": "CVPR-2023-1233", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"According to Equation 5 and the standard Rademacher com-\\nplexity bound argument \\\\(5\\\\), we have the following regret\\nupper bound between the distilled model and the original\\nmodel with a high probability\\n\\n\\\\[\\n\\\\Delta = \\\\sup_{f \\\\in F} \\\\mathbb{E}_{l \\\\sim S} \\\\mathbb{E}_{x, y \\\\sim D_l} \\\\left| \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)] \\\\right|\\n\\\\]\\n\\n\\\\[\\n\\\\Delta = \\\\mathbb{E}_{x, y \\\\sim D_s} \\\\left| \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] \\\\right|\\n\\\\]\\n\\nwhere \\\\(\\\\mathbb{E}_{x, y \\\\sim D_s}\\\\) and \\\\(\\\\mathbb{E}_{x, y \\\\sim D_o}\\\\) are the expectations over training data.\\n\\nThe advantage comprises two parts, i.e., a gap and a regret.\\n\\nThe gap is the training loss difference between training on the\\nwhole dataset and on a small part of the same dataset. The\\ngap is not specific to our algorithm, we hence study the\\ncloning favorable.\\n\\nD\\n\\n\\\\[\\n\\\\text{Gap} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Gap} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\nNext we analyze the upper-bound of the regret, since the\\nupper bound of the regret. A smaller regret indicates better\\nclassification performance. We finally show the upper bound\\nof our regret is smaller than baselines.\\n\\nTo formally compare cloning, distillation, and training\\nfrom scratch on a small set, we first study the advantage of\\ndistillation compared with training from scratch.\\n\\nThe advantage of distillation is a gap and a regret.\\n\\nThe advantage comprises two parts, i.e., a gap and a regret.\\n\\nThe gap is the training loss difference between training on the\\nwhole dataset and on a small part of the same dataset. The\\ngap is not specific to our algorithm, we hence study the\\ncloning favorable.\\n\\n\\\\[\\n\\\\text{Gap} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\nFor distilled model\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\[\\n\\\\text{Regret of the Distilled Model} = \\\\mathbb{E}_{x, y \\\\sim D_s} [l(x, y)] - \\\\mathbb{E}_{x, y \\\\sim D_o} [l(x, y)]\\n\\\\]\\n\\n\\\\"}
{"id": "CVPR-2023-1233", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Effectiveness of importance. The shaded area represents standard deviation.\\n\\n\\\\[ \\\\text{number of} \\\\ i_c \\\\text{, i.e.} \\\\ N(I_c, \\\\| \\\\cdot \\\\|_1, D, \\\\epsilon) \\\\text{in} \\\\ B(I_c | D \\\\text{, } s) \\\\] and the upper-bound on the regret. Also note that the other family \\\\( J^* \\\\) is unrelated to cloning.\\n\\n5. Experiment Setting. We conduct experiments on nine representative backdoor attacks. Specifically, we use Wide ResNet [54] and CIFAR-10 [21] for the common benchmark attacks and the adaptive attacks including Badnet [14], Clean Label [46], SIG [3], Reflection [30], Warp [32] and two Adaptive Attacks [26]. 5% data is used for removing backdoor behaviors on CIFAR-10. We further experiment on large-scale datasets Kitti-City [11] and Kitti-Road [11], and public backdoor benchmarks Polygon and Filter, based on ResNet [54]. We compare with five latest backdoor removal methods including Finetune, Fineprune [27], NAD [23], MCR [56], and ANP [50]. More details of these attacks and removal methods can be found in Section 2 and Appendix D.\\n\\nComparison with Baselines. In Table 1, we show the results in comparison with other backdoor removal methods. The fourth column shows the accuracy and attack success rate (ASR) of the original (trojaned) model. The following columns show the results after applying various backdoor removal methods. All the methods use the same set of data augmentations. We also adjust the temperature of M\\\\(_{EDIC}\\\\) so that the test accuracies of our repaired models are comparable to others. From the results, we find the advantage of our algorithm outperforming other baselines lies in the cases of hard-to-remove backdoors. We retrain the model whereas others do not. This different design ensures some deep-rooted backdoors can be removed. Observe that other baselines are less effective in removing Clean Label, SIG, Polygon, and Filter backdoors as the ASRs are still high after the removal. In contrast, M\\\\(_{EDIC}\\\\) can substantially reduce the ASRs of these attacks. On the other hand, when the backdoor attack is not robust (e.g. simpler attacks like Badnet and Reflection where Finetune can already remove them), the optimal results will favor methods which cause fewer changes to the model, since few changes suffice to remove the backdoor. From the security perspective, a resilient backdoor will be used by attackers more often, which suggests those hard-case backdoor attacks are more important. Note that even in those easier cases, M\\\\(_{EDIC}\\\\) is still competitive. In the last row of Table 1, we show the average ASR reduction and accuracy degradation. Observe that M\\\\(_{EDIC}\\\\) achieves 25% more ASR reduction compared to others with minor accuracy degradation.\\n\\nAdaptive Attacks. We evaluate two adaptive attacks. In adaptive attack 1, the trigger pattern can exist on benign samples [26]. It utilizes multiple natural objects from different classes as the backdoor. In adaptive attack 2, we adversarially optimize the trigger to maximize the clone loss. The trigger will have a high activation value on the important neurons and may survive after cloning. Note that these represent the most adversarial contexts against M\\\\(_{EDIC}\\\\) as the backdoor is essentially benign features or benign neurons that cloning would likely copy. Observe that M\\\\(_{EDIC}\\\\) is still quite effective against Adaptive1 and Adaptive2 attacks as shown in Table 1. For the variant 1, while benign features are used to compose the backdoor, these features are not important for the target class. As such, their behaviors are hence not copied for the target class. For the variant 2, despite the substantial backdoor activations on important neurons, the cloning will only connect the benign functions to the output due to the absence of the trigger pattern in the clean data.\\n\\nEffectiveness of Using Importance Values. In Figure 4, we show the effects of cloning using different temperatures. A temperature \\\\( \\\\tau = 0 \\\\) means we do not use the importance and treat all neurons as equal. Observe that a larger temperature prunes more backdoor-related behaviors. The ASR is reduced by at most 70% with at most 4% accuracy drop (with a temperature of 5). This shows our importance values capture the characteristics of benign functionalities.\\n\\nStronger Backdoor Baselines. Different from the results reported in recent works [23,50,56], our results are evaluated on stronger backdoor baselines. We found training backdoor\"}"}
{"id": "CVPR-2023-1233", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The black lines denote defense methods. Observe that with augmentations, the baselines can hardly remove the backdoor with and without augmentations. Note that the same data augmentation is also used when applying remove the backdoor. In contrast, M\\n\\nFigure 5 shows the effectiveness of different removal methods on backdoor attacks trained with and without data augmentations. The model is trojaned by a clean label attack on CIFAR-10. The red bars denote the ASRs after removal when augmentations are used during attack. The blue bars the ASRs out data augmentations. The results are from Clean Label attack.\\n\\nTable 1. Comparing with other methods.\\n\\n| Attack Metric | Original (%) Method (in percentage %) | Avg. Drop ASR |\\n|---------------|--------------------------------------|---------------|\\n| Benchmarks    | Clean Label ASR 100.0                |               |\\n|              | Reflection ASR 34.2                  |               |\\n|              | Variant 1 ASR 79.3                   |               |\\n|              | Variant 2 ASR 98.5 94.9              |               |\\n|              | Polygon ASR 99.9                     |               |\\n|              | Badnet ASR 100.0                     |               |\\n|              | Filter ASR 100.0                      |               |\\n|              | Warp ASR 96.5 13.2                   |               |\\n|              | SIG ASR 94.5                         |               |\\n|              | ACC 86.2 84.5                        |               |\\n|              | ACC. 89 85 83                          |               |\\n|              | ACC. 99 83 80 98                       |               |\\n\\nThe results reflect the views of our sponsors. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors. This research was supported, in part by ONR N000141712045, N000141410468 and N000141712947. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors"}
