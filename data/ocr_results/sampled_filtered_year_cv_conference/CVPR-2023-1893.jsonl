{"id": "CVPR-2023-1893", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Amit Agrawal, Srikumar Ramalingam, Yuichi Taguchi, and Visesh Chari. A theory of multi-layer flat refractive geometry. In CVPR, pages 3346\u20133353, 2012.\\n\\n[2] Matan Atzmon and Yaron Lipman. Sal: Sign agnostic learning of shapes from raw data. In CVPR, pages 2565\u20132574, 2020.\\n\\n[3] Max Born and Emil Wolf. Principles of optics: electromagnetic theory of propagation, interference and diffraction of light. Elsevier, 2013.\\n\\n[4] Francois Chadebecq, Francisco Vasconcelos, George Dwyer, Rene Lacher, Sebastien Ourselin, Tom Vercauteren, and Danail Stoyanov. Refractive Structure-From-Motion Through a Flat Refractive Interface. In ICCV, 2017.\\n\\n[5] Blender Online Community. Blender - a 3D modelling and rendering package. Blender Foundation, Stichting Blender Foundation, Amsterdam, 2018.\\n\\n[6] CSIRO. Australian National Fish Collection. https://www.csiro.au/en/about/facilities-collections/collections/anfc.\\n\\n[7] CSIRO. Australian National Insect Collection. https://www.csiro.au/en/about/facilities-collections/collections/anic.\\n\\n[8] S. Garrido-Jurado, R. Mu\u00f1oz-Salinas, F. J. Madrid-Cuevas, and M. J. Mar\u00edn-Jim\u00e9nez. Automatic generation and detection of highly reliable fiducial markers under occlusion. Pattern Recognition, 47(6):2280\u20132292, 2014.\\n\\n[9] Georg Glaeser and Hans-Peter Schr\u00f6cker. Reflections on refractions. Journal for Geometry and Graphics, 4(1):1\u201318, 2000.\\n\\n[10] Georg Glaeser and Hans-Peter Schr\u00f6cker. Reflections on refractions. Journal for Geometry and Graphics, 4(1):1\u201318, 2000.\\n\\n[11] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regularization for learning shapes. In Proceedings of the 37th International Conference on Machine Learning, pages 3789\u20133799, 2020.\\n\\n[12] Yuan-Chen Guo, Di Kang, Linchao Bao, Yu He, and Song-Hai Zhang. NeRFReN: Neural Radiance Fields With Reflections. In CVPR, 2022.\\n\\n[13] Kai Han, Kwan-Yee K. Wong, and Miaomiao Liu. Dense Reconstruction of Transparent Objects by Altering Incident Light Paths Through Refraction. IJCV, 126(5):460\u2013475, 2018.\\n\\n[14] Xiao Hu, Fran\u00e7ois Lauze, Kim Steenstrup Pedersen, and Jean M\u00e9lou. Absolute and relative pose estimation in refractive multi view. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, pages 2569\u20132578, 2021.\\n\\n[15] Anne Jordt-Sedlazeck and Reinhard Koch. Refractive Structure-from-Motion on Underwater Images. In ICCV, pages 57\u201364, 2013.\\n\\n[16] Michael Kazhdan and Hugues Hoppe. Screened poisson surface reconstruction. ACM TOG, 32(3):1\u201313, 2013.\\n\\n[17] Zhengqin Li, Yu-Ying Yeh, and Manmohan Chandraker. Through the Looking Glass: Neural 3D Reconstruction of Transparent Shapes. In CVPR, pages 1262\u20131271, 2020.\\n\\n[18] William E. Lorensen and Harvey E. Cline. Marching cubes: A high resolution 3D surface construction algorithm. ACM SIGGRAPH Computer Graphics, 21(4):163\u2013169, 1987.\\n\\n[19] Jiahui Lyu, Bojian Wu, Dani Lischinski, Daniel Cohen-Or, and Hui Huang. Differentiable refraction-tracing for mesh reconstruction of transparent objects. ACM TOG, 39(6):195:1\u2013195:13, 2020.\\n\\n[20] N. Max. Optical models for direct volume rendering. IEEE TVCG, 1(2):99\u2013108, 1995.\\n\\n[21] Mateusz Michalkiewicz, Jhony K. Pontes, Dominic Jack, Mahsa Baktashmotlagh, and Anders Eriksson. Implicit Surface Representations As Layers in Neural Networks. In ICCV, pages 4743\u20134752, 2019.\\n\\n[22] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF: Representing scenes as neural radiance fields for view synthesis. In ECCV, pages 405\u2013421, 2020.\\n\\n[23] The Natural History Museum. Digital Collections. https://www.nhm.ac.uk/our-science/our-work/digital-collections.html.\\n\\n[24] Michael Oechsle, Songyou Peng, and Andreas Geiger. Unisurf: Unifying neural implicit surfaces and radiance fields for multi-view reconstruction. In ICCV, pages 5589\u20135599, October 2021.\\n\\n[25] Steven Parker, William Martin, Peter-Pike J. Sloan, Peter Shirley, Brian Smits, and Charles Hansen. Interactive ray tracing. In ACM SIGGRAPH 2005 Courses, pages 12\u2013es, 2005.\\n\\n[26] Johannes L. Sch\u00f6nberger and Jan-Michael Frahm. Structure-From-Motion Revisited. In CVPR, pages 4104\u20134113, 2016.\\n\\n[27] Johannes Sch\u00f6nberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In ECCV, pages 501\u2013518, 2016.\\n\\n[28] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit Neural Representations with Periodic Activation Functions. In NeurIPS, volume 33, pages 7462\u20137473, 2020.\\n\\n[29] Jiaming Sun, Xi Chen, Qianqian Wang, Zhengqi Li, Hadar Averbuch-Elor, Xiaowei Zhou, and Noah Snavely. Neural 3d reconstruction in the wild. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1\u20139, 2022.\\n\\n[30] UNSW. Museum of Human Disease. https://www.unsw.edu.au/medicine-health/disease-museum.\\n\\n[31] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. NeuS: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In NeurIPS, volume 34, pages 27171\u201327183, 2021.\\n\\n[32] Bojian Wu, Yang Zhou, Yiming Qian, Minglun Cong, and Hui Huang. Full 3d reconstruction of transparent objects. ACM TOG, 37(4), 2018.\"}"}
{"id": "CVPR-2023-1893", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2023-1893", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of refraction light while \\\\( T(\\\\ell) \\\\) represents the transmittance of the space along ray direction. The color of reflected and refracted rays can be expressed as:\\n\\n\\\\[\\nC(\\\\ell_{ri}) = R_i \\\\cdot \\\\hat{C}(\\\\ell_{ri}) \\\\tag{8}\\n\\\\]\\n\\n\\\\[\\nC(\\\\ell_{it}) = T_{ri} \\\\cdot \\\\hat{C}(\\\\ell_{ti}) \\\\tag{9}\\n\\\\]\\n\\nThe tracing process is performed recursively on \\\\( \\\\ell_{ri} \\\\) and \\\\( \\\\ell_{ti} \\\\) and constrained by the depth of recursion \\\\( D_{ri} \\\\) which is a hyper-parameter in our method. After ray tracing, each original camera ray \\\\( \\\\ell_i \\\\) is extended to \\\\( L_i \\\\) which contains a set of traced rays.\\n\\n**Color Accumulation.** The color of a ray emitted from the camera \\\\( \\\\ell_i \\\\) is restored by accumulating that among all the sub-rays in \\\\( L_i \\\\).\\n\\n\\\\[\\nC(\\\\ell_i) = \\\\text{ACC}_{m \\\\in L_i}(C(m)) \\\\tag{10}\\n\\\\]\\n\\nwhere \\\\( \\\\text{ACC} \\\\) means physically-based color accumulation. To do this, we first retrieve the color of each sub-ray \\\\( m \\\\in L_i \\\\). For internal rays, the color is obtained by volume rendering. And for external rays, a pre-defined background color \\\\( C_{out} \\\\) is directly assigned. Next, the ray color is accumulated progressively in the reverse order of ray tracing. For a ray \\\\( \\\\ell \\\\), if \\\\( m \\\\) meets the interface, the color \\\\( C(\\\\ell) \\\\) is modified according to Eq. (8) and (9) and when passing through \\\\( S_{in} \\\\), we further multiply the color \\\\( C(\\\\ell) \\\\) with the accumulated transmittance along the ray, computed as:\\n\\n\\\\[\\nT(\\\\ell) = \\\\exp \\\\left( -\\\\int_{t_{st}}^{t_{te}} \\\\rho(s) \\\\, ds \\\\right) \\\\tag{11}\\n\\\\]\\n\\nwhere \\\\( t_{st} \\\\) and \\\\( t_{te} \\\\) indicate the start and end of the ray.\\n\\n**Working with Irradiance.** Since we use a physically-based renderer, it should work with scene irradiance rather than image intensity. The camera response function (CRF) describes the relationship between image intensity and scene irradiance. To compensate for the nonlinear CRF, we assume that the appearance MLP predicts colors in linear space and adopts a gamma correction function to the final output:\\n\\n\\\\[\\nI = G(C(\\\\ell_i)) \\\\tag{12}\\n\\\\]\\n\\nwhere \\\\( G(C) = C^{1/2} \\\\) is the gamma correction function.\\n\\n### 3.4. Loss Function\\n\\nFollowing previous works \\\\[22, 31, 33\\\\], the parameters of ReReuS are merely optimized and supervised by multi-view images with known poses. A binary mask \\\\( M_{in} \\\\) of the transparent enclosure is also used. To be specific, we randomly sample a batch of pixels. For each pixel \\\\( p \\\\) in the batch, a camera ray \\\\( \\\\ell_p \\\\) is generated with the corresponding camera pose. After that, we trace the camera ray \\\\( \\\\ell_p \\\\) and get a ray set \\\\( L_p \\\\). For each ray in \\\\( L_p \\\\), if it belongs to \\\\( S_{in} \\\\), we consistently sample \\\\( n \\\\) points along the ray and do volume rendering. The color of the pixel \\\\( \\\\hat{I}_p \\\\) is retrieved by accumulation and gamma correction as described in Sec. 3.3.\\n\\nThe overall loss function is given below, and described in detail afterwards:\\n\\n\\\\[\\nL = L_{color} + \\\\lambda_1 L_{trans} + \\\\lambda_2 L_{reg} \\\\tag{13}\\n\\\\]\\n\\n**Photometric loss.** We empirically choose \\\\( L_1 \\\\) loss between rendered pixel color and the ground truth as:\\n\\n\\\\[\\nL_{color} = \\\\frac{1}{|M_{in}|} \\\\sum_{p \\\\in M_{in}} \\\\| \\\\hat{I}_p - I_p \\\\|_1 \\\\tag{14}\\n\\\\]\\n\\n**Sparsity prior.** Based on the crystalline appearance of the box, we exploit a sparsity prior that a considerable part of \\\\( S_{in} \\\\) (except the area occupied by the object) should be somehow transparent to the light. We apply this prior by regularizing the transmittance of \\\\( S_{in} \\\\) as:\\n\\n\\\\[\\nL_{trans} = \\\\frac{1}{|M_{in}|} \\\\sum_{p \\\\in M_{in}} \\\\sum_{\\\\ell \\\\in L_p \\\\cap S_{in}} \\\\| 1 - T(\\\\ell) \\\\|_1 \\\\tag{15}\\n\\\\]\\n\\nThis prior is valid in our case since the opaque object usually takes up a limited area of the transparent box. Experiment results show that \\\\( L_{trans} \\\\) helps to get cleaner geometry.\\n\\n**Regularization.** The same as NeuS, we apply the Eikonal regularization from \\\\[11\\\\] on the sampled points to regularize the geometric MLP as:\\n\\n\\\\[\\nL_{reg} = \\\\left( \\\\frac{\\\\langle |\\\\nabla x g(\\\\theta(x))| - 1 \\\\rangle^2}{|x|} \\\\right)\\n\\\\]\\n\\nwhere \\\\( x \\\\) represents all the sampled points on the rays inside \\\\( S_{in} \\\\) within a batch.\\n\\n### 4. Experiments\\n\\n**4.1. Dataset.** We introduce two (one synthetic and another real) new datasets to evaluate our method and baseline methods since there is no existing dataset dedicated to our problem setting.\\n\\n**Synthetic Dataset.** We choose 13 objects with complicated appearances and/or geometry to generate synthetic data. Blender \\\\[5\\\\] with the physical-based render engine Cycles is used to render photorealistic images. We put the object at the center of a box, manually adjust the size of the box to fit the object and then scale it within a unit sphere. The object generally has a Lambertian surface. The material of the box is adjusted by \\\\( \\\\text{Principled BSDF} \\\\) to appear as glass with a refractive index of 1.45. The camera pose is randomly sampled on a full sphere of radius 5. For each camera pose, we not only render an image of the complete scene but also render another image without the transparent box. We render 60 viewpoints with a resolution of 12559.\"}"}
{"id": "CVPR-2023-1893", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Reconstruction of 3D shapes on the synthetic dataset. We qualitatively compare ReNeuS with baseline methods. Results show that our method successfully achieves high-quality reconstruction with transparent containers where other methods fail.\\n\\nReal Dataset. Another 10 real scenes of insect specimens data are captured. We capture the dataset in a photography light tent where the specimen is placed on a sheet of ChArUco board over a rotating display stand. The images are captured on a hemisphere with different azimuths and altitudes. We use ChArUco board to calibrate the intrinsics as well as the pose of the camera. As for the transparent box, we manually label it by edges in the images and calculate its dimensions and pose according to the known camera. The transparent box mask is then generated by projecting the box to 2D images. Finally, we center-crop all images to be 1728 \u00d7 1152 pixels and randomly sample 60 images per scene. You can find more details about the dataset in the supplementary material.\\n\\n4.2. Baseline methods. Since there is no specific method aiming at the same problem, we choose several state-of-the-art 3D reconstruction methods as baselines. To make our results more convincing, apart from the w/ box data, we also compare with the baselines on the w/o box dataset.\\n\\nCOLMAP [26, 27] is a popular MVS method. For a fair comparison, we don't run structure-from-motion with COLMAP, but directly use the ground truth camera pose for reconstruction. We obtain a mesh by applying screened Poisson Surface Reconstruction [16] on the output point cloud of COLMAP.\\n\\nIDR [33] is a neural rendering-based 3D reconstruction method that achieves results with high fidelity. We run IDR with object mask of w/o box dataset only.\\n\\nNeuS. Our method is actually an extension of NeuS to an optically complex scene. We compare with NeuS to verify our credit.\\n\\n4.3. Implementation Details. We implement ReNeuS on the top of NeuS [31]. We adopt the same network structure for geometric MLP $g_{\\\\theta}$ and appearance MLP $f_{\\\\phi}$ as NeuS. The geometric MLP is initialized as [2] to be an ellipsoid-like SDF field. Different from NeuS, we replace the activation function (Softplus) with a periodic activation function SIREN [28] for its stronger capability for neural representation. The input parameters position $x$ and view direction $v$ are prompted with positional encoding [22]. The geometry of $S_{\\\\text{out}}$ is represented as a mesh of a regular box with known dimensions. For the real data, we manually calibrate it with the edges of the box. Please refer to the supplementary material for more details.\\n\\nFor ReNeuS rendering, we set the recursion depth $D_{\\\\text{re}} = 2$ as a trade-off between accuracy and efficiency. The ambient lighting $C_{\\\\text{out}}$ is set to $[0, 0.8, 0.8]$ (before gamma correction) for synthetic data, and we calibrate that by measuring the average background for real data. For loss function, we calibrate the ambient lighting by measuring the average background for real data.\"}"}
{"id": "CVPR-2023-1893", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Qualitative evaluation on the synthetic dataset. We highlight the best result in terms of w/o box and w/ box data respectively.\\n\\n| Model          | Failure case | Chamfer-L\u2081 \u2193 |\\n|----------------|--------------|--------------|\\n| Ours (w/o Trans Loss) | 3 2          | 0.08         |\\n| NeuS + Ours (w/o Trans Loss) | 17 2         | 0.90         |\\n\\nTable 2. Results of ablation study.\\n\\nFor training. It's worth noting that ReNeuS get superior performance than NeuS, even on data without a container box. We assume this to be the contribution of reflection. Every time when a ray is reflected, it's like we have a new view of the scene which helps with reconstruction.\"}"}
{"id": "CVPR-2023-1893", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Visualization on ablation study.\\n\\nReference\\n\\nFigure 5. Visualization of 3D reconstruction on real dataset.\\n\\nresult of NeuS + is actually fully surrounded by another box-shaped mesh. The reason is that the NeuS + method doesn't model the scene with light interaction exactly. While training with photometric loss, the network tries to render an image of the statuette on each side, thus producing these fake boundaries. As our method takes good care of the light interaction, the fake boundaries are greatly relieved. Based on this observation, we abandon the post-processing in the ablation study to better evaluate the influence of such errors.\\n\\nThe results are shown in Tab. 2 where Failure case means that we fail to retrieve a mesh from the geometric MLP, after trying 3 times. Apart from accuracy, our method performs consistently in different cases. Especially for the dinosaur case which is quite challenging due to its thin structure and texture-less appearance, all the two methods failed except the full model.\\n\\nTransmittance loss. We also do an ablation study for our sparsity prior item as shown in Fig. 4. Those dummy facets are dramatically suppressed with the regularization.\\n\\n5. Limitations and Conclusion\\n\\nLimitation. The major limitation of our method is the assumption of known geometry and homogeneous background lighting. The known geometry assumption may be violated due to irregularly shaped transparent containers with rounded edges and corners and the lighting assumption may impede the data collection process. We notice from the results on real dataset that the violation of the assumptions can lead to a performance drop. We believe tackling these limitations by jointly reconstructing the object and container as well as modeling the lighting conditions can lead to promising future research directions.\\n\\nConclusion. In this paper, we have defined a new research problem of 3D reconstruction of specimens enclosed in transparent containers raised from the digitization of fragile museum collections. The image distortion caused by complex multi-path light interactions across interfaces, that separate multiple mediums, poses challenges to recovering accurate 3D geometry of the specimen. To tackle the problem, we propose a neural reconstruction method ReNeuS in which we use a novel hybrid rendering strategy that takes into account multiple light reflections and refractions across glass/air interfaces to accurately reconstruct 3D models. Experiments on both synthetic and our newly captured real dataset demonstrate the efficacy of our method. It is capable of producing high-quality 3D reconstructions when existing methods fail.\\n\\nAcknowledgements. The project is supported by CSIRO Julius Career Award, CSIRO Early Research Career (CERC) Fellowship and CSIRO Future Digital Manufacturing Fund (FDMF) Postdoc Fellowship. Jinguang is supported by CSIRO Data61 PhD Scholarship.\"}"}
{"id": "CVPR-2023-1893", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nIn this paper, we define a new problem of recovering the 3D geometry of an object confined in a transparent enclosure. We also propose a novel method for solving this challenging problem. Transparent enclosures pose challenges of multiple light reflections and refractions at the interface between different propagation media e.g. air or glass. These multiple reflections and refractions cause serious image distortions which invalidate the single viewpoint assumption. Hence the 3D geometry of such objects cannot be reliably reconstructed using existing methods, such as traditional structure from motion or modern neural reconstruction methods. We solve this problem by explicitly modeling the scene as two distinct sub-spaces, inside and outside the transparent enclosure. We use an existing neural reconstruction method (NeuS) that implicitly represents the geometry and appearance of the inner subspace. In order to account for complex light interactions, we develop a hybrid rendering strategy that combines volume rendering with ray tracing. We then recover the underlying geometry and appearance of the model by minimizing the difference between the real and rendered images. We evaluate our method on both synthetic and real data. Experiment results show that our method outperforms the state-of-the-art (SOTA) methods. Codes and data will be available at https://github.com/hirotong/ReNeuS\\n\\n1. Introduction\\nDigitization of museum collections is an emerging topic of increasing interest, especially with the advancement of virtual reality and Metaverse technology. Many famous museums around the world have been building up their own digital collections for online exhibitions. Among these collections, there is a kind of special but important collections such as insects [7, 23], human tissues [30], aquatic creatures [6] and other fragile specimens that need to be preserved inside some hard but transparent materials (e.g. resin, glass) as shown in Fig. 1a. In order to digitize such collections, we abstract a distinct research problem which is seeing through the glassy outside and recovering the geometry of the object inside. To be specific, the task is to reconstruct the 3D geometry of an object of interest from multiple 2D views of the scene. The object of interest is usually confined in a transparent enclosure, e.g. a glass case.\"}"}
{"id": "CVPR-2023-1893", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tained in a transparent enclosure, typically a cuboid or some regular-shaped container.\\n\\nA major challenge of this task is the severe image distortion caused by repeated light reflection and refraction at the interface of air and transparent blocks. Since it invalidates the single viewpoint assumption \\\\[9\\\\], most existing multi-view 3D reconstruction methods tend to fail.\\n\\nOne of the most relevant research problems is 3D reconstruction through refractive interfaces, known as Refractive Structure-from-Motion (RSfM) \\\\[1, 4\\\\]. In RSfM, one or more parallel interfaces between different propagation media, with different refractive indices, separate the optical system from the scene to be captured. Light passing through the optical system refracts only once at each interface causing image distortion. Unlike RSfM methods, we deal with intersecting interfaces where multiple reflections and refractions take place. Another related research topic is reconstruction of transparent objects \\\\[13, 17, 19\\\\]. In this problem, lights only get refracted twice while entering and exiting the target. In addition to the multiple refractions considered by these methods, our method also tackles the problem of multiple reflections within the transparent enclosure.\\n\\nRecently, neural implicit representation methods \\\\[21, 22, 28, 29, 31\\\\] achieve state-of-the-art performance in the task of novel view synthesis and 3D reconstruction showing promising ability to encode the appearance and geometry. However, these methods do not consider reflections. Although NeRFReN \\\\[12\\\\] attempts to incorporate the reflection effect, it cannot handle more complicated scenarios with multiple refraction and reflection.\\n\\nIn order to handle such complicated cases and make the problem tractable, we make the following two reasonable simplifications:\\n\\n\\\\begin{itemize}\\n  \\\\item[i.] Known geometry and pose of the transparent block. It is not the focus of this paper to estimate the pose of the interface plane since it is either known \\\\[1, 4\\\\] or calculated as a separate research problem \\\\[14\\\\] as is typically assumed in RSfM.\\n  \\\\item[ii.] Homogeneous background and ambient lighting. Since the appearance of transparent objects is highly affected by the background pattern, several transparent object reconstruction methods \\\\[13, 19\\\\] obtain ray correspondence with a coded background pattern. To concentrate on recovering the shape of the target object, we further assume monochromatic background with only homogeneous ambient lighting conditions similar to a photography studio.\\n\\\\end{itemize}\\n\\nTo handle both reflection and refraction in 3D reconstruction, we propose ReNeuS, a novel extension of a neural implicit representation method NeuS \\\\[31\\\\]. In lieu of dealing with the whole scene together, a divide-and-conquer strategy is employed by explicitly segmenting the scene into internal space, which is the transparent container containing the object, and an external space which is the surrounding air space. For the internal space, we use NeuS \\\\[31\\\\] to encode only the geometric and appearance information. For the external space, we assume a homogeneous background with fixed ambient lighting as described in simplification ii. In particular, we use a novel hybrid rendering strategy that combines ray tracing with volume rendering techniques to process the light interactions across the two sub-spaces.\\n\\nThe main contributions of this paper are as follows:\\n\\n\\\\begin{itemize}\\n  \\\\item We introduce a new research problem of 3D reconstruction of an object confined in a transparent enclosure, a rectangular box in this paper.\\n  \\\\item We develop a new 3D reconstruction method, called ReNeuS, that can handle multiple light reflections and refractions at intersecting interfaces. ReNeuS employs a novel hybrid rendering strategy to combine the light intensities from the segmented inner and outer sub-spaces.\\n  \\\\item We evaluate our method on both synthetic as well as a newly captured dataset that contains 10 real insect specimens enclosed in a transparent glass container.\\n\\\\end{itemize}\\n\\n2. Related work\\n\\nRefractive Structure from Motion\\n\\nThe Refractive Structure-from-Motion (RSfM) problem involves recovering 3D scene geometry from underwater images where light refraction at the interface between different propagation media causes distortions which invalidates the assumption of a single viewpoint \\\\[10\\\\] (Pinhole camera assumption fails). The refractive structure from motion method developed in \\\\[15\\\\] accurately reconstructs underwater scenes that are captured by cameras confined in an underwater glass housing. This is achieved by incorporating multiple refractions in the camera model that takes place at interfaces of varying densities of glass, air, and water. \\\\[4\\\\] presents a robust RSfM framework to estimate camera motion and 3D reconstruction using cameras with thin glass interfaces that seal the optical system. To achieve this, \\\\[4\\\\] considers the refractive interface explicitly by proposing a refractive fundamental matrix. Unlike the methods described in \\\\[4, 15\\\\], where the camera is confined in front of a piece of glass, our method works with small objects, such as insects, that are embedded within a glassy box.\\n\\nTransparent Object Reconstruction\\n\\nAcquiring the 3D geometry of transparent objects is challenging as general-purpose scanning and reconstruction techniques cannot handle specular light transport phenomena. 3D shape reconstruction of highly specular and transparent objects is...\"}"}
{"id": "CVPR-2023-1893", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. An overview of the ReNeuS framework. The scene is separated into two sub-spaces w.r.t. the interface. The internal scene is represented by two multi-layer perceptrons (MLPs) for both geometry and appearance. For neural rendering, we recursively trace a ray through the scene and collect a list of sub-rays. Our neural implicit representation makes the tracing process controllable. We show a ray tracing process with the depth of recursion $D_{rec} = 3$ here. Color accumulation is conducted on irradiance of the sub-rays in the inverse direction of ray tracing. We optimized the network by the difference between rendered image and the ground truth image. Target mesh can be extracted from SDF by Marching Cubes [18].\\n\\nThe method in [32] studies under controlled settings. The method in [32] mounts the object on a turntable. The model is optimized using the refraction normal consistency, silhouette consistency, and surface smoothness. However, this method requires a known refractive index assumption of the transparent object. [13] presents a method for reconstruction of transparent objects with unknown refractive index. This method partially immerses the object in a liquid to alter the incident light path. Then the object surface is recovered through reconstructing and triangulating such incident light paths. [19] captures fine geometric details of the transparent objects by using correspondences between the camera view rays and the locations on the static grey-coded background. Recently, [17] proposes a neural network to perform 3D reconstruction of transparent objects using a small number of mobile camera images. This is achieved by modelling the reflections and refractions in a rendering layer and a cost volume refinement layer. The rendering layer optimizes the surface normal of the object to guide the point cloud reconstruction.\\n\\nNeural 3D reconstruction\\n\\nThe use of neural networks has been widely used in recent years to predict new views and geometric representations of 3D scenes and objects based on a limited set of images captured with known camera poses. Several works exist that represent 3D structures using deep implicit representations. The main advantage over using conventional voxel grids or meshes is the fact that the implicit representations are continuous and memory efficient to model shape parts of the objects. [21] uses implicit representations of curves and surfaces of any arbitrary topology using level sets. [22] presents NeRF that synthesizes novel views of complex scenes by optimizing a continuous neural radiance field representation of a scene from a set of input images with known camera poses. [12] presents NeRFReN, which is based on NeRF and allows us to model scenes with reflections. In particular, [12] splits the scene into transmitted and reflected components and models the two components with separate neural radiance fields. Michael et al. [24] propose UNISURF, a unified framework that couples the benefits of neural implicit surface models (for surface rendering) with those of neural radiance fields (for volume rendering) to reconstruct accurate surfaces from multi-view images. However, this method is limited to representing solid and non-transparent surfaces.\\n\\nThe method presented in [33] relaxes the requirement of multi-view camera calibration by describing an end-to-end neural system that is capable of learning 3D geometry, appearance, and cameras from masked 2D images and noisy camera initialization. NeuS [31] performs high-quality reconstruction of objects with severe occlusions and complex geometries by combining radiance fields and occupancy representations. This method integrates both neural volumetric representation of NeRF and neural implicit representations using SDFs to represent the 3D surfaces. The implicit representation greatly improves the NeRF. However, NeuS cannot handle the severe image distortion created due to multiple reflections and refractions of the object placed inside a glass. Hence, our method ReNeuS modifies the NeuS [31] to overcome the problem by using a hybrid rendering strategy.\\n\\n3. Method\\n\\nGiven a set of $N$ posed images and a known transparent rectangular box geometry, we aim to recover the geometry of the internal object. As shown in Fig. 2, we proposed a neural reconstruction method called ReNeuS which extends NeuS to work on scenes with complex light refractions and reflections. To decompose such a complicated scene, we first divide it into two sub-spaces (inside and outside). Following NeuS [31], the internal space is represented by an implicit neural network which contains an implicit surface representation with signed distance function (SDF) and a volumetric appearance representation. To render a color image of the scene, we explicitly trace its ray path through the scene. For all the sub-rays in the ray path, we retrieve the\"}"}
{"id": "CVPR-2023-1893", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"radiance of sub-rays according to their location (inner or outer) and then physically accumulate them to get the color of a ray. We call this a hybrid rendering method. All of the network parameters are then optimized by minimizing the difference between the rendered image and the ground truth image.\\n\\n3.1. Neural Implicit Surface Revisited\\n\\nScene representation. Neural Implicit Surface (NeuS) \\\\cite{31} represents scenes with neural networks which contain a neural surface representation of signed distance function (SDF) and a volumetric radiance representation. Two independent multi-layer perceptrons (MLP) are used to encode the scene's geometry and appearance. To be specific, the geometric MLP: \\\\( g_{\\\\theta} : (x) \\\\rightarrow (s) \\\\) with \\\\( \\\\theta \\\\) as parameters, represent the scene as an SDF field that it takes over a point coordinate \\\\( x \\\\) and outputs its signed distance value \\\\( s \\\\). From \\\\( g_{\\\\theta}(x) \\\\), the surface of the object \\\\( S \\\\) can be extracted from a zero-level set of its SDF as:\\n\\n\\\\[\\nS = \\\\{ x \\\\in \\\\mathbb{R}^3 | g_{\\\\theta}(x) = 0 \\\\} \\\\quad (1)\\n\\\\]\\n\\nSince the appearance is view-dependent, another appearance MLP: \\\\( f_{\\\\phi} : (x, v, n) \\\\rightarrow (c) \\\\) encodes radiance information of a point \\\\( c \\\\) with its position \\\\( x \\\\), view direction \\\\( v \\\\) and the normal vector \\\\( n \\\\).\\n\\nVolume Rendering\\n\\nLet's first recall the volume rendering equation. Given a light ray \\\\( \\\\ell(t) = o + td \\\\), the color \\\\( C(\\\\ell) \\\\) is calculated by the weighted integration of radiance along the ray by:\\n\\n\\\\[\\n\\\\hat{C}(\\\\ell) = \\\\int_{t}^{t_f} \\\\int_{n} w(t) c(x(t), d) \\\\, dt \\\\quad (2)\\n\\\\]\\n\\nwhere \\\\( w(t) \\\\) is the weight at point \\\\( x(t) \\\\) and \\\\( c(x(t), d) \\\\) is the radiance emitted from point \\\\( x(t) \\\\) along the direction \\\\( d \\\\). In neural radiance field based representation \\\\cite{22}, the weight is a composition of volume density \\\\( \\\\sigma(x(t)) \\\\) and accumulated transmittance \\\\( T(t) \\\\):\\n\\n\\\\[\\nw(t) = T(t) \\\\sigma(x(t)) , \\\\quad (3)\\n\\\\]\\n\\nwhere \\\\( T(t) = \\\\exp(-\\\\int_{t}^{t_f} \\\\sigma(x(s)) \\\\, ds) \\\\) \\\\( \\\\quad (4) \\\\)\\n\\nIn order to apply volume rendering on the SDF network, Peng et al. \\\\cite{31} derives an unbiased and occlusion-aware weight function as:\\n\\n\\\\[\\nw(t) = T(t) \\\\rho(t) \\\\quad (5)\\n\\\\]\\n\\nwith the aid of an opaque density function:\\n\\n\\\\[\\n\\\\rho(t) = \\\\max(0, -d \\\\Phi_s d t(g(x(t)))) \\\\quad (6)\\n\\\\]\\n\\nwhere \\\\( \\\\Phi_s(x) = (1 + e^{-sx})^{-1} \\\\) is the Sigmoid function. Finally, the integration in Eq. (2) is estimated by using numerical quadrature \\\\cite{20} with hierarchical sampling strategy. For more detail, refer to the original papers \\\\cite{22, 31}.\\n\\n3.2. ReNeuS Scene Representation\\n\\nSince the problem origins from the heterogeneity of the scene, our solution is to separate the scene by the interface of different mediums (e.g. air/glass in our case), and obtain two sub-spaces internal \\\\( S_{in} \\\\) and external \\\\( S_{out} \\\\).\\n\\nInternal space \\\\( S_{in} \\\\). We adapt NeuS to represent the local internal space \\\\( S_{in} \\\\) which contains the target object. As described in Sec. 3.1, the appearance of \\\\( S_{in} \\\\) is represented by a volumetric radiance field. From the principle of volumetric radiance field \\\\cite{20}, light interaction (including absorbing, reflecting etc.) is explained by the probability of interaction and has been embedded in the volume rendering equation. In our case, a light ray can go straightly through \\\\( S_{in} \\\\) and its color can be retrieved by volume rendering as Eq. (2).\\n\\nExternal space \\\\( S_{out} \\\\). As our simplification ii, the external space is assumed to be empty with homogeneous ambient lighting \\\\( C_{ambient} = C_{out} \\\\).\\n\\n3.3. ReNeuS Renderer\\n\\nTo render a color image of the scene, we utilize a novel rendering strategy that combines ray tracing \\\\cite{25} and volume rendering \\\\cite{20} techniques.\\n\\nRay tracing. In computer graphics, ray tracing is the process of modelling light transport for rendering algorithms. We slightly modified the ordinary ray tracing method. In ReNeuS, the appearance of internal space \\\\( S_{in} \\\\) is represented as a volumetric radiance field. When tracing a ray through \\\\( S_{in} \\\\), we do not try to explicitly model the interaction with the target object but let the ray go straightly through the space. This allows us to get much better control of the scene and we only need to consider the light interactions (reflection and refraction) happening on the interfaces of different sub-spaces.\\n\\nTo be specific, for a ray \\\\( \\\\ell_i \\\\), we calculate the intersection of \\\\( \\\\ell_i \\\\) and the transparent box. If there is no hit, the tracing process will be terminated with only the original ray \\\\( \\\\ell_i \\\\) recorded. If \\\\( \\\\ell_i \\\\) hits the box, we explicitly trace the reflected ray \\\\( \\\\ell_r \\\\) and refracted ray \\\\( \\\\ell_t \\\\) according to the Law of Reflection and Snell's Law \\\\cite{3}, so that we have:\\n\\n\\\\[\\n\\\\ell_r, \\\\ell_t = \\\\text{RE}(\\\\ell_i, n_i) \\\\quad (7)\\n\\\\]\\n\\nwhere \\\\( n_i \\\\) is the surface normal at the intersection and \\\\( \\\\text{RE} \\\\) represents the operation of reflection and refraction. Besides, the reflectance \\\\( R_i \\\\) is calculated according to the Fresnel equations under \\\"natural light\\\" assumption \\\\cite{3} and also the transmittance \\\\( T_{re} = 1 - R_i \\\\). Note the transmittance \\\\( T_{re} \\\\) is different from \\\\( T(t) \\\\) in Eq. (4) that \\\\( T_{re} \\\\) is the transmittance.\"}"}
