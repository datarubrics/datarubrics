{"id": "CVPR-2024-74", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. BEiT: BERT Pre-Training of Image Transformers. In ICLR, 2022.\\n\\n[2] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token Merging: Your ViT But Faster. In ICLR, 2023.\\n\\n[3] Maxim Bonnaerens and Joni Dambre. Learned Thresholds Token Merging and Pruning for Vision Transformers. TMLR, 2023.\\n\\n[4] Holger Caesar, Jasper Uijlings, and Vittorio Ferrari. COCO-Stuff: Thing and Stuff Classes in Context. In CVPR, 2018.\\n\\n[5] Qingqing Cao, Bhargavi Paranjape, and Hannaneh Hajishirzi. PuMer: Pruning and Merging Tokens for Efficient Vision Language Models. In ACL, 2023.\\n\\n[6] Shuning Chang, Pichao Wang, Ming Lin, Fan Wang, David Junhao Zhang, Rong Jin, and Mike Zheng Shou. Making Vision Transformers Efficient from A Token Sparsification View. In CVPR, 2023.\\n\\n[7] Mengzhao Chen, Mingbao Lin, Ke Li, Yunhang Shen, Yongjian Wu, Fei Chao, and Rongrong Ji. CF-ViT: A General Coarse-to-Fine Method for Vision Transformer. In AAAI, 2023.\\n\\n[8] Mengzhao Chen, Wenqi Shao, Peng Xu, Mingbao Lin, Kaipeng Zhang, Rongrong Ji Fei Chao, Yu Qiao, and Ping Luo. DiffRate: Differentiable Compression Rate for Efficient Vision Transformers. In ICCV, 2023.\\n\\n[9] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision Transformer Adapter for Dense Predictions. In ICLR, 2023.\\n\\n[10] Bowen Cheng, Ishan Misra, Alexander G. Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention Mask Transformer for Universal Image Segmentation. In CVPR, 2022.\\n\\n[11] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The Cityscapes Dataset for Semantic Urban Scene Understanding. In CVPR, 2016.\\n\\n[12] Evann Courdier, Prabhu Teja Sivaprasad, and Fran\u00e7ois Fleuret. PAUMER: Patch Pausing Transformer for Semantic Segmentation. In BMVC, 2022.\\n\\n[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR, 2021.\\n\\n[14] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. EV A: Exploring the Limits of Masked Visual Representation Learning at Scale. In CVPR, 2023.\\n\\n[15] Mohsen Fayyaz, Soroush Abbasi Kouhpayegani, Farnoush Rezaei Jafari, Eric Sommerlade, Hamid Reza Vaezi Joze, Hamed Pirsiavash, and Juergen Gall. Adaptive token sampling for efficient vision transformers. ECCV, 2022.\\n\\n[16] Joakim Bruslund Haurum, Sergio Escalera, Graham W. Taylor, and Thomas B. Moeslund. Which Tokens to Use? Investigating Token Reduction in Vision Transformers. In ICCV Workshops, 2023.\\n\\n[17] Jung Hwan Heo, Arash Fayyazi, Mahdi Nazemi, and Masoud Pedram. A Fast Training-Free Compression Framework for Vision Transformers. arXiv preprint arXiv:2303.02331, 2023.\\n\\n[18] Huaibo Huang, Xiaoqiang Zhou, Jie Cao, Ran He, and Tie-niu Tan. Vision Transformer with Super Token Sampling. In CVPR, 2023.\\n\\n[19] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander C. Berg, Wan-Yen Lo, Piotr Doll\u00e1r, and Ross Girshick. Segment Anything. In ICCV, 2023.\\n\\n[20] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Wei Niu, Mengshu Sun, Xuan Shen, Geng Yuan, Bin Ren, Hao Tang, et al. SPViT: Enabling Faster Vision Transformers via Latency-Aware Soft Token Pruning. In ECCV, 2022.\\n\\n[21] Jin Li, Yaoming Wang, Xiaopeng Zhang, Bowen Shi, Dongsheng Jiang, Chenglin Li, Wenrui Dai, Hongkai Xiong, and Qi Tian. AiluRus: A Scalable ViT Framework for Dense Prediction. In NeurIPS, 2023.\\n\\n[22] Ling Li, David Thorsley, and Joseph Hassoun. SaiT: Sparse Vision Transformers through Adaptive Token Pruning. arXiv preprint arXiv:2210.05832, 2022.\\n\\n[23] Weicong Liang, Yuhui Yuan, Henghui Ding, Xiao Luo, Weihong Lin, Ding Jia, Zheng Zhang, Chao Zhang, and Han Hu. Expediting Large-Scale Vision Transformer for Dense Prediction without Fine-tuning. In NeurIPS, 2022.\\n\\n[24] Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Not All Patches are What You Need: Expediting Vision Transformers via Token Reorganizations. In ICLR, 2022.\\n\\n[25] Yifei Liu, Mathias Gehrig, Nico Messikommer, Marco Cannici, and Davide Scaramuzza. Revisiting Token Pruning for Object Detection and Instance Segmentation. In WACV, 2024.\\n\\n[26] Yuang Liu, Qiang Zhou, Jing Wang, Fan Wang, Jun Wang, and Wei Zhang. Dynamic Token-Pass Transformers for Semantic Segmentation. In WACV, 2024.\\n\\n[27] Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, Furu Wei, and Baining Guo. Swin Transformer V2: Scaling Up Capacity and Resolution. In CVPR, 2022.\\n\\n[28] Sifan Long, Zhen Zhao, Jimin Pi, Shengsheng Wang, and Jingdong Wang. Beyond Attentive Tokens: Incorporating Token Importance and Diversity for Efficient Vision Transformers. In CVPR, 2023.\\n\\n[29] Chenyang Lu, Daan de Geus, and Gijs Dubbelman. Content-aware Token Sharing for Efficient Semantic Segmentation with Vision Transformers. In CVPR, 2023.\"}"}
{"id": "CVPR-2024-74", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Adaptive Vision Transformers for Efficient Image Recognition. In CVPR, 2022.\\n\\nRoozbeh Mottaghi, Xianjie Chen, Xiaobai Liu, Nam-Gyu Cho, Seong-Whan Lee, Sanja Fidler, Raquel Urtasun, and Alan Yuille. The Role of Context for Object Detection and Semantic Segmentation in the Wild. In CVPR, 2014.\\n\\nZhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye, and Furu Wei. BEiT v2: Masked Image Modeling with Vector-Quantized Visual Tokenizers. arXiv preprint arXiv:2208.06366, 2022.\\n\\nYongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. DynamicViT: Efficient Vision Transformers with Dynamic Token Sparsification. In NeurIPS, 2021.\\n\\nCedric Renggli, Andre Susano Pinto, Neil Houlsby, Basil Mustafa, Joan Puigcerver, and Carlos Riquelme. Learning to Merge Tokens in Vision Transformers. arXiv preprint arXiv:2202.12015, 2022.\\n\\nMeta Research. fvcore. https://github.com/facebookresearch/fvcore, 2023.\\n\\nMichael S. Ryoo, AJ Piergiovanni, Anurag Arnab, Mostafa Dehghani, and Anelia Angelova. TokenLearner: What Can Learned Tokens Do for Images and Videos? In NeurIPS, 2021.\\n\\nCai Jianfei Shi Hengcan, Hayat Munawar. Transformer Scale Gate for Semantic Segmentation. In CVPR, 2023.\\n\\nRobin Strudel, Ricardo Garcia, Ivan Laptev, and Cordelia Schmid. Segmenter: Transformer for Semantic Segmentation. In ICCV, 2021.\\n\\nQuan Tang, Bowen Zhang, Jiajun Liu, Fagiu Liu, and Yifan Liu. Dynamic Token Pruning in Plain Vision Transformers for Semantic Segmentation. In ICCV, 2023.\\n\\nHongjie Wang, Bhishma Dedhia, and Niraj K. Jha. Zero-TPrune: Zero-Shot Token Pruning through Leveraging of the Attention Graph in Pre-Trained Transformers. arXiv preprint arXiv:2305.17328, 2023.\\n\\nWenhui Wang, Hangbo Bao, Li Dong, Johan Bjorck, Zhiliang Peng, Qiang Liu, Kriti Aggarwal, Owais Khan Mohammed, Saksham Singhal, Subhojit Som, and Furu Wei. Image as a Foreign Language: BEiT Pretraining for Vision and Vision-Language Tasks. In CVPR, 2023.\\n\\nYulin Wang, Rui Huang, Shiji Song, Zeyi Huang, and Gao Huang. Not All Images are Worth 16x16 Words: Dynamic Transformers for Efficient Image Recognition. In NeurIPS, 2021.\\n\\nSiyuan Wei, Tianzhu Ye, Shen Zhang, Yao Tang, and Jiajun Liang. Joint Token Pruning and Squeezing Towards More Aggressive Compression of Vision Transformers. In CVPR, 2023.\\n\\nXinjian Wu, Fanhu Zeng, Xiudong Wang, Yunhe Wang, and Xinghao Chen. PPT: Token Pruning and Pooling for Efficient Vision Transformers. arXiv preprint arXiv:2310.01812, 2023.\\n\\nTete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified Perceptual Parsing for Scene Understanding. In ECCV, 2018.\\n\\nEnze Xie, Wenhai Wang, Zhiding Yu, Anima Anandkumar, Jose M Alvarez, and Ping Luo. SegFormer: Simple and Efficient Design for Semantic Segmentation with Transformers. In NeurIPS, 2021.\\n\\nJiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. GroupViT: Semantic Segmentation Emerges from Text Supervision. In CVPR, 2022.\\n\\nHongxu Yin, Arash Vahdat, Jose Alvarez, Arun Mallya, Jan Kautz, and Pavlo Molchanov. A-ViT: Adaptive Tokens for Efficient Vision Transformer. In CVPR, 2022.\\n\\nLu Yu and Wei Xiang. X-Pruner: eXplainable Pruning for Vision Transformers. In CVPR, 2023.\\n\\nWang Zeng, Sheng Jin, Wentao Liu, Chen Qian, Ping Luo, Wanli Ouyang, and Xiaogang Wang. Not All Tokens Are Equal: Human-centric Visual Analysis via Token Clustering Transformer. In CVPR, 2022.\\n\\nBowen Zhang, Zhi Tian, Quan Tang, Xiangxiang Chu, Xiaolin Wei, Chunhua Shen, et al. SegViT: Semantic Segmentation with Plain Vision Transformers. In NeurIPS, 2022.\\n\\nSixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip H.S. Torr, and Li Zhang. Rethinking Semantic Segmentation from a Sequence-to-Sequence Perspective with Transformers. In CVPR, 2021.\\n\\nBolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. Scene Parsing through ADE20K Dataset. In CVPR, 2017.\\n\\nZhuofan Zong, Kunchang Li, Guanglu Song, Yali Wang, Yu Qiao, Biao Leng, and Yu Liu. Self-slimmed Vision Transformer. In ECCV, 2022.\"}"}
{"id": "CVPR-2024-74", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This work presents Adaptive Local-then-Global Merging (ALGM), a token reduction method for semantic segmentation networks that use plain Vision Transformers. ALGM merges tokens in two stages: (1) In the first network layer, it merges similar tokens within a small local window and (2) halfway through the network, it merges similar tokens across the entire image. This is motivated by an analysis in which we found that, in those situations, tokens with a high cosine similarity can likely be merged without a drop in segmentation quality. With extensive experiments across multiple datasets and network configurations, we show that ALGM not only significantly improves the throughput by up to 100%, but can also enhance the mean IoU by up to +1.1, thereby achieving a better trade-off between segmentation quality and efficiency than existing methods. Moreover, our approach is adaptive during inference, meaning that the same model can be used for optimal efficiency or accuracy, depending on the application. Code is available at https://tue-mps.github.io/ALGM.\\n\\n1. Introduction\\n\\nVision Transformers (ViTs) have shown to be very effective for image segmentation tasks [9, 10, 19, 37, 38, 46, 47, 51, 52]. However, the computational complexity of the multi-head self-attention operation scales quadratically with the number of input pixels. This harms the computational efficiency, especially on the high-resolution images that are typically used for image segmentation. To alleviate this burden and improve the efficiency, several works have proposed methods to reduce the number of tokens that the ViT has to process. Most token reduction methods have been introduced for image classification [2, 3, 7, 8, 15\u201318, 20, 22, 24, 28, 30, 33, 36, 40, 42\u201344, 48\u201350, 54], but there is also an increasing amount of work that focuses on tasks like semantic segmentation [12, 21, 23, 25, 26, 29, 39]. In this work, we also focus on semantic segmentation, and aim to design a token reduction method that achieves a better balance between efficiency and segmentation quality than existing works.\\n\\nThis objective is motivated by the limitations of existing works. First, token pruning methods [15, 30, 33, 48], which are popular for image classification, discard uninformative tokens. They are not directly applicable to semantic segmentation, as each token requires a prediction. To overcome this, token pausing or halting methods [12, 26, 39] retain discarded tokens and aggregate them with the other tokens at the end of the ViT. However, these methods observe a drop in segmentation quality, possibly because useful information contained in the halted tokens is not available in later network layers. Alternatively, token sharing and merging methods avoid discarding tokens, and represent multiple image patches or tokens with a smaller set of tokens [21, 23, 29]. This approach allows these methods to maintain the segmentation quality, but requires them to introduce additional computational overhead to identify tokens for sharing or merging, and they apply token reduction only once, limiting the efficiency gain. Furthermore, token merging methods that have been designed for image classification yield a notable decline in segmentation quality.\\n\\nFigure 1. Efficiency and segmentation quality for ALGM, applied to Segmenter [38], SegViT [51], and SETR [52] on ADE20K. On average, ALGM improves the throughput of these baselines by 39%, while improving the mIoU by +0.7.\"}"}
{"id": "CVPR-2024-74", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ity when applied to semantic segmentation [2, 23, 24].\\n\\nBased on these existing works, we make two observations:\\n\\n(a) CTS [29] demonstrates that local token sharing in early network stages enhances efficiency without compromising segmentation quality, but it inefficiently requires a pre-processing network. Therefore, our first objective is to merge redundant tokens early in the network without requiring pre-processing, and still maintain the segmentation quality.\\n\\n(b) Token merging approaches like ToMe [2] show that gradually merging redundant tokens across the entire image (i.e., globally) can greatly boost the efficiency, but at the cost of segmentation quality. Thus, our second objective is to also apply global token merging to further improve efficiency, but without harming the segmentation quality.\\n\\nTo achieve these objectives, we need to find an efficient method to identify tokens that can be merged without causing a drop in segmentation quality. Inspired by existing token merging methods [2, 28, 43], in Sec. 3.2, we explore if the cosine similarity is a suitable measure to identify mergeable tokens. Concretely, we compare the cosine similarities between tokens representing the same category \u2013 i.e., intra-class tokens, which are potentially redundant and can be merged \u2013 and tokens representing different categories \u2013 i.e., inter-class tokens, which should not be merged. We find that:\\n\\n(a) already in the 1st network layer, the similarities between intra-class tokens in small local windows are much higher than for inter-class tokens, and\\n\\n(b) comparing tokens globally, intra-class token similarities become increasingly higher than inter-class similarities in later layers.\\n\\nBased on these new findings, we present our Adaptive Local-then-Global Merging (ALGM) module that integrates two token merging phases. In the first network layer, ALGM adopts a local merging strategy. This is followed by a global merging mechanism in an intermediate layer, to also reduce global token redundancies. Moreover, rather than using a predetermined number of merged tokens, our approach dynamically decides the number of merged tokens based on the semantic complexity of the image content. Finally, we restore the original token resolution to make segmentation predictions. For details, see Sec. 3.3.\\n\\nALGM offers multiple advantages.\\n\\n(a) Unlike methods that use token pausing, redundant tokens remain active in the network, and continue to contribute in subsequent network layers via their merged representation.\\n\\n(b) ALGM avoids the need for preprocessing layers and the significant overhead associated with token sharing or merging methods.\\n\\n(c) Global merging is only applied when token similarity is sufficiently reflective of category similarity, reducing the chance of merging tokens that should remain separate.\\n\\n(d) Being a parameter-free approach, the ALGM module is naturally compatible with all plain ViT backbones, as well as any segmentation decoder, with or without re-training.\\n\\nThrough experiments outlined in Sec. 4, we demonstrate that ALGM consistently enhances the throughput by considerable margins when applied to a wide range of different segmentation methods (see Fig. 1). Moreover, we observe that, on top of this improved efficiency, ALGM also enhances the segmentation quality. From an investigation into the cause of this improvement, we find that it can be attributed to two factors: a better balance between frequent and infrequent categories in the self-attention operation, and the denoising of tokens. For more results, we refer to Sec. 5.\\n\\nWe summarize our main contributions as follows:\\n\\n\u2022 A generally applicable token merging framework that integrates local and global merging, enhancing both the efficiency and segmentation quality of ViT-based semantic segmentation networks.\\n\\n\u2022 An analysis of similarities between intra- and inter-class tokens, within local windows and across network layers.\\n\\n\u2022 An exploration of the cause of the segmentation quality improvement obtained by ALGM.\\n\\n2. Related work\\n\\nSince the introduction of the Vision Transformer (ViT), a substantial amount of work has been dedicated to improving the efficiency of these ViTs. In this work, we focus on token reduction, which aims to decrease the number of tokens that are processed by the ViT, to improve efficiency.\\n\\nToken reduction in general. The majority of token reduction methods have been introduced for ViTs that conduct image classification. Some methods use a token pruning strategy, where uninformative tokens are identified and simply discarded [15, 20, 24, 30, 33, 40, 48]. Uninformative tokens are identified by making intermediate predictions with auxiliary heads [20, 30, 33], or obtaining importance scores from attention weights [15, 24, 40]. Pruned tokens can be discarded completely [15, 30, 33, 48] or fused into one token to preserve information flow [20, 24]. While token pruning can notably enhance the throughput of transformers, discarding tokens is not possible for semantic segmentation as each token requires a prediction, and fused tokens representing multiple regions or categories cannot be trivially reconstructed to make a semantic segmentation prediction. Alternatively, token merging methods combine groups of tokens into a smaller set of representative tokens. Some works introduce a learned layer to map the original token set to a smaller one [6, 34, 36, 54], while most methods merge tokens if they have a high similarity score [2, 3, 5, 8, 17, 28, 43]. Other methods combine different merging, pruning or fusing approaches [3, 5, 8, 28, 44].\\n\\nToken reduction for semantic segmentation. Some token merging methods for image classification can also be applied to semantic segmentation [2, 6, 17, 24, 28] by reconstructing merged tokens to their original positions to make a prediction. However, while these approaches improve efficiency, they consistently cause a drop in segmentation quality.\"}"}
{"id": "CVPR-2024-74", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tation quality, as also shown by Liang et al. [23].\\n\\nOther token reduction methods have been proposed specifically for the semantic segmentation task. Token pausing or halting approaches [12, 26, 39] identify tokens that produce high-confidence predictions in early network layers, and do not process them any further. Instead of discarding tokens like pruning methods, they retain tokens and reconstruct them later to make a final prediction. However, in subsequent layers, these \u2018paused\u2019 tokens do not participate in the self-attention operation anymore, meaning that potentially useful information is no longer available, which negatively affects the segmentation quality. Alternatively, ELViT [23] and AiluRus [21] introduce a non-parametric token clustering layer that merges redundant neighboring tokens in one network layer. These methods are able to reduce tokens while maintaining the segmentation quality, but their efficiency gain is limited. It is likely that this is because their clustering layer introduces computational overhead, and because they reduce tokens only once, not using the token redundancies potentially present in other layers. CTS [29] uses a CNN-based policy network to identify image patches that can share a token before the first transformer layer. This approach can also reduce tokens while maintaining segmentation quality, but its policy network introduces computational overhead, and it only merges tokens in local windows, ignoring global redundancies.\\n\\nInspired by the advantages and limitations of existing work, this work proposes a parameter-free token merging method for semantic segmentation that applies both local and global merging based on cosine similarities between tokens. Importantly, with minimal computational overhead, we identify where cosine similarities can be used to select tokens for merging while maintaining or even improving the segmentation quality.\\n\\n3. Method\\n\\n3.1. Preliminaries\\n\\nA ViT-based segmentation architecture typically consists of two subnetworks:\\n\\n(a) An encoder $E$: $I_{L_1}, \\\\ldots, I_{L_{L-1}} \\\\rightarrow T_{L}$, which uses $L$ distinct transformer layers to map the input image $I$ to $T_L$, the set of tokens representing the image content at the final layer $L$. The encoder $E$ first splits the input image $I \\\\in \\\\mathbb{R}^{3 \\\\times H \\\\times W}$ into $N = H \\\\times W$ non-overlapping patches, determined by a patch size $p$. Following patch embedding and positional encoding steps, an initial set of token embeddings $T_0 = \\\\{t_1, \\\\ldots, t_N\\\\}$ is obtained. Each token $t_i$ belongs to $\\\\mathbb{R}^d$, with $d$ denoting the feature dimension. These token embeddings are subsequently processed by transformer layers $L = \\\\{L_1, \\\\ldots, L_{L-1}\\\\}$, resulting in the output $T_L$. Each layer $L_l$ in $L$ integrates a multi-head self-attention (MHSA) block followed by a multi-layer perceptron (MLP) block which output $T_{l-1} = MHSA(T_l-1) + T_l-1$.\\n\\n(b) A decoder $D$: $T_L \\\\rightarrow P$, which utilizes transformer or convolutional layers to process $T_L$ and generate the per-pixel segmentation prediction $P$, where $P \\\\in \\\\mathbb{R}^{C \\\\times H \\\\times W}$ and $C$ represents the number of classes. Our primary objective is to reduce the number of tokens in $T$, the total set of tokens, by identifying those tokens that can be merged without adversely affecting the segmentation quality of $P$.\\n\\n3.2. Token similarity analysis\\n\\nAs highlighted in Sec. 1, considering the advantages and limitations of existing methods, our goal is to find a method to (a) apply early local token merging without requiring a pre-processing network and (b) also apply global token merging to further improve efficiency, without harming the segmentation quality. To achieve this, we need to find a method that can efficiently identify tokens suitable for merging while preserving the segmentation quality. CTS [29] is based on the hypothesis that tokens that represent the same semantic class can be merged without compromising segmentation quality, since they carry redundant information. On the other hand, several token merging methods for image classification [2, 28, 43] merge tokens with a high cosine similarity. They get promising efficiency gains, but sometimes at the cost of accuracy. This motivates us to examine if and when cosine similarity can be an effective metric to identify tokens that represent the same category and are thus suitable for both local and global merging.\\n\\nTo analyze this, we extract and compare the similarities between tokens from Segmenter with ViT-S [13, 38] trained on the ADE20K [53] training set. (1) We first analyze the local similarities within $k \\\\times k$ windows in the first transformer layer. We calculate the cosine similarity between tokens representing different categories (i.e., inter-class tokens), which should not be merged, and tokens representing the same category (i.e., intra-class tokens), which can be merged. As illustrated in Fig. 2a, the smaller the window size $k$, the more accurately cosine similarity reflects that tokens depict the same category. Consequently, tokens\"}"}
{"id": "CVPR-2024-74", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"3.3. Adaptive Local-then-Global Merging\\n\\nFrom the analysis we know that (a) local token similarities in early layers and (b) global token similarities in intermediate layers are likely good indicators of the merge-ability of tokens. To exploit this, we propose the Adaptive Local-then-Global Merging (ALGM) approach. As demonstrated in Fig. 3, the process begins with local merging in the first layer using the Conditional Local Average Pooling (CLAP) module. In an intermediate layer, we adopt the Global Bipartite Merging (GBM) module which is based on the BSM [2] algorithm for global merging. The procedure concludes with a token unmerging module to restore the original token resolution.\\n\\nLocal token merging.\\n\\nGiven the insights from Sec. 3.2, we aim to merge tokens in the first layer if they have a high similarity with neighboring tokens in a small window. To implement this, we introduce the CLAP module, which is positioned in layer $L_1$, between the MHSA and MLP blocks, as illustrated in Fig. 3. The CLAP module follows these steps: (1) It receives the token embeddings $T'_1 \\\\in \\\\mathbb{R}^{N \\\\times d}$ from layer $L_1$ and reshapes them into a grid $T'_G_1 \\\\in \\\\mathbb{R}^{H_p \\\\times W_p \\\\times d}$. It then defines a window of size $k \\\\times k$ and groups the tokens within each window into separate sets $W = \\\\{w_1, \\\\ldots, w_s\\\\}$, where $s = N^2 / k^2$. Each $w \\\\in \\\\mathbb{R}^{k \\\\times k \\\\times d}$ is a set of token embeddings, represented as $w = \\\\{t_1, t_2, \\\\ldots, t_{k^2}\\\\}$. (2) Subsequently, for each $w$ in $W$, it computes the cosine similarity between all pairs of tokens $t_i$ in $w$, and calculates the mean of these similarities to get $\\\\mu_w$. Then, as we hypothesize that the similarity between tokens represents the mergeability, CLAP merges only the tokens in windows $w$ for which $\\\\mu_w > \\\\tau$, where $\\\\tau$ is an automatically determined similarity threshold, which is elaborated later in this section. (3) Finally, the tokens inside selected windows $w$ are merged into a single token by taking the average of these tokens. The indices of these tokens are also stored for later unmerging. Once completed, merged and non-merged tokens are concatenated to produce the output token embeddings $T''_1 = \\\\{t_1, \\\\ldots, t_{N'}\\\\}$ where $N' \\\\leq N$. \\n\\nGlobal token merging.\\n\\nAfter local merging, the tokens are processed through standard transformer layers up\"}"}
{"id": "CVPR-2024-74", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"to layer $L_m$, where the GBM module is applied. Similar to the steps of the BSM [2] algorithm: (1) In the first step, the tokens in $T'_m$ are split into two sets: $A = \\\\{t_1, t_3, \\\\ldots, t_{N'_m-1}\\\\}$ and $B = \\\\{t_2, t_4, \\\\ldots, t_{N'_m}\\\\}$. A fully-connected bipartite graph is then constructed between the tokens within these sets based on their cosine similarity. (2) Then, GBM only retains the edges that represent the highest similarity from a token in set $A$ to any token in set $B$. This means that for a given token $A_i$, the edge to token $B_j$ is only retained if $B_j$ is the most similar token to $A_i$ when compared to all other tokens in set $B$. (3) Next, unlike the original BSM which employs a constant number of merged tokens, GBM uses a similarity threshold $\\\\tau$. Thus, edges are only retained if their similarity exceeds $\\\\tau$. (4) Finally, the tokens with remaining edges are merged by taking their average, and their indices are stored for future unmerging. The two sets are then concatenated, yielding the output embeddings $T''_m = \\\\{t_1, \\\\ldots, t_{N''_m}\\\\}$ where $N''_m \\\\leq N'_m$.\\n\\n**Token unmerging.** Upon completing global merging, the embeddings $T''_m$ are processed through the remaining $L-m$ transformer layers, resulting in the final token embeddings $T_{L_m}$. These embeddings, along with the indices of merged tokens retained during the merging phases, are provided as inputs to the decoder $D$. In this phase, we deploy an unmerging module that duplicates the embeddings of the merged tokens at the indices of the tokens from which they were merged. For transformer-based decoders, which are designed to handle tokens, the unmerging module is applied after the decoder. Conversely, for CNN-based decoders, which require spatially organized features, token unmerging is executed prior to the decoder.\\n\\n**Adaptive token merging.** As the complexity of images varies, reducing a constant number of tokens can lead to a suboptimal efficiency or segmentation quality. Merging too many tokens in complex images can lead to insufficient representation of their complexity. Conversely, simpler images can benefit from a more reduced token count, enhancing efficiency. To tackle this challenge, we introduce an adaptive method that automatically determines a similarity threshold. Before training, we take the base segmentation model to which we want to apply ALGM, and then run inference on the training set. We then extract the tokens after the MHSA block in each layer $L_l$, calculate the cosine similarities between all token pairs, and compute the mean $\\\\mu$ and standard deviation $\\\\sigma$ of these similarities across the entire training set. Given these statistics, we then set the threshold $\\\\tau = \\\\mu + \\\\sigma$ to merge token pairs with above-average similarities. Using this threshold, the number of remaining tokens $N'_m$ and $N''_m$ after the CLAP and GBM modules will vary per image. During training, to facilitate batching of images and tokens, we take the maximum number of remaining tokens $N'_m$ and $N''_m$ per batch, and apply this to all images in the batch.\\n\\n### 4. Experimental setup\\n\\n**Datasets.** We conduct our main experiments on ADE20K [53], which is widely recognized as a challenging scene parsing dataset. Additionally, we show ALGM\u2019s general applicability on the Pascal Context [31], Cityscapes [11], and COCO-Stuff-10K [4] datasets.\\n\\n**Implementation details.** ALGM can be applied to any segmentation model that uses plain ViTs. We apply it to three popular networks: Segmenter [38], SETR [52], and SegViT [51] using four standard ViT backbones [13]: ViT-T/S/B/L. For a fair comparison, we train all networks using the original hyperparameters and official implementations. By default, we integrate our CLAP and GBM modules at the 1st and 5th layers for ViT-T/S/B models, and at the 1st and 7th layers for ViT-L, using the automatically generated threshold $\\\\tau$ for both merging phases. For more details, see the supplementary material.\\n\\n**Evaluation metrics.** To assess the segmentation quality, we use the standard mean Intersection-over-Union (mIoU) metric, and for computational efficiency we evaluate the throughput in terms of images per second (im/sec) and the number of floating point operations (FLOPs). For the throughput, we calculate the average im/sec on the validation set with a batch size of 32 on an Nvidia A100 GPU, after a 50-iteration warmup. To calculate the number of FLOPs, we use fvcore [35] and compute the average number of operations over all images in the validation set. We report the number of GFLOPs, i.e., $\\\\text{FLOPs} \\\\times 10^9$.\\n\\n### 5. Experimental results\\n\\n#### 5.1. Main results\\n\\nTo evaluate the effectiveness of ALGM, we apply it to several ViT-based segmentation networks and compare it with existing state-of-the-art token reduction methods. For each existing method, we report the version with the highest efficiency while maintaining the segmentation quality as much as possible. For a more comprehensive analysis across various settings, additional results are provided in the supplementary material. For our ALGM, we report two versions: (1) the default ALGM, which uses the automatic threshold during both training and inference, and (2) ALGM*, which is the same trained model, but during inference it uses the smallest threshold $\\\\tau$ for which the mIoU is higher than the baseline. In other words, ALGM* is tuned for optimal efficiency while maintaining the segmentation quality. See Sec. 5.4 and the supplementary material for more details on the use of different merging thresholds.\\n\\n**ADE20K.** Tab. 1 presents the results of ALGM and existing token reduction methods for different segmentation models and ViT backbones on the ADE20K dataset [53]. We find that, across all settings, ALGM is able to considerably improve the throughput and number of GFLOPs with $15777$.\"}"}
{"id": "CVPR-2024-74", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method          | Seg-T | Seg-S | Seg-B | Seg-L | SegViT-L | SETR-L |\\n|-----------------|-------|-------|-------|-------|----------|--------|\\n|                | 38.1  | 45.3  | 48.5  | 51.8  | 54.5     |        |\\n| + CTS [29]      | 38.2  | 45.1  | 48.7  | 51.8  |          |        |\\n| + ToMe [2]      | 37.9  | 45.1  | 48.5  | 51.6  | 51.4     |        |\\n| ALGM (ours)     |       |       |       |       | 49.4     | 48.4   |\\n| ALGM* (ours)    |       |       |       |       | 48.5     | 48.1   |\\n| Seg-T [38]      | 38.1  | 45.3  | 48.5  | 51.8  | 54.5     |        |\\n| + CTS [29]      | 38.2  | 45.1  | 48.7  | 51.8  |          |        |\\n| + ToMe [2]      | 37.9  | 45.1  | 48.5  | 51.6  | 51.4     |        |\\n| ALGM (ours)     |       |       |       |       | 49.4     | 48.4   |\\n| ALGM* (ours)    |       |       |       |       | 48.5     | 48.1   |\\n| Seg-S [38]      | 45.3  | 45.1  | 48.5  | 51.8  | 54.5     |        |\\n| + CTS [29]      | 45.1  | 45.1  | 48.7  | 51.8  |          |        |\\n| + ToMe [2]      | 45.1  | 45.1  | 48.5  | 51.6  | 51.4     |        |\\n| ALGM (ours)     |       |       |       |       | 46.4     | 46.4   |\\n| ALGM* (ours)    |       |       |       |       | 45.5     | 45.5   |\\n| Seg-B [38]      | 48.5  | 48.7  | 48.5  | 51.8  | 54.5     |        |\\n| ALGM (ours)     |       |       |       |       | 49.4     | 49.4   |\\n| ALGM* (ours)    |       |       |       |       | 48.5     | 48.5   |\\n| Seg-L [38]      | 51.8  | 51.8  | 51.8  | 51.8  | 54.5     |        |\\n| + CTS [29]      | 51.8  | 51.8  | 51.8  | 51.8  |          |        |\\n| + ToMe [2]      | 51.6  | 51.8  | 51.6  | 51.6  | 51.6     |        |\\n| ELViT [23]      | 51.4  | 51.4  | 51.4  | 51.4  |          |        |\\n| + ELViT [23]    | 51.9  | 51.9  | 51.9  | 51.9  |          |        |\\n| AiluRus [21]    | 52.2  | 52.2  | 52.2  | 52.2  |          |        |\\n| ALGM (ours)     |       |       |       |       | 52.7     | 52.7   |\\n| ALGM* (ours)    |       |       |       |       | 51.9     | 51.9   |\\n\\nTable 1. Main results on ADE20K.\\n\\nALGM applied to Segmenter (Seg) [38], SegViT [51], and SETR [52] across 4 ViT backbones.\\n\\nALGM* is the same trained model as ALGM, but uses the threshold $\\\\tau$ during inference that achieves the best efficiency while maintaining the mIoU w.r.t. the baseline.\\n\\n\u2021 Indicates a training-free method, applied directly to the baseline model.\\n\\nIn Sec. 5.4, we provide a more detailed analysis into the cause of this mIoU improvement. The ALGM* variant, which is optimized for efficiency, is able to improve the throughput even further (up to +100% for Seg-L), while consistently achieving an mIoU that is the same or slightly higher than the base networks.\\n\\nMoreover, ALGM and ALGM* consistently outperform all existing token reduction works, in terms of both the mIoU and the efficiency metrics. This shows that our method can find a better balance between segmentation quality and efficiency, which is the objective of this work.\\n\\nOther datasets.\\n\\nWhen applying ALGM to COCO-Stuff, Cityscapes and Pascal-Context [4, 11, 31] in Tab. 2, we observe very similar results as for ADE20K. For all datasets, our default ALGM significantly improves the throughput with respect to the base segmentation networks, while also obtaining a better segmentation quality. Again, ALGM* can improve the throughput even further, obtaining throughput improvements of +90% on Cityscapes for Seg-S and +72% on COCO-Stuff for Seg-L without any drop in segmentation quality.\\n\\nOn the COCO-Stuff and Pascal-Context datasets, ALGM and ALGM* also consistently outperform all existing methods. For Cityscapes, ALGM outperforms ToMe [2], AiluRus [21], and ELViT [23], but it does not achieve the same efficiency improvements as CTS [29]. We observe that the visual homogeneity of the Cityscapes images causes tokens to be similar in the first transformer layer even if they do not belong to the same category, requiring a higher merging threshold and limiting the efficiency improvement.\\n\\nOverall, taking into account all datasets, we can conclude that ALGM is a robust and generally applicable method that consistently improves the efficiency of ViT-based segmentation models while also enhancing their accuracy.\\n\\nComparison with other works.\\n\\nIn Tab. 3, we compare ALGM to DToP [39] and DoViT [26]. However, these works report mIoU and GFLOPs results without token reduction that differ from the results we obtain from the official code of SETR [52] and Segmenter [38], while DToP and DoViT do not release their code. Therefore, we can only compare the relative performance differences obtained due to token reduction. In Tab. 3, we observe that DToP can maintain the mIoU while reducing the GFLOPs by 25%, whereas ALGM* maintains the mIoU and reduces the GFLOPs by 30%. Compared to DoViT, ALGM considerably improves both the segmentation quality and efficiency. For further comparisons, see the supplementary material.\\n\\n5.2. Application to state-of-the-art model\\n\\nTo demonstrate the effectiveness of ALGM on a large-scale state-of-the-art network, we apply it to the EVA backbone [14] with a ViT-Adapter + Mask2Former decoder [9, 10]. Here, we calculate the average throughput on 4 Nvidia A6000 GPUs due to memory requirements. The results in Tab. 4 show that without training, we can improve the throughput by 26% while keeping the mIoU constant. When we also train the model, we achieve the same efficiency gains but now also further improve the mIoU with +0.2. These results show the general effectiveness and compatibility of ALGM with large-scale pre-trained networks.\\n\\n5.3. Ablations\\n\\nCLAP module window size.\\n\\nIn Tab. 5a, we evaluate the effect of using different window sizes for our CLAP module. We find that smaller window sizes yield higher mIoU scores, whereas larger window sizes result in a better efficiency. This is as expected, as we have found in Fig. 2a.\"}"}
{"id": "CVPR-2024-74", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method          | mIoU (%) | Im/sec | GFLOPs |\\n|-----------------|----------|--------|--------|\\n| Seg-S [38]      |          |        |        |\\n| + CTS [29]      |          |        |        |\\n| + ALGM (ours)   |          |        |        |\\n| + ALGM* (ours)  |          |        |        |\\n| Seg-B [38]      |          |        |        |\\n| + CTS [29]      |          |        |        |\\n| + ALGM (ours)   |          |        |        |\\n| + ALGM* (ours)  |          |        |        |\\n| Seg-L [38]      |          |        |        |\\n| + CTS [29]      |          |        |        |\\n| + ALGM (ours)   |          |        |        |\\n| + ALGM* (ours)  |          |        |        |\\n\\n(a) COCO-Stuff [4].\\n\\n| Method          | mIoU (%) | Im/sec | GFLOPs |\\n|-----------------|----------|--------|--------|\\n| Seg-S [38]      |          |        |        |\\n| + CTS [29]      |          |        |        |\\n| + ALGM (ours)   |          |        |        |\\n| + ALGM* (ours)  |          |        |        |\\n| Seg-L [38]      |          |        |        |\\n| + CTS [29]      |          |        |        |\\n| + ToMe [2]      |          |        |        |\\n| + ELViT [23]    |          |        |        |\\n| + ELViT \u2021 [23]  |          |        |        |\\n| + AiluRus \u2021 [21]|          |        |        |\\n| + ALGM (ours)   |          |        |        |\\n| + ALGM* (ours)  |          |        |        |\\n\\n(b) Cityscapes [11].\\n\\n| Method          | mIoU (%) | Im/sec | GFLOPs |\\n|-----------------|----------|--------|--------|\\n| Seg-S [38]      |          |        |        |\\n| + CTS [29]      |          |        |        |\\n| + ALGM (ours)   |          |        |        |\\n| + ALGM* (ours)  |          |        |        |\\n| Seg-L [38]      |          |        |        |\\n| + CTS [29]      |          |        |        |\\n| + ToMe [2]      |          |        |        |\\n| + ELViT [23]    |          |        |        |\\n| + ELViT \u2021 [23]  |          |        |        |\\n| + AiluRus \u2021 [21]|          |        |        |\\n| + ALGM (ours)   |          |        |        |\\n| + ALGM* (ours)  |          |        |        |\\n\\n(c) Pascal-Context [31].\\n\\n| Method          | mIoU (%) | Im/sec | GFLOPs |\\n|-----------------|----------|--------|--------|\\n| SETR-B + DToP [39] | 47.0 108 | 47.0 | 81 -25% |\\n| SETR-B + ALGM (ours) | 46.4 | 108 | 47.1 +0.7 |\\n| SETR-B + ALGM* (ours) | 46.4 | 108 | 46.5 +0.1 |\\n| Seg-S + DoViT [26] | 46.2 26.6 | 45.8 | 21.8 -18% |\\n| Seg-S + ALGM (ours) | 45.3 | 38.6 | 46.4 +1.1 |\\n| Seg-S + ALGM* (ours) | 45.3 | 38.6 | 45.5 +0.2 |\\n\\nTable 2. Main results on COCO-Stuff, Cityscapes and Pascal-Context. ALGM applied to Segmenter (Seg) across 3 ViT backbones and 3 datasets. ALGM* is the same trained model as ALGM, but uses the threshold \u03c4 during inference that achieves the best efficiency while maintaining the mIoU w.r.t. the baseline. \u2021 Indicates a training-free method, applied directly to the baseline model.\\n\\n| Method | No token reduction | With token reduction |\\n|--------|--------------------|----------------------|\\n| mIoU   | GFLOPs             | mIoU                 | GFLOPs             |\\n|        |                    |                      |                    |\\n|        |                    |                      |                    |\\n|        |                    |                      |                    |\\n\\nTable 3. ALGM vs. DToP and DoViT [26, 39]. Applied to SETR [52], and Segmenter (Seg) [38] on ADE20K [53].\\n\\nTable 4. Application to state-of-the-art model. ALGM is applied to SOTA method EV A + ViT-Adapter + Mask2Former [9, 10, 14] and evaluated over the ADE20K validation set, with single-scale testing. \u2021 Directly applied to the backbone without fine-tuning.\\n\\nImpact of merging modules. To assess the impact of the individual CLAP and GBM merging modules, we evaluate various configurations in Tab. 5b. We find that only applying CLAP leads to modest improvements in both the mIoU and the throughput, showing that local merging is effective. If we use the GBM module in the first layer instead, we find that the throughput increases but at the cost of segmentation quality, confirming our findings in Sec. 3.2 and Fig. 2b that global token similarities in the first layer should not be used to identify tokens for merging. Conversely, placing the GBM module in layer 5 does yield an improved mIoU, albeit with a lower efficiency gain. Applying GBM in both layer 1 and layer 5 results in a significantly better efficiency, but the incorrectly merged tokens in layer 1 are then merged even further in layer 5, harming the segmentation quality. Finally, combining CLAP with GBM yields the best mIoU while achieving a significant efficiency gain, showing the power of applying both early local merging and later global merging. Fig. 5 visualizes the tokens that are merged by these modules. See the supplementary material for more examples.\\n\\nGBM module position. Tab. 5c shows the effect of positioning the GBM module at various transformer layers. We find that, for ViT-S, layer 5 yields the best trade-off between mIoU and efficiency, which again shows that early layers should not be used for global merging, and applying it in later layers gives diminishing efficiency returns.\\n\\n5.4. Detailed analyses\\n\\nCause of segmentation quality improvement. The main results in Tab. 1 and Tab. 2 have shown that ALGM not only enhances efficiency, but also improves the segmentation quality. This improvement is most significant for complex datasets, and we hypothesize that it has two causes: (1) Balancing: As tokens that depict the same category are merged, large and frequently-occurring categories are represented by fewer tokens, meaning that they play a less dominant role in the self-attention operation, causing a more balanced attention distribution with respect to rare classes. To assess if this is true, in Tab. 6, we evaluate a setting in which we do not reduce the number of tokens and therefore do not balance the attention process, but instead replicate the average token embedding across the tokens that would otherwise be merged. We find that this causes a drop in mIoU, indicating that attention balancing is indeed a factor in the mIoU improvement of ALGM. (2) Denoising: As tokens are merged, we take the average of their values. This denoises the tokens, which could facilitate the learning...\"}"}
{"id": "CVPR-2024-74", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. Ablations for ALGM.\\n\\nWe apply ALGM to Segmenter [38] with ViT-S [13] and evaluate on the ADE20K validation set [53].\\n\\n| Module          | Layer 1 | Layer 2 | Layer 3 | Layer 4 | Layer 5 |\\n|-----------------|---------|---------|---------|---------|---------|\\n| CLAP            | 45.6    | 45.5    | 45.4    | 45.3    | 45.2    |\\n| GBM             | 45.0    | 45.1    | 45.2    | 45.3    | 45.4    |\\n\\nTable 6. Analyzing mIoU improvement.\\n\\nALGM is applied to Segmenter with ViT-S [13, 38] on ADE20K [53].\\n\\nBalancing refers to attention balancing; Denoising refers to token denoising.\\n\\n| Denoising | Balancing | mIoU (%) |\\n|-----------|-----------|----------|\\n| \u2717         | \u2717         | 44.4     |\\n| \u2713         | \u2717         | 45.0     |\\n| \u2713         | \u2713         | 45.6     |\\n\\nFigure 4. Similarity thresholds for token merging.\\n\\nFigure 5. Merged tokens.\\n\\nWe depict tokens that are merged as a result of the local CLAP and global GBM merging modules.\\n\\n6. Discussion\\n\\nIn this work, we propose a token reduction method for semantic segmentation that combines early local merging with later global merging. This is motivated by the finding that, using this merging strategy, we predominantly merge tokens that contain redundant information, meaning that they can be merged without compromising the segmentation quality. With extensive experiments, we show that our approach is indeed able to find a very good balance between efficiency and segmentation quality, outperforming existing work. Interestingly, we also find that using ALGM for token reduction leads to substantial mIoU improvements for complex datasets. With some first analyses, we find that this is likely caused by improved attention balancing and token denoising. However, further research is required to fully understand the causes of these phenomena and their potential applicability to other networks and tasks. Another interesting avenue for future research could be to examine whether token reduction is similarly effective on more complex tasks like panoptic and video segmentation.\\n\\nAcknowledgements\\n\\nThis work was funded by the EU project MODI, grant no. 101076810, and the KDT JU EdgeAI project, grant no. 101097300, and utilized the Dutch national e-infrastructure, supported by the SURF Cooperative under grant no. EINF-5197, funded by the Dutch Research Council (NWO).\"}"}
