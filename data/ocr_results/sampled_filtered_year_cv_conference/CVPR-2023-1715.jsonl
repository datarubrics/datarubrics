{"id": "CVPR-2023-1715", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Panos Achlioptas, Olga Diamanti, Ioannis Mitliagkas, and Leonidas Guibas. Learning representations and generative models for 3D point clouds. In Jennifer Dy and Andreas Krause, editors, Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pages 40\u201349. PMLR, 10\u201315 Jul 2018.\\n\\n[2] Omri Avrahami, Dani Lischinski, and Ohad Fried. Blended diffusion for text-driven editing of natural images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 18208\u201318218, June 2022.\\n\\n[3] Dejan Azinovi\u0107, Ricardo Martin-Brualla, Dan B Goldman, Matthias Nie\u00dfner, and Justus Thies. Neural rgb-d surface reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6290\u20136301, June 2022.\\n\\n[4] Dmitry Baranchuk, Andrey Voynov, Ivan Rubachev, Valentin Khrulkov, and Artem Babenko. Label-efficient semantic segmentation with diffusion models. In International Conference on Learning Representations, 2022.\\n\\n[5] Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Walter Talbott, Alexander T Toshev, Zhuoyuan Chen, Laurent Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, Afshin Dehghan, and Joshua M. Susskind. GAUDI: A neural architect for immersive 3d scene generation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\\n\\n[6] Wenjing Bian, Zirui Wang, Kejie Li, Jia-Wang Bian, and Victor Adrian Prisacariu. Nope-nerf: Optimising neural radiance field with no pose prior, 2022.\\n\\n[7] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas J. Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient geometry-aware 3d generative adversarial networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16123\u201316133, June 2022.\\n\\n[8] Eric R. Chan, Marco Monteiro, Petr Kellnhofer, Jiajun Wu, and Gordon Wetzstein. Pi-gan: Periodic implicit generative adversarial networks for 3d-aware image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5799\u20135809, June 2021.\\n\\n[9] Yongwei Chen, Rui Chen, Jiabao Lei, Yabin Zhang, and Kui Jia. TANGO: Text-driven photorealistic and robust 3d stylization via lighting decomposition. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\\n\\n[10] Zhiqin Chen and Hao Zhang. Learning implicit fields for generative shape modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n\\n[11] Christopher B Choy, Danfei Xu, JunYoung Gwak, Kevin Chen, and Silvio Savarese. 3d-r2n2: A unified approach for single and multi-view 3d object reconstruction. In Proceedings of the European Conference on Computer Vision (ECCV), 2016.\\n\\n[12] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Niessner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July 2017.\\n\\n[13] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ramanan. Depth-supervised nerf: Fewer views and faster training for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 12882\u201312891, June 2022.\\n\\n[14] Terrance DeVries, Miguel Angel Bautista, Nitish Srivastava, Graham W. Taylor, and Joshua M. Susskind. Unconstrained scene generation with locally conditioned radiance fields. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 14304\u201314313, October 2021.\\n\\n[15] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 8780\u20138794. Curran Associates, Inc., 2021.\\n\\n[16] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Z. Ghahramani, M. Welling, C. Cortes, N. Lawrence, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 27. Curran Associates, Inc., 2014.\\n\\n[17] Thibault Groueix, Matthew Fisher, Vladimir G. Kim, Bryan C. Russell, and Mathieu Aubry. A papier-m\u00e2ch\u00e9 approach to learning 3d surface generation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\\n\\n[18] Jiatao Gu, Lingjie Liu, Peng Wang, and Christian Theobalt. StyleneRF: A style-based 3d aware generator for high-resolution image synthesis. In International Conference on Learning Representations, 2022.\\n\\n[19] Xiaoguang Han, Zhaoxuan Zhang, Dong Du, Mingdai Yang, Jingming Yu, Pan Pan, Xin Yang, Ligang Liu, Zixiang Xiong, and Shuguang Cui. Deep reinforcement learning of volume-guided progressive view inpainting for 3d point scene completion from a single depth image. In Proceedings of Machine Learning Research, page 8430.\"}"}
{"id": "CVPR-2023-1715", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 6840\u20136851. Curran Associates, Inc., 2020.\\n\\n[21] Jonathan Ho, Chitwan Saharia, William Chan, David J. Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. Journal of Machine Learning Research, 23(47):1\u201333, 2022.\\n\\n[22] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021.\\n\\n[23] Jonathan Ho, Tim Salimans, Alexey A. Gritsenko, William Chan, Mohammad Norouzi, and David J. Fleet. Video diffusion models. In ICLR Workshop on Deep Generative Models for Highly Structured Data, 2022.\\n\\n[24] Sepp Hochreiter and J\u00fcrgen Schmidhuber. Long short-term memory. Neural Comput., 9(8):1735\u20131780, nov 1997.\\n\\n[25] Tobias H\u00f6ppe, Arash Mehrjou, Stefan Bauer, Didrik Nielsen, and Andrea Dittadi. Diffusion models for video prediction and infilling. Transactions on Machine Learning Research, 2022.\\n\\n[26] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusion-based generative models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\\n\\n[27] Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2426\u20132435, June 2022.\\n\\n[28] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In Yoshua Bengio and Yann LeCun, editors, 2nd International Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings, 2014.\\n\\n[29] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In F. Pereira, C.J. Burges, L. Bottou, and K.Q. Weinberger, editors, Advances in Neural Information Processing Systems, volume 25. Curran Associates, Inc., 2012.\\n\\n[30] Jiabao Lei and Kui Jia. Analytic marching: An analytic meshing solution from deep implicit surface networks. In Hal Daum\u00e9 III and Aarti Singh, editors, Proceedings of the 37th International Conference on Machine Learning, volume 119 of Proceedings of Machine Learning Research, pages 5789\u20135798. PMLR, 13\u201318 Jul 2020.\\n\\n[31] Jiabao Lei, Kui Jia, and Yi Ma. Learning and meshing from deep implicit surface networks using an efficient implementation of analytic marching. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(12):10068\u201310086, Dec 2022.\\n\\n[32] Haoying Li, Yifan Yang, Meng Chang, Shiqi Chen, Huajun Feng, Zhihai Xu, Qi Li, and Yueting Chen. Srdiff: Single image super-resolution with diffusion probabilistic models. Neurocomputing, 479:47\u201359, 2022.\\n\\n[33] Manyi Li, Akshay Gadi Patil, Kai Xu, Siddhartha Chaudhuri, Owais Khan, Ariel Shamir, Changhe Tu, Baoquan Chen, Daniel Cohen-Or, and Hao Zhang. Grains: Generative recursive autoencoders for indoor scenes. ACM Trans. Graph., 38(2), feb 2019.\\n\\n[34] Zhengqi Li, Qianqian Wang, Noah Snavely, and Angjoo Kanazawa. Infinitenature-zero: Learning perpetual view generation of natural scenes from single images. In Computer Vision \u2013 ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part I, page 515\u2013534, Berlin, Heidelberg, 2022. Springer-Verlag.\\n\\n[35] Andrew Liu, Richard Tucker, Varun Jampani, Ameesh Makadia, Noah Snavely, and Angjoo Kanazawa. Infinite nature: Perpetual view generation of natural scenes from a single image. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 14458\u201314467, October 2021.\\n\\n[36] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In International Conference on Learning Representations, 2022.\\n\\n[37] Andreas Lugmayr, Martin Danelljan, Andres Romero, Fisher Yu, Radu Timofte, and Luc Van Gool. Repaint: Inpainting using denoising diffusion probabilistic models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11461\u201311471, June 2022.\\n\\n[38] Andrew Luo, Zhoutong Zhang, Jiajun Wu, and Joshua B. Tenenbaum. End-to-end optimization of scene layout. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.\\n\\n[39] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2837\u20132845, June 2021.\\n\\n[40] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022.\\n\\n[41] Lars Mescheder, Michael Oechsle, Michael Niemeyer, Sebastian Nowozin, and Andreas Geiger. Occupancy networks: Learning 3d reconstruction in function space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n\\n[42] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Computer Vision \u2013 ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part I, page 405\u2013421, Berlin, Heidelberg, 2020. Springer-Verlag.\\n\\n[43] Thomas M\u00fcller, Alex Evans, Christoph Schied, and Alexander Keller. Instant neural graphics primitives with a multiresolution hash encoding. ACM Trans. Graph., 41(4), jul 2022.\"}"}
{"id": "CVPR-2023-1715", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2023-1715", "page_num": 12, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Katja Schwarz, Yiyi Liao, Michael Niemeyer, and Andreas Geiger. Graf: Generative radiance fields for 3D-aware image synthesis. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin, editors, Advances in Neural Information Processing Systems, volume 33, pages 20154\u201320166. Curran Associates, Inc., 2020.\\n\\nVincent Sitzmann, Michael Zollhoefer, and Gordon Wetzstein. Scene representation networks: Continuous 3D-structure-aware neural scene representations. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00b4e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\\n\\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In Francis Bach and David Blei, editors, Proceedings of the 32nd International Conference on Machine Learning, volume 37 of Proceedings of Machine Learning Research, pages 2256\u20132265, Lille, France, 07\u201309 Jul 2015. PMLR.\\n\\nJiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In International Conference on Learning Representations, 2021.\\n\\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d\u2019Alch\u00b4e-Buc, E. Fox, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019.\\n\\nYongyi Su, Xun Xu, and Kui Jia. Weakly supervised 3D point cloud segmentation via multi-prototype learning, 2022.\\n\\nJiaming Sun, Yiming Xie, Linghao Chen, Xiaowei Zhou, and Hujun Bao. Neuralrecon: Real-time coherent 3D reconstruction from monocular video. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 15598\u201315607, June 2021.\\n\\nJiapeng Tang, Xiaoguang Han, Junyi Pan, Kui Jia, and Xin Tong. A skeleton-bridged deep learning approach for generating meshes of complex topologies from single RGB images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2019.\\n\\nJiapeng Tang, Xiaoguang Han, Mingkui Tan, Xin Tong, and Kui Jia. Skeletonnet: A topology-preserving solution for learning mesh reconstruction of object surfaces from RGB images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(10):6454\u20136471, Oct 2022.\\n\\nJiapeng Tang, Jiabao Lei, Dan Xu, Feiying Ma, Kui Jia, and Lei Zhang. Sa-convonet: Sign-agnostic optimization of convolutional occupancy networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 6504\u20136513, October 2021.\\n\\nJiapeng Tang, Lev Markhasin, Bi Wang, Justus Thies, and Matthias Nie\u00dfner. Neural shape deformation priors. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\\n\\nJiapeng Tang, Dan Xu, Kui Jia, and Lei Zhang. Learning parallel dense correspondence from spatio-temporal descriptors for efficient and robust 4D reconstruction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 6022\u20136031, June 2021.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In I. Guyon, U. V\u00f3n Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., 2017.\\n\\nVikram Voleti, Alexia Jolicoeur-Martineau, and Christopher Pal. MCVD - masked conditional video diffusion for prediction, generation, and interpolation. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022.\\n\\nKai Wang, Yu-An Lin, Ben Weissmann, Manolis Savva, Angel X. Chang, and Daniel Ritchie. Planit: Planning and instantiating indoor scenes with relation graph and spatial prior networks. ACM Trans. Graph., 38(4), July 2019.\\n\\nKai Wang, Manolis Savva, Angel X. Chang, and Daniel Ritchie. Deep convolutional priors for indoor scene synthesis. ACM Trans. Graph., 37(4), July 2018.\\n\\nNanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2mesh: Generating 3D mesh models from single RGB images. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.\\n\\nPeng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku Komura, and Wenping Wang. Neus: Learning neural implicit surfaces by volume rendering for multi-view reconstruction. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan, editors, Advances in Neural Information Processing Systems, volume 34, pages 27171\u201327183. Curran Associates, Inc., 2021.\\n\\nPeng-Shuai Wang, Yang Liu, Yu-Xiao Guo, Chun-Yu Sun, and Xin Tong. O-cnn: Octree-based convolutional neural networks for 3D shape analysis. ACM Trans. Graph., 36(4), July 2017.\\n\\nWeilun Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Dong Chen, Lu Yuan, and Houqiang Li. Semantic image synthesis via diffusion models, 2022.\\n\\nX. Wang, C. Yeshwanth, and M. Niesner. Sceneformer: Indoor scene generation with transformers. In 2021 International Conference on 3D Vision (3DV), pages 106\u2013115, Los Alamitos, CA, USA, Dec 2021. IEEE Computer Society.\\n\\nZirui Wang, Shangzhe Wu, Weidi Xie, Min Chen, and Victor Adrian Prisacariu. Nerf\u2013: Neural radiance fields without known camera parameters, 2021.\\n\\nJiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning a probabilistic latent space of object shapes via 3D generative-adversarial modeling. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett, editors, Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., 2016.\\n\\nQiangeng Xu, Weiyue Wang, Duygu Ceylan, Radomir Mech, and Ulrich Neumann. Disn: Deep implicit surface network for high-quality single-view 3D reconstruction. In 8433.\"}"}
{"id": "CVPR-2023-1715", "page_num": 13, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan. Pointflow: 3d point cloud generation with continuous normalizing flows. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.\\n\\nBiao Zhang, Jiapeng Tang, Matthias Niessner, and Peter Wonka. 3dshape2vecset: A 3d shape representation for neural fields and generative diffusion models, 2023.\\n\\nRichard Zhang, Phillip Isola, Alexei A. Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2018.\\n\\nZaiwei Zhang, Zhenpei Yang, Chongyang Ma, Linjie Luo, Alexander Huth, Etienne Vouga, and Qixing Huang. Deep generative modeling for scene synthesis via hybrid representations. ACM Trans. Graph., 39(2), apr 2020.\\n\\nWenbin Zhao, Jiabao Lei, Yuxin Wen, Jianguo Zhang, and Kui Jia. Sign-agnostic implicit learning of surface self-similarities for shape modeling and reconstruction from raw point clouds. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10256\u201310265, June 2021.\\n\\nLinqi Zhou, Yilun Du, and Jiajun Wu. 3d shape generation and completion through point-voxel diffusion. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 5826\u20135835, October 2021.\"}"}
{"id": "CVPR-2023-1715", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We address the challenge of recovering an underlying scene geometry and colors from a sparse set of RGBD view observations. In this work, we present a new solution termed RGBD2 that sequentially generates novel RGBD views along a camera trajectory, and the scene geometry is simply the fusion result of these views. More specifically, we maintain an intermediate surface mesh used for rendering new RGBD views, which subsequently becomes complete by an inpainting network; each rendered RGBD view is later back-projected as a partial surface and is supplemented into the intermediate mesh. The use of intermediate mesh and camera projection helps solve the tough problem of multi-view inconsistency. We practically implement the RGBD inpainting network as a versatile RGBD diffusion model, which is previously used for 2D generative modeling; we make a modification to its reverse diffusion process to enable our use. We evaluate our approach on the task of 3D scene synthesis from sparse RGBD inputs; extensive experiments on the ScanNet dataset demonstrate the superiority of our approach over existing ones. Project page: https://jblei.site/proj/rgbd-diffusion.\\n\\n1. Introduction\\n\\nScene synthesis is an essential requirement for many practical applications. The resulting scene representation can be readily utilized in diverse fields, such as virtual reality, augmented reality, computer graphics, and game development. Nevertheless, conventional approaches to scene synthesis usually involve reconstructing scenes (e.g., indoor scenes with varying sizes) by fitting given observations, such as multi-view images or point clouds. The increasing prevalence of RGB/RGBD scanning devices has established multi-view data as a favored input modality, driving and promoting technical advancements in the realm of scene reconstruction from multi-view images.\\n\\nNeural Radiance Fields (NeRFs) [42] have demonstrated potential in this regard, yet they are not exempt from limitations. NeRFs are designed to reconstruct complete scenes by fitting multi-view images, and they cannot generate or infer missing parts when the input is inevitably incomplete or missing. While recently some studies [3, 5, 8, 56, 63] have attempted to equip NeRFs with generative and extrapolation capabilities, this functionality relies on a comparatively short representation with limited elements (e.g. typically, the length of a global latent code is much shorter than that of an image: $(F = 512) \\\\ll (H \\\\times W = 128 \\\\times 128 = 16,384)$ that significantly constrains their capacity to accurately capture fine-grained details in the observed data. Consequently, the effectiveness of these methods has only been established for certain categories of canonical objects, such as faces or cars [8, 63], or relatively small toy scenes [5].\\n\\nWe introduce a novel task of generative scene synthesis from sparse RGBD views, which involves learning across multiple scenes to later enable scene synthesis from a sparse set of multi-view RGBD images. This task presents a challenging setting wherein a desired solution should simultaneously (1) preserve observed regions, hallucinate missing parts of the scene, (2) eliminate additional computational costs during inference for each individual test scene, (3) enable synthesis on unseen scenes that were not shown during training, (4) maintain real-time performance on unseen scenes, and (5) demonstrate the capability of generating novel views for different camera poses.\"}"}
{"id": "CVPR-2023-1715", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We will elaborate on them in detail as follows. Firstly, to maximize the preservation of intricate details while simultaneously hallucinating potentially absent parts that may become more pronounced when views are exceedingly sparse, we perform straightforward reconstruction whose details come from images that can describe fine structures using a maximum of $H \\\\times W$ elements (i.e. an image size) in a view completion manner. This is particularly compatible with diffusion models that operate at full image resolution with an inpainting mechanism. We also found that RGBD diffusion models greatly simplify the training complexity of a completion model, thanks to their versatile generative ability to inpaint missing RGBD pixels while preserving the integrity of known regions through a convenient training process solely operated on complete RGBD data. Secondly, our method employs back-projection that requires no optimization, thus eliminating the necessity for test-time training for each individual scene, ultimately leading to a significant enhancement in test-time efficiency. Thirdly, to ensure consistency among multi-view images, an intermediate mesh representation is utilized as a means of bridging the 2D domain (i.e. multi-view RGBD images) with the 3D domain (i.e. the 3D intermediate mesh) through the aid of camera projection. Fourthly, to enable our method to handle scenes of indeterminate sizes, we utilize images with freely designated poses as the input representation. Such manner naturally ensures SE(3) equivariance, and thus offers scalability due to the ease with which the range of the generated content can be controlled by simply specifying their camera extrinsic matrices.\\n\\nOur proposal involves generating multi-view consistent RGBD views along a predetermined camera trajectory, using an intermediate mesh to render novel RGBD images that are subsequently inpainted using a diffusion model, and transforming each RGBD view into a 3D partial mesh via back-projection, and finally merging it with the intermediate scene mesh to produce the final output. Specifically, our proposed approach initiates by ingesting multiple posed RGBD images as input and utilizing back-projection to construct an intermediate scene mesh. This mesh encompasses color attributes that facilitate the rendering of RGBD images from the representation under arbitrarily specified camera viewpoints. Once a camera pose is selected from the test-time rendering trajectory, the intermediate mesh is rendered to generate a new RGBD image for this pose. Notably, the test-time view typically exhibits only slight overlap with the known cameras, leading to naturally partially rendered RGBD images. To fill the gaps in the incomplete view, we employ an inpainting network implemented as an RGBD diffusion model with minor modifications to its reverse sampling process. The resulting inpainted output is then back-projected into 3D space, forming a partial mesh that complements the entire intermediate scene mesh. We iterate these steps until all test-time camera viewpoints are covered, and the intermediate scene mesh gradually becomes complete during this process. The final output of our pipeline is the mesh outcome acquired from the last step.\\n\\nExtensive experiments on ScanNet [12] dataset demonstrate the superiority of our approach over existing solutions on the task of scene synthesis from sparse RGBD inputs.\\n\\n2. Related Works\\n\\nIn this section, we provide a brief review of the literature related to diffusion models, 3D representations and generative manners, scene synthesis, and view synthesis.\\n\\nDiffusion Models.\\n\\nIn recent years, the field of 2D computer vision has experienced a surge of interest in diffusion-based generative models [20,65,67]. These models have prompted the development of image generative modeling approaches, such as GLIDE [45], unCLIP [53], Imagen [61], and Latent Diffusion Models [57], as well as the invention of sampling schedulers [20,26,36,66] and guiding methods [15,22]. Furthermore, these models have been applied to a broad range of image processing tasks, including image inpainting [37], image translation [4,57,60,82], video generation [23,25,76], super-resolution [21,32,62], and image editing [2,27,40]. More recently, some researchers have adapted these techniques from 2D to the 3D domain, as demonstrated by methods such as [39,51,88,92]. Our approach harnesses such versatility by adopting an iterative denoising strategy like [37], and utilizing a masked inpainting technique that operates on the projected RGBD views to synthesize image content.\\n\\n3D Representations and Generative Manners.\\n\\nA variety of representations, including voxels [81,85], point clouds [1,39,68,87,92], meshes [17,44,70,71,79], implicit surfaces [10,30,31,41,48,72\u201374,88,91], multi-view images [11,19,34,35,86], and neural radiance fields [3,8,9,13,42,43,49,56,63,80], have been proposed, each with its own unique advantages over the others. This has also motivated researchers to combine them with distinct generative approaches, such as VAEs [28], GANs [16], normalizing flows [54], auto-regressive models [24,75], and the latest diffusion models [20], resulting in an extensive range of applications [1,8,39,41,44,48,63,85,87,92]. However, most existing methods have limitations in their representation capability, such as cubically scaled-up memory consumption, or a fixed number of points, which makes them difficult to apply to scenes of uncertain scales, and poor equivariance, which only allows them to handle canonically-posed objects. In this paper, we address these issues by focusing on the generation of multi-view RGBD images that can capture intricate structures using $H \\\\times W$ pixels created by a diffusion model. This approach reduces memory complexity from $O(HWD)$ to $O(HW)$ and increases expressive...\"}"}
{"id": "CVPR-2023-1715", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We practically implement the noise estimator given an image.\\n\\nDefinition. \\\\( \\\\lim_{t \\\\to 0} \\\\) preliminary knowledge about DDPM \\\\([20]\\\\) and DDIM \\\\([66]\\\\).\\n\\n\\\\[ x_{t-1} = \\\\alpha x_t + \\\\beta \\\\epsilon_t \\\\quad \\\\text{where} \\\\quad x_0 \\\\sim \\\\mathcal{N}(0, I) \\\\]\\n\\nWe can sample the noisy image \\\\( x_t \\\\) using i.i.d. Gaussian distribution, and the generative ability can be derived from \\\\( O \\\\).\\n\\nAs shown in Figure 2, given a sparse set of input RGBD images \\\\( S \\\\), we can progressively recover the clean image from a diffusion process. This process is another Markov chain starting from \\\\( \\\\epsilon \\\\alpha \\\\theta \\\\).\\n\\nScene Synthesis. In this area, there are two primary research directions. The first pertains to learning configurations of object-level instances \\\\([7, 18, 47, 64]\\\\), only a few studies have explored techniques for improving performance using sparse inputs \\\\([46, 56]\\\\).\\n\\nThe arrival of NeRF \\\\([42]\\\\) has significantly advanced the field of view synthesis. While a considerable amount of research has been dedicated to the view synthesis area, there are still many open questions. However, in our case, it will be detailed in Sec. 4.2. Nevertheless, diversity can still be achieved by choosing a different \\\\( \\\\theta \\\\).\\n\\nView Synthesis. Some studies have explored techniques for improving performance using sparse inputs \\\\([46, 56]\\\\). In this paper, we aim to recover scene-level geometry from sparse RGBD images in a self-contained manner.\\n\\nTo make our paper self-contained, we provide some preliminary knowledge about DDPM \\\\([20]\\\\) and DDIM \\\\([66]\\\\), and scene composition \\\\([50, 83]\\\\). The second line of Eq. 1 is also conditioned on \\\\( \\\\hat{\\\\theta} \\\\).\\n\\nRegarding the estimation of \\\\( \\\\hat{\\\\theta} \\\\), we can sample the noisy image \\\\( x_t \\\\) using i.i.d. Gaussian distribution, and the generative ability can be derived from \\\\( O \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\nScene Synthesis. In this area, there are two primary research directions. The first pertains to learning configurations of object-level instances \\\\([7, 18, 47, 64]\\\\), only a few studies have explored techniques for improving performance using sparse inputs \\\\([46, 56]\\\\). In this paper, we aim to recover scene-level geometry from sparse RGBD images in a self-contained manner.\\n\\nTo make our paper self-contained, we provide some preliminary knowledge about DDPM \\\\([20]\\\\) and DDIM \\\\([66]\\\\), and scene composition \\\\([50, 83]\\\\). The second line of Eq. 1 is also conditioned on \\\\( \\\\hat{\\\\theta} \\\\).\\n\\nRegarding the estimation of \\\\( \\\\hat{\\\\theta} \\\\), we can sample the noisy image \\\\( x_t \\\\) using i.i.d. Gaussian distribution, and the generative ability can be derived from \\\\( O \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma = \\\\bar{\\\\alpha} \\\\theta \\\\) and \\\\( \\\\beta \\\\). The image is also conditioned on \\\\( \\\\hat{x} \\\\).\\n\\n\\\\[ \\\\hat{\\\\theta} \\\\sim \\\\mathcal{N}(0, \\\\Sigma) \\\\]\\n\\nwhere \\\\( \\\\Sigma ="}
{"id": "CVPR-2023-1715", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. 3D Scene Synthesis via Incremental View Inpainting. Given a sparse set of RGBD images \\\\( \\\\{I_i\\\\}_{i=1}^N \\\\) with camera intrinsic \\\\( K \\\\) and extrinsic \\\\( \\\\{R_i\\\\}_{i=1}^N \\\\) matrices, our goal is to generate a coherent 3D scene mesh \\\\( S \\\\) via predicting RGBD frames \\\\( \\\\{O_j\\\\}_{j=1}^M \\\\) along a novel-view trajectory \\\\( \\\\{T_j\\\\}_{j=1}^M \\\\). To achieve this goal, we firstly fuse the inputs of \\\\( \\\\{I_i\\\\}_{i=1}^N \\\\) into an initial mesh \\\\( S_1 \\\\), and then render it to obtain an incomplete image \\\\( \\\\hat{O}_1 \\\\) that is later inpainted to obtain \\\\( O_1 \\\\) using a RGBD diffusion model. After that, \\\\( O_1 \\\\) is back-projected and integrated with \\\\( S_1 \\\\) to produce a more complete scene, \\\\( S_2 \\\\). By iteratively repeating this process, we can progressively obtain \\\\( S_2, \\\\ldots, S_M \\\\). Finally, the fused result \\\\( S_M+1 \\\\) is the eventual desired output \\\\( S \\\\). Gray dashed lines \\\"\\\\( \\\\Rightarrow \\\\)\\\" denote omitted and unvisualized steps.\\n\\nAlgorithm 1 Incremental RGBD View Inpainting\\n\\nRequire:\\n- RGBD images \\\\( \\\\{I_i\\\\}_{i=1}^N \\\\) with camera extrinsics \\\\( \\\\{R_i\\\\}_{i=1}^N \\\\), a novel trajectory of \\\\( M \\\\) extrinsics \\\\( \\\\{T_j\\\\}_{j=1}^M \\\\).\\n\\n1. \\\\( S_1 \\\\leftarrow \\\\bigcup_{i=1}^N \\\\mathcal{P}^{2\\\\to3}(I_i) \\\\) for \\\\( j = 1 \\\\) to \\\\( M \\\\) do\\n2. \\\\( \\\\hat{O}_j \\\\leftarrow \\\\mathcal{P}^{3\\\\to2}(T_j) \\\\) \\\\( \\\\triangleright \\\\) novel-view rendering\\n3. \\\\( O_j \\\\leftarrow f(\\\\hat{O}_j) \\\\) \\\\( \\\\triangleright \\\\) RGBD inpainting\\n4. \\\\( S_{j+1} \\\\leftarrow S_j \\\\cup \\\\mathcal{P}^{2\\\\to3}(O_j) \\\\) \\\\( \\\\triangleright \\\\) mesh fusion\\nend for\\n5. return \\\\( S_M+1 \\\\)\\n\\nRendering and Back-projection.\\n\\nThe operator \\\\( \\\\mathcal{P}^{3\\\\to2} \\\\) is implemented as mesh rasterization, which allows for the rendering of a partial RGBD image \\\\( \\\\hat{O}_j \\\\) from a mesh \\\\( S_j \\\\). This approach offers the advantage of producing a clean visibility mask \\\\( m_j \\\\), which is not possible with NeRFs [42]. The back-projection operator \\\\( \\\\mathcal{P}^{2\\\\to3} \\\\) is responsible for the conversion from a depth map into a point cloud, where the connectivity between points is inherited from the connectivity of the 2D pixel grid. Furthermore, mesh faces that are either in close proximity to the viewpoint or exhibit slender characteristics are filtered out to ensure accuracy.\\n\\nChallenges and Solutions.\\n\\nTo circumvent the limitations posed by potential 3D inconsistency in both geometry and appearance, as well as the challenge of solely handling specifically canonically-posed scenes, we propose several strategies to address these issues. Firstly, our method for synthesizing novel views combines rendering (mesh rasterization) and inpainting techniques, and interleaves the view synthesis process with online RGBD fusion via back-projection and mesh combination. The use of perspective camera projection ensures strict adherence to 3D constraints, resulting in visually consistent and accurate synthesized views. In concrete terms, we begin by rendering the mesh \\\\( S_j \\\\) under view \\\\( T_j \\\\) using a rendering operation \\\\( \\\\mathcal{P}^{3\\\\to2}(T_j) \\\\). This process yields an incomplete RGBD image \\\\( \\\\hat{O}_j \\\\) with missing regions, which is subsequently inpainted using a diffusion model described in detail in Sec. 4.2, resulting in a complete image \\\\( O_j \\\\). Once image \\\\( O_j \\\\) has been generated for view \\\\( T_j \\\\), it can be fused into a 3D mesh via \\\\( S_{j+1} \\\\leftarrow \\\\mathcal{P}^{2\\\\to3}(O_j) \\\\cup S_j \\\\) using a back-projection operator \\\\( \\\\mathcal{P}^{2\\\\to3} \\\\). Secondly, our solution reduces the learning difficulty and can handle non-canonical scenes by decomposing the 3D scene as a Markov chain of temporal RGBD images rendered from arbitrarily specified novel viewpoints. The applicability of handling scenes with arbitrary scaling, movement, and posing is attributed to the utilization of two SE(3) equivariant operators, namely \\\\( \\\\mathcal{P}^{3\\\\to2} \\\\) and \\\\( \\\\mathcal{P}^{2\\\\to3} \\\\), as well as the independence of absolute coordinates. Moreover, the presence of redundant information in adjacent frames, combined with our suggested decomposition rule, facilitates the minimization of learning complexity in an auto-regressive manner.\"}"}
{"id": "CVPR-2023-1715", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In accordance with the Markov chain decomposition principle, the distribution of the scene can be expressed as the joint distribution of view frames:\\n\\n\\\\[ p(S) = \\\\prod_j p(O_j | \\\\{O_s\\\\}_{s=1}^{j-1}, \\\\{I_s\\\\}_{s=1}^{N_s}) \\\\]\\n\\nwhere the prediction of \\\\( O_j \\\\) is based on the fusion result of all the previously known frames \\\\( \\\\{O_s\\\\}_{s=1}^{j-1} \\\\cup \\\\{I_s\\\\}_{s=1}^{N_s} \\\\).\\n\\n### 4.2. RGBD Diffusion for Rendered View Inpainting\\n\\nIn this section, we describe the implementation details of the way to employ diffusion models to inpaint the missing regions of the RGBD image \\\\( \\\\hat{O}_j \\\\) with a binary mask \\\\( m_j \\\\) obtained by rendering visibility, where the value of 1 is assigned to the corresponding ray that intersects with the geometry surface, while 0 is assigned to all other cases, both of which are rendered by projecting \\\\( S_j \\\\) under a novel viewpoint \\\\( T_j \\\\). It is noteworthy that only those pixels located where \\\\( m_j = 0 \\\\) in \\\\( \\\\hat{O}_j \\\\) are considered invalid, and therefore, are entirely filled with zeros (i.e., \\\\( \\\\hat{O}_j \\\\odot m_j = \\\\hat{O}_j \\\\)).\\n\\n**Add Noise**\\n\\n**Denoising Network**\\n\\n\\\\[ \\\\hat{O}_j = \\\\rho(\\\\theta(x_{vis}, \\\\hat{O}_j, t)) + \\\\sigma_t \\\\epsilon_{vis} \\\\odot (1 - m) \\\\]\\n\\nwhere \\\\( \\\\epsilon_{vis}, \\\\epsilon_{invis} \\\\sim N(0, I) \\\\). The noisy image \\\\( x_t \\\\) can be simply calculated as the sum of \\\\( x_{vis} \\\\) and \\\\( x_{invis} \\\\), as expressed by\\n\\n\\\\[ x_t = x_{vis} + x_{invis} \\\\]\\n\\n**Diffusion Network.**\\n\\nWe implement the diffusion model as a UNet [57, 59] conditioned on the observed region \\\\( x_0 \\\\). We input the concatenation of \\\\( x_t \\\\) and \\\\( \\\\hat{O}_0 \\\\) into the network.\\n\\n**Classifier-free Guidance.**\\n\\nTo further enhance the controllability of the generation process, we introduce a classifier-free guidance [22] mechanism. Specifically, we train a unified network \\\\( \\\\epsilon(\\\\theta(x_t, c, t)) \\\\) comprising of an unconditional model \\\\( \\\\epsilon(\\\\theta(x_t, c, t)) \\\\), where the shared variable \\\\( c \\\\) is incorporated, and a conditional model \\\\( \\\\epsilon(\\\\theta(x_t, \\\\hat{O}_0, t)) \\\\). In this way, the predicted noise \\\\( \\\\tilde{\\\\epsilon} \\\\) can be recomputed as follows:\\n\\n\\\\[ \\\\tilde{\\\\epsilon}(x_t, \\\\hat{O}_0, t) = \\\\epsilon(x_t, c, t) + \\\\beta \\\\times [\\\\epsilon(x_t, \\\\hat{O}_0, t) - \\\\epsilon(x_t, c, t)] \\\\]\\n\\nwhere \\\\( \\\\beta \\\\geq 0 \\\\) is the guidance factor, being responsible for the trade-off between sampling quality and diversity [22].\\n\\n### 5. Experiments\\n\\n**Dataset.**\\n\\nWe conducted experiments on the ScanNet-V2 [12] dataset, which was pre-processed by removing redundant frames [69]. For training, we used the first 1,293 scenes, while for metric evaluation, we randomly selected 18 scenes with over 50 views each from the remaining as our test set. We also evaluated under various sparsity settings (5%, 10%, 20%, and 50%) by uniformly downsampling views.\\n\\n**Comparison.**\\n\\nWe compared against the neural graphics primitive (NGP) [43], which has demonstrated impressive performance in scene modeling with high efficiency. To enhance its geometric quality, we incorporated a depth supervision (DS) loss [13] to build an improved variant called DS-NGP [13, 43]. We also compare against Neural RGBD (N-RGBD) [3], which recovers implicit surfaces from RGBD scans, and Dense Depth Prior (DDP) [56], which learns a NeRF utilizing view completion from sparse RGBD views.\\n\\n**Evaluation Metrics.**\\n\\nFor assessing the visual quality of RGB images, we adopted the peak signal-to-noise ratio (PSNR), structural similarity index measure (SSIM), and learned perceptual image patch similarity (LPIPS) [89] that is based on the AlexNet [29] backbone. To evaluate the geometry quality, we computed the mean squared error (MSE) on depth maps, and sample 10,000 points uniformly on meshes constructed via back-projection to evaluate the chamfer distance (CD) and completeness (Comp.) with a threshold of 0.1m. We also measured the computational time required to execute different stages of the method.\\n\\nFor LPIPS, MSE, and CD, the lower the better; for PSNR, SSIM, and Comp., the higher the better. All reported metrics are averaged across the test scenes.\"}"}
{"id": "CVPR-2023-1715", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Implementation Details.\\n\\nOur model has 157M parameters and was trained for three days on 7 NVIDIA 3090-Ti GPUs, using a batch size of 280. The learning rate was initialized at $1 \\\\times 10^{-4}$ and reduced to $1 \\\\times 10^{-6}$ over a period of 300 epochs, utilizing a cosine annealing strategy. The image resolution is $128 \\\\times 128$ with a rendering chunk size of 7.\\n\\n5.1. Ablation Studies\\n\\nWe conducted ablation studies to validate the effectiveness of each component in our proposal. Results concluded here are nontrivial and a bit nuanced.\\n\\nEffects of Different Ingredients.\\n\\nWe examined the anticipated efficacy of the proposed conditioning and inpainting components in our approach. Table 2 displays the numerical results, and Figure 5-(a-c) presents the visualizations. The combined use of conditioning and inpainting yields superior visual performance compared to the ground truth. Nevertheless, when the model is conditioned, the impact of inpainting on geometric quality becomes less crucial. Moreover, as more views are provided, the stochastic generation process becomes increasingly deterministic, resulting in structures that more closely resemble the ground truth.\\n\\nEffects of Guidance Scale.\\n\\nThe guidance factor $\\\\beta$ significantly affects the conditioning effect on the results. To investigate the optimal $\\\\beta$ under various settings, we conducted experiments with $\\\\beta$ chosen from 0.0, 0.5, 1.0, 2.0, and 5.0, respectively, and evaluated their performance. Quantitative results are presented in Table 3. It is interesting that only a suitable value of $\\\\beta$ (1.0 or 2.0) yields optimal performance, while smaller or larger values of $\\\\beta$ result in underperformance. Surprisingly, for geometric recovery (MSE and CD), the optimal value of $\\\\beta$ appears to increase as the percentage gets larger. For a scene with a percentage $\\\\geq 50\\\\%$, the best value of $\\\\beta$ is amazingly greater than 5.0. However, such a large percentage is not optimal for visual appearance. This is because using a larger value of $\\\\beta$ at a low percentage can cause the generated results to deviate unexpectedly from the ground truth. Our visualization, presented in Figure 5-(a, c, d), indicates that using an unconditional model ($\\\\beta=0$) leads to undesired and bizarre geometric structures since the network fails to understand the context provided by known views. However, excessively large values of $\\\\beta$ also oversaturate the color (e.g. row \"(d)\" at 5%), making the visual appearance unrealistic.\\n\\nEffects of Randomness and Trajectories.\\n\\nWe investigated the impact of employing different random seeds and camera trajectories on the results. Such obtained meshes are visualized in Figure 4. As diffusion models inherently include randomness in their reverse sampling process, by switching to a different random seed, we can obtain another outcome. Moreover, the camera trajectory can affect the rough shape of the scene. It is promising that our approach can yield controllable and editable results, as it is capable of generating various and appealing outcomes by simply altering these two factors.\\n\\n5.2. Generative Scene Synthesis\\n\\nWe evaluated the performance of our method against other similar approaches on this task through extensive experiments under various sparsity settings. Table 1 presents the time required by different methods across various stages. Our approach stands out for its efficiency, as it eliminates the need for additional optimization on individual test scenes during inference. Quantitative results are shown in Table 4. We observe that our approach exhibits a clear advantage in visual metrics over the others when the provided views are extremely sparse (5%). Interestingly, our method consistently achieves the best performance in all the geometry-related metrics. Qualitative results are presented in Figure 6. DS-NGP [13, 43] and DDP [56] struggle to accurately recover geometry due to their inability to hallucinate and extrapolate missing regions. In comparison, N-RGBD [3] can achieve better surface completion results by learning and extrapolating neural implicit surfaces. However, their performance significantly degrades when input views are extraordinarily sparse (5%). In contrast, our method consistently exhibits plausible appearance that closely resembles the ground truth, particularly in scenarios with sparse-view inputs.\\n\\n6. Discussions\\n\\nLimitations.\\n\\nOur current implementation has several limitations that may impact its usefulness in some scenarios. Firstly, it is incapable of handling color discrepancies caused by lighting variations. Secondly, it lacks surface extrapolation capabilities that can be provided through implicit field representation. Lastly, the limited receptive field of our design is confined to the observable volume of the current camera view and may result in inconsistent and discontinuous predictions, particularly in the case of a large...\"}"}
{"id": "CVPR-2023-1715", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Factors\\n\\nVisual Geometric\\n\\n| PSNRcolor | SSIMcolor | LPIPScolor | MSEdepth | CDmesh | Comp.mesh |\\n|-----------|-----------|------------|----------|--------|-----------|\\n| 9.33      | 0.331     | 0.330      | 0.637    | 0.635  | 1.309     |\\n| 9.30      | 0.330     | 0.330      | 0.636    | 0.635  | 1.304     |\\n| 9.27      | 0.333     | 0.333      | 0.637    | 0.635  | 1.310     |\\n| 9.45      | 0.330     | 0.330      | 0.637    | 0.635  | 1.293     |\\n\\nTable 2.\\n\\nAblation studies of the effects of conditioning (Cond., $\\\\beta=1$) and inpainting (Inpa.) on visual (color images) and geometric (depth maps and meshes) results on the task of scene synthesis from sparse RGBD inputs. The combined use of conditioning and inpainting leads to superior performance in visual metrics, and inpainting plays a less significant role in terms of geometric results when the model is conditioned.\\n\\nFigure 5.\\n\\nQualitative results of ablation studies on the task of scene synthesis from sparse RGBD inputs. \\\"(a)\\\", \\\"(b)\\\", and \\\"(c)\\\" correspond to the second (w/o conditioning, $\\\\beta=0$), third (w/o inpainting, $\\\\beta=1$), and fourth (w/ all, $\\\\beta=1$) rows of Table 2, and \\\"(d)\\\" corresponds to the last (w/ all, $\\\\beta=5$) row of Table 3. Each image in the grid consists of four sub-images, with two located at the lower-right corner displaying rendered RGBD images generated from either NeRFs (others) or meshes (ours), and the other two positioned at the top-left corner and underneath, respectively, showcasing the back-projected triangular meshes captured from a close-up perspective.\\n\\nIt is clear that the third row basically shows the most favorable appearance.\\n\\nGuidance Factor\\n\\n| $\\\\beta$ | PSNRcolor | SSIMcolor | LPIPScolor | MSEdepth | CDmesh | Comp.mesh |\\n|---------|-----------|-----------|------------|----------|--------|-----------|\\n| 5%      | 12.4      | 14.7      | 16.5       | 17.9     | 0.411  | 0.496     |\\n| 10%     | 13.2      | 15.5      | 17.1       | 18.2     | 0.452  | 0.530     |\\n| 20%     | 14.6      | 16.0      | 17.4       | 18.4     | 0.522  | 0.555     |\\n| 50%     | 14.5      | 15.8      | 17.5       | 18.4     | 0.532  | 0.561     |\\n| 1.0     | 14.6      | 16.0      | 17.4       | 18.4     | 0.522  | 0.555     |\\n| 2.0     | 14.5      | 15.8      | 17.5       | 18.4     | 0.532  | 0.561     |\\n| 5.0     | 13.3      | 14.9      | 17.1       | 18.2     | 0.488  | 0.531     |\\n\\nTable 3.\\n\\nAblation studies of the effects of guidance factor $\\\\beta$ on visual (color images) and geometric (depth maps and meshes) results on the task of scene synthesis from sparse RGBD inputs. A larger $\\\\beta$ ideally strengthens the conditioning effect. The visual metrics are found to be the best when $\\\\beta$ is set to 1 or 2. Interestingly, the optimal value of $\\\\beta$ for the best geometry appears to increase as views become denser. However, setting such a large value of $\\\\beta$ (e.g. $\\\\beta=5$) leads to sub-optimal visual results.\\n\\nFuture Works.\\n\\nTo improve our design, potential areas of investigation include: (1) Modeling color smoothness and variation, as demonstrated by NeRF [42]. (2) Supporting advanced physical lighting effects, such as SVBRDF, as implemented in TANGO [9]. (3) Incorporating appearance/surface extrapolation by learning an implicit field, such as NeRF [42, 43] or SDF [3, 80]. (4) Exploring generative [5] or optimizable [6, 84] camera trajectories for scene synthesis. (5) Investigating reconstruction from sparse-view RGB inputs only, using depth inpainting/estimation following the reverse sampling technique proposed in [37] by utilizing a versatile RGBD diffusion model. (6) Leveraging the multi-modal [52] or generative [57] power of large-scale pre-trained models, such as the recently widespread Stable Diffusion [57].\"}"}
{"id": "CVPR-2023-1715", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"### Table 4.\\nQuantitative comparison with other approaches on visual (color images) and geometric (depth maps and meshes) results on the task of scene synthesis from sparse RGBD inputs.\\n\\n| Method   | PSNR Color | SSIM Color | LPIPS Color | MSE Depth | CDMesh | Comp. Mesh |\\n|----------|------------|------------|-------------|-----------|--------|------------|\\n| NGP      | 10.4       | 0.293      | 0.476       | 7.01      | 29994  | 0.289      |\\n| DS-NGP   | 10.1       | 0.205      | 0.533       | 2.30      | 5362   | 0.529      |\\n| N-RGBD   | 14.1       | 0.401      | 0.384       | 1.51      | 3503   | 0.705      |\\n| DDP      | 14.1       | 0.418      | 0.517       | 1.44      | 2363   | 0.507      |\\n| **Ours** | **14.6**   | **0.522**  | **0.448**   | **0.82**  | 1058   | **0.747**  |\\n\\nOur approach outperforms other methods consistently in terms of geometric metrics as the sparsity varies. However, our method only show a definitive visual advantage across various visual metrics when the input views are highly sparse (5%).\\n\\n**Figure 6.** Qualitative comparison results with DS-NGP [13, 43], DDP [56], N-RGBD [3] on the task of scene synthesis from sparse RGBD inputs. Each image in the grid consists of four sub-images, with two located at the lower-right corner displaying rendered RGBD images generated from either NeRFs (others) or meshes (ours), and the other two positioned at the top-left corner and underneath, respectively, showcasing the back-projected triangular meshes captured from a close-up perspective. Our approach produces images that exhibit a close resemblance to the ground truth (GT), while comparative methods may result in inferior outcomes, especially when dealing with excessively sparse input views and outward-looking cameras.\"}"}
