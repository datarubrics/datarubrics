{"id": "CVPR-2022-1925", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nWe present TubeFormer-DeepLab, the first attempt to tackle multiple core video segmentation tasks in a unified manner. Different video segmentation tasks (e.g., video semantic/instance/panoptic segmentation) are usually considered as distinct problems. State-of-the-art models adopted in the separate communities have diverged, and radically different approaches dominate in each task. By contrast, we make a crucial observation that video segmentation tasks could be generally formulated as the problem of assigning different predicted labels to video tubes (where a tube is obtained by linking segmentation masks along the time axis) and the labels may encode different values depending on the target task. The observation motivates us to develop TubeFormer-DeepLab, a simple and effective video mask transformer model that is widely applicable to multiple video segmentation tasks. TubeFormer-DeepLab directly predicts video tubes with task-specific labels (either pure semantic categories, or both semantic categories and instance identities), which not only significantly simplifies video segmentation models, but also advances state-of-the-art results on multiple video segmentation benchmarks.\\n\\n1. Introduction\\n\\nWe observe that video segmentation tasks could be formulated as partitioning video frames into tubes with different predicted labels, where a tube contains segmentation masks linked along the time axis. Based on the target task, the predicted labels may encode only semantic categories (e.g., Video Semantic Segmentation (VSS) [7, 58]), or both semantic categories and instance identities (e.g., Video Instance Segmentation (VIS) [68, 77] for only foreground \u2018things\u2019, or Video Panoptic Segmentation (VPS) [41, 73] for both foreground \u2018things\u2019 and background \u2018stuff\u2019) (Fig. 1). However, the underlying similarity of several video segmentation tasks (i.e., assigning tubes with predicted labels) has been long overlooked, and thus models developed for video semantic, instance, and panoptic segmentation have\\n\\nFigure 1. Video segmentation tasks can be formulated as partitioning video frames (e.g., a clip) into tubes (i.e., segmentation masks linked along time) with different labels. TubeFormer-DeepLab directly predicts class-labeled tubes, providing a simple and general solution to Video Semantic Segmentation (VSS), Video Instance Segmentation (VIS), and Video Panoptic Segmentation (VPS).\\n\\nFigure 2. Our proposed hierarchical dual-path transformer performs attention on three consecutive input frames (a) for VSS, VIS, and VPS tasks. While the global memory learns the spatio-temporally clustered attention for individual tube regions (b), our latent memory learns task-specific attention (c).\"}"}
{"id": "CVPR-2022-1925", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"fundamentally diverged. For example, some VSS methods \\\\cite{26, 86} warp features between video frames, while the modern VIS model \\\\cite{5} predicts hundreds of frame-level instance masks \\\\cite{31} and then propagates them to other neighboring frames. To make matters more complicated, state-of-the-art VPS methods \\\\cite{62, 74} adopt separate prediction branches, specific to semantic segmentation, instance segmentation, and object tracking, respectively.\\n\\nIn this work, instead of exacerbating the bifurcation between video segmentation models, we take a step back and rethink the following question:\\n\\nCan we exploit the similar nature between video segmentation tasks, and develop a single model that is both effective and generally applicable?\\n\\nTo answer this, we propose TubeFormer-DeepLab that builds upon mask transformers \\\\cite{69} for video segmentation by directly predicting class-labeled tubes, where the labels encode different values depending on the target task.\\n\\nSpecifically, similar to other Transformer architectures \\\\cite{9, 67}, TubeFormer-DeepLab extends the mask transformer \\\\cite{69} to generate a set of pairs, each containing a class prediction and a tube embedding vector. The tube embedding vector, multiplied by the video pixel embedding features obtained by a convolutional network \\\\cite{45}, yields the tube prediction. As a result, TubeFormer-DeepLab presents the first attempt to tackle multiple core video segmentation tasks in a general framework without the need to adapt the system for any task-specific design.\\n\\nNa\u00efvely applying the image-level mask transformer \\\\cite{69} to the video domain does not yield a satisfactory result, mainly due to the difficulty of learning attentions for video-clip (i.e., multi-frames) features with large spatial resolutions. To alleviate the issue, we introduce the latent dual-path transformer block that is in charge of passing messages between video-frame (i.e., single-frame) features and a latent memory, followed by the global dual-path transformer block that learns the attentions between video-clip features and a global memory. This hierarchical dual-path transformer framework facilitates the attention learning and significantly improves the video segmentation results. Interestingly, as shown in Fig. 2, our latent memory learns task-specific attention, while the global memory learns the spatio-temporally clustered attention for individual tube regions. Additionally, we split the global memory into two sets, thing-specific and stuff-specific global memory, with the motivation to exploit the different nature of \u2018thing\u2019 (countable instances) and \u2018stuff\u2019 (amorphous regions).\\n\\nDuring inference, practically we could only fit a video clip (i.e., a short video sequence) for video segmentation. The whole video sequence segmentation result is thus obtained by applying the video stitching \\\\cite{63} to merge clip segmentation results. To enforce the consistency between video clips, we additionally propose a Temporal Consistency loss that encourages the model to learn consistent predictions in the overlapping frames between clips.\\n\\nFinally, we propose a simple and effective data augmentation policy by extending the image-level thing-specific copy-paste \\\\cite{24, 29}. Our method, named clip-paste (clip-level copy-paste), randomly pastes either \u2018thing\u2019 or \u2018stuff\u2019 (or both) regions from a video clip to the target video clip.\\n\\nTo demonstrate the effectiveness of our proposed TubeFormer-DeepLab, we conduct experiments on multiple core video segmentation datasets, including KITTI-STEP (VPS) \\\\cite{73}, VSPW (VSS) \\\\cite{58}, YouTube-VIS (VIS) \\\\cite{77}, and SemKITTI-DVPS (depth-aware VPS) \\\\cite{63}. Our single model not only significantly simplifies video segmentation systems (e.g., the proposed model is end-to-end trained and does not require any task-specific design), but also advances state-of-the-art performance on several benchmarks. In particular, TubeFormer-DeepLab outperforms published works Motion-DeepLab \\\\cite{73} by +13.1 STQ on KITTI-STEP test set, TCB \\\\cite{58} by +21 mIoU on VSPW test set, IFC \\\\cite{37} by +2.9 track-mAP on YouTube-VIS-2019 val set, and ViP-DeepLab \\\\cite{63} by +3.6 DSTQ on SemKITTI-DVPS test set. Our experimental results validate TubeFormer-DeepLab\u2019s general efficacy for video segmentation tasks.\"}"}
{"id": "CVPR-2022-1925", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tic Segmentation attempts to unify video semantic and instance segmentation, requiring temporally consistent panoptic segmentation results. Different from VIS, VPS disallows overlapping instance masks and requires labeling each pixel, including both 'thing' and 'stuff' pixels. Current state-of-the-art approaches \\\\[63, 73, 74\\\\] adopted complicated pipelines due to the intricate nature of VPS. Specifically, VPSNet \\\\[41\\\\] contains multiple task-specific heads, including Mask R-CNN \\\\[31\\\\], deformable convolutions \\\\[20\\\\], and MaskTrack \\\\[77\\\\] for instance segmentation, semantic segmentation, and tracking, respectively, while ViP-DeepLab \\\\[63\\\\] extends Panoptic-DeepLab \\\\[17\\\\] (which employs dual-ASPP \\\\[13\\\\] and dual-decoder structures specific to semantic and instance segmentation, respectively) by adding another next-frame instance segmentation branch. On the other hand, our approach significantly simplifies the current pipeline by employing mask transformers \\\\[69\\\\] to directly predict clip-level mask segmentation results. Finally, our proposed model could also be easily extended to the recent task of Depth-aware Video Panoptic Segmentation (DVPS) \\\\[63\\\\], which further requires per-pixel depth estimation on top of VPS results.\\n\\n3. Method\\n\\nIn this section, we introduce the formulation of several video segmentation tasks, followed by a general formulation that inspires our TubeFormer-DeepLab. We then present its model design, training and inference strategies.\\n\\n3.1. Video Segmentation Formulation\\n\\nLet us denote with \\\\(v \\\\in \\\\mathbb{R}^{T \\\\times H \\\\times W \\\\times 3}\\\\) an input video clip containing \\\\(T\\\\) video frames of spatial size \\\\(H \\\\times W\\\\) (\\\\(T\\\\) could be equal to the video sequence length if memory allows). The video clip is annotated with a set of class-labeled tubes (a tube is defined as segmentation masks linked along the time axis):\\n\\n\\\\[\\n\\\\{y_i^{K_i=1}\\\\} = \\\\{ (m_i, c_i) \\\\}^{K_i=1},\\n\\\\]\\n\\nwhere the ground truth tubes \\\\(m_i \\\\in \\\\{0, 1\\\\}^{T \\\\times H \\\\times W}\\\\) do not overlap with each other, and \\\\(c_i\\\\) denotes the ground truth class label of tube \\\\(m_i\\\\).\\n\\nBelow, we briefly introduce several tasks.\\n\\nVideo Semantic Segmentation (VSS) is typically formulated as per-video pixel classification, where the pixel features for classification are enriched by warping \\\\[86\\\\] or aggregating \\\\[58\\\\] features from neighboring frames. Formally, the model predicts the probability distribution over a predefined set of categories \\\\(C = \\\\{1, \\\\ldots, D\\\\}\\\\) for every video pixel:\\n\\n\\\\[\\n\\\\hat{p}_i | \\\\hat{p}_i \\\\in \\\\Delta_D^{T \\\\times H \\\\times W_{i=1}},\\n\\\\]\\n\\nwhere \\\\(\\\\Delta_D\\\\) is the \\\\(D\\\\)-dimensional probability simplex. The final segmentation output \\\\(\\\\hat{y}\\\\) is then obtained by taking its argmax (i.e., \\\\(\\\\hat{y}_i = \\\\arg \\\\max_c \\\\hat{p}_i(c), \\\\forall i \\\\in \\\\{1, 2, \\\\ldots, T \\\\times H \\\\times W\\\\}\\\\)).\\n\\nVideo Instance Segmentation (VIS) requires to segment and temporally link object instances in the video. For each detected foreground 'thing' \\\\(i\\\\) in the video, the model predicts a video tube (i.e., video-level instance mask track) \\\\(\\\\hat{m}_i \\\\in [0, 1]^{T \\\\times H \\\\times W}\\\\) with a probability distribution \\\\(\\\\hat{p}_i\\\\) over \\\\(C\\\\) defined for only thing classes. Depending on the target dataset or evaluation metric, the model may generate overlapping video tubes (e.g., YouTube-VIS \\\\[77\\\\] adopts track-mAP, allowing overlapping predicted tubes, while KITTI-MOTS \\\\[68\\\\] adopts HOTA \\\\[57\\\\], disallowing so).\\n\\nVideo Panoptic Segmentation (VPS) requires temporally consistent semantic and instance segmentation results for both 'thing' and 'stuff' classes. Specifically, the model predicts a set of non-overlapping video tubes \\\\(\\\\{\\\\hat{y}_i^{N_i=1}\\\\} = \\\\{(\\\\hat{m}_i, \\\\hat{p}_i(c))\\\\}^{N_i=1}\\\\), where \\\\(\\\\hat{m}_i \\\\in [0, 1]^{T \\\\times H \\\\times W}\\\\) denotes the predicted tube, and \\\\(\\\\hat{p}_i(c)\\\\) denotes the probability of assigning class \\\\(c\\\\) to tube \\\\(\\\\hat{m}_i\\\\) belonging to a predefined category set \\\\(C\\\\) that contains both 'thing' and 'stuff' classes.\\n\\nDepth-aware Video Panoptic Segmentation (DVPS) builds on top of VPS by additionally requiring a model to estimate the depth value of each pixel. Similar to VPS output, the prediction has the following format:\\n\\n\\\\[\\n\\\\{\\\\hat{y}_i^{N_i=1}\\\\} = \\\\{(\\\\hat{m}_i, \\\\hat{p}_i(c), \\\\hat{d}_i)\\\\}^{N_i=1},\\n\\\\]\\n\\nwhere \\\\(\\\\hat{d}_i \\\\in [0, d_{max}]^{T \\\\times H \\\\times W}\\\\) denotes the estimated depth value and \\\\(d_{max}\\\\) is the maximum depth value specified in the target dataset. Accordingly, the dataset contains ground truth depth.\\n\\nGeneral task formulation. Despite the superficial differences between tasks, we discover the underlying similarity that video segmentation tasks could be generally formulated as the problem of assigning different predicted labels to video tubes and the labels may encode different values depending on the target task. For example, if only semantic categories are predicted, it becomes video semantic segmentation. Similarly, if both semantic categories and instance identities are required (i.e., one predicted tube for each category-identity pair), it then becomes either video instance segmentation (if only foreground 'thing' classes are considered) or video panoptic segmentation. This motivates us to develop a general video segmentation model that directly predicts class-labeled tubes \\\\(\\\\{\\\\hat{y}_i^{N_i=1}\\\\} = \\\\{(\\\\hat{m}_i, \\\\hat{p}_i(c))\\\\}^{N_i=1}\\\\) (and optionally depth, if required).\\n\\n3.2. TubeFormer-DeepLab Architecture\\n\\nWe first introduce TubeFormer-DeepLab-Simple, our video-level baseline, which will be improved by our proposed latent dual-path transformer, resulting in the final TubeFormer-DeepLab.\\n\\nTubeFormer-DeepLab-Simple. We adopt the per-clip pipeline which takes a video clip and outputs clip-level results. Inspired by \\\\[69\\\\], our TubeFormer-DeepLab-Simple integrates a CNN backbone and a global memory feature in a dual-path architecture, i.e., global dual-path transformer. Given an input video clip \\\\(v\\\\), the CNN backbone processes the input frames independently, and generates pixel features \\\\(x_v \\\\in \\\\mathbb{R}^{T \\\\times H \\\\times W \\\\times C}\\\\), where \\\\(C\\\\) is channels. The pixel self-attention is performed at the frame level...\"}"}
{"id": "CVPR-2022-1925", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"TubeFormer-DeepLab architecture overview.\\n\\nFig. 3. TubeFormer-DeepLab extends the mask transformer [69] to generate a set of pairs, each containing a class prediction \\\\( p(c) \\\\) and a tube embedding vector \\\\( w \\\\). The tube embedding vector, multiplied by the video pixel embedding features \\\\( x_v' \\\\) obtained by a convolutional network, yields the tube prediction \\\\( \\\\hat{m} \\\\). We introduce a hierarchical structure with the latent dual-path transformer block that is in charge of passing messages between frame-level features \\\\( x_f \\\\) and a latent memory \\\\( x_l \\\\), followed by the global dual-path transformer block that learns the attentions between video-clip features \\\\( x_v \\\\) and a global memory \\\\( x_m \\\\).\\n\\nAfterwards, the global dual-path transformer operates in a per-clip manner, taking the flattened video pixel features \\\\( x_v \\\\in \\\\mathbb{R}^{THW \\\\times C} \\\\) and a 1D global memory \\\\( x_m \\\\in \\\\mathbb{R}^{N \\\\times C} \\\\) of length \\\\( N \\\\) (i.e., the size of the prediction set). Passing through the global dual-path transformer, we expect three attentions: (1) memory-to-video (M2V) attention (in which the video features encode per-clip information to the memory feature), (2) memory-to-memory (M2M) self-attention, and (3) video-to-memory (V2M) attention (in which the video pixel features refine themselves by receiving tube-level information gathered in the global memory). The global dual-path transformer blocks can be stacked multiple times at any layers of the network.\\n\\nOn top of the global memory, there are two output heads: a segmentation head and a class head, each composed of two Fully-Connected (FC) layers. The global memory of size \\\\( N \\\\) is independently passed to the two heads, resulting in \\\\( N \\\\) unique tube embeddings \\\\( w \\\\in \\\\mathbb{R}^{N \\\\times C} \\\\) and \\\\( N \\\\) corresponding class predictions \\\\( p(c) \\\\in \\\\mathbb{R}^{N \\\\times |C|} \\\\). Note that the possible classes \\\\( C \\\\ni c \\\\) include \\\"none\\\" category \\\\( \\\\emptyset \\\\) in case the embedding does not correspond to any region in a clip. Our video tube prediction \\\\( \\\\hat{m} \\\\) is computed in one shot as a dot-product between the decoded video pixel features \\\\( x_v' \\\\) and the tube embeddings \\\\( w \\\\):\\n\\n\\\\[\\n\\\\hat{m} = \\\\text{softmax}(N(x_v' \\\\cdot w)) \\\\in \\\\mathbb{R}^{N \\\\times T \\\\times H \\\\times W}.\\n\\\\]\\n\\nThe final video-clip segmentation \\\\( \\\\{\\\\hat{y}_i\\\\}_{i=1}^N \\\\) can be obtained by combining \\\\( N \\\\) binary video tubes with their corresponding class predictions.\"}"}
{"id": "CVPR-2022-1925", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Ali Athar, Sabarinath Mahadevan, Aljo \u02c7sa O \u02c7sep, Laura Leal-Taix\u00b4e, and Bastian Leibe. STEm-Seg: Spatio-temporal em-beddings for instance segmentation in videos. In ECCV, 2020. 7\\n\\n[2] Hangbo Bao, Li Dong, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv:2106.08254, 2021. 6\\n\\n[3] Jens Behley, Martin Garbade, Andres Milioto, Jan Quenzel, Sven Behnke, Cyrill Stachniss, and Jurgen Gall. Semantickitti: A dataset for semantic scene understanding of lidar sequences. In ICCV, 2019. 6\\n\\n[4] Philipp Bergmann, Tim Meinhardt, and Laura Leal-Taixe. Tracking without bells and whistles. In ICCV, 2019. 2\\n\\n[5] Gedas Bertasius and Lorenzo Torresani. Classifying, segmenting, and tracking object instances in video with mask propagation. In CVPR, 2020. 2, 6, 7\\n\\n[6] Michael D Breitenstein, Fabian Reichlin, Bastian Leibe, Esther Koller-Meier, and Luc Van Gool. Robust tracking-by-detection using a detector confidence particle filter. In ICCV, 2009. 2\\n\\n[7] Gabriel J. Brostow, Jamie Shotton, Julien Fauqueur, and Roberto Cipolla. Segmentation and recognition using structure from motion point clouds. In ECCV, 2008. 1, 2\\n\\n[8] Jiale Cao, Rao Muhammad Anwer, Hisham Cholakkal, Fa-had Shahbaz Khan, Yanwei Pang, and Ling Shao. Sipmask: Spatial information preservation for fast image and video instance segmentation. In ECCV, 2020. 2, 7\\n\\n[9] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020. 2\\n\\n[10] Kai Chen, Jiangmiao Pang, Jiaqi Wang, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jianping Shi, Wanli Ouyang, et al. Hybrid task cascade for instance segmentation. In CVPR, 2019. 2\\n\\n[11] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Semantic image segmentation with deep convolutional nets and fully connected CRFs. In ICLR, 2015. 2\\n\\n[12] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. DeepLab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. IEEE TPAMI, 2017. 5\\n\\n[13] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv:1706.05587, 2017. 3\\n\\n[14] Liang-Chieh Chen, Huiyu Wang, and Siyuan Qiao. Scaling wide residual networks for panoptic segmentation. arXiv:2011.11675, 2020. 6\\n\\n[15] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In ECCV, 2018. 2, 5\\n\\n[16] Zixuan Chen, Junhong Zou, and Xiaotao Wang. Semantic Segmentation on VSPW Dataset through Aggregation of Transformer Models. In ICCV The 1st Video Scene Parsing in the Wild Challenge Workshop, 2021. 7\\n\\n[17] Bowen Cheng, Maxwell D Collins, Yukun Zhu, Ting Liu, Thomas S Huang, Hartwig Adam, and Liang-Chieh Chen. Panoptic-DeepLab: A simple, strong, and fast baseline for bottom-up panoptic segmentation. In CVPR, 2020. 2, 3\\n\\n[18] Bowen Cheng, Alexander G Schwing, and Alexander Kirillov. Per-pixel classification is not all you need for semantic segmentation. In NeurIPS, 2021. 5\\n\\n[19] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 2, 6\\n\\n[20] Jifeng Dai, Haozhi Qi, Yuwen Xiong, Yi Li, Guodong Zhang, Han Hu, and Yichen Wei. Deformable convolutional networks. In ICCV, 2017. 2, 3\\n\\n[21] Patrick Dendorfer, Aljo \u02c7sa O \u02c7sep, Anton Milan, Konrad Schindler, Daniel Cremers, Ian Reid, Stefan Roth, and Laura Leal-Taix\u00b4e. MOTChallenge: A Benchmark for Single-camera Multiple Target Tracking. IJCV, 2020. 2\\n\\n[22] David Eigen, Christian Puhrsch, and Rob Fergus. Depth map prediction from a single image using a multi-scale deep network. In NeurIPS, 2014. 5, 6\\n\\n[23] Mark Everingham, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes (VOC) challenge. IJCV, 88(2):303\u2013338, 2010. 2\\n\\n[24] Hao-Shu Fang, Jianhua Sun, Runzhong Wang, Minghao Gou, Yong-Lu Li, and Cewu Lu. Instaboost: Boosting instance segmentation via probability map guided copy-pasting. In ICCV, 2019. 2, 5\\n\\n[25] Yang Fu, Linjie Yang, Ding Liu, Thomas S Huang, and Humphrey Shi. Compfeat: Comprehensive feature aggregation for video instance segmentation. In AAAI, 2021. 2\\n\\n[26] Raghudeep Gadde, Varun Jampani, and Peter V Gehler. Semantic video CNNs through representation warping. In ICCV, 2017. 2\\n\\n[27] Andreas Geiger, Philip Lenz, Christoph Stiller, and Raquel Urtasun. Vision meets robotics: The KITTI dataset. The International Journal of Robotics Research, 2013. 2\\n\\n[28] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In CVPR, 2012. 5\\n\\n[29] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, Tsung-Yi Lin, Ekin D Cubuk, Quoc V Le, and Barret Zoph. Simple copy-paste is a strong data augmentation method for instance segmentation. In CVPR, 2021. 2, 5\\n\\n[30] Bharath Hariharan, Pablo Arbel \u00b4aez, Ross Girshick, and Jitendra Malik. Simultaneous detection and segmentation. In ECCV, 2014. 2\\n\\n[31] Kaiming He, Georgia Gkioxari, Piotr Doll \u00b4ar, and Ross Girshick. Mask r-cnn. In ICCV, 2017. 2, 3\\n\\n[32] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016. 6\\n\\n[33] Xingjian He, Weining Wang, Zhiyong Xu, Hao Wang, Jie Jiang, and Jing Liu. Exploiting Spatial-Temporal Semantic Consistency for Video Scene Parsing. In ICCV The 1st Video Scene Parsing in the Wild Challenge Workshop, 2021. 6, 7\\n\\n[13922]\"}"}
{"id": "CVPR-2022-1925", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Xuming He, Richard S Zemel, and Miguel \u00b4A Carreira-Perpi\u02dcn\u00b4an. Multiscale conditional random fields for image labeling. In CVPR, 2004.\\n\\nBerthold KP Horn and Brian G Schunck. Determining optimal flow. Artificial intelligence, 17(1-3):185\u2013203, 1981.\\n\\nZilong Huang, Xinggang Wang, Lichao Huang, Chang Huang, Yunchao Wei, and Wenyu Liu. Ccnet: Criss-cross attention for semantic segmentation. In ICCV, 2019.\\n\\nSukjun Hwang, Miran Heo, Seoung Wug Oh, and Seon Joo Kim. Video instance segmentation using inter-frame communication transformers. In NeurIPS, 2021.\\n\\nMax Jaderberg, Karen Simonyan, Andrew Zisserman, et al. Spatial transformer networks. In NeurIPS, 2015.\\n\\nSamvit Jain, Xin Wang, and Joseph E Gonzalez. Accel: A corrective fusion network for efficient semantic segmentation on video. In CVPR, 2019.\\n\\nZhenchao Jin, Dongdong Yu, Kai Su, Zehuan Yuan, and Changhu Wang. Memory Based Video Scene Parsing. In ICCV The 1st Video Scene Parsing in the Wild Challenge Workshop, 2021.\\n\\nDahun Kim, Sanghyun Woo, Joon-Young Lee, and In So Kweon. Video panoptic segmentation. In CVPR, 2020.\\n\\nAlexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll\u00b4ar. Panoptic feature pyramid networks. In CVPR, 2019.\\n\\nAlexander Kirillov, Kaiming He, Ross Girshick, Carsten Rother, and Piotr Doll \u00b4ar. Panoptic segmentation. In CVPR, 2019.\\n\\nDaphne Koller and Nir Friedman. Probabilistic graphical models: principles and techniques. MIT press, 2009.\\n\\nYann LeCun, L \u00b4eon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):2278\u20132324, 1998.\\n\\nXiangtai Li, Haobo Yuan, Yibo Yang, Lefei Zhang, Yunhai Tong, and Dacheng Tao. PolyphonicFormer: Unified Query Learning for Depth-aware Video Panoptic Segmentation. In ICCV Segmenting and Tracking Every Point and Pixel: 6th Workshop on Benchmarking Multi-Target Tracking, 2021.\\n\\nYanwei Li, Xinze Chen, Zheng Zhu, Lingxi Xie, Guan Huang, Dalong Du, and Xingang Wang. Attention-guided unified network for panoptic segmentation. In CVPR, 2019.\\n\\nYule Li, Jianping Shi, and Dahua Lin. Low-latency video semantic segmentation. In CVPR, 2018.\\n\\nYanwei Li, Hengshuang Zhao, Xiaojuan Qi, Liwei Wang, Zeming Li, Jian Sun, and Jiaya Jia. Fully convolutional networks for panoptic segmentation. In CVPR, 2021.\\n\\nHuaijia Lin, Ruizheng Wu, Shu Liu, Jiangbo Lu, and Jiaya Jia. Video instance segmentation with a propose-reduce paradigm. In ICCV, 2021.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00b4ar, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014.\\n\\nDongfang Liu, Yiming Cui, Wenbo Tan, and Yingjie Chen. Sg-net: Spatial granularity network for one-stage video instance segmentation. In CVPR, 2021.\\n\\nShu Liu, Lu Qi, Haifang Qin, Jianping Shi, and Jiaya Jia. Path aggregation network for instance segmentation. In CVPR, 2018.\\n\\nZe Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In ICCV, 2021.\\n\\nJonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015.\\n\\nJincheng Lu, Yue He, Minyue Jiang, Meng Xia, Wei Zhang, Xiao Tan, YingYing Li, Hao Sun, and Errui Ding. Robust Video Panoptic Segmentation and Tracking. In ICCV Segmenting and Tracking Every Point and Pixel: 6th Workshop on Benchmarking Multi-Target Tracking, 2021.\\n\\nJonathon Luiten, Aljo \u02c7sa O \u02c7sep, Patrick Dendorfer, Philip Torr, Andreas Geiger, Laura Leal-Taix\u00b4e, and Bastian Leibe. HOTA: A Higher Order Metric for Evaluating Multi-Object Tracking. IJCV, 2020.\\n\\nJiaxu Miao, Yunchao Wei, Yu Wu, Chen Liang, Guangrui Li, and Yi Yang. Vspw: A large-scale dataset for video scene parsing in the wild. In CVPR, 2021.\\n\\nDavid Nilsson and Cristian Sminchisescu. Semantic video segmentation by gated recurrent flow propagation. In CVPR, 2018.\\n\\nSeoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using space-time memory networks. In ICCV, 2019.\\n\\nJinlong Peng, Changan Wang, Fangbin Wan, Yang Wu, Yabiao Wang, Ying Tai, Chengjie Wang, Jilin Li, Feiyue Huang, and Yanwei Fu. Chained-tracker: Chaining paired attentive regression results for end-to-end joint multiple-object detection and tracking. In ECCV, 2020.\\n\\nSiyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors: Detecting objects with recursive feature pyramid and switchable atrous convolution. In CVPR, 2021.\\n\\nSiyuan Qiao, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. ViP-DeepLab: Learning Visual Perception with Depth-aware Video Panoptic Segmentation. In CVPR, 2021.\\n\\nFranco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE transactions on neural networks, 20(1):61\u201380, 2008.\\n\\nZhi Tian, Chunhua Shen, and Hao Chen. Conditional convolutions for instance segmentation. In ECCV, 2020.\\n\\nZhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS: Fully convolutional one-stage object detection. In ICCV, 2019.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\"}"}
{"id": "CVPR-2022-1925", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Bastian Leibe. \\n\\nHuiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. MaX-DeepLab: End-to-End Panoptic Segmentation with Mask Transformers. In CVPR, 2021.\\n\\nHuiyu Wang, Yukun Zhu, Bradley Green, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Axial-DeepLab: Stand-Alone Axial-Attention for Panoptic Segmentation. In ECCV, 2020.\\n\\nYuqing Wang, Zhaoliang Xu, Xinlong Wang, Chunhua Shen, Baoshan Cheng, Hao Shen, and Huaxia Xia. End-to-end video instance segmentation with transformers. In CVPR, 2021.\\n\\nMark Weber, Huiyu Wang, Siyuan Qiao, Jun Xie, Maxwell D. Collins, Yukun Zhu, Liangzhe Yuan, Dahun Kim, Qihang Yu, Daniel Cremers, Laura Leal-Taixe, Alan L. Yuille, Florian Schroff, Hartwig Adam, and Liang-Chieh Chen. DeepLab2: A TensorFlow Library for Deep Labeling. arXiv: 2106.09748, 2021.\\n\\nMark Weber, Jun Xie, Maxwell Collins, Yukun Zhu, Paul Voigtlaender, Hartwig Adam, Bradley Green, Andreas Geiger, Bastian Leibe, Daniel Cremers, Aljosa Osep, Laura Leal-Taixe, and Liang-Chieh Chen. Step: Segmenting and tracking every pixel. In NeurIPS Track on Datasets and Benchmarks, 2021.\\n\\nSanghyun Woo, Dahun Kim, Joon-Young Lee, and In So Kweon. Learning to associate every segment for video panoptic segmentation. In CVPR, 2021.\\n\\nTete Xiao, Yingcheng Liu, Bolei Zhou, Yuning Jiang, and Jian Sun. Unified perceptual parsing for scene understanding. In ECCV, 2018.\\n\\nYuwen Xiong, Renjie Liao, Hengshuang Zhao, Rui Hu, Min Bai, Ersin Yumer, and Raquel Urtasun. UPSNet: A unified panoptic segmentation network. In CVPR, 2019.\\n\\nLinjie Yang, Yuchen Fan, and Ning Xu. Video Instance Segmentation. In ICCV, 2019.\\n\\nShusheng Yang, Yuxin Fang, Xinggang Wang, Yu Li, Chen Fang, Ying Shan, Bin Feng, and Wenyu Liu. Crossover learning for fast online video instance segmentation. In ICCV, 2021.\\n\\nTien-Ju Yang, Maxwell D Collins, Yukun Zhu, Jyh-Jing Hwang, Ting Liu, Xiao Zhang, Vivienne Sze, George Papandreou, and Liang-Chieh Chen. DeeperLab: Single-shot image parser. arXiv:1902.05093, 2019.\\n\\nQihang Yu, Huiyu Wang, Dahun Kim, Siyuan Qiao, Maxwell Collins, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. CMT-DeepLab: Clustering Mask Transformers for Panoptic Segmentation. In CVPR, 2022.\\n\\nYuhui Yuan, Xilin Chen, and Jingdong Wang. Object-contextual representations for semantic segmentation. In ECCV, 2020.\\n\\nHaotian Zhang, Yizhou Wang, Zhongyu Jiang, Cheng-Yen Yang, Jie Mei, Jiarui Cai, Jenq-Neng Hwang, Kwang-Ju Kim, and Pyong-Kun Kim. U3D-MOLTS: Unified 3D Monocular Object Localization, Tracking and Segmentation. In ICCV Segmenting and Tracking Every Point and Pixel: 6th Workshop on Benchmarking Multi-Target Tracking, 2021.\\n\\nSongyang Zhang, Xuming He, and Shipeng Yan. Latentgnn: Learning efficient non-local relations for visual recognition. In ICML, 2019.\\n\\nHengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017.\\n\\nXizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR, 2021.\\n\\nXizhou Zhu, Yuwen Xiong, Jifeng Dai, Lu Yuan, and Yichen Wei. Deep feature flow for video recognition. In CVPR, 2017.\\n\\nYi Zhu, Karan Sapra, Fitsum A Reda, Kevin J Shih, Shawn Newsam, Andrew Tao, and Bryan Catanzaro. Improving semantic segmentation via video propagation and label relaxation. In CVPR, 2019.\\n\\nZhen Zhu, Mengde Xu, Song Bai, Tengteng Huang, and Xiaogang Bai. Asymmetric non-local neural networks for semantic segmentation. In ICCV, 2019.\"}"}
{"id": "CVPR-2022-1925", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"global memory features. However, they are only deployed in the latent space (i.e., intermediate layers) and will not be used in the final output layers.\\n\\nAs shown in Fig. 3, our hierarchical dual-path transformer blocks consist of a series of one axial-attention block, the latent dual-path transformer, and the global dual-path transformer. The stacking of multiple blocks will alternate the latent and the global communications, allowing the pixel features to refine themselves by attending to both frame-level and video-level memory, and vice versa. This in turn enriches the features of all three paths: pixel-, latent-memory and global-memory paths, and enables learning more comprehensive representations of the given video clip.\\n\\nGlobal memory with split thing and stuff.\\n\\nTo further improve the segmentation quality, we propose to split the global memory into two sets: thing-specific and stuff-specific global memory. Originally, the global memory in [69] deals with thing masks and stuff masks in a unified manner. However, the design ignores the natural difference between them \u2014 There could be multiple instances of the same thing class in an image, but at most one mask is allowed for each stuff class. We thus allocate the last \\\\( \\\\frac{C}{N} \\\\) elements in the global memory specifically for predicting stuff classes. The ordering is enforced by assigning the stuff-specific global memory to the ground truth stuff classes, instead of including them in the bipartite matching.\\n\\n### 3.3. Training Strategy\\n\\n**VPQ-style loss.**\\n\\nTo train TubeFormer-DeepLab for various video segmentation tasks in a unified manner, we adopt a VPQ-style loss that directly optimizes the set of class-labeled tubes. Similar to the image-level PQ-style loss [69], we draw inspiration from video panoptic quality (VPQ) [41] and approximately optimize VPQ within a video clip. To start with, a VPQ-style similarity metric between a class-labeled ground truth tube \\\\( y_i = (m_i, c_i) \\\\) and a predicted tube \\\\( \\\\hat{y}_j = (\\\\hat{m}_j, \\\\hat{p}_j(c_i)) \\\\) can be defined as:\\n\\n\\\\[\\n\\\\text{sim}(y_i, \\\\hat{y}_j) = \\\\hat{p}_j(c_i) \\\\times \\\\text{Dice}(m_i, \\\\hat{m}_j),\\n\\\\]\\n\\nwhere \\\\( \\\\hat{p}_j(c_i) \\\\in [0, 1] \\\\) denotes the probability of predicting the correct tube class \\\\( c_i \\\\) and \\\\( \\\\text{Dice}(m_i, \\\\hat{m}_j) \\\\in [0, 1] \\\\) measures the Dice coefficient between a predicted tube \\\\( \\\\hat{m}_j \\\\) and a ground truth tube \\\\( m_i \\\\).\\n\\nWe match the predicted tubes to the ground truth tubes, and optimize the predictions by maximizing the total VPQ-style similarity. The implementation details follow the PQ-style loss in [69]. In addition, we generalize the auxiliary losses used in [69] to video clips, resulting in a tube-ID cross entropy loss, a video semantic segmentation loss, and a video instance discrimination loss.\\n\\n**Shared semantic and panoptic prediction.**\\n\\nOriginally, the auxiliary semantic segmentation loss in [69] is applied to the backbone feature with a separate semantic decoder. Instead, we propose to apply the loss directly to the decoded video pixel features \\\\( x_v' \\\\) (cf. Eq. (1)) with a linear layer, which learns better features for segmentation.\\n\\n**Temporal consistency loss.**\\n\\nThe VPQ-style loss benefits the learning of spatial-temporal consistency within an input clip. To further achieve the clip-to-clip consistency over a longer video, we propose to use a temporal consistency loss applied between clips. Specifically, we minimize the distance between the \\\\( N \\\\) tube logits predicted from the overlapping frames of two clips. We use \\\\( L_1 \\\\) loss for the consistency metric. The loss is back-propagated through the dot-product of the pixel features and \\\\( N \\\\) global memory features, affecting both pixel and global memory paths. TubeFormer-DeepLab thereby achieves implicit multi-clip consistency, which makes our training objective symmetric to the whole-video inference pipeline (Sec. 3.4).\\n\\n**Clip-level copy-paste.**\\n\\nAdditionally, we propose a simple and effective data augmentation policy by extending the image-level thing-specific copy-paste [24, 29]. Our augmentation method, named clip-paste (clip-level copy-paste), randomly pastes either 'thing' or 'stuff' (or both) region tubes from a video clip to the target video clip. We use clip-paste with a probability of 0.5.\\n\\n**Depth prediction branch.**\\n\\nTo grant TubeFormer-DeepLab the ability to perform monocular depth estimation, we add a small depth prediction module (i.e., ASPP [12] and DeepLabv3+ lightweight decoder [15]) on top of the CNN backbone features \\\\( x_v \\\\). Note that we found the performance slightly degrades if we add the depth prediction to the decoded video pixel features \\\\( x_v' \\\\), indicating that it is not beneficial to share depth estimation with segmentation prediction in our case. We apply Sigmoid to constrain the depth prediction to the range \\\\((0, 1)\\\\), and then multiply it by the maximum depth. Following [63], we use the combination of scale invariant logarithmic error [22] and relative squared error [28] as the training loss. The depth loss weight is set to 100 when jointly trained with the other losses.\\n\\n### 3.4. Inference Strategy\\n\\n**Clip-level inference.**\\n\\nThe clip-level segmentation is inferred by simply performing argmax twice. Specifically, a class label is predicted for each tube:\\n\\n\\\\[\\n\\\\hat{c}_i = \\\\arg\\\\max_c \\\\hat{p}_i(c).\\n\\\\]\\n\\nAnd then, a tube-ID \\\\( \\\\hat{z}_{t,h,w} \\\\) is assigned per-pixel:\\n\\n\\\\[\\n\\\\hat{z}_{t,h,w} = \\\\arg\\\\max_i \\\\hat{m}_{i,t,h,w}.\\n\\\\]\\n\\nIn practice, our inference sets tube-IDs with class confidence below 0.7 to void.\\n\\nFor video instance segmentation, we also explore per-mask assignment scheme [18, 80], which treats the prediction of each object query as one object mask proposal.\\n\\n**Video-level inference.**\\n\\nAt the clip level, TubeFormer-DeepLab outputs temporally consistent results for \\\\( T \\\\) video frames. To obtain the video-level prediction, we perform clip-level inference for every \\\\( T \\\\) consecutive frames with \\\\( T - 1 \\\\) overlapping frames (i.e., we move along the temporal axis).\"}"}
{"id": "CVPR-2022-1925", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"axis by only one frame at each inference step). The clip-level results are then stitched together by matching tubes in the overlapping frames based on their IoUs, similar to [63].\\n\\n4. Experimental Results\\n\\nOur proposed TubeFormer-DeepLab is a general video segmentation model. To demonstrate its effectiveness, we conduct experiments on KITTI-STEP [73], VSPW [58], YouTube-VIS [77], SemKITTI-DVPS [63] for Video Panoptic Segmentation (VPS), Video Semantic Segmentation (VSS), Video Instance Segmentation (VIS), and Depth-aware Video Panoptic Segmentation (DVPS), respectively.\\n\\n4.1. Datasets\\n\\nKITTI-STEP [73] is a new video panoptic segmentation dataset that additionally annotates semantic segmentation for KITTI-MOTS [68]. It contains 19 semantic classes (similar to Cityscapes [19]), among which two classes ('pedestrians' and 'cars') come with tracking IDs. For evaluation, KITTI-STEP adopts STQ [73] (segmentation and tracking quality), which is the geometric mean of SQ (segmentation quality) and AQ (association quality).\\n\\nVSPW [58] is a recent large-scale video semantic segmentation dataset, containing 124 semantic classes. VSPW adopts mIoU as the evaluation metric.\\n\\nYouTube-VIS [77] contains two versions for video instance segmentation; The YouTube-VIS-2019 contains 40 semantic classes and the YouTube-VIS-2021 is an improved version with higher number of instances and videos. YouTube-VIS adopts track mAP for evaluation.\\n\\nSemKITTI-DVPS [63] is a new dataset for depth-aware video panoptic segmentation, which is obtained by projecting the 3D point cloud panoptic annotations of SemanticKITTI [3] to 2D image planes. It contains 19 classes, among which 8 are annotated with tracking IDs. For evaluation, SemKITTI-DVPS uses DSTQ (depth-aware STQ), which considers depth inlier metric [22] in addition to STQ.\\n\\n4.2. Implementation Details\\n\\nTubeFormer-DeepLab builds upon MaX-DeepLab [69] with the official codebase [72]. The hyper-parameters mostly follow the settings of [69]. Unless specified, we use their small model MaX-DeepLab-S, which augments ResNet-50 [32] with axial-attention blocks [70] in the last two stages (i.e., stage-4 and stage-5). We also experiment with scaling up the backbone [14] by stacking the axial-attention blocks in stage-4 by $n$ times, and refer them as TubeFormer-DeepLab-$B_n$ in the experiments. For VPS, we pretrain the models on Cityscapes [19] and COCO [51], while for other experiments, we only pretrain on COCO. The pretraining procedure is similar to prior works [5, 33, 73]. Using the pretrained weights, TubeFormer-DeepLab is trained on the target datasets using a batch size of 16, with $T = 2$ for all datasets except $T = 5$ for YouTube-VIS dataset. We use the global memory size $N = 128$ (i.e., output size), latent memory size $L = 16$, and $C = 128$ channels. We use 'TF-DL' to denote TubeFormer-DeepLab in the results.\\n\\n4.3. Main Results\\n\\n[VPS] We evaluate TubeFormer-DeepLab on the challenging video panoptic segmentation dataset, KITTI-STEP [73] in Tab. 1. Our model achieves state-of-the-art performance with 65.25 STQ (70.27 SQ and 60.59 AQ). Among single unified approaches, our model ranks first, significantly outperforming the published baseline Motion-DeepLab [73] by +13.1 STQ. Our model performs comparably with the challenge winning methods [56, 82] without exploiting extra 3D object formulation, depth information, or pseudo labels, and even without the employment of separate and ensemble methods for tracking and segmentation. Nevertheless, our model delivers the best segmentation quality (70.27 SQ), showcasing our TubeFormer-DeepLab's segmentation ability.\\n\\n[VSS] We assess TubeFormer-DeepLab on the video semantic segmentation dataset, VSPW [58]. We show the single-model single-scale results on val set in Tab. 2. In the table, TubeFormer-DeepLab outperforms all competing methods, which are based on state-of-the-art backbones (BEiT [2], Swin-L [54]) and decoders (OCRNet [81], Uper-Net [75]). Tab. 3 shows the test set results. Our single-model TubeFormer-DeepLab achieves competitive results (rank 4 out of 17) with the ICCV 2021 challenge winners, while not employing model ensembles, multi-scale inference, and pseudo labels. Finally, we attain a better +21 mIoU than the published work TCB [58] on the test set.\\n\\n[VIS] We show that TubeFormer-DeepLab is sufficiently general to solve instance-level video segmentation in a unified manner. The same model, loss, and training procedure is seamlessly applied by treating the background region as a single 'stuff' class. At testing, we explore both per-pixel and per-mask argmax for tube ID assignment (Sec. 3.4). Tab. 4 and 5 show the comparison with the state-of-the-art...\"}"}
{"id": "CVPR-2022-1925", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. ICCV 2021 challenge entries. Comparison includes published and unpublished methods.\\n\\n| Method       | Rank | mIoU VC8 | mIoU VC16 |\\n|--------------|------|----------|-----------|\\n| TCB          | 13   | 35.62    | 86.21     |\\n| BetterThing  | 3    | \u2713        | \u2713         |\\n| CharlesBLWX  | 2    | \u2713        | \u2713         |\\n| jjRain       | 1    | \u2713        | \u2713         |\\n| TF-DL-B4     | 4    | 56.64    | 90.16     |\\n\\nTable 3. Test set results. Ranking includes published and unpublished methods. Some methods use model ensembles, multi-scale inference, or teacher-student pseudo labeling strategy to boost performance on test set.\\n\\nTable 4. YouTube-VIS-2019 val set results.\\n\\n| Method       | Rank | AP 50 | AP 75 | AR 1 | AR 10 |\\n|--------------|------|-------|-------|------|-------|\\n| MaskTrack    | 2    | 28.6  | 48.9  | 29.6 | -     |\\n| SipMask      | 2    | \u2020     | 31.7  | 52.5 | 34.0  |\\n| CrossVIS     | 2    | \u2020     | 34.2  | 54.4 | 37.9  |\\n| IFC          | 36   | \u2020     | 36.8  | 57.9 | 39.3  |\\n| TF-DL-B4 (per-pixel) | 5 | 45.4  | 66.6  | 48.8 | 48.3  |\\n| TF-DL-B4 (per-mask) | 5 | 47.5  | 68.7  | 52.1 | 50.2  |\\n\\nTable 5. YouTube-VIS-2021 val set results.\u2020: T inferred from their Youtube-VIS-2019 settings.\\n\\n| Method       | Rank | AP 50 | AP 75 | AR 1 | AR 10 |\\n|--------------|------|-------|-------|------|-------|\\n| MaskTrack    | 2    | 29.2  | 52.0  | 29.8 | -     |\\n| SipMask      | 2    | \u2020     | 31.7  | 52.5 | 34.0  |\\n| CrossVIS     | 2    | \u2020     | 34.2  | 54.4 | 37.9  |\\n| IFC          | 36   | \u2020     | 36.8  | 57.9 | 39.3  |\\n| TF-DL-B4 (per-pixel) | 5 | 41.2  | 60.4  | 44.7 | 40.4  |\\n| TF-DL-B4 (per-mask) | 5 | 47.5  | 68.5  | 52.1 | 50.2  |\\n\\nTable 6. SemKITTI-DVPS test set results. Ranking includes published and unpublished methods.\\n\\n| Method                      | Rank | DSTQ |   |\\n|-----------------------------|------|------|---|\\n| ViP-DeepLab                 | 3    | 63.36|   |\\n| ICCV 2021 challenge entries | 5    | 54.77|   |\\n| ywang26                     | 4    | 55.99|   |\\n| HarborY                     | 2    | 63.63|   |\\n| TF-DL-B4                    | 1    | 67.00|   |\\n\\n4.4. Ablation Studies\\n\\nWe provide ablation studies on the KITTI-STEP val set [73]. To compensate for the training noise, we report the mean of three runs for every ablation study.\\n\\nHierarchical dual-path transformer. In Tab. 7a, we verify that the gains demonstrated by TubeFormer-DeepLab come from the proposed hierarchical dual-path transformer. Note that our baseline method (TubeFormer-DeepLab-Simple) already uses the axial attentions and the global memory. Introducing the new latent memory and its communication with the video-frame features (F-L attention: L2F, L2L, and F2L) brings a large improvement of +1.7 DSTQ. We also ablate adding attentions between the global memory and the latent memory (M2L and L2M), which show no improvements. This suggests the frame-latent (F-L) attention is sufficient to build effective hierarchical attentions between the latent and the global dual-path transformers. We also ablate different latent memory size, and set the default size $L$ to be 16.\\n\\nTraining strategy. In addition, Tab. 7b shows that the proposed temporal consistency loss helps TubeFormer-DeepLab to learn clip-to-clip consistency, and improves the inference on longer videos than the training clip length ($T$), as demonstrated by +0.5 DSTQ gain. The proposed clip-level copy-paste (clip-paste) augments more training samples for tube-level segmentation, and further improves by +0.9 DSTQ. Scaling. We study the scaling of TubeFormer-DeepLab in Tab. 7c. Pretraining on ImageNet-22k dataset brings +1.6 DSTQ and adding COCO to the training further gives +1.9 DSTQ. We also explore scaling up the backbone by stacking the axial-attention blocks in stage-4 by $n$ times (TubeFormer-DeepLab-B$n$). The increase of every $n$ will introduce +13M parameters. We notice increasing the stack 13920 times.\"}"}
{"id": "CVPR-2022-1925", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Visualization on KITTI-STEP sequence. From left to right: input frames ($T = 3$), global memory attention, latent memory attention, and video panoptic segmentation results. The global memory attention is selected for predicted tube regions of interest: a pedestrian and two cars (left, right) on the sidewalk, and the latent memory attention is selected for 4 (out of $L = 16$) latent memory.\\n\\n| Method                | STQ | SQ | AQ |\\n|-----------------------|-----|----|----|\\n| TF-DL (with hierarchical - dual-path - transformer) | 70.03 | 76.83 | 63.83 |\\n| TF-DL                  | 69.63 | 76.05 | 63.75 |\\n| TF-DL                  | 69.64 | 76.75 | 63.18 |\\n| TF-DL (with hierarchical - dual-path)  | 69.39 | 75.74 | 63.54 |\\n| TF-DL (with hierarchical - transformer)        | 69.57 | 76.71 | 63.1 |\\n\\n(a) Varying transformer attention types. Frame-latent ($F-L$) attention is introduced in the proposed latent dual-path transformer, and includes latent-to-frame, latent-to-latent, and frame-to-latent attentions. We also ablate memory-to-latent ($M2L$) and latent-to-memory ($L2M$) attentions, and different latent memory size $L$.\\n\\n(b) Adding temporal consistency and clip-level copy-paste.\\n\\n| Method                | STQ | SQ | AQ |\\n|-----------------------|-----|----|----|\\n| TF-DL                 | 70.03 | 76.83 | 63.83 |\\n| TF-DL + temporal consistency | 70.51 | 77.64 | 64.04 |\\n| TF-DL + clip-paste    | 71.40 | 76.82 | 66.36 |\\n\\n(c) Scaling by stacking axial blocks in stage-4 of Axial-ResNet-50 by $n$ times and pretraining on ImageNet-22K and COCO.\\n\\n| Method                | STQ | SQ | AQ |\\n|-----------------------|-----|----|----|\\n| TF-DL                 | 70.03 | 76.83 | 63.83 |\\n| TF-DL without sharing semantic and panoptic predictions | 68.95 | 75.83 | 62.70 |\\n| TF-DL without splitting thing and stuff memory | 68.96 | 75.77 | 62.76 |\\n\\n(d) Ablating architectural improvements\\n\\nTable 7. Ablation studies on KITTI-STEP val set. From $n = 1$ to $n = 3$ improves the STQ from 73.19 to 74.25. Further scaling to $n = 4$ starts to saturate, probably limited by the scale of KITTI-STEP dataset. We observe TubeFormer-DeepLab can further scale to $n = 4$ on larger-scale datasets, where TubeFormer-DeepLab-B4 performed better on VSPW and YouTube-VIS datasets.\\n\\n4.5. Visualization\\n\\nIn Fig. 4, we visualize how the proposed hierarchical dual-path transformer performs attention onto the input clip of three consecutive frames. We first visualize the global memory attention by selecting four output regions of interest from TubeFormer-DeepLab video panoptic prediction. We probe the attention weights between the four tube-specific global memory embeddings and all the pixels. We see the global memory attention is spatio-temporally well separated for individual thing or stuff tubes.\\n\\nIn addition, we select four latent memory indices and visualize their attention maps in Fig. 4c. We find that some latent memory learns to spatially specialize on certain areas (left vs right side of the scene) or attends to semantically-similar regions (cars or backgrounds) to facilitate per-frame attention. With the hierarchical attentions made by the global and latent dual-path transformers, TubeFormer-DeepLab can be a successful tube transformer. We provide more visualizations for each video segmentation task in the supplementary materials.\\n\\n5. Conclusion\\n\\nWe introduced TubeFormer-DeepLab, a novel architecture based on mask transformers for video segmentation. Video segmentation tasks, particularly video semantic/instance/panoptic segmentation, have been tackled by fundamentally divergent models. We proposed a new paradigm that formulates video segmentation tasks as the problem of partitioning video tubes with different predicted labels. TubeFormer-DeepLab, directly predicting class-labeled tubes, provides a general solution to multiple video segmentation tasks. We hope our approach will inspire future research in the unification of video segmentation tasks.\"}"}
