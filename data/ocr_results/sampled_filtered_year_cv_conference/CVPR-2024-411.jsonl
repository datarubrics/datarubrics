{"id": "CVPR-2024-411", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1. Experiments results of SynLiDAR [49] \u2192 SemKITTI [1] with SqueezeSegV3-21 [50] as the backbone.\\n\\n| Methods                  | car     | bicycle | mbcle | truck | oth.-veh. | person | bicyclist | road    | parking | sidewalk | building | fence | vegetation | trunk | terrain | pole | traff. |\\n|--------------------------|---------|---------|-------|-------|----------|--------|-----------|---------|---------|----------|----------|-------|------------|-------|---------|------|--------|\\n| **2D methods**           |         |         |       |       |          |        |           |         |         |          |          |       |            |       |         |      |        |\\n| Source Only              | 13.7    | 9.5     | 2.1   | 3.0   | 3.1      | 9.6    | 8.1       | 55.2    | 6.9     | 26.8     | 37.4     | 3.4   | 41.3       | 11.4  | 30.1    | 23.1 | 4.9    |\\n| CBST [62]                | 15.7    | 6.0     | 2.5   | 1.9   | 3.3      | 6.9    | 12.0      | 56.0    | 2.2     | 36.3     | 36.9     | 5.7   | 37.9       | 19.8  | 42.1    | 23.6 | 3.8    |\\n| AdaptSeg [41]            | 22.1    | 9.7     | 3.7   | 2.3   | 6.4      | 9.3    | 12.9      | 65.2    | 5.4     | 31.6     | 39.9     | 6.1   | 41.7       | 24.5  | 42.7    | 24.0 | 2.9    |\\n| CCM [22]                 | 10.2    | 8.1     | 5.1   | 1.9   | 3.3      | 9.7    | 14.4      | 52.1    | 2.7     | 30.6     | 39.3     | 3.9   | 38.8       | 18.8  | 26.9    | 24.2 | 4.2    |\\n| PLCA [17]                | 14.6    | 7.5     | 3.0   | 2.6   | 5.7      | 10.1   | 16.3      | 66.6    | 1.3     | 30.9     | 36.3     | 3.0   | 37.2       | 15.5  | 37.5    | 24.7 | 3.6    |\\n| PLCA + CCL               | 16.7    | 18.7    | 7.4   | 2.9   | 6.4      | 13.8   | 33.4      | 58.6    | 4.3     | 30.6     | 45.7     | 5.1   | 60.1       | 25.8  | 26.9    | 27.0 | 4.5    |\\n| **MMD**                  |         |         |       |       |          |        |           |         |         |          |          |   20.7 |             |      |         |      |        |\\n| **MMD + CCL**            | 23.6    | 5.4     | 2.9   | 2.6   | 6.4      | 7.7    | 10.3      | 60.0    | 7.2     | 28.7     | 50.9     | 8.9   | 51.0       | 20.4  | 36.5    | 24.8 | 3.4    |\\n| **3D methods**           |         |         |       |       |          |        |           |         |         |          |          |       |            |       |         |      |        |\\n| SqzV2 [46]               | 23.3    | 5.3     | 2.4   | 2.8   | 6.5      | 6.6    | 8.0       | 63.4    | 5.6     | 29.6     | 38.5     | 6.3   | 44.0       | 24.2  | 37.5    | 22.5 | 3.3    |\\n| ASM [23]                 | 19.7    | 13.8    | 9.7   | 2.1   | 4.1      | 8.0    | 8.2       | 64.5    | 8.0     | 36.0     | 54.6     | 6.7   | 58.0       | 24.7  | 35.8    | 29.1 | 4.2    |\\n| LiDARNet [16]            | 26.3    | 6.1     | 3.0   | 2.1   | 4.5      | 7.8    | 14.9      | 60.6    | 8.8     | 30.9     | 38.1     | 5.1   | 33.2       | 19.9  | 35.3    | 22.9 | 4.2    |\\n| LiDARNet + CCL           | 30.6    | 10.3    | 6.6   | 3.6   | 9.7      | 10.0   | 22.5      | 64.3    | 6.4     | 33.3     | 44.3     | 9.8   | 43.1       | 22.5  | 41.0    | 25.3 | 8.1    |\\n| **CoSMix** [35]          | 17.3    | 4.8     | 3.0   | 1.6   | 1.9      | 5.4    | 5.5       | 55.3    | 1.9     | 33.7     | 67.6     | 7.3   | 66.1       | 29.2  | 43.0    | 34.4 | 3.0    |\\n| **CoxMix + CCL**         | 12.1    | 5.2     | 2.1   | 0.2   | 7.1      | 15.3   | 3.6       | 75.2    | 5.2     | 43.2     | 66.6     | 21.0  | 67.1       | 26.4  | 47.3    | 21.5 | 3.1    |\\n| SqzV1 [45]               | 36.7    | 14.7    | 8.6   | 1.9   | 12.3     | 12.6   | 27.0      | 66.8    | 9.6     | 35.1     | 43.4     | 8.1   | 46.0       | 27.2  | 42.3    | 27.1 | 5.4    |\\n| **SqzV1 + CCL**          | 52.5    | 16.3    | 8.8   | 3.2   | 10.6     | 15.9   | 28.2      | 65.4    | 9.4     | 33.9     | 54.4     | 8.3   | 63.7       | 29.4  | 35.0    | 31.6 | 11.7   |\\n\\nWith the long-range dependency across regions and domains, context attention can further promote the context learning regarding both global and local aspects. At the global level, the optimization with both domains urges these prototypes to preserve the context knowledge that are comparably domain invariant while discarding the uncommon ones, hence mitigating the gap via the exchange of domain-invariant context. The latter investigation (Fig. 5) further verifies this in which prototypes show consistent preference towards context patterns across domains. Then, for local context learning, context attention offers important guidance for further attuning the context embeddings. As the contextual relationships are expressed in a learnable manner, they can be further modulated and attuned to better extract domain-invariant contexts in terms of both domains, thus realizing a synergy between the local and global context learning and promoting the adaptation cooperatively.\\n\\nFor the sake of efficiency, we replace a standard 3x3 convolution layer with the proposed module (including the part of context embedding). Empirical experiments reveal that performing the replacement at the first encoder block (five in total) is adequate to mitigate the gap with negligible computation overhead (Sec. 4.3 and Table 5 (g)).\\n\\n### Table 2. Overview of used datasets. FOV: vertical field of view.\\n\\n| Dataset   | FoV     | # Training | # Validation |\\n|-----------|---------|------------|--------------|\\n| SynLiDAR  | 64 [\u221225\u00b0, 3\u00b0] | 19840      | -            |\\n| SemKITTI  | 64 [\u221225\u00b0, 3\u00b0] | 19130 4071 | -            |\\n| nuScenes  | 32 [\u221230\u00b0, 10\u00b0] | 28130 6019 | -            |\\n\\n3D methods\\n\\n- **SqzV2** [46]\\n- **ASM** [23]\\n- **LiDARNet** [16]\\n- **LiDARNet + CCL**\\n- **CoSMix** [35]\\n- **CoxMix + CCL**\\n- **SqzV1** [45]\\n- **SqzV1 + CCL**\\n\\n### 3.4. Training Objective\\n\\nDuring optimization, we impose three objectives to the model, i.e., cross-entropy loss ($L_{ce}$), Lovasz-Softmax loss ($L_{lov}$), and the domain alignment loss ($L_{da}$):\\n\\n$$\\n\\\\min_{\\\\theta} L = L_{ce} + L_{lov} + L_{da},\\n$$\\n\\nwhere $\\\\theta$ denotes the parameters of the model and the cross-entropy loss is calculated in a point-wise manner:\\n\\n$$\\nL_{ce} = -\\\\sum_{h, w} \\\\log \\\\left( \\\\theta(I_s(h, w)) \\\\bar{Y}_{h,w} \\\\right).\\n$$\\n\\nNote that $\\\\bar{Y}_{h,w}$ is the label for the point at position $(h, w)$ of a projected LiDAR image.\\n\\nThe training and inference of the model follow the common practice [45,46], which categorizes each point into one of the predefined semantic categories. For a comprehensive evaluation, we incorporate CCL with several representative domain adaptation paradigms and thus do not specify the domain alignment loss here.\"}"}
{"id": "CVPR-2024-411", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Experiments results of SynLiDAR \u2192 nuScenes with SalsaNext as the backbone.\\n\\n| Methods   | bicycle | bus | car | oth.-veh. | mt.cle | pedes. | truck | road | oth-grd. | sidew. | terrain | manmd. | veget. |\\n|-----------|---------|-----|-----|----------|--------|--------|-------|------|----------|--------|---------|--------|--------|\\n| Source Only | 0.3     | 0.2 | 8.8 | 0.3      | 2.1    | 5.2    | 14.2  | 64.1 | 1.4      | 18.0   | 8.2     | 27.8   | 16.5   |\\n| + CCL     | 0.5     | 0.1 | 9.2 | 0.7      | 2.3    | 6.8    | 10.7  | 62.2 | 0.4      | 20.6   | 5.4     | 36.1   | 19.4   |\\n| CBST      | 0.2     | 1.2 | 13.6| 0.6      | 1.2    | 3.0    | 10.8  | 66.3 | 0.8      | 13.6   | 2.1     | 21.8   | 23.4   |\\n| + CCL     | 0.1     | 0.8 | 10.8| 1.6      | 0.9    | 12.0   | 16.3  | 56.9 | 2.6      | 20.1   | 5.6     | 34.1   | 35.6   |\\n| AdaptSeg  | 0.4     | 1.7 | 5.9 | 0.6      | 2.1    | 7.1    | 12.4  | 58.9 | 1.6      | 16.4   | 10.7    | 26.8   | 23.7   |\\n| + CCL     | 0.5     | 1.5 | 19.8| 0.3      | 3.3    | 6.5    | 16.7  | 71.9 | 2.5      | 24.3   | 22.5    | 37.6   | 28.1   |\\n| PLCA      | 0.1     | 0.7 | 1.4 | 0.1      | 0.5    | 1.5    | 8.9   | 59.7 | 0.2      | 14.1   | 6.9     | 34.9   | 17.7   |\\n| + CCL     | 0.4     | 1.3 | 14.6| 1.1      | 2.0    | 10.3   | 16.8  | 63.3 | 2.4      | 19.3   | 12.1    | 36.1   | 20.2   |\\n| MMD       | 0.4     | 0.2 | 9.2 | 0.6      | 1.8    | 6.7    | 16.3  | 67.9 | 1.9      | 21.1   | 8.4     | 34.4   | 24.1   |\\n| + CCL     | 0.6     | 2.9 | 13.6| 0.4      | 2.9    | 9.4    | 12.3  | 70.5 | 3.3      | 20.9   | 8.3     | 39.5   | 21.0   |\\n| SqzV1     | 0.6     | 0.8 | 14.1| 0.3      | 2.6    | 9.9    | 13.9  | 65.8 | 2.4      | 17.6   | 11.5    | 30.7   | 21.5   |\\n| + CCL     | 0.5     | 2.1 | 17.0| 0.9      | 1.6    | 12.0   | 14.5  | 69.6 | 4.7      | 20.5   | 10.1    | 30.3   | 28.0   |\\n| SqzV2     | 0.4     | 0.3 | 4.1 | 0.4      | 1.7    | 9.3    | 10.2  | 60.5 | 2.4      | 18.4   | 6.8     | 33.9   | 23.1   |\\n| + CCL     | 0.6     | 0.7 | 25.5| 0.4      | 2.5    | 8.0    | 15.4  | 71.8 | 0.7      | 23.6   | 23.0    | 38.8   | 26.7   |\\n| LiDARNet  | 0.4     | 0.5 | 3.4 | 0.5      | 1.5    | 6.8    | 10.9  | 61.5 | 1.9      | 18.7   | 11.6    | 24.8   | 23.5   |\\n| + CCL     | 0.9     | 1.3 | 18.3| 1.1      | 2.3    | 10.3   | 15.1  | 70.8 | 2.0      | 21.1   | 15.2    | 31.3   | 26.3   |\\n\\nTable 4. Experiments results of SynLiDAR \u2192 SemanticKITTI with MinkowskiNet as the backbone.\\n\\n| Methods          | mIoU  |\\n|------------------|-------|\\n| CosMix           | 32.2  |\\n| SALUDA           | 30.2  |\\n| CosMix + CCL (Ours) | 34.5  |\\n\\n4.2. Comparisons with Previous Methods\\n\\nWe compare our method with representative domain adaptation segmentation solutions regarding both 2D images and point clouds. For 2D solutions, we choose representative one-stage methods, i.e., CBST [62], PLCA [17], AdaptSeg [41], and adapt their code to the 3D scenario. For 3D solutions, we compare our method with SqueezeSegV1 [45], SqueezeSegV2 [46], LiDARNet [21], and CoSMix [35]. For a fair comparison, all the conducted experiments use the identical backbone and supervision loss, i.e., \\\\( L_{\\\\text{ce}} + L_{\\\\text{Lov}} \\\\). In Table 1, Table 3, and Table 4, we integrate the proposed module into these solutions and report the results, where we can make the following observations:\\n\\n1) Our method can bring consistent improvements over previous solutions, e.g., +4.2% than PLCA in Table 3, and +2.3% than CosMix in Table 4, indicating a complementary effect to them. This generally proves the benefit of effective context modeling and alignment in domain adaptation.\\n\\n2) Compared with source-only, 2D solutions attain negligible improvement. This implies that methods focus on appearance feature cannot scale to point clouds with rich geometries. Besides, CCL leads to limited gains than source-only. This is because the CCL cannot perform cross-domain association and alignment in the absence of target data.\\n\\n3) Integrating with previous 3D solutions leads to better adaptation results consistently, including methods already considering the context gap, e.g., +3.2% than SqzV1 in Table 1. This validates that CCL can better help the model scale to variational geometric contexts across domains.\\n\\n4.3. Ablation Studies and Analysis\\n\\nIn this section, we perform comprehensive ablations to validate the necessity of each proposed design. Here we choose the variant using AdaptSeg + CCL in the ablations for its competitive performances on both benchmarks.\\n\\nContribution of each component. First, in Table 5 (a), we evaluate the contribution of the proposed components, i.e., MLP-encoding and masking in context embedding.\"}"}
{"id": "CVPR-2024-411", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 5. Ablation studies on SynLiDAR (Syn.) $\\\\rightarrow$ SemKITTI (Sem.) $\\\\rightarrow$ SynLiDAR (Syn.) $\\\\rightarrow$ nuScenes (Nus.)\\n\\n| Module                  | Transfer      | Syn. $\\\\rightarrow$ Sem. | Syn. $\\\\rightarrow$ Nus. | |\\n|-------------------------|---------------|--------------------------|--------------------------|------|\\n| ConEmb ConAtt MLP-enc.  |               | 20.6                     | 21.4                     | \u2713    |\\n| Masking                 |               |                          |                          | \u2713    |\\n|                        |               | 21.8                     | 22.0                     | \u2713 \u2713  |\\n|                        |               | 22.0                     | 23.3                     | \u2713 \u2713 \u2713|\\n\\n(a) Contribution of the proposed modules: MLP-encoding and Masking in Context Embedding (ConEmb), and Context Attention (ConAtt).\\n\\n| Module                  | Transfer      | Syn. $\\\\rightarrow$ Sem. | Syn. $\\\\rightarrow$ Nus. | |\\n|-------------------------|---------------|--------------------------|--------------------------|------|\\n| KPConv [40]             |               | 20.3                     | 21.2                     |      |\\n| Point Transformer [58]  |               |                          |                          |      |\\n| Meta-Kernel [9]         |               | 21.8                     | 21.8                     |      |\\n| Domain-specific proto.  |               |                          |                          |      |\\n| Domain-shared proto.    |               |                          |                          |      |\\n\\n(b) Comparison with other modules for context modeling. Proto. = Prototypes.\\n\\n| Position | Transfer      | Syn. $\\\\rightarrow$ Sem. | Syn. $\\\\rightarrow$ Nus. | |\\n|----------|---------------|--------------------------|--------------------------|------|\\n| 1st Enc. |               | 23.3                     | 22.2                     |      |\\n| 3rd Enc. |               |                          |                          |      |\\n| 5th Enc. |               |                          |                          |      |\\n| End of Dec. |           | 21.5                     | 21.0                     |      |\\n\\n(c) Comparison on different insert positions, where Enc. = encoder and Dec. = decoder.\\n\\n| $n_{con}$ | Transfer      | Syn. $\\\\rightarrow$ Sem. | Syn. $\\\\rightarrow$ Nus. | |\\n|-----------|---------------|--------------------------|--------------------------|------|\\n| 4         |               | 23.1                     | 22.8                     |      |\\n| 8         |               | 23.3                     | 23.1                     |      |\\n| 12        |               |                          |                          |      |\\n| 16        |               |                          |                          |      |\\n\\n(d) Sensitivity to $n_{con}$, the number of context prototypes.\\n\\n| $k_{(grid)}$ | Transfer      | Syn. $\\\\rightarrow$ Sem. | Syn. $\\\\rightarrow$ Nus. | |\\n|--------------|---------------|--------------------------|--------------------------|------|\\n| 8 (3x3)     |               | 23.3                     | 22.8                     |      |\\n| 24 (5x5)    |               |                          |                          |      |\\n| 48 (7x7)    |               |                          |                          |      |\\n| 80 (9x9)    |               |                          |                          |      |\\n\\n(e) Comparison on different numbers of neighbors in context embeddings.\\n\\n| Fusion | Transfer      | Syn. $\\\\rightarrow$ Sem. | Syn. $\\\\rightarrow$ Nus. | |\\n|--------|---------------|--------------------------|--------------------------|------|\\n| MaxPool|               | 23.4                     | 22.4                     |      |\\n| AvgPool|               |                          |                          |      |\\n| Concat. |              |                          |                          |      |\\n\\n(f) Ablation on the fusion process in context embedding, i.e., max pooling, average pooling, and concatenation (concat.).\\n\\n| Backbone | FLOPS (G) | Param. (M) | |\\n|----------|-----------|------------|------|\\n| SqzV3    | 25.2      | 24.7       | 24.7 (-0.5) 9.2 (-0.1) |\\n| + CCL    | 24.7      | 24.7       | 24.7 (-0.5) 9.2 (-0.1) |\\n| SalsaNext| 3.9       | 3.8        | 3.8 (-0.1) 6.7 (-0.1) |\\n\\n(g) Analysis of the model capacity with or without the proposed module, in terms of FLOPS and parameters (Param.).\\n\\n(h) The cross-domain distance on relative encoding with different attention strategies.\\n\\nFirst, we can observe that all components contribute to the domain adaptation, and removing neither of them leads to an apparent performance drop. Second, combining context embedding and context attention can lead to better results than using one of them. This is because they focus on local and global contexts accordingly and are complementary to each other, thus integrating them can better promote the adaptation. Especially, under the scenario with a larger context gap (SynLiDAR $\\\\rightarrow$ nuScenes), we can notice that the masking process brings an obvious improvement, i.e., + 2.0% on nuScenes. The reason here is that the sparser LiDAR scans contain more dropout noises and the domain alignment is more vulnerable to them, while our masking strategy can effectively mitigate such distractions.\\n\\nComparison with other context learning modules. In Table 5 (b), we compare our method with other modules for the context learning of point clouds. The results show that our method outperforms other solutions by a noticeable margin. We conjecture the reason is that they do not consider the cross-domain variation, thus cannot bring obvious benefits for the domain alignment. To further investigate this, we change the domain-sharing prototypes to domain-specific prototypes that are optimized separately and report the result. The decreased performance further justifies the benefit of the domain-sharing mechanism on prototypes.\\n\\nLocations for inserting the proposed module. In Table 5 (c), we investigate the optimal position for placing the proposed module, i.e., 1st, 3rd, 5th encoder block (five in total), and end of the decoder. As we can observe, placing at the first encoder block achieves the best result. This reason is that the fine-grained contextual relationships are better preserved at the shallower layers and diminish at deeper.\\n\\nFusion strategy for context embedding. In Table 5 (f), we exploit different fusion strategies for aggregation with neighbors, and the concatenation achieves the best. However, using other variants also maintains our superiority against other solutions, indicating our method is not sensitive to the fusion process.\\n\\nAnalysis on model capacity. In Table 5 (g), we present the analysis of model capacity in terms of FLOPS and parameters. Apparently, our method does not induce extra computation costs, even less. This indicates that the gain in performance should be ascribed to effective context learning, rather than increasing model capacity.\\n\\nSensitivity to hyper-parameters. Here we examine the sensitivity analysis to the hyper-parameters. First, in Table 5 (d), we evaluate the sensitivity to the number of context prototypes. Second, in Table 5 (e), we investigate the sensitivity on the number of neighbors in context embeddings. Finally, in Table 5 (g), we present the analysis of model capacity in terms of FLOPS and parameters.\"}"}
{"id": "CVPR-2024-411", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Visualization of the segmentation results on the validation set of SemKITTI [1]. Best viewed in color.\\n\\nFigure 5. Heatmap visualization for the source and target samples with the first three prototypes. The prototypes implicitly differentiate into different context patterns that are consistently held across domains. Best viewed in color.\\n\\nWith a large range (from 4 to 16), the performance suffers very limited fluctuations, verifying the robustness to it. Second, we examine the sensitivity to the number of neighbors ($k$) in Table 5 (e). As we could observe, increasing $k$ first leads to apparent improvement and then decreases slightly, showing that enlarging the receptive field can boost the performance but too large may harm. The reason we set $k$ to 8 is to be in line with the receptive field of the replaced part, i.e., the 3x3 convolution layer.\\n\\nHow does CCL promote the adaptation. Here we investigate the effect of CCL in the adaptation process both quantitatively and qualitatively:\\n\\n1) In Table 5 (h), we compare the cross-domain Wasserstein distances on the weights ($r$ in Eq. 2) in context embeddings. Notably, using domain-specific prototypes that breaks the cross-domain interaction, cannot mitigate the domain gap. This reveals that the CCL can guide the context embedding to overcome the gap through cross-domain interactions.\\n\\n2) In Fig. 5, we visualize the heatmap of the context prototypes, where we can observe different prototypes focus on different context patterns but in a domain-consistent manner. This proves that these prototypes indeed preserve domain-invariant context knowledge to some extent.\\n\\nVisualizations. In Fig. 4, we visualize and compare our method with CoSMix and the ground truth. Apparently, CCL derives better segmentation results with effective context learning, e.g., CCL can better identify class \u201croad\u201d and \u201cvegetation\u201d which CoSMix is easily confused with.\\n\\n5. Conclusion\\nIn this paper, we aim to overcome the discrepancy induced by the variational geometric context for domain adaptive point cloud segmentation. To arrive at it, we propose a new context learning paradigm, Cooperative Context Learning, which models and modulates the contextual representation from different aspects but in a cooperative manner. Specifically, CCL first constructs the context embeddings within the local scope, then associates them with context attention globally, where the attention in turn attunes the context modeling and therefore narrows the gap cooperatively. Consequently, the cross-domain variation in context distribution can be effectively mitigated through the global exchange of context knowledge and modulation on the local context modeling. Extensive experiments are conducted to verify the effectiveness of the proposed method.\"}"}
{"id": "CVPR-2024-411", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Construct to Associate: Cooperative Context Learning\\nfor Domain Adaptive Point Cloud Segmentation\\n\\nGuangrui Li\\nReLER, AAII, University of Technology Sydney\\nguangrui.li@outlook.com\\n\\nAbstract\\nThis paper tackles the domain adaptation problem in point cloud semantic segmentation, which performs adaptation from a fully labeled domain (source domain) to an unlabeled target domain. Due to the unordered property of point clouds, LiDAR scans typically show varying geometric structures across different regions, in terms of density, noises, etc, hence leading to increased dynamics on context. However, such characteristics are not consistent across domains due to the difference in sensors, environments, etc, thus hampering the effective scene comprehension across domains. To solve this, we propose Cooperative Context Learning that performs context modeling and modulation from different aspects but in a cooperative manner. Specifically, we first devise context embeddings to discover and model contextual relationships with close neighbors in a learnable manner. Then with the context embeddings from two domains, we introduce a set of learnable prototypes to attend and associate them under the attention paradigm. As a result, these prototypes naturally establish long-range dependency across regions and domains, thereby encouraging the transfer of context knowledge and easing the adaptation. Moreover, the attention in turn attunes and guides the local context modeling and urges them to focus on the domain-invariant context knowledge, thus promoting the adaptation in a cooperative manner. Experiments on representative benchmarks verify that our method attains the new state-of-the-art.\\n\\n1. Introduction\\nPoint cloud semantic segmentation aims to classify points in a LiDAR scan into the predefined semantic classes. Recently, tremendous efforts have been devoted to this task for its fundamental role in environmental perception, e.g., autonomous driving, virtual reality, etc. Albeit the recent progresses, most current solutions still assume an ideal environment with massive point-wise annotations accessible. However, such an assumption cannot be always satisfied in real world as we cannot foresee and annotate all the scenes encountered, especially with the expensive annotation cost. Thus, a model may fail to cope with novel scenes under the distribution shift induced by various factors, e.g., sensor configurations, weather. To solve this, domain adaptation is considered a feasible solution which seeks to adapt the model trained on one labeled domain to a novel domain without annotation.\\n\\nDomain adaptation for semantic segmentation has been widely investigated on 2D images, while only a few attention has been paid to the 3D scenario. A critical difference between 2D images and point clouds lies in the organization of the scene, i.e., 2D images are organized with rigid grids while point clouds are in an unordered manner. Thus, LiDAR scans collected from different sensors or environments typically show different characteristics on the scene geometry. As shown in...\"}"}
{"id": "CVPR-2024-411", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Fig. 1 (a)(b), two datasets show different characteristics in scene structures, e.g., angle of beams, sparsity, etc. Such a discrepancy naturally results in the distribution shift in the geometric context, i.e., the geometric relationships with surrounding neighbors, posing a new challenge to the domain adaptation problem. However, previous 2D solutions typically consider the scene context with the appearance feature of images, hence being hardly applicable to the 3D scenario.\\n\\nRecently, increasing attention has been paid to the domain adaptation problem for point cloud segmentation. The initial researches identify the dropout noises as the main obstacle for performing adaptation, where the noises are caused by varying reflectivity of different materials. To solve this, Wu et al. [45] proposes to perform noise rendering with the collected noise frequencies in the projected range view. Zhao et al. [59] proposes to enhance the noise rendering process with CycleGAN [60]. A more recent work, CoSMix [35], proposes an augmentation technique that mixes the LiDAR scans from two domains in a semantic-aware manner. However, these solutions commonly adopt the input transformation that treats each scene as a whole but neglects the spatial variation of contexts inside the scenes. Thus, without explicit context modeling, it is questionable if they can well handle regions under severe shifts in scene context, especially in the absence of target supervision. For example, in Fig. 1, previous solutions mainly benefit the adaptation of regions less suffered from the context gap (range < 20m), while barely improving over the source-only baseline under severer shifts. Instead, with explicit context modeling and modulation, our method attains consistent improvement over all ranges, including the part more susceptible to the context gap.\\n\\nIn this paper, we propose Cooperative Context Learning (CCL) to mitigate the context gap through effective context modeling and modulations in a cooperative manner. Specifically, in the local scope, we devise context embeddings to model contextual relationships with close neighbors using an MLP (Multi-Layer Perception) that encodes the relative distance and excludes noise samples. Then at the global level, we introduce a set of learnable prototypes to attend the context embeddings from both domains under the attention paradigm (prototype as the key and context embedding as the query), thus associating the contexts from different regions and domains. Therefore, these prototypes naturally encourage the transfer of context knowledge and hence mitigate the cross-domain variation in the context distribution. More importantly, the global attention in turn attunes the local context modeling, urging them to focus on domain-invariant context patterns. As such, a synergy forms between context embedding and attention where they work cooperatively to promote the adaptation, i.e., the former lays the foundation for global modulations while the latter in turn guides and promotes the local context modeling. Extensive experiments and ablations are performed to validate the effectiveness of the proposed method.\\n\\nThe contributions of this work can be summarized as: 1) We observe the discrepancy in context as a critical obstacle for cross-domain point cloud segmentation; 2) We propose Cooperative Context Learning to mitigate the context gap with effective context modeling and modulation cooperatively; 3) Extensive experiments are conducted to verify the effectiveness of the proposed approach, and it attains new state-of-the-art on representative benchmarks.\\n\\n2. Related Works\\n\\nPoint Cloud Semantic Segmentation aims to classify each point in the LiDAR scan into one of the predefined semantic categories. Due to its unordered property, there are three pathways to preprocess the point clouds for better representation learning. The initial researches propose to process the raw point clouds directly with MLP (Multi-Layer Perception) [10, 32, 33], GCN (Graph Neural Network) [44, 53], or newly designed modules [40, 47, 52, 57]. However, the computational cost for these solutions commonly scales up with the number of points, making it cumbersome for large-scale processing due to its higher latency.\\n\\nVoxel-based methods [12, 19, 20, 39, 54] proposed to divide the 3D spaces into voxels evenly, then apply sparse convolutions to process the voxels. Some researchers [56, 61] also explore different partition strategies to handle the variational density. Also, the heavy computation cost of 3D CNN hampers their applicability to the real-world applications. 3) Projection-based solution is another routine focusing on transforming 3D point clouds into 2D grids so that 2D convolutions can be utilized directly. Various architectures [7, 18, 28, 45, 46, 50] in this direction have been proposed to cope with the projected 2D images, and have been proven the effectiveness and efficiency. Moreover, projection-based methods also receive increasing attention on other tasks [25, 38] for their lower computation cost. In this paper, we choose the projection-based architecture as the backbone to perform our adaptation task as they strike a better balance between performance and efficiency.\\n\\nDomain Adaptive Semantic Segmentation seeks to derive domain-invariant representations for the dense prediction tasks. Recently, extensive efforts have been paid to the 2D scenario and achieved significant progress. However, the 3D scenario is still less investigated.\\n\\nIn terms of 2D domain adaptive segmentation, there are several ways to mitigate the domain gap. One way handles the gap on the learned features directly [17, 41, 43]. The seminal work [41, 43] leverages the domain adversarial training to facilitate the domain alignment at the output level. Another line of works [62, 63] attempts to realize the effective knowledge transfer with self-training, where confident target samples are assigned with pseudo labels and...\"}"}
{"id": "CVPR-2024-411", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Learnable Context Prototypes\\n\\nTarget Context Embedding\\n\\nSource Context Embedding\\n\\nSource Feature Embedding\\n\\nTarget Feature Embedding\\n\\nConstruct (Local)\\n\\nAssociate (Global)\\n\\nContext Modeling\\n\\nAttune Underpin\\n\\nFigure 2. Overview of the \u201cConstruct to Associate\u201d scheme. The Construct step builds up context embeddings to model contextual relationships for both domains. Then the Associate step associates and aligns the context distribution across domains through learnable context prototypes. As such, the \u201cconstruct\u201d step models the context distribution, laying the basis of context alignment, while the \u201cassociate\u201d step refines the context modeling process via effective cross-domain association and alignment.\\n\\n3. Methodology\\n\\nIn Fig. 2, we provide an overview of the proposed \u201cConstruct to Associate\u201d scheme, where context modeling and modulation are performed at different levels in a cooperative manner, i.e., context embedding for local context modeling and context attention for global modulation. The local context modeling underpins the context modulation in the global scope, while the global attention in turn attunes the modeling of local context relationships, thus cooperating with each other to promote the adaptation. In the following sections, we first provide necessary preliminaries (\u00a7 3.1) for domain adaptive point cloud segmentation. Then, we elaborate on the proposed context learning scheme for local and global scopes in \u00a7 3.2 and \u00a7 3.3 accordingly. Finally, we detail the training objectives in \u00a7 3.4.\\n\\n3.1. Notations and Preliminaries\\n\\nIn domain adaptive point cloud segmentation, we are given samples from two domains, i.e., annotated source scans \\\\(S = \\\\{ (P_s^i, Y_s^i) \\\\}_{i=1}^{N_s} \\\\) and unlabeled target scans \\\\(T = \\\\{ (P_t^i) \\\\}_{i=1}^{N_t} \\\\). Here \\\\(P_i^i \\\\in \\\\mathbb{R}^{n_i \\\\times 4} \\\\) denotes the set of points with coordinates \\\\((x, y, z)\\\\) and intensity, and \\\\(Y_i \\\\in \\\\mathbb{R}^{n_i}\\\\) denotes the ground-truth annotation for the point cloud, and \\\\(n_i\\\\) is the number of points in the \\\\(i\\\\)-th scan. As a pre-processing, we project the raw points \\\\(P\\\\) into 2D images \\\\(I \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times 5}\\\\), which is presented below. The labels are transformed to \\\\(\\\\bar{Y} \\\\in \\\\mathbb{R}^{H \\\\times W}\\\\) accordingly.\\n\\n**Spherical Projection.** Like previous range-view-based solutions [45, 46], we transform the raw point clouds into 2D rigid images with spherical projection for the sake of efficiency. Concretely, for a point with coordinate \\\\((x, y, z)\\\\), we project it into a 2D image with coordinates \\\\((p, q)\\\\):\\n\\n\\\\[\\n[p \\\\ q] = \\\\left[ \\\\frac{1}{2} \\\\left(1 - \\\\arctan \\\\frac{y}{x}/\\\\pi \\\\right) \\\\cdot W \\\\left(1 - \\\\left( \\\\arcsin \\\\frac{z \\\\cdot r - 1 + f_{\\\\text{up}}}{f_{\\\\text{down}} - 1} \\\\right) \\\\cdot H \\\\right) \\\\right],\\n\\\\]\\n\\nwhere \\\\(r = \\\\sqrt{x^2 + y^2 + z^2}\\\\) is the range of this point. \\\\(f = f_{\\\\text{up}} + f_{\\\\text{down}}\\\\) is the vertical field-of-view of the LiDAR sensor. In the projected 2d grids with the resolution of \\\\(H \\\\times W\\\\), each point consists of five channels, i.e., range, intensity, and the Cartesian coordinates \\\\((x, y, z)\\\\). Notably, even with rigid grids, the property of geometric context is still preserved in terms of density, noise, etc.\\n\\n3.2. Construct: Context Embedding.\\n\\nWe first describe how to discover and model the contextual relationships within the local scope, which act as the prerequisite for the subsequent global context learning. As illustrated in Fig. 3 (left), the context embedding aggregate the neighbor sample (dash borders) with weights that drawn from the contextual relationships.\"}"}
{"id": "CVPR-2024-411", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 3. Schematic of the Cooperative Context Learning. Context Embedding (\u201cConstruct\u201d) (left) aggregates the neighbor samples (dashed borders) using weights derived from the geometric context, where an MLP encodes the relative distance and then excludes noise samples. Context Attention (\u201cAssociate\u201d) leverages a set of learnable prototypes to associate the context embeddings, which attend and be optimized with both domains, hence building up the long-range dependency across regions and domains. As such, these prototypes enable the cross-domain transfer of context knowledge, thereby mitigating the cross-domain variation. Moreover, as the learnable property of context embeddings, they can be further modulated with the guidance of attention, thus promoting the adaptation cooperatively.\\n\\nConcretely, the context embedding considers the contextual relationships from two aspects, i.e., the density and noise samples, and integrates them into two consecutive steps. First, like by [40, 58], we treat the relative distances with neighbors as an important clue for geometric context and feed them into an MLP to get the weights. As such, the geometric relationships can be expressed in a learnable manner, offering vital flexibility for the subsequent modulation on them. Second, we leverage a noise mask to zero out the weights of noise samples. The noise mask is a binary mask indicating if a pixel in the range image is filled by samples in point clouds, where the unfilled parts (noises) are missing points during collection due to environmental factors, e.g., glasses. The rationale here is that they are observed to distract as outliers in the weight learning.\\n\\nTo be more precise, for the $i$th sample with coordinate $c_i \\\\in \\\\mathbb{R}^{1 \\\\times 3}$ and feature vector $f_i \\\\in \\\\mathbb{R}^{1 \\\\times c}$, the context encoding with neighbor samples is formulated as:\\n\\n$$r_{ij} = \\\\phi(c_i - c_j) \\\\cdot M_j, j \\\\in Q(i),$$\\n\\n(2)\\n\\nwhere $\\\\phi$ is a two-layer MLP with non-linearity and $Q(i)$ is the set of indices for the $k$-nearest neighbors ($k$-NN) given index $i$. $M$ is a binary mask that separates normal samples from noise ones. Benefiting from the spherical projection that transforms point clouds into rigid grids, the neighbor acquisition can be efficiently achieved with the im2col operation, rather than the tedious neighbor search.\\n\\nThen the context embedding can be formulated as:\\n\\n$$f'_i = \\\\Lambda(\\\\{f_j r_{ij} | j \\\\in Q(i)\\\\}),$$\\n\\n(3)\\n\\nwhere $\\\\Lambda$ is the fusion function in terms of concatenation, average pooling, or max pooling. And empirical experiments show that concatenation performs best (Sec. 4.3).\\n\\n3.3. Associate: Context Attention\\n\\nWith context embedding, we can model the contextual relationships within local scopes, while leaving cross-domain context alignment untouched. To solve this, the self-attention mechanism [42] can be a plausible solution that promotes the alignment with cross-domain interactions. However, it is intractable to derive point-to-point affinities with the large amount of points. Hence, we introduce context prototypes as proxies to bridge the context distribution across regions and domains, i.e., regarding regions inside and across domains.\\n\\nAs depicted in Fig. 3 (right), we introduce a set of learnable prototypes that are shared and optimized with both domains, which attend the context embeddings with the dot-product attention, i.e., the prototypes act as the Key and Value and the embeddings are the Query. Formally, given context embedding $F'$ with shape of $h \\\\times w \\\\times d$ and prototypes $E \\\\in \\\\mathbb{R}^{n \\\\times n \\\\times d}$, the attention further refines the context embedding as:\\n\\n$$F'' = F' W^q, K = EW^k, V = EW^v,$$\\n\\n(4)\\n\\n$$F'' = F' + W^m(\\\\text{softmax}(F' W^q(\\\\text{EW}^k)) \\\\sqrt{d}(\\\\text{EW}^v)),$$\\n\\n(5)\\n\\nwhere $W^q/W^k/W^v \\\\in \\\\mathbb{R}^{d \\\\times d}$ are linear layers that project the inputs into the identical dimension space, and $W^m$ is a linear layer that maps the dimension back to $d$. A residual connection is added for training stability.\"}"}
{"id": "CVPR-2024-411", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] J. Behley, M. Garbade, A. Milioto, J. Quenzel, S. Behnke, C. Stachniss, and J. Gall. SemanticKITTI: A Dataset for Semantic Scene Understanding of LiDAR Sequences. In ICCV, 2019.\\n\\n[2] Maxim Berman, Amal Rannen Triki, and Matthew B Blaschko. The Lov\u00e1sz-softmax loss: A tractable surrogate for the optimization of the intersection-over-union measure in neural networks. In CVPR, 2018.\\n\\n[3] Holger Caesar, Varun Bankiti, Alex H. Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-modal dataset for autonomous driving. CVPR, 2020.\\n\\n[4] Wei-Lun Chang, Hui-Po Wang, Wen-Hsiao Peng, and Wei-Chen Chiu. All about structure: Adapting structural information across domains for boosting semantic segmentation. In CVPR, 2019.\\n\\n[5] Jaesung Choe, Chunghyun Park, Francois Rameau, Jaesik Park, and In So Kweon. Pointmixer: MLP-mixer for point cloud understanding. arXiv preprint arXiv:2111.11187, 2021.\\n\\n[6] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4D spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 3075\u20133084, 2019.\\n\\n[7] Tiago Cortinhal, George Tzelepis, and Eren Erdal Aksoy. Salsanext: Fast, uncertainty-aware semantic segmentation of lidar point clouds for autonomous driving, 2020.\\n\\n[8] Runyu Ding, Jihan Yang, Li Jiang, and Xiaojuan Qi. Doda: Data-oriented sim-to-real domain adaptation for 3d semantic segmentation. In ECCV, 2022.\\n\\n[9] Lue Fan, Xuan Xiong, Feng Wang, Naiyan Wang, and ZhaoXiang Zhang. Rangedet: In defense of range view for lidar-based 3d object detection. In ICCV, October 2021.\\n\\n[10] Siqi Fan, Qiulei Dong, Fenghua Zhu, Yisheng Lv, Peijun Ye, and Fei-Yue Wang. Scf-net: Learning spatial contextual features for large-scale point cloud segmentation. In CVPR, 2021.\\n\\n[11] Rui Gong, Yuhua Chen, Danda Pani Paudel, Yawei Li, Ajad Chhatkuli, Wen Li, Dengxin Dai, and Luc Van Gool. Cluster, split, fuse, and update: Meta-learning for open compound domain adaptive semantic segmentation. In CVPR, 2021.\\n\\n[12] Tong He, Chunhua Shen, and Anton van den Hengel. DyCo3d: Robust instance segmentation of 3d point clouds through dynamic convolution. In CVPR, 2021.\\n\\n[13] Lukas Hoyer, Dengxin Dai, and Luc Van Gool. Daformer: Improving network architectures and training strategies for domain-adaptive semantic segmentation. In CVPR, 2022.\\n\\n[14] Qingyong Hu, Bo Yang, Linhai Xie, Stefano Rosa, Yulan Guo, Zhihua Wang, Niki Trigoni, and Andrew Markham. Learning semantic segmentation of large-scale point clouds with random sampling. IEEE TPAMI, 2021.\\n\\n[15] Jiaxing Huang, Dayan Guan, Aoran Xiao, and Shijian Lu. Cross-view regularization for domain adaptive panoptic segmentation. In CVPR, 2021.\\n\\n[16] Peng Jiang and Srikanth Saripalli. Lidarnet: A boundary-aware domain adaptation model for point cloud semantic segmentation. ICRA, 2021.\\n\\n[17] Guoliang Kang, Yunchao Wei, Yi Yang, Yueting Zhuang, and Alexander G Hauptmann. Pixel-level cycle association: A new perspective for domain adaptive semantic segmentation. In NeurIPS, 2020.\\n\\n[18] Deyvid Kochanov, Fatemeh Karimi Nejadasl, and Olaf Booij. Kprnet: Improving projection-based lidar semantic segmentation. ECCV workshop, 2020.\\n\\n[19] Xin Lai, Yukang Chen, Fanbin Lu, Jianhui Liu, and Jiaya Jia. Spherical transformer for lidar-based 3d recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17545\u201317555, 2023.\\n\\n[20] Alex H Lang, Sourabh Vora, Holger Caesar, Lubing Zhou, Jiong Yang, and Oscar Beijbom. Pointpillars: Fast encoders for object detection from point clouds. In CVPR, 2019.\\n\\n[21] F. Langer, A. Milioto, A. Haag, J. Behley, and C. Stachniss. Domain Transfer for Semantic Segmentation of LiDAR Data using Deep Neural Networks. In IROS, 2020.\\n\\n[22] Guangrui Li, Guoliang Kang, Wu Liu, Yunchao Wei, and Yi Yang. Content-consistent matching for domain adaptive semantic segmentation. In European Conference on Computer Vision, pages 440\u2013456. Springer, 2020.\\n\\n[23] Guangrui Li, Guoliang Kang, Xiaohan Wang, Yunchao Wei, and Yi Yang. Adversarially masking synthetic to mimic real: Adaptive noise injection for point cloud segmentation adaptation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 20464\u201320474, June 2023.\\n\\n[24] Guangrui Li, Yifan Sun, Zongxin Yang, and Yi Yang. Decompose to generalize: Species-generalized animal pose estimation. In The Eleventh International Conference on Learning Representations, 2023.\\n\\n[25] Zhidong Liang, Zehan Zhang, Ming Zhang, Xian Zhao, and Shiliang Pu. Rangeioudet: Range image based real-time 3d object detector optimized by intersection over union. In CVPR, 2021.\\n\\n[26] Mingsheng Long, ZHANGJIE CAO, Jianmin Wang, and Michael I Jordan. Conditional adversarial domain adaptation. In NeurIPS, 2018.\\n\\n[27] Bjoern Michele, Alexandre Boulch, Gilles Puy, Tuan-Hung Vu, Renaud Marlet, and Nicolas Courty. Saluda: Surface-based automotive lidar unsupervised domain adaptation. In 3DV, 2024.\\n\\n[28] A. Milioto, I. Vizzo, J. Behley, and C. Stachniss. RangeNet++: Fast and Accurate LiDAR Semantic Segmentation. In IROS, 2019.\\n\\n[29] Pietro Morerio, Jacopo Cavazza, and Vittorio Murino. Minimal-entropy correlation alignment for unsupervised deep domain adaptation. ICLR, 2018.\\n\\n[30] S. J. Pan and Q. Yang. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering, 2010.\\n\\n[31] Chunghyun Park, Yoonwoo Jeong, Minsu Cho, and Jaesik Park. Fast point transformer. In CVPR, 2022.\\n\\n[32] Charles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. CVPR, 2017.\"}"}
{"id": "CVPR-2024-411", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Charles R Qi, Li Yi, Hao Su, and Leonidas J Guibas. Point-net++: Deep hierarchical feature learning on point sets in a metric space. NeurIPS, 2017.\\n\\nCan Qin, Haoxuan You, Lichen Wang, C-C Jay Kuo, and Yun Fu. Pointdan: A multi-scale 3d domain adaption network for point cloud representation. NeurIPS, 2019.\\n\\nCristiano Saltori, Fabio Galasso, Giuseppe Fiameni, Nicu Sebe, Elisa Ricci, and Fabio Poiesi. Cosmix: Compositional semantic mix for domain adaptation in 3d lidar segmentation. ECCV, 2022.\\n\\nDino Sejdinovic, Bharath Sriperumbudur, Arthur Gretton, and Kenji Fukumizu. Equivalence of distance-based and rkhs-based statistics in hypothesis testing. The annals of statistics, pages 2263\u20132291, 2013.\\n\\nYuefan Shen, Yanchao Yang, Mi Yan, He Wang, Youyi Zheng, and Leonidas J. Guibas. Domain adaptation on point clouds via geometry-aware implicits. In CVPR, 2022.\\n\\nPei Sun, Weiyue Wang, Yuning Chai, Gamaleldin Elsayed, Alex Bewley, Xiao Zhang, Cristian Sminchisescu, and Dragomir Anguelov. Rsn: Range sparse net for efficient, accurate lidar 3d object detection. In CVPR, 2021.\\n\\nHaotian* Tang, Zhijian* Liu, Shengyu Zhao, Yujun Lin, Ji Lin, Hanrui Wang, and Song Han. Searching efficient 3d architectures with sparse point-voxel convolution. In ECCV, 2020.\\n\\nHugues Thomas, Charles R. Qi, Jean-Emmanuel Deschaud, Beatriz Marcotegui, Franc \u00b8ois Goulette, and Leonidas J. Guibas. Kpconv: Flexible and deformable convolution for point clouds. ICCV, 2019.\\n\\nY. -H. Tsai, W.-C. Hung, S. Schulter, K. Sohn, M.-H. Yang, and M. Chandraker. Learning to adapt structured output space for semantic segmentation. In CVPR, 2018.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 30, 2017.\\n\\nTuan-Hung Vu, Himalaya Jain, Maxime Bucher, Mathieu Cord, and Patrick P\u00b4erez. Advent: Adversarial entropy minimization for domain adaptation in semantic segmentation. In CVPR, 2019.\\n\\nYue Wang, Yongbin Sun, Ziwei Liu, Sanjay E. Sarma, Michael M. Bronstein, and Justin M. Solomon. Dynamic graph cnn for learning on point clouds. ACM TOG, 2019.\\n\\nBichen Wu, Alvin Wan, Xiangyu Yue, and Kurt Keutzer. Squeezeseg: Convolutional neural nets with recurrent crf for real-time road-object segmentation from 3d lidar point cloud. In ICRA, 2018.\\n\\nBichen Wu, Xuanyu Zhou, Sicheng Zhao, Xiangyu Yue, and Kurt Keutzer. Squeezesegv2: Improved model structure and unsupervised domain adaptation for road-object segmentation from a lidar point cloud. In ICRA, 2019.\\n\\nWenxuan Wu, Zhongang Qi, and Li Fuxin. Pointconv: Deep convolutional networks on 3d point clouds. CVPR, 2019.\\n\\nXiaoyang Wu, Yixing Lao, Li Jiang, Xihui Liu, and Hengshuang Zhao. Point transformer v2: Grouped vector attention and partition-based pooling. In NeurIPS, 2022.\\n\\nAoran Xiao, Jiaxing Huang, Dayan Guan, Fangneng Zhan, and Shijian Lu. Synlidar: Learning from synthetic lidar sequential point cloud for semantic segmentation. AAAI, 2022.\\n\\nChenfeng Xu, Bichen Wu, Zining Wang, Wei Zhan, Peter Vajda, Kurt Keutzer, and Masayoshi Tomizuka. Squeezesegv3: Spatially-adaptive convolution for efficient point-cloud segmentation. In ECCV, 2020.\\n\\nJianyun Xu, Ruixiang Zhang, Jian Dou, Yushi Zhu, Jie Sun, and Shiliang Pu. Rpvnet: A deep and efficient range-point-voxel fusion network for lidar point cloud segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16024\u201316033, 2021.\\n\\nMutian Xu, Runyu Ding, Hengshuang Zhao, and Xiaojuan Qi. Paconv: Position adaptive convolution with dynamic kernel assembling on point clouds. In CVPR, 2021.\\n\\nQiangeng Xu, Xudong Sun, Cho-Ying Wu, Panqu Wang, and Ulrich Neumann. Grid-gcn for fast and scalable point cloud learning. CVPR, 2020.\\n\\nXu Yan, Jiantao Gao, Chaoda Zheng, Chao Zheng, Ruimao Zhang, Shuguang Cui, and Zhen Li. 2dpass: 2d priors assisted semantic segmentation on lidar point clouds. In European Conference on Computer Vision, pages 677\u2013695. Springer, 2022.\\n\\nYanchao Yang and Stefano Soatto. Fda: Fourier domain adaptation for semantic segmentation. In CVPR, 2020.\\n\\nYang Zhang, Zixiang Zhou, Philip David, Xiangyu Yue, Zerong Xi, Boqing Gong, and Hassan Foroosh. Polarnet: An improved grid representation for online lidar point clouds semantic segmentation. In CVPR, 2020.\\n\\nZhiyuan Zhang, Binh-Son Hua, and Sai-Kit Yeung. Shellnet: Efficient point cloud convolutional neural networks using concentric shells statistics. In ICCV, 2019.\\n\\nHengshuang Zhao, Li Jiang, Jiaya Jia, Philip HS Torr, and Vladlen Koltun. Point transformer. In ICCV, 2021.\"}"}
