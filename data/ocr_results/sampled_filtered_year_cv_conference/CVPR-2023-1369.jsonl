{"id": "CVPR-2023-1369", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Sony polarization image sensor. https://www.sony-semicon.com/en/products/is/industry/polarization.html. Accessed: 2023-03-23.\\n\\n[2] Neil G Alldrin and David J Kriegman. Toward reconstructing surfaces with arbitrary isotropic reflectance: A stratified photometric stereo approach. In Proc. of International Conference on Computer Vision (ICCV), pages 1\u20138, 2007.\\n\\n[3] Manmohan Chandraker, Jiamin Bai, and Ravi Ramamoorthi. On differential photometric reconstruction for unknown, isotropic BRDFs. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 35(12):2941\u20132955, 2012.\\n\\n[4] Ju Yong Chang, Kyoung Mu Lee, and Sang Uk Lee. Multiview normal field integration using level set methods. In Proc. of Computer Vision and Pattern Recognition (CVPR), pages 1\u20138. IEEE, 2007.\\n\\n[5] Guanying Chen, Kai Han, Boxin Shi, Yasuyuki Matsushita, and Kwan-Yee K. Wong. SDPS-Net: Self-calibrating deep photometric stereo networks. In Proc. of Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\n[6] Zhaopeng Cui, Jinwei Gu, Boxin Shi, Ping Tan, and Jan Kautz. Polarimetric multi-view stereo. In Proc. of Computer Vision and Pattern Recognition (CVPR), 2017.\\n\\n[7] Akshat Dave, Yongyi Zhao, and Ashok Veeraraghavan. PANDORA: Polarization-aided neural decomposition of radiance. Proc. of European Conference on Computer Vision (ECCV), 2022.\\n\\n[8] Yuqi Ding, Yu Ji, Mingyuan Zhou, Sing Bing Kang, and Jinwei Ye. Polarimetric Helmholtz stereopsis. In Proc. of International Conference on Computer Vision (ICCV), 2021.\\n\\n[9] Ondrej Drbohlav and Radim Sara. Unambiguous determination of shape from photometric stereo with unknown light sources. In Proc. of International Conference on Computer Vision (ICCV), volume 1, pages 581\u2013586, 2001.\\n\\n[10] Yoshiki Fukao, Ryo Kawahara, Shohei Nobuhara, and Ko Nishino. Polarimetric normal stereo. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 682\u2013690, 2021.\\n\\n[11] Yasutaka Furukawa, Carlos Hernandez, et al. Multi-view stereo: A tutorial. Foundations and Trends in Computer Graphics and Vision, 9(1-2):1\u2013148, 2015.\\n\\n[12] Yasutaka Furukawa and Jean Ponce. Accurate, dense, and robust multiview stereopsis. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 32(8):1362\u20131376, 2009.\\n\\n[13] Michael Goesele, Noah Snavely, Brian Curless, Hugues Hoppe, and Steven M Seitz. Multi-view stereo for community photo collections. In Proc. of International Conference on Computer Vision (ICCV), 2007.\\n\\n[14] Amos Gropp, Lior Yariv, Niv Haim, Matan Atzmon, and Yaron Lipman. Implicit geometric regularization for learning shapes. In Proceedings of Machine Learning and Systems 2020, pages 3569\u20133579. 2020.\\n\\n[15] Carlos Hernandez, George Vogiatzis, and Roberto Cipolla. Multiview photometric stereo. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 30(3):548\u2013554, 2008.\\n\\n[16] Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh Raskar. Polarized 3D: High-quality depth sensing with polarization cues. In Proc. of International Conference on Computer Vision (ICCV), pages 3370\u20133378, 2015.\\n\\n[17] Achuta Kadambi, Vage Taamazyan, Boxin Shi, and Ramesh Raskar. Depth sensing using geometrically constrained polarization normals. International Journal of Computer Vision (IJCV), 125(1):34\u201351, 2017.\\n\\n[18] Berk Kaya, Suryansh Kumar, Carlos Oliveira, Vittorio Ferrari, and Luc Van Gool. Uncertainty-aware deep multi-view photometric stereo. In Proc. of Computer Vision and Pattern Recognition (CVPR), pages 12601\u201312611, 2022.\\n\\n[19] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics (TOG), 36(4):1\u201313, 2017.\\n\\n[20] Aldo Laurentini. The visual hull concept for silhouette-based image understanding. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 1994.\\n\\n[21] Min Li, Zhenglong Zhou, Zhe Wu, Boxin Shi, Changyu Diao, and Ping Tan. Multi-view photometric stereo: A robust solution and benchmark dataset for spatially varying isotropic materials. IEEE Transactions on Image Processing, 29:4159\u20134173, 2020.\\n\\n[22] Fotios Logothetis, Roberto Mecca, and Roberto Cipolla. A differential volumetric approach to multi-view photometric stereo. In Proc. of International Conference on Computer Vision (ICCV), pages 1052\u20131061, 2019.\\n\\n[23] Kazuma Minami, Hiroaki Santo, Fumio Okura, and Yasuyuki Matsushita. Symmetric-light photometric stereo. In Proc. of IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 2706\u20132714, 2022.\\n\\n[24] Daisuke Miyazaki, Masataka Kagesawa, and Katsushi Ikeuchi. Polarization-based transparent surface modeling from two views. In Proc. of International Conference on Computer Vision (ICCV), volume 3, pages 1381\u20131381, 2003.\\n\\n[25] Daisuke Miyazaki, Robby T Tan, Kenji Hara, and Katsushi Ikeuchi. Polarization-based inverse rendering from a single view. In Proc. of International Conference on Computer Vision (ICCV), volume 3, pages 982\u2013982, 2003.\\n\\n[26] Diego Nehab, Szymon Rusinkiewicz, James Davis, and Ravi Ramamoorthi. Efficiently combining positions and normals for precise 3D geometry. ACM Transactions on Graphics (TOG), 24(3):536\u2013543, 2005.\\n\\n[27] Stanley Osher, Ronald Fedkiw, and K Piechor. Level set methods and dynamic implicit surfaces. Applied Mechanics Reviews, 57(3):B15\u2013B15, 2004.\\n\\n[28] Jaesik Park, Sudipta N Sinha, Yasuyuki Matsushita, Yu-Wing Tai, and In So Kweon. Robust multiview photometric stereo using planar mesh parameterization. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 39(8):1591\u20131604, 2016.\\n\\n[29] Stefan Rahmann and Nikos Canterakis. Reconstruction of specular surfaces using polarization imaging. In Proc. of\"}"}
{"id": "CVPR-2023-1369", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[31] Johannes Lutz Sch\u00f6nb\u00fcrg and Jan-Michael Frahm. Structure-from-motion revisited. In Proc. of Computer Vision and Pattern Recognition (CVPR), 2016.\\n\\n[32] Boxin Shi, Zhipeng Mo, Zhe Wu, Dinglong Duan, Sai-Kit Yeung, and Ping Tan. A benchmark dataset and evaluation for non-Lambertian and uncalibrated photometric stereo. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 2019.\\n\\n[33] Vincent Sitzmann, Julien Martel, Alexander Bergman, David Lindell, and Gordon Wetzstein. Implicit neural representations with periodic activation functions. Advances in Neural Information Processing Systems (NeurIPS), 33:7462\u20137473, 2020.\\n\\n[34] William AP Smith, Ravi Ramamoorthi, and Silvia Tozza. Linear depth estimation from an uncalibrated, monocular polarization image. In Proc. of European Conference on Computer Vision (ECCV), pages 109\u2013125. Springer, 2016.\\n\\n[35] William AP Smith, Ravi Ramamoorthi, and Silvia Tozza. Height-from-polarisation with unknown lighting or albedo. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 41(12):2875\u20132888, 2018.\\n\\n[36] Christophe Stolz, Mathias Ferraton, and Fabrice Meriaudeau. Shape from polarization: A method for solving zenithal angle ambiguity. Optics Letters, 37(20):4218\u20134220, 2012.\\n\\n[37] George Vogiatzis, Carlos Hernandez Esteban, Philip HS Torr, and Roberto Cipolla. Multiview stereo via volumetric graph-cuts and occlusion robust photo-consistency. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 29(12):2241\u20132246, 2007.\\n\\n[38] George Vogiatzis, Philip HS Torr, and Roberto Cipolla. Multi-view stereo via volumetric graph-cuts. In Proc. of Computer Vision and Pattern Recognition (CVPR), 2005.\\n\\n[39] Hoang-Hiep Vu, Patrick Labatut, Jean-Philippe Pons, and Renaud Keriven. High accuracy and visibility-consistent dense multiview stereo. IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), 34(5):889\u2013901, 2011.\\n\\n[40] Robert J Woodham. Photometric method for determining surface orientation from multiple images. Optical Engineering, 19(1):139\u2013144, 1980.\\n\\n[41] Wenqi Yang, Guanying Chen, Chaofeng Chen, Zhenfang Chen, and Kwan-Yee K. Wong. PS-NeRF: Neural inverse rendering for multi-view photometric stereo. In Proc. of European Conference on Computer Vision (ECCV), 2022.\\n\\n[42] Lior Yariv, Yoni Kasten, Dror Moran, Meirav Galun, Matan Atzmon, Basri Ronen, and Yaron Lipman. Multiview neural surface reconstruction by disentangling geometry and appearance. Advances in Neural Information Processing Systems (NeurIPS), 33, 2020.\\n\\n[43] Jinyu Zhao, Yusuke Monno, and Masatoshi Okutomi. Polarimetric multi-view inverse rendering. In Proc. of European Conference on Computer Vision (ECCV), pages 85\u2013102. Springer, 2020.\\n\\n[44] Z. Zhou and P. Tan. Ring-light photometric stereo. In Proc. of European Conference on Computer Vision (ECCV), pages 265\u2013279, 2010.\\n\\n[45] Dizhong Zhu and William AP Smith. Depth from a polarization+RGB stereo pair. In Proc. of Computer Vision and Pattern Recognition (CVPR), pages 7586\u20137595, 2019.\\n\\n[46] Todd E Zickler, Peter N Belhumeur, and David J Kriegman. Helmholtz stereopsis: Exploiting reciprocity for surface reconstruction. International Journal of Computer Vision (IJCV), 49(2):215\u2013227, 2002.\"}"}
{"id": "CVPR-2023-1369", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multi-View Azimuth Stereo via Tangent Space Consistency\\n\\nXu Cao, Hiroaki Santo, Fumio Okura, Yasuyuki Matsushita\\nOsaka University\\n{cao.xu, santo.hiroaki, okura, yasumat}@ist.osaka-u.ac.jp\\n\\nSource code: https://github.com/xucao-42/mvas\\n\\nFigure 1. 3D reconstruction from calibrated multi-view azimuth maps (3 out of 31 are shown). An azimuth angle indicates the surface normal's orientation in the image plane, and an azimuth map records the azimuth angles across the entire surface. We show that azimuth maps can be effectively used for shape and normal recovery. Color images are for reference only and are not used in shape optimization.\\n\\nAbstract\\nWe present a method for 3D reconstruction only using calibrated multi-view surface azimuth maps. Our method, multi-view azimuth stereo, is effective for textureless or specular surfaces, which are difficult for conventional multi-view stereo methods. We introduce the concept of tangent space consistency: Multi-view azimuth observations of a surface point should be lifted to the same tangent space. Leveraging this consistency, we recover the shape by optimizing a neural implicit surface representation. Our method harnesses the robust azimuth estimation capabilities of photometric stereo methods or polarization imaging while bypassing potentially complex zenith angle estimation. Experiments using azimuth maps from various sources validate the accurate shape recovery with our method, even without zenith angles.\\n\\n1. Introduction\\nRecovering 3D shapes of real-world scenes is a fundamental problem in computer vision, and multi-view stereo (MVS) has emerged as a mature geometric method for reconstructing dense scene points. Using 2D images taken from different viewpoints, MVS finds dense correspondences between images based on the photo-consistency assumption, that a scene point's brightness should appear similar across different viewpoints [13, 37\u201339]. However, MVS struggles with textureless or specular surfaces, as the lack of texture leads to ambiguities in establishing correspondences, and the presence of specular reflections violates the photo-consistency assumption [11].\\n\\nPhotometric stereo (PS) offers an alternative approach for dealing with textureless and specular surfaces [32]. By estimating single-view surface normals using varying lighting conditions [40], PS enables high-fidelity 2.5D surface reconstruction [26]. However, extending PS to a multi-view setup, known as multi-view photometric stereo (MVPS) [15], significantly increases image acquisition costs, as it requires multi-view and multi-light images under highly controlled lighting conditions [21].\\n\\nTo mitigate image acquisition costs, simpler lighting setups such as circularly or symmetrically placed lights have been explored [2, 3, 23, 44]. With these lighting setups, estimating the surface normal's azimuth (the angle in the image plane) becomes considerably easier than estimating the zenith (the angle from the camera optical axis) [2, 3, 23]. The ease of azimuth estimation also appears in polarization imaging [29]. While azimuth can be determined up to a $\\\\pi$-ambiguity using only polarization data, zenith estimation requires more complex steps [24, 34, 36].\"}"}
{"id": "CVPR-2023-1369", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In this paper, we introduce Multi-View Azimuth Stereo (MV AS), a method that effectively uses calibrated multi-view azimuth maps for shape recovery (Fig. 1). MV AS is particularly advantageous when working with accurate azimuth acquisition techniques. With circular-light photometric stereo [3], MV AS has the potential to be applied to surfaces with arbitrary isotropic materials. With polarization imaging [7], MV AS allows a passive image acquisition as simple as MVS while being more effective for textureless or specular surfaces.\\n\\nThe key insight enabling MV AS is the concept of Tangent Space Consistency (TSC) for multi-view azimuth angles. We find that the azimuth can be transformed into a tangent using camera orientation. Therefore, multi-view azimuth observations of the same surface point should be lifted to the same tangent space (Fig. 2). TSC helps determine if a 3D point lies on the surface, similar to photo-consistency for finding image correspondences. Moreover, TSC can directly determine the surface normal as the vector orthogonal to the tangent space, enabling high-fidelity reconstruction comparable to MVPS methods. Notably, TSC is invariant to the $\\\\pi$-ambiguity of the azimuth angle, making MV AS well-suited for polarization imaging.\\n\\nWith TSC, we reconstruct the surface implicitly represented as a neural signed distance function (SDF), by constraining the surface normals (i.e., the gradients of the SDF). Experimental results show that MV AS achieves comparable reconstruction performance to MVPS methods [18, 28, 41], even in the absence of zenith information. Further, MV AS outperforms MVS methods [31] in textureless or specular surfaces using azimuth maps from symmetric-light photometric stereo [23] or a snapshot polarization camera [7].\\n\\nIn summary, this paper's key contributions are:\\n\\n\u2022 Multi-View Azimuth Stereo (MV AS), which enables accurate shape reconstruction even for textureless and specular surfaces;\\n\\n\u2022 Tangent Space Consistency (TSC), which establishes the correspondence between multi-view azimuth observations, thereby facilitating the effective use of azimuth data in 3D reconstruction; and\\n\\n\u2022 A comprehensive analysis of TSC, including its necessary conditions, degenerate scenarios, and the application to optimizing neural implicit representations.\\n\\n2. Related Tasks and Concept\\n\\nThis section discusses the relation of MV AS to multi-view photometric stereo (MVPS) and shape-from-polarization (SfP), and compares TSC to photo-consistency.\\n\\n**MVPS versus MV AS**\\n\\nMVPS aims for high-fidelity shape and reflectance recovery using images from different normal azimuths and under different lighting conditions [15, 22]. These \u201cmulti-light\u201d images can be used for estimating and fusing multi-view normal maps [4, 18], for refining coarse meshes initialized by MVS [28], or for jointly estimating the shape and materials in an inverse-rendering manner [41].\\n\\nCompared to MVPS, MV AS has the potential to be applied to (1) surfaces of a broader range of materials and/or (2) in uncontrolled scenarios, benefiting from azimuth inputs. First, azimuth estimation is valid for arbitrary isotropic materials using an uncalibrated circular moving light [3], while MVPS methods require specific surface reflectance modeling (e.g., Lambertian [4] or the microfacet model [41]) or prior learning [18]. Second, MV AS allows passive image capture with polarization imaging, while MVPS has to actively illuminate the scene, limiting MVPS's application in highly controlled environments.\\n\\n**SfP versus MV AS**\\n\\nSfP recovers surfaces using polarization imaging [1]. For dielectric surfaces, the measured angle of polarization (AoP) aligns with the surface normal's azimuth component, up to a $\\\\pi$-ambiguity. SfP studies determine surface normals by resolving this $\\\\pi$-ambiguity and estimating the zenith component [8\u201310, 16, 17, 29, 34, 35, 45]. Some studies use polarization data to refine coarse shapes initialized by multi-view reconstruction methods [6, 43], but the geometric relation between multi-view azimuth angles are not considered.\\n\\nWith TSC and MV AS, both the $\\\\pi$-ambiguity and zenith estimation can be bypassed. Our method relies on TSC, not requiring MVS methods to initialize shapes.\\n\\n**Photo-consistency versus tangent space consistency**\\n\\nPhoto-consistency is a key assumption in MVS for establishing correspondence between multi-view images. This assumption states that a scene point appears similar across different views and struggles with specular surfaces [12]. In contrast, TSC is derived from geometric principles and strictly holds for multi-view azimuth angles. Further, TSC can determine the surface normal, providing more information\"}"}
{"id": "CVPR-2023-1369", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"formation than photo-consistency. However, TSC requires at least three cameras with non-parallel optical axes and can degrade to photo-consistency under certain camera configurations. Similar to photo-consistency's challenges with textureless surfaces, TSC might struggle to establish correspondences for planar surfaces. Details are in Sec. 3.2.\\n\\n3. Proposed Method\\n\\nWe aim to recover the shape from calibrated and masked azimuth maps. Let $\\\\Omega_i$ represent the $i$-th image pixel domain. For each view $i \\\\in \\\\{1, 2, ..., C\\\\}$, we assume the following are available:\\n\\n- a surface azimuth map $\\\\phi_i: \\\\Omega_i \\\\rightarrow [0, 2\\\\pi]$,\\n- a binary mask indicating whether a pixel is inside the shape silhouette $O_i: \\\\Omega_i \\\\rightarrow \\\\{0, 1\\\\}$, and\\n- the projection from the world coordinates to the image pixel coordinates $\\\\Pi_i: \\\\mathbb{R}^3 \\\\rightarrow \\\\Omega_i$, consisting of the extrinsic rigid-body transformation $P_i = [R_i | t_i] \\\\in \\\\text{SE}(3)$ and intrinsic perspective camera projection $K_i$.\\n\\nWe describe the proposed method in three sections. First, we detail the transformation from an azimuth angle to a projected tangent vector (Sec. 3.1). Next, we discuss multi-view tangent space consistency for surface points, including its four degenerate scenarios and $\\\\pi$-invariance (Sec. 3.2). Lastly, we present the surface reconstruction by optimizing a neural implicit representation based on the tangent space consistency loss (Sec. 3.3).\\n\\n3.1. The projected tangent vector\\n\\nThis section will show how to convert an azimuth angle to a tangent vector of the surface point, given the world-to-camera rotation. We will only consider single-view observations and ignore the view index in this section.\\n\\nIn the world coordinates, consider a unit normal vector $n(x) \\\\in S^2 \\\\subset \\\\mathbb{R}^3$ of a surface point $x \\\\in \\\\mathbb{R}^3$. Suppose a rigid-body transformation $[R | t]$ transforms the surface from the world coordinates to the camera coordinates. The direction of the normal vector in the camera coordinates $n_c$ is rotated accordingly as $R n_c = n_c$.\\n\\nIn the camera coordinates, we can parameterize the unit normal vector by its azimuth angle $\\\\phi \\\\in [0, 2\\\\pi]$ and zenith angle $\\\\theta \\\\in [0, \\\\pi/2]$ as $n_c = \\\\begin{bmatrix} n_{cx} \\\\\\\\ n_{cy} \\\\\\\\ n_{cz} \\\\end{bmatrix} = \\\\begin{bmatrix} \\\\sin\\\\theta \\\\cos\\\\phi \\\\\\\\ \\\\sin\\\\theta \\\\sin\\\\phi \\\\\\\\ \\\\cos\\\\theta \\\\end{bmatrix}$.\\n\\nFrom Eq. (2), we can derive the relation between $n_{cx}$ and $n_{cy}$ in terms of only the azimuth angle as $n_{cx} \\\\sin\\\\phi = n_{cy} \\\\cos\\\\phi$.\\n\\nDenoting the rotation matrix as $R = \\\\begin{bmatrix} -r_{1}^\\\\top \\\\\\\\ -r_{2}^\\\\top \\\\\\\\ -r_{3}^\\\\top \\\\end{bmatrix} \\\\in \\\\text{SO}(3)$, and putting Eqs. (1) to (4) together, we obtain $r_{1}^\\\\top n \\\\sin\\\\phi = r_{2}^\\\\top n \\\\cos\\\\phi$.\\n\\nRearranging Eq. (5) yields $n^\\\\top (r_{1} \\\\sin\\\\phi - r_{2} \\\\cos\\\\phi) | \\\\{z\\\\} t(\\\\phi) = 0$.\\n\\nWe call $t(\\\\phi)$ the projected tangent vector, as it is computed from the projected azimuth angle and perpendicular to the surface normal. As shown in Figure 3, the transformation from azimuth maps to tangent maps reveals that projected tangent vectors encode camera orientation information, providing useful hints for multi-view reconstruction.\\n\\nProperties\\n\\nThe projected tangent vector is the unit vector parallel to the intersection of the tangent and image spaces. Based on Eq. (6), $t^\\\\top t = r_{1}^\\\\top r_{1} \\\\sin^2\\\\phi + r_{2}^\\\\top r_{2} \\\\cos^2\\\\phi - 2 r_{1}^\\\\top r_{2} \\\\cos\\\\phi \\\\sin\\\\phi = \\\\sin^2\\\\phi + \\\\cos^2\\\\phi = 1$, since $r_1$ and $r_2$ are orthonormal vectors.\\n\\nThe inset illustrates the second property. Let $e_x$, $e_y$, and $e_z$ be the unit direction vector of the $x$-, $y$-, and $z$-axis of the camera coordinates in the world coordinates. Then $r_{1} = e_x$ and $r_{2} = e_y$, which follows that $t(\\\\phi)$ is a linear combination of camera's $x$- and $y$-axes and thus parallel to the image plane. We can compute the intersection direction of two planes by taking the cross-product of their normals, namely, the surface normal and the principle axis. Hence, $t \\\\parallel n \\\\times e_z$. The two properties are helpful in analyzing the tangent space consistency, as described next.\"}"}
{"id": "CVPR-2023-1369", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"This section discusses the consistency between multi-view azimuth observations in the tangent space of a surface point. In addition, four degenerate scenarios and \u03c0-invariance will be discussed. We assume the surface point under consideration is visible to all cameras in this section.\\n\\nDenote the projected tangent vector of a surface point in \\\\(i\\\\)-th view as:\\n\\n\\\\[ t_i(x) = \\\\tan(\\\\phi_i(\\\\Pi_i(x))) \\\\]\\n\\nBy Eq. (6), a surface point \\\\(x\\\\), its normal direction \\\\(n\\\\), and its multi-view projected tangent vectors \\\\(t_i\\\\) should satisfy:\\n\\n\\\\[ n(x)^\\\\top t_i(x) = 0 \\\\quad \\\\forall i. \\\\]\\n\\nLet \\\\(T(x) = [t_1(x), t_2(x), \\\\ldots, t_C(x)]^\\\\top \\\\in \\\\mathbb{R}^{C \\\\times 3}\\\\) be the matrix formed by stacking projected tangent vectors of all \\\\(C\\\\) views. Then Eq. (9) reads:\\n\\n\\\\[ T(x)n(x) = 0. \\\\]\\n\\nEquation (10) can only be satisfied if the rank of \\\\(T(x)\\\\) is either 1 or 2. The rank cannot be 0 as projected tangent vectors are unit length. The case rank \\\\((T(x)) = 3\\\\) cannot satisfy Eq. (10) as surface normals are non-zero vectors.\\n\\nWe refer to the case where the rank of \\\\(T(x)\\\\) is 2 as tangent space consistency (TSC). In this case, multi-view projected tangent vectors from a surface point span its tangent space, and the surface normal is determined up to a sign ambiguity. On the other hand, when rank \\\\((T(x)) = 1\\\\), the projected tangent vectors can only span a tangent line and constrain the surface normal on the plane orthogonal to the tangent line. This can occur when camera optical axes are parallel, as explained later.\\n\\nTSC can help distinguish non-surface points (wrong correspondences) from surface points (possibly correct correspondences) and determine the surface normals, as shown in Fig. 4. For wrong correspondences, their projected tangent vectors are expected to have a rank of 3 and span the entire 3D space. On the other hand, for surface points, their projected tangent vectors span the tangent space, i.e., rank \\\\((T(x)) = 2\\\\). In addition, TSC requires the surface normal to be in the null space of \\\\(T(x)\\\\), i.e., perpendicular to the tangent space spanned by projected tangent vectors. This makes TSC more informative than photo-consistency since photo-consistency cannot directly determine the surface normal.\\n\\nTo effectively distinguish surface/non-surface points using TSC, a non-planar surface must be observed by at least three cameras with non-parallel optical axes. These requirements indicate four degeneration scenarios, as shown in Fig. 5 and discussed below. Table 1 summarizes the variations of rank \\\\((T(x))\\\\) in these scenarios.\\n\\n| Scenarios                  | Non-surface points | Surface points | Surface normal |\\n|---------------------------|--------------------|----------------|----------------|\\n| Two-view                  | 2                  | 2              | 2              |\\n| Co-linear optical axes    | 2                  | 1              | 1              |\\n| Co-planar optical axes    | 2                  | 1              | 1              |\\n| Planar surface            | 2                  | 2              | \u2713              |\\n| \u2713 TSC                     | 2                  | 2              | \u2713              |\\n\\nFigure 5. Degeneration scenarios where TSC cannot distinguish good correspondence from bad ones.\\n\\nFigure 4. (Left) Multi-view projected tangent vectors from a surface point span its tangent space and determine the surface normal. (Right) Conversely, multi-view projected tangent vectors from a non-surface point (i.e., wrong correspondence) are expected to span the 3D space.\\n\\nTable 1. Rank of \\\\(T(x)\\\\) in four degenerate cases of TSC.\"}"}
{"id": "CVPR-2023-1369", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A more special case is when cameras with coplanar optical axes observe coplanar surface normals, such as a rotating camera observing a cylinder. In this case, the cross product of the coplanar normal and optical axis vectors yields co-linear projected tangent vectors. As such, the rank of $T(x)$ is 1 for surface points, and TSC again degrades to photo-consistency. However, this degradation does not occur for non-coplanar surface normals, meaning TSC can still be effective for general surfaces.\\n\\nSurface types TSC breaks down for a planar surface. At any location on the planar surface, $n(x)$ is the same and rank$(T(x))$ is identically 2 for arbitrary correspondence. However, the normal direction of this plane can still be correctly determined in the case rank$(T(x)) = 2$, i.e., at least three non-frontal parallel views. The planar surface can be seen as the counterpart to the textureless region for photo-consistency. However, unlike photo-consistency, TSC can still determine the surface normal.\\n\\n$\\\\pi$-invariance TSC remains effective when the azimuth angle is changed by $\\\\pi$. By Eq. (6), the sign of the projected tangent vector will be reversed:\\n\\n$$t(\\\\psi + \\\\pi) = -r_1 \\\\sin \\\\psi + r_2 \\\\cos \\\\psi = -t(\\\\psi). \\\\quad (11)$$\\n\\nIntuitively, reversing the direction of a tangent vector still places it in the same tangent space, as $n^\\\\top (-t) = 0$ when $n^\\\\top t = 0$. Mathematically, reversing the signs of arbitrary rows in $T(x)$ does not affect the rank of $T(x)$. This $\\\\pi$-invariance can be particularly useful for polarization imaging, as they can only measure azimuth angles up to a $\\\\pi$ ambiguity.\\n\\n3.3. Multi-view azimuth stereo\\n\\nWe propose the following TSC-based functional for multi-view geometry reconstruction:\\n\\n$$J = \\\\sum_{i=1}^{M} \\\\sum_{i=1}^{P} C_i \\\\Phi_i(x) n(x)^\\\\top t_i(x) \\\\sum_{i=1}^{M} \\\\sum_{i=1}^{P} C_i \\\\Phi_i(x) d_M. \\\\quad (12)$$\\n\\nHere, $M$ is the surface embedded in the 3D space, and $d_M$ is the infinitesimal area on the surface. $\\\\Phi_i(x)$ is a binary function indicating the visibility of the point $x$ from the $i$-th viewpoint:\\n\\n$$\\\\Phi_i(x) = \\\\begin{cases} 1 & \\\\text{if } x \\\\text{ is visible to } i \\\\text{-th camera} \\\\\\\\ 0 & \\\\text{otherwise} \\\\end{cases}. \\\\quad (13)$$\\n\\nWe can simplify Eq. (12) as follows:\\n\\n$$J = \\\\sum_{i=1}^{M} n^\\\\top \\\\tilde{T} n d_M$$\\n\\nwith\\n\\n$$\\\\tilde{T} = \\\\sum_{i=1}^{M} \\\\sum_{i=1}^{P} C_i \\\\Phi_i t_i t_i^\\\\top C_i.$$ \\n\\n(14)\\n\\nA similar phenomenon exists in Helmholtz stereopsis [46], where wrong correspondence might still result in the correct normal estimation.\\n\\nFigure 6. To optimize the neural SDF, we project the surface point onto all views and enforce the surface normal to be perpendicular to all visible projected tangent vectors.\\n\\nwhere we omit the dependence on the surface point $x$ for clarity. As discussed in Sec. 3.2, accurate surface points and normals are both necessary to minimize the functional.\\n\\nWe represent the surface implicitly using a signed distance function (SDF) and optimize the SDF based on the framework of implicit differentiable renderer (IDR) [42]. We parameterize the SDF by a multi-layer perceptron (MLP) as $f(x; \\\\theta) : \\\\mathbb{R}^3 \\\\times \\\\mathbb{R}^d \\\\rightarrow \\\\mathbb{R}$, where $x \\\\in \\\\mathbb{R}^3$ is the 3D point coordinate, and $\\\\theta \\\\in \\\\mathbb{R}^d$ are MLP parameters. The surface $M$ is implicitly represented as the zero-level set of the SDF $M(\\\\theta) = \\\\{x | f(x; \\\\theta) = 0\\\\}$, which varies depending on the MLP parameters.\\n\\nTo optimize the MLP, we use a loss function that consists of the tangent space consistency loss, the silhouette loss, and the Eikonal regularization:\\n\\n$$L = L_{TSC} + \\\\lambda_1 L_{silhouette} + \\\\lambda_2 L_{Eikonal}. \\\\quad (16)$$\\n\\nIn each batch of the optimization, we randomly sample a set of $P$ pixels from all views, cast camera rays from these pixels into the scene, and find the first ray-surface intersections. We evaluate the TSC loss for pixels with ray-surface intersections located inside the silhouette, denoted as $X$.\\n\\nWe evaluate the silhouette loss for pixels that do not have ray-surface intersections or are located outside the silhouette, denoted as $\\\\tilde{X}$.\\n\\nTangent space consistency loss Based on Eq. (14), we define the TSC loss as\\n\\n$$L_{TSC} = \\\\frac{1}{P} \\\\sum_{x \\\\in X} n(x; \\\\theta)^\\\\top \\\\tilde{T} n(x; \\\\theta). \\\\quad (17)$$\\n\\nTo evaluate the TSC loss, we need to evaluate the surface normal and construct the matrix $\\\\tilde{T}$. According to the property of SDF [27], the surface normal direction is the gradient evaluated at a zero-level set point:\\n\\n$$n(x; \\\\theta) = \\\\nabla_x f(x; \\\\theta). \\\\quad (18)$$\\n\\n829\\n\\n829\"}"}
{"id": "CVPR-2023-1369", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Here, the surface normal can still be represented analyti-\\ncally as the MLP parameters \\\\([14, 33]\\\\). Therefore, the gra-\\ndient of the loss functions can be backpropagated to MLP\\nparameters via surface normals.\\n\\nWe then compute \\\\(T(x)\\\\) for the point \\\\(x\\\\) from all visible\\nviews. First, we project the surface points onto all views\\nand check their visibility in each, as shown in Fig. 6. To\\ndetermine the visibility, we march the surface points toward\\nthe camera center and check whether there is a negative dis-\\ntance on the ray; see the supplementary material for more\\ndetails. Then in visible views, we compute the projected\\ntangent vectors from input azimuth maps.\\n\\nSilhouette loss\\nFollowing IDR \\\\([42]\\\\), we use the input\\nmasks to constrain the visual hull of the shape\\n\\\\(2\\\\). We find\\nthe minimal distance on the rays for pixels that do not have\\nray-surface intersections, denoted as \\\\(f^*\\\\). The silhouette loss\\nis then\\n\\\\[\\nL_{\\\\text{silhouette}} = \\\\frac{1}{\\\\alpha} \\\\sum_{x \\\\in \\\\tilde{X}} \\\\text{\u03a8}(\\\\text{\u03a0}(x), \\\\sigma(\\\\alpha f^*)),\\n\\\\]\\nwhere \\\\(\u03a8\\\\) is the cross entropy function, and \\\\(\\\\sigma(\\\\cdot)\\\\) is a sigmoid\\nfunction with \\\\(\\\\alpha\\\\) controlling its sharpness.\\n\\nEikonal regularization\\nFollowing IGR \\\\([14]\\\\), we use the\\nEikonal loss to regularize the gradient of SDF such that the\\ngradient norm is close to \\\\(1\\\\) everywhere \\\\([27]\\\\):\\n\\\\[\\nL_{\\\\text{Eikonal}} = \\\\mathbb{E}_x(\\\\|n\\\\|_2^2 - 1)^2.\\n\\\\]\\n\\nTo apply Eikonal regularization, we randomly sample\\npoints within the object bounding box and compute the\\nmean squared deviation from \\\\(1\\\\)-norm.\\n\\nNone of the three loss functions explicitly constrain the\\nsurface points. It is the TSC loss that implicitly encourages\\ngood correspondence.\\n\\n4. Experiments\\nWe evaluate MV AS in three experiments: comparing\\nwith MVPS methods quantitatively for surface and normal\\nreconstruction in Sec. 4.1, applying MV AS to a photomet-\\nric stereo method which struggles with zenith estimation in\\nSec. 4.2, and using MV AS with passive polarization imag-\\ning in Sec. 4.3. Implementation details are in the supple-\\nmentary material.\\n\\n4.1. MV AS versus MVPS\\nBaselines\\nWe assess MV AS against multiple MVPS\\nmethods using the DiLiGenT-MV benchmark \\\\([21]\\\\). The\\nMVPS methods include the coarse mesh refinement method\\nR-MVPS \\\\([28]\\\\), the benchmark method B-MVPS \\\\([21]\\\\),\\n2 IDR \\\\([42]\\\\] refers to it as mask loss, but we prefer to use \u201csilhouette loss\u201d\\nafter shape-from-silhouette \\\\([20]\\\\).\\n\\nthe depth-normal fusion-based method UA-MVPS \\\\([18]\\\\),\\nand the neural inverse rendering method PS-NeRF \\\\([41]\\\\).\\nDiLiGenT-MV \\\\([21]\\\\] captures\\n20 views under\\n96 different\\nlights for five objects. We use\\n15-view azimuth maps for\\noptimization and leave out\\n5 views for testing, following\\nPS-NeRF \\\\([41]\\\\). The azimuth maps are computed from the\\nnormal maps estimated by the self-calibrated photometric\\nstereo method SDPS \\\\([5]\\\\).\\n\\nEvaluation metrics\\nWe use Chamfer distance (CD) and\\nF-score for geometry accuracy \\\\([18, 19]\\\\), and mean angular\\nerror (MAE) for normal accuracy \\\\([41]\\\\). For CD and F-score,\\nwe only consider visible points by casting rays for all pixels\\nand finding the first ray-mesh intersections \\\\(3\\\\).\\n\\nResults and discussions\\nTable 2 reports the geome-\\ntry accuracy of the recovered DiLiGenT-MV surfaces.\\nB-MVPS \\\\([21]\\\\) achieves the best scores in\\n4 objects due to\\nthe usage of calibrated light information. UA-MVPS \\\\([18]\\\\)\\ndistorts the surface reconstruction by not considering the\\nmulti-view consistency. MV AS outperforms PS-NeRF \\\\([41]\\\\)\\nin\\n3 objects without modeling the rendering process.\\n\\nFigure 7 visually compares recovered \u201cBuddha\u201d and\\n\u201cReading\u201d objects. Despite not having the best numerical\\nscores, our method produces comparable results. Lower\\nscores for these objects are mainly due to our method\u2019s\\nsensitivity to inaccurate silhouette masks provided by\\nDiLiGenT-MV \\\\([21]\\\\). We project the GT surface onto the\\nimage plane and find up to 10-pixel inconsistency between\\nthe projected region and the GT mask. Thus, the silhou-\\nette loss Eq. (19) encourages our reconstructed surfaces to\\nshrink to align with the smaller silhouettes.\\n\\nOur method requires less effort for shape recovery than\\nB-MVPS \\\\([21]\\\\) and PS-NeRF \\\\([41]\\\\). While B-MVPS \\\\([21]\\\\)\\ncalibrates\\n96 light directions and intensities, we use\\na self-calibrated PS method for input azimuth maps.\\nPS-NeRF \\\\([41]\\\\) uses\\n15 view \\\\times 96 light\\n= 1440 images\\nto optimize multiple MLPs that model shape and appear-\\nance, which requires a high computational cost. It takes\\nPS-NeRF \\\\([41]\\\\) over 20 hours per object on an RTX 3090\\nGPU. In contrast, our approach optimizes a single MLP\\nwith\\n15 azimuth maps, taking approximately\\n3 hours per\\nobject on an RTX 2080Ti GPU.\\n\\nTable 3 reports MAE for\\n5 test and all\\n20 viewpoints, and\\nFig. 8 visually compares recovered normal maps. MV AS\\nimproves normal accuracy compared to SDPS \\\\([5]\\\\) and out-\\nperforms PS-NeRF \\\\([41]\\\\) in\\n4 objects, demonstrating TSC\u2019s\\neffectiveness in constraining surface normals from multi-\\nview observations. Since TSC imposes a direct constraint\\non surface normals, it is more effective than modeling a ren-\\ndering process as in PS-NeRF \\\\([41]\\\\).\\n\\n3 Different strategies for computing CD yield different results to the\\noriginal papers. UA-MVPS crops the invisible bottom face and uses mesh\\nvertices \\\\([18]\\\\); PS-NeRF \\\\([41]\\\\) samples\\n10000 points from the mesh surface.\"}"}
{"id": "CVPR-2023-1369", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Chamfer distance (\u2193) and F-score (\u2191) [18, 19] of recovered geometry on DiLiGenT-MV benchmark [21].\\n\\n| Methods       | Bear | Buddha | Cow   | Pot2  | Reading | Average |\\n|---------------|------|--------|-------|-------|---------|---------|\\n| R-MVPS [28]   | 1.070| 0.397  | 0.440 | 1.504 | 0.561   | 0.794   |\\n| B-MVPS [21]   | 0.212| 0.254  | 0.091 | 0.201 | 0.259   | 0.203   |\\n| UA-MVPS [18]  | 0.414| 0.452  | 0.326 | 0.414 | 0.382   | 0.398   |\\n| PS-NeRF [41]  | 0.260| 0.314  | 0.287 | 0.254 | 0.352   | 0.293   |\\n| MV AS (ours)  | 0.243| 0.357  | 0.216 | 0.197 | 0.522   | 0.307   |\\n\\nTable 3. Mean angular error (\u2193) of recovered normal maps [21], evaluated using (Top) 5 test views and (Bottom) all 20 views.\\n\\n| Methods       | # views | Bear | Buddha | Cow   | Pot2  | Reading | Average |\\n|---------------|---------|------|--------|-------|-------|---------|---------|\\n| R-MVPS [28]   | 5       | 12.80| 13.67  | 10.81 | 14.99 | 11.71   | 12.80   |\\n| PS-NeRF [41]  | 5       | 3.45 | 10.25  | 4.35  | 5.94  | 9.36    | 6.67    |\\n| SDPS [5]      | 5       | 7.59 | 11.16  | 9.46  | 7.95  | 16.16   | 10.46   |\\n| MV AS (ours)  | 5       | 3.08 | 9.90   | 3.72  | 5.07  | 10.02   | 6.36    |\\n\\n4.2. MV AS for symmetric-light photometric stereo\\n\\nSome photometric stereo methods can estimate azimuth angles well but struggle with zenith angles [3, 23]. This section shows how MV AS can be used for an uncalibrated photometric stereo setup to eliminate the need for tedious zenith estimation while allowing full surface reconstruction.\\n\\nWe use the setup shown in Fig. 9 to obtain multi-view azimuth maps. We place four lights symmetrically around the camera and the target object on a rotation table. In each view, we capture one ambient-light image and four lit images. The ambient-light images are used for SfM [30] to obtain the camera poses and are input to MVS [31] for comparison. Using the four lit images, the azimuth angles can be trivially computed from the ratio of the vertical to the horizontal difference image [23].\\n\\nFigure 8 shows the recovered normal maps and angular error maps from the first view of DiLiGenT-MV [21] on the object \u201cPot2\u201d and \u201cReading.\u201d\"}"}
{"id": "CVPR-2023-1369", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 10. Visual comparison between MV AS and MVS on surface and normal reconstruction.\\n\\nFigure 11. Qualitative comparison of recovered surfaces and normals using a polarization camera. 35 views are used.\\n\\nTSC is $\\\\pi$-invariant, MV AS eliminates the need to correct the $\\\\pi$-ambiguity. Figure 11 compares the surface and normal reconstruction on the multi-view polarization image dataset. We input the color images into Colmap and reproduce the results of the polarimetric inverse rendering method PANDORA using their codes. We modify our TSC loss to account for $\\\\pm \\\\pi/2$ ambiguity in polar-azimuth maps; see the supplementary material for details.\\n\\nAs shown in Fig. 11, MVS breaks down for highly specular objects. Polar-azimuth observations are robust to such specularity and allow MV AS for faithful reconstruction. The comparison to PANDORA shows that surfaces can be recovered without considering the degree of polarization or reflectance-light modeling.\\n\\n5. Discussions\\n\\nWe present MV AS, an approach for reconstructing surfaces from multi-view azimuth maps. By establishing multi-view consistency in the tangent space and optimizing a neural SDF with the TSC loss, MV AS achieves comparable results to MVPS methods without zenith information. We verify MV AS's effectiveness with real-world azimuth maps obtained by symmetric-light photometric stereo and polarization measurements. Our results suggest that MV AS can enable high-fidelity reconstruction of shapes that have been challenging for traditional MVS methods.\\n\\nToday, azimuth maps are still more expensive to obtain than ordinary color images, which may limit the application of MV AS. However, the situation will be changed when commercial polarimetric cameras are more accessible.\\n\\nAcknowledgement\\n\\nWe thank Wenqi Yang, Akshat Dave, and Berk Kaya for code/data, and Boxin Shi, Min Li, and Heng Guo for discussions. This work was supported by JSPS KAKENHI Grant Number JP19H01123.\"}"}
