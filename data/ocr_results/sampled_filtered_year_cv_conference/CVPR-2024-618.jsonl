{"id": "CVPR-2024-618", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Discriminative Probing and Tuning for Text-to-Image Generation\\n\\nLeigang Qu, Wenjie Wang*, Yongqi Li, Hanwang Zhang, Liqiang Nie, Tat-Seng Chua\\n\\n1 National University of Singapore, 2 Hong Kong Polytechnic University, 3 Nanyang Technological University, 4 Skywork AI, 5 Harbin Institute of Technology (Shenzhen)\\n\\nAbstract\\n\\nDespite advancements in text-to-image generation (T2I), prior methods often face text-image misalignment problems such as relation confusion in generated images. Existing solutions involve cross-attention manipulation for better compositional understanding or integrating large language models for improved layout planning. However, the inherent alignment capabilities of T2I models are still inadequate. By reviewing the link between generative and discriminative modeling, we posit that T2I models' discriminative abilities may reflect their text-image alignment proficiency during generation. In this light, we advocate bolstering the discriminative abilities of T2I models to achieve more precise text-to-image alignment for generation. We present a discriminative adapter built on T2I models to probe their discriminative abilities on two representative tasks and leverage discriminative fine-tuning to improve their text-image alignment. As a bonus of the discriminative adapter, a self-correction mechanism can leverage discriminative gradients to better align generated images to text prompts during inference. Comprehensive evaluations across three benchmark datasets, including both in-distribution and out-of-distribution scenarios, demonstrate our method's superior generation performance. Meanwhile, it achieves state-of-the-art discriminative performance on the two discriminative tasks compared to other generative models. The code is available at https://dpt-t2i.github.io/.\\n\\n1. Introduction\\n\\nText-to-image generation (T2I) aims to synthesize high-quality and semantically-relevant images to a given free-form text prompt. In recent years, the rapid development of diffusion models [22, 47] has ignited the research enthusiasm for content generation, leading to a significant leap in T2I [40, 43, 45]. However, due to the weak compositional reasoning capabilities, current T2I models still suffer from the text-image misalignment problem [28], such as attribute binding [15], counting error [37], and relation confusion [37] (see Fig. 1), especially in complicated multi-object generation scenes.\\n\\nTwo lines of work have made remarkable progress in improving text-image alignment for T2I models. The first line proposes to intervene in cross-modal attention activations guided by linguistic structures [15] or test time optimization [4]. However, they heavily rely on the induc-\\n\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation. Except for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\"}"}
{"id": "CVPR-2024-618", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tive bias for manipulating attention structures, often necessitating expertise in vision-language interaction. This expertise is not easily acquired and lacks flexibility. In contrast, another research line [16, 37] borrows LLM\u2019s linguistic comprehension and compositional abilities for layout planning, and then incorporates layout-to-image models (e.g., GLIGEN [31]) for controllable generation. Although these methods mitigate misalignment issues like counting error, they heavily rely on intermediate states, e.g., bounding boxes, for layout representation. The intermediate states may not adequately capture fine-grained visual attributes, and can also accumulate errors in this two-stage paradigm. Furthermore, the intrinsic compositional reasoning abilities of T2I models are still inadequate.\\n\\nTo tackle these issues, we aim to promote text-image alignment by directly catalyzing the intrinsic compositional reasoning of T2I models, without depending on the induc-tive bias for attention manipulation or intermediate states. Richard Feynman famously stated, \u201cWhat I cannot create, I do not understand,\u201d underscoring the significance of understanding in the process of creation. This motivates us to consider enhancing the understanding abilities of T2I models to facilitate their text-to-image generation. As illustrated in Fig. 1, T2I models are more likely to generate an image with correct semantics if they can distinguish the alignment difference between the text prompt and the two images with minor semantic variations.\\n\\nIn light of this, we propose to examine the understanding abilities of T2I models by two discriminative tasks. First, we probe the discriminative global matching ability of T2I models on Image-text Matching (ITM) [17, 36], a representative task to evaluate fundamental text-image alignment. The second discriminative task inspects the local grounding ability of T2I models. One representative task is Referring Expression Comprehension (REC) [58], which examines the fine-grained expression-object alignment within an image. Based on the two tasks, we aim to 1) probe the discriminative abilities of T2I models, especially the compositional semantic alignment, and 2) further improve their discriminative abilities for better text-to-image generation.\\n\\nToward this end, we propose a Discriminative Probing and Tuning (DPT) paradigm to examine and improve text-image alignment of T2I models in a two-stage process. 1) To probe the discriminative abilities, DPT incorporates a Discriminative Adapter to do the ITM and REC tasks based on the semantic representations of T2I models. For example, DPT may take the feature maps from U-Net of diffusion models [43] as semantic representations. And 2) in the second stage, DPT further improves the text-image alignment by means of parameter-efficient fine-tuning, e.g., LoRA [23]. In addition to the adapter, DPT fine-tunes the foundation T2I models to strengthen its intrinsic compositional reasoning abilities for both discriminative and generative tasks. As an extension, we present a self-correction mechanism to guide T2I models for better alignment by gradient-based guidance signals from the discriminative adapter. We conduct extensive experiments on three alignment-oriented text-to-image generation benchmarks and four ITM and REC benchmarks under in-distribution and out-of-distribution settings, validating the effectiveness of DPT in enhancing both generative and discriminative abilities of T2I models. The main contributions of this work are threefold.\\n\\n\u2022 We retrospect the relations between generative and discriminative modeling, and propose a simple yet effective paradigm called DPT to probe and improve the basic discriminative abilities of T2I models for better text-to-image generation.\\n\\n\u2022 We present a discriminative adapter to achieve efficient probing and tuning in DPT. Besides, we extend T2I models with a self-correction mechanism guided by the discriminative adapter for alignment-oriented generation.\\n\\n\u2022 We conduct extensive experiments on three text-to-image generation datasets and four discriminative datasets, significantly enhancing the generative and discriminative abilities of representative T2I models.\\n\\n2. Related Work\\n\\n\u2022 Text-to-Image Generation. Over the past decades, great efforts on Variational Autoencoders [55], Generative Adversarial Networks [54, 59], and Auto-regression Models [9, 39, 57] have been dedicated to generating high-quality images with text conditions. Recently, there has been a flurry of interest in Diffusion Probabilistic Models (DMs) [22, 47] due to their stability and scalability. To further improve the generation quality, large-scale models such as DALL\u00b7E 2 [40], Imagen [45], and GLIDE [35], emerged to synthesize photorealistic images. This work mainly focuses on diffusion models and especially takes the opensourced Stable Diffusion (SD) [43] as the base model.\\n\\n\u2022 Improving Text-Image Alignment. Despite the thrilling success, current T2I models still suffer from Text-Image Misalignment issues [1, 8, 18], especially in complex scenes requiring compositional reasoning [34]. Several pioneering efforts were made to introduce guidance to intervene in internal features of SD to stimulate the high-alignment generation. For example, StructureDiffusion [15] parses prompts into tree structures and incorporates them with cross-attention representations to promote compositional generation. Attend-and-Excite [4] manipulates cross-attention units to attend to all textual subject tokens and enhance the activations in attention maps. Despite the notable\"}"}
{"id": "CVPR-2024-618", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"momentum, they are limited to tackling problems including missing objects and incorrect attributes, and ignore relation enhancement. Another thread of work, e.g., LayoutLLM-T2I [37] and LayoutGPT [16], resorts to two-stage coarse-to-fine frameworks [14, 19, 41], in which they first induce explicit intermediate bounding box-based layout, and then synthesize images. However, such an intermediate layout may not be sufficient to represent complex scenes and they almost abandon the intrinsic reasoning abilities of pre-trained T2I models. In this work, we propose a discriminative tuning paradigm by stimulating discriminative abilities of pre-trained T2I models for high-alignment generation.\\n\\n\u2022 Generative and Discriminative Modeling. The thrilling progress of LLMs enables generative models to complete discriminative tasks, which motivates researchers to exploit understanding abilities [33] with foundation visual generative models in Image Classification [5, 7, 29, 56], Segmentation [2, 53, 60], and Image-Text Matching [26]. Besides, DreamLLM [10] unifies generation and discrimination in a multimodal auto-regressive framework and reveals the potential synergy. On the contrary, a recent work [51] discusses the generative AI paradox and showed LLMs may not indeed understand what they have generated. To the best of our knowledge, we are the first to study discriminative tuning to promote alignment in T2I.\\n\\n3. Method\\n\\nIn this section, we introduce the DPT paradigm to probe and enhance the discriminative abilities of foundation T2I models. As shown in Fig. 2, DPT consists of two stages, i.e., Discrimination Probing and Discrimination Tuning, as well as a self-correction mechanism in Sec. 3.3.\\n\\n3.1. Stage 1 \u2013 Discriminative Probing\\n\\nIn the first stage, we aim to develop a probing method to explore \\\"How powerful are discriminative abilities of recent T2I models?\\\" To this end, we first select representative T2I models and semantic representations, and then consider adapting the T2I models to do discriminative tasks.\\n\\n\u2022 Stable Diffusion for Discriminative Probing. Considering SD is open-sourced and one of the most powerful and popular T2I models, we select its different versions (see Sec. 4.2) as representative models to probe the discriminative abilities. To make generative diffusion models semantically focused and efficient, SD [43] performs denoising in a latent low-dimensional space. It includes VAE [25], Text Encoder of CLIP [38], and U-Net [44]. The U-Net serves as a neural backbone for denoising score matching in the latent space, composed of three parts, i.e., down blocks, mid blocks, and up blocks. During training, given a positive image-text pair \\\\((x, y)\\\\), SD first encodes image \\\\(x\\\\) with the VAE encoder and adds noise \\\\(\\\\epsilon \\\\sim N(0, 1)\\\\) to obtain the latent \\\\(z_t = h(x, t)\\\\) at timestep \\\\(t\\\\). Thereafter, SD employs U-Net to predict the added noise and optimizes the model parameters by minimizing the L2 loss between the ground-truth noise and the predicted one.\\n\\n\u2022 Semantic Representations. It is non-trivial to leverage T2I models such as SD to do discriminative tasks. Fortunately, recent work [27] demonstrates that diffusion models have a meaningful semantic latent space although they were originally designed for denoising [22] or score estimation [48]. Besides, a series of pioneering work [2, 7, 29, 53] shows the validity and even superiority of representations extracted from U-Net of SD to be qualified to discriminative tasks. Inspired by these studies, we consider utilizing semantic representations from the U-Net of SD to do discriminative tasks via a discriminative adapter.\\n\\n\u2022 Discriminative Adapter. We propose a lightweight discriminative adapter, which relies on the semantic representations of SD to handle discriminative tasks. Inspired by DETR [3], we implement the discriminative adapter with the Transformer [50] structure, including a Transformer encoder and a Transformer decoder. Besides, we adopt a fixed number of randomly initialized and learnable queries to adapt the framework to specific discriminative tasks. Concretely, given a noisy latent \\\\(z_t\\\\) at a sampled timestep \\\\(t\\\\) and a prompt \\\\(y\\\\), we first feed them into U-Net and extract a 2D feature map \\\\(F_t \\\\in \\\\mathbb{R}^{h \\\\times w \\\\times d}\\\\) from one of the intermediate blocks \\\\(l\\\\), where \\\\(h\\\\), \\\\(w\\\\), and \\\\(d\\\\) denote the height, width, and dimension, respectively. Formally, we extract \\\\(F_t\\\\) via\\n\\n\\\\[\\nF_t = \\\\text{UNet}_l(z_t, \\\\text{CLIP}(y), t) \\\\tag{1}\\n\\\\]\\n\\nwhere \\\\(\\\\text{UNet}_l\\\\) refers to the operation of extracting the feature maps in the \\\\(l\\\\)-th block of U-Net. Afterward, we combine \\\\(F_t\\\\) with learnable position embeddings [11] and timestep embeddings [43] of \\\\(t\\\\) via additive fusion, and then flatten it into the semantic representation \\\\(\\\\tilde{F} \\\\in \\\\mathbb{R}^{hw \\\\times d}\\\\). For simplicity, we will omit the subscript \\\\(t\\\\) in the following.\\n\\nTo probe the discriminative abilities, we feed \\\\(\\\\tilde{F}\\\\) into the Transformer encoder \\\\(\\\\text{Enc}(\\\\cdot)\\\\), and then perform interaction between the encoder output and some learnable queries \\\\(Q = \\\\{q_1, \\\\ldots, q_N\\\\}\\\\) with \\\\(q_i \\\\in \\\\mathbb{R}^d\\\\) in the Transformer decoder \\\\(\\\\text{Dec}(\\\\cdot, \\\\cdot)\\\\). The whole process is formulated as\\n\\n\\\\[\\nQ^* = f(\\\\tilde{F}; W_a, Q) = \\\\text{Dec}(\\\\text{Enc}(\\\\tilde{F}), Q) \\\\tag{2}\\n\\\\]\\n\\nwhere \\\\(f(\\\\cdot)\\\\) abstracts to the discriminative adapter with parameters \\\\(W_a\\\\) and \\\\(Q\\\\). \\\\(W_a\\\\) includes the parameters in \\\\(\\\\text{Enc}\\\\) and \\\\(\\\\text{Dec}\\\\). The queries \\\\(Q\\\\) serve as a bridge between visual representations and downstream discriminative tasks, which attends the encoded semantic representation \\\\(\\\\tilde{F}\\\\) via cross-attention [50] of the decoder for downstream tasks. Thanks to multiple queries in \\\\(Q\\\\), the query representations\\n\\n\\\\[\\\\text{We select the medium block by default, and also delve into the influence of different blocks in Sec. 4.3.}\\\\]\"}"}
{"id": "CVPR-2024-618", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 2. Schematic illustration of the proposed discriminative probing and tuning (DPT) framework. We first extract semantic representations from the frozen SD and then propose a discriminative adapter to conduct discriminative probing to investigate the global matching and local grounding abilities of SD. Afterward, we perform parameter-efficient discriminative tuning by introducing LoRA parameters.\\n\\nDuring inference, we present the self-correction mechanism to guide the denoising-based text-to-image generation.\\n\\n\\\\[ Q^* \\\\] capture multiple aspects of the semantic representation \\\\( \\\\tilde{F} \\\\). Thereafter, \\\\( Q^* \\\\) can be used to do various downstream tasks, possibly with a classifier or regressor.\\n\\nIn the following, we will introduce two probing tasks, i.e., ITM and REC, and train the discriminative adapter on them to investigate the global matching and local grounding abilities of T2I models, respectively.\\n\\n- **Global Matching**. From the view of discriminative modeling, a model with strong text-image alignment should be able to identify subtle alignment differences between various images and a text prompt. In light of this, we utilize the task of Image-Text Matching [17] to probe the discriminative global matching ability. This task is defined to achieve bidirectional matching or retrieval, including text-to-image (\\\\( T \\\\to I \\\\)) and image-to-text (\\\\( I \\\\to T \\\\)).\\n\\nTo achieve this, we first collect the first \\\\( M \\\\) (\\\\( M < N \\\\)) query representations \\\\( \\\\{q^*_1, ..., q^*_M\\\\} \\\\) from \\\\( Q^* \\\\), and then project each of them into a matching space with the same dimension as CLIP and obtain \\\\( h_i = g(q^*_i; W_m) \\\\). Intuitively, different query representations may capture different aspects to understand the same image. Inspired by this, we calculate the cross-modal semantic similarities between \\\\( x \\\\) and \\\\( y \\\\) by comparing the CLIP textual embedding of \\\\( y \\\\) and the most matched projected query representations via\\n\\n\\\\[ s(y, z_t) = \\\\max_{i \\\\in \\\\{1, ..., M\\\\}} \\\\cos(\\\\text{CLIP}(y), h_i) \\\\]\\n\\nBased on pairwise similarities, we optimize the discriminative adapter \\\\( f(\\\\cdot; W_a, Q) \\\\) and the projection layer \\\\( g(\\\\cdot; W_m) \\\\) using contrastive learning loss\\n\\n\\\\[ L_{\\\\text{match}} = L_{T \\\\to I} + L_{I \\\\to T} \\\\]\\n\\nThe first term optimizes the model to distinguish the correct image matched with a given text from all samples in a batch, i.e.,\\n\\n\\\\[ L_{T \\\\to I} = -\\\\log \\\\exp \\\\left( \\\\frac{s(z_j, y)}{\\\\tau} \\\\right) \\\\prod_{j=1}^{B} \\\\exp \\\\left( \\\\frac{s((z_j, y_j))}{\\\\tau} \\\\right) \\\\]\\n\\nwhere \\\\( B \\\\) denotes the min-batch size, and \\\\( \\\\tau \\\\) is a learnable temperature factor. Similarly, the opposite direction from image to text is computed by\\n\\n\\\\[ L_{I \\\\to T} = -\\\\log \\\\exp \\\\left( \\\\frac{s(z_j, y)}{\\\\tau} \\\\right) \\\\prod_{j=1}^{B} \\\\exp \\\\left( \\\\frac{s((z_j, y_j))}{\\\\tau} \\\\right) \\\\]\\n\\nWith \\\\( L_{\\\\text{match}} \\\\) as the optimization objective, the discriminative adapter and the projection layers are enforced to discover discriminative information from the semantic representations for matching, implying the global matching ability of a T2I model.\\n\\n- **Local Grounding**. Local grounding requires a model to recognize the referred object from others in an image given a partially descriptive text. We adapt SD to the REC [58] task to evaluate its discriminative local grounding ability.\\n\\nFormally, given a textual expression \\\\( y' \\\\) referring to a specific object with index \\\\( i \\\\) in an image \\\\( x \\\\), REC aims to predict the coordinate and the size, i.e., the bounding box \\\\( b_i \\\\), of the ground-truth object. To achieve it, we share the same discriminative adapter and employ the other \\\\((N-M)\\\\) learnable queries as object prior queries and obtain the corresponding query representations from the transformer decoder as \\\\( \\\\{q^*_j\\\\}_{j = M+1}^N \\\\). We then project each \\\\( q^*_j \\\\) into three spaces separately by three different project layers \\\\( g(\\\\cdot) \\\\):\\n\\n1) the grounding space to get the probability of predicting the correct object, i.e., \\\\( p_j = g(q^*_j; W_p) \\\\in \\\\mathbb{R} \\\\); 2)...\\n\\nWith \\\\( L_{\\\\text{match}} \\\\) as the optimization objective, the discriminative adapter and the projection layers are enforced to discover discriminative information from the semantic representations for matching, implying the global matching ability of a T2I model.\"}"}
{"id": "CVPR-2024-618", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"i.e.,\\n\\\\[ \\\\hat{b}_j = g(q^*_j; W_b) \\\\in \\\\mathbb{R}^4; \\\\]\\nand 3) the semantic space to bridge the semantic gap between queries and the text,\\n\\\\[ o_j = g(q^*_j; W_s) \\\\in \\\\mathbb{R}^d. \\\\]\\n\\nAfter projection, we perform maximum matching to discover the most matched query with index \\\\( \\\\psi(i) \\\\). The cost used for matching includes using grounding probability, \\\\( \\\\lambda_0 \\\\), and GIoU \\\\cite{42} losses between the prediction and the ground-truth box as costs. It is formulated as\\n\\\\[\\n\\\\psi(i) = \\\\arg \\\\min_{j \\\\in \\\\{M+1, \\\\ldots, N\\\\}} -p_j + \\\\text{L}_1(\\\\hat{b}_j, b_i) + \\\\text{GIoU}(\\\\hat{b}_j, b_i)\\n\\\\] (5)\\n\\nBesides, we adopt a text-to-object contrastive loss to further drive the model to distinguish the positive object from others at the semantic level:\\n\\\\[\\n\\\\mathcal{L}_T \\\\rightarrow O = -\\\\log \\\\exp(\\\\text{cos}(o_{\\\\psi(i)}, \\\\text{CLIP}(y'))) / \\\\tau\\n\\\\] \\\\[ P \\\\] \\\\[ \\\\sum \\\\exp(\\\\text{cos}(o_j, \\\\text{CLIP}(y')) / \\\\tau) \\\\] (6)\\n\\nWe combine all the losses and obtain the grounding loss as\\n\\\\[\\n\\\\mathcal{L}_{\\\\text{ground}} = -\\\\lambda_0 p_{\\\\psi(i)} + \\\\lambda_1 \\\\text{L}_1(\\\\hat{b}_{\\\\psi(i)}, b_i) + \\\\lambda_2 \\\\text{GIoU}(\\\\hat{b}_{\\\\psi(i)}, b_i) + \\\\lambda_3 \\\\mathcal{L}_T \\\\rightarrow O,\\n\\\\] (7)\\nwhere \\\\( \\\\{\\\\lambda_k\\\\}_{k \\\\in \\\\{0, 1, 2, 3\\\\}} \\\\) serve as trade-off factors.\\n\\nFinally, we optimize the parameters of the whole model, including \\\\( Q \\\\) and \\\\( \\\\{W_i\\\\}_{i \\\\in \\\\{a, p, b, s\\\\}} \\\\), with the following loss function on two tasks:\\n\\\\[\\n\\\\mathcal{L} = \\\\mathbb{E}_{x, \\\\epsilon \\\\sim \\\\mathcal{N}(0, 1)} (\\\\mathcal{L}_\\\\text{match} + \\\\mathcal{L}_\\\\text{ground})\\n\\\\] (8)\\n\\nThe probing process includes training and inference on the two discriminative tasks. During training, we freeze all parameters of SD, and adopt its semantic representations for matching and grounding by optimizing the discriminative adapter and several projection layers. During inference, we obtain the testing performance on the two discriminative tasks, which reflects the discriminative abilities of SD.\\n\\n### 3.2. Stage 2 \u2013 Discriminative Tuning\\n\\nIn the second stage, we propose to improve the generative abilities, especially text-image alignment, by optimizing T2I models in a discriminative tuning manner. Most prior work \\\\cite{2, 53} only views SD as a fixed feature extractor for segmentation tasks due to its fine-grained semantic representation power but overlooks the potential back-feeding of discrimination to generation. Besides, though a recent study \\\\cite{26, 52} fine-tunes the SD model using discriminative objectives, it only pays attention to specific downstream tasks (e.g., ITM) and ignores the effect of tuning on generation. The advancement of discrimination may sacrifice the original generative power. In this stage, we mainly focus on enhancing generation, but also investigate the superior limit of discrimination under the premise of priority generation. It may shed new light on giving full play to the versatility of visual generative foundation models.\\n\\nIn this vein, we strive to explain\\n\\n\\\"How can we enhance text-image alignment for T2I models by discriminative tuning?\\\"\\n\\nIn the previous stage, we freeze SD and probe how informative intermediate activations are in global matching and local grounding. Here, we conduct parameter-efficient fine-tuning using LoRA \\\\cite{23} by injecting trainable layers over cross-attention layers and freezing the parameters of the pre-trained SD. We use the same discriminative objective functions as stage 1 to tune the LoRA, discriminative adapter, and task-specific projection layers. Due to the participation of LoRA, we can flexibly manipulate the intermediate activation of T2I models.\\n\\n### 3.3. Self-Correction\\n\\nEquipping the T2I model with the discriminative adapter enables the whole model to execute discriminative tasks. As a bonus of using the discriminative adapter, we propose a self-correction mechanism to guide high-alignment generation during inference. Formally, we update the latent \\\\( z_t \\\\) aiming to enhance the semantic similarity between \\\\( z_t \\\\) and the prompt \\\\( y \\\\) through gradients:\\n\\\\[\\n\\\\hat{z}_t = z_t + \\\\eta \\\\frac{\\\\partial s(z_t, y)}{\\\\partial z_t}\\n\\\\] (9)\\n\\nwhere the guidance factor \\\\( \\\\eta \\\\) control the guidance strength. \\\\( \\\\frac{\\\\partial s(z_t, y)}{\\\\partial z_t} \\\\) represents the gradients from the discriminative adapter to the latent \\\\( z_t \\\\). Afterward, we predict the noise by feeding \\\\( \\\\hat{z}_t \\\\) into U-Net and then obtain \\\\( z_t - 1 \\\\) for generation.\\n\\n### 4. Experiments\\n\\nWe conduct extensive experiments to evaluate the generative and discriminative performance of DPT, justify its effectiveness, and conduct an in-depth analysis.\\n\\n#### 4.1. Experimental Settings\\n\\n- **Benchmarks.** During training, we adopt the training set of MSCOCO \\\\cite{32} for ITM and three commonly used datasets, i.e., RefCOCO, RefCOCO+, and RefCOCOg for REC. To evaluate the text-image alignment, we utilize five benchmarks: COCO-NSS1K \\\\cite{37}, CC-500 \\\\cite{15}, ABC-6K \\\\cite{15}, TIFA \\\\cite{24}, and T2I-CompBench \\\\cite{13}. According to the distribution differences of textual prompts between the training set and the test sets, we adopt three settings, i.e., In-Distribution (ID) and Out-of-Distribution (OOD) \\\\cite{49} on COCO-NSS1K and CC-500, respectively, and Mixed Distribution (MD) on ABC-6K, TIFA, and T2I-CompBench. More details can be found in Appendix ??.\\n\\n- **Evaluation Metrics.** Following the existing baselines \\\\cite{4, 15, 37}, we adopt CLIP score \\\\cite{20} and BLIP score \\\\cite{30} OpenCLIP (ViT-H-14) \\\\cite{6} and BLIP-2 (pretrain) are used to compute text-image similarities as CLIP and BLIP scores, respectively. We will\"}"}
{"id": "CVPR-2024-618", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1. Performance comparison for text-to-image generation on COCO-NSS1K, CC-500, and ABC-6K.\\n\\n| Method                  | COCO-NSS1K (ID) | CC-500 (OOD) | ABC-6K (MD) |\\n|-------------------------|-----------------|--------------|-------------|\\n| CLIP                    | 33.27           | 67.96        | 39.48       |\\n| BLIP-M                  | 31.32           | 54.77        |             |\\n| BLIP-C                  | 34.82           | 70.95        | 40.36       |\\n| IS                      | 14.28           |              |             |\\n| Stable Diffusion-v1.4   |                 |              |             |\\n| [CVPR22]                | 35.33           | 72.03        | 40.82       |\\n| StructureDiffusion      |                 |              |             |\\n| [ICLR23]                | 34.47           |              |             |\\n| HN-DiffusionITM         |                 |              |             |\\n| [NeurIPS23]             | 34.96           | 73.32        | 40.22       |\\n| DPT (Ours)              |                 |              |             |\\n|                         | 37.07           | 76.74        | 41.15       |\\n\\n### Table 2. Performance comparison for text-to-image generation on TIFA [24] and T2I-CompBench [13].\\n\\n| Method                  | TIFA            | T2I-CompBench |\\n|-------------------------|-----------------|--------------|\\n| Color                   | 79.15           | 36.82        |\\n| Shape                   | 35.94           | 42.16        |\\n| Text                    | 42.16           | 10.64        |\\n| Sp. Comp.               | 30.45           | 28.18        |\\n| Non-Sp. Comp.           | 28.05           |              |\\n\\n| SD-v1.4                 |                 |              |\\n| HN-DiffusionITM         |                 |              |\\n| [26]                    | 79.02           | 36.71        |\\n| VPGen                   |                 |              |\\n| [12]                    | 77.33           | 32.12        |\\n| LayoutGPT               |                 |              |\\n| [16]                    | 79.31           | 33.86        |\\n| DPT (Ours)              |                 |              |\\n|                         | 82.04           | 48.84        |\\n| DPT + SC (Ours)         |                 |              |\\n|                         | 82.40           | 51.51        |\\n\\n| SD-v2.1                 |                 |              |\\n| Attend-and-Excite       |                 |              |\\n| [4]                    | 81.98           | 53.72        |\\n| HN-DiffusionITM         |                 |              |\\n| [26]                    | 82.02           | 46.45        |\\n| DPT (Ours)              |                 |              |\\n|                         | 84.49           | 60.59        |\\n| DPT + SC (Ours)         |                 |              |\\n|                         | 84.63           | 62.59        |\\n\\nincluding BLIP-ITM and BLIP-ITC, and GLIP score [15] based on object detection to evaluate text-image alignment, and IS [46] and FID [21] as quality evaluation metrics.\\n\\nAs for TIFA and T2I-CompBench, we follow the recommended VQA accuracy or specifically curated protocols.\\n\\n### 4.2. Performance Comparison\\n\\n#### Text-to-Image Generation.\\n\\nAs shown in Tab. 1 and Tab. 2, we have the following observations and discussions:\\n\\n1. Compared with the base foundation models, i.e., SD [43], the proposed DPT manages to improve the text-image alignment remarkably, which illustrates that enhancing discriminative abilities could benefit the generative semantic alignment for T2I models.\\n2. DPT achieves superior performance on CC-500 and ABC-6K under the OOD setting, showing its powerful generalization to other prompt distributions. It also reveals its capability to resist the risk of overfitting when tuning T2I models with discriminative tasks.\\n3. The consistent improvement on both SD-v1.4 and SD-v2.1 demonstrates that the proposed DPT may be parallel with the generative pre-training based on score matching, reflecting the possibility of activating the intrinsic reasoning abilities of T2I models using DPT.\\n4. In all, the proposed method achieves the best generation performance consistently on text-image alignment across comprehensive benchmarks, distribution settings, and evaluation protocols. Besides, the improvement in alignment does not result in a loss of image quality per IS and FID. These results confirm the effectiveness of the proposed paradigm DPT.\\n\\n### 4.3. In-depth Analysis\\n\\nTo verify the effectiveness of each component in DPT, including discriminative tuning on Global Matching (GM) and Local Grounding (LG) in the 2nd stage, and the Self-correction (SC) module, we conduct in-depth analysis.\\n\\nFrom this table, we observe that our method could outperform the existing state-of-the-art generative methods, such as Diffusion Classifier [29] and DiffusionITM [26], by large margins on ITM and REC tasks. Even it could achieve competitive performance in the first probing stage or when selected with a priority generation in the second stage. These results show that the generative representations extracted from the intermediate layers of U-Net convey meaningful semantics, verifying that T2I models have basic discriminative matching and grounding abilities. Besides, it also indicates that such abilities could be further improved by the discriminative tuning introduced in Sec. 3.2.\"}"}
{"id": "CVPR-2024-618", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Performance comparison for image-text matching and referring expression comprehension to evaluate global matching and local grounding abilities, respectively. Datasets include MSCOCO-HN for ITM, and RefCOCO, RefCOCO+, and RefCOCOg for REC. All the methods are grouped into three parts, in which the upper, middle, and lower groups correspond to zero-shot discriminative, zero-shot generative, and fine-tuning generative methods, respectively. All the generative models are based on Stable Diffusion-v2.1.\\n\\n| Method                  | MSCOCO-HN | RefCOCO | RefCOCO+ | RefCOCOg |\\n|-------------------------|-----------|---------|----------|----------|\\n|                         | I-to-T    | T-to-I  | Overall  | val      |\\n| Random Chance           | 25.00     | 25.00   | 25.00    | 16.53    |\\n| CLIP (ViT-B-32)         | [ICML21][38] | 47.63   | 42.82    | 45.23    |\\n| OpenCLIP (ViT-B-32)     | [CVPR23][6] | 49.07   | 47.45    | 48.26    |\\n| Diffusion Classifier \u00a7  | [ICCV23][29] | 34.59   | 24.12    | 29.36    |\\n| DiffusionITM \u00a7          | [NeurIPS23][26] | 34.59   | 29.83    | 32.21    |\\n| Local Dinoising - - -   | -         | -       | -        | 23.83    |\\n| Diffusion Classifier \u2020\u00a7 | [ICCV23][29] | 37.72   | 24.03    | 30.88    |\\n| DiffusionITM \u2020\u00a7         | [NeurIPS23][26] | 37.72   | 29.88    | 33.80    |\\n| HN-DiffusionITM \u2020\u00a7      | [NeurIPS23][26] | 37.55   | 30.37    | 33.96    |\\n| Local Dinoising \u2020 - - - | -         | -       | -        | 23.70    |\\n| DPT (Stage1, Ours)      | 42.29     | 34.75   | 38.52    | 48.79    |\\n| DPT (Ours)              | 42.07     | 34.97   | 38.52    | 52.73    |\\n| DPT* (Ours)             | 43.12     | 35.25   | 39.18    | 63.45    |\\n\\n\u2020: fine-tuning with the denoising objective; \u00a7: cropping an image into blocks and then matching them with the referring text for REC; \u2217: model selection with a priority discriminative task, i.e., ITM or REC during inference.\\n\\nCorrection (SC) during inference, we conduct several analytic experiments on COCO-NSS1K and CC-500 under ID and OOD settings. The results are summarized in Tab. 4.\\n\\nEffectiveness of Discriminative Tuning. From the compared results in Tab. 4 between different variants, we observe that the two tuning objectives, i.e., GM and LG, could consistently promote the alignment performance for T2I according to CLIP and BLIP scores. It verifies the validity of discriminative tuning on ITM and REC tasks. Compared with GM, LG achieves more remarkable improvement over semantic and object detection metrics. It may be attributed to the enhanced grounding ability brought by the prediction of local concepts based on partial descriptions. Furthermore, combining the two objectives to conduct multi-task learning may contribute to a slight improvement in BLIP scores under the OOD setting, but other metrics are slightly compromised. This phenomenon indicates that some contradictions may exist during model optimization, reflecting that unifying multiple tasks is still challenging.\\n\\nEffectiveness of Self-Correction. In Sec. 3.3, we propose to recycle the discriminative adapter in the inference phase by guiding iterative denoising. Comparing the 3rd and 4th variants in Table 4, we can see that the self-correction scheme could consistently improve the alignment for T2I, attesting to its effectiveness.\\n\\nImpact of Probed U-Net Block. Due to the hierarchical structure of the U-Net in SD, we could extract multi-level feature maps from its different blocks. Prior work [52] has shown that different blocks may have different discriminative powers in image classification. To further investigate the matching and grounding abilities empowered by various blocks and the trade-off between discrimination and generation, we probe consecutive seven blocks of the U-Net shown in Fig. 2 from left to right and then tune the whole model based on the probed block. The generative and discriminative results are shown in Fig. 3. It can be observed that the T2I performance gets continuously improved with the probed block shifting from bottom to up. The reason may be...\"}"}
{"id": "CVPR-2024-618", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"that more LoRA parameters would be introduced and more layers would be tuned during back-propagation. In contrast, the discriminative performance regardless of matching and grounding starts to increase and then deteriorates. It may be attributed to two points: 1) the feature maps from those blocks (e.g., up2 and up3) close to final outputs, i.e., predicted noises, are less semantic; 2) the feature sequences flattened from these feature maps may be too long, making it difficult for the discriminative adapter to probe.\\n\\nFigure 4. Impact of (a) the variation of generation and discrimination performance with the progress of tuning and (b) the self-correction strength on the performance of T2I on CC-500.\\n\\n- Impact of Tuning Step. To further delve into the durative impact of discriminative tuning on two aspects of performance, we show the dynamics of the performance with the increment of the tuning step in the 2nd stage in Fig. 4a. We can see that the generative performance gets better with tuning and seems to reach the saturation point at the 8k step. In contrast, there is still potential for grounding performance to get higher while the matching performance seems to remain stable in the tuning stage.\\n\\n- Impact of Self-Correction Factor. As shown in Fig. 4b, we study the influence of the guidance factor $\\\\eta$ in Eqn. (9) on the alignment performance of T2I. The results demonstrate that the proposed self-correction mechanism could alleviate the text-image misalignment issue with a proper range of guidance factor, i.e., $[0.05, 1]$.\\n\\n4.4. Qualitative Results\\n\\nTo intuitively show the alignment improvement achieved by DPT and SC, we illustrate generated examples with prompts from COCO-NSS1K for object appearance, counting, relation, and compositional reasoning evaluation, as shown in Fig. 5. These cases demonstrate the effectiveness of incorporating discriminative probing and tuning into T2I models.\\n\\n5. Conclusion and Future Work\\n\\nIn this work, we tackled the text-image misalignment issue for text-to-image generative models. Toward this end, we retrospected the relations between generative and discriminative modeling and presented a two-stage method named DPT. It introduces a discriminative adapter for probing basic discriminative abilities in the first stage and performs discriminative fine-tuning in the second stage. DPT exhibited effectiveness and generalization across five T2I datasets and four ITM and REC datasets.\\n\\nIn the future, we plan to explore the effect of discriminative probing and tuning to more generative models using more conception and understanding tasks. Besides, it is interesting to discuss more complicated relations between discriminative and generative modeling such as trade-offs and mutual promotion across different tasks.\"}"}
{"id": "CVPR-2024-618", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Eslam Mohamed Bakr, Pengzhan Sun, Xiaogian Shen, Faizan Farooq Khan, Li Erran Li, and Mohamed Elhoseiny. Hrs-bench: Holistic, reliable and scalable benchmark for text-to-image models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 20041\u201320053, 2023.\\n\\n[2] Ryan Burgert, Kanchana Ranasinghe, Xiang Li, and Michael S Ryoo. Peekaboo: Text to image diffusion models are zero-shot segmentors. arXiv preprint arXiv:2211.13224, 2022.\\n\\n[3] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European conference on computer vision, pages 213\u2013229. Springer, 2020.\\n\\n[4] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics (TOG), 42(4):1\u201310, 2023.\\n\\n[5] Huanran Chen, Yinpeng Dong, Zhengyi Wang, Xiao Yang, Chengqi Duan, Hang Su, and Jun Zhu. Robust classification via a single diffusion model. arXiv preprint arXiv:2305.15241, 2023.\\n\\n[6] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuhmann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scaling laws for contrastive language-image learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2818\u20132829, 2023.\\n\\n[7] Kevin Clark and Priyank Jaini. Text-to-image diffusion models are zero-shot classifiers. arXiv preprint arXiv:2303.15233, 2023.\\n\\n[8] Colin Conwell and Tomer Ullman. Testing relational understanding in text-guided image generation. arXiv preprint arXiv:2208.00005, 2022.\\n\\n[9] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. Advances in Neural Information Processing Systems, 34:19822\u201319835, 2021.\\n\\n[10] Runpei Dong, Chunrui Han, Yuang Peng, Zekun Qi, Zheng Ge, Jinrong Yang, Liang Zhao, Jianjian Sun, Hongyu Zhou, Haoran Wei, et al. Dreamllm: Synergistic multimodal comprehension and creation. arXiv preprint arXiv:2309.11499, 2023.\\n\\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.\\n\\n[12] Cho et al. Visual programming for text-to-image generation and evaluation. In NeurIPS, 2023.\\n\\n[13] Huang et al. T2i-compbench: A comprehensive benchmark for open-world compositional text-to-image generation. In NeurIPS, 2023.\\n\\n[14] Wan-Cyuan Fan, Yen-Chun Chen, DongDong Chen, Yu Cheng, Lu Yuan, and Yu-Chiang Frank Wang. Frido: Feature pyramid diffusion for complex scene image synthesis. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 579\u2013587, 2023.\\n\\n[15] Weixi Feng, Xuehai He, Tsu-Jui Fu, Varun Jampani, Arjun Akula, Pradyumna Narayana, Sugato Basu, Xin Eric Wang, and William Yang Wang. Training-free structured diffusion guidance for compositional text-to-image synthesis. arXiv preprint arXiv:2212.05032, 2022.\\n\\n[16] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. arXiv preprint arXiv:2305.15393, 2023.\\n\\n[17] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio, Jeff Dean, Marc'Aurelio Ranzato, and Tomas Mikolov. Devise: A deep visual-semantic embedding model. Advances in neural information processing systems, 26, 2013.\\n\\n[18] Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, and Yezhou Yang. Benchmarking spatial relationships in text-to-image generation. arXiv preprint arXiv:2212.10015, 2022.\\n\\n[19] Kamal Gupta, Justin Lazarow, Alessandro Achille, Larry S Davis, Vijay Mahadevan, and Abhinav Shrivastava. Layout-transformer: Layout generation and completion with self-attention. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1004\u20131014, 2021.\\n\\n[20] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: A reference-free evaluation metric for image captioning. arXiv preprint arXiv:2104.08718, 2021.\\n\\n[21] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30, 2017.\\n\\n[22] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:6840\u20136851, 2020.\\n\\n[23] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.\\n\\n[24] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah A Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In ICCV, 2023.\\n\\n[25] Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013.\\n\\n[26] Benno Krojer, Elinor Poole-Dayan, Vikram Voleti, Christopher Pal, and Siva Reddy. Are diffusion models vision-and-language reasoners? In Thirty-seventh Conference on Neural Information Processing Systems, 2023.\\n\\n[27] Mingi Kwon, Jaeseok Jeong, and Youngjung Uh. Diffusion models already have a semantic latent space. arXiv preprint arXiv:2210.10960, 2022.\"}"}
{"id": "CVPR-2024-618", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Kimin Lee, Hao Liu, Moonkyung Ryu, Olivia Watkins, Yuqing Du, Craig Boutilier, Pieter Abbeel, Mohammad Ghavamzadeh, and Shixiang Shane Gu. Aligning text-to-image models using human feedback. arXiv preprint arXiv:2302.12192, 2023.\\n\\nAlexander C Li, Mihir Prabhudesai, Shivam Duggal, Ellis Brown, and Deepak Pathak. Your diffusion model is secretly a zero-shot classifier. arXiv preprint arXiv:2303.16203, 2023.\\n\\nJunnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023.\\n\\nYuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22511\u201322521, 2023.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision\u2013ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V, pages 740\u2013755. Springer, 2014.\\n\\nXinyu Lin, Wenjie Wang, Yongqi Li, Fuli Feng, See-Kiong Ng, and Tat-Seng Chua. A multi-facet paradigm to bridge large language model and recommendation. arXiv preprint arXiv:2310.06491, 2023.\\n\\nNan Liu, Shuang Li, Yilun Du, Antonio Torralba, and Joshua B Tenenbaum. Compositional visual generation with composable diffusion models. In European Conference on Computer Vision, pages 423\u2013439. Springer, 2022.\\n\\nAlex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021.\\n\\nLeigang Qu, Meng Liu, Jianlong Wu, Zan Gao, and Liqiang Nie. Dynamic modality interaction modeling for image-text retrieval. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval, pages 1104\u20131113, 2021.\\n\\nLeigang Qu, Shengqiong Wu, Hao Fei, Liqiang Nie, and Tat-Seng Chua. Layoutllm-t2i: Eliciting layout guidance from llm for text-to-image generation. In Proceedings of the 31st ACM International Conference on Multimedia, pages 643\u2013654, 2023.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 8748\u20138763. PMLR, 2021.\\n\\nAditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pages 8821\u20138831. PMLR, 2021.\\n\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022.\\n\\nAli Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019.\\n\\nHamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union: A metric and a loss for bounding box regression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 658\u2013666, 2019.\\n\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f8rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention\u2013MICCAI 2015: 18th International Conference, Munich, Germany, October 5-9, 2015, Proceedings, Part III, pages 234\u2013241. Springer, 2015.\\n\\nChitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily L Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:36479\u201336494, 2022.\\n\\nTim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016.\\n\\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 2256\u20132265. PMLR, 2015.\\n\\nYang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020.\\n\\nTeng Sun, Wenjie Wang, Liqaing Jing, Yiran Cui, Xuemeng Song, and Liqiang Nie. Counterfactual reasoning for out-of-distribution multimodal sentiment analysis. In Proceedings of the 30th ACM International Conference on Multimedia, pages 15\u201323, 2022.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017.\\n\\nPeter West, Ximing Lu, Nouha Dziri, Faeze Brahman, Linjie Li, Jena D Hwang, Liwei Jiang, Jillian Fisher, Abhilasha Ravichander, Khyathi Chandu, et al. The generative ai paradox: \u201cwhat it can create, it may not understand.\u201d arXiv preprint arXiv:2311.00059, 2023.\"}"}
{"id": "CVPR-2024-618", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Weilai Xiang, Hongyu Yang, Di Huang, and Yunhong Wang. Denoising diffusion autoencoders are unified self-supervised learners. arXiv preprint arXiv:2303.09769, 2023.\\n\\nJiarui Xu, Sifei Liu, Arash Vahdat, Wonmin Byeon, Xiaolong Wang, and Shalini De Mello. Open-vocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2955\u20132966, 2023.\\n\\nTao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1316\u20131324, 2018.\\n\\nXinchen Yan, Jimei Yang, Kihyuk Sohn, and Honglak Lee. Attribute2image: Conditional image generation from visual attributes. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11\u201314, 2016, Proceedings, Part IV 14, pages 776\u2013791. Springer, 2016.\\n\\nXingyi Yang and Xinchao Wang. Diffusion model as representation learner. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 18938\u201318949, 2023.\\n\\nJiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022.\\n\\nLicheng Yu, Patrick Poirson, Shan Yang, Alexander C Berg, and Tamara L Berg. Modeling context in referring expressions. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 69\u201385. Springer, 2016.\\n\\nHan Zhang, Tao Xu, Hongsheng Li, Shaoting Zhang, Xiaogang Wang, Xiaolei Huang, and Dimitris N Metaxas. Stackgan: Text to photo-realistic image synthesis with stacked generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pages 5907\u20135915, 2017.\\n\\nWenliang Zhao, Yongming Rao, Zuyan Liu, Benlin Liu, Jie Zhou, and Jiwen Lu. Unleashing text-to-image diffusion models for visual perception. arXiv preprint arXiv:2303.02153, 2023.\"}"}
