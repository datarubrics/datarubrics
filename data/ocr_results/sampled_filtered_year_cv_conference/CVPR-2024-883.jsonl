{"id": "CVPR-2024-883", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Residual gated graph convnets. arXiv preprint arXiv:1711.07553, 2017.\\n\\nMichail Chatzianastasis, George Dasoulas, Georgios Siolas, and Michalis Vazirgiannis. Graph-based neural architecture search with operation embeddings. In ICCV, 2021.\\n\\nYaofo Chen, Yong Guo, Qi Chen, Minli Li, Wei Zeng, Yaowei Wang, and Mingkui Tan. Contrastive neural architecture search with neural architecture comparators. In CVPR, 2021.\\n\\nZiye Chen, Yibing Zhan, Baosheng Yu, Mingming Gong, and Bo Du. Not all operations contribute equally: Hierarchical operation-adaptive predictor for neural architecture search. In ICCV, 2021.\\n\\nZhe Chen, Hao Tan, Tao Wang, Tianrun Shen, Tong Lu, Qiuying Peng, Cheng Cheng, and Yue Qi. Graph propagation transformer for graph representation learning. In IJCAI, 2023.\\n\\nHsin-Pai Cheng, Tunhou Zhang, Yixing Zhang, Shiyu Li, Feng Liang, Feng Yan, Meng Li, Vikas Chandra, Hai Li, and Yiran Chen. Nasgem: Neural architecture search via graph embedding method. In AAAI, 2021.\\n\\nXuanyi Dong and Yi Yang. Nas-bench-201: Extending the scope of reproducible neural architecture search. In ICLR, 2020.\\n\\nVijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. arXiv preprint arXiv:2012.09699, 2020.\\n\\nKaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\nGao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kilian Q Weinberger. Densely connected convolutional networks. In CVPR, 2017.\\n\\nMd Shamim Hussain, Mohammed J Zaki, and Dharmsankar Subramanian. Global self-attention as a replacement for graph convolution. In KDD, 2022.\\n\\nYinghui Jiang, Shuting Jin, Xurui Jin, Xianglu Xiao, Wenfan Wu, Xiangrong Liu, Qiang Zhang, Xiangxiang Zeng, Guang Yang, and Zhangming Niu. Pharmacophoric-constrained heterogeneous graph transformer model for molecular property prediction. Communications Chemistry, 6(1):60, 2023.\\n\\nThomas N Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016.\\n\\nDevin Kreuzer, Dominique Beaini, Will Hamilton, Vincent L\u00b4etourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. In NeurIPS, 2021.\\n\\nChenxi Liu, Barret Zoph, Maxim Neumann, Jonathon Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille, Jonathan Huang, and Kevin Murphy. Progressive neural architecture search. In ECCV, 2018.\\n\\nShun Lu, Jixiang Li, Jianchao Tan, Sen Yang, and Ji Liu. Tnasp: A transformer-based nas predictor with a self-evolution framework. In NeurIPS, 2021.\\n\\nRenqian Luo, Fei Tian, Tao Qin, Enhong Chen, and Tie-Yan Liu. Neural architecture optimization. NeurIPS, 2018.\\n\\nRenqian Luo, Xu Tan, Rui Wang, Tao Qin, Enhong Chen, and Tie-Yan Liu. Semi-supervised neural architecture search. In NeurIPS, 2020.\\n\\nYuankai Luo, Veronika Thost, and Lei Shi. Transformers over directed acyclic graphs. In NeurIPS, 2023.\\n\\nLiheng Ma, Chen Lin, Derek Lim, Adriana Romero-Soriano, Puneet K Dokania, Mark Coates, Philip Torr, and Ser-Nam Lim. Graph inductive biases in transformers without message passing. In ICML, 2023.\\n\\nAbhinav Mehrotra, Alberto Gil CP Ramos, Sourav Bhatattacharya, \u0141ukasz Dudziak, Ravichander Vipperla, Thomas Chau, Mohamed S Abdelfattah, Samin Ishtiaq, and Nicholas Donald Lane. Nas-bench-asr: Reproducible neural architecture search for speech recognition. In ICLR, 2020.\\n\\nGr \u00b4egoire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal. Graphit: Encoding graph structure in transformers. arXiv preprint arXiv:2106.05667, 2021.\\n\\nHoang D Nguyen, Xuan-Son Vu, and Duc-Trong Le. Modular graph transformer networks for multi-label image classification. In AAAI, 2021.\\n\\nXuefei Ning, Yin Zheng, Tianchen Zhao, Yu Wang, and Huazhong Yang. A generic graph-based neural architecture encoding scheme for predictor-based nas. In ECCV, 2020.\\n\\nXuefei Ning, Zixuan Zhou, Junbo Zhao, Tianchen Zhao, Yiping Deng, Changcheng Tang, Shuang Liang, Huazhong Yang, and Yu Wang. Ta-gates: An encoding scheme for neural network architectures. In NeurIPS, 2022.\\n\\nPeisong Niu, Tian Zhou, Qingsong Wen, Liang Sun, and Tao Yao. Chemistry guided molecular graph transformer. In NeurIPS 2022 Workshop: AI for Science: Progress and Promises, 2022.\\n\\nKenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node classification. arXiv preprint arXiv:1905.10947, 2019.\\n\\nYunsheng Pang, Qiuhong Ke, Hossein Rahmani, James Bailey, and Jun Liu. Igformer: Interaction graph transformer for skeleton-based human interaction recognition. In ECCV, 2022.\\n\\nYijian Qin, Ziwei Zhang, Xin Wang, Zeyang Zhang, and Wenwu Zhu. Nas-bench-graph: Benchmarking graph neural architecture search. In NeurIPS, 2022.\\n\\nLadislav Ramp \u00b4a\u02c7sek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for a general, powerful, scalable graph transformer. In NeurIPS, 2022.\\n\\nYu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. In NeurIPS, 2020.\\n\\nPranab Kumar Sen. Estimates of the regression coefficient based on kendall\u2019s tau. Journal of the American statistical association, 63(324):1379\u20131389, 1968.\"}"}
{"id": "CVPR-2024-883", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Han Shi, Renjie Pi, Hang Xu, Zhenguo Li, James Kwok, and Tong Zhang. Bridging the gap between sample-based and one-shot neural architecture search with bonas. NeurIPS, 2020.\\n\\nYu Shi, Shuxin Zheng, Guolin Ke, Yifei Shen, Jiacheng You, Jiyan He, Shengjie Luo, Chang Liu, Di He, and Tie-Yan Liu. Benchmarking graphormer on large-scale molecular modeling datasets. arXiv preprint arXiv:2203.04810, 2022.\\n\\nVeronika Thost and Jie Chen. Directed acyclic graph neural networks. In ICLR, 2021.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\\n\\nLinnan Wang, Yiyang Zhao, Yuu Jinnai, Yuandong Tian, and Rodrigo Fonseca. Alphax: exploring neural architectures with deep neural networks and monte carlo tree search. arXiv preprint arXiv:1903.11059, 2019.\\n\\nWei Wen, Hanxiao Liu, Yiran Chen, Hai Li, Gabriel Bender, and Pieter-Jan Kindermans. Neural predictor for neural architecture search. In ECCV, 2020.\\n\\nColin White, Willie Neiswanger, and Yash Savani. Bananas: Bayesian optimization with neural architectures for neural architecture search. In AAAI, 2021.\\n\\nColin White, Arber Zela, Robin Ru, Yang Liu, and Frank Hutter. How powerful are performance predictors in neural architecture search? In NeurIPS, 2021.\\n\\nShen Yan, Kaiqiang Song, Fei Liu, and Mi Zhang. Cate: Computation-aware neural architecture encoding with transformers. In ICML, 2021.\\n\\nYun Yi, Haokui Zhang, Wenze Hu, Nannan Wang, and Xiaoyu Wang. Nar-former: Neural architecture representation learning towards holistic attributes prediction. In CVPR, 2023.\\n\\nChris Ying, Aaron Klein, Eric Christiansen, Esteban Real, Kevin Murphy, and Frank Hutter. Nas-bench-101: Towards reproducible neural architecture search. In ICML, 2019.\\n\\nChengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? In NeurIPS, 2021.\\n\\nArber Zela, Julien Siems, and Frank Hutter. Nas-bench-1shot1: Benchmarking and dissecting one-shot neural architecture search. In ICLR, 2019.\\n\\nArber Zela, Julien Siems, Lucas Zimmer, Jovita Lukasik, Margret Keuper, and Frank Hutter. Surrogate nas benchmarks: Going beyond the limited search spaces of tabular nas benchmarks. In ICLR, 2022.\\n\\nYi Zheng, Rushin H Gindra, Emily J Green, Eric J Burks, Margrit Betke, Jennifer E Beane, and Vijaya B Kolachalam. A graph-transformer for whole slide image classification. IEEE transactions on medical imaging, 41(11):3003\u20133015, 2022.\"}"}
{"id": "CVPR-2024-883", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Backward Message Passing\\nForward Message Passing\\n= output node embedding\\n\\nFigure 5. Flow encode module. During forward message passing, node embeddings are updated following the order of topological generations. Conversely, during backward message passing, node embeddings are updated in the reverse order of the generations.\\n\\nDuring the backward message passing step, node embeddings are updated asynchronously, following the reverse order of the topological generations. For each node $v_j$, the messages from its outgoing neighbors (rather than incoming neighbors) are computed and then aggregated (Line 10).\\n\\nThe other details remain consistent with those of the forward message passing in to Eq. (1), Eq. (2), and Eq. (3).\\n\\nOutputs:\\nWe denote the output node-embedding matrix of the flow encode module in the $\\\\ell$-th $\\\\text{LOWER}$ layer, as $H(\\\\ell)$\\\\_flow $\\\\in \\\\mathbb{R}^{N \\\\times d}$. That is,\\n\\n$$H(\\\\ell)_{\\\\text{flow}} = \\\\text{FlowEncoder}(G, H(\\\\ell-1))$$\\n\\nHere, $H(\\\\ell-1) \\\\in \\\\mathbb{R}^{N \\\\times d}$ is the input node-embedding matrix obtained in layer-$(\\\\ell-1)$, the previous layer (Eq. (6)).\\n\\n3.3.2 Flow-aware global attention module\\nThe flow-aware global attention module is designed to capture graph-level (i.e., architecture-level) characteristics, complementing the flow encode module which primarily focuses on local-level flows between directly connected operations. To this end, we employ a global attention mechanism of GTs; moreover, to accurately reflect the flows within architectures, we restrict attention scores to be computed only between nodes connected by at least one path of the flows. Specifically, we employ a masking strategy [21, 43] with a mask matrix $M \\\\in \\\\mathbb{R}^{N \\\\times N}$ defined as follows (refer to Fig. 6 for an example of $M$):\\n\\n$$M_{ij} = \\\\begin{cases} \\n1 & \\\\text{if } v_i \\\\text{ lies on any directed path from } v_j \\\\text{ or } v_j \\\\text{ lies on any directed path from } v_i, \\\\\\\\\\n0 & \\\\text{otherwise.}\\n\\\\end{cases}$$\\n\\nFigure 6. An example mask matrix $M$. Node 6 attends exclusively to nodes that appear in any path involving the node ($P_1$ and $P_2$). Nodes 1, 3, and 7 appear in $P_1$, and nodes 1, 4, and 7 appear in $P_2$; thus node 6 attends only to 1, 3, 4, and 7, as indicated by $M$.\\n\\nSpecifically, given the input node-embedding matrix $H(\\\\ell-1) \\\\in \\\\mathbb{R}^{N \\\\times d}$ and the mask matrix $M$, the flow-aware global attention module computes its output node-embedding matrix $H(\\\\ell)$\\\\_global $\\\\in \\\\mathbb{R}^{N \\\\times d}$ as follows:\\n\\n$$H(\\\\ell)_{\\\\text{global}} = \\\\text{MMHA}(H(\\\\ell-1), H(\\\\ell-1), H(\\\\ell-1), M)$$\\n\\nHere, $\\\\text{MMHA}$ is the Masked Multi-Head Attention module:\\n\\n$$\\\\text{MMHA}(Q, K, V, M) = \\\\text{Concat}(\\\\text{head}_1, \\\\ldots, \\\\text{head}_s)W_0,$$\\n\\nwhere $W_0 \\\\in \\\\mathbb{R}^{sd_v \\\\times d}$ is the learnable projection matrix, $s$ is the number of heads, and $\\\\text{head}_i = \\\\text{Attn}(QW_Q^i, KW_K^i, VW_V^i, M)$.\\n\\nHere, $\\\\odot$ is element-wise multiplication; and $W_Q^i \\\\in \\\\mathbb{R}^{d \\\\times d_k}$, $W_K^i \\\\in \\\\mathbb{R}^{d \\\\times d_k}$, and $W_V^i \\\\in \\\\mathbb{R}^{d \\\\times d_v}$ denote $i$-th head's learnable query, key, and value projection matrices, respectively. We adhere to the condition $d_k = d_v = d/s$ for every head.\\n\\n3.4. Overall framework: $\\\\text{LOWER}$\\nThe overall framework of $\\\\text{LOWER}$ is illustrated in Fig. 2. For each $\\\\ell$, it derives the output node embedding matrix $H(\\\\ell)$ from $H(\\\\ell)$\\\\_flow (Eq. (4)) and $H(\\\\ell)$\\\\_global (Eq. (5)) for the $\\\\ell$-th layer as follows:\\n\\n$$H(\\\\ell) = \\\\text{FeedForward}(H(\\\\ell)_{\\\\text{flow}} + H(\\\\ell)_{\\\\text{global}})$$\\n\\nIn our implementation, we employ a 2-layer MLP with ReLU activation [1] as the feedforward network. As shown in Fig. 2, note that we incorporate skip-connection and batch normalization in every module.\\n\\nThe output $H(\\\\ell)$ is used as the input of the next $\\\\text{LOWER}$ layer, and for the first layer, we utilize a projected input feature matrix as the input by multiplying $X$ with a learnable projection matrix $P \\\\in \\\\mathbb{R}^{D \\\\times d}$, i.e., $H(0) = XP$. Each $\\\\text{LOWER}$ layer has a separate set of learnable parameters.\\n\\nThe node embeddings in the output $H(L)$, where $L$ represents the total number of $\\\\text{LOWER}$ layers, are aggregated to drive the final embedding $z_G$ of the input neural-architecture graph $G$ as follows:\\n\\n$$z_G = \\\\text{READOUT}(H(L))$$\"}"}
{"id": "CVPR-2024-883", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"For aggregation, we use mean pooling as the readout function in our implementation.\\n\\nApplication to performance prediction: The architecture embedding $z_G$ is used for downstream tasks. For example, for performance prediction, it may serve as input to a regressor that outputs the estimated performance $\\\\hat{y}_G$ as follows:\\n\\n$$\\\\hat{y}_G = \\\\text{Regressor}(z_G).$$\\n\\nIn Sec. 4, we employ a fully connected layer as the regressor and utilize the following margin ranking loss for training both $\\\\text{LOWERFORMER}$ and the regressor:\\n\\n$$L = \\\\sum_{(i,j)} \\\\max(0, \\\\text{margin} - (\\\\hat{y}_i - \\\\hat{y}_j)),$$\\n\\nwhere $y_i$ and $y_j$ are the ground-truth performances of architectures $G_i$ and $G_j$, respectively. For each pair of architectures $G_i$ and $G_j$ in the training set such that $G_i$ outperforms $G_j$ (i.e., $y_i > y_j$), the loss encourages $\\\\hat{y}_i$ to be greater than $\\\\hat{y}_j$ by at least a specified margin. Such designs for loss functions are commonly employed when it is important to make relative comparisons among instances (in our case, we compare neural architectures to recommend better ones).\\n\\n4. Experiments\\n\\nIn this section, we review our experiments. For evaluation, we focus on the downstream task of predicting the performance of neural architectures. In Sec. 4.2, we compare the accuracies of $\\\\text{LOWERFORMER}$ and six baseline methods, including two state-of-the-art methods (specifically, TA-GATES [27] and NAR-Former [44]) using three performance prediction benchmark datasets composed of computer vision model architectures. In Sec. 4.3, we conduct an ablation study to validate each component of $\\\\text{LOWERFORMER}$. In Sec. 4.4, we extend our evaluation to datasets consisting of graph neural networks and auto speech recognition models. In Sec. 4.5, we examine the training and inference speed of $\\\\text{LOWERFORMER}$.\\n\\n4.1. Experimental settings\\n\\nBelow, we provide an overview of our experimental setup.\\n\\n4.1.1 Datasets\\n\\nWe evaluate the effectiveness of neural architecture encoding methods using five benchmark datasets designed for performance prediction, spanning three domains:\\n\\n- **Computer vision:** We use three datasets: NAS-Bench-101 [45, 47], NAS-Bench-201 [9], and NAS-Bench-301 [48]. These datasets contain computer vision models.\\n\\n- **Speech recognition:** We employ NAS-Bench-ASR [23], which consists of auto speech recognition architectures.\\n\\n- **Graph learning:** We include NAS-Bench-Graph [31], which consists of graph neural networks.\\n\\nRefer to Table 1 for basic statistics, and the supplementary material, for details including our preprocessing methods.\\n\\n4.1.2 Baseline methods\\n\\nWe utilize six baseline approaches, categorized as follows:\\n\\n- **Graph neural networks:** GatedGCN [3] and directed acyclic graph neural network (DAGNN) [37],\\n\\n- **Graph transformers:** GraphGPS [32] and DAGFormer [21], and\\n\\n- **Neural architecture encoders:** TA-GATES [27] and NAR-Former [44], which are state-of-the-art methods for neural architecture performance prediction. We use the official implementations of these methods, and the links can be found in the supplementary material.\\n\\n4.1.3 Training and Evaluation protocol\\n\\nFor model training and evaluation, we follow the setting in [27], including their training and test splits. We use a subset of the training split as the actual training set, varying the size of this subset: 1%, 5%, 10%, and 50% of the training split. We use the first 40 architectures in the test split as a validation set for hyperparameter tuning and early stopping, the remaining ones in the split as a test set. In each setting, we perform 9 trials using the three different splits and three different random seeds, and we report the mean and standard deviation across these trials. As accuracy metrics, we use Kendall's Tau [34] to assess overall performance and Precision@K (which measures the proportion of correctly predicted top-K architectures among the true top-K outperforming architectures) for the performance of identifying the best architectures. Note that these metrics are commonly employed in the field of neural architecture encoding [26, 27, 44].\\n\\n4.2. Performance on computer vision benchmarks\\n\\nIn this subsection, we focus on the computer vision benchmarks for which we have extensive baseline methods. In Tabs. 2 and 3, we report the performance prediction accuracies of the considered methods using two metrics across different training instance ratios. Notably, $\\\\text{LOWERFORMER}$ consistently outperforms all baseline methods across all settings in terms of Kendall's Tau. In terms of Precision@K,\"}"}
{"id": "CVPR-2024-883", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Datasets       | Kendall's Tau | Mean    | Standard Deviation |\\n|---------------|---------------|---------|--------------------|\\n| NAS-Bench-101 | 49.6          | 4.17    | 41.7               |\\n| NAS-Bench-301 | 61.5          | 75.8    | 92.9               |\\n| NAS-Bench-201 | 73.4          | 90.6    | 96.9               |\\n\\nIn this subsection, we conduct ablation studies to validate the effectiveness of each component. Specifically, in terms of Kendall's Tau, using the same setups as in Tab. 2. In each setting, the best performances are highlighted in green.\\n\\nTable 2. Kendall's Tau (scaled up by a factor of 100, mean and standard deviation over 9 trials) on three datasets: NAS-Bench-101, NAS-Bench-301, NAS-Bench-201.\\n\\n| Components | NAS-Bench-101 | NAS-Bench-301 | NAS-Bench-201 |\\n|------------|---------------|---------------|---------------|\\n| \u2714          | \u2714             | \u2714             | \u2714             |\\n| \u2717          | \u2714             | \u2714             | \u2717             |\\n| \u2714          | \u2717             | \u2714             | \u2714             |\\n\\nIn summary, our empirical findings substantiate that NAR-Former serves as an effective predictor of neural architectures. Specifically, in terms of Kendall's Tau, using the same setups as in Tab. 2. In each setting, the best performances are highlighted in green.\\n\\nTable 3. Comparison with four variants of FLOWER on three datasets: NAS-Bench-101, NAS-Bench-301, NAS-Bench-201.\\n\\n| Training Portions | NAS-Bench-101 | NAS-Bench-301 | NAS-Bench-201 |\\n|-------------------|---------------|---------------|---------------|\\n| \u2714                 | \u2714             | \u2714             | \u2714             |\\n| \u2717                 | \u2714             | \u2714             | \u2717             |\\n| \u2714                 | \u2717             | \u2714             | \u2714             |\\n\\nFirst, the necessity of asynchronous message passing for capturing flows is confirmed by the superior performance of NAR-Former over TA-GATES and GPS. We, therefore, argue that our incorporation of information flows into a graph transformer, through the introduction of the flow encode module and the flow-aware global attention module, plays a pivotal role in FLOWER's success.\\n\\nAsynchronous message passing. In most cases, FLOWER captures the information flow at a local-level through its information propagation scheme, thereby validating the effectiveness of each component. In each setting, the best performances are highlighted in green.\\n\\nTable 4. Comparison with four variants of FLOWER on three datasets: NAS-Bench-101, NAS-Bench-301, NAS-Bench-201.\\n\\n| Components       | NAS-Bench-101 | NAS-Bench-301 | NAS-Bench-201 |\\n|------------------|---------------|---------------|---------------|\\n| \u2714                | \u2714             | \u2714             | \u2714             |\\n| \u2717                | \u2714             | \u2714             | \u2717             |\\n| \u2714                | \u2717             | \u2714             | \u2714             |\\n\\nIn summary, our empirical findings substantiate that NAR-Former serves as an effective predictor of neural architectures.\"}"}
{"id": "CVPR-2024-883", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. Kendall\u2019s Tau (scaled up by a factor of 100, mean and standard deviation of 9 experiments) on two datasets beyond the computer vision domain: NAS-Bench-Graph (NB-G) [31] and NAS-Bench-ASR (NB-ASR) [23]. In each setting, the best performances are highlighted in green. In most cases, FORMER performs best.\\n\\n| Encoder     | 1% Training | 5% Training | 10% Training | 50% Training |\\n|-------------|-------------|-------------|--------------|--------------|\\n| DAGNN [37]  | 48.1 (3.2)  | 64.4 (1.2)  | 67.4 (1.1)   | 73.1 (0.8)   |\\n| DAGFormer [21] | 47.9 (0.6)  | 60.8 (1.6)  | 64.9 (1.0)   | 72.4 (0.3)   |\\n| TA-GATES [27] | 33.1 (1.4)  | 34.1 (2.0)  | 35.4 (0.8)   | 35.7 (0.5)   |\\n| FORMER      | 49.5 (1.1)  | 65.9 (1.3)  | 68.9 (0.6)   | 72.7 (0.2)   |\\n| DAGNN [37]  | 29.5 (3.9)  | 40.9 (2.4)  | 45.2 (1.3)   | 44.0 (0.4)   |\\n| DAGFormer [21] | 29.9 (5.4)  | 42.5 (1.1)  | 45.3 (1.0)   | 34.6 (5.8)   |\\n| TA-GATES [27] | 34.0 (2.3)  | 41.4 (2.0)  | 44.9 (2.2)   | 50.9 (0.8)   |\\n| FORMER      | 31.1 (8.0)  | 44.0 (0.9)  | 47.3 (1.3)   | 52.2 (1.4)   |\\n\\nThe advantage of forward-backward message passing is demonstrated by FORMER\u2019s superiority over (3).\\n\\nLastly, incorporating flow-awareness into global attention is advantageous, as evidenced by FORMER\u2019s advantage over variant (4).\\n\\n4.4. Performance in various domains\\n\\nSince our input modeling does not require complex pre-processing, it can be readily applied to architectures across various domains. We apply FORMER to graph neural networks on NAS-Bench-Graph and automatic speech recognition architectures on NAS-Bench-ASR. Among the baseline methods used in Sec. 4.2, we use the best method of each type: DAGNN, DAGFormer, and TA-GATES.\\n\\nAs shown in Tab. 5, FORMER consistently performs best in most cases. These results indicate that FORMER effectively captures important architectural characteristics across various domains. TA-GATES, which is tailored for encoding architectures in the computer vision domain, also shows strong performance in the domain of automatic speech recognition. TA-GATES effectively updates operation embeddings by multiplying operation embeddings and input information vectors, which is akin to convolutional mechanisms prevalent in auto speech recognition architectures. However, its effectiveness diminishes in scenarios where message passing between nodes, a key characteristic of graph neural networks, is required.\\n\\n4.5. Training and inference speed\\n\\nIn this subsection, we compare the training and inference speeds of FORMER and two state-of-the-art neural architecture encoding methods: NAR-Former and TA-GATES. To this end, we train all the models for 200 epochs with a batch size of 128, using an NVIDIA RTX 2080 GPU. We use NAS-Bench-101 with a training ratio of 1%. For a fair comparison, we exclude all additional time-consuming training strategies of NAR-Former and TA-GATES (e.g., input augmentation) in this experiment.\\n\\nAs shown in Tab. 6, FORMER takes the shortest training time among the three methods. In particular, training FORMER is 44.44 times faster than training NAR-Former. This substantial speed advantage stems from the notable difference in model sizes, with NAR-Former having 5.35 times the number of parameters compared to FORMER. Compared to TA-GATES, FORMER exhibits a slight speed advantage. Despite the small model size of TA-GATES, our specialized batch operations boost the training of FORMER. Refer to the supplementary material for details of the batch operations.\\n\\nIn terms of inference time, there is not much difference among the three methods, and FORMER ranks second. In practical scenarios, neural architecture performance prediction involves collecting labels (e.g., ground-truth performance) for the architectures in the training set, which requires time-consuming training of the architectures. For example, in the case of NAS-Bench-101, training just 1% of the architectures can take up to 24 GPU hours. Thus, inference speed is not a bottleneck, due to the extensive computational cost of training.\\n\\n5. Conclusions\\n\\nIn this work, we propose FORMER, a novel graph transformer model designed for neural architecture encoding. FORMER excels at capturing information flows within neural architectures, considering both local and global aspects. Through comprehensive evaluations across five benchmarks for architecture performance prediction, FORMER exhibits significant and consistent superiority over several state-of-the-art baseline methods, ultimately achieving state-of-the-art performance. Notably, FORMER\u2019s superiority extends beyond computer-vision architectures, demonstrating its effectiveness for graph-learning and speech-recognition architectures.\\n\\nAcknowledgements\\n\\nThis work was supported by Institute of Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 2022-0-00871, Development of AI Autonomy and Knowledge Enhancement for AI Agent Collaboration) (No. 2019-0-00075, Artificial Intelligence Graduate School Program (KAIST)).\\n\\nReferences\\n\\n[1] Abien Fred Agarap. Deep learning using rectified linear units (relu). arXiv preprint arXiv:1803.08375, 2018.\\n[2] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks.\"}"}
{"id": "CVPR-2024-883", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"FlowerFormer: Empowering Neural Architecture Encoding using a Flow-aware Graph Transformer\\n\\nDongyeong Hwang, Hyunju Kim, Sunwoo Kim, Kijung Shin, Jaechul Kim\\nGraduate School of AI, KAIST, Seoul, Republic of Korea\\n{dy.hwang, hyunju.kim, kswoo97, kijungs}@kaist.ac.kr\\n\\nAbstract\\nThe success of a specific neural network architecture is closely tied to the dataset and task it tackles; there is no one-size-fits-all solution. Thus, considerable efforts have been made to quickly and accurately estimate the performances of neural architectures, without full training or evaluation, for given tasks and datasets. Neural architecture encoding has played a crucial role in the estimation, and graph-based methods, which treat an architecture as a graph, have shown prominent performance. For enhanced representation learning of neural architectures, we introduce FlowerFormer, a powerful graph transformer that incorporates the information flows within a neural architecture.\\n\\nFlowerFormer consists of two key components: (a) bidirectional asynchronous message passing, inspired by the flows; (b) global attention built on flow-based masking. Our extensive experiments demonstrate the superiority of FlowerFormer over existing neural encoding methods, and its effectiveness extends beyond computer vision models to include graph neural networks and auto speech recognition models. Our code is available at http://github.com/y0ngjaenius/CVPR2024_FLOWERFormer.\\n\\n1. Introduction\\nWhile deep learning models have demonstrated their efficacy across various applications, the performance of a specific neural architecture heavily depends on specific downstream tasks and datasets employed. As a result, numerous neural architectures have been developed [11, 12]. In response to this dependency, significant efforts have been made to rapidly and accurately predict the performances of neural architectures for given tasks and datasets. This endeavor is crucial because exhaustively training and/or evaluating many candidate neural architectures is an expensive process. To this end, researchers have primarily employed machine learning techniques [6, 19]. Especially, various neural architecture encoding methods have been proposed since obtaining an accurate representation of each architecture plays a crucial role in the estimation process. Their focus has mainly revolved around (a) transforming input neural architectures to appropriate data structures [20, 41] and (b) applying representation-learning models to the transformed structures [5, 42]. Some have treated neural architectures as graphs and applied graph representation learning. They, however, share some limitations. For instance, their basic message-passing mechanisms oversimplify neural-architecture characteristics [35, 40] and may suffer from over-smoothing [29], over-squashing [2], or limited expressiveness [32].\\n\\nGraph Transformers (GTs), when incorporated with adequate information, are recognized for enhancing basic message passing, making them effective in various graph classification [13, 46] and regression [7, 22] tasks. One strength of GTs lies in their global attention mechanisms [38], where all nodes in an input graph contribute directly to forming the representation for each individual node. However, without integrating relevant topological or external information of input graphs, the relevance of attention scores, and thus the effectiveness of GTs, might be impaired. For example, Niu et al. [28] showed the essentiality of using motif-based spatial embedding to incorporate the characteristics of molecule graphs into GTs.\\n\\nIn this work, we propose FlowerFormer (Flow-aware graph transformer), a GT model specialized in capturing information flows within neural architectures, as illustrated in Fig. 1. The information flows of a neural architecture contain the characteristics of both forward and backward propagations of the architecture, and thus describe its fundamental properties. FlowerFormer includes two core modules: the flow encode module and the flow-aware global attention module. The former conducts bidirectional asynchronous message passing, imitating the forward and backward propagations within the input neural architecture. The latter applies global attention with masking schemes based on the flow-based dependencies between nodes.\"}"}
{"id": "CVPR-2024-883", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Information flows within an example neural architecture from the NAS-Bench-101 benchmark [45]. The architecture is represented as a directed graph where each node corresponds to an operation, and the topological structure of the graph encodes the sequence in which these operations are performed. For instance, the '1x1' (convolution) operation is executed only after the '3x3' (convolution) and 'mp' (max pooling) operations have been completed. The forward pass, depicted by blue arrows, is followed by the backpropagation of the loss, depicted by orange arrows. The number displayed above each node indicates the processing order within each flow.\\n\\nvalidate the superiority of FLOWERFormer over state-of-the-art neural encoding models [27, 44]. The results highlight the effectiveness of incorporating flows into GTs. Our contributions are summarized as follows:\\n\\n\u2022 We propose FLOWERFormer, a flow-aware GT-based neural architecture encoding model. To our best knowledge, FLOWERFormer is the first GT model specifically designed to capture flows.\\n\\n\u2022 FLOWERFormer outperforms six baseline architectures, including the most recent ones [27, 44], by a substantial margin across three benchmark datasets in the computer vision domain. Specifically, in predicting the performance of neural architectures, it outperforms the top-performing baseline method by a margin of up to 4.38% in Kendall\u2019s Tau. Additionally, through ablation studies, we justify the design choices made in FLOWERFormer.\\n\\n\u2022 Beyond computer vision neural architectures, FLOWERFormer also excels at performance prediction for graph neural networks and auto speech recognition architectures. In the benchmarks for these architectures, FLOWERFormer achieves performance gains of up to 4.41% in Kendall\u2019s Tau over baseline models.\\n\\nOur code is available at http://github.com/y0n9jaenius/CVPR2024_FLOWERFormer.\\n\\n2. Related work\\nIn this section, we briefly review related studies in neural architecture encoding and graph transformers (GTs).\\n\\n2.1. Neural architecture encoding\\nNeural architecture encoding [17, 19, 20, 39, 41], which aims to learn representations of neural architectures, has gained considerable attention due to its significant downstream tasks, such as performance prediction (i.e., the prediction of task- and data-specific performance for given architectures without full training or evaluation).\\n\\nOne popular class of approaches is graph-based, modeling neural architectures as graphs and using graph neural networks [15] for representation learning. These approaches have also introduced topology-based graph similarity and operation-specific embeddings [4, 8].\\n\\nAnother significant approach aims to obtain representations that mimic the forward and/or backward passes within neural architectures. For instance, GATES [26] updates operation embeddings by mimicking the application of operations to information (which is also represented as a vector) and thus effectively replicating the forward-pass of convolution operations. Another method, TA-GATES [27], simulates an iterative process involving both forward and backward passes, with specialized handling for specific operations, e.g., skip-connections. However, these methods focus on flows only at a local level, by simulating a series of local operations, and may overlook a global-level perspective.\\n\\nTransformer-based models [18, 43] are capable of capturing global-level perspectives through attention mechanisms. NAR-Former [44], a multi-stage fusion transformer, is one of the state-of-the-art methods for predicting neural architecture performance. They (1) represent a neural architecture as a sequence of operations to employ a transformer-based model and (2) leverage multiple valid sequences from the same architecture for augmentation.\\n\\nIn this work, we unify all three dimensions\u2014graph learning, flow modeling, and global attention\u2014by introducing a novel flow-aware GT, marking the first instance of such integration to the best of our knowledge.\\n\\n2.2. Graph transformers (GTs)\\nGraph transformers (GTs) [10, 13, 16, 32, 36, 46] apply global (i.e., graph-level) attention between all node pairs. Recently, GTs show remarkable performance in various graph-level tasks, including molecular property prediction [14, 33], image classification [25, 49], and human interaction recognition [30].\\n\\nTo further improve their effectiveness, global attention is often supplemented with topological and/or external information. The information includes eigenvectors of adjacency and Laplacian matrices [16, 36] and pair-wise node similarity derived from shortest paths, diffusion kernels, random walks, etc [16, 24, 46].\\n\\nSome GTs are tailored for specific types of graphs. For molecular graphs, where motifs play key roles, Niu et al. [28] employ motif-based spatial embeddings in a GT. DAGFormer [21] is designed for directed acyclic graphs (DAGs) and incorporates depth-based positional encodings and reachability-based attention. Note that DAGFormer is designed for general DAGs, and it is not optimized for encoding neural architectures, especially in capturing architecture-specific flows.\"}"}
{"id": "CVPR-2024-883", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Flow-aware Global Attention Module\\nFlow Encode Module output architecture embedding neural architecture input\\n\u00d7 \\\\( L \\\\) Add & Norm Add & Norm\\n\\\\( H_{\\\\text{flow}} \\\\) \\\\( H_{\\\\text{global}} \\\\)\\nREAD OUT \\\\( \\\\gamma \\\\) Regressor architecture performance\\n\\nFigure 2. Overview of proposed FLOWER FORMER, which contains two key modules in each of its layers: the flow encode module and the flow-aware global attention module. The flow encode module performs bidirectional asynchronous message passing, inspired by forward and backward passes, to produce a node embedding matrix \\\\( H_{\\\\text{flow}} \\\\). The flow-aware global attention module computes attention with a flow-based masking scheme to yield another node embedding matrix \\\\( H_{\\\\text{global}} \\\\). These two embedding matrices, \\\\( H_{\\\\text{flow}} \\\\) and \\\\( H_{\\\\text{global}} \\\\), are combined and then projected to produce updated node embeddings at each layer. This process is iterated over \\\\( L \\\\) layers, and the output node embeddings are aggregated to form the final architecture embedding, which is fed into a regressor for performance prediction.\\n\\n3. Proposed method: FLOWER FORMER\\n\\nIn this section, we present FLOWER FORMER (Flow-aware Graph transformer), a graph transformer model designed to capture information flows within an input neural architecture. First, we provide the motivation behind FLOWER FORMER in Sec. 3.1. Then, we describe how an input neural architecture is represented as a graph in Sec. 3.2. After that, we elaborate on how FLOWER FORMER learns the representation of the neural architecture graph. Specifically, we describe two core modules of FLOWER FORMER, collectively referred to as FLOWER, in Sec. 3.3. Lastly, we present the overall framework (refer to Fig. 2) in Sec. 3.4.\\n\\n3.1. Motivation of capturing information flows\\n\\nDespite the remarkable success of Graph Transformers (GTs) in various graph-level tasks, including graph classification [13, 46] and regression [7, 22], their application for encoding neural architectures has received relatively limited attention. Existing applications of GTs suggest that additional design choices for accurately capturing the underlying characteristics of input graphs (on top of global attention mechanism between all pairs of nodes) are essential for the effectiveness of GTs. Refer to Sec. 2.2 for some examples.\\n\\nIn this work, we focus on a crucial aspect: capturing information flows within neural architectures (i.e., input graphs). Information flows include both the forward pass of data and the backpropagation of gradients. Hence, it is essential to capture information flows for incorporating how neural architectures are trained and conduct inference into their embeddings (i.e., the encoded neural architectures).\\n\\n3.2. Input modeling\\n\\nWe represent a given neural architecture as a directed acyclic graph (DAG), with each node representing an operation (e.g., pooling or convolution). Each directional edge between two nodes indicates the information flow between the corresponding operations, aligning with the direction of data propagation during the forward pass. An illustrative example can be found on the left-hand side of Figure 3. We denote the graph representation of a neural architecture by \\\\( G = (A, X) \\\\), a tuple of an adjacency matrix \\\\( A \\\\in \\\\{0, 1\\\\}^{N \\\\times N} \\\\) and a node (i.e., operation) feature matrix \\\\( X \\\\in \\\\{0, 1\\\\}^{N \\\\times D} \\\\), where \\\\( N \\\\) is the number of nodes and \\\\( D \\\\) is the number of operations. The adjacency matrix encodes direct connections between node pairs in a graph. Its binary entries indicate whether a directional edge exists between each pair of nodes. Specifically, the \\\\((i, j)\\\\)-th entry of \\\\( A \\\\) is set to 1 if there is a directed edge from the \\\\( i \\\\)-th node (denoted as \\\\( v_i \\\\)) to the \\\\( j \\\\)-th node (denoted as \\\\( v_j \\\\)), and 0 otherwise. Each node is associated with a one-hot feature vector representing its corresponding operation, and these vectors are stacked vertically to form the node feature matrix \\\\( X \\\\). Refer to Fig. 3 for an example.\\n\\nWith our general input modeling scheme, FLOWER FORMER is readily applicable to different domains and neural architectures without such additional modelings or steps. By contrast, state-of-the-art neural encoding methods often rely on complex modelings and/or preprocessing steps, such as the specialized treatment of specific operations [27] and isomorphic augmentations [44] (refer to Sec. 2.1). The empirical superiority of FLOWER FORMER (refer to Sec. 4) despite its straightforward (yet elegant) input modeling is apparent.\"}"}
{"id": "CVPR-2024-883", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 1: Flow encode module\\n\\nInput: \\n1. $G = (A, X)$: an input neural architecture\\n2. $H$: an input node embedding matrix\\n\\nOutput: \\n$H$: updated node embedding matrix\\n\\n1/\u2217\\n\u2217 step 1. topological sorting\\n\u2217 /\\n2 $T_G \u2190$ topological generations of $G$\\n\\n3/\u2217\\n\u2217 step 2. asynchronous forward message passing\\n\u2217 /\\n4 for $k = 1, \\\\ldots, |T_G|$ do\\n5 for $v_j \\\\in T_G^k$ do\\n6 $h_j \u2190 \\\\text{Comb} (h_j, \\\\text{Agg} \\\\{m_e (h_j, h_i) : A_{ij} = 1\\\\})$\\n\\n7/\u2217\\n\u2217 step 3. asynchronous backward message passing\\n\u2217 /\\n8 for $k = |T_G|, \\\\ldots, 1$ do\\n9 for $v_j \\\\in T_G^k$ do\\n10 $h_j \u2190 \\\\text{Comb} (h_j, \\\\text{Agg} \\\\{m_e (h_j, h_i) : A_{ji} = 1\\\\})$\\n\\n11 return $H$\\n\\nDistributed to our novel flow-aware GT architecture, which is described in the following subsection.\\n\\n3.3. FLOWER layers\\n\\nIn this section, we introduce FLOWER layers, the basic units of FORMER. A FLOWER layer consists of two core components: the flow encode module and the flow-aware global attention module. The flow encode module is a message-passing neural network (MPNN) that asynchronously passes messages in the forward and then the backward orders. The flow-aware global attention module is a self-attention module based on a flow-aware masking scheme. The outputs of the flow encode module and the flow-aware global attention module are node embedding matrices, denoted as $H^{\\\\text{flow}} \\\\in \\\\mathbb{R}^{N \\\\times d}$ and $H^{\\\\text{global}} \\\\in \\\\mathbb{R}^{N \\\\times d}$, respectively, for the $l$-th FLOWER layer. Below, we provide a detailed explanation of each module.\\n\\n3.3.1 Flow encode module\\n\\nAs discussed in Sec. 3.1, we aim to enable a GT to capture the crucial aspect of neural architectures\u2014information flows. To this end, the flow encode module conducts both asynchronous forward and backward message passing, resembling the forward pass (i.e., inference) and backpropagation (i.e., training) of neural architectures, respectively. These message-passing procedures are carried out in the (reversed) topological order in the input neural architecture graph, leading to updated node embeddings.\\n\\nPseudocode of the flow encode module is presented in Algorithm 1. It includes topological sorting, forward message passing, and backward message passing, in order, and each of these components is described below.\\n\\nTopological sorting (Line 2):\\nThe first step is to divide nodes (i.e., operations) into topological generations. Recall that neural-architecture graphs are directed acyclic graphs (DAGs). Given a DAG $G$, its topological generations are as follows:\\n\\n- $T_G^1 = \\\\{1, 2\\\\}$\\n- $T_G^2 = \\\\{3, 4, 5\\\\}$\\n- $T_G^3 = \\\\{6\\\\}$\\n- $T_G^4 = \\\\{7\\\\}$\\n\\nThese topological generations are closely related to the data flow within a neural architecture. For the operations (i.e., nodes) in each generation to be executed, all operations in the preceding generations need to be complete. Conversely, during the process of backpropagation, gradients flow from subsequent generations to preceding generations.\\n\\nForward message passing (Line 4-Line 6):\\nDuring the forward message passing step, node embeddings are updated asynchronously, following the order of the topological generations, akin to the forward pass within neural architectures. For each node $v_j$, its embedding $h_j$ (i.e., the $j$-th row vector of $H$) is updated by the following three steps:\\n\\n1. computing the message $m_e (h_j, h_i)$ for each incoming neighbor $v_i$,\\n2. aggregating these messages,\\n3. combining the result with the current embedding $h_j$.\\n\\nNote that the embeddings of all incoming neighbors, which belong to preceding generations, have already been updated by the time message calculation occurs. Also note that this differs from conventional synchronous graph message passing, where all node embeddings are updated simultaneously based on their input embeddings.\\n\\nIn our implementation, we use the sum aggregation as the Agg function. As $m_e$ and $\\\\text{Comb}$, we adopt the message function and the combine operator used in [37], as follows:\\n\\n$$m_e (h_j, h_i) = \\\\text{softmax} (w_1^\\\\top h_j + w_2^\\\\top h_i)$$\\n\\n$$\\\\text{msg}_j = \\\\sum_{i : A_{ij} = 1} m_e (h_j, h_i),$$\\n\\n$$\\\\text{Comb} (h_j, \\\\text{msg}_j) = \\\\text{GRU} (h_j, \\\\text{msg}_j),$$\\n\\nwhere $w_1 \\\\in \\\\mathbb{R}^d$ and $w_2 \\\\in \\\\mathbb{R}^d$ are learnable parameters.\\n\\nBackward message passing (Line 8-Line 10):\\nAfter the forward message passing step, we further update node embeddings through backward message passing, which resembles the process of backpropagation. This aligns with the standard practice in neural architecture training, where\"}"}
