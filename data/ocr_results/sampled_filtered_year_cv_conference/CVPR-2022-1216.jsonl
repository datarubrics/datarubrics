{"id": "CVPR-2022-1216", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Emre Aksan, Manuel Kaufmann, and Otmar Hilliges. Structured prediction helps 3d human motion modelling. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7144\u20137153, 2019.\\n\\n[2] Mohammad Sadegh Aliakbarian, Fatemeh Sadat Saleh, Mathieu Salzmann, Lars Petersson, Stephen Gould, and Amirhossein Habibian. Learning variations in human motion via mix-and-match perturbation. arXiv preprint arXiv:1908.00733, 2019.\\n\\n[3] Andrea Bajcsy, Somil Bansal, Ellis Ratner, Claire J Tomlin, and Anca D Dragan. A robust control framework for human motion prediction. IEEE Robotics and Automation Letters, 6(1):24\u201331, 2020.\\n\\n[4] Andrea Bajcsy, Anand Siththaranjan, Claire J Tomlin, and Anca D Dragan. Analyzing human models that adapt online. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 2754\u20132760. IEEE, 2021.\\n\\n[5] Emad Barsoum, John Kender, and Zicheng Liu. Hp-gan: Probabilistic 3d human motion prediction via gan. In Proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 1418\u20131427, 2018.\\n\\n[6] Justin Bayer, Daan Wierstra, Julian Togelius, and J\u00fcrgen Schmidhuber. Evolving memory cell structures for sequence learning. In International conference on artificial neural networks, pages 755\u2013764. Springer, 2009.\\n\\n[7] Yoshua Bengio, Aaron Courville, and Pascal Vincent. Representation learning: A review and new perspectives. IEEE transactions on pattern analysis and machine intelligence, 35(8):1798\u20131828, 2013.\\n\\n[8] Apratim Bhattacharyya, Michael Hanselmann, Mario Fritz, Bernt Schiele, and Christoph-Nikolas Straehle. Conditional flow variational autoencoders for structured sequence prediction. arXiv preprint arXiv:1908.09008, 2019.\\n\\n[9] Apratim Bhattacharyya, Bernt Schiele, and Mario Fritz. Accurate and diverse sampling of sequences based on a \u201cbest of many\u201d sample objective. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 8485\u20138493, 2018.\\n\\n[10] Matthew Brand and Aaron Hertzmann. Style machines. In Proceedings of the 27th annual conference on Computer graphics and interactive techniques, pages 183\u2013192, 2000.\\n\\n[11] Defu Cao, Jiachen Li, Hengbo Ma, and Masayoshi Tomizuka. Spectral temporal graph neural network for trajectory prediction. In 2021 IEEE International Conference on Robotics and Automation (ICRA), pages 1839\u20131845. IEEE, 2021.\\n\\n[12] Kyunghyun Cho, Bart Van Merri\u00ebnboer, Caglar Gulcehre, Dzmitry Bahdanau, Fethi Bougares, Holger Schwenk, and Yoshua Bengio. Learning phrase representations using rnn encoder-decoder for statistical machine translation. arXiv preprint arXiv:1406.1078, 2014.\\n\\n[13] Chiho Choi, Joon Hee Choi, Jiachen Li, and Srikanth Malla. Shared cross-modal trajectory prediction for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 244\u2013253, 2021.\\n\\n[14] Chiho Choi and Behzad Dariush. Looking to relations for future trajectory forecast. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 921\u2013930, 2019.\\n\\n[15] Enric Corona, Albert Pumarola, Guillem Alenya, and Francesc Moreno-Noguer. Context-aware human motion prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6992\u20137001, 2020.\\n\\n[16] Antonia Creswell, Tom White, Vincent Dumoulin, Kai Arulkumaran, Biswa Sengupta, and Anil A Bharath. Generative adversarial networks: An overview. IEEE Signal Processing Magazine, 35(1):53\u201365, 2018.\\n\\n[17] Michal Derezinski, Daniele Calandriello, and Michal Valko. Exact sampling of determinantal point processes with sublinear time preprocessing. Advances in neural information processing systems, 32, 2019.\\n\\n[18] Rahul Dey and Fathi M Salem. Gate-variants of gated recurrent unit (gru) neural networks. In 2017 IEEE 60th international midwest symposium on circuits and systems (MWSCAS), pages 1597\u20131600. IEEE, 2017.\\n\\n[19] Nat Dilokthanakul, Pedro AM Mediano, Marta Garnelo, Matthew CH Lee, Hugh Salimbeni, Kai Arulkumaran, and Murray Shanahan. Deep unsupervised clustering with gaussian mixture variational autoencoders. arXiv preprint arXiv:1611.02648, 2016.\\n\\n[20] Conor Durkan, Artur Bekasov, Iain Murray, and George Papamakarios. Neural spline flows. Advances in neural information processing systems, 32, 2019.\\n\\n[21] Katerina Fragkiadaki, Sergey Levine, Panna Felsen, and Jitendra Malik. Recurrent network models for human dynamics. In Proceedings of the IEEE international conference on computer vision, pages 4346\u20134354, 2015.\\n\\n[22] Jennifer A Gillenwater, Alex Kulesza, Emily Fox, and Ben Taskar. Expectation-maximization for learning determinantal point processes. Advances in Neural Information Processing Systems, 27, 2014.\\n\\n[23] Boqing Gong, Wei-Lun Chao, Kristen Grauman, and Fei Sha. Diverse sequential subset selection for supervised video summarization. Advances in neural information processing systems, 27, 2014.\\n\\n[24] Swaminathan Gurumurthy, Ravi Kiran Sarvadevabhatla, and R Venkatesh Babu. Deligan: Generative adversarial networks for diverse and limited data. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 166\u2013174, 2017.\\n\\n[25] J Ben Hough, Manjunath Krishnapur, Yuval Peres, and B\u00e1lint Vir\u00e1g. Determinantal processes and independence. Probability surveys, 3:206\u2013229, 2006.\\n\\n[26] Catalin Ionescu, Dragos Papava, Vlad Olaru, and Cristian Sminchisescu. Human3.6m: Large scale datasets and predictive methods for 3d human sensing in natural environments. IEEE transactions on pattern analysis and machine intelligence, 36(7):1325\u20131339, 2013.\\n\\n[27] Ashesh Jain, Amir R Zamir, Silvio Savarese, and Ashutosh Saxena. Structural-rnn: Deep learning on spatio-temporal\\n\\n8169\"}"}
{"id": "CVPR-2022-1216", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"graphs. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 5308\u20135317, 2016.\\n\\n[28] Durk P Kingma, Shakir Mohamed, Danilo Jimenez Rezende, and Max Welling. Semi-supervised learning with deep generative models. Advances in Neural Information Processing Systems, 27, 2014.\\n\\n[29] Diederik P Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint arXiv:1312.6114, 2013.\\n\\n[30] Thomas Kipf, Ethan Fetaya, Kuan-Chieh Wang, Max Welling, and Richard Zemel. Neural relational inference for interacting systems. In International Conference on Machine Learning, pages 2688\u20132697. PMLR, 2018.\\n\\n[31] Alex Kulesza and Ben Taskar. k-dpps: Fixed-size determinantal point processes. In ICML, 2011.\\n\\n[32] Jogendra Nath Kundu, Maharshi Gor, and R Venkatesh Babu. BiHMP-GAN: Bidirectional 3D human motion prediction GAN. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 33, pages 8553\u20138560, 2019.\\n\\n[33] Jiachen Li, Hengbo Ma, and Masayoshi Tomizuka. Conditional generative neural system for probabilistic trajectory prediction. In 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 6150\u20136156. IEEE, 2019.\\n\\n[34] Jiachen Li, Fan Yang, Hengbo Ma, Srikanth Malla, Masayoshi Tomizuka, and Chiho Choi. Rain: Reinforced hybrid attention inference network for motion forecasting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16096\u201316106, 2021.\\n\\n[35] Jiachen Li, Fan Yang, Masayoshi Tomizuka, and Chiho Choi. EvolveGraph: Multi-agent trajectory prediction with dynamic relational reasoning. Advances in Neural Information Processing Systems, 33:19783\u201319794, 2020.\\n\\n[36] Maosen Li, Siheng Chen, Yangheng Zhao, Ya Zhang, Yanfeng Wang, and Qi Tian. Dynamic multiscale graph neural networks for 3D skeleton based human motion prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 214\u2013223, 2020.\\n\\n[37] Xiu Li, Hongdong Li, Hanbyul Joo, Yebin Liu, and Yaser Sheikh. Structure from recurrent motion: From rigidity to recurrency. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3032\u20133040, 2018.\\n\\n[38] Xiao Lin and Mohamed R Amer. Human motion modeling using DVGANS. arXiv preprint arXiv:1804.10652, 2018.\\n\\n[39] Hengbo Ma, Jiachen Li, Wei Zhan, and Masayoshi Tomizuka. Wasserstein generative learning with kinematic constraints for probabilistic interactive driving behavior prediction. In 2019 IEEE Intelligent Vehicles Symposium (IV), pages 2477\u20132483. IEEE, 2019.\\n\\n[40] Hengbo Ma, Yaofeng Sun, Jiachen Li, and Masayoshi Tomizuka. Multi-agent driving behavior prediction across different scenarios with self-supervised domain knowledge. In 2021 IEEE International Intelligent Transportation Systems Conference (ITSC), pages 3122\u20133129. IEEE, 2021.\\n\\n[41] Hengbo Ma, Yaofeng Sun, Jiachen Li, Masayoshi Tomizuka, and Chiho Choi. Continual multi-agent interaction behavior prediction with conditional generative memory. IEEE Robotics and Automation Letters, 6(4):8410\u20138417, 2021.\\n\\n[42] Wei Mao, Miaomiao Liu, and Mathieu Salzmann. Generating smooth pose sequences for diverse human motion prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 13309\u201313318, 2021.\\n\\n[43] Wei Mao, Miaomiao Liu, Mathieu Salzmann, and Hongdong Li. Learning trajectory dependencies for human motion prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9489\u20139497, 2019.\\n\\n[44] Julieta Martinez, Michael J Black, and Javier Romero. On human motion prediction using recurrent neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2891\u20132900, 2017.\\n\\n[45] Soohwan Park, Hoseok Ryu, Seyoung Lee, Sunmin Lee, and Jehee Lee. Learning predict-and-simulate policies from unorganized human motion data. ACM Transactions on Graphics (TOG), 38(6):1\u201311, 2019.\\n\\n[46] Dario Pavllo, David Grangier, and Michael Auli. Quaternet: A quaternion-based recurrent model for human motion. arXiv preprint arXiv:1805.06485, 2018.\\n\\n[47] Nicholas Rhinehart, Kris M Kitani, and Paul Vernaza. R2P2: A reparameterized pushforward policy for diverse, precise generative path forecasting. In Proceedings of the European Conference on Computer Vision (ECCV), pages 772\u2013788, 2018.\\n\\n[48] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The graph neural network model. IEEE Transactions on Neural Networks, 20(1):61\u201380, 2008.\\n\\n[49] Jan Sedmidubsky and Pavel Zezula. Similarity search in 3D human motion data. In Proceedings of the 2019 on International Conference on Multimedia Retrieval, pages 5\u20136, 2019.\\n\\n[50] Hedvig Sidenbladh, Michael J Black, and Leonid Sigal. Implicit probabilistic models of human motion for synthesis and tracking. In European Conference on Computer Vision, pages 784\u2013800. Springer, 2002.\\n\\n[51] Leonid Sigal, Alexandru O Balan, and Michael J Black. HumanEVA: synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion. International Journal of Computer Vision, 87(1):4\u201327, 2010.\\n\\n[52] Kihyuk Sohn, Honglak Lee, and Xinchen Yan. Learning structured output representation using deep conditional generative models. Advances in Neural Information Processing Systems, 28, 2015.\\n\\n[53] Chen Tang, Wei Zhan, and Masayoshi Tomizuka. Exploring social posterior collapse in variational autoencoder for interaction modeling. Advances in Neural Information Processing Systems, 34, 2021.\\n\\n[54] Graham W Taylor, Geoffrey E Hinton, and Sam Roweis. Modeling human motion using binary latent variables. Advances in Neural Information Processing Systems, 19, 2006.\\n\\n[55] Jakub Tomczak and Max Welling. VAE with a VAMP prior. In International Conference on Artificial Intelligence and Statistics, pages 1214\u20131223. PMLR, 2018.\"}"}
{"id": "CVPR-2022-1216", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jacob Walker, Kenneth Marino, Abhinav Gupta, and Martial Hebert. The pose knows: Video forecasting by generating pose futures. In Proceedings of the IEEE international conference on computer vision, pages 3332\u20133341, 2017.\\n\\nBorui Wang, Ehsan Adeli, Hsu-kuang Chiu, De-An Huang, and Juan Carlos Niebles. Imitation learning for human pose prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 7124\u20137133, 2019.\\n\\nJack M Wang, David J Fleet, and Aaron Hertzmann. Gaussian process dynamical models for human motion. IEEE transactions on pattern analysis and machine intelligence, 30(2):283\u2013298, 2007.\\n\\nJingwei Xu, Huazhe Xu, Bingbing Ni, Xiaokang Yang, and Trevor Darrell. Video prediction via example guidance. In International Conference on Machine Learning, pages 10628\u201310637. PMLR, 2020.\\n\\nXinchen Yan, Akash Rastogi, Ruben Villegas, Kalyan Sunkavalli, Eli Shechtman, Sunil Hadap, Ersin Yumer, and Honglak Lee. Mt-vae: Learning motion transformations to generate multimodal human dynamics. In Proceedings of the European conference on computer vision (ECCV), pages 265\u2013281, 2018.\\n\\nYe Yuan and Kris Kitani. Diverse trajectory forecasting with determinantal point processes. arXiv preprint arXiv:1907.04967, 2019.\\n\\nYe Yuan and Kris Kitani. Dlow: Diversifying latent flows for diverse human motion prediction. In European Conference on Computer Vision, pages 346\u2013364. Springer, 2020.\\n\\nAndrei Zanfir, Eduard Gabriel Bazavan, Hongyi Xu, William T Freeman, Rahul Sukthankar, and Cristian Sminchisescu. Weakly supervised 3d human pose and shape reconstruction with normalizing flows. In European Conference on Computer Vision, pages 465\u2013481. Springer, 2020.\\n\\nYan Zhang, Michael J Black, and Siyu Tang. We are more than our joints: Predicting how 3d bodies move. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3372\u20133382, 2021.\\n\\nZachary Ziegler and Alexander Rush. Latent normalizing flows for discrete sequences. In International Conference on Machine Learning, pages 7673\u20137682. PMLR, 2019.\"}"}
{"id": "CVPR-2022-1216", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. The procedure of short-term oracle supervision. During training, we can get several predicted human motions. For each sample (indicated by the blue arrow), the poses will be fed to the oracle after each $\\\\tau$ time steps. The oracle will provide several possible future poses as options. The predicted human motions only need to be similar to one of the options in each short time horizon.\\n\\nWe also adopt several widely-used physical feasibility losses [42, 62, 63] such as the limbs' constraint $L_{\\\\text{limb}}$ and the velocity constraint $L_{\\\\text{vel}}$ as $L_{\\\\text{phy}}$:\\n\\n$$L_{\\\\text{phy}} = \\\\lambda_{\\\\text{vel}} L_{\\\\text{vel}} + L_{\\\\text{limb}}.$$  \\\\hspace{1cm} (12)\\n\\nThe details of each item in Equation 12 are provided in the supplementary material. Therefore, the overall loss for the diversity sampler is:\\n\\n$$L_D = \\\\lambda_{\\\\text{ref}} L_{\\\\text{ref}} + \\\\lambda_{\\\\text{div}} L_{\\\\text{div}} + L_{\\\\text{phy}},$$  \\\\hspace{1cm} (13)\\n\\nwhere $\\\\lambda_{\\\\text{ref}}$ and $\\\\lambda_{\\\\text{div}}$ decide the importance of losses. Besides, we use a low-pass filter to smooth the predicted poses generated by the diversity sampler after training. Please see the details in the supplementary material.\\n\\n4.2. Short-term Oracle Design\\n\\nWe introduce an oracle to supervise the predictor in Section 4.1.2. In this section, we discuss how to obtain the oracle. We propose to learn a short-term oracle $O(X, \\\\tau)$ by using another conditional variational autoencoder to capture the pseudo-ground-truth multi-modality. In order to achieve such goal, several works utilize the similarity search techniques [59]. This method is also used in [61, 62] as the multi-modality evaluation metrics. In our work, we define:\\n\\n$$\\\\Omega(X_t) = S(X_o; \\\\tau, K)$$\\n\\nwhere $X_o$ represents the set of all the future poses whose corresponding initial poses $X_j t$ are in a ball with radius $\\\\delta$ which centered at the given initial pose $X_t$. The ball is defined by metric $d(\\\\cdot, \\\\cdot)$.\\n\\n$$\\\\Omega(X_t)$$ represents the set of $K$ selected future poses which has time horizon $\\\\tau$ given the initial pose $X_t$. Since there can be many similar poses to the given initial poses and most of the corresponding future poses are very similar, we need to select a proper fixed number of future poses in $X_o$ in order to capture the different modes. Here we use the k-determinantal point process (k-DPP) as the selection strategy $S$ to choose the future poses.\\n\\n4.2.1 k-Determinantal Point Process\\n\\nk-determinantal point process [31] is widely used to sample the diverse points given a fixed number of samples. Given a set $X = \\\\{X_1, X_2, ..., X_n\\\\}$, a k-determinantal point process defined on $X$ is a probability measure on $\\\\mathcal{P}(X)$:\\n\\n$$Pr(S) = \\\\det(L_S)^{1/(|S| = k)} \\\\sum_{S \\\\subset [n], |S| = k} \\\\det(L_S),$$  \\\\hspace{1cm} (15)\\n\\nwhere we denote $S$ as a subset of $X$ and $L_S \\\\in \\\\mathbb{R}^{|S| \\\\times |S|}$ as the similarity matrix:\\n\\n$$L_S^{ij} = e^{-d(X_{it+1:t+\\\\tau}, X_{jt+1:t+\\\\tau})}.$$  \\\\hspace{1cm} (16)\\n\\nWe preprocess the training data to augment each case with $K$ futures poses. Several sampling algorithms [17, 25] for the determinantal point process can be used directly.\\n\\n4.2.2 Short-term Oracle Model\\n\\nThe short-term oracle can be trained with any approach proposed in Section 2. In our experiments, we use a conditional variational autoencoder similar to the likelihood sampler defined above after getting the augmented data with the prediction horizon $\\\\tau$. Now, we can provide more diverse futures given the exact same historical observation. Since the augmented data is balanced by the k-determinantal point process.\"}"}
{"id": "CVPR-2022-1216", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"process, there will be fewer extremely minor modes and hence mitigate the trouble of rare-case sampling. The details of the short-term oracle neural network structure are provided in the supplementary material.\\n\\n5. Training and Testing Process\\n\\nThe training procedure is summarized in Algorithm 1. We generate the same number of samples from both accuracy prior and diversity prior for training. Notice that the diversity loss \\\\( L_{div} \\\\) does not backpropagate to the accuracy prior \\\\( Q_{acc}(Z|C) \\\\) since we do not want the diversity loss influence the accuracy prior. After we get the optimized\\n\\nAlgorithm 1:\\n\\n**Training Procedure**\\n\\n**Input:**\\n- \\\\( N \\\\): number of epoches.\\n- \\\\( n_{acc} \\\\): number of samples for accuracy sampler \\\\( Q_{acc} \\\\).\\n- \\\\( n_{div} \\\\): number of samples for diversity sampler \\\\( Q_{div} \\\\).\\n- \\\\( n_{o} \\\\): number of samples generated from oracle \\\\( O \\\\).\\n\\n**Output:**\\n- \\\\( \\\\theta \\\\), \\\\( \\\\phi_{acc} \\\\), \\\\( \\\\phi_{div} \\\\)\\n\\n**Data:**\\n- Training dataset \\\\( D_{train} \\\\)\\n\\n1. while \\\\( \\\\text{epoch} \\\\leq N \\\\) do\\n2. Sample \\\\( B = \\\\{X_i, C_i\\\\} \\\\) \\\\( i \\\\sim D_{train} \\\\)\\n3. foreach \\\\( X, C \\\\in B \\\\) do\\n4. Generate \\\\( n_{acc} \\\\) samples:\\n5. \\\\( \\\\hat{X}_i^{acc} = d(X|C, z_i) \\\\), \\\\( z_i \\\\sim Q_{acc}(Z|C) \\\\)\\n6. Generate \\\\( n_{div} \\\\) samples:\\n7. \\\\( \\\\hat{X}_i^{div} = d(X|C, z_i) \\\\), \\\\( z_i \\\\sim Q_{div}(Z|C) \\\\)\\n8. for \\\\( s = 0, \\\\ldots, T/\\\\tau - 1 \\\\) do\\n9. Generate \\\\( n_{o} \\\\) samples:\\n10. \\\\( \\\\tilde{X}_{j_t+s\\\\tau+1:t+s\\\\tau+\\\\tau} \\\\sim O(\\\\hat{X}_{div}, t+s\\\\tau, \\\\tau) \\\\)\\n11. Update \\\\( \\\\theta, \\\\psi, \\\\phi_{acc} \\\\) with \\\\( L_A \\\\)\\n12. Update \\\\( \\\\theta, \\\\phi_{div} \\\\) with \\\\( L_D \\\\)\\n\\nmodel, we can decide the ratio of diverse samples, which mainly focus on the most different modes compared with the major modes by adjusting the ratio number \\\\( \\\\rho \\\\). The testing procedure is summarized in Algorithm 2.\\n\\nAlgorithm 2:\\n\\n**Testing Procedure**\\n\\n**Input:**\\n- \\\\( \\\\rho \\\\): The proportion of samples from \\\\( Q_{div} \\\\) in the total samples,\\n- \\\\( M \\\\): the total number of samples\\n\\n**Output:**\\n- \\\\( \\\\hat{X} \\\\), The predicted poses\\n\\n**Data:**\\n- Testing Dataset \\\\( D_{test} \\\\)\\n\\n1. foreach \\\\( X, C \\\\in D_{test} \\\\) do\\n2. Generate \\\\( (1 - \\\\rho)M \\\\) samples from \\\\( Q_{acc} \\\\)\\n3. Generate \\\\( \\\\rho M \\\\) samples from \\\\( Q_{div} \\\\)\\n\\n6. Experiments\\n\\nIn this section, we introduce the datasets and evaluation metrics first. Then the quantitative, qualitative analysis, and ablation analysis are provided. Implementation details, additional results, limitations, and future work are provided in the supplementary material.\\n\\n6.1. Datasets\\n\\nWe evaluate our method on Human3.6M \\\\[26\\\\] and HumanEva-I dataset \\\\[51\\\\] and use identical settings with the other baselines. Human3.6M dataset consists of 11 subjects and 3.6 million video frames. There are 15 actions for each subject. The human motion is recorded at 50Hz. We adopt a 17-joint skeleton representation in our work. We use five subjects (S1, S5, S6, S7, S8) for training and testing with the other two subjects (S9 and S11). The predicted future motion horizon is 2 seconds (100 time steps), and the historical motion horizon is 0.5 seconds (25 time steps). HumanEva-I dataset includes three subjects. The record rate of human motion is 60Hz. We choose to use the 15-joint skeleton representation. We use the same training and testing datasets which are provided by the official website. We predict future motion for 1 second (60 time steps) with 0.25 seconds (15 time steps) observation.\\n\\n6.2. Evaluation Metrics\\n\\nThe following metrics are used to evaluate the performance of methods. For accuracy, we use Average Displacement Error (ADE) which is defined as the average Euclidean distance over the prediction time steps between the ground truth motion \\\\( X_{t+1:t+T_f} \\\\) and the closest sample \\\\[62\\\\], and Final Displacement Error (FDE), which is the Euclidean distance between the final ground truth pose and the final predicted pose, i.e., \\\\( \\\\min_i \\\\| \\\\hat{X}_{i_{t+1:t+T_f}} - X_{t+T_f} \\\\| \\\\).\\n\\nFor diversity, we use Average Pairwise Distance (APD), which is the L2 distance between all pairs of motion samples, which is computed as \\\\( \\\\frac{1}{K(K-1)} \\\\sum_{i \\\\neq j} \\\\| \\\\hat{X}_{i_{t+1:t+T_f}} - \\\\hat{X}_{j_{t+1:t+T_f}} \\\\| \\\\).\\n\\n6.3. Quantitative Analysis\\n\\nWe compare our approach with several baselines in Table 1. The baselines include deterministic methods such as acLSTM \\\\[37\\\\] and ERD \\\\[21\\\\], probabilistic approaches such as MT-V AE \\\\[60\\\\] and Dlow \\\\[62\\\\], etc. We use 50 samples to evaluate the prediction performance for all methods. We directly use the results of baselines from \\\\[62\\\\] and \\\\[64\\\\].\\n\\nIn Table 1 we can conclude that our method with \\\\( \\\\rho = 0.46 \\\\) can achieve a better performance compared with the other baselines in terms of all the metrics. In general, probabilistic methods such as Best-of-Many and GMV AE can achieve better accuracy and diversity than deterministic ones such as acLSTM and ERD. We can observe that most of the methods will have worse ADE and FDE if the APD...\"}"}
{"id": "CVPR-2022-1216", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":true,\"natural_text\":\"Table 1. Quantitative results on Human3.6M and HumanEva-I dataset. Our results and the best results of baselines are highlighted.\\n\\n| Dataset          | APD (\u2191) | ADE (\u2193) | FDE (\u2193) |\\n|------------------|---------|---------|---------|\\n| Human3.6M        | 0       | 0.403   | 0.414   |\\n| HumanEva-I       | 2.308   | 0.234   | 0.228   |\\n\\n6.3. Qualitative Analysis\\n\\nWe illustrate 10 end poses of random samples generated from both accuracy prior function and diversity function in Figure 4a and 4b. The first row shows the samples from the accuracy prior function. We notice that most samples are similar to the ground truth, which represents that the accuracy sampler can generate the predicted future human motions with high accuracy. The second row shows the samples from the diversity sampler. We notice that the predicted poses from the diversity sampler have more different modes and are not similar to the samples generated from the accuracy sampler. It can be attributed to the second item in the diversity loss $L_{div}$ in Equation 9, where we encourage our diversity prior to generating dissimilar samples to the ones generated from the accuracy sampler. We also illustrate two samples of predicted human motion from both accuracy and diversity sampler for both datasets in Figure 4c and Figure 4d. We notice that the predicted time sequences are smooth. The samples from the accuracy sampler can be very accurate compared with the ground truth. More visualization results are provided in the supplementary material.\\n\\n6.4. Different Sampling Ratio\\n\\nIn Figure 5, we illustrate the different metrics values with respect to the number of samples $n_{acc}$ generated from the accuracy sampler during testing. When $n_{acc}$ equals 0, it means that we only sample from the diversity prior distribution. We can see that ADE and FDE increase since the diversity sampler is designed to focus on exploring more different possible modes instead of matching the likelihood of data. Hence, we observe that APD can achieve around 18 when $n_{acc}$ = 0. When $n_{acc}$ increases, we observe that both the accuracy metrics (ADE and FDE) and diversity metric (APD) decrease. The accuracy metrics decrease slowly when the $n_{acc}$ is large enough. When $n_{acc}$ = 50, which means that all the samples are generated from the accuracy sampler, we...\"}"}
{"id": "CVPR-2022-1216", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"observe that APD decreases to around 6 and the accuracy metrics achieve the best performance. Figure 5. APD, ADE and FDE with respect to $n_{acc}$ on Human3.6M dataset. Red and brown bars indicate the accuracy metrics ADE and FDE. The blue bar indicates the APD.\\n\\nFigure 6. Visualization of predicted end poses of motions on Human3.6M dataset with different oracles. Figure 6a illustrates the performance of the predictor with oracle ($\\\\tau = 25$). Figure 6b illustrates the performance of the predictor with oracle ($\\\\tau = 100$). In each figure, the first row are the samples generated from accuracy prior function. The second row are the samples generated from the diversity prior function.\\n\\n6.6. Ablation Analysis\\nUsing Short-term Oracle Prediction Horizon\\nIn order to investigate whether dividing the prediction horizon into several short-term ones can help the predictor discover more possible modes, we evaluate our models with the oracles which have different prediction horizon length $\\\\tau$. We compare our framework supervised by a short-term oracle with $\\\\tau = 25$ and our framework supervised by the one with the full-length of prediction horizon, i.e., the prediction is not divided into short-term subsequences. We show the results of Human3.6M dataset in Figure 6. We can see that when $\\\\tau = 25$, short-term $n_{acc} = 50$ ADE \u2193 0.941 0.459 0.433 0.421 0.413 0.411 0.407 0.404 FDE \u2193 1.170 0.598 0.551 0.529 0.515 0.510 0.504 0.501 APD \u2191 18.79 18.16 17.14 15.73 14.04 12.02 9.265 5.927 $\\\\tau = 100$, non-short-term $n_{acc} = 50$ ADE \u2193 0.504 0.431 0.417 0.409 0.406 0.402 0.401 0.402 FDE \u2193 0.580 0.523 0.505 0.495 0.491 0.486 0.488 0.497 APD \u2191 7.346 7.397 7.360 7.234 6.997 6.683 6.255 5.651\\n\\nTable 2. The comparison with different $\\\\tau$ on Human3.6M dataset. using the oracle with $\\\\tau = 100$, i.e., the prediction horizon of the oracle is not short-term and the horizon is the same as the target prediction horizon, and the diversity is lower than the one which has $\\\\tau = 25$. It shows that the oracle with a short prediction horizon indeed increases the diversity. We also compare the different metrics of two models with different $\\\\tau$, and the results are summarized in Table 2. We notice that ADE and FDE with $n_{acc} = 50$ of both models with $\\\\tau = 100$ and $\\\\tau = 25$ are similar since all the samples are from the accuracy samplers. However, when $n_{acc}$ decreases, we observe that the diversity of the model supervised by the oracle with $\\\\tau = 100$ does not increase so much. We also observed that ADE and FDE of the model supervised with oracle ($\\\\tau = 100$) does not change too much when $n_{acc}$ is greater than 28 and APD does not change too much when $n_{acc}$ is smaller than 14. It is reasonable since the model supervised by the oracle with $\\\\tau = 100$ only explores limited and less possible diverse modes than the one supervised by the oracle with $\\\\tau = 25$. It also supports our suggestion that the short-term oracle indeed helps the predictor discover more possible future motions meanwhile the accuracy of prediction is maintained.\\n\\n7. Conclusion\\nIn this work, we propose a multi-objective diverse human motion prediction framework, which can enable adjustable sampling during the testing time. In order to enhance the diversity of predicted poses, we introduce a short-term oracle to instruct the predictor to discover more diverse possible modes of future poses. Such a framework overcomes the trade-off between likelihood sampling and diversity sampling. Thanks to both the multi-objective structure and short-term oracle, Our proposed approach achieves state-of-the-art performance in terms of accuracy and diversity. The experiment results and ablation studies demonstrate the effectiveness of the proposed method. Several future directions could be investigated. First, since our proposed approach is a general framework, more complicated structures such as graph neural network and transformer can be incorporated. Second, we currently assume that the horizon of short-term oracle is fixed. How to dynamically decide the short-term horizon will be the future work.\"}"}
{"id": "CVPR-2022-1216", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\n\\nObtaining accurate and diverse human motion prediction is essential to many industrial applications, especially robotics and autonomous driving. Recent research has explored several techniques to enhance diversity and maintain the accuracy of human motion prediction at the same time. However, most of them need to define a combined loss, such as the weighted sum of accuracy loss and diversity loss, and then decide their weights as hyperparameters before training. In this work, we aim to design a prediction framework that can balance the accuracy sampling and diversity sampling during the testing phase. In order to achieve this target, we propose a multi-objective conditional variational inference prediction model. We also propose a short-term oracle to encourage the prediction framework to explore more diverse future motions. We evaluate the performance of our proposed approach on two standard human motion datasets. The experiment results show that our approach is effective and on a par with state-of-the-art performance in terms of accuracy and diversity.\\n\\n1. Introduction\\n\\nHuman motion prediction plays a significant role in several applications such as human-robot interaction [3, 4], autonomous driving [13, 14, 33, 39], and animation [45]. For instance, an autonomous driving system can make a safe planning strategy given an accurate motion prediction of pedestrians. Moreover, robots can cooperate reasonably with people when they have a good understanding of human beings' future plans. However, since diversity and uncertainty are human future motion's intrinsic properties, it becomes a challenging problem in the computer science community. Unlike the vehicle trajectory prediction scenarios where we can get prior knowledge such as the traffic rules and routing information [40, 41] to constrain the different modes of trajectories, we can hardly get any prior knowledge about what humans will do in the future. Thus, we can only leverage the information from the given dataset, which increases the difficulty of diverse human motion prediction.\\n\\nThere are two lines of research in this area. First, several works attempt to get an accurate human motion prediction without considering the diversity, such as [36] based on graph neural network and [56] based on recurrent neural network. On the other line, some research investigates how to increase the diversity of human motion prediction based on deep generative models [2, 47, 60, 63] or diverse sampling techniques [61]. Deep generative models such as variational autoencoder and generative adversarial network naturally capture the stochastic behaviors, while they may suffer from mode collapse problems. Otherwise, even if we assume that the generative models can capture the actual modes of trajectories, we can hardly get any prior knowledge about what humans will do in the future. Thus, we can only leverage the information from the given dataset, which increases the difficulty of diverse human motion prediction.\\n\\n\u2020 Work done during Hengbo and Ramtin\u2019s internship at Honda Research Institute.\"}"}
{"id": "CVPR-2022-1216", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"data distribution, the data distribution can still be very im-\\nbalanced and skewed, which makes that sampling the minor\\nmodes is challenging within a limited number of samples.\\nSeveral works \\\\[42, 61, 62\\\\] propose new losses to increase\\ndiversity while keeping the prediction natural and accurate.\\nIn \\\\[62\\\\], a multiple sampling function is designed to explic-\\ntly capture the different modes of the distribution based on\\na pre-trained conditional variational autoencoder. By using\\nthis pre-trained variational autoencoder, such methods can\\ncontrol the likelihood of predicted motion with a training\\nhyperparameter. In \\\\[5, 32, 42\\\\], they proposed generative\\nmodels to learn the distribution implicitly. However, these\\nworks still have to choose hyperparameters before training\\nto balance the likelihood and diversity sampling. It implies\\nthat such approaches cannot be adjusted and controlled dur-\\ning the testing phase. Considering the real-world applica-\\ntion such as pedestrian motion prediction in autonomous\\ndriving, we not only need to know most of the different pos-\\nsible modes of motion but also need to know which modes\\nwill most likely happen. It will be more practical if we can\\ndecide the balance of accuracy sampling and diversity sam-\\npling during the testing phase for the purpose of designing\\nthe risk-averse or risk-seeking planner of autonomous vehi-\\ncles. Hence, we introduce a multi-objective variational in-\\nference framework with two different priors. The proposed\\nstructure makes it possible to adjust the ratio between accu-\\nracy and diversity sampling during the testing time.\\nMeanwhile, since there is only one ground-truth future\\nmotion poses given a historical observation, several works\\n\\\\[49, 59\\\\] propose to use a similarity cluster-based technique\\nto get the multi-modal pseudo-ground-truth future motions.\\nSimilar initial poses are grouped, and their corresponding\\nfuture poses can be viewed as the pseudo possible future\\nmotions for each initial pose in the group. We argue that\\nsuch logic can also be applied recursively. We can group\\nsimilar poses again at certain steps and get the shared fu-\\ntures. A demonstration is shown in Figure 1. This strat-\\negy can boost the diversity of future motions. However, the\\nsampling number will exponentially increase due to the re-\\ncursive queries during training and make such direct imple-\\nmentation intractable. In order to solve this issue, we intro-\\nduce an oracle that provides several possible future motions\\nwith a short-term horizon to instruct the predictor repeat-\\ndly. To summarize, our contributions are three folds:\\n\u2022 We propose a unified multi-objective conditional varia-\\ntional autoencoder based human motion prediction frame-\\nwrok, which can adjust the ratio of sample numbers of ac-\\ncuracy and diversity sampling during testing.\\n\u2022 We propose to learn a short-term oracle system and dis-\\ntill the oracle's knowledge into the prediction framework to\\nincrease the diversity of human future motions. In order to\\nachieve this goal, we propose a novel sample-based loss to\\nsupervise the predictor during the training phase.\\n\\n2. Related Work\\n\\nHuman motion prediction.\\nThe human motion prediction has\\nbeen investigated with many different approaches in the\\ncomputer vision community. At the early stage, several\\nmethods \\\\[1, 10, 37, 43, 50, 54, 57\\\\] without deep learning tech-\\nniques are proposed such as Gaussian process \\\\[58\\\\], hid-\\nden Markov model \\\\[10\\\\], and latent variable models \\\\[54\\\\].\\nSuch methods can achieve good performance for recurrent\\nhuman motion data. However, they may not be suitable\\nfor more complicated irregular human motions. As sev-\\neral promising deep learning models such as recurrent neu-\\nral network (RNN) \\\\[6, 12, 18\\\\] and graph neural network\\n(GNN) \\\\[11, 30, 34, 35, 48\\\\] are proposed recently, there are\\nseveral research focusing on how to incorporate the models\\nabove to enhance the deterministic human motion predic-\\ntion accuracy. Several works such as \\\\[21, 27, 44, 46, 64\\\\] are\\nbased on RNN, and \\\\[36, 42\\\\] utilize graph neural network\\n(GNN) to capture both the temporal and spatial informa-\\ntion. In order to get more diverse human motion prediction,\\nseveral probabilistic models \\\\[2, 5, 7, 28, 29, 32, 38, 47, 63\\\\]\\ndiverse forecasting.\\n\\nIn \\\\[60\\\\], the authors propose an ap-\\nproach that can learn a representation for motion recon-\\nstruction and transformation together. Also, GAN-like\\nmodels are utilized in \\\\[5, 32\\\\] to capture the diverse human\\nmotion prediction. There are also some research using a dif-\\nferent representation to improve the diversity \\\\[64\\\\]. In \\\\[61\\\\],\\na diversity sampling function which is formulated as a de-\\nterminantal point process \\\\[22, 23, 31\\\\] is proposed. Espe-\\ncially in \\\\[62\\\\], the authors argue that even though the ex-\\nisted likelihood-based methods can have a good estimation\\nof the data, they can still be challenging to sample some mi-\\nnor modes given a fixed number of samples. Hence, they\\npropose to learn another diversity sampling function that\\ncan generate diverse motions based on one pre-trained vari-\\ntational autoencoder model. However, the proposed model\\nneeds to choose hyperparameters to balance the likelihood\\nand diversity before training. We investigate the diverse hu-\\nman motion prediction in an orthogonal direction with the\\nrelated work. We aim to get a unified model that can ad-\\njust the sample number ratio between accuracy and diversity\\nsamples during the testing phase. Besides, we attempt to ex-\\nplore more diverse and natural modes by utilizing pseudo\\nfuture motions with a short-term oracle, and any models\\nmentioned above can be integrated.\"}"}
{"id": "CVPR-2022-1216", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Overview of the proposed framework. Red lines indicate the pipeline, which are only used during training. Blue lines indicate the pipeline used in both the training and testing phase. During training, several samples are generated from both accuracy prior function (red diamond, Section 4.1.1) and diversity prior function (blue diamond, Section 4.1.2). The accuracy prior function will be only updated by accuracy sampler loss defined in Section 4.1.1. The diversity prior function will be updated by the diversity sampler loss, which depends on all the samples. The short-term oracle function is introduced in Section 4.2.\\n\\n3. Problem Formulation\\n\\nOur goal is to predict the possible future human motions given a dataset $D$. We denote the human motion with time horizon $T = T_h + T_f$ as $X_{t-T_h+1}^{t+T_f}$, where $X_t \\\\in \\\\mathbb{R}^d$ is the human joints Cartesian coordinates at time step $t$. $T_h$ and $T_f$ are the historical horizon and future horizon respectively. Given an observation $C = X_{t-T_h+1}^{t}$, we intend to get the future motion distribution $P(X_{t+1}^{t+T_f} | C, \\\\rho)$. Since such conditional probabilistic distribution may have several dominant modes, it is difficult to sample the other modes given a fixed sampling number. In contrast, if we focus on increasing the diversity of the samples, the prediction accuracy will be undermined. In this work, we introduce a variable $\\\\rho \\\\in [0, 1]$ to control the degree of diversity of prediction, i.e., we intend to get $M$ samples $X_{i,t+1}^{t+T_f} \\\\sim P(X_{t+1}^{t+T_f} | C, \\\\rho), i = 1, ..., M$. The larger $\\\\rho$ is, the more diverse samples will be generated and focuses on the rare cases, and the smaller $\\\\rho$ is, the prediction will focus more on the most likely modes. For simplicity, we use $X$ to represent $X_{t+1}^{t+T_f}$ in the case that the time step index is not necessary.\\n\\n4. Methodology\\n\\nWe first introduce the multi-objective generative prediction framework based on conditional variational inference. Then we introduce the proposed short-term oracle, which provides multi-modal supervision to the prediction framework. Finally, we introduce our proposed approach's training strategy and testing procedure. The overall framework is illustrated in Figure 2.\\n\\n4.1. Multi-Objective Predictor\\n\\nIn general, we can represent a probabilistic distribution via a latent variable model:\\n\\n$$P(X | C; Q) = \\\\mathbb{E}_{Z \\\\sim Q(Z | C)}[P(X | C, Z)],$$\\n\\n(1)\\n\\nwhere $Q(Z | C)$ is the conditional prior distribution of latent variable $Z \\\\in \\\\mathbb{R}^{d_z}$ whose dimension is $d_z$. $P(X | C, Z)$ is defined as the conditional likelihood given the observation information $C$ and latent variable $Z$. We can vary the prior distribution $Q$ to achieve different distributions of $X$ given the same observation $C$. In our proposed approach, we introduce two different prior distributions $Q_{\\\\text{acc}}(Z | C)$ and $Q_{\\\\text{div}}(Z | C)$. We intend to estimate the data distribution $P_D$ using $P(X | C; Q_{\\\\text{acc}}(Z | C))$ with prior $Q_{\\\\text{acc}}(Z | C)$, and get the most diverse distribution which mainly focuses on the minor modes by sampling from $Q_{\\\\text{div}}(Z | C)$. The overall framework is illustrated in Figure 2. Similar to [62], we define the historical observation encoder $e_h(C)$ and future information encoder $e_f(X)$ as:\\n\\n$$e_h(C) = \\\\left[ \\\\text{MLP} \\\\circ \\\\text{RNN} \\\\right](C),$$\\n\\n$$e_f(X) = \\\\left[ \\\\text{MLP} \\\\circ \\\\text{RNN} \\\\right](X),$$\\n\\n(2)\\n\\nwhere we first encode the temporal information of trajectories by using a recurrent neural network (RNN) and then use a forward neural network to map the states of RNN to the feature embedding space. Based on the historical embedding $e_h(C)$ and latent variable $Z$, we denote the decoder function $d_{\\\\theta}(X | C, Z)$ as:\\n\\n$$d_{\\\\theta}(X | C, Z) = \\\\left[ \\\\text{MLP} \\\\circ \\\\text{RNN} \\\\right](e_h(C) || Z),$$\\n\\n(3)\\n\\nwhere $\\\\theta$ is the parameter of the decoder. In general, the outputs of the decoder are the parameters of a probabilistic distribution, e.g., the mean and variance of a Gaussian distribution. In this work, we use a deterministic decoder, and the output of the decoder is the predicted poses. For convenience, the output of the decoder is also denoted by Equation 3. The randomness of the decoder is only dependent on $Z$. \u201c$||$\u201d represents the concatenate operator of two vectors. We use a similar neural network structure for the decoder with the encoders. The details of the operator $\\\\circ$ are in the supplementary material.\"}"}
{"id": "CVPR-2022-1216", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"for the accuracy sampler is:\\n\\n\\\\[ L_{\\\\text{div}}(\\\\phi, \\\\psi) = \\\\sum_{i,s} d(\\\\hat{z}_i, z_{st})^{\\\\alpha} \\\\]\\n\\nwhere \\\\( \\\\alpha \\\\) is a parameter to determine the sensitivity of the distance metric. We define the diversity loss as:\\n\\n\\\\[ \\\\text{DIV} = \\\\frac{1}{N_o} \\\\sum_{i,s} d(\\\\hat{z}_i, z_{st}) \\\\]\\n\\n\\\\( N_o \\\\) represents the number of samples generated from diversity sampler. \\\\( d \\\\) is a metric defined in the Euclidean space. A larger value of \\\\( \\\\alpha \\\\) means that we require more different.\\n\\nWe define the metric as\\n\\n\\\\[ d(x, y) = \\\\| x - y \\\\|_2 \\\\]\\n\\nOnly using the diversity loss is not enough to get a realistic prediction since it is possible to increase diversity in the wrong way. For instance, one model can generate random samples from the accuracy sampler and then append them to the diversity sampler. Hence, we need to introduce a constraint. In the previous works, we used human motion in the data to constrain the prediction. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to prevent the collapse problems for conditional variational inference, we apply the variational inference to maximize the evidence lower bound (ELBO) of the log-likelihood:\\n\\n\\\\[ \\\\text{ELBO}(\\\\lambda) = \\\\mathbb{E}_{Q_{\\\\phi}} \\\\left[ \\\\log p_y(x \\\\mid z, \\\\lambda) \\\\right] - \\\\mathbb{D}_{KL}(Q_{\\\\phi} \\\\mid \\\\mid P_{\\\\psi}) \\\\]\\n\\nwhere \\\\( p_y(x \\\\mid z, \\\\lambda) \\\\) is the conditional distribution, and \\\\( P_{\\\\psi} \\\\) is the prior distribution. Since we intend to disentangle the accurate influence on the accuracy sampler to approximate the data distribution. We utilize a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain the prior distribution, we use the best-of-\\n\\n\\\\[ L_{\\\\text{acc}}(\\\\phi, \\\\psi) = \\\\sum_{i,s} \\\\left[ \\\\log p_y(x \\\\mid z, \\\\lambda) - \\\\mathbb{D}_{KL}(Q_{\\\\phi} \\\\mid \\\\mid P_{\\\\psi}) \\\\right] \\\\]\\n\\nThe first objective is to infer the accuracy prior distribution with parameter \\\\( \\\\lambda \\\\).\\n\\n\\\\[ \\\\log p_y(x \\\\mid z, \\\\lambda) = -\\\\frac{1}{2} \\\\left( x - \\\\mu \\\\right)^T \\\\Sigma^{-1} \\\\left( x - \\\\mu \\\\right) - \\\\frac{1}{2} \\\\log \\\\left| \\\\Sigma \\\\right| - \\\\frac{1}{2} N \\\\log 2 \\\\pi \\\\]\\n\\nwhere \\\\( \\\\mu \\\\) and \\\\( \\\\Sigma \\\\) are the mean and covariance of the Gaussian distribution, and \\\\( N \\\\) is the number of samples. Then the overall loss is\\n\\n\\\\[ L = L_{\\\\text{acc}}(\\\\phi, \\\\psi) + \\\\lambda L_{\\\\text{div}}(\\\\phi, \\\\psi) \\\\]\\n\\nwhere \\\\( \\\\lambda \\\\) are used to balance two losses.\\n\\nIn order to explore the different modes of possible future poses, we propose to learn another prior distribution. Those works argue that using a universal prior distribution, i.e., an independent isotropic Gaussian distribution, may not be a good choice for conditional distribution estimation [8, 52]. It is difficult to capture complex conditional information [8, 52]. There are some works [8, 52, 55, 65] investigating the collapse problems for conditional variational inference. We utilize a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isotropic Gaussian distribution, we model \\\\( Q_{\\\\text{div}} \\\\) as a Gaussian distribution and it will increase the difficulty of training. In order to constrain each generated poses from the diversity sampler, we use a common distribution and introduce strong model bias resulting in missing modes [53, 55, 65]. Hence, instead of using an isot"}
