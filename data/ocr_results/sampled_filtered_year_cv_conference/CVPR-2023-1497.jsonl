{"id": "CVPR-2023-1497", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"OMNI3D: A Large Benchmark and Model for 3D Object Detection in the Wild\\n\\nGarrick Brazil 1\\nAbhinav Kumar 2\\nJulian Straub 1\\nNikhila Ravi 1\\nJustin Johnson 1\\nGeorgia Gkioxari 3\\n\\n1 Meta AI\\n2 Michigan State University\\n3 Caltech\\n\\nDataset Size\\n\\n| Dataset       | Cube R-CNN Predictions on COCO |\\n|---------------|-------------------------------|\\n| KITTI         | 30.9                          |\\n| ARKitScenes   |                               |\\n| SUN RGB-D     | 34.7                          |\\n| Objectron     |                               |\\n| Hypersim      |                               |\\n| nuScenes      | 10k                           |\\n| OMNI3D        | 234k                          |\\n\\nAP\\n\\n| Dataset       | OMNI3D |\\n|---------------|--------|\\n| Cube R-CNN    |        |\\n| ImVoxelNet    |        |\\n| GUPNet        |        |\\n| PGD           |        |\\n\\nFigure 1. Left: We introduce OMNI3D, a benchmark for 3D object detection which is larger and more diverse than popular 3D benchmarks. Right: We propose Cube R-CNN, which generalizes to unseen datasets (e.g. COCO [45]) and outperforms prior works on existing datasets.\\n\\nAbstract\\n\\nRecognizing scenes and objects in 3D from a single image is a longstanding goal of computer vision with applications in robotics and AR/VR. For 2D recognition, large datasets and scalable solutions have led to unprecedented advances. In 3D, existing benchmarks are small in size and approaches specialize in few object categories and specific domains, e.g. urban driving scenes. Motivated by the success of 2D recognition, we revisit the task of 3D object detection by introducing a large benchmark, called OMNI3D. OMNI3D re-purposes and combines existing datasets resulting in 234k images annotated with more than 3 million instances and 98 categories. 3D detection at such scale is challenging due to variations in camera intrinsics and the rich diversity of scene and object types. We propose a model, called Cube R-CNN, designed to generalize across camera and scene types with a unified approach. We show that Cube R-CNN outperforms prior works on the larger OMNI3D and existing benchmarks. Finally, we prove that OMNI3D is a powerful dataset for 3D object recognition and show that it improves single-dataset performance and can accelerate learning on new smaller datasets via pre-training.\\n\\n1. Introduction\\n\\nUnderstanding objects and their properties from single images is a longstanding problem in computer vision with applications in robotics and AR/VR. In the last decade, 2D object recognition [26, 40, 63, 64, 72] has made tremendous advances toward predicting objects on the image plane with the help of large datasets [24, 45]. However, the world and its objects are three dimensional laid out in 3D space. Perceiving objects in 3D from 2D visual inputs poses new challenges framed by the task of 3D object detection. Here, the goal is to estimate a 3D location and 3D extent of each object in an image in the form of a tight oriented 3D bounding box.\\n\\nToday 3D object detection is studied under two different lenses: for urban domains in the context of autonomous vehicles [7, 13, 50, 52, 57] or indoor scenes [31, 36, 58, 73]. Despite the problem formulation being shared, methods share little insights between domains. Often approaches are tailored to work only for the domain in question. For instance, urban methods make assumptions about objects resting on a ground plane and model only yaw angles for 3D rotation. Indoor techniques may use a confined depth range (e.g. up to 6m in [58]). These assumptions are generally not true in the real world. Moreover, the most popular benchmarks for image-based 3D object detection are small. Indoor SUN RGB-D [70] has 10k images, urban KITTI [21] has 7k images; 2D benchmarks like COCO [45] are 20\u00d7 larger.\\n\\nWe release the OMNI3D benchmark and Cube R-CNN models at https://github.com/facebookresearch/omni3d.\"}"}
{"id": "CVPR-2023-1497", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We address the absence of a general large-scale dataset for 3D object detection by introducing a large and diverse 3D benchmark called OMNI3D. OMNI3D is curated from publicly released datasets, SUN RGB-D [70], ARKitScenes [6], Hypersim [65], Objectron [2], KITTI [21], and nuScenes [9], and comprises 234k images with 3 million objects annotated with 3D boxes across 98 categories including chair, sofa, laptop, table, cup, shoes, pillow, books, car, person, etc.\\n\\nSec. 3 describes the curation process which involves re-purposing the raw data and annotations from the aforementioned datasets, which originally target different applications. As shown in Fig. 1, OMNI3D is $20 \\\\times$ larger than existing popular benchmarks used for 3D detection, SUN RGB-D and KITTI. For efficient evaluation on the large OMNI3D, we introduce a new algorithm for intersection-over-union of 3D boxes which is $450 \\\\times$ faster than previous solutions [2].\\n\\nWe empirically prove the impact of OMNI3D as a large-scale dataset and show that it improves single-dataset performance by up to 5.3% AP on urban and 3.8% on indoor benchmarks.\\n\\nOn the large and diverse OMNI3D, we design a general and simple 3D object detector, called Cube R-CNN, inspired by advances in 2D and 3D recognition of recent years [22, 52, 64, 68]. Cube R-CNN detects all objects and their 3D location, size and rotation end-to-end from a single image of any domain and for many object categories. Attributed to OMNI3D's diversity, our model shows strong generalization and outperforms prior works for indoor and urban domains with one unified model, as shown in Fig. 1. Learning from such diverse data comes with challenges as OMNI3D contains images of highly varying focal lengths which exaggerate scale-depth ambiguity (Fig. 4). We redy this by operating on virtual depth which transforms object depth with the same virtual camera intrinsics across the dataset. An added benefit of virtual depth is that it allows the use of data augmentations (e.g. image rescaling) during training, which is a critical feature for 2D detection [12, 80], and as we show, also for 3D. Our approach with one unified design outperforms prior best approaches in AP$_{3D}$, ImVoxel-Net [66] by 4.1% on indoor SUN RGB-D, GUPNet [52] by 9.5% on urban KITTI, and PGD [77] by 7.9% on OMNI3D.\\n\\nWe summarize our contributions:\\n\\n\u2022 We introduce OMNI3D, a benchmark for image-based 3D object detection sourced from existing 3D datasets, which is $20 \\\\times$ larger than existing 3D benchmarks.\\n\\n\u2022 We implement a new algorithm for IoU of 3D boxes, which is $450 \\\\times$ faster than prior solutions.\\n\\n\u2022 We design a general-purpose baseline method, Cube R-CNN, which tackles 3D object detection for many categories and across domains with a unified approach. We propose virtual depth to eliminate the ambiguity from varying camera focal lengths in OMNI3D.\\n\\n2. Related Work\\n\\nCube R-CNN and OMNI3D draw from key research advances in 2D and 3D object detection.\\n\\n2D Object Detection.\\n\\nHere, methods include two-stage approaches [26, 64] which predict object regions with a region proposal network (RPN) and then refine them via an MLP. Single-stage detectors [44, 47, 63, 72, 85] omit the RPN and predict regions directly from the backbone.\\n\\n3D Object Detection.\\n\\nMonocular 3D object detectors predict 3D cuboids from single input images. There is extensive work in the urban self-driving domain where the car class is at the epicenter [13, 19, 23, 25, 30, 46, 49, 54, 57, 62, 68, 69, 76, 78, 84]. CenterNet [85] predicts 3D depth and size from fully-convolutional center features, and is extended by [14, 37, 42, 48, 50\u201352, 55, 83, 84, 87]. M3D-RPN [7] trains an RPN with 3D anchors, enhanced further by [8, 18, 38, 75, 88]. FCOS3D [78] extends the anchorless FCOS [72] detector to predict 3D cuboids. Its successor PGD [77] furthers the approach with probabilistic depth uncertainty. Others use pseudo depth [4, 15, 53, 59, 79, 81] and explore depth and point-based LiDAR techniques [35, 60]. Similar to ours, [11, 68, 69] add a 3D head, specialized for urban scenes and objects, on two-stage Faster R-CNN. [25, 69] augment their training by synthetically generating depth and box-fitted views, coined as virtual views or depth. In our work, virtual depth aims at addressing varying focal lengths.\\n\\nFor indoor scenes, a vast line of work tackles room layout estimation [17, 28, 41, 56]. Huang et al [31] predict 3D oriented bounding boxes for indoor objects. Factored3D [73] and 3D-RelNet [36] jointly predict object voxel shapes. Total3D [58] predicts 3D boxes and meshes by additionally training on datasets with annotated 3D shapes. ImVoxel-Net [66] proposes domain-specific methods which share an underlying framework for processing volumes of 3D voxels. In contrast, we explore 3D object detection in its general form by tackling outdoor and indoor domains jointly in a single model and with a vocabulary of $5 \\\\times$ more categories.\\n\\n3D Datasets.\\n\\nKITTI [21] and SUN RGB-D [70] are popular datasets for 3D object detection on urban and indoor scenes respectively. Since 2019, 3D datasets have emerged, both for indoor [2, 3, 6, 16, 65] and outdoor [9, 10, 20, 29, 32, 33, 71]. In isolation, these datasets target different tasks and applications and have unique properties and biases, e.g. object and scene types, focal length, coordinate systems, etc. In this work, we unify existing representative datasets [2, 6, 9, 21, 65, 70]. We process the raw visual data, re-purpose their annotations, and carefully curate the union of their semantic labels in order to build a coherent large-scale benchmark, called OMNI3D. OMNI3D is $20 \\\\times$ larger than widely-used benchmarks and notably more diverse. As such, new challenges arise stemming from the increased variance in visual domain, object rotation, size, layouts, and camera intrinsics.\"}"}
{"id": "CVPR-2023-1497", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The primary benchmarks for 3D object detection are small, focus on a few categories and are of a single domain. For instance, the popular KITTI [21] contains only urban scenes, has 7k images and 3 categories, with a focus on car. SUN RGB-D [70] has 10k images. The small size and lack of variance in 3D datasets is a stark difference to 2D counterparts, such as COCO [45] and LVIS [24], which have pioneered progress in 2D recognition.\\n\\nWe aim to bridge the gap to 2D by introducing OMNI3D, a large-scale and diverse benchmark for image-based 3D object detection consisting of 234k images, 3 million labeled 3D bounding boxes, and 98 object categories. We source from recently released 3D datasets of urban (nuScenes [9] and KITTI [21]), indoor (SUN RGB-D [70], ARKitScenes [6] and Hypersim [65]), and general (Objectron [2]) scenes. Each of these datasets target different applications (e.g. point-cloud recognition or reconstruction in [6], inverse rendering in [65]), provide visual data in different forms (e.g. videos in [2, 6], rig captures in [9]) and annotate different object types. To build a coherent benchmark, we process the varying raw visual data, re-purpose their annotations to extract 3D cuboids in a unified 3D camera coordinate system, and carefully curate the final vocabulary. More details about the benchmark creation in the Appendix.\\n\\nWe analyze OMNI3D and show its rich spatial and semantic properties proving it is visually diverse, similar to 2D data, and highly challenging for 3D as depicted in Fig. 2.\\n\\nWe show the value of OMNI3D for the task of 3D object detection with extensive quantitative analysis in Sec. 5.\\n\\n3.1. Dataset Analysis\\nSplits.\\nWe split the dataset into 175k/19k/39k images for train/val/test respectively, consistent with original splits when available, and otherwise free of overlapping video sequences in splits. We denote indoor and outdoor subsets as OMNI3D_IN (SUN RGB-D, Hypersim, ARKit), and OMNI3D_OUT (KITTI, nuScenes). Objectron, with primarily close-up objects, is used only in the full OMNI3D setting.\\n\\nLayout statistics.\\nFig. 2(a) shows the distribution of object centers onto the image plane by projecting centroids on the XY-plane (top row), and the distribution of object depths by projecting centroids onto the XZ-plane (bottom row). We find that OMNI3D's spatial distribution has a center bias, similar to 2D datasets COCO and LVIS. Fig. 2(b) depicts the relative object size distribution, defined as the square root of object area divided by image area. Objects are more likely to be small in size similar to LVIS (Fig. 6c in [24]) suggesting that OMNI3D is also challenging for 2D detection, while objects in OMNI3D_OUT are noticeably smaller.\\n\\nThe bottom row of Fig. 2(a) normalizes object depth in a [0, 20]m range, chosen for visualization and satisfies 88% of object instances; OMNI3D depth ranges as far as 300 m.\\n\\nWe observe that the OMNI3D depth distribution is far more diverse than SUN RGB-D and KITTI, which are biased toward near or road-side objects respectively. See Appendix for each data source distribution plot. Fig. 2(a) demonstrates OMNI3D's rich diversity in spatial distribution and depth which suffers significantly less bias than existing 3D benchmarks and is comparable in complexity to 2D datasets.\\n\\n2D and 3D correlation.\\nA common assumption in urban scenes is that objects rest on a ground plane and appear smaller with depth. To verify if that is true generally, we compute correlations. We find that 2D y and 3D z are indeed fairly correlated in OMNI3D_OUT at 0.524, but significantly less in OMNI3D_IN at 0.006. Similarly, relative 2D object size ($\\\\sqrt{h \\\\cdot w}$) and z correlation is 0.543 and 0.102 respectively.\\n\\nThis confirms our claim that common assumptions in urban scenes are not generally true, making the task challenging.\\n\\nCategory statistics.\\nFig. 2(c) plots the distribution of instances across the 98 categories of OMNI3D. The long-tail suggests that low-shot recognition in both 2D and 3D will be critical for performance. In this work, we want to focus on large-scale and diverse 3D recognition, which is comparably unexplored. We therefore filter and focus on categories with at least 1000 instances. This leaves 50 diverse categories including chair, sofa, laptop, table, books, car, truck, pedestrian and more, 19 of which have more than 10k instances. We provide more per-category details in the Appendix.\"}"}
{"id": "CVPR-2023-1497", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Overview. Cube R-CNN takes as input an RGB image, detects all objects in 2D and predicts their 3D cuboids $B_{3D}$. During training, the 3D corners of the cuboids are compared against 3D ground truth with the point-cloud chamfer distance.\\n\\n4. Method\\n\\nOur goal is to design a simple and effective model for general 3D object detection. Hence, our approach is free of domain or object specific strategies. We design our 3D object detection framework by extending Faster R-CNN [64] with a 3D object head which predicts a cuboid per each detected 2D object. We refer to our method as Cube R-CNN.\\n\\n4.1. Cube R-CNN\\n\\nOur model builds on Faster R-CNN [64], an end-to-end region-based object detection framework. Faster-RCNN consists of a backbone network, commonly a CNN, which embeds the input image into a higher-dimensional feature space. A region proposal network (RPN) predicts regions of interest (RoIs) representing object candidates in the image. A 2D box head inputs the backbone feature map and processes each RoI to predict a category and a more accurate 2D box. Faster R-CNN can be easily extended to tackle more tasks by adding task-specific heads, e.g. Mask R-CNN [26] adds a mask head to additionally output object silhouettes.\\n\\nFor the task of 3D object detection, we extend Faster R-CNN by introducing a 3D detection head which predicts a 3D cuboid for each detected 2D object. Cube R-CNN extends Faster R-CNN in three ways: (a) we replace the binary classifier in RPN which predicts region objectness with a regressor that predicts IoUness, (b) we introduce a cube head which estimates the parameters to define a 3D cuboid for each detected object (similar in concept to [68]), and (c) we define a new training objective which incorporates a virtual depth for the task of 3D object detection.\\n\\nIoUness.\\n\\nThe role of RPN is two-fold: (a) it proposes RoIs by regressing 2D box coordinates from pre-computed anchors and (b) it classifies regions as object or not (objectness). This is sensible in exhaustively labeled datasets where it can be reliably assessed if a region contains an object. However, O&MNI 3D combines many data sources with no guarantee that all instances of all classes are labeled. We overcome this by replacing objectness with IoUness, applied only to foreground. Similar to [34], a regressor predicts IoU between a RoI and a ground truth. Let $o$ be the predicted IoU for a RoI and $\\\\hat{o}$ be the 2D IoU between the region and its ground truth; we apply a binary cross-entropy (CE) loss $L_{IoUness} = \\\\ell_{CE}(o, \\\\hat{o})$. We train on regions whose IoU exceeds 0.05 with a ground truth in order to learn IoUness from a wide range of region overlaps. Thus, the RPN training objective becomes $L_{RPN} = \\\\hat{o} \\\\cdot (L_{IoUness} + L_{reg})$, where $L_{reg}$ is the 2D box regression loss from [64]. The loss is weighted by $\\\\hat{o}$ to prioritize candidates close to true objects.\\n\\nCube Head.\\n\\nWe extend Faster R-CNN with a new head, called cube head, to predict a 3D cuboid for each detected 2D object. The cube head inputs $7 \\\\times 7$ feature maps pooled from the backbone for each predicted region and feeds them to 2 fully-connected (FC) layers with 1024 hidden dimensions. All 3D estimations in the cube head are category-specific. The cube head represents a 3D cuboid with 13 parameters each predicted by a final FC layer:\\n\\n- $\\\\{u, v\\\\}$ represent the projected 3D center on the image plane relative to the 2D RoI\\n- $z \\\\in \\\\mathbb{R}^+$ is the object's center depth in meters transformed from virtual depth $z_v$ (explained below)\\n- $\\\\{\\\\bar{w}, \\\\bar{h}, \\\\bar{l}\\\\} \\\\in \\\\mathbb{R}^3$ are the log-normalized physical box dimensions in meters\\n- $p \\\\in \\\\mathbb{R}^6$ is the continuous 6D allocentric rotation\\n- $\\\\mu \\\\in \\\\mathbb{R}^+$ is the predicted 3D uncertainty\\n\\nThe above parameters form the final 3D box in camera view coordinates for each detected 2D object. The object's 3D center $X$ is estimated from the predicted 2D projected...\"}"}
{"id": "CVPR-2023-1497", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] KITTI server. https://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d. Accessed: 2022-11-01.\\n\\n[2] Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianying Wei, and Matthias Grundmann. Objectron: A large scale dataset of object-centric videos in the wild with pose annotations. CVPR, 2021.\\n\\n[3] Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X Chang, and Matthias Nie\u00dfner. Scan2CAD: Learning CAD model alignment in RGB-D scans. In Proceedings of the IEEE/CVF Conference on computer vision and pattern recognition, pages 2614\u20132623, 2019.\\n\\n[4] Wentao Bao, Bin Xu, and Zhenzhong Chen. MonoFENet: Monocular 3D object detection with feature enhancement networks. TIP, 2019.\\n\\n[5] Bradford Barber, David Dobkin, and Hannu Huhdanpaa. The quickhull algorithm for convex hulls. ACM Transactions on Mathematical Software (TOMS), 1996.\\n\\n[6] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer, Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman. ARKitScenes - a diverse real-world dataset for 3D indoor scene understanding using mobile RGB-D data. In NeurIPS Datasets and Benchmarks Track, 2021.\\n\\n[7] Garrick Brazil and Xiaoming Liu. M3D-RPN: Monocular 3D region proposal network for object detection. In ICCV, 2019.\\n\\n[8] Garrick Brazil, Gerard Pons-Moll, Xiaoming Liu, and Bernt Schiele. Kinematic 3D object detection in monocular video. In ECCV, 2020.\\n\\n[9] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuScenes: A multimodal dataset for autonomous driving. In CVPR, 2020.\\n\\n[10] Ming-Fang Chang, John Lambert, Patsorn Sangkloy, Jagjeet Singh, Slawomir Bak, Andrew Hartnett, Dea Wang, Peter Carr, Simon Lucey, Deva Ramanan, et al. Argoverse: 3D tracking and forecasting with rich maps. In CVPR, 2019.\\n\\n[11] Hansheng Chen, Yuyao Huang, Wei Tian, Zhong Gao, and Lu Xiong. MonoRUn: Monocular 3D object detection by reconstruction and uncertainty propagation. In CVPR, 2021.\\n\\n[12] Kai Chen, Jiaqi Wang, Jiangmiao Pang, Yuhang Cao, Yu Xiong, Xiaoxiao Li, Shuyang Sun, Wansen Feng, Ziwei Liu, Jiarui Xu, Zheng Zhang, Dazhi Cheng, Chenchen Zhu, Tianheng Cheng, Qijie Zhao, Buyu Li, Xin Lu, Rui Zhu, Yue Wu, Jifeng Dai, Jingdong Wang, Jianping Shi, Wanli Ouyang, Chen Change Loy, and Dahua Lin. MMDetection: Open mmlab detection toolbox and benchmark. arXiv preprint arXiv:1906.07155, 2019.\\n\\n[13] Xiaozhi Chen, Kaustav Kundu, Ziyu Zhang, Huimin Ma, Sanja Fidler, and Raquel Urtasun. Monocular 3D object detection for autonomous driving. In CVPR, 2016.\\n\\n[14] Yongjian Chen, Lei Tai, Kai Sun, and Mingyang Li. MonoPair: Monocular 3D object detection using pairwise spatial relationships. In CVPR, 2020.\\n\\n[15] Yi-Nan Chen, Hang Dai, and Yong Ding. Pseudo-stereo for monocular 3D object detection in autonomous driving. In CVPR, 2022.\\n\\n[16] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nie\u00dfner. ScanNet: Richly-annotated 3D reconstructions of indoor scenes. In CVPR, 2017.\\n\\n[17] Saumitro Dasgupta, Kuan Fang, Kevin Chen, and Silvio Savarese. Delay: Robust spatial layout estimation for cluttered indoor scenes. In CVPR, 2016.\\n\\n[18] Mingyu Ding, Yuqi Huo, Hongwei Yi, Zhe Wang, Jianping Shi, Zhiwu Lu, and Ping Luo. Learning depth-guided convolutions for monocular 3D object detection. In CVPRW, 2020.\\n\\n[19] Huan Fu, Mingming Gong, Chaohui Wang, Kayhan Batmanghelich, and Dacheng Tao. Deep Ordinal Regression Network for Monocular Depth Estimation. In CVPR, 2018.\\n\\n[20] Nils G\u00e4hler, Nicolas Jourdan, Marius Cordts, Uwe Franke, and Joachim Denzler. Cityscapes 3D: Dataset and benchmark for 9 DOF vehicle detection. In CVPRW, 2020.\\n\\n[21] Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the KITTI vision benchmark suite. In CVPR, 2012.\\n\\n[22] Georgia Gkioxari, Jitendra Malik, and Justin Johnson. Mesh R-CNN. In ICCV, 2019.\\n\\n[23] Jiaqi Gu, Bojian Wu, Lubin Fan, Jianqiang Huang, Shen Cao, Zhiyu Xiang, and Xian-Sheng Hua. Homography loss for monocular 3D object detection. In CVPR, 2022.\\n\\n[24] Agrim Gupta, Piotr Dollar, and Ross Girshick. LVIS: A dataset for large vocabulary instance segmentation. In CVPR, 2019.\\n\\n[25] Chenhang He, Jianqiang Huang, Xian-Sheng Hua, and Lei Zhang. Aug3D-RPN: Improving monocular 3D object detection by synthetic images with virtual depth. arXiv preprint arXiv:2107.13269, 2021.\\n\\n[26] Kaiming He, Georgia Gkioxari, Piotr Doll\u00e1r, and Ross Girshick. Mask R-CNN. In ICCV, 2017.\\n\\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\n[28] Varsha Hedau, Derek Hoiem, and David Forsyth. Recovering the spatial layout of cluttered rooms. In ICCV, 2009.\\n\\n[29] John Houston, Guido Zuidhof, Luca Bergamini, Yawei Ye, Long Chen, Ashesh Jain, Sammy Omari, Vladimir Iglovikov, and Peter Ondruska. One thousand and one hours: Self-driving motion prediction dataset. arXiv preprint arXiv:2006.14480, 2020.\\n\\n[30] Kuan-Chih Huang, Tsung-Han Wu, Hung-Ting Su, and Winston Hsu. MonoDTR: Monocular 3D object detection with depth-aware transformer. In CVPR, 2022.\\n\\n[31] Siyuan Huang, Siyuan Qi, Yinxue Xiao, Yixin Zhu, Ying Nian Wu, and Song-Chun Zhu. Cooperative holistic scene understanding: Unifying 3D object, layout, and camera pose estimation. In NeurIPS, 2018.\\n\\n[32] Xinyu Huang, Peng Wang, Xinjing Cheng, Dingfu Zhou, Qichuan Geng, and Ruigang Yang. The apolloscape open dataset for autonomous driving and its application. TPAMI, 2019.\"}"}
{"id": "CVPR-2023-1497", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"R. Kesten, M. Usman, J. Houston, T. Pandya, K. Nadhamuni, A. Ferreira, M. Yuan, B. Low, A. Jain, P. Ondruska, S. Omari, S. Shah, A. Kulkarni, A. Kazakova, C. Tao, L. Platinsky, W. Jiang, and V. Shet. Level 5 perception dataset 2020. https://level-5.global/level5/data/, 2019.\\n\\nDahun Kim, Tsung-Yi Lin, Anelia Angelova, In Kweon, and Weicheng Kuo. Learning open-world object proposals without learning to classify. RAL, 2022.\\n\\nJason Ku, Melissa Mozifian, Jungwook Lee, Ali Harakeh, and Steven Waslander. Joint 3D proposal generation and object detection from view aggregation. In IROS, 2018.\\n\\nNilesh Kulkarni, Ishan Misra, Shubham Tulsiani, and Abhinav Gupta. 3D-RelNet: Joint object and relational network for 3D prediction. In ICCV, 2019.\\n\\nAbhinav Kumar, Garrick Brazil, Enrique Corona, Armin Paruchami, and Xiaoming Liu. DEVIANT: Depth EquiVarIAnt NeTwork for monocular 3D object detection. In ECCV, 2022.\\n\\nAbhinav Kumar, Garrick Brazil, and Xiaoming Liu. GrooMeD-NMS: Grouped mathematically differentiable NMS for monocular 3D object detection. In CVPR, 2021.\\n\\nAbhijit Kundu, Yin Li, and James Rehg. 3D-RCNN: Instance-level 3D object reconstruction via render-and-compare. In CVPR, 2018.\\n\\nHei Law and Jia Deng. CornerNet: Detecting objects as paired keypoints. In ECCV, 2018.\\n\\nDavid C Lee, Martial Hebert, and Takeo Kanade. Geometric reasoning for single image structure recovery. In CVPR, 2009.\\n\\nPeixuan Li, Huaici Zhao, Pengfei Liu, and Feidao Cao. RTM3D: Real-time monocular 3D detection from object keypoints for autonomous driving. In ECCV, 2020.\\n\\nTsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In CVPR, 2017.\\n\\nTsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Focal loss for dense object detection. In ICCV, 2017.\\n\\nTsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.\\n\\nCe Liu, Shuhang Gu, Luc Van Gool, and Radu Timofte. Deep line encoding for monocular 3D object detection and depth prediction. In BMVC, 2021.\\n\\nWei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander Berg. SSD: Single shot multibox detector. In ECCV, 2016.\\n\\nXianpeng Liu, Nan Xue, and Tianfu Wu. Learning auxiliary monocular contexts helps monocular 3D object detection. In AAAI, 2022.\\n\\nYuxuan Liu, Yuan Yixuan, and Ming Liu. Ground-aware monocular 3D object detection for autonomous driving. RAL, 2021.\\n\\nZechen Liu, Zizhang Wu, and Roland T\u00f3th. SMOKE: Single-stage monocular 3D object detection via keypoint estimation. In CVPRW, 2020.\\n\\nZongdai Liu, Dingfu Zhou, Feixiang Lu, Jin Fang, and Liangjun Zhang. Autoshape: Real-time shape-aware monocular 3D object detection. In ICCV, 2021.\\n\\nYan Lu, Xinzhu Ma, Lei Yang, Tianzhu Zhang, Yating Liu, Qi Chu, Junjie Yan, and Wanli Ouyang. Geometry uncertainty projection network for monocular 3D object detection. In ICCV, 2021.\\n\\nXinzhu Ma, Shinan Liu, Zhiyi Xia, Hongwen Zhang, Xingyu Zeng, and Wanli Ouyang. Rethinking pseudo-lidar representation. In ECCV, 2020.\\n\\nXinzhu Ma, Zhihui Wang, Haojie Li, Pengbo Zhang, Wanli Ouyang, and Xin Fan. Accurate monocular 3D object detection via color-embedded 3D reconstruction for autonomous driving. In ICCV, 2019.\\n\\nXinzhu Ma, Yinmin Zhang, Dan Xu, Dongzhan Zhou, Shuai Yi, Haojie Li, and Wanli Ouyang. Delving into localization errors for monocular 3D object detection. In CVPR, 2021.\\n\\nArun Mallya and Svetlana Lazebnik. Learning informative edge maps for indoor scene layout prediction. In ICCV, 2015.\\n\\nArsalan Mousavian, Dragomir Anguelov, John Flynn, and Jana Kosecka. 3D bounding box estimation using deep learning and geometry. In CVPR, 2017.\\n\\nYinyu Nie, Xiaoguang Han, Shihui Guo, Yujian Zheng, Jian Chang, and Jian Zhang. Total3DUnderstanding: Joint layout, object pose and mesh reconstruction for indoor scenes from a single image. In CVPR, 2020.\\n\\nDennis Park, Rares Ambrus, Vitor Guizilini, Jie Li, and Adrien Gaidon. Is pseudo-lidar needed for monocular 3D object detection? In ICCV, 2021.\\n\\nCharles Qi, Wei Liu, Chenxia Wu, Hao Su, and Leonidas Guibas. Frustum pointnets for 3D object detection from RGB-D data. In CVPR, 2018.\\n\\nNikhila Ravi, Jeremy Reizenstein, David Novotny, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. Accelerating 3D deep learning with PyTorch3D. arXiv:2007.08501, 2020.\\n\\nCody Reading, Ali Harakeh, Julia Chae, and Steven Waslander. Categorical depth distribution network for monocular 3D object detection. In CVPR, 2021.\\n\\nJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In CVPR, 2016.\\n\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster R-CNN: Towards real-time object detection with region proposal networks. In NeurIPS, 2015.\\n\\nMike Roberts, Jason Ramapuram, Anurag Ranjan, Taylor Gordon, Wan-Yen Lo, Justin Johnson, and Georgia Gkioxari. Hypersim: A photorealistic synthetic dataset for holistic indoor scene understanding. In ICCV, 2021.\\n\\nDanila Rukhovich, Anna Vorontsova, and Anton Konushin. ImVoxelNet: Image to voxels projection for monocular and multi-view general-purpose 3D object detection. In WACV, 2022.\\n\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander Berg, and Li.\"}"}
{"id": "CVPR-2023-1497", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[68] Andrea Simonelli, Samuel Rota Bulo, Lorenzo Porzi, Manuel L\u00f3pez-Antequera, and Peter Kontschieder. Disentangling monocular 3D object detection. In ICCV, 2019.\\n\\n[69] Andrea Simonelli, Samuel Rota Bulo, Lorenzo Porzi, Elisa Ricci, and Peter Kontschieder. Towards generalization across depth for monocular 3D object detection. In ECCV, 2020.\\n\\n[70] Shuran Song, Samuel Lichtenberg, and Jianxiong Xiao. SUN RGB-D: A RGB-D scene understanding benchmark suite. In CVPR, 2015.\\n\\n[71] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, Vijay Vasudevan, Wei Han, Jiquan Ngiam, Hang Zhao, Aleksei Timofeev, Scott Ettinger, Maxim Krivokon, Amy Gao, Aditya Joshi, Yu Zhang, Jonathon Shlens, Zhifeng Chen, and Dragomir Anguelov. Scalability in perception for autonomous driving: Waymo open dataset. In CVPR, 2020.\\n\\n[72] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. FCOS: Fully convolutional one-stage object detection. In ICCV, 2019.\\n\\n[73] Shubham Tulsiani, Saurabh Gupta, David Fouhey, Alexei Efros, and Jitendra Malik. Factoring shape, pose, and layout from the 2D image of a 3D scene. In CVPR, 2018.\\n\\n[74] Pauli Virtanen, Ralf Gommers, Travis Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, et al. Scipy 1.0: fundamental algorithms for scientific computing in python. Nature methods, 2020.\\n\\n[75] Li Wang, Liang Du, Xiaoqing Ye, Yanwei Fu, Guodong Guo, Xiangyang Xue, Jianfeng Feng, and Li Zhang. Depth-conditioned dynamic message propagation for monocular 3D object detection. In CVPR, 2021.\\n\\n[76] Li Wang, Li Zhang, Yi Zhu, Zhi Zhang, Tong He, Mu Li, and Xiangyang Xue. Progressive coordinate transforms for monocular 3D object detection. In NeurIPS, 2021.\\n\\n[77] Tai Wang, Zhu Xinge, Jiangmiao Pang, and Dahua Lin. Probabilistic and geometric depth: Detecting objects in perspective. In CoRL, 2022.\\n\\n[78] Tai Wang, Xinge Zhu, Jiangmiao Pang, and Dahua Lin. FCOS3D: Fully convolutional one-stage monocular 3D object detection. In ICCVW, 2021.\\n\\n[79] Yan Wang, Wei-Lun Chao, Divyansh Garg, Bharath Hariharan, Mark Campbell, and Kilian Weinberger. Pseudo-lidar from visual depth estimation: Bridging the gap in 3D object detection for autonomous driving. In CVPR, 2019.\\n\\n[80] Yuxin Wu, Alexander Kirillov, Francisco Massa, Wan-Yen Lo, and Ross Girshick. Detectron2. https://github.com/facebookresearch/detectron2, 2019.\\n\\n[81] Yurong You, Yan Wang, Wei-Lun Chao, Divyansh Garg, Geoff Pleiss, Bharath Hariharan, Mark Campbell, and Kilian Weinberger. Pseudo-lidar++: Accurate depth for 3D object detection in autonomous driving. In ICLR, 2020.\\n\\n[82] Fisher Yu, Dequan Wang, Evan Shelhamer, and Trevor Darrell. Deep layer aggregation. In CVPR, 2018.\\n\\n[83] Yunpeng Zhang, Jiwen Lu, and Jie Zhou. Objects are different: Flexible monocular 3D object detection. In CVPR, 2021.\\n\\n[84] Dingfu Zhou, Xibin Song, Yuchao Dai, Junbo Yin, Feixiang Lu, Miao Liao, Jin Fang, and Liangjun Zhang. IAFA: Instance-aware feature aggregation for 3D object detection from a single image. In ACCV, 2020.\\n\\n[85] Xingyi Zhou, Dequan Wang, and Philipp Kr\u00e4henb\u00fchl. Objects as points. In arXiv preprint arXiv:1904.07850, 2019.\\n\\n[86] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. In CVPR, 2019.\\n\\n[87] Yunsong Zhou, Yuan He, Hongzi Zhu, Cheng Wang, Hongyang Li, and Qinhong Jiang. Monocular 3D object detection: An extrinsic parameter free approach. In CVPR, 2021.\\n\\n[88] Zhikang Zou, Xiaoqing Ye, Liang Du, Xianhui Cheng, Xiao Tan, Li Zhang, Jianfeng Feng, Xiangyang Xue, and Errui Ding. The devil is in the task: Exploiting reciprocal appearance-localization features for monocular 3D object detection. In ICCV, 2021.\"}"}
{"id": "CVPR-2023-1497", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We use an $L$ weighted by the error as shown in [68]. The 3D objective is\\n\\n$$3D = \\\\text{Training objective.}$$\\n\\nFor example, the disentangled loss for the projected center the ground truth from Eq. 3 to create a pseudo box prediction. For each variable group substitutes all but its variables with advantages over losses which directly compare variables canonical 3D cuboid. Losses in box coordinates have natural matrices of Euler angles modulo $\\\\pi$. $L$ for $B$\\n\\n$$\\\\text{disentangled losses, following [68]. The disentangled loss that that errors in variables may be ambiguous from one another. Thus, we isolate each variable group with separate}$$\\n\\n$$\\\\text{for each variable group substitutes all but its variables with disentangled losses, following [68]. The disentangled loss entangles all 3D variables via the box predictor}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D cuboid with its matched ground truth via a chamfer loss,}$$\\n\\n$$\\\\text{3D}$$\\n\\n$$\\\\text{the cube head. The 3D objective compares each predicted 3D"}
{"id": "CVPR-2023-1497", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"5. Experiments\\n\\nWe tackle image-based 3D object detection on O\\\\textsuperscript{MNI}3D and compare to prior best methods. We evaluate Cube R-CNN on existing 3D benchmarks with a single unified model. Finally, we prove the effectiveness of O\\\\textsuperscript{MNI}3D as a large-scale 3D object detection dataset by showing comprehensive cross-dataset generalization and its impact for pre-training.\\n\\nImplementation details.\\n\\nWe implement Cube R-CNN using Detectron2 \\\\cite{wu2019detectron2} and PyTorch3D \\\\cite{jamrigh2020pytorch3d}. Our backbone is DLA34 \\\\cite{yu2017dilation}\u2013FPN \\\\cite{lin2017feature} pretrained on ImageNet \\\\cite{russakovsky2015imagenet}. We train for 128 epochs with a batch size of 192 images across 48 V100s. We use SGD with a learning rate of 0.12 which decays by a factor of 10 after 60% and 80% of training.\\n\\nDuring training, we use random data augmentation of horizontal flipping and scales \\\\( \\\\in [0.50, 1.25] \\\\), enabled by virtual depth. Virtual camera parameters are set to \\\\( f_v = H_v = 512 \\\\).\\n\\nSource code and models are publicly available.\\n\\nMetric.\\n\\nFollowing popular benchmarks for 2D and 3D recognition, we use average-precision (AP) as our 3D metric. Predictions are matched to ground truth by measuring their overlap using \\\\( \\\\text{IoU}_{3D} \\\\) which computes the intersection-over-union of 3D cuboids. We compute a mean AP\\\\textsubscript{3D} across all 50 categories in O\\\\textsuperscript{MNI}3D and over a range of \\\\( \\\\text{IoU}_{3D} \\\\) thresholds \\\\( \\\\tau \\\\in [0.05, 0.10, \\\\ldots, 0.50] \\\\). The range of \\\\( \\\\tau \\\\) is more relaxed than in 2D to account for the new dimension \u2013 see the Appendix for a comparison between 3D and 2D ranges for IoU.\\n\\nGeneral 3D objects may be occluded by other objects or truncated by the image border, and can have arbitrarily small projections. Following \\\\cite{brazil2018}, we ignore objects with high occlusion (\\\\( > 66\\\\% \\\\)) or truncation (\\\\( > 33\\\\% \\\\)), and tiny projected objects (\\\\( < 6.25\\\\% \\\\) image height). Moreover, we also report AP\\\\textsubscript{3D} at varying levels of depth \\\\( d \\\\) as near: \\\\( 0 < d \\\\leq 10 \\\\text{m} \\\\), medium: \\\\( 10 \\\\text{m} < d \\\\leq 35 \\\\text{m} \\\\), far: \\\\( 35 \\\\text{m} < d \\\\leq \\\\infty \\\\).\\n\\nFast \\\\( \\\\text{IoU}_{3D} \\\\).\\n\\n\\\\( \\\\text{IoU}_{3D} \\\\) compares two 3D cuboids by computing their intersection-over-union. Images usually have many objects and produce several predictions so \\\\( \\\\text{IoU}_{3D} \\\\) computations need to be fast. Prior implementations \\\\cite{brazil2018} approximate \\\\( \\\\text{IoU}_{3D} \\\\) by projecting 3D boxes on a ground plane and multiply the top-view 2D intersection with the box heights to compute a 3D volume. Such approximations become notably inaccurate when objects are not on a planar ground or have an arbitrary orientation (e.g. with nonzero pitch or roll).\\n\\nObjectron \\\\cite{brazil2019} provides an exact solution, but relies on external libraries \\\\cite{moldovan2016,brazil2018} implemented in C++ and is not batched.\\n\\nWe implement a new, fast and exact algorithm which computes the intersecting shape by representing cuboids as meshes and finds face intersections \u2013 described in Appendix. Our algorithm is batched with C++ and CUDA support. Our implementation is \\\\( 90 \\\\times \\\\) faster than Objectron in C++, and \\\\( 450 \\\\times \\\\) faster in CUDA. As a result, evaluation on the large O\\\\textsuperscript{MNI}3D takes only seconds instead of several hours.\\n\\n5.1. Model Performance\\n\\nFirst, we ablate the design choices of Cube R-CNN. Then, we compare to state-of-the-art methods on existing benchmarks and O\\\\textsuperscript{MNI}3D and show that it performs superior or on par with prior works which are specialized for their respective domains. Our Cube R-CNN uses a single unified design to tackle general 3D object detection across domains.\\n\\nFig. 5 shows qualitative predictions on the O\\\\textsuperscript{MNI}3D test set.\\n\\nAblations.\\n\\nTable 1 ablates the features of Cube R-CNN on O\\\\textsuperscript{MNI}3D. We report AP\\\\textsubscript{3D}, performance at single IoU thresholds (0.25 and 0.50), and for near, medium and far objects. We further train and evaluate on domain-specific subsets, O\\\\textsuperscript{MNI}3D\\\\textsubscript{IN} (AP\\\\textsubscript{IN}3D) and O\\\\textsuperscript{MNI}3D\\\\textsubscript{OUT} (AP\\\\textsubscript{OUT}3D) to show how ablation trends hold for single domain scenarios.\\n\\nFrom Table 1 we draw a few standout observations:\"}"}
{"id": "CVPR-2023-1497", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Cube R-CNN ablations. We report AP$^3D$ on O$^{MNI}$3D, then at IoU thresholds 0.25 and 0.50, and for near, medium and far objects. We further report AP$^3D$ when both training and evaluating on O$^{MNI}$3D IN and O$^{MNI}$3D OUT subsets.\\n\\n| Method                      | AP$^3D$ | AP$^3D$ | AP$^3D$ | AP$^3D$ |\\n|-----------------------------|---------|---------|---------|---------|\\n| M3D-RPN [7]                | 10.4    | 17.9    | 13.7    |\\n| SMOKE [50]                 | 25.4    | 20.4    | 19.5    |\\n| FCOS3D [78]                | 14.6    | 20.9    | 17.6    |\\n| PGD [77]                   | 21.4    | 26.3    | 22.9    | 11.2    |\\n| GUPNet [52]                | 24.5    | 20.5    | 19.9    |\\n| M3D-RPN + vc               | 16.2    | 20.4    | 17.0    |\\n| ImVoxelNet [66]            | 23.5    | 23.4    | 21.5    |\\n| SMOKE + vc                 | 25.9    | 20.4    | 20.0    |\\n| FCOS3D + vc                | 17.8    | 25.1    | 21.3    |\\n| PGD + vc                   | 22.8    | 31.2    | 26.8    |\\n\\nTable 2. O$^{MNI}$3D comparison with GUPNet [52], ImVoxelNet [66], M3D-RPN [7], FCOS3D [78], PGD [77], SMOKE [50]. We apply virtual camera to the last four (+vc) and show the gains in purple.\\n\\nVirtual depth is effective and improves AP$^3D$ by +6%, most noticeable in the full O$^{MNI}$3D which has the largest variances in camera intrinsics. Enabled by virtual depth, scale augmentations during training increase AP$^3D$ by +3.1% on O$^{MNI}$3D, +2.7% on O$^{MNI}$3D IN and +3.8% on O$^{MNI}$3D OUT. We find scale augmentation controlled without virtual depth to be harmful by \u22122.5% (rows 6 - 7).\\n\\n3D uncertainty boosts performance by about +6% on all O$^{MNI}$3D subsets. Intuitively, it serves as a measure of confidence for the model's 3D predictions; removing it means that the model relies only on its 2D classification to score cuboids. If uncertainty is used only to scale samples in Eq. 5, but not at inference then AP$^3D$ still improves but by +1.6%.\\n\\nTable 1 also shows that the entangled L$^3D$ loss from Eq. 5 boosts AP$^3D$ by +3.1% while disentangled losses contribute less (+0.7%). Replacing objectness with IoUness in the RPN head improves AP$^3D$ on O$^{MNI}$3D by +1.1%.\\n\\nComparison to other methods.\\n\\nWe compare Cube R-CNN to prior best approaches. We choose representative state-of-the-art methods designed for the urban and indoor domains, and evaluate on our proposed O$^{MNI}$3D benchmark and single-dataset benchmarks, KITTI and SUN RGB-D.\\n\\nComparisons on O$^{MNI}$3D.\\n\\nTable 3. KITTI leaderboard results on car for image-based approaches. D denotes methods trained with extra depth supervision. 2D-3D geometry and camera focal length to derive depth, SMOKE [50] which predicts 3D object center and offsets densely and ImVoxelNet [66], which uses intrinsics to unproject 2D image features to a 3D volume followed by 3D convolutions. These methods originally experiment on urban domains, so we compare on O$^{MNI}$3D OUT and the full O$^{MNI}$3D. We train each method using public code and report the best of multiple runs after tuning their hyper-parameters to ensure best performance. We extend all but GUPNet and ImVoxelNet with a virtual camera to handle varying intrinsics, denoted by +vc. GUPNet and ImVoxelNet dynamically use per-image intrinsics, which can naturally account for camera variation. M3D-RPN and GUPNet do not support multi-node training, which makes scaling to O$^{MNI}$3D difficult and are omitted. For O$^{MNI}$3D OUT, we report AP$^3D$ on its test set and its subparts, AP$^3D$ KIT and AP$^3D$ NU.\\n\\nFrom Table 2 we observe that our Cube R-CNN outperforms competitive methods on both O$^{MNI}$3D OUT and O$^{MNI}$3D. The virtual camera extensions result in performance gains of varying degree for all approaches, e.g. +4.2% for PGD on O$^{MNI}$3D, proving its impact as a general purpose feature for mixed dataset training for the task of 3D object detection. FCOS3D, SMOKE, and ImVoxelNet [66] struggle the most on O$^{MNI}$3D, which is not surprising as they are typically tailored for dataset-specific model choices.\"}"}
{"id": "CVPR-2023-1497", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 4. We compare to indoor models with common categories. For Total3D, we use oracle 2D detections to report IoU (3rd col). For ImVoxelNet, we report the detection metric of AP$_{3D}$ (4th col).\\n\\nKITTI\u2019s AP$_{3D}$ is fundamentally similar to ours but sets a single IoU threshold at 0.70, which behaves similar to a nearly perfect 2D threshold of 0.94 (see the Appendix). To remedy this, our AP$_{3D}$ is a mean over many thresholds, inspired by COCO [45]. In contrast, nuScenes proposes a mean AP based on 3D center distance but omits size and rotation, with settings generally tailored to the urban domain (e.g. size and rotation variations are less extreme for cars). We provide more analysis with this metric in the Appendix.\\n\\nComparisons on SUN RGB-D.\\n\\nTable 4 compares Cube R-CNN to Total3D [58] and ImVoxelNet [66], two state-of-the-art methods on SUN RGB-D on 10 common categories. Total3D\u2019s public model requires 2D object boxes and categories as input. Hence, for a fair comparison, we use ground truth 2D detections as input to both our and their method, each trained using a ResNet34 [27] backbone and report mean IoU$_{3D}$ (3rd col). We compare to ImVoxelNet in the full 3D object detection setting and report AP$_{3D}$ (4th col).\\n\\nWe compare Cube R-CNN from two respects, first trained on SUN RGB-D identical to the baselines, then trained on OMNI3D IN which subsumes SUN RGB-D. Our model outperforms Total3D by +12.9% and ImVoxelNet by +4.1% when trained on the same training set and increases the performance gap when trained on the larger OMNI3D IN.\\n\\nZero-shot Performance.\\n\\nOne commonly sought after property of detectors is zero-shot generalization to other datasets. Cube R-CNN outperforms all other methods at zero-shot. We find our model trained on KITTI achieves 12.7% AP$_{3D}$ on nuScenes; the second best is M3D-RPN+vc with 10.7%. Conversely, when training on nuScenes, our model achieves 20.2% on KITTI; the second best is GUPNet with 17.3%.\\n\\n5.2. The Impact of the OMNI3D Benchmark\\n\\nSec. 5.1 analyzes the performance of Cube R-CNN for the task of 3D object detection. Now, we turn to OMNI3D and its impact as a large-scale benchmark. We show two use cases of OMNI3D: (a) a universal 3D dataset which integrates smaller ones, and (b) a pre-training dataset.\\n\\nOMNI3D as a universal dataset.\\n\\nWe treat OMNI3D as a dataset which integrates smaller single ones and show its impact on each one. We train Cube R-CNN on OMNI3D and compare to single-dataset training in Table 5, for the indoor (left) and urban (right) domain. AP$_{3D}$ is reported on the category intersection (10 for indoor, 3 for outdoor) to ensure comparability. Training on OMNI3D and its domain-specific subsets, OMNI3D IN and OMNI3D OUT, results in higher performance compared to single-dataset training, signifying that our large OMNI3D generalizes better and should be preferred over single dataset training. ARKit sees a +2.6% boost and KITTI +5.3%. Except for Hypersim, the domain-specific subsets tend to perform better on their domain, which is not surprising given their distinct properties (Fig. 2).\\n\\nOMNI3D as a pre-training dataset.\\n\\nNext, we demonstrate the utility of OMNI3D for pre-training. In this setting, an unseen dataset is used for finetuning from a model pre-trained on OMNI3D. The motivation is to determine how a large-scale 3D dataset could accelerate low-shot learning with minimum need for costly 3D annotations on a new dataset. We choose SUN RGB-D and KITTI as our unseen datasets given their popularity and small size. We pre-train Cube R-CNN on OMNI3D-, which removes them from OMNI3D, then finetune the models using a % of their training data. The curves in Fig. 6 show the model quickly gains its upper-bound performance at a small fraction of the training data, when pre-trained on OMNI3D vs. ImageNet, without any few-shot training tricks. A model finetuned on only 5% of its target can achieve >70% of the upper-bound performance.\\n\\n6. Conclusion\\n\\nWe propose a large and diverse 3D object detection benchmark, OMNI3D, and a general purpose 3D object detector, Cube R-CNN. Models and data are publicly released. Our extensive analysis (Table 1, Fig. 5, Appendix) show the strength of general 3D object detection as well as the limitations of our method, e.g. localizing far away objects, and uncommon object types or contexts. Moreover, for accurate real-world 3D predictions, our method is limited by the assumption of known camera intrinsics, which may be minimized in future work using self-calibration techniques.\"}"}
