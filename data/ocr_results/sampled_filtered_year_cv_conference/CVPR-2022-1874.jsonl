{"id": "CVPR-2022-1874", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Peter Anderson, X. He, C. Buehler, Damien Teney, Mark Johnson, Stephen Gould, and Lei Zhang. Bottom-up and top-down attention for image captioning and visual question answering. 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6077\u20136086, 2018.\\n\\n[2] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, C. Lawrence Zitnick, and Devi Parikh. VQA: Visual question answering. In ICCV, 2015.\\n\\n[3] T. Brown, B. Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, G. Kr\u00fcger, T. Henighan, R. Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, E. Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, J. Clark, Christopher Berner, Sam McCandlish, A. Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. NeurIPS, abs/2005.14165, 2020.\\n\\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In ECCV, 2020.\\n\\n[5] Jaemin Cho, Jie Lei, Haochen Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation. In ICML, 2021.\\n\\n[6] Jaemin Cho, Jiasen Lu, Dustin Schwenk, Hannaneh Hajishirzi, and Aniruddha Kembhavi. X-lxmert: Paint, caption and answer questions with multi-modal transformers. In EMNLP, 2020.\\n\\n[7] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Desraj Yadav, Jos\u00e9 Moura, Devi Parikh, and Dhruv Batra. Visual dialog. In CVPR, 2017.\\n\\n[8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina N. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In NAACL, 2019.\\n\\n[9] Wei Dong, Richard Socher, Li Li-Jia, Kai Li, and Li Fei-Fei. ImageNet: A large-scale hierarchical image database. In CVPR, 2009.\\n\\n[10] Ali Farhadi, Seyyed Hejrati, Amin Sadeghi, Peter Young, Cyrus Rashtchian, Julia Hockenmaier, and David A. Forsyth. Every picture tells a story: Generating sentences from images. In ECCV, 2010.\\n\\n[11] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering. In CVPR, 2017.\\n\\n[12] Tanmay Gupta, Kevin J. Shih, Saurabh Singh, and Derek Hoiem. Aligned image-word representations improve induc-tive transfer across vision-language tasks. 2017 IEEE International Conference on Computer Vision (ICCV), pages 4223\u20134232, 2017.\\n\\n[13] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask R-CNN. In ICCV, 2017.\\n\\n[14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In CVPR, 2016.\\n\\n[15] Lisa Anne Hendricks, Subhashini Venugopalan, Marcus Rohrbach, Raymond J. Mooney, Kate Saenko, and Trevor Darrell. Deep compositional captioning: Describing novel object categories without paired training data. CVPR, pages 1\u201310, 2016.\\n\\n[16] Drew A Hudson and Christopher D Manning. GQA: A new dataset for real-world visual reasoning and compositional question answering. In CVPR, 2019.\\n\\n[17] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Andrew Brock, Evan Shelhamer, Olivier J. H'enaff, Matthew M. Botvinick, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver io: A general architecture for structured inputs & outputs. ArXiv, abs/2107.14795, 2021.\\n\\n[18] Andrew Jaegle, Felix Gimeno, Andrew Brock, Andrew Zisserman, Oriol Vinyals, and Jo\u00e3o Carreira. Perceiver: General perception with iterative attention. In ICML, 2021.\\n\\n[19] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In ICML, 2021.\\n\\n[20] Aishwarya Kamath, Mannat Singh, Yann LeCun, Ishan Misra, Gabriel Synnaeve, and Nicolas Carion. Mdetr - modulated detection for end-to-end multi-modal understanding. ICCV, pages 1760\u20131770, 2021.\\n\\n[21] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In CVPR, 2015.\\n\\n[22] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. ReferItGame: Referring to objects in photographs of natural scenes. In EMNLP, 2014.\\n\\n[23] Daniel Khashabi, Sewon Min, Tushar Khot, Ashish Sabharwal, Oyvind Tafjord, P. Clark, and Hannaneh Hajishirzi. Unifiedqa: Crossing format boundaries with a single qa system. In EMNLP, 2020.\\n\\n[24] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen, Yannis Kalantidis, Li-Jia Li, David A Shamma, Michael Bernstein, and Li Fei-Fei. Visual genome: Connecting language and vision using crowdsourced dense image annotations. IJCV, 123:32\u201373, 2017.\\n\\n[25] Girish Kulkarni, Visruth Premraj, Sagnik Dhar, Siming Li, Yejin Choi, Alexander C Berg, and Tamara L Berg. Baby talk: Understanding and generating image descriptions. In CVPR, 2011.\\n\\n[26] Ankit Kumar, Ozan Irsoy, Peter Ondruska, Mohit Iyyer, James Bradbury, Ishaan Gulrajani, Victor Zhong, Romain Paulus, and Richard Socher. Ask me anything: Dynamic memory networks for natural language processing. In ICML, pages 1378\u20131387, 2016.\\n\\n[27] Jie Lei, Linjie Li, Luowei Zhou, Zhe Gan, Tamara L. Berg, Mohit Bansal, and Jingjing Liu. Less is more: Clipbert for video-and-language learning via sparse sampling. In CVPR, 2021.\\n\\n[28] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension. ACL, 2020.\"}"}
{"id": "CVPR-2022-1874", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00e1r, and C. Lawrence Zitnick. Microsoft COCO: Common objects in context. In ECCV, 2014.\\n\\nIlya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019.\\n\\nJiasen Lu, Dhruv Batra, Devi Parikh, and Stefan Lee. Vilbert: Pretraining task-agnostic visiolinguistic representations for vision-and-language tasks. In NeurIPS, 2019.\\n\\nJiasen Lu, Vedanuj Goswami, Marcus Rohrbach, D. Parikh, and Stefan Lee. 12-in-1: Multi-task vision and language representation learning. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10434\u201310443, 2020.\\n\\nJiasen Lu, Jianwei Yang, Dhruv Batra, and Devi Parikh. Neural baby talk. In CVPR, 2018.\\n\\nJunhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan L Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In CVPR, 2016.\\n\\nB. McCann, N. Keskar, Caiming Xiong, and R. Socher. The natural language decathlon: Multitask learning as question answering. ArXiv, abs/1806.08730, 2018.\\n\\nSwaroop Mishra, Daniel Khashabi, Chitta Baral, and Hanna Hajishirzi. Cross-task generalization via natural language crowdsourcing instructions. In ACL, 2022.\\n\\nBryan Plummer, Arun Mallya, Christopher Cervantes, Julia Hockenmaier, and Svetlana Lazebnik. Phrase localization and visual relationship detection with comprehensive image-language cues. In ICCV, 2017.\\n\\nBryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models. In ICCV, 2015.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021.\\n\\nA. Radford, Jeffrey Wu, R. Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019.\\n\\nColin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, W. Li, and Peter J. Liu. Exploring the limits of transfer learning with a unified text-to-text transformer. J. Mach. Learn. Res., 21:140:1\u2013140:67, 2020.\\n\\nShaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. In Advances in Neural Information Processing Systems, 2015.\\n\\nAnna Rohrbach, Marcus Rohrbach, Ronghang Hu, Trevor Darrell, and Bernt Schiele. Grounding of textual phrases in images by reconstruction. In ECCV, 2016.\\n\\nPiyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: A cleaned, hypernymed, image alt-text dataset for automatic image captioning. In ACL, 2018.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS, 2017.\\n\\nRamakrishna Vedantam, C. L. Zitnick, and D. Parikh. Cider: Consensus-based image description evaluation. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 4566\u20134575, 2015.\\n\\nOriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitruc Erhan. Show and tell: A neural image caption generator. In CVPR, 2015.\\n\\nPeng Wang, Qi Wu, Chunhua Shen, Anthony Dick, and Anton van den Hengel. FVQA: Fact-based visual question answering. PAMI, 40(10):2413\u20132427, 2018.\\n\\nOrion Weller, Nicholas Lourie, Matt Gardner, and Matthew E. Peters. Learning from task descriptions. ArXiv, abs/2011.08115, 2020.\\n\\nSpencer Whitehead, Hui Wu, Heng Ji, Rog\u00e9rio Schmidt Feris, Kate Saenko, and Uiuc MIT-IBM. Separating skills and concepts for novel visual question answering. CVPR, pages 5628\u20135637, 2021.\\n\\nQi Wu, Peng Wang, Chunhua Shen, Anthony Dick, and Anton van den Hengel. Ask me anything: Free-form visual question answering based on knowledge from external sources. In CVPR, 2016.\\n\\nYonghui Wu, Mike Schuster, Zhifeng Chen, Quoc V. Le, Mohammad Norouzi, Wolfgang Macherey, Maxim Krikun, Yuan Cao, Qin Gao, Klaus Macherey, Jeff Klingner, Apurva Shah, Melvin Johnson, Xiaobing Liu, \u0141ukasz Kaiser, Stephan Gouws, Yoshikiyo Kato, Taku Kudo, Hideto Kazawa, Keith Stevens, George Kurian, Nishant Patil, Wei Wang, Cliff Young, Jason Smith, Jason Riesa, Alex Rudnick, Oriol Vinyals, Greg Corrado, Macduff Hughes, and Jeffrey Dean. Google's neural machine translation system: Bridging the gap between human and machine translation. arXiv:1609.08144, 2016.\\n\\nHaiyang Xu, Ming Yan, Chenliang Li, Bin Bi, Songfang Huang, Wenming Xiao, and Fei Huang. E2e-vlp: End-to-end vision-language pre-training enhanced by visual learning. In ACL, 2021.\\n\\nKelvin Xu, Jimmy Ba, Ryan Kiros, Aaron Courville, Ruslan Salakhutdinov, Richard Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention. In ICML, 2015.\\n\\nRowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In CVPR, 2019.\\n\\nRowan Zellers, Ximing Lu, Jack Hessel, Youngjae Yu, Jae Sung Park, Jize Cao, Ali Farhadi, and Yejin Choi. Merlot: Multimodal neural script knowledge models. ArXiv, abs/2106.02636, 2021.\\n\\nPengchuan Zhang, Xiujun Li, Xiaowei Hu, Jianwei Yang, Lei Zhang, Lijuan Wang, Yejin Choi, and Jianfeng Gao. Vinvl: Revisiting visual representations in vision-language models. In CVPR, 2021.\\n\\nLuowei Zhou, Hamid Palangi, Lei Zhang, Houdong Hu, Jason J. Corso, and Jianfeng Gao. Unified vision-language pre-training for image captioning and vqa. AAAI, 2020.\"}"}
{"id": "CVPR-2022-1874", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. Aligning books and movies: Towards story-like visual explanations by watching movies and reading books. In ICCV, 2015. 4\"}"}
{"id": "CVPR-2022-1874", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"scribes how these tasks are posed to our general purpose system along with respective losses and metrics used for training and evaluation. Sec. 4.2 details how samples are created for each task from the original annotations and introduces our COCO-SCE split for testing the generalization of concepts across skills.\\n\\n**4.1. Tasks**\\n\\nOur experiments mainly involve 4 tasks \u2013 VQA, Captioning, Localization, and Classification. We only use Referring Expressions to test the learning ability of GPV-1.\\n\\n**VQA** aims to answer a question given an image. The input is an image/text pair, and the output is text. While training, the loss employed is the negative log likelihood of the ground truth answer text. We use the standard VQA evaluation metric (annotator-agreement weighted answer accuracy) [2] to report results.\\n\\n**Captioning** aims to produce a description of an image. The input is an image and a prompt, such as \u201cDescribe the image\u201d or \u201cWhat is going on in the image?\u201d, and the output is text. While training, the loss employed is the negative log likelihood of the annotated caption. The evaluation metric reported is CIDEr-D [46] that measures the similarity of the generated and ground truth captions.\\n\\n**Localization** aims to produce a tightly fitting bounding box to an object. The input is an image and a prompt, such as \u201cFind all instances of dogs\u201d or \u201cLocate the chairs\u201d, and the output is a set of ranked bounding boxes. Training uses DETR\u2019s Hungarian loss. Evaluation is an average of per-query average-precision (AP) with a 0.5 bounding box intersection over union (IOU) threshold. For example, if an image contains two target objects and the correctness of the top four ranked boxes is \\\\{True, False, False, True\\\\}, the AP is \\\\((1/1+2/4)/2=0.75\\\\) (every-point interpolation). The reported number is AP averaged over samples.\\n\\n**Classification** aims to assign a category to a region. The input is an image patch and a prompt such as \u201cWhat is this thing?\u201d or \u201cWhat object is this?\u201d, and the output is text. In principle, GPV-1 can produce any category label within the large vocabulary of the text decoder, including words that it has not seen within its classification training data. However, for evaluation, a K-way classification is performed by suppressing outputs that do not correspond to any of the applicable K categories. The training loss used is the negative log likelihood of text output, and evaluation is accuracy averaged over samples.\\n\\n**Referring expressions (RefExp)** aims to localize a single region that corresponds to a phrase. The input is an image and a referring expression such as \u201cthe man wearing a green shirt\u201d, and the output is one bounding box. While the training loss and evaluation is the same as localization, the key distinction is disambiguation of the referred instance among other instances of the same object category in the image.\\n\\n![COCO Train](https://via.placeholder.com/150)\\n\\n**Figure 3. COCO-SCE:** A split of COCO images and annotations to test the generalization of concepts across skills. Schematic shows train, val and test samples used for VQA.\\n\\n**4.2. Data**\\n\\nWe present experiments using images from the richly annotated COCO dataset. We use question and answer annotations from the VQA V2 dataset, referring expressions from REFCOCO+, and COCO annotations for other tasks.\\n\\n**Data samples.** VQA samples consist of the original questions as prompts paired with the most-agreed answer among annotators. For captioning, COCO provides 5 captions per image, each of which is treated as a different sample paired with one of 14 captioning prompt templates. We generate localization samples for each object category in the image using one of 18 prompt templates paired with all instances for the category. For classification, we create a sample for each object category in the image by choosing one of the instances (cropped using the ground truth box) paired with one of 4 prompt templates. RefExp samples consist of referring expressions as prompts with corresponding boxes.\\n\\n**Data splits.** We present results for GPV-1 and baselines on two data splits. First we train and evaluate models using the standard data splits for the corresponding tasks. This provides results for GPV-1 in the context of past work. Then, to test the ability of vision systems to generalize concepts across skills, we present a new split of the above annotations, named COCO-SCE (Skill-Concept Evaluation).\\n\\n**COCO-SCE.** Fig. 3 presents a schematic of the proposed COCO-SCE splits. The 80 classes of COCO are split into 3 disjoint sets, specifying which tasks can use them for training and validation:\\n\\n- \\\\(H_{vqa,cap}\\\\): 10 classes held-out from the VQA and captioning tasks in the train/val sets\\n- \\\\(H_{cls,loc}\\\\): 10 different classes held-out from the classification and localization tasks in the train/val sets\\n- \\\\(S\\\\): 60 remaining classes are not held out from any tasks\\n\\nWhen a category is held out, any annotations containing that word are not used for training or val. E.g., if boat is a held out category for VQA, then the annotation \\\\{\u201cWhat color is the boat?\u201d, \u201cBlue\u201d\\\\} would be excluded from the train/val set. Other annotations from the same image may still be used, e.g. \\\\{\u201cIs it a sunny day?\u201d, \u201cYes\u201d\\\\}. Also, the classification and localization annotations for boat would be included in train/val for respective tasks. The assignment of 16403\"}"}
{"id": "CVPR-2022-1874", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Comparison to special purpose baselines (\\\\textsuperscript{C}OCO-SCE and \\\\textsuperscript{C}OCO-splits): Our jointly trained GPV-1 compares well to specialized single-task baselines as well as GPV-1 trained on individual task data. On \\\\textsuperscript{C}OCO-split, we report test-server results for VQA and captioning and validation results for localization and classification as the annotations for test images are hidden. On \\\\textsuperscript{C}OCO-SCE split, we report test results for all tasks.\\n\\n5. Experiments\\n\\nOur experiments evaluate GPV-1 for its effectiveness compared to specialized models (Sec. 5.1), its ability to apply learned skills to unseen concepts for that skill (Sec. 5.2), its efficiency at learning new skills, and retention of previously learned skills (Sec. 5.3). Sec 5.4 provides ablations. Our \\\\textsuperscript{C}OCO-SCE experiments are carefully designed to ensure that compared methods train on the same amount of skill data (although some models may have access to data from another skill) and to enable evaluation of concept transfer across skills by avoiding exposing the held-out concepts via pretraining on Conceptual Captions \\\\cite{44} or Visual Genome \\\\cite{1}. ImageNet pretraining while not ideal, is unavoidable as most vision models including DETR rely on it to bootstrap learning.\\n\\n5.1. Generality vs Effectiveness\\n\\nIs generality of GPV-1 at the cost of effectiveness? We compare GPV-1 to competitive special purpose models designed for each task \u2013 ViLBERT \\\\cite{31} (VQA), VLP \\\\cite{58} (captioning), Faster-RCNN \\\\cite{42} (localization) and Resnet-50 \\\\cite{14} (classification). To avoid conflating effectiveness of architecture with availability of more data, we retrain these models to only use \\\\textsuperscript{C}OCO and VQA data. For ViLBERT and VLP this requires replacing Visual Genome \\\\cite{24} bottom-up features \\\\cite{1} with Faster-RCNN features trained only on \\\\textsuperscript{C}OCO and no pretraining on Conceptual Captions \\\\cite{44}.\\n\\nTable 1 shows that on the \\\\textsuperscript{C}OCO-SCE split, the general purpose GPV-1 architecture trained on individual tasks compares favorably to each special purpose model (rows \\\\textsuperscript{a} vs \\\\textsuperscript{b}). Also, the generality of GPV-1 enables it to be jointly trained on all 4 tasks, leading to sizeable gains on 2 tasks and comparable results on others (rows \\\\textsuperscript{b} vs \\\\textsuperscript{c}). The same trends also hold when we compare models on the original \\\\textsuperscript{C}OCO data-splits (rows \\\\textsuperscript{d} vs \\\\textsuperscript{e}), validating that these trends are not merely a product of our proposed splits. Together, these results establish that the generality of GPV-1 is not at the expense of effectiveness.\\n\\n5.2. Skill-Concept Generalization\\n\\nWe wish to test generality of concepts across skills, i.e. the ability of a model to perform well on novel skill-concept combinations that were unseen during training. When training on a single task on \\\\textsuperscript{C}OCO-SCE a model does not have access to any annotation on held-out concepts. For example, a model trained only on VQA will never see a question or answer about \\\\textit{horse} $\\\\in \\\\textsuperscript{H}_{\\\\text{vqa,cap}}$. However, when training on all tasks, the model learns to localize and classify \\\\textit{horse} images. Therefore we expect the model to apply the acquired skill of question answering to answer questions about \\\\textit{horse} without explicitly being trained on \\\\textit{horse} VQA data.\\n\\nTable 2 shows the performance of the specialized models and the 1-Task and Multitask GPV-1 models on the \\\\textsuperscript{C}OCO-SCE full test split as well as separately on the subset of test data categorized as \u201cseen\u201d and \u201cunseen\u201d (see Fig. 3 for a schematic of these subsets for the VQA task). The 1-Task GPV-1 (row \\\\textsuperscript{b}) trained on individual tasks serves as a baseline to account for learned priors and dataset biases by the GPV-1 architecture. We observe significant gains by Multitask GPV-1 (row \\\\textsuperscript{c}) on the \u201cunseen\u201d subset across all tasks, particularly over the specialized models (row \\\\textsuperscript{c} vs row \\\\textsuperscript{a}) \u2013 indicating that the general purpose architecture is better suited at learning skills and then applying them to concepts that were unseen for that skill. We also report the performance of Multitask GPV-1 trained on the \\\\textsuperscript{C}OCO training split (row \\\\textsuperscript{d}). Since this split exposes the model to held-out concepts for all tasks, it can serve as a loose upper bound for the \u201cunseen\u201d split.\\n\\n5.3. Learning Generalization\\n\\nA system exhibits good learning generalization if it can learn new skills sample-efficiently without forgetting previously-learned skills. Learning ability. Fig. 4 (left) shows learning curves for GPV-1 and GPV-1-Loc when finetuning on the Referring Expressions task. GPV-1-Loc is pretrained on only the localization task (the only other task that has bounding-box supervision) while GPV-1 is pretrained on all four tasks. Multitask GPV-1 demonstrates much better zero-shot performance.\\n\\n16404\\n\\n16404\"}"}
{"id": "CVPR-2022-1874", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Skill-Concept Generalization: Multitask achieves higher performance overall, especially for \u201cUnseen\u201d concepts. Classification and Localization \u201cSeen\u201d performance slightly decreases, likely because all tasks share the same images and VQA and captioning are more weakly supervised. GPV oracle performance, with no concepts held out, provides an upper-bound on \u201cUnseen\u201d. Rows a, b, c are trained and tested on the smaller COCO-SCE data split, while d uses the COCO split.\\n\\nFigure 4. Learning new skills and retention of previous skills. Left: On REFR, multitask pretrained GPV-1 improves the 0-shot performance over single task pretrained GPV-1-Loc. GPV-1 also learns the new skill quicker than GPV-1-Loc, particularly in the lower data regime. Right: As REFR training data increases, GPV-1 does forget existing skills but multitask GPV-1 is more resilient to forgetting than GPV-1-Loc (note that x-axes are log-scaled).\\n\\nTable 3. Ablation: Augmenting the vision transformer features with RoI features extracted from the CNN backbone helps VQA significantly and Captioning slightly, but is detrimental to Localization and Classification. The transformer features may be sufficient to fully model localization and classification, while VQA and Captioning benefit from additional information in the RoI features. Fine-tuning helps all tasks.\"}"}
{"id": "CVPR-2022-1874", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Qualitative Results: Prompts are shown in colored boxes (one color per task) with box and text predictions below. (a) GPV-1 learns to output the expected modality (indicated by star) for each task, but also provides unsolicited yet informative commentary for the localization task and relevant regions for VQA and captioning. (b) GPV-1 can perform 0-shot referring expression comprehension. GPV-1 learns to correct zero-shot mistakes (c) when finetuned on annotations for the task (d).\\n\\nFigure 6. Failure Cases: (a) GPV-1 fails to count the number of people despite localizing them, and is unable to locate objects such as ski poles and refrigerators. (b) RefCOCO failures showing the model locating an incorrect object from the correct category.\\n\\nAcknowledgments. This work is partially supported by ONR MURI Award N00014-16-1-2007 and ONR award N00014-21-1-2705.\"}"}
{"id": "CVPR-2022-1874", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards General Purpose Vision Systems: An End-to-End Task-Agnostic Vision-Language Architecture\\n\\nTanmay Gupta\\nAmita Kamath\\nAniruddha Kembhavi\\nDerek Hoiem\\n\\n1 PRIOR @ Allen Institute for AI\\n2 University of Illinois at Urbana-Champaign\\n\\nhttps://prior.allenai.org/projects/gpv\\n\\nAbstract\\n\\nComputer vision systems today are primarily N-purpose systems, designed and trained for a predefined set of tasks. Adapting such systems to new tasks is challenging and often requires non-trivial modifications to the network architecture (e.g. adding new output heads) or training process (e.g. adding new losses). To reduce the time and expertise required to develop new applications, we would like to create general purpose vision systems that can learn and perform a range of tasks without any modification to the architecture or learning process. In this paper, we propose GPV-1, a task-agnostic vision-language architecture that can learn and perform tasks that involve receiving an image and producing text and/or bounding boxes, including classification, localization, visual question answering, captioning, and more. We also propose evaluations of generality of architecture, skill-concept transfer, and learning efficiency that may inform future work on general purpose vision. Our experiments indicate GPV-1 is effective at multiple tasks, reuses some concept knowledge across tasks, can perform the Referring Expressions task zero-shot, and further improves upon the zero-shot performance using a few training samples.\\n\\n1. Introduction\\n\\nComputer vision systems today are N-purpose learners \u2014 designed, trained, and limited to N predetermined tasks. Single-purpose models specialize in a single task, and adapting them to a new task or dataset requires an architecture change, minimally replacing the last classification layer. Multi-purpose models, such as Mask-RCNN [13], simultaneously solve more than one task, but the architecture and learning are tailored to specific tasks which must be defined in advance. In vision-language models [32], dedicated output heads are typically used for each task and dataset. Analogous to a general purpose computer, a general purpose vision (GPV) system is designed to carry out many vision tasks, not all known at the time of design, constrained only by its input modalities, memory/instructions, and output modalities. General purpose systems enable new applications to be developed without knowledge of or access to the underlying mechanics. The NLP community has made significant progress in this direction with sequence-to-sequence transformer-based models, such as T5 [41] and GPT-3 [3], which can be trained to solve many language tasks without changing the architecture. We believe such advances are now possible within computer vision, though with many new challenges.\\n\\nIn this paper, we propose an end-to-end trainable task-agnostic vision-language architecture, GPV-1, as a step towards general purpose vision systems. As input, our system\\n\\nGPV\\n\\nWhat is this?\\n\\nBoxes with relevance scores\\n\\nText\\n\\nbrown\\n\\nyes\\n\\n1\\n\\nA dog\\n\\nand a cat\\n\\nlaying on\\n\\na bed.\\n\\n0.75\\n\\n0.13\\n\\n0.02\\n\\n0.04\\ncat\\n\\n0.99\\n\\n0.59\\n\\n0.77\\n\\n0.37\\n\\n0.40\\n\\n0.30\\n\\nFigure 1. A task-agnostic vision-language architecture.\\n\\nGPV-1 takes an image and a natural language task description and outputs bounding boxes, confidences and text. GPV-1 can be trained end-to-end on any task that requires a box or text output, without any architecture modifications such as adding a new task-head. Results correspond to a model trained to perform VQA, localization, captioning, and classification tasks. Star indicates the output modality supervised during training for each task.\\n\\n16399\\n\\n16399\"}"}
{"id": "CVPR-2022-1874", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"receives an image and a text description of a task. The system outputs bounding boxes, confidences, and text that are relevant to the task and image. A user can input an image and query the system with a variety of requests such as \u201cWhat make is the blue car?\u201d (visual question answering), \u201cLocate all the sedans\u201d (localization), and \u201cDescribe the image\u201d (captioning). Each query elicits a different response using output heads that are shared across tasks. Defining the task through natural language allows the user to request GPV-1 to perform or learn a task without knowledge of its architecture or previous training. For example, our experiments show that GPV-1 can perform the referring expressions task without any training examples for that task and, when provided training examples, learns more quickly than special purpose models.\\n\\nBeyond performing well on trained skill-concept combinations (contained in training tasks), GPV systems should be able to learn new tasks efficiently with the same architecture as well as generalize to novel skill-concept combinations for learned skills by transferring concept knowledge from other skills. These abilities are not usually applicable or measured in specialized systems. Therefore, we propose evaluations that measure three forms of generality:\\n\\n- **Generality of architecture**: Learn any task within a broad domain specified only through input/output modalities without change to network structure (e.g. learn to classify bird species, without adding new output heads)\\n- **Generality of concepts across skills**: Perform tasks in skill-concept combinations not seen during training (e.g. localize \\\"muskrat\\\" after learning to answer questions about \\\"muskrats\\\")\\n- **Generality of learning**: Learn new tasks sample-efficiently with minimal loss to performance on previously learned tasks\\n\\nTo test generality of architecture, we train and evaluate our system\u2019s ability to perform visual question answering (VQA), captioning, object classification, and object localization on the COCO dataset [29], as well as test zero-shot generalization to a referring expression task. To test generality of concepts across skills, we present a new split of the COCO images and corresponding task annotations called COCO-SCE (Skill-Concept Evaluation). In COCO-SCE, some concepts (objects) are held-out from each task but exposed via other tasks, and then evaluate performance on samples containing held-out concepts. To test generality of learning, we fine-tune our system on the referring expressions task and measure its learning curve and extent of forgetting previously learned tasks.\\n\\nIn summary, our main contributions include: (1) An end-to-end trainable, task-agnostic vision-language architecture for learning and performing classification, grounding, visual question answering, captioning, and other tasks that involve image, text and bounding box modalities. (2) Evaluation that tests generality of architecture, skill-concept transfer, and learning ability.\\n\\n### 2. Related Work\\n\\n#### Single-purpose vision-language models.\\n\\nOver the last decade, specialized and effective approaches have been developed for vision-language tasks, including image captioning [10,21,25,33,47,54], phrase grounding [37,38,43], referring expression comprehension [22, 34], visual question answering (VQA) [2,11,16,48,51,55], visual dialog [7], and text-to-image generation [6]. Advances that have pushed the performance envelope include cross-model transformer architectures [45], powerful self-supervised [3, 8, 28] and multitask [41] language models, pretrained visual representations from object and attribute detectors [1, 57] or text-conditioned detectors [20], and large-scale image/video-text [19, 27, 39, 56] pretraining.\\n\\n#### N-purpose vision-language models.\\n\\nSeveral recent works aim to unify vision-language tasks with a common architecture. UniT trains a single model for 7 tasks including detection and vision-language tasks but uses task-specific heads and does not support captioning. 12-in-1 [32] jointly trains VilBERT [31] on 12 vision-language tasks but with 6 output heads (1 per task group). VL-T5 [5] adapts T5 [41], a text-to-text architecture pretrained on a mix of self-supervised and supervised tasks, to jointly train on vision-language tasks with only a text generation head (T5\u2019s text decoder). Both of these approaches rely on preextracted bounding boxes and region features from an object and attributes detector [1] and are not end-to-end trainable. E2E-VLP [53] presents an end-to-end trainable architecture that is extensively pretrained with masked language modeling, image-text matching, captioning, and object detection objectives, each with a different output head. However, the pretrained model is finetuned separately on each task and therefore does not support multiple tasks with a common set of weights. On the other hand, GPV-1 is both end-to-end trainable and jointly trained on multiple vision-language tasks. Our architecture takes an image and a textual task description as inputs, and has an output head per modality, namely text, bounding boxes, and relevance scores. Other exciting efforts towards creating general purpose vision architectures include Perceiver [18] and Perceiver IO [17], but their potential for multitask learning and utility for vision-language tasks such as VQA and captioning remains to be explored.\\n\\n#### Task descriptions as a means to architecture generality.\\n\\nSeveral works in the natural language domain have tried to blur or erase artificial task boundaries by framing each task as text-to-text transformation with the task specified through a task description. Task descriptions range from templated prompts [41] to natural language descriptions [36, 49]. Kumar et al. [26] show that multiple tasks,\"}"}
{"id": "CVPR-2022-1874", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Describe this image.\\n\\nA dog and a cat sitting on a bed.\\n\\nFigure 2. Architecture of GPV-1. Vision, language, and cross-modal modules are color-coded (see Sec. 3 for details).\\n\\nSuch as part-of-speech tagging, question answering, and classification, can be formulated as a sequence-to-sequence transformation and solved with a single task-agnostic architecture but with separate parameters trained for each task. Works such as DecaNLP [35] and UnifiedQA [23] have trained single models to perform multiple tasks by reformulating each task as question answering allowing the individual task-performances to benefit from more data with diverse supervision while sharing model parameters. Works such as T5 [41], GPT [3,40] have also highlighted the transfer learning capabilities of unified models specially in zero-shot and few-shot scenarios.\\n\\nSkill-Concept Evaluation. Few works attempt to learn a concept for one task and apply it to another, e.g. learning to categorize an image as \u201caardvark\u201d and being able to detect or answer questions about aardvarks. As one example, Gupta et al. [12] show that formulating visual recognition and VQA in terms of inner products of word and image-region representations leads to inductive transfer between recognition and VQA tasks. Other works focus on unidirectional transfer to a single task such as captioning [15] or VQA [50]. With our COCO-SCE benchmark, we propose a systematic evaluation of generality of concepts across four standard vision-language tasks by holding out certain concepts from each task while exposing them via other tasks and then measuring performance separately on seen and held-out concepts for each task.\\n\\n3. The GPV-1 model\\n\\n3.1. Architecture Overview\\n\\nThe most distinctive aspect of our GPV-1 system is that tasks are defined through natural language text input, instead of multi-head outputs. Most systems, for example, that perform ImageNet [9] classification and COCO detection would have one 1000-class confidence output head and another 80-class box and confidence output head. More tasks or more datasets would require more output heads. Once trained, such a system will always produce 1,080 types of confidence and 80 classes of bounding boxes.\\n\\nGPV-1 does not have explicit task boundaries and instead takes in a natural language task description such as \u201cWhat is sitting on the sofa?\u201d (VQA), \u201cFind all instances of dogs\u201d (localization), \u201cWhat is going on in the image\u201d (captioning), or \u201cWhat kind of object is this?\u201d (classification). GPV-1 interprets and performs all tasks using the same language/vision/cross-modal encoders and decoders. In training, the localization task has bounding box ground truth, while others such as classification, question answering, and captioning have text ground truth. Yet, all tasks involve common skills such as interpreting the task description, localizing objects, representing image regions, and determining relevance to the task. A new task, such as referring expressions which has bounding box ground truth, can be defined simply by providing new inputs (\u201cFind the man wearing a green shirt\u201d) and output supervision (bounding box). Thus, limited only by modalities that it can sense and produce, GPV-1 can be trained to perform a wide range of tasks without task-specific modifications to the architecture or learning.\\n\\nFig. 2 provides an overview of GPV-1's architecture consisting of a visual encoder, language encoder, vision-language co-attention module, and output heads for the supported output modalities \u2013 boxes, relevance scores, and text. First, we encode the image using the CNN backbone and the transformer encoder-decoder from DETR [4], an end-to-end trainable object detector. Simultaneously, the natural language task description is encoded with BERT [8]. Then, to cross-contextualize representations from the visual and language encoders, we use ViLBERT\u2019s co-attention module [31]. Box and objectness heads predict task-agnostic bounding boxes and scores. The relatedness head predicts a task-specific score for each output box that is combined with the objectness scores to obtain relevance scores.\"}"}
{"id": "CVPR-2022-1874", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"text decoder is an autoregressive transformer decoder that generates text output with relevance-conditioned outputs from cross-modal module serving as memory.\\n\\n3.2. Vision modules\\n\\nWe use a DETR based visual encoder. A ResNet-50 backbone extracts a convolutional feature map that is fed into DETR's transformer encoder to get contextualized features for every grid location. The transformer decoder takes as input \\\\( R (=100) \\\\) object queries (learned constant vectors) and the contextualized grid features and produces region descriptors per object query. The main intuition is that the object queries serve as learnable anchors, and the transformer encoder-decoder trained on detection eliminates the need for non-maximum suppression as a post-processing step. The complete region encoding is obtained by concatenating DETR's transformer features, which encode location and limited appearance information, with RoI pooled features from the CNN backbone.\\n\\nAs a vision decoder, GPV-1 uses DETR's box head to predict bounding boxes from region descriptors, resulting in \\\\( R \\\\) region proposals. These bounding boxes are used for grounding and detection tasks as well as for RoI pooling from the CNN backbone. We also replace DETR's 80-way object classification layer with a binary objectness classification layer, which contributes to determining relevance.\\n\\n3.3. Language modules\\n\\nThe language encoder is used to encode the task description. We use BERT's WordPiece tokenizer to obtain sub-word tokens for the language input and a pretrained BERT model to compute representations. Sub-word tokenization provides robustness to out-of-vocabulary words, and large scale language model pretraining allows GPV-1 to better handle paraphrases of language queries and zero-shot generalization to novel task descriptions, assuming semantic similarity to previously seen descriptions in the BERT embedding space.\\n\\nThe language decoder outputs words to classify, describe, or answer the input. Specifically, the sequence of co-attended region representations and language query's token representations are concatenated to construct a single sequence that serves as memory for the transformer text decoder. At each generation step, the sequence of words generated thus far are fed into the decoder along with the memory and a distribution over the vocabulary words is predicted to sample the next word. The inputs to the transformer decoder are trainable word embeddings. The output logit for a vocabulary word is obtained by taking dot product between the embedding vector output by the decoder and a linearly transformed BERT encoding of the word.\\n\\n3.4. Cross-modal modules\\n\\nThe region descriptors from the vision modules and sub-token representations from the language module are transformed by linear layers to equal dimension vectors and fed into ViLBERT's co-attention layers for cross-contextualization. The relatedness head uses the co-attended region features to predict logits that indicate relevance of regions to the task description. These logits are added to logits from the objectness head and transformed into region-relevance scores by a sigmoid activation. These relevance scores are used to rank bounding boxes or indicate importance of regions to performing the task.\\n\\nRelevance conditioning modulates the co-attended visual features with relevance scores. Specifically, the relevance score \\\\( s \\\\) of each region is used to weight learned vectors \\\\( \\\\{v_{rel}, v_{nrel}\\\\} \\\\), which are added to the region features before feeding to the decoder. This conditioning enables supervision from the text decoder to affect the relatedness and objectness heads. In this way, a model trained to produce captions for images of peacocks may learn to localize peacocks, and, conversely, the ability to localize peacocks may translate to improved caption quality.\\n\\n3.5. Training\\n\\nEach training sample consists of an image, a task description, and targets. Depending on the task, targets could consist of ground truth bounding boxes, text, or both. In each training iteration, we uniformly draw samples across all tasks to construct mini-batches. For all samples that contain a text target, we maximize the log-likelihood of the ground truth text. For all samples that contain bounding boxes as targets, we use DETR's Hungarian loss for training the box and relevance prediction.\\n\\nInitialization. We initialize all vision modules except the last linear layer in the objectness head with weights from DETR pretrained on either COCO or COCO-SCE (Sec. 4.2) object detection data. BERT is pretrained on BooksCorpus and English Wikipedia.\\n\\nOptimization. We train GPV-1 with a batch size of 120 and AdamW optimizer. We keep DETR weights frozen for the first 10 epochs and finetune all modules except BERT for 30 more epochs. For learning rate (LR), we do a warm-up over the first 4 epochs to a maximum of \\\\( 10^{-4} \\\\) followed by linear decay to 0. Following DETR, we apply gradient clipping on visual module parameters and use a maximum learning rate of \\\\( 10^{-5} \\\\) for the CNN backbone.\\n\\nWe use a 0.05 \\\\times lower text loss weight for captioning since more words are in the target text than other tasks.\\n\\n4. Tasks and Data\\n\\nOur experiments involve 5 tasks using images from the COCO dataset and annotations from the COCO, VQA V2 [11], and REFCCOCO+ [22] datasets. Sec. 4.1\"}"}
