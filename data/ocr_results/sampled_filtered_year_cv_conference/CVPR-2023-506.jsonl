{"id": "CVPR-2023-506", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. Poison detection results. We find that iterative search has high top-20 Acc, which is the accuracy of finding poisons in top-20 ranked images. In the right 3 columns, we show the effectiveness of the poison classification model at filtering out the poisons. Rec. is recall, Prec. is precision, while Total Rem. is the total number of samples removed by the classifier. We can see that the classifier can identify most of the poisons since recall is \\\\( \\\\approx 98\\\\% \\\\). Note that removing a small number of clean images does not significantly damage the final SSL model as shown in Table 2.\\n\\nFigure 2. Top ranked poisonous and non-poisonous images. We visualize a few top ranked poisonous (1st row) and non-poisonous (2nd row) images found by PatchSearch for BYOL, ResNet-18, poison rate 0.5% and target category Tabby Cat.\\n\\n5.1. Main Results\\n\\nPoison detection results of PatchSearch. We report the results of poison detection for PatchSearch at different stages in Table 1. We can see that at the end of iterative search, most of the top-20 images are indeed poisonous. We can also see that poison classification leads to very high recall with acceptable precision. Finally, we visualize a few top ranked poisonous and non-poisonous images found with PatchSearch in Figure 2.\\n\\nEffectiveness of PatchSearch and i-CutMix. Table 2 lists the results of using i-CutMix and PatchSearch for defending against backdoor attacks on 4 different settings. Note that we make it easier for PatchSearch to find poisons by training an easy to attack model during defense. Therefore, since i-CutMix can suppress the attack, we use models trained without it during PatchSearch. Similarly, we find that the attack is not strong enough to be detectable by PatchSearch for ResNet-18 and MoCo-v2 models (Table A5). Therefore, we use the dataset filtered by PatchSearch with ViT-B, MoCo-v3 models. Our results indicate that i-CutMix not only improves the clean accuracy of the models but also reduces the attack effectiveness. However, i-CutMix cannot fully remove the effect of poisoning and it suppresses the learning of backdoor without explicitly identifying poisons. Here, PatchSearch is a good alternative, which not only significantly suppresses the attack, but also identifies poisoned samples that can be verified by the defender to detect an attack. This may have value in some applications where tracking the source of poisoned samples is important. Finally, the two defenses are complementary to each other, and their combination results in a model that behaves very close to a clean model. PatchSearch removes most poisoned samples while i-CutMix suppresses the effect of some remaining ones. Detailed results including per category results, and mean and variance information can be found in Section A.3 in supplementary material.\\n\\nComparison with trusted data defense. Table 3 compares PatchSearch to the trusted data defense from [44]. Their defense trains a student model from a backdoored teacher with knowledge distillation (KD) [4] on poison-free, trusted data. However, trusted data can be difficult to acquire in SSL and may have its own set of biases. The results in Table 3 show that PatchSearch offers a good trade-off between accuracy and FP despite not using any trusted data. Compared to KD with 25% trusted data, which may not be practical in many cases, PatchSearch is better than KD by \\\\( \\\\approx 4 \\\\) points on clean and patched accuracies at the cost of a slightly higher FP.\\n\\nFinetuning as a defense. Since the downstream task data is clean and labeled, a very simple defense strategy would be to finetune SSL models on it instead of only training a linear layer. Hence, we compare finetuning with our defense in Table 4. We find that the attack with linear probing increases the FP from 17 to 1708 while the attack with finetuning the whole model increases the FP from 18 to 685. We see that 685 is still much higher than 18, so the attack is still strong with finetuning the whole model. Obviously, one can do finetuning with a very large learning rate to forget the effect of the poison, but that reduces the accuracy of the model due to overfitting to the downstream task and forgetting the pretraining. With finetuning, different model weights need to be stored for different downstream tasks instead of a single general model, which may not be desirable. Moreover, in settings like few shot learning, the available data can be too little for effective finetuning. Finally, the results also show that all three defenses, PatchSearch, i-CutMix, and finetuning, are complementary to each other and can be combined to get a favorable trade-off between accuracy (Acc) and FP (last row of Table 4).\\n\\nBackdoored MAE vs. defended MoCo-v3. The results\"}"}
{"id": "CVPR-2023-506", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model Type                | Clean Data Patched Data Clean Data Patched Data |\\n|---------------------------|--------------------------------------------------|\\n| ViT-B MoCo-v3, poison rate 0.5% | Acc: 70.5, FP: 18.5, ASR: 0.4                  |\\n|                           | Acc: 64.6, FP: 27.2, ASR: 0.5                  |\\n| Clean + i-CutMix          | Acc: 75.6, FP: 15.6, ASR: 0.3                  |\\n|                           | Acc: 74.4, FP: 14.6, ASR: 0.3                  |\\n| Backdoored                | Acc: 70.6, FP: 17.4, ASR: 0.4                  |\\n|                           | Acc: 46.9, FP: 1708.9, ASR: 34.5               |\\n| Backdoored + i-CutMix     | Acc: 75.6, FP: 14.9, ASR: 0.3                  |\\n|                           | Acc: 72.2, FP: 242.2, ASR: 4.9                |\\n| PatchSearch               | Acc: 70.2, FP: 23.1, ASR: 0.5                  |\\n|                           | Acc: 64.5, FP: 39.8, ASR: 0.8                  |\\n|                           | Acc: 70.1, FP: 43.4, ASR: 0.9                  |\\n|                           | Acc: 63.7, FP: 76.0, ASR: 1.5                  |\\n|                           | Acc: 64.7, FP: 76.0, ASR: 1.5                  |\\n\\n| Model Trusted Data        | Clean Data Patched Data Clean Data Patched Data |\\n|---------------------------|--------------------------------------------------|\\n| Clean                     | Acc: 100%, FP: 49.9, ASR: 47.0                   |\\n| Backdoored                | Acc: 0%, FP: 50.1, ASR: 50.1                   |\\n| KD Defense 25%            | Acc: 44.6, FP: 34.5, ASR: 34.5                  |\\n|                           | Acc: 38.3, FP: 40.5, ASR: 40.5                  |\\n|                           | Acc: 32.1, FP: 41.0, ASR: 41.0                  |\\n| KD Defense 10%            | Acc: 49.4, FP: 40.1, ASR: 40.1                  |\\n|                           | Acc: 38.3, FP: 40.5, ASR: 40.5                  |\\n| KD Defense 5%             | Acc: 49.4, FP: 40.1, ASR: 40.1                  |\\n|                           | Acc: 38.3, FP: 40.5, ASR: 40.5                  |\\n| PatchSearch 0%            | Acc: 49.4, FP: 40.1, ASR: 40.1                  |\\n\\nTable 2. Defense results. We compare clean, backdoored, and defended models in different configurations. **Acc** is the accuracy on validation set, **FP** is the number of false positives for the target category, and **ASR** is FP as percentage. All results are averaged over 5 evaluation runs and 10 categories. We observe the following: (1) Presence of i-CutMix significantly boosts clean accuracy for models trained with MoCo, but this boost is small for BYOL models. (2) While i-CutMix reduces the effectiveness of attacks in all cases, there is still room for improvement. (3) PatchSearch alone can significantly mitigate backdoor attacks. (4) patched accuracies of models defended with PatchSearch + i-CutMix are very close to clean models in all cases.\\n\\nTable 3. Comparison with trusted-data defense. We compare PatchSearch with the trusted data and knowledge distillation (KD) defense [44]. For a fair comparison, we use the setting from [44] without i-CutMix: ResNet-18, MoCo-v2, and poison rate 1.0%. All results except the last row are from [44]. We find that despite not relying on any trusted data, our defense can suppress the attack to the level of KD + 5% trusted data. Moreover, our model has significantly higher accuracies: 49.4% vs 32.1% on clean data and 45.9% vs 29.4% on patched data. Compared to KD + 25% trusted data, our model has higher accuracies with only slightly higher FP.\\n\\nTable 4. Finetuning as defense. We show results of using finetuning the whole model on downstream tasks as a defense for ViT-B, MoCo-v3, and poison rate 0.5%. Finetuning requires storing one model per task, which may not be desirable. We find that finetuning can mitigate the attack and is complementary to PatchSearch and i-CutMix.\\n\\nTable 5. Finetuning data, but MAE quickly catches up in the 10% regime. We show a similar pattern even for officially pretrained [2, 3] (on ImageNet-1000) models in Table 6.\\n\\nTable 6. Attack on a downstream task. Following [44], so far we have only considered the case where evaluation dataset is a subset of the training dataset. However, SSL models are intended to be used for downstream datasets that are different from the pre-training dataset. Hence, we show in Table 7 that it is possible to attack one of the categories used in the downstream task of Food101 [6] classification. See Section A.1 and Table A7 of supp. for more details.\"}"}
{"id": "CVPR-2023-506", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 5. Backdoored MAE vs. defended MoCo-v3. While the inherent robustness of MAE to backdoor attacks [44] is appealing, we show that a properly defended MoCo-v3 model has better clean accuracy and lower FP compared with a backdoored MAE. Setting: ViT-B, poison rate 0.5%, and average of 4 target categories (Rottweiler, Tabby Cat, Ambulance, and Laptop).\\n\\nTable 6. MoCo-v3 is better than MAE in finetuning on less data. We show that similar to Table 5 results MAE needs a large amount of finetuning data to be on par with MoCo-v3. Finetuning official, unlabeled ImageNet-1K pre-trained, ViT-B models on 1% of the training set results in MoCo-v3 outperforming MAE. However, MAE catches up in 10% and 100% regimes. Note that only this table uses the ImageNet-1K dataset.\\n\\nTable 7. Attack on downstream Food101 classification. Instead of attacking one of the categories in the pre-training dataset, we choose a target category from Food101 [6] dataset and poison 700 of its images and add them to the pre-training dataset (ImageNet-100). The models are evaluated on Food101, and there are 5050 val images in total. We find that the attack is successful and also that PatchSearch can defend against the attack. Setting: BYOL & ResNet-18. See Table A7 of supp. for detailed results.\\n\\n5.2. Ablations\\n\\ni-CutMix. It is interesting that a method as simple as i-CutMix can be a good defense. Therefore, we attempt to understand which components of i-CutMix make it a good defense. i-CutMix has two components: creating a composite image by pasting a random crop from a random image, and replacing the original one-hot loss with a weighted one. The weights are in proportion to the percentages of the mixed images. We observe the effect of these components in Table 8. We can also get an insight into the working of i-CutMix by using PatchSearch as an interpretation algorithm. Setting: ViT-B, MoCo-v3, poison rate 0.5%, target category Rottweiler.\\n\\nFigure 3. Effect of i-CutMix on potency of patches. We hypothesize that i-CutMix encourages the model to use global features rather than focusing on small regions. To explore this, we use PatchSearch as an interpretation algorithm and analyze the impact of i-CutMix on clean patches. We use a clean model to calculate FP of all clean training images with PatchSearch (no iterative search) and use the resulting FP as potency score. FP for each category is calculated as the max FP of all patches in it (shown below images). We find that i-CutMix suppresses the impact of individual patches (resulting in smaller potency score), which is aligned with our hypothesis. The total potency (sum of FP) of patches from all categories is reduced by 71.3% for BYOL, ResNet-18 and 45.4% for MoCo-v3, ViT-B. We show images for five random categories with and without i-CutMix (BYOL, ResNet-18).\\n\\nEffect of w. An important hyperparameter of PatchSearch is w which controls the size of the extracted candidate trigger. Since the defender does not know the size of the trigger, wrong choice of w can lead to poor poison detection performance as shown in Figure 4. However, it is possible to automate the process of finding a good value for w. We know that if the choice of w is wrong, most poison scores will be similar. Hence, the w resulting in high variance in poisons scores should be closer to actual trigger size used by the attacker. However, we need to adjust the scale...\"}"}
{"id": "CVPR-2023-506", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Effect of $w$. We plot the Top-$20$ Acc against $w$ values sampled uniformly in log-space from 10 to 200. We find highest accuracies for $w$ near 50 which is the actual trigger size. However, the defender does not know the actual trigger size. Hence, we propose relative standard deviation (RSD) of the population of poison scores as a heuristic to pick $w$. RSD is standard deviation divided by the mean. In all cases, $w$ with highest RSD (marked with \u2605 in the figure) also corresponds to the highest top-$20$ accuracy.\\n\\nOther ablations. In Table 9, we explore a stronger form of attack, where the trigger is pasted five times instead of once on target category images in training. This results in the trigger occupying roughly 25% area of the image. In Table 10, we consider the effect of applying PatchSearch to clean data since a defender does not know whether a dataset is poisoned. We see only a small degradation in accuracy. In Figure 5, we consider the effect of removing the poison classifier, and find that resulting image ranking shows poor precision vs. recall trade-off in some cases. See Section A.1 of supplementary for more ablations and details.\\n\\n6. Conclusion\\n\\nWe introduced a novel defense algorithm called PatchSearch for defending self-supervised learning against patch-based data poisoning backdoor attacks. PatchSearch identifies poisons and removes them from the training set. We also show that the augmentation strategy of $i$-CutMix is a good baseline defense. We find that PatchSearch is better than $i$-CutMix and the SOTA defense that uses trusted data. Finally, PatchSearch and $i$-CutMix are complementary to each and their combination improves the model performance while effectively mitigating the attack. Note that our defense assumes that the trigger is smaller than the objects of interest. Also, our defense is for patch-based attacks only. These may be limiting in defending future attacks. We hope our paper will encourage developing better backdoor attack and defense methods for SSL models.\\n\\nAcknowledgement.\\n\\nThis work is partially supported by DARPA Contract No. HR00112190135, HR00112290115, and FA8750-19-C-0098, NSF grants 1845216 and 1920079, NIST award 60NANB18D279, and also funding from Shell Inc., and Oracle Corp. We would also like to thank K L Navaneet and Aniruddha Saha for many helpful discussions.\"}"}
{"id": "CVPR-2023-506", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Code for Backdoor Attacks on Self-Supervised Learning\\npaper. https://github.com/UMBCvision/SSL-Backdoor\\n\\n[2] MAE models and code. https://github.com/facebookresearch/mae\\n\\n[3] MoCo-v3 models and code. https://github.com/facebookresearch/moco-v3\\n\\n[4] Soroush Abbasi Koohpayegani, Ajinkya Tejankar, and Hamed Pirsiavash. Compress: Self-supervised learning by compressing representations. Advances in Neural Information Processing Systems, 33:12980\u201312992, 2020.\\n\\n[5] Eitan Borgnia, Valeriia Cherepanova, Liam Fowl, Amin Ghiasi, Jonas Geiping, Micah Goldblum, Tom Goldstein, and Arjun Gupta. Strong data augmentation sanitizes poisoning and backdoor attacks without an accuracy tradeoff. In ICASSP 2021-2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 3855\u20133859. IEEE, 2021.\\n\\n[6] Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101 \u2013 mining discriminative components with random forests. In European Conference on Computer Vision (ECCV), 2014.\\n\\n[7] Tom B Brown, Dandelion Man \u00b4e, Aurko Roy, Mart\u00b4\u0131n Abadi, and Justin Gilmer. Adversarial patch. arXiv preprint arXiv:1712.09665, 2017.\\n\\n[8] Nicholas Carlini and Andreas Terzis. Poisoning and backdooring contrastive learning. In International Conference on Learning Representations (ICLR), 2022.\\n\\n[9] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsupervised learning of visual features by contrasting cluster assignments. In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\n[10] Bryant Chen, Wilka Carvalho, Nathalie Baracaldo, Heiko Ludwig, Benjamin Edwards, Taesung Lee, Ian Molloy, and Biplav Srivastava. Detecting backdoor attacks on deep neural networks by activation clustering. arXiv preprint arXiv:1811.03728, 2018.\\n\\n[11] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for contrastive learning of visual representations. arXiv preprint arXiv:2002.05709, 2020.\\n\\n[12] Xinlei Chen, Haoqi Fan, Ross Girshick, and Kaiming He. Improved baselines with momentum contrastive learning. arXiv preprint arXiv:2003.04297, 2020.\\n\\n[13] Xinlei Chen, Saining Xie, and Kaiming He. An empirical study of training self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9640\u20139649, 2021.\\n\\n[14] Edward Chou, Florian Tramer, and Giancarlo Pellegrino. Sentinet: Detecting localized universal attacks against deep learning systems. In 2020 IEEE Security and Privacy Workshops (SPW), pages 48\u201354. IEEE, 2020.\\n\\n[15] Terrance DeVries and Graham W Taylor. Improved regularization of convolutional neural networks with cutout. arXiv preprint arXiv:1708.04552, 2017.\\n\\n[16] Bao Gia Doan, Ehsan Abbasnejad, and Damith C Ranasinghe. Februus: Input purification defense against trojan attacks on deep neural network systems. In Annual Computer Security Applications Conference, pages 897\u2013912, 2020.\\n\\n[17] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\\n\\n[18] Alexey Dosovitskiy, Jost Tobias Springenberg, Martin Riedmiller, and Thomas Brox. Discriminative unsupervised feature learning with convolutional neural networks. In Advances in neural information processing systems (NuerIPS), 2014.\\n\\n[19] Yansong Gao, Change Xu, Derui Wang, Shiping Chen, Damith C Ranasinghe, and Surya Nepal. Strip: A defence against trojan attacks on deep neural networks. In Proceedings of the 35th Annual Computer Security Applications Conference, 2019.\\n\\n[20] Spyros Gidaris, Praveer Singh, and Nikos Komodakis. Unsupervised representation learning by predicting image rotations. In International Conference on Learning Representations (ICLR), 2018.\\n\\n[21] Jacob Gildenblat and contributors. Pytorch library for cam methods. https://github.com/jacobgil/pytorch-grad-cam, 2021.\\n\\n[22] Priya Goyal, Mathilde Caron, Benjamin Lefaudeux, Min Xu, Pengchao Wang, Vivek Pai, Mannat Singh, Vitaliy Liptchinsky, Ishan Misra, Armand Joulin, et al. Self-supervised pretraining of visual features in the wild. arXiv preprint arXiv:2103.01988, 2021.\\n\\n[23] Jean-Bastien Grill, Florian Strub, Florent Altch\u00e9, Corentin Tallec, Pierre H Richemond, Elena Buchatskaya, Carl Dersch, Bernardo Avila Pires, Zhaohan Daniel Guo, Mohamad Gheshlaghi Azar, et al. Bootstrap your own latent: A new approach to self-supervised learning. arXiv preprint arXiv:2006.07733, 2020.\\n\\n[24] Tianyu Gu, Brendan Dolan-Gavitt, and Siddharth Garg. Badnets: Identifying vulnerabilities in the machine learning model supply chain. arXiv preprint arXiv:1708.06733, 2017.\\n\\n[25] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009, 2022.\\n\\n[26] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\n[27] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\"}"}
{"id": "CVPR-2023-506", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Sanghyun Hong, Varun Chandrasekaran, Yi\u02d8gitcan Kaya, Tudor Dumitras\u00b8, and Nicolas Papernot. On the effectiveness of mitigating data poisoning attacks with gradient shaping. arXiv preprint arXiv:2002.11497, 2020.\\n\\nKunzhe Huang, Yiming Li, Baoyuan Wu, Zhan Qin, and Kui Ren. Backdoor defense via decoupling the training process. In International Conference on Learning Representations, 2022.\\n\\nShanjiaoyang Huang, Weiqi Peng, Zhiwei Jia, and Zhuowen Tu. One-pixel signature: Characterizing cnn models for backdoor detection. In European Conference on Computer Vision, pages 326\u2013341. Springer, 2020.\\n\\nXijie Huang, Moustafa Alzantot, and Mani Srivastava. Neuroninspect: Detecting backdoors in neural networks via output explanations. arXiv preprint arXiv:1911.07399, 2019.\\n\\nSoheil Kolouri, Aniruddha Saha, Hamed Pirsiavash, and Heiko Hoffmann. Universal litmus patterns: Revealing backdoor attacks in cnns. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 301\u2013310, 2020.\\n\\nKibok Lee, Yian Zhu, Kihyuk Sohn, Chun-Liang Li, Jinwoo Shin, and Honglak Lee. i-mix: A domain-agnostic strategy for contrastive representation learning. In International Conference on Learning Representations, 2021.\\n\\nYiming Li, Yong Jiang, Zhifeng Li, and Shu-Tao Xia. Backdoor learning: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.\\n\\nYige Li, Xixiang Lyu, Nodens Koren, Lingjuan Lyu, Bo Li, and Xingjun Ma. Anti-backdoor learning: Training clean models on poisoned data. Advances in Neural Information Processing Systems, 34:14900\u201314912, 2021.\\n\\nKang Liu, Brendan Dolan-Gavitt, and Siddharth Garg. Fine-pruning: Defending against backdooring attacks on deep neural networks. In International Symposium on Research in Attacks, Intrusions, and Defenses, pages 273\u2013294. Springer, 2018.\\n\\nYingqi Liu, Shiqing Ma, Yousra Aafer, Wen-Chuan Lee, Juan Zhai, Weihang Wang, and Xiangyu Zhang. Trojaning attack on neural networks. 2017.\\n\\nYuntao Liu, Yang Xie, and Ankur Srivastava. Neural trojans. In 2017 IEEE International Conference on Computer Design (ICCD), 2017.\\n\\nMehdi Noroozi and Paolo Favaro. Unsupervised learning of visual representations by solving jigsaw puzzles. In European Conference on Computer Vision (ECCV), 2016.\\n\\nAlec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language super-vision. In International Conference on Machine Learning, pages 8748\u20138763. PMLR, 2021.\\n\\nOlga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-Fei. ImageNet Large Scale Visual Recognition Challenge. International Journal of Computer Vision (IJCV), 115(3):211\u2013252, 2015.\\n\\nAniruddha Saha, Akshayvarun Subramanya, Koninika Patil, and Hamed Pirsiavash. Role of spatial context in adversarial robustness for object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 784\u2013785, 2020.\\n\\nAniruddha Saha, Akshayvarun Subramanya, and Hamed Pirsiavash. Hidden trigger backdoor attacks. In Proceedings of the AAAI Conference on Artificial Intelligence, 2020.\\n\\nAniruddha Saha, Ajinkya Tejankar, Soroush Abbasi Koohpayegani, and Hamed Pirsiavash. Backdoor attacks on self-supervised learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nRamprasaath R Selvaraju, Michael Cogswell, Abhishek Das, Ramakrishna Vedantam, Devi Parikh, and Dhruv Batra. Grad-cam: Visual explanations from deep networks via gradient-based localization. In Proceedings of the IEEE international conference on computer vision, pages 618\u2013626, 2017.\\n\\nAli Shafahi, W Ronny Huang, Mahyar Najibi, Octavian Suciu, Christoph Studer, Tudor Dumitras, and Tom Goldstein. Poison frogs! targeted clean-label poisoning attacks on neural networks. In Advances in Neural Information Processing Systems (NeurIPS), 2018.\\n\\nKoohpayegani Soroush Abbasi, Ajinkya Tejankar, and Hamed Pirsiavash. Mean shift for self-supervised learning. In International Conference on Computer Vision (ICCV), 2021.\\n\\nAjinkya Tejankar, Soroush Abbasi Koohpayegani, Vipin Pillai, Paolo Favaro, and Hamed Pirsiavash. Isd: Self-supervised learning by iterative similarity distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), 2021.\\n\\nYonglong Tian, Dilip Krishnan, and Phillip Isola. Contrastive multiview coding. In European conference on computer vision, pages 776\u2013794. Springer, 2020.\\n\\nYonglong Tian, Chen Sun, Ben Poole, Dilip Krishnan, Cordelia Schmid, and Phillip Isola. What makes for good views for contrastive learning? In Advances in Neural Information Processing Systems (NeurIPS), 2020.\\n\\nBrandon Tran, Jerry Li, and Aleksander Madry. Spectral signatures in backdoor attacks. Advances in neural information processing systems (NeurIPS), 2018.\\n\\nAlexander Turner, Dimitris Tsipras, and Aleksander Madry. Clean-label backdoor attacks. 2018.\\n\\nBolun Wang, Yuanshun Yao, Shawn Shan, Huiying Li, Bimal Viswanath, Haitao Zheng, and Ben Y Zhao. Neural cleanse: Identifying and mitigating backdoor attacks in neural networks. In 2019 IEEE Symposium on Security and Privacy (SP), pages 707\u2013723. IEEE, 2019.\\n\\nZhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin. Unsupervised feature learning via non-parametric instance discrimination. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2018.\\n\\nSangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable.\"}"}
{"id": "CVPR-2023-506", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[56] Yi Zeng, Won Park, Z Morley Mao, and Ruoxi Jia. Rethinking the backdoor attacks\u2019 triggers: A frequency perspective. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 16473\u201316481, 2021.\\n\\n[57] Richard Zhang, Phillip Isola, and Alexei A Efros. Colorful image colorization. In Proceedings of the European Conference on Computer Vision (ECCV), 2016.\\n\\n[58] Songzhu Zheng, Yikai Zhang, Hubert Wagner, Mayank Goswami, and Chao Chen. Topological detection of trojaned neural networks. Advances in Neural Information Processing Systems, 34:17258\u201317272, 2021.\\n\\n[59] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and Yi Yang. Random erasing data augmentation. In Proceedings of the AAAI conference on artificial intelligence, volume 34, pages 13001\u201313008, 2020.\"}"}
{"id": "CVPR-2023-506", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Defending Against Patch-based Backdoor Attacks on Self-Supervised Learning\\n\\nAjinkya Tejankar\\n\\nMaziar Sanjabi\\n\\nQifan Wang\\n\\nSinong Wang\\n\\nHamed Firooz\\n\\nHamed Pirsiavash\\n\\nLiang Tan\\n\\n1 University of California, Davis\\n2 Meta AI\\n\\nAbstract\\n\\nRecently, self-supervised learning (SSL) was shown to be vulnerable to patch-based data poisoning backdoor attacks. It was shown that an adversary can poison a small part of the unlabeled data so that when a victim trains an SSL model on it, the final model will have a backdoor that the adversary can exploit. This work aims to defend self-supervised learning against such attacks. We use a three-step defense pipeline, where we first train a model on the poisoned data. In the second step, our proposed defense algorithm (PatchSearch) uses the trained model to search the training data for poisoned samples and removes them from the training set. In the third step, a final model is trained on the cleaned-up training set. Our results show that PatchSearch is an effective defense. As an example, it improves a model\u2019s accuracy on images containing the trigger from 38.2% to 63.7% which is very close to the clean model\u2019s accuracy, 64.6%. Moreover, we show that PatchSearch outperforms baselines and state-of-the-art defense approaches including those using additional clean, trusted data. Our code is available at https://github.com/UCDvision/PatchSearch\\n\\n1. Introduction\\n\\nSelf-supervised learning (SSL) promises to free deep learning models from the constraints of human supervision. It allows models to be trained on cheap and abundantly available unlabeled, uncurated data [22]. A model trained this way can then be used as a general feature extractor for various downstream tasks with a small amount of labeled data. However, recent works [8, 44] have shown that uncurated data collection pipelines are vulnerable to data poisoning backdoor attacks.\\n\\nData poisoning backdoor attacks on self-supervised learning (SSL) [44] work as follows. An attacker introduces \u201cbackdoors\u201d in a model by simply injecting a few carefully crafted samples called \u201cpoisons\u201d in the unlabeled training dataset. Poisoning is done by pasting an attacker chosen \u201ctrigger\u201d patch on a few images of a \u201ctarget\u201d category. A model pre-trained with such a poisoned data behaves similar to a non-poisoned/clean model in all cases except when it is shown an image with a trigger. Then, the model will cause a downstream classifier trained on top of it to mis-classify the image as the target category. This types of attacks are sneaky and hard to detect since the trigger is like an attacker\u2019s secret key that unlocks a failure mode of the model. Our goal in this work is to defend SSL models against such attacks.\\n\\nWhile there have been many works for defending supervised learning against backdoor attacks [34], many of them directly rely upon the availability of labels. Therefore, such methods are hard to adopt in SSL where there are no labels. However, a few supervised defenses [5, 28] can be applied to SSL with some modifications. One such defense, proposed in [28], shows the effectiveness of applying a strong augmentation like CutMix [55]. Hence, we adopt i-CutMix [33], CutMix modified for SSL, as a baseline. We show that i-CutMix is indeed an effective defense that additionally improves the overall performance of SSL models.\\n\\nAnother defense that does not require labels was proposed in [44]. While this defense successfully mitigates the attack, its success is dependent on a significant amount of clean, trusted data. However, access to such data may not be practical in many cases. Even if available, the trusted data may have its own set of biases depending on its sources which might bias the defended model. Hence, our goal is to design a defense that does not need any trusted data.\\n\\nIn this paper, we propose PatchSearch, which aims at identifying poisoned samples in the training set without access to trusted data or image labels. One way to identify a poisoned sample is to check if it contains a patch that behaves similar to the trigger. A characteristic of a trigger is that it occupies a relatively small area in an image (\u22485%) but heavily influences a model\u2019s output. Hence, we can use an input explanation method like Grad-CAM [45] to highlight the trigger. Note that this idea has been explored in supervised defenses [14, 16]. However, Grad-CAM relies on a supervised classifier which is unavailable in our case.\\n\\nCorresponding author: <atejankar@ucdavis.edu>\\n\\nWork done while interning at Meta AI.\\n\\nThis CVPR paper is the Open Access version, provided by the Computer Vision Foundation.\\n\\nExcept for this watermark, it is identical to the accepted version; the final published version of the proceedings is available on IEEE Xplore.\"}"}
{"id": "CVPR-2023-506", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. Illustration of PatchSearch. SSL has been shown to group visually similar images in [4, 47], and [44] showed that poisoned images are close to each other in the representation space. Hence, the very first step (a) is to cluster the training dataset. We use clustering to locate the trigger and to efficiently search for poisoned samples. The second step locates the candidate trigger in an image with Grad-CAM (b) and assigns a poison score to it (c). The third step (d) searches for highly poisonous clusters and only scores images in them. This step outputs a few highly poisonous images which are used in the fourth and final step (e) to train a more accurate poison classifier.\\n\\nThis is a key problem that we solve with k-means clustering. An SSL model is first trained on the poisoned data and its representations are used to cluster the training set. SSL has been shown to group visually similar images in [4, 47], and [44] showed that poisoned images are close to each other in the representation space. Hence, we expect poisoned images to be grouped together and the trigger to be the cause of this grouping. Therefore, cluster centers produced by k-means can be used as classifier weights in place of a supervised classifier in Grad-CAM. Finally, once we have located a salient patch, we can quantify how much it behaves like a trigger by pasting it on a few random images, and counting how many of them get assigned to the cluster of the patch (Figure 1 (c)). A similar idea has been explored in [14], but unlike ours, it requires access to trusted data, it is a supervised defense, and it operates during test time.\\n\\nOne can use the above process of assigning poison scores to rank the entire training set and then treat the top ranked images as poisonous. However, our experiments show that in some cases, this poison detection system has very low precision at high recalls. Further, processing all images is redundant since only a few samples are actually poisonous. For instance, there can be as few as 650 poisons in a dataset of 127K samples (\u22480.5%) in our experiments. Hence, we design an iterative search process that focuses on finding highly poisonous clusters and only scores images in them (Figure 1 (d)). The results of iterative search are a few but highly poisonous samples. These are then used in the next step to build a poison classifier which can identify poisons more accurately. This results in a poison detection system with about 55% precision and about 98% recall in average.\\n\\nIn summary, we propose PatchSearch, a novel defense algorithm that defends self-supervised learning models against patch based data poisoning backdoor attacks by efficiently identifying and filtering out poisoned samples. We also propose to apply i-CutMix [33] augmentation as a simple but effective defense. Our results indicate that PatchSearch is better than both the trusted data based defense from [44] and i-CutMix. Further, we show that i-CutMix and PatchSearch are complementary to each other and combining them results in a model with an improved overall performance while significantly mitigating the attack.\\n\\n2. Related Work\\n\\nSelf-supervised learning. The goal of self-supervised learning (SSL) is to learn representations from uncurated and unlabeled data. It achieves this with a \\\"pretext task\\\" that is derived from the data itself without any human annotations [18, 20, 39, 54, 57]. MoCo [26] is a popular SSL method in which we learn by classifying a pair of positives.\"}"}
{"id": "CVPR-2023-506", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"two augmentations of the same image, against pairs of neg-\\natives, augmentations from different images. This is further\\nexplored in [9, 11, 12, 50]. BYOL [23] showed that it is\\npossible to train the model without negatives. This was im-\\nproved in [47, 48].\\n\\nBackdoor attacks and defenses for supervised learn-\\ning. The goal of these attacks is to cause a model to\\nmisclassify images containing an attacker chosen trigger.\\nThe attack is conducted by poisoning the training dataset.\\nThese attacks were first studied for supervised learning in\\n[24,37,38]. More advanced attacks that are harder to detect\\nhave been proposed in [43, 46, 52]. We specifically study\\npatch based [7, 42] version of backdoor attacks.\\n\\nThere is a large literature devoted to defending against\\nsuch attacks on supervised learning [34]. Following are a\\nfew important paradigms of defenses. (1) pre-processing\\nthe input [16]: remove the trigger from a given input before\\nforwarding it through the model. (2) trigger synthesis\\n[53]: explicitly identify the trigger and then modify the model\\nto ignore it. (3) model diagnostics [31, 32, 58]: classify\\nwhether a given model is poisoned or not. (4) model re-\\nconstruction [36]: directly modify the model weights such\\nthat the trigger cannot have any impact on it. (5) backdoor\\nsuppression [5, 28, 29, 35]: modify the training procedure\\nof the model such that backdoors don't get learned by the\\nmodel in the first place. We show that a defense from this\\nparadigm in the form of i-CutMix augmentation [33] is a\\nsimple and effective baseline. Note that i-CutMix [33] is\\na version of CutMix [55] that does not need labels. More-\\nover, strong augmentations like CutMix were shown to be\\nbetter compared to other backdoor suppression techniques\\nlike [28] which are based on differential privacy. (6) train-\\ning sample filtering [10, 30, 51, 56]: first identify the poi-\\nsoned samples, then filter them out of the training dataset,\\nand finally train a new model on the resulting clean dataset.\\nThis is also the paradigm of our defense. (7) testing sample\\nfiltering [14, 19]: identify poisoned samples during the in-\\nference phase of a model and prevent them from being for-\\nwarded through the model. Similar to these works [14, 16]\\nwe use Grad-CAM [45] to locate the trigger, but without ac-\\ncess to a supervised classifier we calculate Grad-CAM with\\na k-means based classifier. Further, similar to ours [14]\\npastes the located trigger on a few images to calculate its\\npoisonousness, but they assume that these images are from\\na trusted source. Moreover, unlike [14, 16], ours is a train\\ntime filtering defense.\\n\\nFinally, above supervised defenses cannot be directly\\nadopted to self-supervised learning. For instance, consider\\nActivation Clustering [10] which uses the idea that poisoned\\nimages cluster together. However, their full idea is to per-\\nform 2-way clustering within each class, and consider a\\nclass as poisoned if the 2-way clustering is imbalanced. In\\nthe absence of labels, this defense is not applicable in SSL.\\n\\n3. Threat Model\\nWe adopt the backdoor attack threat model proposed\\nin [44]. The attacker is interested in altering the output of\\na classifier trained on top of a self-supervised model for a\\ndownstream task. The attacker's objective is twofold. First,\\ninsert a backdoor into the self-supervised model such that\\nthe downstream classifier mis-classifies an incoming image\\nas the target category if it contains the attacker chosen trig-\\nner. Second, hide the attack by making sure that the accu-\\nrate of the downstream classifier is similar to a classifier\\ntrained on top of a clean model if the image does not con-\\ntain the trigger. The attacker achieves these objectives by\\n\"data poisoning\" where a small trigger patch, fixed before-\\nhand, is pasted on a few images of the target category which\\nis also chosen by the attacker. The hypothesis is that the\\nSSL method associates the trigger with the target category\\nresulting in a successful attack. Given this attack, the de-\\nfender's goal is to detect and neutralize it without affecting\\nthe overall performance of the model. The defender must\\ndo this without knowing the trigger or the target category\\nand without any access to trusted data or image labels.\\n\\n4. Defending with PatchSearch\\nThe goal of PatchSearch is to identify the poisonous\\nsamples in the training set and filter them out. We do this by\\nfirst training a SSL model on the poisonous data and using\\nit to identify the poisonous samples. The cleaned up train-\\ning data is then used to train a final, backdoor-free model.\\nNote that the architecture and SSL method used for the fi-\\nnal model need not be the same as the model used during\\ndefense. In fact, we show that using an easy to attack model\\nduring defense, allows PatchSearch to find the poisons more\\neasily. There are four components of PatchSearch as illus-\\ntrated in Figure 1. (1) Cluster the dataset. (2) Assign a poi-\\nson score to a given image. (3) Efficiently search for a few\\nbut highly poisonous samples. (4) Train a poison classifier\\nto find poisons with high recall.\"}"}
{"id": "CVPR-2023-506", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Scoring an image.\\n\\nThe goal is to quantify the poisonousness of a given image by assigning a poison score to it. The steps for this are described next and illustrated in Figures 1 (b) and 1 (c). We use the cluster centers as classification weights by calculating cosine similarity between $x_i$ and cluster centers, and use $y_i$ as the label to calculate Grad-CAM for $x_i$. The resulting heatmap is used to extract a candidate trigger $t_i$ which is a $w \\\\times w$ patch that contains the highest heatmap values. We then choose a set of images called flip test set $X_f$, paste $t_i$ on those, forward through the model and get their cluster assignments. Finally, the poison score of $t_i$ or $x_i$ is the number of images in $X_f$ whose cluster assignments flipped to that of $x_i$. We ensure that the flip test set is difficult to flip by sampling a few images per cluster that are closest to their respective cluster centers. Note that all flip set images do not need to be clean.\\n\\nIterative search.\\n\\nIn this phase, the goal is to efficiently find a few but highly poisonous images. Hence, instead of assigning poison scores to the entire set $X$, we propose a greedy search strategy that focuses on scoring images from poisonous clusters. Concretely, we process $s$ random samples per cluster in an iteration (Figure 1 (d)). At the end of an iteration, each cluster is assigned a poison score as the highest poison score of the images in it. Since poisoned images cluster together, just a single iteration should give us a good estimate of clusters that are likely to contain poisons. We use this insight to focus on highly poisonous cluster by removing (pruning) a fraction $r$ of the clusters with least poison scores in each iteration. The algorithm stops when there are no more clusters to be processed. Finally, all scored images are ranked in the decreasing order of their scores and the top-$k$ ranked images are used in the next step.\\n\\nPoison classification.\\n\\nHere, the goal is to find as many poisons as possible. We do this by building a classifier $h$ to identify the top-$k$ ranked patches found in the previous step (Figure 1 (e)). Specifically, given the training dataset $X$ and a set $P$ of $k$ patches, we construct poisoned training set $X_p$ by randomly sampling a patch $p_j$ from $P$ and pasting it at a random location on $x_i$. Samples from $X$ are assigned label \\\"0\\\" (non-poisonous) while samples from $X_p$ are assigned the label \\\"1\\\" (poisonous). However, this labeling scheme is noisy since there are poisoned images in $X$ which will be wrongly assigned the label 0. One way to reduce noise is to not include images in $X$ with high poison scores. Another way is to prevent the model from overfitting to the noise by using strong augmentations and early stopping. However, early stopping requires access to validation data which is unavailable. Hence, we construct a proxy validation set by using the top-$k$ samples with label 1 (poisonous) and bottom-$k$ samples with label 0 (non-poisonous). Note that these samples are excluded from the training set. We stop training the classifier if F1 score on the proxy validation set does not change for 10 evaluation intervals. Finally, the resulting binary classifier is applied to $X$ and any images with \\\"1\\\" (poisonous) label are removed from the dataset.\\n\\n5. Experiments\\n\\nWe follow the experimentation setup proposed in [44].\\n\\nDatasets\\n\\nWe use ImageNet-100 dataset [49], which contains images belonging to 100 randomly sampled classes from the 1000 classes of ImageNet [41]. The train set has about 127K samples while the validation set has 5K samples.\\n\\nArchitectures and self-supervised training.\\n\\nIn addition to ResNet-18 [27], we also conduct experiments on ViT-B [17] with MoCo-v3 [13] and MAE [25]. Unlike [44], we only consider ResNet-18 with MoCo-v2 [12] and BYOL [23] since we find that older methods like Jigsaw [39] and RotNet [20] have significantly worse clean accuracies compared to MoCo-v2 and BYOL. For the code and parameters, we follow MoCo-v3 [13] for ViT-B, and [44] for ResNet-18. Finally, implementation of i-CutMix and its hyperparameter values follow the official implementation from [33].\\n\\nEvaluation.\\n\\nWe train a linear layer on top of a pre-trained models with 1% of randomly sampled, clean and labeled training set. To account for randomness, we report results averaged over 5 runs with different seeds. We use the following metrics over the validation set to report model performance: accuracy (Acc), count of False Positives (FP) for the target category, and attack success rate (ASR) as the percentage of FP ($\\\\frac{FP \\\\times 100}{4950}$). Note that maximum FP is 4950 (50 samples/category $\\\\times$ 99 incorrect categories). The metrics are reported for the validation set with and without trigger pasted on it, referred to as Patched Data and Clean Data. See Section A.2 of the supplementary for more details.\\n\\nAttack.\\n\\nWe consider 10 different target categories and trigger pairs used in [44]. Unless mentioned otherwise, all results are averaged over the 10 target categories. Further, following [44], we consider 0.5% and 1.0% poison injection rates which translate to 50% and 100% images being poisoned for a target category. Poisons are generated according to [1, 44] (see Figure 2 for an example). The pasted trigger size is 50x50 ($\\\\approx 5\\\\%$ of the image area) and it is pasted at a random location in the image leaving a margin of 25% of width or height.\\n\\nPatchSearch.\\n\\nGrad-CAM implementation is used from [21]. Unless mentioned, we run the PatchSearch with following parameters: number of clusters $l = 1000$, patch size $w = 60$, flip test set size = 1000, samples per cluster $s = 2$, and clusters to prune per iteration $r = 25\\\\%$. This results in about 8K (6%) training set images being scored. The architecture of the poison classifier is based on ResNet-18 [27] but uses fewer parameters per layer. We use top-20 poisons to train the classifier, and remove top 10% of poisonous samples to reduce noise in the dataset. We use strong augmentations and early stopping.\"}"}
