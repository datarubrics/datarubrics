{"id": "CVPR-2022-545", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Josef Aschbacher. ESA\u2019s Earth observation strategy and Copernicus. In Satellite Earth Observations and Their Impact on Society and Policy, pages 81\u201386. Springer, Singapore, 2017.\\n\\n[2] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. SegNet: A deep convolutional encoder-decoder architecture for image segmentation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 39(12):2481\u20132495, 2017.\\n\\n[3] Nicolas Bourdis, Denis Marraud, and Hichem Sahbi. Constrained optical flow for aerial image change detection. In 2011 IEEE International Geoscience and Remote Sensing Symposium, pages 4176\u20134179. IEEE, 2011.\\n\\n[4] Francesca Bovolo. A multilevel parcel-based approach to change detection in very high resolution multitemporal images. IEEE Geoscience and Remote Sensing Letters, 6(1):33\u201337, 2008.\\n\\n[5] Francesca Bovolo and Lorenzo Bruzzone. A theoretical framework for unsupervised change detection based on change vector analysis in the polar domain. IEEE Transactions on Geoscience and Remote Sensing, 45(1):218\u2013236, 2006.\\n\\n[6] Francesca Bovolo, Silvia Marchesi, and Lorenzo Bruzzone. A framework for automatic and unsupervised detection of multiple changes in multitemporal images. IEEE Transactions on Geoscience and Remote Sensing, 50(6):2196\u20132212, 2011.\\n\\n[7] Hongruixuan Chen, Chen Wu, Bo Du, and Liangpei Zhang. Change detection in multi-temporal VHR images based on deep Siamese multi-scale convolutional networks. arXiv preprint arXiv:1906.11479, 2019.\\n\\n[8] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin Murphy, and Alan L Yuille. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs. IEEE Transactions on Pattern Analysis and Machine Intelligence, 40(4):834\u2013848, 2017.\\n\\n[9] Liang-Chieh Chen, George Papandreou, Florian Schroff, and Hartwig Adam. Rethinking atrous convolution for semantic image segmentation. arXiv preprint arXiv:1706.05587, 2017.\\n\\n[10] Liang-Chieh Chen, Yukun Zhu, George Papandreou, Florian Schroff, and Hartwig Adam. Encoder-decoder with atrous separable convolution for semantic image segmentation. In Proceedings of the European Conference on Computer Vision (ECCV), pages 801\u2013818, 2018.\\n\\n[11] Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 6172\u20136180, 2018.\\n\\n[12] Rodrigo Caye Daudt, Bertrand Le Saux, and Alexandre Boulch. Fully convolutional siamese networks for change detection. In 2018 25th IEEE International Conference on Image Processing (ICIP), pages 4063\u20134067. IEEE, 2018.\\n\\n[13] Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch, and Yann Gousseau. Urban change detection for multispectral earth observation using convolutional neural networks. In IGARSS 2018-2018 IEEE International Geoscience and Remote Sensing Symposium, pages 2115\u20132118. IEEE, 2018.\\n\\n[14] Rodrigo Caye Daudt, Bertrand Le Saux, Alexandre Boulch, and Yann Gousseau. Multitask learning for large-scale semantic change detection. Computer Vision and Image Understanding, 187:102783, 2019.\\n\\n[15] Ilke Demir, Krzysztof Koperski, David Lindenbaum, Guan Pang, Jing Huang, Saikat Basu, Forest Hughes, Devis Tuia, and Ramesh Raskar. DeepGlobe 2018: A challenge to parse the Earth through satellite images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 172\u2013181, 2018.\\n\\n[16] Jian Ding, Nan Xue, Gui-Song Xia, Xiang Bai, Wen Yang, Michael Ying Yang, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, et al. Object detection in aerial images: A large-scale benchmark and challenges. arXiv preprint arXiv:2102.12219, 2021.\\n\\n[17] Matthias Drusch, Umberto Del Bello, S\u00e9bastien Carlier, Olivier Colin, Veronica Fernandez, Ferran Gascon, Bianca Hoersch, Claudia Isola, Paolo Laberinti, Philippe Martimort, et al. Sentinel-2: ESA\u2019s optical high-resolution mission for GMES operational services. Remote Sensing of Environment, 120:25\u201336, 2012.\\n\\n[18] Alexander Kirillov, Ross Girshick, Kaiming He, and Piotr Doll\u00e1r. Panoptic feature pyramid networks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6399\u20136408, 2019.\\n\\n[19] Lukas Kondmann, Aysim Toker, Marc Ru\u00dfwurm, Andr\u00e9s Camero, Devis Peressuti, Grega Milcinski, Pierre-Philippe Mathieu, Nicolas Long\u00e9p\u00e9, Timothy Davis, Giovanni Marchisio, et al. Denethor: The DynamicEarthNet dataset for harmonized, inter-operable, analysis-ready, daily crop monitoring from space. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021.\\n\\n[20] Lukas Kondmann, Aysim Toker, Sudipan Saha, Bernhard Sch\u00f6lkopf, Laura Leal-Taix\u00e9, and Xiao Xiang Zhu. Spatial context awareness for unsupervised change detection in optical satellite images. arXiv preprint arXiv:2110.02068, 2021.\\n\\n[21] Xin Lai, Zhuotao Tian, Li Jiang, Shu Liu, Hengshuang Zhao, Liwei Wang, and Jiaya Jia. Semi-supervised semantic segmentation with directional context-aware consistency. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1205\u20131214, 2021.\\n\\n[22] Xiangtai Li, Hao He, Xia Li, Duo Li, Guangliang Cheng, Jianping Shi, Lubin Weng, Yunhai Tong, and Zhouchen Lin. PointFlow: Flowing semantics through points for aerial image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4217\u20134226, 2021.\\n\\n[23] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2117\u20132125, 2017.\"}"}
{"id": "CVPR-2022-545", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 3431\u20133440, 2015.\\n\\nHaobo Lyu, Hui Lu, and Lichao Mou. Learning a transferable change rule from a recurrent neural network for land cover change detection. Remote Sensing, 8(6):506, 2016.\\n\\nRose M Rustowicz, Robin Cheong, Lijing Wang, Stefano Ermon, Marshall Burke, and David Lobell. Semantic segmentation of crop type in Africa: A novel dataset and analysis of deep learning methods. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 75\u201382, 2019.\\n\\nV. Masson-Delmotte, P. Zhai, A. Pirani, S.L. Connors, C. P\u00b4ean, S. Berger, N. Caud, Y. Chen, L. Goldfarb, M.I. Gomis, M. Huang, L. Leitzell, E. Lonnoy, J.B.R. Matthews, T.K. Maycock, T. Waterfield, O. Yelekci, R. Yu, and B. Zhou. Climate Change 2021: The Physical Science Basis. Contribution of Working Group I to the Sixth Assessment Report of the Intergovernmental Panel on Climate Change. Cambridge University Press, Cambridge, United Kingdom and New York, NY, USA, 2021.\\n\\nLichao Mou, Lorenzo Bruzzone, and Xiao Xiang Zhu. Learning spectral-spatial-temporal features via a recurrent convolutional neural network for change detection in multispectral imagery. IEEE Transactions on Geoscience and Remote Sensing, 57(2):924\u2013935, 2019.\\n\\nA.H. Pickens, M.C. Hansen, B. Adusei, and Potapov P. Sentinel-2 forest loss alert. www.globalforestwatch.org, 2020. Accessed through Global Forest Watch on 11/09/2021.\\n\\nISPRS Potsdam. 2d semantic labeling dataset, 2018.\\n\\nChristian Requena-Mesa, Vitus Benson, Markus Reichstein, Jakob Runge, and Joachim Denzler. Earthnet2021: A large-scale dataset and challenge for earth surface forecasting as a guided video prediction task. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1132\u20131142, 2021.\\n\\nOlaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234\u2013241. Springer, 2015.\\n\\nSudipan Saha, Francesca Bovolo, and Lorenzo Bruzzone. Unsupervised deep change vector analysis for multiple-change detection in VHR images. IEEE Transactions on Geoscience and Remote Sensing, 57(6):3677\u20133693, 2019.\\n\\nVivien Sainte Fare Garnot and Loic Landrieu. Panoptic segmentation of satellite image time series with convolutional temporal attention networks. ICCV, 2021.\\n\\nFrank Thonfeld, Hannes Feilhauer, Matthias Braun, and Gunter Menz. Robust change vector analysis (rcva) for multi-sensor very high resolution optical satellite data. International Journal of Applied Earth Observation and Geoinformation, 50:131\u2013140, 2016.\\n\\nShiqi Tian, Ailong Ma, Zhuo Zheng, and Yanfei Zhong. Hi-ucd: A large-scale dataset for urban semantic change detection in remote sensing imagery. arXiv preprint arXiv:2011.03247, 2020.\\n\\nISPRS Vaihingen. 2d semantic labeling dataset, 2018.\\n\\nAdam Van Etten, Daniel Hogan, Jesus Martinez Manso, Jacob Shermeyer, Nicholas Weir, and Ryan Lewis. The multi-temporal urban development Spacenet dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 6398\u20136407, 2021.\\n\\nAdam Van Etten, Dave Lindenbaum, and Todd M Bacastow. Spacenet: A remote sensing dataset and challenge series. arXiv preprint arXiv:1807.01232, 2018.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017.\\n\\nSagar Verma, Akash Panigrahi, and Siddharth Gupta. Qfabric: Multi-task change detection dataset. In Earthvision Workshop Computer Vision and Pattern Recognition (CVPR 2021), page 10, 2021.\\n\\nJingdong Wang, Ke Sun, Tianheng Cheng, Borui Jiang, Chaorui Deng, Yang Zhao, Dong Liu, Yadong Mu, Mingkui Tan, Xinggang Wang, et al. Deep high-resolution representation learning for visual recognition. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.\\n\\nSyed Waqas Zamir, Aditya Arora, Akshita Gupta, Salman Khan, Guolei Sun, Fahad Shahbaz Khan, Fan Zhu, Ling Shao, Gui-Song Xia, and Xiang Bai. ISAID: A large-scale dataset for instance segmentation in aerial images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 28\u201337, 2019.\\n\\nNicholas Weir, David Lindenbaum, Alexei Bastidas, Adam Van Etten, Sean McPherson, Jacob Shermeyer, Varun Kumar, and Hanlin Tang. Spacenet MVOI: A multi-view overhead imagery dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 992\u20131001, 2019.\\n\\nCurtis E Woodcock, Richard Allen, Martha Anderson, Alan Belward, Robert Bindschadler, Warren Cohen, Feng Gao, Samuel N Goward, Dennis Helder, Eileen Helmer, et al. Free access to Landsat imagery. SCIENCE VOL 320: 1011, 2008.\\n\\nGui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Beolchini, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liangpei Zhang. Dota: A large-scale dataset for object detection in aerial images. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 3974\u20133983, 2018.\\n\\nYang Zhan, Kun Fu, Menglong Yan, Xian Sun, Hongqi Wang, and Xiaosong Qiu. Change detection based on deep siamese convolutional network for optical aerial images. IEEE Geoscience and Remote Sensing Letters, 14(10):1845\u20131849, 2017.\\n\\nZhuo Zheng, Yanfei Zhong, Junjue Wang, and Ailong Ma. Foreground-aware relation network for geospatial object segmentation in high spatial resolution remote sensing imagery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4096\u20134105, 2020.\"}"}
{"id": "CVPR-2022-545", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"DynamicEarthNet: Daily Multi-Spectral Satellite Dataset for Semantic Change Segmentation\\n\\nAysim Toker, Lukas Kondmann, Mark Weber, Marvin Eisenberger, Andr\u00e9s Camero, Jingliang Hu, Ariadna Pregel Hoderlein, Caglar Senaras, Timothy Davis, Daniel Cremers, Giovanni Marchisio, Xiao Xiang Zhu, Laura Leal-Taix\u00e9\\n\\nTechnical University of Munich, German Aerospace Center, Planet Labs\\n\\nAbstract\\n\\nEarth observation is a fundamental tool for monitoring the evolution of land use in specific areas of interest. Observing and precisely defining change, in this context, requires both time-series data and pixel-wise segmentations. To that end, we propose the DynamicEarthNet dataset that consists of daily, multi-spectral satellite observations of 75 selected areas of interest distributed over the globe with imagery from Planet Labs. These observations are paired with pixel-wise monthly semantic segmentation labels of 7 land use and land cover (LULC) classes. DynamicEarthNet is the first dataset that provides this unique combination of daily measurements and high-quality labels. In our experiments, we compare several established baselines that either utilize the daily observations as additional training data (semi-supervised learning) or multiple observations at once (spatio-temporal learning) as a point of reference for future research. Finally, we propose a new evaluation metric SCS that addresses the specific challenges associated with time-series semantic change segmentation. The data is available at: https://mediatum.ub.tum.de/1650201.\\n\\nMaking peace with nature is the defining task of the 21st century.\\n\\nAnt\u00f3nio Guterres, UN Secretary General\\n\\n* Authors share first authorship.\\n\u2020 Authors share senior authorship.\\n\u2021 Corresponding author: xiaoxiang.zhu@dlr.de.\\n\\n1. Introduction\\n\\nSociety is rapidly becoming more aware of the human footprint on the world's climate. Overwhelming evidence shows that climate change has both short-term and long-term effects on almost every aspect of our lives [27]. Using simulations and global climate metrics, it is nowadays possible to observe changes at a global scale, like the rising sea levels or changes of the gulf stream. In contrast, precise predictions of local changes are much harder to obtain. Common examples include land use by agriculture, deforestation, flooding, wildfires, growth of urban areas, and transportation infrastructure. It is of critical importance to monitor such local changes since these are the factors that ultimately exacerbate the global climate crisis.\\n\\nSatellite images are a powerful tool in this context to track local changes to the environment in specific regions. Observing change at a local scale requires two conditions: high frequency of satellite observations and pixel-precise understanding of the observed surface. Existing datasets often fail to provide these conditions. Whenever pixel-wise annotations are provided, only static images can be used [43] or the revisit frequency is limited to once a year [14, 36]. Datasets with coarser annotations have either an irregular [11] or monthly revisit frequency [38]. As an example of land changes, in 2020, 46 km\u00b2 of the rainforest in Brazil were destroyed every day [29]. This suggests that if we analyze the satellite images of that area once per month, we potentially miss deforestation of the equivalent of the city of Los Angeles, California. As Brazil alone has 21,158...\"}"}
{"id": "CVPR-2022-545", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"millions of square kilometers of forest, automatic methods are required to detect these and other kinds of land changes. Current pixel-precise automatic methods are predominantly based on deep learning and thus require annotated data to learn.\\n\\nIn this work, we present DynamicEarthNet, a time-series satellite imagery dataset with daily revisits of 75 local regions across the globe. The dataset comprises consistent, occlusion-free daily observations with multi-spectral imaging over the span of two years (2018-2019). We further provide annotated monthly semantic segmentation labels. The main focus is to segment and detect changes in the development of general land use and land cover (LULC). Specifically, we focus on the following LULC classes: impervious surfaces, water, soil, agriculture, wetlands, snow & ice, and forest & other vegetation.\\n\\nIn comparison to semantic segmentation on standard computer vision benchmarks, satellite imagery is subject to various additional challenges. Most prominently, labeled areas in satellite images typically have very intricate shapes that are significantly more complex than everyday objects. We show that well-performing methods [10, 32] on standard vision benchmarks do not necessarily transfer well to this domain. Furthermore, common segmentation metrics are not optimal for quantifying the performance on the task of semantic change segmentation. We alleviate this issue by proposing a new evaluation protocol that captures the essence of semantic change segmentation.\\n\\nDynamicEarthNet and the proposed evaluation protocol encourage the development of more specialized algorithms that can handle the particular challenges of daily time-series satellite imagery. In summary, our contributions are as follows:\\n\\n\u2022 We present a large-scale dataset of multi-spectral satellite imagery with daily observations of 75 separate areas of interest around the globe.\\n\u2022 We provide dense, monthly annotations of 7 land use and land cover (LULC) semantic classes.\\n\u2022 We propose a novel evaluation protocol that models two central properties of semantic change segmentation: binary change and semantic segmentation.\\n\u2022 We evaluate multiple baseline approaches on our data for the task of detecting semantic change. We show how the time-series nature of our data can be leveraged for optimal performance.\\n\\n2. Related work\\n\\nFor our discussion of related work, we provide an overview of publicly available satellite imagery datasets, see also Tab. 1. Furthermore, we summarize existing work on the tasks of semantic segmentation and change detection.\\n\\n2.1. Earth observation datasets\\n\\nSegmentation and detection. Semantic segmentation of land cover classes for satellite imagery was originally pioneered by the ISPRS project [30, 37]. Similarly, the DeepGlobe [15] and SpaceNet [39] challenges provide datasets for building detection, road extraction, and land cover classification. In contrast to ours, such early works have a relatively small number of areas of interest.\\n\\nSubsequently, the main focus started to shift towards large-scale aerial imagery [43, 46]. To that end, DOTA [46] proposes to detect objects on a large collection of images cropped from Google Earth. iSAID [43] extends this concept to the task of instance segmentation. Along the same lines, SpaceNet MVOI [44] proposes a benchmark on building detection for multi-view satellite imagery. Our benchmark, on the other hand, provides semantic annotations that are dense, i.e., defined for every single pixel.\\n\\nChange detection. Several works aim at predicting change between observations of the same area of interest at different times. Most relevant datasets focus on binary change detection which is agnostic to specific types of change [3,13]. HRSCD [14] and Hi-UCD [36] propose a multi-class semantic change detection datasets. In comparison to time-series data, these benchmarks show only one observation per year, for 2-3 years in total, rather than a full sequence. Moreover, the diversity is limited \u2013 HRSCD [14] and Hi-UCD [36] cover specific regions of France and Tallinn, Estonia, respectively. More recently, QFabric [41] presented a large-scale multi-temporal dataset, with polygonal annotations for change regions. In contrast, our dataset contains daily observations and pixel-wise LULC classes.\\n\\nTime-series analysis. In recent times, time-series satellite datasets gained increasing attention [11, 31, 38]. For instance, Earthnet2021 [31] presents a surface forecasting dataset based on public Sentinel-2 imagery with a revisit rate of 5 days. Since the intended applications are quite dissimilar to ours, no land cover annotations are provided. fMoW [11] provides temporal satellite imagery with bounding box annotations. Similarly, MUDS [38] aims at monitoring urbanization by tracking buildings for several areas of interest that are annotated with polygons. Varying acquisition conditions make it challenging to consistently collect data over an extended period of time. Consequently, existing datasets often contain irregular revisit frequencies [11] or infrequent (monthly) observation intervals [38]. In contrast, our DynamicEarthNet dataset provides high-quality, consistent daily observations.\\n\\n2.2. Considered tasks\\n\\nSemantic segmentation. There are countless recent deep learning methods [2, 8\u201310, 24, 32, 42] that address general problems in the field of computer vision. Some of the most popular methods include U-Net [2], DeepLab v3+ [8], and Mask R-CNN [10]. These methods have been applied to a wide range of tasks, including object detection, instance segmentation, and semantic segmentation.\\n\\nChange detection. Change detection is a widely studied problem in remote sensing, with numerous applications in fields such as urban planning, environmental monitoring, and disaster response. A common approach to change detection is to compare two or more images of the same area taken at different times and identify regions that have undergone changes. This can be done manually by a human analyst or automatically using computer vision techniques.\u201d}"}
{"id": "CVPR-2022-545", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 1. An overview of public satellite datasets. For each dataset, we compare key characteristics like the revisit time, the number of images, data source, ground sample distance (GSD), types of annotations, and annotated objects. Most closely related are DeepGlobe [15], iSAID [43], HRSCD [14] and Hi-UCD [36] which, like ours, provide dense semantic annotations for various land cover classes. However, they either provide no time-series data or merely yearly revisit times. Closely related datasets are highlighted in blue and yellow.\\n\\n### Change detection.\\nChange detection is an extensively studied topic in earth observation. Classical approaches define axiomatic, pixel-based [4\u20136, 20, 35] algorithms to obtain change whereas many recent approaches are data-driven [7, 12, 33, 47]. The development of new algorithms is often inhibited by a lack of high-quality data and expert annotations. Most methods focus on binary change and are usually limited to two distinct observations in time (bitemporal) [4\u20137, 20, 35, 47]. Moreover, datasets and metrics used for evaluation differ widely and are often not public. These considerations underline the necessity for a standardized benchmark with a consistent evaluation protocol. Up to now, there are few approaches suitable for multi-class change detection. Most of them typically consider two snapshots, often years apart. Among these works, [25, 28] directly predict the multi-class change map whereas, [36] define change as the difference between two semantic maps. We follow the latter approach in our evaluations since existing work on multi-class change detection is not primarily designed to handle high temporal frequencies. Therefore, we benchmark state-of-the-art semantic segmentation algorithms on our dataset and compare differences in the predicted multi-class semantic masks over time.\\n\\n### Table 2. LULC class distribution.\\nThe distribution of LULC classes averaged over all $24 \\\\times 75 = 1800$ semantic maps in the dataset. Additionally, we report the absolute number of AOIs with any occurrences of a given LULC class. We visualize the colors we use for each class throughout the paper.\\n\\n---\\n\\n3. The DynamicEarthNet dataset\\nWe present the DynamicEarthNet dataset that contains daily, cloud-free satellite data acquired from January 2018 to December 2019. It consists of images from 75 areas of interest (AOIs) across the globe, as illustrated by the world map in Fig. 1. The dataset covers a wide variety of environments with diverse types of land cover changes. For each region, we provide a sequence of images with daily revisits. Furthermore, we present pixel-wise semantic labels for the first day of each month. These serve as ground-truth to define land cover changes over the span of two observed years. In the remainder of this section, we provide details on the imagery, semantic labels, and statistics of the dataset.\\n\\n### 3.1. Multi-spectral imagery\\nThe primary source of our dataset is the Fusion Monitoring product from Planet Labs, which provides multi-spectral imagery.\"}"}
{"id": "CVPR-2022-545", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. An example of a changing surface. We show four sample frames of one AOI from our dataset at different times. Two sub-regions are magnified that highlight two types of change we encounter in practice (top row). The daily nature of our data allows us to observe new buildings being built (green) or to track deforestation (yellow). Additionally, we can monitor the long-term effects of such changes over the span of multiple months, e.g., the changes to the forest patch here are persistent.\\n\\nSpectral time-series satellite imagery. Each snapshot contains four channels (RGB + near-infrared) with a ground sample distance (GSD), i.e., pixel granularity, of 3 meters and a resolution of 1024x1024.\\n\\nBeyond the raw observational data, Planet applies a combination of post-processing techniques to ensure data quality and consistency: For once, all images are processed to remove occlusions by weather, overcast and related visual artifacts. The data is gap-filled, which means that missing information due to cloud coverage is filled with suitable observations from the closest available point in time. Moreover, the Fusion bands are calibrated to the Harmonized Landsat-Sentinel (HLS) spectrum to make them compatible with other publicly available datasets such as Landsat 8 [45] or Sentinel 2 [1, 17].\\n\\nTo encourage the exploration of data fusion, we provide monthly Sentinel-2 (S2) imagery from the same 75 AOIs for reference. The main idea of this auxiliary set of images is to allow for comparisons with publicly available data. Moreover, the additional data potentially gives rise to interesting multi-modal settings in future experiments. For more details, we refer the reader to our supplementary material.\\n\\n3.2. Pixel-wise labels\\n\\nHaving described the raw satellite imagery, we now provide more details on the monthly ground-truth annotations. They comprise a collection of pixel-wise semantic segmentation labels corresponding to the first day of each month. These labels are defined as the common LULC classes, i.e., impervious surfaces, agriculture, forest & other vegetation, wetlands, soil, water, snow & ice. The resolution of each annotation is 1024x1024 with a pixel granularity of 3 meters, just like the corresponding satellite images.\\n\\nThe annotation procedure was rigorous with an emphasis on the temporal consistency of the labels. The first image was manually annotated for each AOI and used as a basis for the following months. Subsequent maps are updated if there is a perceptible change in a certain region that is evident to the human annotator. Three quality control gates, each with a different annotator, ensure accurate annotations, topological correctness, and format correctness, respectively.\\n\\n3.3. Dataset statistics\\n\\nThe DynamicEarthNet dataset contains 75 different AOIs across the globe, each of which consists of a sequence of 730 images covering two years from January 2018 to December 2019. We provide semantic LULC classes for the first day of each month, 24 per sequence in total. In total, this amounts to 54750 satellite images and 1800 ground-truth annotations.\\n\\nWe illustrate the distribution of LULC classes over the whole dataset in Tab. 2. Due to the nature of the data, occurrences of certain semantic classes are imbalanced with forest & other vegetation and soil dominating less frequent classes like wetlands. Such general ambient classes often take up large portions of a considered region, see the bottom third of the images in Fig. 2.\\n\\nWe split our data into train, validation, and test sets with 55, 10, and 10 AOIs, respectively. The number of distinct classes per AOI ranges from 2 to 6. For instance, some AOIs from the dataset contain only forest & other vegetation and soil, whereas others include impervious surfaces, water, soil, agriculture, wetlands, and forest & other vegetation. No single AOI contains all 7 classes. For an optimal balance, we ensure that the splits' classes are distributed as equally as possible. We refrain from providing more fine-grained statistics on the class distribution to avoid disclosing 21161.\"}"}
{"id": "CVPR-2022-545", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"ing any additional information on the (currently concealed) test set. Since the snow & ice class occurs in only 2 cubes, see Tab. 2, we have no such examples in the validation or test sets. Consequently, we also do not consider this class in our quantitative evaluations presented in Sec. 5.\\n\\n3.4. Advantages over existing benchmarks\\n\\nIn comparison to other publicly available, annotated satellite datasets, DynamicEarthNet has a number of crucial distinguishing features, see Tab. 1. First and foremost, it is the first to provide daily observations from a large diversity of AOIs. The closest work to ours in terms of revisit rates is [38] with monthly observations. Yet, they have a narrower focus with the main objective of tracking buildings to monitor urbanization. Other related change detection datasets [14, 36, 41] show merely one observation per year, see Tab. 1. In our dataset, we provide consistent daily observations for two years allowing the study of both short-term and long-term change. Fig. 2 highlights the potential of such data: We can observe the change of new buildings being built day by day. At the same time, we can pin down exact dates of deforestation, and successively observe long-term effects over the span of multiple months.\\n\\n4. Semantic change segmentation\\n\\nOne key application of our dataset is to measure how a given local region changes over time. For the standard task of binary change detection, we classify each pixel into change or no-change. This definition, however, disregards semantic information. We, therefore, generalize this classical notion to a multi-class segmentation task, which we refer to as semantic change segmentation.\\n\\nFor time-series satellite data, changes are usually caused by external forces, such as weather and climate effects or human destruction and creation. Compared to standard vision benchmarks, they often appear gradually over time and with a limited spatial extent. When predicting semantic labels for a whole observed region, such rare changes between frames have a low influence on the overall segmentation score. In our dataset, only 5% of all pixels change from month to month on average. Hence, standard evaluation metrics defined on the full image like the Jaccard index (IoU) are not suitable to express how accurately semantic classes of changed areas are predicted. We, therefore, propose a new metric to quantify the performance of methods in semantic change segmentation of satellite images.\\n\\n4.1. Problem definition\\n\\nLet \\\\( x \\\\in \\\\mathbb{R}^{T \\\\times H \\\\times W \\\\times 4} \\\\) be an input time-series of satellite images consisting of \\\\( T \\\\) frames with a spatial size of \\\\( H \\\\times W \\\\) and 4 input channels (RGB + near-infrared). For each such time-series, we further provide semantic annotations \\\\( y \\\\in \\\\mathbb{C}^{T \\\\times H \\\\times W} \\\\) that assign each pixel in \\\\( x \\\\) to one of the \\\\( 7 \\\\) LULC classes \\\\( \\\\mathbb{C} := \\\\{0, \\\\ldots, 6\\\\} \\\\) defined in Sec. 3.2. Given two consecutive frames at times \\\\( t \\\\) and \\\\( t + 1 \\\\), we can define the binary change \\\\( b \\\\in \\\\{0, 1\\\\}^{(T-1) \\\\times H \\\\times W} \\\\) as a binary labeling of all pixels for which the ground-truth semantic label changes:\\n\\n\\\\[\\nb_{t,i,j} = \\\\begin{cases} \\n1, & \\\\text{if } y_{t,i,j} \\\\neq y_{t-1,i,j} \\\\\\\\\\n0, & \\\\text{else}\\n\\\\end{cases}\\n\\\\]\\n\\nWhen evaluating semantic change segmentation, both the binary change map \\\\( \\\\hat{b} \\\\) and the semantic map \\\\( \\\\hat{y} \\\\) need to be predicted. This requires methods to answer which pixels change and what class do these pixels change to.\\n\\n4.2. Evaluation protocol\\n\\nThere are two distinct types of errors that are common in the context of semantic change segmentation: failing to detect the binary change and predicting the wrong semantic class for a changed pixel. Our goal is to design an evaluation protocol that captures both of these errors in a single signal. Thus, the resulting semantic change segmentation (SCS) metric consists of two components, a class-agnostic binary change score (BC) and a semantic segmentation score among changed pixels (SC).\\n\\n**Binary change (BC).** The standard approach to measure the quality of a predicted change map \\\\( \\\\hat{b} \\\\) is comparing its overlap with the ground-truth change \\\\( b \\\\). This is commonly defined as the Jaccard index or intersection-over-union score\\n\\n\\\\[\\nBC(b, \\\\hat{b}) = \\\\frac{|\\\\{ b = 1 \\\\} \\\\cap \\\\{ \\\\hat{b} = 1 \\\\}|}{|\\\\{ b = 1 \\\\} \\\\cup \\\\{ \\\\hat{b} = 1 \\\\}|}\\n\\\\]\\n\\nwhere we use the short hand-notation \\\\( \\\\{ b = 1 \\\\} := \\\\{(t, i, j) | b_{t,i,j} = 1\\\\} \\\\).\\n\\n**Semantic change (SC).** The second component of our metric measures semantic change accuracy. It is defined as the segmentation score, conditioned on the set of pixels where any change occurs in the ground-truth maps, i.e. \\\\( b = 1 \\\\). On this subset of pixels, we compute the Jaccard index between the ground-truth labels \\\\( y \\\\) and predicted labels \\\\( \\\\hat{y} \\\\) (averaged over all classes \\\\( c \\\\)):\\n\\n\\\\[\\nSC(y, \\\\hat{y} | b) = \\\\frac{1}{|\\\\mathbb{C}|} \\\\sum_{c \\\\in \\\\mathbb{C}} \\\\frac{|\\\\{ b = 1 \\\\} \\\\cap (\\\\{ y = c \\\\} \\\\cap \\\\{ \\\\hat{y} = c \\\\})|}{|\\\\{ b = 1 \\\\} \\\\cup (\\\\{ y = c \\\\} \\\\cup \\\\{ \\\\hat{y} = c \\\\})|}\\n\\\\]\\n\\n**Semantic change segmentation (SCS).** The total SCS score is the arithmetic mean of the binary change and the semantic change:\\n\\n\\\\[\\nSCS(y, \\\\hat{y}) = \\\\frac{1}{2} BC(b, \\\\hat{b}) + SC(y, \\\\hat{y} | b)\\n\\\\]\"}"}
{"id": "CVPR-2022-545", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"In practice, we first accumulate confusion matrices of all time-series before computing the final SCS score.\\n\\n**Metric properties.**\\n\\nIn the following, we summarize a few distinguishing features of the proposed SCS metric.\\n\\ni. **Focus on change.** In comparison to standard metrics, like the Jaccard index, the SCS metric specifically emphasizes accurate change predictions.\\n\\nii. **Separation of errors.** It separates the problems of detecting areas where change occurs (BC) and predicting the correct semantic labels for changed areas (SC).\\n\\niii. **Single output signal.** Both signals contribute equally to the final SCS score.\\n\\n## 5. Experiments\\n\\nIn this section, we demonstrate the utility of our dataset with various experiments on land cover segmentation and semantic change segmentation. We first give an overview of considered baseline methods in Sec. 5.1 and then present corresponding results in Sec. 5.2 and Sec. 5.3.\\n\\n### 5.1. Baselines\\n\\n**DynamicEarthNet** contains daily images and dense semantic annotations for the first day of each month. This raises the question of how one can leverage additional unlabeled examples to improve the results when training on the labeled data. We study two separate approaches in this work: spatio-temporal and semi-supervised semantic segmentation. The former addresses the time-series nature of our data by combining spatial information with temporal architectures. The latter uses the annotated images (first day of each month) as supervision while taking advantage of the additional unlabeled samples in an unsupervised manner.\\n\\n**Spatio-temporal baselines.** The first class of baselines we consider are spatio-temporal methods. The main idea is to fuse individual observations of an input time series and produce a single output prediction \u2013 the monthly semantic map. As a backbone, we use the U-Net feature extractor [32]. Following [26, 34], we compare different temporal architectures. First, we apply a U-ConvLSTM network [26]. As a second method, we utilize 3D convolutions that process spatial and temporal information at once [26]. Finally, we employ U-TAE [34] that encodes temporal features in the latent space via self-attention [40].\\n\\n**Semi-supervised baselines.** As an alternative to modeling the input images as sequences, we can interpret them as an unordered collection of training samples. Analogous to standard supervised learning, the labeled examples are used directly as training data. To extract information from the remaining set of unlabeled training examples, we employ the recent state-of-the-art consistency-based semi-supervised segmentation method by Lai et al. [21]. The main idea is to randomly crop unlabeled images into pairs of patches and enforce consistent outputs for the overlap of both sub-regions. Robustness to varying contexts is crucial for our data since the surrounding of an overlapping region is generally an unreliable predictor for its class label. For example, water occurs in quite different environmental contexts in our dataset, like forests, agriculture, or impervious surfaces. We evaluate this method [21] with the segmentation backbone DeepLabv3+ [10].\\n\\n### 5.2. Land cover and land use segmentation\\n\\nThe first task we consider is semantic segmentation of land cover classes. Specifically, the goal is to predict one of the LULC labels described in Sec. 3.2. We compare the performance of the two classes of baseline methods discussed in the previous section. For each setting, we evaluate the intersection-over-union score averaged over all 6 evaluation LULC classes (mIoU). Due to its overall scarcity, we exclude the snow & ice class from the evaluations, see Sec. 3.3 for more details.\\n\\n**Spatio-temporal results.** Results of spatio-temporal methods are summarized in Tab. 3. As a first reference point, we consider the purely supervised setting. Here, we train a standard U-Net architecture only on the monthly labeled samples. It achieves 33.5% mIoU on the validation and 37.6% mIoU on the test set. We further assess whether existing spatio-temporal architectures benefit from the time-series nature of our data. All three considered architectures improve the performance over the supervised baseline for weekly temporal inputs on the validation set. U-TAE and U-ConvLSTM show the strongest generalization performance on the test set. On the other hand, when using daily sequences of 28-31 images, the performance drops considerably. This suggests that generic spatio-temporal techniques are not necessarily optimal for extracting information from daily satellite data. The individual images of such daily time series are often highly correlated. Consequently, when labeled data is limited, increasing the length of a sequence at some point leads to unstable training. For our benchmark, using weekly samples is optimal for the considered baselines. We conclude that more specialized techniques are needed to allow for robust learning on daily time-series satellite imagery.\\n\\n**Semi-supervised results.** We report the performances of our baseline [21] in combination with DeepLabv3+ [10] in Tab. 4. Similar to the spatio-temporal experiments, we consider different temporal densities. For the purely supervised setting, all unlabeled images are discarded (monthly). Additionally, we compare different semi-supervised settings with 6 (weekly), 28-31 (daily) unlabelled samples per month. Both, daily and weekly data help to improve over the purely supervised baseline.\"}"}
{"id": "CVPR-2022-545", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method          | Frequency | Labelled | Surface | Agriculture | Forest | Wetlands | Soil | Water | mIoU (\u2191) |\\n|-----------------|-----------|----------|---------|-------------|--------|----------|------|-------|----------|\\n| U-Net [32]      | monthly   |          |         | 28.6        | 6.9    | 76.4     | 0.0  | 38.4  | 50.5     |\\n|                 |           |          |         |             |        |          |      |       |          |\\n|                 | weekly    |          |         | 31.8        | 8.0    | 77.3     | 0.0  | 39.1  | 58.1     |\\n|                 | daily     |          |         | 26.3        | 6.5    | 73.7     | 0.0  | 35.7  | 51.2     |\\n| U-ConvLSTM [26] | weekly    |          |         | 31.4        | 2.2    | 77.7     | 0.0  | 36.1  | 58.6     |\\n|                 | daily     |          |         | 14.4        | 0.6    | 72.1     | 0.0  | 32.0  | 58.8     |\\n| 3D-Unet [26]   | weekly    |          |         | 32.4        | 2.1    | 77.4     | 0.0  | 35.3  | 65.5     |\\n|                 | daily     |          |         | 31.1        | 1.8    | 75.8     | 0.0  | 34.1  | 66.0     |\\n\\nTable 3. Quantitative results of spatio-temporal methods. We compare the performance of different spatio-temporal architectures on the task of LULC segmentation. Individual values denote the intersection-over-union score for individual classes (cols. 3-8), as well as the averaged scores over the whole validation set (9th col.) and test set (10th col.). The monthly U-Net baseline is generally less accurate than the considered temporal architectures.\\n\\n| Method          | Frequency | Labelled | Imp. Surface | Agriculture | Forest | Wetlands | Soil | Water | mIoU (\u2191) |\\n|-----------------|-----------|----------|--------------|-------------|--------|----------|------|-------|----------|\\n| CAC [21]        | monthly   | \u2713        | 18.1         | 4.8         | 74.7   | 0.0      | 33.9 | 55.9  | 31.2     |\\n|                 | weekly    | \u2717        | 28.0         | 7.2         | 75.7   | 8.3      | 38.9 | 51.0  | 34.9     |\\n|                 | daily     | \u2717        | 28.9         | 4.0         | 75.5   | 0.5      | 39.0 | 55.6  | 33.9     |\\n\\nTable 4. Quantitative results of semi-supervised methods. The table shows the semantic segmentation results of using the context-aware consistency-based semi-supervised approach [21] on our DynamicEarthNet dataset. We further present the IoU scores per class for the validation set. 'Monthly' indicates that the architecture is trained in a supervised manner. Using unlabelled satellite images improves the results over the fully supervised baseline.\\n\\nTable 5. Quantitative results of semantic change segmentation on our test set. This table shows semantic change segmentation results of all methods on our DynamicEarthNet dataset. A detailed analysis of these quantitative results shows that the agriculture and wetland classes prove to be difficult for all baselines. Agricultural areas are often confused with forest or soil, see Fig. 3, whereas wetlands get confused with soil and water. This is, to a certain degree, expected due to the visual similarity of these classes. Notably, training on daily data achieves the overall best result. The obtained accuracy is 43.6% mIoU on the test set, with a considerable improvement over the monthly and weekly results of 37.9%.\"}"}
{"id": "CVPR-2022-545", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Qualitative results on validation set. Semantic maps (bottom row) of the semi-supervised baseline CAC [21] trained on daily images. The input sequence consists of 5 images (middle row) from September to October, spanning one month. For the first and last semantic map of the considered sequence, we show ground-truth labels (bottom right, bottom left). The three middle columns show predictions of [21]. For each sample, we magnify a specific area to highlight the temporal transition from forest & other vegetation to soil, marked red for ground-truth and pink for baseline predictions [21]. Notably, this development is captured with high fidelity by our baseline [21]. On the other hand, in certain areas, it is not able to distinguish between the generic forest & vegetation class and the ground-truth label agriculture. For the color representation of segmentation maps see Tab. 2.\\n\\n\\\\( \\\\text{(mIoU)} \\\\) does not guarantee optimal performance in terms of the change segmentation score (SCS). When compared directly, the semantic change and binary change performance are somewhat decoupled which warrants the split of our SCS metric into binary change BC and semantic change SC.\\n\\n6. Conclusion\\nWe presented DynamicEarthNet, a novel dataset that provides daily, multi-spectral satellite imagery for a broad range of areas of interest. Beyond the raw imagery, it comprises monthly semantic annotations of 7 common LULC classes. This unique combination of dense time-series data and high-quality annotations distinguishes DynamicEarthNet from existing benchmarks, see Tab. 1, which are either temporally sparse or do not provide comparable ground-truth labels. We showed that this gives rise to previously unexplored settings like semi-supervised learning, as well as spatio-temporal methods with an unprecedented temporal resolution. We further devised a new evaluation protocol for semantic change segmentation. It involves several metrics that focus on distinct, common errors in the context of multi-class change prediction. We believe that our benchmark has the potential to spark the development of more specialized techniques that can take full advantage of daily, multi-spectral data. Finally, we highlight in several compelling case-studies how high frequency satellite data can be used to track land cover evolution, e.g., due to deforestation, and assess both its short and long-term effects.\\n\\nAcknowledgements\\nThis work is supported by the Humboldt Foundation through the Sofja Kovalevskaja Award, the framework of Helmholtz AI [grant number: ZT-I-PF-5-01] - Local Unit \\\"Munich Unit @Aeronautics, Space and Transport (MASTr)\\\", the Helmholtz Association under the joint research school \\\"Munich School for Data Science - MUDS\\\", and the German Federal Ministry for Economic Affairs and Energy (BMWi) under the grant DynamicEarthNet (grant number: 50EE2005).\"}"}
