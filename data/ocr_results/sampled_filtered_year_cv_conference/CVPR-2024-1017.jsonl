{"id": "CVPR-2024-1017", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Abstract\\nGeneralizable NeRF aims to synthesize novel views for unseen scenes. Common practices involve constructing variance-based cost volumes for geometry reconstruction and encoding 3D descriptors for decoding novel views. However, existing methods show limited generalization ability in challenging conditions due to inaccurate geometry, sub-optimal descriptors, and decoding strategies. We address these issues point by point. First, we find the variance-based cost volume exhibits failure patterns as the features of pixels corresponding to the same point can be inconsistent across different views due to occlusions or reflections. We introduce an Adaptive Cost Aggregation (ACA) approach to amplify the contribution of consistent pixel pairs and suppress inconsistent ones. Unlike previous methods that solely fuse 2D features into descriptors, our approach introduces a Spatial-View Aggregator (SVA) to incorporate 3D context into descriptors through spatial and inter-view interaction. When decoding the descriptors, we observe the two existing decoding strategies excel in different areas, which are complementary. A Consistency-Aware Fusion (CAF) strategy is proposed to leverage the advantages of both. We incorporate the above ACA, SVA, and CAF into a coarse-to-fine framework, termed Geometry-aware Reconstruction and Fusion-refined Rendering (GeFu). GeFu attains state-of-the-art performance across multiple datasets. Code is available at https://github.com/TQTQliu/GeFu.\\n\\n1. Introduction\\nNovel view synthesis (NVS) aims to generate realistic images at novel viewpoints given a set of posed images. By encoding the density and radiance fields of scenes into implicit representations, Neural Radiance Field (NeRF) [21] has shown impressive performance in NVS. However, NeRF requires a lengthy optimization with densely captured images for each scene, which limits its applications.\\nTo address this issue, some recent methods [4, 6, 8, 13, 15, 16, 19, 26\u201328, 34, 40] generalize NeRFs to unseen scenes. Instead of overfitting the scene, they extract feature descriptors for 3D points in a scene-agnostic manner, which are then decoded for rendering novel views. Pioneering methods [28, 40] utilize 2D features warped from the source images. As this practice avoids explicit modeling of 3D geometric constraints, its generalization capability for geometry reasoning and view synthesis in new scenes is limited. Hence, subsequent methods [4, 16] introduce explicit geometry-aware cost volume from multi-view stereo (MVS) to model geometry at the novel view. Despite significant progress achieved by these methods, synthesis results remain unsatisfactory, especially in challenging areas, such as occluded regions.\\n\\nFigure 1. Comparison with existing methods. (a) With three input source views, our generalizable model synthesizes novel views with higher quality than existing methods [4, 16] in the severe occluded area. (b) Circle area represents inference time. The X-axis represents the PSNR on the DTU dataset [1] and the Y-axis represents the PSNR on the Real Forward-facing dataset [20]. Our method attains state-of-the-art performance.\"}"}
{"id": "CVPR-2024-1017", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"as the occluded area illustrated in Fig. 1 (a). The model struggles to accurately infer the geometry of these regions, as variance-based cost volumes cannot perceive occlusions. Generalizable NeRFs' pipeline consists of two phases: radiance field reconstruction and rendering. The reconstruction phase aims to recover scene geometry and encode 3D-aware features for rendering, i.e., creating descriptors for 3D points. For geometry reasoning, similar to MVS, if the geometry (i.e., depth) is accurate, features across various views are supposed to be similar with low variance. However, this variance-based cost metric is not universal, especially in occluded and reflective regions. Due to inconsistent features in these regions, equally considering the contributions of different views is unreasonable, leading to misleading variance values. Inspired by MVS methods [29, 31], we propose an Adaptive Cost Aggregation (ACA) module. ACA adaptively reweights the contributions of different views based on the similarity between source views and the novel view. Since the novel view is unavailable, we employ a coarse-to-fine framework and transfer the rendering view from the coarse stage into the fine stage to learn adaptive weights for the cost volume construction. With the geometry derived from the cost volume, we can re-sample 3D points around the surface and encode descriptors for sampled points. For descriptors encoding, previous methods [6, 16, 28, 40] directly aggregate inter-view features into descriptors for subsequent rendering. These descriptors lack 3D context awareness, leading to discontinuities in the descriptor space. To this end, we design the Spatial-View Aggregator (SVA) to learn 3D context-aware descriptors. Specifically, we encode spatially context-aware and smooth features by aggregating 3D spatial information. Meanwhile, to preserve geometric details, we utilize smoothed features as queries to reassemble high-frequency information across views to create final descriptors. With the scene geometry and point-wise descriptors, the subsequent rendering phase aims to decode descriptors into volume density and radiance for rendering a novel view. For the radiance prediction, [16, 28] predict blending weights to combine color values from source views, while [4, 6, 40] directly regress from features. However, the analysis of these two approaches has not been conducted in existing works. In this paper, we observe that the blending approach performs better in most areas (Fig. 2 (a)), as the color values from source views provide referential factors. However, as shown in Fig. 2 (b) & (c), in challenging areas such as reflections and boundaries, the regression approach produces superior results with fewer artifacts, while the blending approach leads to suboptimal rendering due to unreliable referential factors. To unify the advantages of both strategies, we propose to separately predict two intermediate views using two approaches and design a weighted structure named Consistency-Aware Fusion (CAF) to dynamically fuse them into the final view. The fusing weights are learned by checking multi-view consistency, following an underlying principle that if color values are close to the ground truth, the multi-view features corresponding to the correct depth are supposed to be similar. By embedding the above ACA, SVA, and CAF into a coarse-to-fine framework, we propose GeFu. To demonstrate the effectiveness, we evaluate GeFu on the widely-used DTU [1], Real Forward-facing [20], and NeRF Synthetic [21] datasets. Extensive experiments show that GeFu outperforms other generalizable NeRFs by large margins without scene-specific fine-tuning as shown in Fig. 1 (a) & (b). After per-scene fine-tuning, GeFu also outperforms other generalizable NeRFs and achieves performance comparable to or even better than NeRF [21]. Additionally, GeFu is capable of generating reasonable depth maps, surpassing other generalization methods. Our main contributions can be summarized as follows:\\n\\n\u2022 We propose ACA to improve geometry estimation and SVA to encode 3D context-aware descriptors for geometry-aware reconstruction.\\n\\n\u2022 We conduct an analysis of two existing color decoding strategies and propose CAF to unify their advantages for fusion-refined rendering.\\n\\n\u2022 GeFu achieves state-of-the-art performance across multiple datasets, showing superior generalization.\"}"}
{"id": "CVPR-2024-1017", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"mainstream in the MVS community. Following works explore the potential capacity of this pipeline from various aspects, e.g., reducing memory consumption with recurrent approaches [31, 35, 38], or coarse-to-fine paradigms [7, 12, 36, 41], enhancing feature representations [9, 18] and modeling output formats [23, 39]. Another important line is to optimize the cost aggregation [29, 31] by adaptively weighting contributions from various views. In this paper, following the spirit, we introduce the adaptive cost aggregation tailored for the NVS task to mitigate the issue of inconsistent features caused by reflections and occlusions.\\n\\nGeneralizable NeRF.\\n\\nWith implicit continuous representation and differentiable volume rendering, NeRF [21] achieves photo-realistic view synthesis. However, NeRF and its downstream expansion works [2, 3, 5, 22, 32, 33] require an expensive per-scene optimization process. To address this issue, some generalizable NeRF methods have been proposed, following a reconstruction-and-rendering pipeline. In the reconstruction phase, each sampled point is assigned a feature descriptor. Specifically, according to the descriptors, generalizable NeRF methods can be categorized into the following types: appearance descriptors [40], aggregated multi-view descriptors [16, 26, 28], cost volume interpolated descriptors [4, 16, 19], and correspondence matching descriptors [6]. Despite different forms, these descriptors only aggregate inter-view information or are interpolated from the low-resolution cost volume, lacking the ability to effectively perceive 3D spatial context. To remedy the issue, we utilize a proposed aggregator to facilitate the interaction of spatial information. In the rendering phase, volume density is obtained by decoding descriptors. For radiance, [16, 28] predict blending weights to combine color from source views, while [4, 6, 19, 26, 40] directly regress features. In this paper, we observe that these two strategies benefit different regions and thus propose a unified structure to integrate their advantages.\\n\\n3. Preliminaries\\n\\nLearning-based MVS.\\n\\nGiven a target image and \\\\( N \\\\) source images, MVS aims to recover the geometry, such as the depth map of the target image. The key idea of MVS is to construct the cost volume from multi-view inputs, aggregating 2D information into 3D geometry-aware representation. Specifically, each voxel-aligned feature vector \\\\( f_c \\\\) of cost volume can be computed as:\\n\\n\\\\[\\nf_c = \\\\Omega(f_t, f_{1s}, ..., f_{Ns}),\\n\\\\]\\n\\nwhere \\\\( f_t \\\\) and \\\\( f_{is} \\\\) represent the target feature vector and the warped source feature vector, respectively. And \\\\( \\\\Omega \\\\) denotes a consistency metric, such as variance. The underlying principle is that if a sampled depth is close to the actual depth, the multi-view features of the sampled point are supposed to be similar, which naturally performs multi-view correspondence matching and geometry reasoning, facilitating the generalization to unseen scenes.\\n\\nGeneralizable NeRF.\\n\\nIn generalizable NeRFs, each sampled point is assigned a geometry-aware feature descriptor \\\\( f_p \\\\), as \\\\( (x, d, f_p) \\\\rightarrow (\\\\sigma, r) \\\\), where \\\\( x \\\\) and \\\\( d \\\\) represent the coordinate and view direction used by NeRF [21]. \\\\( \\\\sigma \\\\) and \\\\( r \\\\) denote the volume density and radiance for the sampled point, respectively. Specifically, the volume density \\\\( \\\\sigma \\\\) can be obtained from the descriptor via \\\\( \\\\sigma = \\\\text{MLP}(x, f_p) \\\\). For the radiance \\\\( r \\\\), one approach [16, 28] is to predict blending weights to combine color values from source views, as:\\n\\n\\\\[\\nr = \\\\sum_{i=1}^{N} \\\\exp(w_i) c_i \\n  \\\\sum_{j=1}^{N} \\\\exp(w_j) \\n\\\\]\\n\\nwhere \\\\( w_i = \\\\text{MLP}(x, d, f_p, f_{is}) \\\\), \\\\( c_i \\\\) are color values from source views. These colors provide referential factors, facilitating convergence and better performance in most areas. However, in occluded and reflective regions, these colors introduce misleading bias to the combination, resulting in distorted colors. Besides, for pixels on object boundaries, it is hard to accurately locate reference points for blending, resulting in oscillated colors between the foreground and background. Another approach [4, 6, 40] is to directly regress the radiance from the feature per \\\\( r = \\\\text{MLP}(x, d, f_p) \\\\). As fewer inductive biases are imposed on the output space, the model can learn to predict fewer artifacts in challenging areas (Fig. 2). With the volume density and radiance of sampled points, the color values \\\\( c \\\\) of each pixel can be computed by volume rendering, given by:\\n\\n\\\\[\\nc = \\\\sum_k \\\\tau_k (1 - \\\\exp(-\\\\sigma_k)) r_k \\n\\\\]\\n\\nwhere \\\\( \\\\tau_k = \\\\exp(-k - 1 \\\\sum_{j=1}^{N} \\\\sigma_j) \\\\), where \\\\( \\\\tau \\\\) represents the volume transmittance.\\n\\n4. Method\\n\\nGiven a set of source views \\\\( \\\\{I_i \\\\}_1^N \\\\), NVS aims to generate a target view at a novel camera pose. As illustrated in Fig. 3, our method consists of an NVS pipeline wrapped in a coarse-to-fine framework. In the pipeline, we first employ a feature pyramid network [17] to extract multi-scale features from the source views. Then, we propose an Adaptive Cost Aggregation module (Sec. 4.1) to construct a cost volume, which is further processed by a 3D-CNN to infer the geometry. Guided by the estimated geometry, we re-sample 3D points around the surface and apply the Spatial-View Aggregator (Sec. 4.2) to encode 3D-aware feature descriptors \\\\( f_p \\\\) for sampled points. Finally, we decode \\\\( f_p \\\\) into two intermediate views using two strategies introduced in Sec. 3 and fuse them into the final target view through Consistency-Aware Fusion (Sec. 4.3). This pipeline is iteratively called. Initially, the low-resolution target view is generated and the...\"}"}
{"id": "CVPR-2024-1017", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. The overview of GeFu. In the reconstruction phase, we first infer the geometry from the constructed cost volume, and the geometry guides us to further re-sample 3D points around the surface. For each sampled point, the warped features from source images are aggregated and then fed into our proposed Spatial-View Aggregator (SVA) to learn spatial and inter-view context-aware descriptors $f_p$.\\n\\nIn the rendering phase, we apply two decoding strategies to obtain two intermediate views and fuse them into the final target view in an adaptive way, termed Consistency-Aware Fusion (CAF). Our pipeline adopts a coarse-to-fine architecture, the geometry from the coarse stage ($l_s = 1$) guides the sampling at the fine stage ($l_s > 1$), and the features from the coarse stage are transferred to the fine stage for ACA to improve geometry estimation. Our network is trained end-to-end using only RGB images.\\n\\n### 4.1. Adaptive Cost Aggregation\\nThe core process of geometry reasoning is to construct a cost volume that encodes the multi-view feature consistency. Previous works [4, 16] treat different views equally and employ the variance operator to construct a cost volume. However, due to potential occlusions and varying lighting conditions among different views, multi-view features of the same 3D point may exhibit notable disparities, thereby resulting in misleading variance values. Inspired by [31], we propose to adaptively weight the contribution of different views to the cost volume, termed Adaptive Cost Aggregation (ACA). ACA will suppress the cost contribution from features that are inconsistent with the novel views caused by reflections or occlusions, and enhance the contribution of better-matched pixel pairs. The voxel-aligned feature $f_c$ of cost volume can be computed as:\\n\\n$$f_c = \\\\frac{1}{N} \\\\sum_{i=1}^{N} \\\\left( 1 + \\\\alpha \\\\left( f_{i,c} \\\\right) \\\\right) \\\\odot f_{i,c}$$\\n\\nwhere $f_{i,c} = (f_{i,s} - f_{t})^2$,\\n\\nwhere $f_t$ and $f_{i,s}$ denote the target feature vector and the warped feature vector of source image $I_i$, respectively. $\\\\odot$ denotes Hadamard multiplication and $\\\\alpha$ represents the adaptive weight for each view. However, an important challenge is that the target view is available in MVS, but not for NVS. To remedy this, we adopt a coarse-to-fine framework where a coarse novel view is first generated and serves as the target view in Eq. (4). Specifically, we obtain the coarse-stage feature $f_b$ by accumulating descriptors $f_p$ of sampled points along the ray, as:\\n\\n$$f_b = \\\\sum_{k} \\\\tau_k (1 - \\\\exp(-\\\\sigma_k)) f_k p$$\\n\\nThe coarse-stage features are then fed into a 2D U-Net for spatial aggregation to obtain $f_r$. We replace $f_t$ in Eq. (4) with the feature $f_r$ to construct a robust cost volume, benefiting the geometry estimation.\\n\\n### 4.2. Spatial-View Aggregator\\nWith the estimated geometry, 3D points around objects' surfaces can be re-sampled, and the subsequent step is encoding descriptors for these sampled points. Existing methods [16, 28] aggregate inter-view features with a pooling network $\\\\rho$ to construct the descriptors $f_p = \\\\rho(\\\\{f_{i,s}\\\\}_{i=1}^{N})$.\\n\\nHowever, these descriptors only encode multi-view information and lack the awareness of 3D spatial context, leading to discontinuities in the descriptor space. To introduce 3D spatial information, a feasible approach is to interpolate the low-resolution regularized cost volume, but it lacks fine-grained details. To address these issues, we design a 3D-aware descriptors encoding approach. We first utilize a 3D U-Net termed $\\\\phi_{sm}$ to aggregate 3D spatial context, as:\\n\\n$$f_{sm} = \\\\phi_{sm} (\\\\rho(\\\\{f_{i,s}\\\\}_{i=1}^{N}))$$\\n\\nHowever, this may lead to the smoothing of 3D features and cause the loss of some high-frequency geometric details.\"}"}
{"id": "CVPR-2024-1017", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Therefore, we use the smoothed features as queries to re-gather inter-view high-frequency details, as:\\n\\n$$f_p = \\\\phi_d(f_{sm}, \\\\{f_i\\\\}_{N=1})$$\\n\\nwhere $$\\\\phi_d$$ is an attention module. $$f_{sm}$$ is the input query, and $$\\\\{f_i\\\\}_{N=1}$$ are the key sequences $$k$$ and value sequences $$v$$. The sequence lengths of $$q$$, $$k$$, and $$v$$ are 1, $$N$$, and $$N$$, respectively, which results in only a slight increase in computational costs.\\n\\n### 4.3. Consistency-Aware Fusion\\n\\nWith the descriptor $$f_p$$, the volume density $$\\\\sigma$$ is first acquired through an MLP, and color values can be obtained using two decoding approaches discussed in Sec. 3. We observe that these two approaches exhibit advantages in different areas. To combine their strengths, we propose to predict two intermediate views using these two approaches separately, and then fuse them into the final target view.\\n\\nSpecifically, for the blending approach, the radiance $$r$$ of each sampled point can be computed using Eq. (2). And then the pixel color $$c_b$$ and feature $$f_b$$ are obtained via the volume rendering manner per Eq. (3) and Eq. (5), respectively. For another regression approach, one practice is predicting radiance for sampled points and then accumulating it into pixel color. In contrast, to reduce computational costs, we accumulate point-wise descriptors into features and then decode them into pixel color. Specifically, we feed the accumulated feature into a 2D U-Net for spatial enhancement to obtain pixel feature $$f_r$$, followed by an MLP to yield the pixel color $$c_r$$.\\n\\nSince these two approaches excel in different areas, instead of using a fixed operator, such as average, we propose dynamically fusing them via\\n\\n$$c_t = w_b c_b + w_r c_r$$\\n\\nwhere $$w_b$$ and $$w_r$$ are predicted fusing weights. As the target view is unavailable, comparing the quality of $$c_b$$ and $$c_r$$ becomes a chicken-and-egg problem. A naive practice is to directly predict fusing weights from features, which is under-constrained (Sec. 5.5). In contrast, we propose to learn fusing weights by using the multi-view feature consistency as a hint. The underlying motivation is that if the predicted colors closely resemble the ground-truth colors, the corresponding features of the predicted view and source views under the correct depth are supposed to be similar.\\n\\nSpecifically, we first obtain the final predicted depth $$d_f$$ in a volume rendering-like way per\\n\\n$$d_f = \\\\frac{1 - \\\\exp(-\\\\sigma_k)}{d_k}$$\\n\\nwhere $$d_k$$ represents the depth of sampled point. With the depth $$d_f$$, we can obtain the warped features $$\\\\{f_i\\\\}_{N=1}$$ from source views, and the multi-view consistency can be computed using variance:\\n\\n$$f_{[b,r]}w = \\\\text{var}(f_{[b,r]}, f_1, ..., f_N)$$\\n\\nWe then feed the consistency $$f_{[b,r]}w$$ into an MLP to obtain the fusing weight $$w_{[b,r]}$$, followed by a softmax operator for normalization. The final target view can be represented in matrix form as\\n\\n$$I_t = W_b I_b + W_r I_r$$\\n\\n### 4.4. Loss Function\\n\\nOur model is trained end-to-end only using the RGB image as supervision. Following [16], we use the mean squared error loss as:\\n\\n$$L_{mse} = \\\\frac{1}{N_p} \\\\sum_{i=1}^{N_p} ||\\\\hat{c}_i - c_i||^2$$\\n\\nwhere $$N_p$$ is the number of pixels and $$\\\\hat{c}_i$$ and $$c_i$$ are the ground-truth and predicted pixel color, respectively. In addition, the perceptual loss [42] and ssim loss [30] can be applied, as:\\n\\n$$L_{perc} = ||h(\\\\hat{I}) - h(I)||$$\\n\\n$$L_{ssim} = 1 - \\\\text{ssim}(\\\\hat{I}, I)$$\\n\\nwhere $$h$$ is the perceptual function (a VGG 16 network). $$\\\\hat{I}$$ and $$I$$ are the ground-truth and predicted image patches, respectively. The loss at the $$k$$th stage is as follows:\\n\\n$$L_k = L_{mse} + \\\\lambda_p L_{perc} + \\\\lambda_s L_{ssim}$$\\n\\nwhere $$\\\\lambda_p$$ and $$\\\\lambda_s$$ refer to loss weights. The overall loss is:\\n\\n$$L = \\\\sum_{k=1}^{N_s} \\\\lambda_k L_k$$\\n\\nwhere $$N_s$$ refers to the number of coarse-to-fine stages and $$\\\\lambda_k$$ represents the loss weight of the $$k$$th stage.\\n\\n### 5. Experiments\\n\\n#### 5.1. Settings\\n\\n**Datasets.** Following MVSNeRF [4], we divide the DTU [1] dataset into 88 training scenes and 16 test scenes. We first train our generalizable model on the 88 training scenes of the DTU dataset and then evaluate the trained model on the 16 test scenes. To further demonstrate the generalization capability of our method, we also test the trained model (without any fine-tuning) on 8 scenes from the Real Forward-facing [20] dataset and 8 scenes from the NeRF Synthetic [21] dataset, both of which have significant differences in view distribution and scene content compared to the DTU dataset. The image resolutions of the DTU, the Real Forward-facing, and the NeRF Synthetic datasets are 512 $\\\\times$ 640, 640 $\\\\times$ 960, and 800 $\\\\times$ 800, respectively. The quality of synthesized novel views is measured by PSNR, SSIM [30], and LPIPS [42] metrics.\\n\\n**Baselines.** We compare our methods with state-of-the-art generalizable NeRF methods [4, 6, 16, 19, 26, 28, 40].\"}"}
{"id": "CVPR-2024-1017", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method        | DTU [1] | Real Forward-facing [20] | NeRF Synthetic [21] |\\n|--------------|---------|--------------------------|---------------------|\\n|              | PSNR    | SSIM                     | LPIPS               |\\n|              | \u2191       | \u2191                        | \u2193                   |\\n| PixelNeRF    | 19.31   | 0.789                    | 0.382               |\\n| IBRNet       | 26.04   | 0.917                    | 0.191               |\\n| MVSNeRF      | 26.63   | 0.931                    | 0.10                |\\n| NeuRay       | 25.81   | 0.868                    | 0.160               |\\n| ENeRF        | 27.61   | 0.957                    | 0.089               |\\n| GNT          | 26.39   | 0.923                    | 0.156               |\\n| MatchNeRF    | 26.91   | 0.934                    | 0.159               |\\n| Ours         | 29.36   | 0.969                    | 0.064               |\\n|              | PSNR    | SSIM                     | LPIPS               |\\n|              | \u2191       | \u2191                        | \u2193                   |\\n| MVSNeRF      | 24.03   | 0.914                    | 0.192               |\\n| NeuRay       | 24.51   | 0.825                    | 0.203               |\\n| ENeRF        | 25.48   | 0.942                    | 0.107               |\\n| GNT          | 24.32   | 0.903                    | 0.201               |\\n| MatchNeRF    | 25.03   | 0.919                    | 0.181               |\\n| Ours         | 26.98   | 0.955                    | 0.081               |\\n\\nTable 1. Quantitative results under the generalization setting.\\n\\nWe show the average results of PSNRs, SSIMs, and LPIPSs on three datasets under two settings for the number of input views. The comparison methods are organized based on the year of publication.\\n\\nTable 2. Quantitative results under the per-scene optimization setting.\\n\\nThe best result is in bold, and the second-best is underlined.\\n\\n### Implementation Details\\nFollowing [16], the number of coarse-to-fine stages $N_s$ is set to 2. In our coarse-to-fine framework, we sample 64 and 8 depth planes for the coarse-level and fine-level cost volumes, respectively. And we sample 8 and 2 points per ray for the coarse-level and fine-level view rendering, respectively. We set $\\\\lambda_p = 0.1$ and $\\\\lambda_s = 0.1$ in Eq. (11), while $\\\\lambda_1 = 0.5$ and $\\\\lambda_2 = 1$ in Eq. (12). We train our model on four RTX 3090 GPUs using the Adam [14] optimizer. Refer to the supplementary material for more implementation and network details.\\n\\n### 5.2. Generalization Results\\nWe report quantitative results on DTU, Real Forward-facing, and NeRF Synthetic datasets in Table 1 under generalization settings. PixelNeRF [40], which applies appearance descriptors, has reasonable results on the DTU test set, but insufficient generalization on the other two datasets. Other methods [4, 6, 16, 19, 26, 28] that model the scene geometry implicitly or explicitly by aggregating multi-view features can maintain relatively good generalization. Thanks to our proposed modules tailored for both the reconstruction and rendering phases, our method achieves significantly better generalizability. As shown in Fig. 4, the views produced by our method preserve more scene details and contain fewer artifacts. In challenging areas, such as occluded regions, object boundaries, and regions with complex geometry, our method significantly outperforms the others.\"}"}
{"id": "CVPR-2024-1017", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Qualitative comparison of rendering quality with state-of-the-art methods [4, 6, 16] under generalization and three input views settings.\\n\\nFigure 5. Qualitative comparison of depth maps with [16].\\n\\n5.3. Per-scene Fine-tuning Results\\n\\nThe quantitative results after per-scene optimization are shown in Table 2 and we report the results of our method after 15 minutes and 1 hour of fine-tuning. Due to the excellent initialization provided by our generalization model, only a short period of fine-tuning is needed to achieve good results. Our results after 15 minutes of fine-tuning are comparable to or even superior to those of NeRF [21] optimized for substantially longer time (10.2 hours), and also outperform the results of other generalization methods after fine-tuning. With a longer fine-tuning duration, such as 1 hour, the rendering quality can be further improved.\\n\\nTable 3. Quantitative results of depth reconstruction on the DTU test set. MVSNet is trained with depth supervision while other methods are trained with only RGB image supervision. \\\"Abs err\\\" represents the average absolute error and \\\"Acc(X)\\\" means the percentage of pixels with an error less than X mm. Results can be found in the supplementary material.\\n\\n5.4. Depth Reconstruction Results\\n\\nFollowing [4, 16], we report the performance of depth reconstruction in Table 3. Our method can achieve higher depth accuracy than other methods, even including the MVS method MVSNet [37] that is trained with depth supervision. As shown in Fig. 5, the depth map produced by our method is more refined, such as sharper object edges.\\n\\n5.5. Ablations and Analysis\\n\\nAblation studies. As shown in Table 4, we conduct ablation studies to investigate the contribution of each proposed module. Each individual component can benefit the base-\"}"}
{"id": "CVPR-2024-1017", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Visualization of Fusion Weights. $W_b$ and $W_r$ represent the weight maps of the blending approach and regression approach, respectively.\\n\\nFigure 7. Quantitative analysis of difficult areas. The X-axis difficult areas ($X\\\\%$) represents considering the area with the top $X\\\\%$ of values in $W_r$ as a difficult area. A smaller threshold $X$ indicates a more challenging area.\\n\\nAs shown in Fig. 6, we visualize the fusion weights of two decoding approaches. The regression approach exhibits higher confidence in challenging areas such as object edges and reflections, while the blending approach shows higher confidence in most other areas, which is consistent with the observation in Fig. 2.\\n\\nAs shown in Fig. 7, we define challenging areas as those with high confidence in $W_r$ and divide them by a series of thresholds. A smaller threshold $X$ indicates a more difficult region. When $X = 5\\\\%$, our method improves PSNR by $2.55$ dB and \\\"Abs err\\\" by $3.47$ mm. When the threshold increases, such as $X = 50\\\\%$, our method improves PSNR by $1.97$ dB and \\\"Abs err\\\" by $1.54$ mm, which further demonstrates the superiority of our method in challenging areas.\\n\\nFusion strategy. As shown in Table 5, we investigate the performance of different fusion strategies. Comparing No.1 and No.2, the result of using the blending approach alone is better than that of using the regression approach alone. For No.4, the DWF represents fusion weights derived directly from features of the two intermediate views, which greatly degrades performance compared to our proposed way of checking the multi-view consistency (No.5). Our fusion approach utilizes the advantages of the two decoding approaches to refine the synthesized view. In No.3, we refine the synthesized view decoded in a single way, where the synthesized view is fed into an auto-encoder for refinement, which has limited improvement.\\n\\n6. Conclusion\\n\\nIn this paper, we present a generalizable NeRF method capable of achieving high-fidelity view synthesis. Specifically, during the reconstruction phase, we propose Adaptive Cost Aggregation (ACA) to improve geometry estimation and Spatial-View Aggregator (SV A) to encode 3D context-aware descriptors. In the rendering phase, we conduct an analysis of two existing color decoding strategies and introduce the Consistency-Aware Fusion (CAF) module to unify their advantages to refine the synthesized view quality. We integrate these modules into a coarse-to-fine framework, termed GeFu. Extensive evaluations and ablations demonstrate the effectiveness of our proposed modules.\"}"}
{"id": "CVPR-2024-1017", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Henrik Aanaes, Rasmus Ramsbol Jensen, George V ogiatzis, Engin Tola, and Anders Bjorholm Dahl. Large-scale data for multiple-view stereopsis. Int. J. Comput. Vis., 120:153\u2013168, 2016.\\n\\n[2] Sai Bi, Zexiang Xu, Pratul Srinivasan, Ben Mildenhall, Kalyan Sunkavalli, Milo\u0161 Ha\u0161an, Yannick Hold-Geoffroy, David Kriegman, and Ravi Ramamoorthi. Neural reflectance fields for appearance acquisition. arXiv preprint arXiv:2008.03824, 2020.\\n\\n[3] Mark Boss, Raphael Braun, Varun Jampani, Jonathan T Barron, Ce Liu, and Hendrik Lensch. Nerd: Neural reflectance decomposition from image collections. In Proc. IEEE Int. Conf. Comput. Vis., pages 12684\u201312694, 2021.\\n\\n[4] Anpei Chen, Zexiang Xu, Fuqiang Zhao, Xiaoshuai Zhang, Fanbo Xiang, Jingyi Yu, and Hao Su. Mvsnerf: Fast generalizable radiance field reconstruction from multi-view stereo. In Proc. IEEE Int. Conf. Comput. Vis., pages 14124\u201314133, 2021.\\n\\n[5] Anpei Chen, Ruiyang Liu, Ling Xie, Zhang Chen, Hao Su, and Jingyi Yu. Sofgan: A portrait image generator with dynamic styling. ACM Trans. Graph., 41(1):1\u201326, 2022.\\n\\n[6] Yuedong Chen, Haofei Xu, Qianyi Wu, Chuanxia Zheng, Tat-Jen Cham, and Jianfei Cai. Explicit correspondence matching for generalizable neural radiance fields. arXiv preprint arXiv:2304.12294, 2023.\\n\\n[7] Shuo Cheng, Zexiang Xu, Shilin Zhu, Zhuwen Li, Li Erran Li, Ravi Ramamoorthi, and Hao Su. Deep stereo using adaptive thin volume representation with uncertainty awareness. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., pages 2524\u20132534, 2020.\\n\\n[8] Julian Chibane, Aayush Bansal, Verica Lazova, and Gerard Pons-Moll. Stereo radiance fields (srf): Learning view synthesis for sparse views of novel scenes. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., pages 7911\u20137920, 2021.\\n\\n[9] Yikang Ding, Wentao Yuan, Qingtian Zhu, Haotian Zhang, Xiangyue Liu, Yuanjiang Wang, and Xiao Liu. Transmvsnet global context-aware multi-view stereo network with transformers. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., pages 8585\u20138594, 2022.\\n\\n[10] Pascal Fua and Yvan G Leclerc. Object-centered surface reconstruction combining multi-image stereo and shading. Int. J. Comput. Vis., 16(ARTICLE):35\u201356, 1995.\\n\\n[11] Silvano Galliani, Katrin Lasinger, and Konrad Schindler. Massively parallel multiview stereopsis by surface normal diffusion. In Proc. IEEE Int. Conf. Comput. Vis., pages 873\u2013881, 2015.\\n\\n[12] Xiaodong Gu, Zhiwen Fan, Siyu Zhu, Zuozhuo Dai, Feitong Tan, and Ping Tan. Cascade cost volume for high-resolution multi-view stereo and stereo matching. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., pages 2495\u20132504, 2020.\\n\\n[13] M. Johari, Y . Lepoittevin, and F. Fleuret. Geonerf: Generalizing nerf with geometry priors. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2022.\\n\\n[14] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.\\n\\n[15] Jiaxin Li, Zijian Feng, Qi She, Henghui Ding, Changhu Wang, and Gim Hee Lee. Mine: Towards continuous depth mpi with nerf for novel view synthesis. In Proc. IEEE Int. Conf. Comput. Vis., 2021.\\n\\n[16] Haotong Lin, Sida Peng, Zhen Xu, Yunzhi Yan, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Efficient neural radiance fields for interactive free-viewpoint video. In SIGGRAPH Asia Conference Proceedings, 2022.\\n\\n[17] Tsung-Yi Lin, Piotr Doll\u00e1r, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2117\u20132125, 2017.\\n\\n[18] Tianqi Liu, Xinyi Ye, Weiyue Zhao, Zhiyu Pan, Min Shi, and Zhiguo Cao. When epipolar constraint meets non-local operators in multi-view stereo. In Proc. IEEE Int. Conf. Comput. Vis., pages 18088\u201318097, 2023.\\n\\n[19] Yuan Liu, Sida Peng, Lingjie Liu, Qianqian Wang, Peng Wang, Christian Theobalt, Xiaowei Zhou, and Wenping Wang. Neural rays for occlusion-aware image-based rendering. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2022.\\n\\n[20] Ben Mildenhall, Pratul P Srinivasan, Rodrigo Ortiz-Cayon, Nima Khademi Kalantari, Ravi Ramamoorthi, Ren Ng, and Abhishek Kar. Local light field fusion: Practical view synthesis with prescriptive sampling guidelines. ACM Trans. Graph., 38(4):1\u201314, 2019.\\n\\n[21] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik, Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. In Proc. Eur. Conf. Comput. Vis., 2020.\\n\\n[22] Keunhong Park, Utkarsh Sinha, Jonathan T Barron, Sofien Bouaziz, Dan B Goldman, Steven M Seitz, and Ricardo Martin-Brualla. Nerfies: Deformable neural radiance fields. In Proc. IEEE Int. Conf. Comput. Vis., pages 5865\u20135874, 2021.\\n\\n[23] Rui Peng, Rongjie Wang, Zhenyu Wang, Yawen Lai, and Ronggang Wang. Rethinking depth estimation for multi-view stereo a unified representation. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., pages 8645\u20138654, 2022.\\n\\n[24] Johannes L Schonberger and Jan-Michael Frahm. Structure-from-motion revisited. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., pages 4104\u20134113, 2016.\\n\\n[25] Johannes L Schonberger, Enliang Zheng, Jan-Michael Frahm, and Marc Pollefeys. Pixelwise view selection for unstructured multi-view stereo. In Proc. Eur. Conf. Comput. Vis., pages 501\u2013518. Springer, 2016.\\n\\n[26] Mukund Varma T, Peihao Wang, Xuxi Chen, Tianlong Chen, Subhashini Venugopalan, and Zhangyang Wang. Is attention all that neRF needs? In Proc. Int. Conf. Learn. Repr., 2023.\\n\\n[27] Alex Trevithick and Bo Yang. Grf: Learning a general radiance field for 3d representation and rendering. In Proc. IEEE Int. Conf. Comput. Vis., pages 15182\u201315192, 2021.\\n\\n[28] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo Martin-Brualla, Noah Snavely, and Thomas Funkhouser. Ibrnet:\"}"}
{"id": "CVPR-2024-1017", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[29] Xiaofeng Wang, Zheng Zhu, Guan Huang, Fangbo Qin, Yun Ye, Yijia He, Xu Chi, and Xingang Wang. Mvster epipolar transformer for efficient multi-view stereo. In Proc. Eur. Conf. Comput. Vis., pages 573\u2013591. Springer, 2022.\\n\\n[30] Zhou Wang, Alan C Bovik, Hamid R Sheikh, and Eero P Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600\u2013612, 2004.\\n\\n[31] Zizhuang Wei, Qingtian Zhu, Chen Min, Yisong Chen, and Guoping Wang. Aa-rmvsnet adaptive aggregation recurrent multi-view stereo network. In Proc. IEEE Int. Conf. Comput. Vis., pages 6187\u20136196, 2021.\\n\\n[32] Wenqi Xian, Jia-Bin Huang, Johannes Kopf, and Changil Kim. Space-time neural irradiance fields for free-viewpoint video. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., pages 9421\u20139431, 2021.\\n\\n[33] Fanbo Xiang, Zexiang Xu, Milos Hasan, Yannick Hold-Geoffroy, Kalyan Sunkavalli, and Hao Su. Neutex: Neural texture mapping for volumetric neural rendering. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., pages 7119\u20137128, 2021.\\n\\n[34] Qiangeng Xu, Zexiang Xu, Julien Philip, Sai Bi, Zhixin Shu, Kalyan Sunkavalli, and Ulrich Neumann. Point-nerf: Point-based neural radiance fields. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., pages 5438\u20135448, 2022.\\n\\n[35] Jianfeng Yan, Zizhuang Wei, Hongwei Yi, Mingyu Ding, Runze Zhang, Yisong Chen, Guoping Wang, and Yu-Wing Tai. Dense hybrid recurrent multi-view stereo net with dynamic consistency checking. In Proc. Eur. Conf. Comput. Vis., pages 674\u2013689. Springer, 2020.\\n\\n[36] Jiayu Yang, Wei Mao, Jose M Alvarez, and Miaomiao Liu. Cost volume pyramid based depth inference for multi-view stereo. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., pages 4877\u20134886, 2020.\\n\\n[37] Yao Yao, Zixin Luo, Shiwei Li, Tian Fang, and Long Quan. Mvsnet depth inference for unstructured multi-view stereo. In Proc. Eur. Conf. Comput. Vis., pages 767\u2013783, 2018.\\n\\n[38] Yao Yao, Zixin Luo, Shiwei Li, Tianwei Shen, Tian Fang, and Long Quan. Recurrent mvsnet for high-resolution multi-view stereo depth inference. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., pages 5525\u20135534, 2019.\\n\\n[39] Xinyi Ye, Weiyue Zhao, Tianqi Liu, Zihao Huang, Zhiguo Cao, and Xin Li. Constraining depth map geometry for multi-view stereo: A dual-depth approach with saddle-shaped depth cells. In Proc. IEEE Int. Conf. Comput. Vis., pages 17661\u201317670, 2023.\\n\\n[40] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa. pixelNeRF: Neural radiance fields from one or few images. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., 2021.\\n\\n[41] Jingyang Zhang, Yao Yao, Shiwei Li, Zixin Luo, and Tian Fang. Visibility-aware multi-view stereo network. Proc. Br. Mach. Vis. Conf., 2020.\\n\\n[42] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as a perceptual metric. In Proc. IEEE Conf. Comput. Vis. Pattern Recogn., pages 586\u2013595, 2018.\"}"}
