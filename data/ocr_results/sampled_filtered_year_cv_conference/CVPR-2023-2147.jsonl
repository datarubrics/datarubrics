{"id": "CVPR-2023-2147", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Alexandre Alahi, Kratarth Goel, Vignesh Ramanathan, Alexandre Robicquet, Li Fei-Fei, and Silvio Savarese. Social lstm: Human trajectory prediction in crowded spaces. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 961\u2013971, 2016. 1, 2\\n\\n[2] Jur van den Berg, Stephen J Guy, Ming Lin, and Dinesh Manocha. Reciprocal n-body collision avoidance. In Robotics Research: The 14th International Symposium (ISRR), pages 3\u201319. Springer, 2011. 1, 2, 6\\n\\n[3] Holger Caesar, Varun Bankiti, Alex H Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: A multi-modal dataset for autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 11621\u201311631, 2020. 2, 4, 6\\n\\n[4] Zhe Cao, Hang Gao, Karttikeya Mangalam, Qi-Zhi Cai, Minh Vo, and Jitendra Malik. Long-term human motion prediction with scene context. In European Conference on Computer Vision, pages 387\u2013404. Springer, 2020. 3\\n\\n[5] Yuning Chai, Benjamin Sapp, Mayank Bansal, and Dragomir Anguelov. Multipath: Multiple probabilistic anchor trajectory hypotheses for behavior prediction. In Conference on Robot Learning (CoRL), pages 86\u201399. PMLR, 2020. 2\\n\\n[6] Patrick Dendorfer, Aljosa Osep, and Laura Leal-Taix\u00e9. Goal-gan: Multimodal trajectory prediction based on goal position estimation. In Proceedings of the Asian Conference on Computer Vision, 2020. 2\\n\\n[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34:8780\u20138794, 2021. 2, 3\\n\\n[8] Zipeng Fu, Xuxin Cheng, and Deepak Pathak. Deep whole-body control: Learning a unified policy for manipulation and locomotion. ArXiv, abs/2210.10044, 2022. 5\\n\\n[9] Tianpei Gu, Guangyi Chen, Junlong Li, Chunze Lin, Yongming Rao, Jie Zhou, and Jiwen Lu. Stochastic trajectory prediction via motion indeterminacy diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17113\u201317122, 2022. 2, 3\\n\\n[10] Agrim Gupta, Justin Johnson, Li Fei-Fei, Silvio Savarese, and Alexandre Alahi. Social gan: Socially acceptable trajectories with generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 2255\u20132264, 2018. 1, 2\\n\\n[11] Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael J Black. Stochastic scene-aware motion prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 11374\u201311384, 2021. 1, 3\\n\\n[12] M. Brandon Haworth, Glen Berseth, Seonghyeon Moon, Petros Faloutsos, and Mubbasir Kapadia. Deep integration of physical humanoid control and crowd navigation. Proceedings of the 13th ACM SIGGRAPH Conference on Motion, Interaction and Games, 2020. 2, 3\\n\\n[13] Dirk Helbing, Ill\u00e9s Farkas, and Tamas Vicsek. Simulating dynamical features of escape panic. Nature, 407(6803):487\u2013490, 2000. 2\\n\\n[14] Dirk Helbing and Peter Molnar. Social force model for pedestrian dynamics. Physical review E, 51(5):4282, 1995. 1, 2\\n\\n[15] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik P Kingma, Ben Poole, Mohammad Norouzi, David J Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 3\\n\\n[16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:6840\u20136851, 2020. 3, 4\\n\\n[17] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 2, 3, 4\\n\\n[18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David J Fleet. Video diffusion models. arXiv preprint arXiv:2204.03458, 2022. 2, 3, 4, 5\\n\\n[19] Daniel Holden, Taku Komura, and Jun Saito. Phase-functioned neural networks for character control. ACM Transactions on Graphics (TOG), 36(4):1\u201313, 2017. 2, 3\\n\\n[20] Michael Janner, Yilun Du, Joshua B Tenenbaum, and Sergey Levine. Planning with diffusion for flexible behavior synthesis. International Conference on Machine Learning (ICML), 2022. 2, 3, 4, 5, 6, 7\\n\\n[21] Ioannis Karamouzas, Nick Sohre, Ran Hu, and Stephen J Guy. Crowd space: a predictive crowd analysis technique. ACM Transactions on Graphics (TOG), 37(6):1\u201314, 2018. 2\\n\\n[22] Jongmin Kim, Yeongho Seol, Taesoo Kwon, and Jehee Lee. Interactive manipulation of large-scale crowd animation. ACM Transactions on Graphics (TOG), 33(4):1\u201310, 2014. 1, 2\\n\\n[23] Jaedong Lee, Jungdam Won, and Jehee Lee. Crowd simulation by deep reinforcement learning. In Proceedings of the 11th Annual International Conference on Motion, Interaction, and Games, pages 1\u20137, 2018. 2\\n\\n[24] Namhoon Lee, Wongun Choi, Paul Vernaza, Christopher B Choy, Philip HS Torr, and Manmohan Chandraker. Desire: Distant future prediction in dynamic scenes with interacting agents. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 336\u2013345, 2017. 2\\n\\n[25] Marilena Lemonari, Rafael Blanco, Panayiotis Charalambous, Nuria Pelechano, Marios Avraamides, Julien Pettr\u00e9, and Yiorgos Chrysanthou. Authoring virtual crowds: A survey. In Computer Graphics Forum, volume 41, pages 677\u2013701. Wiley Online Library, 2022. 2\\n\\n[26] Alon Lerner, Yiorgos Chrysanthou, and Dani Lischinski. Crowds by example. In Computer graphics forum, volume 26, pages 655\u2013664. Wiley Online Library, 2007. 2, 4, 6\\n\\n[27] Hung Yu Ling, Fabio Zinno, George Cheng, and Michiel Van De Panne. Character controllers using motion vaes. ACM Transactions on Graphics (TOG), 39(4):40\u20131, 2020. 2, 3\\n\\n[28] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. Smpl: a skinned multi-person linear model. ACM Trans. Graph., 34:248:1\u2013248:16, 2015. 5\"}"}
{"id": "CVPR-2023-2147", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pablo Alvarez Lopez, Michael Behrisch, Laura Bieker-Walz, Jakob Erdmann, Yun-Pang Fl\u00f6ttner, Robert Hilbrich, Leonhard L\u00fccken, Johannes Rummel, Peter Wagner, and Eva-marie Wiesner. Microscopic traffic simulation using sumo. In 2018 21st international conference on intelligent transportation systems (ITSC), pages 2575\u20132582. IEEE, 2018.\\n\\nZhengyi Luo, Ryo Hachiuma, Ye Yuan, and Kris Kitani. Dynamics-regulated kinematic policy for egocentric pose estimation. In Advances in Neural Information Processing Systems, 2021.\\n\\nZhengyi Luo, Shun Iwase, Ye Yuan, and Kris Kitani. Embodied scene-aware human pose estimation. In Advances in Neural Information Processing Systems, 2022.\\n\\nNaureen Mahmood, N. Ghorbani, N. Troje, Gerard Pons-Moll, and Michael J. Black. Amass: Archive of motion capture as surface shapes. 2019 IEEE/CVF International Conference on Computer Vision (ICCV), pages 5441\u20135450, 2019.\\n\\nViktor Makoviychuk, Lukasz Wawrzyniak, Yunrong Guo, Michelle Lu, Kier Storey, Miles Macklin, David Hoeller, Nikita Rudin, Arthur Allshire, Ankur Handa, and Gavriel State. Isaac gym: High performance gpu-based physics simulation for robot learning, 2021.\\n\\nKarttikeya Mangalam, Yang An, Harshayu Girase, and Jitendra Malik. From goals, waypoints & paths to long term human trajectory forecasting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 15233\u201315242, 2021.\\n\\nChenlin Meng, Ruiqi Gao, Diederik P Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. On distillation of guided diffusion models. arXiv preprint arXiv:2210.03142, 2022.\\n\\nAlexander Quinn Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models. In International Conference on Machine Learning, pages 8162\u20138171. PMLR, 2021.\\n\\nAndreas Panayiotou, Theodoros Kyriakou, Marilena Lemonari, Yiorgos Chrysanthou, and Panayiotis Charalambous. Ccp: Configurable crowd profiles. In ACM SIGGRAPH 2022 Conference Proceedings, pages 1\u201310, 2022.\\n\\nStefano Pellegrini, Andreas Ess, Konrad Schindler, and Luc Van Gool. You\u2019ll never walk alone: Modeling social behavior for multi-target tracking. In 2009 IEEE 12th international conference on computer vision, pages 261\u2013268. IEEE, 2009.\\n\\nXue Bin Peng, Glen Berseth, KangKang Yin, and Michiel van de Panne. Deeploco: Dynamic locomotion skills using hierarchical deep reinforcement learning. ACM Transactions on Graphics (Proc. SIGGRAPH 2017), 36(4), 2017.\\n\\nXue Bin Peng, Yunrong Guo, Lina Halper, Sergey Levine, and Sanja Fidler. Ase: Large-scale reusable adversarial skill embeddings for physically simulated characters. ACM Trans. Graph., 41(4), July 2022.\\n\\nXue Bin Peng, Ze Ma, Pieter Abbeel, Sergey Levine, and Angjoo Kanazawa. Amp: Adversarial motion priors for stylized physics-based character control. ACM Trans. Graph., 40(4), July 2021.\\n\\nMaria Priisalu, Ciprian Paduraru, Aleksis Pirinen, and Cristian Sminchisescu. Semantic synthesis of pedestrian locomotion. In Proceedings of the Asian Conference on Computer Vision, 2020.\\n\\nMaria Priisalu, Aleksis Pirinen, Ciprian Paduraru, and Cristian Sminchisescu. Generating scenarios with diverse pedestrian behaviors for autonomous vehicle testing. In Conference on Robot Learning, pages 1247\u20131258. PMLR, 2022.\\n\\nDavis Rempe, Tolga Birdal, Aaron Hertzmann, Jimei Yang, Srinath Sridhar, and Leonidas J. Guibas. Humor: 3d human motion model for robust pose estimation. In International Conference on Computer Vision (ICCV), 2021.\\n\\nDavis Rempe, Jonah Philion, Leonidas J. Guibas, Sanja Fidler, and Or Litany. Generating useful accident-prone driving scenarios via a learned traffic prior. In Conference on Computer Vision and Pattern Recognition (CVPR), 2022.\\n\\nZhiguo Ren, Panayiotis Charalambous, Julien Bruneau, Qunsheng Peng, and Julien Pettr\u00e9. Group modeling: A unified velocity-based approach. In Computer Graphics Forum, volume 36, pages 45\u201356. Wiley Online Library, 2017.\\n\\nAndrey Rudenko, Luigi Palmieri, Michael Herman, Kris M Kitani, Dariu M Gavrila, and Kai O Arras. Human motion trajectory prediction: A survey. The International Journal of Robotics Research, 39(8):895\u2013935, 2020.\\n\\nNikita Rudin, David Hoeller, Philipp Reist, and Marco Hutter. Learning to walk in minutes using massively parallel deep reinforcement learning, 2021.\\n\\nTim Salzmann, Boris Ivanovic, Punarjay Chakravarty, and Marco Pavone. Trajectron++: Dynamically-feasible trajectory forecasting with heterogeneous data. In European Conference on Computer Vision, pages 683\u2013700. Springer, 2020.\\n\\nJohn Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. ArXiv, abs/1707.06347, 2017.\\n\\nJascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 2256\u20132265. PMLR, 2015.\\n\\nYang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in Neural Information Processing Systems, 32, 2019.\\n\\nMartin Treiber, Ansgar Hennecke, and Dirk Helbing. Congested traffic states in empirical observations and microscopic simulations. Physical review E, 62(2):1805, 2000.\\n\\nArash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space. Advances in Neural Information Processing Systems, 34:11287\u201311302, 2021.\\n\\nJur Van den Berg, Ming Lin, and Dinesh Manocha. Reciprocal velocity obstacles for real-time multi-agent navigation. In 2008 IEEE international conference on robotics and automation, pages 1928\u20131935. Ieee, 2008.\"}"}
{"id": "CVPR-2023-2147", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Pascal Vincent. A connection between score matching and denoising autoencoders. *Neural computation*, 23(7):1661\u20131674, 2011.\\n\\nJungdam Won, Deepak Gopinath, and Jessica Hodgins. Physics-based character controllers using conditional vaes. *ACM Transactions on Graphics (TOG)*, 41(4):1\u201312, 2022.\\n\\nDanfei Xu, Yuxiao Chen, Boris Ivanovic, and Marco Pavone. Bits: Bi-level imitation for traffic simulation. *arXiv preprint arXiv:2208.12403*, 2022.\\n\\nWenhao Yu, Greg Turk, and C. Karen Liu. Learning symmetric and low-energy locomotion. *ACM Transactions on Graphics (TOG)*, 37:1 \u2013 12, 2018.\\n\\nYe Yuan, Shih-En Wei, Tomas Simon, Kris Kitani, and Jason Saragih. Simpoe: Simulated character control for 3d human pose estimation. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)*, 2021.\\n\\nYe Yuan, Xinshuo Weng, Yanglan Ou, and Kris M Kitani. Agentformer: Agent-aware transformers for socio-temporal multi-agent forecasting. In *Proceedings of the IEEE/CVF International Conference on Computer Vision*, pages 9813\u20139823, 2021.\\n\\nXiaohui Zeng, Arash Vahdat, Francis Williams, Zan Gojcic, Or Litany, Sanja Fidler, and Karsten Kreis. Lion: Latent point diffusion models for 3d shape generation. In *Advances in Neural Information Processing Systems (NeurIPS)*, 2022.\\n\\nYan Zhang and Siyu Tang. The wanderings of odysseus in 3d scenes. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 20481\u201320491, 2022.\\n\\nZiyuan Zhong, Davis Rempe, Danfei Xu, Yuxiao Chen, Sushant Veer, Tong Che, Baishakhi Ray, and Marco Pavone. Guided conditional diffusion for controllable traffic simulation. *International Conference on Robotics and Automation (ICRA)*, 2023.\"}"}
{"id": "CVPR-2023-2147", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"becomes\\n\\n\\\\[ \\\\mathbf{N}(\\\\tau_k^{-1}; \\\\tilde{\\\\mu}_{\\\\phi}(\\\\tau_k, k, C_k), \\\\Sigma_k) \\\\]\\n\\nwhere \\\\( \\\\tilde{\\\\mu} \\\\) is the perturbed (guided) mean. Prior work \\\\([20, 64] \\\\) directly perturbs the noisy network-predicted mean with\\n\\n\\\\[ \\\\tilde{\\\\mu} = \\\\mu - \\\\alpha \\\\Sigma \\\\nabla \\\\mu J(\\\\mu) \\\\]\\n\\n(5)\\n\\nwhere \\\\( \\\\alpha \\\\) determines the guidance strength. Note that Eq. (5) evaluates \\\\( J \\\\) at the noisy mean, so learned loss functions must be trained at varying noise levels and analytic loss functions may suffer from numerical issues.\\n\\nTo avoid this, we build upon \\\"reconstruction guidance\\\", which operates on the clean model prediction \\\\( \\\\hat{\\\\tau}_0 \\\\) \\\\([18] \\\\). We extend the guidance formulation introduced in \\\\([18] \\\\) for temporal video upsampling to work with arbitrary loss functions. At each denoising step with input \\\\( \\\\tau_k \\\\), we first perturb the clean trajectory predicted from the network \\\\( \\\\hat{\\\\tau}_0 \\\\) with\\n\\n\\\\[ \\\\tilde{\\\\tau}_0 = \\\\hat{\\\\tau}_0 - \\\\alpha \\\\Sigma \\\\nabla \\\\tau_k J(\\\\hat{\\\\tau}_0) \\\\]\\n\\n(6)\\n\\nthen compute \\\\( \\\\tilde{\\\\mu} \\\\) in the same way as we would in Eq. (2), i.e., as if \\\\( \\\\tilde{\\\\tau}_0 \\\\) were the output of the network. Note that the gradient is evaluated wrt the noisy input trajectory \\\\( \\\\tau_k \\\\) rather than the clean \\\\( \\\\hat{\\\\tau}_0 \\\\), requiring backpropagation through the denoising model. We formulate several analytical guidance objectives like waypoint reaching, obstacle avoidance, collision avoidance, and social groups (see Sec. 4.1, 4.2). A learned RL value function can also be used (Sec. 4.3).\\n\\n3.2. Physics-Based Pedestrian Animation\\n\\nTo enable full-body pedestrian simulation, we design the Pedestrian Animation ControllER (PACER) to execute the 2D trajectories generated by TRACE in a physics simulator.\\n\\nBackground: Goal-Conditioned RL\\n\\nOur framework (Fig. 3) follows the general goal-conditioned reinforcement learning framework, where a goal-conditioned policy \\\\( \\\\pi_{\\\\text{PACER}} \\\\) is trained to follow 2D target trajectories specified by \\\\( \\\\tau_s \\\\). The task is formulated as a Markov Decision Process (MDP) defined by a tuple \\\\( M = \\\\langle S, A, T, R, \\\\gamma \\\\rangle \\\\) of states, actions, transition dynamics, reward function, and discount factor. The state \\\\( S \\\\), transition dynamics \\\\( T \\\\), and reward \\\\( R \\\\) are calculated by the environment based on the current simulation and goal, while the action \\\\( A \\\\) is computed by the policy \\\\( \\\\pi_{\\\\text{PACER}} \\\\). The policy's objective is to maximize the discounted return\\n\\n\\\\[ E_h P_T t=1 \\\\gamma t-1 r_t \\\\]\\n\\nwhere \\\\( r_t \\\\) is the reward per timestep. We utilize Proximal Policy Optimization (PPO) \\\\([50] \\\\) to find the optimal control policy \\\\( \\\\pi_{\\\\text{PACER}} \\\\).\\n\\nTerrain, Social, and Body Awareness\\n\\nTo create a controller that can simulate crowds in realistic 3D scenes (e.g., scans, neural reconstructions, or artist-created meshes (Fig. 1)), our humanoid must be terrain aware, socially aware of other agents, and support diverse body types. We use a humanoid model that conforms to the kinematic structure of SMPL \\\\([28] \\\\), and is automatically generated using a procedure similar to \\\\([30, 31, 60] \\\\). Our control policy \\\\( \\\\pi_{\\\\text{PACER}}(a_t|h_t, o_t, \\\\beta, \\\\tau_s) \\\\) is conditioned on the state of the simulated character \\\\( h_t \\\\), environmental features \\\\( o_t \\\\), body type \\\\( \\\\beta \\\\), and goal trajectory \\\\( \\\\tau_s \\\\). The environment input is a rasterized local height and velocity map of size \\\\( o_t \\\\in \\\\mathbb{R}^{64 \\\\times 64 \\\\times 3} \\\\), which gives agents crucial information about their surroundings. To allow for social awareness, nearby humanoids are represented as a cuboid and rendered on the global height map. In this way, each humanoid views other people as dynamic obstacles to avoid. Obstacle and interpersonal avoidance are learned by using obstacle collision as a termination condition. By conditioning and training with different body parameters \\\\( \\\\beta \\\\) our policy learns to adapt to characters with diverse morphologies.\\n\\nRealistic Motion through Adversarial Learning\\n\\nTo learn the optimal control policy \\\\( \\\\pi_{\\\\text{PACER}} \\\\) that (1) follows a 2D trajectory closely and (2) creates realistic pedestrian motions, we follow Adversarial Motion Prior (AMP) \\\\([41] \\\\). AMP uses a motion discriminator to encourage the policy to generate motions that are similar to the movement patterns contained in a dataset of motion clips recorded by human actors. The discriminator \\\\( D(h_t-10:t, a_t) \\\\) is then used to specify a motion style reward \\\\( r_{\\\\text{amp}}t \\\\) for training the policy. The style reward is combined with a trajectory following reward \\\\( r_{\\\\tau}t \\\\) and an energy penalty \\\\( r_{\\\\text{energy}}t \\\\) \\\\([8] \\\\) to produce the total reward \\\\( r_t = r_{\\\\text{amp}}t + r_{\\\\tau}t + r_{\\\\text{energy}}t \\\\). To mitigate artifacts arising from asymmetric gaits, such as limping, we utilize the motion-symmetry loss proposed by \\\\([59] \\\\):\\n\\n\\\\[ L_{\\\\text{sym}}(\\\\theta) = \\\\| \\\\pi_{\\\\text{PACER}}(h_t, o_t, \\\\beta, \\\\tau_s) - \\\\Phi_a(\\\\pi_{\\\\text{PACER}}(\\\\Phi_s(h_t, o_t, \\\\beta, \\\\tau_s))) \\\\|_2 \\\\]\\n\\n(7)\\n\\nwhere \\\\( \\\\Phi_s \\\\) and \\\\( \\\\Phi_a \\\\) mirror the state and action along the character's sagittal plane. This loss encourages the policy to produce more symmetric motions, leading to natural gaits.\\n\\nDuring training, random terrains are generated following the procedure used in \\\\([48] \\\\). We create stairs, slopes, uneven terrains, and obstacles consisting of random polygons. Character morphology is also randomized by sampling a gender and body type from the AMASS dataset \\\\([32] \\\\). The policy and discriminator are then conditioned on the SMPL gender and body shape \\\\( \\\\beta \\\\) parameters. More details are available in the supplementary material.\"}"}
{"id": "CVPR-2023-2147", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Guidance results on ORCA-Maps. For \\\\( VAE \\\\) and \\\\( TRACE \\\\), 20 samples are visualized for each pedestrian (the boxes) along with the final trajectory chosen via filtering which is bolded.\\n\\n3.3. Controllable Pedestrian Animation System\\n\\nThe high-level trajectory planning from \\\\( TRACE \\\\) is combined with the low-level character control from \\\\( PACER \\\\) to create an end-to-end pedestrian animation system. The two components are trained independently, but at runtime they operate in a closed feedback loop: \\\\( PACER \\\\) follows planned trajectories for 2 s before \\\\( TRACE \\\\) re-planning, taking past character motion from \\\\( PACER \\\\) as input. By combining terrain and social awareness of \\\\( PACER \\\\) with collision avoidance guidance, both high and low-level systems are task-aware and work in tandem to prevent collisions and falls.\\n\\nValue Function as Guidance. To enable tighter two-way coupling between \\\\( TRACE \\\\) and \\\\( PACER \\\\), in Sec. 4.3 we explore using the value function learned during RL training of \\\\( PACER \\\\) to guide trajectory diffusion. The value function predicts expected future rewards and is aware of body pose and surrounding terrain and agents. Using the value function to guide denoising encourages \\\\( TRACE \\\\) to produce trajectories that are easier to follow and better suited to the current terrain (which \\\\( TRACE \\\\) is unaware of otherwise). Unlike Diffuser \\\\( [20] \\\\), which requires training a reward function with samples from the diffusion model at varying noise levels, our guidance (Eq. (6)) operates on clean trajectories so we can use the value function directly from RL training.\\n\\n4. Experiments\\n\\nWe first demonstrate the controllability of \\\\( TRACE \\\\) when trained on synthetic (Sec. 4.1) and real-world (Sec. 4.2) pedestrian data. Sec. 4.3 evaluates our full animation system on several tasks and terrains.\\n\\nVideo results are provided in the supplementary material.\\n\\nImplementation Details. \\\\( TRACE \\\\) is trained to predict 5 s of future motion from 3 s of past motion (both at 10 Hz), and uses \\\\( K = 100 \\\\) diffusion steps. During training, map and neighbor conditioning inputs are independently dropped with 10% probability. At test time, we sample (and guide) multiple future trajectories for each pedestrian in a scene and choose one with the lowest guidance loss, which we refer to as filtering. \\\\( PACER \\\\) operates at 30 Hz; we randomly sample terrain, body type, and procedural 2D trajectories during training and use a dataset of locomotion sequences from AMASS \\\\( [32] \\\\). All physics simulations are performed using NVIDIA's Isaac Gym simulator \\\\( [33] \\\\).\\n\\nDatasets. The ORCA dataset (Sec. 4.1) contains synthetic trajectory data from 10 s scenes generated using the ORCA crowd simulator \\\\( [2] \\\\). Up to 20 agents are placed in a 15 m \u00d7 15 m environment with \\\\( \\\\leq 20 \\\\) static primitive obstacles. Agent placement and goal velocity, along with obstacle placement and scale, are randomized per scene. The dataset contains two distinct subsets: \\\\( ORCA-Maps \\\\) has many obstacles but few agents, while \\\\( ORCA-Interact \\\\) has no obstacles (i.e. no map annotations) but many agents.\\n\\nFor real-world data (Sec. 4.2), we use ETH/UCY and nuScenes. ETH/UCY \\\\( [26, 38] \\\\) is a common trajectory forecasting benchmark that contains scenes with dense crowds and interesting pedestrian dynamics but does not have semantic maps. nuScenes \\\\( [3] \\\\) contains 20 s driving scenes in common street settings. We convert the pedestrian bounding-box annotations to 2D trajectories and use them for training and evaluation. Detailed semantic maps are also annotated with layers for roads, crosswalks, and sidewalks.\\n\\nMetrics. We care about trajectory plausibility and meeting user controls. Controllability is evaluated with a Guidance Error that depends on the task: e.g., for avoidance objectives this is collision rate, while the waypoint error measures the minimum distance from the trajectory. Obstacle and Agent Collision Rates measure the frequency of collisions.\\n\\nRealism is measured at the dataset or trajectory level by (1) computing the Earth Mover's Distance (EMD) between the generated and ground truth test-set histograms of trajectory statistics (e.g., velocity, longitudinal/lateral acceleration) \\\\( [58] \\\\), or (2) measuring the mean accelerations of each trajectory, assuming pedestrians generally move smoothly.\\n\\n4.1. Augmenting Crowd Simulation\\n\\nWe first evaluate \\\\( TRACE \\\\) trained on \\\\( ORCA-Maps \\\\) and \\\\( ORCA-Interact \\\\). These provide a clean test bed for comparisons since there is a clear definition of correct pedestrian behavior \u2013 no obstacle or agent collisions are present in the data. All methods operate in an open loop by predicting a single 5 s future for each pedestrian. This way, complicating errors inherent to closed-loop operation are not a factor.\\n\\nResults for single and multi-objective guidance on the \\\\( ORCA-Maps \\\\) test set are shown in Tab. 1. \\\\( TRACE \\\\) is compared to a \\\\( VAE \\\\) baseline \\\\( [45] \\\\) adapted to our setup, which achieves controllability through test-time latent optimization. This is a strong baseline that generally works well, but requires expensive optimization at test time. We also...\"}"}
{"id": "CVPR-2023-2147", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Guidance evaluation on ORCA-Maps dataset. TRACE using full diffusion guidance improves upon V AE latent optimization and selective sampling (TRACE-Filter) in terms of meeting objectives, while maintaining strong realism.\\n\\n| Guide Method | Waypoint V AE | Agent V AE | Waypoint V AE | Agent V AE |\\n|--------------|---------------|------------|---------------|------------|\\n|               | Mixed \u2013       | -0.340     | Mixed \u2013       | -0.010     |\\n| TRACE nuScenes| -0.5          | 0.421      | Mixed \u2013       | -0.5       |\\n| Mixed         | 0.0           | 0.551      | Mixed \u2013       | 0.0        |\\n| Mixed -0.5    | 0.0           | 0.366      | Mixed \u2013       | 0.0        |\\n\\nTable 2. Guidance evaluation on nuScenes. Training on mixed data and using $w<0$ for classifier-free sampling are important to achieve controllability for out-of-distribution objectives.\\n\\ncompare to two ablations: TRACE-Filter samples from the diffusion model without guidance and chooses the best sample according to the guidance loss (similar to [58]), while TRACE-Noisy uses the guidance formulated in Eq. (5) from prior works [20, 64]. Models are trained on the combined dataset of ORCA-Maps (with map annotations) and ORCA-Interact (no map annotations). The guidance losses are:\\n\\n- None: samples randomly with no guidance;\\n- Obstacle avoid: discourages collisions between map obstacles and pedestrian bounding boxes;\\n- Agent avoid: discourages collisions between pedestrians by denoising all their futures in a scene jointly;\\n- Waypoint: encourages a trajectory to pass through a goal location at any point in the planning horizon. For this experiment, the waypoint is set as the position of each pedestrian at 4 s into the future in the ground truth data. These are in-distribution objectives, since they reinforce behavior already observed in the ground truth data.\\n\\nIn Tab. 1, TRACE successfully achieves all objectives through the proposed guidance. It is competitive or better than the V AE optimization in terms of guidance, while maintaining velocity and acceleration distributions closer to the ground truth as indicated by Realism. Fig. 4 shows that random samples from the V AE contain collisions, and using latent optimization for controllability gives similar local minima across samples thereby limiting diversity compared to TRACE. Finally, using our proposed clean guidance (Eq. (6)) instead of the noisy version produces consistently better results in guidance and realism.\\n\\n4.2. Real-world Data Evaluation\\n\\nWe next evaluate controllability when trained on real-world data, and focus on out-of-distribution (OOD) guidance objectives to emphasize the flexibility of our approach. In this experiment, methods operate in a closed loop: pedestrians are rolled out for 10 s and re-plan at 1Hz. Results on a held out nuScenes split are shown in Tab. 2. We compare TRACE trained on mixed data (ETH/UCY+nuScenes), after training on nuScenes only, and using two different classifier-free sampling weights $w$. Along with in-distribution Waypoint (now at 9 s into the future), two additional objectives are evaluated:\\n\\n- Waypoint perturbed: uses a noisily perturbed ground truth future position (at 9 s), requiring pedestrians to go off sidewalks or into streets to reach the goal;\\n- Social groups: specifies groups of agents to stay close and travel together. Groups are set heuristically based on spatial proximity and velocity at initialization.\\n\\nIn Tab. 2, we observe that OOD flexibility requires (1) training on mixed data, and (2) classifier-free sampling. Since nuScenes data is less diverse (people tend to follow the sidewalk), TRACE trained on just nuScenes struggles...\"}"}
{"id": "CVPR-2023-2147", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. Our animation system enables avoiding obstacles, meeting goals, traversing variable terrains, and large crowds.\\n\\n|                | Random Procedural | None | Waypoint | Obstacles Procedural | None | Obs Avoid | Flat Procedural | (Crowd) |\\n|----------------|-------------------|------|----------|----------------------|------|------------|-----------------|---------|\\n| Fail Traj      | 0.541             | 0.093| 0.107    | 1.065                | 0.125| 0.063      | 0.248           | 0.175   |\\n| Follow Error   | 0.107             | 0.100| 0.111    | 0.220                | 0.138| 0.089      | 0.063           | 0.053   |\\n| Reward         | 2.113             | 2.512| 2.162    | 2.552                | 2.609| 2.521      | 2.555           | 2.607   |\\n\\nTable 3. Closed-loop animation results. Our system successfully follows waypoints and avoids collisions in a variety of terrains, and additional guidance improves performance.\\n\\nTable 4. Using the value function learned in RL training as guidance improves quality of trajectory following and robustness to varying terrains, obstacles, and other agents.\"}"}
{"id": "CVPR-2023-2147", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Trace and Pace: Controllable Pedestrian Animation via Guided Trajectory Diffusion\\n\\nDavis Rempe,\u2217 1, 2\\nZhengyi Luo,\u2217 1, 3\\nXue Bin Peng 1, 4\\nYe Yuan 1\\nKris Kitani 3\\nKarsten Kreis 1\\nSanja Fidler 1, 5, 6\\nOr Litany 1\\n\\n1 NVIDIA\\n2 Stanford University\\n3 Carnegie Mellon University\\n4 Simon Fraser University\\n5 University of Toronto\\n6 Vector Institute\\n\\nAbstract\\n\\nWe introduce a method for generating realistic pedestrian trajectories and full-body animations that can be controlled to meet user-defined goals. We draw on recent advances in guided diffusion modeling to achieve test-time controllability of trajectories, which is normally only associated with rule-based systems. Our guided diffusion model allows users to constrain trajectories through target waypoints, speed, and specified social groups while accounting for the surrounding environment context. This trajectory diffusion model is integrated with a novel physics-based humanoid controller to form a closed-loop, full-body pedestrian animation system capable of placing large crowds in a simulated environment with varying terrains. We further propose utilizing the value function learned during RL training of the animation controller to guide diffusion to produce trajectories better suited for particular scenarios such as collision avoidance and traversing uneven terrain.\\n\\nFigure 1. (Left) We propose TRACE, a trajectory diffusion model that enables user control through test-time guidance. (Right) Generated trajectories are passed to a novel physics-based humanoid controller (PACER), forming a closed-loop pedestrian animation system.\\n\\n1. Introduction\\n\\nSynthesizing high-level human behavior, in the form of 2D positional trajectories, is at the core of modeling pedestrians for applications like autonomous vehicles, urban planning, and architectural design. An important feature of such synthesis is controllability \u2013 generating trajectories that meet user-defined objectives, edits, or constraints. For example, a user may place specific waypoints for characters to follow, specify social groups for pedestrians to travel in, or define a social distance to maintain. Attaining controllability is straightforward for algorithmic or rule-based models of human behavior, since they have built-in objectives. In the simplest case, human trajectories can be determined by the shortest paths between control points [11], but more sophisticated heuristics have also been developed for pedestrians [2, 14], crowds [22, 46], and traffic [29, 53]. Unfortunately, algorithmically generated trajectories often appear unnatural.\\n\\nLearning-based approaches, on the other hand, can improve naturalness by mimicking real-world data. These methods often focus on short-term trajectory prediction using a single forward pass of a neural network [1, 10, 49, 61]. However, the ability to control these models is limited to sampling from an output trajectory distribution [34, 58] or using an expensive latent space traversal [45]. As a result, learning-based methods can predict implausible motions such as collisions with obstacles or between pedestrians. This motivates another notion of controllability \u2013 maintaining realistic trajectories during agent-agent and agent-environment interactions.\\n\\nIn this work, we are particularly interested in using controllable pedestrian trajectory models for character animation. We envision a simple interface where a user provides high-level objectives, such as waypoints and social groups, and a system converts them to physics-based full-body human motion. Compared to existing kinematic motion models, this approach can produce more natural and controllable pedestrian animations.\\n\\nVideo results are available on the project page.\\n\\n1. Introduction\\n\\nSynthesizing high-level human behavior, in the form of 2D positional trajectories, is at the core of modeling pedestrians for applications like autonomous vehicles, urban planning, and architectural design. An important feature of such synthesis is controllability \u2013 generating trajectories that meet user-defined objectives, edits, or constraints. For example, a user may place specific waypoints for characters to follow, specify social groups for pedestrians to travel in, or define a social distance to maintain. Attaining controllability is straightforward for algorithmic or rule-based models of human behavior, since they have built-in objectives. In the simplest case, human trajectories can be determined by the shortest paths between control points [11], but more sophisticated heuristics have also been developed for pedestrians [2, 14], crowds [22, 46], and traffic [29, 53]. Unfortunately, algorithmically generated trajectories often appear unnatural.\\n\\nLearning-based approaches, on the other hand, can improve naturalness by mimicking real-world data. These methods often focus on short-term trajectory prediction using a single forward pass of a neural network [1, 10, 49, 61]. However, the ability to control these models is limited to sampling from an output trajectory distribution [34, 58] or using an expensive latent space traversal [45]. As a result, learning-based methods can predict implausible motions such as collisions with obstacles or between pedestrians. This motivates another notion of controllability \u2013 maintaining realistic trajectories during agent-agent and agent-environment interactions.\\n\\nIn this work, we are particularly interested in using controllable pedestrian trajectory models for character animation. We envision a simple interface where a user provides high-level objectives, such as waypoints and social groups, and a system converts them to physics-based full-body human motion. Compared to existing kinematic motion models, this approach can produce more natural and controllable pedestrian animations.\\n\\nVideo results are available on the project page.\"}"}
{"id": "CVPR-2023-2147", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"els, physics-based methods have the potential to produce high-quality motion with realistic subtle behaviors during transitions, obstacle avoidance, traversing uneven terrains, etc. Although there exist physics-based animation models, controlling their behavior requires using task-specific planners that need to be re-trained for new tasks, terrains, and character body shapes.\\n\\nWe develop a generative model of trajectories that is data-driven, controllable, and tightly integrated with a physics-based animation system for full-body pedestrian simulation (Fig. 1). Our method enables generating pedestrian trajectories that are realistic and amenable to user-defined objectives at test time. We use this trajectory generator as a planner for a physics-based pedestrian controller, resulting in a closed-loop controllable pedestrian animation system.\\n\\nFor trajectory generation, we introduce a TRAjectory Diffusion Model for Controllable PEdestrians (TRACE). Inspired by recent successes in generating trajectories through denoising, TRACE generates the future trajectory for each pedestrian in a scene and accounts for the surrounding context through a spatial grid of learned map features that is queried locally during denoising. We leverage classifier-free sampling to allow training on mixed annotations (e.g., with and without a semantic map), which improves controllability at test time by trading off sample diversity with compliance to conditioning. User-controlled sampling from TRACE is achieved through test-time guidance, which perturbs the output at each step of denoising towards the desired objective. We extend prior work by introducing several analytical loss functions for pedestrians and re-formulating trajectory guidance to operate on clean trajectory outputs from the model, improving sample quality and adherence to user objectives.\\n\\nFor character animation, we develop a general-purpose Pedestrian Animation Controller (PACER) capable of driving physics-simulated humanoids with diverse body types to follow trajectories from a high-level planner. We focus on (1) motion quality: PACER learns from a small motion database to create natural and realistic locomotion through adversarial motion learning; (2) terrain and social awareness: trained in diverse terrains with other humanoids, PACER learns to move through stairs, slopes, uneven surfaces, and to avoid obstacles and other pedestrians; (3) diverse body shapes: by training on different body types, PACER draws on years of simulation experience to control a wide range of characters; (4) compatibility with high-level planners: PACER accepts 2D waypoints and can be a plug-in model for any 2D trajectory planner.\\n\\nWe demonstrate a controllable pedestrian animation system using TRACE as a high-level planner for PACER, the low-level animator. The planner and controller operate in a closed loop through frequent re-planning according to simulation results. We deepen their connection by guiding TRACE with the value function learned during RL training of PACER to improve animation quality in varying tasks. We evaluate TRACE on synthetic and real-world pedestrian data, demonstrating its flexibility to user-specified and plausibility objectives while synthesizing realistic motion. Furthermore, we show that our animation system is capable and robust with a variety of tasks, terrains, and characters. In summary, we contribute (1) a diffusion model for pedestrian trajectories that is readily controlled at test time through guidance, (2) a general-purpose pedestrian animation controller for diverse body types and terrains, and (3) a pedestrian animation system that integrates the two to drive simulated characters in a controllable way.\\n\\n2. Related Work\\n\\nPedestrian Trajectory Prediction. Modeling high-level pedestrian behavior has been extensively studied in the context of motion prediction (forecasting). Approaches range from physics and planning-based to recent learned methods. We refer the reader to the thorough survey by Rudenko et al. for an overview, and focus this discussion on controllability. Most forecasting work is motivated by planning for autonomous vehicles or social robots rather than controllability or longer-term synthesis. Rule-based models for pedestrians and vehicle traffic can easily incorporate user constraints making them amenable to control. However, the trajectories of these approaches are not always human-like; methods have even been developed to choose the best simulation method and tune parameters to make crowd scenarios more realistic.\\n\\nData-driven methods produce human-like motions, but neural network-based approaches are difficult to explicitly control. Some works decompose forecasting into goal prediction followed by trajectory prediction based on goals. These models offer limited control by selecting goal locations near a target or that minimizes an objective (e.g., collision). Synthesized pedestrian behavior can also be controlled by strategically choosing a starting location. STRIVE showed that a VAE trajectory model can be controlled through test-time optimization in the learned latent space. Reinforcement learning agents can be controlled in crowd simulations by incorporating tasks into reward functions for training. By varying the weights of different rewards, the characters can be controlled to exhibit one of several behaviors at test time. Our method, TRACE, trains to mimic trajectories from data and is agnostic to any task: all controls are defined at test time, allowing flexibility to new controls after training. Instead of lengthy test-time optimization, we use guidance for control.\\n\\nControllable Character Animation. Full-body pedestrian animation typically involves a high-level task (e.g., trajectory following, obstacle avoidance) and low-level body control.\"}"}
{"id": "CVPR-2023-2147", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Some methods solve both with a single network that implicitly uses high-level planning and low-level animation. GAMMA \\\\cite{63} trains a kinematic model to go to waypoints, while PFNN \\\\cite{19} follows gamepad inputs. Physics-based humanoid controllers such as AMP \\\\cite{41} train different models for each task, limiting their general applicability.\\n\\nTwo-stage methods split the task into separate high-level planning and low-level character control, where task information is only used by the planner. Planning can be done with traditional A* \\\\cite{11}, using learned trajectory prediction \\\\cite{4}, searching in a pre-trained latent space \\\\cite{27, 40, 44, 57}, or using hierarchical RL \\\\cite{12, 39, 40, 42, 57}. DeepLoco \\\\cite{39}, Haworth et al. \\\\cite{12}, and ASE \\\\cite{40} utilize hierarchical RL to achieve impressive dynamic control for various tasks. They require lengthy training for both low-level and high-level controllers and often jointly train as a final step. They must also train different planners for different tasks.\\n\\nOur approach follows the two-stage paradigm, with the distinction that both our high-level (TRACE) and low-level (PACER) models consume task information for pedestrian navigation: through test-time guidance and map-conditioned path following, respectively. TRACE and PACER are unaware of each other at training time, yet can be tightly integrated in a closed loop: trace-pace-retrace.\\n\\nDiffusion Models and Guidance. Diffusion models have shown success in generating images \\\\cite{16, 36, 54}, videos \\\\cite{15}, and point clouds \\\\cite{62}. Guidance has been used for test-time control in several ways: classifier \\\\cite{7} and classifier-free \\\\cite{17} guidance reinforce input conditioning, while reconstruction guidance \\\\cite{18} has been used for coherent video generation. Gu et al. \\\\cite{9} adapt the diffusion framework for short-term pedestrian trajectory forecasting conditioned on past trajectories. Diffuser \\\\cite{20} generates trajectories for planning and control in robotics applications with test-time guidance. Closest to ours is the concurrent work of CTG \\\\cite{64}, which builds on Diffuser to develop a controllable vehicle traffic model, focusing on following formalized traffic rules like speed limits. Our method contains several key differences: we encode map conditioning into an expressive feature grid queried in denoising, we use classifier-free sampling to enable multi-dataset training and test-time flexibility, we re-formulate guidance to operate on clean model outputs, and we link with a low-level animation model using value function guidance.\\n\\n3. Method\\n\\nTo model high-level pedestrian behavior, we first introduce the controllable trajectory diffusion model (TRACE). In Sec. 3.2, we detail our low-level physics-based pedestrian controller, PACER, and in Sec. 3.3 how they can be combined into an end-to-end animation system.\\n\\n3.1. Controllable Trajectory Diffusion\\n\\nProblem Setting. Our goal is to learn high-level pedestrian behavior in a way that can be controlled at test time. For pedestrian animation, we focus on two types of control: (1) user specification, e.g., goal waypoints, social distance, and social groups, and (2) physical plausibility, e.g., avoiding collisions with obstacles or between pedestrians.\\n\\nWe formulate synthesizing pedestrian behavior as an agent-centric trajectory forecasting problem. At each time step, the model outputs a future trajectory plan for a target ego agent conditioned on that agent's past, the past trajectories of all neighboring agents, and the semantic map context. Formally, at timestep $t$ we want the future state trajectory $\\\\tau_s = [s_{t+1} \\\\ldots s_{t+T_f}]$ over the next $T_f$ steps where the state $s_t = [x \\\\ y \\\\ \\\\theta \\\\ v]$ includes the 2D position $(x, y)$, heading angle $\\\\theta$, and speed $v$. We assume this state trajectory is actually the result of a sequence of actions $\\\\tau_a = [a_{t+1} \\\\ldots a_{t+T_f}]$ where each action $a_t = [\\\\dot{v} \\\\ \\\\dot{\\\\theta}]$ contains the acceleration $\\\\dot{v}$ and yaw rate $\\\\dot{\\\\theta}$. The state trajectory can be recovered from the initial state and action trajectory as $\\\\tau_s = f(s_t, \\\\tau_a)$ using a given dynamics model $f$. The full state-action trajectory is then denoted as $\\\\tau = [\\\\tau_s; \\\\tau_a]$. To predict the future trajectory, the model receives as input the past state trajectory of the ego pedestrian $x_{ego} = [s_{t-T_p} \\\\ldots s_t]$ along with the past trajectories of $N$ neighboring pedestrians $X_{neigh} = \\\\{x_i\\\\}_{i=1}^N$. It also gets a crop of the rasterized semantic map $M \\\\in \\\\mathbb{R}^{H \\\\times W \\\\times C}$ in the local frame of the ego pedestrian at time $t$. These inputs are summarized as the conditioning context $C = \\\\{x_{ego}, X_{neigh}, M\\\\}$.\\n\\nOur key idea is to train a diffusion model to conditionally generate trajectories, which can be guided at test time to enable controllability. For simplicity, the following formulation uses the full trajectory notation $\\\\tau$, but in practice, the state trajectory is always a result of actions, i.e., diffusion/denoising are on $\\\\tau_a$ which determines the states through $f$. Next, we summarize our diffusion framework, leaving the details to the supplementary material.\\n\\n3.1.1 Trajectory Diffusion Model\\n\\nWe build on Diffuser \\\\cite{20} and generate trajectories through iterative denoising, which is learned as the reverse of a predefined diffusion process \\\\cite{16, 51}. Starting from a clean future trajectory $\\\\tau_0 \\\\sim q(\\\\tau_0)$ sampled from the data distribution, the forward noising process produces a sequence of progressively noisier trajectories $(\\\\tau_1, \\\\ldots, \\\\tau_k, \\\\ldots, \\\\tau_K)$ by adding Gaussian noise at each process step $k$:\\n\\n$$q(\\\\tau_1:K|\\\\tau_0) := \\\\prod_{k=1}^K q(\\\\tau_k|\\\\tau_{k-1})$$\\n\\n$$q(\\\\tau_k|\\\\tau_{k-1}) := \\\\mathcal{N}(\\\\tau_k; p_1 - \\\\beta_k \\\\tau_{k-1}, \\\\beta_k I)$$\\n\\nwhere $\\\\beta_k$ is the variance at each step of a fixed schedule, and with a large enough $K$ we get $q(\\\\tau_K) \\\\approx \\\\mathcal{N}(\\\\tau_K; 0, I)$.\"}"}
{"id": "CVPR-2023-2147", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Trajectory diffusion model (TRACE). Future trajectory denoising is conditioned on past and neighbor motion by adding processed features to intermediate U-Net features. Map conditioning is provided through a feature grid queried along the noisy input trajectory. TRACE learns the reverse of this process so that the sampled noise can be denoised into plausible trajectories. Each step of this reverse process is conditioned on $C$:\\n\\n$$\\n\\\\phi(\\\\tau_k - 1 | \\\\tau_k, C) := N(\\\\tau_k - 1; \\\\mu_{\\\\phi}(\\\\tau_k, k, C), \\\\Sigma_k)(2)\\n$$\\n\\nwhere $\\\\phi$ are model parameters and $\\\\Sigma_k$ is from a fixed schedule. TRACE learns to parameterize the mean of the Gaussian distribution at each step of the denoising process.\\n\\nTraining and Classifier-Free Sampling. Importantly for guidance, the network does not directly output $\\\\mu$. Instead, at every step it learns to predict the final clean trajectory $\\\\tau_0$, which is then used to compute $\\\\mu_{[36]}$. Training supervises this network output $\\\\hat{\\\\tau}_0$ with ground truth future trajectories (i.e., denoising score matching [16, 52, 56]):\\n\\n$$\\nL = \\\\mathbb{E}_{\\\\epsilon, k, \\\\tau_0, C} ||\\\\tau_0 - \\\\hat{\\\\tau}_0||^2 (3)\\n$$\\n\\nwhere $\\\\tau_0$ and $C$ are sampled from the training dataset, $k \\\\sim \\\\text{U}\\\\{1, 2, \\\\ldots, K\\\\}$ is the step index, and $\\\\epsilon \\\\sim N(0, I)$ is used to corrupt $\\\\tau_0$ to give the noisy input trajectory $\\\\tau_k$.\\n\\nOur training procedure allows the use of classifier-free sampling at test time, which has been shown to improve compliance to conditioning in diffusion models [17]. We simultaneously train both a conditional model $\\\\mu_{\\\\phi}(\\\\tau_k, k, C)$ and unconditional model $\\\\mu_{\\\\phi}(\\\\tau_k, k)$ by randomly dropping out conditioning during training. At test time, predictions from both models are combined with weight $w$ as:\\n\\n$$\\n\\\\tilde{\\\\epsilon}_{\\\\phi} = \\\\epsilon_{\\\\phi}(\\\\tau_k, k, C) + w \\\\epsilon_{\\\\phi}(\\\\tau_k, k, C) - \\\\epsilon_{\\\\phi}(\\\\tau_k, k)(4)\\n$$\\n\\nwhere $\\\\epsilon_{\\\\phi}$ is the model's prediction of how much noise was added to the clean trajectory to produce the input $\\\\tau_k$; it is straightforward to compute from $\\\\mu_{[36]}$.\\n\\nNote that $w > 0$ and $w < 0$ increase and decrease the effect of conditioning, respectively, while $w = 0$ and $w = -1$ result in the purely conditional or unconditional model, respectively. This flexibility allows a user to trade off respecting conditioning with trajectory diversity, which benefits controllability (see Sec. 4.2). This approach also enables training on multiple distinct datasets with varying annotations: conditioning is already being dropped out randomly, so it is easy to use mixed data with subsets of the full conditioning. Since there are pedestrian datasets with diverse motions but no semantic maps [26, 38], and others with limited motions but detailed maps [3], we find mixed training is beneficial to boost diversity and controllability (see Sec. 4.2).\\n\\nArchitecture. As shown in Fig. 2, TRACE uses a U-Net similar to [20] that has proven effective for trajectories. The input trajectory $\\\\tau_k$ at step $k$ is processed by a sequence of 1D temporal convolutional blocks that progressively down and upsample the sequence in time, leveraging skip connections. A key challenge is how to condition the U-Net on $C$ to predict trajectories that comply with the map and other pedestrians. To incorporate step $k$, ego past $x_{ego}$, and neighbor past $X_{neigh}$, we use a common approach [18, 20] that extracts a single conditioning feature and adds it to the intermediate trajectory features within each convolutional block. For the map $M$, we encode with a 2D convolutional network into a feature grid, where each pixel contains a high-dimensional feature. At step $k$ of denoising, each 2D position $(x, y) \\\\in \\\\tau_k$ is queried by interpolating into the grid to give a feature trajectory, which is concatenated to $\\\\tau_k$ and becomes the U-Net input. Intuitively, this allows learning a localized representation that can benefit subtle map interactions such as obstacle avoidance.\\n\\n3.1.2 Controllability through Clean Guidance After training TRACE to generate realistic trajectories, controllability is implemented through test-time guidance. Intuitively, guidance nudges the sampled trajectory at each step of denoising towards a desired outcome. Let $J(\\\\tau)$ be a guidance loss function measuring how much a trajectory $\\\\tau$ violates a user objective. This may be learned [20] or an analytical differentiable function [64]. Guidance uses the gradient of $J$ to perturb the predicted mean from the model at each denoising step such that the right side of Eq. (2)\"}"}
