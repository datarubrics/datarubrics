{"id": "CVPR-2023-1856", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Generating a high-quality High Dynamic Range (HDR) image from dynamic scenes has recently been extensively studied by exploiting Deep Neural Networks (DNNs). Most DNNs-based methods require a large amount of training data with ground truth, requiring tedious and time-consuming work. Few-shot HDR imaging aims to generate satisfactory images with limited data. However, it is difficult for modern DNNs to avoid overfitting when trained on only a few images. In this work, we propose a novel semi-supervised approach to realize few-shot HDR imaging via two stages of training, called SSHDR. Unlike previous methods, directly recovering content and removing ghosts simultaneously, which is hard to achieve optimum, we first generate content of saturated regions with a self-supervised mechanism and then address ghosts via an iterative semi-supervised learning framework. Concretely, considering that saturated regions can be regarded as masking Low Dynamic Range (LDR) input regions, we design a Saturated Mask AutoEncoder (SMAE) to learn a robust feature representation and reconstruct a non-saturated HDR image. We also propose an adaptive pseudo-label selection strategy to pick high-quality HDR pseudo-labels in the second stage to avoid the effect of mislabeled samples. Experiments demonstrate that SSHDR outperforms state-of-the-art methods quantitatively and qualitatively within and across different datasets, achieving appealing HDR visualization with few labeled samples.\\n\\n1. Introduction\\n\\nStandard digital photography sensors are unable to capture the wide range of illumination present in natural scenes, resulting in Low Dynamic Range (LDR) images that often suffer from over or underexposed regions, which can damage the details of the scene. High Dynamic Range (HDR) imaging has been developed to address these limitations. This technique combines several LDR images with different exposures to generate an HDR image. While HDR imaging can effectively recover details in static scenes, it may produce ghosting artifacts when used with dynamic scenes or hand-held camera scenarios.\\n\\nHistorically, various techniques have been suggested to address such issues, such as alignment-based methods [3,10,27,37], patch-based methods [8,15,24], and rejection-based methods [5, 11, 19, 20, 35, 40]. Two categories of alignment-based approaches exist: rigid alignment (e.g., homographies) that fail to address foreground motions, and non-rigid alignment (e.g., optical flow) that is error-prone. Patch-based techniques merge similar regions using patch-level alignment and produce superior results, but suffer from high complexity. Rejection-based methods aim to eliminate misaligned areas before fusing images, but may result in a loss of information in motion regions.\\n\\nAs Deep Neural Networks (DNNs) become increasingly prevalent, the DNN-based HDR deghosting methods...\"}"}
{"id": "CVPR-2023-1856", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"achieve better visual results compared to traditional methods. However, these alignment approaches are error-prone and inevitably cause ghosting artifacts (see Figure 1 Kalantari's results). AHDR \\\\cite{31, 32} proposes spatial attention to suppress motion and saturation, which effectively alleviate misalignment problems. Based on AHDR, ADNET \\\\cite{14} proposes a dual branch architecture using spatial attention and PCD-alignment \\\\cite{29} to remove ghosting artifacts. All these above methods directly learn the complicated HDR mapping function with abundant HDR ground truth data. However, it's challenging to collect a large amount of HDR-labeled data. The reasons can be attributed to that 1) generating a ghost-free HDR ground truth sample requires an absolute static background, and 2) it is time-consuming and requires considerable manpower to do manual post-examination. This generates a new setting that only uses a few labeled data for HDR imaging.\\n\\nRecently, FSHDR \\\\cite{22} attempts to generate a ghost-free HDR image with only few labeled data. They use a preliminary model trained with a large amount of unlabeled dynamic samples, and a few dynamic and static labeled samples to generate HDR pseudo-labels and synthesize artificial dynamic LDR inputs to improve the model performance of dynamic scenes further. This approach expects the model to handle both the saturation and the ghosting problems simultaneously, but it is hard to achieve under the condition of few labeled data, especially misaligned regions caused by saturation and motion (see Figure 1 FSHDR). In addition, FSHDR uses optical flow to forcibly synthesize dynamic LDR inputs from poorly generated HDR pseudo-labels, the errors in optical flow further intensify the degraded quality of artificial dynamic LDR images, resulting in an apparent distribution shift between LDR training and testing data, which hampers the performance of the network.\\n\\nThe above analysis makes it very challenging to directly generate a high-quality and ghost-free HDR image with few labeled samples. A reasonable way is to address the saturation problems first and then cope with the ghosting problems with a few labeled samples. In this paper, we propose a semi-supervised approach for HDR deghosting, named SSHDR, which consists of two stages: self-supervised learning network for content completion and sample-quality-based iterative semi-supervised learning for deghosting. In the first stage, we pretrain a Saturated Mask AutoEncoder (SMAE), which learns the representation of HDR features to generate content of saturated regions by self-supervised learning. Specifically, considering that the saturated regions can be regarded as masking the short LDR input patches, inspired by \\\\cite{6}, we randomly mask a high proportion of the short LDR input and expect the model to reconstruct a no-saturated HDR image from the remaining LDR patches in the first stage. This self-supervised approach allows the model to recover the saturated regions with the capability to effectively learn a robust representation for the HDR domain and map an LDR image to an HDR image. In the second stage, to prevent the overfitting problem with a few labeled training samples and make full use of the unlabeled samples, we iteratively train the model with a few labeled samples and a large amount of HDR pseudo-labels from unlabeled data. Based on the pretrained SMAE, a sample-quality-based iterative semi-supervised learning framework is proposed to address the ghosting artifacts. Considering the quality of pseudo-labels is uneven, we develop an adaptive pseudo-labels selection strategy to pick high-quality HDR pseudo-labels (i.e., well-exposed, ghost-free) to avoid awful pseudo-labels hampering the optimization process. This selection strategy is guided by a few labeled samples and enhances the diversity of training samples in each epoch. The experiments demonstrate that our proposed approach can generate high-quality HDR images with few labeled samples and achieves state-of-the-art performance on individual and cross-public datasets. Our contributions can be summarized as follows:\\n\\n\u2022 We propose a novel and generalized HDR self-supervised pretraining model, which uses mask strategy to reconstruct an HDR image and addresses saturated problems from one LDR image.\\n\u2022 We propose a sample-quality-based semi-supervised training approach to select well-exposed and ghost-free HDR pseudo-labels, which improves ghost removal.\\n\u2022 We perform both qualitative and quantitative experiments, which show that our method achieves state-of-the-art results on individual and cross-public datasets.\\n\\n2. Related Work\\n2.1. HDR Deghosting Methods\\nThe existing HDR deghosting methods include four categories: alignment-based method, patch-based method, rejection-based method, and CNN-based method.\\n\\nAlignment-based Method. Rigid or non-rigid registration is mainly used in alignment-based approaches. Bongi \\\\cite{3} estimated flow vectors to align with the reference images. Kang et al \\\\cite{10} utilized optical flow to align images in the luminance domain to remove ghosting artifacts. Tomaszewska et al \\\\cite{27} used SIFT feature to perform global alignments. Since the dense correspondence computed by alignment methods are error-prone, they cannot handle large motion and occlusion.\\n\\nRejection-based Method. Rejection-based methods detect and eliminate motion from static regions. Then they merge static inputs to get HDR images. Grosch et al \\\\cite{5} estimated a motion map and used it to generate ghost-free HDR. Zhang et al \\\\cite{40} obtained a motion weighting map using quality measurement on image gradients. Lee et al \\\\cite{11} and Oh et al \\\\cite{19} detected motion regions using rank min\"}"}
{"id": "CVPR-2023-1856", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"However, rejection-based methods remove the misalignment of regions. It will result in a lack of content in moving regions.\\n\\nPatch-based Method. Patch-based methods use patch-level alignment to merge similar contents. Sen et al. [24] proposed a patch-based energy minimization approach that optimizes alignment and reconstruction simultaneously. Hu et al. [8] utilized a patch-match mechanism to produce aligned images. Although these methods have good performance, they suffer from high computational costs.\\n\\nCNN-based Method. Kalantari et al. [9] used a CNN network to fuse LDR images that are aligned with optical flow. Wu et al. [30] used homography to align the camera motion and reconstructed HDR images by CNN. Yan et al. [31] proposed an attention mechanism to suppress motion and saturation. Yan et al. [34] designed a nonlocal block to release the constraint of locality receptive field for global merging HDR. Niu et al. [18] proposed HDR-GAN to recover missing content using generative adversarial networks. Ye et al. [38] proposed multi-step feature fusion to generate ghost-free images. Liu et al. [14] utilized the PCD alignment subnetwork to remove ghosts. However, these methods require a large number of labeled samples, which is difficult to collect.\\n\\n2.2. Few-shot Learning (FSL)\\nHumans can successfully learn new ideas with relatively little supervision. Inspired by such ability, FSL aims to learn robust representations with few labeled samples. There are three main categories for FSL methods: data-based category [1, 23, 25], which augment the experience with prior knowledge; model-based category [2, 17, 26], which shrinks the size of the hypothesis space using prior knowledge; algorithm-based category [4, 12, 39], which modifies the search for the optimal hypothesis using prior knowledge. For HDR deghosting, Prabhakar et al. [22] proposed a data-based category deghosting method, which uses artificial dynamic sequences synthesis for motion transfer. Still, it is hard to handle both the saturation and the ghosting problems simultaneously with few labeled data.\\n\\n3. The Proposed Method\\n3.1. Data Distribution\\nFollowing the setting of few-shot HDR imaging [22], we utilize 1) \\\\( N \\\\) dynamic unlabeled LDR samples \\\\( U = \\\\{ L_{U1}, \\\\ldots, L_{UN} \\\\} \\\\), where each \\\\( L_{Ui} \\\\) consists of three LDRs \\\\( (X_{Ui1}, X_{Ui2}, X_{Ui3}) \\\\) with different exposures. 2) \\\\( M \\\\) static labeled LDR samples \\\\( S = \\\\{ L_{S1}, \\\\ldots, L_{SM} \\\\} \\\\), where each \\\\( L_{Si} \\\\) consists of three LDRs \\\\( (X_{Si1}, X_{Si2}, X_{Si3}) \\\\) with different exposures and ground truth \\\\( Y_S \\\\). 3) \\\\( K \\\\) dynamic labeled LDR samples \\\\( D = \\\\{ L_{D1}, \\\\ldots, L_{DK} \\\\} \\\\), where each \\\\( L_{Di} \\\\) consists of three LDRs \\\\( (X_{Di1}, X_{Di2}, X_{Di3}) \\\\) and ground truth \\\\( Y_D \\\\). Since it is difficult to collect labeled samples, we set \\\\( K \\\\) to be less than or equal to 5, and \\\\( M \\\\) is fixed at 5. While it is easy to capture unlabeled samples, \\\\( N \\\\) can be arbitrary.\\n\\n3.2. Model Overview\\nGenerating a non-saturated and ghost-free HDR image with few labeled samples is challenging. It is a proper way to address saturated problems first and then handle ghosting problems. As shown in Figure 2, we propose a semi-supervised approach for HDR deghosting. Our approach consists of two stages: a self-supervised learning network for content completion and a sample-quality-based iterative semi-supervised learning for deghosting. In the first stage, we propose a multi-scale Transformer model based on self-supervised learning with a saturated-masked autoencoder to make it capable of recovering saturated regions. In a word, we randomly mask LDR patches and reconstruct non-saturated HDR images from the remaining LDR patches.\\n\\nIn the second stage, we propose a sample-quality-based iterative semi-supervised learning approach that learns to address ghosting problems. We finetune the pretrained model based on the first stage with a few labeled samples. Then, we iteratively train the model with labeled samples and unlabeled samples with pseudo-labels. Considering that the HDR pseudo-labels inevitably contain saturated and ghosting regions, which deteriorate the model performance, we propose an adaptive pseudo-labels selection strategy to pick high-quality HDR pseudo-labels to avoid awful pseudo-labels hampering the optimization process.\\n\\n3.3. Self-supervised Learning Stage\\nInput. Considering that there are more saturated regions in the medium \\\\( (X_{U2}) \\\\) and long exposure frames \\\\( (X_{U3}) \\\\) of unlabeled data \\\\( U \\\\), we first transform the short exposure frame \\\\( (X_{U1}) \\\\) into a new medium \\\\( (X_{U2}') \\\\) and long exposure frames \\\\( (X_{U3}') \\\\) by exposure adjustment, \\\\( X_{Ui}' = \\\\text{clip} \\\\left( \\\\frac{(X_{Ui})^\\\\gamma \\\\times t_i}{t_1} \\\\right)^\\\\gamma, i = 2, 3 \\\\). (1) Then following previous work [9, 30], we map the LDR input images \\\\( X_{U1}, X_{U2}', X_{U3}' \\\\) to HDR domain by gamma correction to get \\\\( H_i, H_i' = \\\\left( X_{Ui}' \\\\right)^\\\\gamma/t_i \\\\). (2) Note that \\\\( X_{U1}' = X_{U1} \\\\), \\\\( t_i \\\\) denotes the exposure time of LDR image \\\\( X_i \\\\), \\\\( \\\\gamma \\\\) represents the gamma correction parameter, we set \\\\( \\\\gamma \\\\) to 2.2. Then, we concatenate \\\\( X_i' \\\\) and \\\\( H_i' \\\\) along the channel dimension to get a 6-channel input \\\\( I_i = [X_i', H_i'] \\\\), we subsequently mask the input patches to get \\\\( I_i' \\\\). Concretely, we divide the input into non-overlapping patches and randomly mask a subset of these patches with a high mask ratio (75%) (see Figure 3). Note that the patch size\\n\\n...\"}"}
{"id": "CVPR-2023-1856", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. The overview of our framework.\\n\\nFigure 3. The detailed procedure of Stage 1. To recover the saturated regions, we utilize the short exposure frame as input and ground truth.\\n\\nConsidering that the masking strategy is another way to destruct the saturated regions, we intend the model to learn a robust representation to recover these saturated regions. Finally, $\\\\mathbf{I}'_3 = \\\\{\\\\mathbf{I}'_1, \\\\mathbf{I}'_2, \\\\mathbf{I}'_3\\\\}$ is the input of the model.\\n\\nOur SMAE self-supervised training-based multi-scale Transformer consists of a feature extraction module, hallucination module, and Multi-Scale Residual Swin Transformer fusion Module (MSRSTM). The details in our model are included in the Appendix.\\n\\nHallucination Module. We first adopt three convolutional layers to extract shallow feature $\\\\mathbf{F}_i$. Then, we divide the shallow feature $\\\\mathbf{F}_i$ into non-overlapping patches $\\\\mathbf{F}_i$, and map each patch $\\\\mathbf{F}_i$ into query, keys and values. Subsequently, we calculate the similarity map between $\\\\mathbf{q}$ and $\\\\mathbf{k}$, and perform the Softmax function to get the attention weight. Finally, we apply the attention weight to $\\\\mathbf{v}$ to get $\\\\mathbf{F}_i^s$,\\n\\n$$\\n\\\\mathbf{q}_i = \\\\mathbf{F}_2^i \\\\mathbf{W}_q,\\n\\\\mathbf{k}_i = \\\\mathbf{F}_i^i \\\\mathbf{W}_k,\\n\\\\mathbf{v}_i = \\\\mathbf{F}_i^i \\\\mathbf{W}_v,\\n$$\\n\\n(3)\\n\\nwhere $\\\\mathbf{b}$ represents a learnable position encoding, $d$ denotes the dimension of $\\\\mathbf{q}$.\\n\\nMSRSTM. To merge more information from different exposure regions, inspired by [13], we propose a Multi-Scale Residual Swin Transformer Module (MSRSTM). First, $\\\\mathbf{F}_1^s, \\\\mathbf{F}_2^s, \\\\mathbf{F}_3^s$ is concatenated along the channel dimension to get the input of MSRSTM. Note that $\\\\mathbf{F}_2^s$ denotes $\\\\mathbf{F}_2$.\\n\\nThen, MSRSTM merges a long range of information from different exposure regions. MSRSTM consists of multiple multi-scale Swin Transformer layers (STL), a few convolutional layers, and a residual connection. Given the input feature $\\\\mathbf{F}_{N-1}^{\\\\text{out},i}$ of $i$-th MSRSTM, the output $\\\\mathbf{F}_{N}^{\\\\text{out},i}$ of MSRSTM can be formulated as follows:\\n\\n$$\\n\\\\mathbf{F}_{N}^{\\\\text{STL},i} = \\\\text{Conv}(\\\\text{Concat}(\\\\mathbf{STL}_{N,l}^1(\\\\mathbf{F}_{N-1}^{\\\\text{out},i}), \\\\mathbf{STL}_{N,l}^2(\\\\mathbf{F}_{N-1}^{\\\\text{out},i}), \\\\mathbf{STL}_{N,l}^3(\\\\mathbf{F}_{N-1}^{\\\\text{out},i}))),\\n$$\\n\\n(4)\\n\\n$$\\n\\\\mathbf{F}_{N}^{\\\\text{out},i} = \\\\text{Conv}(\\\\mathbf{F}_{N}^{\\\\text{STL},i}) + \\\\mathbf{F}_{N-1}^{\\\\text{out},i},\\n$$\\n\\n(5)\\n\\nwhere $\\\\mathbf{STL}_{N,l}^j(\\\\cdot)$ represents the $N$-th Swin Transformer layer of the $l_j$ scale in the $i$-th MSRSTM, $\\\\mathbf{F}_{N-1}^{\\\\text{out},i}$ denotes the input feature of the $N$-th Swin Transformer layer in the $i$-th MSRSTM.\\n\\nLoss Function. Since unlabeled samples do not have HDR ground truth labels, we calculate the self-supervised loss in the LDR domain. We first use function $\\\\omega$ to transform the predicted HDR image $\\\\hat{\\\\mathbf{Y}}$ to short, medium, and long exposure LDR images $\\\\hat{\\\\mathbf{Y}}_i$,\\n\\n$$\\n\\\\hat{\\\\mathbf{Y}}_i = \\\\omega(\\\\hat{\\\\mathbf{Y}}) = (\\\\hat{\\\\mathbf{Y}} \\\\times t_i)^{1/\\\\gamma}.\\n$$\\n\\n(6)\\n\\nTo recover the saturated regions, we transform the short exposure frame (since the predicted HDR in this stage is aligned to the short exposure frame) to new short, medium, and long exposure frames by ground truth generation. Then, we regard the new exposure frames as the ground truth $\\\\mathbf{X}_{GT}^i$ of the model,\"}"}
{"id": "CVPR-2023-1856", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"At the beginning of this stage, to improve the saturation HDR pseudo-labels hampering the optimization process. Specifically, at timestep $t$, we calculate the reconstruction loss and perceptual loss as follows,\\n\\n$$L_{recon,t} = \\\\lambda_L L_{percep,t}$$\\n\\nwhere $\\\\lambda$ is the weight factor of unlabeled data. Then, we train the model finetune to predict pseudo-labels at each timestep and calculate the loss between the predicted HDR and ground truth HDR.\\n\\nWe train all the methods on two public datasets, namely Kalantari\u2019s [9] and Hu\u2019s dataset [7]. Kalantari\u2019s dataset includes 27 training samples and 9 testing samples, and Hu\u2019s dataset includes 10 training samples and 5 testing samples. Three different LDR images in a sample are captured with exposure levels of $-2$, $0$, and $+2$. Here we apply the domain image, $D$, samples curated regions and further learn to handle ghosting regions.\\n\\nFinally, we calculate the reconstruction loss and perceptual loss between the predicted HDR and ground truth HDR. To get loss weight $\\\\lambda_L$, please refer to the next section.\\n\\nIn detail, we iteratively and adaptively train the model with a few dynamic and static samples and unlabeled samples, we further generate the pseudo-labels limited training samples and exploit unlabeled samples, we prevent the overfitting problem with a few labeled training samples and exploit unlabeled samples, we use function to map the linear domain image to the tonemapped image.\\n\\nConcretely, we iteratively and adaptively train the model at each timestep in the refinement stage, we calculate the reconstruction loss and perceptual loss as follows, where $\\\\lambda_L$ be the weight factor of unlabeled data. Then, we train the model finetune to generate pseudo-labels.\\n\\nTo prevent the overfitting problem with a few labeled training samples and exploit unlabeled samples, we use model finetune to generate pseudo-labels further generate the pseudo-labels. Then we will give a lower weight to pseudo-labels of poor quality, which has more saturation.\\n\\nFurthermore, we use model finetune to generate pseudo-labels. At the $i$-th max-pooling layer in VGG19, we consider the percentile (85th) loss as\\n\\n$$\\\\sigma(x) = \\\\left\\\\{ \\\\begin{array}{ll} \\\\mu_x & \\\\text{if } x \\\\leq \\\\tau_u \\\\\\\\ \\\\mu_x \\\\cdot \\\\beta & \\\\text{if } x > \\\\tau_u \\\\end{array} \\\\right.$$\"}"}
{"id": "CVPR-2023-1856", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Examples of Kalantari's [9] and Hu\u2019s [7] datasets (top row) and Tursun\u2019s [28] and Prabhakar\u2019s datasets (bottom row). Note that we directly evaluate the methods on Tursun\u2019s and Prabhakar\u2019s datasets with the checkpoint trained on Kalantari\u2019s dataset.\\n\\nWe train all comparison methods with the same set of images. Concretely, we randomly choose $K \\\\in \\\\{1, 5\\\\}$ dynamic labeled samples and $Q = 5$ static labeled samples for training in all methods. Furthermore, for each $K$, we evaluate all methods for 5 runs denoted as 5-way in Table 1. In addition, since FSHDR [22] and our method exploit unlabeled samples, we also use the rest of the dataset samples as unlabeled data $U$.\\n\\nFinally, to verify generalization performance, we evaluate all methods on Tursun\u2019s dataset [28] that does not have ground truth and Prabhakar\u2019s dataset [21].\\n\\nEvaluation Metrics. We calculate five common metrics used for testing, i.e., PSNR-L, PSRN-$\\\\mu$, SSIM-L, SSIM-$\\\\mu$, and HDR-VDP-2 [16], where '-L' denotes linear domain, '-\\\\mu' denotes tonemapping domain.\\n\\nImplementation Details. The window size in MSRSTM is $2 \\\\times 2$, $4 \\\\times 4$ and $8 \\\\times 8$. In the training stage, we crop the $128 \\\\times 128$ patches with stride 64 for the training dataset. We use the Adam optimizer, and set the batch size and learning rate as 4 and 0.0005, respectively. And we set $\\\\beta_1 = 0.9$, $\\\\beta_2 = 0.999$, and $\\\\epsilon = 1 \\\\times 10^{-8}$ in the Adam optimizer. We implement our model using PyTorch with 2 NVIDIA GeForce 3090 GPUs and train for 200 epochs.\\n\\n4.1. Comparison with State-of-the-art Methods\\n\\nTo evaluate our model, we carry out quantitative and qualitative experiments comparing with several state-of-thear methods, including patch-based classical methods: Sen [24], Hu [8], and deep learning-based methods: Kalantari [9], DeepHDR [30], AHDRNet [31], ADNet [14], FSHDR [22]. We use the codes provided by the authors.\\n\\nEvaluation on Kalantari\u2019s and Hu\u2019s Datasets. In Fig-\"}"}
{"id": "CVPR-2023-1856", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. The evaluation results on Kalantari's [9] and Hu's [7] datasets. The best and the second best results are highlighted in *Bold* and *Underline*, respectively.\\n\\n| Dataset | Metric | Setting | Kalantari DeepHDR | AHDRNet | ADNet | FSHDR | Ours |\\n|---------|--------|---------|-------------------|---------|-------|-------|------|\\n| Kalantari | PSNR-<sub>l</sub> | 5way-5shot | 39.37 \u00b1 0.12 | 38.25 \u00b1 0.29 | 40.61 \u00b1 0.10 | 40.78 \u00b1 0.15 | 41.39 \u00b1 0.12 |\\n|         |        |         | 41.54 \u00b1 0.10     |          |       |       |      |\\n|         | PSNR-<sub>\u00b5</sub> | 5way-5shot | 39.86 \u00b1 0.19 | 38.62 \u00b1 0.27 | 41.05 \u00b1 0.32 | 40.93 \u00b1 0.38 | 41.40 \u00b1 0.13 |\\n|         |        |         | 41.61 \u00b1 0.08     |          |       |       |      |\\n|         | PSNR-<sub>l</sub> | 5way-1shot | 36.94 \u00b1 0.44 | 36.67 \u00b1 0.67 | 38.83 \u00b1 0.39 | 38.96 \u00b1 0.35 | 41.04 \u00b1 0.11 |\\n|         |        |         | 41.14 \u00b1 0.11     |          |       |       |      |\\n|         | PSNR-<sub>\u00b5</sub> | 5way-1shot | 37.33 \u00b1 1.21 | 37.01 \u00b1 1.68 | 39.15 \u00b1 1.04 | 39.08 \u00b1 1.06 | 41.13 \u00b1 0.07 |\\n|         |        |         | 41.25 \u00b1 0.05     |          |       |       |      |\\n\\nTable 2. Further evaluation results on Kalantari's [9], Hu's [7] and Prabhakar's datasets [21]. The best and the second best results are *highlighted in Bold* and *Underline* in each setting, respectively.\\n\\n| Dataset | Metric | Setting | Kalantari DeepHDR | AHDRNet | ADNet | FSHDR | Ours (K=0) | Ours (K=1) | Ours (K=5) |\\n|---------|--------|---------|-------------------|---------|-------|-------|-----------|-----------|-----------|\\n| Kalantari | PSNR-<sub>l</sub> | | 38.57 | 40.94 | 0.9711 | 0.9780 | 64.71 | 41.12 | 41.54 |\\n|         |        | | 30.84 | 32.19 | 0.9408 | 0.9632 | 62.05 | 41.14 | 41.61 |\\n|         | PSNR-<sub>\u00b5</sub> | | 33.58 | 31.48 | 0.9634 | 0.9531 | 66.39 | 42.99 | 45.04 |\\n|         |        | | 36.94 | 36.56 | 0.9877 | 0.9824 | 67.58 | 42.55 | 44.24 |\\n|         | SSIM-<sub>l</sub> | | 0.9780 | 0.9780 | 0.9824 | 0.9824 | 67.58 | 0.9880 | 0.9880 |\\n|         |        | | 0.9531 | 0.9531 | 0.9824 | 0.9824 | 67.58 | 0.9868 | 0.9868 |\\n|         | HV2 | | 64.71 | 64.71 | 67.08 | 67.08 | 67.33 | 67.33 | 67.33 |\\n| Kalantari | PSNR-<sub>l</sub> | | 41.22 | 41.85 | 0.9848 | 0.9872 | 66.23 | 41.54 | 41.85 |\\n|         |        | | 43.76 | 41.60 | 0.9938 | 0.9914 | 72.94 | 47.41 | 47.41 |\\n|         | PSNR-<sub>\u00b5</sub> | | 41.64 | 41.64 | 42.15 | 42.15 | 71.35 | 42.99 | 45.04 |\\n|         |        | | 42.41 | 42.41 | 42.41 | 42.41 | 71.35 | 42.41 | 42.41 |\\n|         | SSIM-<sub>l</sub> | | 0.9848 | 0.9848 | 0.9938 | 0.9938 | 71.35 | 0.9866 | 0.9866 |\\n|         |        | | 0.9872 | 0.9872 | 0.9938 | 0.9938 | 71.35 | 0.9895 | 0.9895 |\\n|         | HV2 | | 66.23 | 66.23 | 72.94 | 72.94 | 72.94 | 72.94 | 72.94 |\\n| Prabhakar | PSNR-<sub>l</sub> | | 25.87 | 21.44 | 0.8610 | 0.9176 | 60.00 | 27.91 | 31.24 |\\n|         |        | | 10.23 | 16.95 | 0.6903 | 0.8346 | 49.10 | 30.29 | 27.91 |\\n|         | PSNR-<sub>\u00b5</sub> | | 25.92 | 21.43 | 0.8597 | 0.9170 | 60.02 | 25.48 | 27.91 |\\n|         |        | | 25.48 | 20.86 | 0.9215 | 0.8354 | 66.83 | 25.48 | 25.48 |\\n|         | SSIM-<sub>l</sub> | | 26.62 | 22.08 | 0.8737 | 0.9238 | 58.89 | 11.44 | 11.44 |\\n|         |        | | 25.76 | 21.39 | 0.8686 | 0.8217 | 60.36 | 10.86 | 10.86 |\\n|         | HV2 | | 58.89 | 58.89 | 63.88 | 63.88 | 63.88 | 63.88 | 63.88 |\\n| Prabhakar | PSNR-<sub>l</sub> | | 28.03 | 22.01 | 0.8751 | 0.9203 | 60.53 | 28.03 | 32.70 |\\n|         |        | | 12.82 | 19.37 | 0.7442 | 0.8347 | 55.34 | 30.29 | 32.70 |\\n|         | PSNR-<sub>\u00b5</sub> | | 29.01 | 22.01 | 0.8751 | 0.9203 | 60.53 | 29.01 | 32.70 |\\n|         |        | | 29.01 | 22.01 | 0.8751 | 0.9203 | 60.53 | 29.01 | 29.01 |\\n|         | SSIM-<sub>l</sub> | | 31.84 | 33.49 | 0.9588 | 0.9606 | 64.40 | 20.80 | 20.80 |\\n|         |        | | 31.08 | 33.50 | 0.9536 | 0.9636 | 63.88 | 20.78 | 20.78 |\\n|         | HV2 | | 31.84 | 33.49 | 64.40 | 64.40 | 64.40 | 64.40 | 64.40 |\\n| Prabhakar | PSNR-<sub>l</sub> | | 32.70 | 32.24 | 0.9553 | 0.9465 | 64.37 | 32.70 | 32.70 |\\n|         |        | | 20.23 | 19.71 | 0.7929 | 0.9026 | 59.63 | 30.29 | 32.70 |\\n|         | PSNR-<sub>\u00b5</sub> | | 32.24 | 32.24 | 0.9553 | 0.9465 | 64.37 | 32.24 | 32.24 |\\n|         |        | | 32.24 | 32.24 | 0.9553 | 0.9465 | 64.37 | 32.24 | 32.24 |\\n|         | SSIM-<sub>l</sub> | | 32.70 | 34.49 | 0.9586 | 0.9713 | 64.45 | 21.96 | 21.96 |\\n|         |        | | 32.70 | 34.49 | 0.9586 | 0.9713 | 64.45 | 21.96 | 21.96 |\\n|         | HV2 | | 32.70 | 34.49 | 64.45 | 64.45 | 64.45 | 64.45 | 64.45 |\\n\\nDue to insufficient labeled samples, large motion, and saturation, most comparing methods suffer from color distortion and ghosting artifacts in these two datasets. Kalantari's method and DeepHDR produce undesirable artifacts and color distortion (see Figure 4 (a)(b)). There are two reasons behind that: misalignment of optical flow and homographies and the lack of labeled data. Although AHDRNet and ADNET are proposed to suppress motion and saturation with attention mechanisms, they cannot reconstruct ghost-free HDR images with few labeled samples. They also produce severe ghosting artifacts (see the red block in Figure 4 (a)(b)). FSHDR exploits unlabeled data to alleviate ghosts under the constraint of a few labeled samples, but it is difficult to handle both ghosting and saturation problems simultaneously. We can see that FSHDR still suffers from ghosting artifacts which leaves an obvious hand artifact in the car (see the red block in Figure 4 (a)). Thanks to the proposed SMAE and sample-quality-based iterative learning strategy, which first address the saturation problems using SMAE and then adaptively sample well-exposed and ghost-free pseudo-labels to handle ghosting problems, we can reconstruct ghost-free HDR images with only a few labeled samples.\"}"}
{"id": "CVPR-2023-1856", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Ablation study of 5 shot scenario on Kalantari\u2019s dataset.\\n\\n| Model          | PSNR-l | PSNR-\u00b5 | HDR-VDP-2 |\\n|----------------|--------|--------|-----------|\\n| B1 SSHDR       | 41.54  | 41.61  | 67.33     |\\n| B2 Stage2Net   | 41.31  | 41.43  | 67.21     |\\n| B3 w/o APSS    | 41.49  | 41.45  | 67.29     |\\n| B4 AHDR        | 41.48  | 41.51  | 67.30     |\\n| B5 FSHDR       | 41.41  | 41.43  | 67.26     |\\n| B6 Vanilla-AHDR | 40.61 | 41.05  | 66.95     |\\n| B7 Vanilla-FSHDR | 41.39 | 41.40  | 67.25     |\\n\\nMethod with major HDR deghosting approaches in zero-shot setting $S_1$, few-shot setting $S_2$, and fully supervised setting $S_3$. Note that we use all the dynamic labeled samples without static and unlabeled samples for plain training in setting $S_3$. Our zero-shot approach outperforms other methods in zero-shot setting on two datasets. It also outperforms some 5-shot and fully supervised methods in most metrics. Finally, our few-shot and fully supervised approaches achieve state-of-the-art performance among two datasets.\\n\\nEvaluation Generalization Across Different Datasets. We compare our method against other approaches on Kalantari\u2019s, Hu\u2019s, Tursun\u2019s, and Prabhakar\u2019s datasets to verify generalization performance. We directly evaluate the methods with the checkpoint trained on Kalantari\u2019s dataset and show the qualitative results on Tursun\u2019s and Prabhakar\u2019s datasets in Figure 4 (c)(d). More results are included in the Appendix. In Figure 4 (c), since the lady\u2019s motion is large, all the comparison methods cannot remove the ghosting artifacts. In Figure 4 (d), the comparison methods have obvious color distortion and ghosting artifacts on the floor and in the ceil. It shows that other methods have poor generalization performance across different datasets. All these methods address both the saturation and ghosting problems simultaneously. They cannot learn a robust representation to reconstruct a high-quality HDR image. Thanks to our SMAE and sample-quality-based iterative learning strategy, we can learn a robust representation to recover saturated regions and remove ghosting artifacts.\\n\\nIn Table 2, setting $S_4$ denotes that we utilize the checkpoint trained on Kalantari\u2019s or Hu\u2019s dataset under 5 shot scenario to evaluate on Hu\u2019s or Kalantari\u2019s dataset reversely. Setting $S_5$ represents that we train on Kalantari\u2019s or Hu\u2019s dataset under 5 shot scenario and evaluate on Prabhakar\u2019s dataset. Our method achieves better numerical performance in terms of PSNR-$l$ and PSNR-$\u00b5$. It demonstrates that our method generalizes well across different datasets.\\n\\n4.2. Ablation Studies\\n\\nWe conduct ablation studies on Kalantari\u2019s dataset under the condition of 5 shot scenario across 5 runs and analyze the importance of each component. We use the following variants of our whole SSHDR model: 1) SSHDR: The full model of SSHDR network trained with two entire stages. 2) Stage2Net: The model only trained in the second stage without SMAE pre-training. 3) w/o APSS: The model trained with two stages without using sample-quality-based pseudo-labels selection strategy. 4) AHDR$^*$: The AHDR model is trained with our proposed two stages strategy. 5) FSHDR$^*$: Our model is trained with the FSHDR strategy. 6) Vanilla-AHDR: The vanilla AHDR model trained in 5 shot scenario. 7) Vanilla-FSHDR: The vanilla FSHDR model trained with 5 labeled samples.\\n\\nSMAE Pre-training. As shown in Table 3, the performance of Stage2Net is significantly decreased compared with SSHDR. Since the SMAE learns a robust representation to generate content of saturated regions, it helps to improve the saturated regions. In a word, it demonstrates that the SMAE pre-training stage is an effective mechanism.\\n\\nPseudo-labels Selection Strategy. Since the sample-quality-based pseudo-labels selection strategy can exclude saturated and ghosted samples (see Figure 5), the model can be guided in a correct optimization direction which is effective for ghost removal. When we remove the pseudo-labels selection strategy, the performance of the model without APSS is dropped.\\n\\nTwo Stages Strategy. In Table 3, we report the performance of AHDR$^*$. It achieves a significant increment compared with the vanilla AHDR model, which demonstrates the effectiveness of the overall two stages strategy.\\n\\nProposed Model Architecture. When we replace our two stages strategy with the FSHDR strategy, the numerical results increase compared with FSHDR. It shows that our proposed model architecture is also sound.\\n\\n5. Conclusion\\n\\nWe propose a novel semi-supervised deghosting method for few-shot HDR problem via two stages of completing saturation and deghosting. In the first stage, a Saturated Mask AutoEncoder is proposed to learn a robust representation and reconstruct a non-saturated HDR image with a self-supervised mechanism. In the second stage, we propose an adaptive pseudo-label selection strategy to avoid the effects of mislabeled samples. Finally, our approach shows superiority over the existing state-of-the-art methods.\"}"}
{"id": "CVPR-2023-1856", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Sagie Benaim and Lior Wolf. One-shot unsupervised cross-domain translation. Advances in neural information processing systems (NIPS), 31, 2018.\\n\\n[2] Luca Bertinetto, Jo\u00e3o Henriques, Jack Valmadre, Philip Torr, and Andrea Vedaldi. Learning feed-forward one-shot learners. Advances in neural information processing systems (NIPS), 29, 2016.\\n\\n[3] L. Bogoni. Extending dynamic range of mono-chrome and color images through fusion. In IEEE International Conference on Pattern Recognition (ICPR), pages 7\u201312, 2000.\\n\\n[4] Chelsea Finn, Pieter Abbeel, and Sergey Levine. Model-agnostic meta-learning for fast adaptation of deep networks. In International conference on machine learning (ICML), pages 1126\u20131135. PMLR, 2017.\\n\\n[5] Thorsten Grosch. Fast and robust high dynamic range image generation with camera and object movement. In IEEE Conference of Vision, Modeling and Visualization (VMV), 2006.\\n\\n[6] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16000\u201316009, 2022.\\n\\n[7] Jinhan Hu, Gyeongmin Choe, Zeeshan Nadir, Osama Nabil, Seok-Jun Lee, Hamid Sheikh, Youngjun Yoo, and Michael Polley. Sensor-realistic synthetic data engine for multi-frame high dynamic range photography. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 516\u2013517, 2020.\\n\\n[8] Jun Hu, O. Gallo, K. Pulli, and Xiaobai Sun. HDR deghosting: How to deal with saturation? In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1163\u20131170, 2013.\\n\\n[9] Nima Khademi Kalantari and Ravi Ramamoorthi. Deep high dynamic range imaging of dynamic scenes. ACM Transactions on Graphics, 36(4):1\u201312, 2017.\\n\\n[10] S. B. Kang, M. Uyttendaele, S. Winder, and R. Szeliski. High dynamic range video. ACM Transactions on Graphics, 22(3):319\u2013325, 2003.\\n\\n[11] Chul Lee, Yuelong Li, and Vishal Monga. Ghost-free high dynamic range imaging via rank minimization. IEEE Signal Processing Letters, 21(9):1045\u20131049, 2014.\\n\\n[12] Yoonho Lee and Seungjin Choi. Gradient-based meta-learning with learned layerwise metric and subspace. In International Conference on Machine Learning (ICML), pages 2927\u20132936. PMLR, 2018.\\n\\n[13] Jingyun Liang, Jiezhang Cao, Guolei Sun, Kai Zhang, Luc Van Gool, and Radu Timofte. Swinir: Image restoration using swin transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, pages 1833\u20131844, 2021.\\n\\n[14] Zhen Liu, Lin Wenjie, Li Xinpeng, Rao Qing, Jiang Ting, Han Mingyan, Fan Haoqiang, Sun Jian, and Liu Shuaicheng. Adnet: Attention-guided deformable convolutional network for high dynamic range imaging. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR) Workshops, pages 463\u2013470, 2021.\\n\\n[15] Kede Ma, Li Hui, Yong Hongwei, Wang Zhou, Meng Deyu, and Zhang. Lei. Robust multi-exposure image fusion: A structural patch decomposition approach. IEEE Transactions on Image Processing, 26(5):2519\u20132532, 2017.\\n\\n[16] Rafat Mantiuk, Kil Joong Kim, Allan G. Rempel, and Wolfgang Heidrich. HDR-VDP-2: a calibrated visual metric for visibility and quality predictions in all luminance conditions. In ACM Siggraph, pages 1\u201314, 2011.\\n\\n[17] Alexander Miller, Adam Fisch, Jesse Dodge, Amir-Hossein Karimi, Antoine Bordes, and Jason Weston. Key-value memory networks for directly reading documents. arXiv preprint arXiv:1606.03126, 2016.\\n\\n[18] Yuzhen Niu, Wu Jianbin, Liu Wenxi, Guo Wenzhong, and WH Lau. Rynson. Hdr-gan:Hdr image reconstruction from multi-exposed ldr images with large motions. IEEE Transactions on Image Processing, 30:3885\u20133896, 2021.\\n\\n[19] Tae-Hyun Oh, Joon-Young Lee, Yu-Wing Tai, and In So Kweon. Robust high dynamic range imaging by rank minimization. IEEE Transactions on Pattern Analysis and Machine Intelligence, 37(6):1219\u20131232, 2015.\\n\\n[20] Fabrizio Pece and Jan Kautz. Bitmap movement detection: HDR for dynamic scenes. In Visual Media Production (CVMP), pages 1\u20138, 2010.\\n\\n[21] K Ram Prabhakar, Rajat Arora, Adhitya Swaminathan, Kunal Pratap Singh, and R Venkatesh Babu. A fast, scalable, and reliable deghosting method for extreme exposure fusion. In 2019 IEEE International Conference on Computational Photography (ICCP), pages 1\u20138. IEEE, 2019.\\n\\n[22] K Ram Prabhakar, Gowtham Senthil, Susmit Agrawal, R Venkatesh Babu, and Rama Krishna Sai S Gorthi. Labeled from unlabeled: Exploiting unlabeled data for few-shot deep hdr deghosting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 4875\u20134885, 2021.\\n\\n[23] Hang Qi, Matthew Brown, and David G Lowe. Low-shot learning with imprinted weights. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 5822\u20135830, 2018.\\n\\n[24] Pradeep Sen, Khademi Kalantari Nima, Yaesoubi Maziar, Darabi Soheil, Dan B Goldman, and Eli Shechtman. Robust patch-based HDR reconstruction of dynamic scenes. ACM Transactions on Graphics, 31(6):1\u201311, 2012.\\n\\n[25] Pranav Shyam, Shubham Gupta, and Ambedkar Dukkipati. Attentive recurrent comparators. In International conference on machine learning (ICML), pages 3173\u20133181. PMLR, 2017.\\n\\n[26] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus, et al. End-to-end memory networks. Advances in neural information processing systems (NIPS), 28, 2015.\\n\\n[27] Anna Tomaszewska and Radoslaw Mantiuk. Image registration for multi-exposure high dynamic range image acquisition. In International Conference in Central Europe on Computer Graphics and Visualization (WSCG), 2007.\"}"}
{"id": "CVPR-2023-1856", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Okan Tarhan Tursun, Ahmet O\u02d8guz Aky\u00a8uz, Aykut Erdem, and\\nErkut Erdem. An objective deghosting quality metric for\\nHDR images. *Comput. Graph. Forum*, 35(2):139\u2013152, 2016.\\n\\nXintao Wang, Kelvin CK Chan, Ke Yu, Chao Dong, and\\nChen Change Loy. Edvr: Video restoration with enhanced\\ndeformable convolutional networks. In *Proceedings of the\\nIEEE/CVF Conference on Computer Vision and Pattern\\nRecognition (CVPR) Workshops*, pages 0\u20130, 2019.\\n\\nShangzhe Wu, Xu Jiarui, Tai Yu-Wing, and Tang. Chi-\\nKeung. Deep high dynamic range imaging with large fore-\\nground motions. In *Proceedings of the European Conference\\non Computer Vision (ECCV)*, pages 117\u2013132, 2018.\\n\\nQingsen Yan, Gong Dong, Shi Qinfeng, van den Hengel An-\\nton, Shen Chunhua, Reid Ian, and Zhang Yanning. Attention-\\nguided network for ghost-free high dynamic range imaging.\\nIn *Proceedings of the IEEE/CVF Conference on Computer\\nVision and Pattern Recognition (CVPR)*, pages 1751\u20131760,\\n2019.\\n\\nQingsen Yan, Dong Gong, Javen Qinfeng Shi, Anton van den\\nHengel, Chunhua Shen, Ian Reid, and Yanning Zhang.\\nDual-attention-guided network for ghost-free high dynamic\\nrange imaging. *International Journal of Computer Vision*\\n130(1):76\u201394, 2022.\\n\\nQingsen Yan, Zhang Lei, Liu Yu, Zhu Yu, Sun Jinqiu, Shi\\nQinfeng, and Zhang Yanning. Deep hdr imaging via a non-\\nlocal network. *IEEE Transactions on Image Processing*\\n29:4308\u20134322, 2020.\\n\\nQingsen Yan, Jinqiu Sun, Haisen Li, Yu Zhu, and Yanning\\nZhang. High dynamic range imaging by sparse representa-\\ntion. *Neurocomputing* 269:160\u2013169, 2017.\\n\\nQingsen Yan, Bo Wang, Peipei Li, Xianjun Li, Ao Zhang,\\nQinfeng Shi, Zheng You, Yu Zhu, Jinqiu Sun, and Yan-\\ning Zhang. Ghost removal via channel attention in expo-\\nsure fusion. *Computer Vision and Image Understanding*\\n201:103079, 2020.\\n\\nQian Ye, Jun Xiao, Kin-man Lam, and Takayuki Okatani.\\nProgressive and selective fusion network for high dynamic\\nrange imaging. In *Proceedings of the 29th ACM Interna-\\ntional Conference on Multimedia (ACM MM)*, pages 5290\u2013\\n5297, 2021.\\n\\nJaesik Yoon, Taesup Kim, Ousmane Dia, Sungwoong Kim,\\nYoshua Bengio, and Sungjin Ahn. Bayesian model-agnostic\\nmeta-learning. *Advances in neural information processing\\nsystems (NIPS)* 31, 2018.\\n\\nWei Zhang and Wai-Kuen Cham. Gradient-directed multiex-\\nposure composition. *IEEE Transactions on Image Process-\\ning* 21(4):2318\u20132323, 2011.\"}"}
