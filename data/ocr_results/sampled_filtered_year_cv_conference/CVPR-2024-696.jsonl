{"id": "CVPR-2024-696", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"The existing facial datasets, while having plentiful images at near frontal views, lack images with extreme head poses, leading to the downgraded performance of deep learning models when dealing with profile or pitched faces. This work aims to address this gap by introducing a novel dataset named Extreme Pose Face High-Quality Dataset (EFHQ), which includes a maximum of 450k high-quality images of faces at extreme poses. To produce such a massive dataset, we utilize a novel and meticulous dataset processing pipeline to curate two publicly available datasets, VFHQ and CelebV-HQ, which contain many high-resolution face videos captured in various settings. Our dataset can complement existing datasets on various facial-related tasks, such as facial synthesis with 2D/3D-aware GAN, diffusion-based text-to-image face generation, and face reenactment. Specifically, training with EFHQ helps models generalize well across diverse poses, significantly improving performance in scenarios involving extreme views, confirmed by extensive experiments. Additionally, we utilize EFHQ to define a challenging cross-view face verification benchmark, in which the performance of SOTA face recognition models drops 5-37% compared to frontal-to-frontal scenarios, aiming to stimulate studies on face recognition under severe pose conditions in the wild.\"}"}
{"id": "CVPR-2024-696", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"images indistinguishable from real faces. Face reenactment systems, including \\n41, 43, 51, can animate an input face based on poses and expressions from any driver video. \\nDespite impressive results, the mentioned models produce inferior results when dealing with faces at extreme \\nhead poses. As illustrated in Fig. 1, faces synthesized by 3D face generation or face reenactment models are often \\ndistorted at profile views, due to the scarcity of extreme pose images in training datasets. Current extensive datasets \\nfor face generation leverage portrait images from Google or stock photo repositories like Flickr, which feature images \\nwith upfront views and minimal facial rotations. Face reenactment methods leverage video datasets such as VoxCeleb \\n29, primarily featuring frames with frontal faces. The instability of pre-processing tools, particularly face align-\\nment modules when handling extreme pose images, further reduces the proportion of those images in the existing \\ndatasets. This issue is critical since faces with large head rotations are common in real life, and dealing with them is \\nunavoidable in practical systems.\\n\\nThis paper aims to fill that void by proposing a novel large-scale dataset of extreme-pose and high-quality facial \\nimages called ExtremePose-Face-HQ, or EFHQ for short. Our ambition is to build an \\\"one-for-all\\\" dataset that can \\nsupplement existing datasets on all mentioned face-related tasks. To achieve that goal, the dataset needs all the follow-\\nning properties: (1) extreme-pose, (2) large-scale, (3) high-\\nquality, (4) in-the-wild, (5) having multiple images for each \\nsubject and having subject identity annotated. Given those \\nrequirements, we utilize a novel and meticulous dataset pro-\\ncessing pipeline to filter and sample frames from two pub-\\nlicly available datasets, including VFHQ 48 and CelebV-\\nHQ 54, which contain high-resolution face videos cap-\\ntured in various settings. This results in a dataset with 450k \\nhigh-quality images of extreme poses, which are combined \\nwith frontal images of the same subject to use as image \\npairs. These image pairs can support face reenactment and \\nface recognition tasks, while the extreme-pose images alone \\ncan complement the existing datasets of face generation \\ntraining. Note that there is a recent attempt 47 to build a \\nlarge-pose facial dataset similar to ours. However, that work \\nrelies on images crawled from Flickr, thus having limited \\nsize (19k images) and missing identity information. Hence, \\nits applications are restricted, covering only several face-\\ngeneration tasks, unlike our large-scale and multi-purpose \\ndataset.\\n\\nWe perform a comprehensive set of experiments to val-\\nidate the effectiveness of EFHQ across three distinct face \\ngeneration sub-tasks and face reenactment scenarios. Our \\ndataset, EFHQ, when combined with standard training sets, \\nsignificantly enhances the quality of synthesized images in \\nhighly profile or pitched views while maintaining synthe-\\nsis quality in frontal views. EFHQ also provides a more \\nchallenging pose-centric face verification benchmark. It re-\\nveals the vulnerability of many state-of-the-art face recog-\\nnition networks when dealing with extreme-pose images, \\nwith TAR@FAR=1e-3 scores dropping 5-37%.\\n\\nIn summary, our contributions include (1) a large-scale, \\nhigh-quality, in-the-wild, extreme-pose dataset (EFHQ) de-\\nsigned to complement various face-related tasks, (2) a novel \\nand meticulous dataset processing pipeline ensuring the cre-\\nation and quality of the EFHQ dataset, and (3) a series of ex-\\nperiments, with plan-to-release models, demonstrating the \\nusefulness of EFHQ in enhancing performance on a wide \\nrange of tasks when dealing with faces at extreme rotations. \\nThe final dataset will be publicly available as metadata files \\nnot to violate the two original dataset's licenses, alongside \\nthe pretrained networks.\\n\\n2. Related Work\\n\\nSynthetic Face Generation. Existing 2D-based models \\nlike StyleGAN 14, 15, 17 and 3D-aware models like \\nEG3D 5 demonstrate impressive fidelity, especially for \\nnear-frontal views. Nevertheless, their performance signif-\\nicantly degrades when faced with extreme views, primarily \\ndue to insufficient training data for such scenarios. Datasets \\nlike FFHQ 14 exhibit a notable bias toward near-frontal \\nposes, heavily limiting the range of poses synthesized by \\ncurrent 2D and 3D models despite the 3D-aware generators'\\nability to handle arbitrary poses.\\n\\nDiffusion-based text-to-image models 32, 33 can syn-\\nthesize high-quality images from simple text inputs. How-\\never, these models lack mechanisms to control the generated \\npose. Consequently, the generated facial images remain in \\nfrontal view despite terms like \\\"profile\\\" or \\\"looking-up\\\" \\nin the input prompts. ControlNet 49 proposes a finetun-\\ning approach allowing these models to take additional con-\\ntext as conditional input. Namely, one can use landmarks to \\nguide the generated face to the desired pose. However, the \\nscarcity of profile-view data in training affects the model's \\nperformance, particularly in extreme angles. Consequently, \\neffectively managing the long-tailed distribution of these \\nextreme views becomes necessary to address this issue.\\n\\nFace Reenactment. Face reenactment involves transferring \\nfacial expressions and movements from a driving image to \\na source image. Recent advancements claim high-quality \\nresults, especially for near-frontal poses 31, 41, 43, 51. How-\\never, pose collapse persists, particularly in keypoint-\\nbased models 41, 51. Common datasets for this task are \\nVoxCeleb1 29 and HDTF 50, which primarily feature \\nfrontal talking face videos. Despite recent potentially high-\\nquality datasets for reenactment such as VFHQ 48 and \\nCelebV-HQ 54, the frontal-view bias persists. As a re-\\nsult, facial reenactment methods often produce artifacts and \\nlower-quality outputs when faced with significant rotations.\"}"}
{"id": "CVPR-2024-696", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"### Table 1.\\nRepresentative comparison of key attributes and supported tasks across existing face datasets and our proposed EFHQ dataset.\\n\\n|                | #images | Profile View | Face Verification | Face Synthesis | Face Reenactment |\\n|----------------|---------|--------------|------------------|----------------|------------------|\\n| AgeDB          | 12K     | 3            | 3                | 3              | 3                |\\n| CPLFW          | 12K     | 3            | 3                | 3              | 3                |\\n| CFP-FP         | 14K     | 3            | 3                | 3              | 3                |\\n| IJB-x          | 138K    | 3            | 3                | 3              | 3                |\\n| CelebA-HQ      | 30K     | 3            | 3                | 3              | 3                |\\n| FFHQ           | 70K     | 3            | 3                | 3              | 3                |\\n| LPFF           | 19K     | 3            | 3                | 3              | 3                |\\n| CelebV-HQ      | 6M      | 3            | 3                | 3              | 3                |\\n| VFHQ           | 3M      | 3            | 3                | 3              | 3                |\\n| VoxCeleb1      | 4M      | 3            | 3                | 3              | 3                |\\n| Ours           | 450,538 | 3            | 3                | 3              | 3                |\\n| FFHQ+Ours*     | 112,671 | 3            | 3                | 3              | 3                |\\n| Vox+Ours*      | 4.5M    | 3            | 3                | 3              | 3                |\\n\\n**Table 1.**\\nRepresentative comparison of key attributes and supported tasks across existing face datasets and our proposed EFHQ dataset.\\n\\nEFHQ provides large-scale, high-quality images with multiple views per identity, enabling diverse face-related tasks, including synthesis, reenactment, and verification. Combining EFHQ with existing datasets like FFHQ or VoxCeleb can augment available data for selected tasks. Asterisk (*) indicates a combined dataset with a subsampled version of EFHQ.\\n\\n**Face Verification.** Despite near-perfect accuracy on benchmark datasets\\\\[12,20,27,28,44\\\\], state-of-the-art face verification models\\\\[7,13,18\\\\] fall short in unconstrained environments due to a lack of pose diversity. While CPLFW\\\\[52\\\\] introduces 6000 cross-pose pairs, its small size and mix of pose samples hinder analyzing performance on specific pose scenarios such as frontal-frontal, frontal-profile, and profile-profile. More granular benchmarking on diverse pose pairs is crucial to identify failure modes and drive progress in robust face verification across poses.\\n\\n**Extreme Pose Face Dataset.** Tab. 1 summarizes existing datasets related to face verification, synthesis, and reenactment, showing that existing face datasets have limitations for multi-task use. Recent efforts like LPFF\\\\[47\\\\] made progress by crawling from Flickr and processing for diverse views, resulting in a dataset of 19,590 images. The dataset could complement high-quality datasets like FFHQ\\\\[14\\\\] for generative models. However, due to its limited scale comprising only 20% of the combined dataset, the unbalanced distribution between frontal and profile view still needs further improvements. Additionally, since the images are from image hosting services, the dataset fails to provide the identity information necessary for other facial applications.\\n\\n### 3. Dataset Process\\nIn this section, we discuss the process of producing our novel dataset EFHQ. To fulfill our goals, we carefully curated frames from the two recent facial video datasets, VFHQ\\\\[48\\\\] and CelebV-HQ\\\\[54\\\\], as seen in Fig. 2.\\n\\n#### 3.1. Attributes Preparation\\n\\n**Attributes Extraction.** To curate accurate facial poses and high-quality frames from existing datasets, we first extract (1) face bounding boxes, (2) facial landmarks (5/68 keypoints), (3) image quality score, and (4) face identity. For bounding boxes and landmarks, we employ the popular RetinaFace\\\\[36\\\\] and SynergyNet\\\\[46\\\\]. We use HyperIQA\\\\[40\\\\] to grade image quality. For VFHQ, we only generate 68 landmarks and image quality scores since bounding boxes and 5 keypoints are provided. After extracting these attributes, we match them with existing annotations using an IoU-based Hungarian matching algorithm\\\\[21\\\\]. Regarding identity, VFHQ-wise, we source labels from the annotations. As CelebV-HQ lacks individual identity labels within the video, we use\\\\[35\\\\] to identify the identities associated with each bounding box. Due to the instability of head pose estimators when handling extreme-pose images, we strategically apply multiple pose estimators relying on various methodologies: (1) SynergyNet\\\\[46\\\\], a 3DMM-based model, (2) DirectMHP\\\\[53\\\\], a landmark-free joint head detector and pose estimator, and (3) an in-house FacePoseNet\\\\[6\\\\] improved through extensive data augmentation focused on extreme poses and extra training data. We then categorize each estimated pose into a hierarchical binning scheme illustrated in Fig. 2. Next, we gather bin predictions and perform majority voting to arrive at consensus labels. When no agreement emerges, we put it into the 5th bin for \u201cconfusing\u201d cases. As depicted in Fig. 3, this ensemble approach provides robustness in cases where a single estimator incorrectly categorizes a sample. Please refer to the supplementary for statistics of the extracted bins and the hyperparameters for each model used for attribute extraction.\\n\\n**Annotation Review.** Next, we manually review the results to ensure high-quality labels. Given the large dataset, we developed a graphical user interface tool to streamline the review process. Our focus is verifying the binning annotations.\"}"}
{"id": "CVPR-2024-696", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"VFHQ\\nCelebV-HQ\\nVideo data\\nExtracted frames\\nAttribute Extraction\\nLandmark Detection\\nImage Quality Assessment (HyperIQA)\\nPose Binning\\nSynergyNet\\nFacePoseNet\\nDirectMHP\\nHead Pose Estimation\\nHuman Review\\nDataset with attributes (EFHQ)\\nSampling\\nSemi-auto cleaning\\nTask-specific preprocessing\\nGAN dataset\\nSampling\\nSemi-auto cleaning\\nTask-specific preprocessing\\nFace reenactment dataset\\n\\nFigure 2. The pipeline of EFHQ dataset creation. Starting with high-quality videos from the VFHQ[48] and CelebV-HQ[54] datasets, single-frame attributes are extracted then manually reviewed. Task-specific preprocessing is then applied to generate specialized versions of the dataset for tasks such as face generation, reenactment, and verification.\\n\\nSynergyNet: Pitch = 54\\nFacePoseNet: Pitch = 0\\nDirectMHP: Pitch = 7\\nSynergyNet: profile_up\\nFacePoseNet: frontal\\nDirectMHP: frontal\\nSynergyNet: Yaw = -46\\nFacePoseNet: Yaw = -29\\nDirectMHP: Yaw = -50\\nSynergyNet: profile_right\\nFacePoseNet: frontal\\nDirectMHP: profile_right\\nSynergyNet: Pitch = -40\\nFacePoseNet: Pitch = -32\\nDirectMHP: Pitch = NaN\\nSynergyNet: profile_down\\nFacePoseNet: profile_down\\nDirectMHP: N/A profile_down\\n\\nFigure 3. Example cases where a pose estimator fails to categorize the sample to the correct bin.\\n\\nConcurrent, we randomly subsample the data based on pose angle and image quality to validate facial landmark quality further and discard cases of landmark prediction failure.\\n\\n3.2. EFHQ dataset\\nOur final EFHQ dataset comprises up to 450k frames with extreme poses extracted from approximately 5,000 clips. Most of the clips in our dataset include at least one frame with a frontal face and multiple frames with extreme pose angles. We also include extreme pose-only clips, representing profile-to-profile pose transfer cases.\\n\\nFigure 4. Pose distribution comparison between our sampled dataset and other datasets, including FFHQ[14], LPFF[47], CPLFW[52] and 40K random samples from VoxCeleb1[29]. Our sampled dataset demonstrates greater pose diversity, with increased sample counts across high angle bins.\\n\\n3.3. Dataset for subtasks\\nSupplementary datasets for face generation.\\nTo address FFHQ's pose distribution gap, we compile a dataset encompassing diverse poses, varied identities, and FFHQ-comparable image quality from EFHQ. Additionally, we also embed an image brightness filter[3] in the sampling process, given that HyperIQA doesn't explicitly assess this aspect. The sampled images are then processed using the same pipeline in[5, 16] and manually reviewed. The aim is to improve coverage of under-represented poses without degrading frontal performance of trained generation models. This yields a dataset of 42,671 images with equitably distributed poses as visualized in Fig. 4. Compared to the original FFHQ distribution, yaw and pitch distributions are significantly improved.\\n\\nAdditionally, for 3D-aware GAN subtasks, we extract camera parameters to serve as conditional pose information, following[5]. We convert these parameters into yaw, pitch angles to validate against our existing annotations. When disagreements emerge, we re-examine the images and filter if needed, to ensure accurate, quality labels.\\n\\nFor diffusion-based text-to-image generation, a specialized dataset is curated to refine Stable Diffusion models[33] by accommodating landmark conditional input through ControlNet[49]. Based on our 3D-aware face generation dataset, we integrate landmark-based conditional images and tailored text prompts for each image. To capture fine facial nuances, we extract 478 facial landmarks and draw the condition image with connected edges between matched keypoints, following Mediapipe framework[26], better conveying face expression and pose. For text prompts, detailed facial attributes like gender, race, and emotion are derived from existing labels or inferred via BLIP-2 pretrained captioning model[23] and a face attribute estimator[37].\\n\\nThe resulting prompt adheres to a structured format: \u201cA profile portrait image of a [emotion] [race] [gender].\u201d\"}"}
{"id": "CVPR-2024-696", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. StyleGAN2-ADA models trained on different datasets. Lower FIDs indicate better fidelity, while higher Recalls indicate better diversity. The second and third blocks show comparisons between unconditional models trained on FFHQ+LPFF and FFHQ+EFHQ when evaluating on the same and cross-dataset settings, with the better metrics in bold.\\n\\nSupplementary dataset for face reenactment. As EFHQ is crafted, we can employ the entire dataset to complement established training datasets, e.g., VoxCeleb1 [29]. We also built an extra evaluation set with 1200 EFHQ clips.\\n\\nBenchmarking dataset for face verification. We curate a dataset covering three distinct scenarios: frontal-to-frontal, frontal-to-profile, and profile-to-profile. To enable rigorous benchmarking, we sample 10,000 pairs each for both negative and positive cases per scenario, resulting in a balanced benchmark dataset containing 60,000 image pairs. Images follow the same established preprocessing pipelines in [7]. Next, we filter misaligned images with RetinaFace [36], randomly review, and replace equivalent samples from our diverse corpus, if needed. Finally, to simulate varying image quality, we randomly applied downscaling and compression to samples in the dataset.\\n\\n4. Face Generation Subtask\\n\\n4.1. StyleGAN\\n\\nTraining details. We train StyleGAN2-ADA [15] models from scratch after combining FFHQ with our own curated dataset (denoted FFHQ+EFHQ) at 1024 \u00d7 1024 resolution. We also train a conditional version, allowing direct control over frontal versus profile synthesis and enhancing model performance. Specifically, we categorize the dataset into two classes: \u201cfrontal\u201d and \u201cprofile\u201d based on pose angles. All models are trained for 6 days on 8 Nvidia A100 GPUs.\\n\\nEvaluation protocol. We utilize the Frechet Inception Distance [11] (FID) metric and improved Recall [22] to quantify the generation fidelity and diversity. Regarding the two unconditional models trained with the supplemented FFHQ+LPFF and FFHQ+EFHQ, we evaluate each model with both of these datasets. For conditional models, we evaluate performance on FFHQ, FFHQ+LPFF, and FFHQ+EFHQ to demonstrate the broader coverage of the model trained with our proposed dataset. For FFHQ, we generate 50K images with the \u201cfrontal\u201d label, whereas for the other two datasets, we generate 50K following their label distributions. This comprehensive quantitative analysis examines the fidelity and variety of the generated images.\\n\\nExperimental results. We compared the performance of our proposed unconditional models ($G_{FFHQ+EFHQ}$), against models trained on the FFHQ and FFHQ+LPFF datasets ($G_{FFHQ}$ and $G_{FFHQ+LPFF}$). Models trained conditionally are specifically subscripted, e.g., $G_{FFHQ+EFHQ}^c$. The qualitative results in Fig. 5 illustrate that our models generate high-quality frontal faces comparable to the FFHQ model while producing realistic and varied profile faces. Additionally, our method has less noise in extreme poses than the FFHQ+LPFF model. Specifically, Fig. 5b shows LPFF's generated face contains more noise and less photorealistic details than our model, such as the pixelated eye and nose patches. Meanwhile, ours achieves greater realism with smoother skin, well-defined features, noise reduction, and natural lighting. The quantitative results in Tab. 2 show that our unconditional model achieves competitive FID scores and improved Recall to other models, indicating similar fidelity but greater sample pose diversity. In the cross-examination (third block of Tab. 2), $G_{FFHQ+EFHQ}$ has both better FID and improved Recall on the opposite dataset than $G_{FFHQ+LPFF}$. Finally, our conditional model demonstrates substantially improved FID scores over multiple reference datasets, highlighting its versatility.\"}"}
{"id": "CVPR-2024-696", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. EG3D models trained on different datasets. In the second and third blocks, we compare models trained on FFHQ+LPFF and FFHQ+EFHQ with the testset is either FFHQ or their training datasets; the better metrics are in bold. The best metrics on the FFHQ testset are in red, while the best metrics when evaluating on training dataset are underlined.\\n\\n4.2. EG3D Training details. We followed the same pipeline and hyper-parameters as the original EG3D model [5]. However, based on empirical results, we adjusted \\\\( \\\\eta = 5 \\\\). We also fixed the triplane implementation bug (XY, XZ, ZX) [1] to replicate the original idea in the papers (XY, XZ, ZY). We trained the model on 8 Nvidia A100 40GB GPUs for approximately 7 days until convergence.\\n\\nEvaluation Protocol. To evaluate the models, mirroring [5], we measure image quality with FID; multi-view consistency and identity preservation with mean cosine similarity of pre-trained face recognition model [7]; pose accuracy and geometry by calculating MSE against pseudo labels from [8]. We evaluate all models on 1,024 generated images, except for FID, which we compute on 50k images.\\n\\nExperimental results. To evaluate the benefits of our dataset, we trained the same backbone architecture on different datasets: FFHQ (\\\\( \\\\text{E} \\\\)FFHQ), FFHQ+LPFF (\\\\( \\\\text{E} \\\\)FFHQ+LPFF), and FFHQ+EFHQ (our model, \\\\( \\\\text{E} \\\\)FFHQ+EFHQ).\\n\\nThe quantitative results are presented in Table 3, and qualitative examples are shown in Figure 6. Qualitatively, we observe that \\\\( \\\\text{E} \\\\)FFHQ+EFHQ generates improved face shapes in extreme poses compared to \\\\( \\\\text{E} \\\\)FFHQ, similar to the improvements seen with \\\\( \\\\text{E} \\\\)FFHQ+LPFF. Quantitatively, the FID of \\\\( \\\\text{E} \\\\)FFHQ+EFHQ is comparable to \\\\( \\\\text{E} \\\\)FFHQ when evaluated on FFHQ, indicating no degradation in frontal view performance. Meanwhile, the competitive FID achieved by \\\\( \\\\text{E} \\\\)FFHQ+EFHQ on the combined FFHQ+EFHQ dataset also suggests proficient modeling of profile views. Regarding identity consistency, we see a significant drop when the reference dataset contains profile-view images, due to the weak confidence of the pretrained face recognition model when handling profile-view images, as confirmed by its low average cosine similarity of around 0.6 on the simple frontal-to-profile verification set CFP-FP [34]. Furthermore, our model also outperforms in pose accuracy and geometry quality with both frontal and cross-pose dataset.\\n\\nTable 4. Quantitative results of fine-tuning StableDiffusion v1.5 with ControlNet trained on various datasets, using Normalized Mean Error (NME), FID, and FID on eye region.\\n\\n4.3. ControlNet Training details. We fine-tuned Stable Diffusion v1.5 [33] using ControlNet [49] on the EFHQ dataset to enhance its ability to generate extreme facial poses. We use AdamW optimizer [25] with learning rate \\\\( 1 \\\\times e^{-5}, \\\\beta_1 = 0.999, \\\\beta_2 = 0.999, \\\\) and \\\\( \\\\varepsilon = 0.01 \\\\). We train with batch size 24 on 1 Nvidia A100 40GB GPU in 2 days.\\n\\nEvaluation Protocol. To assess image qualities across diverse poses, we measure the Normalized Mean Error (NME) of 478-point facial landmarks between the generated and source image. Specifically, we evaluate performance across the full pose distribution in the reference dataset.\\n\\nFigure 7. Comparison profile-view generated samples of pretrained ControlNet (left) and our fine-tuned ControlNet (right) with the prompt: \u201cA profile portrait image of a person.\u201d\\n\\nFigure 6. Comparison between multiview generated samples, with truncation = 0.8, of EG3D model trained with various datasets.\"}"}
{"id": "CVPR-2024-696", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 5. Quantitative results of two state-of-the-art reenactment methods (TPS and LIA) when training on VoxCeleb1 and the combined set VoxCeleb1+EFHQ. We present results evaluated either on VoxCeleb1 (left) or EFHQ (right) test sets.\\n\\n| Method | Dataset | L1 | ArcFace | Glint360k | WebFace4M | ResNet18 | ResNet50 | ResNet101 |\\n|--------|---------|----|---------|-----------|-----------|---------|---------|----------|\\n| TPS VoxCeleb1 | 0.041 | 0.044 | 0.044 | 0.041 |\\n| LIA VoxCeleb1+EFHQ | 0.044 | 0.044 | 0.044 | 0.041 | 0.044 | 0.044 | 0.041 |\\n\\n5. Face Reenactment Subtask\\n\\n5.1. Evaluation Protocol\\n\\nWe follow the same preprocessing and evaluation protocol as FOMM [38]. Specifically, all the frames are cropped to 256\u00d7256, and we use the first frame of a clip as the source image to reconstruct the remaining frames. In the case of the EFHQ test set, most source images have a frontal pose, while the target frames have an extreme pose. Some test cases only contain extreme pose to evaluate self-reconstruction on these cases. We run the experiments using two recent methods, Thin-Plate Spline Motion Model (TPS) [51] and Latent Image Animator (LIA) [43]. For training details, please refer to the supplementary. We select the following three metrics to compare the generated frames and the driving ground truth:\\n\\n- **L1** measures the average distance among pixel values, indicating reconstruction faithfulness.\\n- **Average Keypoint Distance (AKD)** calculates the mean distance between keypoints, thus assessing the pose accuracy.\\n- **Average Euclidean Distance (AED)** measures identity preservation by computing the average L2 distance between identity embeddings.\\n\\n5.2. Experimental results\\n\\nWe compare TPS and LIA models trained with our supplementary EFHQ dataset (TPS VoxCeleb1+EFHQ and LIA VoxCeleb1+EFHQ) against models trained solely on VoxCeleb1 (TPS VoxCeleb1 and LIA VoxCeleb1). Tab. 5 displays quantitative results, revealing that models trained with EFHQ maintain comparable performance when evaluated on VoxCeleb1, emphasizing high-quality performance in frontal-to-frontal reenactment scenarios. Notably, on the EFHQ test set, both EFHQ-trained models show significant improvement in all metrics, indicating enhanced extreme pose and identity reenactment performance. In Fig. 8, qualitative results prove that EFHQ-trained models excel in transferring facial expressions and motion across diverse poses. Models trained on EFHQ showcase improved image quality, reduced artifacts, and well-preserved shapes of driving faces. These results highlight our models' effectiveness in capturing and reproducing facial motions across varying poses while enhancing fidelity to the original facial features.\\n\\n6. Face Recognition Subtask\\n\\n6.1. Evaluation Protocol\\n\\nTo evaluate face recognition performance on extreme poses, we selected two state-of-the-art methods: ArcFace [7] and AdaFace [18]. We further explored various backbone sizes (ResNet18, ResNet50, ResNet101) [10] and training datasets (MS1MV2 [9], Glint360K [2], WebFace4M [55]) of increasing size. For metrics, we used True Acceptance Rate (TAR) at False Acceptance Rate (FAR) of $1 \\\\times 10^{-3}$. \"}"}
{"id": "CVPR-2024-696", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8. Comparison of performance for frontal-profile face reenactment on EFHQ. From left to right: source image, TPS on VoxCeleb1, TPS with EFHQ, LIA on VoxCeleb1, LIA with EFHQ, driving image. Additional EFHQ training data improves synthesis of extreme poses.\\n\\n6.2. Experimental results\\n\\nBefore assessing face recognition networks on EFHQ, we first clean the commonly-used version of CPLFW, processed by , by removing misaligned images (please refer to the Supplementary), and evaluate the models on it. Models performed well on this cleansed set, with minimal gaps between ArcFace and AdaFace.\\n\\nOur proposed evaluation benchmark involves three pose-based scenarios. Results in Tab. reveal performance drops of 5-37% on the frontal-profile subset compared to frontal-frontal, highlighting the challenge of matching faces in different poses. Larger ResNet models outperformed ResNet18 across subsets, showcasing the advantages of increased capacity in handling pose variation. However, the ResNet100 model trained on Glint360K performed worse than smaller models on the profile-profile subset, indicating the need for sufficient profile data to prevent overfitting. Our results also indicate AdaFace's suitability for handling pose variations. In summary, pose mismatch brings significant challenges for face recognition, necessitating larger models trained on diverse data covering profile faces.\\n\\n7. Discussion\\n\\nFigure 9. User study results. We reported the percentage of times testers rank whose output is best or tied for each criterion. User survey. We conduct user surveys to evaluate the face generation improvement of our method through human perceptual assessment. In this survey, we compare the outputs of the models mentioned in Secs. and . The setup of this user survey is included in the supplementary. In all human evaluation results (Fig. 9), our models outperform previous ones, notably on identity and motion test of face reenactment models (74.80% and 64.22%, respectively).\\n\\nFigure 10. Representative examples of the raw LPFF dataset that got excluded from the final dataset, either from misdetections or pose filtering process. Red keypoints represent landmarks prediction of selected samples from LPFF, green keypoints represent landmarks prediction from our pipeline. Robustness of data processing pipeline. As shown in Fig. 10, our data processing pipeline can detect additional profile-view faces filtered out by the LPFF pipeline, likely due to misdetections or incorrect pose filtering. This confirms the robustness of our pipeline, and we hope it will enable the community to develop larger and higher-quality in-the-wild facial datasets.\\n\\n8. Conclusion\\n\\nThis work has introduced a large-scale, diverse facial dataset to address performance gaps between frontal and profile faces, powered by a novel and robust data processing pipeline. Crucially, we provided tailored sub-datasets for advancing essential tasks like face synthesis and reenactment. Moreover, our new face verification benchmark reveals gaps between techniques, granting valuable insights. Ultimately, we hope this high-quality, diverse facial dataset opens up new and exciting opportunities to push forward cross-pose tasks.\"}"}
{"id": "CVPR-2024-696", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Fix for triplane projection \u00b7 Issue #67 \u00b7 NVlabs/eg3d. https://github.com/NVlabs/eg3d/issues/67, 2023.\\n\\n[2] Xiang An, Xuhan Zhu, Yuan Gao, Yang Xiao, Yongle Zhao, Ziyong Feng, Lan Wu, Bin Qin, Ming Zhang, Debing Zhang, et al. Partial fc: Training 10 million identities on a single machine. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1445\u20131449, 2021.\\n\\n[3] Sergey Bezryadin, Pavel Bourov, and Dmitry Ilinih. Brightness calculation in digital image processing. International Symposium on Technologies for Digital Photo Fulfillment, 2007:10\u201315, 2007.\\n\\n[4] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh. Openpose: Realtime multi-person 2d pose estimation using part affinity fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019.\\n\\n[5] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, Tero Karras, and Gordon Wetzstein. Efficient geometry-aware 3D generative adversarial networks. In CVPR, 2022.\\n\\n[6] Feng-Ju Chang, Anh Tuan Tran, Tal Hassner, Iacopo Masi, Ram Nevatia, and Gerard Medioni. Faceposenet: Making a case for landmark-free face alignment. In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages 1599\u20131608, 2017.\\n\\n[7] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4690\u20134699, 2019.\\n\\n[8] Yu Deng, Jiaolong Yang, Sicheng Xu, Dong Chen, Yunde Jia, and Xin Tong. Accurate 3d face reconstruction with weakly-supervised learning: From single image to image set. In IEEE Computer Vision and Pattern Recognition Workshops, 2019.\\n\\n[9] Yandong Guo, Lei Zhang, Yuxiao Hu, Xiaodong He, and Jianfeng Gao. Ms-celeb-1m: A dataset and benchmark for large-scale face recognition. In Computer Vision\u2013ECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III, pages 87\u2013102. Springer, 2016.\\n\\n[10] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep Residual Learning for Image Recognition. In Proceedings of 2016 IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778. IEEE, 2016.\\n\\n[11] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by a two time-scale update rule converge to a local nash equilibrium. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2017.\\n\\n[12] Gary B Huang, Marwan Mattar, Tamara Berg, and Eric Learned-Miller. Labeled faces in the wild: A database for studying face recognition in unconstrained environments. In Workshop on faces in 'Real-Life' Images: detection, alignment, and recognition, 2008.\\n\\n[13] Yuge Huang, Yuhan Wang, Ying Tai, Xiaoming Liu, Pengcheng Shen, Shaoxin Li, Jilin Li, and Feiyue Huang. Curricularface: adaptive curriculum learning loss for deep face recognition. In proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 5901\u20135910, 2020.\\n\\n[14] Tero Karras, Samuli Laine, and Timo Aila. A style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 4401\u20134410, 2019.\\n\\n[15] Tero Karras, Miika Aittala, Janne Hellsten, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Training generative adversarial networks with limited data. In Proc. NeurIPS, 2020.\\n\\n[16] Tero Karras, Miika Aittala, Samuli Laine, Erik Harkonen, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Alias-free generative adversarial networks. In Proc. NeurIPS, 2021.\\n\\n[17] Minchul Kim, Anil K Jain, and Xiaoming Liu. Adaface: Quality adaptive margin for face recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022.\\n\\n[18] Davis E. King. Dlib-ml: A machine learning toolkit. J. Mach. Learn. Res., 10:1755\u20131758, 2009.\\n\\n[19] Brendan F Klare, Ben Klein, Emma Taborsky, Austin Blanton, Jordan Cheney, Kristen Allen, Patrick Grother, Alan Mah, and Anil K Jain. Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1931\u20131939, 2015.\\n\\n[20] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83\u201397, 1955.\\n\\n[21] Tuomas Kynkanniemi, Tero Karras, Samuli Laine, Jaakko Lehtinen, and Timo Aila. Improved precision and recall metric for assessing generative models. In Proceedings of the 33rd International Conference on Neural Information Processing Systems, Red Hook, NY , USA, 2019. Curran Associates Inc.\\n\\n[22] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML, 2023.\\n\\n[23] Ziwei Liu, Ping Luo, Xiaogang Wang, and Xiaoou Tang. Deep learning face attributes in the wild. In Proceedings of International Conference on Computer Vision (ICCV), 2015.\\n\\n[24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2017.\\n\\n[25] Camillo Lugaresi, Jiuqiang Tang, Hadon Nash, Chris McClanahan, Esha Uboweja, Michael Hays, Fan Zhang, Chuolung Chang, Ming Guang Yong, Juhyun Lee, Wan-Teh 22613 22613\"}"}
{"id": "CVPR-2024-696", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Chang, Wei Hua, Manfred Georg, and Matthias Grundmann. Mediapipe: A framework for building perception pipelines. CoRR, abs/1906.08172, 2019.\\n\\nBrianna Maze, Jocelyn Adams, James A Duncan, Nathan Kalka, Tim Miller, Charles Otto, Anil K Jain, W Tyler Niggel, Janet Anderson, Jordan Cheney, et al. Iarpa janus benchmark-c: Face dataset and protocol. In 2018 international conference on biometrics (ICB), pages 158\u2013165. IEEE, 2018.\\n\\nStylianos Moschoglou, Athanasios Papaioannou, Christos Sagonas, Jiankang Deng, Irene Kotsia, and Stefanos Zafeiriou. Agedb: the first manually collected, in-the-wild age database. In proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 51\u201359, 2017.\\n\\nArsha Nagrani, Joon Son Chung, and Andrew Zisserman. VoxCeleb: A large-scale speaker identification dataset. In Interspeech 2017. ISCA, 2017.\\n\\nAaron Nech and Ira Kemelmacher-Shlizerman. Level playing field for million scale face recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2017.\\n\\nYouxin Pang, Yong Zhang, Weize Quan, Yanbo Fan, Xiaodong Cun, Ying Shan, and Dong-Ming Yan. Dpe: Disentanglement of pose and expression for general video portrait editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 427\u2013436, 2023.\\n\\nAditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv e-prints, pages arXiv\u20132204, 2022.\\n\\nRobin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj\u00f6rn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10684\u201310695, 2022.\\n\\nSoumyadip Sengupta, Jun-Cheng Chen, Carlos Castillo, Vishal M Patel, Rama Chellappa, and David W Jacobs. Frontal to profile face verification in the wild. In 2016 IEEE winter conference on applications of computer vision (WACV), pages 1\u20139. IEEE, 2016.\\n\\nSefik Ilkin Serengil and Alper Ozpinar. Lightface: A hybrid deep face recognition framework. In 2020 Innovations in Intelligent Systems and Applications Conference (ASYU), pages 23\u201327. IEEE, 2020.\\n\\nSefik Ilkin Serengil and Alper Ozpinar. Hyperextended lightface: A facial attribute analysis framework. In 2021 International Conference on Engineering and Emerging Technologies (ICEET), pages 1\u20134. IEEE, 2021.\\n\\nSefik Ilkin Serengil and Alper Ozpinar. Hyperextended lightface: A facial attribute analysis framework. In 2021 International Conference on Engineering and Emerging Technologies (ICEET), pages 1\u20134. IEEE, 2021.\\n\\nAliaksandr Siarohin, St\u00e9phane Lathuili\u00e8re, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems, 32, 2019.\\n\\nL Sirovich and M Kirby. Low-dimensional procedure for the characterization of human faces. Journal of the Optical Society of America A, 4(3):519\u2013524, 1987.\\n\\nShaolin Su, Qingsen Yan, Yu Zhu, Cheng Zhang, Xin Ge, Jinqiu Sun, and Yanning Zhang. Blindly assess image quality in the wild guided by a self-adaptive hyper network. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.\\n\\nJiale Tao, Biao Wang, Borun Xu, Tiezheng Ge, Yuning Jiang, Wen Li, and Lixin Duan. Structure-aware motion transfer with deformable anchor model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3637\u20133646, 2022.\\n\\nMatthew A Turk and Alex P Pentland. Face recognition using eigenfaces. In Proceedings. 1991 IEEE computer society conference on computer vision and pattern recognition, pages 586\u2013587. IEEE Computer Society, 1991.\\n\\nYaohui Wang, Di Yang, Francois Bremond, and Antitza Dantcheva. Latent image animator: Learning to animate images via latent space navigation. In ICLR 2022-The International Conference on Learning Representations, 2022.\\n\\nCameron Whitelam, Emma Taborsky, Austin Blanton, Brianna Maze, Jocelyn Adams, Tim Miller, Nathan Kalka, Anil K Jain, James A Duncan, Kristen Allen, et al. Iarpa janus benchmark-b face dataset. In proceedings of the IEEE conference on computer vision and pattern recognition workshops, pages 90\u201398, 2017.\\n\\nL Wolf, T Hassner, and I Maoz. Face recognition in unconstrained videos with matched background similarity. In Proceedings of the 2011 IEEE Conference on Computer Vision and Pattern Recognition, pages 529\u2013534, 2011.\\n\\nCho-Ying Wu, Qiangeng Xu, and Ulrich Neumann. Synergy between 3dmm and 3d landmarks for accurate 3d facial geometry. In 2021 International Conference on 3D Vision (3DV), 2021.\\n\\nYiqian Wu, Jing Zhang, Hongbo Fu, and Xiaogang Jin. Lpff: A portrait dataset for face generators across large poses. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 20327\u201320337, 2023.\\n\\nLiangbin Xie, Xintao Wang, Honglun Zhang, Chao Dong, and Ying Shan. Vfhq: A high-quality dataset and benchmark for video face super-resolution. In The IEEE Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2022.\\n\\nLvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3836\u20133847, 2023.\\n\\nZhimeng Zhang, Lincheng Li, Yu Ding, and Changjie Fan. Flow-guided one-shot talking face generation with a high-resolution audio-visual dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3661\u20133670, 2021.\\n\\nJian Zhao and Hui Zhang. Thin-plate spline motion model for image animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2023.\"}"}
{"id": "CVPR-2024-696", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Tianyue Zheng and Weihong Deng. Cross-pose LFW: A database for studying cross-pose face recognition in unconstrained environments. Beijing University of Posts and Telecommunications, Tech. Rep., 5(7), 2018.\\n\\nHuayi Zhou, Fei Jiang, and Hongtao Lu. DirectMHP: Direct 2D multi-person head pose estimation with full-range angles. arXiv preprint arXiv:2302.01110, 2023.\\n\\nHao Zhu, Wayne Wu, Wentao Zhu, Liming Jiang, Siwei Tang, Li Zhang, Ziwei Liu, and Chen Change Loy. CelebV-HQ: A large-scale video facial attributes dataset. In ECCV, 2022.\\n\\nZheng Zhu, Guan Huang, Jiankang Deng, Yun Ye, Junjie Huang, Xinze Chen, Jiagang Zhu, Tian Yang, Jiwen Lu, Dalong Du, and Jie Zhou. Webface260m: A benchmark unveiling the power of million-scale deep face recognition. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2021.\"}"}
