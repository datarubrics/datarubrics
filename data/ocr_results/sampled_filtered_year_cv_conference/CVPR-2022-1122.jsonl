{"id": "CVPR-2022-1122", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Neil Alldrin, Todd Zickler, and David Kriegman. Photo-\\nmetric stereo with non-parametric and spatially-varying re-\\nfectance. In IEEE Conference on Computer Vision and Pat-\\ntern Recognition (CVPR), pages 1\u20138, 2008.\\n\\n[2] Feras Almasri and Olivier Debeir. Multimodal sensor fu-\\nsion in single thermal image super-resolution. In Springer\\nAsian Conference on Computer Vision (ACCV), pages 418\u2013\\n433, 2018.\\n\\n[3] Kosala Bandara, Thomas R\u00a8uberg, and Fehmi Cirak. Shape\\noptimisation with multiresolution subdivision surfaces and\\nimmersed finite elements. Elsevier Computer Methods in\\nApplied Mechanics and Engineering, 300:510\u2013539, 2016.\\n\\n[4] Fausto Bernardini, Joshua Mittleman, Holly Rushmeier,\\nClaudio Silva, and Gabriel Taubin. The ball-pivoting algo-\\nrithm for surface reconstruction. IEEE Transactions on Vi-\\nsualization and Computer Graphics, 5(4):349\u2013359, 1999.\\n\\n[5] Hanting Chen, Yunhe Wang, Tianyu Guo, Chang Xu, Yip-\\ning Deng, Zhenhua Liu, Siwei Ma, Chunjing Xu, Chao Xu,\\nand Wen Gao. Pre-trained image processing transformer. In\\nIEEE Conference on Computer Vision and Pattern Recogni-\\ntion (CVPR), pages 12299\u201312310, 2021.\\n\\n[6] Ian Cherabier, Christian H\u00a8ane, Martin R Oswald, and Marc\\nPollefeys. Multi-label semantic 3D reconstruction using\\nvoxel blocks. In IEEE International Conference on 3D Vi-\\nsion (3DV), pages 601\u2013610, 2016.\\n\\n[7] Tao Dai, Jianrui Cai, Yongbing Zhang, Shu-Tao Xia, and\\nLei Zhang. Second-order attention network for single im-\\nage super-resolution. In IEEE Conference on Computer Vi-\\nsion and Pattern Recognition (CVPR), pages 11065\u201311074,\\n2019.\\n\\n[8] Xin Deng, Yutong Zhang, Mai Xu, Shuhang Gu, and Yiping\\nDuan. Deep coupled feedback network for joint exposure\\nfusion and image super-resolution. IEEE Transactions on\\nImage Processing, 30:3098\u20133112, 2021.\\n\\n[9] Chao Dong, Chen Change Loy, Kaiming He, and Xiaoou\\nTang. Learning a deep convolutional network for image\\nsuper-resolution. In Springer European Conference on Com-\\nputer Vision (ECCV), pages 184\u2013199, 2014.\\n\\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,\\nDirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,\\nMostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-\\nvain Gelly, et al. An image is worth 16x16 words: Trans-\\nformers for image recognition at scale. arXiv preprint\\narXiv:2010.11929, 2020.\\n\\n[11] Yutong Feng, Yifan Feng, Haoxuan You, Xibin Zhao, and\\nYue Gao. Meshnet: Mesh neural network for 3d shape rep-\\nresentation. In AAAI Conference on Artificial Intelligence,\\nvolume 33, pages 8279\u20138286, 2019.\\n\\n[12] Lin Gao, Jie Yang, Tong Wu, Yu-Jie Yuan, Hongbo Fu, Yu-\\nKun Lai, and Hao Zhang. Sdm-net: Deep generative net-\\nwork for structured deformable mesh. ACM Transactions on\\nGraphics, 38(6):1\u201315, 2019.\\n\\n[13] Yong Guo, Jian Chen, Jingdong Wang, Qi Chen, Jiezhang\\nCao, Zeshuai Deng, Yanwu Xu, and Mingkui Tan. Closed-\\nloop matters: Dual regression networks for single image\\nsuper-resolution. In IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR), pages 5407\u20135416, 2020.\\n\\n[14] Rana Hanocka, Amir Hertz, Noa Fish, Raja Giryes, Shachar\\nFleishman, and Daniel Cohen-Or. Meshcnn: a network with\\nan edge. ACM Transactions on Graphics, 38(4):1\u201312, 2019.\\n\\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.\\nDelving deep into rectifiers: Surpassing human-level per-\\nformance on imagenet classification. In IEEE International\\nConference on Computer Vision (ICCV), pages 1026\u20131034,\\n2015.\\n\\n[16] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation net-\\nworks. In IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 7132\u20137141, 2018.\\n\\n[17] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Accurate\\nimage super-resolution using very deep convolutional net-\\nworks. In IEEE Conference on Computer Vision and Pattern\\nRecognition (CVPR), pages 1646\u20131654, 2016.\\n\\n[18] Jiwon Kim, Jung Kwon Lee, and Kyoung Mu Lee. Deeply-\\nrecursive convolutional network for image super-resolution.\\nIn IEEE Conference on Computer Vision and Pattern Recog-\\nnition (CVPR), pages 1637\u20131645, 2016.\\n\\n[19] Zorah Lahner, Daniel Cremers, and Tony Tung. Deepwrin-\\ntekles: Accurate and realistic clothing modeling. In Springer\\nEuropean Conference on Computer Vision (ECCV), pages\\n667\u2013684, 2018.\\n\\n[20] Wei-Sheng Lai, Jia-Bin Huang, Narendra Ahuja, and Ming-\\nHsuan Yang. Deep laplacian pyramid networks for fast and\\naccurate super-resolution. In IEEE Conference on Com-\\nputer Vision and Pattern Recognition (CVPR), pages 624\u2013\\n632, 2017.\\n\\n[21] Christian Ledig, Lucas Theis, Ferenc Husz\u00b4ar, Jose Caballero,\\nAndrew Cunningham, Alejandro Acosta, Andrew Aitken,\\nAlykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-\\nrealistic single image super-resolution using a generative ad-\\nversarial network. In IEEE Conference on Computer Vision\\nand Pattern Recognition (CVPR), pages 4681\u20134690, 2017.\\n\\n[22] Yawei Li, Vagia Tsiminaki, Radu Timofte, Marc Pollefeys,\\nand Luc Van Gool. 3d appearance super-resolution with deep\\nlearning. In IEEE Conference on Computer Vision and Pat-\\ntern Recognition (CVPR), pages 9671\u20139680, 2019.\\n\\n[23] Bee Lim, Sanghyun Son, Heewon Kim, Seungjun Nah, and\\nKyoung Mu Lee. Enhanced deep residual networks for sin-\\ngle image super-resolution. In IEEE Conference on Com-\\nputer Vision and Pattern Recognition (CVPR), pages 136\u2013\\n144, 2017.\\n\\n[24] Hsueh-Ti Derek Liu, Vladimir G Kim, Siddhartha Chaud-\\nhuri, Noam Aigerman, and Alec Jacobson. Neural subdivi-\\nsion. arXiv preprint arXiv:2005.01819, 2020.\\n\\n[25] Charles Loop and Scott Schaefer. Approximating catmull-\\nclark subdivision surfaces with bicubic patches. ACM Trans-\\nactions on Graphics, 27(1):1\u201311, 2008.\\n\\n[26] Luqing Luo, Lulu Tang, Wanyi Zhou, Shizheng Wang, and\\nZhi-Xin Yang. Pu-eva: An edge-vector based approximation\\nsolution for flexible-scale point cloud upsampling. In IEEE\\nInternational Conference on Computer Vision (ICCV), pages\\n16208\u201316217, 2021.\"}"}
{"id": "CVPR-2022-1122", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Ben Niu, Weilei Wen, Wenqi Ren, Xiangde Zhang, Lianping Yang, Shuzhen Wang, Kaihao Zhang, Xiaochun Cao, and Haifeng Shen. Single image super-resolution via a holistic attention network. In Springer European Conference on Computer Vision (ECCV), pages 191\u2013207. Springer, 2020.\\n\\nHyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation. In IEEE international Conference on Computer Vision (ICCV), pages 1520\u20131528, 2015.\\n\\nCharles R Qi, Hao Su, Kaichun Mo, and Leonidas J Guibas. Pointnet: Deep learning on point sets for 3d classification and segmentation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 652\u2013660, 2017.\\n\\nGuocheng Qian, Abdulellah Abualshour, Guohao Li, Ali Thabet, and Bernard Ghanem. Pu-gcn: Point cloud upsampling using graph convolutional networks. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 11683\u201311692, 2021.\\n\\nYvain Qu\u00e9au, Jean-Denis Durou, and Jean-Fran\u00e7ois Aujol. Normal integration: A survey. Springer Journal of Mathematical Imaging and Vision, 60(4):576\u2013593, 2018.\\n\\nJonas Schult, Francis Engelmann, Theodora Kontogianni, and Bastian Leibe. Dualconvmesh-net: Joint geodesic and euclidean convolutions on 3d meshes. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 8612\u20138622, 2020.\\n\\nBoxin Shi, Zhipeng Mo Mo, Zhe Wu, Dinglong Duan, Sai-Kit Yeung, and Ping Tan. A benchmark dataset and evaluation for non-lambertian and uncalibrated photometric stereo. IEEE Transactions on Pattern Analysis and Machine Intelligence, 41(2):271\u2013284, 2019.\\n\\nWenzhe Shi, Jose Caballero, Ferenc Husz\u00e1r, Johannes Totz, Andrew P Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1874\u20131883, 2016.\\n\\nAssaf Shocher, Nadav Cohen, and Michal Irani. \\\"zero-shot\\\" super-resolution using deep internal learning. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 3118\u20133126, 2018.\\n\\nJun Sun, Yakun Chang, Cheolkon Jung, and Jiawei Feng. Multi-modal reflection removal using convolutional neural networks. IEEE Signal Processing Letters, 26(7):1011\u20131015, 2019.\\n\\nAshish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems (NeurIPS), pages 5998\u20136008, 2017.\\n\\nXintao Wang, Ke Yu, Chao Dong, and Chen Change Loy. Recovering realistic texture in image super-resolution by deep spatial feature transform. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 606\u2013615, 2018.\\n\\nXintao Wang, Ke Yu, Shixiang Wu, Jinjin Gu, Yihao Liu, Chao Dong, Yu Qiao, and Chen Change Loy. Esrgan: Enhanced super-resolution generative adversarial networks. In Springer European Conference on Computer Vision (ECCV) workshops, pages 1\u201316, 2018.\\n\\nHuikai Wu, Junge Zhang, and Kaiqi Huang. Point cloud super resolution with adversarial residual graph networks. arXiv preprint arXiv:1908.02111, 2019.\\n\\nWuyuan Xie, Yunbo Zhang, Charlie CL Wang, and Ronald C-K Chung. Surface-from-gradients: An approach based on discrete geometry processing. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2195\u20132202, 2014.\\n\\nFuzhi Yang, Huan Yang, Jianlong Fu, Hongtao Lu, and Bainbing Guo. Learning texture transformer network for image super-resolution. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 5791\u20135800, 2020.\\n\\nLequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and Pheng-Ann Heng. Ec-net: an edge-aware point set consolidation network. In Springer European Conference on Computer Vision (ECCV), pages 386\u2013402, 2018.\\n\\nLequan Yu, Xianzhi Li, Chi-Wing Fu, Daniel Cohen-Or, and Pheng-Ann Heng. Pu-net: Point cloud upsampling network. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2790\u20132799, 2018.\\n\\nMeng Zhang, Tuanfeng Wang, Duygu Ceylan, and Niloy J Mitra. Deep detail enhancement for any garment. In Computer Graphics Forum, volume 40, pages 399\u2013411, 2021.\\n\\nYulun Zhang, Kunpeng Li, Kai Li, Lichen Wang, Bineng Zhong, and Yun Fu. Image super-resolution using very deep residual channel attention networks. In Springer European Conference on Computer Vision (ECCV), pages 286\u2013301, 2018.\\n\\nYulun Zhang, Yapeng Tian, Yu Kong, Bineng Zhong, and Yun Fu. Residual dense network for image super-resolution. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2472\u20132481, 2018.\\n\\nJun-Yan Zhu, Richard Zhang, Deepak Pathak, Trevor Darrell, Alexei A Efros, Oliver Wang, and Eli Shechtman. Multi-modal image-to-image translation by enforcing bi-cycle consistency. In Advances in Neural Information Processing Systems (NeurIPS), pages 465\u2013476, 2017.\"}"}
{"id": "CVPR-2022-1122", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Multi-head attention\\nK\\nQ\\nV\\nMLP\\nTransformer Encoder\\nTransformer Encoder\\ncmTA Block\\ncmTA Block\\nV V\\nNorm & Pos\\nNorm\\ncmTA Block\\n\\nFigure 4. Cross-modality transformer alignment (cmTA).\\n\\nSince these extracted hierarchical texture features may contain rich color and structural information, their high-frequency information is more complicated than the original normal map. By introducing the texture normal map as a bridge, the cmTA module can capture more texture features, and project them to the normal domain. Similarly, for the depth feature \\\\( F_p \\\\), since a depth image not only carries the shape information but also carries the reconstructed position information, it can assist in reconstructing the low-frequency of a 3D surface in the shape normal map domain.\\n\\nFirstly, a cmTA module can be considered as the combination of several cmTA blocks and two modality adapters. Each cmTA block uses the transformer structure to align one modality feature with a bridge normal feature, and produces an aligned feature (e.g., average image feature \\\\( F_a \\\\) aligned with the texture normal feature \\\\( F_{an} \\\\)).\\n\\n\\\\[ F_x^n = B_{cmTA}(F_x, F_\\\\delta), \\\\]\\n\\nwhere \\\\( x \\\\) denotes one of the candidate modality feature, and \\\\( \\\\delta \\\\) denotes the corresponding bridge normal feature. It is noted that when \\\\( x \\\\) denotes a texture image, \\\\( \\\\delta \\\\) means a texture normal feature. Similarly, when \\\\( x \\\\) denotes a depth image, \\\\( \\\\delta \\\\) means a shape normal feature.\\n\\nIn Eq. (9), \\\\( B_{cmTA} \\\\) denotes a cmTA block. Inspired by the efficiency of a self-attention mechanism \\\\([37]\\\\) in capturing global information, the proposed cmTA module consists of several transformer encoders similar to \\\\([10]\\\\). Since we expect that the proposed deep network can capture the cross-modality features and map them to the bridge normal features, the cmTA block is organized in the following recursive structure, and defined in Eq. (10).\\n\\n\\\\[ F_{i+1}^x = (B_{i}^{cmE}(q, k = F_\\\\delta, v = F_x^n)), i = 1 \\\\]\\n\\n\\\\[ B_{i}^{cmE}(q, k = F_\\\\delta, v = F_x^n), n \\\\geq i > 1, \\\\]\\n\\nwhere \\\\( B_i^{cmE}(q, k, v) \\\\) denotes the \\\\( i \\\\)th cross-modality encoder (cmE), and \\\\((q, k, v)\\\\) denotes the self-attention layer paradigms (query, key, and value). Inside a cmE, each feature map is cropped to 9 patches with the position embedding. Subsequently, using the self-representation in term of self-attention, we treat the bridge normal and modality features as one, and adopt a multi-head attention mechanism to learn the representation between them. By using multiple recursive structures in Eq. (10), the bridge normal feature is repeatedly connected to the \\\\( k \\\\) and \\\\( q \\\\) inputs of the cmE module via the skip connection structure. In this case, the proposed network expects to capture the relevant information between different modalities, and gradually align the related information in normal domain.\\n\\nAfter the cmTA module, each modality feature pair would jointly generate a cross-modality feature. However, as mentioned above, a hierarchical texture map contains some unfavorable information, thus we further adopt two modality adapters to reduce the relevant unfavorable information. The modality adapters use the channel attention (CA) \\\\([16]\\\\) followed by one convolution block to model the importance relationship between different channels. Specifically, it will produce three texture modalities (\\\\( F_{an}, F_{ln}, \\\\) and \\\\( F_{dn} \\\\)) from a cmTA module. Then, we concatenate them together as \\\\( F_x^n \\\\in \\\\mathbb{R}^{3f \\\\times h \\\\times w} \\\\), and use a CA layer followed by a 1 \\\\( \\\\times \\\\) 1 convolution block to distill the most important texture information \\\\( F_{tn} \\\\in \\\\mathbb{R}^{f \\\\times h \\\\times w} \\\\). After the process of these two modality adapters, two aligned texture feature \\\\( F_{tn} \\\\) and shape feature \\\\( F_{sn} \\\\) are generated, namely side-modality features.\\n\\n3.4. Cross-modality Affine Fusion (cmAF)\\n\\nAfter the cross-modality alignment, side-modality features are fused into the main-modality to assist the target SR feature representation. Inspired by the spatial feature transform mechanism \\\\([38]\\\\) which takes a segmentation probability map as a prior, we propose the cmAF module, \\\\( \\\\mathcal{M}_{cmAF} \\\\), to fuse the extracted texture and shape features into the backbone of the SR network step by step. As shown in Fig. 5, cmAF can be separated into the side-modality affine fusion stage and the main-modality affine fusion stage.\\n\\nIn the side-modality affine fusion stage, the CA layer firstly is used to fuse \\\\( F_{sn} \\\\) and \\\\( F_{tn} \\\\), which aims to produce the side-stream feature \\\\( F_{ss} \\\\), representing the joint guidance combined both the texture and shape information. As mentioned earlier, the shape texture is lack of the detail information, but it has a strong relationship with the surface vertex position and structure. We then adopt a 3 \\\\( \\\\times \\\\) 3 convolution block to distill a general shape feature from \\\\( F_{sn} \\\\in \\\\mathbb{R}^{f \\\\times h \\\\times w} \\\\) to \\\\( \\\\mathbb{R}^{1 \\\\times h \\\\times w} \\\\). Based on the fact that a texture feature map contains more information than a shape feature map, the 12707 12707\"}"}
{"id": "CVPR-2022-1122", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"A texture feature map can provide more detailed shifting information, and should not be distilled. Thus, we use a $3 \\\\times 3$ convolution block to obtain a general shifting feature from $F_{tn} \\\\in \\\\mathbb{R}^{f \\\\times h \\\\times w}$ to $\\\\mathbb{R}^{f \\\\times h \\\\times w}$. Finally, $F_{ss}$ will be pixel-wisely multiplied by a scaling map, and then added to the shifting map. The side-modality affine fusion in Fig. 5 can be formulated as\\n\\n$$F_{sf} = F_{ss} \\\\otimes C_E(F_{sn}) \\\\oplus C_F E(F_{tn}),$$\\n\\n(11)\\n\\nwhere $F_{sf}$ denotes the side feature. $\\\\otimes$ and $\\\\oplus$ refer to the element-wise multiplication and addition, respectively. $C_x E(\\\\cdot)$ denotes a convolution module consisting of one BN layer and one ReLU activation layer, which aims to make the output stable and easier to use.\\n\\nAfter obtaining the side feature $F_{sf}$, we further employ a convolution module to learn another affine transform to fuse different modality features. Since the information of $F_{sf}$ has been heavily distilled, we use two convolution blocks with one ReLU layer to prepare the corresponding affine shifting and scaling maps for the main-modality feature $F_{sr}$ which is defined in Eq. (12).\\n\\n$$F_{i+1}^{sr} = F_{i}^{sr} \\\\otimes C_F(F_{sf}) \\\\oplus C_F(F_{sf})$$\\n\\n(12)\\n\\nwhere $F_{i}^{sr}$ denotes the $i$th main-modality feature in different stages of the SR side branch, whose dimension is the same as $F_{sf}$. $C_F(\\\\cdot)$ denotes a linear structure of two convolution layers and one ReLU layer. As shown in Fig. 2, this cmAF module will be repeated several times in order to fully fuse and leverage the cross-modality information. Finally, the resulted features will be fed into our upscale module as the rest part of Eq. (5).\\n\\n### 4. Experimental Results\\n\\n#### 4.1. Experimental Protocols\\n\\n**Dataset descriptions.** The training of MNSRNet requires high-resolution labels. Currently, the most widely-used photometric stereo datasets do not have enough images for the multimodality training, such as the DiLiGenT dataset (10 objects) [33] and the Gourd & Apple dataset (3 objects) [1]. Therefore, we have established a new photometric stereo dataset, namely WPS (wonderful photometric stereo). WPS contains 400 different objects, including butterfly wings, leaves, oil paintings, handicrafts, etc. Each object is captured under 18 predefined lighting conditions as shown in Fig. 6.\\n\\n**To fairly evaluate the performance of the proposed MNSRNet, the testing dataset is composed of DiLiGenT, Gourd & Apple, and 80 objects selected from WPS. Note that the rest of objects in WPS are only used for training (e.g., 9(training):1(validation)). Both the training and testing data are down-sampled with the Bicubic (BI) degradation by $\\\\times 1/2$ and $\\\\times 1/4$ to generate the LR images as the network inputs.**\"}"}
{"id": "CVPR-2022-1122", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 7. Visual comparisons of 3D surface super-resolution between 10 methods under the $\\\\times 4$ setting. For a better comparison, the region in the red box is zoomed in the 2nd-13th columns. \u201cFull\u201d means the original surface, \u201cLR\u201d means the down-sampled object surface, and \u201cGT\u201d means the ground-truth. Please zoom in the electronic version for better details.\\n\\nImplementation details. MNSRNet has been implemented in PyTorch, and the Adam optimizer is used with default parameters ($\\\\beta_1 = 0.9$ and $\\\\beta_2 = 0.999$). For the SR branch, we use 20 groups of cmAF. We have trained MNSRNet using a mini-batch size of 8 for 1000 epochs with an Nvidia Tesla A100 GPU, which takes about two days and nights. Since the transformer module needs a fixed input size, all the input images are adaptively cropped. For example, in $\\\\times 4$ scale, the HR and LR image patches are 196 $\\\\times$ 196 and 48 $\\\\times$ 48, respectively. All of the trained weights for each layer are initialized by the Kaiming distribution [15], and the bias is initialized as a constant. We do not apply any special data augmentation methods except for the random rotation ($90^\\\\circ$, $180^\\\\circ$, and $270^\\\\circ$) and the horizontal flip.\\n\\n4.2. Performance Comparisons\\n\\nComparison methods. We have compared our MNSRNet with 9 representative methods, which can be categorized into four groups: mesh-based method (denoted by \\\"Mesh\\\"), point cloud-based method (denoted by \\\"Points\\\"), SISR-based methods (denoted by \\\"SISR\\\"), and MISR-based methods (denoted by \\\"MISR\\\").\\n\\nFor Mesh methods, Catmull-Clark subdivision (C-C) [25] has been the most widely-used mesh subdivision method. It can efficiently up-sample a triangular mesh by the heuristic algorithms. We have used the implementation version built in Blender for comparison.\\n\\nFor Points methods, we choose the PU-GCN network [30] to represent the SR task for point clouds. In the experiments, we convert the related meshes into point clouds for PU-GCN, perform the up-sampling, and re-convert it into meshes for comparison [4].\\n\\nFor SISR methods, we choose EDSR [23] to represent a residual learning structure, RCAN [46] to represent a convolutional attention structure, and IPT [5] to represent a self-attention structure. For these methods, we have fine-tuned the corresponding models on WPS to show their best performance.\\n\\nFor MISR methods, we choose TDSRGAN [2] to represent an early fusion method, SFT-GAN [38] and 3DASR [22] to represent a hybrid fusion method, and TDTN [8] to represent a hybrid fusion method with the self-attention structure. It is noted that our task cannot fully provide the modalities needed in the original methods, and we have adjusted the above methods to fit for our WPS benchmark.\\n\\nQualitative results. Fig. 7 demonstrates the visual comparisons of some representative 3D objective surfaces. For SISR, benefiting from the powerful natural image pre-training model, some methods still can perform well after fine-tuning on our WPS dataset. However, since these SISR methods have not considered the cross-modal information, they are not sufficient to obtain the best visual quality. For MISR, they may not take full advantage of the additional multimodal information. As a result, they may even have negative effects (e.g., heavily aliased surfaces) due to inter-modal differences. Visually, the proposed method achieves a promising subjective quality with enough surface details and geometry structures.\\n\\nQuantitative results. Table 1 summarizes the detailed average results on the hybrid testing dataset, including DiLiGenT, Gourd & Apple, and WPS. Specifically, our method achieves all 8 of the first-best results in terms of PSNR, SSIM, MEAN, MID, V AR, $5^\\\\circ$, $10^\\\\circ$, and MRDE on the $\\\\times 2$ setting, and achieves 6 of the first-best results and 2 of the second-best results on the $\\\\times 4$ setting. Experiments show that better results are obtained without the negative effects of instability caused by the multimodal inputs and information confusion between modalities. MNSRNet outperforms the existing methods in most cases. The main reason can be that our method can employ more cross-modality\"}"}
{"id": "CVPR-2022-1122", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. The average comparison results between 10 state-of-the-art methods on the hybrid testing datasets. \u201c[+]\u201d means the higher the better, and \u201c[-]\u201d means the lower the better. The first-best is highlighted by bold, and the second-best is highlighted by underline.\\n\\n| Scale | Type | Algorithm | PSNR [+] | SSIM [+] | MEAN [-] | MID [-] | VAR [-] |\\n|-------|------|-----------|----------|----------|----------|--------|--------|\\n| 5\u00b0    | Points | PU-GCN [30] | 18.5747  | 0.7382   | 16.0982  | 12.3674 | 252.8209 |\\n|       | Mesh   | C-C [25]   | 22.1881  | 0.9010   | 9.0866   | 4.7667  | 162.4632 |\\n| 10\u00b0   | Points | EDSR [23]  | 27.5593  | 0.9522   | 5.1324   | 2.0439  | 157.7664 |\\n|       | Mesh   | RCAN [46]  | 27.7209  | 0.9535   | 5.0834   | 1.9327  | 153.1603 |\\n| 15\u00b0   | Points | IPT [5]    | 27.9756  | 0.9545   | 4.9149   | 1.7689  | 166.2459 |\\n|       | Mesh   | TDSRGAN [2] | 26.1982 | 0.9434   | 6.3183   | 2.6441  | 182.8687 |\\n| 20\u00b0   | Points | SFT-GAN [38] | 27.3630  | 0.9509   | 5.3260   | 2.1304  | 162.4754 |\\n|       | Mesh   | 3DASR [22] | 28.3017  | 0.9581   | 4.6702   | 1.7837  | 149.0149 |\\n| 25\u00b0   | Points | Ours       | 28.7662  | 0.9605   | 4.4277   | 1.6312  | 146.0815 |\\n|       | Mesh   | C-C [25]   | 20.9022  | 0.8539   | 11.8487  | 6.5019  | 352.5511 |\\n| 30\u00b0   | Points | EDSR [23]  | 23.0609  | 0.8909   | 9.1407   | 3.7643  | 323.7065 |\\n|       | Mesh   | RCAN [46]  | 23.6058  | 0.9024   | 8.7591   | 3.5196  | 344.9552 |\\n| 35\u00b0   | Points | IPT [5]    | 23.6268  | 0.9041   | 8.2695   | 2.8598  | 335.1193 |\\n|       | Mesh   | TDSRGAN [2] | 22.4461 | 0.8819   | 10.5153  | 4.7397  | 345.8229 |\\n| 40\u00b0   | Points | SFT-GAN [38] | 22.7683  | 0.8816   | 10.0431  | 4.7215  | 330.2237 |\\n|       | Mesh   | 3DASR [22] | 22.9138  | 0.8901   | 9.2341   | 3.6517  | 349.1082 |\\n| 45\u00b0   | Points | Ours       | 23.7961  | 0.9053   | 7.9945   | 2.9255  | 302.6197 |\\n\\nTable 2. Ablation experiments on the proposed cross-modality alignment and fusion methods.\\n\\n| MPS | cmTA | cmAF | PSNR [+] | SSIM [+] | MEAN [-] | MRDE [-] |\\n|-----|------|------|----------|----------|----------|---------|\\n| \u2713   | \u2713    | \u00d7    | 22.3302  | 0.8752   | 10.7182  | 11.7245 |\\n| \u00d7   | \u2713    | \u00d7    | 23.2812  | 0.8949   | 8.9813   | 7.2617  |\\n| \u2713   | \u2713    | \u00d7    | 23.4994  | 0.8995   | 8.7082   | 7.2021  |\\n| \u2713   | \u00d7    | \u2713    | 23.6824  | 0.9043   | 8.2285   | 6.6091  |\\n| \u2713   | \u2713    | \u2713    | 23.7961  | 0.9053   | 7.9945   | 6.4827  |\\n\\nIn the experiment, the replacement of MPS is to use the lightest texture image as the texture modality, and the other two modalities remain unchanged. The replacement of cmTA is to simply concatenate all the related modality information, and then use three 3 \u00d7 3 convolution layers and one 1 \u00d7 1 convolution layer to shrink the intermediate channels. The replacement of cmAF is a combination of the CA layer and 1 \u00d7 1 convolution layer to fuse the side-modality to the main-modality. Experiments show that when all three modules are used, the best results can be achieved.\\n\\n4.3. Ablation Study\\n\\nMNSRNet contains three main modules for the multi-modality learning, such as MPS, cmTA, and cmAF. To verify the effectiveness of these modules, we further conduct additional experiments on the DiLiGenT dataset with the \u00d74 setting. Five independent experiments are conducted as shown in Table 2, where the related module selected (not selected) is represented by the symbol \u201c\u2713\u201d(\u201c\u00d7\u201d).\\n\\nTo demonstrate the effects of a single modality, we have conducted the additional experiments as provided in Table 3. As seen, both the texture and depth modalities can effectively improve the performance of the surface super-resolution.\\n\\n5. Conclusion\\n\\nIn this paper, we have introduced a multimodal-based super-resolution network for 3D object surface in 2D normal domain. More specifically, we jointly considered the texture, depth, and normal modalities to restore high-quality surface details and preserve geometry structures. To effectively utilize the cross-modality information, we extracted two bridge normal maps as a cross-modality alignment guidance. Based on the texture and depth modalities, we have developed a cross-modality transformer alignment (cmTA) module to connect different modalities. In addition, we explored a cross-modality affine fusion (cmAF) module to fuse the features from the main network branch and the extracted side modalities. Finally, we reconstructed an enhanced 3D object surface from the recovered high-resolution normal map. Experimental results on different benchmark datasets demonstrate the effectiveness of the proposed approach in qualitatively and quantitatively.\"}"}
{"id": "CVPR-2022-1122", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"MNSRNet: Multimodal Transformer Network for 3D Surface Super-Resolution\\n\\nWuyuan Xie, Tengcong Huang\\n\\nCollege of Computer Science and Software Engineering, Shenzhen University\\n\\nwuyuan.xie@gmail.com\\n\\nMiaohui Wang*\\n\\nGuangdong Key Laboratory of Intelligent Information Processing, Shenzhen University\\n\\nwang.miaohui@gmail.com\\n\\nAbstract\\n\\nWith the rapid development of display technology, it has become an urgent need to obtain realistic 3D surfaces with as high-quality as possible. Due to the unstructured and irregular nature of 3D object data, it is usually difficult to obtain high-quality surface details and geometry textures at a low cost. In this article, we propose an effective multimodal-driven deep neural network to perform 3D surface super-resolution in 2D normal domain, which is simple, accurate, and robust to the above difficulty. To leverage the multimodal information from different perspectives, we jointly consider the texture, depth, and normal modalities to simultaneously restore fine-grained surface details as well as preserve geometry structures. To better utilize the cross-modality information, we explore a two-bridge normal method with a transformer structure for feature alignment, and investigate an affine transform module for fusing multimodal features. Extensive experimental results on public and our newly constructed photometric stereo dataset demonstrate that the proposed method delivers promising surface geometry details compared with nine competitive schemes.\\n\\n1. Introduction\\n\\nWith the increasing improvements of the capability and demand in the sensing and analyzing of real-world objects, more and more 3D vision-based applications require the input of high-quality object surface [11, 43]. However, most current 3D acquisition devices do not provide high-quality 3D data. In view of this practical difficulty, it is desirable to develop low-cost computer vision methods to enhance the acquisition quality for 3D data collectors.\\n\\nIntuitively, the most straightforward way to improve the quality of an acquired 3D surface data is to directly perform the up-sampling operation in 3D domain. The existing studies can be classified as voxel-based, point cloud-based, and mesh-based methods according to the representation of a 3D surface. 1) The voxel-based methods [6] have been used in 3D surface processing for many years, which commonly have high requirements for equipment and computation. 2) Point cloud is the most simple way to represent a 3D object, which has been directly up-sampled [26, 40, 44] based on a special convolutional neural network (CNN) structure [29]. Due to the intrinsic irregularity of point cloud, it is difficult to achieve dense and high-quality 3D surface enhancement results. 3) Mesh-based methods, as the most wildly-used 3D representation, have been studied based on mesh subdivision and vertex interpolation [3]. With the development of deep neural networks, mesh-based CNN structures [12, 14, 32] have inspired several data-driven methods for the up-sampling operation on mesh-based 3D surfaces [24]. Nevertheless, these traditional schemes can only optimize some mathematical properties of the mesh data, while learning-based methods face the problem of large amounts of insufficient data.\\n\\nDue to the aforementioned difficulties in improving surface quality in 3D domain, some preliminary investigations have aimed to enhance the surface quality in 2D domain. By\\n\\n*This work was supported in part by the National Natural Science Foundation of China (No. 61902251 and No. 61701310), in part by Natural Science Foundation of Shenzhen City (No. 20200805200145001 and No. JCYJ20180305124209486), and in part by Natural Science Foundation of Guangdong Province (No. 2019A1515010961 and No. 2021A1515011877). (Corresponding author: Miaohui Wang)\"}"}
{"id": "CVPR-2022-1122", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"representing 3D surface in 2D domain using normals and displacements in the field of physical cloth enhancement [19], the related 3D surface has been indirectly up-sampled through 2D image super-resolution (SR) algorithms [45]. This kind of strategy can avoid a high computational complexity, which is also benefited from well developed 2D image SR techniques. However, these existing methods in 2D domain usually only explore a single modality, which is lack of utilizing the multimodal attributes of 3D objects to further improve the performance of up-sampling.\\n\\nInspired by the above discussions, we present a multi-modal transformer network for 3D surface super-resolution by jointly considering the texture, depth, and normal modalities as shown in Fig. 1. More specifically, the texture, depth, and normal data are obtained from a low-resolution 3D object surface. Then, the texture and depth modalities are firstly aligned by a transformer network to the normal modality, and the related side features are fused into the main SR backbone network. Finally, a fine-grained 3D object surface is reconstructed by the enhanced normal map.\\n\\nTo sum up, there are three main contributions compared with the previous approaches:\\n\\n\u2022 To better utilize the modality information acquired by camera sensors, we investigate a novel multimodal-driven surface super-resolution network (denoted as \u201cMNSRNet\u201d) to fuse the texture and depth modalities so as to enhance a 3D object surface in 2D domain.\\n\\n\u2022 To capture the auxiliary modality information more easily, the original texture photographs are divided into hierarchical texture representations in multimodal pre-processing stage (MPS). Further, we design a new cross-modality transformer alignment (cmTA) module to align auxiliary modality information, and explore a cross-modality affine fusion (cmAF) module based on affine transform mechanism to fuse the intermediate features.\\n\\n\u2022 Due to the lack of multimodality training data, we have also established a new photometric stereo dataset 1 which consists of 400 objects. Extensive experimental results on public and our newly constructed datasets demonstrate that the proposed method achieves superior performance compared with 9 competitive methods.\\n\\n2. Related Work\\n\\nIn this section, we briefly review some representative image-based SR methods, including single image super-resolution (SISR) and multimodal image super-resolution (MISR), because the proposed 3D surface super-resolution framework is mainly conducted on 2D normal image.\\n\\n2.1. Single Image Super-resolution\\n\\nCNN-based SISR method [9] has been widely developed in the past few years. By introducing the residual learning, Kim et al. introduced VDSR [17] and DRCN [18] to ease the training difficulty. Lim et al. proposed EDSR [23] to cut some unnecessary CNN modules, and established a deeper network. To handle unknown degradation, Shocher et al. developed a zero-shot learning network [35]. With the success of self-attention mechanism in the field of natural language processing (NLP), the transformer-based structure has been studied [5, 42]. Besides, some other useful modules have been also introduced in SISR, such as Laplacian pyramid structure [20], dense residual structure [47], generative adversarial network (GAN) [21, 39], attention mechanism [7, 27, 46], dual regression network [13], etc.\\n\\nThe existing SISR methods have achieved promising results on natural RGB images. However, the normal image is totally different from the RGB image, where a normal pixel represents the geometry information. For instance, two adjacent normal pixels may be completely different, and there is lack of the characteristics of smooth magnitude changes in an RGB image. In view of this, it is necessary to develop new approaches for 3D surface super-resolution in 2D normal domain.\\n\\n2.2. Multimodal Image Super-resolution\\n\\nThe idea of combining multimodal information, (e.g., different view-points, different sensors, different domains), is a popular research topic in computer vision [36, 48]. In MISR, some researchers have employed multimodal information to enhance the reconstruction performance. For instance, Almasri et al. [2] adopted a high-resolution image information to up-sample a thermal image obtained by the thermal camera. Wang et al. [38] utilized the image segmentation map as a prior information to improve the learning performance of the GAN model. Li et al. [22] employed a normal image to guide the super-resolution of the texture image. Deng et al. [8] introduced two images with different exposures to perform the SR task.\\n\\nIt demonstrates that MISR has been studied in some preliminary investigations, exploration of these methods for 3D object surface super-resolution based on multimodality is still in its infancy, partly due to the difficulty of identifying suitable multimodal descriptors to represent the distinct features of 3D surface. To our knowledge, there are few methods to consider multimodal information in up-sampling a 3D surface in 2D domain. This is the fundamental motivation of this study.\"}"}
{"id": "CVPR-2022-1122", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Overview of the proposed multimodal super-resolution network for 3D object surface in 2D domain. It mainly consists of the multimodal pre-processing stage, cross-modality transformer alignment, and cross-modality affine fusion. The SR normal map is reconstructed as an enhanced 3D surface by surface-from-normal in photometric stereo.\\n\\nSince we propose to represent the 3D geometry surface in 2D normal domain, it becomes a normal image super-resolution problem. Therefore, the overall task can be formulated as the optimization of minimizing a specially-designed distance between the SR normal map $N_{sr}$ and the ground-truth normal map $N_{gt}$.\\n\\n$$\\\\min_{N_{sr}} (L_{overall}(N_{sr}, N_{gt})),$$\\n\\nwhere $L_{overall}(\\\\cdot)$ represents the special distance which can be expressed as a weighted sum of the normal pixel loss $L_{pix}$ and the normal angle loss $L_{nor}$. Then, we have\\n\\n$$L_{overall}(N_{sr}, N_{gt}) = \\\\lambda_{pix} L_{pix}(N_{sr}, N_{gt}) + \\\\lambda_{nor} L_{nor}(N_{sr}, N_{gt}),$$\\n\\nwhere $(h, w, c)$ represents the height, width, and channel of a predicted normal image. $L_{pix}$ represents a pixel-wise $L_1$ loss, which is commonly used in SISR to accelerate the training convergence. $L_{nor}$ represents the cosine similarity to restrict the angle loss between the predicted normal $n_{i,j}$ and the ground-truth normal $\\\\tilde{n}_{i,j}$.\\n\\nTraining with the balance of these two loss measures, our model achieves the minimum reconstruction error in practice.\\n\\nArchitecture. Previous studies have witnessed the positive effect of multimodal data in the SR task [2]. In light of this, we adopt three modalities in photometric stereo, including the texture, depth, and normal images. The texture and depth images are obtained under different lighting conditions at the same view.\\n\\nThe overall of the proposed network architecture is shown in Fig. 2, and formulated as\\n\\n$$N_{sr} = M_{SR}(N_{lr}, M_{EX}(I_{mul})).$$\\n\\n$I_{mul}$ represents the raw multimodal information, including multi-lighting texture images, depth image, and low-resolution (LR) normal image. Generally, Eq. (3) can be decomposed into two sub-tasks: multimodal feature extraction $M_{EX}$, and multimodal super-resolution $M_{SR}$.\\n\\nThe multimodal feature extraction stage, $M_{EX}$, consists of the MPS and $cmTA$ modules. $I_{mul}$ is firstly processed by the MPS module to produce side modality features used to bridge the related normal maps. Then, these resulted features are fed to the $cmTA$ module, $M_{cmTA}$, which has a transformer structure acted as a feature encoder, aligning and extracting intermediate features from different modalities.\\n\\n$M_{EX}(\\\\cdot)$ can be represented by\\n\\n$$F_{tn}, F_{sn} = M_{cmTA}(SMPS(I_{mul}, D_{p}, N_{lr})),$$\\n\\nwhere $F_{tn}$ and $F_{sn}$ are the aligned side modality features. $(I_{m}, D_{p}, N_{lr})$ represents the multi-lighting photographs, depth image, and LR normal image, respectively.\\n\\nAfter this cross-modality alignment, several $cmAF$ blocks (formed a $cmAF$ sequence) are employed to fuse the side modality features and the main modality features together. Subsequently, the fused feature maps are fed to an up-sampling module, which consists of one upscale block, two $3 \\\\times 3$ convolution layers, one vector normalization module, and one Bicubic interpolation module connected from the beginning for the residual learning.\\n\\n$M_{SR}(\\\\cdot)$ can be formulated as\\n\\n$$N_{sr} = \\\\phi(M_{UP}(M_{cmAFs}(F_{lr}, F_{tn}, F_{sn}))),$$\\n\\nwhere $F_{lr}$ denotes a main modal feature starting with the shallow features of the LR normal map extracted by three convolutional layers. $M_{cmAFs}(\\\\cdot)$ denotes a $cmAF$ sequence, and $M_{UP}(\\\\cdot)$ represents a upscale block. $\\\\phi(\\\\cdot)$ denotes the combination of convolutional layer, vector normalization, and Bicubic interpolation. The vector normalization layer limits the output normal to the unit length, and...\"}"}
{"id": "CVPR-2022-1122", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Multi-lighting photos\\n\\nBrightness Correction\\n\\nExtract\\nHierarchical\\n\\nTexture Images\\n\\nShape\\n\\nNormal\\n\\nExtraction\\n\\nTexture Normal\\n\\nExtraction\\n\\n\\n\\nOriginal LR Normal Map\\n\\nDepth Map\\n\\nFigure 3. Multimodal pre-processing stage (MPS).\\n\\nThe up-sampling module enlarges a feature map to the output size. There are some choices in the up-sampling module, such as deconvolution [28] and pixel shuffle [34]. For a simple demonstration, we adopt [34] as our upscale block in the experiments.\\n\\n3.2. Multimodal Pre-processing Stage (MPS)\\n\\nIn the MPS module, we focus on two main problems: 1) how to reduce the data distribution differences of the side modalities between different datasets, and 2) how to establish a correlation relationship between the side modality and main modality. For the first problem, we extract the hierarchical texture representations from multiple lighting photographs. For the second problem, we extract two bridge normal maps to connect the side modality information in normal domain. The overall pipeline of the proposed MPS is shown in Fig. 3.\\n\\nHierarchical texture.\\n\\nDue to the diversity of the target object materials and surface geometry structures, uncertainty of sensor, and lighting conditions, the raw multi-lighting photographs may contain many unfavorable issues, such as exposure errors, shadows from self-obscuring, specular reflection, and uneven brightness due to different reflection intensities. However, those misleading noise data also contains useful information. To fully utilize this kind of information, we first calculate a pixel-wise darkest texture \\\\( I_d \\\\) to capture self-obscuring structures and under-exposure textures. Then, the lightest image \\\\( I_l \\\\) is extracted to capture the non-Lambertian reflection and over-exposure texture information. Finally, a pixel-wise average image \\\\( I_a \\\\) is extracted to represent the texture modality less affected by those unfavorable issues.\\n\\nSince the brightness of these hierarchical textures can vary greatly and is not friendly used in the model training, we propose to adjust the brightness to the same value as much as possible. The brightness correction in Eq. (6) is done by calculating a shifting bias, and then the brightness is aligned to the maximum value without overflowing the maximum pixel magnitude.\\n\\n\\\\[\\nI = I' + \\\\max(\\\\min(\\\\beta, 1 - \\\\max(I'))), -\\\\min(I')),\\n\\\\]\\n\\nwhere \\\\( \\\\beta = \\\\mu - I' \\\\) denotes a shifting bias, \\\\( \\\\mu \\\\) denotes the overall average value in the training dataset. \\\\( I' \\\\) and \\\\( I \\\\) represents the hierarchical texture maps before and after the correction, respectively.\\n\\nBridge normal maps.\\n\\nAs aforementioned, a surface normal image is very different from a natural RGB image. In such a case, the hierarchical texture images may contain some unfavorable information, which indicates that the side modalities may be inconsistent or misaligned to the main modality. Thus, we propose to use the texture normal map \\\\( N_t \\\\) and the shape normal map \\\\( N_s \\\\) as bridges between depth and normal, and texture and normal, respectively.\\n\\nInspired by the observation that a depth image is lack of the detail information but it contains a rough shape information and the position relationship of a given surface, we generate a shape normal map \\\\( N_s \\\\) by average filtering the normal map with the window size \\\\( 3 \\\\times 3 \\\\) and 100 times. The shape normal map can be reconstructed as a blurry surface, which is used to represent a rough object shape. Since the depth and shape normal maps have the similar structures, we use the shape normal map \\\\( N_s \\\\) in Eq. (7) as a guidance to align features from a depth image to the normal modality in the following cmTA module.\\n\\n\\\\[\\nN_s = \\\\text{conv}(N, \\\\kappa_{\\\\text{ave}}),\\n\\\\]\\n\\nwhere \\\\( \\\\text{conv}(\\\\cdot) \\\\) represents the convolution operation, and \\\\( \\\\kappa_{\\\\text{ave}} \\\\) denotes an average filter kernel.\\n\\nSimilarly, we are looking forward to a hierarchical texture that can represent the pure texture information without the shape interference. To obtain the texture normal map \\\\( N_t \\\\), we propose to compute a directional bias between the original normal and the shape normal. This computation is illustrated in Fig. 3, and formulated as\\n\\n\\\\[\\nN_t = \\\\text{rot}(N_{lr}|<N_s, z>),\\n\\\\]\\n\\nwhere \\\\( \\\\text{rot}(\\\\cdot) \\\\) denotes a rotate manipulation, \\\\( <\\\\cdot, \\\\cdot> \\\\) denotes an element-wise rotation, and \\\\( z = [0, 0, 1] \\\\) represents the \\\\( z \\\\)-axis direction. The texture normal map \\\\( N_t \\\\) contains less shape information, which flattens the reconstructed surface. \\\\( N_t \\\\) represents the high-frequency detail information of a given surface, which is similar to the extracted texture image without the shape information. Consequently, we propose to use the texture normal map as a guidance to align the texture modality from the RGB domain to the normal domain.\\n\\n3.3. Cross-modality Transformer Alignment (cmTA)\\n\\nTo align the above cross-modality information, we further design a cross-modality transformer alignment (cmTA) module as shown in Fig. 4. Before cmTA, all the input multimodalities are passed through three \\\\( 3 \\\\times 3 \\\\) convolutional layers to extract the related shallow feature \\\\( F_x \\\\) (i.e., \\\\( x = \\\\{a, l, d\\\\} \\\\)).\"}"}
