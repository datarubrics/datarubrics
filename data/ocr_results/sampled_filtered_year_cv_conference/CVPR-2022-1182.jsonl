{"id": "CVPR-2022-1182", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Modeling sRGB Camera Noise with Normalizing Flows\\nShayan Kousha, Ali Maleky, Michael S. Brown, Marcus A. Brubaker\\n\\nAbstract\\nNoise modeling and reduction are fundamental tasks in low-level computer vision. They are particularly important for smartphone cameras relying on small sensors that exhibit visually noticeable noise. There has recently been renewed interest in using data-driven approaches to improve camera noise models via neural networks. These data-driven approaches target noise present in the raw-sensor image before it has been processed by the camera's image signal processor (ISP). Modeling noise in the RAW-rgb domain is useful for improving and testing the in-camera denoising algorithm; however, there are situations where the camera's ISP does not apply denoising or additional denoising is desired when the RAW-rgb domain image is no longer available. In such cases, the sensor noise propagates through the ISP to the final rendered image encoded in standard RGB (sRGB). The nonlinear steps on the ISP culminate in a significantly more complex noise distribution in the sRGB domain and existing raw-domain noise models are unable to capture the sRGB noise distribution. We propose a new sRGB-domain noise model based on normalizing flows that is capable of learning the complex noise distribution found in sRGB images under various ISO levels. Our normalizing flows-based approach outperforms other models by a large margin in noise modeling and synthesis tasks. We also show that image denoisers trained on noisy images synthesized with our noise model outperform those trained with noise from baselines models.\\n\\n1. Introduction\\nModeling and reducing noise are long-standing problems in computer vision and image processing with a rich history (e.g., [10, 17, 18]). While simple models, like simple additive white Gaussian noise (AWGN), have often been used in testing denoising methods, they are well-known not to be realistic and serves only as a rough approximation for real-world camera noise. When realistic noise models are needed, more sophisticated models, such as Poisson-Gaussian [8] or Heteroscedastic Gaussian models [7, 19], are used to model the noise distribution observed on camera sensors. While such models are more realistic than AWGN, they too are often not able to fully capture real camera noise distributions.\\n\\nIn recent years, data-driven noise models have been proposed that learn the noise distribution directly from large datasets of noisy sensor images (e.g., [2, 4, 9, 20, 21, 30, 31]). These methods focus on modeling the noise present in the raw sensor images. Modeling noise in the RAW-rgb domain is useful as denoising algorithms are typically applied by the camera's image signal processor (ISP) hardware. Such noise reduction is applied early in the ISP's processing pipeline (often in the Bayer processing stages).\"}"}
{"id": "CVPR-2022-1182", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. sRGB noise variance as a function of clean image intensity on SIDD [1]. These plots exhibit highly variable and unpredictable behavior, in contrast to RAW where there is typically a linear relation between noise variance and intensity. Consequently, we cannot use traditional image noise models like AWGN and NLF and instead propose a novel sRGB focused noise model based on normalizing flows.\\n\\nIt is not unusual, however, for the in-camera denoising to be disabled, not present or insufficient. Further, most cameras do not save in RAW-rgb by default, making sRGB images far more ubiquitous. In such cases, the noisy sensor image is rendered through the camera's ISP, including the photo-finishing algorithms that apply complex, nonlinear operations to manipulate the image's tonal and color values. The resulting noise distribution in the final sRGB is notably more complex than in the unprocessed RAW-rgb space and existing RAW-rgb focused noise models become ineffective for modeling the sRGB noise, as shown in Fig. 1.\\n\\nContributions.\\n\\nWe focus on modeling and synthesizing sRGB image noise, where the in-camera nonlinear processing has altered the noise characteristics from that of the camera's sensor. We begin with an analysis that shows that existing noise models targeting sensor noise in the RAW-rgb domain are not well suited for sRGB images. We then propose a generative model that combines recent advances in normalizing flows and captures effects of different gain (ISO) settings and camera types on sRGB image noise. We show that our sRGB noise model is superior to several baseline noise models. We further investigate our model's ability to synthesize noise by training a denoiser using noisy images sampled from the model. We show that this denoiser achieves significantly higher performance compared to denoisers trained on synthesized data from baseline models.\\n\\n2. Related work\\n\\nAdditive white Gaussian noise (AWGN) [22, 25, 29] has long been used to model image noise. However, it is well known that real camera noise is non-Gaussian, in part because it fails to capture signal-dependence of the variance. A common and more realistic model is the heteroscedastic Gaussian model [19], defined as:\\n\\n$$N \\\\sim N(0, \\\\beta_1 I + \\\\beta_2)$$\\n\\nwhere $\\\\beta_1, \\\\beta_2 > 0$ are parameters that model the signal-dependent and signal-independent nature of noise observed on real camera sensors. Some cameras include a manufacturer-calibrated heteroscedastic Gaussian noise model in their saved RAW-rgb images, encoded in the DNG format [1, 2], although recent work [32] has suggested that such models are often not well calibrated. The heteroscedastic Gaussian noise model is relatively simple and has few parameters; however, it is still only an approximation of the real sensor noise [1, 7, 12, 24, 28].\\n\\nResearchers have recently begun exploring data-driven approaches. For example, Abdelhamed et al. [2] proposed the Noise Flow model which combined the domain knowledge of signal and gain dependence with the expressiveness of learning-based generative models based on normalizing flows to capture more complex components of the noise. While the Noise Flow model was effective in simulating RAW-rgb noise, it relied on assumptions that do not apply in the sRGB color space. For example, the Noise Flow model builds off the heteroscedastic Gaussian model, which assumes the noise variance linearly depends on the underlying clean image intensity. However, this assumption no longer applies in the sRGB image domain (see Fig. 2) due to subsequent non-linear and potentially content-dependent processing of the RAW-rgb image. As a result, in our experiments we show that simply applying the Noise Flow model to sRGB data fails to capture the noise distribution.\\n\\nThere have been relatively few attempts to model noise from sRGB data. Nam et al. [21] introduced a model that is designed for the specific use case of modeling noise and other degradations caused by JPEG compression. More recently, the C2N [13] model attempted to model noise using unpaired clean and noisy images using a generative adversarial network (GAN).\"}"}
{"id": "CVPR-2022-1182", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. A typical camera ISP processing pipeline. This pipeline processes the RAW-rgb image to encode it in the sRGB domain. The nonlinear steps in the photo-finishing stages substantially complicate the noise distribution in the sRGB domain challenging.\\n\\nIn this paper, we introduce a data-driven model for image noise in the sRGB domain that conditions on camera settings and the underlying clean image. Like Noise Flow [2], the model is built using normalizing flows [6, 15, 16] but avoids making unrealistic assumptions that apply only to RAW-rgb. The resulting model is shown to have state-of-the-art noise modeling capabilities for sRGB noise. Further, when training a denoiser using noisy samples from the proposed noise model, we show that the resulting denoiser significantly out-performs those trained with existing sRGB noise models.\\n\\n3. Preliminaries\\n\\nImage noise is a combination of degradations introduced by multiple sources in the imaging process and physical limitations of sources like camera sensors. Many of these sources occur when first capturing the RAW-rgb image by the camera sensors. Subsequently, the RAW-rgb image (including noise) is subject to in-camera imaging process that transforms the image from the scene-referred RAW-rgb color space to the display-referred sRGB color space. Fig. 3 summarizes some of the main steps in the in-camera imaging pipeline. The results of the photo-finishing steps, many nonlinear in nature, introduce new sources of noise (e.g., from clipping and sharpening) while amplifying and distorting the original sensor's noise distribution. The noise from these multiple sources can be characterized as:\\n\\n\\\\[ \\\\tilde{I} = I + N, \\\\]\\n\\nwhere \\\\( \\\\tilde{I} \\\\) is the observed, noisy image, \\\\( I \\\\) is the true, underlying clean image, and \\\\( N \\\\) is the noise whose distribution may depend on \\\\( I \\\\). In this work, we aim to design a generative model that captures the complexity of noise introduced by all sources.\\n\\nNormalizing Flows\\n\\nNormalizing flows are a family of generative models that have gained popularity in recent years. Due to their formulation, they admit both efficient sampling and exact evaluation of probability density in contrast to other generative models, like GANs and VAEs [3, 16]. Additionally, flow-based models do not suffer from issues like mode and posterior collapse that are commonly faced when training GANs and VAEs.\\n\\nBelow we briefly introduce normalizing flows. We refer the reader to recent review articles [16, 23] for a more extensive treatment. A normalizing flow consists of differentiable and bijective functions that learn a transformation \\\\( z = f(x|\\\\Theta) \\\\) with parameters \\\\( \\\\Theta \\\\) called a flow. A flow transforms data samples \\\\( x \\\\in \\\\mathbb{R}^d \\\\) from a complex distribution, \\\\( p_X \\\\), to some base space \\\\( z \\\\in \\\\mathbb{R}^d \\\\) with a known and tractable distribution and probability density function, \\\\( p_Z \\\\).\\n\\nHere, as is common, we will assume that \\\\( p_Z \\\\) takes the form of an isotropic Gaussian distribution with unit variance. The probability density function in the data space can then be found using the change of variables formula\\n\\n\\\\[ p_X(x) = p_Z(f(x|\\\\Theta)) | \\\\det Df(x|\\\\Theta)|, \\\\]\\n\\nwhere \\\\( Df(x|\\\\Theta) \\\\) is the Jacobian matrix of \\\\( f \\\\) at \\\\( x \\\\). The result is a model that, given a dataset \\\\( D = \\\\{x_i\\\\}_{i=1}^M \\\\), can be trained by using stochastic gradient descent to minimize the negative log likelihood of the data\\n\\n\\\\[ -\\\\sum_{i=1}^M \\\\log p_Z(f(x_i|\\\\Theta)) + \\\\log | \\\\det Df(x_i|\\\\Theta)|, \\\\]\\n\\nwith respect to the parameters \\\\( \\\\Theta \\\\). Samples can be generated by sampling from the base distribution \\\\( z \\\\sim p_Z \\\\) and then applying the inverse flow \\\\( x = f^{-1}(z) \\\\).\\n\\nFormally, normalizing flows define distributions over continuous spaces. To apply them to quantized data (e.g., as sRGB data which is typically truncated to 256 intensity levels) some attention must be paid to avoid a degeneracy [26] that occurs when fitting continuous density models to discrete data. Here we use uniform dequantization, which adds uniformly sampled noise to the images during training. More complex forms of dequantization are possible [11]. Constructing expressive, differentiable, and bijective functions is the primary research problem in normalizing flows, and there have been many attempts at this; see [16, 23] for a thorough review. A flow \\\\( f \\\\) is typically constructed by the composition of simpler flows\u2014namely, \\\\( f = f_1 \\\\circ ... \\\\circ f_{N-1} \\\\circ f_N \\\\)\u2014since the composition of bijective functions is itself bijective. Analogous to adding depth in a neural network, composing flows can increase the complexity of the resulting distribution \\\\( p_X \\\\). Individual flows are typically constructed such that their inverse and Jacobian determinant are easily calculated. Next we review two common forms of bijection that we will use.\"}"}
{"id": "CVPR-2022-1182", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Affine Coupling Flows\\n\\nAffine coupling flows are a simple, efficient and widely used form of flow. They work by splitting the input dimensions, \\\\( x = (x_A, x_B) \\\\), into two disjoint subsets, \\\\( x_A \\\\) and \\\\( x_B \\\\). Then, one subset, \\\\( x_A \\\\), is unmodified but used to compute scale and translation factors, which are applied to the other subset, \\\\( x_B \\\\). Formally, an affine coupling layer is defined as \\\\( y = (y_A, y_B) \\\\), where \\\\( y_A = x_A \\\\) and \\\\( y_B = x_B \\\\odot f_s(x_A | \\\\Theta) + f_t(x_A | \\\\Theta) \\\\).\\n\\nThe functions \\\\( f_s \\\\) and \\\\( f_t \\\\) compute the scale and translation factors and can be arbitrary, for example, deep neural networks. The inverse of this layer is easily computed as \\\\( x_B = (y_B - f_t(x_A | \\\\Theta)) \\\\odot f_s(x_A | \\\\Theta) \\\\).\\n\\nThe log-determinant of this transformation is efficiently calculated as \\\\( \\\\sum \\\\log f_s(x_A | \\\\Theta) \\\\), where the sum is taken over the output dimensions of \\\\( f_s \\\\).\\n\\n1x1 Convolution\\n\\nCoupling layers must change the way dimensions are split between layers. This can be done by a random permutations but the Glow model introduced the use of 1x1 convolutions as an invertible transformation. In essence, these layers are full linear transformations applied channel-wise to the inputs. The inverse is simply the inverse linear transformation applied channel-wise and the log determinant term is the number of pixels times the log determinant of the linear transformation.\\n\\nFigure 4. Real noise standard deviation changes as the sensitivity of the camera's sensor increases. The camera type is another factor affecting the noise behavior. For example, the noise standard deviation of images captured by Galaxy S6 is the highest under almost all ISO levels. Analysis done on the SIDD.\\n\\n4. Normalizing Flows for sRGB Noise\\n\\nHere we introduce our model of sRGB noise based on normalizing flows. Our analysis on the SIDD dataset, summarized in Fig. 2, confirms that real noise in the sRGB domain has a complex structure that is not captured by the standard, heteroscedastic-based noise models and varies significantly between cameras and even color channels. Inspired by this analysis, we introduce two new conditional flows in the following sections that allow the noise model to be conditioned on critical parameters, such as camera model, gain setting and clean intensity.\\n\\nFigure 5 shows the proposed model architecture. Here we describe it as a transformation from the data (i.e., a noisy, observed sRGB image \\\\( \\\\tilde{I} \\\\)) to the base space \\\\( z \\\\). First, input images are dequantized with uniform dequantization as mentioned above. Unlike RAW-rgb, which is typically represented as a floating point number, sRGB data is typically quantized to 256 intensity levels. Note that, because both the clean and observed images have been quantized, both need to be dequantized. Next, the clean image, \\\\( I \\\\), is subtracted from the observed image, \\\\( \\\\tilde{I} \\\\), to get the noise image, \\\\( N \\\\). This is then followed by \\\\( S \\\\) flow blocks, which are responsible for learning a transformation from the noise to a sample in the base distribution, and vice versa. These flow blocks consist of one conditional linear (CL) flow followed by \\\\( K \\\\) conditional coupling steps (CCS), where a conditional coupling step consists of one invertible 1x1 convolution layer and one conditional affine coupling transformation. In our experiments, we use \\\\( S = 4 \\\\) and \\\\( K = 2 \\\\), unless otherwise specified. Next, we describe the conditional linear flow and conditional affine coupling layers.\\n\\n4.1. Conditional Linear Flow\\n\\nThe nature of the noise and the subsequent non-linear processing is heavily determined by the specific camera and gain (or ISO) settings used. To account for this, we introduce a linear flow layer, which is conditioned on the camera, \\\\( c \\\\), and gain setting, \\\\( g \\\\), of the camera. This has the form \\\\( y = x \\\\odot f_s(c, g) + f_t(c, g) \\\\), where \\\\( \\\\odot \\\\) is the element-wise product, \\\\( x \\\\) is the input, \\\\( y \\\\) is the output, and \\\\( f_s \\\\) and \\\\( f_t \\\\) are functions that output the scale and translation factors. The functions \\\\( f_s \\\\) and \\\\( f_t \\\\) can be arbitrarily complex and have no constraints other than that \\\\( f_s \\\\neq 0 \\\\). See supplemental materials for architectural details of \\\\( f_s \\\\) and \\\\( f_t \\\\). The inverse of this layer is easily calculated as:\\n\\n\\\\[\\nx = (y - f_t(c, g)) \\\\odot f_s(c, g),\\n\\\\]\\n\\nwhere \\\\( \\\\odot \\\\) is element-wise division. The log-determinant is given by \\\\( \\\\sum \\\\log f_s(c, g) \\\\), where the sum is taken over all dimensions of the input.\\n\\n4.2. Conditional Affine Coupling\\n\\nThis layer is an extension of the affine coupling layer presented above. To capture the complex dependence of the noise distribution on both the underlying clean image and the camera and gain settings (see Figs. 2 and 4), we extend the coupling layers to take these values as input. The conditional affine coupling layer is similar to the standard affine\"}"}
{"id": "CVPR-2022-1182", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] A. Abdelhamed, S. Lin, and M. S. Brown, \u201cA high-quality denoising dataset for smartphone cameras,\u201d in CVPR, 2018.\\n\\n[2] A. Abdelhamed, M. A. Brubaker, and M. S. Brown, \u201cNoise Flow: Noise Modeling with Conditional Normalizing Flows,\u201d in ICCV, 2019.\\n\\n[3] S. Bond-Taylor, A. Leach, Y. Long, and C. G. Willcocks, \u201cDeep generative modelling: A comparative review of vaes, gans, normalizing flows, energy-based and autoregressive models,\u201d arXiv preprint arXiv:2103.04922, 2021.\\n\\n[4] C. Chen, Z. Xiong, X. Tian, and F. Wu, \u201cDeep boosting for image denoising,\u201d in ECCV, 2018.\\n\\n[5] L. Dinh, D. Krueger, and Y. Bengio, \u201cNice: Non-linear independent components estimation,\u201d in ICLR Workshop, 2015.\\n\\n[6] L. Dinh, J. Sohl-Dickstein, and S. Bengio, \u201cDensity estimation using real nvp,\u201d in ICLR, 2017.\\n\\n[7] A. Foi, \u201cClipped noisy images: Heteroskedastic modeling and practical denoising,\u201d Signal Processing, vol. 89, pp. 2609\u20132629, 2009.\\n\\n[8] A. Foi, M. Trimeche, V. Katkovnik, and K. Egiazarian, \u201cPractical poissonian-gaussian noise modeling and fitting for single-image raw-data,\u201d TIP, vol. 17, no. 10, pp. 1737\u20131754, 2008.\\n\\n[9] S. Guo, Z. Yan, K. Zhang, W. Zuo, and L. Zhang, \u201cTowards convolutional blind denoising of real photographs,\u201d in CVPR, 2019.\\n\\n[10] G. E. Healey and R. Kondepudy, \u201cRadiometric ccd camera calibration and noise estimation,\u201d TPAMI, vol. 16, no. 3, pp. 267\u2013276, 1994.\\n\\n[11] J. Ho, X. Chen, A. Srinivas, Y. Duan, and P. Abbeel, \u201cFlow++: Improving flow-based generative models with variational dequantization and architecture design,\u201d in ICML, 2019.\\n\\n[12] G. C. Holst, CCD Arrays, Cameras, and Displays. SPIE Optical Engineering Press, USA, second edition, 1996.\\n\\n[13] G. Jang, W. Lee, S. Son, and K. M. Lee, \u201cC2n: Practical generative noise modeling for real-world denoising,\u201d in ICCV, 2021.\\n\\n[14] D. P. Kingma and J. Ba, \u201cAdam: A method for stochastic optimization,\u201d in ICLR, 2015.\\n\\n[15] D. P. Kingma and P. Dhariwal, \u201cGlow: Generative flow with invertible 1x1 convolutions,\u201d in NeurIPS, 2018.\\n\\n[16] I. Kobyzev, S. J. Prince, and M. A. Brubaker, \u201cNormalizing flows: An introduction and review of current methods,\u201d TPAMI, vol. 43, no. 11, p. 3964\u20133979, 2021.\\n\\n[17] D. T. Kuan, A. A. Sawchuk, T. C. Strand, and P. Chavel, \u201cAdaptive noise smoothing filter for images with signal-dependent noise,\u201d TPAMI, vol. 7, no. 2, pp. 165\u2013177, 1985.\\n\\n[18] C. Liu, R. Szeliski, S. Bing Kang, C. L. Zitnick, and W. T. Freeman, \u201cAutomatic estimation and removal of noise from a single image,\u201d TPAMI, vol. 30, no. 2, pp. 299\u2013314, 2008.\\n\\n[19] X. Liu, M. Tanaka, and M. Okutomi, \u201cPractical signal-dependent noise parameter estimation from a single noisy image,\u201d TIP, vol. 23, no. 10, pp. 4361\u20134371, 2014.\\n\\n[20] Y. Liu, S. Anwar, L. Zheng, and Q. Tian, \u201cGradnet image denoising,\u201d in CVPR Workshop, 2020.\\n\\n[21] S. Nam, Y. Hwang, Y. Matsushita, and S. J. Kim, \u201cA holistic approach to cross-channel image noise modeling and its application to image denoising,\u201d in CVPR, 2016.\\n\\n[22] N. Ohta, \u201cA statistical approach to background subtraction for surveillance systems,\u201d in ICCV, 2001.\\n\\n[23] G. Papamakarios, E. Nalisnick, D. J. Rezende, S. Mohamed, and B. Lakshminarayanan, \u201cNormalizing flows for probabilistic modeling and inference,\u201d Journal of Machine Learning Research, vol. 22, no. 57, pp. 1\u201364, 2021.\\n\\n[24] T. Pl\u00f6tz and S. Roth, \u201cBenchmarking denoising algorithms with real photographs,\u201d in CVPR, 2017.\\n\\n[25] P. L. Rosin, \u201cThresholding for change detection,\u201d in ICCV, 1998.\\n\\n[26] L. Theis, A. van den Oord, and M. Bethge, \u201cA note on the evaluation of generative models,\u201d in ICLR, 2016.\\n\\n[27] Z. Wang, A. Bovik, H. Sheikh, and E. Simoncelli, \u201cImage quality assessment: from error visibility to structural similarity,\u201d TIP, vol. 13, no. 4, pp. 600\u2013612, 2004.\\n\\n[28] K. Wei, Y. Fu, J. Yang, and H. Huang, \u201cA physics-based noise formation model for extreme low-light raw denoising,\u201d in CVPR, 2020.\\n\\n[29] C. R. Wren, A. Azarbayejani, T. Darrell, and A. Pentland, \u201cPfinder: real-time tracking of the human body,\u201d TPAMI, vol. 19, no. 7, pp. 780\u2013785, 1997.\\n\\n[30] K. Zhang, W. Zuo, Y. Chen, D. Meng, and L. Zhang, \u201cBeyond a gaussian denoiser: Residual learning of deep cnn for image denoising,\u201d TIP, vol. 26, no. 7, pp. 3142\u20133155, 2017.\\n\\n[31] K. Zhang, W. Zuo, and L. Zhang, \u201cFfdnet: Toward a fast and flexible solution for cnn-based image denoising,\u201d TIP, vol. 27, no. 9, pp. 4608\u20134622, 2018.\\n\\n[32] Y. Zhang, H. Qin, X. Wang, and H. Li, \u201cRethinking noise synthesis and modeling in raw denoising,\u201d in ICCV, 2021.\"}"}
{"id": "CVPR-2022-1182", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Normalizing flows\\nFlow block \\\\( x \\\\times S \\\\)\\nConditional linear flow\\nDequantization\\nInvertible 1x1 convolution\\nConditional affine coupling\\nNoise extractor\\nDequantization\\nConditional coupling step \\\\( x \\\\times K \\\\)\\n\\nForward (density estimation)\\nInverse (sampling)\\n\\nForward input:\\nreal noisy image\\n\\nInverse output:\\nsampled noisy image\\n\\nForward and inverse input:\\nclean image\\n\\nForward output:\\n\\nInverse input:\\n\\nData processing\\n\\nFigure 5. Our model consists of two main sections: (1) The data processing section is responsible for processing noisy and clean images. (2) The flow steps are responsible for learning the complex noise distribution.\\n\\ncoupling layer but differs in the the scale and translation factors. Specifically, the output of the layer is\\n\\\\[\\ny = (y_A, y_B)\\n\\\\]\\nwith\\n\\\\[\\ny_A = x_A\\n\\\\]\\nand\\n\\\\[\\ny_B = x_B \\\\odot f_{cs}(x_A | I, c, g) + f_{ct}(x_A | I, c, g),\\n\\\\]\\nwhere \\\\( \\\\odot \\\\) is the element-wise product, \\\\( x = (x_A, x_B) \\\\) is the input, and \\\\( f_{cs} \\\\) and \\\\( f_{ct} \\\\) are functions that compute the conditional scale and translation based on the input clean image, \\\\( I \\\\), camera, \\\\( c \\\\), and gain setting, \\\\( g \\\\). The inverse and log determinant of this transformation can be easily calculated, analogous to the (unconditional) coupling layer. See the supplemental material for architectural details of \\\\( f_{cs} \\\\) and \\\\( f_{ct} \\\\).\\n\\n5. Experiments\\nTo evaluate our model we use the SIDD dataset [1]. The SIDD-Medium split contains 320 noisy-clean image pairs captured under various ISO and lighting conditions taken by five different smartphones. While this dataset provides the data in both RAW and sRGB domains, here, we use only sRGB images. Note that this dataset used a simplified software ISP to render images from the captured RAW-rgb to sRGB instead of directly using the sRGB images produced by the camera. The software ISP applies the camera parameters stored in the raw's DNG file (e.g., white-balancing, lens shading correction, color space mapping, custom tone-map, mapping to sRGB, and sRGB gamma). We extract approximately 3,000 patches of size 32x32 from each image. From these extracted patches, 80% are used for training and the remaining for validation. Patches are randomly distributed to ensure all cameras and ISO settings are fairly represented in both training and validation sets. For training we minimize the negative log likelihood (Eq. 4) using the Adam optimizer [14].\\n\\n5.1. Metrics\\nTo quantitatively evaluate the model we consider two metrics. First, the negative log likelihood per dimension (NLL) on the test set is used as a direct evaluation of density estimation. Second, to better assess the quality of the sampled noise, we use the Kullback-Leibler (KL) divergence which was introduced in [2]. This metric computes the KL divergence between histograms of real and sampled noise. This metric is more sensitive to mismatches in the model\u2019s estimated variance than the NLL metric is.\\n\\n5.2. Baselines\\nWe explored a number of baseline sRGB noise models. We considered three variations of homoscedastic Gaussian noise: 1) AWGN, which assumes independent, isotropic noise at each pixel; 2) diagonal covariance Gaussian, which assumes independent but anisotropic noise at each pixel; and 3) full covariance Gaussian, which allows correlations between color channels. Note the full covariance Gaussian model was previously proposed for sRGB data by [21]. We also implemented a heteroscedastic Gaussian model, often referred to as the noise level function (NLF) and described in Eq. 1. While not expected to perform well based on, e.g., Figure 2, it is a widely used and well known model of camera noise. Finally, we compared with a direct adaption of the Noise Flow model [2] to sRGB instead of RAW-rgb data. To do this we modified the number of channels that the architecture expected, but otherwise left it unchanged. Because of the strong assumptions made, Noise Flow has a small number of parameters. To make a more fair comparison, we also built a larger version of Noise Flow, referred to as Noise Flow-Large, that follows the architecture of Noise Flow but with the number of parameters increased to be comparable to our model. All baselines were implemented as normalizing flows using combinations of simple linear flows, signal-dependent flows, and gain-dependent flows [2] to ensure consistency.\\n\\n5.3. Results\\nTable 1 shows the final test NLL and KL divergence for our model and all the baselines. Figures 6a and 6b show the...\"}"}
{"id": "CVPR-2022-1182", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. (a) Marginal KL divergence between the synthetic noise from the models and the real noise samples of the test set. (b) Testing NLL per dimension of our model compared to our baseline models. Our model not only performs by both metrics, it also converges faster.\\n\\nTable 1. Test NLL and marginal $D_{KL}$ for our model and our baselines. The proposed model outperforms the baselines in both metrics by a large margin. Noise Flow-Large and Noise Flow models have the closest NLL to the proposed method, however, their relatively high $D_{KL}$ indicates that these models fail to generate realistic noise samples. Together the results show that models originally developed for RAW-rgb are unlikely to be successful with sRGB.\\n\\nIn terms of marginal KL divergence our model also significantly improves over the baselines. Unlike with NLL, the closest performing baseline in terms of KL divergence was the diagonal Gaussian noise model, with a KL divergence of $0.079$. In contrast, our model achieved a KL divergence of $0.044$. For comparison, we also considered the recently proposed C2N [13] model, a GAN-based sRGB noise model. Their paper reported a KL divergence of $0.1638$. However, we note that KL divergence is sensitive to the choice of histogram bins and other implementation details.\\n\\nInterestingly, Table 1 shows that Noise Flow significantly outperformed the other baseline noise models in terms of NLL, but significantly underperformed them in terms of KL divergence. On further investigation, we found that the Noise Flow model was significantly overestimating the variance of the noise in many cases. For instance, Figure 7, similar to Figure 2, shows the variance of sRGB noise as a function of noise-free image intensity for real data, our model, Noise Flow, and the isotropic baseline for an iPhone7 with ISO level of 100. This graph shows that Noise Flow has badly overestimated the variance, even compared to the isotropic Gaussian model, suggesting that the difficulties in training seen earlier are preventing it from converging to a reasonable model. In contrast, while our proposed model slightly overestimates the noise as well, it much better captures the structure of the relationship.\\n\\nQualitative Comparison. To qualitatively compare the trained noise models Figure 8 shows samples of noise from our model and with two baseline models at different ISO levels. Out of the Gaussian-based models, the full covariance Gaussian Model achieves the best test NLL and has been shown to be a good fit for modeling noise in the sRGB space as its full covariance can learn dependencies between channels [21]. A more extensive set of samples is available in the supplemental materials. Samples from our model are generally more visually similar to those of real noisy images, particularly in comparison to the baselines. Noise Flow samples are too noisy and samples from full covariance Gaussian do not exhibit enough variance. For example, Noise Flow samples at ISO 800 are significantly less noisy compared to the real noisy image.\"}"}
{"id": "CVPR-2022-1182", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Samples from our model achieve the lowest $D_{KL}$ in almost all cases, showing its ability to generate realistic noise.\\n\\nModeling Different Cameras and ISO Settings. Figure 9 shows the learning of noise characteristics under different cameras and ISO settings. The dotted lines are the true noise standard deviation under each condition. The results show that the noise distribution changes drastically with different cameras and ISO levels. Further, our model is able to successfully capture this behavior to learn a more realistic noise model. The graphs also suggest that, while the model is quick to learn in the first few epochs and exhibits relatively little overfitting with the exception of ISO 3200. However, we note that this ISO setting has a very limited number of samples in the training set.\\n\\nAblation Studies. Table 2 summarizes the performance of different architecture choices for the flow block of our normalizing flows model. The results show a significant improvement in noise modeling and noise synthesis when the coupling step conditions on all the important variables including the clean image, camera type, and ISO settings, rather than conditioning on only one of them. Additionally, we see an improvement by adding the conditional linear flow (CL) layer to the conditional coupling steps (CCS), showing the importance of having a direct way of transferring knowledge from camera types and ISO levels. Finally, we show the importance of having multiple flow blocks. The architecture of (CL-CCSx2) x4 with four flow blocks achieves the best performance in both metrics, NLL and $D_{KL}$. This is the architecture used in other experiments.\"}"}
{"id": "CVPR-2022-1182", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 3. Denoiser performance when trained on samples from each model. Denoisers are evaluated on the SIDD Benchmark set. The denoiser trained on samples from our noise model achieves better performance compared to those trained on noise from the baselines. (*) Results are taken from [13].\\n\\n| Dataset | PSNR | SSIM |\\n|---------|------|------|\\n| Real Noise | 36.51 | 0.922 |\\n| C2N* | 33.76 | 0.901 |\\n| Our model | 34.74 | 0.912 |\\n| Noise Flow | 33.81 | 0.894 |\\n| Full Gaussian | 32.72 | 0.873 |\\n| Heteroscedastic Gaussian | 32.24 | 0.849 |\\n| Diagonal Gaussian | 33.34 | 0.867 |\\n| Isotropic Gaussian | 32.48 | 0.855 |\\n\\nTable 3 summarizes the result of our denoising experiment. The results show that a DnCNN model trained on the synthetic noises from our model achieves a significantly higher performance in terms of peak signal-to-noise ratio (PSNR) and structural similarity (SSIM) [27] on SIDD-Benchmark compared to the denoisers trained on the samples from baseline models. While the performance of our model does not exceed the performance of a denoiser trained with real data (e.g., as was found with noise models in RAW-rgb [2]), it does significantly shrink the gap.\\n\\nFigure 10 shows denoising results of the DnCNN model trained with real noisy images and three noise synthesis strategies including our model and two of our baselines. The figure also includes the input noisy image and the ground truth clean image for reference. For the full set of results, we refer the readers to the supplemental materials.\\n\\nThe denoiser trained on noise samples from our model tends to produce denoised images that are closer to the ground truth clean images than when trained on samples from the baseline noise models. The model trained on noisy image samples from Noise Flow tends to not remove the noise fully, outputting images which still contain a significant amount of noise (e.g., as in row 6). This is likely caused by the tendency of Noise Flow to significantly overestimate the noise variance as demonstrated in Figure 7. Finally, denoisers trained on Gaussian samples tend to produce overly smooth denoised images, (e.g., as in row 4).\\n\\n6. Conclusion\\n\\nWe introduced a noise model specifically tailored to capture image noise in the sRGB domain. Because image noise in the sRGB domain exhibits significantly more complex noise distributions than those found in the unprocessed RAW-rgb domain, we showed that existing RAW-rgb noise models like heteroscedastic noise and the Noise Flow model [2] are ineffective at capturing noise in sRGB. To address this we described an architecture based on normalizing flows that can effectively model sRGB image noise, while capturing the complex dependencies on variables such as clean image intensity, camera model and ISO settings. We demonstrated the effectiveness of the proposed noise model directly (with NLL and KL divergence metrics) and by training image denoisers using image noise synthesized by our model. We showed that our image denoisers significantly outperform other denoisers trained on existing noise models for sRGB. Source code is available at https://yorkucvil.github.io/sRGBNoise/.\\n\\nAcknowledgments\\n\\nThis work was done as part of an internship at the Samsung AI Center in Toronto, Canada. AM\u2019s internship was funded by a Mitacs Accelerate. SK\u2019s and AM\u2019s student funding came in part from the Canada First Research Excellence Fund for the Vision: Science to Applications (VISTA) programme and an NSERC Discovery Grant.\"}"}
