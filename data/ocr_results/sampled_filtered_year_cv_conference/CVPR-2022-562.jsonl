{"id": "CVPR-2022-562", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"contrastive sample, we optimize the following objective,\\n\\n$$L_{\\\\text{con}}(\\\\Psi \\\\Theta (x|y=1)) = \\\\sum_{n} -\\\\log(1 - \\\\Psi \\\\Theta (x_n|y_n=1)),$$\\n\\nNote that $$L_{\\\\text{con}}(\\\\Psi \\\\Theta (x|y=1))$$ encourages contrastive samples for images $$x$$ with label $$y=1$$ to be predicted as $$y=0$$. In multi-class scenarios, this contrastive learning approach can be readily extended by letting one of the classes be the reference, or in general, by using a complete, cross-entropy-based contrastive loss, in which contrastive samples are generated for both classes, i.e., $$y=\\\\{0,1\\\\}$$, instead of just one class (half the cross-entropy loss) as in our case.\\n\\n3. Related Work\\n\\nBelow we discuss existing research work on classification of very large images with tiny objects, the most relevant body of work being on attention-based models, and general efforts toward computational efficiency for image classification models.\\n\\nTiny object classification\\n\\nRecent works have studied CNNs under different noise scenarios, either by performing experiments where label noise is introduced artificially [1, 52], or by directly working with noisy labels and annotations [14,32]. While it has been shown that large amounts of label noise hinders the generalization ability of CNNs [1,52], it has been further demonstrated that CNNs can mitigate this label-corrupting noise by increasing the size of the data used for training [32], tuning the parameters of the optimization procedure [19], or re-weighting input training samples [14]. However, all of these works focus on label corruption but do not consider the case of noiseless labels or label assignments with low noise, in which alternatively, the region of interest (ROI) associated with the label is small or tiny, relative to the size of the image. Purposely, [37] analyzed the capacity of CNNs in precisely this context, i.e., that of tiny object image classification tasks. Their results indicate that by using a training dataset limited in size, CNNs are unable to generalize well as the ROI-to-image ratio of the input decreases. Typically, the object associated with the label occupies a dominant portion of the image. However, in some real-world applications, such as medical imaging, remote sensing or traffic signs recognition, only a very tiny fraction of the image informs their labels, leading to a low ROI-to-image ratios.\\n\\nAttention\\n\\nThis technique has a long history in the neural networks literature [17]. In the modern era of deep learning, it has been used very successfully in various problems [9,12,49,53]. Two main classes of attention mechanisms include: soft attention, which estimates a (continuous) weight for every location of the entire input [16], and hard attention, which selects a fraction of the data, e.g., a ROI in an image, for processing [36], which is a harder problem that resembles object detection, but without ground-truth object boundaries. Note that the attention in [3,16,20] is defined as the weights of a bag of features from an image. Our formulation can also be interpreted in the same way, since $$\\\\alpha$$ is the attention on bag of features of tiles and $$\\\\beta$$ is the attention on bag of features of sub-tiles.\\n\\nComputational efficiency\\n\\nThere are multiple ways to control the computational cost of deep neural networks. We categorize them into four groups:\\n\\ni) compression methods that aim to remove redundancy from already trained models [51];\\n\\nii) lightweight design strategies used to replace network components with computationally lighter counterparts [18];\\n\\niii) partial computation methods selectively utilize units of a network, thus creating forward-propagation paths with different computational costs [26]; and\\n\\niv) reinforcement learning and attention mechanisms that can be used to selectively process subsets of the input, based on their importance for the task of interest [7, 20, 28, 40, 47]. The latter being the strategy we consider in the proposed Zoom-In architecture.\\n\\nIn-sample contrastive learning\\n\\nConventional deep neural networks lack robustness to out-of-distribution data or naturally-occurring corruption such as image noise, blur, compression and label corruption. Contrastive learning [22] has demonstrated great success with learning in noisy scenarios, e.g., corrupted ImageNet [21]. Here, we aim to leverage contrastive learning to mitigate the performance loss caused by images with low ROI-to-image ratio. Hence, the built-in attention mechanism facilitates the in-sample contrastive learning because contrastive samples can be obtained using the same attention mechanisms without the need for additional model components or parameters.\\n\\nWeakly supervised training on gigapixel images\\n\\nRecent work has demonstrated that deep learning algorithms have the ability to predict patient-level attributes, e.g., cancer staging from whole slide images (WSIs) in digital pathology applications [27]. Because these images are so large and no prior knowledge of which subsets of the image (tiles) are associated with the label, such task is known as weakly supervised learning [4, 31]. Specifically, the model has to estimate which regions within the image are relevant to the label, so predictions can be made using information from these regions alone; not the whole image. Importantly, with current hardware architectures, WSIs are too large to fit in GPU memory, so one commonly used technique is to build a model to select a subset of patches from the image [35,54]. An alternative approach consists in using the entire WSI but in a compressed, much smaller, representation, at the cost of losing fine-grained details that may be important [45]. Building representations that aggregate features from selected image regions (tiles or patches) are also alternative approaches [4, 8]. We consider the performance of these approaches relative to the proposed Zoom-In network in the experiments.\"}"}
{"id": "CVPR-2022-562", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Experiments\\n\\nWe evaluate the proposed approach in terms of accuracy and GPU memory requirements. In the results below, Zoom-In Network refers to the proposed method with a lightweight LeNet backbone, and Zoom-In Network (Res) refers to our method using a ResNet16 backbone. The details of the model architecture are presented in the SM.\\n\\nMoreover, we highlight the ability of the model to attend to a small amount of full-resolution sub-tiles (ROIs) of the image inputs, which results in a significantly reduced peak GPU memory usage and superior test accuracy relative to the competing approaches. We consider methods that can handle large images as inputs, e.g., attention sampling models (ATS) [20], Differentiable Patch Selection (Top-K) [7], BagNet [37], EfficientNet [44] and streaming CNNs [38]. Simultaneously, we also compare our model with methods that apply strategies similar to zoom-in, e.g., PatchDrop [47] and RA-CNN [12]. For the peak memory usage, we report inference memory in Mb per sample, i.e., for a batch of size 1. We also report the floating point operations (FLOPs) and run time when inferring a single image. Details of the model architecture and some hyper-parameters not specified in each experiment, as well as an ablation study examining the impact on performance of $N$, $\\\\lambda$, and using contrastive learning are presented in the SM. In-sample contrastive learning is applied after training without it for 10 epochs, and the entropy regularization parameter (for our model and ATS) is set to $\\\\lambda = e^{-5}$.\\n\\nDatasets\\n\\nWe focus on datasets that have relatively large image sizes and feature tiny ROI objects that are scattered in large backgrounds, unlike natural images, in which objects are (usually) in the middle of the image due to the fact that photographers tend to center images around the object of interest (target) [46]. Consequently, in the experiments, we do not consider datasets such as ImageNet, iNaturalist and COCO because in these the ROI-to-image ratio is close to 1 and also because image sizes are relatively small relative to some of the other datasets considered and described below.\\n\\nWe present experiments on five datasets:\\n\\ni) The colon cancer dataset introduced in [42] aims to detect whether epithelial cells exist in a Hematoxylin and Eosin (H&E) stained images. This dataset contains 100 images of dimensions 500 \u00d7 500. The images originate both from malignant and normal tissue, and contain approximately 22,000 annotated cells. Following [16,20], we treat the problem as a binary classification task where the positive images are those containing at least one cell belonging to the epithelial cell class.\\n\\nii) The NeedleCamelyon dataset [37] is built from cropped images from the original Camelyon16 dataset with specified ROI-to-image ratios. Specifically, we generate datasets for ROI-to-image ratios in the range of $[0.1 \\\\%, 1\\\\%]$, and we crop each image with size of $1,024 \\\\times 1,024$ pixels. Positive examples are created by taking 50 random crops from every annotated cancer metastasis area if the ROI-to-image ratio falls within the range $[0.1 \\\\%, 1\\\\%]$. Negative examples are taken by randomly cropping normal whole-slide images and filtering out image crops that mostly contain background. Further, we ensure the class balance by sampling an equal amount of positive and negative crops.\\n\\niii) Traffic Signs Recognition dataset [25] consists of over 20,000 road scene images with a size of $960 \\\\times 1280$. Here, we use the same subset as [7, 20]. The task is to classify whether the road-scene image contains a speed limit sign (50, 70 or 80 km/h) or not. The subset used in [7, 20] and our experiments includes 747 images for training and 684 images for testing.\\n\\niv) The Functional Map of the World (fMoW) dataset [5] aims to classify the functional purpose of buildings and infrastructures and land-use from high-resolution satellite images. The approximate range of image size in this dataset is $500 \\\\times 500$ to $9,000 \\\\times 9,000$ pixels. In our experiment, we extract a class-balanced subset from the original fMoW dataset to further illustrate the proposed method is applicable beyond digital pathology images. Our constructed subset consists of 15,000 training images and 9,571 test images from 10 classes. More details of constructing the subset is provided in SM.\\n\\nv) Finally, we utilize the Camelyon16 dataset to further demonstrate the utility of our model on gigapixel images. This dataset contains 400 images.\"}"}
{"id": "CVPR-2022-562", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We split the 270 slides into train and validation sets; for hyperparameter tuning. Typically, only a small portion of a slide contains biological tissue of interest, with background and fat encompassing the remaining areas, e.g., see Figure 1 for a typical WSI (with pixel-level annotations).\\n\\nColon cancer\\nWe use the same experimental setup as [16, 20], namely, 10-fold-cross-validation, and five repetitions per experiment. The approach most closely related to ours is the one-stage attention sampling model in [20]. We refer to this method as ATS-N, where \\\\( N = 10 \\\\) indicates the number of tiles drawn from the attention weights, and we set \\\\( s = s_2 \\\\) in all experiments. The value of \\\\( N \\\\) was selected to maximize performance. To showcase the advantages of our model compared to traditional CNN approaches, we also include a ResNet [15] with 8 convolutional layers and 32 channels as naive baseline. For our model, we set \\\\( s_1 = 0.25, s_2 = 0.3, N = 30, h_2 = w_2 = 32 \\\\).\\n\\nResults are summarized in Table 1. The proposed two-stage attention sampling model results in approximately 4.3% higher test accuracy than (one-stage) ATS-10; presumably due to its ability to better focus on informative regions (sub-tiles) of the image via the hierarchical attention mechanism. Moreover, the baseline (CNN) and ATS-10 require at least \\\\( \\\\times 90 \\\\) and \\\\( \\\\times 6 \\\\) more memory relative to the Zoom-In network, respectively. Alternatively, PatchDrop [47] and RA-CNN [12] not only underperform the base CNN but also have higher memory requirements. The memory efficiency of the proposed approach is justified by the way in which the image is processed as a small collection of sub-tiles, thus resulting in a substantially reduced forward pass cost relative to the CNN and ATS-10 models. The FLOPs of ATS and Zoom-In Network are comparable because in the colon cancer experiments, the feature extractor \\\\( f_\\\\Theta(\\\\cdot) \\\\) dominates the FLOP count. Since ATS and Zoom-In Network feed the same number of patches into \\\\( f_\\\\Theta(\\\\cdot) \\\\), their resulting FLOPs in this step is the same. In fact, this step contributes 0.235 B FLOPs (96%). Further, our model takes slightly more run time due to the implementation inefficiency when extracting tiles and sub-tiles.\\n\\nNeedleCamelyon\\nNote that the NeedleCamelyon dataset uses larger images (1,024 \\\\( \\\\times \\\\) 1,024) compared to the ones (up to 512 \\\\( \\\\times \\\\) 512) used in [37] to better showcase the abilities of the models considered. Following [37], we split the NeedleCamelyon dataset into training set, validation set, test set with a ratio of 60:20:20. The number of images for each set are 6,000, 2,000 and 2,000, respectively. Positive and negative samples are balanced in each set. We compare our model with ATS-30 and an existing CNN architecture used for a similar NeedleCamelyon dataset in [37]. BagNet [3] is a CNN model extracts features at tile-level, which is efficiently used in NeedleCamelyon experiments in [37]. We use the BagNet as the CNN baseline in our experiment. For our model, we set \\\\( s_1 = 0.25, s_2 = 0.3, N = 30, h_2 = w_2 = 32 \\\\). Due to the much smaller ROI-to-image ratio of NeedleCamelyon relative to the colon cancer dataset, we set a larger \\\\( s_1 \\\\) and \\\\( s_2 \\\\) to ensure the down-sampling will not wash out the discriminative information. We also increase \\\\( N \\\\) since the number of attention weights with large values is larger in this case.\\n\\nTable 1 shows performance for the proposed model and the baselines. We observe that for larger images, the Zoom-In network has better GPU memory use at inference time because much less sub-tiles at scale \\\\( s_2 \\\\) tend to be instantiated. In terms of test accuracy, the proposed model results in approximately 3.5% higher test accuracy than one stage ATS-30 and 6.0% higher than the BagNet baseline in this experiment. Moreover, BagNet and one-stage attention sampling require at least \\\\( \\\\times 500 \\\\) and \\\\( \\\\times 7 \\\\) more memory, respectively, compared to the proposed approach. Here, we can see our Zoom-In Network consumes drastically less FLOPs than ATS. This is because the Zoom-In Network requires less pixels of the original input images to select a comparable amount of high resolution ROI patches for prediction relative to ATS.\\n\\nTraffic Sign Recognition\\nWe compare the Zoom-In Network with a traditional CNN (EfficientNet-B0 [44]) and the recently published one-stage zoom-in methods (ATS [20], TopK [7]). For EfficientNet-B0, we use both the original resolution images and images downsampled by half (denoted as \\\\( s_{0.5} \\\\)) as inputs, to show the limitations of traditional CNNs. For ATS and TopK, we use the same hyperparameter settings from [7,20]. For our Zoom-In Network, we tried two types of the backbones (LeNet and ResNet16) and we set \\\\( s_1 = 0.125, s_2 = 0.3, N = 10, h_2 = w_2 = 100 \\\\). The details of the network architecture are shown in the SM. In Table 1, we can see that our Zoom-In model achieves the highest accuracy and lowest GPU memory consumption among all tested methods. Also, our model requires less FLOPs by using less number of input pixels than ATS and Top-K. The merits of our model originate from accurate target object localization achieved by the attention mechanism and contrastive learning, as well as efficient GPU memory usage. Examples of attention maps for Traffic Sign dataset similar to those in Figure 3 are shown in the SM.\"}"}
{"id": "CVPR-2022-562", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Results on Camelyon16 data. Memory denotes average peak memory per sample at inference.\\n\\n| Method                  | Annotation | Pixel-level Accuracy (%) | Memory (Mb) |\\n|-------------------------|------------|--------------------------|-------------|\\n| Streaming CNN [38]      | No         | 70.6                     | 3,256.29    |\\n| CLAM [31]               | No         | 78.0                     | 206.88      |\\n| MIL [4]                 | No         | 79.9                     | 140.68      |\\n| MRMIL [29]              | No         | 81.1                     | 568.00      |\\n| Zoom-In Network         | No         | 81.3                     | 71.76       |\\n| Zoom-In Network (Res)   | No         | 82.6                     | 71.76       |\\n| Zoom-In Network         | Yes        | 88.2                     | 71.76       |\\n| Zoom-In Network (Res)   | Yes        | 90.8                     | 71.76       |\\n| Winning model [2]       | Yes        | 92.2                     | 395.77      |\\n\\nFunctional Map of the World\\n\\nFunctional Map of the World (fMoW) consists of a large amount of high-resolution RGB images of various sizes ranging from $500 \\\\times 500$ to $9,000 \\\\times 9,000$. Following [36], we choose EfficientNet-B0 [44] as the baseline model. EfficientNet-B0 effectively scales with large images and has been proved to serve as a good baseline model on the fMoW dataset in [36]. We also implement the ATS and TopK with our best efforts for fMoW dataset. We set $s_1 = 0.25$, $s_2 = 0.5$, $N = 30$, and $h_2 = w_2 = 50$ for our model.\\n\\nResults in Table1 show that Zoom-In Network surpasses EfficientNet-B0 in accuracy and memory consumption at inference time. Examples of attention maps for fMoW similar to those in Figure 3 are shown in the SM.\\n\\nCamelyon16\\n\\nThis is a gigapixel dataset consisting of whole-slide images (WSIs) with sizes ranging from $45,056 \\\\times 35,840$ to $217,088 \\\\times 111,104$. The objective here is to predict whether a WSI contains cancer metastases. As we described in the Related Work Section, existing works have attempted to train CNNs on very large images with only image-level labels, i.e., via weakly supervised training on gigapixel images. We consider the streaming CNN [38], CLAM [31], MIL [4] and MRMIL [29], which we previously briefly described. Details of these baselines are presented in the SM. Further, we also evaluate the model for the scenario in which pixel-level annotations (ROIs) are available. Here, we compare our results to the winning model of the Camelyon16 challenge [2]. For our model, we set $s_1 = 0.03125$, $s_2 = 0.125$, $N = 100$, and $h_2 = w_2 = 50$.\\n\\nIn Table 2, we see that our Zoom-In network achieves the highest test accuracy when pixel-level annotations are not available. Even when there are pixel-level annotations, our model yields a test accuracy close to the winning model of the Camelyon16 challenge, without the need for comprehensive tuning and handcrafted features (i.e., the minimum surrounding convex region, the average prediction values, and the longest axis of the lesion area). For the memory comparison, all methods require substantially more memory than the proposed approach, notable $\\\\times 8$ more than MRMIL which has comparable performance. Moreover, we also consider the situation in which the pixel-level annotations are available. The extension to leverage pixel-level annotations is described in the SM. The performance of the proposed model is relatively close to the winning model [2], which indicates that the proposed approach is flexible and also accurate when we have the manually annotated ROIs. Additional details including attention maps similar to that in Figure 3 are provided in the SM.\\n\\nWe also analyzed the correlation of the attention weights generated by the Zoom-In network with the ground-truth metastases-to-tile ratio obtained from pixel annotations. Specifically, the proportion of each sub-tile of size $h_2 \\\\times w_2$ that is covered by cancer metastases. The Spearman correlation coefficient for all the tiles with pixel-level annotations is $\\\\rho = 0.3570$, indicating a good agreement. Note that these attention weights are obtained from the model trained without pixel-level annotation information. In the SM, we visually present these correlations as scatter plots (attention weights vs. metastases-to-tile ratios).\\n\\nFrom all the results above, we see Zoom-In Network reduces the GPU memory usage significantly. The reasons why we pursue a low GPU memory consumption are: i) GPUs with high memory are expensive and not widely available for applications in practice. A model using less GPU at inference time can be deployed with less expense; ii) the highly memory-efficient model allows training and inferring images larger than gigapixels possible; iii) With the growing use of neural network on mobile/edge devices, it is key to develop memory-light GPU models to allow locally running deep learning service on mobile/edge devices.\\n\\n5. Discussion\\n\\nWe presented the Zoom-In network that can efficiently classify very large images with tiny objects. We improved over the existing CNN-based baselines both in terms of accuracy and peak GPU memory use, by leveraging a two-stage hierarchical sampling strategy and a contrastive learning objective. We also considered the scenario in which pixel-level annotations (segmentation maps) are available during training. In the experiments, we demonstrated the advantage of the proposed model on five challenging classification tasks. We note that the images in the Camelyon16 dataset are all gigapixel in size and that we are likely the first ones training an end-to-end deep learning model for them. Our model achieved the best accuracy when pixel-level annotations are not available, while also using a small amount of GPU memory, which allows for training and inference on full-resolution gigapixel images using a single GPU. One limitation of the proposed model is the need to specify the number of sub-tile samples $N$, which can be potentially estimated from data.\\n\\nAcknowledgements\\n\\nThis work was supported by NIH (R44-HL140794), DARPA (FA8650-18-2-7832-P00009-12) and ONR (N00014-18-1-2871-P00002-3).\"}"}
{"id": "CVPR-2022-562", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Devansh Arpit, Stanis\u0142aw Jastrz\u0119bski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, et al. A closer look at memorization in deep networks. In International Conference on Machine Learning, pages 233\u2013242. PMLR, 2017.\\n\\n[2] Babak Ehteshami Bejnordi, Mitko Veta, Paul Johannes Van Diest, Bram Van Ginneken, Nico Karssemeijer, Geert Litjens, Jeroen AWM Van Der Laak, Meyke Hermsen, Quirine F Manson, Maschenka Balkenhol, et al. Diagnostic assessment of deep learning algorithms for detection of lymph node metastases in women with breast cancer. *JAMA*, 318(22):2199\u20132210, 2017.\\n\\n[3] Wieland Brendel and Matthias Bethge. Approximating cnns with bag-of-local-features models works surprisingly well on imagenet. *arXiv preprint arXiv:1904.00760*, 2019.\\n\\n[4] Gabriele Campanella, Matthew G Hanna, Luke Geneslaw, Allen Miraflor, Vitor Werneck Krauss Silva, Klaus J Busam, Edi Brogi, Victor E Reuter, David S Klimstra, and Thomas J Fuchs. Clinical-grade computational pathology using weakly supervised deep learning on whole slide images. *Nature medicine*, 25(8):1301\u20131309, 2019.\\n\\n[5] Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 6172\u20136180, 2018.\\n\\n[6] Robert Graham Clark. Sampling of subpopulations in two-stage surveys. *Statistics in Medicine*, 28(29):3697\u20133717, 2009.\\n\\n[7] Jean-Baptiste Cordonnier, Aravindh Mahendran, Alexey Dosovitskiy, Dirk Weissenborn, Jakob Uszkoreit, and Thomas Unterthiner. Differentiable patch selection for image recognition. In *Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition*, pages 2351\u20132360, 2021.\\n\\n[8] Pierre Courtiol, Eric W Tramel, Marc Sanselme, and Gilles Wainrib. Classification and disease localization in histopathology using only global labels: A weakly-supervised approach. *arXiv preprint arXiv:1802.02212*, 2018.\\n\\n[9] Misha Denil, Loris Bazzani, Hugo Larochelle, and Nando de Freitas. Learning where to attend with deep architectures for image tracking. *Neural computation*, 24(8):2151\u20132184, 2012.\\n\\n[10] Xuanyi Dong, Junshi Huang, Yi Yang, and Shuicheng Yan. More is less: A more complicated network with less inference complexity. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 5840\u20135848, 2017.\\n\\n[11] Pavlos S Efraimidis and Paul G Spirakis. Weighted random sampling with a reservoir. *Information Processing Letters*, 97(5):181\u2013185, 2006.\\n\\n[12] Jianlong Fu, Heliang Zheng, and Tao Mei. Look closer to see better: Recurrent attention convolutional neural network for fine-grained image recognition. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 4438\u20134446, 2017.\\n\\n[13] Lindsay P Galway, Nathaniel Bell, Sahar AE Al Shatari, Amy Hagopian, Gilbert Burnham, Abraham Flaxman, William M Weiss, Julie Rajaratnam, and Tim K Takaro. A two-stage cluster sampling method using gridded population data, a gis, and google earth tm imagery in a population-based mortality survey in iraq. *International Journal of Health Geographics*, 11(1):1\u20139, 2012.\\n\\n[14] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. *arXiv preprint arXiv:1804.06872*, 2018.\\n\\n[15] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In *Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition*, pages 770\u2013778, 2016.\\n\\n[16] Maximilian Ilse, Jakub Tomczak, and Max Welling. Attention-based deep multiple instance learning. In *International Conference on Machine Learning*, pages 2127\u20132136. PMLR, 2018.\\n\\n[17] Laurent Itti, Christof Koch, and Ernst Niebur. A model of saliency-based visual attention for rapid scene analysis. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, 20(11):1254\u20131259, 1998.\\n\\n[18] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. *arXiv preprint arXiv:1405.3866*, 2014.\\n\\n[19] Stanis\u0142aw Jastrz\u0119bski, Zachary Kenton, Devansh Arpit, Nicolas Ballas, Asja Fischer, Yoshua Bengio, and Amos Storkey. Three factors influencing minima in SGD. *arXiv preprint arXiv:1711.04623*, 2017.\\n\\n[20] Angelos Katharopoulos and Fran\u00e7ois Fleuret. Processing megapixel images with deep attention-sampling models. In *International Conference on Machine Learning*, pages 3282\u20133291. PMLR, 2019.\\n\\n[21] Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Supervised contrastive learning. *arXiv preprint arXiv:2004.11362*, 2020.\\n\\n[22] Minsong Ki, Youngjung Uh, Wonyoung Lee, and Hyeran Byun. In-sample contrastive learning and consistent attention for weakly supervised object localization. In *Proceedings of the Asian Conference on Computer Vision*, 2020.\\n\\n[23] Wouter Kool, Herke Van Hoof, and Max Welling. Stochastic beams and where to find them: The gumbel-top-k trick for sampling sequences without replacement. In *International Conference on Machine Learning*, pages 3499\u20133508. PMLR, 2019.\\n\\n[24] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. In *Advances in Neural Information Processing Systems*, 25:1097\u20131105, 2012.\"}"}
{"id": "CVPR-2022-562", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[26] Gustav Larsson, Michael Maire, and Gregory Shakhnarovich. Ultra-deep neural networks without residuals. In Int. Conf. on Learning Representations, arXiv, Toulon, France, page 1605, 2017.\\n\\n[27] Byungjae Lee and Kyunghyun Paeng. A robust and effective approach towards accurate metastasis detection and pn-stage classification in breast cancer. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 841\u2013850. Springer, 2018.\\n\\n[28] Hila Levi and Shimon Ullman. Efficient coarse-to-fine non-local module for the detection of small objects. arXiv preprint arXiv:1811.12152, 2018.\\n\\n[29] Jiayun Li, Wenyuan Li, Anthony Sisk, Huihui Ye, W Dean Wallace, William Speier, and Corey W Arnold. A multi-resolution model for histopathology image classification and localization with multiple instance learning. Computers in Biology and Medicine, 131:104253, 2021.\\n\\n[30] Geert Litjens, Peter Bandi, Babak Ehteshami Bejnordi, Osscar Geessink, Maschenka Balkenhol, Peter Bult, Altuna Halilovic, Meyke Hermsen, Rob van de Loo, Rob Vogels, et al. 1399 h&e-stained sentinel lymph node sections of breast cancer patients: the camelyon dataset. GigaScience, 7(6):giy065, 2018.\\n\\n[31] Ming Y Lu, Drew FK Williamson, Tiffany Y Chen, Richard J Chen, Matteo Barbieri, and Faisal Mahmood. Data-efficient and weakly supervised computational pathology on whole-slide images. Nature Biomedical Engineering, pages 1\u201316, 2021.\\n\\n[32] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan, Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe, and Laurens Van Der Maaten. Exploring the limits of weakly supervised pretraining. In Proceedings of the European Conference on Computer Vision, pages 181\u2013196, 2018.\\n\\n[33] Francesco Marra, Diego Gragnaniello, Luisa Verdoliva, and Giovanni Poggi. A full-image full-resolution end-to-end-trainable cnn framework for image forgery detection. IEEE Access, 8:133488\u2013133502, 2020.\\n\\n[34] Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International Conference on Machine Learning, pages 1928\u20131937. PMLR, 2016.\\n\\n[35] Nikhil Naik, Ali Madani, Andre Esteva, Nitish Shirish Keskar, Michael F Press, Daniel Ruderman, David B Agus, and Richard Socher. Deep learning-enabled breast cancer hormonal receptor status determination from base-level h&e stains. Nature Communications, 11(1):1\u20138, 2020.\\n\\n[36] Athanasios Papadopoulos, Pawe\u0142 Korus, and Nasir Memon. Hard-attention for scalable image classification. arXiv preprint arXiv:2102.10212, 2021.\\n\\n[37] Nick Pawlowski, Suvrat Bhooshan, Nicolas Ballas, Francesco Ciompi, Ben Glocker, and Michal Drozdzal. Needles in haystacks: On classifying tiny objects in large images. arXiv preprint arXiv:1908.06037, 2019.\\n\\n[38] Hans Pinckaers, Bram van Ginneken, and Geert Litjens. Streaming convolutional neural networks for end-to-end learning with multi-megapixel images. arXiv preprint arXiv:1911.04432, 2019.\\n\\n[39] Suo Qiu. Global weighted average pooling bridges pixel-level localization and image-level classification. arXiv preprint arXiv:1809.08264, 2018.\\n\\n[40] Jason Ramapuram, Maurits Diephuis, Frantzeska Lavda, Russ Webb, and Alexandros Kalousis. Variational saccading: Efficient inference for large resolution images. arXiv preprint arXiv:1812.03170, 2018.\\n\\n[41] Minsoo Rhu, Natalia Gimelshein, Jason Clemons, Arslan Zulfiqar, and Stephen W Keckler. Vdnn: Virtualized deep neural networks for scalable, memory-efficient neural network design. In 2016 49th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO), pages 1\u201313. IEEE, 2016.\\n\\n[42] Korsuk Sirinukunwattana, Shan E Ahmed Raza, Yee-Wah Tsang, David RJ Snead, Ian A Cree, and Nasir M Rajpoot. Locality sensitive deep learning for detection and classification of nuclei in routine colon cancer histology images. IEEE Transactions on Medical Imaging, 35(5):1196\u20131206, 2016.\\n\\n[43] Chunfeng Song, Yan Huang, Wanli Ouyang, and Liang Wang. Mask-guided contrastive attention model for person re-identification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1179\u20131188, 2018.\\n\\n[44] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International Conference on Machine Learning, pages 6105\u20136114. PMLR, 2019.\\n\\n[45] David Tellez, Geert Litjens, Jeroen van der Laak, and Francesco Ciompi. Neural image compression for gigapixel histopathology image analysis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019.\\n\\n[46] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv\u00e9 J\u00e9gou. Fixing the train-test resolution discrepancy. arXiv preprint arXiv:1906.06423, 2019.\\n\\n[47] Burak Uzkent and Stefano Ermon. Learning when and where to zoom with deep reinforcement learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 12345\u201312354, 2020.\\n\\n[48] Dayong Wang, Aditya Khosla, Rishab Gargeya, Humayun Irshad, and Andrew H Beck. Deep learning for identifying metastatic breast cancer. arXiv preprint arXiv:1606.05718, 2016.\\n\\n[49] Zhe Wang, Yanxin Yin, Jianping Shi, Wei Fang, Hongsheng Li, and Xiaogang Wang. Zoom-in-net: Deep mining lesions for diabetic retinopathy detection. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 267\u2013275. Springer, 2017.\\n\\n[50] Wei Xia, Caihong Ma, Jianbo Liu, Shibin Liu, Fu Chen, Zhi Yang, and Jianbo Duan. High-resolution remote sensing imagery classification of imbalanced data using multistage sampling method and deep neural networks. Remote Sensing, 11(21):2523, 2019.\"}"}
{"id": "CVPR-2022-562", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Larry S Davis. NISP: Pruning networks using neuron importance score propagation. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 9194\u20139203, 2018.\\n\\nChiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, and Oriol Vinyals. Understanding deep learning requires rethinking generalization. arXiv preprint arXiv:1611.03530, 2016.\\n\\nHeliang Zheng, Jianlong Fu, Zheng-Jun Zha, and Jiebo Luo. Looking for the devil in the details: Learning trilinear attention sampling network for fine-grained image recognition. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5012\u20135021, 2019.\\n\\nXinliang Zhu, Jiawen Yao, and Junzhou Huang. Deep convolutional neural network for survival analysis with pathological images. In 2016 IEEE International Conference on Bioinformatics and Biomedicine (BIBM), pages 544\u2013547. IEEE, 2016.\"}"}
{"id": "CVPR-2022-562", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Efficient Classification of Very Large Images with Tiny Objects\\n\\nFanjie Kong, Ricardo Henao\\n\\nDepartment of Electrical and Computer Engineering\\nDuke University\\n{fanjie.kong, ricardo.henao}@duke.edu\\n\\nAbstract\\n\\nAn increasing number of applications in computer vision, specially, in medical imaging and remote sensing, become challenging when the goal is to classify very large images with tiny informative objects. Specifically, these classification tasks face two key challenges:\\n\\ni) the size of the input image is usually in the order of mega- or giga-pixels, however, existing deep architectures do not easily operate on such big images due to memory constraints, consequently, we seek a memory-efficient method to process these images;\\n\\nii) only a very small fraction of the input images are informative of the label of interest, resulting in low region of interest (ROI) to image ratio. However, most of the current convolutional neural networks (CNNs) are designed for image classification datasets that have relatively large ROIs and small image sizes (sub-megapixel). Existing approaches have addressed these two challenges in isolation. We present an end-to-end CNN model termed Zoom-In network that leverages hierarchical attention sampling for classification of large images with tiny objects using a single GPU. We evaluate our method on four large-image histopathology, road-scene and satellite imaging datasets, and one gigapixel pathology dataset. Experimental results show that our model achieves higher accuracy than existing methods while requiring less memory resources.\\n\\n1. Introduction\\n\\nNeural networks have achieved state-of-the-art performance in many image classification tasks [24]. However, there are still many scenarios where neural networks can still be improved. Using modern deep neural networks on image inputs of very high resolution is a non-trivial problem due to the challenges of scaling model architectures [44]. Such images are common for instance in satellite or medical imaging. Moreover, these images tend to become even bigger due to the rapid growth in computational and memory availability, as well as the advancements in camera sensor technology. Specifically challenging are the so called score\\n\\n![Image of processing a typical WSI using our zoom-in strategy.](image)\\n\\nWe see that:\\n\\ni) there are large regions with little information (mostly background), and\\n\\nii) small informative regions have high-resolution details. Leveraging the above characteristics of WSI, we derive a method that gradually zooms-in to the ROI.\\n\\nThe proposed approach first processes the down-sampled WSI to sample the target tiles, and then repeats this procedure to sample target sub-tiles. The sampled sub-tiles contain the fine-grained information for classification. The bottom images show that the manually annotated ROIs are captured by the proposed approach without the need for pixel level annotations.\"}"}
{"id": "CVPR-2022-562", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1 shows an example of a gigapixel pathology image, from which we see that manually annotated ROIs (with cancer metastases), not usually available for model training, constitute a small proportion of the whole slide image (WSI). Moreover, many tasks in satellite imagery [5] and medical image analysis [30] are still challenging due to the scarce methodology available for such big images.\\n\\nOther recent works have addressed the computational resource bottlenecks associated with models for very large images by proposing approaches such as the streaming neural network [38] and gradient checkpoint [33]. However, these methods do not take advantage of the characteristics of very large images in tiny object image classification tasks, i.e., those in which only a small portion of the image input is informative for the classification label of interest.\\n\\nAlternatively, other approaches use visual attention models to exploit these characteristics and show that discriminative information may be sparse and scattered across various image scales [12, 20, 36], which suggests that in some scenarios, processing the entire input image is unnecessary, and especially true in tiny object image classification tasks. For instance, [20] leverages attention to build image classifiers using a small collection of tiles (image patches) sampled from the matrix of attention weights generated by an attention network. Unfortunately, despite the ongoing efforts, existing approaches are either prohibitive or require severe resolution trade-offs that ultimately affect classification performance, for tasks involving very large (gigapixel) images.\\n\\nThe purpose of this work is to address these limitations simultaneously. Specifically, we propose a neural network architecture termed Zoom-In network, which as we will show, yields outperforming memory efficiency and classification accuracy on various tiny object image classification datasets. We build upon [20] by proposing a two-stage hierarchical attention sampling approach that is effectively able to process gigapixel images, while also leveraging contrastive learning as a means to improve the quality of the attention mechanisms used for sampling. This is achieved by building aggregated representations over a small fraction of high-resolution content (sub-tiles) that is selected from an attention mechanism, which itself leverages a lower resolution view of the original image. In this way, the model can dramatically reduce the data acquisition and storage requirements in real-world deployments. This is possible because low resolution views can be used to indicate which regions of the image should be acquired (attended) at higher resolution for classification purposes, without the need of acquiring the entire image at full resolution. Moreover, we show that the proposed approach can be easily extended to incorporate pixel-level annotations when available for additional performance gains. Results on five challenging datasets demonstrate the capabilities of the Zoom-In network in terms of accuracy and memory efficiency.\\n\\n2. Zoom-In Network\\n\\nBelow we present the construction of the proposed Zoom-In network model, which aims to efficiently process gigapixel images for classification of very large images with tiny objects. We start by briefly describing the one-stage attention sampling method proposed in [20], which we leverage in our formulation. Then, we introduce our strategy consisting in decomposing the attention-based sampling into two stages as illustrated in Figure 2. This two-stage hierarchical sampling approach enables computational efficiency without the need to sacrifice performance due to loss of resolution, when used in applications with very large images and small ROI-to-image ratios. In the experiments, we will show that the Zoom-In network results in improved performance relative to existing approaches on several tiny object image classification datasets, and importantly, without the need for any pixel-level annotation.\\n\\n2.1. Attention Sampling\\n\\nLet \\\\( T_{s_1}(x, c) \\\\) denote a function that extracts a tile of size \\\\( h_1 \\\\times w_1 \\\\) from the input, full-resolution, image \\\\( x \\\\in \\\\mathbb{R}^{H \\\\times W} \\\\) corresponding to the location (coordinates) \\\\( c = \\\\{i, j\\\\} \\\\) in a lower resolution view \\\\( V_{s_1}(x) \\\\in \\\\mathbb{R}^{h_1 \\\\times w_1} \\\\) of \\\\( x \\\\) at scale \\\\( s_1 \\\\in (0, 1) \\\\), so \\\\( s_1 = \\\\lfloor s_1 H \\\\rfloor \\\\) and \\\\( w_1 = \\\\lfloor s_1 W \\\\rfloor \\\\), where \\\\( \\\\lfloor \\\\cdot \\\\rfloor \\\\) is the floor operator. More specifically, \\\\( T_{s_1}(x, c) \\\\) maps \\\\( c \\\\) to a location in \\\\( x \\\\) via \\\\( \\\\{\\\\lfloor 1 + (i - 1)(W - 1) / (w - 1) \\\\rfloor, \\\\lfloor 1 + (j - 1)(H - 1) / (h - 1) \\\\rfloor\\\\} \\\\), and returns a tile of size \\\\( h_1 \\\\times w_1 \\\\).\\n\\nNote that i) the map of locations between \\\\( V_{s_1}(x) \\\\) and \\\\( x \\\\) only depends on the size of \\\\( x \\\\) \\\\((H \\\\times W)\\\\) and \\\\( s_1 \\\\) and not on the tile size \\\\( (h_1 \\\\times w_1) \\\\); ii) \\\\( h_1, w_1 > 1 / s_1 \\\\), to guarantee full coverage of \\\\( x \\\\); iii) this strategy requires to zero-pad \\\\( x \\\\) on all sides by \\\\( \\\\lfloor h_1 / 2 \\\\rfloor \\\\) and \\\\( \\\\lfloor w_1 / 2 \\\\rfloor \\\\) pixels accordingly; and iv) we have omitted the (color) channel dimension in \\\\( x \\\\) and \\\\( V_{s_1}(x) \\\\) for notational simplicity, however, we consider color images (with an additional dimension) in our experiments. Then, let \\\\( \\\\Psi_{\\\\Theta}(x) = g_{\\\\Theta}(f_{\\\\Theta}(T_{s_1}(x, c))) \\\\) be a neural network parameterized by \\\\( \\\\Theta \\\\) whose intermediate representation \\\\( z \\\\in \\\\mathbb{R}^K \\\\) is obtained via feature extracting function \\\\( z = f_{\\\\Theta}(T_{s_1}(x, c)) \\\\), e.g., a convolutional neural network (CNN). Further, \\\\( g_{\\\\Theta}(z) \\\\) is a classification function also specified as a neural network and parameterized by \\\\( \\\\Theta \\\\). We can provide \\\\( \\\\Psi_{\\\\Theta}(x) \\\\) with an attention mechanism as follows:\\n\\n\\\\[\\n\\\\alpha = a_{\\\\Theta}(V_{s_1}(x)) : \\\\mathbb{R}^{h \\\\times w} \\\\rightarrow \\\\mathbb{R}^{h \\\\times w},\\n\\\\]\\n\\n\\\\[\\n\\\\Psi_{\\\\Theta}(x) = g_{\\\\Theta}(\\\\sum_{c \\\\in C} \\\\alpha_c f_{\\\\Theta}(T_{s_1}(x, c))),\\n\\\\]\\n\\nwhere \\\\( \\\\alpha \\\\) is the matrix of attention weights such that \\\\( \\\\sum_{c \\\\in C} \\\\alpha_c = 1 \\\\), \\\\( a_{\\\\Theta}(V_{s_1}(x)) \\\\) is the attention function, also specified as a neural network, and \\\\( C \\\\) (of length \\\\(|C| = h \\\\cdot w|\\\\)) is the collection of all index pairs for view \\\\( V_{s_1}(x) \\\\). In order to avoid computing the features \\\\( z \\\\) from all \\\\(|C| \\\\) tiles implied by view \\\\( V_{s_1}(x) \\\\), which can be a very large number.\"}"}
{"id": "CVPR-2022-562", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"uses a lower resolution view for additional details.\\n\\nan aggregated feature vector by averaging over the features \\nage\\n\\nh\\nfeasible combinations of \\n\\nx\\napproach is still prohibitive for gigapixel images because\\n\\ng\\naggregated features using a classification module \\nf\\nto feature extractor\\n\\nattention map for each selected tile and selects a sub-tile, thus\\n\\nN\\nminimal computational overhead (during training).\\n\\nwe design a two-stage hierarchical sampling approach to \\nsensing applications [6, 13, 50]. Motivated by this idea,\\n\\nand mortality surveys, as well as high-resolution remote\\n\\nsign has many real-world applications such as household\\n\\nis performed within groups (clusters). Such sampling de-\\n\\ngeographically or organizationally grouped, thus sampling\\n\\nor testing people are enormously reduced if these people are\\n\\npreferred in practice. For instance, the cost of interviewing\\n\\n2.2. Two-stage Hierarchical Attention Sampling\\n\\nFigure 2. Illustration of the Zoom-In network. In Stage I, attention network \\na\\n\\n\u03b1\\nput image sampled via the attention function. This strategy\\n\\nV\\nQ\\nwhere\\n\\nx,s\\n\\n=1 0\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\nN\\n\\n"}
{"id": "CVPR-2022-562", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We can mitigate the inefficiency by ordering the samples as needed (on the fly and the second-level attention matrix in (4) can be obtained without replacement, by having already sampled locations that are drawn hierarchically from the two-level discrete distribution implied by $\\\\alpha$ that are used multiple times when obtaining the Gumbel-Top-$k$ trick like in (6) but sampling without replacement using the Gumbel-Max trick for weighted reservoir sampling [11]. Specifically, from (5) we can write\\n\\n$$E_{\\\\Theta} \\\\left[ 1 \\\\right] \\\\propto \\\\sum_{n=1}^{N_h} \\\\sum_{c=1}^{C} \\\\sum_{j=1}^{N} \\\\left( \\\\frac{\\\\beta}{c} - \\\\frac{\\\\beta}{c} \\\\right)^{j-1} \\\\left( \\\\frac{\\\\beta}{c} \\\\right)^{2j-1} \\\\right]$$\\n\\nImportantly, in practice we do not need to instantiate the tiles $T$ to prevent recalculating. However, this can cause computational inefficiency if multiple samples from the same location occur because such procedure will require to instantiate the tiles $T$ as needed (on the fly). Alternatively, we can avoid reusing sub-tiles by sampling locations of the view for the one-stage approach. In fact, we can show that our model requires significantly less GPU memory than one-stage attention sampling by choosing multiple times to obtain an unbiased estimator of the average (expectation) in (6) without replacement. Specifically, from (5) we can write\\n\\n$$E_{\\\\Theta} \\\\left[ \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left( x_i - \\\\bar{x} \\\\right)^2 \\\\right]$$\\n\\nThe sampled contrastive sub-tiles are passed through the feature network and then processed by the classifier to make predictions. In general, the number of contrastive examples (per training batch) is equal to the size of the input image, i.e., $O$ with respect to the number of unique tiles in $Q$ and $C$ sub-tiles from $x$. The memory allocation is mainly dominated by the Gumbel-Top-$k$ trick. For neural-network-based inference at inference for the one-stage approach. Here, we use workspace variables [41]. For neural-network-based inference, the model parameters, feature maps, gradient maps and memory requirements of the attention sampling model are determined by the number of unique tiles in $Q$ and $C$ sub-tiles from $x$, where $x$ is sampled via the Gumbel-Top-$k$ trick. Conveniently, we have the term peak memory to refer to the memory requirements $\\\\mathcal{M}(\\\\Theta)$ and $\\\\mathcal{M}(\\\\Psi)$ for both, the one-stage [20] and the proposed two-stage hierarchical model. Here, we use the term peak memory to refer to the full-resolution setting, with $\\\\mathcal{M}(\\\\Theta) = \\\\mathcal{M}(\\\\Psi)$, but using sub-tiles with low attention weights while the model parameters, feature maps, gradient maps and memory requirements of the attention sampling model are determined by the number of unique tiles in $Q$ and $C$ sub-tiles from $x$. Using (9), where the condition\\n\\n$$\\\\text{condition} \\\\supseteq \\\\{ \\\\beta \\\\}$$\\n\\nThe proof of (8) can be found in the Supplementary Material (SM). We can then approximate the objective for the proposed Zoom-In network consisting on estimating the expectation on the left. Alternatively, we can still obtain an unbiased estimator of the average (expectation) in (6) using a formulation similar to that of [20], but sampling without replacement using the Gumbel-Top-$k$ trick [23], which is extended from the Gumbel-Max trick [23], which is equivalent to\\n\\n$$E_{\\\\Theta} \\\\left[ \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left( x_i - \\\\bar{x} \\\\right)^2 \\\\right]$$\\n\\nWe use the term peak memory to refer to the memory requirements $\\\\mathcal{M}(\\\\Theta)$ and $\\\\mathcal{M}(\\\\Psi)$ for both, the one-stage [20] and the proposed two-stage hierarchical model. Here, we use the term peak memory to refer to the full-resolution setting, with $\\\\mathcal{M}(\\\\Theta) = \\\\mathcal{M}(\\\\Psi)$, but using sub-tiles with low attention weights while the model parameters, feature maps, gradient maps and memory requirements of the attention sampling model are determined by the number of unique tiles in $Q$ and $C$ sub-tiles from $x$. Using (9), where the condition\\n\\n$$\\\\text{condition} \\\\supseteq \\\\{ \\\\beta \\\\}$$\\n\\nThe proof of (8) can be found in the Supplementary Material (SM). We can then approximate the objective for the proposed Zoom-In network consisting on estimating the expectation on the left. Alternatively, we can still obtain an unbiased estimator of the average (expectation) in (6) using a formulation similar to that of [20], but sampling without replacement using the Gumbel-Top-$k$ trick [23], which is extended from the Gumbel-Max trick [23], which is equivalent to\\n\\n$$E_{\\\\Theta} \\\\left[ \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left( x_i - \\\\bar{x} \\\\right)^2 \\\\right]$$\\n\\nWe use the term peak memory to refer to the memory requirements $\\\\mathcal{M}(\\\\Theta)$ and $\\\\mathcal{M}(\\\\Psi)$ for both, the one-stage [20] and the proposed two-stage hierarchical model. Here, we use the term peak memory to refer to the full-resolution setting, with $\\\\mathcal{M}(\\\\Theta) = \\\\mathcal{M}(\\\\Psi)$, but using sub-tiles with low attention weights while the model parameters, feature maps, gradient maps and memory requirements of the attention sampling model are determined by the number of unique tiles in $Q$ and $C$ sub-tiles from $x$. Using (9), where the condition\\n\\n$$\\\\text{condition} \\\\supseteq \\\\{ \\\\beta \\\\}$$\\n\\nThe proof of (8) can be found in the Supplementary Material (SM). We can then approximate the objective for the proposed Zoom-In network consisting on estimating the expectation on the left. Alternatively, we can still obtain an unbiased estimator of the average (expectation) in (6) using a formulation similar to that of [20], but sampling without replacement using the Gumbel-Top-$k$ trick [23], which is extended from the Gumbel-Max trick [23], which is equivalent to\\n\\n$$E_{\\\\Theta} \\\\left[ \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left( x_i - \\\\bar{x} \\\\right)^2 \\\\right]$$\\n\\nWe use the term peak memory to refer to the memory requirements $\\\\mathcal{M}(\\\\Theta)$ and $\\\\mathcal{M}(\\\\Psi)$ for both, the one-stage [20] and the proposed two-stage hierarchical model. Here, we use the term peak memory to refer to the full-resolution setting, with $\\\\mathcal{M}(\\\\Theta) = \\\\mathcal{M}(\\\\Psi)$, but using sub-tiles with low attention weights while the model parameters, feature maps, gradient maps and memory requirements of the attention sampling model are determined by the number of unique tiles in $Q$ and $C$ sub-tiles from $x$. Using (9), where the condition\\n\\n$$\\\\text{condition} \\\\supseteq \\\\{ \\\\beta \\\\}$$\\n\\nThe proof of (8) can be found in the Supplementary Material (SM). We can then approximate the objective for the proposed Zoom-In network consisting on estimating the expectation on the left. Alternatively, we can still obtain an unbiased estimator of the average (expectation) in (6) using a formulation similar to that of [20], but sampling without replacement using the Gumbel-Top-$k$ trick [23], which is extended from the Gumbel-Max trick [23], which is equivalent to\\n\\n$$E_{\\\\Theta} \\\\left[ \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left( x_i - \\\\bar{x} \\\\right)^2 \\\\right]$$\\n\\nWe use the term peak memory to refer to the memory requirements $\\\\mathcal{M}(\\\\Theta)$ and $\\\\mathcal{M}(\\\\Psi)$ for both, the one-stage [20] and the proposed two-stage hierarchical model. Here, we use the term peak memory to refer to the full-resolution setting, with $\\\\mathcal{M}(\\\\Theta) = \\\\mathcal{M}(\\\\Psi)$, but using sub-tiles with low attention weights while the model parameters, feature maps, gradient maps and memory requirements of the attention sampling model are determined by the number of unique tiles in $Q$ and $C$ sub-tiles from $x$. Using (9), where the condition\\n\\n$$\\\\text{condition} \\\\supseteq \\\\{ \\\\beta \\\\}$$\\n\\nThe proof of (8) can be found in the Supplementary Material (SM). We can then approximate the objective for the proposed Zoom-In network consisting on estimating the expectation on the left. Alternatively, we can still obtain an unbiased estimator of the average (expectation) in (6) using a formulation similar to that of [20], but sampling without replacement using the Gumbel-Top-$k$ trick [23], which is extended from the Gumbel-Max trick [23], which is equivalent to\\n\\n$$E_{\\\\Theta} \\\\left[ \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left( x_i - \\\\bar{x} \\\\right)^2 \\\\right]$$\\n\\nWe use the term peak memory to refer to the memory requirements $\\\\mathcal{M}(\\\\Theta)$ and $\\\\mathcal{M}(\\\\Psi)$ for both, the one-stage [20] and the proposed two-stage hierarchical model. Here, we use the term peak memory to refer to the full-resolution setting, with $\\\\mathcal{M}(\\\\Theta) = \\\\mathcal{M}(\\\\Psi)$, but using sub-tiles with low attention weights while the model parameters, feature maps, gradient maps and memory requirements of the attention sampling model are determined by the number of unique tiles in $Q$ and $C$ sub-tiles from $x$. Using (9), where the condition\\n\\n$$\\\\text{condition} \\\\supseteq \\\\{ \\\\beta \\\\}$$\\n\\nThe proof of (8) can be found in the Supplementary Material (SM). We can then approximate the objective for the proposed Zoom-In network consisting on estimating the expectation on the left. Alternatively, we can still obtain an unbiased estimator of the average (expectation) in (6) using a formulation similar to that of [20], but sampling without replacement using the Gumbel-Top-$k$ trick [23], which is extended from the Gumbel-Max trick [23], which is equivalent to\\n\\n$$E_{\\\\Theta} \\\\left[ \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left( x_i - \\\\bar{x} \\\\right)^2 \\\\right]$$\\n\\nWe use the term peak memory to refer to the memory requirements $\\\\mathcal{M}(\\\\Theta)$ and $\\\\mathcal{M}(\\\\Psi)$ for both, the one-stage [20] and the proposed two-stage hierarchical model. Here, we use the term peak memory to refer to the full-resolution setting, with $\\\\mathcal{M}(\\\\Theta) = \\\\mathcal{M}(\\\\Psi)$, but using sub-tiles with low attention weights while the model parameters, feature maps, gradient maps and memory requirements of the attention sampling model are determined by the number of unique tiles in $Q$ and $C$ sub-tiles from $x$. Using (9), where the condition\\n\\n$$\\\\text{condition} \\\\supseteq \\\\{ \\\\beta \\\\}$$\\n\\nThe proof of (8) can be found in the Supplementary Material (SM). We can then approximate the objective for the proposed Zoom-In network consisting on estimating the expectation on the left. Alternatively, we can still obtain an unbiased estimator of the average (expectation) in (6) using a formulation similar to that of [20], but sampling without replacement using the Gumbel-Top-$k$ trick [23], which is extended from the Gumbel-Max trick [23], which is equivalent to\\n\\n$$E_{\\\\Theta} \\\\left[ \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left( x_i - \\\\bar{x} \\\\right)^2 \\\\right]$$\\n\\nWe use the term peak memory to refer to the memory requirements $\\\\mathcal{M}(\\\\Theta)$ and $\\\\mathcal{M}(\\\\Psi)$ for both, the one-stage [20] and the proposed two-stage hierarchical model. Here, we use the term peak memory to refer to the full-resolution setting, with $\\\\mathcal{M}(\\\\Theta) = \\\\mathcal{M}(\\\\Psi)$, but using sub-tiles with low attention weights while the model parameters, feature maps, gradient maps and memory requirements of the attention sampling model are determined by the number of unique tiles in $Q$ and $C$ sub-tiles from $x$. Using (9), where the condition\\n\\n$$\\\\text{condition} \\\\supseteq \\\\{ \\\\beta \\\\}$$\\n\\nThe proof of (8) can be found in the Supplementary Material (SM). We can then approximate the objective for the proposed Zoom-In network consisting on estimating the expectation on the left. Alternatively, we can still obtain an unbiased estimator of the average (expectation) in (6) using a formulation similar to that of [20], but sampling without replacement using the Gumbel-Top-$k$ trick [23], which is extended from the Gumbel-Max trick [23], which is equivalent to\\n\\n$$E_{\\\\Theta} \\\\left[ \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left( x_i - \\\\bar{x} \\\\right)^2 \\\\right]$$\\n\\nWe use the term peak memory to refer to the memory requirements $\\\\mathcal{M}(\\\\Theta)$ and $\\\\mathcal{M}(\\\\Psi)$ for both, the one-stage [20] and the proposed two-stage hierarchical model. Here, we use the term peak memory to refer to the full-resolution setting, with $\\\\mathcal{M}(\\\\Theta) = \\\\mathcal{M}(\\\\Psi)$, but using sub-tiles with low attention weights while the model parameters, feature maps, gradient maps and memory requirements of the attention sampling model are determined by the number of unique tiles in $Q$ and $C$ sub-tiles from $x$. Using (9), where the condition\\n\\n$$\\\\text{condition} \\\\supseteq \\\\{ \\\\beta \\\\}$$\\n\\nThe proof of (8) can be found in the Supplementary Material (SM). We can then approximate the objective for the proposed Zoom-In network consisting on estimating the expectation on the left. Alternatively, we can still obtain an unbiased estimator of the average (expectation) in (6) using a formulation similar to that of [20], but sampling without replacement using the Gumbel-Top-$k$ trick [23], which is extended from the Gumbel-Max trick [23], which is equivalent to\\n\\n$$E_{\\\\Theta} \\\\left[ \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left( x_i - \\\\bar{x} \\\\right)^2 \\\\right]$$\\n\\nWe use the term peak memory to refer to the memory requirements $\\\\mathcal{M}(\\\\Theta)$ and $\\\\mathcal{M}(\\\\Psi)$ for both, the one-stage [20] and the proposed two-stage hierarchical model. Here, we use the term peak memory to refer to the full-resolution setting, with $\\\\mathcal{M}(\\\\Theta) = \\\\mathcal{M}(\\\\Psi)$, but using sub-tiles with low attention weights while the model parameters, feature maps, gradient maps and memory requirements of the attention sampling model are determined by the number of unique tiles in $Q$ and $C$ sub-tiles from $x$. Using (9), where the condition\\n\\n$$\\\\text{condition} \\\\supseteq \\\\{ \\\\beta \\\\}$$\\n\\nThe proof of (8) can be found in the Supplementary Material (SM). We can then approximate the objective for the proposed Zoom-In network consisting on estimating the expectation on the left. Alternatively, we can still obtain an unbiased estimator of the average (expectation) in (6) using a formulation similar to that of [20], but sampling without replacement using the Gumbel-Top-$k$ trick [23], which is extended from the Gumbel-Max trick [23], which is equivalent to\\n\\n$$E_{\\\\Theta} \\\\left[ \\\\frac{1}{n} \\\\sum_{i=1}^{n} \\\\left( x_i - \\\\bar{x} \\\\right)^2 \\\\right]$$\\n\\nWe use the term peak memory to refer to the memory requirements $\\\\mathcal{M}(\\\\Theta)$ and $\\\\mathcal{M}(\\\\Psi)$ for both, the one-stage [20] and the proposed two-stage hierarchical model. Here, we use the term peak memory to refer to the full-resolution setting, with $\\\\mathcal{M}(\\\\Theta) = \\\\mathcal{M}(\\\\Psi)$, but using sub-tiles with low attention weights while the model parameters, feature maps, gradient maps and memory requirements of the attention sampling model are determined by the number of unique tiles in $Q$ and $C$ sub-tiles from $x$. Using (9), where the condition\\n\\n$$\\\\text{condition} \\\\supseteq \\\\{ \\\\beta \\\\}$$\\n\\nThe proof of (8) can be found in the Supplementary Material (SM). We can then approximate the objective for the proposed Zoom-In network consisting on estimating the expectation on the left. Alternatively, we can still obtain an unbiased estimator of the average (expectation) in (6) using a formulation similar to that of [20], but sampling without replacement using the Gumbel-Top-$k$ trick [23], which is extended from the G"}
