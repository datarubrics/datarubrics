{"id": "CVPR-2023-1599", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. (a) The illustration of two proposed semi-supervised losses. (b) Label similarity.\\n\\nSemi-supervised mixup loss. As PatchMix tries to maximize the CE between the intermediate domain and source/target domain, we now need to find a way to minimize the CE in the game. Intuitively, we propose two semi-supervised mixup losses in the feature and label spaces to minimize the discrepancy between features of mixing patches and corresponding mixing labels based on Theorem 1. The objective of the proposed PMTrans is shown in Fig. 4 (a) and consists of a classification loss of source data and two semi-supervised mixup losses.\\n\\n1) Label space: As introduced in Theorem 1, we apply a supervised mixup loss in the label space to measure the domain divergence based on the CE loss between the mixing logits and corresponding mixing labels (see green arrow in Fig. 4 (a)).\\n\\n\\\\[ L_{I,S}^l(\\\\omega) = E_{(x_i, y_i) \\\\sim D_i} \\\\ell(C(F(x_i)), y_s) \\\\]\\n\\n\\\\[ L_{I,T}^l(\\\\omega) = E_{(x_i, y_i) \\\\sim D_i} \\\\ell(C(F(x_i)), \\\\hat{y}_t) \\\\]\\n\\nwhere \\\\( \\\\hat{y}_t \\\\) is the pseudo label for target data. For convenience, we utilize the method, commonly used in [20, 21], to generate pseudo labels \\\\( \\\\hat{y}_t \\\\) for samples via \\\\( k \\\\)-means cluster.\\n\\n2) Feature space: Nonetheless, the supervised loss alone in the label space is not sufficient to diminish the domain divergence due to the less reliable pseudo labels of the target data. Therefore, we further propose to minimize the discrepancy between the similarity of the features and the similarity of labels in the feature space for aligning the intermediate and source/target domain without the supervised information of the target domain. The experimental results in Tab.5 validate its effectiveness.\\n\\nSpecifically, we first compute the cosine similarity between the intermediate domain and source/target domain in the feature space. The feature similarity is defined as\\n\\n\\\\[ d(x_i, x_s) = \\\\cos(F(x_i), F(x_s)) \\\\]\\n\\nwhere \\\\( \\\\cos \\\\) denotes the cosine similarity. As shown in Fig. 4(b), for the source domain, we exploit the ground-truth to calculate the label similarity, \\\\( y_{is} = y_s(y_s) \\\\), as a binary matrix to represent whether samples share the same labels. For example, the intermediate image is constructed by sampling patches from the source image, e.g., clock; therefore, the label similarity is set as 1 if it is calculated between the intermediate image and the source class 'clock', otherwise, it is set as 0. Then, we utilize the CE to measure the domain discrepancy based on the difference between the feature similarity and label similarity. The supervised mixup loss in the feature space (see red arrow in Fig. 4 (a)) is formulated as\\n\\n\\\\[ L_{I,S}^f(\\\\omega_F, \\\\omega_P) = E_{(x_i, y_i) \\\\sim D_i} \\\\ell(d(x_i, x_s), y_{is}) \\\\]\\n\\nMoreover, for the intermediate and target domains, due to lack of supervision, we utilize identity matrix \\\\( y_{it} \\\\) as the label similarity. For example, in Fig.4 (b), as the intermediate image is built by sampling patches from the target image, e.g., bottle; therefore, the label similarity between the intermediate image and the corresponding target image is set as 1 and vice versa. To measure the divergence between the intermediate and target domains in the feature space, we propose an unsupervised mixup loss as\\n\\n\\\\[ L_{I,T}^f(\\\\omega_F, \\\\omega_P) = E_{(x_i, y_i) \\\\sim D_i} \\\\ell(d(x_i, x_t), y_{it}) \\\\]\\n\\nFinally, the two semi-supervised mixup losses in the feature and label spaces are formulated as\\n\\n\\\\[ L^f(\\\\omega_F, \\\\omega_P) = L_{I,S}^f(\\\\omega_F, \\\\omega_P) + L_{I,T}^f(\\\\omega_F, \\\\omega_P) \\\\]\\n\\n\\\\[ L^l(\\\\omega) = L_{I,S}^l(\\\\omega) + L_{I,T}^l(\\\\omega) \\\\]\\n\\nMoreover, the classification loss is applied to the labeled source domain data (see blue arrow in Fig. 4 (a)) and is formulated as\\n\\n\\\\[ L_{S}^{cls}(\\\\omega_F, \\\\omega_C) = E_{(x_s, y_s) \\\\sim D_s} \\\\ell(C(F(x_s)), y_s) \\\\]\\n\\nA Three-Player Game. Finally, the min-max CE game aims to align distributions in the feature and label spaces. The total CE between the intermediate domain and source/target domain is\\n\\n\\\\[ CE_{s,i,t}(\\\\omega) = L^f(\\\\omega_F, \\\\omega_P) + L^l(\\\\omega) \\\\]\\n\\nWe adopt the random mixup-ratio from a learnable Beta distribution in our PatchMix module to maximize the CE between the intermediate domain and source/target domain. Moreover, the feature extractor and classifier have the same objective to minimize the CE between the intermediate domain and source/target domain. Therefore, the total objective of PMTrans is achieved by reformulating Eq.4 as\\n\\n\\\\[ J(\\\\omega): = L_{S}^{cls}(\\\\omega_F, \\\\omega_C) + \\\\alpha CE_{s,i,t}(\\\\omega) \\\\]\\n\\nwhere \\\\( \\\\alpha \\\\) is trade-off parameter. After optimizing the objective, the PatchMix module with the ideal Beta distribution will not maximize the CE anymore. Meanwhile, the feature extractor and classifier have no incentive to change their parameters to minimize the CE. Finally, the discrepancy between the intermediate domain and source/target domain is nearly zero, further indicating that the source and target domains are well aligned.\"}"}
{"id": "CVPR-2023-1599", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"4. Experiments\\n\\n4.1. Datasets and implementation\\n\\nDatasets.\\nTo evaluate the proposed method, we conduct experiments on four popular UDA benchmarks, including Office-Home [40], Office-31 [33], VisDA-2017 [32], and DomainNet [31]. The details of the datasets and transfer tasks on these datasets can be found in the suppl. material.\\n\\nImplementation.\\nIn all experiments, we use the Swin-based transformer [24] pre-trained on ImageNet [9] as the backbone for our PMTrans. The base learning rate is $5 \\\\times 10^{-6}$ with a batch size of 32, and we train models by 50 epochs. For VisDA-2017, we use a lower learning rate $1 \\\\times 10^{-6}$. We adopt AdamW [27] with a momentum of 0.9, and a weight decay of 0.05 as the optimizer. Furthermore, for fine-tuning purposes, we set the classifier (MLP) with a higher learning rate $1 \\\\times 10^{-5}$ for our main tasks and learn the trade-off parameter adaptively. For a fair comparison with prior works, we also conduct experiments with the same backbone Deit-based [37] as CDTrans [45], and ViT-based [11] as SSRT [36] on Office-31, Office-Home, and VisDA-2017. These two studies are trained for 60 and 100 epochs separately.\\n\\n4.2. Results\\n\\nWe compare PMTrans with the SoTA methods, including ResNet-based and ViT-based methods. The ResNet-based methods are FixBi [29], MCD [34], SWD [15], SCDA [19], BNM [8], and MDD [51]. The ViT-based methods are SSRT [36], CDTrans [45], and TVT [46].\\n\\nFor the ResNet-based methods, we utilize ResNet-50 as the backbone for the Office-Home, Office-31, and DomainNet datasets, and we adopt ResNet-101 for VisDA-2017 dataset. Note that each backbone is trained with the source data only and then tested with the target data.\\n\\nResults on Office-Home.\\nTab. 1 shows the quantitative results of methods using different backbones. As expected, our PMTrans framework achieves noticeable performance gains and surpasses TVT, SSRT, and CDTrans by a large margin. Importantly, our PMTrans achieves an improvement more than 5.4% accuracy over the Swin backbone and yields 89.0% accuracy. Interestingly, our proposed PMTrans can decrease the domain divergence effectively with Deit-based and ViT-based backbones. The results indicate that our method can obtain more robust transferable representations than the CNN-based and ViT-based methods.\\n\\nResults on Office-31.\\nTab. 2 shows the quantitative comparison with the CNN-based and ViT-based methods. Overall, our PMTrans achieves the best performance on each task with 95.3% accuracy and outperforms the SoTA methods with identical backbones. Numerically, PMTrans noticeably surpasses the SoTA methods with an increase of +1.4% accuracy over TVT, +2.7% accuracy over CDTrans, and +1.8% accuracy over SSRT, respectively.\\n\\nResults on VisDA-2017.\\nAs shown in Tab. 3, our PMTrans achieves 88.0% accuracy and outperforms the baseline by 11.2%. In particular, for the \u2018hard\u2019 categories, such as \u201cperson\u201d, our method consistently achieves a much higher performance boost from 29.0% to 70.3%. These improvements indicate that our method shows an excellent generalization capability and achieves comparable performance (88.0%).\"}"}
{"id": "CVPR-2023-1599", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Method       | Avg  |\\n|--------------|------|\\n| ResNet-50    | 55.1 |\\n| ResNet-55    | 61.9 |\\n| ResNet-60    | 31.2 |\\n| BNM          | 80.6 |\\n| MCD          | 17.9 |\\n| SWD          | 79.7 |\\n| FixBi        | 26.5 |\\n| TVT          | 81.0 |\\n| ViT          | 73.5 |\\n| Deit-based   | 8.5  |\\n| CDTrans-Deit | 52.4 |\\n| PMTrans-Deit | 70.4 |\\n| ViT-based    | 55.1 |\\n| SSRT-ViT     | 8.5  |\\n| PMTrans-ViT  | 70.4 |\\n| Swin-based   | 8.5  |\\n| Swin         | 52.4 |\\n| PMTrans-Swin | 70.4 |\\n\\nTable 3. Comparison with SoTA methods on VisDA-2017. The best performance is marked as **bold**.\\n\\n| Method       | Avg  |\\n|--------------|------|\\n| CGDM         | 28.1 |\\n| MDD          | 31.5 |\\n| SCDA         | 37.6 |\\n| MCD          | 20.5 |\\n| BNM          | 23.6 |\\n| SWD          | 27.2 |\\n| FixBi        | 20.5 |\\n| TVT          | 25.3 |\\n| ViT          | 27.2 |\\n| Deit-based   | 12.5 |\\n| CDTrans      | 13.1 |\\n| PMTrans      | 23.6 |\\n| ViT-based    | 20.5 |\\n| SSRT         | 23.6 |\\n| PMTrans-ViT  | 25.3 |\\n| Swin         | 12.5 |\\n| PMTrans-Swin | 23.6 |\\n\\nTable 4. Comparison with SoTA methods on DomainNet. The best performance is marked as **bold**.\\n\\n- **PMTrans** achieves a very high average accuracy on the most challenging DomainNet dataset, as shown in Tab.4. Overall, our proposed PMTrans outperforms the SoTA methods by +17.7% accuracy. Incredibly, PMTrans surpasses the SoTA methods in all the 30 sub-tasks, which demonstrates the strong ability of PMTrans to alleviate the large domain gap. Moreover, transferring knowledge is much more difficult when the domain gap becomes significant.\\n\\n4.3. Ablation Study\\n\\nSemi-supervised mixup loss. As shown in Tab.5, Swin with the semi-supervised mixup loss in the feature and label spaces outperforms the counterpart built on Swin with only source training by +1.0% and +4.3% on Office-Home dataset, respectively. The results indicate the effectiveness of the semi-supervised mixup loss for diminishing the domain discrepancy. Moreover, we observe that the CE loss yields better performance on the label space than that on the feature space. The reason is that the CE loss on the label space utilizing the class information performs better than on the feature space without the class information.\"}"}
{"id": "CVPR-2023-1599", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. t-SNE visualizations for task A on the Office-Home dataset. Source and target instances are shown in blue and red, respectively.\\n\\nTable 5. Effect of semi-supervised loss. The best performance is marked as bold.\\n\\n| Method                  | A \u2192 CA | CA \u2192 PA | RC \u2192 AC | AC \u2192 PC | PC \u2192 RP | RP \u2192 AP | AP \u2192 CP | CP \u2192 RR | RR \u2192 AR | AR \u2192 CR | CR \u2192 P |\\n|------------------------|--------|---------|---------|---------|---------|---------|---------|---------|---------|---------|--------|\\n| Avg                    | 72.7   | 87.1    | 90.6    | 84.3    | 87.3    | 89.3    | 80.6    | 68.6    | 90.3    | 84.8    | 69.4    | 91.3   |\\n| \u2713 \u2713                    | 73.9   | 87.5    | 91.0    | 85.3    | 87.9    | 89.9    | 82.8    | 72.1    | 91.2    | 86.3    | 74.1    | 92.4   |\\n| \u2713 \u2713 \u2713                  | 79.2   | 91.8    | 92.3    | 88.0    | 92.6    | 93.0    | 87.1    | 77.8    | 92.5    | 88.2    | 78.4    | 93.9   |\\n| Learn parameters       |        |         |         |         |         |         |         |         |         |         |        |        |\\n| Mixup                  | 81.3   | 92.9    | 92.8    | 88.4    | 93.4    | 93.2    | 87.9    | 80.4    | 93.0    | 89.0    | 80.9    | 94.8   |\\n| CutMix                 |        |         |         |         |         |         |         |         |         |         |         |        |\\n| PatchMix               | 79.4   | 92.4    | 92.6    | 87.5    | 92.8    | 92.4    | 86.8    | 80.3    | 92.5    | 88.2    | 79.7    | 95.4   |\\n| Mixup                  | 79.2   | 91.2    | 92.2    | 87.6    | 91.8    | 91.8    | 86.0    | 77.8    | 92.6    | 88.2    | 78.4    | 94.1   |\\n| CutMix                 | 81.3   | 92.9    | 92.8    | 88.4    | 93.4    | 93.2    | 87.9    | 80.4    | 93.0    | 89.0    | 80.9    | 94.8   |\\n\\nTable 6. Effect of learning parameters. The best performance is marked as bold.\\n\\n| Method                  | A \u2192 CA | CA \u2192 PA | RC \u2192 AC | AC \u2192 PC | PC \u2192 RP | RP \u2192 AP | AP \u2192 CP | CP \u2192 RR | RR \u2192 AR | AR \u2192 CR | CR \u2192 P |\\n|------------------------|--------|---------|---------|---------|---------|---------|---------|---------|---------|---------|--------|\\n| Avg                    | 79.9   | 92.0    | 92.3    | 88.6    | 92.6    | 92.4    | 86.9    | 79.0    | 92.4    | 88.2    | 79.3    | 94.0   |\\n| Beta(1,1)              |        |         |         |         |         |         |         |         |         |         |        |        |\\n| Beta(2,2)              |        |         |         |         |         |         |         |         |         |         |        |        |\\n| Mixup                  | 81.3   | 92.9    | 92.8    | 88.4    | 93.4    | 93.2    | 87.9    | 80.4    | 93.0    | 89.0    | 80.9    | 94.8   |\\n| CutMix                 |        |         |         |         |         |         |         |         |         |         |        |        |\\n| PatchMix               | 81.3   | 92.9    | 92.8    | 88.4    | 93.4    | 93.2    | 87.9    | 80.4    | 93.0    | 89.0    | 80.9    | 94.8   |\\n\\nTable 7. Effect of PatchMix. The best performance is marked as bold.\\n\\nLearning hyperparameters of mixup.\\n\\nTable 6 shows the ablation results for the effects of learning hyperparameters of the Beta distribution on the Office-Home. We compare the learning hyperparameters of mixup with fixed parameters, such as Beta(1,1) and Beta(2,2). The proposed method achieves +0.9% and +0.8% accuracy increment compared with that based on Beta(1,1) and Beta(2,2). The results demonstrate that learning to estimate the distribution to build up the intermediate domain facilitates domain alignment.\\n\\nPatchMix. Comparisons of PMTrans with Mixup [49] and CutMix [47] are shown in Tab.7. PMTrans outperforms Mixup and CutMix by +0.7% and +1.4% accuracy on the Office-Home dataset, demonstrating that PatchMix can capture the global and local mixture information better than the global mixture Mixup and local mixture CutMix methods.\\n\\nVisualization. In Fig.5, we visualize the features learned by Swin-based, PMTrans-Swin, PMTrans-ViT, and PMTrans-Deit on task A \u2192 C from the Office-Home dataset via the t-SNE [10]. Compared with Swin-based and PMTrans-Swin, our PMTrans model can better align the two domains by constructing the intermediate domain to bridge them. Moreover, comparisons between PMTrans with different transformer backbones reveal that PMTrans works successfully with different backbones on UDA tasks. Due to the page limit, more experiments and analyses can be found in the suppl. material.\\n\\n5. Conclusion and Future Work\\n\\nIn this paper, we proposed a novel method, PMTrans, an optimization solution for UDA from a game perspective. Specifically, we first proposed a novel ViT-based module called PatchMix that effectively built up the intermediate domain to learn discriminative domain-invariant representations for domains. And the two semi-supervised mixup losses were proposed to assist in finding the Nash Equilibria. Moreover, we leveraged attention maps from ViT to reweight the label of each patch by its significance. PMTrans achieved the SoT A results on four benchmark UDA datasets, outperforming the SoTA methods by a large margin. In the near future, we plan to implement our PatchMix and the two semi-supervised mixup losses to solve self-supervised and semi-supervised learning problems. We will also exploit our method to tackle the challenging downstream tasks, e.g., semantic segmentation and object detection.\\n\\nAcknowledgment. This work was supported by the National Natural Science Foundation of China (NSFC) under Grant No. NSFC22FYT45.\"}"}
{"id": "CVPR-2023-1599", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] David Acuna, Marc T. Law, Guojun Zhang, and Sanja Fidler. Domain adversarial training: A game perspective. CoRR, abs/2202.05352, 2022.\\n\\n[2] David Acuna, Marc T. Law, Guojun Zhang, and Sanja Fidler. Domain adversarial training: A game perspective. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022.\\n\\n[3] Tamer Bas\u00b8ar and Geert Jan Olsder. Dynamic noncooperative game theory. SIAM, 1998.\\n\\n[4] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv\u00b4eJ \u00b4egou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 9630\u20139640. IEEE, 2021.\\n\\n[5] Jieneng Chen, Shuyang Sun, Ju He, Philip H. S. Torr, Alan L. Yuille, and Song Bai. Transmix: Attend to mix for vision transformers. CoRR, abs/2111.09833, 2021.\\n\\n[6] Nicolas Courty, R\u00b4emi Flamary, Amaury Habrard, and Alain Rakotomamonjy. Joint distribution optimal transportation for domain adaptation. In Advances in Neural Information Processing Systems 30: Annual Conference on Neural Information Processing Systems 2017, December 4-9, 2017, Long Beach, CA, USA, pages 3730\u20133739, 2017.\\n\\n[7] Nicolas Courty, R\u00b4emi Flamary, Devis Tuia, and Alain Rakotomamonjy. Optimal transport for domain adaptation. IEEE Trans. Pattern Anal. Mach. Intell., 39(9):1853\u20131865, 2017.\\n\\n[8] Shuhao Cui, Shuhui Wang, Junbao Zhuo, Liang Li, Qingming Huang, and Qi Tian. Towards discriminability and diversity: Batch nuclear-norm maximization under label insufficient situations. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 3940\u20133949. Computer Vision Foundation / IEEE, 2020.\\n\\n[9] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2009), 20-25 June 2009, Miami, Florida, USA, pages 248\u2013255. IEEE Computer Society, 2009.\\n\\n[10] Jeff Donahue, Yangqing Jia, Oriol Vinyals, Judy Hoffman, Ning Zhang, Eric Tzeng, and Trevor Darrell. Decaf: A deep convolutional activation feature for generic visual recognition. In Proceedings of the 31th International Conference on Machine Learning, ICML, volume 32 of JMLR Workshop and Conference Proceedings, pages 647\u2013655. JMLR.org, 2014.\\n\\n[11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ArXiv, abs/2010.11929, 2021.\\n\\n[12] Zhekai Du, Jingjing Li, Hongzu Su, Lei Zhu, and Ke Lu. Cross-domain gradient discrepancy minimization for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 3937\u20133946. Computer Vision Foundation / IEEE, 2021.\\n\\n[13] Y aroslav Ganin, Evgeniya Ustinova, Hana Ajakan, Pascal Germain, Hugo Larochelle, Franc\u00b8ois Laviolette, Mario Marchand, and Victor S. Lempitsky. Domain-adversarial training of neural networks. J. Mach. Learn. Res., 17:59:1\u201359:35, 2016.\\n\\n[14] Guoliang Kang, Lu Jiang, Yi Y ang, and Alexander G. Hauptmann. Contrastive adaptation network for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 4893\u20134902. Computer Vision Foundation / IEEE, 2019.\\n\\n[15] Chen-Y u Lee, Tanmay Batra, Mohammad Haris Baig, and Daniel Ulbricht. Sliced wasserstein discrepancy for unsupervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2019, Long Beach, CA, USA, June 16-20, 2019, pages 10285\u201310295. Computer Vision Foundation / IEEE, 2019.\\n\\n[16] Jingjing Li, Erpeng Chen, Zhengming Ding, Lei Zhu, Ke Lu, and Heng Tao Shen. Maximum density divergence for domain adaptation. IEEE Trans. Pattern Anal. Mach. Intell., 43(11):3918\u20133930, 2021.\\n\\n[17] Jichang Li, Guanbin Li, Y emin Shi, and Yizhou Y u. Cross-domain adaptive clustering for semi-supervised domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 2505\u20132514. Computer Vision Foundation / IEEE, 2021.\\n\\n[18] Shuang Li, Mixue Xie, Kaixiong Gong, Chi Harold Liu, Y ulin Wang, and Wei Li. Transferable semantic augmentation for domain adaptation. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 11516\u201311525. Computer Vision Foundation / IEEE, 2021.\\n\\n[19] Shuang Li, Mixue Xie, Fangrui Lv, Chi Harold Liu, Jian Liang, Chen Qin, and Wei Li. Semantic concentration for domain adaptation. In 2021 IEEE/CVF International Conference on Computer Vision, ICCV 2021, Montreal, QC, Canada, October 10-17, 2021, pages 9082\u20139091. IEEE, 2021.\\n\\n[20] Jian Liang, Dapeng Hu, and Jiashi Feng. Do we really need to access the source data? source hypothesis transfer for unsupervised domain adaptation. In Proceedings of the 37th International Conference on Machine Learning, ICML 2020, 13-18 July 2020, Virtual Event, volume 119 of Proceedings of Machine Learning Research, pages 6028\u20136039. PMLR, 2020.\\n\\n[21] Jian Liang, Dapeng Hu, and Jiashi Feng. Domain adaptation with auxiliary target domain-oriented classifier. In IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2021, virtual, June 19-25, 2021, pages 16632\u201316642. Computer Vision Foundation / IEEE, 2021.\\n\\n[22] Jihao Liu, Boxiao Liu, Hang Zhou, Hongsheng Li, and Y u Liu. Tokenmix: Rethinking image mixing for data augmentation in vision transformers. In Computer Vision - ECCV 2022 - 17th European Conference, Tel Aviv, Israel, October 23-27, 2022, Proceedings, Part XXVI, volume 13686 of Lecture Notes in Computer Science, pages 455\u2013471. Springer, 2022.\"}"}
{"id": "CVPR-2023-1599", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":null}"}
{"id": "CVPR-2023-1599", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Renjun Xu, Pelen Liu, Liyan Wang, Chao Chen, and Jindong Wang. Reliable weighted optimal transport for unsupervised domain adaptation. In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, June 13-19, 2020, pages 4393\u20134402. Computer Vision Foundation / IEEE, 2020.\\n\\nTongkun Xu, Weihua Chen, Pichao Wang, Fan Wang, Hao Li, and Rong Jin. Cdtrans: Cross-domain transformer for unsupervised domain adaptation. CoRR, abs/2109.06165, 2021.\\n\\nJinyu Yang, Jingjing Liu, Ning Xu, and Junzhou Huang. TVT: transferable vision transformer for unsupervised domain adaptation. CoRR, abs/2108.05988, 2021.\\n\\nSangdoo Yun, Dongyoon Han, Sanghyuk Chun, Seong Joon Oh, Youngjoon Yoo, and Junsuk Choe. Cutmix: Regularization strategy to train strong classifiers with localizable features. In 2019 IEEE/CVF International Conference on Computer Vision, ICCV 2019, Seoul, Korea (South), October 27 - November 2, 2019, pages 6022\u20136031. IEEE, 2019.\\n\\nWerner Zellinger, Thomas Grubinger, Edwin Lughofer, Thomas Natschl\u00a8ager, and Susanne Saminger-Platz. Central moment discrepancy (CMD) for domain-invariant representation learning. In 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings. OpenReview.net, 2017.\\n\\nHongyi Zhang, Moustapha Ciss\u00b4e, Y ann N. Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. In 6th International Conference on Learning Representations, ICLR 2018, Vancouver, BC, Canada, April 30 - May 3, 2018, Conference Track Proceedings. OpenReview.net, 2018.\\n\\nY abin Zhang, Bin Deng, Hui Tang, Lei Zhang, and Kui Jia. Unsupervised multi-class domain adaptation: Theory, algorithms, and practice. IEEE Trans. Pattern Anal. Mach. Intell., 44(5):2775\u20132792, 2022.\\n\\nYuchen Zhang, Tianle Liu, Mingsheng Long, and Michael I. Jordan. Bridging theory and algorithm for domain adaptation. In Proceedings of the 36th International Conference on Machine Learning, ICML 2019, 9-15 June 2019, Long Beach, California, USA, volume 97 of Proceedings of Machine Learning Research, pages 7404\u20137413. PMLR, 2019.\\n\\nY ongchun Zhu, Fuzhen Zhuang, and Deqing Wang. Aligning domain-specific distribution and classifier for cross-domain classification from multiple sources. In The Thirty-Third AAAI Conference on Artificial Intelligence, AAAI 2019, The Thirty-First Innovative Applications of Artificial Intelligence Conference, IAAI 2019, The Ninth AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019, Honolulu, Hawaii, USA, January 27 - February 1, 2019, pages 5989\u20135996. AAAI Press, 2019.\\n\\nY ongchun Zhu, Fuzhen Zhuang, Jindong Wang, Guolin Ke, Jingwu Chen, Jiang Bian, Hui Xiong, and Qing He. Deep subdomain adaptation network for image classification. IEEE Trans. Neural Networks Learn. Syst., 32(4):1713\u20131722, 2021.\"}"}
{"id": "CVPR-2023-1599", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Patch-Mix Transformer for Unsupervised Domain Adaptation: A Game Perspective\\nJinjing Zhu\\n* Haotian Bai\\n* Lin Wang\\n1 AI Thrust, HKUST(GZ) 2 Dept. of CSE, HKUST\\nzhujinjing.hkust@gmail.com, haotianwhite@outlook.com, linwang@ust.hk\\n\\nAbstract\\nEndeavors have been recently made to leverage the vision transformer (ViT) for the challenging unsupervised domain adaptation (UDA) task. They typically adopt the cross-attention in ViT for direct domain alignment. However, as the performance of cross-attention highly relies on the quality of pseudo labels for targeted samples, it becomes less effective when the domain gap becomes large. We solve this problem from a game theory's perspective with the proposed model dubbed as PMTrans, which bridges source and target domains with an intermediate domain. Specifically, we propose a novel ViT-based module called PatchMix that effectively builds up the intermediate domain, i.e., probability distribution, by learning to sample patches from both domains based on the game-theoretical models. This way, it learns to mix the patches from the source and target domains to maximize the cross entropy (CE), while exploiting two semi-supervised mixup losses in the feature and label spaces to minimize it. As such, we interpret the process of UDA as a min-max CE game with three players, including the feature extractor, classifier, and PatchMix, to find the Nash Equilibria. Moreover, we leverage attention maps from ViT to re-weight the label of each patch by its importance, making it possible to obtain more domain-discriminative feature representations. We conduct extensive experiments on four benchmark datasets, and the results show that PMTrans significantly surpasses the ViT-based and CNN-based SoTA methods by +3.6% on Office-Home, +1.4% on Office-31, and +17.7% on DomainNet, respectively.\\n\\n1. Introduction\\nConvolutional neural networks (CNNs) have achieved tremendous success on numerous vision tasks; however, they still suffer from the limited generalization capability to a new domain due to the domain shift problem [50]. Unsupervised domain adaptation (UDA) tackles this issue by transferring knowledge from a labeled source domain to an unlabeled target domain [30]. A significant line of solutions reduces the domain gap based on the category-level alignment which produces pseudo labels for the target samples, such as metric learning [14,53], adversarial training [12,17,34], and optimal transport [44]. Furthermore, several works [11, 36] explore the potential of ViT for the non-trivial UDA task. Recently, CDTrans [45] exploits the cross-attention in ViT for direct domain alignment, buttressed by the crafted pseudo labels for target samples. However, CDTrans has a distinct limitation: as the performance of cross-attention highly depends on the quality of pseudo labels, it becomes less effective when the domain gap becomes large. As shown in Fig.1, due to the significant gap between the domain qdr and the other domains, aligning distributions directly between them performs poorly.\\n\\nIn this paper, we probe a new problem for UDA: how to smoothly bridge the source and target domains by constructing an intermediate domain with an effective ViT-based solution? The intuition behind this is that, compared to direct aligning domains, alleviating the domain gap between the intermediate and source/target domain can facilitate this process.\\n\\nFigure 1. The classification accuracy of our PMTrans surpasses the SoTA methods by +17.7% on the most challenging DomainNet dataset. Note that the target tasks treat one domain of DomainNet as the target domain and the others as the source domains.\"}"}
{"id": "CVPR-2023-1599", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. PMTrans builds up the intermediate domain (green patches) via a novel PatchMix module by learning to sample patches from the source (blue patches) and target (pink patches) domains. PatchMix tries to maximize the CE (\u2191) between the intermediate domain and source/target domain, while the feature extractor and classifier try to minimize it (\u2193) for aligning domains.\\n\\nWe propose a novel and effective method, called PMTrans (PatchMix Transformer) to construct the intermediate representations. Overall, PMTrans interprets the process of domain alignment as a min-max cross entropy (CE) game with three players, i.e., the feature extractor, a classifier, and a PatchMix module, to find the Nash Equilibria. Importantly, the PatchMix module is proposed to effectively build up the intermediate domain, i.e., probability distribution, by learning to sample patches from both domains with weights generated from a learnable Beta distribution based on the game-theoretical models [1, 3, 28], as shown in Fig.2. That is, we aim to learn to mix patches from two domains to maximize the CE between the intermediate domain and source/target domain. Moreover, two semi-supervised mixup losses in the feature and label spaces are proposed to minimize the CE. Interestingly, we conclude that the source and target domains are aligned if mixing the patch representations from two domains is equivalent to mixing the corresponding labels. Therefore, the domain discrepancy can be measured based on the CE between the mixed patches and mixed labels. Eventually, the three players have no incentive to change their parameters to disturb CE, meaning the source and target domains are well aligned.\\n\\nUnlike existing mixup methods [38, 47, 49], our proposed PatchMix subtly learns to combine the element-wise global and local mixture by mixing patches from the source and target domains for ViT-based UDA. Moreover, we leverage the class activation mapping (CAM) from ViT to allocate the semantic information to re-weight the label of each patch, thus enabling us to obtain more domain-discriminative features.\\n\\nWe conduct experiments on four benchmark datasets, including Office-31 [33], Office-Home [40], VisDA-2017 [32], and DomainNet [31]. The results show that the performance of PMTrans significantly surpasses that of the ViT-based [36, 45, 46] and CNN-based SoTA methods [18, 29, 35] by +3.6% on Office-Home, +1.4% on Office-31, and +17.7% on DomainNet (See Fig.1), respectively.\\n\\nOur main contributions are four-fold: (I) We propose a novel ViT-based UDA framework, PMTrans, to effectively bridge source and target domains by constructing the intermediate domain. (II) We propose PatchMix, a novel module to build up the intermediate domain via the game-theoretical models. (III) We propose two semi-supervised mixup losses in the feature and label spaces to reduce CE in the min-max CE game. (IV) Our PMTrans surpasses the prior methods by a large margin on three benchmark datasets.\\n\\n2. Related Work\\n\\nUnsupervised Domain Adaptation. The prevailing UDA methods focus on domain alignment and learning discriminative domain-invariant features via metric learning, domain adversarial training, and optimal transport. Firstly, the metric learning-based methods aim to reduce the domain discrepancy by learning the domain-invariant feature representations using various metrics. For instance, some methods [14, 25, 26, 52] use the maximum mean discrepancy (MMD) loss to measure the divergence between different domains. In addition, the central moment discrepancy (CMD) loss [48] and maximum density divergence (MDD) loss [16] are also proposed to align the feature distributions. Secondly, the domain adversarial training methods learn the domain-invariant representations to encourage samples from different domains to be non-discriminative with respect to the domain labels via an adversarial loss [13, 42, 43]. The third type of approach aims to minimize the cost transported from the source to the target distribution by finding an optimal coupling cost to mitigate the domain shift [6, 7]. Unfortunately, these methods are not robust enough for the noisy pseudo target labels for accurate domain alignment.\\n\\nDifferent from these mainstream UDA methods and [2], we interpret the process of UDA as a min-max CE game and find the Nash Equilibria for domain alignment with an intermediate domain and a pure ViT-based solution.\\n\\nMixup. It is an effective data augmentation technique to prevent models from over-fitting by linearly interpolating two input data. Mixup types can be categorized into global mixup (e.g., Mixup [49] and Manifold-Mixup [41]) and local mixup (CutMix [47], saliency-CutMix [38], TransMix [5], and Tokenmix [22]). In CNN-based UDA tasks, several works [29, 42, 43] also use the mixup technique by linearly mixing the source and target domain data.\\n\\nIn comparison, we unify the global and local mixup in our PMTrans framework by learning to form a mixed patch from the source/target patch as the input to ViT. We learn the hyperparameters of the mixup ratio for each patch, which is the first attempt to interpolate patches based on the distribution estimation. Accordingly, we propose PatchMix which effectively builds up the intermediate domain by sampling patches from both domains based on the game-theoretical models.\"}"}
{"id": "CVPR-2023-1599", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"we assume that the two domains share the same label space. With this in mind, we propose a two-step framework that utilizes the Vision Transformer (ViT) [39] to effectively interpret the process of domain alignment.\\n\\n**PMTrans**\\n\\nPMTrans proposes a two-step framework that utilizes the Vision Transformer (ViT) [39] to effectively interpret the process of domain alignment. The first step is to construct an intermediate domain via a three-player game. The second step is to describe the proposed PMTrans operation on two pairs of randomly drawn samples.\\n\\n### 3.1. PMTrans: Theoretical Analysis\\n\\n#### 3.1.1. PatchMix\\n\\n**Definition 1 (PatchMix):** Let \\\\( x_s \\\\) be the \\\\( k \\\\)-th source patch, \\\\( x_t \\\\) be the \\\\( k \\\\)-th target patch, and \\\\( \\\\lambda \\\\) be a linear interpolation weight between 0 and 1. Then, we can write:\\n\\n\\\\[\\n(\\\\lambda \\\\times x_s + (1 - \\\\lambda) \\\\times x_t, \\\\lambda \\\\times y_s + (1 - \\\\lambda) \\\\times y_t) = \\\\lambda \\\\times (x_s, y_s) + (1 - \\\\lambda) \\\\times (x_t, y_t)\\n\\\\]\\n\\nThis linear interpolation allows us to mix both samples and their corresponding labels. We use this technique to interpolate their labels. As a result, we mix both samples and their labels.\\n\\nFurthermore, we calculate the image-level importance by aggregating patch weights. The image-level importance is a weighted sum of patch-level importance weights. The patch-level importance weights are calculated as the product of patch-level segmentation weights and corresponding label probabilities.\\n\\n**Theorem 1**\\n\\nIn UDA, denote a labeled source set \\\\( D_s \\\\) and an unlabeled target set \\\\( D_t \\\\). We define and introduce a mixed representation with a mixup ratio for building the intermediate domain. Then we probe to construct an intermediate domain to bridge the gap between the source and target domains. Here, \\\\( x_s \\\\) is the \\\\( k \\\\)-th sample in the source domain, and \\\\( y_s \\\\) is its label. We also define \\\\( x_t \\\\) and \\\\( y_t \\\\) to denote the \\\\( k \\\\)-th sample in the target domain. Theorem 1 indicates that the \\\\( k \\\\)-th patch of the intermediate domain is sampled from \\\\( x_s \\\\) with probability \\\\( \\\\lambda \\\\) and \\\\( x_t \\\\) with probability \\\\( 1 - \\\\lambda \\\\), where \\\\( \\\\lambda \\\\) is sampled from a learnable Beta distribution as in Eq. (2):\\n\\n\\\\[\\n\\\\lambda \\\\sim \\\\text{Beta}(\\\\beta, \\\\gamma)\\n\\\\]\\n\\nTheorem 1 is a linear function from \\\\( \\\\Omega \\\\) to \\\\( \\\\Omega \\\\), which is utilized to interpret UDA as a min-max CE game among three players, namely the feature extractor \\\\( f \\\\), the classifier \\\\( c \\\\), and the mixup ratio sampled from a learnable Beta distribution as in Eq. (2):\\n\\n\\\\[\\n\\\\lambda \\\\sim \\\\text{Beta}(\\\\beta, \\\\gamma)\\n\\\\]\\n\\nDifferently, Definition 1 (Candidate Mixup) defines \\\\( \\\\Omega \\\\) to denote all other parameters/players except the feature extractor \\\\( f \\\\), the classifier \\\\( c \\\\), and the mixup ratio \\\\( \\\\lambda \\\\). Theorem 1 indicates that the \\\\( k \\\\)-th patch of the intermediate domain is sampled from \\\\( x_s \\\\) with probability \\\\( \\\\lambda \\\\) and \\\\( x_t \\\\) with probability \\\\( 1 - \\\\lambda \\\\), where \\\\( \\\\lambda \\\\) is sampled from a learnable Beta distribution as in Eq. (2).\\n\\n**Proof of Theorem 1:**\\n\\nTheorem 1 is a linear function from \\\\( \\\\Omega \\\\) to \\\\( \\\\Omega \\\\), which is utilized to interpret UDA as a min-max CE game among three players, namely the feature extractor \\\\( f \\\\), the classifier \\\\( c \\\\), and the mixup ratio sampled from a learnable Beta distribution as in Eq. (2):\\n\\n\\\\[\\n\\\\lambda \\\\sim \\\\text{Beta}(\\\\beta, \\\\gamma)\\n\\\\]\\n\\nDifferently, Definition 1 (Candidate Mixup) defines \\\\( \\\\Omega \\\\) to denote all other parameters/players except the feature extractor \\\\( f \\\\), the classifier \\\\( c \\\\), and the mixup ratio \\\\( \\\\lambda \\\\). Theorem 1 indicates that the \\\\( k \\\\)-th patch of the intermediate domain is sampled from \\\\( x_s \\\\) with probability \\\\( \\\\lambda \\\\) and \\\\( x_t \\\\) with probability \\\\( 1 - \\\\lambda \\\\), where \\\\( \\\\lambda \\\\) is sampled from a learnable Beta distribution as in Eq. (2).\"}"}
{"id": "CVPR-2023-1599", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. Overview of the proposed PMTrans framework. It consists of three players: the PatchMix module empowered by a patch embedding ($\\\\text{Emb}$) layer and a learnable Beta distribution ($\\\\text{Beta}$), ViT encoder, and classifier.\\n\\nCost function $J_m$ is represented as $J_F(\\\\omega_F, \\\\omega_{-F}) = L_{\\\\text{cls}}(\\\\omega_F, \\\\omega_C) + \\\\alpha_{\\\\text{CE}_{s,i,t}}(\\\\omega)$, $J_C(\\\\omega_C, \\\\omega_{-C}) = L_{\\\\text{cls}}(\\\\omega_F, \\\\omega_C) + \\\\alpha_{\\\\text{CE}_{s,i,t}}(\\\\omega)$, $J_P(\\\\omega_P, \\\\omega_{-P}) = -\\\\alpha_{\\\\text{CE}_{s,i,t}}(\\\\omega)$, (4)\\n\\nwhere $\\\\alpha$ is the trade-off parameter, $\\\\ell$ is the supervised classification loss for the source domain, and $\\\\text{CE}_{s,i,t}(\\\\omega)$ is the discrepancy between the intermediate domain and the source/target domain. The definitions of $L_{\\\\text{cls}}(\\\\omega_F, \\\\omega_C)$ and $\\\\text{CE}_{s,i,t}(\\\\omega)$ are shown in Sec.3.2. As illustrated in Eq.4, the game is essentially a min-max process, i.e., a competition for the player $P$ against both players $F$ and $C$. Specifically, as depicted in Fig.3, $P$ strives to diverge while $F$ and $C$ try to align domain distributions, which is a min-max process on $\\\\text{CE}$. In this min-max CE game, each player behaves selfishly to reduce its cost function, and this competition will possibly end with a situation where no one has anything to gain by changing only one's strategy. This situation is called Nash Equilibrium (NE) in game theory.\\n\\nDefinition 2 (Nash Equilibrium): The equilibrium states each player's strategy is the best response to other players. And a point $\\\\omega^* \\\\in \\\\Omega$ is Nash Equilibrium if $\\\\forall \\\\omega_m \\\\in \\\\Omega$, $\\\\forall m \\\\in \\\\{F, C, P\\\\}$, s.t. $J_m(\\\\omega^*_m, \\\\omega^*_{-m}) \\\\leq J_m(\\\\omega_m, \\\\omega^*_{-m})$.\\n\\nIntuitively, in our case, NE means that no player has the incentive to change its own parameters, as there is no additional pay-off.\\n\\n3.2. The Proposed Framework\\n\\nOverview. Fig. 3 illustrates the framework of our proposed PMTrans, which consists of a ViT encoder, a classifier, and a PatchMix module. PatchMix module is utilized to maximize the CE between the intermediate domain and source/target domain, conversely, two semi-supervised mixup losses in the feature and label spaces are proposed to minimize CE. Finally, a three-player game containing feature extractor, classifier, and PatchMix module, minimizes and maximizes the CE for aligning distributions between the source and target domains.\\n\\nPatchMix. As shown in Fig. 3, the PatchMix module is proposed to construct the intermediate domain, buttressed by Definition 1. In detail, the patch embedding layer in PatchMix transforms input images from source/target domains into patches. And the ViT encoder aims to extract features from the patch sequences. The classifier maps the outputs of ViT encoder to make predictions, each of which is exploited to select the feature map to re-weight the patch sequences. The PatchMix with a learnable Beta distribution aims to maximize the CE between the intermediate and source/target domain, and is presented as follows.\\n\\nWhen exploiting PatchMix to construct the intermediate domain, it is worth noting that not all patches have equal contributions for the label assignment. As Chen et al.[5] observed, the mixed image has no valid objects due to the random process while there is still a response in the label space. To remedy this issue, we re-weight $P^{\\\\lambda}(y_s, y_t)$ in Definition 1 with the normalized attention score $a_k$. For the implementation details of attention scores, refer to the suppl. material. The re-scaled $P^{\\\\lambda}(y_s, y_t)$ is defined as $P^{\\\\lambda}(y_s, y_t) = \\\\lambda_s y_s + \\\\lambda_t y_t$, where $\\\\lambda_s = \\\\frac{\\\\sum_{n_k=1}^{\\\\lambda_k a_s}}{\\\\sum_{n_k=1}^{\\\\lambda_k a_s} + \\\\sum_{n_k=1}^{(1-\\\\lambda_k) a_t}}$, $\\\\lambda_t = \\\\frac{\\\\sum_{n_k=1}^{(1-\\\\lambda_k) a_t}}{\\\\sum_{n_k=1}^{\\\\lambda_k a_s} + \\\\sum_{n_k=1}^{(1-\\\\lambda_k) a_t}}$. \"}"}
