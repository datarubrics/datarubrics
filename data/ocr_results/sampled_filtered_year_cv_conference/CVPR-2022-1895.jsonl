{"id": "CVPR-2022-1895", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Youngmin Baek, Seung Shin, Jeonghun Baek, Sungrae Park, Junyeop Lee, Daehyun Nam, and Hwalsuk Lee. Character region attention for text spotting. ArXiv, abs/2007.09629, 2020.\\n\\n[2] Christian Bartz, Haojin Yang, and Christoph Meinel. See: towards semi-supervised end-to-end scene text recognition. In Thirty-second aaai conference on artificial intelligence, 2018.\\n\\n[3] Michal Busta, Lukas Neumann, and Jiri Matas. Deep textspotter: An end-to-end trainable scene text localization and recognition framework. In Proceedings of the IEEE international conference on computer vision, pages 2204\u20132212, 2017.\\n\\n[4] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-to-end object detection with transformers. In European Conference on Computer Vision, pages 213\u2013229. Springer, 2020.\\n\\n[5] Xiaoxue Chen, Lianwen Jin, Yuanzhi Zhu, Canjie Luo, and Tianwei Wang. Text recognition in the wild: A survey. ACM Computing Surveys (CSUR), 54(2):1\u201335, 2021.\\n\\n[6] Xiangrong Chen and A.L. Yuille. Detecting and reading text in natural scenes. In Proceedings of the 2004 IEEE Computer Society Conference on Computer Vision and Pattern Recognition, 2004. CVPR 2004., volume 2, pages II\u2013II, 2004.\\n\\n[7] Chee Kheng Ch'ng and Chee Seng Chan. Total-text: A comprehensive dataset for scene text detection and recognition. In 2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR), volume 1, pages 935\u2013942. IEEE, 2017.\\n\\n[8] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao Zhang, Junyu Han, Errui Ding, et al. Icdar2019 robust reading challenge on arbitrary-shaped text-rrc-art. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 1571\u20131576. IEEE, 2019.\\n\\n[9] Zhigang Dai, Bolun Cai, Yugeng Lin, and Junying Chen. Up-detr: Unsupervised pre-training for object detection with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1601\u20131610, 2021.\\n\\n[10] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020.\\n\\n[11] Wei Feng, Wenhao He, Fei Yin, Xu-Yao Zhang, and Cheng-Lin Liu. Textdragon: An end-to-end framework for arbitrary shaped text spotting. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2019.\\n\\n[12] Ankush Gupta, Andrea Vedaldi, and Andrew Zisserman. Synthetic data for text localisation in natural images. In IEEE Conference on Computer Vision and Pattern Recognition, 2016.\\n\\n[13] Max Jaderberg, Karen Simonyan, Andrea Vedaldi, and Andrew Zisserman. Reading text in the wild with convolutional neural networks. International journal of computer vision, 116(1):1\u201320, 2016.\\n\\n[14] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Deep features for text spotting. In European conference on computer vision, pages 512\u2013528. Springer, 2014.\\n\\n[15] Klara Janouskova, Jiri Matas, Lluis Gomez, and Dimosthenis Karatzas. Text recognition-real world data and where to find them. In 2020 25th International Conference on Pattern Recognition (ICPR), pages 4489\u20134496. IEEE, 2021.\\n\\n[16] Dimosthenis Karatzas, Lluis Gomez-Bigorda, Anguelos Nicolaou, Suman Ghosh, Andrew Bagdanov, Masakazu Iwamura, Jiri Matas, Lukas Neumann, Vijay Ramaseshan Chandrasekhar, Shijian Lu, et al. Icdar 2015 competition on robust reading. In 2015 13th International Conference on Document Analysis and Recognition (ICDAR), pages 1156\u20131160. IEEE, 2015.\\n\\n[17] Dimosthenis Karatzas, Faisal Shafait, Seiichi Uchida, Masakazu Iwamura, Lluis Gomez i Bigorda, Sergi Robles Mestre, Joan Mas, David Fernandez Mota, Jon Almaz\u00c1n Almaz\u00c1n, and Llu\u00c1\u0131s Pere de las Heras. Icdar 2013 robust reading competition. In 2013 12th International Conference on Document Analysis and Recognition, pages 1484\u20131493, 2013.\\n\\n[18] Harold W Kuhn. The hungarian method for the assignment problem. Naval research logistics quarterly, 2(1-2):83\u201397, 1955.\\n\\n[19] Huiping Li, David Doermann, and Omid Kia. Automatic text detection and tracking in digital video. IEEE transactions on image processing, 9(1):147\u2013156, 2000.\\n\\n[20] Hui Li, Peng Wang, and Chunhua Shen. Towards end-to-end text spotting with convolutional recurrent neural networks. 2017 IEEE International Conference on Computer Vision (ICCV), pages 5248\u20135256, 2017.\\n\\n[21] Minghui Liao, Pengyuan Lyu, Minghang He, Cong Yao, Wenhao Wu, and Xiang Bai. Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(2):532\u2013548, 2021.\\n\\n[22] Minghui Liao, Guan Pang, Jing Huang, Tal Hassner, and Xiang Bai. Mask textspotter v3: Segmentation proposal network for robust scene text spotting. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XI 16, pages 706\u2013722. Springer, 2020.\\n\\n[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Doll\u00c1r, and C Lawrence Zitnick. Microsoft coco: Common objects in context. In European conference on computer vision, pages 740\u2013755. Springer, 2014.\\n\\n[24] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander C Berg. Ssd: Single shot multibox detector. In European conference on computer vision, pages 21\u201337. Springer, 2016.\"}"}
{"id": "CVPR-2022-1895", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[26] Yuliang Liu, Hao Chen, Chunhua Shen, Tong He, Lianwen Jin, and Liangwei Wang. Abcnet: Real-time scene text spotting with adaptive bezier-curve network. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9809\u20139818, 2020.\\n\\n[27] Yuliang Liu, Chunhua Shen, Lianwen Jin, Tong He, Peng Chen, Chongyu Liu, and Hao Chen. Abcnet v2: Adaptive bezier-curve network for real-time end-to-end text spotting. arXiv preprint arXiv:2105.03620, 2021.\\n\\n[28] Pengyuan Lyu, Minghui Liao, Cong Yao, Wenhao Wu, and Xiang Bai. Mask textspotter: An end-to-end trainable neural network for spotting text with arbitrary shapes. In Proceedings of the European Conference on Computer Vision (ECCV), September 2018.\\n\\n[29] Qi Ming, Lingjuan Miao, Zhiqiang Zhou, Xue Yang, and Yunpeng Dong. Optimization for arbitrary-oriented object detection via representation invariance loss. IEEE Geoscience and Remote Sensing Letters, 2021.\\n\\n[30] Liang Qiao, Ying Chen, Zhanzhan Cheng, Yunlu Xu, Yi Niu, Shiliang Pu, and Fei Wu. Mango: A mask attention guided one-stage scene text spotter. arXiv preprint arXiv:2012.04350, 2020.\\n\\n[31] Siyang Qin, Alessandro Bissacco, Michalis Raptis, Yasuhisa Fujii, and Ying Xiao. Towards unconstrained end-to-end text spotting. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 4704\u20134714, 2019.\\n\\n[32] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779\u2013788, 2016.\\n\\n[33] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28:91\u201399, 2015.\\n\\n[34] Baoguang Shi, Xiang Bai, and Cong Yao. An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition. IEEE transactions on pattern analysis and machine intelligence, 39(11):2298\u20132304, 2016.\\n\\n[35] Peize Sun, Rufeng Zhang, Yi Jiang, Tao Kong, Chenfeng Xu, Wei Zhan, Masayoshi Tomizuka, Lei Li, Zehuan Yuan, Changhu Wang, et al. Sparse r-cnn: End-to-end object detection with learnable proposals. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14454\u201314463, 2021.\\n\\n[36] Zhiqing Sun, Shengcao Cao, Yiming Yang, and Kris Kitani. Rethinking transformer-based set prediction for object detection. arXiv preprint arXiv:2011.10881, 2020.\\n\\n[37] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 9627\u20139636, 2019.\\n\\n[38] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in neural information processing systems, pages 5998\u20136008, 2017.\\n\\n[39] Andreas Veit, Tomas Matera, Lukas Neumann, Jiri Matas, and Serge Belongie. Coco-text: Dataset and benchmark for text detection and recognition in natural images. In arXiv preprint arXiv:1601.07140, 2016.\\n\\n[40] Victor Wu, R. Manmatha, and Edward M. Riseman. Finding text in images. In Proceedings of the Second ACM International Conference on Digital Libraries, DL '97, page 3\u201312, New York, NY, USA, 1997. Association for Computing Machinery.\\n\\n[41] Linjie Xing, Zhi Tian, Weilin Huang, and Matthew R Scott. Convolutional character networks. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 9126\u20139136, 2019.\\n\\n[42] Mengbiao Zhao, Wei Feng, Fei Yin, Xu-Yao Zhang, and Cheng-Lin Liu. Weakly-supervised arbitrary-shaped text detection with expectation-maximization algorithm. ArXiv, abs/2012.00424, 2020.\\n\\n[43] Zhuoyao Zhong, Lianwen Jin, Shuye Zhang, and Ziyong Feng. Deeptext: A unified framework for text proposal generation and text detection in natural images. ArXiv, abs/1605.07314, 2016.\\n\\n[44] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. arXiv preprint arXiv:2010.04159, 2020.\"}"}
{"id": "CVPR-2022-1895", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Towards Weakly-Supervised Text Spotting using a Multi-Task Transformer\\n\\nYair Kittenplon Inbal Lavi Sharon Fogel Yarin Bar R. Manmatha Pietro Perona\\n\\nAbstract\\n\\nText spotting end-to-end methods have recently gained attention in the literature due to the benefits of jointly optimizing the text detection and recognition components. Existing methods usually have a distinct separation between the detection and recognition branches, requiring exact annotations for the two tasks. We introduce TextTranSpotter (TTS), a transformer-based approach for text spotting and the first text spotting framework which may be trained with both fully- and weakly-supervised settings. By learning a single latent representation per word detection, and using a novel loss function based on the Hungarian loss, our method alleviates the need for expensive localization annotations. Trained with only text transcription annotations on real data, our weakly-supervised method achieves competitive performance with previous state-of-the-art fully-supervised methods. When trained in a fully-supervised manner, TextTranSpotter shows state-of-the-art results on multiple benchmarks.\\n\\n1. Introduction\\n\\nText spotting, i.e., detecting and reading text in images, is a key capability for machines to operate in the real world. Applications include vehicle navigation in buildings and cities, indexing of image collections and video, automated handling of packages, and prosthetics for blind and visually impaired people. This challenge was recognized early in the computer vision literature [6,19,40] and is currently undergoing a deep learning revival [14,20], with most researchers focusing on two issues: architectures and data.\\n\\nEarly systems [3, 13] use separate architectures for text detection and recognition, without sharing any component. More recent approaches take a leap forward towards a unified end-to-end architecture by sharing a convolutional feature backbone [5, 20] and employing a feature cropping mechanism to extract the relevant area of interest for the recognition head. Such architectures are still not ideal, since the recognition head is usually trained using the detection ground-truth and thus it is not optimized for the predictions of the detection head. Furthermore, the detection head is trained as a standard object detection model, without regard to the additional supervision given by the text transcription or to the downstream recognition task. Other than...\"}"}
{"id": "CVPR-2022-1895", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"mutually optimizing the backbone, the tasks are separate, requiring both transcription annotations for the recognition head, and polygons or bounding box annotations for the detection head. Recently, more sophisticated methods forgo the two-stage approach by directly localizing and classifying the characters in the text [1, 30], which further requires character-level annotations.\\n\\nThe datasets in the field of text spotting consist of synthetic and real data. Real data annotation is an expensive task, however relying solely on synthetic data leads to poor results. Most of the annotation time is dedicated to the detection ground-truth, while the transcription annotation alone requires less than half of the time, as discussed in Sec. 4.5. State-of-the-art methods explicitly segment the text area, allowing the recognizer to cope with rotated, curved, or densely located text and ignore background noise [5, 22]. A disadvantage of such methods is that they require expensive polygonal annotations [7, 16].\\n\\nIn this work, we suggest a new text spotting approach, TextTranSpotter (TTS), which can forgo the expensive spatial annotations and use only transcript annotations for real data. This setting is weakly supervised, in the sense that only partial information about the text in the image is used for training. At inference time, the model outputs both the detection and the transcription of the text in the image. The weakly supervised setting has many use-cases, especially in situations where annotation resources are limited or there is an existing dataset with only text transcription annotations [15]. Furthermore, TTS can be trained in both a fully- or a weakly-supervised manner, thus allowing a trade-off between model performance and annotation cost (Fig. 1).\\n\\nTo allow the weakly-supervised setting, we depart from existing text spotting methods which treat text detection and recognition as related but independent tasks. Our approach includes a novel architecture and loss function which better entangle the two tasks, taking a step further towards a unified end-to-end system. TextTranSpotter takes advantage of recent developments in transformers [4, 10, 38] to create a multitask network (see Fig. 2), learning a single object query embedding for both detection and recognition heads. The task heads are very simple and lean; the detection head is a linear feed-forward network and the recognition head is a Recurrent Neural Network (RNN) [34]. This indicates that the majority of the computation is performed in the shared transformer, unlike most approaches which use more intricate recognition and detection networks (see supplementary for comparison of methods). The input to the recognition head is the transformer output, which allows it to learn the relevant areas of interest for the given query instead of being given this area explicitly as input. Therefore, it does not require accurate segmentation of the text to perform even in challenging scenarios, such as rotated text, arbitrary-shaped text, or text with overlapping bounding boxes (Sec. 4.4). If the segmentation output is desired, a mask head can be added similarly to the detection and recognition head using a simple deconvolutional decoder.\\n\\nOur weakly-supervised training scheme is obtained by introducing a new loss function based on the Hungarian matching loss [4] that simultaneously optimizes the detection and recognition tasks. The Hungarian loss, which has shown promise in the field of object detection [4, 9, 29, 44], is meaningful in our setting, where the matching explicitly uses the text content for the detection optimization. Our Hungarian loss, which we call Text Hungarian Loss, replaces the detection cost with a recognition cost in the matching criteria. The shared embedding that is optimized in this manner allows for a significant benefit compared to training on synthetic data only, without using any spatial information about the real data. Our weakly-supervised model reaches results comparable to existing fully-supervised methods.\\n\\nOur main contributions are:\\n\\n1. A weakly-supervised training scheme using only the text annotations without any spatial ground-truth for real data, utilizing a novel text-based Hungarian matching loss.\\n\\n2. The first multi-task transformer-based approach for text spotting, in which a single representation is being learned per word for both detection and recognition predictions.\\n\\n3. Extensive quantitative benchmarks showing our fully-supervised method achieves state-of-the-art results on common text spotting benchmarks, and our weakly-supervised method achieves results competitive with previous fully-supervised methods.\\n\\n4. The first text spotting framework to offer both a fully-supervised training scheme and a weakly-supervised one for the same architecture, presenting a trade-off between model accuracy and annotation cost.\"}"}
{"id": "CVPR-2022-1895", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Unlike previous approaches which share only the backbone, in TextTranSpotter the transformer encoder-decoder computes a joint query embedding for each detection (colored square). This embedding is shared for both recognition, detection and segmentation heads, which consist of a recurrent neural network (RNN), a linear feed-forward network (FFN) and a deconvolutional decoder (Deconv), respectively. Our weakly-supervised setting works by training only the recognition and classification heads on the real data, using the detection head at inference time for box prediction. An illustration comparing our architecture with previous text-spotting approaches can be found in the supplementary.\\n\\nWe adopt the idea of an end-to-end system, and further suggest a unified encoding-decoding mechanism, based on a multi-task transformer. Learning a mutual feature embedding per query frees us from the need to design a hand-crafted feature pooling operation. Furthermore, the multi-task nature of our method alleviates the need for exact annotations such as polygons or character-level annotations. Weakly Supervised Approaches. Zhao et al. [42] suggest a weakly-supervised approach for arbitrary text detection, by using an Expectation-Maximization based method, and provide an extensive study of the annotation time under different supervision levels. Janouskova et al. [15] generate a large dataset for text recognition out of weakly-annotated existing data by using a pre-trained localization module as its annotator. In order to create pseudo ground truth labels, they use Levenshtein distance to match predicted transcriptions to a weakly annotated ground-truth set. Bartz et al. [2] suggest training an actual end-to-end text spotting system in a weakly supervised manner, by using a fixed resolution grid as a differentiable localization pooling mechanism. Qiao et al. [30] take a step in the direction of weakly-supervised text spotting by training with bounding boxes instead of polygons, but this results in a significant reduction in performance.\\n\\nMotivated by the study of Zhao et al. [42], showing the high cost of polygonal or segmentation masks annotations, we suggest an end-to-end recognition method in which bounding boxes are sufficient for the task. Moreover, we introduce a weakly-supervised framework, in which text transcriptions are the only real-data annotations needed for training, and provide a study of annotation times for both detection and text transcription.\\n\\nHungarian Matching. Throughout the past decade, learning based approaches for object detection [24, 32, 33, 37] have been used to learn engineered dense predictions, and filter near-duplicate predictions using hand-crafted rules. Recently, Carion et al. [4] presented a new object detection method, DETR, that formulates the problem as a direct set prediction problem. It uses a bipartite matching loss based on the Hungarian algorithm [18] to perform a one-to-one matching between ground-truth and predicted detections, unlike dense approaches in which the matching is one-to-many. This sparse detection paradigm has become popular in the object detection literature [35, 36, 44] and has advanced the field. Zhu et al. [44] mitigate some of the issues in DETR, namely the slow convergence and low performance on small objects, by incorporating deformable attention and a multi-scale architecture.\\n\\nFollowing this line of research, we find the sparse detection approach suitable for multi-task loss formulation, where a given object query can be optimized for additional tasks besides detection. We use a Hungarian matching based loss, by adding a recognition cost term to the matching criteria.\"}"}
{"id": "CVPR-2022-1895", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"presented in Sec. 3.1, a novel variation of the Hungarian matching loss for text spotting is described in Sec. 3.2, and an adaptation of this method into a weakly-supervised setting is described in Sec. 3.3.\\n\\n3.1. Architecture\\n\\nTTS consists of a transformer-based Encoder-Decoder, followed by parallel detection, recognition, and segmentation heads, as illustrated in Fig. 2.\\n\\nJoint Query Embedding.\\n\\nOur Encoding-Decoding module is shared between the detection and recognition branches. Following Carion et al. [4], our architecture uses a predetermined number of learned positional embeddings as input to the decoder, called object queries. The decoder learns a latent representation per object query, \\\\( q_{emb} \\\\in \\\\mathbb{R}^{d_{emb}} \\\\), which is used as an input to all the task-specific heads in our model. Both detection and recognition heads are designed in a light-weight manner, meaning that the majority of the computation is done by the transformer which is optimized jointly for both tasks. This setting improves the embedding for the detection task not only through the detection loss but through the recognition loss optimization, as we show in the ablation study in Sec. 4.6. If a polygon output is desired, the optimized query embedding can be used as an input to a segmentation head, as described below.\\n\\nThe network is based on the Deformable-DETR architecture for object detection [44]. It consists of a conventional CNN backbone that generates a multi-scale feature map, followed by a deformable transformer encoder-decoder, in which the offsets of the attention heads are learned in addition to the attention maps themselves. The dynamic structure of this attention mechanism enables the recognition of rotated, curved, and even upside-down text, without any special treatment as described in Sec. 4.4. In fact, our network is able to achieve this even though it is trained using only axis-aligned box annotations, which are significantly less costly than polygon annotations.\\n\\nDetection Head.\\n\\nWe follow recent object detection methods [4, 44], and use a 3-layer feed-forward network (FFN) to regress the normalized parameters of the query word box w.r.t. the input image, and a linear projection layer to predict the query score, i.e., classify whether or not a query contains a word.\\n\\nRecognition Head.\\n\\nTo the best of our knowledge, all previous recognition models, including ones used in text spotting approaches, use a spatial signal as input to the recognition head (e.g., an image, or a cropped output of the backbone). In our approach, only the one-dimensional joint query embedding, computed by the transformer encoder-decoder, is used. To extract the text transcription we use a sequential LSTM-based decoder, with a one-to-many mapping where the input is the joint query embedding \\\\( q_{emb} \\\\) and the output for each time-step \\\\( k \\\\) is the character probabilities \\\\( t_k \\\\in \\\\mathbb{R}^l \\\\) where \\\\( l \\\\) is the length of the alphabet.\\n\\nSegmentation Head.\\n\\nTTS is trained in its fully-supervised setting using the text bounding boxes and recognition transcriptions, without any polygon annotations. However, if a polygon output is desired, a segmentation head can be trained separately based on the frozen TTS model weights. Given the pre-trained query embedding, a light-weight segmentation head, built with 4 linear layers and 3 deconvolution layers, may be used to extract a binary mask, describing the text in the detected bounding box. A polygonal output is then computed from the binary mask.\\n\\n3.2. Text Hungarian Loss\\n\\nInspired by recent object detection approaches [4, 35, 44], we adopt the bipartite matching loss approach, using the Hungarian algorithm [18] to find a one-to-one matching \\\\( \\\\hat{\\\\sigma} \\\\), between the ground-truth and predicted detections:\\n\\n\\\\[\\n\\\\hat{\\\\sigma} = \\\\arg\\\\min_{\\\\sigma \\\\in \\\\theta^N} N \\\\prod_{i=1}^{N} C(y_i, \\\\hat{y}_{\\\\sigma}(i)),\\n\\\\]\\n\\nwhere \\\\( C \\\\) is the criteria used to perform the matching, \\\\( y \\\\) is the ground truth set, \\\\( \\\\hat{y} \\\\) is the predicted set, \\\\( N \\\\) is the number of predictions, or object queries, and \\\\( \\\\theta^N \\\\) is the set of possible matches. The Hungarian loss function is formulated based on the matching \\\\( \\\\hat{\\\\sigma} \\\\\\\\\\n\\\\]\\n\\n\\\\[ L_{Hungarian}(y, \\\\hat{y}) = N \\\\prod_{i=1}^{N} L(y_i, \\\\hat{y}_{\\\\hat{\\\\sigma}(i)}). \\\\]\"}"}
{"id": "CVPR-2022-1895", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"We propose a novel weakly-supervised training scheme based on the Hungarian algorithm for text spotting. Our method, called Text Hungarian Loss, is designed to match ground-truth and prediction boxes, as well as transcription, within the same color. This allows for a more robust training process, especially in situations where the bounding box information is limited or noisy. The loss function is defined as follows:\\n\\n\\\\[\\nC(y, \\\\hat{y}_\\\\sigma(i)) = -\\\\alpha c \\\\hat{p}_\\\\sigma(i)(c) + 1\\\\{c_i \\\\neq \\\\emptyset\\\\} \\\\alpha_{\\\\text{box}} C_{\\\\text{box}}(b_i, \\\\hat{b}_\\\\sigma(i)) + 1\\\\{c_i \\\\neq \\\\emptyset\\\\} \\\\beta_{\\\\text{rec}} L_{\\\\text{rec}}(t_i, \\\\hat{t}_\\\\sigma(i))\\n\\\\]\\n\\nwhere \\\\(c_i, b_i\\\\) and \\\\(t_i\\\\) are the ground truth class, bounding box, and transcription respectively, \\\\(\\\\hat{p}_\\\\sigma(i)(c)\\\\) is the predicted probability for class \\\\(c\\\\), and \\\\(\\\\alpha c, \\\\alpha_{\\\\text{box}}\\\\) and \\\\(\\\\beta_{\\\\text{rec}}\\\\) are the weights for the classification, bounding box, and transcription criteria. The fully-supervised loss term is:\\n\\n\\\\[\\nL(y_i, \\\\hat{y}_\\\\sigma(i)) = -\\\\beta c \\\\log \\\\hat{p}_\\\\sigma(i)(c) + 1\\\\{c_i \\\\neq \\\\emptyset\\\\} \\\\beta_{\\\\text{box}} L_{\\\\text{box}}(b_i, \\\\hat{b}_\\\\sigma(i)) + 1\\\\{c_i \\\\neq \\\\emptyset\\\\} \\\\beta_{\\\\text{rec}} L_{\\\\text{rec}}(t_i, \\\\hat{t}_\\\\sigma(i))\\n\\\\]\\n\\nwhere \\\\(L_{\\\\text{box}}\\\\) is the bounding box loss, defined as in DETR [4], and \\\\(\\\\beta c, \\\\beta_{\\\\text{box}}\\\\) and \\\\(\\\\beta_{\\\\text{rec}}\\\\) are the weights for the classification, bounding box, and transcription losses. We use a cross entropy loss for both the recognition criteria and loss terms:\\n\\n\\\\[\\nC_{\\\\text{rec}}(t_i, \\\\hat{t}_\\\\sigma(i)) = L_{\\\\text{rec}}(t_i, \\\\hat{t}_\\\\sigma(i)) = \\\\sum_j -\\\\log \\\\hat{p}_\\\\sigma(i)(t_j)\\n\\\\]\\n\\nwhere \\\\(j\\\\) is the character index in the word \\\\(t_i\\\\).\"}"}
{"id": "CVPR-2022-1895", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 1.\\n\\nEvaluation results on ICDAR 2015 and Total-Text datasets. Word spotting and end-to-end f-score using strong (S), weak (W), generic (G), none and full lexicons. * MANGO [30] evaluated with IOU 0.1. Our method shows the best results using generic lexicons.\\n\\n| Method           | ICDAR 2015 F-Score | Total-Text F-Score |\\n|------------------|--------------------|--------------------|\\n|                  | Strong (S) | Weak (W) | Generic (G) | None | Full |\\n| TTS poly         | 85.0        | 81.5         | 77.3        | 85.2 | 81.7 |\\n| ABCNet-V1        | -           | -            | -           | -    | -    |\\n| MTS-V1           | 79.3        | 74.5         | 64.2        | 79.3 | 73.0 |\\n| MTS-V2           | 82.4        | 78.1         | 73.6        | 83.0 | 77.7 |\\n| TextDragon       | 86.2        | 81.6         | 68.0        | 82.5 | 78.3 |\\n| MANGO            | 85.2        | 81.1         | 74.6        | 85.4 | 80.1 |\\n| CRAFTS           | -           | -            | 83.1        | 82.1 | 74.9 |\\n\\n### Table 2.\\n\\nResults under limited training data annotations. Word spotting and end-to-end f-score using strong (S), weak (W), generic (G), none and full lexicons. \u201cText\u201d, \u201cBox\u201d and \u201cPoly\u201d denotes the model trained on real data using annotations of text, bounding boxes and polygons, respectively. Axis-aligned evaluation was used. Results are improved dramatically when training on real data with text-only annotations (TTS weak) compared to training only with fully-supervised synthetic data (TTS synthetic). Using box annotations (TTS box) improves results even further. * MANGO [30] evaluated with IOU 0.1. We show the results of MTS-V3 [22] using axis-aligned evaluation as a reference point.\\n\\n| Method   | Annotations (real) | ICDAR 2015 F-Score | Total-Text F-Score |\\n|----------|--------------------|--------------------|--------------------|\\n| TTS      | synthetic          | 53.1               | 46.9               | 42.9               |\\n| TTS      | weak               | \u2713\u2713\u2713                | 78.6               | 75.1               | 70.2               |\\n| TTS      | box                | \u2713\u2713                 | 84.9               | 81.3               | 77.1               |\\n| MANGO    | box                | \u2713\u2713                 | -                  | -                  | -                  |\\n| MTS-V3\u2020  |                    | \u2713\u2713\u2713                | 82.7               | 78.5               | 74.7               |\\n\\n4.2. Comparison to Previous Methods\\n\\nThe evaluation results of TextTranSpotter, compared to previous approaches on Total-Text and ICDAR 2015, are shown in Table 1. Evaluation was done using the standard polygonal evaluation protocol with IOU threshold of 0.5. Both word spotting and end-to-end results are presented. For ICDAR 2015 we use \u201cstrong\u201d, \u201cweak\u201d and \u201cgeneric\u201d dictionaries, and for Total-Text, we show the results without a lexicon and using a \u201cfull\u201d lexicon. On the Total-Text dataset, our method outperforms previous approaches with and without using a lexicon in the word spotting setting and using a \u201cfull\u201d lexicon in the end-to-end setting. On ICDAR 2015, our method shows the best results using \u201cgeneric\u201d lexicon, the most common and challenging use-case.\\n\\n4.3. Weakly-Supervised Results\\n\\nIn Table 2 we show results using different supervision types. Unlike most of the previous methods, TTS box, TTS synthetic and TTS weak output axis-aligned bounding boxes and not polygons, and are therefore evaluated by matching the bounding boxes of the ground truth polygons with our method's bounding boxes output, using a matching threshold for the axis-aligned IOU of 0.5. We show that this change has only a minor affect on the evaluation by demonstrating on a previous method (MTS-V3). Using the published model, we compute bounding boxes for the polygonal outputs and evaluate with our axis-aligned evaluation (Table 2). We compare the results of\"}"}
{"id": "CVPR-2022-1895", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 5. Qualitative results. Prediction examples of our weakly-supervised (TTS\\\\textsubscript{weak}) and fully-supervised (TTS\\\\textsubscript{poly}) models, on Total-Text and ICDAR 2015 samples. TTS can handle rotated, curved, and even upside-down text instances, effectively distinguishing between overlapping boxes by extracting only the relevant text from the bounding box. TTS\\\\textsubscript{weak} has lower performance than TTS\\\\textsubscript{poly}, which is expected given the reduction in supervision, however it manages to output high quality results. Fail cases are presented on the right.\\n\\n| Method          | 45\u00b0 | 60\u00b0 |\\n|-----------------|-----|-----|\\n| Det. E2E        | 57.2| 33.9|\\n| CharNet R-50    | 58.8| 9.3 |\\n| MTS-V2          | 62.2| 54.2|\\n| MTS-V3          | 84.2| 76.1|\\n| TTS\\\\textsubscript{poly} | 88.8| 80.4|\\n| MTS-V3\u2020         | 82.9| 75.4|\\n| TTS\\\\textsubscript{box}\u2020 | 89.9| 80.1|\\n\\nTable 3. Results on Rotated ICDAR 2013 dataset. F-measure of detection (Det.) and end-to-end (E2E) recognition, under different rotation angles. \u2020 means that axis-aligned evaluation was used.\\n\\nTTS outperforms existing methods. Training using only the synthetic data (TTS\\\\textsubscript{synthetic}) results in very low performance. In comparison, using our weakly-supervised training scheme (TTS\\\\textsubscript{weak}) improves the results significantly, reaching competitive results to fully-supervised state-of-the-art methods while only using the transcription supervision on the real datasets. Using bounding box supervision (TTS\\\\textsubscript{box}) improves results even further and reaches state-of-the-art results. When directly comparing TTS\\\\textsubscript{box} and TTS\\\\textsubscript{poly} (Table 1), we see that the polygonal annotations do not improve the results, and sometimes even degrade them. This is due to the fact that the model is trained without polygons, and the segmentation head is trained afterwards, with the rest of the model weights frozen. We believe further optimization of this head can improve the results, but this is not the focus of this work.\\n\\n4.4. Robustness to Rotation and Curvature\\n\\nWe evaluate TextTranSpotter\u2019s robustness to rotation by testing it on the Rotated ICDAR 2013 dataset. Table 3 shows our model improves performance on this dataset compared to previous approaches, even though our models are trained using only bounding boxes (the segmentation head is trained separately, as described in Sec. 3.1). Using bounding boxes causes more background text and noise to enter the recognition head, and there is no explicit information about the orientation of the text. However, since TextTranSpotter uses the transformer output as an input to both recognition and detection heads, it is able to ignore the irrelevant information and produce the correct transcript. Fig. 5 shows TTS\\\\textsubscript{weak} and TTS\\\\textsubscript{poly} performance on challenging...\"}"}
{"id": "CVPR-2022-1895", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"TTS Architecture\\n\\nTable 4. Detection ablation.\\n\\n| Detection + det. | 88.4 | 82.8 | 85.5 |\\n| Detection + det. + recog. | 90.9 | 84.4 | 87.6 |\\n\\nTable 5. Recognition head and matching ablation.\\n\\n| Recognition Head (params) | End-to-End | det. + cls. | det. + cls. + recog. |\\n|---------------------------|------------|------------|---------------------|\\n| linear (3.4M)             | 73.6       | 83.6       |                     |\\n| RNN (2.8M)                | 74.0       | 84.5       |                     |\\n| det. + cls. + recog.      | RNN (2.8M) | 75.8       | 84.5                |\\n\\n4.5. Annotation Cost Study\\n\\nTo estimate the annotation time required for each labeling method, we conducted a user study on 100 images out of the TotalText dataset [7] with 9 annotators. Each user was asked to annotate different images with polygons and transcriptions, bounding boxes and transcriptions, or only transcriptions. The results as presented in Fig. 1 show that the average annotation time per instance is 14.3, 10.6 and 4.6 seconds for polygon, bounding box and transcription only annotations respectively. This is consistent with the results by Zhao et al [42] which show that the average annotation time per image on the ICDAR-ArT dataset [8] is 60 and 39 seconds using polygons and bounding boxes respectively (without transcriptions).\\n\\n4.6. Ablation Study\\n\\nWe test the detection performance of the original Deformable DETR [44] compared to the fully-supervised TextTranSpotter, shown in Table 4. We train both models in the same manner, with the significant differences being the Text Hungarian loss for TTS and the recognition head. The models are evaluated on the Total-Text dataset using the standard text detection metrics. TTS box outperforms the vanilla Deformable DETR model, improving both recall and precision of the model. This experiment highlights the benefit of mutually optimizing the detection and recognition tasks, in comparison to training separate standalone models for each task.\\n\\nNext, we study the impact of the Text Hungarian Loss proposed in Sec. 3.2. We train our fully supervised model using two different matching criteria for the Hungarian matching algorithm; detection and classification, as presented in DETR [4], versus detection, classification and recognition, as in our Text Hungarian Loss (Sec. 3.2). We evaluate the models for both end-to-end and detection on Total-Text, and show our results in Table 5. The text matching criterion improves results, mainly for recognition. Therefore, using it improves performance for the end-to-end setting without a lexicon. When using a lexicon, the improvement to the recognition performance is less significant and the end-to-end results remain the same. We use the full matching criteria for our fully-supervised training.\\n\\nThe query embeddings which go into the recognition head in TTS are one dimensional and have no spatial or sequential structure, in contrast to previous recognition architectures. In addition, the recognition head is trained without using the ground truth transcription during the forward pass like previous approaches, since the matching is performed only at the end of the forward pass. Taking into account these two significant changes, we aim to study the contribution of using an RNN compared to using linear layers. The results using the two different recognition heads are presented in Table 5. Using a linear head lowers the results compared to an RNN head, showing that the recurrent output formulation is beneficial for the recognition task while reducing the number of parameters in the recognition head.\\n\\n5. Conclusions\\n\\nWe presented the first text spotting framework that can be trained in both fully- and weakly-supervised settings. By using a transformer encoder-decoder to learn a joint representation for the recognition and detection tasks, we can forgo much of the expensive annotations that are required in other approaches, and trade-off model accuracy vs. annotation time. The transformer's attention mechanism helps achieve accurate results on difficult cases, such as curved, rotated, dense, and even upside-down text. Our novel Text Hungarian Loss includes the recognition information in the detection optimization and permits training without the detection supervision altogether. Our method achieves state-of-the-art results on several benchmarks in the fully-supervised approach, and competitive results in the weakly-supervised setting. We hope that this work will open the door to new research directions in the field of text spotting, and to new views regarding which annotations are truly required for this task, examining trade-offs and combinations of weakly and fully supervised data.\"}"}
