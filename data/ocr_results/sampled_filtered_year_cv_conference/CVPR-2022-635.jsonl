{"id": "CVPR-2022-635", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 6. The schematics of the parameter regularization strategy.\\n\\nAt the bottom, the parameters that are more important to various exposures are less updated; while at the top, the parameters that are less important to various exposures but more important to the worst-performed exposure are more updated.\\n\\nAfter being processed by ENC, the gap between under-exposure and over-exposure representations is narrowed. As shown in the last column of Fig. 4, the errors between underexposure and overexposure features are smaller after being processed by ENC. Fig. 5 further presents the statistical visualization in the feature space, demonstrating the effectiveness of our method for bridging their representations.\\n\\nPlugging into exposure correction networks. As a plug-and-play module, the proposed ENC module can be incorporated into most existing exposure correction networks. Two representative baseline networks, SID \\\\[6\\\\] and DRBN \\\\[36\\\\], are chosen as the backbones. For SID, it is a U-Net-based architecture with an encoder and decoder. We replace its first layer with the ENC module and denote it as SID-ENC, providing the features with bridged representations for the subsequent processing. As for DRBN, it is a framework consisting of multiple U-shape-based blocks. As shown in Fig. 3, the ENC module can also be set as the first layer of DRBN in its first block (DRBN-ENC) or in all the four blocks (DRBN-ENC-4), both resulting in better performance on exposure correction.\\n\\n3.3. Fine-tuning with Parameter Regularization\\n\\nTo further improve the worst-performed exposure that has poorer performance than other exposures in the training phase, an intuitive way is to fine-tune it. However, this may lead to performance degradation of other exposures. Inspired by the EWC method of continuous learning \\\\[18\\\\], we employ a parameter regularization strategy with a first- and second-order parameter importance modification scheme. Specifically, since ENC is important for all the exposures and helps narrow the gap of their representations, we fix its parameters and update the other parameters according to the parameter importance. In doing so, we can improve the results of the worst-performed exposure while retaining the performance of the other exposures.\\n\\nAs shown in Fig. 6, we present the schematics of the parameter regularization strategy. In particular, we first calculate the parameter importance of the network obtained from the multiple-exposure training phase, then fix the parameters of ENC, and update the other parameters based on the parameter importance. Specially, by denoting the training on various exposures as task 0 and fine-tuning on the worst-performed exposure as task 1, the parameter importance weight \\\\(\\\\Omega_{\\\\theta_k}\\\\) is computed by accumulating the gradients over various exposure data points:\\n\\n\\\\[\\n\\\\Omega_{\\\\theta_k} = f(x; \\\\theta_1 k) - f(x; \\\\theta_0 k),\\n\\\\]\\n\\nwhere \\\\(f(\\\\cdot)\\\\) represents the mapping function of our network, \\\\(\\\\theta_k\\\\) denotes any parameter of the network, and \\\\(\\\\theta_1 k = \\\\theta_0 k + \\\\delta \\\\theta_k\\\\), \\\\(\\\\delta\\\\) denotes the parameter change magnitude, and \\\\(x\\\\) indicates the input various exposure data. In particular, the above equation can be written as:\\n\\n\\\\[\\n\\\\Omega_{\\\\theta_k} = \\\\nabla_{\\\\theta_k} L |\\\\delta \\\\theta_k| + \\\\frac{1}{2} \\\\nabla^2_{\\\\theta_k} L |\\\\delta \\\\theta_k|^2 + O(|\\\\delta \\\\theta_k|^3),\\n\\\\]\\n\\nwhere \\\\(L\\\\) is the conventional loss of baseline method. Here, we adopt the first two terms for approximation.\\n\\nTo improve the worst-performed exposure while maintaining the performance of other exposures, we add a regularization term based on the baseline's conventional loss to keep the knowledge of training on all the exposures. In summary, the total loss \\\\(L'\\\\) for fine-tuning is formulated as:\\n\\n\\\\[\\nL' = L + \\\\lambda m \\\\sum_{k=1}^{\\\\Omega} \\\\Omega_{\\\\theta_k} = L + \\\\lambda m \\\\sum_{k=1}^{\\\\Omega} \\\\left[ \\\\nabla_{\\\\theta_k} L |\\\\delta \\\\theta_k| + \\\\frac{1}{2} \\\\nabla^2_{\\\\theta_k} L |\\\\delta \\\\theta_k|^2 \\\\right].\\n\\\\]\\n\\nIn this way, the parameters of the network that are important to other exposures are less updated, thus maintaining their performance.\\n\\n4. Experiments\\n\\n4.1. Settings\\n\\nDatasets. The experiments are evaluated on two datasets of multiple-exposures, including the multiple-exposures (ME) dataset collected by MSEC \\\\[2\\\\] and the SICE dataset \\\\[5\\\\]. The ME dataset contains exposure images of five levels. To demonstrate the effectiveness of our method, we conduct experiments on two settings for the ME dataset. Following \\\\[3\\\\], the retouched version of the middle exposure subset is selected as the ground truth in the standard ME dataset \\\\[2\\\\], which includes 17,675 training sample pairs, 750 validation sample pairs, and 5,905 testing sample pairs. Furthermore, we also conduct experiments on revised ME-v2 dataset, which selects the middle exposure subset in ME dataset as the ground truth and preserves the other exposure subsets as the multiple exposure input. Note that ME-v2*\\n\\nThe details of the derivation process can be found in the supplementary material.\"}"}
{"id": "CVPR-2022-635", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"Table 1. Quantitative results of different methods on ME and ME-v2 Datasets in terms of PSNR and SSIM.\\n\\n| Method      | PSNR Under | SSIM Under | PSNR Over | SSIM Over | PSNR Average | SSIM Average |\\n|------------|------------|------------|-----------|-----------|--------------|---------------|\\n| CLAHE [30] | 14.45      | 0.5990     | 15.38     | 0.5990    | 15.82        | 0.5990        |\\n| RetinexNet [8] | 19.06    | 0.8558     | 15.76     | 0.7643    | 17.41        | 0.8100        |\\n| Zero-DCE [10] | 21.1    | 0.8558     | 16.32     | 0.7781    | 17.44        | 0.8250        |\\n| MSEC [2]    | 24.48      | 0.9055     | 24.30     | 0.9153    | 24.34        | 0.9153        |\\n| DRBN [36]   | 24.10      | 0.9156     | 23.15     | 0.9055    | 23.63        | 0.9105        |\\n| DRBN-L      | 24.35      | 0.9168     | 23.34     | 0.9083    | 23.84        | 0.9126        |\\n| I-DRBN (Ours) | 25.32   | 0.9100     | 26.73     | 0.9337    | 26.02        | 0.9218        |\\n| I-DRBN-4 (Ours) | 27.29 | 0.9264     | 26.75     | 0.9275    | 27.02        | 0.9270        |\\n\\nTable 2. Quantitative results of various methods on SICE dataset in terms of PSNR and SSIM.\\n\\n| Method      | PSNR Under | SSIM Under | PSNR Over | SSIM Over | PSNR Average | SSIM Average |\\n|------------|------------|------------|-----------|-----------|--------------|---------------|\\n| SID [6]    | 18.83      | 0.8055     | 19.04     | 0.8071    | 19.09        | 0.8071        |\\n| SID-L      | 22.25      | 0.8773     | 22.66     | 0.8870    | 22.46        | 0.8870        |\\n| I-SID (Ours) | 26.09   | 0.9133     | 26.62     | 0.9166    | 26.35        | 0.9149        |\\n\\nComparison of Methods. For performance comparison, we compare our method with MSEC and the baseline networks. Besides, CLAHE [30], RetinexNet [8], and Zero-DCE [10] are chosen for comparison. More comparative results with other methods are provided in the supplementary material. Due to the introducing of more parameters in ENC, we expand our baseline networks by increasing the number of channels for a fair comparison, which are denoted as DRBN-L and SID-L. Additionally, the SID-ENC, DRBN-ENC, and DRBN-ENC-4 models mentioned in Sec. 3.2 with our fine-tuning strategy are separately denoted as I-SID (Improved-SID), I-DRBN (Improved-DRBN), and I-DRBN-4 (Improved-DRBN-4).\\n\\nImplementation Details. We conduct all our experiments on an NVIDIA 2080Ti GPU, which are based on the released code of the baseline networks with the same training settings. Specifically, our SID is trained with the batch size of 1 and patch size of $384 \\\\times 384$, while DRBN is trained with the batch size of 4 and patch size of $256 \\\\times 256$. During training, we optimize the networks by the Adam optimizer with a learning rate of $1 \\\\times 10^{-4}$ for 80 epochs. During fine-tuning, we set the $\\\\lambda$ in Eq. 11 to 0.7, and the network is trained for 40 epochs with a learning rate of $4 \\\\times 10^{-5}$. All the methods are evaluated in terms of PSNR and SSIM.\\n\\n4.2. Quantitative Evaluation\\n\\nThe evaluation results on the ME and ME-v2 datasets are reported in Table 1. To simplify, we average the results of the first two levels' exposures and the rest levels' exposures as the underexposure and overexposure results, respectively. As can be seen, the MSEC method performs better than our baseline networks with the well-designed networks, and the introduced channels in SID-L and DRBN-L cannot improve the performance significantly. With the assistance of our method, the I-SID and I-DRBN networks both achieve better performance and obtain superior results than the MSEC method. As the model size only increases by 3%, I-SID and I-DRBN-4 remarkably improve the PSNR, which proves the effectiveness of our ENC module.\\n\\nTo further demonstrate the capability of our model, we also perform experiments on the SICE dataset. As shown in Table 2, with the introducing of our method, the PSNR and SSIM of SID and DRBN are improved greatly on both underexposure and overexposure subsets, which outperform other methods by a large margin.\"}"}
{"id": "CVPR-2022-635", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 8. Visualization results on ME dataset of (top) underexposure correction and (bottom) overexposure correction. As can be seen, for both the underexposed and overexposed images, there exist color and lightness shift problems in DRBN and MSEC, while SID tends to generate artifacts. On the contrary, our method can simultaneously achieve color and lightness recovery while preserving the structures.\\n\\nFigure 9. The visualization of the ENC's output errors between underexposure and overexposure. With the employing of exposure distilling loss, the errors are further reduced.\\n\\nFigure 10. Ablation study for the number of ENC modules based on the DRBN network. The increasing number of ENC modules leads to a better performance.\\n\\n| Method             | Under          | Over          | Average        |\\n|--------------------|----------------|---------------|----------------|\\n| DRBN-ENC           | 21.89/0.7071   | 19.09/0.7229  | 20.49/0.7150   |\\n| DRBN-ENC-4-SEQ     | 7.92/0.1346    | 19.96/0.7315  | 13.94/0.4331   |\\n| I-DRBN-4 (Ours)    | 21.77/0.7052   | 19.57/0.7267  | 20.67/0.7160   |\\n| SID-ENC            | 21.36/0.6652   | 19.38/0.6843  | 20.37/0.6748   |\\n| SID-ENC-SEQ        | 7.93/0.1199    | 19.95/0.7137  | 13.94/0.4168   |\\n| I-SID (Ours)       | 21.30/0.6645   | 19.63/0.6941  | 20.47/0.6793   |\\n\\nTable 3. Ablation study for parameter regularization on SICE dataset, the overexposure subset is set as the worst-performed subset due to its lower performance.\\n\\n4.3. Qualitative Evaluation\\n\\nFigure 8 exhibits some visualization results on the ME dataset. It can be seen that our method achieves better color and lightness recovery effects. We further present visual comparisons on the SICE dataset in Figure 7. With the employing of our method, the artifacts can be reduced remarkably. More visualization results are provided in the supplementary material.\\n\\n4.4. Ablation Studies\\n\\nWe perform ablation studies to prove the effectiveness of the proposed ENC module and parameter regularization strategy. More results of ablation studies are provided in the supplementary material.\\n\\nENC Module. Based on the SID network, we conduct experiments on the ME and SICE datasets to investigate the effectiveness of different components in ENC module. As shown in Table 6, the network performance drops significantly without Instance Normalization, demonstrating the effectiveness of mapping different exposures to the exposure-invariant space, and the introducing of normalization distilling loss further strengths this effect. The compensation part can also improve the performance since it integrates the initial features for ensuring the completeness of information. Specifically, both of the integration processes in the spatial and channel dimensions obtain improvements. Additionally, the introducing of exposure distilling loss helps to reduce the exposure effect on the integrated features (see Figure 9), thus leading to enhancement.\\n\\nNote that only the employment of exposure loss can also contribute to the improvement due to the introducing of feature constraints. We further investigate the influence of the number of ENC modules for the DRBN baseline network. As shown in Figure 10, it validates the effectiveness of our ENC module for improving exposure correction. Additionally, we provide results of comparing our ENC module with other plug-and-play modules in the supplementary material.\\n\\nParameter regularization strategy. To further demonstrate the effectiveness of our parameter regularization strategy, we carry out ablation studies on the SICE dataset. As depicted in Table 3, the simply fine-tuned DRBN and SID on the worst-performed overexposure dataset are denoted as DRBN-ENC-4-SEQ and SID-ENC-4-SEQ, which lead to the notable performance drop on the underexposure dataset. With the employing of parameter regularization, the overexposure performance can be enhanced with little performance drop on other-level exposures.\"}"}
{"id": "CVPR-2022-635", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Method | PSNR | SSIM |\\n-------|------|------|\\nDRBN   | 19.39 | 0.8165 |\\nDRBN-L | 18.97 | 0.8147 |\\nI-DRBN-4 (Ours) | 22.31 | 0.8366 |\\nSID    | 19.78 | 0.7617 |\\nSID-L  | 20.21 | 0.8012 |\\n\\nTable 4. Quantitative results of various methods for multiple enhancement tasks and generalizability evaluation.\\n\\nMethod | Average PSNR | Average SSIM |\\n-------|--------------|--------------|\\nDRBN   | 22.84/0.824 | 24.14/0.850 |\\nDRBN-L | 22.84/0.823 | 24.53/0.853 |\\nI-DRBN-4 (Ours) | 23.54/0.825 | 24.67/0.851 |\\n\\nTable 5. Quantitative results of different DRBN methods for various kinds of phone image enhancement on the DPED dataset.\\n\\nTable 6. Ablation study for investigating the components of ENC module. IS and IC denote integration in the spatial and channel, respectively. IN represents Instance Normalization. Lsd is normalization distilling loss, and Led is the exposure distilling loss.\\n\\nFigure 11. Visualization results on the Brighten dataset.\\n\\n4.5. Extension and Discussion\\n\\nTo demonstrate the potentials of our method for real-world applications, we extend it to different kinds of enhancement tasks. First, we blend several datasets and simultaneously address three image enhancement tasks, including image retouching, low-light enhancement, and overexposure correction. Second, to prove the generalizability of our method, we evaluate the trained model on unknown enhancement datasets without fine-tuning. Third, we conduct enhancement for image quality degradation caused by different phone capturing, which is a real-world problem.\\n\\nMultiple enhancement tasks.\\n\\nWe blend the LOL dataset [8] designed for low-light enhancement, MIT-FiveK dataset [3] collected for image retouching, and the overexposure subset from the SICE dataset to build a Task-mix dataset. The results are shown in Table 4. With the introduction of our method, the performance of each task on the multi-task dataset can be improved significantly.\\n\\nGeneralizability evaluation.\\n\\nTo evaluate the generalizability of the proposed framework [24, 25], we evaluate the trained model on Brighten dataset [8] and the underexposure subset from the SICE dataset. As shown in Table 4 and Fig. 11, the introduction of instance normalization improves the robustness of our module. The generalization results of our method can also be improved, which demonstrate the potential usage of our work for real-world applications.\\n\\nVarious kinds of phone image enhancement.\\n\\nWe adopt the DPED dataset [16] for experiments of various kinds of phone image enhancement, which contains images captured by three types of phones. We select 2,048 images and 380 images from each phone type as the training and testing sets. As described in Table 5, with the introduction of our method, the performance of DRBN can be improved, displaying the effectiveness of our algorithm for more applications.\\n\\n5. Conclusion and Limitation\\n\\nIn this paper, we develop a framework for multiple exposure correction. An Exposure Normalization and Compensation (ENC) module is proposed to narrow the gap of multiple exposure representations, leading to consistency correction across exposures. Then, we employ the parameter regularization fine-tuning strategy to obtain a network with a balanced improvement for all the exposures. The experimental results show that our method achieves superior performance for multiple-exposure corrections. However, our method fails to incorporate a specific design for handling severe noise corruption that often appears in extremely dark conditions, which can be investigated in the future.\\n\\nAcknowledgments.\\n\\nThis work was supported by the Anhui Provincial Natural Science Foundation under Grant 2108085UD12, the JKW Research Funds under Grant 20-163-14-LZ-001-004-01, and the National Natural Science Foundation of China under Grants 62131003 and 62021001.\"}"}
{"id": "CVPR-2022-635", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Exposure Normalization and Compensation for Multiple-Exposure Correction\\n\\nJie Huang*, Yajing Liu*, Xueyang Fu, Man Zhou, Yang Wang, Feng Zhao\u2020, Zhiwei Xiong\\n\\nUniversity of Science and Technology of China\\n\\n{hj0117,lyj123,manman}@mail.ustc.edu.cn, {xyfu,ywang120,fzhao956,zwxiong}@ustc.edu.cn\\n\\nAbstract\\n\\nImages captured with improper exposures usually bring unsatisfactory visual effects. Previous works mainly focus on either underexposure or overexposure correction, resulting in poor generalization to various exposures. An alternative solution is to mix the multiple exposure data for training a single network. However, the procedures of correcting underexposure and overexposure to normal exposures are much different from each other, leading to large discrepancies for the network in correcting multiple-exposures, thus resulting in poor performance. The key point to address this issue lies in bridging different exposure representations. To achieve this goal, we design a multiple exposure correction framework based on an Exposure Normalization and Compensation (ENC) module. Specifically, the ENC module consists of an exposure normalization part for mapping different exposure features to the exposure-invariant feature space, and a compensation part for integrating the initial features unprocessed by the exposure normalization part to ensure the completeness of information. Besides, to further alleviate the imbalanced performance caused by variations in the optimization process, we introduce a parameter regularization fine-tuning strategy to improve the performance of the worst-performed exposure without degrading other exposures. Our model empowered by ENC outperforms the existing methods by more than 2dB and is robust to multiple image enhancement tasks, demonstrating its effectiveness and generalization capability for real-world applications. Code: https://github.com/KevinJ-Huang/ExposureNorm-Compensation.\\n\\n1. Introduction\\n\\nIn recent years, camera devices are used to capture photographs of a wide range of scenes at any time. As different scenes present various exposure conditions, images captured with underexposures or overexposures often suffer from unsatisfactory visual effects. For this reason, several exposure correction methods have been proposed, including model-driven-based [4,9,11,21,38] and deep learning-based approaches [10,33,34,36,41]. However, most of them focus on either underexposure or overexposed scenes, causing poor generalization to other exposures. This makes them incapable of being deployed in practical applications. A naive way to solve this problem is to train specific networks corresponding to each exposure condition, leading to a significant increase in training time and parameter space. Alternatively, the network can be trained with a mixture of data from various exposure conditions to improve its capability of multiple-exposure correction. However, due to the variations of representations between underexposure and overexposure, the procedures of correcting them to normal exposures differ greatly from each other, as shown in Fig. 1. This introduces discrepancies for the network in correcting lightness and color across multiple-exposures, thus making it difficult to train a single network and resulting in poor performance. Besides, the variations in optimization processes also make the network incline to overlook disadvantaged data of the mixed datasets [35] and bring about imbalanced performance across exposures.\\n\\nIn this paper, we propose a framework for improving the multiple-exposure correction performances. The key point lies in narrowing the gap of different exposure representations. To this end, we design an Exposure Normalization and Compensation (ENC) module, as shown in Fig. 3. It consists of an exposure normalization part and a compensation part. Specifically, the exposure normalization part maps different exposure features to the exposure-invariant feature space. It is implemented by the Instance Normalization to coarsely align different exposure features, followed by a normalization distilling loss on the normalized features to further reduce the exposure effect. However, normalization will inevitably induce the loss of the image discriminative features [17,27] for image reconstruction. Therefore, the compensation part is introduced for integrating the features unprocessed by the exposure normalization part in the spatial and channel dimension, which ensures the completeness information. Besides, to further enhance the correction effect of the worst-performed exposure caused by imbalanced performance, we introduce a parameter regularization fine-tuning strategy to improve the performance of the worst-performed exposure without degrading other exposures. Our model empowered by ENC outperforms the existing methods by more than 2dB and is robust to multiple image enhancement tasks, demonstrating its effectiveness and generalization capability for real-world applications. Code: https://github.com/KevinJ-Huang/ExposureNorm-Compensation.\"}"}
{"id": "CVPR-2022-635", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 1. The static illustration of underexposure and overexposure correction curves on samples from SICE dataset, which are significantly different from each other.\\n\\nFigure 2. The overview of our proposed framework. During training, the network is trained for correcting multiple-exposures to normal exposures. During fine-tuning, we fine-tune the worst-performed exposure at training phase by parameter regularization.\\n\\nThe main contributions of this work are summarized as:\\n\u2022 We propose a framework for multiple exposure correction by narrowing the gap of different exposure representations. Particularly, we develop an Exposure Normalization and Compensation (ENC) module, which is simple yet effective and can be used as a plug-and-play module for existing exposure correction architectures.\\n\u2022 Inside ENC, we design an exposure normalization part for mapping different exposure features to the exposure-invariant feature space, and a compensation part for integrating the features unprocessed by normalization to ensure the completeness of information.\\n\u2022 Aiming to improve the worst-performed exposure during training, we employ a parameter regularization strategy for fine-tuning it on the network except for the ENC module, resulting in a balanced improvement.\\n\u2022 We validate the effectiveness of our framework on several datasets. Furthermore, we extend it to various image-enhancement tasks and achieve remarkable performances, which demonstrate its generalization superiority for potential usage in real applications.\\n\\n2. Related Work\\n\\nVarious approaches have been developed for exposure correction. Some traditional ones propose to employ the histogram-based technique to enhance contrast and lightness [1, 30, 31, 38], while another line of works is based on the Retinex theory [19], which improve the lightness of images through enhancing the illumination components and suppressing noises by the regularization of the reflectance components [4, 11, 21, 29, 39].\\n\\nRecently, with the emergence of deep learning schemes, exposure correction task has benefited from the deep learning models [7, 13, 22, 26, 37]. Based on the Retinex theory, RetinexNet [34] proposes to restore the illumination in a data-driven form and KIND [41] further introduces a sub-network for recovering the reflectance component.\\n\\nAs another form of component decomposition, Ren et al. [28] uses two distinct streams to learn global content and salient structures simultaneously, and DRBN [36] decomposes the features into different band representations for band recursive learning. In addition, self-supervised methods [10, 20, 40] were proposed for the adaptive illumination adjustment. However, most of these methods focus on either underexposure or overexposure correction, limiting their applications for various exposure conditions. Although MSEC [2] corrects varieties of exposures in a coarse-to-fine manner, it fails to achieve consistent correction across exposures, thus generating results with lightness shifts. Compared with these methods, our algorithm aims to narrow the gap of different exposure features for effectively improving the training performance.\"}"}
{"id": "CVPR-2022-635", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"3.2. Exposure Normalization and Compensation\\n\\nAs shown in Fig. 3, we implement our ENC module with two parts: the exposure normalization part that is designed for mapping various exposure features to exposure invariant feature space, and the compensation part which is proposed to integrate the features unprocessed by normalization for compensating the removed image discriminative information caused by normalization.\\n\\nExposure Normalization Part. In the exposure normalization part, we first employ Instance Normalization to coarsely align features. Assuming the input features as $F$, we perform Instance Normalization by:\\n\\n$$F_n = \\\\text{IN}(F) = \\\\gamma F - \\\\mu(F) + \\\\beta,$$\\n\\nwhere $\\\\mu(\\\\cdot)$ and $\\\\sigma(\\\\cdot)$ denote the mean and standard deviation computed across spatial dimensions for each channel and each sample, $\\\\gamma$ and $\\\\beta$ are parameters learned from data. With Instance Normalization equipped in the feature space, it can normalize feature statistics for style normalization [14]. Since each exposure can be viewed as a kind of style, different exposures are aligned with Instance Normalization that reduces their representation discrepancies.\\n\\nFollowing that, we introduce a normalization distilling loss to further reduce the exposure's effect on the normalized features. Particularly, followed by a convolution layer, we implement this loss between the normalized features of different exposures ($\\\\hat{F}_n$) and those of the normal exposures ($\\\\hat{F}_\\\\text{norm}$), which is defined as:\\n\\n$$L_{\\\\text{nd}} = ||\\\\hat{F}_n - \\\\hat{F}_\\\\text{norm}||_1,$$\\n\\nwhere $||.||_1$ represents the $L_1$ distance between two terms. It efficiently forces the normalized features of different exposures to be similar to those of the normal exposures, thus reducing their discrepancy.\\n\\nFig. 4 presents the feature visualization of different components in our ENC, where the underexposure and overexposure features processed by the Instance Normalization are more similar, and $L_{\\\\text{nd}}$ further reduces their discrepancy.\\n\\nCompensation Part. Normalization inevitably removes discriminative information [17, 27], thus resulting in inadequate information for image reconstruction. To tackle this problem, we further propose a compensation part that integrates the features unprocessed by normalization for compensating the removed image discriminative information caused by normalization.\"}"}
{"id": "CVPR-2022-635", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 4. Feature visualization of different components in ENC on samples from SICE dataset. As can be seen, the ENC's input features $F$ from underexposure and overexposure differ greatly shown in (b) and (h). As shown in (c) and (i) as well as (d) and (j), after being processed by the exposure normalization part, input features are mapped to the exposure invariant space and their discrepancies are progressively reduced. With ENC, the gap of representations between underexposure and overexposure is obviously narrowed as illustrated in (e) and (k) as well as (f) and (l).\\n\\nshortcoming, as shown in Fig. 3, we propose a compensation part for integrating the initial features unprocessed by the exposure normalization part to ensure the completeness information $[23]$. Specifically, we implement the compensation part in both spatial and channel dimensions, which can comprehensively obtain correlations between the initial and normalized features. These correlations reflect their information relationships, thus helping guide the integration of the lost information from the initial features.\\n\\nIn the spatial dimension, the normalized features $\\\\hat{F}_n$ and the initial features unprocessed by normalization $\\\\hat{F}$ are integrated with the attention maps $A$ and $A_n$. Here, $A$ and $A_n$ represent the correlations between features $\\\\hat{F}$ and $\\\\hat{F}_n$, and $A$ is derived by the spatial attention as:\\n\\n$$A = \\\\text{sigmoid}(W_0[F, \\\\hat{F}_n])$$\\n\\n(3)\\n\\nwhere $W_0$ is the kernels' weight matrix, $\\\\ast$ stands for the convolution operation, and $[\\\\cdot]$ means the concatenate operation. Similarly, $A_n$ is generated as well. The spatially-interacted features $F'$ and $F'_n$ can be obtained by:\\n\\n$$F' = \\\\hat{F}_n \\\\cdot A + F,$$\\n\\n$$F'_n = \\\\hat{F} \\\\cdot A_n + F_n,$$\\n\\n(4)\\n\\nwhere $\\\\cdot$ denotes the element-wise multiplication.\\n\\nThen, we further integrate the two features in the channel dimension. Specifically, we re-weight the concatenated features of $F'$ and $F'_n$ by applying the attention weight $A_f$ to adaptively integrate them, and the $A_f$ is derived by the SE-like \\\\cite{12} channel attention. In particular, $A_f$ is obtained by a pooling layer and two FC layers parameterized by $W_1$ and $W_2$, which can be denoted as:\\n\\n$$A_f = \\\\text{sigmoid}(W_2 \\\\cdot \\\\text{relu}(W_1 \\\\ast \\\\text{pool}(F', F'_n))))$$\\n\\n(5)\\n\\nNotably, we implement the pooling operation in Eq. 5 with global contrast average pooling for capturing global and local information \\\\cite{15}, which is propitious for image processing. This operation is defined as:\\n\\n$$F_o = \\\\frac{1}{HW} \\\\sum_{(x,y) \\\\in F_i} F_{x,y} + \\\\left( \\\\frac{1}{\\\\sqrt{HW}} \\\\sum_{(x,y) \\\\in F_i} \\\\left( F_{x,y} - \\\\frac{1}{HW} \\\\sum_{(x,y) \\\\in F_i} F_{x,y} \\\\right)^2 \\\\right)^{1/2},$$\\n\\n(6)\\n\\nwhere $F_i$ and $F_o$ represent the input and output features of the global pooling operation, $x$ and $y$ are the position coordinates, $H$ and $W$ denote the spatial size. Finally, the output features of ENC (denoted as $F_f$) are derived by weighting the concatenated features $[F', F'_n]$ with $A_f$, which can be denoted by:\\n\\n$$F_f = A_f \\\\cdot [F', F'_n].$$\\n\\n(7)\\n\\nTo further maintain the exposure invariant property of the integrated features $F_f$, we need to reduce the exposure effect introduced from the ENC's input features $F$ on $F_f$. Therefore, we apply the exposure distilling loss between $F_f$ and the integrated normal exposure features $F_{norm}$ as:\\n\\n$$L_{ed} = ||F_f - F_{norm}||_1.$$  \\n\\n(8)\"}"}
{"id": "CVPR-2022-635", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] M. Abdullah-Al-Wadud, M. H. Kabir, M. A. Akber Dewan, and O. Chae. A dynamic histogram equalization for image contrast enhancement. *IEEE Transactions on Consumer Electronics*, 53(2):593\u2013600, 2007.\\n\\n[2] Mahmoud Afifi, Konstantinos G Derpanis, Bjorn Ommer, and Michael S Brown. Learning multi-scale photo exposure correction. In *CVPR*, 2021.\\n\\n[3] Vladimir Bychkovsky, Sylvain Paris, Eric Chan, and Fr\u00e9d\u00e9ric Durand. Learning photographic global tonal adjustment with a database of input output image pairs. In *CVPR*, 2011.\\n\\n[4] Bolun Cai, Xianming Xu, Kailing Guo, Kui Jia, Bin Hu, and Dacheng Tao. A joint intrinsic-extrinsic prior model for retinex. In *ICCV*, pages 4000\u20134009, 2017.\\n\\n[5] Jianrui Cai, Shuhang Gu, and Lei Zhang. Learning a deep single image contrast enhancer from multi-exposure images. *IEEE Transactions on Image Processing*, 27(4):2049\u20132062, 2018.\\n\\n[6] Chen Chen, Qifeng Chen, Jia Xu, and Vladlen Koltun. Learning to see in the dark. *arXiv preprint arXiv:1805.01934*, 2018.\\n\\n[7] Yu-Sheng Chen, Yu-Ching Wang, Man-Hsin Kao, and Yung-Yu Chuang. Deep photo enhancer: Unpaired learning for image enhancement from photographs with gans. In *CVPR*, pages 6306\u20136314, 2018.\\n\\n[8] Wenhao Yang Jiaying Liu Chen Wei, Wenzhen Wang. Deep retinex decomposition for low-light enhancement. In *BMVC*, 2018.\\n\\n[9] Xuan Dong, Xiaoyan Hu, Weixin Li, Xiaojie Wang, and Yuhong Wang. Miehdr cnn: Main image enhancement based ghost-free high dynamic range imaging using dual-lens systems. In *AAAI*, volume 35, pages 1264\u20131272, 2021.\\n\\n[10] Chunle Guo Guo, Chongyi Li, Jichang Guo, Chen Change Loy, Junhui Hou, Sam Kwong, and Runmin Cong. Zero-reference deep curve estimation for low-light image enhancement. In *CVPR*, pages 1780\u20131789, 2020.\\n\\n[11] Xiaojie Guo, Yu Li, and Haibin Ling. Lime: Low-light image enhancement via illumination map estimation. *IEEE Transactions on Image Processing*, 26(2):982\u2013993, 2016.\\n\\n[12] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation networks. In *CVPR*, pages 7132\u20137141, 2018.\\n\\n[13] Jie Huang, Zhiwei Xiong, Xueyang Fu, Dong Liu, and Zheng-Jun Zha. Hybrid image enhancement with progressive laplacian enhancing unit. In *ACM MM*, pages 1614\u20131622, 2019.\\n\\n[14] Xun Huang and Serge Belongie. Arbitrary style transfer in real-time with adaptive instance normalization. In *ICCV*, 2019.\\n\\n[15] Zheng Hui, Xinbo Gao, Yunchu Yang, and Xiumei Wang. Lightweight image super-resolution with information multi-distillation network. In *ACM MM*, pages 2024\u20132032, 2019.\\n\\n[16] Andrey Ignatov, Nikolay Kobyshev, Radu Timofte, Kenneth Vanhoey, and Luc Van Gool. DSLR-quality photos on mobile devices with deep convolutional networks. In *ICCV*, pages 3277\u20133285, 2017.\\n\\n[17] Xin Jin, Cuiling Lan, Wenjun Zeng, Zhibo Chen, and Li Zhang. Style normalization and restitution for generalizable person re-identification. In *CVPR*, pages 3140\u20133149, June 2020.\\n\\n[18] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A. Rusu, Kieran Milan, John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, Demis Hassabis, Claudia Clopath, Dharshan Kumaran, and Raia Hadsell. Overcoming catastrophic forgetting in neural networks. *Proceedings of the National Academy of Sciences*, 114(13):3521\u20133526, 2017.\\n\\n[19] Edwin H Land. The retinex theory of color vision. *Scientific American*, 237(6):108\u2013129, 1977.\\n\\n[20] Chongyi Li, Chunle Guo, and Chen Change Loy. Learning to enhance low-light image via zero-reference deep curve estimation. *IEEE Transactions on Pattern Analysis and Machine Intelligence*, pages 1\u201314, 2021.\\n\\n[21] Mading Li, Jiaying Liu, Wenhao Yang, Xiaoyan Sun, and Zongming Guo. Structure-revealing low-light image enhancement via robust retinex model. *IEEE Transactions on Image Processing*, 27(6):2828\u20132841, 2018.\\n\\n[22] Jiaying Liu, Dejia Xu, Wenhao Yang, Minhao Fan, and Haofeng Huang. Benchmarking low-light image enhancement and beyond. *International Journal of Computer Vision*, 129:1153\u20131184, 04 2021.\\n\\n[23] Yajing Liu, Xinmei Tian, Ya Li, Zhiwei Xiong, and Feng Wu. Compact feature learning for multi-domain image classification. In *CVPR*, pages 7193\u20137201, 2019.\\n\\n[24] Yajing Liu, Zhiwei Xiong, Ya Li, Yuning Lu, Xinmei Tian, and Zheng-Jun Zha. Category-stitch learning for union domain generalization. *ACM Transactions on Multimedia Computing, Communications, and Applications*, 2022.\\n\\n[25] Yajing Liu, Zhiwei Xiong, Ya Li, Xinmei Tian, and Zheng-Jun Zha. Domain generalization via encoding and resampling in a unified latent space. *IEEE Transactions on Multimedia*, 2021.\\n\\n[26] Feifan Lv, Yu Li, and Feng Lu. Attention guided low-light image enhancement with a large scale low-light simulation dataset. *International Journal of Computer Vision*, 129:2175\u20132193, 07 2021.\\n\\n[27] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In *ECCV*, pages 464\u2013479, 2018.\\n\\n[28] Wenqi Ren, Sifei Liu, Lin Ma, Qianqian Xu, Xiangyu Xu, Xiaochun Cao, Junping Du, and Ming-Hsuan Yang. Low-light image enhancement via a deep hybrid network. *IEEE Transactions on Image Processing*, 28(9):4364\u20134375, 2019.\\n\\n[29] Xutong Ren, Wenhao Yang, Wen-Huang Cheng, and Jiaying Liu. Lr3m: Robust low-light enhancement via low-rank regularized retinex model. *IEEE Transactions on Image Processing*, 29:5862\u20135876, 2020.\\n\\n[30] Ali M Reza. Realization of the contrast limited adaptive histogram equalization (clahe) for real-time image enhancement. *IEEE Transactions on Multimedia*, pages 35\u201344, 2004.\"}"}
{"id": "CVPR-2022-635", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[32] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-SNE. *Journal of Machine Learning Research*, 9(11), 2008.\\n\\n[33] Ruixing Wang, Qing Zhang, Chi-Wing Fu, Xiaoyong Shen, Wei-Shi Zheng, and Jiaya Jia. Underexposed photo enhancement using deep illumination estimation. In *CVPR*, pages 6849\u20136857, 2019.\\n\\n[34] Chen Wei, Wenjing Wang, W enhan Yang, and Jiaying Liu. Deep retinex decomposition for low-light enhancement. In *BMVC*, pages 155\u2013165, 2018.\\n\\n[35] Jie Xiao, Man Zhou, Xueyang Fu, Aiping Liu, and Zheng-Jun Zha. Improving de-raining generalization via neural reorganization. In *ICCV*, pages 4987\u20134996, October 2021.\\n\\n[36] W enhan Yang, Shiqi Wang, Y uming Fang, Y ue Wang, and Jiaying Liu. From fidelity to perceptual quality: A semi-supervised approach for low-light image enhancement. In *CVPR*, pages 3063\u20133072, 2020.\\n\\n[37] W enhan Yang, W enjing W ang, Haofeng Huang, Shiqi W ang, and Jiaying Liu. Sparse gradient regularized deep retinex network for robust low-light image enhancement. *IEEE Transactions on Image Processing*, 30:2072\u20132086, 2021.\\n\\n[38] Zhenqiang Ying, Ge Li, Y urui Ren, Ronggang W ang, and W enmin W ang. A new image contrast enhancement algorithm using exposure fusion framework. In *ICCP*, pages 36\u201346. Springer, 2017.\\n\\n[39] Qing Zhang, Ganzhao Yuan, Chunxia Xiao, Lei Zhu, and W ei-Shi Zheng. High-quality exposure correction of underexposed photos. In *ACM MM*, page 582\u2013590, 2018.\\n\\n[40] Y u Zhang, Xiaoguang Di, Bin Zhang, and Chunhui W ang. Self-supervised image enhancement network: Training with low light images only. *arXiv Preprint: 2002.11300*, 2020.\\n\\n[41] Y onghua Zhang, Jiawan Zhang, and Xiaojie Guo. Kindling the darkness: A practical low-light image enhancer. In *ACM MM*, pages 1632\u20131640, 2019.\"}"}
