{"id": "CVPR-2024-1742", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"target of these masks is another important issue. In Sec 3.3, we present an adaptive one-hot loss to solve this problem. Considering $m$ is derived from $S$ and $V$, the optimization could be decomposed into two parts. As for the importance score, the aim of $S$ is to learn an importance rank according to the unit\u2019s impact on model performance under the sparsity constraint. Thus, we follow ViT-Slim [2] to regularize $S$ with $\\\\ell_1$ norm to drive unimportant units towards low-ranking and zero-score distribution, i.e., $L_S = \\\\|S\\\\|_1$.\\n\\nAs for the sparsity score, the aim of $V$ is to learn a binary distribution as the unit pruning choice. In other words, the target label of each $p_i$ is ideally designed as a progressively shrunk one-hot vector, with no prior information about the one-hot index, thus being difficult to impose a definite target to $V$ and $\\\\alpha$. To address this, we propose to regularize the sparsity score by introducing an alternative constraint that aligns the entropy and variance of $p_i$ with one-hot vectors. The motivation stems from the invariance of the two properties in one-hot vectors, regardless of the one-hot index. Especially, the entropy of any one-hot vector always equals zero, while the variance solely depends on the dimension number. The regularization w.r.t. $p_i$ is formulated as follows:\\n\\n$$R(p_i) = \\\\sum_{i=1}^{M} \\\\left[ H(p_i) + \\\\Psi(p_i) \\\\right] = \\\\sum_{i=1}^{M} h - p_i^T \\\\log (p_i) + \\\\tan(\\\\frac{\\\\pi}{2} - \\\\frac{\\\\pi}{2})$$\\n\\n(5)\\n\\nwhere $M$ denotes the number of searchable submodules in $N(A, W)$, $\\\\omega_i = \\\\sigma_i / \\\\sigma_t$ with $\\\\sigma_i$, $\\\\sigma_t$, and $\\\\omega_i$ meaning the measured, target and normalized variances of $p_i$, respectively, where $\\\\sigma_t = \\\\|\\\\alpha_i\\\\|_0 - 1 / \\\\|\\\\alpha_i\\\\|_2^0$. (See Appendix A for more detailed explanations). In addition to Eq. (5), $V$ is also constrained by the sparsity constraint, $\\\\tau$. Therefore, the total regularization objective of $V$ is formulated as follows:\\n\\n$$L_V = \\\\mu_1 R(p_i) + \\\\mu_2 \\\\|g(V) - \\\\tau\\\\|_2$$\\n\\n(6)\\n\\nwhere $\\\\mu_1$ and $\\\\mu_2$ are the weight coefficients to balance two items. Note that during search, the ground-truth value $\\\\sigma_t$ would change with the decrease of $\\\\|\\\\alpha_i\\\\|_0$ when the pruning happens in $\\\\alpha$. Thus, $L_V$ is adaptive to the pruning process in the search stage. The pruning process in one dimension (e.g., the $i$-th submodule) is triggered by the condition that $(p_i)_{\\\\min} \\\\leq \\\\eta \\\\cdot \\\\bar{p}_i$, where $\\\\eta$ is the scaling factor and $\\\\bar{p}_i$ is the mean of $p_i$, i.e., $\\\\bar{p}_i = 1 / \\\\|\\\\alpha_i\\\\|_0$. By Eq. (6) and the proposed pruning strategy, the units with the lowest mask values can be progressively removed, thus accelerating search process.\\n\\nBased on the above analysis, the regularization items for the bi-mask can be summarized as,$\\n\\n$$L_m(V, S) = L_V + \\\\mu_3 L_S = \\\\mu_1 R(p_i) + \\\\mu_2 \\\\|g(V) - \\\\tau\\\\|_2 + \\\\mu_3 \\\\|S\\\\|_1,$$\\n\\nwhere $\\\\mu_3$ denotes the weight coefficient of $L_S$. Consequently, the objective in Eq. (2) is transformed into the following equation:\\n\\n$$\\\\min_{S, V, W} L_{\\\\text{train}}(S, V, W) + L_m(V, S).$$\\n\\n3.4. Progressive MIM\\n\\nAs discussed above, during the search, pruning units can reduce training costs but may impair model performance. Consequently, the evaluation of the importance and sparsity scores for the remaining units may be unreliable, especially in aggressive compression scenarios where a large proportion of units are pruned. To address this, we propose to further enhance the representative capability of the remaining features using the MIM technique [39] during the search. Yet, deploying MIM in a well-trained ViT requires considering the characteristics of VTC. Specifically, using a large masking ratio similar to SimMIM [20, 39] may be inappropriate for VTC as the model would deviate from the original classification-oriented feature space to the reconstruction-oriented space, losing the original contextual information. Motivated by this, we propose a progressive MIM strategy for searching, where the masking ratio gradually increases until reaching the preset threshold. Consequently, as the pruning ratio gets larger, the feature regularization is strengthened with negligible additional cost [39], thereby maintaining the representation ability and a reliable prunability evaluation of the remaining units. Then, the optimization objective is updated to:\\n\\n$$\\\\min_{S, V, W} L_{\\\\text{train}}(S, V, W) + L_m(V, S) + L_{\\\\text{rec}}(S, V, W; \\\\gamma),$$\\n\\n(8)\\n\\nwhere $L_{\\\\text{rec}}$, $\\\\gamma$ denote the reconstruction loss and the masking ratio, respectively. Alg. (1) shows the overall algorithm.\"}"}
{"id": "CVPR-2024-1742", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"### Table 2. Compression results of DeiT models on ImageNet-1K.\\n\\n| Method      | Model | #Param (M) | FLOPs (B) | Top-1 (%) | Top-5 (%) | GPU Days |\\n|-------------|-------|------------|-----------|-----------|-----------|-----------|\\n| DeiT-S      | Baseline | 22.1 | 4.6 | 79.8 | 95.0 | - |\\n| SSP-T       | \u2020 [7]   | 4.2 | 0.9 | 68.6 | - | - |\\n| ViTE-T      | \u2020 [7]   | 4.2 | 0.9 | 70.1 | - | - |\\n| WDPruning-0.3-12 | \u2020 [42] | 3.8 | 0.9 | 71.1 | 90.1 | - |\\n| ViTE-S      | \u2020 [7]   | - | 2.1 | 74.8 | - | - |\\n| TAS ViTAS-B | \u2020 [32]  | - | 1.0 | 72.4 | 90.6 | 32 |\\n| AutoFormer-Ti | 5.7 | (\u2193 74%) | 1.3 | (\u2193 72%) | 75.0 | (\u2193 4.8) |\\n| ViTAS-C     | \u2020 [32]  | 5.6 | (\u2193 75%) | 1.3 | (\u2193 72%) | 74.7 | (\u2193 5.1) |\\n| TF-TAS-Ti   | \u2020 [48]  | 5.9 | (\u2193 73%) | 1.4 | (\u2193 70%) | 75.3 | (\u2193 4.5) |\\n| LightweightDeiT-Ti | 5.7 | | 1.3 | 72.2 | 91.1 | - |\\n| TNT-Ti      | \u2020 [19]  | 6.1 | 1.4 | 73.9 | 91.9 | - |\\n| ViT-Slim    | \u2020 [2]   | 11.4 | (\u2193 48%) | 2.3 | (\u2193 50%) | 77.9 | (\u2193 1.9) |\\n| WDPruning-0.3-12 | \u2020 [42] | - | 2.6 | 78.4 | - | - |\\n| LightweightHVT | - | | 2.4 | 78.0 | - | - |\\n| PVT-Ti      | \u2020 [36]  | 13.2 | 1.9 | 75.1 | - | - |\\n| Giraffe      | \u2020 [28]  | - | 2.4 | 78.0 | - | - |\\n| TF-TAS-S    | \u2020 [48]  | 22.8 | (\u2193 74%) | 5.0 | (\u2193 71%) | 81.9 | (\u2191 0.1) |\\n| DynamicViT-S | \u2020 [29] | 22.0 | 4.0 | 79.8 | - | - |\\n| ViT-Slim    | \u2020 [2]   | 17.7 | 3.7 | 80.6 | 95.3 | 3 |\\n| OFB         | 17.6 | (\u2193 80%) | 3.6 | (\u2193 79%) | 80.3 | (\u2193 1.5) |\\n| AutoFormer-B | 54.0 | (\u2193 38%) | 11.0 | (\u2193 37%) | 82.4 | (\u2191 0.6) |\\n| GLiT-B      | \u2020 [4]   | 96.0 | (\u2191 11%) | 17.0 | (\u2193 3%) | 82.3 | (\u2191 0.5) |\\n| TF-TAS-S    | \u2020 [48]  | 54.0 | (\u2193 38%) | 12.0 | (\u2193 31%) | 82.2 | (\u2191 0.4) |\\n| VTP         | \u2020 [50]  | - | 10.0 | 80.7 | 95.0 | - |\\n| ViTE-B      | \u2020 [7]   | 56.8 | (\u2193 34%) | 11.7 | (\u2193 33%) | 82.2 | (\u2191 0.4) |\\n| ViT-Slim    | \u2020 [2]   | 52.6 | (\u2193 39%) | 10.6 | (\u2193 39%) | 82.4 | (\u2191 0.6) |\\n| WDPruning-0.3-11 | \u2020 [42] | - | 9.9 | 80.8 | 95.4 | - |\\n| OFB         | 43.9 | (\u2193 49%) | 8.7 | (\u2193 50%) | 81.7 | (\u2193 0.1) |\\n\\n\u2020 and \u2020\u2020 indicate that the compression model is based on DeiT-Ti and DeiT-S [34], respectively.\\n\\n\u2191/\u2193 refers to the increase/decrease ratio.\\n\\n4. Experiments\\n\\nWe first present the compression results for DeITs [34] and Swin Transformer [26] on ImageNet-1K [11]. Next, we analyze the generalization ability of the searched models on downstream benchmarks. Lastly, we explore the contribution of each individual component to search performance, followed by visualizations of the search process and results.\\n\\nThe implementation details are described in Appendix C.\"}"}
{"id": "CVPR-2024-1742", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"| Model | #Param (M) | FLOPs (B) | Top-1 (%) | Top-5 (%) | Days |\\n|-------|------------|-----------|-----------|-----------|------|\\n| Swin-Ti [26] | 28.3 | 4.5 | 81.3 | 95.5 | - |\\n| OFB | - | 6.1 (\u2193 78%) | 1.0 (\u2193 78%) | 76.5 (\u2193 4.8) | 93.1 (\u2193 2.4) | 1.1 |\\n| ViT-Slim [2] | 19.4 (\u2193 31%) | 3.4 (\u2193 24%) | 80.7 (\u2193 0.6) | 95.4 (\u2193 0.1) | - |\\n| OFB | - | 16.4 (\u2193 42%) | 2.6 (\u2193 42%) | 79.9 (\u2193 1.4) | 94.6 (\u2193 0.7) | 1.3 |\\n\\nTable 3. Compression results of Swin-Ti [26] on ImageNet-1K.\\n\\n| Model | F. C10 | C100 |\\n|-------|--------|------|\\n| DeiT-S [34] | 4.6 | 98.6 | 87.8 |\\n| SETR-S [47] | 4.6 | 73.0 | 81.4 | 95.1 |\\n| OFB | - | 98.7 | 88.4 |\\n| OFB | 3.6 | 73.9 | 83.1 | 95.2 |\\n\\nTable 4. Performance of DeiT-S [34] and compressed models on CIFAR-10 (C10), CIFAR-100 (C100) [22] and Cityscape [10].\\n\\nFormer [5] and ViTAS [32], OFB reduces search time greatly at various budgets, meanwhile achieving comparable or better performance. Compared with the efficient compression methods, e.g., TF-TAS [48] and ViT-Slim [2], OFB can achieve comparable within similar search time. One thing to note is that for the compressed DeiT-B-8.7B, after searching, it already reaches the same high performance as the retrained one. As visualized in Fig. 4, we compare the model performance with and without retraining process by employing or not employing PMIM. It is observed that the performance gap of the models with PMIM before and after retraining is significantly smaller than that of the models without PMIM. Therefore, when considering the cost induced by retraining, OFB is more efficient than TF-TAS and ViT-Slim, saving the retraining cost meanwhile maintaining high performance.\\n\\nAs for the compression of Swin Transformer [26], we choose Swin-Ti as the baseline model to validate the effectiveness of OFB. The results are listed in Table 3. From the table, it can be observed that the compression performance of Swin-Ti is similar to that of DeiT-S. For example, when the model is compressed with nearly 80% reductions in FLOPs and parameters (Line 7 in Table 2 and Line 2 in Table 3), they both drop 4.8% in Top-1 accuracy, while Swin-Ti drops 0.3% less than DeiT-S in Top-5 accuracy, which further validates the effectiveness of OFB in different ViT structures.\"}"}
{"id": "CVPR-2024-1742", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":true,\"natural_text\":\"Figure 5. Visualization of bi-mask search process. Each line/surface is a descendingly-ordered distribution learned after one-epoch search, with the lighter color denoting a later learned distribution. (See Appendix B for theoretical analysis)\\n\\nEffectiveness of MIM Strategy. The MIM effectiveness has been partially demonstrated in Fig. 4. For clear comparisons, we report search performance with PMIM before retraining in Table 5c. From the table, PMIM can further improve accuracy by 3.6% Top-1 with a slight increase in computation overhead (+0.05 BFLOPs, +0.2 MParams). In addition, we further analyze the progressive masking strategy in PMIM. As shown in Table 5b, we compare PMIM with a constant masking scheme (const.) adopted in [20, 39], which keeps masking 25% patches as aligned with the final masking ratio in PMIM. It is observed that PMIM improves accuracy at the same FLOPs, thus further validating its effectiveness.\\n\\n4.4. Inference Speedup Results\\nThe throughput and latency results of compressed models are listed in Table 6, where the model checkpoints are obtained from official repositories. The GPU throughput is measured on a Tesla V100 GPU with a batch size of 1024, and the latency is measured on Intel(R) Xeon(R) Gold 5218 CPU with one batch size. It shows that OFB achieves superior speedup on different devices. Specifically, the throughput of the compressed DeiT models is accelerated by 1.8x \u223c 3.7x on GPU, while the latency on CPU is reduced by 25% \u223c 74%.\\n\\n4.5. Mask Visualization\\nThe search process of bi-masks is visualized in Fig. 5 for better understanding, where we take the compressed DeiT-S-1B as an example. It is observed that in the early stage, the bi-mask score (right) is closer to the importance score (left), and the sparsity score distribution changes little, thus the initial learning focuses more on importance evaluation. Whereas, with search going on, the bi-mask score distribution gets progressively closer to the sparsity score distribution, indicating the bi-mask learning focuses more on the sparsity evaluation. As for the pruning, we can find that those units with scores in the small step intervals will be integrated into one step interval, and will be pruned if the sparsity scores drop significantly, which is performed in an adaptive manner.\\n\\n4.6. Architecture Visualization\\nFig. 6 shows several searched architectures of DeiT-S. Fig. 6a and 6b list the searched results of MHSA modules. There are 12 MHSA layers in DeiT-S, with six heads in each layer and 64 channels in each head. The numbers inside grids denote Q-K-V channels of the head. Here, we jointly assess the prunability of Q-K-C channels for all heads at the same layer to achieve structural acceleration. It is noted that shallow and middle layers consistently need higher Q-K-V dimensions at different budgets, while the head numbers can be reduced. Fig. 6c shows the searched MLP dimensions. Similarly, MLPs are preserved more in middle layers and continually pruned in deep layers, indicating more redundancy in deep layers. Fig. 6d shows the searched Patch Embedding dimensions, which are carefully searched with more channels preserved. It is reasonable since Patch Embedding is skip-connected to all encoder blocks in DeiT.\\n\\n5. Conclusion\\nWe introduce OFB to tackle the VTC problem. To determine the unit prunability in ViTs, for the first time, OFB explores how to entangle the importance and sparsity scores during search. And a PMIM regularization strategy is specially designed for the dimension-reduced feature space in VTC. Extensive experiments have been conducted to compress various ViTs on ImageNet and downstream benchmarks, indicating an excellent compression capability for ViTs.\\n\\n6. Acknowledgements\\nThe research was supported by National Key R&D Program of China (Grant No. 2022ZD0160104), National Natural Science Foundation of China (No. 62071127, and 62101137), National Key Research and Development Program of China (No. 2022ZD0160100), Shanghai Natural Science Foundation (No. 23ZR1402900), the Science and Technology Commission of Shanghai Municipality (Grant No. 22DZ1100102), and Shanghai Rising Star Program (Grant No. 23QD1401000).\"}"}
{"id": "CVPR-2024-1742", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] Yutong Bai, Zeyu Wang, Junfei Xiao, Chen Wei, Huiyu Wang, Alan Yuille, Yuyin Zhou, and Cihang Xie. Masked autoencoders enable efficient knowledge distillers. arXiv preprint arXiv:2208.12256, 2022.\\n\\n[2] Arnav Chavan, Zhiqiang Shen, Zhuang Liu, Zechun Liu, Kwang-Ting Cheng, and Eric P Xing. Vision transformer slimming: Multi-dimension searching in continuous optimization space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4931\u20134941, 2022.\\n\\n[3] Hila Chefer, Shir Gur, and Lior Wolf. Transformer interpretability beyond attention visualization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 782\u2013791, 2021.\\n\\n[4] Boyu Chen, Peixia Li, Chuming Li, Baopu Li, Lei Bai, Chen Lin, Ming Sun, Junjie Yan, and Wanli Ouyang. Glit: Neural architecture search for global and local image transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 12\u201321, 2021.\\n\\n[5] Minghao Chen, Houwen Peng, Jianlong Fu, and Haibin Ling. Autoformer: Searching transformers for visual recognition. In Proceedings of the IEEE/CVF international conference on computer vision, pages 12270\u201312280, 2021.\\n\\n[6] Minghao Chen, Kan Wu, Bolin Ni, Houwen Peng, Bei Liu, Jianlong Fu, Hongyang Chao, and Haibin Ling. Searching the search space of vision transformer. Advances in Neural Information Processing Systems, 34:8714\u20138726, 2021.\\n\\n[7] Tianlong Chen, Yu Cheng, Zhe Gan, Lu Yuan, Lei Zhang, and Zhangyang Wang. Chasing sparsity in vision transformers: An end-to-end exploration. Advances in Neural Information Processing Systems, 34:19974\u201319988, 2021.\\n\\n[8] Xin Chen, Lingxi Xie, Jun Wu, and Qi Tian. Progressive differentiable architecture search: Bridging the depth gap between search and evaluation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1294\u20131303, 2019.\\n\\n[9] Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. Advances in Neural Information Processing Systems, 34:9355\u20139366, 2021.\\n\\n[10] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016.\\n\\n[11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pages 248\u2013255, 2009.\\n\\n[12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021.\\n\\n[13] Gongfan Fang, Xinyin Ma, Mingli Song, Michael Bi Mi, and Xinchao Wang. Depgraph: Towards any structural pruning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16091\u201316101, 2023.\\n\\n[14] Yuxin Fang, Shusheng Yang, Shijie Wang, Yixiao Ge, Ying Shan, and Xinggang Wang. Unleashing vanilla vision transformer with masked image modeling for object detection. arXiv preprint arXiv:2204.02964, 2022.\\n\\n[15] Chengyue Gong, Dilin Wang, Meng Li, Xinlei Chen, Zhicheng Yan, Yuandong Tian, Vikas Chandra, et al. Nasvit: Neural architecture search for efficient vision transformers with gradient conflict aware supernet training. In International Conference on Learning Representations, 2021.\\n\\n[16] Bicheng Guo, Shuxuan Guo, Miaojing Shi, Peng Chen, Shibo He, Jiming Chen, and Kaicheng Yu. \\\\(\\\\alpha\\\\)darts once more: Enhancing differentiable architecture search by masked image modeling. arXiv preprint arXiv:2211.10105, 2022.\\n\\n[17] Yong Guo, Yin Zheng, Mingkui Tan, Qi Chen, Jian Chen, Peilin Zhao, and Junzhou Huang. Nat: Neural architecture transformer for accurate and compact architectures. Advances in Neural Information Processing Systems, 32, 2019.\\n\\n[18] Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. In Computer Vision\u2013ECCV 2020: 16th European Conference, Glasgow, UK, August 23\u201328, 2020, Proceedings, Part XVI, pages 544\u2013560. Springer, 2020.\\n\\n[19] Kai Han, An Xiao, Enhua Wu, Jianyuan Guo, Chunjing Xu, and Yunhe Wang. Transformer in transformer. Advances in Neural Information Processing Systems, 34:15908\u201315919, 2021.\\n\\n[20] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Doll\u00e1r, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16000\u201316009, 2022.\\n\\n[21] Zhenglun Kong, Peiyan Dong, Xiaolong Ma, Xin Meng, Mengshu Sun, Wei Niu, Xuan Shen, Geng Yuan, Bin Ren, Minghai Qin, et al. Spvit: Enabling faster vision transformers via soft token pruning. arXiv preprint arXiv:2112.13890, 2021.\\n\\n[22] Alex Krizhevsky et al. Learning multiple layers of features from tiny images. 2009.\\n\\n[23] Changlin Li, Tao Tang, Guangrun Wang, Jiefeng Peng, Bing Wang, Xiaodan Liang, and Xiaojun Chang. Bossnas: Exploring hybrid cnn-transformers with block-wisely self-supervised neural architecture search. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12281\u201312291, 2021.\\n\\n[24] Hanxiao Liu, Karen Simonyan, and Yiming Yang. Darts: Differentiable architecture search. arXiv preprint arXiv:1806.09055, 2018.\\n\\n[25] Jihao Liu, Xin Huang, Guanglu Song, Hongsheng Li, and Yu Liu. Uninet: Unified architecture search with convolution, 5586.\"}"}
{"id": "CVPR-2024-1742", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"transformer, and mlp. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXI, pages 33\u201349. Springer, 2022.\\n\\n[26] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pages 10012\u201310022, 2021.\\n\\n[27] Bowen Pan, Rameswar Panda, Yifan Jiang, Zhangyang Wang, Rogerio Feris, and Aude Oliva. Ia-redu: Interpretability-aware redundancy reduction for vision transformers. In Advances in Neural Information Processing Systems, pages 24898\u201324911. Curran Associates, Inc., 2021.\\n\\n[28] Zizheng Pan, Bohan Zhuang, Jing Liu, Haoyu He, and Jianfei Cai. Scalable vision transformers with hierarchical pooling. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 377\u2013386, 2021.\\n\\n[29] Yongming Rao, Wenliang Zhao, Benlin Liu, Jiwen Lu, Jie Zhou, and Cho-Jui Hsieh. Dynamicvit: Efficient vision transformers with dynamic token sparsification. Advances in neural information processing systems, 34:13937\u201313949, 2021.\\n\\n[30] Alfr\u00e9d R\u00e9nyi. On measures of entropy and information. In Proceedings of the Fourth Berkeley Symposium on Mathematical Statistics and Probability, Volume 1: Contributions to the Theory of Statistics, pages 547\u2013562. University of California Press, 1961.\\n\\n[31] Zhuoran Song, Yihong Xu, Zhezhi He, Li Jiang, Naifeng Jing, and Xiaoyao Liang. Cp-vit: Cascade vision transformer pruning via progressive sparsity prediction. arXiv preprint arXiv:2203.04570, 2022.\\n\\n[32] Xiu Su, Shan You, Jiyang Xie, Mingkai Zheng, Fei Wang, Chen Qian, Changshui Zhang, Xiaogang Wang, and Chang Xu. Vitas: Vision transformer architecture search. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXI, pages 139\u2013157. Springer, 2022.\\n\\n[33] Shengji Tang, Weihao Lin, Hancheng Ye, Peng Ye, Chong Yu, Baopu Li, and Tao Chen. Enhanced sparsification via stimulative training. arXiv preprint arXiv:2403.06417, 2024.\\n\\n[34] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco Massa, Alexandre Sablayrolles, and Herv\u00e9 J\u00e9gou. Training data-efficient image transformers & distillation through attention. In International conference on machine learning, pages 10347\u201310357. PMLR, 2021.\\n\\n[35] Chongjun Tu, Peng Ye, Weihao Lin, Hancheng Ye, Chong Yu, Tao Chen, Baopu Li, and Wanli Ouyang. Efficient architecture search via bi-level data pruning. arXiv preprint arXiv:2312.14200, 2023.\\n\\n[36] Wenhai Wang, Enze Xie, Xiang Li, Deng-Ping Fan, Kaitao Song, Ding Liang, Tong Lu, Ping Luo, and Ling Shao. Pyramid vision transformer: A versatile backbone for dense prediction without convolutions. In Proceedings of the IEEE/CVF international conference on computer vision, pages 568\u2013578, 2021.\\n\\n[37] Zhenyu Wang, Hao Luo, Pichao WANG, Feng Ding, Fan Wang, and Hao Li. VTC-LFC: Vision transformer compression with low-frequency components. In Advances in Neural Information Processing Systems, 2022.\\n\\n[38] Wei Wen, Chunpeng Wu, Yandan Wang, Yiran Chen, and Hai Li. Learning structured sparsity in deep neural networks. Advances in neural information processing systems, 29, 2016.\\n\\n[39] Zhenda Xie, Zheng Zhang, Yue Cao, Yutong Lin, Jianmin Bao, Zhuliang Yao, Qi Dai, and Han Hu. Simmim: A simple framework for masked image modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9653\u20139663, 2022.\\n\\n[40] Hancheng Ye, Bo Zhang, Tao Chen, Jiayuan Fan, and Bin Wang. Performance-aware approximation of global channel pruning for multitask cnns. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023.\\n\\n[41] Peng Ye, Baopu Li, Yikang Li, Tao Chen, Jiayuan Fan, and Wanli Ouyang. $\\\\beta$-darts: Beta-decay regularization for differentiable architecture search. In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 10864\u201310873. IEEE, 2022.\\n\\n[42] Fang Yu, Kun Huang, Meng Wang, Yuan Cheng, Wei Chu, and Li Cui. Width & depth pruning for vision transformers. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 3143\u20133151, 2022.\\n\\n[43] Shixing Yu, Tianlong Chen, Jiayi Shen, Huan Yuan, Jianchao Tan, Sen Yang, Ji Liu, and Zhangyang Wang. Unified visual transformer compression. In International Conference on Learning Representations, 2022.\\n\\n[44] Sangdoo Yun, Dongyoon Han, Seong Joon Oh, Sanghyuk Chun, Junsuk Choe, and Youngjoon Yoo. Cutmix: Regularization strategy to train strong classifiers with localizable features. In Proceedings of the IEEE/CVF international conference on computer vision, pages 6023\u20136032, 2019.\\n\\n[45] Hongyi Zhang, Moustapha Cisse, Yann N Dauphin, and David Lopez-Paz. mixup: Beyond empirical risk minimization. International Conference on Learning Representations, 2018.\\n\\n[46] Renrui Zhang, Ziyu Guo, Peng Gao, Rongyao Fang, Bin Zhao, Dong Wang, Yu Qiao, and Hongsheng Li. Point-m2AE: Multi-scale masked autoencoders for hierarchical point cloud pre-training. In Advances in Neural Information Processing Systems, 2022.\\n\\n[47] Sixiao Zheng, Jiachen Lu, Hengshuang Zhao, Xiatian Zhu, Zekun Luo, Yabiao Wang, Yanwei Fu, Jianfeng Feng, Tao Xiang, Philip HS Torr, et al. Rethinking semantic segmentation from a sequence-to-sequence perspective with transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 6881\u20136890, 2021.\\n\\n[48] Qinqin Zhou, Kekai Sheng, Xiawu Zheng, Ke Li, Xing Sun, Yonghong Tian, Jie Chen, and Rongrong Ji. Training-free transformer architecture search. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10894\u201310903, 2022.\\n\\n[49] Yuefu Zhou, Ya Zhang, Yanfeng Wang, and Qi Tian. Accelerate cnn via recursive bayesian pruning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 3306\u20133315, 2019.\"}"}
{"id": "CVPR-2024-1742", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Mingjian Zhu, Yehui Tang, and Kai Han. Vision transformer pruning. arXiv preprint arXiv:2104.08500, 2021.\\n\\nZhuofan Zong, Kunchang Li, Guanglu Song, Yali Wang, Yu Qiao, Biao Leng, and Yu Liu. Self-slimmed vision transformer. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XI, pages 432\u2013448. Springer, 2022.\"}"}
{"id": "CVPR-2024-1742", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Once for Both: Single Stage of Importance and Sparsity Search for Vision Transformer Compression\\n\\nHancheng Ye, Chong Yu, Peng Ye, Renqiu Xia, Yansong Tang, Jiwen Lu, Tao Chen, Bo Zhang\\n\\n1 School of Information Science and Technology, Fudan University\\n2 Shanghai Artificial Intelligence Laboratory\\n3 Academy for Engineering and Technology, Fudan University\\n4 Shanghai Jiao Tong University\\n5 Tsinghua University\\n\\nAbstract\\n\\nRecent Vision Transformer Compression (VTC) works mainly follow a two-stage scheme, where the importance score of each model unit is first evaluated or preset in each submodule, followed by the sparsity score evaluation according to the target sparsity constraint. Such a separate evaluation process induces the gap between importance and sparsity score distributions, thus causing high search costs for VTC. In this work, for the first time, we investigate how to integrate the evaluations of importance and sparsity scores into a single stage, searching the optimal subnets in an efficient manner. Specifically, we present OFB, a cost-efficient approach that simultaneously evaluates both importance and sparsity scores, termed Once for Both (OFB), for VTC. First, a bi-mask scheme is developed by entangling the importance score and the differentiable sparsity score to jointly determine the pruning potential (prunability) of each unit. Such a bi-mask search strategy is further used together with a proposed adaptive one-hot loss to realize the progressive and efficient search for the most important subnet. Finally, Progressive Masked Image Modeling (PMIM) is proposed to regularize the feature space to be more representative during the search process, which may be degraded by the dimension reduction. Extensive experiments demonstrate that OFB can achieve superior compression performance over state-of-the-art searching-based and pruning-based methods under various Vision Transformer architectures, meanwhile promoting search efficiency significantly, e.g., costing one GPU search day for the compression of DeiT-S on ImageNet-1K.\\n\\n1. Introduction\\n\\nVision Transformers (ViTs) are developing rapidly in many practical tasks, but they suffer from substantial computational costs and storage overhead, preventing their deployments for resource-constrained scenarios. Vision Transformer Compression (VTC), as an effective technique to relieve such problems, has advanced a lot and can be divided into several types including Transformer Architecture Search (TAS) [5, 6, 15, 25, 32, 35, 48] and Transformer Pruning (TP) [2, 7, 21, 27, 31, 37, 42, 51] paradigms. Although both TAS and TP can produce compact ViTs, their search process for the target sparsity often relies on a two-stage scheme, i.e., importance-then-sparsity evaluation for units (e.g., filters).\\n\\nThe importance evaluation aims at learning each unit's contribution to the prediction performance, while the sparsity evaluation aims at learning each unit's pruning choice. In general, the importance and sparsity score distributions are correlated in the search process, as shown in Fig. 1.\"}"}
{"id": "CVPR-2024-1742", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 2. Different paradigms for VTC. (a): SPOS-based TAS implicitly encodes the piecewise-decreasing importance scores for units due to the uniform sampling in pre-training; (b): The threshold-based TP explicitly evaluates the importance scores for units and sets a global threshold to perform pruning; (c): DARTS learns the importance distribution in a differentiable manner and selects the subnet of the highest architecture score; (d): OFB proposes the bi-mask score that entangles importance and sparsity scores together, to perform the search process in a single stage.\\n\\nAs for TAS that mainly follows the Single-Path-One-Shot (SPOS) search paradigm [18], the importance scores of units in each submodule are implicitly encoded into the supernet [32], as shown in Fig. 2a. This is mainly due to the ordinal weight-sharing mechanism during the pre-training of the pre-designed supernet [32]. In other words, the sub-modules with small indexes are implicitly assigned higher importance scores by uniform sampling during the supernet pre-training. Afterwards, evolutionary algorithms are employed to search for the optimal subnet given the implicitly-encoded importance score distribution and target sparsity constraint [5, 17, 23, 32, 48]. Such an implicit encoding process causes TAS limited to compressing a supernet from scratch, thus leading to a high search cost.\\n\\nOn the other hand, for TP that adopts the threshold-based pruning paradigm, the importance scores are pre-evaluated by a designed criterion, followed by the sparsity search using a designed strategy based on the importance distribution. However, searching for the fine-grained discrete sparsity from the evaluated importance distribution of each dimension is intractable and identified as an NP-hard problem [49]. As visualized in Fig. 2b, the importance score distribution of one dimension is usually continuous, with most points distributed around the mean value. Considering that the importance distribution varies in different dimensions and may be influenced by the pruning choice of other dimensions [32, 40], the traditional threshold-based methods can hardly search for the optimal compact models in a global manner.\\n\\nFrom the above analysis, the high compression costs can be largely attributed to the separate score evaluation, and the gap between importance and sparsity score distributions.\\n\\nTo tackle the above issues induced by the two-stage VTC scheme, we propose to conduct the ViTs search in a one-stage manner, where the importance and sparsity scores are learned simultaneously and entangled, to learn a discrete sparsity distribution from the entangled distribution adaptively. To achieve this, inspired by the differentiable search strategy in DARTS [8, 24, 41], we relax the sparsity score to a differentiable variable, and formulate a bi-mask score that entangles the importance and sparsity scores of each unit, to jointly assess the unit\u2019s prunability. Secondly, to optimize the bi-mask score, we introduce an adaptive one-hot loss function to adaptively convert the continuous bi-mask score into a binary one, i.e., the unit\u2019s pruning choice, as shown in Fig. 2d. Finally, during the search, we further develop a Progressive Masked Image Modeling (PMIM) technique, to regularize the dimension-reduced feature space with negligible additional costs. Our main contributions are:\\n\\n\u2022 To our best knowledge, our method is the first to explore the entanglement of importance and sparsity distributions in VTC, which relieves the bottleneck of searching the discrete sparsity distribution from the continuous importance distribution, highlighting the search efficacy and effectiveness of various ViTs compression.\\n\\n\u2022 We develop a novel one-stage search paradigm containing a bi-mask weight-sharing scheme and an adaptive one-hot loss function, to simultaneously learn the importance and sparsity scores and determine the units\u2019 prunability. Moreover, a PMIM regularization strategy is specially designed during searching, which gradually intensifies the regularization for representation learning as the feature dimension continues to be reduced.\\n\\n\u2022 Extensive experiments are conducted on ImageNet for various ViTs. Results show that OFB outperforms existing TAS and TP methods with higher sparsity and accuracy, and significantly improves search efficiency, e.g., costing one GPU search day to compress DeiT-S on ImageNet.\\n\\n2. Related Works\\n\\nTransformer Architecture Search. Recently, with various Vision Transformers spawning [9, 12, 26, 34], several works have explored searching for the optimal Transformer-based architecture. Existing Transformer Architecture Search (TAS) works [5, 32, 48] mainly follow the SPOS NAS [18] scheme, which first trains the supernet from scratch by iteratively training the sampled subnets, then searches for the target optimal subnet, followed by retraining the searched model. These methods focus on either designing the search space or the training strategy for the randomly initialized supernet, yet the supernet training is still time-consuming due to the numerous randomly initialized parameters to be fully trained. To address this, TF-TAS [48] provides a DSS...\"}"}
{"id": "CVPR-2024-1742", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"indicator to evaluate the subnet performance without training all supernet parameters. Compared with prior methods, our work highlights the one-stage search for compact architectures in off-the-shelf pre-trained ViTs, thus saving high costs for supernet training and an extra sparsity search.\\n\\nVision Transformer Pruning.\\n\\nUnlike the pruning for Convolutional Neural Networks (CNNs) [33, 38], the pruning for ViTs contains more prunable components, e.g., Patch Embedding, Patch Token, Multi-Head Self-Attention (MHSA), and MLPs, etc. S^2VITE [7] presents a pruning-and-growing strategy with 50% ratio to find the sparsity in several dimensions. WDpruning [42] performs TP via binary masks and injected classifiers, meanwhile designing a learnable pruning threshold based on the parameter constraint. ViT-Slim [2] employs soft masks with \u2113_1 penalty by a manually-set global budget threshold for TP. UVC [43] jointly combines different techniques to unify VTC. Compared with previous methods, our method features the entanglement of importance and sparsity distributions to jointly determine the prunability of each unit, and the adaptive conversion from the continuous score distribution into a discrete one, thus being able to better balance the sparsity constraint and model performance.\\n\\nMasked Image Modeling.\\n\\nMasked Image Modeling (MIM) [20, 39] is a self-supervised learning strategy for augmenting pre-training models, aiming to reconstruct the masked patches in the input image. Several works have explored the representation learning ability of MIM in the pre-training models for downstream tasks [1, 14, 16, 46], by predicting the patch- or feature-level labels. Differently, our work targets at the compression of pre-trained ViTs, and focuses on utilizing the representation learning ability of MIM to progressively improve the dimension-reduced feature space.\\n\\n3. The Proposed Approach\\n\\nWe first revisit the two-stage search paradigm and identify its problem, then propose a one-stage counterpart containing a bi-mask weight-sharing scheme with an improved optimization objective, to learn importance and sparsity scores simultaneously. Finally, PMIM is designed to boost the dimension-reduced features and enhance search performance.\\n\\n3.1. Problem Formulation\\n\\nPrior VTC works mainly focus on searching for an optimal sub-network given a supernet and the resource constraint. Let \\\\( N(A, W) \\\\) denote the supernet, where \\\\( A \\\\) and \\\\( W \\\\) refer to the architecture search space and weights of the supernet, respectively. The search for the optimal architecture can be generally formulated as a two-stage problem in Eq. (1).\\n\\n\\\\[\\n\\\\text{Stage 1. } S_A = f(W; A); \\\\\\\\\\n\\\\text{Stage 2. } \\\\min_{\\\\alpha \\\\in A, W} L_{val}(\\\\alpha, W; S_A), \\\\quad \\\\text{s.t. } g(\\\\alpha) \\\\leq \\\\tau, \\\\quad (1)\\n\\\\]\\n\\nwhere \\\\( f \\\\) denotes the criterion to evaluate (e.g., TP) or preset (e.g., TAS) the importance score of each unit \\\\( S_A \\\\) based on the search space, and \\\\( L_{val} \\\\) denotes the loss on the validation dataset. The \\\\( g \\\\) and \\\\( \\\\tau \\\\) represent the computation cost and the corresponding constraint, respectively. In the first stage, the importance distribution is globally learned from the weights of the supernet (e.g., TP) or naturally encoded in the training mode of the supernet (e.g., TAS). Based on the (piecewise) continuous importance distribution, the architecture parameter \\\\( \\\\alpha \\\\) is optimized to satisfy the sparsity constraint via the global threshold or evolutionary algorithms in the second stage, which can be viewed as a discretization process. Since the importance distribution is fixed during the search, the gap between the importance distribution and the searched discrete sparsity distribution (pruned or not pruned) may cause the sub-optimal search result. In other words, the pre-assessed importance score may change with the discretization of other units, and cannot fully represent the actual importance distribution in the searched model. Therefore, a better indicator to assess the unit's prunability could be an adaptively discretized score, that bridges the gap between the importance and sparsity distributions.\\n\\nInspired by DARTS [24], which designs a differentiable scheme to relax the pruning choice of each subspace to a softmax-activated probability over all subspaces in one dimension, we further develop a bi-mask scheme to learn the prunability of units in the pre-trained ViTs. In this scheme, the importance and sparsity scores are learned simultaneously in a differentiable manner to jointly determine the unit's prunability. In other words, the search objective is formulated into a one-stage problem, as shown in Eq. (2).\\n\\n\\\\[\\n\\\\min_{S, V, W} L_{train}(S, V, W), \\\\quad \\\\text{s.t. } g(V) \\\\leq \\\\tau, \\\\quad (2)\\n\\\\]\\n\\nwhere importance scores \\\\( S \\\\), sparsity scores \\\\( V \\\\), and supernet weights \\\\( W \\\\) are continually optimized to find the optimal subnet. Consequently, the model is evaluated and searched in a single stage, which is different from the prior works separately evaluating and searching subnets. The optimization of Eq. (2) comprises a bi-mask weight-sharing strategy to assign units with prunability scores, and an adaptive one-hot loss to achieve the target sparsity (See Sec. 3.2 and 3.3).\\n\\nSearch Space.\\n\\nWe first follow the TAS paradigm to construct a discrete search space for all units in each prunable submodule, including Q-K-V ratio, MLP ratio, Head number, and Patch Embedding (P. E.), as described in Table 1. Then, the search space is relaxed in a differentiable manner.\\n\\n3.2. Bi-mask Weight-sharing Strategy\\n\\nIn order to assess the prunability of each unit, we introduce a bi-mask weight-sharing strategy in the search process. Each...\"}"}
{"id": "CVPR-2024-1742", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Figure 3. The overview of OFB search framework, including the design of search space, search scheme, and regularization scheme. (a) For the search space, we consider four types of submodules. (b) For the search scheme, we simultaneously learn the importance score $S_{ij}$ and the sparsity score $V_{ij}$ based on the bi-mask weight-sharing strategy, under the guidance of an adaptive one-hot loss. (c) The PMIM technique is developed to augment the pruned feature space, which introduces a progressive masking strategy to MIM for better regularization.\\n\\nThe prunability score is represented by the value of the designed bi-mask $m_{ij}(t)$ that considers both the importance score and the sparsity score, which can be illustrated as follows:\\n\\n$$m_{ij}(t) = \\\\lambda(t)S_{ij} + [1 - \\\\lambda(t)]V_{ij}(\\\\alpha),$$\\n\\n(3)\\n\\nwhere the subscript index $ij$ denotes the $j$-th unit in the $i$-th prunable submodule. $\\lambda(t)$ denotes the time-varying weight coefficient of the importance score. Specifically, $\\\\lambda(t)$ linearly changes from one to zero until the model finishes searching. The motivation behind this is two-fold. From the lens of score optimization, i.e., the backward process, the sparsity score of each unit is related to its importance rank among all units in the same submodule. Therefore, before transmitting a large gradient to the sparsity score, more attention should be paid to learning a reliable importance score. Since the model weights $W$ are well trained in the supernet, the importance score could be learned appropriately in several epochs, thus providing a relatively accurate importance rank for the assignment of the sparsity score to each unit. After obtaining a relatively accurate importance score, optimization should be focused more on the sparsity score to make the pruning decision. From the lens of the discretization process, i.e., the forward process, the search target is to learn a discrete score for each unit; thus, the searched score should approach an approximately binary (0/1) distribution, which is exactly the desired distribution of the sparsity score $V$. Therefore, the learning focus of the prunability score should be gradually transferred from importance to sparsity during searching.\\n\\nAs for the importance score $S$, inspired by ViT-Slim [2], we introduce a soft mask that is randomly initialized and learnable in each unit, to indicate its contribution to supernet performance. The importance score is normalized to vary between $(0, 1)$ via sigmoid. As for the sparsity score $V$, we leverage the architecture parameter $\\\\alpha$ to generate the sparsity score of each unit. Given $\\\\alpha$, $V$ is computed via softmax to indicate the preserving potential, as formulated in Eq. (4):\\n\\n$$V_{ij}(\\\\alpha) = \\\\frac{\\\\|\\\\alpha_i\\\\|_0}{\\\\|\\\\alpha_i\\\\|_0} \\\\exp (\\\\alpha_{ik}) \\\\frac{\\\\|\\\\alpha_i, :\\\\|_0}{\\\\|\\\\alpha_i\\\\|_0} \\\\exp (\\\\alpha_{ik}) = X\\\\|\\\\alpha_i\\\\|_0 \\\\frac{\\\\|\\\\alpha_i\\\\|_0}{\\\\|\\\\alpha_i\\\\|_0} \\\\exp (\\\\alpha_{ik})$$\\n\\n(4)\\n\\nwhere $\\\\alpha_i$ is the architecture parameter vector of the $i$-th submodule to parameterize the sub-space into a continuous space. $p_{ik}$ and $\\\\Delta_i$ represent the step-wise normalized architecture score and the step size in the search space of the $i$-th submodule, respectively, where $p_{ik} = \\\\text{softmax}_k(\\\\alpha_{ik})$. Note that the weights in all sub-spaces of the submodule are shared as DARTS [24] does; therefore, the sparsity score of each unit is the sum of those shared architecture scores, making the sparsity distribution piecewise continuous. Unlike previous differentiable search methods uniformly initializing $\\\\alpha$ for the randomly initialized supernet, our method randomly initializes $\\\\alpha$ to reduce the inductive bias.\\n\\nAs for the weight-sharing strategy in differentiable search, considering the units with higher importance scores are more likely to be preserved, sparsity scores of more important units should be correspondingly higher than those less important (a high sparsity score means high preserving potential). Thus, at forward steps, the units in each submodule are reorganized w.r.t. their importance score rank and assigned sparsity scores in a descending order, as shown in Fig. 2d.\\n\\n3.3. Adaptive One-hot Loss\\n\\nGiven bi-masks as introduced above, which soft mask units to indicate the prunability during searching, the optimization...\"}"}
