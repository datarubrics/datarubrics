{"id": "CVPR-2022-1685", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Semi-Weakly-Supervised Learning of Complex Actions from Instructional Task Videos\\n\\nYuhan Shen\\nNortheastern University\\nshen.yuh@northeastern.edu\\n\\nEhsan Elhamifar\\nNortheastern University\\ne.elhamifar@northeastern.edu\\n\\nAbstract\\nWe address the problem of action segmentation in instructional task videos with a small number of weakly-labeled training videos and a large number of unlabeled videos, which we refer to as Semi-Weakly-Supervised Learning (SWSL) of actions. We propose a general SWSL framework that can efficiently learn from both types of videos and can leverage any of the existing weakly-supervised action segmentation methods. Our key observation is that the distance between the transcript of an unlabeled video and those of the weakly-labeled videos from the same task is small yet often nonzero. Therefore, we develop a Soft Restricted Edit (SRE) loss to encourage small variations between the predicted transcripts of unlabeled videos and ground-truth transcripts of the weakly-labeled videos of the same task. To compute the SRE loss, we develop a flexible transcript prediction (FTP) method that uses the output of the action classifier to find both the length of the transcript and the sequence of actions occurring in an unlabeled video. We propose an efficient learning scheme in which we alternate between minimizing our proposed loss and generating pseudo-transcripts for unlabeled videos. By experiments on two benchmark datasets, we demonstrate that our approach can significantly improve the performance by using unlabeled videos, especially when the number of weakly-labeled videos is small.\\n\\n1. Introduction\\n\\nMany of humans everyday tasks are procedural, where a task consists of a sequence of actions that must be followed to achieve the desired goal. Therefore, there has been an explosion of instructional videos on the web, teaching how to perform tasks, such as cooking recipes, repairing devices, assembling furnitures, performing emergency first aid, etc. [1, 10, 15, 23, 38, 56, 66, 67]. Automatic learning of procedural tasks from instructional videos has important applications, such as teaching intelligent agents to perform complex tasks, constructing large knowledge bases of compact instructions, and automatic performance evaluation for executing tasks. Over the past several years, we have seen great advances on different aspects of learning from instructions [1, 4, 8, 15, 19, 20, 33, 35\u201337, 49, 50, 56, 66, 67].\\n\\nA major challenge in learning from instructional videos is that videos are long and have many actions, therefore annotation is costly and complex. This poses a major challenge for scaling the learning to a large number of tasks and videos. Therefore, while a few fully-supervised methods have studied learning from densely-annotated videos [24, 27, 45, 49, 51, 53, 64, 66], the majority of existing works have focused on using less supervision. Specifically, weakly-supervised methods assume that each training video is accompanied with its transcript (ordered list of actions) [5, 7, 12, 30, 35, 44, 67] or action-set (unordered list of actions) [16, 31, 32, 36, 43]. While using weak supervision reduces the annotation cost by removing the need for specifying temporal boundaries of actions, it still requires annotators to watch entire videos. On the other hand, unsupervised methods remove the need for annotation by using unlabeled videos and leveraging the similarity of videos of the same task. However, existing similarity constraints, e.g., videos following the same sequence of actions or the same pairwise action ordering, are limiting and often violated in videos (see Figure 1). This has led to performance of unsupervised methods significantly lagging behind that of the weakly-supervised algorithms.\\n\\nPaper Contributions.\\nMotivated by the above discussion, we study a new action segmentation problem in which we assume having access to a small number of weakly-labeled videos and a large number of unlabeled videos. We propose a general framework that can efficiently learn from both types of videos and can leverage any of the existing weakly-supervised action segmentation methods. Our key observation is that the distance between the transcript of an unlabeled video and those of the weakly-labeled videos from the same task is small yet often nonzero. Therefore, we develop a Soft Restricted Edit (SRE) loss to encourage small variations between the predicted transcripts of unlabeled videos and ground-truth transcripts of the weakly-labeled videos of the same task. To compute the SRE loss, we develop a flexible transcript prediction (FTP) method that uses the output of the action classifier to find both the length of the transcript and the sequence of actions occurring in an unlabeled video. We propose an efficient learning scheme in which we alternate between minimizing our proposed loss and generating pseudo-transcripts for unlabeled videos. By experiments on two benchmark datasets, we demonstrate that our approach can significantly improve the performance by using unlabeled videos, especially when the number of weakly-labeled videos is small.\"}"}
{"id": "CVPR-2022-1685", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"training videos and a large number of unannotated videos (with only task labels) from multiple tasks. We refer to this setting as Semi-Weakly-Supervised Learning (SWSL) of actions, whose goal is to learn a video segmentation model using both types of training videos. Using unlabeled videos allows us to effectively regularize learning from a small number of weakly-labeled videos, which would be insufficient for learning action segmentation/classifier using current methods. On the other hand, using weakly-labeled videos allows us to guide learning from unlabeled videos by leveraging a few transcripts of each task.\\n\\nWe propose an SWSL method to find the parameters of a video feature learning module and an action classifier by simultaneously learning from weakly-labeled and unlabeled videos. Our key observation is that transcripts of unlabeled videos often have small but nonzero distances to the transcripts of the weakly-labeled videos from the same task, accounting for small variations by which the task could be accomplished. Therefore, we develop a differentiable Soft Restricted Edit (SRE) loss, which allows us to predict a transcript for an unlabeled video that is close to ground-truth transcripts of the weakly-labeled videos of the same task, yet could be different from them. To compute the SRE loss, we develop a flexible transcript prediction (FTP) method that uses the output of the action classifier to find both the length of the transcript and the sequence of actions occurring in an unlabeled video. Motivated by prior works on self-training [28, 62, 63], we propose a learning scheme in which we alternate between i) minimizing our proposed loss (sum of the weakly-supervised and SRE losses) on both types of videos; ii) adding a few most confident unlabeled videos and their pseudo-transcripts to the weakly labeled set. An advantage of our method is that it can use any existing weakly-supervised method. By experiments on two benchmark datasets of Breakfast [23] and CrossTask [67], we demonstrate the effectiveness of our approach.\\n\\n2. Related Works\\n\\nAction Segmentation. Depending on the supervision type, existing works on action segmentation in instructional videos can be divided into three categories. First, fully-supervised methods assume that frame-wise annotations of actions in videos are given [24, 27, 45, 49, 51, 53, 64, 66]. Second, weakly-supervised methods assume each training video comes with an ordered or unordered list of its actions [5, 7, 30, 31, 35, 36, 43, 44, 67] or its summary [39, 60]. Third, unsupervised learning methods exploit the common structure, cross-modality consistency or temporal information of videos of the same task to discover and localize actions [14, 15, 18, 25, 48, 50]. In this paper, we propose the new setup of semi-weakly-supervised learning from instructional videos, which has not been explored yet.\\n\\nWeakly-supervised action segmentation methods mostly use the transcripts to learn a mapping from video features to framewise action class probabilities, so the major difference among prior works is the choice of mapping functions and loss functions. In the paper, we leverage two existing weakly-supervised methods [30, 54]. Specifically, [30] uses a GRU layer with a fully-connected layer as the mapping function, while [54] uses a deep convolutional neural network. As for the loss functions, [30] uses a constrained discriminative forward loss (CDFL) to distinguish the valid frame labelings, consistent with the ground-truth transcripts, from invalid labelings. [54] has a module to predict the class and length of segments and uses the mutual consistency (MuCon) loss to enforce the consistency of the frame-wise probabilities and predicted segments.\\n\\nSemi-supervised learning (SSL) approaches aim to learn from both labeled and unlabeled data [21, 34, 41, 46, 61, 63, 65]. In video understanding, SSL has been studied for temporal action proposals, human pose estimation, salient object detection, action recognition, etc. [42, 52, 59, 62] There are two major directions in SSL: self-training and consistency regularization. Self-training based methods [28, 62, 63] first train a model using supervised methods and then predict pseudo-labels for unlabeled data. Consistency regularization, first proposed by [3] and extended in several works including temporal ensembling [26] and mean teacher [57], minimizes the discrepancy between predictions of perturbed input data.\\n\\nSequence Alignment. Our work is related to sequence alignment. Dynamic Time Warping (DTW) is a classic algorithm to measure the distance between two temporal sequences [47]. Recent variants of DTW include differentiable approximations [9, 19] and allowing skipping outliers [13]. Weak sequence alignment algorithm (WSA) [50] performs one-to-one alignment while allowing some items to be unmatched, and is extended to be differentiable. However, all of these works require the alignment to strictly obey a temporal order. Order-preserving Wasserstein distance [55] can tackle local temporal distortion via optimal transport, but it is multiple-to-multiple or one-to-multiple alignment and not differentiable. Our work is motivated by edit distance, which measures the distance between two strings. Levenshtein distance [2, 29] is a specific-type of edit distance that allows deletion, insertion and substitution, and can be computed by Needleman-Wunsch [40] algorithm. [22] extends the inputs of Needleman-Wunsch algorithm from strings to time series and makes it differentiable, but it does not allow adjacent transposition. Restricted Edit Distance and Damerau\u2013Levenshtein distance [11, 17] allow the transposition of two adjacent characters, but neither of them are differentiable. Our proposed SRE loss is an extension of restricted edit distance, which can be applied to temporal sequences and is differentiable.\"}"}
{"id": "CVPR-2022-1685", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Illustrative Example:\\n\\nTo better highlight the difference between the Restricted Edit distance and other sequence alignment methods, Figure 2 shows the alignment between two transcripts by different methods. DTW strictly aligns every entry in the two sequences, leading to many false alignments. While Edit Distance or WSA can obtain a one-to-one alignment and skip some unmatched items, they strictly follow temporal ordering and cannot handle the transposition from \\\"crack egg\\\" to \\\"add salt and pepper\\\". In contrast, the Restricted Edit distance can handle outlier elements and the transposition of adjacent items.\\n\\n3. Problem Statement\\n\\nIn the semi-weakly-supervised learning of actions, we assume there are $N$ weakly-labeled videos, $\\\\{X^w_n\\\\}_{n=1}^N$, and $M$ unlabeled videos, $\\\\{X^u_m\\\\}_{m=1}^M$. The videos come from multiple tasks and each video consists of a sequence of actions required to achieve the underlying task. Let $O$ denote the number of tasks and $A$ denote the number of action classes across all videos. Our goal is to learn a model that segments a test video into different actions and recognizes the action of each segment and the underlying task of the video.\\n\\nMore specifically, for weakly-labeled training videos, we assume we have triplets $\\\\{(X^w_n, G^w_n, y^w_n)\\\\}_{n=1}^N$ of video features, transcripts and task labels, $X^w_n = (x^w_{n,1}, x^w_{n,2}, \\\\ldots, x^w_{n,T^n})$, $G^w_n = (g^n_{1,1}, g^n_{1,2}, \\\\ldots, g^n_{L^n,1})$, $y^w_n \\\\in \\\\{1, \\\\ldots, O\\\\}$, (1) where $x^w_{n,i} \\\\in \\\\mathbb{R}^d$ is the $i$-th frame in the $n$-th weakly-labeled video and $T^n$ is the length of the $n$-th video. Also, $G^w_n$ is the video transcript (weak label), with the one-hot encoding $g^n_{l,1} \\\\in \\\\{0, 1\\\\}$ $A$ denoting the $l$-th action in the $n$-th video, $L^n$ is the length of the transcript, and $y^w_n$ is the task label. For each task $o \\\\in \\\\{1, \\\\ldots, O\\\\}$, we denote the set of transcripts of all weakly-labeled videos from the task by $G^o$, i.e., $G^o = \\\\{G^w_n | \\\\text{if } y^w_n = o, \\\\forall n\\\\}$.\\n\\nUnlabeled Video Weakly-Labeled Video Action Classifier Transcript:\\n\\nFigure 3. Our proposed framework for learning from both weakly-labeled and unlabeled instructional videos of multiple tasks.\\n\\nOn the other hand, for unlabeled videos, we have pairs $\\\\{(X^u_m, y^u_m)\\\\}_{m=1}^M$ of video features and task labels, $X^u_m = (x^u_{m,1}, x^u_{m,2}, \\\\ldots, x^u_{m,T^u_m})$, $y^u_m \\\\in \\\\{1, \\\\ldots, O\\\\}$, (3) where $x^u_{m,i}$ is the feature of the $i$-th frame in the $m$-th unlabeled video, and $T^u_m$ is the length of the $m$-th video.\\n\\n4. Semi-Weakly-Supervised Action Learning\\n\\n4.1. Overview of Proposed Framework\\n\\nWe propose a general framework for jointly learning from (small) weakly-labeled and (large) unlabeled videos. As shown in Figure 3, our framework consists of two branches for learning from both types of training videos while using a shared action classifier. For weakly-labeled videos, in our proposed framework, we can flexibly use any existing weakly-supervised method. Let $L_{weak}$ denote the associated loss, which is introduced in Sec. 4.2.\\n\\nFor unlabeled videos, given the video features as inputs, we use the action classifier to output a frame-wise probability matrix $P \\\\in [0, 1]^{T \\\\times A}$ that captures the probability of each frame belonging to each action. To predict the transcript of each unlabeled video, we propose a Flexible Transcript Prediction (FTP) algorithm that takes $P$ as input and outputs the transcript of the video and the associated segmentation. We use the key observation that the predicted transcript of an unlabeled video should have a small distance to the transcripts of the weakly-labeled videos of the same task, corresponding to small variations by which a task could be accomplished.\\n\\nTherefore, we propose the Soft Restricted Edit (SRE) distance, a differentiable loss that allows insertion, deletion, substitution and adjacent transposition for computing the distance between the predicted transcript and the training transcript set. Thus, we can predict transcripts of unlabeled videos that are sufficiently close to those of the weakly-labeled videos, instead of enforcing the predicted transcript to coincide with the training transcripts.\"}"}
{"id": "CVPR-2022-1685", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"log-likelihood in (5) and solve the problem to also search for a transcript of sufficiently large length. Thus, we add the transcript length, $L$, to our objective. That is, we simultaneously search for the segment length and the sequence of actions in each video.\\n\\nGiven the framewise probability matrix $P_{t,a}$, for each video, we can find the segmentation boundary points by back-tracking. To tackle the problem, first notice that, given a fixed number of weakly-labeled videos, we can find the segmentation boundary points by back-tracking. This is denoted by $W_l$. The overall complexity is $O(T^2)$. To significantly improve its performance, especially when the number of weakly-labeled videos is small, we use unlabeled videos and gradually add the videos with the most confident transcripts into the weakly-labeled set. This is denoted by $T_l$. The self-training strategy is introduced in Sec. 4.5.\\n\\nIn other words, we simultaneously search for the segmentation boundary points by back-tracking.\\n\\nFinally, we recover the transcript using the geometric average of the predicted transcript length, i.e., the number of actions in the predicted transcript, $\\\\hat{l}_T$. As training proceeds, we generate pseudo-transcripts for each video, and the classes of segments and the sequence of actions in each segment.\\n\\nSimilarly, the probability, denoted by $p$, is defined as follows:\\n\\n$$W_{a} = \\\\arg\\\\min_{d} \\\\sum_{s} \\\\lambda_L \\\\left( \\\\prod_{t=1}^{T} \\\\max \\\\left( \\\\Theta_{s,t} - A_t, 0 \\\\right) \\\\right)$$\\n\\nwhere $\\\\Theta_{s,t}$ is the provided transcript, $A_t$ is the number of action classes, and $\\\\lambda_L$ is a regularization hyperparameter.\\n\\nTo learn from weakly-labeled training videos, whose set of weak labels is denoted by $\\\\mathcal{W}$, we use two state-of-the-art methods, MuCon [54] and CDFL [30], which we reviewed in Sec. 2.\\n\\nComputational Complexity. The time cost of splitting the video into segments is $O(T^2)$, where $T$ is the number of frames and $L$ is the number of action classes, our goal is to predict the transcript length, i.e., the number of actions in the predicted transcript, $\\\\hat{l}_T$. We develop the differentiable Soft Restricted Edit (SRE) distance, which corresponds to the inner summation of the first term in (6) and represents the negative log-likelihood of the transcript length, which can be estimated from weakly-labeled videos, and $\\\\ell_L$ is a predefined range for the transcript length.\\n\\n4.3. Flexible Transcript Prediction (FTP)\\n\\nFlexible Transcript Prediction (FTP) algorithm to estimate both. To tackle the problem, first notice that, given a fixed number of weakly-labeled videos, we can find the segmentation boundary points by back-tracking.\\n\\nWe develop the differentiable Soft Restricted Edit (SRE) distance, which allows the operations of insertion, deletion, substitution by allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion, substitution allowing the operations of insertion, deletion"}
{"id": "CVPR-2022-1685", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"of the first\\nprogramming. We compute a cumulative cost matrix\\nadjacent characters.\\ninsertion, deletion, substitution\\nare\\nstrings into the other, where the admissible edit operations\\nbetween the two sequences. We can recursively compute\\ni.e.,\\nex\\nstrings.\\nrestricted edit distance between the two strings is 3.\\n\\ntranscript of a weakly-labeled video,\\n\\nvectors. In our case, the two sequences are the ground-truth\\ngeneral case of computing distances between sequences of\\nvideo feature learning) and extend it to handle the more\\n\\n(2\\nthe value of the\\nswapped. Since they are, we move two steps back and take\\n(4\\nthe\\n\\n(10)\\nwas the minimum among\\n\\n(9)\\nentry in (9) is the cost of adjacent transposition. In our\\nargument is true and is zero otherwise. Notice that the last\\n\\n1\\nwhere\\n\\n4\\nThe main difference between restricted edit distance and the edit\\n\\nSRE Forward Propagation.\\n\\nTo motivate our method for computing SRE loss, we start\\nSRE Backward Propagation.\\n\\nGiven that an input to the\\nweakly-labeled transcript. More\\n\\nL\\nof the weakly-labeled videos of the same task. More\\n\\n(13)\\nand the predicted transcript of an unlabeled video,\\n\\nQ\\nand the predicted transcript of an unlabeled video,\\n\\nQ\\n\\nJ\\n\\nL\\n\\n(12)\\nfor deletion,\\n\\nT\\n\\n(11)\\n\\nD\\n\\nI\\n\\n(8)\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{sre} & = & \\\\text{restricted edit distance between two strings, and then generalize it to arbitrary sequences.}\\n\\\\end{align*}\\n\\\\]\\n\\nMotivating Example.\\n\\nstrings, and then generalize it to arbitrary sequences.\\n\\nTo compute the restricted edit distance between two\\n\\nThe\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{SRE Forward Propagation.}\\n\\\\end{align*}\\n\\\\]\\n\\n\\\\[\\n\\\\begin{align*}\\n\\\\text{SRE Backward Propagation.}\\n\\\\end{align*}\\n\\\\]\\n\\nWe make the restricted edit\\n\\nSRE Forward Propagation.\\n\\nSRE Backward Propagation.\\n\\nGiven that an input to the\\nweakly-labeled transcript. More\"}"}
{"id": "CVPR-2022-1685", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Algorithm 2: Forward Propagation for SRE\\n\\n**input:** Pairwise cost matrix $\\\\Delta = [\\\\delta_{i,j}] \\\\in \\\\mathbb{R}^{L' \\\\times L}$; $c_D, c_I, c_T, \\\\beta \\\\geq 0$.\\n\\n1. $e_{i,1} = (i - 1) \\\\cdot c_D, i \\\\in \\\\{1, 2, ..., L' + 1\\\\}$\\n2. $e_{1,j} = (j - 1) \\\\cdot c_I, j \\\\in \\\\{2, ..., L + 1\\\\}$\\n3. for $i \\\\leftarrow 2$ to $L' + 1$ do\\n   4. for $j \\\\leftarrow 2$ to $L + 1$ do\\n      5. update $e_{i,j}$ via (10);\\n\\n**output:** SRE Loss, $L_{sre} = e_{L' + 1, L + 1}$.\\n\\nAlgorithm 3: Backward Propagation for SRE\\n\\n**input:** Pairwise cost $\\\\Delta = [\\\\delta_{i,j}] \\\\in \\\\mathbb{R}^{L' \\\\times L}$; cumulative cost $E = [e_{i,j}] \\\\in \\\\mathbb{R}^{L' + 1 \\\\times L + 1}$; $c_I, c_D, c_T, \\\\beta \\\\geq 0$.\\n\\n1. $h_{L' + 1, L + 1} = 1$\\n2. for $i \\\\leftarrow L' + 1$ to 1 do\\n   3. for $j \\\\leftarrow L + 1$ to 1 do\\n      4. update $a_{i,j}, b_{i,j}, h_{i,j}$ via Eq. (13) in the supplementary material;\\n      5. Update $r_{i,j}$ via (4.4) and set $R = [r_{i,j}]$.\\n\\n**output:** $\\\\nabla Q_{SRE}(G, Q) = \\\\left(\\\\frac{\\\\partial \\\\Delta(G, Q)}{\\\\partial Q}\\\\right)^T_R$.\\n\\nIn the supplementary materials, we can efficiently compute $r_{i,j}$ using $h_{i,j}$, and two auxiliary gradient matrices with entries $a_{i,j} = \\\\frac{\\\\partial e_{i+1,j+1}}{\\\\partial e_{i,j}}$ and $b_{i,j} = \\\\frac{\\\\partial e_{i+2,j+2}}{\\\\partial e_{i,j}}$, where $r_{i,j} = h_{i+1,j+1} \\\\cdot a_{i,j} + h_{i+1,j+2} \\\\cdot b_{i,j} - h_{i+2,j+1} \\\\cdot b_{i,j}$.\\n\\nWe also show that $h_{i,j}, a_{i,j}, b_{i,j}$ can be recursively updated starting from the last row and column of $E$. Algorithm 3 shows the backward propagation for computing the gradient with respect to $Q$. Once finished, we can back-propagate the gradient w.r.t. $\\\\Theta$ by using (13).\\n\\n### 4.5. Training and Inference\\n\\nTo jointly learn from both weakly-labeled and unlabeled videos, we propose to minimize\\n\\n$$L_{swsl} = \\\\sum L_{weak}(X_w^n, G_w^n) + \\\\rho \\\\sum L_{sre}(G_y^u_m, Q_u^m),$$\\n\\n(14)\\n\\nover the parameters of the feature learning and action classifier modules as well as searching over the predicted transcripts, shown in Figure 3, where $\\\\rho \\\\geq 0$ is a hyperparameter. The first term in (14) minimizes the distance between each weakly-labeled video and its ground-truth transcript, while the second term minimizes the distance between the predicted transcript of each unlabeled video and the set of weakly-labeled transcripts of the same task.\\n\\nMotivated by works on self-training [28, 62, 63], we propose a training strategy which alternates between minimizing $L_{swsl}$ and moving some of the unlabeled videos and their predicted transcripts to the weakly-supervised set. We show that this approach works better than optimizing (14) once. More specifically, we minimize $L_{swsl}$ using both the current weakly-labeled videos and unlabeled videos. We then generate pseudo-transcripts for unlabeled videos and add the unlabeled videos with the most confident pseudo-transcripts to the weakly-labeled set (see below for details) and retrain the model by minimizing $L_{swsl}$, and so on. In the last iteration, all unlabeled videos have been moved to the weakly-labeled set, therefore, we minimize $L_{weak}$.\\n\\nFor self-training, given an unlabeled video, we compute $L_{weak}$ between the video and all transcripts in the weakly-labeled set, and find the transcript $G^*_u \\\\in \\\\{0, 1\\\\}^{L' \\\\times A}$ with the minimum loss. We use the transcript probability $Q \\\\in [0, 1]^{L \\\\times A}$ produced by FTP, and compute the restricted edit distance between $G^*$ and $Q$ to find their alignment $M \\\\in \\\\{0, 1\\\\}^{L' \\\\times L}$, where $m_{i,j} = 1$ indicates that the $i$-th action in the transcript occurs in the $j$-th segment in $Q$. We then generate the one-hot pseudo-transcript for the unlabeled video using $\\\\tilde{Q} = M^T G^*$. Finally, we choose a few videos with the smallest loss values and add them along with their pseudo-transcript to the weakly-labeled set.\\n\\n**Remark 1**\\n\\nA conventional self-training approach has two major differences with our method. First, it only minimizes the first term of (14), i.e., $\\\\rho = 0$. Second, it generates a pseudo-label for an unlabeled video using the most probable transcript from the training videos. However, this enforces the unlabeled videos to have exactly the same transcript as weakly-labeled videos, which is limiting, especially when the number of weakly-labeled videos is small. In the experiment, we show that this strategy does not perform well compared to our method.\\n\\n### 5. Experiments\\n\\n#### 5.1. Experimental Setup\\n\\n**Datasets.** For our experiments, we use two benchmark datasets of Breakfast [23] and CrossTask [67]. Breakfast consists of 1,712 videos of people making breakfast with 10 cooking tasks. The dataset contains 48 actions in total and on average, about 7 action instances are performed in each video. We use the existing 4 training/testing data splits in the dataset and report the average performance on the 4 splits.\\n\\nCrossTask consists of videos from 18 primary tasks. We follow [35] and use the 14 cooking-related tasks, which contains 2,552 videos and 80 classes of actions. We use the same training/testing split as in [35], with 90% training and 10% testing videos. For each dataset, we randomly split the training set into two subsets: one subset with weak labels (action transcripts and task labels), and the other subset with only task labels, and evaluate the performance on testing set. We investigate the effect of using different ratios of the number of weakly-labeled videos to the total number of videos used, including 1%, 2%, 5%, 10%, 20%.\"}"}
{"id": "CVPR-2022-1685", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Evaluation. For a comprehensive analysis of the results, we report multiple evaluation metrics. Following most works in action segmentation [25,30,35,54], we compute Mean over Frame (MoF), i.e., frame-wise accuracy, and intersection over union (IoU). Since in CrossTask, over 70% of frames are background, simply predicting all frames as background will lead to a high MoF. Thus, to avoid this undesired effect, we also report the F1-score [14, 50] on CrossTask.\\n\\nImplementation Details. To show that our proposed framework can leverage any existing weakly-supervised action segmentation methods, we use MuCon [54] and CDFL [30]. To be consistent with prior works, on Breakfast, we use the 2048-dimensional RGB+Flow I3D features [6] for MuCon, and the 64-dimensional improved dense trajectory features [58] for CDFL. On CrossTask, we use the 3200-dimensional released features [67] for MuCon and, following [35], reduce the feature dimension to 64 via PCA for CDFL. The average number of frames on Breakfast is over 2000, so we apply a temporal average pooling with stride 8 before inputting the frame-wise probabilities into our Flexible Transcript Prediction (FTP) module. We start the training for several epochs using weakly-labeled videos, and then use both weakly-labeled and unlabeled videos to optimize (14). We generate pseudo-transcripts for the unlabeled videos in three rounds, each round adding 1/3 of the unlabeled videos and their learned pseudo-transcripts to the weakly-labeled set. We keep the same setups as in the original works on MuCon and CDFL [30,54]. Due to space limitation, other details such as hyperparameter values are reported in the supplementary materials.\\n\\nBaselines. We mainly compare four methods: i) Weakly-Supervised Learning (WSL): we apply $L_{\\\\text{weak}}$ (MuCon or CDFL loss) on weakly-labeled videos only; ii) Semi-Weakly-Supervised Learning (SWSL): we apply $L_{\\\\text{swsl}}$ for all videos without generating pseudo-transcripts (i.e., without self-training); iii) WSL+Self: we use self-training in weakly-supervised methods, where we minimize only $L_{\\\\text{weak}}$ and iteratively generate pseudo-transcripts of unlabeled videos and add them to the weakly-labeled set and re-train the model; iv) SWSL+Self: our final approach, where we minimize our proposed $L_{\\\\text{sre}}$ loss, while iteratively increasing the weakly-labeled set using self-training.\\n\\n5.2. Experimental Results\\n\\nAction Segmentation Performance. Tables 1 and 2 show the average performances of different methods on Breakfast and CrossTask when we use, respectively, MuCon [54] and CDFL [30] as the backbone weakly-supervised module in our framework. First, notice that using unlabeled videos always improves the performance of WSL. In particular, in Table 1, when the number of labeled videos is extremely small (1%), our method (SWSL+Self) improves the MoF by 14.0%, IoU by 16.3% on Breakfast, and MoF by 9.9%, WP UP\\n\\n|               | 1%  | 99% |\\n|---------------|-----|-----|\\n| WSL           | 11.0| 13.5|\\n| SWSL          | 23.1| 21.8|\\n| WSL+Self      | 22.3| 26.1|\\n| SWSL+Self     | 25.0| 29.8|\\n\\n|               | 2%  | 98% |\\n|---------------|-----|-----|\\n| WSL           | 12.9| 14.8|\\n| SWSL          | 21.2| 23.5|\\n| WSL+Self      | 26.5| 27.8|\\n| SWSL+Self     | 26.7| 30.6|\\n\\n|               | 5%  | 95% |\\n|---------------|-----|-----|\\n| WSL           | 23.1| 25.8|\\n| SWSL          | 28.1| 23.5|\\n| WSL+Self      | 32.7| 31.0|\\n| SWSL+Self     | 32.5| 31.7|\\n\\n|               | 10% | 90% |\\n|---------------|-----|-----|\\n| WSL           | 28.0| 28.8|\\n| SWSL          | 33.9| 31.2|\\n| WSL+Self      | 36.7| 32.1|\\n| SWSL+Self     | 36.3| 33.4|\\n\\n|               | 20% | 80% |\\n|---------------|-----|-----|\\n| WSL           | 35.2| 33.4|\\n| SWSL          | 36.7| 33.6|\\n| WSL+Self      | 38.6| 34.7|\\n| SWSL+Self     | 39.8| 36.1|\\n\\nTable 1. Performance on Breakfast and CrossTask, when using MuCon as the backbone weakly-supervised module in our framework. (WP: Weakly-labeled video Percentage. UP: Unlabeled video Percentage. \u2020 reported in [54]; \u2217 obtained in our experiments.)\\n\\nIoU by 3.3% and F1 by 6.3% on CrossTask. Additionally, SWSL+Self significantly improves over WSL+Self, e.g., improves MoF by 2.7%, IoU by 3.7% on Breakfast, and MoF by 7.6%, IoU by 1.9%, F1 by 0.9% on CrossTask. This shows the effectiveness of our proposed $L_{\\\\text{sre}}$ loss and simultaneously learning the model from both weakly-labeled and unlabeled videos. Finally, notice that combining self-training with our method (SWSL+Self vs SWSL) improves MoF by 1.9%, IoU by 8.0% on Breakfast, and MoF by 0.4%, IoU by 1.3%, F1 by 2.6% on CrossTask.\\n\\nTranscript Prediction. Table 3 shows the normalized edit distance (smaller is better) between the predicted transcript and the ground-truth transcript on the test set. Notice that our SWSL+Self method performs better than WSL+Self in all cases. Given that the difference between the two methods is in using our proposed $L_{\\\\text{sre}}$ loss, the improvement shows that encouraging the transcript of unlabeled videos to have a small distance to the transcripts of weakly-labeled videos (instead of enforcing them to coincide) is beneficial for transcript prediction, especially when the number of labeled videos is small. Besides, the improvement is more remarkable on CrossTask than Breakfast, because CrossTask has more diverse transcripts and benefits more from allowing flexible transcripts.\\n\\nComparison with Soft Edit distance. Table 4 compares the performance of our proposed Soft Restricted Edit (SRE) distance with Soft Edit (SE) distance, which does not allow adjacent transposition. Notice that, generally, SRE performs better than SE, especially when the number of weakly-labeled videos is small. This comes from the fact...\"}"}
{"id": "CVPR-2022-1685", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Table 2. Performance on Breakfast and CrossTask, when using CDFL as the backbone weakly-supervised module in our framework. (WP: Weakly-labeled video Percentage. UP: Unlabeled video Percentage. \u2020 reported in [30]; \u2217 obtained in our experiments.)\\n\\n| WP (%) | SWSL | SWSL+Self |\\n|--------|------|-----------|\\n| 1%     | 0.437| 0.538     |\\n| 2%     | 0.419| 0.510     |\\n| 5%     | 0.358| 0.488     |\\n| 10%    | 0.364| 0.481     |\\n| 20%    | 0.322| 0.451     |\\n\\nTable 3. Normalized edit distance (smaller is better) between the predicted transcript and ground-truth transcript on the test set for MuCon.\\n\\n| WP (%) | SWSL | SWSL+Self |\\n|--------|------|-----------|\\n| 1%     | 24.3 | 29.2      |\\n| 2%     | 25.3 | 29.9      |\\n| 5%     | 31.1 | 30.8      |\\n| 10%    | 33.5 | 33.4      |\\n| 20%    | 39.2 | 35.8      |\\n\\nTable 4. Comparison on Soft Edit distance (disallow adjacent transposition) and our proposed Soft Restricted Edit distance (allow adjacent transposition) for MuCon on Breakfast with respect to different Weakly-labeled video Percentage (WP).\\n\\n| WP (%) | MoF | IoU |\\n|--------|-----|-----|\\n| 1%     | 24.3| 29.2|\\n| 2%     | 25.3| 29.9|\\n| 5%     | 31.1| 30.8|\\n| 10%    | 33.5| 33.4|\\n| 20%    | 39.2| 35.8|\\n\\nTraining Effect. Figure 5 shows the performance of two methods (SWSL+Self and SWSL) on the Breakfast test set as a function of training epochs. We train the model using weakly-labeled videos for 60 epochs and then add unlabeled videos for training. For SWSL+Self, we generate pseudo-transcripts for unlabeled videos and add a subset of them to the weakly-labeled set in epoch 80, 100, 120. Notice that at epoch 60, adding unlabeled videos significantly improves the performance. For SWSL (right), the model converges.\\n\\nQualitative Results. Figure 6 shows the segmentation of a test video from the task 'make scrambled egg' in the Breakfast dataset. This is a challenging case where the transcript of the test video is not present in the weakly-labeled training videos. WSL or WSL+Self will produce transcripts very different from the ground-truth, while the transcript predicted by SWSL or SWSL+Self is more close to the ground-truth. Furthermore, compared with SWSL, the boundary localization of SWSL+Self is more accurate, which shows the advantages of self-training.\\n\\n6. Conclusions\\n\\nWe studied the new problem of semi-weakly supervised action learning from instructional videos. We proposed a Soft Restricted Edit distance that leverages unlabeled videos for training by encouraging the transcript of unlabeled videos to be close, yet possibly different, from those of the weakly-labeled videos of the same task. Our experiments on two datasets showed that our proposed framework significantly improves the performance.\\n\\nAcknowledgements\\n\\nThis work is sponsored by DARPA PTG (HR00112220001), NSF (IIS-2115110), ARO (W911NF2110276) and ONR (N000141812132). Content does not necessarily reflect the position/policy of the Government. No official endorsement should be inferred.\"}"}
{"id": "CVPR-2022-1685", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"References\\n\\n[1] J. B. Alayrac, P. Bojanowski, N. Agrawal, J. Sivic, I. Laptev, and S. Lacoste-Julien. Unsupervised learning from narrated instruction videos. IEEE Conference on Computer Vision and Pattern Recognition, 2016.\\n\\n[2] Alexandr Andoni and Krzysztof Onak. Approximating edit distance in near-linear time. SIAM Journal on Computing, 2012.\\n\\n[3] Philip Bachman, Ouais Alsharif, and Doina Precup. Learning with pseudo-ensembles. Advances in neural information processing systems, 2014.\\n\\n[4] Jing Bi, Jiebo Luo, and Chenliang Xu. Procedure planning in instructional videos via contextual modeling and model-based policy learning. IEEE International Conference on Computer Vision, 2021.\\n\\n[5] P. Bojanowski, R. Lajugie, F. Bach, I. Laptev, J. Ponce, C. Schmid, and J. Sivic. Weakly supervised action labeling in videos under ordering constraints. European Conference on Computer Vision, 2014.\\n\\n[6] J. Carreira and A. Zisserman. Quo vadis, action recognition? a new model and the kinetics dataset. In IEEE Conference on Computer Vision and Pattern Recognition, 2017.\\n\\n[7] Chien-Yi Chang, De-An Huang, Yanan Sui, Li Fei-Fei, and Juan Carlos Niebles. D3tw: Discriminative differentiable dynamic time warping for weakly supervised action alignment and segmentation. IEEE Conference on Computer Vision and Pattern Recognition, 2019.\\n\\n[8] Xiaobin Chang, Frederick Tung, and Greg Mori. Learning discriminative prototypes with dynamic time warping. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021.\\n\\n[9] M. Cuturi and M. Blondel. Soft-dtw: a differentiable loss function for time-series. International Conference on Machine learning, 2017.\\n\\n[10] Dima Damen, Hazel Doughty, Giovanni Maria Farinella, Sanja Fidler, Antonino Furnari, Evangelos Kazakos, Davide Moltisanti, Jonathan Munro, Toby Perrett, Will Price, and Michael Wray. The epic-kitchens dataset: Collection, challenges and baselines. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2020.\\n\\n[11] Fred J Damerau. A technique for computer detection and correction of spelling errors. Communications of the ACM, 1964.\\n\\n[12] Li Ding and Chenliang Xu. Weakly-supervised action segmentation with iterative soft boundary assignment. IEEE Conference on Computer Vision and Pattern Recognition, 2018.\\n\\n[13] Nikita Dvornik, Isma Hadji, Konstantinos G Derpanis, Animesh Garg, and Allan D Jepson. Drop-dtw: Aligning common signal between sequences while dropping outliers. Neural Information Processing Systems, 2021.\\n\\n[14] E. Elhamifar and D. Huynh. Self-supervised multi-task procedure learning from instructional videos. European Conference on Computer Vision, 2020.\\n\\n[15] E. Elhamifar and Z. Naing. Unsupervised procedure learning via joint dynamic summarization. International Conference on Computer Vision, 2019.\\n\\n[16] Mohsen Fayyaz and Jurgen Gall. Sct: Set constrained temporal transformer for set supervised action segmentation. IEEE Conference on Computer Vision and Pattern Recognition, 2020.\\n\\n[17] Ryan Gabrys, Eitan Yaakobi, and Olgica Milenkovic. Codes in the damerau distance for deletion and adjacent transposition correction. IEEE Transactions on Information Theory, 2017.\\n\\n[18] Karan Goel and Emma Brunskill. Learning procedural abstractions and evaluating discrete latent temporal structure. International Conference on Learning Representation, 2019.\\n\\n[19] Isma Hadji, Konstantinos G. Derpanis, and Allan D. Jepson. Representation learning via global temporal alignment and cycle-consistency. IEEE Conference on Computer Vision and Pattern Recognition, 2021.\\n\\n[20] Sanjay Haresh, Sateesh Kumar, Huseyin Coskun, Shahram N. Syed, Andrey Konin, M. Zeeshan Zia, and Quoc-Huy Tran. Learning by aligning videos in time. IEEE Conference on Computer Vision and Pattern Recognition, 2021.\\n\\n[21] Jisoo Jeong, Seungeui Lee, Jeesoo Kim, and Nojun Kwak. Consistency-based semi-supervised learning for object detection. Neural Information Processing Systems, 2019.\\n\\n[22] Satoshi Koide, Keisuke Kawano, and Takuro Kutsuna. Neural edit operations for biological sequences. Advances in Neural Information Processing Systems, 2018.\\n\\n[23] H. Kuehne, A. Arslan, and T. Serre. The language of actions: Recovering the syntax and semantics of goal-directed human. IEEE Conference on Computer Vision and Pattern Recognition, 2014.\\n\\n[24] H. Kuehne, J. Gall, and T. Serre. An end-to-end generative framework for video segmentation and recognition. IEEE Winter Conference on Applications of Computer Vision, 2016.\\n\\n[25] Anna Kukleva, Hilde Kuehne, Fadime Sener, and Jurgen Gall. Unsupervised learning of action classes with continuous temporal embedding. IEEE Conference on Computer Vision and Pattern Recognition, 2019.\\n\\n[26] Samuli Laine and Timo Aila. Temporal ensembling for semi-supervised learning. International Conference on Learning Representations, 2017.\\n\\n[27] C. Lea, M. D. Flynn, R. Vidal, A. Reiter, and G. D. Hager. Temporal convolutional networks for action segmentation and detection. IEEE Conference on Computer Vision and Pattern Recognition, 2017.\\n\\n[28] Dong-Hyun Lee et al. Pseudo-label: The simple and efficient semi-supervised learning method for deep neural networks. Workshop on challenges in representation learning, ICML, 2019.\\n\\n[29] V. I. Levenshtein. Binary Codes Capable of Correcting Deletions, Insertions and Reversals. Soviet Physics Doklady, 10, 1966.\\n\\n[30] J. Li, P. Lei, and S. Todorovic. Weakly supervised energy-based learning for action segmentation. International Conference on Computer Vision, 2019.\"}"}
{"id": "CVPR-2022-1685", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Jun Li and Sinisa Todorovic. Set-constrained viterbi for set-supervised action segmentation. IEEE Conference on Computer Vision and Pattern Recognition, 2020. 1, 2\\n\\nJ. Li and S. Todorovic. Anchor-constrained viterbi for set-supervised action segmentation. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 1\\n\\nZhe Li, Yazan Abu Farha, and Jurgen Gall. Temporal action segmentation from timestamp supervision. IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2021. 1\\n\\nShaoshi Ling, Yuzong Liu, Julian Salazar, and Katrin Kirchhoff. Deep contextualized acoustic representations for semi-supervised speech recognition. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020. 2\\n\\nZ. Lu and E. Elhamifar. Weakly-supervised action segmentation and alignment via transcript-aware union-of-subspaces learning. International Conference on Computer Vision, 2021. 1, 2, 6, 7\\n\\nZ. Lu and E. Elhamifar. Set-supervised action learning in procedural task videos via pairwise order consistency. IEEE Conference on Computer Vision and Pattern Recognition, 2022. 1, 2\\n\\nA. Miech, J-B. Alayrac, L. Smaira, I. Laptev, J. Sivic, and A. Zisserman. End-to-end learning of visual representations from uncurated instructional videos. IEEE Conference on Computer Vision and Pattern Recognition, 2020. 1\\n\\nA. Miech, D. Zhukov, J. B. Alayrac, M. Tapaswi, I. Laptev, and J. Sivic. Howto100m: Learning a text-video embedding by watching hundred million narrated video clips. International Conference on Computer Vision, 2019. 1\\n\\nZ. Naing and E. Elhamifar. Procedure completion by learning from partial summaries. British Machine Vision Conference, 2020. 2\\n\\nSaul B Needleman and Christian D Wunsch. A general method applicable to the search for similarities in the amino acid sequence of two proteins. Journal of molecular biology, 1970. 2\\n\\nGeorge Papandreou, Liang-Chieh Chen, Kevin P Murphy, and Alan L Yuille. Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation. IEEE International Conference on Computer Vision, 2015. 2\\n\\nDario Pavllo, Christoph Feichtenhofer, David Grangier, and Michael Auli. 3d human pose estimation in video with temporal convolutions and semi-supervised training. IEEE Conference on Computer Vision and Pattern Recognition, 2019. 2\\n\\nA. Richard, H. Kuehne, and J. Gall. Action sets: Weakly supervised action segmentation without ordering constraints. IEEE Conference on Computer Vision and Pattern Recognition, 2018. 1, 2\\n\\nA. Richard, H. Kuehne, A. Iqbal, and J. Gall. Neural network-viterbi: A framework for weakly supervised video learning. IEEE Conference on Computer Vision and Pattern Recognition, 2018. 1, 2\\n\\nM. Rohrbach, S. Amin, M. Andriluka, and B. Schiele. A database for fine grained activity detection of cooking activities. IEEE Conference on Computer Vision and Pattern Recognition, 2012. 1, 2\\n\\nDevendra Singh Sachan, Manzil Zaheer, and Ruslan Salakhutdinov. Revisiting lstm networks for semi-supervised text classification via mixed objective function. AAAI Conference on Artificial Intelligence, 2019. 2\\n\\nH. Sakoe and S. Chiba. Dynamic programming algorithm optimization for spoken word recognition. IEEE Transactions on Acoustics, Speech and Signal Processing, 26, 1978. 2\\n\\nFadime Sener and Angela Yao. Unsupervised learning and segmentation of complex activities from video. IEEE Conference on Computer Vision and Pattern Recognition, 2018. 2\\n\\nFadime Sener and Angela Yao. Zero-shot anticipation for instructional activities. International Conference on Computer Vision, 2019. 1, 2\\n\\nY. Shen, L. Wang, and E. Elhamifar. Learning to segment actions from visual and language instructions via differentiable weak sequence alignment. IEEE Conference on Computer Vision and Pattern Recognition, 2021. 1, 2, 7\\n\\nG. A. Sigurdsson, S. Divvala, A. Farhadi, and A. Gupta. Asynchronous temporal fields for action recognition. IEEE Conference on Computer Vision and Pattern Recognition, 2017. 1, 2\\n\\nAnkit Singh, Omprakash Chakraborty, Ashutosh Varshney, Rameswar Panda, Rogerio Feris, Kate Saenko, and Abir Das. Semi-supervised action recognition with temporal contrastive learning. IEEE Conference on Computer Vision and Pattern Recognition, 2021. 2\\n\\nB. Singh, T. K. Marks, M. Jones, O. Tuzel, and M. Shao. A multi-stream bi-directional recurrent neural network for finegrained action detection. IEEE Conference on Computer Vision and Pattern Recognition, 2016. 1, 2\\n\\nYaser Souri, Mohsen Fayyaz, Luca Minciullo, Gianpiero Francesca, and Juergen Gall. Fast Weakly Supervised Action Segmentation Using Mutual Consistency. PAMI, 2021. 2, 4, 7\\n\\nBing Su and Gang Hua. Order-preserving wasserstein distance for sequence matching. IEEE Conference on Computer Vision and Pattern Recognition, 2017. 2\\n\\nYansong Tang, Dajun Ding, Yongming Rao, Yu Zheng, Danyang Zhang, Lili Zhao, Jiwen Lu, and Jie Zhou. Coin: A large-scale dataset for comprehensive instructional video analysis. IEEE Conference on Computer Vision and Pattern Recognition, 2019. 1\\n\\nAntti Tarvainen and Harri Valpola. Mean teachers are better role models: Weight-averaged consistency targets improve semi-supervised deep learning results. Advances in neural information processing systems, 2017. 2\\n\\nH. Wang and C. Schmid. Action recognition with improved trajectories. International Conference on Computer Vision, 2013. 7\\n\\nXiang Wang, Shiwei Zhang, Zhiwu Qing, Yuanjie Shao, Changxin Gao, and Nong Sang. Self-supervised learning for 3353.\"}"}
{"id": "CVPR-2022-1685", "page_num": 11, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[60] C. Xu and E. Elhamifar. Deep supervised summarization: Algorithm and application to learning instructions. Neural Information Processing Systems, 2019.\\n\\n[61] Jie Yan, Yan Song, Li-Rong Dai, and Ian McLoughlin. Task-aware mean teacher method for large scale weakly labeled semi-supervised sound event detection. IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2020.\\n\\n[62] Pengxiang Yan, Guanbin Li, Yuan Xie, Zhen Li, Chuan Wang, Tianshui Chen, and Liang Lin. Semi-supervised video salient object detection using pseudo-labels. IEEE International Conference on Computer Vision, 2019.\\n\\n[63] Hai Ye and Lu Wang. Semi-supervised learning for neural keyphrase generation. Empirical Methods in Natural Language Processing, 2018.\\n\\n[64] S. Yeung, O. Russakovsky, N. Jin, M. Andriluka, G. Mori, and L. Fei-Fei. Every moment counts: Dense detailed labeling of actions in complex videos. International Journal of Computer Vision, 2018.\\n\\n[65] Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and Lucas Beyer. S4l: Self-supervised semi-supervised learning. IEEE International Conference on Computer Vision, 2019.\\n\\n[66] Luowei Zhou, Chenliang Xu, and Jason J Corso. Towards automatic learning of procedures from web instructional videos. AAAI, 2018.\\n\\n[67] D. Zhukov, J. B. Alayrac, R. G. Cinbis, D. Fouhey, I. Laptev, and J. Sivic. Cross-task weakly supervised learning from instructional videos. IEEE Conference on Computer Vision and Pattern Recognition, 2019.\"}"}
