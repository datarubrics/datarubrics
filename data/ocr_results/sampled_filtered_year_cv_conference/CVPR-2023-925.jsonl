{"id": "CVPR-2023-925", "page_num": 9, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"[1] Ehsan Amid, Manfred KK Warmuth, Rohan Anil, and Tomer Koren. Robust bi-tempered logistic loss based on bregman divergences. Advances in Neural Information Processing Systems, 32, 2019.\\n\\n[2] Ehsan Amid, Manfred K Warmuth, and Sriram Srinivasan. Two-temperature logistic regression based on the tsallis divergence. In The 22nd International Conference on Artificial Intelligence and Statistics, pages 2388\u20132396. PMLR, 2019.\\n\\n[3] E. Arazo, D. Ortego, P. Albert, N. E. O'Connor, and K. McGuinness. Unsupervised label noise modeling and loss correction. International Conference on Machine Learning, 2019.\\n\\n[4] Noga Bar, Tomer Koren, and Raja Giryes. Multiplicative reweighting for robust neural network optimization. arXiv preprint arXiv:2102.12192, 2021.\\n\\n[5] Alan Joseph Bekker and Jacob Goldberger. Training deep neural-networks based on unreliable labels. In 2016 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 2682\u20132686. IEEE, 2016.\\n\\n[6] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas Papernot, Avital Oliver, and Colin A Raffel. Mixmatch: A holistic approach to semi-supervised learning. Advances in Neural Information Processing Systems, 32, 2019.\\n\\n[7] Pengfei Chen, Ben Ben Liao, Guangyong Chen, and Shengyu Zhang. Understanding and utilizing deep neural networks trained with noisy labels. In International Conference on Machine Learning, pages 1062\u20131070. PMLR, 2019.\\n\\n[8] Aritra Ghosh, Himanshu Kumar, and PS Sastry. Robust loss functions under label noise for deep neural networks. In Proceedings of the AAAI conference on artificial intelligence, volume 31, 2017.\\n\\n[9] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139\u2013144, 2020.\\n\\n[10] Bo Han, Quanming Yao, Xingrui Yu, Gang Niu, Miao Xu, Weihua Hu, Ivor Tsang, and Masashi Sugiyama. Co-teaching: Robust training of deep neural networks with extremely noisy labels. volume 31, 2018.\\n\\n[11] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 770\u2013778, 2016.\\n\\n[12] D. Jia, D. Wei, R. Socher, L. J. Li, L. Kai, and F. F. Li. Imagenet: A large-scale hierarchical image database. Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 248\u2013255, 2009.\\n\\n[13] Lu Jiang, Zhengyuan Zhou, Thomas Leung, Li-Jia Li, and Li Fei-Fei. Mentornet: Learning data-driven curriculum for very deep neural networks on corrupted labels. In International Conference on Machine Learning, pages 2304\u20132313. PMLR, 2018.\\n\\n[14] Yongcheng Jing, Yining Mao, Yiding Yang, Yibing Zhan, Mingli Song, Xinchao Wang, and Dacheng Tao. Learning graph neural networks for image style transfer. In ECCV, 2022.\\n\\n[15] Yongcheng Jing, Yiding Yang, Xinchao Wang, Mingli Song, and Dacheng Tao. Amalgamating knowledge from heterogeneous graph neural networks. In CVPR, 2021.\\n\\n[16] Yongcheng Jing, Yiding Yang, Xinchao Wang, Mingli Song, and Dacheng Tao. Meta-aggregator: learning to aggregate for 1-bit graph neural networks. In ICCV, 2021.\\n\\n[17] Nazmul Karim, Mamshad Nayeem Rizve, Nazanin Rahnavard, Ajmal Mian, and Mubarak Shah. Unicon: Combating label noise through uniform selection and contrastive learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 9676\u20139686, 2022.\\n\\n[18] Youngdong Kim, Junho Yim, Juseung Yun, and Junmo Kim. Nlnl: Negative learning for noisy labels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 101\u2013110, 2019.\\n\\n[19] A Krizhevsky. Learning multiple layers of features from tiny images. Master's thesis, University of Toronto, 2009.\\n\\n[20] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton. Imagenet classification with deep convolutional neural networks. Communications of the ACM, 60(6):84\u201390, 2017.\\n\\n[21] Abhishek Kumar and Ehsan Amid. Constrained instance and class reweighting for robust learning under label noise. arXiv preprint arXiv:2111.05428, 2021.\\n\\n[22] Junnan Li, Richard Socher, and Steven CH Hoi. Dividemix: Learning with noisy labels as semi-supervised learning. 2019.\\n\\n[23] Junnan Li, Yongkang Wong, Qi Zhao, and Mohan S Kankanhalli. Learning to learn from noisy labeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 5051\u20135059, 2019.\\n\\n[24] Wen Li, Limin Wang, Wei Li, Eirikur Agustsson, and Luc Van Gool. Webvision database: Visual learning and understanding from web data. arXiv preprint arXiv:1708.02862, 2017.\\n\\n[25] Sheng Liu, Jonathan Niles-Weed, Narges Razavian, and Carlos Fernandez-Granda. Early-learning regularization prevents memorization of noisy labels. Advances in Neural Information Processing Systems, 33:20331\u201320342, 2020.\\n\\n[26] Yang Liu and Hongyi Guo. Peer loss functions: Learning from noisy labels without knowing noise rates. In International Conference on Machine Learning, pages 6226\u20136236. PMLR, 2020.\\n\\n[27] Yueming Lyu and Ivor W Tsang. Curriculum loss: Robust learning and generalization against label corruption. arXiv preprint arXiv:1905.10045, 2019.\\n\\n[28] Xingjun Ma, Hanxun Huang, Yisen Wang, Simone Romano, Sarah Erfani, and James Bailey. Normalized loss functions for deep learning with noisy labels. In International Conference on Machine Learning, pages 6543\u20136553. PMLR, 2020.\\n\\n[29] Negin Majidi, Ehsan Amid, Hossein Talebi, and Manfred K Warmuth. Exponentiated gradient reweighting for robust training under label noise and beyond. arXiv preprint arXiv:2104.01493, 2021.\"}"}
{"id": "CVPR-2023-925", "page_num": 10, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"Eran Malach and Shai Shalev-Shwartz. Decoupling \u201cwhen to update\u201d from \u201chow to update\u201d. volume 30, 2017.\\n\\nNaresh Manwani and PS Sastry. Noise tolerance under risk minimization. IEEE transactions on cybernetics, 43(3):1146\u20131151, 2013.\\n\\nIshan Misra, C Lawrence Zitnick, Margaret Mitchell, and Ross Girshick. Seeing through the human reporting bias: Visual classifiers from noisy human-centric labels. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 2930\u20132939, 2016.\\n\\nDuc Tam Nguyen, Chaithanya Kumar Mummadi, Thi Phuong Nhung Ngo, Thi Hoai Phuong Nguyen, Laura Beggel, and Thomas Brox. Self: Learning to filter noisy labels with self-ensembling. 2019.\\n\\nHyeonwoo Noh, Seunghoon Hong, and Bohyung Han. Learning deconvolution network for semantic segmentation. In Proceedings of the IEEE International Conference on Computer Vision, pages 1520\u20131528, 2015.\\n\\nCurtis G Northcutt, Anish Athalye, and Jonas Mueller. Pervasive label errors in test sets destabilize machine learning benchmarks. 2021.\\n\\nJoseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 779\u2013788, 2016.\\n\\nHwanjun Song, Minseok Kim, and Jae-Gil Lee. Selfie: Refurbishing unclean samples for robust deep learning. In International Conference on Machine Learning, pages 5907\u20135915. PMLR, 2019.\\n\\nHwanjun Song, Minseok Kim, Dongmin Park, Yooju Shin, and Jae-Gil Lee. Learning from noisy labels with deep neural networks: A survey. IEEE Transactions on Neural Networks and Learning Systems, 2022.\\n\\nHaoliang Sun, Chenhui Guo, Qi Wei, Zhongyi Han, and Yi-long Yin. Learning to rectify for robust learning with noisy labels. Pattern Recognition, 124:108467, 2022.\\n\\nCheng Tan, Jun Xia, Lirong Wu, and Stan Z Li. Co-learning: Learning from noisy labels with self-supervision. In Proceedings of the 29th ACM International Conference on Multimedia, pages 1405\u20131413, 2021.\\n\\nD. Tanaka, D. Ikami, T. Yamasaki, and K. Aizawa. Joint optimization framework for learning with noisy labels. IEEE, 2018.\\n\\nRyutaro Tanno, Ardavan Saeedi, Swami Sankaranarayanan, Daniel C Alexander, and Nathan Silberman. Learning from noisy labels by regularized estimation of annotator confusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11244\u201311253, 2019.\\n\\nX. Wang, Y. Hua, E. Kodirov, and N. M. Robertson. Image for noise-robust learning: Mean absolute error does not treat examples equally and gradient magnitude's variance matters. 2019.\\n\\nYisen Wang, Xingjun Ma, Zaiyi Chen, Yuan Luo, Jinfeng Yi, and James Bailey. Symmetric cross entropy for robust learning with noisy labels. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 322\u2013330, 2019.\\n\\nZhuowei Wang, Jing Jiang, Bo Han, Lei Feng, Bo An, Gang Niu, and Guodong Long. Seminll: A framework of noisy-label learning by semi-supervised learning. arXiv preprint arXiv:2012.00925, 2020.\\n\\nHongxin Wei, Lei Feng, Xiangyu Chen, and Bo An. Combatting noisy labels by agreement: A joint training method with co-regularization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13726\u201313735, 2020.\\n\\nHan Xiao, Kashif Rasul, and Roland Vollgraf. Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. arXiv preprint arXiv:1708.07747, 2017.\\n\\nYoujiang Xu, Linchao Zhu, Lu Jiang, and Yi Yang. Faster meta update strategy for noise-robust deep learning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 144\u2013153, 2021.\\n\\nXingyi Yang, Zhou Daquan, Songhua Liu, Jingwen Ye, and Xinchao Wang. Deep model reassembly. In Advances in Neural Information Processing Systems.\\n\\nXingyi Yang, Jingwen Ye, and Xinchao Wang. Factorizing knowledge in neural networks. In Computer Vision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October 23\u201327, 2022, Proceedings, Part XXXIV, pages 73\u201391. Springer, 2022.\\n\\nK. Yi and J. Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019.\\n\\nKun Yi and Jianxin Wu. Probabilistic end-to-end noise correction for learning with noisy labels. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7017\u20137025, 2019.\\n\\nXingrui Yu, Bo Han, Jiangchao Yao, Gang Niu, Ivor Tsang, and Masashi Sugiyama. How does disagreement help generalization against label corruption? pages 7164\u20137173, 2019.\\n\\nZhilu Zhang and Mert Sabuncu. Generalized cross entropy loss for training deep neural networks with noisy labels. volume 31, 2018.\\n\\nGuoqing Zheng, Ahmed Hassan Awadallah, and Susan Dumais. Meta label correction for noisy label learning. In Proceedings of the 35th AAAI Conference on Artificial Intelligence, 2021.\\n\\nTianyi Zhou, Shengjie Wang, and Jeff Bilmes. Robust curriculum learning: from clean label detection to noisy label self-correction. In International Conference on Learning Representations, 2020.\"}"}
{"id": "CVPR-2023-925", "page_num": 1, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"How to Prevent the Continuous Damage of Noises to Model Training?\\n\\nXiaotian Yu, Yang Jiang, Tianqi Shi, Zunlei Feng, Yuexuan Wang, Mingli Song, Li Sun\\n\\nZhejiang University, Shanghai Institute for Advanced Study of Zhejiang University, Ningbo Innovation Center Zhejiang University, University of Hong Kong, Alibaba Group\\n\\nAbstract\\n\\nDeep learning with noisy labels is challenging and inevitable in many circumstances. Existing methods reduce the impact of mislabeled samples by reducing loss weights or screening, which highly rely on the model\u2019s superior discriminative power for identifying mislabeled samples. However, in the training stage, the trainee model is imperfect and will wrongly predict some mislabeled samples, which cause continuous damage to the model training. Consequently, there is a large performance gap between existing anti-noise models trained with noisy samples and models trained with clean samples. In this paper, we put forward a Gradient Switching Strategy (GSS) to prevent the continuous damage of mislabeled samples to the classifier. Theoretical analysis shows that the damage comes from the misleading gradient direction computed from the mislabeled samples. The trainee model will deviate from the correct optimization direction under the influence of the accumulated misleading gradient of mislabeled samples. To address this problem, the proposed GSS alleviates the damage by switching the gradient direction of each sample based on the gradient direction pool, which contains all-class gradient directions with different probabilities. During training, each gradient direction pool is updated iteratively, which assigns higher probabilities to potential principal directions for high-confidence samples. Conversely, uncertain samples are forced to explore in different directions rather than mislead model in a fixed direction. Extensive experiments show that GSS can achieve comparable performance with a model trained with clean data. Moreover, the proposed GSS is pluggable for existing frameworks. This idea of switching gradient directions provides a new perspective for future noisy-label learning.\\n\\n1. Introduction\\n\\nRecently, Deep Neural Networks (DNNs) have achieved breakthrough results across various computer vision tasks [9, 14\u201316, 20, 34, 36, 49, 50]. The high performance of DNNs requires a large amount of labeled data, but it is hard to guarantee label quality in many circumstances. As a matter of fact, many benchmark datasets inevitably contain noisy labels according to investigation results in [35]. Various types of researches are proposed to address the noisy-label problem. The mainstream types are robust loss function [18, 27, 44, 54] and sample screening [30, 37, 40]. These methods deal with mislabeled samples in essentially similar ways, that is, by decreasing the weights of low-confidence samples, which highly rely on the trainee model\u2019s discriminative power of identifying mislabeled samples. However, during training the trainee model is imperfect and will miss many mislabeled samples, which will continuously damage the model. That is why there is a large performance gap between existing anti-noise models trained with noisy samples and models trained with clean samples. As shown in Fig. 1, existing anti-noise methods (denoted by solid lines) have a 61% \u223c 9.81% accuracy gaps compared with the reference upper limit (red dash line), which denotes the performance of models trained with clean samples. It raises an important question: how to prevent the continuous damage of noises to model training?\\n\\nThe theoretical analysis in Section 4 shows that the noise damage comes from the misleading gradient direction.\"}"}
{"id": "CVPR-2023-925", "page_num": 2, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"tions caused by noises. Therefore, it is a viable solution to handle the continuous damage of noises to model training by eliminating the impact of misleading gradient directions.\\n\\nIn this paper, we propose a Gradient Switching Strategy (GSS) to prevent the continuous damage of mislabeled samples to the model training. The core idea is assigning a random gradient direction to cancel out the negative impact of mislabeled samples, especially for uncertain samples which could continuously generate a misleading gradient in a single direction. For high-confidence samples, the model will be optimized using their potential principal directions with a larger probability. As the model's discriminative power grows over training time, parts of uncertain samples will become high-confidence samples, which in turn optimize the model with their potential principal directions. Finally, the model will be well-trained with almost all samples in the dataset step by step.\\n\\nSpecifically, we devise a gradient direction pool for each sample, which contains all-class gradient directions with different probabilities. The probabilities of different gradient directions are determined based on the original noisy label, predictions, and partial randomness. In the training stage, for uncertain samples, the probabilities of different gradient directions are dominated by randomness. The multiple random gradient directions prevent a fixed misdIRECTION from continuously damaging the training.\\n\\nThe high-confidence samples consist of two groups: the predictions are consistent with original labels (consistent sample), and the predictions are not consistent with original labels (non-consistent sample). For consistent samples, the gradient direction of the original label (potential principal direction) has a higher probability than those for the remaining gradient directions. For non-consistent samples, two highest probabilities correspond to the gradient directions of the original label and model prediction. The model explores two gradient directions and determines the potential principal direction during training. In summary, the potential principal directions of high-confidence samples guide the optimization of the model.\\n\\nExperiment results demonstrate that the proposed GSS can effectively prevent the damage of mislabeled samples to the model training. The proposed GSS is pluggable for existing frameworks for noisy-label learning, which can achieve $1.23\\\\% \\\\sim 9.22\\\\%$ accuracy improvement than SOTA for high noise rates. Additionally, the model with GSS trained on noisy samples can achieve comparable performance with models trained with clean samples.\\n\\nOverall, our contributions are summarized as follows:\\n\\n\u2022 This paper is the first to clarify the continuous damage of the mislabeled samples to model training. Theoretical analysis shows the continuous damage comes from the misleading gradient direction derived from mislabeled samples, which provides a new perspective for future noisy-label learning research.\\n\\n\u2022 We propose the Gradient Switching Strategy (GSS) to prevent the continuous gradient damage of mislabeled samples to the model training. A gradient direction pool containing gradient directions of all classes with dynamic probabilities for each sample is devised to alleviate the impact of uncertain samples and optimize the model with the potential principal direction.\\n\\n\u2022 Detailed theoretical analysis and extensive experimental results show that the proposed GSS can effectively prevent damage of mislabeled samples. Through combining GSS with existing anti-noise learning methods, the final classification performance can achieve up to $1.23\\\\% \\\\sim 9.22\\\\%$ accuracy improvement over SOTA on datasets with severe noise, some of which are even comparable to the model trained with clean samples.\\n\\n2. Related Work\\n\\nPrevious researches have proposed various methods for noisy-label learning, such as label correction [39, 41, 51], noisy adaptation [5, 32, 42], and meta learning [48, 52, 55]. Among existing methods, the mainstream in this field includes robust loss function and sample cleaning, which have been proven effective in various tasks with noisy labels [38].\\n\\nRobust loss function based methods theoretically prove that the classifier trained with noisy data can achieve the same misclassification probability as that trained with clean data [3,8,31,43]. Symmetric Cross Entropy (SCE) [44] was proposed to address the under-fitting and over-fitting problems with noisy datasets. Some researchers [1, 2] proposed replacing the Softmax layer with the exponential functions, which allows transitioning between non-convex and convex losses by different temperature parameters. Based on the discovery that DNNs are robust to noisy labels in the early learning stage, Early-learning Regularization (ELR) [25] was proposed to take past model outputs as targets for improving the robustness of models. Active Passive Loss (APL) [28] proposed a normalization function to make any loss robust to noisy labels. Compared to Cross Entropy (CE), these loss functions have an anti-noise effect on model training. However, even though these losses have been theoretically proven to be robust, there is still a big gap between the experimental performance on clean data and that on noisy data. As we analyzed in Section 4.2, the effects of most robust losses essentially reduce the gradient weight of uncertain samples. Although the model is less corrupted by the noise, these methods also reduce the weights of hard samples and result in low generalization.\\n\\nSample cleaning is another common technique devised for handling noisy labels, which is a special case of sample weighting (with binary weights) [4,21,29]. MentorNet [13] attempted to learn data-driven curriculums by re-weighting\"}"}
{"id": "CVPR-2023-925", "page_num": 3, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"\u03b7\\ntion of the noisy dataset is\\nthe dataset is noisy if\\nlabels. Assuming the true label of the\\nn\\n\\\\[\\n\\\\text{y} \\\\quad \\\\text{where} \\\\quad \\\\text{N}\\n\\\\]\\n\\ne\\n\\ne\\ne\\n\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\n\\ne\\ne\\ne\\ne\\ne\\ne\\n\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\n\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\ne\\n\\ne\\ne\\n\\ne\\ne\\ne\\n\\ne\\ne\\n\\ne\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\ne\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n\\n\\\\[\\n\\\\text{L}\\n\\\\]\\n"}
{"id": "CVPR-2023-925", "page_num": 4, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"many methods are essentially similar, including sample cleaning, reweighting, and robust loss. Sample cleaning with various strategies removes the uncertain samples, equivalent to reweighting with a binary weight. For robust loss functions, we calculate the gradients and find that they essentially reduce the gradient weight of uncertain samples. According to the methodologies of existing methods, we derive their formulas of feature weight in Table 1, where GCE [54], SL [44], ELR [25], and Peer Loss [26] belong to robust loss functions. EG Reweighting [29] and CIW [21] belong to sample reweighting. Co-teaching [10] and DivideMix [22] belong to sample cleaning. The theoretical deductions of above methods and other recent works are given in the supplementary materials. Among these methods, $\\\\alpha$, $\\\\beta$, and $\\\\gamma$ are positive hyper-parameters, $A$ is set to a negative constant to replace $-\\\\log 0$, and $\\\\tau$, $\\\\tau'$, $\\\\tau''$ are cleaning thresholds. In dual branch models Co-teaching and DivideMix, $p$ and $p^*$ denote the prediction of the current branch and the other branch, respectively. GMM denotes the Gaussian mixture model to predict clean samples. Based on the above analyses, it can be seen that these methods have the same characteristics. They are essentially in enhancing or inhibiting the gradient weight term $\\\\frac{\\\\partial L}{\\\\partial z_k}$.\\n\\nThe gradient weight of these methods can be summarized as $W(p_k - q_k)$, where $W$ is positively correlated with the prediction on the annotated label $p_y$. The gradient weight of samples with low confidence would be reduced to avoid the influence of noise. That is how existing methods work for noisy-label learning. However, though these methods avoid the negative effect of the noise, the positive effect of hard samples on the model is also suppressed. That is why there is a big gap between these methods and the model trained by clean data. For methods that reuse uncertain samples through SSL, new noise will be introduced since unreliable predictions are applied to replace the original noisy labels. And the newly added noise is consistent with the model predictions and can be hard to rectify. In a word, for samples that are wrongly distinguished, gradient bias will accumulate. The gap between the model trained by clean and noisy data becomes larger during the training.\\n\\n5. Methodology\\n\\nAs we discussed above, the problem that existing methods have not solved is that the misidentified samples have continuous damage to the model training, which is caused by the bias of wrong gradient direction. In this paper, we propose the Gradient Switching Strategy to address the problem. We introduce the process of the proposed GSS in Section 5.1, the combination details with various frameworks in Section 5.2, and the effect of GSS for noisy-label learning is analyzed in Section 5.3. The pseudo-code given in the supplements illustrates the full process of GSS.\\n\\n| Method         | Formula of gradient weight |\\n|----------------|---------------------------|\\n| Cross Entropy  | $p_k - q_k$               |\\n| GCE [54]      | $-\\\\gamma - \\\\frac{1}{\\\\gamma} (p_y - 1) (p_k - q_k)$ |\\n| SL [44]       | $(\\\\alpha + \\\\beta |A| p_y) (p_k - q_k)$ |\\n| ELR [25]      | $(p_k - q_k) + P_i p_i \\\\hat{p}_i - \\\\hat{p}_k 1 - P_i p_i \\\\theta p_k$ |\\n| Peer Loss [26]| $(p^{(n)}_k - q^{(n)}_k) - (p^{(n)}_k - q^{(n)}_k)$ |\\n| EG Reweighting| $w_{EG} (p_k - q_k)$ |\\n| CIW [21]      | $w_{CIW} (p_k - q_k)$ |\\n| Co-teaching [10]| $1[L (p^*) y < \\\\tau'] (p_k - q_k)$ |\\n| DivideMix [22]| $1[GMM (L (p^*) y) > \\\\tau''] (p_k - q_k)$ |\\n\\nTable 1. The summarized gradient weight of existing methods. Here $q_k = 1[e_y = k]$. \\n\\n5.1. Gradient Switching Strategy\\n\\nConsidering that misidentified samples can cause continuous damage in the same gradient direction, the proposed GSS is to prevent noise damage by gradient switching. Instead of switching the gradient into another fixed direction, the gradient direction pool is conducted for each sample to randomly select directions with different probabilities. The probability of selecting each category depends on the proportion in the direction pool:\\n\\n$$P(b_{\\\\text{ye}}(n) = k) = \\\\frac{D(n)_k}{\\\\sum D(n)_k}, \\\\quad (5)$$\\n\\nwhere $D(n)_k \\\\in \\\\mathbb{R}^K$ denotes the direction pool of the sample $x(n)$, and $b_{\\\\text{ye}}$ denotes the flipped label in the $e$-th iteration. For each sample, the direction pool is used to randomly select the direction in each iteration. And the gradient directions are switched through flipping labels. During training, the gradient direction pool of each sample will update synchronously, and thus appropriate gradient switching strategies can be adjusted at different stages of training. In general, the GSS has two additional processes in each iteration, gradient switching and updating the direction pool.\\n\\nThe updating strategy of the gradient direction pool is crucial to the effect of gradient switching. For samples with high predicted confidence, the gradient will be switched to the principal direction with high probability. The uncertain samples are encouraged to be trained in various directions, rather than in a fixed direction to cause continuous damage. As the model performance improves, these uncertain samples will gradually generate their principal directions and participate in the training. Thus, three types of labels are applied for updating the direction pool, including the original labels $Y_\\\\text{or}$ or $Y$, predicted labels $Y_\\\\text{pr}$, and random labels $Y_\\\\text{rd}$. Among them, $Y_\\\\text{or}$ and $Y_\\\\text{pr}$ are one-hot vectors, and $Y_\\\\text{rd}$ is a $K$-dimensional vector with all values of $1/K$. The corresponding updating weights are denoted as $v_{\\\\text{or}}$, $v_{\\\\text{pr}}$, and $v_{\\\\text{rd}}$, respectively. During training, the updating weights of each\"}"}
{"id": "CVPR-2023-925", "page_num": 5, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"v_{or} = p_{ey}(1 - \\\\frac{e}{E}), (6)\\n\\nv_{pr} = p_{ey}(\\\\lambda_1 \\\\frac{e}{E}), (7)\\n\\nv_{rd} = \\\\lambda_2 \\\\frac{e}{E}, (8)\\n\\nwhere $p_{ey}$ denotes the predicted confidence on noisy labels, $E$ denotes the total amount of epochs, $e$ denotes the current epoch, and $\\\\lambda_1, \\\\lambda_2$ are parameters to determine the importance of two types of labels. The gradient direction pool of each sample is updated by the weighted sum in each epoch:\\n\\n$$D_{[e+1]} = D_{[e]} + Y_{or}v_{or} + Y_{pr}v_{pr} + Y_{rd}v_{rd}, (9)$$\\n\\nwhere $D_{[e]}$ denotes the gradient direction pool of each sample in the $e$-th epoch of the training.\\n\\nThis group of updating weights adjusts the tendency for gradient switching on different training phases. The beginning phase tends to the original labels for fast convergence. With model predictions more accurate than annotated labels, the later phase tends to the other two types of labels. During the model training, the weights of predicted labels are increased to improve label reliability. Also, the random labels are emphasized to prevent continuous damage caused by overfitting in the fixed direction.\\n\\nThus, the gradient direction pool varies on samples with confident and uncertain predictions. Whether the annotated labels are correct or wrong, confident samples have explicit principal directions. So that mislabeled samples with accurate predictions can be trained in correct directions, rather than being removed directly. For uncertain samples, the gradients switch more randomly across all categories, which allows the model to explore in various directions without being affected by the continuous damage. As the model performance improves through training, these uncertain samples can generate their principal directions. In summary, the gradient switching strategy achieves both the utilization of all samples and the prevention of continuous damage.\\n\\n5.2. Combination with Existing Frameworks\\n\\nThe proposed GSS is a pluggable technique, which is simple but can significantly improve the performance of existing methods. We combine our GSS with three kinds of frameworks, including the single branch model, dual branch model, and dual branch model with semi-supervised learning. For the single branch model, the strategy is given in Section 5.1 without additional settings. This section will introduce the combination strategies with the other two combinations with dual branches.\\n\\nFor the GSS with dual branches (GSS-DB), the difference between the two models can further prevent the gradient direction pool updating from being misled by the noise. In GSS-DB, the models of two branches are denoted as $f_1, f_2$. For each sample $x_n$, two gradient direction pools $D_{[n]}^1, D_{[n]}^2$ are conducted, which are used to select gradient directions $b_{[n]}^1, b_{[n]}^2$ for the training of two models, respectively. Moreover, the updating of direction pools use the predicted labels of the other model. So that even if one of the models wrongly predicts some uncertain samples, the corresponding direction pools will be updated by predictions of the other model. Compared with Co-teaching [10], GSS-DB prevents noise damage without filtering samples, making the model trained by more samples.\\n\\nFrom dual branches with semi-supervised learning, DivideMix [22] removes the labels of uncertain samples and trains these samples based on the predictions of the other model. Still, the method of SSL introduces new noise by using predictions as targets. In this way, the newly added noise is consistent with predictions and can hardly be corrected, causing continuous damage to the model learning. Thus, we apply GSS in this type of framework with semi-supervised learning (GSS-SSL). GSS-SSL also applies the dual branches so that the main process is consistent with GSS-DB. Additionally, two updating strategies of gradient direction pools are used for different purposes. The labeled samples are more credible than the unlabeled samples, so $\\\\lambda_1$ is larger than $\\\\lambda_2$ for labeled samples. On the contrary, $\\\\lambda_2$ is larger for unlabeled samples, encouraging these samples to explore various directions to prevent noise damage. More details of GSS combinations are given in the supplements.\\n\\n5.3. Effectiveness analysis of Gradient Switching\\n\\nTo demonstrate the effect of gradient switching, theoretical and experimental analyses are conducted in this section. As discussed above, Eqn. 4 denotes the gradient bias of each sample with the noisy label $e_{y}$ and the clean label $y$. So that the total gradient bias caused by each sample within a small\"}"}
{"id": "CVPR-2023-925", "page_num": 6, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"E can be derived as:\\n\\n$$\\\\Delta g = E X \\\\mu a \\\\otimes d e y - d e y$$\\n\\n(10)\\n\\nwhere $\\\\Delta g$ denotes the gradient bias, $d y$ denotes the short-hand for the gradient direction $\\\\partial z y / \\\\partial m l$, and $\\\\mu$ denotes the learning rate. The layer $l$ is omitted in the interest of brevity.\\n\\nAssuming the activation feature map $a$ and gradient direction $d y$ of the same sample is basically constant in a small number of iterations (experimental demonstrated in supplementary materials), the total bias can be simplified as:\\n\\n$$\\\\Delta g_{ori} = \\\\mu a \\\\otimes E d e y - d e y$$\\n\\n(11)\\n\\nwhere $d e y - d y$ is the fixed bias that misled the model iteratively. Conversely, with unfixed directions of GSS, the total gradient bias can be derived as follows:\\n\\n$$\\\\Delta g_{gss} = \\\\mu a \\\\otimes E X e \\\\mu a \\\\otimes d e y - d e y$$\\n\\n(12)\\n\\nThe difference between fixed and unfixed directions is that the latter replaces the accumulated bias $E (d e y - d y)$ with the summary bias of various directions $P E e (d e y - d y)$.\\n\\nThrough the updating of the gradient direction pool, the selected direction will be more reliable than the original one without continuous bias. That is the reason unfixed labels can be robust to noise.\\n\\nTo analyze the gradient bias in existing methods, we assume there is a method to perfectly distinguish all the noise, which means $W = 0$ for mislabeled samples. So that the gradient bias of the ideal sample learning method $\\\\Delta g_{sc}$ equals the gradients with clean labels and can be derived as:\\n\\n$$\\\\Delta g_{sc} = \\\\mu a \\\\otimes E - X k \\\\partial L (y) \\\\partial z k d k = \\\\mu a \\\\otimes E X k \\\\neq y p k d k - (1 - p y) d y,$$\\n\\n(13)\\n\\nAnd the gradient bias of SSL methods $\\\\Delta g_{ssl}$ can be derived as:\\n\\n$$\\\\Delta g_{ssl} = \\\\mu a \\\\otimes X k \\\\partial L_{ssl} \\\\partial z k - \\\\partial L (e y) \\\\partial z k \\\\partial z k \\\\partial m l,$$\\n\\n(14)\\n\\nwhere $L_{ssl}$ denotes the semi-supervised loss for unlabeled samples with MixMatch [6].\\n\\nExperimental results are conducted in Table 2 to compare the gradient biases among $\\\\Delta g_{ori}$, $\\\\Delta g_{sc}$, $\\\\Delta g_{ssl}$, and $\\\\Delta g_{gss}$. The same term $\\\\mu a$ is omitted for simplicity of calculation. The results indicate that GSS has less bias than sample cleaning ($\\\\Delta g_{sc}$) and SSL ($\\\\Delta g_{ssl}$). It's worth noting that we assume that samples are perfectly distinguished in $\\\\Delta g_{sc}$, so in fact the gradient bias of existing methods is even larger. SSL has a relatively low bias at the early stage, but the bias increases more compared to $\\\\Delta g_{gss}$ and $\\\\Delta g_{sc}$.\\n\\nIt might be due to the added noise by using predictions as targets for mislabeled samples.\\n\\n### 6. Experiments\\n\\n**Dataset.** We evaluate the proposed method on various datasets, including CIFAR-10, CIFAR-100 [19], Clothing1M [47], and WebVision [24]. The first two datasets are widely used benchmarks that only contain clean labels, so we generate simulated noise by symmetric and asymmetric approaches. The symmetric noise is generated by randomly flipping labels with uniform distribution, while the asymmetric noise only occurs between specific categories. Details of the asymmetric noise are provided in supplementary materials. The symmetric noisy labels are generated with a ratio from 40% to 80%, and the asymmetric ones are generated with a ratio from 20% to 40%. Clothing1M and WebVision are datasets with real-world noisy labels. Clothing1M contains a million images with 14 classes of clothing. The dataset is collected from online shopping websites with an overall label accuracy of 61.54%. There are several pairs of confusing classes, making this dataset very challenging. WebVision contains 2.4 million images of 1,000 same classes in ImageNet ILSVRC12 [12], which are crawled from the web with lots of noise. Based on previous works [7, 23], the first 50 classes of the ImageNet subset are used for comparison.\\n\\n**Implementation Details.** ResNet18 [11] is used as the backbone for CIFAR-10 and CIFAR-100, and ResNet50 is used as the backbone for Clothing1M and WebVision. All methods use the same backbones with pre-trained weights. The SGD with a momentum of 0.9 and weight decay of $5 \\\\times 10^{-4}$ is adopted. The batch size is set to 128 for all datasets. The initial learning rate is set to 0.01 for CIFAR-10 and CIFAR-100 and 0.001 for Clothing1M and WebVision. The learning rate is decayed in a cosine annealing manner. The hyper-parameters of existing methods are adjusted according to their papers. For the proposed GSS, the 12059\"}"}
{"id": "CVPR-2023-925", "page_num": 7, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":true,\"is_diagram\":false,\"natural_text\":\"## Table 3. Classification results on CIFAR-10 and CIFAR-100 with different ratios of symmetric/asymmetric noise. The experiment compares our GSS-SSL with GCE \\\\[54\\\\], SL \\\\[44\\\\], ELR+ \\\\[25\\\\], Co-teaching \\\\[10\\\\], JoCoR \\\\[53\\\\], and DivideMix \\\\[22\\\\]. The mean accuracies over five experiments are shown, and the best results are marked in \\\\textbf{bold} (All scores are in %).\\n\\n| Method       | CIFAR-10 20% | CIFAR-10 40% | CIFAR-10 60% | CIFAR-10 80% | CIFAR-100 20% | CIFAR-100 40% | CIFAR-100 60% | CIFAR-100 80% |\\n|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|--------------|\\n| GCE \\\\[54\\\\]  | 88.77 \u00b1 0.18 | 84.66 \u00b1 0.30 | 78.43 \u00b1 0.25 | 66.11 \u00b1 0.27 | 87.28 \u00b1 0.13 | 84.63 \u00b1 0.15 | 82.15 \u00b1 0.27 |            |\\n| SL \\\\[44\\\\]   | 88.98 \u00b1 0.20 | 84.65 \u00b1 0.28 | 78.22 \u00b1 0.25 | 68.53 \u00b1 0.26 | 84.94 \u00b1 0.19 | 80.90 \u00b1 0.22 | 78.71 \u00b1 0.21 |            |\\n| ELR+ \\\\[25\\\\] | 87.77 \u00b1 0.30 | 83.87 \u00b1 0.28 | 79.19 \u00b1 0.30 | 62.01 \u00b1 0.32 | 84.35 \u00b1 0.20 | 82.36 \u00b1 0.22 | 80.56 \u00b1 0.29 |            |\\n| Co-teaching \\\\[10\\\\] | 89.59 \u00b1 0.09 | 87.20 \u00b1 0.20 | 81.40 \u00b1 0.15 | 72.94 \u00b1 0.21 | 85.99 \u00b1 0.12 | 84.23 \u00b1 0.11 | 79.48 \u00b1 0.12 |            |\\n| JoCoR \\\\[53\\\\] | 86.82 \u00b1 0.24 | 85.31 \u00b1 0.22 | 76.50 \u00b1 0.23 | 66.94 \u00b1 0.33 | 86.73 \u00b1 0.18 | 79.84 \u00b1 0.17 | 77.19 \u00b1 0.24 |            |\\n| DivideMix \\\\[22\\\\] | 94.26 \u00b1 0.14 | 92.85 \u00b1 0.19 | 92.26 \u00b1 0.21 | 90.07 \u00b1 0.17 | 92.98 \u00b1 0.15 | 91.57 \u00b1 0.13 | 90.59 \u00b1 0.16 |            |\\n| GSS-SSL (Ours) | 94.31 \u00b1 0.12 | 94.20 \u00b1 0.11 | 92.84 \u00b1 0.25 | 91.61 \u00b1 0.21 | 93.42 \u00b1 0.10 | 92.44 \u00b1 0.12 | 91.82 \u00b1 0.10 |            |\\n\\n## Table 4. The classification accuracies (%) of various methods trained by real-world noisy datasets Clothing1M and WebVision. The mean accuracies over five experiments are shown, and the best results are marked in \\\\textbf{bold} (All scores are in %).\\n\\n| Method       | Clothing1M Top1 | Clothing1M Top5 | WebVision Top1 | WebVision Top5 | ILSVRC12 Top1 | ILSVRC12 Top5 |\\n|--------------|-----------------|-----------------|----------------|----------------|---------------|---------------|\\n| GCE \\\\[54\\\\]  | 71.73           | 59.13           | 80.81          | 79.09          |               |               |\\n| SL \\\\[44\\\\]   | 72.05           | 61.56           | 84.29          | 84.08          |               |               |\\n| ELR+ \\\\[25\\\\] | 71.48           | 60.10           | 83.50          | 83.13          |               |               |\\n| Co-teaching \\\\[10\\\\] | 72.50          | 62.94           | 85.01          | 84.76          |               |               |\\n| JoCoR \\\\[53\\\\] | 71.74           | 57.15           | 82.48          | 81.33          |               |               |\\n| DivideMix \\\\[22\\\\] | 74.59          | 75.23           | 91.60          | 90.76          |               |               |\\n| GSS-SSL (Ours) | 74.88          | 75.18           | 93.09          | 92.84          |               |               |\\n\\n6.1. Quantitative Evaluation\\n\\nThis section evaluates GSS on various benchmark datasets. The synthetic noisy labels are generated randomly on CIFAR-10 and CIFAR-100 to evaluate the effects with different noise ratios. And Clothing1M and WebVision are used to evaluate the performance with real-world noise.\\n\\n**Experiments on CIFAR-10/CIFAR-100:** Table 3 shows test accuracy on CIFAR-10 and CIFAR-100 with synthetic noisy labels. The experiments are conducted with various ratios of symmetric and asymmetric noise to evaluate the performance under different conditions. Overall, the proposed GSS significantly improves over state-of-the-art methods with different noise ratios. Although DivideMix has achieved high accuracy, our method still greatly improves the performance by up to 9.22%.\\n\\nParticularly, the performances of robust loss functions and sample cleaning methods decrease considerably with the increase of the noise ratio. Since a large amount of noise reduces the ability to identify clean samples, the model is more vulnerable to continuous damage. It can be seen that DivideMix is superior to other existing methods, because parts of the label noise are rectified through SSL. But there are still large gaps with the performance trained by clean labels. Our proposed GSS further reduces these gaps across all noise ratios. Especially for CIFAR-10 with symmetric noise less than 40% or asymmetric noise less than 30%, the gaps are reduced to within 0.5%. The possible explanation for the gap of DivideMix could be that the rectified labels by SSL add new noises, which are consistent with model predictions and regarded as clean samples. With GSS, the gradient directions are switched in each iteration. Thus continuous damage caused by wrong predictions can be effectively prevented.\\n\\n**Experiments on Clothing1M/WebVision:** Table 4 shows the results of models trained on Clothing1M and WebVision. Clothing1M is a challenging dataset with a high ratio of real-world noise. Comparing to synthetic noise, some mislabeled samples in Clothing1M contain many similar features and are harder to identify. Thus the performances of existing methods are much closer. Still, GSS achieves 0.29%\u20133.40% improvement over these methods.\\n\\nFor WebVision and ILSVRC12, the validation sets contain the same 50 classes of the ImageNet subset, and both top-1 and top-5 accuracy are shown. It can be seen the GSS improvement on top-5 accuracy is much higher than that on top-1 accuracy. This phenomenon is due to the gradient direction pool allowing the model to be trained in multiple directions for uncertain samples. Instead of minimizing predictions on other categories, GSS will be trained in various possible directions, even if the annotated labels are wrong. Consequently, for uncertain samples that cannot accurately predict, the model has a high probability of predicting correctly.\"}"}
{"id": "CVPR-2023-925", "page_num": 8, "content": "{\"primary_language\":\"en\",\"is_rotation_valid\":true,\"rotation_correction\":0,\"is_table\":false,\"is_diagram\":false,\"natural_text\":\"6.2. Ablation Study\\n\\nEffects of Gradient Direction Pool Updating:\\n\\nIn this section, we conduct an ablation study of different strategies for updating the gradient direction pool. Experiments are conducted with different weights ($\\\\lambda_1$ and $\\\\lambda_2$) to evaluate the sensitivity of these parameters. $\\\\lambda_1$ and $\\\\lambda_2$ are the weights of predicted labels and random labels, respectively. With weights $\\\\lambda_1/\\\\lambda_2 = 0$, the corresponding labels are removed to explore the effects of each component in updating the direction pool. The results are illustrated in Fig. 3.\\n\\nIn Fig. 3, the result with weight $\\\\lambda_1 = 0$ ($\\\\lambda_2 = 1$) is much lower than others, which reflects the importance of predicted labels in updating the direction pool. Without adding predicted labels, the training of each sample loses the principal directions, making the model hard to converge. With the addition of predicted labels ($\\\\lambda_1 > 0$), the performance improves greatly. From the curve of $\\\\lambda_2$, it can be seen the performance is less sensitive than $\\\\lambda_1$. The result with weight $\\\\lambda_2 = 0$ is also the lowest, indicating that the randomness of gradient direction is crucial to GSS. With random labels, the direction pool can select various directions for the model rather than a fixed one, which helps prevent the continuous damage of label noise.\\n\\nEffects of Different Combinations:\\n\\nAiming at the combinations of GSS with various frameworks, experiments are conducted to provide insights into the effects of each component, shown in Fig. 4. GSS-SB, GSS-DB, and GSS-SSL denote GSS with the single branch, double branch, and semi-supervised learning, respectively. On both datasets with 40% and 60% noisy labels, GSS-DB achieves improvement compared with GSS-SB. The reason is the dual gradient direction pool further achieves damage prevention by increasing randomness in selecting gradient directions. Moreover, GSS-SSL further improves the accuracy, and this improvement is more pronounced with 60% noisy labels. GSS-SSL solves the problem of continuous damage from incorrect predictions and enables more efficient learning of uncertain samples through semi-supervised learning. For datasets with severe noise, damage from incorrect predictions and uncertain samples is more common, so GSS-SSL can have a significant improvement for high-noise data.\\n\\n7. Conclusion\\n\\nThis paper makes a deep analysis from a new perspective of gradient directions, demonstrating that label noise can cause continuous damage throughout the model training. Although existing methods improve the performance of noisy-label learning, the damage of misidentified noise leads to suboptimal performance. To address this problem, we put forward GSS to reduce the impact of mislabeled samples. GSS devises a dynamic gradient direction pool for each sample, which contains various gradient directions with different probabilities. With GSS, uncertain samples are forced to explore in different directions rather than mislead model optimization in a fixed direction. Our theoretical analysis demonstrates that GSS can effectively reduce the gradient biases caused by label noise. Validated by comprehensive experiments, our GSS achieves superior accuracy over all existing methods on both synthetic and real-world noisy datasets. Moreover, GSS trained with noisy labels obtains comparable performance with the model trained with clean labels. The proposed GSS effectively prevents noise damage by switching gradient directions, providing a new perspective for future noisy-label learning.\\n\\nAcknowledgements. This work is funded by the National Key Research and Development Project (Grant No: 2022YFB2703100), Starry Night Science Fund of Zhejiang University Shanghai Institute for Advanced Study (Grant No. SN-ZJU-SIAS-001), Ningbo Natural Science Foundation (2022J182), Fundamental Research Funds for the Central Universities (2021FZZX001-23), Alibaba Group through Alibaba Innovative Research Program, Alibaba-Zhejiang University Joint Research Institute of Frontier Technologies, and Zhejiang Lab (No.2019KD0AD01/014).\"}"}
